[{"number": 37494, "title": "Importing tensorflow just after installation", "body": "(base) C:\\Users\\rafra>python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\rafra\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\rafra\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\rafra\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\rafra\\Anaconda3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\rafra\\Anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Users\\rafra\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\rafra\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\rafra\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\rafra\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\rafra\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\rafra\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\rafra\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\rafra\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\rafra\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\rafra\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\rafra\\Anaconda3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\rafra\\Anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "In this case, you need to update MSVC redistributable (install the latest one) and then you can use 2.1.\r\n\r\nClosing as duplicate.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156\r\n\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37494\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37494\">No</a>\n"]}, {"number": 37493, "title": "Fixing graph mkl related tests", "body": "Can you cherry-pick this to r2.2?", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37493) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 37492, "title": "tf.linalg.diag doesn't use \"k\" value", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): ubuntu 16.04\r\n- TensorFlow installed from (source or\r\nbinary): binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: N/A cpu\r\n- GPU model and memory: N/A cpu\r\n\r\n**Describe the current behavior**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\ndiagonal = np.array([[1, 2, 3],  \r\n                     [4, 5, 6]])\r\ntf.linalg.diag(diagonal, k = 1)\r\n```\r\n\r\nresponds with\r\n\r\n```\r\n<tf.Tensor: shape=(2, 3, 3), dtype=int64, numpy=\r\narray([[[1, 0, 0],\r\n        [0, 2, 0],\r\n        [0, 0, 3]],\r\n\r\n       [[4, 0, 0],\r\n        [0, 5, 0],\r\n        [0, 0, 6]]])>\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nAccording to the docs, it should give me\r\n\r\n```\r\n[[[0, 1, 0, 0],  # Output shape: (2, 4, 4)\r\n        [0, 0, 2, 0],\r\n        [0, 0, 0, 3],\r\n        [0, 0, 0, 0]],\r\n       [[0, 4, 0, 0],\r\n        [0, 0, 5, 0],\r\n        [0, 0, 0, 6],\r\n        [0, 0, 0, 0]]]\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nhttps://colab.research.google.com/drive/1Hj6Sks-m_V0gqw6SK9eowpRpIhOlHYXk\r\n\r\nThis notebook shows that, in fact, the `k` argument appears to be ignored completely.  \r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@jacksonloper \r\nThe code is running as expected in nightly version,please find the [gist](https://colab.sandbox.google.com/gist/Saduf2019/85ba436b4ff63317503a1380b30798b8/37492.ipynb) for the same", "@jacksonloper\r\ncould you please update on the above comment", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37492\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37492\">No</a>\n"]}, {"number": 37491, "title": "Resolve GPU delegate issues for OpenGL/OpenCL backends", "body": "Required to resolve accuracy and correctness issues related to the OpenGL and OpenCL backends.", "comments": []}, {"number": 37490, "title": "Please help me to add audio on bounding box", "body": "A Thesis Project, Thanks for helping!\r\n\r\nTensorflow with audio", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37490) for more info**.\n\n<!-- need_sender_cla -->", "This is not a PR. It could have been an issue if it were related to a bug/feature request. It could have been a question for StackOverflow otherwise.\r\n\r\nBut no one is going to write a full thesis just because a \"PR\" is created"]}, {"number": 37489, "title": "UK collection", "body": "                         To update or to get new version", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37489) for more info**.\n\n<!-- need_sender_cla -->", "@kiranchhetri1 thank you for your contribution, please sign CLA.", "What is this supposed to solve?", "@kiranchhetri1 Can you please check @mihaimaruseac's comments and keep us posted. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on."]}, {"number": 37488, "title": "tf.numpy_function with lambda function is much slower starting from TF 1.15", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 18.04 (our machine), and also COLAB.**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **No**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **1.14 vs 1.15.2 vs 2.1.0**\r\n- Python version: **3.6.9**\r\n- Bazel version (if compiling from source): **nope**\r\n- GCC/Compiler version (if compiling from source): **nope**\r\n- CUDA/cuDNN version: **10.1 / 7.6.2**\r\n- GPU model and memory: **V100 (our machine) and T4 in COLAB. But the issue is on the CPU side**\r\n\r\n**Describe the current behavior**\r\nWhen using `tf.numpy_function` (or `tf.compat.v1.py_func`) in combination with a lambda function (in order to pass some fixed, non-tensor parameters to it), performance decreased massively from 1.14 to 1.15 onwards. (depending on the problem and the hardware, in my tests between 2x and 8x; the provided code is about 2x slower on a Colab T4 instance, and 4x slower on a V100 machine; some other much more involved code got slowed down up to 8x; but that's mainly because with slower GPUs, the bottleneck may lie elsewhere; I haven't done specific tests using one line only)\r\n\r\n**Describe the expected behavior**\r\nThe invoked function being quite literally the same, the time should be also virtually identical. And more generally, code shouldn't get slower with a new code version...\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nLink to Colab: https://colab.research.google.com/drive/1MZVOBlAY7piqgeKwGzA8goeU3xtPy5gp\r\n\r\nThe code above runs a full \"training\" pipeline using a keras model, estimators framework, and data generation via numpy_function. All data is created randomly, so the model won't learn anything of course. The main difference is between the line:\r\n\r\n`tf.numpy_function(a_slow_numpy_function, [tensor, [x1, y1, x2, y2]], tf.float32)`\r\n\r\nand the line\r\n\r\n`tf.numpy_function(lambda ex: a_slow_numpy_function(ex, [x1, y1, x2, y2]), [tensor], tf.float32`\r\n\r\nHere, tensor is... a tensor, while x1, y1, x2, y2 are integers. Therefore, both lines are valid, in the first one the list is implicitly converted to a tensor (and then to a numpy array), in the second one a temporary function has those parameters as fixed.\r\n\r\nUntil TF 1.14 both lines produced the same runtime, from TF 1.15 on the second one gets much slower (the first one stays the same).\r\n\r\nHowever, unfortunately I wasn't able to obtain reproducible results by only invoking the line above in a standalone session.\r\n\r\n**Other info / logs** \r\n\r\nLogs on the Colab T4 can be seen from the link (for all TF versions 1.14, 1.15, 2.1). The following logs were obtained on our V100 machine:\r\n\r\n**TF 1.14, no lambda:**\r\nINFO:tensorflow:loss = 0.2136189, step = 0\r\nINFO:tensorflow:global_step/sec: 32.7136\r\nINFO:tensorflow:loss = 0.03677105, step = 100 (3.057 sec)\r\nINFO:tensorflow:global_step/sec: 38.7334\r\nINFO:tensorflow:loss = 0.038587682, step = 200 (2.581 sec)\r\nINFO:tensorflow:global_step/sec: 38.3866\r\nINFO:tensorflow:loss = 0.037003815, step = 300 (2.605 sec)\r\nINFO:tensorflow:global_step/sec: 38.1023\r\nINFO:tensorflow:loss = 0.037481233, step = 400 (2.625 sec)\r\nINFO:tensorflow:global_step/sec: 27.7657\r\nINFO:tensorflow:loss = 0.03703531, step = 500 (3.602 sec)\r\nINFO:tensorflow:global_step/sec: 38.0632\r\nINFO:tensorflow:loss = 0.03458761, step = 600 (2.627 sec)\r\nINFO:tensorflow:global_step/sec: 36.4805\r\nINFO:tensorflow:loss = 0.03637625, step = 700 (2.741 sec)\r\nINFO:tensorflow:global_step/sec: 35.7584\r\nINFO:tensorflow:loss = 0.036259696, step = 800 (2.797 sec)\r\nINFO:tensorflow:global_step/sec: 36.7052\r\nINFO:tensorflow:loss = 0.036455855, step = 900 (2.725 sec)\r\n\r\n**TF 1.14, with lambda:**\r\nINFO:tensorflow:loss = 0.23772891, step = 0\r\nINFO:tensorflow:global_step/sec: 32.9305\r\nINFO:tensorflow:loss = 0.03767074, step = 100 (3.037 sec)\r\nINFO:tensorflow:global_step/sec: 37.3223\r\nINFO:tensorflow:loss = 0.03685203, step = 200 (2.679 sec)\r\nINFO:tensorflow:global_step/sec: 36.7882\r\nINFO:tensorflow:loss = 0.038278855, step = 300 (2.718 sec)\r\nINFO:tensorflow:global_step/sec: 37.7637\r\nINFO:tensorflow:loss = 0.03842716, step = 400 (2.648 sec)\r\nINFO:tensorflow:global_step/sec: 27.7136\r\nINFO:tensorflow:loss = 0.036828667, step = 500 (3.608 sec)\r\nINFO:tensorflow:global_step/sec: 35.5345\r\nINFO:tensorflow:loss = 0.03633987, step = 600 (2.814 sec)\r\nINFO:tensorflow:global_step/sec: 37.1818\r\nINFO:tensorflow:loss = 0.037201233, step = 700 (2.690 sec)\r\nINFO:tensorflow:global_step/sec: 37.2266\r\nINFO:tensorflow:loss = 0.036516882, step = 800 (2.686 sec)\r\nINFO:tensorflow:global_step/sec: 38.3496\r\nINFO:tensorflow:loss = 0.039767925, step = 900 (2.608 sec)\r\n\r\n**TF 1.15, no lambda:**\r\nINFO:tensorflow:loss = 0.25934136, step = 0\r\nINFO:tensorflow:global_step/sec: 34.4488\r\nINFO:tensorflow:loss = 0.037474792, step = 100 (2.903 sec)\r\nINFO:tensorflow:global_step/sec: 37.6249\r\nINFO:tensorflow:loss = 0.037868354, step = 200 (2.658 sec)\r\nINFO:tensorflow:global_step/sec: 38.2594\r\nINFO:tensorflow:loss = 0.0373457, step = 300 (2.614 sec)\r\nINFO:tensorflow:global_step/sec: 39.2323\r\nINFO:tensorflow:loss = 0.036336176, step = 400 (2.549 sec)\r\nINFO:tensorflow:global_step/sec: 26.6403\r\nINFO:tensorflow:loss = 0.035495166, step = 500 (3.754 sec)\r\nINFO:tensorflow:global_step/sec: 37.6238\r\nINFO:tensorflow:loss = 0.038571935, step = 600 (2.658 sec)\r\nINFO:tensorflow:global_step/sec: 37.4171\r\nINFO:tensorflow:loss = 0.03556432, step = 700 (2.672 sec)\r\nINFO:tensorflow:global_step/sec: 35.1526\r\nINFO:tensorflow:loss = 0.035202924, step = 800 (2.845 sec)\r\nINFO:tensorflow:global_step/sec: 35.4907\r\nINFO:tensorflow:loss = 0.03740203, step = 900 (2.818 sec)\r\nINFO:tensorflow:Loss for final step: 0.03607688.\r\n\r\n**TF 1.15, with lambda:**\r\nINFO:tensorflow:loss = 0.25370103, step = 0\r\nINFO:tensorflow:global_step/sec: 8.59509\r\nINFO:tensorflow:loss = 0.037242886, step = 100 (11.635 sec)\r\nINFO:tensorflow:global_step/sec: 8.02817\r\nINFO:tensorflow:loss = 0.03554675, step = 200 (12.456 sec)\r\nINFO:tensorflow:global_step/sec: 12.4317\r\nINFO:tensorflow:loss = 0.03646993, step = 300 (8.044 sec)\r\nINFO:tensorflow:global_step/sec: 8.62791\r\nINFO:tensorflow:loss = 0.035594724, step = 400 (11.590 sec)\r\nINFO:tensorflow:global_step/sec: 8.80279\r\nINFO:tensorflow:loss = 0.036044836, step = 500 (11.360 sec)\r\nINFO:tensorflow:global_step/sec: 9.01617\r\nINFO:tensorflow:loss = 0.036771618, step = 600 (11.091 sec)\r\nINFO:tensorflow:global_step/sec: 9.50426\r\nINFO:tensorflow:loss = 0.038547777, step = 700 (10.522 sec)\r\nINFO:tensorflow:global_step/sec: 9.18885\r\nINFO:tensorflow:loss = 0.03686235, step = 800 (10.883 sec)", "comments": ["Was able to reproduce the issue. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/54472b5156b814d46865161a8f93daea/37488.ipynb). Thanks!", "I tested the code snippet with TF 2.5 and I see identical time elapsed for both versions with and without lambda.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37488\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37488\">No</a>\n"]}, {"number": 37487, "title": "NFC - minor spelling tweaks under lite/experimental directory", "body": "This PR addresses minor spelling tweaks under `tensorflow/lite/experimental` directory.\r\nfollow-on of #35286", "comments": ["cc @mihaimaruseac"]}, {"number": 37486, "title": "Debug windows builds, add `-s` bazel flag", "body": "", "comments": []}, {"number": 37485, "title": "AssertionError on using model.fit() in tf.distribute.MirroredStrategy", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 18.04.4 LTS\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 2.2.0-dev20200308\r\n- Python version: 3.7.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n\r\n\r\n**AssertionError on using model.fit() in tf.distribute.MirroredStrategy**\r\n\r\nI'm trying to build an LSTM Autoencoder model to make time series prediction on a large dataset(~30 million rows in train data, 16 million rows each in valid and test data). I've followed the [tutorial](https://www.tensorflow.org/tutorials/distribute/keras) in the Tensorflow documentation to distribute my model across available GPUs using MirroredStrategy with HierarchicalCopyAllReduce as follows:\r\n\r\n```\r\nwith strategy.scope():\r\n    model = tf.keras.Sequential([\r\n        tf.keras.layers.LSTM(32, activation = 'relu', input_shape = (timesteps, features), return_sequences = True),\r\n        tf.keras.layers.LSTM(16, activation = 'relu', return_sequences = False),\r\n        tf.keras.layers.RepeatVector(timesteps),\r\n        tf.keras.layers.LSTM(16, activation = 'relu', return_sequences = True),\r\n        tf.keras.layers.LSTM(32, activation = 'relu', return_sequences = True),\r\n        tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(num_classes, activation = 'softmax'))\r\n    ])\r\n\r\n    model.compile(loss = tf.keras.losses.CategoricalCrossentropy(),\r\n                  optimizer = tf.keras.optimizers.Adam(),\r\n                  metrics = ['accuracy'])\r\n```\r\nWhen I try to fit the model using the following code, I get the following assertion error:\r\n\r\n```\r\nmodel.fit(X_train_tf, y_train_tf, epochs = 12, validation_data = (X_valid_tf, y_valid_tf), verbose = 1, callbacks=callbacks)\r\n\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-17-96b3da4e4eff> in <module>\r\n----> 1 model.fit(X_train_tf, y_train_tf, epochs = 12, validation_data = (X_valid_tf, y_valid_tf), verbose = 1, callbacks=callbacks)\r\n\r\n~/.conda/envs/user/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    783         max_queue_size=max_queue_size,\r\n    784         workers=workers,\r\n--> 785         use_multiprocessing=use_multiprocessing)\r\n    786 \r\n    787   def evaluate(self,\r\n\r\n~/.conda/envs/user/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_distributed.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n    617         validation_split=validation_split,\r\n    618         shuffle=shuffle,\r\n--> 619         epochs=epochs)\r\n    620     if not dist_utils.is_distributing_by_cloning(model):\r\n    621       with model._distribution_strategy.scope():\r\n\r\n~/.conda/envs/user/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py in _distribution_standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, validation_split, shuffle, epochs, allow_partial_batch)\r\n   2165         x = ds.batch(batch_size, drop_remainder=drop_remainder)\r\n   2166       else:\r\n-> 2167         assert isinstance(x, dataset_ops.DatasetV2)\r\n   2168         training_utils.validate_dataset_input(x, y, sample_weight,\r\n   2169                                               validation_split)\r\n\r\nAssertionError:\r\n```\r\nHow should I resolve this error? My train, test and validation data are of tensor type.\r\n\r\nThanks in advance!", "comments": ["@fionapatriciamoss,\r\nPlease share the complete standalone code to reproduce the issue. Thanks!", "Please find the code below:\r\n\r\n[link to code](https://colab.research.google.com/drive/1bFuQzoPi160uOaoY3C6lkE0xj9KFyqS5)\r\n", "Im getting same error // TPU \r\nhttps://colab.research.google.com/drive/1AO7Lz5G1gTOAV6FOR3H2azTOZzBHZwYt\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/34346", "I can reproduce the issue with the above colab that uses recent `tf-nightly`. Thanks!", "It seems like you are disabling eager and using a mix of v1 and v2 APIs here. Can you try with the normal v2 stack (no Session, no disable eager)?", "@karmel are you talking about my Code ?\r\nAnyone is taking care about this bug ?", "@fionapatriciamoss have you found a solution for this issue? \r\nI'am having the same problem using tensorflow v1.15 and tf.keras.utils.Sequence as my input data. ", "@fionapatriciamoss  \r\nCan you please verify with tf-nightly if the issue still exist.\r\n", "@fionapatriciamoss @ErvisTusha  as mentioned above,  the colab link referred to in this issue, is mixing v1 and v2 apis. Can you run your code again without disabling eager execution and let us know if you run into any issues. ", "@fionapatriciamoss and @ErvisTusha did you get a chance to implement the suggestion given on 10/12? Please note that  on 11/19 I will close the issue due to inactivity..\r\nThanks", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37485\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37485\">No</a>\n", "> @fionapatriciamoss have you found a solution for this issue?\r\n> I'am having the same problem using tensorflow v1.15 and tf.keras.utils.Sequence as my input data.\r\n\r\nsame here"]}, {"number": 37484, "title": "58", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): \r\n- Python version: - Bazel\r\nversion (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37484\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37484\">No</a>\n", "Closing as there is no information supplied"]}, {"number": 37483, "title": "fix the core_dump in reduce_max with input type tf.complex64  ", "body": "As report in this issue: https://github.com/tensorflow/tensorflow/issues/37446\r\nWhen the input type of tf.complex64 , reduce_max op will core dump.\r\nThis PR add the condition judgement to avoid core-dump directly. ", "comments": ["Hi @nickdesaulniers Sorry to interrupt. But any comments? ", "Add more unsupported types", "Hi @sanjoy, Thanks for the review. Fix the minor nits."]}, {"number": 37482, "title": "Benchmark tool", "body": "Is there available \"benchmarking tool\" for TensorFlow Lite for Microcontrollers? ", "comments": ["@njeffrie Do you want to take this?", "Definitely - currently we have a very basic benchmark utility under tensorflow/lite/micro/testing/micro_benchmark.h.  Currently, you can only benchmark entire methods, and this is done by running the benchmark like a test.  Work is underway to bring up a better end-to-end solution with a specific benchmark runner.\r\n\r\nIn the near future, I will be submitting few instructions for creating and running benchmarks under the new directory tensorflow/lite/micro/benchmarks."]}, {"number": 37481, "title": "Fix exception causes in session.py", "body": "I recently went over [Matplotlib](https://github.com/matplotlib/matplotlib/pull/16706) and [Pandas](https://github.com/pandas-dev/pandas/pull/32322), fixing a small mistake in the way that Python 3's exception chaining is used. If you're interested, I can do it here too. I've done it on just one file right now.\r\n\r\nThe mistake is this: In some parts of the code, an exception is being caught and replaced with a more user-friendly error. In these cases the syntax `raise new_error from old_error` needs to be used.\r\n\r\nPython 3's exception chaining means it shows not only the traceback of the current exception, but that of the original exception (and possibly more.) This is regardless of `raise from`. The usage of `raise from` tells Python to put a more accurate message between the tracebacks. Instead of this:\r\n\r\n```\r\nDuring handling of the above exception, another exception occurred:\r\n```\r\n\r\nYou'll get this:\r\n\r\n```\r\nThe above exception was the direct cause of the following exception:\r\n```\r\n\r\nThe first is inaccurate, because it signifies a bug in the exception-handling code itself, which is a separate situation than wrapping an exception.\r\n\r\nLet me know what you think!\r\n\r\n", "comments": ["@cool-RR Thanks for this change! Am I right in thinking that this chaining syntax is only available in Python 3? Would `six.raise_from()` work in this case?\r\n\r\n@martinwicke Do we have a timeline for accepting changes that use Python 3 features?", "@mrry You're welcome, yes and yes.", "Great! Would you mind switching the PR to use `six.raise_from()` and then I'd be happy to approve the change?", "Yep, done. Just curious though, [looking here](https://www.tensorflow.org/install/pip) it says that Python 3.5+ is required. What reason is there to avoid native Python 3 syntax like `raise foo from bar`?", "Thanks! Yes, I was hoping @martinwicke could clarify our policy on that. Even though we are no longer building PIP packages for Python 2.7, I believe we still have a few non-PIP users that are stuck on that version of Python, which prevents us from adopting new syntax (just yet). ", "@mrry: I understand. I'll wait with fixing this problem in the entire project until after you've moved to Python 3 completely, so we could switch straight to `raise foo from bar`. (Assuming I won't be too busy then.) Once you've left Python 2 completely, feel free to send me a message here.", "@cool-RR Any update on this PR, please. Thanks!", "@gbaned See my last message.", "@mrry  Any update on this PR, please. Thanks!", "@gbaned: See @cool-RR's last message. This PR is on hold until we get confirmation that TensorFlow can use Python 3\u2013specific features.", "@mrry, @cool-RR  This PR is in draft, any update on this? Please. Thanks!", "@gbaned I moved this PR to ready state now. As far as I'm concerned we can move forward, just need confirmation that Python 2 support is dropped in TensorFlow.\r\n\r\nHere's a blog post I made where I explained my effort to push this Python 3 syntax in many open-source packages: https://blog.ram.rachum.com/post/621791438475296768/improving-python-exception-chaining-with\r\n\r\nThere's now a PyLint rule that checks for this syntax.", "@mrry Can you please take a look on the above comment from @cool-RR. Thanks!", "@cool-RR We are soooo close to being able to actually using Python3-only syntax. I'm re-running the tests and I will merge this if I can using the six as is. \r\n\r\n@ematejska FYI this would make a good test case whether we can really break Py2.", "@cool-RR Can you please check build failures. Thank you!", "@gbaned I rebased and pushed. It's been 4 hours and the CI hasn't started running yet. Is this normal?", "triggered it now", "@cool-RR Still, build failures are appearing, can you please fix those?. Thank you!", "Looks like you have a test `test_propagates_exception_trace` that checks things I don't understand about the exception traceback. Looks like @yifeif wrote it. Yifey, if you have a solution here, let me know. Otherwise this is getting too hairy for me.", "Took a brief look, and seems like six changed the exception string, and you might need to update the test to reflect that. Here is the current error from CI\r\n```\r\nAssertionError: '<string>' not found in ['session.py', 'monitored_session.py'] : The exception was raised from an unrecognized file. This unit test probably needs to be updated. Traceback:\r\n[<FrameSummary file /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/monitored_session_test.runfiles/org_tensorflow/tensorflow/python/training/monitored_session_test.py, line 750 in test_propagates_exception_trace>, <FrameSummary file /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/monitored_session_test.runfiles/org_tensorflow/tensorflow/python/training/monitored_session.py, line 1384 in run>, <FrameSummary file /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/monitored_session_test.runfiles/six_archive/six.py, line 703 in reraise>, <FrameSummary file /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/monitored_session_test.runfiles/org_tensorflow/tensorflow/python/training/monitored_session.py, line 1369 in run>, <FrameSummary file /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/monitored_session_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py, line 1689 in run>, <FrameSummary file /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/monitored_session_test.runfiles/org_tensorflow/tensorflow/python/client/session.py, line 976 in run>, <FrameSummary file /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/monitored_session_test.runfiles/org_tensorflow/tensorflow/python/client/session.py, line 1202 in _run>, <FrameSummary file /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/monitored_session_test.runfiles/org_tensorflow/tensorflow/python/client/session.py, line 1380 in _do_run>, <FrameSummary file /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/monitored_session_test.runfiles/org_tensorflow/tensorflow/python/client/session.py, line 1407 in _do_call>, <FrameSummary file <string>, line 3 in raise_from>]\r\n```\r\nHere is where the test was getting the name of the file exception was raised from (where the change needs to be made),\r\nhttps://github.com/tensorflow/tensorflow/blob/f618ab49554dba03c104ef27cea5d893063fd373/tensorflow/python/training/monitored_session_test.py#L756\r\n\r\n", "I'm gonna bow out, this is getting too complicated for me."]}, {"number": 37480, "title": "'third_party/tensorflow/compiler/aot:codegen_test' No such directory found", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/aot/codegen_test.cc\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n// To update the golden file, flip update_golden to true and run the\r\n  // following:\r\n  // bazel test --test_strategy=local \\\r\n  //   third_party/tensorflow/compiler/aot:codegen_test\r\n### Clear description\r\n\r\nLine 154\r\n\r\nI wanted to update the golden file, but the given directory is not present in the third_party library.\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@ayushmankumar7 Can you please be clear on what you are trying to explain here. Thanks!", "In line 154, it says // To update the golden file, flip update_golden to true and run the\r\n  // following:\r\n  // bazel test --test_strategy=local \\\r\n  //   third_party/tensorflow/compiler/aot:codegen_test\r\n\r\n\r\nbut /tensorflow/compiler/aot:codegen_test not present in third_party folder\r\n"]}, {"number": 37479, "title": "Excessive memory consumption and preparation runtime of tf.keras.backend.max in custom layer with masking", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- Mobile device if the issue happens on mobile device: -\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.2.0-dev20200303\r\n- Python version: 3.6.9\r\n- Bazel version: -\r\n- GCC/Compiler version: -\r\n- CUDA/cuDNN version: CPU only\r\n- GPU model and memory: CPU only\r\n\r\n**Describe the current behavior**\r\nMemory consumption seems to be proportional to `num_iterations` and thus excessive, most likely being a memory leak. Runtime until seeing the first fit result is also extremely slow: 15 seconds until the first fit call, 55 seconds until seeing the result of the first fit, and the other fits run through in less than a second. Apparently, runtime is due to memory management and not due to the actual max function evaluation.\r\n\r\nWhen using `tf.keras.backend.max` for computing a mask with `tf.stack` in a real setup, memory consumption increases steadily until running out of memory at approx. 30 GB. In contrast, without `compute_mask`, memory consumption doesn't go beyond approx 1 GB.\r\n\r\n**Describe the expected behavior**\r\nI would expect memory consumption to be independent of `num_iterations` and thus being much lower, plus preparation runtime being much lower.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\nbatch_size = 100\r\ndim_input = 100\r\ndim_output = 1\r\nnum_iterations = 100 # will consume approx. 5 GB RAM when set to 1000\r\n\r\n\r\nclass CustomMask(tf.keras.layers.Layer):\r\n  def __init__(self):\r\n    super(CustomMask, self).__init__()\r\n\r\n  def compute_mask(self, inputs, mask=None):\r\n    batch_size = inputs.shape[0]\r\n\r\n    batch_maxes = tf.keras.backend.max(inputs, axis=1)\r\n\r\n    for batch in range(batch_size):\r\n      for i in range(num_iterations):\r\n        max = tf.keras.backend.max(batch_maxes[batch])\r\n\r\n    return None\r\n\r\n  def call(self, inputs, mask=None):\r\n    return inputs\r\n\r\n\r\nmodel = tf.keras.Sequential()\r\n\r\nmodel.add(tf.keras.layers.Input(batch_input_shape=(batch_size, dim_input)))\r\n\r\nmodel.add(CustomMask())\r\n\r\nmodel.add(tf.keras.layers.Dense(dim_output))\r\n\r\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n\r\ntraining_input = np.zeros([batch_size, dim_input])\r\ntraining_output = np.zeros([batch_size, dim_output])\r\n\r\nmodel.fit(training_input, training_output, batch_size=batch_size)\r\n```\r\n\r\n**Other info / logs**\r\nIf my usage of `tf.keras.backend.max` is wrong with regard to memory consumption and / or runtime, please let me know. I need to call it frequently within `compute_mask` for computing a custom mask in conjunction with `tf.stack`. However, the latter does not seem to be the problem, which is why I left it out in the stripped down code.\r\n", "comments": ["@padoremu, When i tried to execute the issue by setting `num_iterations=1000` session got crashed. Which shows, it took more than 12GB of RAM. Please find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/cbeef0eba4a6ad187de71fe8cdc6019a/untitled436.ipynb) and confirm the issue. Thanks!", "Thanks. I can confirm the issue.", "I would like to kindly ask, if there are any news on this issue?", "This issue has been inactive for one month now. I would appreciate some feedback very much. Thank you.", "@gadagashwini @gowthamkpr @fchollet Is there any chance that a tensorflower comments on this issue? That would be very kind. Thank you.", "@padoremu \r\nCan you please try on latest tf version and in case the issue persist.\r\n\r\nPlease post this issue on keras-team/keras repo.\r\nTo know more refer to:\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999", "@Saduf2019\r\nThank you for asking after so long. I just tried with a fresh tf-nightly installation (2.7.0-dev20210920) and using the initially posted code, and I can still perfectly reproduce the problem: the larger you set `num_iterations`, the more memory consumption increases. I recommend setting `num_iterations = 1000` and observe the evolution with e.g. top on Linux. Memory consumption steadily increases. After 15 minutes, I even have > 20 GB.\r\n\r\nSince this issue was created one and a half years ago, as you can imagine I had to find ways to avoid needing this kind of functionality. Please feel free to move this issue to kears-team/keras repo. My motivation to invest more time in communicating and documenting this problem is limited. Of course I would still be interested in a solution / fix. Anybody can easily reproduce the problem with the initially posted code - that's all one needs.\r\n\r\nThank you.\r\n", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37479\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37479\">No</a>\n"]}, {"number": 37478, "title": "Tensorflow Website not working properly in Firefox", "body": "## Description of issue (what needs changing):\r\nTensorflow website UI doesn't seem right when opened in Firefox. I'm not sure if it's happening only in my case or not...\r\n\r\n### Request visuals, if applicable\r\n![Screenshot_2020-03-10 tf keras callbacks TensorBoard TensorFlow Core v2 1 0](https://user-images.githubusercontent.com/29497701/76317231-de111500-6301-11ea-8654-03cd0d29eeb9.png)\r\n", "comments": ["This is a result of icons failing to load and being replaced by their `alt` tag. I'm not sure what is causing this in the reported case given TensorFlow logo and Github log being displayed correctly - I'm not able to reproduce this atm on Firefox 71", "@ManishAradwad \r\nPlease refer to the screenshot attached ,I have verified on Firefox version \"68.5.0esr (64-bit)\" and it displays as expected, could you please check your network and Firefox version due to which you would be facing this issue.\r\n![37478](https://user-images.githubusercontent.com/59822926/76403465-f852fd80-63aa-11ea-870e-98b4151fd4e5.png)\r\n", "I'm using Firefox version 73.0.1 (64-bit), no network issues.", "@ManishAradwad \r\nCould you please try it in private browsing mode and share the results", "It's working fine in private window\r\n\r\n![Screenshot_2020-03-11 tf keras callbacks TensorBoard TensorFlow Core v2 1 0](https://user-images.githubusercontent.com/29497701/76418914-7c19e380-63c5-11ea-8ea5-78ee4e4d9655.png)\r\n", "Did clearing cookie and cache help?\r\nThis seems like a problem with cached files - Firefox believes the image assets are cached but they aren't at the browser is falling back to the text in the alt tag", "@ManishAradwad \r\nPlease clear cache and set the Firefox settings to default settings this should help, is this the case with just tensor-flow website or other websites as well.", "@Saduf2019 It is now working properly. Thanks for looking into this!\r\nYou may close this issue, if it's fine.", "Proceeding to close the issue as its resolved"]}, {"number": 37477, "title": "Additional mode for loading of segmentation data in tf.keras.preprocessing.image.ImageDataGenerator.flow_from_dataframe()", "body": "**System information**\r\n- TensorFlow version (you are using): 2.1.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nAs of now, one has to come around with a workaround to load segmentation data in `tf.keras.preprocessing.image.ImageDataGenerator` to load images and segmentation masks in a generator. A current workaround could look like:\r\n    \r\n    datagen = tf.keras.preprocessing.image.ImageDataGenerator()\r\n    \r\n    df = pd.DataFrame({'images': images,\r\n                       'masks': masks})\r\n    # seed, so that image_generator and mask_generator will rotate and shuffle equivalently\r\n    seed = 42\r\n    image_generator = datagen.flow_from_dataframe(df, \r\n                                          directory='.', \r\n                                          x_col='images', \r\n                                          y_col='masks', \r\n                                          batch_size=1, \r\n                                          class_mode=None, \r\n                                          seed=seed)\r\n    mask_generator = datagen.flow_from_dataframe(df, \r\n                                         directory='.', \r\n                                         x_col='masks', \r\n                                         y_col='images', # Or whatever \r\n                                         batch_size=1, \r\n                                         class_mode=None, \r\n                                         seed=seed)\r\n    \r\n    train_generator = zip(image_generator, mask_generator)\r\n   \r\n    # same for validation data generator\r\n\r\n    model.fit(x=train_generator, epochs=EPOCHS, \r\n                    validation_data=validation_generator, use_multiprocessing=False)\r\n\r\nIt would be much cleaner, if there would be an option for `class_mode` (e.g. `class_mode='mask'`) that allows you to load the files specified by the paths in `y_col` as images. \r\n\r\n**Will this change the current api? How?**\r\n\r\nAn additional option for the argument `class_mode` will be added. \r\n\r\n**Who will benefit with this feature?**\r\nEveryone who looks for a simple way to load images and corresponding segmentation data. \r\n\r\n**Any Other info.**\r\n", "comments": ["Okay, I tried to solve it on my own. However, there is still a dependency on [keras-preprocessing](https://github.com/keras-team/keras-preprocessing) that makes things a bit difficult. Are there any plans to integrate keras-preprocessing into TensorFlow?", "Hi @RobinBaumann, do you have a more complete colab example or the stack trace so we know what is the dependency issue? We do have an equivalent `dataset_from_directory` but not ingesting dataframe for now.", "In `tensorflow/python/keras/preprocessing/image.py` you can find an import statement to:\r\n\r\n    from keras_preprocessing import image\r\n\r\nWhich is later used in:\r\n\r\n    class ImageDataGenerator(image.ImageDataGenerator):\r\n        ....\r\n\r\nSo all calls to the generator are handled by `keras_preprocessing.image.ImageDataGenerator` instead of `tf.keras.preprocessing.image.ImageDataGenerator`, beause the latter doesn't provide implementations of `flow_from_dataframe`, `flow_from_directory` etc.  This is not an error, I just think that it would be cleaner to integrate all functionality `keras_preprocessing` into `tf.keras`, especially since Keras is already integrated in TF2 and will no longer be maintained standalone. \r\n\r\n    ", "> In `tensorflow/python/keras/preprocessing/image.py` you can find an import statement to:\r\n> \r\n> ```\r\n> from keras_preprocessing import image\r\n> ```\r\n> \r\n> Which is later used in:\r\n> \r\n> ```\r\n> class ImageDataGenerator(image.ImageDataGenerator):\r\n>     ....\r\n> ```\r\n> \r\n> So all calls to the generator are handled by `keras_preprocessing.image.ImageDataGenerator` instead of `tf.keras.preprocessing.image.ImageDataGenerator`, beause the latter doesn't provide implementations of `flow_from_dataframe`, `flow_from_directory` etc. This is not an error, I just think that it would be cleaner to integrate all functionality `keras_preprocessing` into `tf.keras`, especially since Keras is already integrated in TF2 and will no longer be maintained standalone.\r\n\r\nOIC. Yeah what we currently have is dataset_from_directory, which is the equivalent of flow_from_directory, as experimental, we don't have a dataset_from_dataframe yet", "> \r\n> OIC. Yeah what we currently have is dataset_from_directory, which is the equivalent of flow_from_directory, as experimental, we don't have a dataset_from_dataframe yet\r\n\r\nAh okay,  is currently anybody working on the `dataset_from_dataframe` or can this be considered a `good first issue` for me? :) ", "> > OIC. Yeah what we currently have is dataset_from_directory, which is the equivalent of flow_from_directory, as experimental, we don't have a dataset_from_dataframe yet\r\n> \r\n> Ah okay, is currently anybody working on the `dataset_from_dataframe` or can this be considered a `good first issue` for me? :)\r\n\r\nNo one is working on that currently, you're welcome for contribution :-)", "@RobinBaumann We're planning to get rid of keras-preprocessing dependency as well. If you want to contribute, that'd be great as well :-)", "@RobinBaumann On another note, I would think it's probably not the best place to do the shuffle and rotate here. IMHO, it's better to separate the logic of 'load things from files' from 'preprocess those things', i.e.,:\r\nstep 1: load the image:\r\nimg = dataset_from_dataframe(...)\r\nseg = dataset_from_dataframe(...)\r\n\r\nstep 2: preprocess it.\r\ndef map_fn(x):\r\n  return tf.image.rotate(x, seed)  # Or something like that, we launched Keras Image Preprocessing layers.\r\n\r\npreproc_img = img.shuffle(your_seed).map(map_fn)\r\npreproc_seg = seg.shuffle(your_seed).map(map_fn)", "> @RobinBaumann We're planning to get rid of keras-preprocessing dependency as well. If you want to contribute, that'd be great as well :-)\r\n\r\nSounds good \ud83d\udc4d I'm having a look into it this weekend and will come back to you, if i have some questions, if that's okay? ", "> > @RobinBaumann We're planning to get rid of keras-preprocessing dependency as well. If you want to contribute, that'd be great as well :-)\r\n> \r\n> Sounds good \ud83d\udc4d I'm having a look into it this weekend and will come back to you, if i have some questions, if that's okay?\r\n\r\nOf course!", "I'm gonna close this for now, since we all agreed we should implement the dataset way. Please file a PR once it's ready. Thanks!"]}, {"number": 37476, "title": "support tf.Unique legalize throught hlo/lhlo ", "body": "This pr purposed a way to lower tf.Unqiue down to mlir core dialect, by adding relevant hlo/lhlo op, and a runner_utils which call a external function to actually do the unique thing with a hashmap, any discussion is welcomed for this pr as  we are trying to get things done right .\r\nThe reason why we import a external function to lower the relevant unique logic is that the hashmap type is not supported  currently in mlir core, a discussion is purposed in llvm discussion https://llvm.discourse.group/t/will-mlir-support-hashmap-type-in-near-future/691/2  \r\nSo for code like \r\n```\r\n%1:2 = \"tf.Unique\"(%arg0)  : (tensor<?xi64>) -> (tensor<?xi64>, tensor<?xi64>)\r\n```\r\nwill be first lowered to lhlo code \r\n```\r\n%0 = alloc() {temp = true} : memref<i64>\r\n    \"xla_lhlo.unique_count\"(%arg0, %0) : (memref<?xi64>, memref<i64>) -> ()\r\n    %1 = load %0[] : memref<i64>\r\n    %2 = index_cast %1 : i64 to index\r\n    %3 = alloc(%2) {temp = true} : memref<?xi64>\r\n    %4 = dim %arg0, 0 : memref<?xi64>\r\n    %5 = alloc(%4) {temp = true} : memref<?xi64>\r\n    \"xla_lhlo.unique\"(%arg0, %3, %5) : (memref<?xi64>, memref<?xi64>, memref<?xi64>) -> ()\r\n    dealloc %0 : memref<i64>\r\n    dealloc %3 : memref<?xi64>\r\n    dealloc %5 : memref<?xi64>\r\n```\r\nand then be lowered by newly added -lhlo-legalize-to-std pass to code like\r\n```\r\n%0 = alloc() {temp = true} : memref<i64>\r\n    %1 = dim %arg0, 0 : memref<?xi64>\r\n    %2 = call @_global_get_unique_ids_count(%arg0, %1) : (memref<?xi64>, index) -> i64\r\n    store %2, %0[] : memref<i64>\r\n    %3 = load %0[] : memref<i64>\r\n    %4 = index_cast %3 : i64 to index\r\n    %5 = alloc(%4) {temp = true} : memref<?xi64>\r\n    %6 = dim %arg0, 0 : memref<?xi64>\r\n    %7 = alloc(%6) {temp = true} : memref<?xi64>\r\n    call @_global_unique_i64_i64(%arg0, %5, %7) : (memref<?xi64>, memref<?xi64>, memref<?xi64>) -> ()\r\n    dealloc %0 : memref<i64>\r\n    dealloc %5 : memref<?xi64>\r\n    dealloc %7 : memref<?xi64>\r\n```\r\nso the two external function  _global_get_unique_ids_count and _global_unique_i64_i64 is called to actually do unique , which is now located in runner_utils lib", "comments": ["@qqsun8819 Can you please check reviewer comments and keep us posted. Thanks!", "Could you create a discussion on the TF MLIR SIG about adding the op? We are trying to make adding operations and discussing them very open and not all follow PRs.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@qqsun8819 Can you please check @joker-eph, @jpienaar comments and resolve conflicts?. Thanks!\r\n", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 37475, "title": "Introduce external delegate provider to benchmark model.", "body": "Allows TFLite benchmark model to load an external delegate through dlopen.\r\n\r\nNote: This allows only a single external delegate plugin to be loaded at the moment.", "comments": ["@jdduke could you have a look?\r\nAs per comment, this allows a single external delegate plugin at the moment but should be good enough for initial testing.\r\nThank you in advance.", "Apologies @jdduke I overwrote the previous commit instead of pushing a patch set on top.", "@GeorgeARM Can you please address Ubuntu Sanity errors? Thanks!", "> @GeorgeARM Can you please address Ubuntu Sanity errors? Thanks!\r\n\r\n@gbaned Done", "@multiverse-tf @gbaned do I have to address any issues?", "> @multiverse-tf @gbaned do I have to address any issues?\r\n\r\n@GeorgeARM Nothing to do at this moment. We will come back if anything required. Thank you!", "> > @multiverse-tf @gbaned do I have to address any issues?\r\n> \r\n> @GeorgeARM Nothing to do at this moment. We will come back if anything required. Thank you!\r\n\r\nThank you for your contribution! Yes, We are handling the merging issue right now, and your changes will be merged ", "@GeorgeARM, a fix (https://github.com/tensorflow/tensorflow/commit/1f28da1bd5f48cf56dbddb69ca38c878d4c47776) has been submitted to address test failures of this PR. I think you might need to rebase your changes as the BUILD file has been changed to unblock the merge. Thx!"]}, {"number": 37474, "title": "Building bazel on OpenSuSE Leap 15.1 fails due to unrecognized option.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n```\r\n- OS Platform and Distribution:    OpenSuSE Leap 15.1 (x86_64)\r\n- Mobile device                              N/A\r\n- TensorFlow installed from         source\r\n- TensorFlow version:                   0.22.0 (bazel-0.22.0-lp151.2.1.x86_64.rpm, provided in the OpenSuSE repositories)\r\n- Python version:                           2.7.14\r\n- Installed using:                            virtualenv? pip? conda?: Unknown (python is a foreign language for me)\r\n- GPU model and memory:           Intel Corporation HD Graphics 630 (rev 04) (prog-if 00 [VGA controller])\r\n```\r\n\r\n\r\n**Describe the problem**\r\nInstallation fails with the following message:\r\n```\r\nERROR: Unrecognized option: --experimental_repo_remote_exec\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\n- zypper install bazel\r\n- ./configure\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n[BazelInstall.log](https://github.com/tensorflow/tensorflow/files/4311797/BazelInstall.log)\r\n\r\n", "comments": ["please use bazel-2.0\r\nrefer to this PR: https://github.com/tensorflow/tensorflow/pull/36913", "@jlturriff,\r\nCould you please check @Leslie-Fang's comment and let us know if it works? Thanks!", "Any updates regarding this issue? Thanks!", "Hi, I have tried this above suggestion of upgrading to Bazel 2.0. I don't see the `ERROR: Unrecognized option: --experimental_repo_remote_exec` anymore  but upgrading to Bazel 2.0 is resulting in a different error with the Bazel build itself, please see this and the comments for this commit: https://github.com/tensorflow/serving/commit/162f72949c6ecbe9e610182c923dec0aa5924cf2\r\n\r\nAs for this specific issue, Upgrading to Bazel 2.0 resolves it.", "> Any updates regarding this issue? Thanks!\r\n\r\nAutomatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37474\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37474\">No</a>\n", "looks like the error has returned\r\n```\r\nroot@jetson:/home/tensorflow# bazel build --config=opt --config=monolithic //tensorflow/tools/lib_package:libtensorflow\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=248\r\nINFO: Reading rc options for 'build' from /home/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nERROR: Unrecognized option: --experimental_repo_remote_exec\r\nINFO: Invocation ID: 51e33f1d-1cff-49a1-be8a-db6386079866\r\n```", "> looks like the error has returned\r\n\r\n@moeiscool,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the issue template, so that we can track the issue there. Thanks!"]}, {"number": 37473, "title": "Tensorflow import error //ImportError: DLL load failed: The specified module could not be found.", "body": "Traceback (most recent call last):\r\n  File \"C:\\Users\\Research\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-7-db26ddd430d7>\", line 1, in <module>\r\n    mnist = tf.keras.mnist\r\nAttributeError: module 'tensorflow' has no attribute 'keras'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Research\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'AttributeError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Research\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Research\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Research\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Research\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Research\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Research\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\r\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\r\n  File \"C:\\Users\\Research\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\r\n    return f(*args, **kwargs)\r\n  File \"C:\\Users\\Research\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\r\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\r\n  File \"C:\\Users\\Research\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\r\n    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\r\n  File \"C:\\Users\\Research\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\r\n    filename = getsourcefile(frame) or getfile(frame)\r\n  File \"C:\\Users\\Research\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\r\n    if getattr(getmodule(object, filename), '__loader__', None) is not None:\r\n  File \"C:\\Users\\Research\\Anaconda3\\lib\\inspect.py\", line 733, in getmodule\r\n    if ismodule(module) and hasattr(module, '__file__'):\r\n  File \"C:\\Users\\Research\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\Research\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\Research\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"C:\\Users\\Research\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\r\n    from . _api.v2 import audio\r\n  File \"C:\\Users\\Research\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\_api\\v2\\audio\\__init__.py\", line 10, in <module>\r\n    from tensorflow.python.ops.gen_audio_ops import decode_wav\r\n  File \"C:\\Users\\Research\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\gen_audio_ops.py\", line 9, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n  File \"C:\\Users\\Research\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\Research\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\Research\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\Research\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Research\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Research\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-7-db26ddd430d7>\", line 1, in <module>\r\n    mnist = tf.keras.mnist\r\nAttributeError: module 'tensorflow' has no attribute 'keras'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Research\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'AttributeError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Research\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Research\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Research\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Research\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Research\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "@gadagashwini Would you please help me with this issue, still it is not clear for me what to do?\r\nThank you,", "@MahdiBayat95,\r\nCan you check whether your CPU model support AVX instruction sets or not. Since, TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets. Thanks!", "Closing as duplicate.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156\r\n\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37473\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37473\">No</a>\n"]}, {"number": 37472, "title": "Configurable attribute \"srcs\" doesn't match this configuration", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 16.04.2 LTS (GNU/Linux 4.4.0-38-generic aarch64)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0\r\n- Python version: 2.7\r\n- Installed using virtualenv? pip? conda?: N\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source): 9.1.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\nBazel build failed when I am trying to build the tflite benchmark\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nbazel build -c opt tensorflow/lite/tools/benchmark:benchmark_model\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=133\r\nINFO: Reading rc options for 'build' from /users/Wei_Hao/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /users/Wei_Hao/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Found applicable config definition build:v2 in file /users/Wei_Hao/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:linux in file /users/Wei_Hao/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /users/Wei_Hao/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nERROR: /users/Wei_Hao/.cache/bazel/_bazel_Wei_Hao/fcdad47d36a19e877fa240235b3a74d1/external/cpuinfo/BUILD.bazel:101:1: Configurable attribute \"srcs\" doesn't match this configuration (would a default condition help?).\r\nConditions checked:\r\n @cpuinfo//:linux_x86_64\r\n @cpuinfo//:macos_x86_64\r\n @cpuinfo//:android_armv7\r\n @cpuinfo//:android_arm64\r\n @cpuinfo//:android_x86\r\n @cpuinfo//:android_x86_64\r\n @cpuinfo//:ios_x86_64\r\n @cpuinfo//:ios_x86\r\n @cpuinfo//:ios_armv7\r\n @cpuinfo//:ios_arm64\r\n @cpuinfo//:ios_arm64e\r\n @cpuinfo//:watchos_x86_64\r\n @cpuinfo//:watchos_x86\r\n @cpuinfo//:watchos_armv7k\r\n @cpuinfo//:watchos_arm64_32\r\n @cpuinfo//:emscripten_wasm\r\nERROR: Analysis of target '//tensorflow/lite/tools/benchmark:benchmark_model' failed; build aborted:\r\n\r\n/users/Wei_Hao/.cache/bazel/_bazel_Wei_Hao/fcdad47d36a19e877fa240235b3a74d1/external/cpuinfo/BUILD.bazel:101:1: Configurable attribute \"srcs\" doesn't match this configuration (would a default condition help?).\r\nConditions checked:\r\n @cpuinfo//:linux_x86_64\r\n @cpuinfo//:macos_x86_64\r\n @cpuinfo//:android_armv7\r\n @cpuinfo//:android_arm64\r\n @cpuinfo//:android_x86\r\n @cpuinfo//:android_x86_64\r\n @cpuinfo//:ios_x86_64\r\n @cpuinfo//:ios_x86\r\n @cpuinfo//:ios_armv7\r\n @cpuinfo//:ios_arm64\r\n @cpuinfo//:ios_arm64e\r\n @cpuinfo//:watchos_x86_64\r\n @cpuinfo//:watchos_x86\r\n @cpuinfo//:watchos_armv7k\r\n @cpuinfo//:watchos_arm64_32\r\n @cpuinfo//:emscripten_wasm\r\nINFO: Elapsed time: 1.036s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (1 packages loaded, 17 targets configured)\r\n", "comments": ["@WeiHao97 \r\nCan you please check this [link](https://stackoverflow.com/questions/57014559/how-to-use-select-to-specify-src-config-of-a-lower-level-library-dependency) and let us know if it helps", "> @WeiHao97\r\n> Can you please check this [link](https://stackoverflow.com/questions/57014559/how-to-use-select-to-specify-src-config-of-a-lower-level-library-dependency) and let us know if it helps\r\n\r\n@Saduf2019 Hi, I tried --cpu=arm and get the same error, I am wondering if tflite can be build on ubuntu with arm64 architecture since this config does not appear in the select() options.", "@Saduf2019\r\n tflite can be build on ubuntu with arm64 architectureh: ttps://www.tensorflow.org/lite/guide/build_arm64\r\n\r\nBut my goal is to run the benchmark with bazel. I am not sure how to make this done. Please help ", "@WeiHao97 ,\r\nIs this still an issue?\r\nCould you please update TensorFlow to the latest stable version v.2.7 and let us know if you are facing the same error. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37472\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37472\">No</a>\n"]}, {"number": 37471, "title": "TFLite allocate tensors fails: (CONCATENATION) failed to prepare", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **v2.1.0-rc2-17-ge5bf8de 2.1.0**\r\n- Python version: **3.6.9**\r\n- Bazel version (if compiling from source): **N/A**\r\n- GCC/Compiler version (if compiling from source): **N/A**\r\n- CUDA/cuDNN version: **N/A**\r\n- GPU model and memory: **N/A**\r\n\r\n**Describe the current behavior**\r\n\r\nCreating a `tf.keras` model and exporting it to `tflite` causes an error when trying to allocate the tensors for inference. The error is:\r\n\r\n> RuntimeError: tensorflow/lite/kernels/concatenation.cc:68 t->dims->size != t0->dims->size (0 != 4)Node number 3 (CONCATENATION) failed to prepare.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe exported model should load without error.\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\nColab gist reproducing the error can be found [here](https://colab.research.google.com/gist/moberweger/9dca6edfdabf996a57dab42ffe044666/untitled0.ipynb#scrollTo=2-Tz7Z1KnVZk)\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Input, Conv2D, Concatenate, Add\r\nfrom tensorflow.keras.models import Model\r\n\r\nif __name__ == '__main__':\r\n    print(tf.version.GIT_VERSION, tf.version.VERSION)\r\n    test_images = np.random.randn(8, 388, 420, 1)\r\n\r\n    # model\r\n    m_input = Input(shape=test_images.shape[1:])\r\n    d1 = Conv2D(8, 3, dilation_rate=1, padding='same', use_bias=False)(m_input)\r\n    d2 = Conv2D(8, 3, dilation_rate=2, padding='same', use_bias=False)(m_input)\r\n    d16 = Conv2D(8, 3, dilation_rate=16, padding='same', use_bias=False)(m_input)\r\n    add1 = Add()([d2, d16])\r\n    m_output = Concatenate()([d1, d2, add1])\r\n\r\n    model = Model(inputs=m_input, outputs=m_output)\r\n    model.summary()\r\n    model.compile(optimizer='rmsprop', loss='mse')\r\n    pred = model.predict(test_images)\r\n    print(pred.shape)\r\n\r\n    # conversion\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n    tflite_model = converter.convert()\r\n    with open(\"model.tflite\", \"wb\") as fp:\r\n        fp.write(tflite_model)\r\n\r\n    # inference\r\n    interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\r\n    input_details = interpreter.get_input_details()\r\n    interpreter.allocate_tensors()\r\n    print(\"DONE\")\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nRuntimeError                              Traceback (most recent call last)\r\n\r\n<ipython-input-3-4adc506bd32d> in <module>()\r\n     31     interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\r\n     32     input_details = interpreter.get_input_details()\r\n---> 33     interpreter.allocate_tensors()\r\n     34     print(\"DONE\")\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/interpreter.py in allocate_tensors(self)\r\n    245   def allocate_tensors(self):\r\n    246     self._ensure_safe()\r\n--> 247     return self._interpreter.AllocateTensors()\r\n    248 \r\n    249   def _safe_to_run(self):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py in AllocateTensors(self)\r\n    108 \r\n    109     def AllocateTensors(self):\r\n--> 110         return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\n    111 \r\n    112     def Invoke(self):\r\n\r\nRuntimeError: tensorflow/lite/kernels/concatenation.cc:68 t->dims->size != t0->dims->size (0 != 4)Node number 3 (CONCATENATION) failed to prepare.\r\n```\r\n", "comments": ["@moberweger \r\nRunning your code on nightly does not throw any error,please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/05700439ae5c12ea117a518f7a434d3a/37471.ipynb)\r\nWould you please check this [link](https://github.com/tensorflow/tensorflow/issues/34345) for reference", "Thanks @Saduf2019  works with nightly, so I am closing this one.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37471\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37471\">No</a>\n"]}, {"number": 37470, "title": "Cannot import tensorflow", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64bit\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.6.4\r\n- Installed using virtualenv? pip? conda?:pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI used pip install tensorflow to install TensorFlow in my pc. But when I run import TensorFlow as tf ,I get this message\r\n\r\nITraceback (most recent call last):\r\n  File \"C:\\Users\\bhavs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\bhavs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\bhavs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\bhavs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\bhavs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<pyshell#0>\", line 1, in <module>\r\n    import tensorflow\r\n  File \"C:\\Users\\bhavs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\bhavs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\r\n    from . _api.v2 import audio\r\n  File \"C:\\Users\\bhavs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\_api\\v2\\audio\\__init__.py\", line 10, in <module>\r\n    from tensorflow.python.ops.gen_audio_ops import decode_wav\r\n  File \"C:\\Users\\bhavs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_audio_ops.py\", line 9, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n  File \"C:\\Users\\bhavs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\bhavs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\bhavs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\bhavs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\bhavs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\bhavs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\bhavs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\bhavs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\bhavs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\bhavs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "so how do I run it offline??", "In this case, you need to update MSVC redistributable (install the latest one) and then you can use 2.1.\r\n\r\nClosing as duplicate.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156\r\n\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37470\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37470\">No</a>\n"]}, {"number": 37469, "title": "Correct getting dataset.element_spec", "body": "fixed a document error: dataset.element_spec", "comments": []}, {"number": 37468, "title": "Neural network with several target vectors to run on TPU", "body": "I try to create my own neural network with several outputs (it follows that there are several target vectors); these target vectors dynamically change during training and depend on the predictions of the neural network. That is, it looks like the following: `y_proba = f(input), y_true = g(y_proba)`, where `input` - input vector, `f` - function of the neural network, `y_proba` - predicted target vector, `g` - function of changing the predicted target vector, `y_true` - target vector.\r\nThe problem is that I cannot implement this neural network using Keras and TPUEstimator in TensorFlow 2.1 to run on TPU.\r\nI tried to solve this problem in two ways.\r\n\r\n**1. The first way to solve the problem**\r\n\r\nI changed the source code of Keras inside TensorFlow to run on CPU and TPU to solve this problem.\r\n\r\nChanges to run on CPU in `tensorflow_core/python/keras/engine/training_arrays.py` inside `model_iteration` function:\r\n```\r\n#calling my own function to change ins_batch\r\nbatch_outs = f(ins_batch)\r\n```\r\n\r\nChanges to run on TPU in `tensorflow_core/python/keras/engine/training_eager.py` inside `_model_loss` function:\r\n```\r\nif targets:\r\n  targets = training_utils.cast_if_floating_dtype_and_mismatch(targets, outs)\r\n#calling my own function to change targets\r\n```\r\n\r\nChanges to run on TPU without experimental functions (setting `experimental_run_tf_function=False` when compiling the model as described in [this](https://github.com/tensorflow/tensorflow/issues/33729#issuecomment-562656807)) in `tensorflow_core/python/keras/engine/training_arrays.py` inside `model_iteration` function:\r\n```\r\nactual_inputs = ins()\r\n#calling my own function to change actual_inputs\r\nbatch_outs = f(actual_inputs)\r\n```\r\n\r\nTo run on CPU, I can dynamically change the target vector inside TensorFlow, and the neural network is trained. But adding this feature inside TensorFlow to run on TPU has many nuances and works like magic and I couldn\u2019t train the neural network.\r\n\r\n**2. The second way to solve the problem**\r\nI tried to implement my neural network based on `TPUEstimator` using `RegressionHead` and `MultiHead`. To do this, I used examples ([mnasnet](https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet) and [convolutional_network](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/convolutional_network.py)).\r\n\r\nThis is the part of the program that implements my own neural network using `TPUEstimator`:\r\n\r\n```\r\ndef get_model(features, input_shape, reuse):\r\n    with tf.variable_scope('model', reuse=reuse):\r\n        seqs = []\r\n        n_filters = 8\r\n        for i in range(n):\r\n            seqs.append(get_conv2d(\r\n                          seqs[-1].get_x(),\r\n                          (3, 3, 8, n_filters),\r\n                          reuse=reuse))\r\n        outputs = []\r\n        heads = []\r\n        for x in seqs:\r\n            outputs.append(OutputLayer(name=\"output_\"+x.name)(x.get_x()))\r\n            heads.append(tf.estimator.RegressionHead(\r\n                                           label_dimension=32*32*8,\r\n                                           name=\"head_\"+x.name))\r\n        head = tf.estimator.MultiHead(heads)\r\n    return head, outputs\r\n\r\ndef model_fn(features, labels, mode, params):\r\n    batch_size = 8 * params['batch_size']\r\n\r\n    head, logits_train = get_model(features, params['input_shape'], reuse=False)\r\n    logits_train_dic = {}\r\n    for i in range(n):\r\n        logits_train[i] = tf.reshape(logits_train[i], (batch_size, 32*32*8,))\r\n        logits_train_dic[\"head_{}\".format(i)] = logits_train[i]\r\n    pred_classes = tf.argmax(logits_train, axis=1)\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        return tf.estimator.tpu.TPUEstimatorSpec(mode, predictions=pred_classes)\r\n\r\n    new_labels = {}\r\n    for key in labels:\r\n        new_labels[key] = labels[key][0]\r\n    loss = 0.0\r\n    for key in logits_train_dic:\r\n        logit_train = logits_train_dic[key]\r\n        loss += tf.square(labels[key]-logit_train)\r\n    loss_op = tf.reduce_mean(loss)\r\n    optimizer = tf.train.AdamOptimizer(learning_rate=params['learning_rate'])\r\n    if params['use_tpu']:\r\n        optimizer = tf.tpu.CrossShardOptimizer(optimizer)\r\n    train_op_fn = lambda loss_op: optimizer.minimize(\r\n                                  loss_op,\r\n                                  global_step=tf.train.get_global_step())\r\n                \r\n    estim_specs = head.create_estimator_spec(\r\n                  features={\"x\": features},\r\n                  labels=new_labels,\r\n                  mode=mode,\r\n                  logits=logits_train_dic,\r\n                  train_op_fn=train_op_fn)\r\n    return estim_specs\r\n\r\nmodel = tf.estimator.tpu.TPUEstimator(\r\n          model_fn, use_tpu=True,\r\n          config=config,\r\n          train_batch_size=train_batch_size,\r\n          params=params)\r\n```\r\nAs I understand it, this way of implementing a neural network makes it possible to dynamically change labels (in particular, target vectors).\r\nI implemented the neural network itself using `TPUEstimator`, but I faced with another problem - \"Graph was finalized\" is displayed, but the training of the neural network does not start.\r\n\r\nHow to implement a neural network with several dynamically changing target vectors, which depend on neural network predictions, on TPU?", "comments": ["@Kirill94a, Will it be possible to post the complete standalone code to analyze the reported issue. Thanks!", "I found the cause of the problem - I made a error when connecting to TPU. Issue may be closed. Thank you!"]}, {"number": 37467, "title": "No gradient defined for op", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\nauto entropy_0 = Xlogy(scope_loss.WithOpName(\"xlogy0\"), input_labels_var, out_segmentation);\r\n- OS Platform: Windows7\r\n- VS2015\r\n- Python version: 3.6\r\n- Bazel version: 0.26.0\r\n- TensorFlow version: tensorflow 2.0\r\n- CUDA/cuDNN version: Cuda 10.0 && Cudnn 7.4.1\r\n- GPU : Geforce GTX 1070 8GB:\r\n\r\nHey, I got some error when I using op like Concat and Xlogy, which remind me that I have to add C++ gradient. But as https://github.com/tensorflow/tensorflow/issues/19944#issuecomment-556315768 says, the gradient op is existing, so how should I do to use the gradient op?\r\nThanks !\r\n\r\n\r\n", "comments": ["@minjac \r\nPlease share simple indented stand alone code, for us to replicate the issue faced by you in our local environment.", "> \r\n> \r\n> @minjac\r\n> Please share simple indented stand alone code, for us to replicate the issue faced by you in our local environment.\r\n------------------------------------------------------------\r\n```\r\n#include \"iostream\"\r\n#include \"vector\"\r\n#include \"iostream\"\r\n#include \"numeric\"\r\n#include \"stdlib.h\"\r\n#include \"tensorflow/core/public/session.h\"\r\n#include \"tensorflow/cc/client/client_session.h\"\r\n#include \"tensorflow/cc/ops/standard_ops.h\"\r\n#include \"tensorflow/cc/framework/gradients.h\"\r\n#include \"tensorflow/core/framework/tensor.h\"\r\n#include \"tensorflow/core/public/session.h\"\r\n#include \"tensorflow/cc/ops/array_ops.h\"\r\n\r\nusing namespace std;\r\nusing namespace tensorflow;\r\nusing namespace tensorflow::ops;\r\nvoid main()\r\n{\r\n\tScope scope = Scope::NewRootScope();\r\n\tauto input = Placeholder(scope.WithOpName(\"input\"), DT_FLOAT);\r\n\r\n\tmap<string, Output> vars;\r\n\tmap<string, Output> assigns;\r\n\tvars[\"0\"] = Variable(scope.WithOpName(\"w1\"), { 1 }, DT_FLOAT);\r\n\tvars[\"1\"] = Variable(scope.WithOpName(\"w2\"), { 1 }, DT_FLOAT);\r\n\tassigns[\"0\"] = Assign(scope.WithOpName(\"assign1\"), vars[\"0\"], Input::Initializer(0.f, { 1 }));\r\n\tassigns[\"1\"] = Assign(scope.WithOpName(\"assign2\"), vars[\"1\"], Input::Initializer(0.f, { 1 }));\r\n\r\n\tvector<Output> weights;\r\n\tweights.push_back(vars[\"0\"]);\r\n\tweights.push_back(vars[\"1\"]);\r\n\r\n\tauto mul1 = Mul(scope.WithOpName(\"mul1\"), input, vars[\"0\"]);\r\n\tauto mul2 = Mul(scope.WithOpName(\"mul2\"), input, vars[\"1\"]);\r\n\r\n\tauto xlogy = Xlogy(scope.WithOpName(\"xlogy\"), mul1, mul2);\r\n\r\n\tauto label = Placeholder(scope.WithOpName(\"label\"), DT_FLOAT);\r\n\tauto loss = SquaredDifference(scope.WithOpName(\"label\"), xlogy, label);\r\n\r\n\tvector<Output> grad_outputs;\r\n\tTF_CHECK_OK(AddSymbolicGradients(scope, { loss }, weights, &grad_outputs));\r\n\r\n\tint index = 0;\r\n\tvector<Operation> out_grads;\r\n\tfor (pair<string, Output> i : vars)\r\n\t{\r\n\t\tstring s_index = to_string(index);\r\n\t\tauto adam = ApplyGradientDescent(scope, i.second, 0.01f, { grad_outputs[index] });\r\n\t\tout_grads.push_back(adam.operation);\r\n\t\tindex++;\r\n\t}\r\n\tTensor train_input(DT_FLOAT, {1});\r\n\ttrain_input.scalar<float>()(0) = 0.1;\r\n\tTensor train_label(DT_FLOAT, {1});\r\n\ttrain_label.scalar<float>()(0) = 0.2;\r\n\r\n\tunique_ptr<ClientSession> t_session;\r\n\tvector<Output> ops_to_run;\r\n\tfor (pair<string, Output>i : assigns)\r\n\t{\r\n\t\tops_to_run.push_back(i.second);\r\n\t}\r\n\tt_session = unique_ptr<ClientSession>(new ClientSession(scope));\r\n\tTF_CHECK_OK(t_session->Run(ops_to_run, nullptr));\r\n\r\n\tvector<Tensor> out_tensors;\r\n\tTF_CHECK_OK(t_session->Run({ { input, train_input},{ label, train_label } },{loss},out_grads,&out_tensors));\r\n}\r\n```\r\n--------------------------------------------\r\nthanks", "@minjac\r\nI ran your code on nightly and [face this error](https://colab.sandbox.google.com/gist/Saduf2019/aa6a1895a9c279c80022fd124bfd6307/37467.ipynb)", "> \r\n> \r\n> @minjac\r\n> I ran your code on nightly and [face this error](https://colab.sandbox.google.com/gist/Saduf2019/aa6a1895a9c279c80022fd124bfd6307/37467.ipynb)\r\n\r\nI had not faced this error, because I ran my code on VS2015 by using tensorflow_cc.dll.", "Gradient for `Xlogy` is not implemented yet. But you are welcome to send a PR! You can take a look at the existing C++ gradient implementations [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/cc/gradients) and the python gradient for `Xlogy` [here](https://github.com/tensorflow/tensorflow/blob/311792b00583b90e507e2c7f8365b4c5ff728874/tensorflow/python/ops/math_grad.py#L660).", "@minjac It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version 2.5 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37467\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37467\">No</a>\n"]}, {"number": 37466, "title": "Update api_def_Ceil.pbtxt", "body": "tensorflow/core/api_def/base_api/api_def_Ceil.pbtxt", "comments": ["Similar to #37605 and #37604, we need to first decide if doctests should go into the global API files instead of in Python ones.", "@mihaimaruseac  can you once check issue #25802 and the pull requests associated with it, they are updating the pbtxt's.", "Thank you for the link to that issue. It seems that only a few of the PRs from there are still in code as of now, most got reverted. Which gives more sense to me asking to wait on this for a decision about whether to include Python tests on general language API defs.\r\n\r\nI will come back to all these PRs in a few days once a decision is taken and will guide on how to progress.", "@mihaimaruseac off topic doubt,if some thing is there in nightly version but not in stable when does it get included in stable version? I mean does it get included in the next TF release like 2.2.x?", "For stable releases it needs to be on `master` when the branch for the release gets cut. So it will go to TF 2.3 now."]}, {"number": 37465, "title": "Trigger sync only if exiting async_scope without exception.", "body": "If code in async_scope already throws an exception, triggering another remote sync might cause derived errors to be returned and lead to unexpected exception types. (Fixed b/151117430)\r\n\r\nPiperOrigin-RevId: 300013192\r\nChange-Id: Id310ca105ce2fdfbc0f19c5362834a56906fb35f", "comments": []}]