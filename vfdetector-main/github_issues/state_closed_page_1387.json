[{"number": 11453, "title": "python create model and c++ uses model without bazel", "body": "I just have used python to create ckpt model. But i have to use c++ to test my picture without bazel. Some one can help me? I am so eager to solve this problem! Thanks!!", "comments": ["Here's [a tutorial](https://www.tensorflow.org/performance/xla/tfcompile) for compiling standalone programs from a TensorFlow graph.\r\n\r\nPS: Your question is more suited for [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) with the tag tensorflow because it's not a bug report."]}, {"number": 11452, "title": "20 minutes for compiling a single file(conv_grad_ops_3d.cc)? ", "body": "-----------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n2533ada7dd45b84d60677b8735e013d21044651a\r\n\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\n\r\nIt took about 20 minutes to compile \"conv_grad_ops_3d.cc\"\r\nThe whole build took:\r\nINFO: Elapsed time: 2241.764s, Critical Path: 1509.12s\r\n\r\nTherefore this file took about 50% compile time. Why? \r\n\r\n### Source code / logs\r\nhttp://ci.tensorflow.org/job/tf-master-win-bzl/1239/consoleFull\r\n\r\n\r\n", "comments": ["I don't know, but individual files' build times are not necessarily all that meaningful. The build for that file may be blocked on I/O by other build processes. Since we build several files in parallel, 20 min is much less than half the build time. \r\n\r\nI see this was a Windows build -- @meteorcloudy may have some insight on what may be happening. \r\n\r\nAt this point I don't think this is a bug, so I will close this issue.", "Yes, I noticed this before. See https://github.com/tensorflow/tensorflow/issues/10521"]}, {"number": 11451, "title": "Enable building label_image with jpeg/gif/png decoder for Android.", "body": "This PR adds the Android platform building of the  jpeg, gif and png decoder to the C++ image classification demo \"label_image\". \r\nIt enables the evaluation of practical dataset such as ImageNet on Android platform devices, for both model precision, and hardware performance. ", "comments": ["Can one of the admins verify this patch?", "Perhaps Andrew Harp andrewharp@google.com<mailto:andrewharp@google.com> or Peter Warden petewarden@google.com<mailto:petewarden@google.com>.\r\n\r\nHi Peter and Andrew,\r\nNot quite sure if you can help to review this PR.\r\nThank you so much!\r\n\r\nJi\r\n\r\nFrom: Tensorflow Jenkins [mailto:notifications@github.com]\r\nSent: Wednesday, July 12, 2017 6:44 PM\r\nTo: tensorflow/tensorflow\r\nCc: Ji Qiu (\u90b1\u5409); Author\r\nSubject: Re: [tensorflow/tensorflow] Enable building label_image with jpeg/gif/png decoder for Android. (#11451)\r\n\r\n\r\nCan one of the admins verify this patch?\r\n\r\n\u2014\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/pull/11451#issuecomment-314724530>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AXR6uuI3ISZMtEqOvME2PJXJyN0_k_YEks5sNKNYgaJpZM4OVcc_>.\r\n", "Jenkins, test this please.", "Hi Pete, \r\nI can fully understand your concern. Please review the new PR #11475.\r\nThank you!\r\n\r\n\r\nJi\r\n\r\n\r\n  \r\n\r\n\r\n\r\n\r\n"]}, {"number": 11450, "title": "Link to Benchmarks not working", "body": "The link to [Benchmarks ](https://www.tensorflow.org/community/benchmarks)on the page [https://www.tensorflow.org/community/](https://www.tensorflow.org/community/) is not working. ", "comments": ["Very odd, not sure why that file disappeared.  Fixed.", "Thanks @dr4b ."]}, {"number": 11449, "title": "Fix linking options issued by bazel in order to make gradients register", "body": "[Link](https://github.com/tensorflow/tensorflow/issues/11395#issuecomment-313956256) to the discussion", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I have just signed it!!!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please."]}, {"number": 11448, "title": "Updating install_golang.sh - bumping to 1.8.3", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "FYI @av8ramit @gunan \r\n", "Jenkins, test this please.\r\n\r\nThe CI failures seem like infrastructure failures.\r\nTrying again", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 11446, "title": "Is tf.split() not create a new  object? Using tf.split() and tf.concat() will has some bugs......", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution , :\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from *:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n1.2\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n8.0\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\ntf.split() is not create a new  object. Using tf.split() and tf.concat() will has some bugs......\r\nFor example, I has a [64,20] tensor, and split to [64,15], and then concat it with a [64,1] tensor, but I obtain a [64,21] tensor instead [64,16]\r\n\r\n### Source code / logs\r\n```\r\n`import tensorflow as tf\r\nimport  numpy\r\nfrom tensorflow.python.ops import tensor_array_ops, control_flow_ops\r\ngen_x = tensor_array_ops.TensorArray(dtype=tf.int32,size=1,dynamic_size=True, infer_shape=True)\r\ntmp = tf.constant(-1,shape=[64,20])\r\nt = tf.split(tmp,[15],1)\r\nt = tf.squeeze(t)\r\ngen_x = gen_x.write(0,tf.constant(1,shape=[64]))\r\ngen_tmp = tf.transpose(gen_x.stack(), perm=[1, 0])\r\nfeature = tf.concat([t,gen_tmp],1)\r\n\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.Session(config=config)\r\nprint sess.run(tf.shape(feature))`\r\n```\r\n\r\nResult:\r\n\r\n$ python test.py \r\n\r\n[64 21]\r\n\r\n\r\n", "comments": ["This is not a bug. You've made a mistake. In fact, your example will produce an exception:\r\n\r\n```python\r\ntmp = tf.constant(-1, shape=[64, 20])\r\nt = tf.split(tmp, [15], 1)\r\n```\r\n>ValueError: Sum of output sizes must match the size of the original Tensor along the split dimension or the sum of the positive sizes must be less if it contains a -1 for 'split' (op: 'SplitV') with input shapes: [64,20], [1], [] and with computed input tensors: input[1] = <15>, input[2] = <1>.\r\n\r\n", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 11445, "title": "Disable nn_test on Windows", "body": "Disable `//tensorflow/python:nn_test` util it's fixed.\r\nRelated issue: https://github.com/tensorflow/tensorflow/issues/11345\r\n@gunan ", "comments": ["Jenkins, retest this please", "failure message is unrelated. change safe to merge."]}, {"number": 11444, "title": "Fix typos", "body": "This PR fixes some typos: `Compatble`, `objets`, `overriden`, `reseting`, `an an`, `libraryh`, and `a a`.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 11443, "title": "For deep learning", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "The commits in this PR has already been merged into master."]}, {"number": 11442, "title": "The window in tf.contrib.rnn.AttentionCellWrapper()", "body": "It is very strange to have a fixed-sized window in the implementation of AttentionCellWrapper, because there is no description about it in the Bahdanau paper. Of course, the behave of the window is not clear, which makes me confused. Can anyone explain it?", "comments": ["This is old legacy code.  I hope we can delete it soon.  Use the attention mechanism in tf.contrib.seq2seq instead.", "@ebrevdo Is there a way then to harness the new AttentionWrapper (from tf.contrib.seq2seq) for sequence classification (attention reduction)? E.g. as a words-to-sentence reduction step.\r\nI mostly refer to the [Hierarchical Attention Networks for Document Classification](https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf) paper. tensorflow/nmt tutorial mostly shows it in use with a decoder.\r\nThanks."]}, {"number": 11441, "title": "wide_n_deep Tutorial processing large data", "body": "I am a freshman of tensorflow, and want to use the wide & deep network.\r\n## I see the code of wide_n_deep tutorial, find that in \"def train_and_eval()\" function reading the total data using pandas at once. \r\n## I confuse if the data is large maybe 5 GB or more, and I cannot read all the data in memory, and how can I processing it? \r\n\r\nThank you~~~\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 11440, "title": "Linear Model Tutorial: How to extract prediction? ", "body": "Similar to this issue: https://github.com/tensorflow/tensorflow/issues/97 \r\n\r\nFor the \"[TensorFlow Linear Model Tutorial](https://www.tensorflow.org/tutorials/wide)\", the project implies that it will end with a program that, based on input data, outputs a 0 or 1: \r\n\r\n> Given census data about a person such as age, gender, education and occupation (the features), we will try to predict whether or not the person earns more than 50,000 dollars a year (the target label). We will train a logistic regression model, **and given an individual's information our model will output a number between 0 and 1, which can be interpreted as the probability that the individual has an annual income of over 50,000 dollars.**\r\n\r\nHowever, it seems that the tutorial is incomplete. The last steps have you calculate the accuracy of the trained model: \r\n\r\n> The first line of the output should be something like accuracy: 0.83557522, which means the accuracy is 83.6%. Feel free to try more features and transformations and see if you can do even better!\r\n\r\nAnd then point you in the direction of the full example code: \r\n\r\n> If you'd like to see a working end-to-end example, you can download our [example code.](https://www.github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/learn/wide_n_deep_tutorial.py) and set the model_type flag to wide. \r\n\r\nWhen I run the final program, my output looks like this: \r\n\r\n> `accuracy: 0.989583`\r\n> `accuracy/baseline_label_mean: 0.364583`\r\n> `accuracy/threshold_0.500000_mean: 0.989583`\r\n> `auc: 1.0`\r\n> `auc_precision_recall: 1.0`\r\n> `global_step: 3000`\r\n> `labels/actual_label_mean: 0.364583`\r\n> `labels/prediction_mean: 0.369466`\r\n> `loss: 0.0242721`\r\n> `precision/positive_threshold_0.500000_mean: 0.972222`\r\n> `recall/positive_threshold_0.500000_mean: 1.0`\r\n\r\nOnly `accuracy` is explained in the instructions, and it doesn't seem that there are final steps to complete the tutorial: to take a set of given values, and predict `income_bracket.` Can someone provide a code example, or point to documentation on how to extract final predictions after training the model?\r\n\r\nThanks! ", "comments": ["@jhseu I don't know who owns this tutorial, but it does seem as if it would be nice to add instructions on outputting the predictions.", "I have the same issue with the next tutorial as well ([Wide & Deep Learning](https://www.tensorflow.org/tutorials/wide_and_deep)), which uses the same \"end to end\" example code: https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/learn/wide_n_deep_tutorial.py \r\n\r\nThe tutorial ends without a practical explanation on how to use the model. How would I take a set of given values (i.e., `\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\", \"hours_per_week\", \"native_country\"`) and predict `income_bracket`? ", "Adding @hengtze who owns these tutorials.", "I could extract prediction on this tutorials as below code.\r\n\r\n```python\r\npredict = m.predict(input_fn=input_fn(test_file.name, num_epochs=1, shuffle=False))\r\n\r\nfor i, p in enumerate(predict):\r\n    print(\"Prediction : {}, Probablities : {}\".format(p[\"classes\"], p[\"probabilities\"]))\r\n```", "i'm not finding m.predict function in main code. Can you please explain how this prediction code works", "@ryutah The answer still isn't very clear to us newbies, so I've created a github question (with a small bounty) to get a working example of how to extract the prediction:\r\nhttps://stackoverflow.com/questions/47494658/saving-and-running-wide-deep-py-model ", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "@jhseu do you know who the current owner of the tutorial is? It would be nice to get this fixed.", "Neal, since you last touched the tutorial, it would be good to add .predict() examples there. Also, marked contributions welcome since the tutorial code is in on github under tensorflow/docs_src/tutorials", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Are we getting the updated tutorial anytime soon with prediction ?", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Added a PR #17276 for the fix."]}, {"number": 11439, "title": "Branch 161598138", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 11438, "title": "Switching Session to MonitoredTrainingSession Produces Negative Dimension Tensors from Queues", "body": "I already [posted](https://stackoverflow.com/questions/44831580/receiving-negative-input-dimensions-with-tensorflow-monitoredtrainingsession) on StackOverflow, but received no response.\r\n\r\nI've made some changes to my code since posting on StackOverflow, but the problem is still the same: the readers and queues I use without a problem with a vanilla `tf.Session()` work fine, but if I try to switch to a `tf.train.MonitoredTrainingSession`, I get the following error:\r\n\r\n`InvalidArgumentError (see above for traceback): Shape [15,-1,4] has negative dimensions\r\n\t [[Node: define_inputs/y = Placeholder[dtype=DT_FLOAT, shape=[15,?,4], _device=\"/job:local/replica:0/task:0/cpu:0\"]()]]`\r\n\r\nI'm using TensorFlow v1.2.0-5-g435cdfc 1.2.1. Here's my code. This works:\r\n\r\n\r\n    # fix random seed to permit comparison between training runs\r\n    tf.set_random_seed(seed=0)\r\n\r\n    # define graph\r\n    model = import_model()\r\n\r\n    with tf.Session() as monitored_sess:\r\n\r\n        monitored_sess.run(tf.global_variables_initializer())\r\n\r\n        # create coordinator to handle threading\r\n        coord = tf.train.Coordinator()\r\n\r\n        # start threads to enqueue input minibatches for training\r\n        threads = tf.train.start_queue_runners(sess=monitored_sess, coord=coord)\r\n\r\n        data = monitored_sess.run([training_data])\r\n        x, y, x_lengths, y_lengths = data[0]\r\n\r\n        # when done, ask the threads to stop\r\n        coord.request_stop()\r\n\r\n        # wait for threads to finish\r\n        coord.join(threads)\r\n\r\nBut then, this code doesn't work:\r\n\r\n    tf.set_random_seed(seed=0)\r\n\r\n    # define graph\r\n    model = import_model()\r\n\r\n    # create a one process cluster with an in-process server\r\n    server = tf.train.Server.create_local_server()\r\n\r\n    # define hooks for writing summaries and model variables to disk\r\n    hooks = construct_training_hooks(model.summary_op, model.loss, train_log_directory)\r\n\r\n    # create monitored training session to write model variables and summaries to disk\r\n    with tf.train.MonitoredTrainingSession(master=server.target,\r\n                                           config=tf.ConfigProto(allow_soft_placement=True),\r\n                                           is_chief=True,\r\n                                           hooks=hooks) as monitored_sess:\r\n\r\n        # create coordinator to handle threading\r\n        coord = tf.train.Coordinator()\r\n\r\n        # start threads to enqueue input minibatches for training\r\n        threads = tf.train.start_queue_runners(sess=monitored_sess, coord=coord)\r\n\r\n        # train\r\n        data = monitored_sess.run([training_data])\r\n        x, y, x_lengths, y_lengths = data[0]\r\n\r\n        # when done, ask the threads to stop\r\n        coord.request_stop()\r\n\r\n        # wait for threads to finish\r\n        coord.join(threads)\r\n\r\nThe function `construct_training_hooks` is pretty straightforward:\r\n\r\n\r\n    def construct_training_hooks(summary_op, loss, train_log_directory):\r\n        hooks = [tf.train.StopAtStepHook(last_step=tf.flags.FLAGS.max_steps),\r\n                 tf.train.CheckpointSaverHook(checkpoint_dir=train_log_directory,\r\n                                              saver=tf.train.Saver(),\r\n                                              save_steps=5),\r\n                 tf.train.SummarySaverHook(output_dir=train_log_directory,\r\n                                           summary_op=summary_op,\r\n                                           save_steps=1),\r\n                 tf.train.NanTensorHook(loss_tensor=loss)]\r\n\r\n        return hooks\r\n", "comments": ["Is it possible for you to try the code at head? I think this may be fixed (at least that error is no longer generated by the shape code).", "Sorry if this is a really stupid question, but what does \"try the code at head\" mean?", "I'm sorry I wasn't clear! (Not a stupid question :) I meant to try the most recently checked-in code which I think has fixed this problem, rather than the version 1.2.0 that you report above.\r\n\r\nIf you don't want to wait for the next release that would mean [installing from sources](https://www.tensorflow.org/install/install_sources) or trying the [nightly binaries](https://github.com/tensorflow/tensorflow).", "@MicaelCarvalho Any update on then this is going to be release?\r\nI see the same problem when restoring a saved session. ", "I'm unable to reproduce this problem.\r\n\r\nPlease provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "@ali01 , previously I was using TensorFlow v1.2.0-5-g435cdfc 1.2.1 on macOS Sierra version 10.12.6. I used pip to install.\r\n\r\nI'll try to create a simplified example that recreates the problem, but I had to abandon that code because it wasn't working and I needed to make progress on my internship project, so I don't know if/when I'll get to it.", "@mrry, do you have any insight?", "The original error is printed in TF 1.2 when you call `sess.run()` on some tensor/op that depends on a placeholder and do not feed that placeholder. (The error message is improved in TF 1.3.) The cause is probably a hidden `sess.run()` call in one of the queue runners.\r\n\r\nIIRC, you should never call `tf.train.start_queue_runners(sess)` where `sess` is a `MonitoredSession`, because it will attempt to run hooks in every queue runner `run()` call. These hooks (e.g. for summaries) might be what is causing the placeholder to be evaluated. Instead, you must rely on the `MonitoredSession` to start the queue runners for you.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Closing due to lack of activity.", "i have the save questions. is there any examples to show how to start queue_runners rely on MonitoredSession ? thanks. @mrry "]}, {"number": 11437, "title": "Fix unpickling/copying tf.app.flags.FLAGS", "body": "pickle.load() and copy.copy() check for the presence of  __setstate__().\r\nThe problem is that this check is made in the freshly allocated instance which\r\nhas not been __init__()-ed. Thus it's __dict__ is completely empty and\r\n__getattr__() fail with KeyError.\r\n\r\nThe fix is to check if __parsed is in the __dict__ and raise\r\nAttributeError if it isn't.", "comments": ["Jenkins, test this please.", "Looks like `//tensorflow/python/kernel_tests:barrier_ops_test` is flacky on macOS or just too little timeout - no problems apart from it."]}, {"number": 11436, "title": "Add CODEOWNERS", "body": "Added what we know about contrib mainly, and some well-separated components.\r\n\r\nThe list of names behind each component is not perfect, I took this from a partially definitely outdated doc, but I suggest we maintain it here instead of in that doc.", "comments": ["Awesome!"]}, {"number": 11435, "title": "Feature request / possible bug: cannot take second derivative of a MaxPool", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OSX El Capitan\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: v1.0.0-65-g4763edf-dirty 1.0.1, but I've tested it on later versions too, and looking at the source code in master it doesn't seem like it's available\r\n- **Python version**: 3.5.1, but I've reproduced it on 2.7\r\n\r\n### Describe the problem\r\n\r\nIf I understand correctly, MaxPool operations should be many-times differentiable, but in Tensorflow you can only differentiate them once, which makes it impossible to inspect second derivatives for CNNs or do [double backpropogation](http://yann.lecun.com/exdb/publis/pdf/drucker-lecun-91.pdf).\r\n\r\n### Source code / logs\r\n\r\nYou can reproduce it very simply this way:\r\n```python\r\nimport tensorflow as tf\r\nX = tf.Variable(tf.random_normal([1,1,1,1]))\r\nP = tf.nn.max_pool(X, [1,1,1,1], [1,1,1,1], 'SAME')\r\ng1 = tf.gradients(P, X)[0]\r\ng2 = tf.gradients(g1, X)[0] # this line fails\r\n```\r\nraises\r\n```\r\nLookupError: No gradient defined for operation 'gradients_17/MaxPool_10_grad/MaxPoolGrad' (op type: MaxPoolGrad)\r\n```", "comments": ["Related: https://github.com/tensorflow/tensorflow/issues/6143", "> Related: #6143\r\n\r\nAgh, I hadn't tested with a new enough version. Thanks \ud83d\ude05 "]}, {"number": 11434, "title": "My CNN is not learning ,who can help me fix this problem? Thx !", "body": "[CNN.txt](https://github.com/tensorflow/tensorflow/files/1139040/CNN.txt)\r\n\r\nThat's the whole code of my CNN, the data for training, validation, test are all organized myself, which is not the problem.\r\nWhile training this CNN, the result in Console showed that all weights and biases did not change. If I replace the RELU in six convolutional layers with Sigmoid, the CNN performed well.\r\nWho can help me fix this problem? Thanks anyway ~", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 11433, "title": "My CNN is not learning , who can help me fix this porblem ?", "body": "import tensorflow as tf\r\nimport Transform_Data_New as td\r\nimport numpy as np\r\n\r\nmax_accuracy=0\r\ntimer=0\r\nEpoch=300\r\nlearning_rate=1e-3\r\n\r\ntraining_batch_size=100\r\nvalidation_batch_size=40\r\ntest_batch_size=40\r\n\r\nlen_training_data, validation, test = td.Initialization(validation_batch_size,test_batch_size)\r\niteration_time=int(len_training_data/training_batch_size)+1\r\n\r\nsess=tf.InteractiveSession()\r\n\r\ndef weight_variable(shape,Name):\r\n    initial = tf.random_normal(shape,stddev=0.2,mean=0.5)\r\n    return tf.Variable(initial,name=Name)\r\n\r\ndef bias_variable(shape,Name):\r\n    initial=tf.constant(0.1, shape=shape,)\r\n    return tf.Variable(initial,name=Name)\r\n\r\ndef conv2d(x,w):\r\n    return tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding='SAME')\r\n\r\ndef max_pool_2x2(x):\r\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1],padding='SAME')\r\n\r\nwith tf.name_scope('Input'):\r\n    with tf.name_scope('Input_x'):\r\n        x = tf.placeholder(tf.float32,shape=[None,1024])\r\n    with tf.name_scope('Input_y'):\r\n        y_ = tf.placeholder(tf.float32,shape=[None,7])\r\n\r\nwith tf.name_scope('Conv_1'):\r\n    with tf.name_scope('W_conv1'):\r\n        w_conv1=weight_variable([3,3,1,8],'w_conv1')\r\n    with tf.name_scope('B_conv1'):\r\n        b_conv1=bias_variable([8],'b_conv1')\r\n    with tf.name_scope('x_image'):\r\n        x_image=tf.reshape(x,[-1,32,32,1])\r\n    with tf.name_scope('H_conv1'):\r\n        h_conv1=tf.nn.relu(tf.nn.bias_add(conv2d(x_image,w_conv1),b_conv1))\r\n\r\nwith tf.name_scope('Conv_2'):\r\n    with tf.name_scope('W_conv2'):\r\n        w_conv2=weight_variable([3,3,8,16],'w_conv2')\r\n    with tf.name_scope('B_conv2'):\r\n        b_conv2=bias_variable([16],'b_conv2')\r\n    with tf.name_scope('H_conv2'):\r\n        h_conv2=tf.nn.relu(tf.nn.bias_add(conv2d(h_conv1,w_conv2),b_conv2))\r\n    with tf.name_scope('H_pool2'):\r\n        h_pool2=max_pool_2x2(h_conv2)\r\n\r\nwith tf.name_scope('Conv_3'):\r\n    with tf.name_scope('W_conv3'):\r\n        w_conv3=weight_variable([3,3,16,32],'w_conv3')\r\n    with tf.name_scope('B_conv3'):\r\n        b_conv3=bias_variable([32],'b_conv3')\r\n    with tf.name_scope('H_conv3'):\r\n        h_conv3=tf.nn.relu(tf.nn.bias_add(conv2d(h_pool2,w_conv3),b_conv3))\r\n\r\nwith tf.name_scope('Conv_4'):\r\n    with tf.name_scope('W_conv4'):\r\n        w_conv4=weight_variable([3,3,32,64],'w_conv4')\r\n    with tf.name_scope('B_conv4'):\r\n        b_conv4=bias_variable([64],'b_conv4')\r\n    with tf.name_scope('H_conv4'):\r\n        h_conv4=tf.nn.relu(tf.nn.bias_add(conv2d(h_conv3,w_conv4),b_conv4))\r\n    with tf.name_scope('H_pool4'):\r\n        h_pool4=max_pool_2x2(h_conv4)\r\n\r\nwith tf.name_scope('Conv_5'):\r\n    with tf.name_scope('W_conv5'):\r\n        w_conv5=weight_variable([3,3,64,128],'w_conv5')\r\n    with tf.name_scope('B_conv5'):\r\n        b_conv5=bias_variable([128],'b_conv5')\r\n    with tf.name_scope('H_conv5'):\r\n        h_conv5=tf.nn.relu(tf.nn.bias_add(conv2d(h_pool4,w_conv5),b_conv5))\r\n\r\nwith tf.name_scope('Conv_6'):\r\n    with tf.name_scope('W_conv6'):\r\n        w_conv6=weight_variable([3,3,128,256],'w_conv6')\r\n    with tf.name_scope('B_conv6'):\r\n        b_conv6=bias_variable([256],'b_conv6')\r\n    with tf.name_scope('H_conv6'):\r\n        h_conv6=tf.nn.relu(tf.nn.bias_add(conv2d(h_conv5,w_conv6),b_conv6))\r\n    with tf.name_scope('H_pool6'):\r\n        h_pool6=max_pool_2x2(h_conv6)\r\n\r\nwith tf.name_scope('Full_Connected_Layer_1'):\r\n    with tf.name_scope('W_fc1'):\r\n        w_fc1=weight_variable([4*4*256,1024],'w_fc1')\r\n    with tf.name_scope('B_fc1'):\r\n        b_fc1=bias_variable([1024],'b_fc1')\r\n    with tf.name_scope('H_pool_flat'):\r\n        h_pool_flat=tf.reshape(h_pool6,[-1,4*4*256])\r\n    with tf.name_scope('H_fc1'):\r\n        h_fc1=tf.nn.relu(tf.matmul(h_pool_flat,w_fc1)+b_fc1)\r\n\r\nwith tf.name_scope('Full_Connected_Layer_2'):\r\n    with tf.name_scope('W_fc2'):\r\n        w_fc2=weight_variable([1024,7],'w_fc2')\r\n    with tf.name_scope('B_fc2'):\r\n        b_fc2=bias_variable([7],'b_fc2')\r\n    with tf.name_scope('Y_conv'):\r\n        y_conv=tf.nn.softmax(tf.matmul(h_fc1,w_fc2)+b_fc2)\r\n\r\nwith tf.name_scope('Cross_Entropy'):\r\n    cross_entropy=-tf.reduce_sum(y_*tf.log(tf.clip_by_value(y_conv,1e-10,1.0)))\r\n    tf.summary.scalar('Cross_Entropy',cross_entropy)\r\nwith tf.name_scope('Train_Step'):\r\n    train_step=tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\r\nwith tf.name_scope('Correct_prediction'):\r\n    distribution=[tf.arg_max(y_,1),tf.arg_max(y_conv,1)]\r\n    correct_prediction=tf.equal(distribution[0],distribution[1])\r\nwith tf.name_scope('Accuracy'):\r\n    accuracy=tf.reduce_mean(tf.cast(correct_prediction,\"float\"))\r\n    tf.summary.scalar('Accuracy',accuracy)\r\n\r\nmerged=tf.summary.merge_all()\r\nwriter=tf.summary.FileWriter('D:/Log',sess.graph)\r\nepoch=0\r\n\r\nsaver=tf.train.Saver()\r\nFirst_training=True\r\ncheckpoint_dir='D:\\\\Checkpoint\\\\model.ckpt'\r\n\r\nif First_training==False:\r\n    ckpt=tf.train.get_checkpoint_state(checkpoint_dir)\r\n    if ckpt and ckpt.model_checkpoint_path:\r\n        saver.restore(sess,checkpoint_dir)\r\nelse:\r\n    sess.run(tf.global_variables_initializer())\r\n\r\nfor i in range(Epoch*iteration_time+1):\r\n    #Batch Size\r\n    batch = td.next_batch(training_batch_size)\r\n    train_step.run(feed_dict={x: batch[0], y_: batch[1]})\r\n    if i % (iteration_time) == 0 and i != 0:\r\n        epoch += 1\r\n        train_accuracy=accuracy.eval(feed_dict={x: batch[0], y_: batch[1]})\r\n        validation_accuracy_resultSet = []\r\n        for j in range(len(validation)):\r\n            validation_accuracy = accuracy.eval(feed_dict={x: validation[j][0], y_: validation[j][1]})\r\n            validation_accuracy_resultSet.append(validation_accuracy)\r\n        validation_accuracy = int(np.sum(validation_accuracy_resultSet)) / len(validation_accuracy_resultSet)\r\n        if validation_accuracy<=0.7:\r\n            learning_rate=1e-3\r\n        if validation_accuracy>0.7 and validation_accuracy<=0.8:\r\n            learning_rate=5e-4\r\n        if validation_accuracy>0.8 and validation_accuracy<=0.9:\r\n            learning_rate=1e-4\r\n        if validation_accuracy>0.9:\r\n            learning_rate=5e-5\r\n        print(\"Epoch %d , training accuracy %g,Validation Accuracy: %g\" % (epoch, train_accuracy, validation_accuracy))\r\n\r\n        result = sess.run(merged, feed_dict={x: batch[0], y_: batch[1]})\r\n        writer.add_summary(result, epoch)\r\n        saver.save(sess,checkpoint_dir,global_step=epoch)\r\n        if validation_accuracy>max_accuracy:\r\n            max_accuracy=validation_accuracy\r\n            timer=0\r\n        else:\r\n            timer+=1\r\n            if timer>10:   #\u539f\u6765\u662f10\r\n                break\r\n\r\nconfusion_matrics=np.zeros([7,7],dtype=\"int\")\r\ntest_accuracy_resultSet=[]\r\nfor j in range(len(test)):\r\n    matrix_row, matrix_col = sess.run(distribution, feed_dict={x: test[j][0], y_: test[j][1]})\r\n    for m, n in zip(matrix_row, matrix_col):\r\n        confusion_matrics[m][n] += 1\r\n    test_accuracy = accuracy.eval(feed_dict={x: test[j][0], y_: test[j][1]})\r\n    test_accuracy_resultSet.append(test_accuracy)\r\ntest_accuracy = np.sum(test_accuracy_resultSet) / len(test_accuracy_resultSet)\r\n\r\nprint(\"Test Accuracy :\",test_accuracy)\r\nprint(np.array(confusion_matrics.tolist()))\r\n\r\n\r\n\r\nThat's the whole code of my CNN, the data for training, validation, test are all organized myself, which is not the problem.\r\nWhile training this CNN, the result in Console showed that all weights and biases did not change. If I replace the RELU in six convolutional layers with Sigmoid, the CNN performed well.\r\nWho can help me fix this problem? Thanks anyway ~\r\n", "comments": []}, {"number": 11432, "title": "Enable building grpc+verbs runtime on any Linux box", "body": "I don't have a Windows machine so it is probably broken on Windows. Currently I don't have an idea on how to achieve cross-platform portability. Any suggestions?", "comments": ["Can one of the admins verify this patch?", "Is there any advantage to compile the system-level library? If one wants to compile verbs, hopefully he has the hardware, or at least install the verbs library first. For those boxes that has verbs library pre-installed, if the existing verbs version is different from the compiled one listed in your PR, will that be a problem. My other concern is that tensor flow compile time is already long, this just adds to that."]}, {"number": 11431, "title": "Rank calculated incorrectly using 'gen_array_ops.rank()'", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 'v1.2.1-0-gb4957ff', '1.2.1'\r\n- **Python version**:  2.7.12\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: No GPU\r\n- **GPU model and memory**: No GPU\r\n- **Exact command to reproduce**: //tensorflow/python/kernel_tests:transpose_op_test\r\n\r\n### The problem\r\nWhile running TensorFlow tests such as  `//tensorflow/python/kernel_tests:transpose_op_test\r\n` or `//tensorflow/contrib/distributions:mixture_test`, \r\non a big endian system they fail with error: `ValueError: Dimension must be 2 but is 0 for 'Mixture_1/sample/Categorical/sample/transpose' (op: 'Transpose') with input shapes: [1,4], [0].`  \r\n\r\nAfter debugging a little, realized that, while calculating `rank` of a tensor in [transpose method](https://github.com/tensorflow/tensorflow/blob/v1.2.1/tensorflow/python/ops/array_ops.py#L1276), the rank is calculated incorrectly via `gen_array_ops.rank(a)`.\r\n\r\nI tried replacing `rank = gen_array_ops.rank(a)` to use the `array_ops.rank(a)` method instead and it works perfectly fine.\r\n\r\nI tried the same change on an x86 machine and it doesn't break anything there.\r\n\r\nI am unable to understand the difference between the two methods though. Also is there some major impact of above change which is not very obvious?", "comments": ["Switching to `array_ops.rank(a)` presumably fixes the problem because it's using the statically inferred rank from Python. I expect this would fail if, for some reason, `a` had a statically unknown rank (e.g. `a = tf.placeholder(tf.float32, shape=None)`).\r\n\r\nGiven the type punning involved in the [implementation of `TensorShape::dims()`](https://github.com/tensorflow/tensorflow/blob/99a38ffd9d77c55ca6d0c373c6d4b72686284ac5/tensorflow/core/framework/tensor_shape.h#L219), I'm not totally surprised that it fails on a big-endian machine. Hopefully this provides a pointer to the correct solution.", "Thanks @mrry for your comments.\r\n\r\nSo switching to `array_ops.rank(a)` would not be the right thing to do even though majority of the tests are passing with this!  \r\nI am going through `tensor_shape.h` and trying to figure out where exactly things are going wrong on big endian.\r\nAnother point to note is that `gen_array_ops.rank(a)` was working fine in TensorFlow version 0.10.0 and transpose tests were passing there.  There are no major changes in tensor_shape.h from v0.10.0 to v1.2.1 !\r\n\r\nHave a question : Could you please point which rank definition does the `gen_array_ops.rank(a)` pick? The `def rank` from `gen_array_ops.py` just seems to call `_op_def_lib.apply_op(\"Rank\", input=input, name=name)`.\r\n", "@namrata-ibm `gen_array_ops.rank()` is a generated wrapper for the `Rank` operator, which is defined at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/array_ops.cc?utf8=%E2%9C%93#L2191-L2212 .\r\nThe kernel associated with that operator is defined at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/shape_ops.h#L85-L98 .\r\nThe code that puts in place the mapping between kernel and operator is at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/shape_ops.cc#L193-L248 .", "@frreiss, Thank you for your reply.\r\n\r\nI am debugging this further, and realized that \r\n[const int rank = inp.dims();](https://github.com/tensorflow/tensorflow/blob/v1.2.1/tensorflow/core/kernels/shape_ops.h#L91) returns value `2` as expected. \r\nMay be the output tensor is going wrong somewhere. Needs more debugging here.", "IIRC the size of an `int` on POWER is 64 bits. Perhaps the assignment at line 94 (`out->scalar<int32>()() = rank;`) is writing 8 bytes into the Tensor instead of 4? What happens if you change the type of `rank` from `int` to `int32_t`?\r\nSource code for `Tensor::scalar()` is at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/tensor.h?utf8=%E2%9C%93#L641-L645 .\r\nThat function is a thin veneer over `TensorMap::operator()`, which is defined at https://bitbucket.org/eigen/eigen/src/8d1ccfd9c5a0bf1a662bd0fe35858f4e527d2bac/unsupported/Eigen/CXX11/src/Tensor/TensorMap.h?at=default&fileviewer=file-view-default#TensorMap.h-221:226", "@frreiss, I am working on s390x arch. Replacing `int` to `int32_t` yields the same result.\r\nThe `rank` kernel hasn't changed much since v0.10.0 where the same was working. \r\nInspecting `Tensor::scalar()` further now.", "@frreiss @mrry We are able to narrow down the issue to the point of failure. For s390x, we are getting issue with [TF_RETURN_IF_ERROR(c->WithValue(perm_elems, rank, &perm_elems));](https://github.com/tensorflow/tensorflow/blob/v1.2.1/tensorflow/core/ops/array_ops.cc#L1797)\r\n\r\nAdding a check is solving the issue for us:\r\n```\r\nif (!c->ValueKnown(perm_elems))\r\nTF_RETURN_IF_ERROR(c->WithValue(perm_elems, rank, &perm_elems)); \r\n```\r\n\r\nPlease let us know if the solution looks ok to you.", "@mrry, I was looking at the statically inferred rank vs statically unknown rank calculation. \r\nFrom the [code](https://github.com/tensorflow/tensorflow/blob/v1.2.1/tensorflow/python/ops/array_ops.py#L1276) : \r\n```\r\n  with ops.name_scope(name, \"transpose\", [a]) as name: \r\n      if perm is None: \r\n        rank = gen_array_ops.rank(a) \r\n        perm = (rank - 1) - gen_math_ops._range(0, rank, 1) \r\n```\r\n\r\nI printed the `range` calculated above, in 2 scenarios\r\n1.  when , rank = gen_array_ops.rank(a) \r\n```\r\n(Pdb) p gen_math_ops._range(0, rank, 1)\r\n<tf.Tensor 'transpose/Range:0' shape=(0,) dtype=int32>\r\n```\r\nwhereas \r\n2. If we change,  rank_a = rank(a)\r\n```\r\n(Pdb) p gen_math_ops._range(0, rank_a, 1)\r\n<tf.Tensor 'transpose/Range:0' shape=(2,) dtype=int32>\r\n```\r\n\r\nIn both cases, **rank / rank_a** is:\r\n```\r\n(Pdb) p rank\r\n<tf.Tensor 'transpose/Rank:0' shape=() dtype=int32>\r\n(Pdb) p rank.shape\r\nTensorShape([])\r\n(Pdb) p rank.eval()\r\n2\r\n(Pdb) p (gen_math_ops._range(0, rank, 1)).eval()\r\narray([0, 1], dtype=int32)\r\n```\r\n\r\nIs there some other property of rank tensor which I haven't printed and is affecting the calaculation of shape of `Range`? \r\n\r\n\r\n", "Can you also try tracing the execution of `tf.contrib.util.constant_value(gen_array_ops.rank(a))`? My suspicion is that it's returning `0` incorrectly, but I'm not sure why that would be.", "@mrry, I had to use `tensor_util.constant_value(gen_array_ops.rank(a))` as ` tf.contrib.util.constant_value(gen_array_ops.rank(a))` gave errors like tf, contrib modules missing. I assume the former is equivalent in v1.2.1. \r\n\r\nThe same failure is seen with this too.  :( \r\nThis change works on Intel x86 though.\r\n\r\n\r\nLittle analysis:\r\n```\r\n(Pdb) p rank_a\r\narray(0, dtype=int32)\r\n(Pdb) p rank_a.shape\r\n()\r\n(Pdb) p rank_a.eval()\r\n*** AttributeError: AttributeError(\"'numpy.ndarray' object has no attribute 'eval'\",)\r\n(Pdb) p (gen_math_ops._range(0, rank_a, 1)).eval()\r\narray([], dtype=int32)\r\n\r\n(Pdb) p tensor_util.constant_value(gen_array_ops.rank(a))\r\narray(0, dtype=int32)\r\n```\r\n\r\nI could see calls made to [cpp_shape_handler.cc/ProtoFromShapeHandle](https://github.com/tensorflow/tensorflow/blob/v1.2.1/tensorflow/python/framework/cpp_shape_inference.cc#L34) . This implementation seems to be added after v0.10.0. Are these files involved during tensor object conversion from python to cpp? Do you see a possibility of loss of shape happening there? But I wonder why it doesn't occur for other operations and happens only for Range.\r\n", "Got the root cause of the issue. The conversion from ndarray to Tensor is causing trouble on big endian machine. Following is the change in file `tensorflow/python/framework/tensor_util.py` which is solving the problem (this file is from tag 1.3.0) :\r\n```\r\n--- a/tensorflow/python/framework/tensor_util.py\r\n+++ b/tensorflow/python/framework/tensor_util.py\r\n@@ -617,8 +617,9 @@ def _ConstantValue(tensor):\r\n   elif tensor.op.type == \"Rank\":\r\n     input_shape = tensor.op.inputs[0].get_shape()\r\n     if input_shape.ndims is not None:\r\n-      return np.ndarray(shape=(), buffer=np.array([input_shape.ndims]),\r\n-                        dtype=np.int32)\r\n+      return np.array([input_shape.ndims], np.int32) \r\n```\r\nNow the tests failing on rank for big endian are passing. ", "Correcting the fix given in the above comment:\r\n```\r\n--- a/tensorflow/python/framework/tensor_util.py\r\n+++ b/tensorflow/python/framework/tensor_util.py\r\n@@ -617,7 +617,7 @@ def _ConstantValue(tensor):\r\n   elif tensor.op.type == \"Rank\":\r\n     input_shape = tensor.op.inputs[0].get_shape()\r\n     if input_shape.ndims is not None:\r\n-      return np.ndarray(shape=(), buffer=np.array([input_shape.ndims]),\r\n+      return np.ndarray(shape=(), buffer=np.array([input_shape.ndims], dtype=np.int32),\r\n                         dtype=np.int32)\r\n     else:\r\n       return None\r\n```\r\nThe missing data type in the nparray is behaving differently on the big endian machines. This solution works fine for Intel as well zSystems architecture. Will create a PR for same. ", "Thanks for digging into that, @shahidhs-ibm! I'd be delighted to accept a PR that fixes this on your architecture. Is it possible that you've uncovered a bug in the underlying NumPy implementation as well? ", "@mrry Yes, indeed this is a bug in NumPy implementation.\r\n\r\nOn Intel:\r\n```\r\n# python\r\nPython 2.7.12 (default, Nov 19 2016, 06:48:10)\r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import numpy as np\r\n>>> z = np.ndarray(shape=(), buffer=np.array([1,2,3]), dtype=np.int32)\r\n>>> z\r\narray(1, dtype=int32)\r\n>>> z = np.ndarray(shape=(), buffer=np.array([1,2,3], dtype=np.int32), dtype=np.int32)\r\n>>> z\r\narray(1, dtype=int32)\r\n>>>\r\n\r\n```\r\nOn zSystems:\r\n```\r\n# python\r\nPython 2.7.12 (default, Nov 19 2016, 06:48:10)\r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import numpy as np\r\n>>> z = np.ndarray(shape=(), buffer=np.array([1,2,3]), dtype=np.int32)\r\n>>> z\r\narray(0, dtype=int32)\r\n>>> z = np.ndarray(shape=(), buffer=np.array([1,2,3], dtype=np.int32), dtype=np.int32)\r\n>>> z\r\narray(1, dtype=int32)\r\n>>>\r\n```\r\nWe will log a bug on NumPy for the incorrect conversion happening for ndarray.", "Closing this issue as it is resolved"]}, {"number": 11430, "title": "tf.matrix_inverse doesn't support complex tensor", "body": "I'm trying to run the next code:\r\n\r\nCode:\r\n\r\n    x=tf.placeholder(tf.float64,[2,2])\r\n    y=tf.matrix_inverse(x)\r\n\r\nResult OK :\r\n\r\n    <tf.Tensor 'MatrixInverse_2:0' shape=(2, 2) dtype=float64>\r\n\r\nCode:\r\n\r\n    x=tf.placeholder(tf.complex64,[2,2])\r\n    y=tf.matrix_inverse(x)\r\n\r\nResult (Error):\r\n\r\n    Traceback (most recent call last):\r\n\r\n    File \"<ipython-input-163-f259114be54a>\", line 2, in <module>\r\n    y=tf.matrix_inverse(x)\r\n\r\n    File \"c:\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_linalg_ops.py\", line 330, in \r\n    matrix_inverse\r\n    name=name)\r\n\r\n    File \"c:\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 585, \r\n    in apply_op\r\n    param_name=input_name)\r\n\r\n    File \"c:\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 61, \r\n    in _SatisfiesTypeConstraint\r\n    \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\r\n\r\n    TypeError: Value passed to parameter 'input' has DataType complex64 not in list of allowed values: \r\n    float64, float32\r\n\r\n\r\n\r\nIt seems that complex matrix inverse is not supported, Any idea for workaround ?", "comments": ["In the matrix_inverse function source the inverse is computed only if adjoint is set to true. \r\nCheck here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_grad.py\r\n\r\nFor real numbers we have to calculate the adjoint of the matrix in order to get the inverse but for the case of complex numbers in a matrix the inverse is calculated using conjugate transpose or hermitian transpose.\r\n\r\nMore here:\r\nhttps://en.wikipedia.org/wiki/Conjugate_transpose\r\n\r\nSo for the time being it works only fot real numbers.\r\nHope this helps.\r\n", "I currently have the next workaround for this problem:\r\nI'm using py_func to import np.inalg.inv function\r\n\r\n    x=tf.placeholder(tf.complex64,[2,2])\r\n    #y=tf.matrix_inverse(x)\r\n    y = tf.py_func(np.linalg.inv, [x], x.dtype)\r\n    y = tf.reshape(y, [2,2])", "So is the above code working for you or are you  facing any problems?\r\n", "Seems to work fine, but I haven't checked it with training yet"]}, {"number": 11429, "title": "modify SaverDef default version with v2", "body": "SaverDef V1 has been deprecated, so modify default version with V2.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 11428, "title": "NameError: name 'eval_input_fn' is not defined", "body": "OS : MacOS 10.12\r\nTensorVersion : 1.2\r\nPythonVersion : 2.7\r\n\r\nRun the example of 'A custom model' in the document '[Getting Started With TensorFlow](https://www.tensorflow.org/get_started/get_started#a_custom_model)', occur the exception:\r\n`Traceback (most recent call last):\r\n  File \"linear_regression_tf_contrib_learn_custom_model.py\", line 35, in <module>\r\n    eval_loss = estimator.evaluate(input_fn=eval_input_fn)\r\nNameError: name 'eval_input_fn' is not defined`.\r\n\r\nFix : Need to add the code `eval_input_fn = tf.contrib.learn.io.numpy_input_fn({\"x\": x_eval}, y_eval, 4, num_epochs=1000)` at line 30.", "comments": ["Thanks! @dr4b could you take a look?", "This is a duplicate of #11123 and has been fixed in the master branch but won't show up as root until 1.3 is released."]}, {"number": 11427, "title": "wrong with tf.norm, it will produce nan", "body": "I use version 1.0.0 in server, in my code, the result will be nan if i use :\r\n```\r\nresult = tf.square(tf.norm(penalize))\r\n``` \r\nbut if i change it to be:\r\n```\r\nresult = tf.reduce_sum(tf.square(penalize))\r\n```\r\neverything will be ok, and result is some float between 0 and 1. \r\n\r\n", "comments": ["I was unable to reproduce the problem with v1.2.1. Could you please post a code snippet to reproduce the problem. BTW I would suggest that you use `tf.reduce_sum(...)` over `tf.square(...)` as it will likely be more efficient to compute.", "my related code snipper is:\r\n```        \r\n# softmax\r\n# A.shape is [2, t]\r\nA = tf.nn.softmax(A)\r\n\r\npenalize = tf.matmul(A, tf.transpose(A)) - tf.diag(tf.ones(2))\r\npenalize = tf.reduce_sum(tf.square(penalize))\r\n\r\n# penalize = tf.square(tf.norm(penalize))\r\n```\r\n\r\nand what's the implement of your suggestion about use tf.reduce_sum(...) over tf.square(...)? Thx.", "I tried this code snippet and it works perfectly fine on my machine with tf v1.2.1. Let me know if I'm missing something.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nA = tf.constant([[0.1, 0.2, 0.5], [0.3, 0.1, 0.2]])\r\nA = tf.nn.softmax(A)\r\n\r\npenalize = tf.matmul(A, tf.transpose(A)) - tf.diag(tf.ones(2))\r\nresult1 = tf.reduce_sum(tf.square(penalize))\r\nresult2 = tf.square(tf.norm(penalize))\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(result1))\r\n    print(sess.run(result2))\r\n```\r\n\r\nWhen I said that you should prefer tf.reduce_sum(...) over tf.square(...), I meant that you should prefer `result = tf.reduce_sum(tf.square(penalize))` over `result = tf.square(tf.norm(penalize))`", "this simple program works fine in my server, too. but my original program is quite big and it takes time to run and print related logs. So I hope I can find what exactly happens with the numbers some day ...", "Thanks for helping our friend @lakshayg."]}, {"number": 11426, "title": "Resource exhausted error for translate.py  Seq2seq model (CPU)", "body": "Windows 10\r\nTensorflow: 1.0.0\r\nPython version : 3.5\r\n I get the following error when i try to run translate.py for Seq2seq model \r\n\r\nResource exhausted: OOM when allocating tensor with shape[576,1024]\r\nIs there any way i could reduce the size in translate.py so that the error does not occur?\r\nThanks\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 11425, "title": "Imply default argument in docstring properly", "body": "Since the default is `as_text=True` the current docstring is a real gotcha IMO. Clearer to flip the explanation around.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 11424, "title": "Branch 161478803", "body": "", "comments": ["@frankchn There are duplicate op definitions in tensorflow/go. I pushed a new commit to try to fix it. Hopefully all tests will pass this time (except the flaky barrier_ops_test on Mac)", "Jenkins, test this please.", "Thanks @caisq for handling this!", "This looks new... https://ci.tensorflow.org/job/tensorflow-pull-requests-gpu/5701/console\r\n\r\nERROR: /workspace/tensorflow/stream_executor/BUILD:39:1: undeclared inclusion(s) in rule '//tensorflow/stream_executor:cuda_platform':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/stream_executor/cuda/cuda_timer.cc':\r\n\r\nLet's try again to see if it is reproducible. @tensorflow-jenkins test this please", "Looks like the aforementioned error is not 100% reproducible. Could be specific to gpu5-slave. Merging PR."]}, {"number": 11423, "title": "Fix secure urls and typos", "body": "Replaced `https://tensorflow.org` with `https://www.tensorflow.org` to prevent expired website security certificate `NET::ERR_CERT_DATE_INVALID` warning", "comments": ["Can one of the admins verify this patch?", "The 86b18ca9fa86762306bb7ee3cf8ab99a13df6f53 commit fixes a typo and cleans up some Python code by conforming to the Google Python Style Guide.", "Jenkins, test this please."]}]