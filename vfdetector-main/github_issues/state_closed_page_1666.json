[{"number": 2925, "title": "Always bind \"localhost\" when starting workers", "body": "Now we have tried distributed TensorFlow and run multiple workers. It works but we found that it always bind \"localhost\" when I would like to bind one of my NICs.\n\nIs it the bug? Or do I miss something because I found nothing to configure it in any document.\n### Environment info\n\nOperating System: Ubuntu 16.04\n\nInstalled version of CUDA and cuDNN:  No\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n\n```\nroot@100cd4fb5bca:/notebooks# pip --version\npip 8.1.1 from /usr/local/lib/python2.7/dist-packages (python 2.7)\n```\n1. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\n```\nroot@100cd4fb5bca:/notebooks# python -c \"import tensorflow; print(tensorflow.__version__)\"\n0.8.0\n```\n### Steps to reproduce\n1. docker run -it tensorflow/tensorflow bash\n2. Write the server code of `one_worker.py`.\n3. `python ./one_worker.py`\n\n```\nroot@100cd4fb5bca:/notebooks# cat one_worker.py\nimport tensorflow as tf\n\nworker1 = \"www.a.com:2222\"\nworker_hosts = [worker1]\n\ncluster_spec = tf.train.ClusterSpec({\"worker\": worker_hosts})\n\nserver = tf.train.Server(cluster_spec, job_name=\"worker\", task_index=0)\n\nserver.join()\n```\n### Logs or other output that would be helpful\n\n```\nroot@100cd4fb5bca:/notebooks# python ./one_worker.py\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {localhost:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2222\n```\n", "comments": ["Excellent question! The short answer is that it's not a bug. We always start an in-process server on localhost. So to use this correctly, you actually need to have a separate script which logs onto www.a.com and start the server. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/distributed/index.md\nfor details.\n\nHere's the slightly longer answer should you be interested:\n\nWhen this line is called on www.a.com,\n  server = tf.train.Server(cluster_spec, job_name=\"worker\", task_index=0)\na GrpcServer is created with target grpc://localhost:2222. This server knows how to talk to the tasks in the same job via GrpcChannels:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc#L150\n\nHope that helps. :)\n\nSherry\n", "Thanks @sherrym and it helps a lot.\n\nBut I have another question. If the target is always \"localhost\", how can the client which is in another host connect with this server? We can specify the \"server.target\" when initializing \"Session\" but it doesn't work to connect with \"www.a.com:2222\" or the ip.\n\nIf the `server.target` has to be `localhost:$post`, the client has to be in the same host with the worker. Is that right?\n", "> If the server.target has to be localhost:$post, the client has to be in the same host with the worker. Is that right?\n\nNo, the client and worker do not have to be on the same host. The `tf.train.Server` class is a Python wrapper for a server in the _local process_, so it always binds to a port on `localhost`. (It ignores the hostname that you specify for the corresponding task in the `tf.train.ClusterSpec`, so that you can use the same `ClusterSpec` in all processes.)\n\nThe client only needs to create a `tf.Session`, and does not need to create a `tf.train.Server` (though it can if you like). For example, you can run a server on `\"www.a.com\"` that listens on port 2222, and connect to it from another machine by creating a `tf.Session(\"grpc://www.a.com:2222\")`.\n", "Sorry for bothering. I was wrong when I test with `www.a.com` and definitely we can run servers and clients in different hosts.\n\nThanks for your help! @sherrym @mrry \n"]}, {"number": 2924, "title": "Problems and solutions in installing tf on Redhat", "body": "I encountered several problems in installing tensorflow on Redhat system. I solved them by googling solutions. Hope my experience may be helpful to some users.\n\nOS version: Red Hat Enterprise Linux Server release 6.2 (Santiago)\nLinux kernel: 2.6.32-220.el6.x86_64\n\nProblems and solutions:\n\n **1. glibc version is too low** \n\nInstall higer version glibc, e.g. glibc-2.17.\nPlease use at least version 2.17. You can configure the installation using --prefix option to specify the path to install, and then modify LD_LIBRARY_PATH to include the new glibc library.\n\n **2. link problem after install higher version of glibc**\n\nError info looks like:\n\"\nerror while loading shared libraries: __vdso_time: invalid mode for dlopen():Invalid argument\n\"\nYou need to use the following method to run a program (eg. python):\n/path/to/glibc-2.17/lib/ld-linux-x86-64.so.2 --library-path /path/to/glibc-2.17/lib:$LD_LIBRARY_PATH:/path/to/gcc-5.2.0/lib64:/usr/lib64/ /path/to/anaconda/bin/python2.7\n\n **3. numpy version problem** \n\nInstall new version of numpy using pip.\n\n **4. protobuf3 not found, and** \nError in python after 'import tensorflow': TypeError: **init**() got an unexpected keyword argument 'syntax'\n\npip install 'protobuf>=3.0.0a3'\n\n **5.  failed call to cuInit: CUDA_ERROR_NO_DEVICE**\nExplicitly specify a cuda device in you environment variable:\nexport CUDA_VISIBLE_DEVICES=0\n\n **6. libcuda.so.1 not found**\n\nExplicitly specify the path of this lib in your LD_LIBRARY_PATH environment variable. \nlibcuda.so is in usually in /usr/lib64\n\n **7. ELFCLASS32 error**\nelf is incorrect, should use a 64-bits .so file (usually in */lib64)\n\n **8. glibc 2.15 cannot create regular file `/var/db/Makefile': Permission denied**\nUse glibc 2.17 or  even higher version.\n", "comments": ["Thanks, @jinfengfeng . \n", "@jinfengfeng I can not understand the second point `2. link problem after install higher version of glibc` in your above solution. How to solve this error:\r\n`while loading shared libraries: __vdso_time: invalid mode for dlopen():Invalid argument`\r\n?", "@jinfengfeng I did not understand Point 2 either, could you please help. @IvyGongoogle Were you able to solve the issue?\r\n", "I did not understand Point 2 either, could you please help. @IvyGongoogle Were you able to solve the issue?", "I tried what was mentioned in point 2, but running a command like \r\n`/opt/glibc-2.17/lib/ld-linux-x86-64.so.2 --library-path /opt/glibc-2.17/lib:$LD_LIBRARY_PATH:usr/lib64/ /home/njafer/python/bin/python3` at least works, but a command like \r\n `/opt/glibc-2.17/lib/ld-linux-x86-64.so.2 --library-path /opt/glibc-2.17/lib:$LD_LIBRARY_PATH:usr/lib64/ ls` gives me a \r\n`ls: error while loading shared libraries: ls: cannot open shared object file`", "@naveenjafer \r\nTo run ` ls`, you may need something like\r\n\r\n`/opt/glibc-2.17/lib/ld-linux-x86-64.so.2 --library-path /opt/glibc-2.17/lib:$LD_LIBRARY_PATH:usr/lib64/ /bin/ls`"]}, {"number": 2923, "title": "Error:use tensorflow_cifar-10 model on tensorflow android camera", "body": "I am very new to tensorflow. Now I am trying to use tensorflow_cifar-10 model on tensorflow android camera, the app is running but there is no output.\n\nAt first I have tried this example:\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android\nThis example uses the .pb file at:\nhttps://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip\nIt works well.\n\nThen I tried to replace the .pb file with the tensorflow_cifar-10. I followed some of the instructions here:\nhttps://github.com/tensorflow/tensorflow/issues/616\nand add the save__graph() function in /tensorflow/models/image/cifar10/cifar10_train.py\n\ncode:\n\ndef save_graph():\n\n```\ng = tf.Graph()\nvars = {}\nwith g.as_default():\n    with tf.Session() as sess:\n        d = np.ones([1,24,24,3],dtype=np.float32)\n        input_data = tf.placeholder(tf.float32,shape=[1,24,24,3], name=\"input_placeholder\")\n        images, labels = cifar10.distorted_inputs()\n        logits = cifar10.inference(images)\n        init = tf.initialize_all_variables()\n        sess.run(init)\n        saver = tf.train.Saver(tf.trainable_variables(),max_to_keep=0)\n        saver.restore(sess,os.path.join(FLAGS.train_dir, 'model.ckpt'))\n        print (sess.run(logits,{images:d}))\n        for v in tf.trainable_variables():\n            vars[v.value().name] = sess.run(v)\n\ng2 = tf.Graph()\nconsts = {}\nwith g2.as_default():\n    with tf.Session() as sess:\n        for k in vars.keys():\n            consts[k] = tf.constant(vars[k])\n        tf.import_graph_def(g.as_graph_def(),input_map={name:consts[name] for name in consts.keys()})\n        tf.train.write_graph(sess.graph_def,'/home/eli/Documents/TensorflowProjetos/ProtobufFiles','graph.pb',False)\n\nreturn os.path.join('/home/eli/Documents/TensorflowProjetos/ProtobufFiles','graph.pb')\n```\n\nI use the save__graph() to get the .pb file after the end of training. Then I replace the tensorflow_inception_graph.pb, whice is from inception5h.zip and used in tensorflow_android_camera, with the new .pb file from the cifar10 model.\n\nFinally I run the tensorflow_android_camera app but there is no output. The error in log is: \nError during inference: Not found: FeedInputs: unable to find feed output input:0\nI do not know why it happened, please help me ,thank you!\n", "comments": ["You will need to either name your input_data \"input\", or change the demo code to feed \"input_placeholder:0\".\n", "@sherrym Thanks for your help. I have done this, but nothing is changed.\n", "@qemb01 Please can you supply the exact code you are now running and the command lines necessary to reproduce this problem?\n", "@prb12 Of course, here is my code in cifar10_train.py:\n\n```\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datetime import datetime\nimport os.path\nimport time\nimport os\n\nimport numpy as np\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\nimport os\nfrom tensorflow.python.platform import gfile\n\nfrom tensorflow.models.image.cifar10 import cifar10\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string('train_dir', '/tmp/cifar10_train',\n                           \"\"\"Directory where to write event logs \"\"\"\n                           \"\"\"and checkpoint.\"\"\")\ntf.app.flags.DEFINE_integer('max_steps', 20000,\n                            \"\"\"Number of batches to run.\"\"\")\ntf.app.flags.DEFINE_boolean('log_device_placement', False,\n                            \"\"\"Whether to log device placement.\"\"\")\n\n\ndef train():\n  \"\"\"Train CIFAR-10 for a number of steps.\"\"\"\n  with tf.Graph().as_default():\n    global_step = tf.Variable(0, trainable=False)\n\n    # Get images and labels for CIFAR-10.\n    images, labels = cifar10.distorted_inputs()\n\n    # Build a Graph that computes the logits predictions from the\n    # inference model.\n    logits = cifar10.inference(images)\n\n    # Calculate loss.\n    loss = cifar10.loss(logits, labels)\n\n    # Build a Graph that trains the model with one batch of examples and\n    # updates the model parameters.\n    train_op = cifar10.train(loss, global_step)\n\n    # Create a saver.\n    saver = tf.train.Saver(tf.all_variables())\n\n    # Build the summary operation based on the TF collection of Summaries.\n    summary_op = tf.merge_all_summaries()\n\n    # Build an initialization operation to run below.\n    init = tf.initialize_all_variables()\n\n    # Start running operations on the Graph.\n    sess = tf.Session(config=tf.ConfigProto(\n        log_device_placement=FLAGS.log_device_placement))\n    sess.run(init)\n\n\n    # Start the queue runners.\n    tf.train.start_queue_runners(sess=sess)\n\n    summary_writer = tf.train.SummaryWriter(FLAGS.train_dir,\n                                            graph_def=sess.graph_def)\n\n    for step in xrange(FLAGS.max_steps):\n      start_time = time.time()\n      _, loss_value = sess.run([train_op, loss])\n      duration = time.time() - start_time\n\n      assert not np.isnan(loss_value), 'Model diverged with loss = NaN'\n\n      if step % 10 == 0:\n        num_examples_per_step = FLAGS.batch_size\n        examples_per_sec = num_examples_per_step / duration\n        sec_per_batch = float(duration)\n\n        format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f '\n                      'sec/batch)')\n        print (format_str % (datetime.now(), step, loss_value,\n                             examples_per_sec, sec_per_batch))\n\n      if step % 100 == 0:\n        summary_str = sess.run(summary_op)\n        summary_writer.add_summary(summary_str, step)\n\n      # Save the model checkpoint periodically.\n      if step % 1000 == 0 or (step + 1) == FLAGS.max_steps:\n        checkpoint_path = os.path.join(FLAGS.train_dir, 'model.ckpt')\n        saver.save(sess, checkpoint_path, global_step=step)\n\ndef save_graph():\n\n    g = tf.Graph()\n    vars = {}\n    with g.as_default():\n        with tf.Session() as sess:\n            d = np.ones([128,24,24,3],dtype=np.float32)\n            images, labels = cifar10.distorted_inputs()\n            logits = cifar10.inference(images)\n\n            init = tf.initialize_all_variables()\n            sess.run(init)\n            saver = tf.train.Saver(tf.trainable_variables(),max_to_keep=0)\n            saver.restore(sess,os.path.join(FLAGS.train_dir, 'model.ckpt-19999'))\n\n            print (sess.run(logits,{images:d}))\n            for v in tf.trainable_variables():\n                vars[v.value().name] = sess.run(v)\n\n\n    g2 = tf.Graph()\n    consts = {}\n    with g2.as_default():\n        with tf.Session() as sess:\n            for k in vars.keys():\n                consts[k] = tf.constant(vars[k])\n            tf.import_graph_def(g.as_graph_def(),input_map={name:consts[name] for name in consts.keys()})\n\n            tf.train.write_graph(sess.graph_def,'/home/eli/Documents/TensorflowProjetos/ProtobufFiles','graph.pb',False)\n\n    return os.path.join('/home/eli/Documents/TensorflowProjetos/ProtobufFiles','graph.pb')\n\n\ndef main(argv=None):  # pylint: disable=unused-argument\n  cifar10.maybe_download_and_extract()\n  if tf.gfile.Exists(FLAGS.train_dir):\n    tf.gfile.DeleteRecursively(FLAGS.train_dir)\n  tf.gfile.MakeDirs(FLAGS.train_dir)\n  train()\n  save_graph()\n\nif __name__ == '__main__':\n  tf.app.run()\n```\n\nI have run the cifar10_train.py to get pb file. Then I replace the tensorflow_inception_graph.pb with it. The tensorflow_inception_graph.pb is in the /tensorflow/tensorflow/tree/master/tensorflow/examples/android/assets/ of tensorflow android camera demo.\n\nTensorflow android camera demo:\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android\n", "I'm sorry - but this code looks pretty confused to me.  As @sherrym said above, the android example is expecting a graphdef with a placeholder node called 'input' into which it will presumably feed images from the camera.  Your _original_ code had an _unused_ placeholder node called `input_placeholder`: \n\n```\ninput_data = tf.placeholder(tf.float32,shape=[1,24,24,3], name=\"input_placeholder\")\n```\n\nThe _new_ code above calls cifar10.distorted_inputs() which I believe generates a graph to read input images from files in the filesystem (whose names are retrieved from a queue!) and randomly mutates them.  If you want this to work you will need to create a graphdef which accepts images via a node called 'input' and feeds them into cifar10.inference. \n\nSince it isn't a TensorFlow bug, but really a general programming question, you should probably re-ask this on StackOverflow. (please ensure you tag the question 'tensorflow').   \n", "@prb12 Thank you very much. I will try to do it.\n", "@qemb01  Have you successful to replace the model file?"]}, {"number": 2922, "title": "iris_custom_decay_dnn.py does not work", "body": "The code below is the current version of iris_custom_decay_dnn.py. The custom decay portion of which does not appear to work as whenever I run it I get the Traceback below. Through so light testing I discovered that the code works when the custom decay portion is removed.\n# iris_custom_decay_dnn.py\n\n``` python\nfrom sklearn import datasets, metrics\nfrom sklearn.cross_validation import train_test_split\n\nimport tensorflow as tf\n\niris = datasets.load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris.data,\n                                                    iris.target,\n                                                    test_size=0.2,\n                                                    random_state=42)\n#setup exponential decay function\ndef exp_decay(global_step):\n    return tf.train.exponential_decay(\n        learning_rate=0.1, global_step=global_step,\n        decay_steps=100, decay_rate=0.001)\n\n# use customized decay function in learning_rate\noptimizer = tf.train.AdagradOptimizer(learning_rate=exp_decay)\nclassifier = tf.contrib.learn.DNNClassifier(hidden_units=[10, 20, 10],\n                                            n_classes=3,\n                                            optimizer=optimizer)\nclassifier.fit(X_train, y_train, steps=800)\nscore = metrics.accuracy_score(y_test, classifier.predict(X_test))\n```\n# Traceback\n\nTraceback (most recent call last):\n  File \"/home/d3v/Py/SkFlow Iris DNN.py\", line 22, in <module>\n    classifier.fit(X_train, y_train, steps=800)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 182, in fit\n    monitors=monitors)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 449, in _train_model\n    train_op, loss_op = self._get_train_ops(features, targets)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py\", line 113, in _get_train_ops\n    return super(DNNClassifier, self)._get_train_ops(features, targets)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py\", line 170, in _get_train_ops\n    dnn_vars)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py\", line 356, in _get_dnn_training_ops\n    return [self._dnn_optimizer.apply_gradients(zip(dnn_grads, dnn_vars))]\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 300, in apply_gradients\n    self._prepare()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/adagrad.py\", line 70, in _prepare\n    name=\"learning_rate\")\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 620, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/constant_op.py\", line 179, in _constant_tensor_conversion_function\n    return constant(v, dtype=dtype, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/constant_op.py\", line 162, in constant\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_util.py\", line 421, in make_tensor_proto\n    tensor_proto.string_val.extend([compat.as_bytes(x) for x in proto_values])\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_util.py\", line 421, in <listcomp>\n    tensor_proto.string_val.extend([compat.as_bytes(x) for x in proto_values])\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/compat.py\", line 44, in as_bytes\n    raise TypeError('Expected binary or unicode string, got %r' % bytes_or_text)\nTypeError: Expected binary or unicode string, got <function exp_decay at 0x7f84e7161e18>\n", "comments": ["convert_to_tensor expects value to be \"An object whose type has a registered `Tensor` conversion function.\", not an op.\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L572\n", "More importantly, AdagradOptimizer's learning_rate parameter is supposed to be \n\n```\nConstruct a new Adagrad optimizer.\n   Args:\n      learning_rate: A `Tensor` or a floating point value.  The learning rate.\n```\n\nThe example code above passes in a python function which returns a Tensor.\n\nI'm pretty sure that convert_to_tensor doesn't accept a python function:\n\n```\n13:19:46 :~$ python\nPython 2.7.6 (default, Jun 22 2015, 17:58:13) \n[GCC 4.8.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import tensorflow as tf\n>>> def foo():\n...   return tf.constant(0.1)\n... \n>>> tf.convert_to_tensor(foo)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 620, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/constant_op.py\", line 179, in _constant_tensor_conversion_function\n    return constant(v, dtype=dtype, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/constant_op.py\", line 162, in constant\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py\", line 421, in make_tensor_proto\n    tensor_proto.string_val.extend([compat.as_bytes(x) for x in proto_values])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/compat.py\", line 44, in as_bytes\n    raise TypeError('Expected binary or unicode string, got %r' % bytes_or_text)\nTypeError: Expected binary or unicode string, got <function foo at 0x7f80b90b3140>\n```\n\nWhereas this is fine:\n\n```\n>>> tf.convert_to_tensor(foo())\n<tf.Tensor 'Const:0' shape=() dtype=float32>\n>>> \n```\n", "Is there currently a workaround? I have a good bit of skflow scripts that I need to update to the current version.\n", "The way to pass anything to optimizer is by passing a `callable` function as optimizer, like this:\n\n```\ndef get_optimizer():\n  def exp_decay(global_step):\n      return tf.train.exponential_decay(\n          learning_rate=0.1, global_step=global_step,\n          decay_steps=100, decay_rate=0.001)\n\n  # use customized decay function in learning_rate\n  return tf.train.AdagradOptimizer(learning_rate=exp_decay)\n\nclassifier = tf.contrib.learn.DNNClassifier(\n    hidden_units=[10, 20, 10], optimizer=get_optimizer, ...)\n```\n", "I followed the suggestion by @ilblackdragon to pass a callable function, however, it still results in the same Traceback (TypeError: Expected binary or unicode string, got <function exp_decay at 0x11e9fc410>) once fit is called. Replacing exp_decay with a float works. Does not appear to be working.\n\nRelevant code:\n\n```\nX_train,y_train,X_valid,y_valid,X_test,y_test,_,_ = loadData()\n\nfeature_columns = learn.infer_real_valued_columns_from_input(X_train)\n\ndef get_optimizer():\n    def exp_decay(global_step):\n      return tf.train.exponential_decay(\n          learning_rate=0.00001, global_step=global_step,\n          decay_steps=100, decay_rate=0.97, starcase=True)\n    # use customized decay function in learning_rate\n    return tf.train.AdagradOptimizer(learning_rate=exp_decay)\n\nestimator = learn.DNNRegressor(\n    model_dir='./Model3/',\n    hidden_units=[int(np.mean([X_train.shape[1],1]))],\n    feature_columns=feature_columns,\n    enable_centered_bias=True,\n    optimizer=get_optimizer\n    )\n\nepochs=20\n\nestimator.fit(X_train,y_train,steps=epochs*y_train.shape[0])\n```\n", "Oh, this should work:\n\n```\ndef get_optimizer():\n    def exp_decay(global_step):\n      return tf.train.exponential_decay(\n          learning_rate=0.00001, global_step=global_step,\n          decay_steps=100, decay_rate=0.97, staircase=True)\n    # use customized decay function in learning_rate\n    return tf.train.AdagradOptimizer(\n         learning_rate=exp_decay(tf.contrib.framework.get_global_step()))\n```\n\nor just:\n\n```\ndef get_optimizer():\n    return tf.train.AdagradOptimizer(\n          learning_rate=tf.train.exponential_decay(\n               learning_rate=0.00001, global_step=tf.contrib.framework.get_global_step(),\n               decay_steps=100, decay_rate=0.97, staircase=True))\n```\n"]}, {"number": 2921, "title": "No gradient defined for operation 'ExtractImagePatches'", "body": "I tried to  use [tf.extract_image_patches](https://www.tensorflow.org/versions/r0.9/api_docs/python/array_ops.html#extract_image_patches) to implement a locally layer as #2605 said. However an error occurred:\nLookupError: No gradient defined for operation 'local/ExtractImagePatches' (op type: ExtractImagePatches)  \n\nI checked that tf.space_to_depth has the gradient definition but tf.extract_image_patches does not. And it seems not easy for me to implement the gradient. Any advice? \n@martinwicke @benoitsteiner @mrry \n", "comments": ["@jyshee go for it if you need it.\n", "I tried to implement the gradient of tf.extract_image_patches using tf.reshape and tf.pad Ops as:\n\n``````\n   @ops.RegisterGradient(\"ExtractImagePatches\")\n   def _ExtractImagePatchesGrad(op,grad):\n      ksize=op.get_attr(\"ksizes\")[1]\n      im=grad\n      batch_size = im.get_shape()[0].value\n      h = im.get_shape()[1].value\n      w = im.get_shape()[2].value\n      ch = im.get_shape()[3].value\n      h_o = h + (ksize - 1)\n      w_o = w + (ksize - 1)\n      ch_o = ch / ksize / ksize\n      im_o = tf.Variable(tf.zeros([batch_size, h_o, w_o, ch_o]), trainable=False)\n      for i in xrange(h):\n         for j in xrange(w):\n            res=tf.reshape(im[:,i,j,:],[-1,ksize,ksize,int(ch/ksize/ksize)])\n            padres=tf.pad(res,[[0,0],[i,h_o-i-ksize],[j,w_o-j-ksize],[0,0]])\n            im_o+=padres\n      return tf.identity(im_o)```\n\n\nAnd it seems work...\n``````\n", "I've hit this problem also. A quick test of the @jyshee solution fails in my example in the reshape line due to a mismatched number of elements\n", "@hughsalimbeni could you show more details about your error when you test the codes above? \n", "My code is rather messy and very far from a minimal example, but here it is:\n\n```\n    def make_x_hat(self, x):\n        L, H = self.L, self.H\n        #put two cols of zeros\n        # TODO sort out messy tranpose\n        padded = tf.pad(tf.transpose(x), [[0, 0], [1, 1]], 'CONSTANT')\n        #make 4d and swap to make indexing work more nicely\n        x4 = tf.expand_dims(tf.expand_dims(tf.transpose(padded), 0), 3)\n        #rectangular patches of 2 (cols) by L+1 rows\n        ksizes = [1, 2, L + 1, 1]\n        strides = [1, ]*4\n        rates = [1, ]*4\n        patches = tf.extract_image_patches(x4, 'VALID', ksizes, strides, rates)\n        #first layer: [0, :, 0, -L:] \n        #middle layers [0, :, 0,  1:-1]\n        #final layer [0, :, 4, 1:L+1]\n\n        raw = tf.squeeze(tf.transpose(patches, (0, 2, 1, 3)))\n\n        ret = []\n        ret.append(raw[:, 0, L+1:2*L+1])\n        for h in range(1, H):\n            ret.append(raw[:, h, 1:(2*L+1)])\n        ret.append(raw[:, H, 1:L+1])\n        return ret\n```\n\nI was intending to use extract_image_patches to avoid a loop over h (I was going to combine the middle layers to a single array, if it had worked), but the performance gain is so minimal I just used this instead:\n\n```\n    def make_x_hat(self, x):\n        x = tf.transpose(x)\n        N, L, H = self.N, self.L, self.H    \n        n = N + L \n        ret = []        \n        a = []\n        for l in range(L):\n            a.append(x[l:n-L+l, 0])\n        ret.append(tf.transpose(tf.pack(a)))\n\n        for h in range(1, H):\n            a = []\n            b = []\n            for l in range(L):\n                a.append(x[l:n-L+l, h])\n                b.append(x[l+1:n-L+l+1, h-1])\n\n            A = tf.transpose(tf.pack(a))\n            B = tf.transpose(tf.pack(b))\n            ret.append(tf.concat(1, (B, A)))\n\n        a = []\n        for l in range(L):\n            a.append(x[l+1:n-L+l+1, H-1])\n        ret.append(tf.transpose(tf.pack(a)))\n\n        return ret\n```\n\nwhich works fine. \n", "I have an implementation of this gradient -- should I create a pull request?\n", "Yes please, that would be great!\n"]}, {"number": 2920, "title": "dso_loader on MacOS opens libcudnn.dylib instead of libcudnn.5.dylib", "body": "I'm getting error below after syncing. Was there something in last 24 hours that affected how libcudnn filename is resolved?\n\n`Couldn't open CUDA library libcudnn.dylib. LD_LIBRARY_PATH: /usr/local/cuda/lib\n`\nInstead I have these files\n\n```\n-rwxr-xr-x@ 1 yaroslavvb  staff  58270280 Apr 22 17:19 libcudnn.5.dylib\n-rw-r--r--@ 1 yaroslavvb  staff  55551064 Apr 22 17:19 libcudnn_static.a\n```\n\nThis worked for me as a work-around:\n\n```\nbash-3.2$ cd /usr/local/cuda/lib\nbash-3.2$ ln -s libcudnn.5.dylib libcudnn.dylib\n\n```\n\nPS: my configure run is below. \n\n```\nbash-3.2$ ./configure\nPlease specify the location of python. [Default is /usr/bin/python]: \nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] n\nNo Google Cloud Platform support will be enabled for TensorFlow\nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc nvcc should use as the host compiler. [Default is /usr/bin/gcc]: \nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 7.5\nPlease specify the location where CUDA 7.5 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n[Default is: \"3.5,5.2\"]: 3.0\n\n```\n\nNote: one\u00a0productivity sink is that `git pull` overwrites files that are written by `./configure` because they are checked in, so at each `pull` you need to run `./configure` or [stash/merge](http://stackoverflow.com/questions/37758333/how-to-prevent-checked-in-files-from-overwriting-local-versions). Also, Bazel is not fully hermetic, so if you `bazel build` with incorrect `configure` once, you will then need to `bazel clean`. This issue can be non obvious -- things run but give numerically incorrect results\n", "comments": ["The ./configure script will hopefully disappear soon, to be replaced by bazel build rules alone. That should take care of the overwriting problem.\n\nThe cuDNN thing, is it still an issue?\n", "I dunno, symlink work around works for me. I'm traveling and noone else is complaining so I'll close it for now\n"]}, {"number": 2919, "title": "feed_dict performance", "body": "I have been experimenting with different way of reading data in tensorflow, namely:\n- input ops / queues\n- feed_dict / placeholders\n\nI am finding that feed_dict is much slower on my alexnet benchmark. I have adapted the alexnet benchmark code from the [convnet-benchmarks](https://github.com/soumith/convnet-benchmarks/blob/master/tensorflow/benchmark_alexnet.py). You can find my benchmarking code [here](https://gist.github.com/nanddalal/a0fdd6798ea52afaaeb6e1a78aaeb6db). The main diff is the addition of a `feed_dict` flag which switches between using tf.Variable and tf.Placeholder for the inputs/labels.\n\nIn all my tests, I only run the fprop.\nMy first test was running the benchmark on a GPU using NCHW:\n\n```\nfeed_dict=True: 102 ms/minibatch\nfeed_dict=False 28 ms/minibatch\n```\n\nAs you can see, with `feed_dict=False`, it matches the benchmark, but with `feed_dict=True`, it is 4-5x slower. You can find the full output for `feed_dict=True` [here](https://github.com/tensorflow/tensorflow/files/319610/benchmark_with_feed_dict.txt)\n and `feed_dict=False` [here](https://github.com/tensorflow/tensorflow/files/319611/benchmark_without_feed_dict.txt). These contain yappi profiling output which shows that the bottleneck does not seem to be in the python code. I have also attached the tf timeline output and would appreciate if you can tell me how to interpret it in chrome://tracing. \n[timeline_with_feed_dict.json.txt](https://github.com/tensorflow/tensorflow/files/319613/timeline_with_feed_dict.json.txt)\n[timeline_without_feed_dict.json.txt](https://github.com/tensorflow/tensorflow/files/319614/timeline_without_feed_dict.json.txt)\n\nI also ran the same test on CPU using NHWC:\n\n```\nfeed_dict=True: 727 ms/minibatch\nfeed_dict=False: 664 ms/minibatch\n```\n\nSo using `feed_dict` appears to have a constant/one time cost.\n\nI was able to gain almost 2x speedup by changing this [line](https://github.com/tensorflow/tensorflow/blob/9425f822d8a5dc657022eed5c5142b4bf7b1087a/tensorflow/python/client/session.py#L619) from `np.array` to `np.asarray`, but feed_dict is still quite slow.\n\nPlease let me know what your thoughts are about this and whether feed_dict is supposed to be this much slower than a pipeline using tfrecords/queues.\n", "comments": ["Feed_dict does a single-threaded memcpy of contents from Python runtime into TensorFlow runtime. If data is needed on GPU, then you'll have an additional CPU->GPU transfer. I'm used to seeing up to 10x improvement in performance when switching from feed_dict to native TensorFlow (Variable/Queue)\n", "Thanks for the quick reply!\n\nIt seems like there are a lot of copies being made of the values in the feed dict as it moves from the numpy array to the tensor proto's bytes. Do you think we can reduce some of these? For example changing the `np.array` to `np.asarray`. Or even better is it possible to directly assign the tensor proto's bytes field to look at the numpy array's pointer?\n", "@nanddalal extra copy in `np.array` seems like a bug, good catch. \n\nAs far as feasibility of removing the extra copy, I believe the memcpy is happening in [TF_wrapper_helper](https://github.com/tensorflow/tensorflow/blob/d42facc3cc9611f0c9722c81551a7404a0bd3f6b/tensorflow/python/client/tf_session_helper.cc#L445). There's a note from @mrry that memcpy is there to prevent having to acquire GIL on deallocation.\n\nSince numpy array object is owned by Python runtime, when it's destroyed Python will try to de-allocate the data. If there's a way to tell numpy to release the ownership of the data buffer, that could be a work-around.\n\nPerhaps an intermediate step would be to replace memcpy with multi-threaded memcpy. There's a six-year old [post on SO](http://stackoverflow.com/questions/4260602/how-to-increase-performance-of-memcpy) suggesting 6x improvement\n", "@mrry Did you just fix this?\n", "@prb12: The s/`np.array`/`np.asarray`/ change should appear soon (obsoleting #2931), but it was intended to reduce memory pressure&mdash;I haven't measured the performance impact, although the comments here suggest it will be a ~2x improvement for some patterns....\n", "> Since numpy array object is owned by Python runtime, when it's destroyed Python will try to de-allocate the data. If there's a way to tell numpy to release the ownership of the data buffer, that could be a work-around.\n\nI see there's already a Py_DECREF \"associated\" with the numpy input arrays when they are wrapped in unique_ptr's  with `make_safe`. I guess this is to ensure they're there to encourage release of the numpy base arrays on exit of TF_wrapper_helper. Am I reading this correctly?\n\nHow about using Py_INCREF on the numpy inputs to make sure they exist for the duration of `session.run` (which is guarded by the GIL) and then Py_DECREF them afterwards . This should prevent release of the underlying buffer during `session.run` which I guess is what the guard is for?\n", "@mrry did this change get pushed?\n", "Yep, in https://github.com/tensorflow/tensorflow/commit/f68cbf19fb8f67a5dce889a5464a358ec0035f18\n", "@nanddalal is this fixed adequately now?\n", "Automatically closing due to lack of recent activity. Please reopen when further information becomes available.\n", "Does the above fix mean that using `feed_dict` isn't slow now?\n", "@JohnAllen it's still slow\n", "@JohnAllen I would recommend https://github.com/tensorflow/tensorflow/issues/3377#issuecomment-239969809\n"]}, {"number": 2918, "title": "Branch 125120308", "body": "", "comments": []}, {"number": 2917, "title": "tf.fill does not have dtype", "body": "[tf.fill](https://www.tensorflow.org/versions/r0.9/api_docs/python/constant_op.html#fill)\n\nDoes not provide a dtype keyword like tf.ones, tf.zeros.  Is there a particular reason or is just missing ? You can force it by casting the number but I don't think is clean `tf.fill([10,10], np.int16(8))`.\n\nEDIT: seems also tf.linspace and tf.range\n", "comments": ["@rmlarsen Could you take a quick look pls?\n", "@Mistobaan Sorry for missing this one. I don't think this is a good change. By leaving the explicit casting outside the op, the user is unable to provide a value that is not  in the range of the desired type, and therefore less likely to be surprised by getting back a tensor with a different value than the one they specified.\n", "Automatically closing due to lack of recent activity. Please reopen when further information becomes available.\n", "Automatically closing due to lack of recent activity. Please reopen when further information becomes available.\n"]}, {"number": 2916, "title": "[Feature] RDMA support for distribued Tensorflow", "body": "Tensorflow paper (http://arxiv.org/pdf/1605.08695v1.pdf) states that Tensorflow supports multiple communication protocols such as gRPC over TCP, and RDMA over Converged Ethernet. Current repo, on the other hand, only has gRPC implementations. \n\nDo you have any plan to introduce RDMA support? It should be beneficial especially for GPU-to-GPU communications across servers. \n", "comments": ["I'm reading distributed tensorflow code recently, hope I can give a help.\n", "The only concern of external contribution of adding RDMA support is whether Google will release its own implementation in the future, which will render external effort less meaningful. \n\nCan we have a member in the TF team confirm that it will not be open sourced, at least not recently?\n", "There's some DMA-related code in open-source TensorFlow, @poxvoculi probably knows how far it is from being usable to do RDMA in open-source\n", "RDMA for open source TF is an issue that's starting to get some attention from the TF team, though I can't yet predict timing for a solution.  gRPC is not likely to be extended in the necessary way, and the internal system referred to in the cited paper is not suitable for open sourcing.  Presumably if you are interested in this problem, you're already using a cluster with GPUs and RDMA capable networking.  If so,  would you mind saying what kind of rpc/networking you're using on that cluster?\n", "One possible way to leverage RDMA is to use MPI version of TensorFlow which is available with Machine Learning Toolkit on Extreme Scale (MaTEx): https://github.com/abhinavvishnu/matex\n", "I've only had a quick look but it seems rather ad-hoc (implemented in MPI-Python and run single-node TF in it). Plus it is certainly not the same as what appears in the TF paper. I believe what we are talking about would be a truly remote API that support sync/async communication and send/recv graph partitioning, or pretty much everything in the current gRPC runtime.\n", "OK. Actually mpi-python will provide better performance than gRPC because\nMPI can use high speed interconnect. Can you be more specific as to how it\nis different than the TF paper? Does gRPC support collective operations?\nsend/recv with parameter server will not work for more than a few compute\nnodes.\n\nOn Tue, Oct 18, 2016 at 8:13 AM, Bairen Yi notifications@github.com wrote:\n\n> I've only had a quick look but it seems rather ad-hoc (implemented in\n> MPI-Python and run single-node TF in it). Plus it is certainly not the same\n> as what appears in the TF paper. I believe what we are talking about would\n> be a truly remote API that support sync/async communication and send/recv\n> graph partitioning, or pretty much everything in the current gRPC runtime.\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2916#issuecomment-254538689,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AGMD7zysqQZuOkq8epf83zCmmiRsiQw8ks5q1OItgaJpZM4I35Qd\n> .\n\n## \n\nAbhinav Vishnu\nResearch Scientist\nPacific Northwest National Lab\nRichland, WA 99352\n", "Basically explicit messaging needs to be handled by MPI users, while the interface provided by TF is higher (e.g. node placement for model parallelism) and the design of send/recv node makes it compatible with single-device graph execution.\n\nPlease refer to section 3.2 of the TF white paper for details.\n", "Is the RdmaSession the correct way to go for the RDMA support? We're also investigating the RDMA solution.", "MPI is the way to go for RDMA support on compute clusters - this is the most common setup. There is also RDMA versions of Hadoop/Spark/Hbase. So the data side is distributed. As for the distributed learning, TF is behind CNTK - which runs on MPI and thus has RDMA for free. gRPC is dead end, particularly given GPUDirect RDMA.", "Google is planning to transition to gRPC for all internal networking, so it's not a dead-end in terms of development. Also, I've been told by Baidu ML people that they have to maintain their own fork of MPI because the official one has bugs. That said, getting RDMA for free seems to be a good reason to support MPI", "If there is a convergence between distributed learning and distributed data - it will have to take into account investments people made on both ends. HPC community put down a lot into low latency networking to go with the compute clustering. Very different paradigm to high latency networking, commodity hardware and high redundancy suited for high data volume / low computation needs.\r\n\r\nMPI is mature and stable - powers HPC throughout the world since mid 90s. Has RDMA on all implementations microsoft, intel, and OpenMPI. Which one Baidu referred to? \r\n\r\nThe first ML vendor to present RDMA / Infiniband support along the entire data path will win big. ", "@yaroslavvb Indeed, as @avader906 implied, MPI is a standard API (http://mpi-forum.org/) with many implementations.  According to [this paper](https://arxiv.org/vc/arxiv/papers/1501/1501.02876v1.pdf), Baidu uses [MVAPICH2](http://mvapich.cse.ohio-state.edu/), which has GPUdirect support.  I'll hazard a guess that the bugs they see are related to GPUdirect and may in fact be performance defects, rather than functional correctness issues.\r\n\r\nIn any case, please do not create [FUD](https://en.wikipedia.org/wiki/Fear,_uncertainty_and_doubt) about MPI.  No MPI implementation is perfect, and some are better than others.", "I would like to join in -- if possible. We have been identifying minimal\nchanges in TF runtime to support MPI based parallelism in the optimizer\nclass. We expect the implementation to be ready in a few weeks.\n\nPlease let me know, if you would like to discuss this further.\n\nSincerely,\n\n:- Abhinav\n\nOn Sun, Jan 29, 2017 at 2:47 AM, avader906 <notifications@github.com> wrote:\n\n> @jeffhammond <https://github.com/jeffhammond> and @yaroslavvb\n> <https://github.com/yaroslavvb> are the two people who's contribution to\n> get TF on RDMA would be invaluable.\n>\n> TF does not scale beyond one host only in the multiple node / multiple GPU\n> distributed learning environment. Just look at the calculation bandwidth\n> that is now possible:\n> https://devblogs.nvidia.com/parallelforall/benchmarking-\n> gpudirect-rdma-on-modern-server-platforms/\n>\n> MPI provides what we need today while gRPC might get there with time. What\n> would be architectural and API choices to make sure TF could be compiled\n> against both ?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/2916#issuecomment-275906182>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AGMD7xpjB6-dsmuZWxZzMIDXJfRxeob3ks5rXG4ngaJpZM4I35Qd>\n> .\n>\n\n\n\n-- \nAbhinav Vishnu\nResearch Scientist\nPacific Northwest National Lab\nRichland, WA 99352\n", "@avader906 Some numbers/concrete performance goals would be useful here. IE, what is an important application, how fast is it now in TF GRPC, how fast is it in competing implementation?\r\n\r\nNote that there are two use-cases -- using distributed TensorFlow on public cloud, and using TensorFlow on dedicated compute clusters. Existing TF is optimized for the public cloud case because that's what compute at Google is like -- CPU and network resources are shared among many competing users, so there's variability in communication/computation performance across workers and individual workers can die without warning. At OpenAI we use distributed tensorflow to train RL agents like [this](https://github.com/openai/universe-starter-agent) on public cloud providers and maintaining performance despite some workers slowing down/restarting/network flakiness is an important feature.\r\n\r\nThe \"dedicated compute cluster\" scenario is somewhat alien to Google, so this would need community to help define goals/requirements.\r\n\r\nIt seems @abhinavvishnu has done some work on integrating MPI. For a brief glance at the matex repo, it seems the approach is well-supported by current TF abstractions -- using `session.run` to do the computation, then using MPI for data transfers. The performance question is the latency introduced by going through TF client API. I've done preliminary investigation of the overhead introduced by `session.run` and it seems it can be reduced by 50% by using lower level TF_SessionRun (easy), and possibly another 50-95% by removing the memory copy (harder)", "@avader906 contributions are welcome in this area. One starting point might be evaluating @abhinavvishnu 's approach for combining TensorFlow and MPI and identifying/removing bottlenecks. BTW, has anyone evaluated CNTK vs TF grpc on those architectures? Resnet CIFAR is implemented under [models repo](https://github.com/tensorflow/models/tree/master/resnet) and is well maintained. Resnet training has low bandwidth/compute ratio, and async SGD shouldn't be sensitive to latency, so it would be curious to see if current solution matches CNTK in performance already.", "@avader906 Thanks for the link. Note that the study compares CNTK toolkit which uses MPI to distribute to multiple GPUs, to single-machine TensorFlow which uses direct GPU->GPU transfers. The 3.31x speedup they get on CIFAR resnet50 using 4 GPUs on CNTK suggests that MPI approach is quite efficient. Not sure what's going on with single-machine TF getting only 1.65x scaling on 4x GPUs, that would need some investigation", "@avader906 I'm not going to wade too far into the MPI vs TCP/IP or Ethernet vs InfiniBand debates, other than to say that the real advantage of MPI is the ability to target any of these technologies and more while outsourcing the low-level performance optimization to someone else (who is usually an expert).  For example, IBM used MPI when running deep learning on their [Blue Gene/Q](http://www-03.ibm.com/systems/technicalcomputing/solutions/bluegene/) system [1] and my Intel colleagues used MPI when running the same code across AWS and a [Cray XC](http://www.cray.com/products/computing/xc-series) supercomputer [2].\r\n\r\n1. [Parallel Deep Neural Network Training for Big Data on Blue Gene/Q](https://doi.org/10.1109/TPDS.2016.2626289)\r\n2. [Distributed Deep Learning Using Synchronous Stochastic Gradient Descent](https://arxiv.org/abs/1602.06709)", "@jeffhammond There is no debate. Questions where asked why this (RDMA) feature is important and what could be improved in TF / gRPC. \r\n\r\nDetailed technical argument requires networking and distributed systems design knowledge - not a requirement per say to code TF and meet clearly articulated users' needs. \r\n\r\nThere are now sufficient references in this thread - industry use cases, single host benchmarking and estimates of gains in distributed environment to focus on delivering the feature via an MPI implementation.\r\n\r\nPatiently waiting for updates from @abhinavvishnu and wish him and hist team all the best\r\n\r\nPS: @jeffhammond Would be great if Intel could donate Parallel Studio Cluster edition (to @abhinavvishnu ) for the implementation check (against reference MPI implementation) and benchmarking (against gRPC).\r\n\r\nPS2: \u0434\u043b\u044f \u0433\u043e\u0441\u043f\u043e\u0434\u0438\u043d\u0430 \u0411. / @yaroslavvb  \u044f \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u044e \u043e\u0437\u043d\u0430\u043a\u043e\u043c\u0438\u0442\u044c\u0441\u044f \u0441 \u043c\u0430\u0442\u0435\u0440\u0438\u0430\u043b\u0430\u043c\u0438 \u043d\u0430 \u0441\u0442\u0440\u0430\u043d\u0438\u0447\u043a\u0435 http://parallel.ru/tech/tech_dev/mpi.html", "Thanks for the detailed discussion.\n\nWe are currently making code changes in the TF runtime. The reason we have\na memcpy at this point is because we had issues with pyfunc not working\ncorrectly for all versions. More as soon as we have updates.\n\nSincerely,\n\n:- Abhinav\n\nOn Mon, Jan 30, 2017 at 2:33 AM, avader906 <notifications@github.com> wrote:\n\n> @jeffhammond <https://github.com/jeffhammond> There is no debate.\n> Questions where asked why this (RDMA) feature is important and what could\n> be improved in TF / gRPC.\n>\n> Detailed technical argument requires networking and distributed systems\n> design knowledge - not a requirement per say to code TF and meet clearly\n> articulated users' needs.\n>\n> There are now sufficient references in this thread - industry use cases,\n> single host benchmarking and estimates of gains in distributed environment\n> to focus on delivering the feature.\n>\n> Some more introductory insights into technical aspects could be inferred\n> from here:\n>\n> cuda-aware 2 node MPI benchmarking:\n> https://devblogs.nvidia.com/parallelforall/benchmarking-cuda-aware-mpi/\n>\n> GPUDirect on RDMA\n> https://devblogs.nvidia.com/parallelforall/benchmarking-\n> gpudirect-rdma-on-modern-server-platforms/\n>\n> Patiently waiting for updates from @abhinavvishnu\n> <https://github.com/abhinavvishnu> and wish him and hist team all the best\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/2916#issuecomment-276028310>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AGMD7yt4va4i5cGSME_KlSNxQazUL-pbks5rXbx1gaJpZM4I35Qd>\n> .\n>\n\n\n\n-- \nAbhinav Vishnu\nResearch Scientist\nPacific Northwest National Lab\nRichland, WA 99352\n", "@abhinavvishnu are you modifying C++ core runtime (ie files like `executor.cc`) or just the Python parts? Changes to core runtime would take significant effort to integrate since they touch everything. Modifying python parts and compiled code loaded through `tf.load_library` mechanism are much easier to integrate.", "We are starting with Python. Our objective is to minimize changes to C++,\nbut we may have to. We will keep you posted of some of the decisions!\n\nThanks,\n\n:- Abhinav\n\nOn Mon, Jan 30, 2017 at 4:49 PM, Yaroslav Bulatov <notifications@github.com>\nwrote:\n\n> @abhinavvishnu <https://github.com/abhinavvishnu> are you modifying C++\n> core runtime (ie files like executor.cc) or just the Python parts?\n> Changes to core runtime would take significant effort to integrate since\n> they touch everything. Modifying python parts and compiled code loaded\n> through tf.load_library mechanism are much easier to integrate.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/2916#issuecomment-276239884>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AGMD708n0OAKJJQck4RiMOeOUn4WwtUjks5rXoUpgaJpZM4I35Qd>\n> .\n>\n\n\n\n-- \nAbhinav Vishnu\nResearch Scientist\nPacific Northwest National Lab\nRichland, WA 99352\n", "@abhinavvishnu what is the ballpark overhead / performance penalty of python route vs refactoring transport architecture dependency in core ? 20% ? 40%?\r\n\r\n@yaroslavvb what are unit tests for gRPC dependencies in the repository ? ease of integration and various ugliness of same nature should never be a development objective. what is your estimate for refactoring ?", "Does anybody have a neural net that benefits significantly from fast networking (e.g. infiniband, etc)?\r\n\r\nI played with the vgg model from https://github.com/tensorflow/models/tree/master/slim. I compared two cases: (1) single-node-single-gpu, (2) 1-worker-1-ps, where the worker was on a GPU node and the parameter server was on a CPU node. In both cases, I used batch size 32 and I saw almost identical training speed (i.e. sec/step). If communication was bottleneck, I should see a lot slower with the worker/ps configuration.\r\n\r\nI could not run more than one workers with slim/vgg. I got out-of-memory error on the chief worker (I was using Nvidia K80 with 12GB memory). \r\n\r\nIf anyone knows a good model that runs on multiple nodes and shows potential speedup with faster networking, I would like to know. \r\n\r\nThanks.\r\n\r\n", "@avader906 Thanks for your informative reply.\r\n\r\nI have no doubt that high-speed interconnection helps in a multi-node setting. We see it with Caffe, more precisely CaffeOnSpark since Caffe does not go beyond single node. I however have yet to see it with Tensorflow.\r\n\r\nThe chart you provided \"PerformanceChart.png\" does not scale to more than 1-node for tensorflow. The Alexey-Kamenev tensorflow benchmark  shows only 1-node-4-gpu case. I do not see multi-node-multi(or single)-gpu settings in dlbench web page either (or maybe I missed it).\r\n\r\nI was not expecting speed-up in the 1-worker-1ps case, instead I was hoping to see slow-down compared to training the same VGG on a single GPU. But to my supprise, I see more or less the same speed. ", "@junshi15 Sorry, but I do not understand the purpose of your test and how does it relate to RDMA. If you want to test scalability - run TF on two GPUs on the same node and compare it with other kits. This is the absolute performance (on 2 GPUs) you can get regardless the interconnect used. The question is then how much you loose replacing PCI bus connection between both with various remote networking technologies - including appropriate hardware and software stacks. \r\n\r\nRegarding Hadoop/Spark - are you running builds supporting RDMA in infiniband environment? What order of speedup have you seen from moving away from ethernet? Regarding benchmarks - you will have to modify source codes to suit your exact needs.", "@avader906 Let's say I want to train a model with 32 GPU's. The maximum number of GPUs I can cram in a single box is 8. So I end up with 4 nodes. Then I connect the nodes with 1Gbps ethernet (those are all I have at this moment).  I launch the distributed version of Tensorflow, start training. It may take weeks to finish. I am not happy. Let's say I can not change the neural nets to reduce the computation. My next step may be upgrading ethernet cards to those shiny infiniband cards. I then somehow need to get tensorflow to work with inifiniband, maybe with help from this thread.\r\n\r\nNow before I invest in infiniband, I would like to know if the upgrade has any impact on my training. My VGG example, albeit artificial,  shows there is no need to upgrade the network for that particular case, since even with ethernet, my 1-worker-1-ps is as good as single-gpu, the best one can get.\r\n\r\nYes, CaffeOnSpark, https://github.com/yahoo/CaffeOnSpark, supports RDMA if you build with infiniband flag on and running in GPU mode. We see 2x improvement in speed although this  depends on a lot of settings.\r\n", "This seems to be model specific, for instance https://github.com/rafaljozefowicz/lm becomes 8x slower if you use 8 1-GPU machines with 1Gbps connectivity compared to 8 GPUs in a single process.", "Chainer folks claim that they are 5.5 times faster than TF in the distributed environment (128 GPUs, Resnet-50):\r\nhttp://chainer.org/general/2017/02/08/Performance-of-Distributed-Deep-Learning-Using-ChainerMN.html\r\n\r\nI think it is mostly because of RDMA/InfiniBand (MXNet and CNTK are doing much better, most likely because of it).", "Chainer is showing results for ResNet-50, which is much easier to scale the\nAlexNet/GoogleNet. They are using MPI based implementation as well. So it\nis pretty good, but I am not sure, if they are doing any other\noptimizations such as asynchronous communication, layer-wise etc.\n\n:- Abhinav\n\nOn Thu, Feb 9, 2017 at 6:10 AM, Yaroslav Goncharov <notifications@github.com\n> wrote:\n\n> Chainer folks claim that they are 5.5 times faster than TF in the\n> distributed environment (128 GPUs, Resnet-50):\n> http://chainer.org/general/2017/02/08/Performance-of-\n> Distributed-Deep-Learning-Using-ChainerMN.html\n>\n> I think it is mostly because of RDMA/InfiniBand (MXNet and CNTK are doing\n> much better than TF most likely because of it).\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/2916#issuecomment-278612491>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AGMD7ydCkSmF_Y25cG7eo53dms3EHP94ks5ravQxgaJpZM4I35Qd>\n> .\n>\n\n\n\n-- \nAbhinav Vishnu\nResearch Scientist\nPacific Northwest National Lab\nRichland, WA 99352\n", "We recently released a package called TensorflowOnSpark (https://github.com/yahoo/tensorflowonspark), which includes a RDMA implementation for tensorflow (https://github.com/yahoo/tensorflow/tree/yahoo/tensorflow/core/distributed_runtime/rdma).\r\n\r\nThe RDMA implementation is not based on MPI, but rather based on ibverbs C++ API. We keep the GRPC protocol for cluster setup, graph exchange, etc, but replace all the tensor send/receive with RDMA write. You can find more info in the reame file (https://github.com/yahoo/tensorflow/blob/yahoo/tensorflow/core/distributed_runtime/rdma/rdma_readme.md).\r\n\r\nWhile training large networks like VGG_19, we saw 2x faster on infiniband than GRPC on ethernet.  The improvement on inception_v3 was found limited, maybe less than 10% improvement for a 2-node cluster, a bit more for a 4-node cluster.\r\n\r\n\r\n", "Please checkout this [RDMA enhancement ](https://github.com/yahoo/tensorflow/blob/yahoo/tensorflow/core/distributed_runtime/rdma/rdma_readme.md)of TensorFlow mentioned by @junshi15. You could use it independent of TensorFlowOnSpark. \r\n\r\nIt is designed to work in a cloud setting, where a set of nodes are dynamically allocated. \r\nThat's why we avoid MPI option. ", "@anfeng What specific issues do you see with using MPI in a cloud setting?\r\n\r\nMPI has supported dynamic process management since version [MPI 2.0](http://mpi-forum.org/docs/mpi-2.0/mpi2-report.pdf), which was released in 2003.  Granted, high-quality support for functions like [MPI_Comm_spawn_multiple](https://linux.die.net/man/3/mpi_comm_spawn_multiple) has not always been widely available, but that is only because the majority of MPI users do not care about that feature set (because they are using MPI in a static resource environment).\r\n\r\nDynamically subtracting nodes from MPI jobs is a slightly more complicated issue than adding nodes, but this is intimately related to fault-tolerance activities that are of great interest to many in the MPI community right now.\r\n\r\nMPI-in-the-cloud proof points include:\r\n* A friend of mine has enabled MPICH to work with Apache YARN as described [here](http://lists.mpich.org/pipermail/devel/2016-July/000717.html).\r\n* Mesos appears to support MPICH via [this project](https://github.com/mesosphere/mesos-hydra).\r\n* AWS supports MPI as described [here](https://cyclecomputing.com/running-mpi-applications-in-amazon-ec2/).\r\n* MPI plus Hadoop is described by Intel [here](https://software.intel.com/en-us/articles/intel-mpi-library-supporting-the-hadoop-ecosystem).\r\n\r\nOne of the advantages of using MPI is that you can reuse a wide range of numerical linear algebra software from the HPC community, e.g. [CTF](https://github.com/solomonik/ctf) achieves close to 50% of peak on 1000+ node supercomputers ([details](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-143.html)) for tensor operations.  I'm not aware of any distributed-memory deep learning frameworks that can do this yet.\r\n\r\nDisclaimer: I work for Intel and was involved in two of the aforementioned efforts (which two should be obvious within two clicks).", "Let them stare on to shadows. Everything in that thread - including\r\ntheoretical speedup from various interconnect / distributed computing stack\r\ncombinations can be calculated with a piece of paper. Latencies are known\r\nat every point and so is bandwidth. Scalability factors could be\r\napproximated from various benchmarks around there. If you give people\r\nnumbers and they resist, the issue is something else. There are multiples\r\nof alternatives to TF - it was not the first and not the last and is\r\nfundamentally flawed from the bottom of architecture up.\r\n\r\nOn Tue, Feb 14, 2017 at 1:44 AM, Jeff Hammond <notifications@github.com>\r\nwrote:\r\n\r\n> @anfeng <https://github.com/anfeng> What specific issues do you see with\r\n> using MPI in a cloud setting?\r\n>\r\n> MPI has supported dynamic process management since version MPI 2.0\r\n> <http://mpi-forum.org/docs/mpi-2.0/mpi2-report.pdf>, which was released\r\n> in 2003. Granted, high-quality support for functions like\r\n> MPI_Comm_spawn_multiple\r\n> <https://linux.die.net/man/3/mpi_comm_spawn_multiple> has not always been\r\n> widely available, but that is only because the majority of MPI users do not\r\n> care about that feature set (because they are using MPI in a static\r\n> resource environment).\r\n>\r\n> Dynamically subtracting nodes from MPI jobs is a slightly more complicated\r\n> issue than adding nodes, but this is intimately related to fault-tolerance\r\n> activities that are of great interest to many in the MPI community right\r\n> now.\r\n>\r\n> MPI-in-the-cloud proof points include:\r\n>\r\n>    - A friend of mine has enabled MPICH to work with Apache YARN as\r\n>    described here\r\n>    <http://lists.mpich.org/pipermail/devel/2016-July/000717.html>.\r\n>    - Mesos appears to support MPICH via this project\r\n>    <https://github.com/mesosphere/mesos-hydra>.\r\n>    - AWS supports MPI as described here\r\n>    <https://cyclecomputing.com/running-mpi-applications-in-amazon-ec2/>.\r\n>    - MPI plus Hadoop is described by Intel here\r\n>    <https://software.intel.com/en-us/articles/intel-mpi-library-supporting-the-hadoop-ecosystem>\r\n>    .\r\n>\r\n> One of the advantages of using MPI is that you can reuse a wide range of\r\n> numerical linear algebra software from the HPC community, e.g. CTF\r\n> <https://github.com/solomonik/ctf> achieves close to 50% of peak on 1000+\r\n> node supercomputers (details\r\n> <https://www2.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-143.html>)\r\n> for tensor operations. I'm not aware of any distributed-memory deep\r\n> learning frameworks that can do this yet.\r\n>\r\n> Disclaimer: I work for Intel and was involved in two of the aforementioned\r\n> efforts (which two should be obvious within two clicks).\r\n>\r\n> \u2014\r\n> You are receiving this because you were mentioned.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/2916#issuecomment-279570898>,\r\n> or mute the thread\r\n> <https://github.com/notifications/unsubscribe-auth/AVoAWGhhd_fa8Szyo3DZLsmURlFplg5wks5rcPjngaJpZM4I35Qd>\r\n> .\r\n>\r\n", "@junshi15 Could this difference caused by this issue: https://github.com/tensorflow/tensorflow/issues/6116?", "@llhe I think there are two causes. First, infiniband RDMA is faster than Ethernet. Second, the problem you highlighted in issue #6116. Our work was based on TF 0.12.1, the new GRPC patch was not applied. ", "@junshi15 I am very interesting to test out your RDMA implementation. However I see difference only on C++ part; did you provide any interface for Python API users to use your RDMA implementation?", "@byronyi A quick instruction is [here](https://github.com/yahoo/tensorflow/issues/2). Please post specific questions over there so we don't pollute this thread. Thanks.", "It says hybrid protocol is not supported, and I wonder if that is something we could work on. Traces in current TF [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/worker.proto#L225) and [there](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/worker.proto#L247) suggests that hybrid protocol is supported by the original TF in Google, yet these facilities are not used in your implementation.", "@byronyi \"Hybrid\" means some of the nodes in the cluster are connected with Ethernet and others are with Infiniband. We have not found such a use case. If you think that's something interesting to you, feel free to extend the code. ", "S-Caffe state of the art implementation details - including CUDA IPC / NCCL, GPUDirect RDMA  in the context of hybrid MPI + CUDA RDMA aware application using 160 GPU multi-host system.\r\n\r\nhttp://dl.acm.org/citation.cfm?id=3018769\r\n\r\nmore background :\r\nhttp://www.hpcadvisorycouncil.com/events/2017/stanford-workshop/pdf/DKPanda_DesigningMiddleware_DeepHPC_Wednesday_02082017.pdf", "someone made a PR to add MPI interface -- https://github.com/tensorflow/tensorflow/pull/7710", "Baidu Research All-Reduce MPI patch https://github.com/baidu-research/tensorflow-allreduce\r\nhttp://www.tomshardware.com/news/baidu-svail-ring-allreduce-library,33691.html", "I guess it depends on the model and how much the computation part is VS the\nnetwork part of the model.\nCan u elaborate about the application/model/network being used ?\n\nOn Tue, Feb 28, 2017 at 7:39 AM, kaizhigaosu <notifications@github.com>\nwrote:\n\n> I has a distributed tensorflow case which has 2 workers, but the speed up\n> is only near 1. I don't know if it is related with this issue you\n> discussed. Can you have a look?\n> http://stackoverflow.com/questions/42500739/between-\n> graph-replication-of-ptb-rnn-model-is-slower-than-single-gpu-version\n> <http://url>\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/2916#issuecomment-282947494>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AVPgz8aN30NBFHj6Wqc0DHUzovjtmUKfks5rg7MngaJpZM4I35Qd>\n> .\n>\n\n\n\n-- \n*Ido*\n", "Hi All:\n\nWe have checked in an initial support for distributed TF, such that the\nchanges for distributed memory are abstracted from the user. The code is\navailable at:\n\nhttps://github.com/abhinavvishnu/matex/tree/master/src/deeplearning\n\nPlease find the documentation at https://github.com/abhinavvishnu/matex/wiki\n\nWe have tested it on GPU clusters and CPU clusters.\n\nTo do:\n\n1) multi-GPU support\n2) Keras integration\n3) A document which describes our changes.\n\nThanks, please let us know if you have any questions.\n\nSincerely,\n\n;- Abhinav\n\nOn Tue, Feb 28, 2017 at 4:37 AM, Ido Shamay <notifications@github.com>\nwrote:\n\n> I guess it depends on the model and how much the computation part is VS the\n> network part of the model.\n> Can u elaborate about the application/model/network being used ?\n>\n> On Tue, Feb 28, 2017 at 7:39 AM, kaizhigaosu <notifications@github.com>\n> wrote:\n>\n> > I has a distributed tensorflow case which has 2 workers, but the speed up\n> > is only near 1. I don't know if it is related with this issue you\n> > discussed. Can you have a look?\n> > http://stackoverflow.com/questions/42500739/between-\n> > graph-replication-of-ptb-rnn-model-is-slower-than-single-gpu-version\n> > <http://url>\n> >\n> > \u2014\n> > You are receiving this because you are subscribed to this thread.\n> > Reply to this email directly, view it on GitHub\n> > <https://github.com/tensorflow/tensorflow/issues/\n> 2916#issuecomment-282947494>,\n> > or mute the thread\n> > <https://github.com/notifications/unsubscribe-auth/\n> AVPgz8aN30NBFHj6Wqc0DHUzovjtmUKfks5rg7MngaJpZM4I35Qd>\n> > .\n> >\n>\n>\n>\n> --\n> *Ido*\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/2916#issuecomment-283027829>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AGMD779fbceq-W4fGBc3Jv7zB1xHGVJ9ks5rhBTsgaJpZM4I35Qd>\n> .\n>\n\n\n\n-- \nAbhinav Vishnu\nResearch Scientist\nPacific Northwest National Lab\nRichland, WA 99352\n", "We created a PR #8943 to enable RDMA based distributed training on TensorFlow about 2 weeks ago. This PR was merged into master yesterday with help from TensorFlow team. \r\n\r\nPlease give it a try. We welcome your feedbacks. ", "@anfeng where are the benchmarks? ", "We (@bkovalev and me) did some testing on the new verbs code VS the default gRPC over TCP.\r\nSome of the results in - https://docs.google.com/spreadsheets/d/1UIqmp5sKwPnxWpZkJ1rCcGrJz-Iqr4_1NZdZdRqJ7N8/pubhtml.\r\n\r\nYou can see there up to ~25% improvement with verbs in Inception-V3 (with only 14GPUs K40 in 7 nodes) and good results in VGG-16 case, where TCP can hardly get it running.\r\nThe TCP issue is caused by a 100% CPU utilization on the workers cores sending the tensors (~5Gbit/s parameters/gradient vectors), we tried some optimizations (TCP tunings, IRQ affinity methods) but nothing helped to this moment (we are still working to tune performance in this case.\r\n\r\nWe are working to extend the scale and optimize the verbs code even further.\r\nThanks @junshi15 @anfeng for the excellent code contribution.", "@shamoya On what network (IB or RoCE, link speed) are you performing the testing?", "@shamoya Is that possible that maybe your Test about TCP is actually run over ipoib ?", "@thinxer @jcf94 \r\nTCP results are from 100GE network.\r\nThe RDMA test ran on IB (EDR) network.\r\nWe are starting soon a new cycle with P100 (PCIe) GPUs and RDMA running on RoCE (same 100GE network as TCP).", "@shamoya one-for-one on the hardware side (GPUs and nodes), RoCE (100GE) will give back that 25% vs pure IB (EDR, RDMA). The overhead will depend on the number of connections rather than amount of data - bandwidth would be approximately equivalent with lower latency for IB. The main ethernet NIC parameters for tuning are offloading Tx/Rx checksumming - if NICs are capable of doing it you wont see the mentioned CPU loads. NUMA nodes/Affinity is super important for [IB HCA performance](https://community.mellanox.com/docs/DOC-2489).\r\n\r\nAbout GPUdirect over RDMA - is that something in the pipeline?\r\n- [RDMA primer](https://htor.inf.ethz.ch/blog/index.php/2016/05/15/what-are-the-real-differences-between-rdma-infiniband-rma-and-pgas/)\r\n- [NVIDIA GPUdirect doc](http://docs.nvidia.com/cuda/gpudirect-rdma/#abstract)\r\n- [Mellanox GPUdirect doc](http://www.mellanox.com/related-docs/prod_software/Mellanox_GPUDirect_User_Manual_v1.0.pdf)\r\n- [Benchmarking GPUdirect RDMA](https://devblogs.nvidia.com/parallelforall/benchmarking-gpudirect-rdma-on-modern-server-platforms/)\r\n", "Hi @avader906, \r\n\r\nRoCE V2 penalty VS IB EDR is around 8%, this is from the same machines I ran the tests:\r\n\r\n<pre>\r\nib_write_bw -s 8388608 -d mlx5_1 --report_gbits 10.143.119.22\r\n---------------------------------------------------------------------------------------       \r\n                    RDMA_Write BW Test                                                        \r\n Dual-port       : OFF          Device         : mlx5_1                                       \r\n Number of qps   : 1            Transport type : IB                                           \r\n Connection type : RC           Using SRQ      : OFF                                          \r\n TX depth        : 128                                                                        \r\n CQ Moderation   : 100                                                                        \r\n Mtu             : 4096[B]                                                                    \r\n Link type       : IB                                                                   \r\n Max inline data : 0[B]                                                                       \r\n rdma_cm QPs     : OFF                                                                        \r\n Data ex. method : Ethernet                                                                   \r\n---------------------------------------------------------------------------------------       \r\n local address: LID 0x0b QPN 0x01e9 PSN 0xfe065 RKey 0x0ce502 VAddr 0x007f01ab331000          \r\n remote address: LID 0x0c QPN 0x01d3 PSN 0x4347ae RKey 0x0351af VAddr 0x007fe4e5351000        \r\n---------------------------------------------------------------------------------------       \r\n #bytes     #iterations    BW peak[Gb/sec]    BW average[Gb/sec]   MsgRate[Mpps]              \r\n8388608    5000             99.11              99.00              0.001475                   \r\n---------------------------------------------------------------------------------------\r\n\r\nib_write_bw -s 8388608 -d mlx5_0 -x 0 --report_gbits 10.143.119.22\r\n---------------------------------------------------------------------------------------\r\n                    RDMA_Write BW Test\r\n Dual-port       : OFF          Device         : mlx5_0\r\n Number of qps   : 1            Transport type : IB\r\n Connection type : RC           Using SRQ      : OFF\r\n TX depth        : 128\r\n CQ Moderation   : 100\r\n Mtu             : 1024[B]\r\n Link type       : Ethernet\r\n GID index       : 0\r\n Max inline data : 0[B]\r\n rdma_cm QPs     : OFF\r\n Data ex. method : Ethernet\r\n---------------------------------------------------------------------------------------\r\n local address: LID 0000 QPN 0x016b PSN 0x1833eb RKey 0x002d9c VAddr 0x007f12498bd000\r\n GID: 254:128:00:00:00:00:00:00:126:254:144:255:254:32:230:120\r\n remote address: LID 0000 QPN 0x016b PSN 0xa4b427 RKey 0x00962b VAddr 0x007f79e76f3000\r\n GID: 254:128:00:00:00:00:00:00:126:254:144:255:254:32:232:60\r\n---------------------------------------------------------------------------------------\r\n #bytes     #iterations    BW peak[Gb/sec]    BW average[Gb/sec]   MsgRate[Mpps]\r\n 8388608    5000             91.57              91.57              0.001365\r\n---------------------------------------------------------------------------------------\r\n\r\n</pre>\r\n\r\n25% is only on the Inception case (and tested on small scale - potentially more with larger scale), with VGG (which is much harder to scale) it's uncomparable improvement for now (gRPC TCP is literally unusable here).\r\n\r\nAll TX/RX NIC Ethernet features are enabled (RX/TX Checksums, TSO/GRO/LRO etc..).\r\nWe tried disabled TSO (CPU intensive) and played with Affinities to split the TCP stack code from the gRPC application and that also didn't helped.\r\nThe problem here I think is the memory management done by the gRPC (allocation and freeing of Tensor buffers) - I'm still looking at it and will share once I know more.\r\n\r\nGPU Direct RDMA is in the pipeline, along with memory optimizations (ODP) and other improvement (connections management, etc..). I'm preparing a plan (depends on the resources we'll have here) and I will gladly share it soon I have it.\r\n\r\nAs the above optimizations are related to verbs work provided by @junshi15 (which is a P2P RDMA),\r\nI think we must also talk about future work which will introduce collectives notation of reduction between the workers (thus remove the need of parameter server) *without integrating MPI* (but rather a \"native\" approach of collectives in tensorflow).", "@shamoya tremendous work! P2P RDMA is more usefull for the majority of users who do not run Tesla. Regarding RoCE vs IB - reoccurring theme in this thread is missing the role of latency. You have calculated the bandwidth comparison based on one connection and MTU. Depending on MTU (which is tuned to NUMA/Affinity) and the number of threads/connections - that bandwidth will change. Because of latencies (overhead in establishing and replying to a connection), RoCE will always have a lower practical throughput than otherwise indicated by bandwidth measurement alone. The more frequent the data interchange - the shorter the packet size, the more performance RoCE will suffer. This is by design - as DMA and packet networks are two different technologies at core.", "@avader906 - In general I agree with the impact of RoCE added latency and number of connections, but I don't think it matters at all in our specific use case.\r\nWe care about large messages (Tensors) so latency ~ throughput,\r\nand number of connections is not in large scale (dozens of nodes at max) so I don't expect any penalties in HW caches, and all of them are established beforehand.\r\nDo you agree or I misunderstood your point ? ", "@shamoya it was a general reflection on scalability and the compound effect latencies might have within platform architecture decisions. i agree with you that specific performance is use case dependent.", "Ok, I see now the bad performance with gRPC TCP on VGG-16 (large Tensors) is explained in #6116 (Thanks for the great analysis @llhe ). \r\nWaiting for #7466 to be merged and and we will re-run the gRPC TCP results again.\r\nThis will give us much better indication of the RDMA improvement in large tensors.", "@shamoya  is there a testbed in tensorflow specifically for gRPC? or even a crude hello world.\r\nI have all the protobufs converted over to swift 3.1 but now just trying to wrap head around dropping down to gRCP https://github.com/nubbel/swift-tensorflow\r\n", "We published a RDMA based version here: https://github.com/CGCL-codes/Tensorflow-RDMA\r\nFYI.", "Closing due to lack of recent activity, but please let me know if I'm mistaken. Thank you.", "@jvishnuvardhan I believed this issue is obsolete and we do have RDMA support with networking plugins in contrib/verbs, contrib/mpi, and contrib/gdr. Those plugins are migrating to https://github.com/tensorflow/networking, so it is better to direct related discussions there.", "@byronyi Thanks! I will close this issue.", "> We published a RDMA based version here: https://github.com/CGCL-codes/Tensorflow-RDMA FYI.\r\n\r\nCan you paste the source code?"]}, {"number": 2915, "title": "Error when compiling TensorFlow from source for GPU", "body": "When trying to compile TF from source, I am seeing error\n### Environment info\n\nOperating System:\nUbuntu 16.04\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n /usr/local/cuda/lib/libcudadevrt.a\n/usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\n/usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n/usr/local/cuda/lib/libcudart.so.7.5.18\n/usr/local/cuda/lib/libcudart_static.a\n\nIf installed from sources, provide the commit hash:\n679f95e9d8d538c3c02c0da45606bab22a71420e\n### Steps to reproduce\n1. Git clone from TF source: `git clone https://github.com/tensorflow/tensorflow.git tf_source``\n2. Install Baze and reqs\n3. Run ./configure\n4. Run \n\n```\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\npip install `echo /tmp/tensorflow_pkg/tensorflow-*.whl`\n```\n### Logs or other output that would be helpful\n\n```\n____Building...\n____[0 / 4] Symlinking //tensorflow/tools/pip_package:build_pip_package\n____[3 / 54] Creating source manifest for //tensorflow/tensorboard:tensorboard\n____[4 / 59] Creating runfiles tree bazel-out/local_linux-py3-opt/bin/tensorflow/tensorboard/tensorboard.runfiles\n____[13 / 288] Creating source manifest for //tensorflow/tools/proto_text:gen_proto_text_functions [for host]\n____[20 / 487] Executing genrule //third_party/gpus/cuda:cuda_config_check [for host]\nERROR: /tf_source/third_party/gpus/cuda/BUILD:204:1: declared output 'third_party/gpus/cuda/cuda.config' is a dangling symbolic link.\nERROR: /tf_source/third_party/gpus/cuda/BUILD:204:1: not all outputs were created.\n____Building complete.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\n____Elapsed time: 243.360s, Critical Path: 0.42s\n```\n", "comments": ["Looks like the cuDNN install path is different from `/usr/local/cuda`. Error from my side.\n"]}, {"number": 2914, "title": "Fixup url to MNIST fully-connected feed tutorial", "body": "Documentation at https://www.tensorflow.org/versions/r0.9/get_started/basic_usage.html ends with \" See the MNIST fully-connected feed tutorial (source code) for a larger-scale example of feeds.\", including a link to a broken url.  This changeset changes the source code url from https://www.tensorflow.org/code/tensorflow/g3doc/tutorials/mnist/fully_connected_feed.py to https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/fully_connected_feed.py so that the link resolves to relevant source code.\n", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, test this please.\n", "Jenkins, test this please.\n"]}, {"number": 2913, "title": "Merging release notes changes from r0.9 back into master.", "body": "@martinwicke not sure if we should merge the entire tree back or whether I should cherry-pick one at a time.\n", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "Entire tree is fine. We'll do that after exiting rc, but we might as well\ndo it in between.\nOn Thu, Jun 16, 2016 at 13:11 googlebot notifications@github.com wrote:\n\n> We found a Contributor License Agreement for you (the sender of this pull\n> request) and all commit authors, but as best as we can tell these commits\n> were authored by someone else. If that's the case, please add them to this\n> pull request and have them confirm that they're okay with these commits\n> being contributed to Google. If we're mistaken and you did author these\n> commits, just reply here to confirm.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2913#issuecomment-226599473,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAjO_U-aYErWwJPsjIqmN6jk-5QEQzr1ks5qMa3xgaJpZM4I3wzr\n> .\n"]}, {"number": 2912, "title": "Branch 125083559", "body": "", "comments": ["Jenkins, test this please.\n", "ping for CLA check?\n"]}, {"number": 2911, "title": "Fix build for PPC.", "body": "This CL includes 2 parts -\n1) - provided a speciliazation for vector types on non-sse\nplatforms,\nthe default implementation does not compile for Packet4f type.\n2) - fixed the default implementation (which is unreachable) for 2\nfunctions, the origin version does not compile for Packet4f type.\n\nTested:\nBuilding on ppc machines and x86 machines.\n", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, test this please.\n", "Hi Benoit & Jenkins, thanks. Please test with CL3 (e1bdac5), I ran a local test (via blaze) for CL2, it exposed a few runtime errors, which I fixed in CL3. More detail in CL3's comment.\n\nThanks,\n", "Jenkins, test this please.\n"]}, {"number": 2910, "title": "Hard-coding \"1\" for CUDA version breaks existing MacOS GPU build with CUDA 7.5", "body": "Commit https://github.com/tensorflow/tensorflow/commit/f45874b4a969a60fa9ced86e6769012e4912b5ca broke existing GPU builds on MacOS with CUDA 7.5\n\nIt nows fails\n`I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1078] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.1.dylib; dlerror: dlopen(libcuda.1.dylib, 5): image not found\n`\nA work-around is to create a symlink\n\n```\ncd /usr/local/cuda/lib/\nln -s libcuda.dylib libcuda.1.dylib\n\n```\n", "comments": ["cc @3XX0 and @rdadolf \n\nlife is hard\n", "It was `libcuda.so.1` previously too: https://github.com/tensorflow/tensorflow/commit/d0a822fbcb04d95a643d8efe65699a8d1cdce98b\nBut then the `.1` suddenly disappeared here: https://github.com/tensorflow/tensorflow/commit/8bf6ef1337359993a8be057c0dc90da8f5a6e4fa\n", "I think the .1 makes more sense, so I'd rather someone figure out a way to special-case this for OS X rather than change it back.\n", "@vrv What fun would life be without incompatible versioning?\n\nSo it seems like this is coming from mutual incompatibility in nvidia's tools.\n\nOn the OS X cuda 7.5 default install, most of the libraries exist in `/Developer/NVIDIA/CUDA-7.5/lib`, and these are symlinked into `/usr/local/cuda/lib`. There is actually only one non-symlinked file, which is `/usr/local/cuda/lib/libcuda.dylib`. There are no version numbers on it (as @yaroslavvb pointed out).\n\nIn the `nvidia/cuda:7.5-cudnn4-devel` docker image (which TensorFlow's docker image is built from), the cuda libraries are in `/usr/local/nvidia/lib64`, and there are two files: `libcuda.so.1` and `libcuda.so.352.63`.\n\nSo it looks like any solution will not be confined to `dso_loader.cc`.\n\nI see three solutions:\n1. Decide the canonical name of libcuda is \"libcuda\". Revert f45874b. Add a line to the tensorflow dockerfile which creates a symlink from `.1.so` to `.so`.\n2. Decide the canonical name of libcuda is \"libcuda.1\". Make users symlink on install or convince nvidia to add another symlink to their installer. The former seems unwieldy, and the latter seems unlikely to happen.\n3. Add a TF_LIBCUDA_VERSION configuration variable. Users wouldn't actually have to configure this, if @3XX0 is correct in that it won't change. There's already system-dependent checks in [configure](https://github.com/tensorflow/tensorflow/blob/05df7e3cea808f26931370f099b0c4352375afce/configure#L181), so we could add another that simply generates a \".1\" for linux and a \"\" for OS X. Then we add a third function along with [GetCudaVersion](https://github.com/tensorflow/tensorflow/blob/d0a822fbcb04d95a643d8efe65699a8d1cdce98b/tensorflow/stream_executor/dso_loader.cc#L40) and GetCudnnVersion, which will end up getting text-substituted [just like the other two](https://github.com/tensorflow/tensorflow/blob/14ac2235699509f512b44b71160239c153ab413d/configure#L238). This doesn't actually make it any more safe (because we're just moving the hardcoded number into the configure script instead of the `dso_loader.cc`), but it does give tensorflow more control over where to look for things instead of trying to rely solely on nvidia's tools.\n\nI think 3 sounds more reasonable than the other two. I can probably work up the pull request if this seems like a viable plan for everyone.\n", "Why am I not surprised...\n\nWe decided to hardcode the `libcuda.so` symlink inside `nvidia-docker` because too many things are broken without it, so reverting is one solution. Fixing cuDNN is a must have though.\n\nOtherwise, as @rdadolf pointed out you could have two functions `GetCudaDriverVersion` and `GetCudaVersion` which would return the cuda version (e.g. \"7.5) and \"1\" (**\"\" for OSX**) respectively.\n", "@3XX0: Can you clarify what things TensorFlow should do and what needs to be fixed elsewhere?  I've seen your comments in other github threads but I've had trouble understanding what your full set of recommendations is (for cuda, cudnn, etc).\n\nAlso @zheng-xq for TF opinions\n", "I agree add \".1\" for OSX is a good idea. \n\nAdding the stream-executor owner, @henline here. \n", "So adding \".1\" isn't really a solution inside TensorFlow, it's a solution that nvidia or the user would have to do alongside TF. It basically means adding @yaroslavvb's symlink workaround as part of the official install instructions. I.E., whenever a user installs a version of cuda on a Mac (including upgrading versions), they'd have to also add the symlink manually. I'm not sure this should really be part of the TF install scripts, since it's modifying previously-installed libraries. Adding it into `configure` is probably not a good idea, and it would probably require sudo-invoking the configure script (since the cuda libraries could be in system locations).\n\nThe third solution I pitched is sort of a compromise. The dso loader now wouldn't care about the particulars of the system-dependent behavior of libcuda, and it's no less portable than before (because libcudnn and libcudart already require it). I wrote up a [patch](https://github.com/rdadolf/tensorflow/commit/9255c1e75f7c3f9185293c31d8ca655664055afb) which implements this. I can certainly submit it as a pull request if you like. (But until #2865 is resolved, we'll have to check the CI tests manually to see if it works.)\n\n@3XX0 I guess my other question is why the nvidia cuda installer _doesn't_ create a libcuda.1.dylib and symlink it. Every other file is done this way, and you believe that [libcuda.dylib is not the correct name](https://github.com/NVIDIA/nvidia-docker/blob/master/tools/src/nvidia/volumes.go#L263). Is this a bug in the installer?\n", "@rdadolf  Yes this is totally our fault. I will try to escalate this issue internally to our OSX packaging team but in the meantime your patch is the only (clean) way to solve it.\n\n@vrv I guess this answers your question for libcuda.\nFor libcudnn, I talked about it [here](https://github.com/tensorflow/tensorflow/issues/2525#issuecomment-223523008). In short, it would be nice to detect the libcudnn soname and set it by default in the configure script.\nOtherwise, if users try to run a precompiled version of Tensorflow where cudnn hasn't been configured with the script (e.g. the Tensorflow Docker runtime image), it will fail when it tries to look for `libcudnn.so`. Precompiled Tensorflow should be looking for `libcudnn.so.<soname it has been compiled with>`\n", "(I think I fixed all I could safely do in a bunch of other commits.  Just waiting for the cuda mac install to do the right thing).\n", "Hi guys. Fresh Xoogler here, I think we talked when my ldap was jucor@. Quick update on this: I've just installed TF r0.11 on OSX 10.11.3 in a virtualenv with Cuda 8.0.47 and cuDNN 8.0-5.1, and I had the same error as @yaroslavvb. Bad news: manually symlinking `libcuda.so` to `libcuda.1.so` was still necessary. Good news: it was also sufficient.\n"]}, {"number": 2909, "title": "Failure to build from source with GPU support on macOS", "body": "I am trying to follow the installation instructions for Mac with GPU support.  I have CUDA 7.5 and cuDNN installed (and Theano currently works with GPU support on this system).\n\nThe contents of my /usr/local/cuda/lib include the following:\n\n```\n(py35)Dans-iMac:tensorflow dan$ ls -l /usr/local/cuda/lib/libcud*\n-rwxr-xr-x  1 root  wheel  8280 Apr 13 00:02 /usr/local/cuda/lib/libcuda.dylib\nlrwxr-xr-x  1 root  wheel    45 Apr 13 00:03 /usr/local/cuda/lib/libcudadevrt.a -> /Developer/NVIDIA/CUDA-7.5/lib/libcudadevrt.a\nlrwxr-xr-x  1 root  wheel    50 Apr 13 00:03 /usr/local/cuda/lib/libcudart.7.5.dylib -> /Developer/NVIDIA/CUDA-7.5/lib/libcudart.7.5.dylib\nlrwxr-xr-x  1 root  wheel    46 Apr 13 00:03 /usr/local/cuda/lib/libcudart.dylib -> /Developer/NVIDIA/CUDA-7.5/lib/libcudart.dylib\nlrwxr-xr-x  1 root  wheel    49 Apr 13 00:03 /usr/local/cuda/lib/libcudart_static.a -> /Developer/NVIDIA/CUDA-7.5/lib/libcudart_static.a\n```\n\nMy configure step doesn't provide any errors, as shown in the following:\n\n```\n(py35)Dans-iMac:tensorflow dan$ ./configure\nPlease specify the location of python. [Default is /Users/dan/anaconda/envs/py35/bin/python]:\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N]\nNo Google Cloud Platform support will be enabled for TensorFlow\nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc nvcc should use as the host compiler. [Default is /usr/bin/gcc]:\nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 7.5\nPlease specify the location where CUDA 7.5 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]:\nPlease specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n[Default is: \"3.5,5.2\"]: 3.0\nSetting up Cuda include\nSetting up Cuda lib\nSetting up Cuda bin\nSetting up Cuda nvvm\nSetting up CUPTI include\nSetting up CUPTI lib64\nConfiguration finished\n```\n\nHowever the first bazel build step results in the following error:\n\n```\n(py35)Dans-iMac:tensorflow dan$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\nERROR: /Users/dan/tensorflow/tensorflow/contrib/session_bundle/BUILD:160:1: //tensorflow/contrib/session_bundle:manifest_proto_py: no such attribute 'imports' in 'py_library' rule.\nERROR: /Users/dan/tensorflow/tensorflow/tools/pip_package/BUILD:23:1: Target '//tensorflow/contrib/session_bundle:all_files' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:build_pip_package'.\nERROR: Loading failed; build aborted.\nINFO: Elapsed time: 0.092s\n```\n\nI tried looking at build files, but don't know bazel well enough to independently trace this further back.\n\nAll of this is using commit `19bda20635c91060996fe44fe201875b3b9ae7a2`\n", "comments": ["@kirilg, @nfiedel is there still something amiss with the session_bundle?\n", "@dansbecker can you tell us which bazel version you're using?\n", "My apologies.  As @martinwicke figured, I was using an old version of bazel (0.1.4). \n\nUpgrading to 0.2.3 resolved the error.\n\nIf there's any other info I can add here that you think might be useful for people who stumble on this thread in the future, let me know.  And thanks for the help.\n", "How to upgrade Bazel?"]}, {"number": 2908, "title": "cond", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n\n1.\n2.\n3.\n### What have you tried?\n\n1.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": []}, {"number": 2907, "title": "cond", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n\n1.\n2.\n3.\n### What have you tried?\n\n1.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": []}, {"number": 2906, "title": "cond", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n\n1.\n2.\n3.\n### What have you tried?\n\n1.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": []}, {"number": 2905, "title": "Why has the RNN API changed so drastically?", "body": "The new 0.9 version has 3 changes which has broken my existing functionality.\n1. Have to use tensorflow.python.ops.rnn_cell instead of tensorflow.models.rnn.rnn_cell\n2. Have to use cell(inputs, state..) instead of rnn.rnn(cell, inputs, state ...)\n3. Initial_state keyword arg has been removed\n4. Worst of all, i can't find any mention of variable sequence length in the new version of rnn_cell code\n\nCan someone at least advice on how to use variable sequence lengths in this version? For now I'm reverting back to 0.8 so my code works again\n", "comments": ["@ebrevdo is there a conversion howto? That would be helpful.\n", "The RNN api wasn't documented until 0.9, so it was liable to change.  Now that it is documented and \"officially released\", it has the same guarantees as the rest of core.\n\nThat said, none of the functionality has changed - it was just moved.\n\nYou can use tensorflow.nn.rnn_cell.*, tensorflow.nn.rnn(), tensorflow.nn.dynamic_rnn(), etc.  You do not have to call the cell directly.\n\ninitial_state is available to tensorflow.nn.rnn()\n\n@martinwicke for whatever reason, the rnn documentation does not show up [here](https://www.tensorflow.org/versions/r0.9/api_docs/python/index.html).  Do these api docs need to be updated?\n", "r0.9 docs reflect changes to the docs made before we branched. Are those\nchanges on the r0.9 branch?\nOn Thu, Jun 16, 2016 at 20:25 ebrevdo notifications@github.com wrote:\n\n> The RNN api wasn't documented until 0.9, so it was liable to change. Now\n> that it is documented and \"officially released\", it has the same guarantees\n> as the rest of core.\n> \n> That said, none of the functionality has changed - it was just moved.\n> \n> You can use tensorflow.nn.rnn_cell.*, tensorflow.nn.rnn(),\n> tensorflow.nn.dynamic_rnn(), etc. You do not have to call the cell directly.\n> \n> initial_state is available to tensorflow.nn.rnn()\n> \n> @martinwicke https://github.com/martinwicke for whatever reason, the\n> rnn documentation does not show up here\n> https://www.tensorflow.org/versions/r0.9/api_docs/python/index.html. Do\n> these api docs need to be updated?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2905#issuecomment-226671773,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAjO_eUHKwyTKg2qhJinJ2MSVlLbFb_Lks5qMhPBgaJpZM4I3kCq\n> .\n", "The documentation will be fixed by #2933 \n"]}, {"number": 2904, "title": "Fix markdown list in dtypes documentation", "body": "The markdown processor can't deal with empty lines in between list items, which leads to a broken list here:\nhttps://www.tensorflow.org/versions/master/api_docs/python/framework.html#DType\n\nJust removing the empty lines should be fine I think.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please.\n", "Thanks for the cleanup!\n"]}, {"number": 2903, "title": "Importing Gtk before tensorflow causes crash", "body": "### Environment info\n\nUbuntu 16.04\n`uname -a\nLinux p900 4.4.0-22-generic #40-Ubuntu SMP Thu May 12 22:03:46 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux`\n\nInstalled version of CUDA and cuDNN: \nCuda compilation tools release 7.5, V7.5.17 and CUDNN is v 4\n\n`ls -l /usr/local/cuda-7.5/lib/libcud*\n-rw-r--r-- 1 root root 189170 Jun  2 10:18 /usr/local/cuda-7.5/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Jun  2 10:18 /usr/local/cuda-7.5/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Jun  2 10:18 /usr/local/cuda-7.5/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Jun  2 10:18 /usr/local/cuda-7.5/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 Jun  2 10:18 /usr/local/cuda-7.5/lib/libcudart_static.a`\n\npip package installed:\n`tensorflow-0.9.0rc0-cp35-cp35m-linux_x86_64.whl`\n\nOutput of `python3 -c \"import tensorflow; print(tensorflow.__version__)\"`:\n...\n`0.9.0rc0`\n\nPython version:\nPython 3.5.1+\n\nGtk version:\n3.0\n### Steps to reproduce\n1. `import gi`\n2. `from gi.repository import Gtk`\n3. `import tensorflow`\n### What I have tried\n1. Removed any Ubuntu protobuf package (Ubuntu package is v 2.6.1):\n   `sudo apt purge protobuf*`\n2. Upgraded protobuf via pip3:\n   `pip3 install --upgrade protobuf`\n   ...\n   `Requirement already up-to-date: protobuf in /usr/local/lib/python3.5/dist-packages\n   Requirement already up-to-date: six>=1.9 in /usr/lib/python3/dist-packages (from protobuf)\n   Requirement already up-to-date: setuptools in /usr/local/lib/python3.5/dist-packages (from protobuf)`\n3. Checked protobuf version installed via pip3:\n   `ls /usr/local/lib/python3.5/dist-packages/proto*`\n   `/usr/local/lib/python3.5/dist-packages/protobuf-3.0.0b2-py2.7-nspkg.pth`\n   \n   `/usr/local/lib/python3.5/dist-packages/protobuf-3.0.0b2.dist-info:`\n   `DESCRIPTION.rst  INSTALLER  METADATA  metadata.json  namespace_packages.txt  RECORD  top_level.txt  WHEEL`\n### Output after importing tensorflow\n\n`[libprotobuf FATAL google/protobuf/stubs/common.cc:61] This program requires version 3.0.0 of the Protocol Buffer runtime library, but the installed version is 2.6.1.  Please update your library.  If you compiled the program yourself, make sure that your headers are from the same version of Protocol Buffers as your link-time library.  (Version verification failed in \"external/protobuf/src/google/protobuf/any.pb.cc\".)\nterminate called after throwing an instance of 'google::protobuf::FatalException'\n  what():  This program requires version 3.0.0 of the Protocol Buffer runtime library, but the installed version is 2.6.1.  Please update your library.  If you compiled the program yourself, make sure that your headers are from the same version of Protocol Buffers as your link-time library.  (Version verification failed in \"external/protobuf/src/google/protobuf/any.pb.cc\".)\nAborted (core dumped)`\n### Additional comments\n\nAs before ( #1373) this ONLY occurs when importing Gtk BEFORE tensorflow. The opposite order causes no crash, nor have I experienced any other crashes associated with tensorflow.\n\nCuriously, when using version 0.8 of tensorflow, a very similar crash was caused by the opposite order of import (ie tensorflow before Gtk), but in that case the output was that: `This program requires version 2.6.1 of the Protocol Buffer runtime library, but the installed version is 3.0.0.` That is, the problem seemed to be caused by the reverse relationship between the protobuf versions. However, when upgrading tensorflow, I did not change the protobuf version. It was kept the same throughout.\n", "comments": ["The error message seems to indicate that you need the newer version of the protobuf runtime library, and not the python interface to protobuf that you install via `pip install`. Can you try `aptitude versions libprotobuf` to see what versions of the protobuf runtime library are available for Ubuntu 16.04, and if 3.0.0 is indeed available, install that?\n", "@keveman: Only libprotobuf 2.6.1-1.3.\nBut, I will see if I can find some other way to install 3.0.\n", "Sorry for the delay.\nInspite of doing my best to remove protobuf with apt-get I did still have it installed, and it was version 2.6:\n`$ locate libprotobuf`\n...\n`/usr/lib/x86_64-linux-gnu/libprotobuf-lite.a`\n`/usr/lib/x86_64-linux-gnu/libprotobuf-lite.so`\n`/usr/lib/x86_64-linux-gnu/libprotobuf-lite.so.9`\n`/usr/lib/x86_64-linux-gnu/libprotobuf-lite.so.9.0.1`\n`/usr/lib/x86_64-linux-gnu/libprotobuf.a`\n`/usr/lib/x86_64-linux-gnu/libprotobuf.so`\n`/usr/lib/x86_64-linux-gnu/libprotobuf.so.9`\n`/usr/lib/x86_64-linux-gnu/libprotobuf.so.9.0.1`\n...\n\nThus, I removed those files, compiled protobuf-3.0.0-beta-3.2 and moved the new files (as above but 10 instead of 9) to `/usr/lib/x86_64-linux-gnu/` so that I now have:\n`locate libprotobuf`\n...\n`/usr/lib/x86_64-linux-gnu/libprotobuf-lite.a`\n`/usr/lib/x86_64-linux-gnu/libprotobuf-lite.so`\n`/usr/lib/x86_64-linux-gnu/libprotobuf-lite.so.10`\n`/usr/lib/x86_64-linux-gnu/libprotobuf-lite.so.10.0.0`\n`/usr/lib/x86_64-linux-gnu/libprotobuf.a`\n`/usr/lib/x86_64-linux-gnu/libprotobuf.so`\n`/usr/lib/x86_64-linux-gnu/libprotobuf.so.10`\n`/usr/lib/x86_64-linux-gnu/libprotobuf.so.10.0.0`\n...\n\nHowever, Gtk still needs `libprotobuf-lite.so.9`, so:\n\n`In [1]: import gi`\n `In [2]: from gi.repository import Gtk`\n`** (process:16293): WARNING **: Failed to load shared library 'libgdk-3.so.0' referenced by the typelib: libprotobuf-lite.so.9: cannot open shared object file: No such file or directory`\n...\n\nI returned the libprotobuf 9 files to `/usr/lib/x86_64-linux-gnu/` and the Gtk import works again, but importing tensorflow afterwards results in a crash. I.e. back to square 0.\n\nHowever, this is not a particular big problem for me since it works if tensorflow is imported first, and I have removed the dependence on Gtk.\n", "Exactly the same issue here...\nAny solutions?\nHow to thoroughly remove protobuf 2.6.1, and replace it by protobuf 3.0.0b3\n", "@jiapei100 Importing `tensorflow` before anything else sometimes resolves this issue\n", "Sometimes it resolves it, sometimes not :)\n", "All I can think of is after apt-get purge command you may need to manually remove libprotobuf files under  `/usr/lib/x86_64-linux-gnu/`, if they are still there. Then install the correct protobuf version using `pip` as described in tensorflow.org.", "Closing this, since either:\r\n\r\n* it's an install issue with inconsistent versions of protobuf\r\n* it's a problem where you are trying to load multiple versions of protobuf in the same process space. You can't do that. You could attempt to use the `multiprocess` library if you are intent on this.\r\n\r\nFeel free to reopen if that's not the case or if you want more information."]}, {"number": 2902, "title": "build_pip_package fails with locally installed python3: script unsets LD_LIBRARY_PATH", "body": "build_pip_package fails here with\n\n`ERROR: /path/to/src/tensorflow/tensorflow/contrib/session_bundle/example/BUILD:38:1: Executing genrule //tensorflow/contrib/session_bundle/example:half_plus_two failed: bash failed: error executing command`\n\ncaused by\n\n`error while loading shared libraries: libpython3.5m.so.1.0: cannot open shared object file: No such file or directory`\n\nexecuting\n\n`(cd path/to/execroot/tensorflow && \\\n  exec env - \\\n    PATH=somepath \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; rm -rf /tmp/half_plus_two; path_to/python3 bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two`\n\nit looks like\n\n`exec env -`\n\nunsets LD_LIBRARY_PATH\n\nand therefore python3 cannot find its libraries anymore\n\nInstallation of tensorflow 0.8 works\n", "comments": ["What version is this about? r0.9? master? If master, at which commit?\n", "@nfiedel, we disabled this is in 0.9, and I thought this was fixed in master, but seems that maybe it isn't.\n", "yes 0.9 master, sorry forgot to mention it.\n\ncommit is\n\ncommit bdc43730c9e6037c608adb91fe55d6c1a82879a8\nDate:   Tue Jun 14 06:19:48 2016 +0100\n", "still happens with a fresh cloned branch i.e.\n\ncommit b327647368117db19b42c6fd75d0aa67a66dbf26\nMerge: b8652a2 7e32f92\nDate:   Thu Jun 16 20:53:14 2016 -0700\n", "Which version of bazel are you using?\n", "we first had the issue with 0.2.2b but it also happens with the newest version 0.3.0\n", "Also happens with master@70de76e696c21da617fd2e6435cf7fedab220db8 and bazel 0.3.0 for me \n", "I think this issue is now resolved. Our continuous tests on py3 seem to be all working.\r\nPlease reopen if I am mistaken."]}, {"number": 2901, "title": "problem in checkpoint", "body": "I used tf 0.9. \n\nWhen I restore the checkpoint i get the error:\n\nTraceback (most recent call last):\n\n  File \"<ipython-input-42-7082fb551371>\", line 1, in <module>\n    runfile('/media/m/E/Deep/proj/1/t.py', wdir='/media/m/E/Deep/proj/1')\n\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/spyderlib/widgets/externalshell/sitecustomize.py\", line 714, in runfile\n    execfile(filename, namespace)\n\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/spyderlib/widgets/externalshell/sitecustomize.py\", line 89, in execfile\n    exec(compile(f.read(), filename, 'exec'), namespace)\n\n  File \"/media/m/E/Deep/proj/1/t.py\", line 70, in <module>\n    saver.restore(sess, ckpt.model_checkpoint_path) # restore all variables\n\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1105, in restore\n    {self.saver_def.filename_tensor_name: save_path})\n\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 333, in run\n    The optional `options` argument expects a [`RunOptions`] proto. The options\n\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 573, in _run\n    elif isinstance(fetches, dict):\n\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 648, in _do_run\n    for fetch, result in zip(unique_fetches, results):\n\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 668, in _do_call\n    \"\"\"Runs a step based on the given fetches and feeds.\n\nNotFoundError: Tensor name \"global_step_7\" not found in checkpoint files ./ckpt_dir/model.ckpt-0\n     [[Node: save_18/restore_slice_438 = RestoreSlice[dt=DT_INT32, preferred_shard=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_18/Const_0, save_18/restore_slice_438/tensor_name, save_18/restore_slice_438/shape_and_slice)]]\nCaused by op 'save_18/restore_slice_438', defined at:\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/spyderlib/widgets/externalshell/start_ipython_kernel.py\", line 205, in <module>\n    **ipythonkernel**.start()\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(_args, *_kwargs)\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(_args, *_kwargs)\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(_args, *_kwargs)\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2831, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-42-7082fb551371>\", line 1, in <module>\n    runfile('/media/m/E/Deep/proj/1/t.py', wdir='/media/m/E/Deep/proj/1')\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/spyderlib/widgets/externalshell/sitecustomize.py\", line 714, in runfile\n    execfile(filename, namespace)\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/spyderlib/widgets/externalshell/sitecustomize.py\", line 89, in execfile\n    exec(compile(f.read(), filename, 'exec'), namespace)\n  File \"/media/m/E/Deep/proj/1/t.py\", line 57, in <module>\n    saver = tf.train.Saver()\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 845, in __init__\n    restore_sequentially=restore_sequentially)\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 515, in build\n    filename_tensor, vars_to_save, restore_sequentially, reshape)\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 271, in _AddRestoreOps\n    values = self.restore_op(filename_tensor, vs, preferred_shard)\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 186, in restore_op\n    preferred_shard=preferred_shard)\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/io_ops.py\", line 201, in _restore_slice\n    file_pattern, tensor_name, shape_and_slice, base_type,\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 325, in _restore_slice\n    def _restore_slice(file_pattern, tensor_name, shape_and_slice, dt,\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/op_def_library.py\", line 704, in apply_op\n    op_def=op_def)\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2249, in create_op\n    # specified for this op_type.\n  File \"/home/m/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1224, in __init__\n    raise TypeError(\"Control input must be an Operation, \"\n\nhow can i solve this problem?\n", "comments": ["Are you sure that the ckpt_dir/model.ckpt-0 file exists in your checkpoint directory?  Normally a later step would be loaded, after some training had been done. But basically tf.restore is trying to restor me a specific file. Make sure the file you specify is there. \n", "Yes , this file exists.\n\nThis is my code:\n\n`import tensorflow as tf\nimport input_data\nimport os \n\ncheckpoint_dir='./ckpt_dir/'                \n\nmnist = input_data.read_data_sets(\"MNIST_data\", one_hot = True)\n\nx = tf.placeholder(tf.float32, shape = [None , 784])\ny_ = tf.placeholder(tf.float32, [None, 10])\n\nsess = tf.InteractiveSession()\n\ndef load_model(sess, saver, checkpoint_dir ):  \n        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n        if ckpt and ckpt.model_checkpoint_path:\n            print(ckpt.model_checkpoint_path)  \n            saver.restore(sess, ckpt.model_checkpoint_path)  \n        else:\n            if not os.path.exists(checkpoint_dir):\n                os.makedirs(checkpoint_dir)\n            sess.run(init)\n        return\n\ndef weight_variable(shape):\n    initial = tf.truncated_normal(shape, stddev = 0.1)\n    return tf.Variable(initial)\n\ndef bias_variable(shape):\n    initial = tf.constant(0.1, shape= shape)\n    return tf.Variable(initial)\n\ndef conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides = [1, 1, 1, 1], padding = \"SAME\")\n\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1],\n                          padding = \"SAME\")\n\nW_conv1 = weight_variable([5, 5, 1, 32])\nb_conv1 = bias_variable([32])\n\nx_image = tf.reshape(x, [-1, 28, 28, 1])\n\n#\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1))\nh_pool1 = max_pool_2x2(h_conv1)\n\n#\nW_conv2 = weight_variable([5, 5, 32, 64])\nb_conv2 = bias_variable([64])\n\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2))\nh_pool2 = max_pool_2x2(h_conv2)\n\nW_fc1 = weight_variable([7_7_64, 1024])\nb_fc1 = bias_variable([1024])\n\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7_7_64])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\n#\nkeep_prob = tf.placeholder(tf.float32)\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\n#\nW_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\n\ny_conv = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2) +b_fc2)\n\n#\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ \\* tf.log(y_conv), reduction_indices = [1]))\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n\ncorrect_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\ninit = tf.initialize_all_variables()\n\nsaver = tf.train.Saver() \n\nload_model(sess, saver, checkpoint_dir)\n\nfor i in range(1):\n    batch = mnist.train.next_batch(50)\n    if i%10 == 0:\n        train_accuracy = accuracy.eval(feed_dict = {x : batch[0] , y_ : batch[1], keep_prob : 1.0})\n        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n\n```\ntrain_step.run(feed_dict = {x : batch[0], y_ : batch[1], keep_prob : 0.5})\n```\n\nprint(\"test accuracy %g\"%accuracy.eval(feed_dict={\n    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))        \n\ntf.scalar_summary(\"accuracy\", accuracy)\n\nsaver.save(sess,checkpoint_dir+'model.ckpt')`\n", "1. You are repeatedly creating the Saver, that's why you have 8 global_steps. You can fix it by either explicitly creating a new graph tf.Graph(), or reset the default graph by calling tf.reset_default_graph().\n\nIf you print out your graph now by calling tf.train.write_graph, you will see that you have 8 copies of what you are building.\n1. The saver should only be called once after you have created the graph, and must be called before init = tf.initializes_all_variable() because without arguments the Saver adds operations to the graph.\n2. tf.initialize_all_variables() just creates an op. It doesn't initialize anything. You need to explicitly run it by calling\n\nsess.run(init)\nor\ninit.run()\n\nHope that helps.\n\nSherry\n", "Please reopen if this doesn't fix the problem.\n"]}, {"number": 2900, "title": "When will the installation support cuDNN 5 ?", "body": "", "comments": ["Closing since this is effectively a duplicate of #2937.\n"]}, {"number": 2899, "title": "[make] gen_file_lists.sh output to wrong directory", "body": "The gen_file_lists.sh script in contrib/makefile outputs the generated lists to the incorrect directory:\n\n```\nbazel query 'kind(\"source file\", deps(//tensorflow/core:android_tensorflow_lib))' | \\\ngrep \"//tensorflow/.*\\.cc$\" | \\\ngrep -v \"gen_proto_text\" | \\\ngrep -E -v \"jpeg\" | \\\ngrep -E -v \"png\" | \\\nsed -E 's#^//##g' | \\\nsed -E 's#:#/#g' \\\n> make/tf_cc_files.txt\n```\n\nThe \"make/\" should be removed as \"make\" expects the lists to be in the same directory as the Makefile.\n", "comments": ["Thanks for the report! This should be fixed in #2936, closing now.\n"]}, {"number": 2898, "title": "A few code hygiene fixes ", "body": "- Remove some unused variables\n- Mark functions as overriding virtual functions where necessary\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please.\n", "Thanks for this cleanup!\n"]}, {"number": 2897, "title": "It is too painful to install a gpu-surpport edition on Ubuntu 16.04", "body": "", "comments": ["You'll need CUDA 8.0 and latest NVidia drivers. Here are instructions I followed to install them\nhttps://gist.github.com/yaroslavvb/07b2d9a828f82792ebef2a080e814f0f\n\nA small side-effect is that X server will no longer work after this, but you'll be able to run TensorFlow through SSH\n", "I will consider this resolved. :)\n", "I walked through how to do it with Ubuntu 16.04 and using Cuda 7-5 using apt-get.\n\nhttps://www.youtube.com/watch?v=HM57GyqwGxA\n"]}, {"number": 2896, "title": "compile_ios_tensorflow.sh failing", "body": "Hi there\n\nI'm following the instructions on your website to build a version of tensor flow for iOS:\n\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile\n\nUnfortunately when I get to final step:\n\ntensorflow/contrib/makefile/compile_ios_tensorflow.sh\n\nIt errors with:\n\n /Projects/ML/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core-armv7.a -arch armv7 -miphoneos-version-min=9.2 -Xlinker -S -Xlinker -x -Xlinker -dead_strip -all_load -L/Projects/ML/tensorflow/tensorflow/contrib/makefile/gen/protobuf_ios/lib -lz -lstdc++ -lprotobuf -lm\nUndefined symbols for architecture armv7:\n  \"tensorflow::shape_inference::InferenceContext::Merge(tensorflow::shape_inference::Shape const_, tensorflow::shape_inference::Shape const_, tensorflow::shape_inference::Shape const*_)\", referenced from:\n      tensorflow::$_0::operator()(tensorflow::shape_inference::InferenceContext_) const in libtensorflow-core-armv7.a(math_ops.o)\nld: symbol(s) not found for architecture armv7\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\nmake: **\\* [/Projects/ML/tensorflow/tensorflow/contrib/makefile/gen/bin/benchmark] Error 1\n- '[' 2 -ne 0 ']'\n- echo 'armv7 compilation failed.'\n  armv7 compilation failed.\n- exit 1\n", "comments": ["That problem is caused by the out-of-date tf_cc_files.txt file that is in the repo. That version doesn't include newer files such as shape_inference.cc, the source of the missing symbols in your error.\n\nIf you run ./gen_file_lists.sh from tensorflow/contrib/makefile it will generate a new tf_cc_files.txt for you. Then run the compile script again and see if it works.\n\nNote that you have to remove \"make/\" from lines 26, 32, 38, 46, 52, and 58 in gen_file_list.sh in order to make the script put the files into the same directory as the Makefile. I forgot to raise this an issue when I found it.\n", "Piggybacking on this issue (let me know if it's better suited as a separate one). I changed gen_file_list.sh, ran it and then attempted building again, now I'm instead getting the following:\n\n```\nUndefined symbols for architecture x86_64:\n  \"_deflate\", referenced from:\n      tensorflow::io::ZlibOutputBuffer::Deflate(int) in zlib_outputbuffer.o\n  \"_deflateEnd\", referenced from:\n      tensorflow::io::ZlibOutputBuffer::Close() in zlib_outputbuffer.o\n  \"_deflateInit2_\", referenced from:\n      tensorflow::io::ZlibOutputBuffer::ZlibOutputBuffer(tensorflow::WritableFile*, int, int, tensorflow::io::ZlibCompressionOptions const&) in zlib_outputbuffer.o\n  \"_inflate\", referenced from:\n      tensorflow::io::ZlibInputBuffer::Inflate() in zlib_inputbuffer.o\n  \"_inflateEnd\", referenced from:\n      tensorflow::io::ZlibInputBuffer::~ZlibInputBuffer() in zlib_inputbuffer.o\n  \"_inflateInit2_\", referenced from:\n      tensorflow::io::ZlibInputBuffer::ZlibInputBuffer(tensorflow::RandomAccessFile*, unsigned long, unsigned long, tensorflow::io::ZlibCompressionOptions const&) in zlib_inputbuffer.o\nld: symbol(s) not found for architecture x86_64\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\nmake: *** [/Users/albhaf/src/github.com/tensorflow/tensorflow/tensorflow/contrib/makefile/gen/host_bin/proto_text] Error 1\n+ '[' 2 -ne 0 ']'\n+ echo 'armv7 compilation failed.'\narmv7 compilation failed.\n+ exit 1\n```\n", "@albhaf That looks like a different problem.\n\nI had a quick look and I think that it's being caused by support being added in the main tensorflow build for reading compressed files. The download dependencies script in the makefile directory is now out of date, along with the Makefile.\n\nA quick work-around should be to remove the two zlib references from your generated tf_cc_files.txt. I haven't checked that yet, but the iOS Camera Demo doesn't use zlib so it might be ok.\n", "Apologies about the breakage! I'm working to make the update a bit less of a manual process.\n\nI'll investigate how we can avoid the new zlib dependency (since we don't want additional libraries to link in), and hopefully have an answer soon.\n", "Thanks everyone! I've tested this again and it should be fixed in #2936, so I'll close this. Comment or reopen if you're still hitting problems.\n", "Thanks! Managed to successfully build with 582e6c8.\n\nRecent changes in master (I suspect 7764e8f6 from a brief look), appears to have caused it to regress again if it's of any interest:\n\n```\nUndefined symbols for architecture x86_64:\n  \"tensorflow::PosixFileSystem::NewWritableFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tensorflow::WritableFile**)\", referenced from:\n      vtable for tensorflow::LocalPosixFileSystem in env.o\n  \"tensorflow::PosixFileSystem::NewAppendableFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tensorflow::WritableFile**)\", referenced from:\n      vtable for tensorflow::LocalPosixFileSystem in env.o\n  \"tensorflow::PosixFileSystem::NewRandomAccessFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tensorflow::RandomAccessFile**)\", referenced from:\n      vtable for tensorflow::LocalPosixFileSystem in env.o\n  \"tensorflow::PosixFileSystem::NewReadOnlyMemoryRegionFromFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tensorflow::ReadOnlyMemoryRegion**)\", referenced from:\n      vtable for tensorflow::LocalPosixFileSystem in env.o\n```\n", "Shoot, probably my fault, I'll send a fix.\n", "I'm actually looking at it now too. It builds fine using the makefile on Linux, so it's not just a new file added. I'm running build_all_ios.sh to repro the problem, and I'll update with more details when I have them.\n", "Actually, I can't find any reference to those symbols -- my change removed the old APIs and converted to the new ones, so I'm not sure why we're seeing that.\n", "Checking if I can reproduce it and making sure it's on a clean state, in case it was in an inconsistent one from jumping between refs.\n", "It seemed like it was, I must have missed to clean in between checkouts.\nI can build it from master now, sorry for the noise.\n", "Great to hear, and no worries! Thanks for the update and the report.\n", "Am I doing it right? I want to run ios examples and trying to compile lib in this way:\r\n```\r\nJohns-Mac-mini:makefile a.yakovlev$ sh compile_ios_tensorflow.sh\r\nmake: tensorflow/contrib/makefile/Makefile: No such file or directory\r\nmake: *** No rule to make target `tensorflow/contrib/makefile/Makefile'.  Stop.\r\narmv7 compilation failed.\r\nJohns-Mac-mini:makefile a.yakovlev$ \r\n```"]}]