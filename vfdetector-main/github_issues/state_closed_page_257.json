[{"number": 46738, "title": "ImportError: DLL load failed while importing _pywrap_tensorflow_internal:", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version: 2.4.1\r\n- Python version:3.8\r\n- Installed using virtualenv? pip? conda?:pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nI am not able to install tensorflow using pip on anaconda (& sypder)\r\nIt has worked on my other Computer but not on the Windows10, i have uninstalled and reinstalled everything several times, but it just wont work. I can install other libraries, but not tensorflow - i do not understand why.\r\n\r\nI know this post is a duplicate, but the answers in previous posts didnt help me since i am new to coding and everything....Sorry for bothering though\r\nAnd no i do not want to use google collab, but install & use tensorflow on my computer\r\n\r\nI would be very happy if someone could help me!!\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\go-pa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: Das angegebene Modul wurde nicht gefunden.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@paulemoe,\r\nYou might be facing this issue because of the following reasons\r\n\r\n- You are running 32-bit Python or 32-bit OS\r\n- You have not installed the [Microsoft Visual C++ Redistributable](https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads) package\r\n- Your CPU does not support AVX instructions. \r\n\r\nPlease take a look at the [system requirements](https://www.tensorflow.org/install/pip#system-requirements) and check if you have the correct dependencies installed.\r\n\r\nAlso, check this similar duplicate issue: [#46124](https://github.com/tensorflow/tensorflow/issues/46124#issuecomment-753665428)\r\n\r\nThanks!", "Thank you for your answer!\r\n\r\n- I am runnning 64-bit Python.\r\n- I installed Microsoft Visual C++ Redistributable from the link and reinstalled python & tensorflow, didnt solve the issue\r\n- I have an intel i5, very standart CPU, nothing fancy\r\n\r\ntensorflow still refuses to run, any more advice?", "@paulemoe,\r\nThank you for the update. \r\n\r\n> * I have an intel i5, very standart CPU, nothing fancy\r\n\r\nCould you please let us know the exact make and model of the CPU on your machine?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46738\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46738\">No</a>\n", "This simply means that your anaconda version and tensorflow version are not compatible. So, in this case I uninstalled anaconda and then installed the latest version.\r\nIt worked.  ", "### @paulemoe Try Creating a virtual environment for you project\r\n> `pip freeze > requirements.txt` / then / `pip install virtualenv`\r\n> activate env [`\"env/Scripts/activate\"` / or / `./env/Scripts/activate`]\r\n> `pip install -r requirements.txt`\r\n\r\n- Make sure _pip install_ installed **Tensor flow** [`pip install  tensorflow`]"]}, {"number": 46737, "title": "bqml kmeans -> tflite: Call_once op doesn't support multiple subgraphs with inputs", "body": "Filing on request of tflite group\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 & macOS 11.0.1\r\n- TensorFlow installed from (source or binary): pip tf-nightly \r\n- TensorFlow version (use command below): 2.5.0-dev20210123 (git: v1.12.1-49539-g18d8bcbe72b)\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nExport saved model from bqml to GCS. Load saved model and test:\r\n![Screen Shot 2021-01-26 at 12 21 34 PM](https://user-images.githubusercontent.com/5230786/106028324-ccf80080-6099-11eb-9e45-a24188eee451.png)\r\n\r\ncreate tflite model using from_saved_model:\r\n![Screen Shot 2021-01-26 at 12 26 28 PM](https://user-images.githubusercontent.com/5230786/106028363-da14ef80-6099-11eb-8aa4-d7a2c13c848b.png)\r\n\r\nload tflite model with interpreter to test:\r\n![Screen Shot 2021-01-26 at 12 29 29 PM](https://user-images.githubusercontent.com/5230786/106028401-e39e5780-6099-11eb-93a4-3c229194501a.png)\r\n\r\n**Describe the expected behavior**\r\nLoad tflite model and make predictions using interpreter to validate successful conversion\r\n\r\n**Standalone code to reproduce the issue**\r\ntoy saved model from bqml k-means, notebook to repro, and tflite model can be found here: https://drive.google.com/drive/folders/1HfAU7ZK6CHvRFl_hSfw42RzlAfvXS-PJ?usp=sharing\r\n", "comments": ["Hi @loweew \r\n\r\nI can reproduce your issue on my side. Will take a look. Thanks", "I have a working fix for this problem. However, it will take a while to submit the fix. I can share the converted model to you for verification.\r\n\r\nhttps://drive.google.com/file/d/103ph9bHBYhR_NpLfHY4E-_k5HokmTAvm/view?usp=sharing", "Great, I was able to load and allocate tensors successfully. But invoke failed:\r\n\r\n![Screen Shot 2021-01-28 at 10 45 35 AM](https://user-images.githubusercontent.com/5230786/106164124-a8625e00-6157-11eb-9992-629042d62507.png)\r\n\r\nI suspect this is a different issue, but I'm unable to verify that this is working for inference at this point. Are hash tables not supported using the tf builtin? Our end goal is the deploy a k-means model trained in bqml on iOS. So, if this is just a python issue, it's not the end of the world. But it would be nice to confirm this is working correctly in python before trying to implement it in iOS.  \r\n\r\n", "Actually, we provide hash table custom ops separately. We are working on adding them in the builtin op set. I verified the model working well with hash table custom ops.\r\n\r\nPlease take a look: https://www.tensorflow.org/lite/guide/ops_custom\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/kernels/hashtable", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46737\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46737\">No</a>\n"]}, {"number": 46736, "title": "fix conjugate-transpose for matrices of certain sizes (issue #19200)", "body": "When using `tf.linalg.matrix_transpose` or `tf.linalg.adjoint` to conjugate-transpose a complex tensor, the functions might fail to conjugate the tensor under certain circumstances.\r\n\r\nGiven a complex matrix with m rows and n columns, if the following conditions are met:\r\n1. The operation runs on GPU.\r\n2. (m > 96 and n < 16) or (n > 96 and n < 16).\r\nThen the matrix will not be conjugated (although it will be transposed).\r\n\r\nCode snippet which reproduces the bug (before patch):\r\n```\r\nimport tensorflow as tf\r\n\r\nwith tf.device(\"GPU\"):\r\n    A = tf.complex(0., tf.ones(shape=(97,2)))\r\n    Ah = tf.linalg.matrix_transpose(A, conjugate=True)\r\n\r\nA[0][0] == Ah[0][0] ### True - but should be False\r\n```", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46736) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46736) for more info**.\n\n<!-- need_sender_cla -->", "@OmriSteiner  Can you please sign CLA. Thanks!", "@gbaned My employer signed the CLA and added me as an authorized contributor, but apparently it takes several days for approval?", "@googlebot I signed it!", "Yeah, I could write a unit-test for this.", "@allenlavoie I've expanded the test to also test complex128."]}, {"number": 46735, "title": "RaggedTensor processing float type  causes memory leak", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n\r\n\r\n**System information**\r\n- TF 1.15.3\r\n- Linux\r\n- python 3.7\r\n\r\n**Describe the current behavior**\r\ntext_input_flatten is a 3D tensor, [batch_size, seq_length, embedding_size], dtype=tf.float32\r\ntext_mask is a 2D tensor, [batch_size, seq_length], dtype=tf.bool\r\n\r\ntext_input_ragged = tf.RaggedTensor.from_tensor(text_input_flatten)\r\ntext_input_ragged = tf.ragged.boolean_mask(text_input_ragged, text_mask, name=\"ragged_text_input\")\r\nnew_text_input = text_input_ragged.to_tensor(default_value=0)\r\nnew_length = tf.shape(new_text_input)[1]\r\nnew_text_input = tf.pad(new_text_input, [[0, 0], [0, seq_length * emb_size - new_length], [0, 0]], constant_values=0)\r\n\r\nthe memory usage is increasing rapidly, finally, the program will crash.\r\nWhen I changed the text_input_flatten dtype to tf.int32, it works normally.\r\n\r\n![memory](https://user-images.githubusercontent.com/8108725/106013911-e7f95d80-60f7-11eb-97e5-b4fe31c38bfc.png)\r\n**Describe the expected behavior**", "comments": ["@skywalker1990 \r\nTensorFlow 1.x is not actively supported. Could you please update TensorFlow to the latest stable version v2.4 and check if you are facing the same issue. \r\nIf the issue persists in 2.4 version,can you please share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "Sorry, the version of our production environment is 1.15.x. We have no plan to upgrade the version to 2.x yet.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 46734, "title": "Saved_model should be able to save any CompositeTensor signature.", "body": "Fix for #45797.\r\n\r\nThe allows the saved_model library to save a signature for a function that returns a RaggedTensor (or any CompositeTensor).\r\n\r\nMy understanding is that function traces use signatures where any CompositeTensor argument is expanded to its dense tensor components via ```nest.flatten(arg, expand_composites=True)```; the saved_model save method has the same behaviour. Relaxing the check in signature_serialization should be correct as long as CompositeTensors support ```nest.flatten```.", "comments": ["A gentle bump after six weeks - we would be grateful for a review.\r\n\r\nThanks & cheers.", "It seems the changes landed independently to this commit, 7 days ago, in b68d87647ca1. Therefore, this PR can be closed, I believe.\r\n\r\nCheers.", "Closing due to fix committed upstream."]}, {"number": 46732, "title": "How to create representative_dataset with multiple inputs on TensorFlow Lite Converter?", "body": "I cannot find any examples online, if I was to simulate data would it look like this?\r\n\r\n```\r\ndef representative_dataset():\r\n    for _ in range(100):\r\n      data1, data2 = np.random.rand(1, 256), np.random.rand(1, 50)\r\n      yield [data1.astype(np.float32), data2.astype(np.float32)]\r\n```", "comments": ["The inputs seem to be fed in arbitrary order. Not sure why. tf lite doesn't seem to pay attention to the ordering of the input signatures.", "> The inputs seem to be fed in arbitrary order. Not sure why. tf lite doesn't seem to pay attention to the ordering of the input signatures.\r\n\r\nSorry, I made a wrong observation. It appears that the ordering goes in consistent with the order of the input signatures.", "Yes, got it working. If you need representative dataset use the method like above in my original post ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46732\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46732\">No</a>\n"]}, {"number": 46731, "title": "cleanup `c/experimental/gradients/` part 4", "body": "@saxenasaurabh I have a lot of things that I would like to discuss with you, please take a look at it. Thank you ! \ud83d\ude04 \r\n\r\n---\r\n\r\n\r\n8f2852f: IMO, `mnist_grad_test` doesn't do anything really interesting. We have \r\n- 1 grad test for `MatMul` ( added in `math_grad_test`).\r\n- 2 forward tests which test `ops` and aren't related to gradients.\r\n- An another test for `ops` ( `ScalarMul` ).\r\n- 1 `MNIST` training test but it only checks for the `status`.\r\n\r\nSo I removed it. I also removed `mnist_gradients_testutil` and `gradients_util` which are no longer used.\r\n\r\n---\r\n\r\n20db3b8: I am trying to add templates for `TestTensorHandleWithDims` and `TestScalarTensorHandle`. I think it won't break anything because these headers are used for testing only.\r\n\r\n---\r\n\r\nI am working on a bare minimum infrastructure for getting model trained fully in C++ ( optimizer, loss, metrics, ... ). Currently, I follow the interface of `model` in `keras`. You could see it here: https://github.com/vnvo2409/tensorflow/commit/9e42f7ea613c73b303829bfc833bc81d474d9218. With this infrastructure, `MNIST` achieves 96% accuracy ( with a real dataset downloaded from http://yann.lecun.com/exdb/mnist/ ) in `mnist_grad_test`. Hopefully we could use it for `resnet` soon. Below are the problems I have when building it.\r\n\r\n---\r\n\r\nHow to pass `DataType` to `ops` ( e.g https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/cast#summary ). We might need it in the future. For an example, `MNIST` metric is essentially `tf.mean(tf.cast(tf.equal()))`\r\n\r\n---\r\n\r\nShould we rewrite the `ops` to something like \r\n```cpp\r\nStatus Equal(AbstractContext* ctx, AbstractTensorHandle* const x,\r\n             AbstractTensorHandle* const y,\r\n             absl::Span<AbstractTensorHandle*> outputs, const char* name,\r\n             bool incompatible_shape_error = false);\r\n```\r\nand getting rid of this pattern\r\n```cpp\r\nStatus Log1p(AbstractContext* ctx,\r\n             absl::Span<AbstractTensorHandle* const> inputs,\r\n             absl::Span<AbstractTensorHandle*> outputs, const char* name);\r\n```\r\n\r\nIMO, the former pattern is clearer and could help us avoid some confusion. An example, consider [`ops::Concat`](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/concat#summary) ( because it both has `InputList` and `Input` ). \r\n\r\n---\r\n\r\nWhen I used `AddInputList` for adding input to `ops::Concat`, it threw https://github.com/tensorflow/tensorflow/blob/c5012222b1d9accc07ea8fa4140c66039883ba2b/tensorflow/core/common_runtime/eager/eager_operation.cc#L403\r\n\r\n`AddInputList` for `AddN` is working fine though.\r\n\r\n---\r\n\r\n[`ops::ApplyGradientDescent`](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/apply-gradient-descent#summary) requires a `TF_FLOAT_REF` instead of `TF_FLOAT`. How to use it ?\r\n\r\n---\r\n\r\nMemory leaks: These code will leak because we lost access to the output of `MatMul` and `Relu` will replace it with another tensor without calling `Unref`. Is there an intuitive solution for this problem ? Currently, we could wrap it inside a `AbstractTensorHandlePtr` or save the pointer manually before passing it around but both ways require manual effort.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/c5012222b1d9accc07ea8fa4140c66039883ba2b/tensorflow/c/eager/mnist_gradients_testutil.cc#L120-L129\r\n\r\n---\r\n\r\nThank you for your time once again ! \ud83d\ude04 ", "comments": ["@kkimdev , @qqfish I see that you are the codeowers of `c_api_test_util`. Could you take a look at the template too ? Thank you ! \ud83d\ude04 ", "@saxenasaurabh Could you take a look please ? Thank you !", "> How to pass `DataType` to `ops` ( e.g https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/cast#summary ). We might need it in the future. For an example, MNIST metric is essentially `tf.mean(tf.cast(tf.equal()))`\r\n\r\nHow about this one ?\r\n\r\n> When I used AddInputList for adding input to ops::Concat, it threw\r\nhttps://github.com/tensorflow/tensorflow/blob/c5012222b1d9accc07ea8fa4140c66039883ba2b/tensorflow/core/common_runtime/eager/eager_operation.cc#L403\r\nI will need to debug this. Let me know if this is blocking thing change. Else we can debug it with your other PR.\r\n\r\nWe can debug it in other PR.\r\n\r\nI wonder if we should merge https://github.com/vnvo2409/tensorflow/commit/9e42f7ea613c73b303829bfc833bc81d474d9218 into `tensorflow` in another PR. WDYT ? \r\n", "@saxenasaurabh Could you take a look ? Thank you !", "> > How to pass `DataType` to `ops` ( e.g https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/cast#summary ). We might need it in the future. For an example, MNIST metric is essentially `tf.mean(tf.cast(tf.equal()))`\r\n> \r\n> How about this one ?\r\n\r\nCould we use [SetAttrType](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/c/eager/abstract_operation.h;l=93;drc=master)?\r\n\r\n> \r\n> > When I used AddInputList for adding input to ops::Concat, it threw\r\n> > https://github.com/tensorflow/tensorflow/blob/c5012222b1d9accc07ea8fa4140c66039883ba2b/tensorflow/core/common_runtime/eager/eager_operation.cc#L403\r\n> > \r\n> > I will need to debug this. Let me know if this is blocking thing change. Else we can debug it with your other PR.\r\n> \r\n> We can debug it in other PR.\r\n> \r\n> I wonder if we should merge [vnvo2409@9e42f7e](https://github.com/vnvo2409/tensorflow/commit/9e42f7ea613c73b303829bfc833bc81d474d9218) into `tensorflow` in another PR. WDYT ?\r\n\r\nYeah, let's review that separately.", "> Could we use SetAttrType?\r\n\r\nI haven't tried it yet but I think it won't work cause with `ops::Cast`, we have a separated attr ( https://www.tensorflow.org/api_docs/cc/struct/tensorflow/ops/cast/attrs#structtensorflow_1_1ops_1_1_cast_1_1_attrs ).\r\n\r\nI think we need `AddInputType` ( and probably `TF_AddInputType` ). \r\n\r\nHowever, we don't need this ops now so we could leave it for later PR. I just want to bring it up so we could have some initial thoughts about it.", "> > Could we use SetAttrType?\r\n> \r\n> I haven't tried it yet but I think it won't work cause with `ops::Cast`, we have a separated attr ( https://www.tensorflow.org/api_docs/cc/struct/tensorflow/ops/cast/attrs#structtensorflow_1_1ops_1_1_cast_1_1_attrs ).\r\n> \r\n> I think we need `AddInputType` ( and probably `TF_AddInputType` ).\r\n> \r\n> However, we don't need this ops now so we could leave it for later PR. I just want to bring it up so we could have some initial thoughts about it.\r\n\r\nThe destination type is an [attribute](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/ops/math_ops.cc;l=171;drc=86b694333ab8932e7394528e6bdf18ad15f44346) so I think we should be able to use `SetAttrType(\"DstT\", DataType)` ?", "Ah I didn't see that. The function signature on the homepage did confuse me a lot. Unfortunately, my build cache is invalidated so I couldn't do anything until tomorrow.", "@saxenasaurabh Sorry, you are right. We can use `SetAttrType` for `ops::Cast`. I have updated the PR as per your review. Please take a look ! Thank you !"]}, {"number": 46730, "title": "Some **function** symbols are exported as DATA from prebuilt Windows CPU tensorflow.dll", "body": "**System information**\r\n- Windows 10 x64\r\n- Used `libtensorflow-cpu-windows-x86_64-2.3.1.zip` binaries.\r\n\r\n**Describe the current behavior**\r\nWhen I build Haskell `tensorflow` package with my own MSVC toolset-based GHC, I get `undefined symbol` link errors for  `TF_GetCode`, `TF_Message`, `TF_DeleteStatus`, `TF_NewStatus`, `TF_SetStatus` symbols.\r\n\r\nAnd indeed, inspecting `tensorflow.lib` import library I see that these symbols are exported as `DATA` symbols, and linker doesn't generate thunks for them. But they all are functions.\r\n\r\nI suspect the tests don't catch this because if we testing a C/\u0421++ code, the, e.g., Visual C++ complier  sees that these functions are declared as `__declspec(dllimport)` and correctly generates indirect calls for them, thus using `__imp_`-decorated names for these functions.\r\n\r\nAlso we shouldn't, perhaps, have the problems (I haven't tried this tho) when compiling this code with stock mingw-based GHC. This is because mingw tools (GNU ld + GCC startup code) implement runtime pseudo-relocs, which allows the client object code to link against `__imp_foo` anywhere the code expects `foo`.\r\n\r\nI've glanced into the code and have understood that export of the symbols is implemented in a python script which uses heuristics to guess if it shall export the symbol as `DATA` or not.\r\n\r\nApparently these heuristics are broken.\r\n\r\nApart from this: if tensorflow API already have `TF_CAPI_EXPORT` annotations everywhere, why doesn't it use the standard  `dllexport` definition for `TF_CAPI_EXPORT` **when building** the library to export all relevant symbols, why does it implement some broken hard-to-maintain custom mechanism?\r\n\r\nSorry if this is a dumb question, there should exist compelling reasons to do so, I believe.", "comments": ["It might be a mistake that showed up during code evolution. Sadly we didn't treat Windows with the same care as Linux.", "@awson \r\nCan you try updating to the latest stable version 2.6.0 and let us know if the issue still persists? as many bugs have been fixed in the latest version.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "I haven't tested 2.6.0, but looked quickly into `tensorflow.lib` from 2.6.0 distro, and I see these particular symbols are now exported correctly.", "@awson \r\nPlease move this to closed status as it is resolved now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46730\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46730\">No</a>\n"]}, {"number": 46729, "title": "Cash on tflite::Interpreter::AllocateTensors()", "body": "I'm trying to make an audio classifier as [android YAMNet](https://farmaker47.medium.com/classification-of-sounds-using-android-mobile-phone-and-the-yamnet-ml-model-539bc199540) with C++ language on JNI, however, I got the crash on tflite::Interpreter::AllocateTensors. And my tflite version maybe 2.3.\r\n\r\nHere is my code snippet\r\n```\r\n  bool YAMNet::Classify(const std::vector<float> &wave_data, const int sample_rate,\r\n                          const int top_k, std::vector<Recognition> &recognitions) {\r\n\r\n        static const int kDefaultSampleRate = 16000;// Hz\r\n        if (kDefaultSampleRate != sample_rate) {\r\n            Trace_Err(\"Error: YAMNet input must be 16kHz.\");\r\n            assert(false);\r\n            return false;\r\n        }\r\n\r\n        std::vector<int> input_tensor_indices;\r\n        input_tensor_indices = interpreter_ -> inputs();\r\n        interpreter_->ResizeInputTensor(input_tensor_indices[input_index_of_wave_],\r\n                                        {1, (int)wave_data.size()});\r\n\r\n        interpreter_->AllocateTensors();\r\n...\r\n```\r\nAnd then, I got the crash as following:\r\n```\r\n2021-01-27 18:15:36.478 20012-20012/com.tomato.ketchup A/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x28 in tid 20012 (.tomato.ketchup), pid 20012 (.tomato.ketchup)\r\n2021-01-27 18:15:36.839 20012-20055/com.tomato.ketchup E/ketchup: ###### CSeedUdp::ProcessSend sendto() failed, errno=101(Network is unreachable), fd=58\r\n2021-01-27 18:15:36.859 20105-20105/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\n2021-01-27 18:15:36.859 20105-20105/? A/DEBUG: Build fingerprint: 'xiaomi/wayne/wayne:9/PKQ1.180904.001/V11.0.5.0.PDCCNXM:user/release-keys'\r\n2021-01-27 18:15:36.859 20105-20105/? A/DEBUG: Revision: '0'\r\n2021-01-27 18:15:36.859 20105-20105/? A/DEBUG: ABI: 'arm64'\r\n2021-01-27 18:15:36.859 20105-20105/? A/DEBUG: pid: 20012, tid: 20012, name: .tomato.ketchup  >>> com.tomato.ketchup <<<\r\n2021-01-27 18:15:36.859 20105-20105/? A/DEBUG: signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x28\r\n2021-01-27 18:15:36.859 20105-20105/? A/DEBUG: Cause: null pointer dereference\r\n2021-01-27 18:15:36.859 20105-20105/? A/DEBUG:     x0  0000000000000000  x1  0000000000000000  x2  0000007c4b800000  x3  0000000000000004\r\n2021-01-27 18:15:36.859 20105-20105/? A/DEBUG:     x4  0000000000000096  x5  0000007c633a3848  x6  00003e8000000001  x7  00003e8000000001\r\n2021-01-27 18:15:36.859 20105-20105/? A/DEBUG:     x8  0000000000000000  x9  0000007c3abfe358  x10 00000000000000d9  x11 0000007c4b896650\r\n2021-01-27 18:15:36.859 20105-20105/? A/DEBUG:     x12 0000000000000008  x13 0000000000000000  x14 0000000000000011  x15 0000000000000001\r\n2021-01-27 18:15:36.859 20105-20105/? A/DEBUG:     x16 0000007c4fa760f0  x17 0000007c4f973910  x18 00000000ffffffff  x19 0000000000000000\r\n2021-01-27 18:15:36.859 20105-20105/? A/DEBUG:     x20 0000000000000000  x21 0000007c4b86ff80  x22 0000007c4fa25993  x23 0000000000000000\r\n2021-01-27 18:15:36.859 20105-20105/? A/DEBUG:     x24 0000000000000000  x25 0000000000000000  x26 0000007cf1aaa5e0  x27 0000007c4fa71a08\r\n2021-01-27 18:15:36.859 20105-20105/? A/DEBUG:     x28 0000007c3abfe360  x29 0000007fdb66e470\r\n2021-01-27 18:15:36.859 20105-20105/? A/DEBUG:     sp  0000007fdb66e410  lr  0000007c4f974fc4  pc  0000007c4f975018\r\n2021-01-27 18:15:37.182 20105-20105/? A/DEBUG: backtrace:\r\n2021-01-27 18:15:37.182 20105-20105/? A/DEBUG:     #00 pc 0000000000235018  /data/app/com.tomato.ketchup-V1Z97Zn_3BrgXuDk3RSqeA==/base.apk (offset 0xedbb000) (tflite::Subgraph::ModifyGraphWithDelegate(TfLiteDelegate*)+252)\r\n2021-01-27 18:15:37.182 20105-20105/? A/DEBUG:     #01 pc 0000000000238b98  /data/app/com.tomato.ketchup-V1Z97Zn_3BrgXuDk3RSqeA==/base.apk (offset 0xedbb000) (tflite::Interpreter::AllocateTensors()+220)\r\n```\r\n\r\nDose anyone have any idea on this?", "comments": ["Could you try the tf-nightly version instead to follow the original blog post?", "Hi Sun,\r\n\r\nThis mem segfault issue is likely fixed by https://github.com/tensorflow/tensorflow/commit/849682d6a03d571df1cb3c854c2aa44806afe381, could you try a version beyond this commit? like tf-nightly? It looks like this fix isn't included in tf2.4 release based on the [2.4 branch code](https://github.com/tensorflow/tensorflow/blob/r2.4/tensorflow/lite/interpreter.cc#L184-L230)", "Well cool, 2.4 branch solved the problem. However, there are some performance problems being arise. I'm trying to struggle with them, and I will call for help on other issue if I need, thanks for all. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46729\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46729\">No</a>\n"]}, {"number": 46728, "title": "What is the accuracy of PoseNet??", "body": "Is anyone here tried re-training posenet?\r\nI recognised there are no official codes to re-train a posenet model,\r\nso I'm going to make a TF based pose-estimation architecture in python -> train it with COCO data -> convert it to TFLite style.\r\nIf there is anyone else already tried something similar to this, how did you guys compare the accuracy (and loss) of the newer posenet with the official one? (or other pose estimations like alphapose/openpose with posenet...)\r\nBecause I could not find the accuracy of the pre-trained posenet model from any official place (like official blogs or papers).\r\nplease help\r\n", "comments": ["Yeah I am not sure if there is any published accuracy documentation of the model we host. Your plan to train your own sounds good, hopefully ensure that the ops you use are similar to what the [current model](https://tfhub.dev/tensorflow/lite-model/posenet/mobilenet/float/075/1/default/1) uses (try visualizing with Netron if needed); this will ensure seamless conversion to TFLite.", "Thanks Sachin for your immediate response.\r\nI wanted to try dropping the loss (and hopefully increase the accuracy) of the current model bc while adapting the model to my project, it seemed like the model could not detect many poses right (sometimes it gave a bit messed up results for simple poses...).\r\nSo I tried changing the confidence score of each keypoints from 0.5 to 0.9, but the result was not that much satisfying.\r\nIf such problems occur bc of low acc (and high loss), I thought re-training (or adapting transfer-learning to) the current model might give back some pleasing answers, however as you said there are no available current accuracy and loss of the model that I can compare with... so I guess re-training the model might not be a meaningful next step for me.\r\nDo you have any recommendation to increase the model performance?", "@joyccino First thing, we can check if the way you are running inference with the PoseNet model aligns with what the model expects. [This Medium post](https://medium.com/roonyx/pose-estimation-and-matching-with-tensorflow-lite-posenet-model-ea2e9249abbd) shows some of the details in how to generate inputs & process outputs from a TFLite PostNet model. Could you check if you are doing the same logic in your code? Maybe also try with the same model that the author uses, for a fair comparison.", "@joyccino Is this still an issue for you. I see a page on Pose estimation that has benchmark performance for different datasets. https://www.tensorflow.org/lite/examples/pose_estimation/overview\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 46725, "title": "Reshape before Softmax leads runtime error:Unnecessary dynamic-sized tensors", "body": "### 1. System information\r\n\r\n- OS: Win10 & Ubuntu 1804\r\n- TensorFlow installation: Pip\r\n- TensorFlow library: 2.4 & 2.4.1\r\n\r\n### 2. Code\r\n\r\n`\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\ndef convert_to_lite(model,out_path,enable_selected_tf_ops=False):\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n    out_model = converter.convert()\r\n    with open(out_path, \"wb\") as fp:\r\n        fp.write(out_model)\r\n\r\ndef failed_on_android_gpu_for_dynamic_size():\r\n    ipt = layers.Input((256,64,3))\r\n    x = ipt\r\n    x = layers.Reshape((16384,3))(x)\r\n    x = layers.Softmax(axis=1)(x)\r\n    x = layers.Flatten()(x)\r\n    x = layers.Dense(1)(x)\r\n    model = keras.Model(inputs=[ipt],outputs=[x])\r\n    convert_to_lite(model,'failed_on_android_gpu.tflite')\r\n\r\ndef work_reshape_only():\r\n    ipt = layers.Input((256,64,3))\r\n    x = ipt\r\n    x = layers.Reshape((16384,3))(x)\r\n    # x = layers.Softmax(axis=1)(x)\r\n    x = layers.Flatten()(x)\r\n    x = layers.Dense(1)(x)\r\n    model = keras.Model(inputs=[ipt],outputs=[x])\r\n    convert_to_lite(model,'work_reshape_only.tflite')\r\n[Reshape_before_Softmax_leads_runtime_error.zip](https://github.com/tensorflow/tensorflow/files/5879355/Reshape_before_Softmax_leads_runtime_error.zip)\r\n\r\n\r\ndef work_softmax_only():\r\n    ipt = layers.Input((256,64,3))\r\n    x = ipt\r\n    # x = layers.Reshape((16384,3))(x)\r\n    x = layers.Softmax(axis=1)(x)\r\n    x = layers.Flatten()(x)\r\n    x = layers.Dense(1)(x)\r\n    model = keras.Model(inputs=[ipt],outputs=[x])\r\n    convert_to_lite(model,'work_softmax_only.tflite')\r\n\r\nif __name__ == \"__main__\":\r\n    failed_on_android_gpu_for_dynamic_size()\r\n    work_reshape_only()\r\n    work_softmax_only()\r\n`\r\n### 5. (optional) Any other info / logs\r\nRun the 'failed_on_android_gpu.tflite' on android with gpu delegate cause exception:\r\nInternal error: Failed to apply delegate: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.\r\nThe other 2 tflite files which with Reshape/Softmax only works fine, so the dynamic-size tensor is unnecessary.\r\n\r\nI also tested nn.softmax or math.reduced_sum, has same problem. Maybe 'reduce_sum_with_dims' is the real \r\n\r\nproblem.", "comments": ["In the model with the Reshape & Softmax, there is a sequence on ops (SHAPE -> STRIDED_SLICE -> PACK) that produces the 'shape' input to the Reshape op. Since this input is not a constant, I assume that the Reshape output is marked dynamic in the runtime.\r\n\r\nAs pointed out by @Robird , this dynamic-ness seems unnecessary.\r\n\r\n@karimnosseir , could you take a look and see why the converter might be doing this?\r\n", "Keras by default have dynamic batch size, so the actual input to the model is\r\n[?x256x64x3xf32] and the output is [?x1xf32]\r\n\r\nThe current converter honor what is passed, so we keep everything dynamic as is and that's why the tensors are dynamic.\r\nIf you don't care about the batch size, then you can set batch_size=1 to the input layer.\r\nExample:\r\n```\r\ndef failed_on_android_gpu_for_dynamic_size():\r\n  ipt = layers.Input((256, 64, 3), batch_size=1)\r\n  x = ipt\r\n  x = layers.Reshape((16384, 3))(x)\r\n  x = layers.Softmax(axis=1)(x)\r\n  x = layers.Flatten()(x)\r\n  x = layers.Dense(1)(x)\r\n  model = keras.Model(inputs=[ipt], outputs=[x])\r\n  convert_to_lite(model, \"failed_on_android_gpu.tflite\")\r\n```\r\n\r\n\r\nSimilar question https://github.com/tensorflow/tensorflow/issues/43882#issuecomment-731636562", "@karimnosseir Thanks for your analyse! I don't think the dynamic batch size is the real point, because other models with dynamic batch size (such as Reshape/Softmax single only) works fine on gpu delegate, no dynamic-sized tensor problem.\r\nHowever the fixed batch size really works."]}, {"number": 46724, "title": "`tf.matmul` and `tf.tensordot` behave different in converted concrete function in TensorFlowLite ", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution: MacOS 10.15 and Ubuntu 18.04 LTS on Colab machine\r\n- TensorFlow installation (pip package or built from source): pip\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): `tensorflow                    2.4.0`\r\n\r\n### 2. Code\r\n\r\nThis notebook demonstrates the bug with the simplest example I came up with.\r\n\r\nhttps://colab.research.google.com/gist/ebraraktas/ab87170deb38eae979b37795015e44bc\r\n\r\n### 3. Failure after conversion\r\n\r\nI implemented RFFT for TFLite using `tf.matmul` and saved the module concrete function. But invoking saved tflite model repeatedly returns different results. However, replacing `tf.matmul`\u00a0with `tf.tensordot` fixes the strange behavior. Therefore, I have prepared the notebook above to demonstrate the bug. I have realized interesting cases which change the behavior:\r\n\r\n- If negative sign is removed from output returned from `DummyMatmul` or `DummyTensordot` (`result` variable), outputs are same\r\n- If we use `tf.Module` directly, outputs are same (colab demo shows it)\r\n- Somehow, size of the right hand side matrix matters (colab demo shows it)\r\n- Difference occurs after first iteration (colab demo shows it), and for some inputs it gets larger with every iteration \r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46724\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46724\">No</a>\n"]}, {"number": 46723, "title": "Adjust types of loop counters", "body": "Reduces some warnings about comparison of integers of different signs.", "comments": []}, {"number": 46722, "title": "[WIP] Use BlasLtMatmul APIs in matmul_op_impl", "body": "This is a rebase of https://github.com/tensorflow/tensorflow/pull/43237 (part of which was reverted due to regressions) to be used for further performance investigation. I've labelled it WIP because we should identify the cause of the regressions before merging.\r\n\r\n- Integrates BlasLtMatmul with autotuning into the implementation of the BatchMatMul and Einsum ops.\r\n- This integration is only used when the CUDA version is >= 11.0.\r\n\r\ncc @nluehr @timshen91 ", "comments": ["@benbarsdell Can you please check @AyanmoI's comments and resolve conflicts?. Thanks!", "@benbarsdell  Any update on this PR? Please. Thanks!", "@benbarsdell Any update on this PR? and please resolve conflicts Thanks!", "It has been 19 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Closing for now."]}, {"number": 46721, "title": "*** The TAGS command line option is no longer supported in the TFLM Makefile..", "body": "@tensorflow/micro\r\n\r\ni was following guides from Build Arm Cortex-M voice assistant with Google TensorFlow Lite, link as below:\r\nhttps://developer.arm.com/solutions/machine-learning-on-arm/developer-material/how-to-guides/build-arm-cortex-m-voice-assistant-with-google-tensorflow-lite \r\n\r\nso when i come across the command below:\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=\"CMSIS disco_f746ng\" generate_micro_speech_mbed_project\r\n\r\nThis error pops out:\r\n*** The TAGS command line option is no longer supported in the TFLM Makefile..\r\n\r\ni tried to remove TAGS=\"...\" but it after it compiles it did not work as expected.\r\nI also tried OPTIMIZED_KERNEL_DIR, it returns error below:\r\ntensorflow/lite/micro/tools/make/Makefile:552: disco_f746ng.inc: No such file or directory\r\nmake: *** No rule to make target 'disco_f746ng.inc'.  Stop.\r\n\r\nCorrect me if i am wrong, based on my understanding CMSIS and disco_f746ng are folders specified in ./tensorflow/tensorflow/lite/micro/examples/micro_speech\r\nWhich the original TAGS are used to search out the makefile.inc inside both folders and make the file. So the question may be what can i use to replace what TAGS? OR are there any other ways to use OPTIMIZED_KERNEL_DIR?\r\n\r\nThank you in advanced for helping.\r\n\r\n", "comments": ["@jvishnuvardhan  plz don't assign TFL Micro issue to me.", "The mbed project generation bits might be a bit out of date.\r\n\r\nTagging @MatthiasHertel80 to check what the current recommended approach is for using TFLM with mbed.\r\n\r\nFrom my perspective (as a TFLM maintainer, but not the one maintaining the mbed integration), you should be able to rename [this folder](https://github.com/tensorflow/tensorflow/blob/8ddc7459a8f7d2526249f8b9d496d0812a7aff55/tensorflow/lite/micro/examples/micro_speech/arduino/Makefile.inc#L1) to mbed,\r\n\r\nand modify this line:\r\nhttps://github.com/tensorflow/tensorflow/blob/8ddc7459a8f7d2526249f8b9d496d0812a7aff55/tensorflow/lite/micro/examples/micro_speech/disco_f746ng/Makefile.inc#L2\r\n\r\nto check for the target (instead of tage) similar to arduino:\r\nhttps://github.com/tensorflow/tensorflow/blob/8ddc7459a8f7d2526249f8b9d496d0812a7aff55/tensorflow/lite/micro/examples/micro_speech/arduino/Makefile.inc#L1\r\n\r\n\r\nThen `TARGET=mbed OPTIMIZED_KERNEL_DIR=cmsis_nn` should do the trick.", "@advaitjain \r\n\r\nHi, thank you for the reply. I am modifying Line 2 in this file\r\n\r\n[https://github.com/tensorflow/tensorflow/blob/8ddc7459a8f7d2526249f8b9d496d0812a7aff55/tensorflow/lite/micro/examples/micro_speech/disco_f746ng/Makefile.inc](url)\r\n`ifneq ($(filter disco_f746ng,$(ALL_TAGS)),)`\r\n\r\nto the code like this\r\n\r\n`ifeq ($(TARGET),$(filter $(TARGET),disco_f746ng))`\r\n\r\nand then I run the make command\r\n\r\n`make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed OPTIMIZED_KERNEL_DIR=cmsis_nn generate_micro_speech_mbed_project`\r\n\r\nBut I result in the error\r\n\r\n> make: *** No rule to make target 'tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/hello_world/mbed/tensorflow/lite/c/common.c', needed by 'generate_hello_world_mbed_project'.  Stop.\r\n\r\nDoes that mean I have something need to modify in the Makefile? Thank you.", "I have similar issue, but slightly different (no modification on Makefile.inc)\r\n\r\nI was able to make without problem with the make command\r\n`make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed OPTIMIZED_KERNEL_DIR=cmsis_nn generate_micro_speech_mbed_project`\r\n\r\nBut the lib's (AUDIO_DISCO_F746NG.lib,  BSP_DISCO_F746NG.lib, SDRAM_DISCO_F746NG.lib, LCD_DISCO_F746NG.lib) in tensorflow/lite/micro/examples/micro_speech/disco_f746ng/Makefile.inc will be not included due to the TARGET was set to 'mbed', but not 'disco_f746ng'.\r\n\r\nWhen I modified the make command of setting TARGET=disco_f746ng\r\n`make -f tensorflow/lite/micro/tools/make/Makefile TARGET=disco_f746ng OPTIMIZED_KERNEL_DIR=cmsis_nn generate_micro_speech_mbed_project`\r\n\r\nIt did reproduce the similar error message.\r\n> make: *** No rule to make target 'tensorflow/lite/micro/tools/make/targets/disco_f746ng_makefile.inc'.  Stop.\r\n\r\nAnd set TARGET=\"mbed disco_f746ng\" will not help either.\r\nFrom user perspective, it is confusing between in Makefile TARGET setting between mbed and disco_f746ng (or other embed platforms like st32f4).", "@angsiokcheng @marconi1964 @zyang498 I don't know if this is still relevant for you but I just uploaded PR #48659 . It updates the micro_speech mbed generation. It should enable you to compile the micro_speech example for STM32F746, if you follow the instructions in tensorflow/lite/micro/examples/micro_speech/README.md. The instructions on developer.arm.com that you posted above have not been updated yet.", "@patriklaurell your PR works very well on micro_speech example without \"OPTIMIZED_KERNEL_DIR=cmsis_nn\" option. Thank you very much.\r\nBut still \"collect2: error: ld returned 1 exit status\" with \"OPTIMIZED_KERNEL_DIR=cmsis_nn\"\r\nAnd, since it changes the way how 'make' works, the other examples of DISCO_F746NG needs to be updated accordingly, too. ", "@marconi1964 Good to hear that it at least partly solved the problem. For micro_speech I can compile locally using the instructions in the readme. Could you provide some more details on how to reproduce?\r\nFor hello_world, I will upload a PR today to update the example", "@patriklaurell I [uploaded a PR](https://github.com/tensorflow/tensorflow/pull/48740) for the `hello_world` example, and also updated some of the other outdated information in the README.md of both `hello_world` and `micro_speech` (like confirming Python 3/pip3 support, and updating the reference to the MBED CLI installation instructions). I also added the commands for copying the old `arm_math.h` and `cmsis_gcc.h` dependencies [that you linked to the other issue I posted](https://github.com/tensorflow/tensorflow/issues/48741#issuecomment-826782020). ", "@patriklaurell Just to confirm my issue reproduction, I remove the repo and git clone again. And (nice) surprisingly, it works well with cmsis_nn enabled. The issue is gone. Great and thanks.", "@angsiokcheng With all these discussions with fixes and code merged, assume this issue can be closed?", "Thanks for the great work on this. I'm going to close this issue. Feel free to reopen if needed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46721\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46721\">No</a>\n"]}, {"number": 46720, "title": " An error occurs when processing data delivered to tensorflow serving.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.4\r\n- Python version: 3.7.9\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n**Describe the current behavior**\r\nAn error occurs when transferring data that is confirmed to be operating normally to tensorflow serving\r\n\r\n**Describe the expected behavior**\r\nWhen transmitting data confirmed to be working normally to TensorFlow Serving, it should operate normally\r\n\r\n**Standalone code to reproduce the issue**\r\nThis Is my model's signature\r\n```\r\nsignature_def['serving_default']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n    inputs['age'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 1)\r\n        name: serving_default_age:0\r\n    inputs['balance'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 1)\r\n        name: serving_default_balance:0\r\n    inputs['campaign'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 1)\r\n        name: serving_default_campaign:0\r\n    inputs['contact'] tensor_info:\r\n        dtype: DT_STRING\r\n        shape: (-1, 1)\r\n        name: serving_default_contact:0\r\n    inputs['day'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 1)\r\n        name: serving_default_day:0\r\n    inputs['default'] tensor_info:\r\n        dtype: DT_STRING\r\n        shape: (-1, 1)\r\n        name: serving_default_default:0\r\n    inputs['duration'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 1)\r\n        name: serving_default_duration:0\r\n    inputs['education'] tensor_info:\r\n        dtype: DT_STRING\r\n        shape: (-1, 1)\r\n        name: serving_default_education:0\r\n    inputs['housing'] tensor_info:\r\n        dtype: DT_STRING\r\n        shape: (-1, 1)\r\n        name: serving_default_housing:0\r\n    inputs['job'] tensor_info:\r\n        dtype: DT_STRING\r\n        shape: (-1, 1)\r\n        name: serving_default_job:0\r\n    inputs['loan'] tensor_info:\r\n        dtype: DT_STRING\r\n        shape: (-1, 1)\r\n        name: serving_default_loan:0\r\n    inputs['marital'] tensor_info:\r\n        dtype: DT_STRING\r\n        shape: (-1, 1)\r\n        name: serving_default_marital:0\r\n    inputs['month'] tensor_info:\r\n        dtype: DT_STRING\r\n        shape: (-1, 1)\r\n        name: serving_default_month:0\r\n    inputs['pdays'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 1)\r\n        name: serving_default_pdays:0\r\n    inputs['poutcome'] tensor_info:\r\n        dtype: DT_STRING\r\n        shape: (-1, 1)\r\n        name: serving_default_poutcome:0\r\n    inputs['previous'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 1)\r\n        name: serving_default_previous:0\r\n    inputs['seq'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 5)\r\n        name: serving_default_seq:0\r\n    inputs['y'] tensor_info:\r\n        dtype: DT_STRING\r\n        shape: (-1, 1)\r\n        name: serving_default_y:0\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['dense_1'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 171)\r\n        name: StatefulPartitionedCall:0\r\n  Method name is: tensorflow/serving/predict\r\n```\r\nload the model and predict with sample_data works fine\r\n\r\n```\r\nloded_model = tf.keras.models.load_model(model_path, compile=True)\r\nsample_data = {\r\n        'seq': np.expand_dims(np.array([110, 115, 49, 71, 50]), axis=0),\r\n        'age': np.expand_dims(np.array(32), axis=0),\r\n        'job': np.expand_dims(np.array('management'), axis=0),\r\n        'marital': np.expand_dims(np.array('married'), axis=0),\r\n        'education': np.expand_dims(np.array('primary'), axis=0),\r\n        'default': np.expand_dims(np.array('no'), axis=0),\r\n        'balance': np.expand_dims(np.array(67), axis=0),\r\n        'housing': np.expand_dims(np.array('no'), axis=0),\r\n        'loan': np.expand_dims(np.array('no'), axis=0),\r\n        'contact': np.expand_dims(np.array('unknown'), axis=0),\r\n        'day': np.expand_dims(np.array(24), axis=0),\r\n        'month': np.expand_dims(np.array('jun'), axis=0),\r\n        'duration': np.expand_dims(np.array(9), axis=0),\r\n        'campaign': np.expand_dims(np.array(1), axis=0),\r\n        'pdays': np.expand_dims(np.array(-1), axis=0),\r\n        'previous': np.expand_dims(np.array(0), axis=0),\r\n        'poutcome': np.expand_dims(np.array('unknown'), axis=0),\r\n        'y': np.expand_dims(np.array('no'), axis=0)\r\n    }\r\nloded_model(sample_data)\r\n<tf.Tensor: shape=(1, 171), dtype=float32, numpy=\r\narray([[5.27819619e-02, 7.91415647e-02, 2.05127634e-02, 4.92982864e-02,\r\n        8.63614008e-02, 5.65255731e-02, 4.22200933e-02, 6.70532882e-02,\r\n        2.94332802e-02, 5.92661090e-02, 3.92056666e-02, 6.91237226e-02,\r\n        1.33277355e-02, 8.19469616e-03, 5.87656125e-02, 5.18164076e-02,\r\n        6.20538630e-02, 9.00777951e-02, 4.28347066e-02, 2.19823830e-02,\r\n        1.49062146e-07, 2.01256057e-07, 1.53366202e-07, 1.27340172e-07,\r\n        1.86510434e-07, 1.98491620e-07, 1.82937896e-07, 1.32476117e-07,\r\n        1.31851834e-07, 1.40986472e-07, 1.99838468e-07, 1.58420590e-07,\r\n        1.17166692e-07, 9.00412758e-08, 1.95417840e-07, 1.31040650e-07,\r\n        1.60380964e-07, 1.07113330e-07, 1.39082644e-07, 1.42982330e-07,\r\n        1.21180648e-07, 1.54600315e-07, 1.02207764e-07, 2.41572224e-07,\r\n        1.50535001e-07, 1.23603499e-07, 1.76933810e-07, 1.29627537e-07,\r\n        2.25440814e-07, 2.21079134e-07, 1.40433457e-07, 1.21948943e-07,\r\n        1.12487960e-07, 1.82974887e-07, 1.94294685e-07, 1.49468548e-07,\r\n        9.63142881e-08, 1.64101479e-07, 8.75015900e-08, 1.78585822e-07,\r\n        1.18338065e-07, 1.34275794e-07, 1.94435557e-07, 9.40549114e-08,\r\n        7.68365709e-08, 9.09561138e-08, 2.07228780e-07, 1.23126767e-07,\r\n        1.52446063e-07, 1.45874452e-07, 1.97502175e-07, 1.05272626e-07,\r\n        2.01461347e-07, 2.27837347e-07, 2.44192165e-07, 1.04354719e-07,\r\n        1.47167256e-07, 1.86789734e-07, 1.95981102e-07, 1.83554263e-07,\r\n        1.42765145e-07, 9.52495824e-08, 2.68968222e-07, 1.77189975e-07,\r\n        9.28885981e-08, 1.18023948e-07, 1.40940770e-07, 1.22296697e-07,\r\n        1.37569558e-07, 1.37506873e-07, 8.40115391e-08, 1.25378747e-07,\r\n        2.05835846e-07, 1.08780618e-07, 5.62952067e-08, 1.59157153e-07,\r\n        1.29730566e-07, 1.50789603e-07, 1.63185859e-07, 1.54063102e-07,\r\n        1.42086819e-07, 1.31032266e-07, 1.95424562e-07, 8.60203855e-08,\r\n        2.31950693e-07, 1.25126334e-07, 2.37775524e-07, 1.93945539e-07,\r\n        7.71898456e-08, 1.51705976e-07, 1.75589378e-07, 2.89475594e-07,\r\n        1.35602036e-07, 1.80231439e-07, 2.15656939e-07, 1.21509544e-07,\r\n        1.46606226e-07, 1.69434031e-07, 8.56758717e-08, 1.21774391e-07,\r\n        2.94665796e-07, 1.73765784e-07, 1.44480254e-07, 1.25448224e-07,\r\n        1.26027047e-07, 1.53907735e-07, 1.21600536e-07, 1.80686797e-07,\r\n        1.34774865e-07, 1.55523523e-07, 1.34890200e-07, 1.37089174e-07,\r\n        1.40633560e-07, 1.36167046e-07, 1.17650742e-07, 1.60875459e-07,\r\n        1.26850381e-07, 1.48223222e-07, 1.11685516e-07, 1.30754032e-07,\r\n        1.99773112e-07, 9.96520626e-08, 1.44601273e-07, 2.22673521e-07,\r\n        1.37150110e-07, 9.27915593e-08, 1.32079876e-07, 1.45855807e-07,\r\n        1.98044447e-07, 8.85873419e-08, 1.73917329e-07, 9.99040353e-08,\r\n        2.03707074e-07, 9.97928140e-08, 1.81921166e-07, 1.35442946e-07,\r\n        1.58751504e-07, 1.62741699e-07, 1.55544441e-07, 1.72405649e-07,\r\n        8.22981931e-08, 2.17250758e-07, 1.59198137e-07, 1.26780478e-07,\r\n        1.79329419e-07, 1.99787223e-07, 2.10311356e-07, 1.92279757e-07,\r\n        1.51305883e-07, 1.15883971e-07, 1.22970349e-07]], dtype=float32)>\r\n\r\n```\r\n\r\nHowever, an error occurs when serving with same data.\r\n\r\n```\r\ndef processing_input(seq = list, age=int, job=str, marital=str, education=str, default=str, balance=int,\r\n                         housing=str, loan=str, contact=str, day=int, month=str, duration=int,campaign=int, pdays=int, previous=int,poutcome=str,y=str):\r\n    input = {\r\n        'seq': np.expand_dims(np.array(seq), axis=0).tolist(),\r\n        'age': np.expand_dims(np.array(age), axis=0).tolist(),\r\n        'job': np.expand_dims(np.array(job), axis=0).tolist(),\r\n        'marital': np.expand_dims(np.array(marital), axis=0).tolist(),\r\n        'education': np.expand_dims(np.array(education), axis=0).tolist(),\r\n        'default': np.expand_dims(np.array(default), axis=0).tolist(),\r\n        'balance': np.expand_dims(np.array(balance), axis=0).tolist(),\r\n        'housing': np.expand_dims(np.array(housing), axis=0).tolist(),\r\n        'loan': np.expand_dims(np.array(loan), axis=0).tolist(),\r\n        'contact': np.expand_dims(np.array(contact), axis=0).tolist(),\r\n        'day': np.expand_dims(np.array(day), axis=0).tolist(),\r\n        'month': np.expand_dims(np.array(month), axis=0).tolist(),\r\n        'duration': np.expand_dims(np.array(duration), axis=0).tolist(),\r\n        'campaign': np.expand_dims(np.array(campaign), axis=0).tolist(),\r\n        'pdays': np.expand_dims(np.array(pdays), axis=0).tolist(),\r\n        'previous': np.expand_dims(np.array(previous), axis=0).tolist(),\r\n        'poutcome': np.expand_dims(np.array(poutcome), axis=0).tolist(),\r\n        'y': np.expand_dims(np.array(y), axis=0).tolist()\r\n    }\r\n    data = json.dumps({'inputs': input})\r\n    return data\r\n\r\ndata = processing_input([0,0,0,152,151],32,'tech','none','string','no',3,'no','no','unknown',3,'june',2,1,5,2,'name','no')\r\njson_response = requests.post(f'http://{model_adr}/v1/models/{model_name}:predict', data=data)\r\nprediction = json.loads(str(json_response.content, 'utf-8'))\r\n\r\n```\r\nand i get \r\n\r\n```\r\n{'error': 'Invalid reduction dimension (1 for input with 1 dimension(s)\\n\\t [[{{node model/category_encoding_8/Min}}]]'}\r\n```\r\n\r\nI put data that can be accepted as input in the model, why doesn't it work at serving time?\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@karrdy89 \r\nPlease create this issue on TF serving repo and move this to closed status.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46720\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46720\">No</a>\n"]}, {"number": 46718, "title": "Typo Recognize Flowers with TensorFlow Lite on Android", "body": "## URL(s) with the issue:\r\nhttps://codelabs.developers.google.com/codelabs/recognize-flowers-with-tensorflow-on-android#4\r\n\r\n## Description of issue (what needs changing):\r\nCopy the TensorFlow Lite model model.tflite and label.txt that you trained earlier to assets folder at `lite/codelabs/flower_classification/start/app/src/main/assets/.`\r\n\r\nThe path used above is incorrect and needs to be updated to `lite/codelabs/flower_classification/android/start/app/src/main/assets/`\r\n\r\nThanks,\r\nGeorge\r\n", "comments": ["@lintian06  could you take a look?", "Thank you for the suggestion! It is going to be fixed soon. :-)", "I see the [codelab](https://codelabs.developers.google.com/codelabs/recognize-flowers-with-tensorflow-on-android#0) has been revamped and the issue no longer persists. Please let us know if you have any difficulties. Thanks!"]}, {"number": 46717, "title": "Add relevant shape check for tf.reshape to prevent crash", "body": "This PR tries to address the issue raised in #46693 where\r\na shape with large number of elements will cause the\r\ntf.reshape to crash.\r\n\r\nThis PR adds relevant shape check so that error message can\r\nbe returned gracefully.\r\n\r\nThis PR fixes #46693\r\n\r\nThis PR also fixes #46699\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @allenlavoie for the review. Indeed that was a tab issue. I have updated the PR and fixed it. Please take a look.", "The merge isn't working because we have some presubmits which run the test in a 1.x-style Graph+Session. Can you please decorate the test method with \"@test_util.run_v2_only\"?"]}, {"number": 46716, "title": "Add uint32 and uint64 support for PopulationCount", "body": "PopulationCount is part of the bitwise operations (BitwiseAnd/Or/Xor/etc)\r\nthough unlike other bitwise operatios, PopulationCount does not\r\nhave uint32 and uint64 support.\r\n\r\nThis PR add uint32 and uint64 support and fixes #46676.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 46715, "title": "enable tf.shape on ragged keras input tensor", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.4.1\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\ncurrent behavior:\r\ncurrently. tf.shape can't be called on ragged keras input tensor. for example, \r\n```shell\r\n#!/usr/bin/python3\r\nimport tensorflow as tf;\r\n\r\ninputs = tf.keras.Input((None, 1), ragged = True);\r\nbatch = tf.keras.layers.Lambda(lambda x: tf.shape(x)[0])(inputs);\r\n```\r\ncan't get a variable to be infered. tf.shape is necessary because the input tensor shape is not decided before a specific tensor is feed into the keras model.\r\n\r\nfeature:\r\nsupport tf.shape on ragged keras input tensor.\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo change to current api\r\n\r\n**Who will benefit with this feature?**\r\n\r\ndeveloper who use functional or sequential tf.keras.Model to handle NLP related application.\r\n\r\n**Any Other info.**\r\n", "comments": ["Workaround:\r\nUse ```type_spec_from_value``` instead of ```tf.shape```. This works for both dense and ragged tensors.\r\nas in:\r\n```\r\nbatch = tf.keras.layers.Lambda(lambda x: tf.type_spec_from_value(x).shape[0])(inputs)\r\n```\r\n\r\nFull example:\r\nhttps://colab.research.google.com/drive/1mu8r2iuJsw2vjnqxNE3AAoC3LuRpgiw3?usp=sharing\r\n\r\nIt does seem odd that tf.shape doesn't return the underlying shape. I guess that with ragged tensors there is ambiguity between the shape specification and the bounding_shape(); perhaps the TF team decided not to support the ```tf.shape``` operation; imho it would make the API more consistent if tf.shape was defined and returned the inferred type_spec shape.", "@edloper should be able to call `tf.shape()` on a ragged tensor?", "@fchollet No, `tf.shape` is not defined for ragged tensors -- the shape of a ragged tensor can't be described by a single list of integers.  Internally, we have the RaggedTensorDynamicShape type, which can encode the full shape of a ragged tensor; and we may eventually update it to be a composite tensor, and update `tf.shape(x: RaggedTensor)` to return a `RaggedTensorShape` object, but we haven't done that yet.  (There are good reasons why we do *not* have `tf.shape(x: RaggedTensor)` return `x.bounding_shape()` -- if you're interested, we can talk about them sometime.)\r\n\r\n@pedro-r-marques A simpler workaround is `batch = tf.keras.layers.Lambda(lambda x: x.shape[0])(inputs)`.  `Tensor.shape` (and `RaggedTensor.shape`) returns a `TensorShape` object, which describes what's known about a shape at compile time (and may include unknown dimensions).  `tf.shape(x: Tensor)` returns a `Tensor` object that gives the runtime shape of `x` (and will never include unknown dimensions).  So as long as the dimension size is known statically, you can just use `x.shape[0]` instead of `tf.shape(x)[0]`, and it will work for both tensors and ragged tensors.  (Note that `tf.type_spec_from_value(x).shape` is just a more verbose way of saying `x.shape` -- i.e., the result is a `TensorShape`, not a `Tensor`.)  If you *don't* know the batch shape statically, then neither `x.shape[0]` nor `tf.type_spec_from_value(x).shape[0]` will work (both will return `None`).  In that case, you'd currently need to do something like `batch = x.nrows() if isinstance(x, tf.RaggedTensor) else tf.shape(x)[0]`.\r\n", "BTW, if you are interested in batch size only [edited], it should be fine to use `bounding_shape`, i.e.,\r\n```python\r\nbatch_size = tf.keras.layers.Lambda(lambda x: x.bounding_shape(0))(inputs)\r\n```\r\nor more naturally as\r\n```python\r\nbatch_size = inputs.bounding_shape(0)\r\n```\r\n\r\nCheers.", "@breadbread1984 Can you please check whether @edloper suggestion helped in resolving the issue. Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 46714, "title": "Port int8 and float versions of space_to_batch to TFLM", "body": "Commit 1 copies the TFLite operator into TFLM\r\nCommit 2 implements basic float, int8 and error checking tests along with float and int8 implementations of space_to_batch_nd\r\n\r\nThis version requires that the flat size of input matches output, since TFLM does not support tensor resizing.\r\n\r\n#45693", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@njeffrie  Can you please resolve conflicts? Thanks!"]}, {"number": 46713, "title": "Issue with stateful metrics initialization in ProgbarLogger inside the model.evaluate() function", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from PyPI\r\n- TensorFlow version: 2.4.0\r\n- Python version: 3.6\r\n\r\n\r\n**Expected behavior**\r\n\r\nWhen using `model.evaluate(validation_dataset)` function, the `ProgbarLogger` callback, that is initialized inside it, should log all the Model's metrics as-is (register them as stateful metrics). \r\n\r\nFrom `tf.keras.callbacks.ProgbarLogger` docs:\r\n> \r\n>stateful_metrics | Iterable of string names of metrics that should\u00a0not\u00a0be averaged over an epoch. Metrics in this list will be logged as-is. All others will be averaged over time (e.g. loss, etc). If not provided, defaults to the\u00a0Model's metrics. \r\n>-- | --\r\n\r\n**Behavior in TF 2.3.1**\r\n\r\nThe `_maybe_init_progbar()` function, which initializes the stateful metrics list, is called inside the `on_test_batch_end()` method. This means that the Model has been evaluated at least once, and therefore the Model's `metrics` attribute is available for the `ProgbarLogger` to initialize the list of stateful metrics.\r\n\r\nFrom `tf.keras.Model` docs:\r\n\r\n> Note: metrics_names are available only after a keras.Model has been trained/evaluated on actual data.\r\n\r\n**Current behavior in TF 2.4.0**\r\n\r\nAfter b31f069d3abc8446a5d370c6fa65f9df0a7720b6 commit the `_maybe_init_progbar()` function is called inside `on_epoch_begin()`, `on_test_begin()` and `on_predict_begin()`, which means that the Model does not yet contain the list of compiled metrics and the `stateful_metrics` list is not initialized properly. This causes simple metrics such as `tf.keras.metrics.Accuracy` to be averaged over time and then logged to the console.\r\n", "comments": ["@evgeniya-egupova \r\n\r\nCan you please share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "I have an issue that looks related:\r\n\r\nSince using 2.4.1 (I used 2.3.0 before), during training the values reported for my metrics seem to be incorrect. For example the \"loss\" reported is different than the one passed to 'on_epoch_end'. Maybe there is double averaging going on.\r\n\r\nEDIT: For example at the start of the training my loss is above 11. At the end of the first epoch, with 2.4.1, the progress bar final 'loss' is indicated at 9.2957, while the loss passed to the on_epoch_end is 8.6675. This is why I suspect double averaging.", "@ravikyram \r\n\r\nHere is [the gist showing the tf.keras.applications.MobileNetV2 evaluation](https://gist.github.com/evgeniya-egupova/f50bb4e5ec37e152c1992db965bc04ab). The accuracy on imagenet is 71.85%, however the logged accuracy is 72.03%.\r\n\r\nNote that there are some evaluation issues with batch size > 1, so I suggest not to change it.", "Allow me to summarize the observation of the bug a bit more accurately: tensorflow writes bogus values for training metrics on the console if verbose=1 ([as is the default](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit)). The console output is fine with verbose=2. Regardless of that parameter, the correct values are logged in history (and to tensorboard, when requested). Validation metrics are unaffected.\r\n\r\nSo a workaround is to set verbose=2, or simply to continue enjoying the dancing numbers but ignoring them when the dance is over.\r\n\r\n[Here is a much simpler demonstration](https://gist.github.com/ssomers/59c16f042c8c356a442636492f258cc2).", "@ssomers it's true the problem seems to be in the Progbar\r\n\r\n\r\nRelated issue https://github.com/tensorflow/tensorflow/issues/47099 has been closed, could you check if the present issue is solved too on the new commit 979c5093da6238eae2f137e2f5538cf6373c4c02?", "The fresh [tf-nightly 2.6.0.dev20210421](https://pypi.org/project/tf-nightly/) doesn't help for this issue, but I can't find which commit this is built on (and meanwhile it seems the Windows wheels have disappeared). Going over to trying to build tf myself.", "I'm pretty sure I tested the wrong thing yesterday. Today, a simple `pip install tf-nightly` in a fresh python 3.9 virtual environment (which at the moment of this writing is the same 2.6.0.dev20210421) ends in a flawless display of loss and custom metrics.\r\n\r\nI'm not so sure that that means this issue is entirely fixed and is the same as all the linked issues.", "This is fixed with latest tf-nightly as mentioned by ssomers.\r\nSee [gist](https://colab.research.google.com/gist/jvishnuvardhan/ab673604087e19dc352b131c550992e8/untitled101.ipynb) for your reference. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46713\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46713\">No</a>\n", "@ymodak I am on tf-nightly-2.7.0.dev20210630 and am still observing that stateful metrics are not as described (ie, they continue to be averaged over the running epoch)"]}, {"number": 46712, "title": "Add reference fallback for TARGET_ARCH!=hifimini", "body": "A follow-up change will add in calls to the XaNNLib to get an optimized softmax implementation.\r\n\r\nTested that keyword_benchmark has a latency increase of ~1000 ticks for the Fusion F1.\r\n\r\nThis latency improvement does not matter since the current change is setting the stage for pulling in the optimized implementation.\r\n\r\nThis command:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade test_keyword_benchmark -j8\r\n```\r\n\r\nbefore this change:\r\n```\r\nInitializeKeywordRunner() took 279548 ticks (279 ms)\r\nKeywordRunNIerations(1) took 151249 ticks (151 ms)\r\nKeywordRunNIerations(10) took 1511997 ticks (1511 ms)\r\n```\r\n\r\nafter this change:\r\n```\r\nInitializeKeywordRunner() took 201464 ticks (201 ms)\r\nKeywordRunNIerations(1) took 152158 ticks (152 ms)\r\nKeywordRunNIerations(10) took 1521087 ticks (1521 ms)\r\n```\r\n\r\nProgress towards http://b/177457688\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "tagging @pnikam-cad @nyadla-sys "]}, {"number": 46710, "title": "micro: prepare to port operator FLOOR_DIV kernel from lite with test", "body": "Implement skeleton (non-working) code for operator and test.\r\nHeader files changed.\r\nNamespaces changed.\r\nSome original code deleted.\r\nSome original code modified.\r\n\r\nThis represents PR step 4 of the work to port operator FLOOR_DIV as tracked in Issue #45657", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@ddavis-2015 can you please check sanity build failures ?", "> @ddavis-2015 can you please check sanity build failures ?\r\n\r\nThe errors are unrelated to the current PR and can be ignored."]}, {"number": 46709, "title": "Refactor softmax to share code between reference and optimized implementations.", "body": "", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 46707, "title": "Converting a model with Int8 fake quant nodes from a saved model file fails", "body": "**System information**\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.6\r\n- TensorFlow installed from (source or binary): TensorFlow 2.4.1 PyPI pip package.\r\n- TensorFlow version (or github SHA if from source): 2.4.1.\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\nSuppose `keras_model` is a Keras model containing QAT fake quantiser ops, like `tf.quantization.fake_quant_with_min_max_vars`. Then, the following conversion code using `from_saved_model` fails to output a valid Int8 model and throws an error.\r\n\r\n```python\r\nwith open(\"converted_saved_model.tflite\", \"wb\") as f:\r\n    tf.keras.models.save_model(keras_model, \"tmp-saved-model\", save_format=\"tf\")\r\n    converter = tf.lite.TFLiteConverter.from_saved_model(\"tmp-saved-model\")\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.inference_input_type = tf.int8\r\n    converter.inference_output_type = tf.int8\r\n    f.write(converter.convert())\r\n```\r\n\r\nNote that `from_keras_model` works correctly.\r\n\r\nSee a minimal reproduction with the following colab notebook: https://colab.research.google.com/drive/1s3AkxetmaIHKcB3M69cBSVbOfhNx8106?usp=sharing\r\n\r\n**The output from the converter invocation**\r\n\r\nThe above snippet fails with:\r\n\r\n```\r\nValueError: The inference_input_type and inference_output_type must be tf.float32.\r\n```\r\n\r\nFor more detail, see the linked colab notebook.\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\nCan be found in the linked colab notebook.\r\n\r\n**Failure details**\r\n\r\nAttempting to convert and setting the inference input/output type to int8 causes the error above.\r\n\r\nAttempting to convert without setting those types (such that they default to float) doesn't cause an error but does yield a broken Int8 model with lots of dangling quant/dequant ops pairs:\r\n\r\n<img width=\"500\" alt=\"image\" src=\"https://user-images.githubusercontent.com/7688302/105897373-f9673a80-600f-11eb-95ad-683671cbf825.png\">", "comments": ["This seems to stem from the fact that the following check does not return true:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/8027470e1e43a17645bf843815a236b92390d311/tensorflow/lite/python/lite.py#L410-L420\r\n\r\nWhen converting from the saved model, the `node_def.op` values are the following:\r\n\r\n```\r\nVarHandleOp\r\nReadVariableOp\r\nVarHandleOp\r\nReadVariableOp\r\nVarHandleOp\r\nReadVariableOp\r\nVarHandleOp\r\nReadVariableOp\r\nVarHandleOp\r\nReadVariableOp\r\nVarHandleOp\r\nReadVariableOp\r\nVarHandleOp\r\nReadVariableOp\r\nNoOp\r\nConst\r\nPlaceholder\r\nStatefulPartitionedCall\r\nPlaceholder\r\nStatefulPartitionedCall\r\nStatefulPartitionedCall\r\n```\r\n\r\nwhereas when using `from_keras_model` they are:\r\n\r\n```\r\nPlaceholder\r\nConst\r\nConst\r\nConst\r\nConst\r\nConst\r\nConst\r\nConst\r\nIdentity\r\nIdentity\r\nIdentity\r\nIdentity\r\nIdentity\r\nIdentity\r\nIdentity\r\nFakeQuantWithMinMaxVars\r\n```\r\n\r\n(using the model from the colab notebook)\r\n\r\nThis is why the check is true when running `from_keras_model`, but false when running `from_saved_model`.\r\n\r\nSlightly related: #46704.", "I have tried in colab with TF version 2.4, Nightly version(`2.5.0-dev20210126`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/1f48dcd87a3bc22d0fbf5363f8ef2b32/untitled631.ipynb). Thanks!", "@MeghnaNatraj could you take a look at this?", "It looks like the model has not been generated correctly. Reference [colab](https://colab.research.google.com/gist/MeghnaNatraj/6245ffa33364f2e2c7147f37c7b243c2/untitled631.ipynb).\r\n\r\nRefer to the last cell in this notebook. \r\n\r\n\r\n```\r\nwith open(\"converted_saved_model.tflite\", \"wb\") as f:\r\n    tf.saved_model.save(keras_model, \"tmp-saved-model\")\r\n    keras_model_ = tf.saved_model.load(\"tmp-saved-model\")\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(keras_model_)\r\n    # tf.keras.models.save_model(keras_model, \"tmp-saved-model\")\r\n    # converter = tf.lite.TFLiteConverter.from_saved_model(\"tmp-saved-model\")\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.inference_input_type = tf.int8\r\n    converter.inference_output_type = tf.int8\r\n    f.write(converter.convert())\r\n```\r\n\r\n```\r\nWARNING:absl:Found untraced functions such as int8_fake_quantize_1_layer_call_fn, int8_fake_quantize_1_layer_call_and_return_conditional_losses, int8_fake_quantize_1_layer_call_fn, int8_fake_quantize_1_layer_call_and_return_conditional_losses, int8_fake_quantize_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\r\nWARNING:absl:Found untraced functions such as int8_fake_quantize_1_layer_call_fn, int8_fake_quantize_1_layer_call_and_return_conditional_losses, int8_fake_quantize_1_layer_call_fn, int8_fake_quantize_1_layer_call_and_return_conditional_losses, int8_fake_quantize_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\r\nINFO:tensorflow:Assets written to: tmp-saved-model/assets\r\nINFO:tensorflow:Assets written to: tmp-saved-model/assets\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-5-400338f2c78a> in <module>()\r\n      8     converter.inference_input_type = tf.int8\r\n      9     converter.inference_output_type = tf.int8\r\n---> 10     f.write(converter.convert())\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in convert(self)\r\n    837     # to None.\r\n    838     # Once we have better support for dynamic shapes, we can remove this.\r\n--> 839     if not isinstance(self._keras_model.call, _def_function.Function):\r\n    840       # Pass `keep_original_batch_size=True` will ensure that we get an input\r\n    841       # signature including the batch dimension specified by the user.\r\n\r\nAttributeError: '_UserObject' object has no attribute 'call'\r\n```\r\n\r\nWill get back once I have an update on how this can be fixed.", "It just works if you use this save/load function.\r\n\r\n    with open(\"converted_saved_model.tflite\", \"wb\") as f:\r\n        tf.keras.models.save_model(keras_model, \"tmp-saved-model\", save_format=\"tf\")\r\n        keras_model_ = tf.keras.models.load_model(\"tmp-saved-model\")\r\n        converter = tf.lite.TFLiteConverter.from_keras_model(keras_model_)\r\n        \r\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n        converter.inference_input_type = tf.int8\r\n        converter.inference_output_type = tf.int8\r\n        f.write(converter.convert())\r\n\r\n\r\nI think there's a bug on converter validator that seems not check nested function.\r\nI'll try to fix it. Thanks.", "Here's the fix: 6be1ebb8498e2ccdb5d05ad673678b98a9373346\r\nPlease reopen the bug if you have any problem with this changes.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46707\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46707\">No</a>\n", "I tested this by pasting in your patch into my local `lite.py` and it fixed both issues I saw. Thanks for the help @MeghnaNatraj @Xhark!"]}, {"number": 46705, "title": "x", "body": "x", "comments": []}, {"number": 46704, "title": "Consider `FakeQuantWithMinMaxArgs` to be a train-time quant op in the TFLite Converter", "body": "`tf.quantization.fake_quant_with_min_max_vars` is commonly used for training-time Int8 fake-quantisation in TF, and when used correctly will cause the TFLite converter to output int8 (rather than float) ops.\r\n\r\n`tf.quantization.fake_quant_with_min_max_vars` requires a min/max quantisation scale to be passed in as TF variables. This means that the scales can be updated during training. However, one could conceivably instead use `tf.quantization.fake_quant_with_min_max_args`, which is the same op with the exception that the min/max scales are passed in as constants rather than variables.\r\n\r\nAt the moment, attempting to convert an model that contains only `tf.quantization.fake_quant_with_min_max_args` ops does not work correctly, because the function `contains_training_quant_op` incorrectly returns false, which means that `is_training_time_int8_allow_float` returns false:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/2833b3d9457aa04de808578bbc0fa70ec136c63f/tensorflow/lite/python/lite.py#L249-L251\r\n\r\nThis PR fixes the problem by updating `contains_training_quant_op` to recognise `FakeQuantWithMinMaxArgs` and `FakeQuantWithMinMaxArgsPerChannel` as training-time quant ops.", "comments": ["@MeghnaNatraj could you review this PR request? Thanks"]}, {"number": 46703, "title": "[TFLM] Add headers and API for optimized kernels for CEVA-DSP BX1 and SP500", "body": "Added common headers and a scratch allocation file for CEVA platforms.\r\n\r\nThese are linked against an internal CEVA static lib containing all the optimized code. Since the library is not open source (or part of a third_party_download), we are currently adding the headers directly to the TFLM repository.\r\n\r\nThis is the same reason why these headers match the CEVA API and style (as opposed to the Google C++ style) and are not reviewed beyond looking at license compliance.\r\n\r\nIn the future, there might be value in pulling in the library and headers as a combined third_party_download. If that turns out to be useful, we would remove these headers from the TFLM repository.\r\n\r\nRelevant github issue: https://github.com/tensorflow/tensorflow/issues/45607\r\n\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "Even though we're ok with the headers not following the google-style, we will still need to clang-format the files."]}, {"number": 46701, "title": "floating point exception in tf.signal.stft ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n**Describe the current behavior**\r\nfloating point exception in `tf.signal.stft`\r\n\r\n**Describe the expected behavior**\r\nexpect no crash\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.signal.stft(signals=np.array([3.2031196e+38], dtype=np.float32), frame_step=1, frame_length=1, fft_length=False, pad_end=True)\r\n~~~\r\n\r\nOutput:\r\n~~~python\r\nFloating point exception (core dumped)\r\n~~~", "comments": ["@ymodak \r\nI ran the code on tf 2.4 and nightly the session crashes, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/5a87792308d2539e92c2e8abe772f182/untitled509.ipynb)", "Tested with TF 2.5, now the error is raised and session no longer crashes.\r\n```python\r\nInvalidArgumentError: Obtained a FFT shape of 0 elements: [1,0] [Op:RFFT]\r\n```", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46701\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46701\">No</a>\n"]}]