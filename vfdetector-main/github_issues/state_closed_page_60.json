[{"number": 53486, "title": "No way to unstack ragged tensors", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: NO\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: COLAB\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**: N/A\r\n-   **TensorFlow installed from (source or binary)**: N/A\r\n-   **TensorFlow version (use command below)**: latest COLAB\r\n-   **Python version**: latest COLAB\r\n-   **Bazel version (if compiling from source)**: N/A\r\n-   **GCC/Compiler version (if compiling from source)**: COLAB\r\n-   **CUDA/cuDNN version**: COLAB\r\n-   **GPU model and memory**: COLAB\r\n-   **Exact command to reproduce**:\r\n\r\n\r\n\r\n\r\n### Describe the problem\r\nI want to be able to unstack a ragged tensor so I can pass the unstacked tensors to a function with multiple inputs.\r\n\r\nHere is some example code of what I want to do.\r\n\r\n```\r\nimport tensorflow as tf\r\nX = tf.ragged.constant([[0, 1, 2], [0, 1]])\r\n\r\ndef sum_func(x1,x2):\r\n  return tf.reduce_sum(x1) + tf.reduce_sum(x2)\r\n\r\nsum_func(tf.unstack(X))\r\n```\r\n\r\n> ValueError: TypeError: object of type 'RaggedTensor' has no len()\r\n\r\nPlease note I know there are other ways to achieve the above with this function but this is just a minimal example", "comments": ["@ablanch5 \r\nRagged tensor do not have \"len\" defined for them because of their dynamic shape capabilities, hence tf.unstack  does not work with ragged tensors.\r\nKindly open this in discussion forum in case of further queries, and move this to closed status.", "Hey thanks for the reply! I realize the reason for the error but i was making a feature request. You're saying I should make features requests in discussion forum? Or that this is just not possible to be implemented?", "@ablanch5 \r\nBut its understood that with a none dimension you cant stack or unstack, it is like adding two tensors with none dimension, you cannot do that because you don't know if its broad-castable.\r\nas requested you can open this in discussion forum for more information and move this to closed status. If you still want a feature request please feel free to create a PR.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53486\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53486\">No</a>\n"]}, {"number": 53485, "title": "Saved model evaluate and validation always giving 0.50 accuracy despite reaching 96% on exact same data during training", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 21h2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):  binary\r\n- TensorFlow version (use command below):  v2.7.0-rc1-69-gc256c071bb2 2.7.0\r\n- Python version: 3.9.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.3/8.2.1\r\n- GPU model and memory: RTX 3080Ti 12GB\r\n\r\n**Describe the current behavior**\r\nWhen training a normal CNN model using the horse VS human dataset (binary image classification), the training accuracy reached 0.96+, but validation is forever stuck at 0.5. What's more interesting is after I load the saved model and called `model.evaluate` on the training set, it also gives 0.50, despite the training set being exactly same and was yielding 0.96+ during the actual training.\r\n\r\n**Describe the expected behavior**\r\nmodel to perform normally in both training and testing\r\n\r\nmodel weight: https://drive.google.com/file/d/11oqaGvy4vV0v77WmWzQxhtQSgEH4PUhA/view?usp=sharing\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\nimport tensorflow as tf\r\nimport urllib\r\nimport zipfile\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\nfrom tensorflow.keras.layers import *\r\n\r\n\r\ndef download_data():\r\n    _TRAIN_URL = \"https://storage.googleapis.com/download.tensorflow.org/data/horse-or-human.zip\"\r\n    _TEST_URL = \"https://storage.googleapis.com/download.tensorflow.org/data/validation-horse-or-human.zip\"\r\n    urllib.request.urlretrieve(_TRAIN_URL, 'horse-or-human.zip')\r\n    local_zip = 'horse-or-human.zip'\r\n    zip_ref = zipfile.ZipFile(local_zip, 'r')\r\n    zip_ref.extractall('tmp/horse-or-human/')\r\n    zip_ref.close()\r\n    urllib.request.urlretrieve(_TEST_URL, 'testdata.zip')\r\n    local_zip = 'testdata.zip'\r\n    zip_ref = zipfile.ZipFile(local_zip, 'r')\r\n    zip_ref.extractall('tmp/testdata/')\r\n    zip_ref.close()\r\n\r\n\r\ndef solution_model():\r\n    train_datagen = ImageDataGenerator(rescale=1. / 255,\r\n                                       rotation_range=40,\r\n                                       width_shift_range=0.2,\r\n                                       height_shift_range=0.2,\r\n                                       shear_range=0.2,\r\n                                       zoom_range=0.2,\r\n                                       horizontal_flip=True,\r\n                                       fill_mode='nearest')\r\n    test_datagen = ImageDataGenerator(rescale=1. / 255)\r\n\r\n    train_generator = train_datagen.flow_from_directory(\r\n        'tmp/horse-or-human',  # This is the source directory for training images\r\n        target_size=(300, 300),\r\n        batch_size=16,\r\n        class_mode='binary')\r\n\r\n    print('Validation data')\r\n    validation_generator = test_datagen.flow_from_directory(\r\n        'tmp/testdata',\r\n        target_size=(300, 300),\r\n        batch_size=32)\r\n\r\n    opt = tf.keras.optimizers.Adam(learning_rate=1e-3, epsilon=1e-6)\r\n    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\n    epoch = 20\r\n    batch_size = 16\r\n    callbacks = [\r\n        tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, patience=5, verbose=1,\r\n                                         mode='auto', baseline=None, restore_best_weights=True),\r\n        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.05, patience=5, verbose=1)\r\n    ]\r\n\r\n    xInput = Input([300, 300, 3])\r\n    x = tf.cast(xInput, tf.float32)\r\n    resnet50 = tf.keras.applications.efficientnet.EfficientNetB0(include_top=False, weights='imagenet')\r\n    x = resnet50(x)\r\n    x = Flatten()(x)\r\n    x = Dropout(0.2)(x)\r\n    x = Dense(256)(x)\r\n    x = BatchNormalization(epsilon=1.001e-5)(x)\r\n    x = Activation('relu')(x)\r\n    xOutput = Dense(1)(x)  # from logits so no need activation\r\n    model = tf.keras.models.Model(xInput, xOutput)\r\n\r\n    model.compile(optimizer=opt, loss=loss, metrics='accuracy')\r\n    model.summary()\r\n    model.fit(train_generator, validation_data=validation_generator, steps_per_epoch=train_generator.samples // batch_size, epochs=epoch, callbacks=callbacks, verbose=1)\r\n    return model\r\n\r\n\r\n\r\nif __name__ == '__main__':\r\n    # download_data()  # only need to run this once\r\n    model = solution_model()\r\n    model.save(\"mymodel.h5\")\r\n```\r\n\r\nEvaluation code:\r\n\r\n```\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\nfrom tensorflow.keras.models import load_model\r\n\r\ntest_datagen = ImageDataGenerator(rescale=1. / 255)\r\n\r\nprint('Validation data')\r\nvalidation_generator = test_datagen.flow_from_directory(\r\n    'tmp/horse-or-human',\r\n    target_size=(300, 300),\r\n    batch_size=32,\r\n    class_mode='categorical')\r\n\r\nmodel = load_model('mymodel.h5')\r\nmodel.summary()\r\nmodel.evaluate(validation_generator)  # should be >0.96\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n`33/33 [==============================] - 13s 125ms/step - loss: 0.7544 - accuracy: 0.5000`\r\n\r\nAs you can see, loss value is actually normal (it started training with 2+ loss, now only 0.75)\r\n", "comments": ["Hi @aliencaocao ! \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) \r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) . Thanks!", "Ok i have transferred it. Will close this now, thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53485\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53485\">No</a>\n"]}, {"number": 53484, "title": "[PluggableDevice] Export Grappler symbols for Windows' C API", "body": null, "comments": []}, {"number": 53483, "title": "Add FusedLayerNorm (Eigen and CUDA)", "body": "This PR adds FusedLayerNorm and FusedLayerNormGrad to the TF with Eigen and CUDA implementations for CPU and GPU respectively.\r\n\r\n\r\nFYI: @nluehr ", "comments": ["Could you explain the motivation for a new kernel for fused layer normalization? There is already a keras layer for LayerNormalization that uses the fused batch norm for implementation https://github.com/keras-team/keras/blob/v2.7.0/keras/layers/normalization/layer_normalization.py#L29-L363 ", "Yes, actually, the next step following this PR is to plumb in this fused layer norm to the keras layer.\r\n\r\nThe current keras LayerNormalization does a \"partially\" fused implementation, which takes advantage of fused_batch_norm for the normalization and then a scale/offset op. The reason why we cannot wrap up all the normalization+scale/offset to fused_batch_norm is because the layer norm's var/mean has different shapes with scale/offset, whereas the fused_batch_norm requires them to be the same.\r\n\r\nThis PR implements a fully fused layer norm and our plan is to call sth like `gen_ops.fused_layer_norm` underneath the keras LayerNormalization after this gets approved.", "@kaixih It will be great if you could add bfloat16 support for CPU. Eigen library supports several operations in bfloat16 numeric.", "I am wondering if we can add the bfloat16 support later as a separate PR. To support it in Eigen/CPU, it seems I need to add bfloat16 in `Attr(\"T: {half, float}\")`. But that means the GPU also need to deal with such inputs. I am not familiar how to convert bfloat16 to float. Does it support sth like `static_cast<float>` as in the half->float?", "@kaixih Thanks for the quick reply. Yes, we can do it as separate PR.", "Adding @fchollet from Keras to comment on this as well. In general, I'm a little hesitant to add to the existing very large op set that TF already supports but if we don't have a clear alternative and this unlocks better performance, we can do this. Do you have some performance numbers or potential impact (over the existing implementation) that could justify this better?", "@fchollet Can you please take a look on this PR ? Thanks!", "We are thinking to put the support of LayerNorm into the library, e.g. cuDNN in the future. Then, maybe we can \"hide\" the TF support path in the FusedBatchNormV3, meaning the FusedBatchNormV3 will be more a generic norm (like the nn.batch_normalization which can support any shapes/broadcast). So, at this point, we might want to put this PR on hold. How do you think?", "We plan to resort to the cuDNN library for this functionality. And the integration will probably follow https://github.com/tensorflow/tensorflow/pull/53483#issuecomment-1049261917. Will close this PR for now."]}, {"number": 53482, "title": "ImportError: DLL load failed while importing trace: The specified module could not be found.", "body": "OS Name\tMicrosoft Windows 10 Pro\r\nVersion\t10.0.19044 Build 19044\r\nOther OS Description \tNot Available\r\nOS Manufacturer\tMicrosoft Corporation\r\nSystem Name\tDESKTOP-045H3MF\r\nSystem Manufacturer\tLENOVO\r\nSystem Model\t80XL\r\nSystem Type\tx64-based PC\r\nSystem SKU\tLENOVO_MT_80XL_BU_idea_FM_ideapad 320-15IKB\r\nProcessor\tIntel(R) Core(TM) i5-7200U CPU @ 2.50GHz, 2712 Mhz, 2 Core(s), 4 Logical Processor(s)\r\nBIOS Version/Date\tLENOVO 4WCN47WW, 6/30/2020\r\nSMBIOS Version\t3.0\r\nEmbedded Controller Version\t1.47\r\nBIOS Mode\tUEFI\r\nBaseBoard Manufacturer\tLENOVO\r\nBaseBoard Product\tLNVNB161216\r\nBaseBoard Version\tNO DPK\r\nPlatform Role\tMobile\r\nSecure Boot State\tOff\r\nPCR7 Configuration\tElevation Required to View\r\nWindows Directory\tC:\\Windows\r\nSystem Directory\tC:\\Windows\\system32\r\nBoot Device\t\\Device\\HarddiskVolume1\r\nLocale\tUnited States\r\nHardware Abstraction Layer\tVersion = \"10.0.19041.1151\"\r\nUser Name\tDESKTOP-045H3MF\\LENOVO\r\nTime Zone\tBangladesh Standard Time\r\nInstalled Physical Memory (RAM)\t8.00 GB\r\nTotal Physical Memory\t7.88 GB\r\nAvailable Physical Memory\t1.69 GB\r\nTotal Virtual Memory\t14.9 GB\r\nAvailable Virtual Memory\t5.98 GB\r\nPage File Space\t7.00 GB\r\nPage File\tC:\\pagefile.sys\r\nKernel DMA Protection\tOff\r\nVirtualization-based security\tNot enabled\r\nDevice Encryption Support\tElevation Required to View\r\nHyper-V - VM Monitor Mode Extensions\tYes\r\nHyper-V - Second Level Address Translation Extensions\tYes\r\nHyper-V - Virtualization Enabled in Firmware\tYes\r\nHyper-V - Data Execution Protection\tYes\r\n###import requests\r\nimport cartopy.crs as ccrs \r\nimport matplotlib.pyplot as plt\r\nimport os\r\nfrom PIL import Image\r\nimport random\r\nimport pyttsx3\r\nengine = pyttsx3.init('sapi5')\r\nvoices = engine.getProperty('voices')\r\nengine.setProperty('voices',voices[0].id)\r\n\r\ndef Speak(audio):\r\n    print(\" \")\r\n    print(f\": {audio}\")\r\n    engine.say(audio)\r\n    engine.runAndWait()\r\n    print(\" \")\r\n\r\nApi_Key = \"FxwAGG1PWubblMdnFrHZAgHy0KdT86QPDRwLz2Is\"\r\n\r\ndef NasaNews(Date):\r\n\r\n    Speak(\"Extracting Data From Nasa . \")\r\n\r\n    Url = \"https://api.nasa.gov/planetary/apod?api_key=\" + str(Api_Key)\r\n\r\n    Params = {'date':str(Date)}\r\n    \r\n    r = requests.get(Url,params = Params)\r\n\r\n    Data = r.json()\r\n\r\n    Info = Data['explanation']\r\n\r\n    Title = Data['title']\r\n\r\n    Image_Url = Data['url']\r\n\r\n    Image_r = requests.get(Image_Url)\r\n\r\n    FileName = str(Date) + '.jpg'\r\n\r\n    with open(FileName,'wb') as f:\r\n\r\n        f.write(Image_r.content)\r\n\r\n    Path_1 = \"C:\\\\AI\\\\\" + str(FileName)\r\n\r\n    Path_2 = \"C:\\\\AI\\\\DataBase\\\\NasaDataBase\\\\\" + str(FileName)\r\n\r\n    os.rename(Path_1, Path_2)\r\n\r\n    img = Image.open(Path_2)\r\n\r\n    img.show()\r\n\r\n    Speak(f\"Title : {Title}\")\r\n    Speak(f\"According To Nasa : {Info}\")\r\n\r\ndef Summary(Boby):\r\n\r\n    list__ = ('2','3','4','5')\r\n\r\n    value = random.choice(list__)\r\n\r\n    path = \"C:\\\\AI\\\\DataBase\\\\NasaDataBase\\\\ImageUsed\\\\\" + str(value) + \".jpg\"\r\n\r\n    os.startfile(path)\r\n\r\n    name = str(Boby)\r\n\r\n    url = \"https://hubblesite.org/api/v3/glossary/\" + str(name)\r\n\r\n    r = requests.get(url)\r\n\r\n    Data = r.json()\r\n\r\n    if len(Data) != 0:\r\n        retur =  Data['definition']\r\n\r\n        Speak(f\"According To The Nasa : {retur}\")\r\n\r\n    else:\r\n\r\n        Speak(\"No Data Available , Try Again Later!\")\r\ndef MarsImage():\r\n\r\n    name = 'curiosity' \r\n\r\n    date = '2020-12-3'\r\n\r\n    Api_ = str(Api_Key)\r\n\r\n    url = f\"https://api.nasa.gov/mars-photos/api/v1/rovers/{name}/photos?earth_date={date}&api_key={Api_}\"\r\n    r = requests.get(url)\r\n\r\n    Data = r.json()\r\n\r\n    Photos = Data['photos'][:20]\r\n\r\n    try:\r\n\r\n        for index , photo in enumerate(Photos):\r\n\r\n            camera = photo['camera']\r\n\r\n            rover = photo['rover']\r\n\r\n            rover_name = rover['name']\r\n\r\n            camera_name = camera['name']\r\n\r\n            full_camera_name = camera['full_name']\r\n\r\n            date_of_photo = photo['earth_date']\r\n\r\n            img_url = photo['img_src']\r\n\r\n            p = requests.get(img_url)\r\n\r\n            img = f'{index}.jpg'\r\n\r\n            with open(img,'wb') as file:\r\n                file.write(p.content)\r\n\r\n            Path_1 = \"C:\\\\AI\\\\\" + str(img)\r\n\r\n            Path_2 = \"C:\\\\AI\\\\DataBase\\\\NasaDataBase\\\\MarsImage\\\\\" + str(img)\r\n\r\n            os.rename(Path_1,Path_2)\r\n\r\n            os.startfile(Path_2)\r\n\r\n            Speak(f\"This Image Was Captured With : {full_camera_name}\")\r\n\r\n            Speak(f\"This Image Was Captured On : {date_of_photo}\")\r\n\r\n    except:\r\n        Speak(\"There IS An Error!\")\r\ndef IssTracker():\r\n\r\n    url = \"http://api.open-notify.org/iss-now.json\"\r\n\r\n    r = requests.get(url)\r\n\r\n    Data = r.json()\r\n\r\n    dt = Data['timestamp']\r\n\r\n    lat = Data['iss_position']['latitude']\r\n\r\n    lon = Data['iss_position']['longitude']\r\n\r\n    print(f\"Time And Date : {dt}\")\r\n    print(f\"Latitude : {lat}\")\r\n    print(f\"Longitude : {lon}\")\r\n\r\n    plt.figure(figsize=(10,8))\r\n\r\n    ax = plt.axes(projection = ccrs.PlateCarree())\r\n\r\n    ax.stock_img()\r\n\r\n    plt.scatter(float(lon),float(lat),color = 'blue' , marker= 'o')\r\n\r\n    plt.show()\r\nIssTracker() \r\n   ###\r\n\r\nPS C:\\AI> pip install numpy\r\nCollecting numpy\r\n  Using cached numpy-1.21.4-cp310-cp310-win_amd64.whl (14.0 MB)\r\nInstalling collected packages: numpy\r\nSuccessfully installed numpy-1.21.4\r\nPS C:\\AI> & C:/Users/LENOVO/AppData/Local/Programs/Python/Python310/python.exe c:/AI/Nasa.py\r\nTraceback (most recent call last):\r\n  File \"c:\\AI\\Nasa.py\", line 2, in <module>\r\n    import cartopy.crs as ccrs\r\n  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\cartopy\\__init__.py\", line 109, in <module>\r\n    import cartopy.crs\r\n  File \"C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\cartopy\\crs.py\", line 26, in <module>\r\n    import cartopy.trace\r\nImportError: DLL load failed while importing trace: The specified module could not be found.\r\nPS C:\\AI> \r\n", "comments": ["@hasinonpc \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),please have a look at the similar[ issue1](https://github.com/tensorflow/tensorflow/issues/43192), [issue2](https://github.com/tensorflow/tensorflow/issues/35749).Let us know if it helps!\r\nThanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53481, "title": "[oneDNN] Keras LayerNormalization fusion with oneDNN CPU backend", "body": "- Explicitly cast `size_t` to `ptrdiff_t`\r\n- Cast `void*` to `char*` before doing pointer arithmetic. `void` pointer arithmetic is illegal in C++ and is a GCC extension. ", "comments": ["This arrived after #51903 but we rolled that back :(\r\n\r\nCan you merge this with a rollforward of #51903 please?", "@mihaimaruseac I reverted the rollback and reapplied my fixes.", "@PatriceVignola @mihaimaruseac I am from Intel tensorflow dev team , wanted thank you for fixing https://github.com/tensorflow/tensorflow/pull/51903.Looks like this PR is approved and hopefully will be merged, but let us know if need any help regarding https://github.com/tensorflow/tensorflow/pull/51903", "Hi. This should be merged in a few minutes, internal testing just finished"]}, {"number": 53480, "title": "[oneDNN] Enable Pad+Conv3D/_FusedConv3D fusion", "body": "This PR enables Pad+Conv3D and Pad+_FusedConv3D fusion and adds tests.", "comments": ["Hi @ezhulenev \r\nI see that the PR was approved but some internal tests are failing. Can you please share the failure?\r\nThanks!!"]}, {"number": 53479, "title": "[oneDNN] Remove is_filter_const attr from _FusedConv3D registeration", "body": "Removes the extra attr added to the _FusedConv3D registeration", "comments": []}, {"number": 53478, "title": "how to change model from default efficientnet_lit0 in tflite model maker image_classifier.create()", "body": "Hi,\r\n\r\nI wanted to know in tflite model maker, if i want to use a custom model and not the default model to train and quantize with custom data, how do i do that? image.classifier.create() seems to be using efficientnet_lite0 as default and i cannot seem to change it to any other model, say mobilenet_v2.\r\n\r\nthanks", "comments": ["@suyash-narain ,\r\nWe see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced] or if possible share a colab gist with the issue reported.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Also please take a look at this [link](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/image_classifier/create) which provides more information.It helps!", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53477, "title": "[ROCM] enable self_adjoint_eig op (#1511)", "body": "This pr enables self_adjoint_eig_v2_op on rocm. ", "comments": ["@gbaned can you rescan the CLA? I made sure my amd email was associated with the CLA. ", "> @gbaned can you rescan the CLA? I made sure my amd email was associated with the CLA.\r\n\r\n@stevenireeves  I can see you have signed CLA. Thank you. \r\n\r\n", "@micmelesse Can you please sign CLA. Thanks!", "@gbaned It says that I have signed it. I added my amd email just in case. Can you rescan it?", "@gbaned The CLA still appears to be failing on my email, although I updated the CLA to have both my personal and my work email. Is there a way fix this issue? ", "@micmelesse I have checked, still it shows you have not signed CLA.  Thank you!", "@gbaned I updated my username from a link to github to just the username. Can you try to rerun the cla again?", "@gbaned  both @micmelesse  and I have double checked our CLA and made sure that everything is as needed. On the details it shows that Michael's CLA is correct, but mine is not. I added my work email to the CLA associated with my github account. I think the CLA just needs to be rerun. Thanks! ", "> @gbaned both @micmelesse and I have double checked our CLA and made sure that everything is as needed. On the details it shows that Michael's CLA is correct, but mine is not. I added my work email to the CLA associated with my github account. I think the CLA just needs to be rerun. Thanks!\r\n\r\n@stevenireeves  Now, CLA looks good for both of you. Thank you.", "@gbaned thanks!", "> > @gbaned both @micmelesse and I have double checked our CLA and made sure that everything is as needed. On the details it shows that Michael's CLA is correct, but mine is not. I added my work email to the CLA associated with my github account. I think the CLA just needs to be rerun. Thanks!\r\n> \r\n> @stevenireeves Now, CLA looks good for both of you. Thank you.\r\n\r\nIf the question comes up again how to trigger rerunning the CLA check, I did it by clicking the Details link on the right of the cla/google line, and on that page there was a button Re-run all checks.", "Seems there is an issue with the AMD ROCm Community CI Build, can you please take a look?", "@akuegel The Community CI build uses rocm4.3.1, the features included in this PR were introduced in rocm4.5. @micmelesse will provide a solution to this. ", "The changes in this pr will be part of an upcoming pr from @stevenireeves that integrates hipsolver."]}, {"number": 53476, "title": "Performance regression for FP16 Relu kernel from TF2.6 to TF2.7", "body": "**Current behavior**\r\nMigrating from TF2.6 to TF2.7 FP16 Relu kernel has changed from EigenMetaKernel to MLIR. \r\nTested on Volta and Ampere, the memory throughput gets halved and compute throughput is also reduced. Collected by nsight compute. \r\n| Throughput %                    | compute | memory |\r\n|---------------------------------|---------|--------|\r\n| EigenMetaKernel                 | 35.79   | 84.20  | \r\n| Relu_GPU_DT_HALF_DT_HALF_kernel | 28.17   | 45.02  |\r\n\r\nA roughly 2x slowdown of the forward pass is observed. The better bandwidth utilization of EigenMetaKernel is contributed from wider vectors for GPU-half implementation for libeigen. \r\n\r\n", "comments": ["Hi @wenscarl ! Could you please provide a minimal stand alone code to reproduce this issue?", "```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras import mixed_precision\r\n\r\npolicy = mixed_precision.Policy('mixed_float16')\r\nmixed_precision.set_global_policy(policy)\r\n\r\ninputs = keras.Input(shape=(6400,), name='digits')\r\nnum_units = 8192\r\n\r\ndense1 = layers.Dense(num_units, activation='relu', name='dense_1')\r\nx = dense1(inputs)\r\ndense2 = layers.Dense(num_units, activation='relu', name='dense_2')\r\nx = dense2(x)\r\n\r\nx = layers.Dense(1000, name='dense_logits')(x)\r\noutputs = layers.Activation('relu', dtype='float32', name='predictions')(x)\r\n\r\nmodel = keras.Model(inputs=inputs, outputs=outputs)\r\nmodel.compile(loss='sparse_categorical_crossentropy',\r\n              optimizer=keras.optimizers.RMSprop(),\r\n              metrics=['accuracy'])\r\n\r\nx_train = x_train.reshape(60000, 6400).astype('float32') / 255\r\ny_train = np.random.randint(10, size=60000)\r\n\r\nhistory = model.fit(x_train, y_train,\r\n                    batch_size=8192,\r\n                    epochs=5,\r\n                    validation_split=0.2)\r\n```\r\nOn TF2.7, I collected nsight-compute profile by\r\n`ncu -k Relu_GPU_DT_HALF_DT_HALF_kernel --target-processes all -f -o repo_TF27  python repo.py`\r\nOn TF2.6,  `ncu -k EigenMetaKernel  --target-processes all -f -o repo_TF26  python repo.py.`.  For this example, the mem throughput is about 88%(TF2.7) VS 68%(TF2.6).  The demangled name for EigenMetaKernel is \r\n`\"void Eigen::internal::EigenMetaKernel<Eigen::TensorEvaluator<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<Eigen::half, (int)1, (int)1, long>, (int)16, Eigen::MakePointer>, const Eigen::TensorCwiseBinaryOp<Eigen::internal::scalar_max_op<const Eigen::half, const Eigen::half, (int)1>, const Eigen::TensorMap<Eigen::Tensor<const Eigen::half, (int)1, (int)1, long>, (int)16, Eigen::MakePointer>, const Eigen::TensorCwiseNullaryOp<Eigen::internal::scalar_constant_op<const Eigen::half>, const Eigen::TensorMap<Eigen::Tensor<const Eigen::half, (int)1, (int)1, long>, (int)16, Eigen::MakePointer>>>>, Eigen::GpuDevice>, long>(T1, T2`", "Hi @Saduf2019 ! Could you please look at this issue?", "Thanks for opening this issue. Development of keras moved to separate repository https://github.com/keras-team/keras/issues\r\n\r\nPlease post this issue on keras-team/keras repo.\r\nTo know more see;\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999\r\nThank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Posted on keras-team/keras repo as suggested, [here](https://github.com/keras-team/keras/issues/15857). Though the repo code is in keras, the root cause should relate to TF directly. ", "Closing this one as duplicate of https://github.com/keras-team/keras/issues/15857."]}, {"number": 53475, "title": "Remove references to HCC", "body": "https://github.com/tensorflow/tensorflow/blob/b56e6db5e7cccfffa824bbc1b5e018c6cc413c21/tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc#L739\r\n\r\nHCC was removed from ROCm in Summer 2020; references to it are likely cruft / removable.\r\n\r\nMark", "comments": ["@searlmc1 \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),\r\nThanks!", "The Pr created has been merged hence moving this to closed status."]}, {"number": 53474, "title": "Fixed a minor logging issue in fused batch norm", "body": "This PR fixed a minor logging issue in fused batch norm.", "comments": []}, {"number": 53473, "title": "Update oneDNN version for AArch64 builds", "body": "Builds on AArch64 with `--config=mkl_aarch64` will now pickup\r\noneDNN version 2.5 rather than 2.4.", "comments": ["HI @penpornk \r\n\r\nHere is the patch to update the oneDNN version for mkl_aarch64 builds."]}, {"number": 53472, "title": "error in running application and this error is showing", "body": "Traceback (most recent call last):\r\n  File \"E:\\New folder\\face_mask\\facemask1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"E:\\New folder\\face_mask\\app\\app.py\", line 7, in <module>\r\n    from deeplearning import face_mask_prediction\r\n  File \"E:\\New folder\\face_mask\\app\\deeplearning.py\", line 3, in <module>\r\n    import tensorflow as tf\r\n  File \"E:\\New folder\\face_mask\\facemask1\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"E:\\New folder\\face_mask\\facemask1\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n  File \"E:\\New folder\\face_mask\\facemask1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 79, in <module>\r\n    raise ImportError(\r\nImportError: Traceback (most recent call last):\r\n  File \"E:\\New folder\\face_mask\\facemask1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\r\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.", "comments": ["@NamanDubey03 ,\r\nCan you please take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156) from the issue with similar error.It helps.Also in order to expedite the trouble-shooting process, could you please provide the following information\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version:\r\nPython version:\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\n\r\nand the exact sequence of commands / steps that you executed before running into the problem.Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53472\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53472\">No</a>\n"]}, {"number": 53471, "title": "Can't install packages within docker image 2.7.0-gpu on Mac OS", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 12.0.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): docker (Docker version 20.10.10)\r\n- TensorFlow version: 2.7.0\r\n- Python version: from docker\r\n- Installed using virtualenv? pip? conda?: docker\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory:\r\n\r\n**Describe the problem**\r\n\r\nHello, I have issues when using the docker image `tensorflow/tensorflow:2.7.0-gpu` on Mac OS. I'm using this image even if I don't have a GPU on my local machine because I use the same Dockerfile on the Linux machines with GPUs I use remotely.  \r\n\r\nMy issue is that when using this docker image on my Mac, I have an error (see below) when trying to install a package with `apt`. This issue *doesn't happen on Mac* when using the image `tensorflow/tensorflow:2.6.0-gpu` or `tensorflow/tensorflow:2.7.0`, and it *doesn't happen on Debian GNU/Linux 10* when using the image `tensorflow/tensorflow:2.7.0-gpu`.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nYou can reproduce this issue on Mac with the following Dockerfile:\r\n\r\n```Dockerfile\r\nFROM tensorflow/tensorflow:2.7.0-gpu\r\n\r\nRUN apt-get update --fix-missing && apt-get install -y ssh\r\n```\r\n\r\nand doing `docker build .`\r\n\r\n**Any other info / logs**\r\n\r\n```bash\r\n$ docker build .\r\n[+] Building 10.8s (6/10)\r\n => [internal] load build definition from Dockerfile                                                                                                                                        0.1s\r\n => => transferring dockerfile: 839B                                                                                                                                                                   0.1s\r\n => [internal] load .dockerignore                                                                                                                                                                      0.0s\r\n => => transferring context: 35B                                                                                                                                                                       0.0s\r\n => [internal] load metadata for docker.io/tensorflow/tensorflow:2.7.0-gpu                                                                                                                             0.7s\r\n => [internal] load build context                                                                                                                                                                      0.0s\r\n => => transferring context: 234B                                                                                                                                                                      0.0s\r\n => CACHED [1/6] FROM docker.io/tensorflow/tensorflow:2.7.0-gpu@sha256:fc5eb0604722c7bef7b499bb007b3050c4beec5859c2e0d4409d2cca5c14d442                                                                0.0s\r\n => ERROR [2/6] RUN apt-get update --fix-missing && apt-get install -y ssh                                                                                                                             9.9s\r\n------\r\n > [2/6] RUN apt-get update --fix-missing && apt-get install -y ssh:\r\n#5 0.603 Hit:1 http://archive.ubuntu.com/ubuntu focal InRelease\r\n#5 0.624 Get:2 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\r\n#5 0.668 Get:3 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\r\n#5 0.681 Ign:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\r\n#5 0.775 Get:5 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\r\n#5 0.981 Get:6 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [797 kB]\r\n#5 1.293 Get:7 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [33.6 kB]\r\n#5 1.305 Get:8 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1108 kB]\r\n#5 1.371 Ign:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  InRelease\r\n#5 1.466 Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Release [696 B]\r\n#5 1.510 Get:11 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [828 kB]\r\n#5 1.559 Hit:12 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  Release\r\n#5 1.602 Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Release.gpg [836 B]\r\n#5 1.738 Get:14 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [1758 kB]\r\n#5 2.086 Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [517 kB]\r\n#5 2.394 Get:17 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [733 kB]\r\n#5 2.893 Get:18 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [21.7 kB]\r\n#5 2.908 Get:19 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [50.8 kB]\r\n#5 2.912 Get:20 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [1335 kB]\r\n#5 3.371 Get:21 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [30.1 kB]\r\n#5 3.471 Fetched 7550 kB in 3s (2502 kB/s)\r\n#5 3.471 Reading package lists...\r\n#5 4.458 Reading package lists...\r\n#5 5.473 Building dependency tree...\r\n#5 5.678 Reading state information...\r\n#5 5.923 The following additional packages will be installed:\r\n#5 5.924   libbsd0 libcbor0.6 libedit2 libfido2-1 libwrap0 libx11-6 libx11-data libxau6\r\n#5 5.925   libxcb1 libxdmcp6 libxext6 libxmuu1 ncurses-term openssh-client\r\n#5 5.926   openssh-server openssh-sftp-server python3-distro ssh-import-id ucf wget\r\n#5 5.926   xauth\r\n#5 5.928 Suggested packages:\r\n#5 5.928   keychain libpam-ssh monkeysphere ssh-askpass molly-guard ufw\r\n#5 5.982 The following NEW packages will be installed:\r\n#5 5.983   libbsd0 libcbor0.6 libedit2 libfido2-1 libwrap0 libx11-6 libx11-data libxau6\r\n#5 5.984   libxcb1 libxdmcp6 libxext6 libxmuu1 ncurses-term openssh-client\r\n#5 5.985   openssh-server openssh-sftp-server python3-distro ssh ssh-import-id ucf wget\r\n#5 5.985   xauth\r\n#5 6.086 0 upgraded, 22 newly installed, 0 to remove and 27 not upgraded.\r\n#5 6.086 Need to get 2841 kB of archives.\r\n#5 6.086 After this operation, 15.9 MB of additional disk space will be used.\r\n#5 6.086 Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 libbsd0 amd64 0.10.0-1 [45.4 kB]\r\n#5 6.201 Get:2 http://archive.ubuntu.com/ubuntu focal/main amd64 libedit2 amd64 3.1-20191231-1 [87.0 kB]\r\n#5 6.250 Get:3 http://archive.ubuntu.com/ubuntu focal/main amd64 libcbor0.6 amd64 0.6.0-0ubuntu1 [21.1 kB]\r\n#5 6.258 Get:4 http://archive.ubuntu.com/ubuntu focal/main amd64 libfido2-1 amd64 1.3.1-1ubuntu2 [47.9 kB]\r\n#5 6.276 Get:5 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 openssh-client amd64 1:8.2p1-4ubuntu0.3 [671 kB]\r\n#5 6.493 Get:6 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 openssh-sftp-server amd64 1:8.2p1-4ubuntu0.3 [51.5 kB]\r\n#5 6.518 Get:7 http://archive.ubuntu.com/ubuntu focal/main amd64 ucf all 3.0038+nmu1 [51.6 kB]\r\n#5 6.529 Get:8 http://archive.ubuntu.com/ubuntu focal/main amd64 libwrap0 amd64 7.6.q-30 [46.3 kB]\r\n#5 6.554 Get:9 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 openssh-server amd64 1:8.2p1-4ubuntu0.3 [377 kB]\r\n#5 6.630 Get:10 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 ssh all 1:8.2p1-4ubuntu0.3 [5080 B]\r\n#5 6.633 Get:11 http://archive.ubuntu.com/ubuntu focal/main amd64 libxau6 amd64 1:1.0.9-0ubuntu1 [7488 B]\r\n#5 6.635 Get:12 http://archive.ubuntu.com/ubuntu focal/main amd64 libxdmcp6 amd64 1:1.1.3-0ubuntu1 [10.6 kB]\r\n#5 6.638 Get:13 http://archive.ubuntu.com/ubuntu focal/main amd64 libxcb1 amd64 1.14-2 [44.7 kB]\r\n#5 6.652 Get:14 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libx11-data all 2:1.6.9-2ubuntu1.2 [113 kB]\r\n#5 6.689 Get:15 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libx11-6 amd64 2:1.6.9-2ubuntu1.2 [575 kB]\r\n#5 6.876 Get:16 http://archive.ubuntu.com/ubuntu focal/main amd64 libxext6 amd64 2:1.3.4-0ubuntu1 [29.1 kB]\r\n#5 6.886 Get:17 http://archive.ubuntu.com/ubuntu focal/main amd64 libxmuu1 amd64 2:1.1.3-0ubuntu1 [9728 B]\r\n#5 6.889 Get:18 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 wget amd64 1.20.3-1ubuntu2 [348 kB]\r\n#5 7.005 Get:19 http://archive.ubuntu.com/ubuntu focal/main amd64 xauth amd64 1:1.1-0ubuntu1 [25.0 kB]\r\n#5 7.018 Get:20 http://archive.ubuntu.com/ubuntu focal/main amd64 ncurses-term all 6.2-0ubuntu2 [249 kB]\r\n#5 7.093 Get:21 http://archive.ubuntu.com/ubuntu focal/main amd64 python3-distro all 1.4.0-1 [14.6 kB]\r\n#5 7.097 Get:22 http://archive.ubuntu.com/ubuntu focal/main amd64 ssh-import-id all 5.10-0ubuntu1 [10.0 kB]\r\n#5 7.214 debconf: Perl may be unconfigured (strict.pm did not return a true value at (eval 1) line 2.\r\n#5 7.214 BEGIN failed--compilation aborted at (eval 1) line 2.\r\n#5 7.214 ) -- aborting\r\n#5 7.279 Fetched 2841 kB in 1s (2498 kB/s)\r\n#5 7.315 Selecting previously unselected package libbsd0:amd64.\r\n(Reading database ... 5660 files and directories currently installed.)\r\n#5 7.345 Preparing to unpack .../00-libbsd0_0.10.0-1_amd64.deb ...\r\n#5 7.355 Unpacking libbsd0:amd64 (0.10.0-1) ...\r\n#5 7.410 Selecting previously unselected package libedit2:amd64.\r\n#5 7.416 Preparing to unpack .../01-libedit2_3.1-20191231-1_amd64.deb ...\r\n#5 7.423 Unpacking libedit2:amd64 (3.1-20191231-1) ...\r\n#5 7.480 Selecting previously unselected package libcbor0.6:amd64.\r\n#5 7.483 Preparing to unpack .../02-libcbor0.6_0.6.0-0ubuntu1_amd64.deb ...\r\n#5 7.489 Unpacking libcbor0.6:amd64 (0.6.0-0ubuntu1) ...\r\n#5 7.534 Selecting previously unselected package libfido2-1:amd64.\r\n#5 7.539 Preparing to unpack .../03-libfido2-1_1.3.1-1ubuntu2_amd64.deb ...\r\n#5 7.547 Unpacking libfido2-1:amd64 (1.3.1-1ubuntu2) ...\r\n#5 7.599 Selecting previously unselected package openssh-client.\r\n#5 7.603 Preparing to unpack .../04-openssh-client_1%3a8.2p1-4ubuntu0.3_amd64.deb ...\r\n#5 7.617 Unpacking openssh-client (1:8.2p1-4ubuntu0.3) ...\r\n#5 7.742 Selecting previously unselected package openssh-sftp-server.\r\n#5 7.745 Preparing to unpack .../05-openssh-sftp-server_1%3a8.2p1-4ubuntu0.3_amd64.deb ...\r\n#5 7.752 Unpacking openssh-sftp-server (1:8.2p1-4ubuntu0.3) ...\r\n#5 7.835 Selecting previously unselected package ucf.\r\n#5 7.838 Preparing to unpack .../06-ucf_3.0038+nmu1_all.deb ...\r\n#5 7.847 Moving old data out of the way\r\n#5 7.849 Unpacking ucf (3.0038+nmu1) ...\r\n#5 7.906 Selecting previously unselected package libwrap0:amd64.\r\n#5 7.909 Preparing to unpack .../07-libwrap0_7.6.q-30_amd64.deb ...\r\n#5 7.917 Unpacking libwrap0:amd64 (7.6.q-30) ...\r\n#5 7.967 Selecting previously unselected package openssh-server.\r\n#5 7.969 Preparing to unpack .../08-openssh-server_1%3a8.2p1-4ubuntu0.3_amd64.deb ...\r\n#5 7.996 Unpacking openssh-server (1:8.2p1-4ubuntu0.3) ...\r\n#5 8.083 Selecting previously unselected package ssh.\r\n#5 8.086 Preparing to unpack .../09-ssh_1%3a8.2p1-4ubuntu0.3_all.deb ...\r\n#5 8.097 Unpacking ssh (1:8.2p1-4ubuntu0.3) ...\r\n#5 8.136 Selecting previously unselected package libxau6:amd64.\r\n#5 8.139 Preparing to unpack .../10-libxau6_1%3a1.0.9-0ubuntu1_amd64.deb ...\r\n#5 8.147 Unpacking libxau6:amd64 (1:1.0.9-0ubuntu1) ...\r\n#5 8.189 Selecting previously unselected package libxdmcp6:amd64.\r\n#5 8.194 Preparing to unpack .../11-libxdmcp6_1%3a1.1.3-0ubuntu1_amd64.deb ...\r\n#5 8.200 Unpacking libxdmcp6:amd64 (1:1.1.3-0ubuntu1) ...\r\n#5 8.249 Selecting previously unselected package libxcb1:amd64.\r\n#5 8.252 Preparing to unpack .../12-libxcb1_1.14-2_amd64.deb ...\r\n#5 8.259 Unpacking libxcb1:amd64 (1.14-2) ...\r\n#5 8.295 Selecting previously unselected package libx11-data.\r\n#5 8.299 Preparing to unpack .../13-libx11-data_2%3a1.6.9-2ubuntu1.2_all.deb ...\r\n#5 8.305 Unpacking libx11-data (2:1.6.9-2ubuntu1.2) ...\r\n#5 8.398 Selecting previously unselected package libx11-6:amd64.\r\n#5 8.401 Preparing to unpack .../14-libx11-6_2%3a1.6.9-2ubuntu1.2_amd64.deb ...\r\n#5 8.405 Unpacking libx11-6:amd64 (2:1.6.9-2ubuntu1.2) ...\r\n#5 8.506 Selecting previously unselected package libxext6:amd64.\r\n#5 8.510 Preparing to unpack .../15-libxext6_2%3a1.3.4-0ubuntu1_amd64.deb ...\r\n#5 8.515 Unpacking libxext6:amd64 (2:1.3.4-0ubuntu1) ...\r\n#5 8.563 Selecting previously unselected package libxmuu1:amd64.\r\n#5 8.566 Preparing to unpack .../16-libxmuu1_2%3a1.1.3-0ubuntu1_amd64.deb ...\r\n#5 8.570 Unpacking libxmuu1:amd64 (2:1.1.3-0ubuntu1) ...\r\n#5 8.606 Selecting previously unselected package wget.\r\n#5 8.609 Preparing to unpack .../17-wget_1.20.3-1ubuntu2_amd64.deb ...\r\n#5 8.614 Unpacking wget (1.20.3-1ubuntu2) ...\r\n#5 8.675 Selecting previously unselected package xauth.\r\n#5 8.682 Preparing to unpack .../18-xauth_1%3a1.1-0ubuntu1_amd64.deb ...\r\n#5 8.686 Unpacking xauth (1:1.1-0ubuntu1) ...\r\n#5 8.729 Selecting previously unselected package ncurses-term.\r\n#5 8.734 Preparing to unpack .../19-ncurses-term_6.2-0ubuntu2_all.deb ...\r\n#5 8.739 Unpacking ncurses-term (6.2-0ubuntu2) ...\r\n#5 9.139 Selecting previously unselected package python3-distro.\r\n#5 9.144 Preparing to unpack .../20-python3-distro_1.4.0-1_all.deb ...\r\n#5 9.150 Unpacking python3-distro (1.4.0-1) ...\r\n#5 9.199 Selecting previously unselected package ssh-import-id.\r\n#5 9.203 Preparing to unpack .../21-ssh-import-id_5.10-0ubuntu1_all.deb ...\r\n#5 9.209 Unpacking ssh-import-id (5.10-0ubuntu1) ...\r\n#5 9.265 Setting up libxau6:amd64 (1:1.0.9-0ubuntu1) ...\r\n#5 9.281 Setting up python3-distro (1.4.0-1) ...\r\n#5 9.339 Traceback (most recent call last):\r\n#5 9.340   File \"/usr/bin/py3compile\", line 34, in <module>\r\n#5 9.340     from debpython.version import SUPPORTED, debsorted, vrepr, \\\r\n#5 9.341 ImportError: cannot import name 'SUPPORTED' from 'debpython.version' (/usr/share/python3/debpython/version.py)\r\n#5 9.347 dpkg: error processing package python3-distro (--configure):\r\n#5 9.347  installed python3-distro package post-installation script subprocess returned error exit status 1\r\n#5 9.347 Setting up wget (1.20.3-1ubuntu2) ...\r\n#5 9.366 dpkg: dependency problems prevent configuration of ssh-import-id:\r\n#5 9.366  ssh-import-id depends on python3-distro; however:\r\n#5 9.366   Package python3-distro is not configured yet.\r\n#5 9.366\r\n#5 9.366 dpkg: error processing package ssh-import-id (--configure):\r\n#5 9.366  dependency problems - leaving unconfigured\r\n#5 9.366 Setting up libcbor0.6:amd64 (0.6.0-0ubuntu1) ...\r\n#5 9.374 Setting up libwrap0:amd64 (7.6.q-30) ...\r\n#5 9.386 Setting up libx11-data (2:1.6.9-2ubuntu1.2) ...\r\n#5 9.395 Setting up ucf (3.0038+nmu1) ...\r\n#5 9.411 strict.pm did not return a true value at /usr/share/debconf/frontend line 5.\r\n#5 9.411 BEGIN failed--compilation aborted at /usr/share/debconf/frontend line 5.\r\n#5 9.413 dpkg: error processing package ucf (--configure):\r\n#5 9.413  installed ucf package post-installation script subprocess returned error exit status 255\r\n#5 9.413 Setting up libfido2-1:amd64 (1.3.1-1ubuntu2) ...\r\n#5 9.425 Setting up libbsd0:amd64 (0.10.0-1) ...\r\n#5 9.442 Setting up ncurses-term (6.2-0ubuntu2) ...\r\n#5 9.461 Setting up libxdmcp6:amd64 (1:1.1.3-0ubuntu1) ...\r\n#5 9.475 Setting up libxcb1:amd64 (1.14-2) ...\r\n#5 9.489 dpkg: dependency problems prevent configuration of openssh-server:\r\n#5 9.493  openssh-server depends on ucf (>= 0.28); however:\r\n#5 9.493   Package ucf is not configured yet.\r\n#5 9.493\r\n#5 9.493 dpkg: error processing package openssh-server (--configure):\r\n#5 9.495  dependency problems - leaving unconfigured\r\n#5 9.496 Setting up libedit2:amd64 (3.1-20191231-1) ...\r\n#5 9.511 Setting up libx11-6:amd64 (2:1.6.9-2ubuntu1.2) ...\r\n#5 9.526 Setting up libxmuu1:amd64 (2:1.1.3-0ubuntu1) ...\r\n#5 9.543 dpkg: dependency problems prevent configuration of ssh:\r\n#5 9.544  ssh depends on openssh-server (>= 1:8.2p1-4ubuntu0.3); however:\r\n#5 9.544   Package openssh-server is not configured yet.\r\n#5 9.546\r\n#5 9.546 dpkg: error processing package ssh (--configure):\r\n#5 9.547  dependency problems - leaving unconfigured\r\n#5 9.549 Setting up openssh-client (1:8.2p1-4ubuntu0.3) ...\r\n#5 9.588 warnings.pm did not return a true value at /usr/sbin/addgroup line 32.\r\n#5 9.588 BEGIN failed--compilation aborted at /usr/sbin/addgroup line 32.\r\n#5 9.590 dpkg: error processing package openssh-client (--configure):\r\n#5 9.590  installed openssh-client package post-installation script subprocess returned error exit status 255\r\n#5 9.591 Setting up libxext6:amd64 (2:1.3.4-0ubuntu1) ...\r\n#5 9.607 Setting up xauth (1:1.1-0ubuntu1) ...\r\n#5 9.620 dpkg: dependency problems prevent configuration of openssh-sftp-server:\r\n#5 9.620  openssh-sftp-server depends on openssh-client (= 1:8.2p1-4ubuntu0.3); however:\r\n#5 9.621   Package openssh-client is not configured yet.\r\n#5 9.622\r\n#5 9.622 dpkg: error processing package openssh-sftp-server (--configure):\r\n#5 9.623  dependency problems - leaving unconfigured\r\n#5 9.626 Processing triggers for libc-bin (2.31-0ubuntu9.2) ...\r\n#5 9.634 /sbin/ldconfig.real: /usr/local/cuda/targets/x86_64-linux/lib/libnvrtc-builtins.so.11.2.152 is not an ELF file - it has the wrong magic bytes at the start.\r\n#5 9.635\r\n#5 9.638 /sbin/ldconfig.real: /usr/local/cuda/targets/x86_64-linux/lib/libnvrtc.so.11.2.152 is not an ELF file - it has the wrong magic bytes at the start.\r\n#5 9.640\r\n#5 9.643 /sbin/ldconfig.real: /usr/local/cuda/targets/x86_64-linux/lib/libnvrtc-builtins.so.11.2 is not an ELF file - it has the wrong magic bytes at the start.\r\n#5 9.643\r\n#5 9.645 /sbin/ldconfig.real: /usr/local/cuda/targets/x86_64-linux/lib/libnvrtc.so.11.2 is not an ELF file - it has the wrong magic bytes at the start.\r\n#5 9.647\r\n#5 9.663 /sbin/ldconfig.real: File /usr/local/cuda/lib64/stubs/libcuda.so is empty, not checked.\r\n#5 9.664 /sbin/ldconfig.real: File /usr/local/cuda/lib64/stubs/libcuda.so.1 is empty, not checked.\r\n#5 9.685 Errors were encountered while processing:\r\n#5 9.685  python3-distro\r\n#5 9.685  ssh-import-id\r\n#5 9.685  ucf\r\n#5 9.685  openssh-server\r\n#5 9.685  ssh\r\n#5 9.685  openssh-client\r\n#5 9.685  openssh-sftp-server\r\n#5 9.705 E: Sub-process /usr/bin/dpkg returned an error code (1)\r\n------\r\nexecutor failed running [/bin/bash -c apt-get update --fix-missing && apt-get install -y ssh]: exit code: 100\r\n```\r\n", "comments": ["Hi @Saduf2019 ! Could you please look at this issue?", "Just built the Dockerfile you provided @anth2o, it seems that this issue didn't occur on my side.\r\nFor information I am using Mac OS 10.14.6.", "@anth2o \r\nCould you please verify if you have followed all steps, if possible please try on tf 2.6 and let us know.", "What do you mean by \"followed all steps\" ? The minimal example is just running `docker build .` with the Dockerfile I showed in the description  of the issue.  \r\nAlso, this issue doesn't happen when using tf2.6-gpu neither with tf2.7", "@Saduf2019 I tried reproducing the issue after cleaning docker (`brew remove docker`), reinstalling Docker Desktop and cleaning all my docker cache (`docker system prune -a`), the issue didn't happen again.  \r\nMaybe the `tensorflow/tensorflow:2.7.0-gpu` image I downloaded at first was corrupted, but I find it strange that docker didn't catch it.  \r\nThanks for your answer", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53471\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53471\">No</a>\n", "@anth2o \r\nThank you for your update, glad the issue is resolved."]}, {"number": 53470, "title": "EinsumDense layer ignores `global_dtype_policy` when imported in a certain way", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 11.5.1\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): `2.6.2`\r\n- Python version: `3.8`\r\n\r\n**Describe the current behavior**\r\n\r\nI tried using the `EinsumDense` layer in a `mixed_float16` training and it seems like that the way you import this layer is very crucial, as it tends to ignore the `dtype_policy` when I import it with `from tensorflow.python.keras.layers.einsum_dense import EinsumDense`.\r\n\r\n**Describe the expected behavior**\r\nAll layers should use the `global_dtype_policy`, regardless of the way they are imported.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import mixed_precision\r\nfrom tensorflow.keras.layers.experimental import EinsumDense as EinsumDense1\r\nfrom tensorflow.python.keras.layers.einsum_dense import EinsumDense as EinsumDense2\r\nfrom keras.layers.einsum_dense import EinsumDense as EinsumDense3\r\n\r\npolicy = mixed_precision.Policy(\"mixed_float16\")\r\nmixed_precision.set_global_policy(policy)\r\n\r\nlayer1 = EinsumDense1(\"abc,cd->abd\",\r\n                      output_shape=(None, 64),\r\n                      bias_axes=\"d\")\r\n\r\nlayer2 = EinsumDense2(\"abc,cd->abd\",\r\n                      output_shape=(None, 64),\r\n                      bias_axes=\"d\")\r\n\r\nlayer3 = EinsumDense3(\"abc,cd->abd\",\r\n                      output_shape=(None, 64),\r\n                      bias_axes=\"d\")\r\n\r\nlayer4 = tf.keras.layers.experimental.EinsumDense(\"abc,cd->abd\",\r\n                      output_shape=(None, 64),\r\n                      bias_axes=\"d\")\r\n\r\ninput_tensor = tf.keras.Input(shape=[32, 128])\r\n\r\nprint('from tensorflow.keras.layers.experimental import EinsumDense')\r\nprint(layer1(input_tensor))\r\nprint(layer1.dtype_policy) # float16\r\nprint('------------------------------------------------------------')\r\n\r\nprint('from tensorflow.python.keras.layers.einsum_dense import EinsumDense')\r\nprint(layer2(input_tensor))\r\nprint(layer2.dtype_policy) # float32\r\n\r\nprint('------------------------------------------------------------')\r\n\r\nprint('from keras.layers.einsum_dense import EinsumDense')\r\nprint(layer3(input_tensor))\r\nprint(layer3.dtype_policy) # float16\r\nprint('------------------------------------------------------------')\r\n\r\nprint('from keras.layers.einsum_dense import EinsumDense')\r\nprint(layer4(input_tensor))\r\nprint(layer4.dtype_policy) # float16\r\n```\r\n", "comments": ["@gcuder \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "https://github.com/keras-team/keras/issues/15806", "@gcuder Thank you for the update! \r\nAs per the [comment](https://github.com/keras-team/keras/issues/15806#issuecomment-1000457994)  , tensorflow.python.keras is now deprecated and will be removed at any point. Also, importing python modules: should be avoided &  tf.keras.* should be used .It is working as intended .\r\nCould you please move this issue to closed status as it is resolved?\r\nThank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53470\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53470\">No</a>\n"]}, {"number": 53468, "title": "[tflite] fix coreml delegate tmp peak memory issue", "body": "fix https://github.com/tensorflow/tensorflow/issues/53461\r\n\r\nIt seems some memory management behaviors changed in Xcode 13 or later so that we need a `@autorelease` block to reduce peak memory usage.", "comments": []}, {"number": 53467, "title": "MacOS: Problem getting numpy include path.", "body": "<em>Please make sure that this is a bug. As per our\r\n**Describe the current behavior**\r\nI am able to build on my development Mac. However, running on the CI Mac node, we get an error that looks like this:\r\n\r\n```\r\n2021-12-17T01:27:56.6979400Z Analyzing: target //tensorflow/lite/ios:TensorFlowLiteC_static_framework (1 packages loaded, 0 targets configured)\r\n2021-12-17T01:27:58.6232140Z Analyzing: target //tensorflow/lite/ios:TensorFlowLiteC_static_framework (21 packages loaded, 10 targets configured)\r\n2021-12-17T01:27:58.8729360Z INFO: Repository local_execution_config_python instantiated at:\r\n2021-12-17T01:27:58.8731780Z   /Users/runner2/.conan/data/tensorflow-lite/2.5.0.18+5089e313/ci/branch_feat_2.7.0/build/3b95326f653baf403ae4d70dae74814d9c4fef8a/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n2021-12-17T01:27:58.8734480Z   /Users/runner2/.conan/data/tensorflow-lite/2.5.0.18+5089e313/ci/branch_feat_2.7.0/build/3b95326f653baf403ae4d70dae74814d9c4fef8a/tensorflow/tensorflow/workspace2.bzl:1108:19: in workspace\r\n2021-12-17T01:27:58.8737750Z   /Users/runner2/.conan/data/tensorflow-lite/2.5.0.18+5089e313/ci/branch_feat_2.7.0/build/3b95326f653baf403ae4d70dae74814d9c4fef8a/tensorflow/tensorflow/workspace2.bzl:84:27: in _tf_toolchains\r\n2021-12-17T01:27:58.8739200Z   /private/var/tmp/_bazel_runner2/14b6f7e45d646cdf7b1c6e3a405fb4ce/external/tf_toolchains/toolchains/remote_config/configs.bzl:6:28: in initialize_rbe_configs\r\n2021-12-17T01:27:58.8740200Z   /private/var/tmp/_bazel_runner2/14b6f7e45d646cdf7b1c6e3a405fb4ce/external/tf_toolchains/toolchains/remote_config/rbe_config.bzl:158:27: in _tensorflow_local_config\r\n2021-12-17T01:27:58.8740920Z Repository rule local_python_configure defined at:\r\n2021-12-17T01:27:58.8742180Z   /Users/runner2/.conan/data/tensorflow-lite/2.5.0.18+5089e313/ci/branch_feat_2.7.0/build/3b95326f653baf403ae4d70dae74814d9c4fef8a/tensorflow/third_party/py/python_configure.bzl:275:41: in <toplevel>\r\n2021-12-17T01:27:58.8812530Z INFO: Repository local_config_python instantiated at:\r\n2021-12-17T01:27:58.8814680Z   /Users/runner2/.conan/data/tensorflow-lite/2.5.0.18+5089e313/ci/branch_feat_2.7.0/build/3b95326f653baf403ae4d70dae74814d9c4fef8a/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n2021-12-17T01:27:58.8816660Z   /Users/runner2/.conan/data/tensorflow-lite/2.5.0.18+5089e313/ci/branch_feat_2.7.0/build/3b95326f653baf403ae4d70dae74814d9c4fef8a/tensorflow/tensorflow/workspace2.bzl:1108:19: in workspace\r\n2021-12-17T01:27:58.8818170Z   /Users/runner2/.conan/data/tensorflow-lite/2.5.0.18+5089e313/ci/branch_feat_2.7.0/build/3b95326f653baf403ae4d70dae74814d9c4fef8a/tensorflow/tensorflow/workspace2.bzl:94:21: in _tf_toolchains\r\n2021-12-17T01:27:58.8818980Z Repository rule python_configure defined at:\r\n2021-12-17T01:27:58.8820110Z   /Users/runner2/.conan/data/tensorflow-lite/2.5.0.18+5089e313/ci/branch_feat_2.7.0/build/3b95326f653baf403ae4d70dae74814d9c4fef8a/tensorflow/third_party/py/python_configure.bzl:294:35: in <toplevel>\r\n2021-12-17T01:27:58.8864240Z ERROR: An error occurred during the fetch of repository 'local_execution_config_python':\r\n2021-12-17T01:27:58.8865240Z    Traceback (most recent call last):\r\n2021-12-17T01:27:58.8867660Z \tFile \"/Users/runner2/.conan/data/tensorflow-lite/2.5.0.18+5089e313/ci/branch_feat_2.7.0/build/3b95326f653baf403ae4d70dae74814d9c4fef8a/tensorflow/third_party/py/python_configure.bzl\", line 213, column 39, in _create_local_python_repository\r\n2021-12-17T01:27:58.8869530Z \t\tnumpy_include = _get_numpy_include(repository_ctx, python_bin) + \"/numpy\"\r\n2021-12-17T01:27:58.8870840Z \tFile \"/Users/runner2/.conan/data/tensorflow-lite/2.5.0.18+5089e313/ci/branch_feat_2.7.0/build/3b95326f653baf403ae4d70dae74814d9c4fef8a/tensorflow/third_party/py/python_configure.bzl\", line 187, column 19, in _get_numpy_include\r\n2021-12-17T01:27:58.8871650Z \t\treturn execute(\r\n2021-12-17T01:27:58.8872840Z \tFile \"/Users/runner2/.conan/data/tensorflow-lite/2.5.0.18+5089e313/ci/branch_feat_2.7.0/build/3b95326f653baf403ae4d70dae74814d9c4fef8a/tensorflow/third_party/remote_config/common.bzl\", line 219, column 13, in execute\r\n2021-12-17T01:27:58.8873970Z \t\tfail(\r\n2021-12-17T01:27:58.8874300Z Error in fail: Problem getting numpy include path.\r\n2021-12-17T01:27:58.8874970Z Traceback (most recent call last):\r\n2021-12-17T01:27:58.8875640Z   File \"<string>\", line 1, in <module>\r\n2021-12-17T01:27:58.8876820Z ModuleNotFoundError: No module named 'numpy'\r\n2021-12-17T01:27:58.8877460Z Is numpy installed?\r\n```\r\n\r\nThe only difference is, CI node uses Homebrew python 3.8, while the dev system is using system python. \r\nI've tried using venv on both machines, which doesn't change the outcome. \r\nI've also attempted setting `PYTHON_BIN_PATH` and `PYTHON_LIB_PATH` (for both venv and direct Homebrew python scenario) with no luck.\r\n\r\n**Describe the expected behavior**\r\nI'd expect it to build no matter which python is being used.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@w3sip ,\r\nCan you please take a look at this SO link [1](https://stackoverflow.com/questions/60453261/how-to-default-python3-8-on-my-mac-using-homebrew),[2](https://stackoverflow.com/questions/60435566/python-official-newest-version-homebrew-version-and-installed-version-differ-on) and [issue](https://github.com/tensorflow/tensorflow/issues/51352) with the similar error.It helps.Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53467\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53467\">No</a>\n"]}, {"number": 53466, "title": "tf.debugging.assert_type throws error when checking type of RaggedTensor", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04.6 LTS**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device: **N/A**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **v2.7.0-rc1-69-gc256c071bb2 2.7.0**\r\n- Python version: **3.9.0**\r\n- Bazel version (if compiling from source): **N/A**\r\n- GCC/Compiler version (if compiling from source): **N/A**\r\n- CUDA/cuDNN version: **N/A**\r\n- GPU model and memory: **N/A**\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen calling `tf.debugging.assert_type` on a `RaggedTensor`, a `ValueError: TypeError: object of type 'RaggedTensor' has no len()` is thrown.\r\n\r\n**Describe the expected behavior**\r\nThe assertion check should pass if the ragged tensor is the correct type or fail otherwise.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): **no**\r\n- Briefly describe your candidate solution(if contributing): **N/A**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\na = tf.ragged.constant([[1, 2], [1]])\r\ntf.debugging.assert_type(a, tf_type=tf.int32)  # Should pass\r\n>>> ValueError: TypeError: object of type 'RaggedTensor' has no len()\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n**Workaround**\r\n```\r\nimport tensorflow as tf\r\n\r\na = tf.ragged.constant([[1, 2], [1]])\r\nassert a.dtype == tf.int32\r\n```", "comments": ["Is there a larger issue capturing methods that have not been updated to support ragged tensors? ", "Hi @abhmul ! According to this [document](https://www.tensorflow.org/api_docs/python/tf/debugging/assert_type#args), Supported tensor types are -A\u00a0Tensor,\u00a0SparseTensor\u00a0or\u00a0tf.Variable\u00a0. Attaching resolved [gist](https://colab.sandbox.google.com/gist/mohantym/08e1b9b15481d5281ccd5004f8ec1338/github_53466.ipynb#scrollTo=l4DULmprhfKW) for reference. Thanks!\r\n\r\n\r\n", "Could we make this a feature reqeust? I don't see the option for me to change the labels.", "@chunduriv ! Could you please look at this feature request?", "@abhmul I think this was resolved in recent `tf-nightly` (2.9.0-dev20220308). [Here](https://colab.research.google.com/gist/jvishnuvardhan/f666cbed13a93cdf656f39abf9d204fc/untitled1180.ipynb) is a gist for reference. Thanks\r\n\r\nI am closing this issue as this was resolved. Please feel free to reopen if the issue persists again. Thanks!"]}, {"number": 53465, "title": "[determinism] Add v2.8 release notes", "body": "Add op-determinism changes to the version 2.8 release notes in the master branch (prior to the r2.8 branch cut).\r\n\r\n@reedwm, please will you review. This is not much different than the notes for version 2.7. I'm wondering if there any additional enhancements in version 2.8 that I'm not yet aware of.", "comments": ["Yesterday I added a brief section in the release notes, in d5b368107de08b6b55b6685b4eec72dab1b8e677, but it doesn't list the 2.7 changes. I don't think it is worth listing the 2.7 changes for TF 2.8 despite having not listing them in the TF 2.7 release, since there are no known ops that are nondeterministic and do not raise an error. So users don't need to be aware of which ops are nondeterministic by reading the release notes, because they all should be deterministic. Granted, there are almost certainly bugs that still will cause nondeterminism, but hopefully such bugs will be rare.\r\n\r\nI have a small XLA change that makes XLA aware of the TF global determinism variable, which will allow determinism to work correctly when `tf.function(jit_compile=True)` is used. I'm hoping it will be submitted within an hour of now, by the branch cut, and I'll let you know when it is submitted. I also added exceptions to some FakeQuant ops in ae1581faa8d7f24d7974c4021394bc559ed6110b. Unfortunately, my changes to make tf.data faster with determinism haven't landed yet.\r\n\r\nAnyway, I'm happy to use your wording of the release notes, or keep my wording, but I don't think we should list 2.7 changes. It does make sense to mention the 2.8 changes though, so feel free to add those in, whether we go with my wording or yours.", "> I'm hoping it will be submitted within an hour of now, by the branch cut, and I'll let you know when it is submitted.\r\n\r\nActually looks like the change already landed 5 minutes ago, in 4dc6d4d59008b4558040afa1a5a5993f583bb48e.", "@reedwm, I'm sorry I missed your addition to the release notes.\r\n\r\nI have merged your change with mine in this PR. I have also added notes about the two changes you listed above. Thank you for sharing. I put it in the \"Major Features and Improvements\" section because it's a new, official API feature.\r\n\r\nWhen possible, I would like to document the incremental changes for this functionality in the release notes. For example, some users are likely going to care, going forward, when an exception is replaced with an implementation. Also, if it makes sense to include the changes that are 2.8-specific then why not include the changes that were not reported for the 2.7 release? I tend to err on the side of being detail-oriented about tracking and reporting these changes because I'm trying to make this functionality as explicit and thoroughly documented as possible (for those who care), to minimize potential confusion.", "Agreed we should document incremental changes. I was a bit reluctant to include 2.7 changes in the 2.8 release notes, but you're right that this makes sense since we didn't report them in 2.7.\r\n\r\nI pushed a commit to this PR that keeps the `enable_op_determinism` function in the \"Major Features\" section but moves the individual op changes to the \"Bug Fixes and Other Changes\" section. Since determinism was previously inaccessible through a public API, the entirety of the determinism work can be considered a major new feature, so it's not worth listing the individual determinism changes in that section IMO.", "We cut the branch for TF 2.8 now. Can you rebase this on r2.8 branch?\r\n\r\nApologies for the conflict, but if we keep this against master then the changes will be missed after the release when we merge the release notes from the branch back into master.", "I think the PR author may be offline until January, and I'm not sure if it's possible for me to rebase a PR that I didn't author (although I can create commits). Can I create a cherrypick into 2.8 instead?\r\n\r\nIf necessary, I can create a new PR as well.", "Yes, let's land in master and then do a cherrypick. Sorry it needs extra work, I should have checked these a few days ago", "Thank you, @reedwm. Nice improvements. Also, thanks for making that cherry-pick happen. Note that some of the indentation got lost somewhere, which PR [53720](https://github.com/tensorflow/tensorflow/pull/53720) addresses."]}, {"number": 53463, "title": "Cannot create hexagon delegate on my tablet", "body": "Hello I'm trying to create a dsp/hexagon delegate on my android tablet, but for some reason ioctrl fails as reported on device log\r\nAny help!\r\nRegards,\r\nWalter\r\n\r\nRelevant C++ code:\r\n\r\n//TfLiteHexagonInit(); // also desnt works\r\nTfLiteHexagonInitWithPath(\"dsp;/data/dsp;/system/vendor/lib/rfsa/adsp\");\r\nTfLiteHexagonDelegateOptions hexagon_opts = {0};\r\nTfLiteDelegate* hexagonDelegate = TfLiteHexagonDelegateCreate(&hexagon_opts)  // fails here\r\n\r\nLog:\r\n\r\nI/tflite: Initialized TensorFlow Lite runtime.\r\nE/adsprpc: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:688::error: -1: 0 == ioctl(dev, FASTRPC_IOCTL_INIT, (unsigned long)&init)\r\nE/adsprpc: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:764::error: -1: !apps_dev_init(domain)\r\nE/adsprpc: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:431::error: -1: -1 != open_dev(domain)\r\nE/adsprpc: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:688::error: -1: 0 == ioctl(dev, FASTRPC_IOCTL_INIT, (unsigned long)&init)\r\nE/adsprpc: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:764::error: -1: !apps_dev_init(domain)\r\nE/adsprpc: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:329::error: -1: -1 != (dev = open_dev(domain))\r\nW/tflite: Failed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide.\r\nI/tflite: Hexagon Delegate is not supported.\r\n\r\ncat /proc/cpuinfo | grep Hardware\r\nHardware        : Qualcomm Technologies, Inc APQ8096\r\n\r\nadb shell getprop ro.product.device\r\nmsm8996\r\n\r\nadb shell getprop ro.board.platform\r\nmsm8996\r\n\r\nUsing Hexagon libraries v1.20.0.1 downladed from here https://www.tensorflow.org/lite/performance/hexagon_delegate\r\n\r\nBenchmark app throws this log:\r\n\r\nSTARTING!\r\nLog parameter values verbosely: [0]\r\nGraph: [mobilenet_v1_0.25_192_quant.tflite]\r\nUse Hexagon: [1]\r\nLoaded model mobilenet_v1_0.25_192_quant.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nloaded libadsprpc.so\r\nFunc remote_handle_control not available on this device (NULL).\r\nWARNING: Failed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide.\r\nINFO: Hexagon Delegate is not supported.\r\n\r\nCould not create Hexagon delegate: platform may not support delegate or required libraries are missing\r\nThe input model file size (MB): 0.497264\r\nInitialized session in 285.927ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=68 first=17405 curr=6455 min=6361 max=17405 avg=7354.15 std=2021\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=143 first=6432 curr=6750 min=6371 max=12065 avg=6967.24 std=676\r\n\r\nInference timings in us: Init: 285927, First inference: 17405, Warmup (avg): 7354.15, Inference (avg): 6967.24\r\nNote: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\nMemory footprint delta from the start of the tool (MB): init=0.972656 overall=2.53125", "comments": ["@walex ,\r\n In order to expedite the trouble-shooting process, could you please provide a complete code and the TensorFlow version you are using.\r\n", "Also please take a look at this [issue](https://github.com/tensorflow/tensorflow/issues/39024) with the similar error.It helps.Thanks", "Hello, this is the complete code: (Its a big system so I remove some no needed parts)\r\n\r\n\r\n// C++\r\n\r\n jlong JNICALL Java_com_dsense_PersonScannerWrapper_nativeInit(JNIEnv* env, jobject thiz , jstring modelPath) {\r\n\r\n\r\n\tconst char* pModelPath = env->GetStringUTFChars(modelPath, 0);\r\n\t\r\n\t// load model\r\n\tTfLiteModel* model = TfLiteModelCreateFromFile(pModelPath);\r\n\r\n\t// create interpreter\r\n\tTfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();\r\n\tTfLiteInterpreterOptionsSetNumThreads(options, 1);\r\n\tTfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model , options);\r\n\tTfLiteInterpreterOptionsDelete(options);\r\n\r\n\t// release model\r\n\tTfLiteModelDelete(model);\r\n\r\n\tenv->ReleaseStringUTFChars(modelPath, pModelPath);\r\n\t\r\n\treturn reinterpret_cast<jlong>(interpreter);\r\n}\r\n\r\n// JAVA\r\n\r\npublic class PersonScannerWrapper {\r\n\r\n    static\r\n    {\r\n        System.loadLibrary(\"tensorflowlite_jni\");\r\n        System.loadLibrary(\"tensorflowlite_hexagon_jni\");\r\n        System.loadLibrary(\"PersonScanner\");\r\n    }\r\n\r\n    private static native long nativeInit(String modelPath);\r\n  \r\n\r\n    private long mHandle = 0;\r\n    public PersonScannerWrapper(String modelPath) throws Exception {\r\n\r\n\r\n        long handle = PersonScannerWrapper.nativeInit(modelPath);\r\n        if (handle == 0) {\r\n\r\n            throw new Exception();\r\n        }\r\n\r\n        this.mHandle = handle;\r\n    }\r\n}\r\n\r\npublic class MainActivity extends AppCompatActivity {\r\n\r\n    static String modelName = \"mobilenet_v1_0.25_192_quant.tflite\";\r\n    PersonScannerWrapper mScanner = null;\r\n    AssetsUtils mAssetsUtils = null;\r\n\r\n    @Override\r\n    protected void onCreate(Bundle savedInstanceState) {\r\n\r\n      \r\n        super.onCreate(savedInstanceState);\r\n        setContentView(R.layout.activity_main);\r\n        Toolbar toolbar = findViewById(R.id.toolbar);\r\n        setSupportActionBar(toolbar);\r\n\r\n\t\t// copy model to android device local storage\r\n        String modelPathName = this.getFilesDir().getAbsolutePath() + modelName;\r\n        mAssetsUtils = new AssetsUtils(this);\r\n        mAssetsUtils.copyFile(modelName, modelPathName);\r\n\r\n   \r\n    }\r\n\r\n\r\n    @Override\r\n    protected void onResume() {\r\n\r\n        try {\r\n            String modelPathName = this.getFilesDir().getAbsolutePath() + modelName;\r\n            mScanner = new PersonScannerWrapper(modelPathName);\r\n        } catch (Exception e) {\r\n            e.printStackTrace();\r\n            mScanner = null;\r\n        }\r\n        super.onResume();\r\n    }\r\n\r\n    @Override\r\n    protected void onPause() {\r\n\r\n        super.onPause();\r\n    }\r\n}\r\n\r\n\r\n\r\n", "Please follow [this](https://www.tensorflow.org/lite/performance/hexagon_delegate) document and make sure you are using the  hexagon_nn libraries with the [compatible version](https://www.tensorflow.org/lite/performance/hexagon_delegate#step_2_add_hexagon_libraries_to_your_android_app_2) of interface library.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53462, "title": "tensorflow-cpu install from conda-forge fails with 'requires grpc-cpp >=1.39.1,<1.40.0a0, but none of the providers can be installed'", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04.3 LTS\r\n- TensorFlow installed from (source or binary): binary, `tensorflow-cpu` latest as of 12 Dec 2021\r\n- TensorFlow version: latest (should install 2.6.2, installs 2.6.0 due to existing packages)\r\n- Python version: 3.9.7\r\n- Installed using virtualenv? pip? conda?: `mamba` `/ conda`\r\n- GPU model and memory: none, CPU only\r\n\r\n**Describe the problem**\r\n\r\nRun following Docker container:\r\n\r\n```\r\ndocker container run -it --rm --entrypoint=bash --user=0 \\\r\n  jupyter/all-spark-notebook\r\n```\r\n\r\nRun these commands:\r\n\r\n```\r\ncat << EOF > req.txt\r\njupyterhub\r\nnbgitpuller\r\npyspark\r\nboto3\r\nelasticsearch\r\naltair\r\nbeautifulsoup4\r\nbokeh\r\nbottleneck\r\ncloudpickle\r\ncython\r\ndask\r\nh5py\r\nipympl\r\nipywidgets\r\nmatplotlib-base\r\nnltk\r\ngensim\r\nnumba\r\nnumexpr\r\npandas\r\npatsy\r\nprotobuf\r\npytables\r\nscikit-learn\r\nscikit-image\r\nscipy\r\nseaborn\r\nstatsmodels\r\nxlrd\r\nEOF\r\n\r\nmamba install -q --file req.txt -y\r\nmamba install -v -y -c conda-forge tensorflow-cpu\r\n```\r\n\r\nWill result in:\r\n\r\n```\r\ninfo     Problem count: 1\r\nEncountered problems while solving:\r\n  - package tensorflow-base-2.6.0-cpu_py39h7e79a0b_2 requires grpc-cpp >=1.39.1,<1.40.0a0, but none of the providers can be installed\r\n```\r\n\r\nNotice that:\r\n\r\n- 2.6.2 is latest version of tensorflow, but this appears to install 2.6.0\r\n- grpc-cpp 1.42.0 is installed already prior to tensorflow-cpu attempted install\r\n\r\nSo let's try:\r\n\r\n```\r\nmamba install -v -y -c conda-forge tensorflow-cpu==2.6.2\r\n```\r\n\r\nResult:\r\n\r\n```\r\ninfo     Problem count: 1\r\nEncountered problems while solving:\r\n  - package grpc-cpp-1.42.0-ha1441d3_1 requires libprotobuf >=3.19.1,<3.20.0a0, but none of the providers can be installed\r\n```\r\n\r\nFunny thing is, `libprotobuf` 3.19.1 _is_ installed:\r\n\r\n```\r\nmamba list | grep proto\r\n# libprotobuf               3.19.1               h780b84a_0    conda-forge\r\n```", "comments": ["Hi @Saduf2019! Could you please look at this issue?", "I believe the root of this is that `grpc-cpp` 1.42 gets installed by those previous requirements from reqs.txt, and is unable to downgrade to >=1.39.1,<1.40.0a0.\r\n\r\nSuffice it to say, we have had a miserable time trying to install any version at all of `tensorflow-cpu` alongside this list of scientific computing packages.", "FYI I have opened a separate issue at https://github.com/mamba-org/mamba/issues/1354. This install also fails using `conda`, and takes quite literally over an hour to fail to solve.", "@brsolomon-deloitte \r\nCould you please try it on tf 2.6 and let us know if you still face the issue.", "> @brsolomon-deloitte Could you please try it on tf 2.6 and let us know if you still face the issue.\r\n\r\nCan you clarify what you mean? This install was attempted for version 2.6.x.", "@brsolomon-deloitte \r\nCould you please try a version below and let us know, if you face the issue on tf 2.5 as well", "> @brsolomon-deloitte Could you please try a version below and let us know.\r\n\r\nWhat is \"a version below\"? This install fails with both tensorflow-cpu==2.6.0 and tensorflow-cpu==2.6.2.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53462\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53462\">No</a>\n", "Please reopen, this was never resolved.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "not stale", "As mentioned here https://github.com/tensorflow/tensorflow/issues/54154, conda-forge is a community build and these are not supported by Tensorflow.\r\nFor any issue specific to `conda-forge` for tensorflow you can refer to https://github.com/conda-forge/tensorflow-feedstock and regarding issues on mamba you can refer `mamba-org/mamba` repository, I can already see your related issues filed on the same repository for specific error message.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53462\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53462\">No</a>\n"]}, {"number": 53461, "title": "[TFLite] iOS app crashes with EXC_RESOURCE when building with XCode 13", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: iOS 15\r\n- Mobile device if the issue happens on mobile device: iPhone\r\n- TensorFlow installed from: source\r\n- TensorFlow version: 2.6.0\r\n- Python version: Python 3.9.7\r\n- Bazel version: 4.2.1-homebrew\r\n- GCC/Compiler version: Apple clang version 13.0.0 (clang-1300.0.29.3)\r\n- CUDA/cuDNN version: n.a.\r\n- GPU model and memory: n.a.\r\n\r\n**Describe the current behavior**\r\nThe app crashes with `EXC_RESOURCE` when running TFLite model on iOS device.\r\n* This problem occurs only when the app is built with XCode 13.0 / 13.1 / 13.2 and using Core ML backend.\r\n* The problem does not occur when the app is built with XCode 12.\r\n\r\n**Describe the expected behavior**\r\nApp run without crash when built with XCode 13\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): yes\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n@freedomtan has identified the issue:\r\n> FYR. I upgraded my macOS, iOS, and Xcode to 12.1, 15.2, and Xcode 13.2 yesterday. Then, I spent some time profiling and found that the culprit is [CoreML delegate's invoke function].(https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/coreml/coreml_delegate_kernel.mm#L207-L230). Adding `@autoreleasepool {}` to it resolved the EXC_RESOURCE issue. Tested on iPhone 11 Pro and iPhone 13 running iOS 15.2. Supposedly, it worked for earlier Xcode 13.x and iOS.\r\n\r\n**Standalone code to reproduce the issue**\r\nNot possible since the app need to be built and run.\r\n\r\n**Other info / logs**\r\n\r\nThe app used is located here: https://github.com/mlcommons/mobile_app_open\r\n\r\nCrash log:\r\n```\r\nflutter: Running Benchmark:IS_float32 in performance mode...\r\n\r\n...\r\n\r\nli:cpp/flutter/main.cc:252@run_backend4\r\n2021-10-02 14:21:30.769776: I cpp/backends/external.cc:135] Using default allocator\r\nEnabling CoreML delegate 0x2836b5a00\r\nCoreML delegate: 76 nodes delegated out of 77 nodes, with 1 partitions.\r\nINFO: CoreML delegate: 76 nodes delegated out of 77 nodes, with 1 partitions.\r\n\r\nli:cpp/flutter/main.cc:257@run_backend4\r\n2021-10-02 14:21:31.456619: E cpp/datasets/ade20k.cc:76] Failed to list all the ground truth files in provided path. Only measuring performance.\r\nli:cpp/flutter/main.cc:285@run_backend4\r\nli:cpp/flutter/main.cc:289@run_backend4\r\n* thread #26, name = 'DartWorker', queue = 'com.apple.CoreMLBatchProcessingQueue', stop reason = EXC_RESOURCE RESOURCE_TYPE_MEMORY (limit=2098 MB, unused=0x0)\r\n    frame #0: 0x00000001f37d864c libsystem_platform.dylib`_platform_memmove + 76\r\nlibsystem_platform.dylib`_platform_memmove:\r\n->  0x1f37d864c <+76>: stnp   q0, q1, [x3]\r\n    0x1f37d8650 <+80>: add    x3, x3, #0x20             ; =0x20 \r\n    0x1f37d8654 <+84>: ldnp   q0, q1, [x1]\r\n    0x1f37d8658 <+88>: add    x1, x1, #0x20             ; =0x20 \r\nTarget 0: (Runner) stopped.\r\n```\r\n![135967965-1e284466-a072-4b15-8895-9bbd6001a70a](https://user-images.githubusercontent.com/85728587/146400916-bc8d5e6d-5053-41fb-b4ba-2df5bca58fc1.png)\r\n\r\n", "comments": ["@anhappdev This issue will be closed once the [PR](https://github.com/tensorflow/tensorflow/pull/53468) is merged.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53461\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53461\">No</a>\n"]}, {"number": 53460, "title": "[Go] Return correct backing device name for TensorHandle", "body": "In Go, `th.BackingDeviceName` currently returns `TFE_TensorHandleDeviceName`, which may differ from the backing device.", "comments": []}, {"number": 53459, "title": "UnimplementedError:  Fusion is not implemented: [BiasAdd,Tanh] (only when tanh activation functions are used)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution: Linux Ubuntu 18.04.6 LTS)\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: Python 3.8.5\r\n\r\n\r\nI am trying to build a keras model to solve a problem in which I define a custom loss function, the model has tanh activation functions and in the custom loss function I generate random input values and then I do some manipulations to prepare the input of the model that takes 3 inputs and outputs just one output. The loss function is then defined as a function of the output and some of its derivatives with respect to the inputs which are computed using tf.gradients. \r\n\r\nThe code is shown below\r\n\r\n```python\r\nimport sys\r\nimport os.path\r\nimport tensorflow as tf\r\nimport math as m\r\nimport numpy as np\r\nimport scipy.io\r\nfrom scipy.interpolate import griddata\r\nimport time\r\nfrom itertools import product, combinations\r\nfrom sklearn.utils import shuffle\r\nfrom sklearn.preprocessing import MinMaxScaler\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Layer, Activation, Dense, BatchNormalization\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\n\r\ninputs = tf.keras.Input(shape=(3,),dtype='float32')\r\nlayer1 = Dense(units=200, activation='tanh',dtype='float32')(inputs)\r\nlayer2 = Dense(units=200, activation='tanh',dtype='float32')(layer1)\r\nlayer3 = Dense(units=200, activation='tanh',dtype='float32')(layer2)\r\nlayer4 = Dense(units=200, activation='tanh',dtype='float32')(layer3)\r\nlayer5 = Dense(units=200, activation='tanh',dtype='float32')(layer4)\r\nlayer6 = Dense(units=200, activation='tanh',dtype='float32')(layer5)\r\nlayer7 = Dense(units=200, activation='tanh',dtype='float32')(layer6)\r\nlayer8 = Dense(units=200, activation='tanh',dtype='float32')(layer7)\r\nlayer9 = Dense(units=200, activation='tanh',dtype='float32')(layer8)\r\npredictions = Dense(units=1, activation='linear',dtype='float32')(layer9)\r\nmodel = tf.keras.Model(inputs=inputs, outputs=predictions)\r\n\r\n# Define custom loss\r\ndef custom_loss():\r\n\r\n    # Create a loss function\r\n    def loss(y_true,y_pred):\r\n        \r\n        # Write here the loss function\r\n\r\n        x0 = tf.random.uniform(shape=[2,100],maxval = 1)\r\n        y0 = tf.random.uniform(shape=[2,100],maxval = 1)\r\n        t0 = tf.random.uniform(shape=[100,2],maxval = 1)\r\n        pY0 = tf.transpose(tf.concat([tf.ones([1,y0.shape[1]]) * y0[0,:], tf.zeros([1,y0.shape[1]], dtype='float32'), tf.ones([1,y0.shape[1]]) * y0[1,:]], 0))\r\n        pY1 = tf.transpose(tf.concat([tf.ones([1,y0.shape[1]]) * y0[0,:], tf.ones([1,y0.shape[1]], dtype='float32'), tf.ones([1,y0.shape[1]]) * y0[1,:]], 0))\r\n        pX0 = tf.transpose(tf.concat([tf.zeros([1,x0.shape[1]], dtype='float32'), tf.ones([1,x0.shape[1]]) * x0[0,:], tf.ones([1,x0.shape[1]]) * x0[1,:]], 0))\r\n        pX1 = tf.transpose(tf.concat([tf.ones([1,x0.shape[1]], dtype='float32'), tf.ones([1,x0.shape[1]]) * x0[0,:], tf.ones([1,x0.shape[1]]) * x0[1,:]], 0))\r\n        pIC = tf.concat([t0, tf.zeros([t0.shape[0],1], dtype='float32')], 1)\r\n        pF = tf.random.uniform(shape=[1000,3],maxval = 1)\r\n        uX0 = model(pX0, training=True)\r\n        uX1 = model(pX1, training=True)\r\n        uY0 = model(pY0, training=True)\r\n        uY1 = model(pY1, training=True)\r\n        uIC = model(pIC, training=True)\r\n        u = model(pF, training=True)\r\n\r\n        # compute the derivatives of u\r\n        u_grad = tf.gradients(u, pF)[0]\r\n        u_Dot = tf.gradients(u, pF)[0]\r\n        u_t = u_grad[:,2]\r\n        u_x = u_grad[:,0]\r\n        u_y = u_grad[:,1]\r\n        u_xx = (tf.gradients(u_x, pF))[0][:,0]\r\n        u_yy = (tf.gradients(u_y, pF))[0][:,1]\r\n\r\n        return tf.reduce_mean(tf.square(uX0)) + tf.reduce_mean(tf.square(uX1)) + \\\r\n        tf.reduce_mean(tf.square(uY0)) + tf.reduce_mean(tf.square(uY1)) + \\\r\n        tf.reduce_mean(tf.square(uIC - tf.transpose(tf.math.sin(pi*pIC[:,0])*tf.math.sin(pi*pIC[:,1]) * tf.ones([1,1], dtype='float32')) )) + \\\r\n        tf.reduce_mean(tf.square(u_t-u_xx-u_yy))\r\n\r\n    # Return a function\r\n    return loss\r\n\r\n# Compile the model\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4) ,\r\n              loss=custom_loss(),\r\n              # Call the loss function with the selected layer\r\n              metrics=['accuracy'])\r\n\r\ninput_T = tf.linspace([0.0, 0.0, 0.0], [1.0, 1.0, 1.0], 1000) # We define a dummy input in order to use the fit method\r\noutput_T = tf.linspace([0.0], [1.0], 1000) # We define a dummy output\r\nmodel.fit(input_T, output_T,epochs=10, batch_size = input_T.shape[0])\r\n\r\n````\r\n\r\n\r\nI tried to run the code on my PC using the CPU and I got the following error \r\n\r\n```\r\n`File \"NewTest_HeatEq.py\", line 169, in <module>\r\n    model.fit(input_T, output_T,epochs=10, batch_size = input_T.shape[0])\r\n  File \"/home/saddam/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 1100, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n  File \"/home/saddam/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/saddam/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 888, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/home/saddam/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2942, in __call__\r\n    return graph_function._call_flat(\r\n  File \"/home/saddam/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1918, in _call_flat\r\n    return self._build_call_outputs(self._inference_function.call(\r\n  File \"/home/saddam/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 555, in call\r\n    outputs = execute.execute(\r\n  File \"/home/saddam/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.UnimplementedError:  Fusion is not implemented: [BiasAdd,Tanh]\r\n\t [[node loss/model/dense/Tanh_3 (defined at NewTest_HeatEq.py:100) ]] [Op:__inference_train_function_4519]\r\n\r\nFunction call stack:\r\ntrain_function\r\n```\r\n\r\nHowever, the code worked when I ran it on a GPU machine, and also the code works when I run it on my PC changing only the activation functions to sigmoid ones. I find the behavior not normal, do you have any idea on what has caused that and how to fix the issue. \r\n\r\n\r\n", "comments": ["@saddamhijazi ,\r\nI ran the code shared and face a different error, please find the gist [here](https://colab.research.google.com/gist/tilakrayal/6b3c40e17ad26a4b74c8c98a2a60cc16/untitled1488.ipynb) and share all dependencies to replicate the issue or share a colab gist with the reported error.\r\n\r\n", "Also please try to test your code in latest tf v2.7 and let us know if the issue still persists.Thanks!", "> @saddamhijazi , I ran the code shared and face a different error, please find the gist [here](https://colab.research.google.com/gist/tilakrayal/6b3c40e17ad26a4b74c8c98a2a60cc16/untitled1488.ipynb) and share all dependencies to replicate the issue or share a colab gist with the reported error.\r\n\r\nSorry for the error, I forgot to include this line\r\n\r\n`pi = tf.constant(m.pi)`\r\n\r\nNow the code runs in a normal way without error, I don't know if the problem is caused by the tensorflow version on my laptop. May I ask you if it is possible to install the latest version using anaconda ?\r\n\r\nIn the following repository the latest available version is 2.6 not 2.7 \r\n\r\nhttps://repo.anaconda.com/pkgs/main/linux-64/", "@saddamhijazi ,\r\nInstallation issues within the Anaconda environment are tracked in the Anaconda repo.Thanks!", "I have managed to run the code without problems on TensorFlow 2.7 but it still does not work on 2.4, for me it is not a problem I can use the 2.7 version. The issue could be closed. \r\n\r\nThank you very much.", "@saddamhijazi ,\r\n There is a high possibility that issue was fixed with later TF versions.Glad the suggestion worked for you.Please feel feel to move this issue to closed status as issue got resolved in latest stable version.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53459\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53459\">No</a>\n"]}, {"number": 53458, "title": "Deprecate non-tuple nd-indices", "body": "This is deprecated in NumPy (numpy/numpy#9686) and in Jax already removed (google/jax#4867) but TensorFlow still uses this feature as can be seen by using NumPy arrays and Jax arrays:\r\n```py\r\n>>> from tensorflow.keras import Sequential\r\n>>> from jax.numpy import array\r\n>>> Sequential().predict(array([0]))\r\n[...]\r\nTypeError: Using a non-tuple sequence for multidimensional indexing is not allowed; use `arr[array(seq)]` instead of `arr[seq]`. See https://github.com/google/jax/issues/4564 for more information.\r\n[...]\r\n>>> from numpy import array\r\n>>> Sequential().predict(array([0]))\r\narray([0])\r\n```\r\nFor now, I opened an issue on Jax (google/jax#8980) and asked to revert this change, but anyway I think TensorFlow really should accept the deprecation.", "comments": ["https://github.com/google/jax/issues/8980#issuecomment-996026127"]}, {"number": 53457, "title": "Improve `saved_model.load.get_config`", "body": "This PR makes two minor, nonfunctional improvements to the `saved_model.load.get_config` function:\r\n\r\n1. **it\u2019s easier to ask for forgiveness than permission:**  \r\nIdiomatic python: It's better to catch the exception of the exceptional case, rather than checking if the attribute exists beforehand. [more on this](https://devblogs.microsoft.com/python/idiomatic-python-eafp-versus-lbyl/)\r\n2.  **Error description:** \r\nThe NotImplementedError now describes two likely root causes of the problem.", "comments": ["It looks like your PR relates to the Keras component. Please submit it to the github.com/keras-team/keras repository instead. Thankyou."]}, {"number": 53456, "title": "Writing Tensor objects to Numpy arrays in Tensorflow 1.12", "body": "I am building a model which is supposed to be a composition of different models. I need the output of one model to be fed as input to another. To do so, I followed the standard approach of extending tf.keras.Model and modifying the call() function suitably. However, that didn\u2019t work out since Model2 expects an input of the type np.ndarray whereas Model1 returns an output of the type tf.tensor hence their composition isn\u2019t possible.\r\n\r\nI have tried multiple ways of doing the above including converting the tensor to a tf.Variable and using the Variable.assign() method, writing out values to a list and then using np.stack() to get a np.tensor as output. Nothing has worked so far.\r\n\r\nI request the community to kindly help me out in solving the above issue.", "comments": ["@snehalstomar We see that you are using `TF v1.12` which is not actively supported. Could you please try to upgrade to `TF` `v2.7.0` and please refer to the [migration guide](https://www.tensorflow.org/guide/migrate) for more information?Thank you!  ", "Hi!\r\n\r\nI am actually trying to build upon a base model written in Tensorflow 1.12 for my research. I would highly appreciate it if you could kindly give me insights to make it work in TF v1.12 ", "@snehalstomar You can only convert tensors to numpy arrays during Eager execution. Since you're using a version older than 2.0 this is not enabled by default.\r\nIn any case, you can call this after importing tensorflow;\r\n`tf.compat.v1.enable_eager_execution()`\r\n\r\nDepending on your framework and usecase, you might also have to run tf.config.run_functions_eagerly (if you have tf.function defined anywhere). For better support of the Eager mode, you should upgrade tensorflow to the newest version and use tf.keras as your code might not work properly with older standalone versions of Keras. In newer versions, you can specify your keras model to run eagerly  as follows:\r\n`model.run_eagerly = True`\r\n\r\nPlease refer to similar [issue1](https://stackoverflow.com/questions/56205844/how-to-convert-a-tensorflow-tensor-to-a-numpy-array-within-tf-dataset-map-whil),[issue2](https://stackoverflow.com/questions/64004445/how-to-convert-tensor-into-numpy-array)\r\n\r\nAs TensorFlow version 1.x is out of support window so for any further queries  please post this issue in [TF Forum](https://discuss.tensorflow.org/) where there is a larger community to help .Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53455, "title": "When I use ObjectDetection Demo,it tips Invalid tensor index 1, max index is 0.How can I do?", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):MacBook Pro (13-inch, 2017, Two Thunderbolt 3 ports) 11.6 (20G165)\r\n- TensorFlow installed from (source or binary):TensorFlowLiteSwift','2.6.0'\r\n- TensorFlow version (or github SHA if from source):TensorFlowLiteSwift','2.6.0'\r\n\r\nquestion:\r\nWhen I use ObjectDetection Demo,it tips Invalid tensor index 1, max index is 0,How can I do?  \r\nI do not know where is Error ,I just use my tflitemodel to replace Demo tflitemodel ,it show last tips:\r\n\r\nInitialized TensorFlow Lite runtime.\r\nINFO: Initialized TensorFlow Lite runtime.\r\nFailed to invoke the interpreter with error: Invalid tensor index 1, max index is 0.\r\nFailed to invoke the interpreter with error: Invalid tensor index 1, max index is 0.\r\nFailed to invoke the interpreter with error: Invalid tensor index 1, max index is 0.\r\n...", "comments": ["@csdf-ssm ,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.", "Also please take a look at this issue [1](https://github.com/tensorflow/tensorflow/issues/33976) and [2](https://github.com/tensorflow/tensorflow/issues/39803) with the similar error.It helps.Thanks!", "@tilakrayal  I use yolov5 to train model  and output only 1 data ,so the demo is  not suit me,thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53455\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53455\">No</a>\n"]}]