[{"number": 45875, "title": "How to create Flex-free model in TFLite", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): Installed via PIP\r\n- TensorFlow version (or github SHA if from source): Tensorflow 2.3.1\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n```python\r\nimport tensorflow as tf\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\n  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n]\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n\r\n```\r\nNOTE: Model is successfully converted.\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- TFlite model is working fine at CPU Linux 16.04 and RaspberryPi. But it gives BUS ERROR at edge device. [Details](https://github.com/tensorflow/tensorflow/issues/45504).\r\n\r\nThe original model is in PyTorch. I have used PyTorch-ONNX-Tensorflow-TFLite approach to convert the model in TFLite.\r\nI am sharing graphs of ONNX and TFLite model. [Link](https://drive.google.com/drive/folders/1jzLZKus-9Dey2vgcMWjspBZfCRJbLCWW?usp=sharing)\r\n\r\nGeeks says \r\n> Using Flex op requires to build TF op kernels which is difficult to support various targets.\r\nIf the required Flex op is only FlexMul, I think you might be able to create Flex-free model with some refactoring.\r\n\r\nHow can do this \"create Flex-free \"? \r\n", "comments": ["Could you share your FlexMul op details? Which type being calculated? Which input shape being calculated?\r\n\r\nTFLite Mul can support F32, I32, QI8, QUI8, and QI16 inputs and support up to 4 dims if broadcasting is needed or the same input shapes regardless of dimensions.\r\n\r\nIf you can modify your model to meet the above Mul op constraints, the FlexMul op requirement will be gone.", "> Could you share your FlexMul op details? Which type being calculated? Which input shape being calculated?\r\n> \r\n> TFLite Mul can support F32, I32, QI8, QUI8, and QI16 inputs and support up to 4 dims if broadcasting is needed or the same input shapes regardless of dimensions.\r\n> \r\n> If you can modify your model to meet the above Mul op constraints, the FlexMul op requirement will be gone.\r\n\r\nI have already shared graphs.[Link](https://drive.google.com/drive/folders/1jzLZKus-9Dey2vgcMWjspBZfCRJbLCWW)\r\nInput to model is = [1,500,120]\r\n\r\n> If you can modify your model to meet the above Mul op constraints, the FlexMul op requirement will be gone.\r\nHOW?\r\n", "Could you share your Flex-enabled tflite model file to us instead of the graph image if possible? And it would be helpful to understand why the FlexMul op is introduced if you can share some warning logs like the below example. You can see the following warnings in tf-nightly version when you are converting the model.\r\n\r\n```\r\nI1120 16:16:16.002125 2530115 flatbuffer_export.cc:...] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following flex op(s):\r\n\r\nFlex ops: tf.MaxPool\r\nDetails:\r\n        tf.MaxPool {data_format = \"NCHW\", device = \"\", explicit_paddings = [], ksize = [1, 1, 2, 2], padding = \"VALID\", strides = [1, 1, 2, 2]}\r\n        tf.MaxPool {data_format = \"NCHW\", device = \"\", explicit_paddings = [], ksize = [1, 1, 3, 3], padding = \"VALID\", strides = [1, 1, 1, 1]}\r\n```\r\n\r\nBased on the above information, you can find which one is the corresponding operator in the source graph for the FlexMul op in the TensorFlow Lite and you can find a way to replace the source operator with the supported operator case by modifying the source graph.", "@nehasoni3 can you share the screenshot of the tabs that appear when you click Mul op and FlexMul op in the netron app?\r\n\r\nAnd can you share the input tensor type of Mul op and input shape information of Mul op?", "> @nehasoni3 can you share the screenshot of the tabs that appear when you click Mul op and FlexMul op in the netron app?\r\n> \r\n> And can you share the input tensor type of Mul op and input shape information of Mul op?\r\n\r\n![MatMUL_ONNX](https://user-images.githubusercontent.com/31642462/102889931-894e1d80-4481-11eb-8099-a845c3e75ba0.png)\r\n\r\n![FlexMUL_details](https://user-images.githubusercontent.com/31642462/102890151-fb266700-4481-11eb-85db-f424f667c7ae.png)\r\n", "If you can click the + buttons at the netron, which are located at each input and each output, there is more information regarding input types and shapes. It would be better to get information from both graphs, one for Mul op and one for FlexMul op.", "If I use this only,\r\n`converter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()`\r\nThis works but every op is of Flex.\r\n![image](https://user-images.githubusercontent.com/31642462/102893714-0e3c3580-4488-11eb-9151-f5278bd0bf66.png)\r\n\r\nBut If I use, \r\n`converter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS]\r\ntflite_model = converter.convert()`\r\nI got this log-\r\n`---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    212                                                  debug_info_str,\r\n--> 213                                                  enable_mlir_converter)\r\n    214       return model_str\r\n\r\n4 frames\r\nException: <unknown>:0: error: loc(callsite(callsite(\"Mul@__inference___call___2921\" at \"PartitionedCall@__inference_signature_wrapper_2928\") at \"PartitionedCall\")): 'tf.Mul' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"add_2@__inference___call___2921\" at \"PartitionedCall@__inference_signature_wrapper_2928\") at \"PartitionedCall\")): 'tf.AddV2' op is neither a custom op nor a flex op`\r\n\r\n", "Yes, this is an expected behavior. The current model input for conversion can not be covered by the TFLite builtin op set. So we need flex ops or custom ops.", "If you can fix the input shapes in the model during conversion, those Flex op requirements might be gone.", "> If you can fix the input shapes in the model during conversion, those Flex op requirements might be gone.\r\n\r\nHow can I fix the input shapes. Do I need to change its dtype or what?", "I am assuming that the model is using float32 type, which is a typical choice among ML model. If it is true, the Flex requirement will be introduced because of the input shapes. The input shape can be fixed by (1) modifying the original source graph or (2) using TFLite converter V1 API's input_shapes argument https://www.tensorflow.org/api_docs/python/tf/compat/v1/lite/TFLiteConverter", "@abattery Now, FlexMUL is removed but FlexAddV2 op is still there. Will you guide me to remove that FlexAddV2 op?", "The same strategy can be applied to FlexAddV2 ops. Fixing the input shape of the FlexAddV2 ops can help replacing them with TFLite Add ops.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45875\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45875\">No</a>\n"]}, {"number": 45874, "title": "TPU guide doesn't work, but used to - `Op type not registered 'DecodeImage' in binary`", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/guide/tpu\r\nhttps://github.com/tensorflow/docs/blob/master/site/en/guide/tpu.ipynb\r\n\r\n## Description of issue (what needs changing):\r\n\r\nLast time I tried it worked: \r\n  - [gcloud_scripts/test.bash @ `851b306`](https://github.com/SamuelMarks/ml-glaucoma/blob/851b306/gcloud_scripts/test.bash) using [tensorflow/docs/site/en/guide/tpu.ipynb @ `7931afd`](https://github.com/tensorflow/docs/blob/7931afd/site/en/guide/tpu.ipynb)\r\n\r\nBut now it does not work:\r\n  - [gcloud_scripts/test.bash @ `a1dffb1`](https://github.com/SamuelMarks/ml-glaucoma/blob/a1dffb1/gcloud_scripts/test.bash) using [tensorflow/docs/site/en/guide/tpu.ipynb @ `945a448`](https://github.com/tensorflow/docs/blob/945a448/site/en/guide/tpu.ipynb)\r\n\r\n### Clear description\r\n\r\nMy scripts are pretty simple, I made them from my [**Nothing to TPU in 5 Google Cloud commands** gist](https://gist.github.com/SamuelMarks/d5e4e4233c0a1401635f783994366daa), they just use the [`gcloud` CLI](https://cloud.google.com/sdk/gcloud), and it's as easy as:\r\n\r\n```sh\r\n$ git clone https://github.com/SamuelMarks/ml-glaucoma\r\n# Set env vars (you'll want to update the various IDs here\u2026):\r\n$ . ./ml-glaucoma/gcloud_scripts/.env.sh\r\n# Create network, firewall, compute, and tpu:\r\n$ . ./ml-glaucoma/gcloud_scripts/init.sh\r\n# Install Python deps, convert the Jupyter Notebook to a Python script, then run it:\r\n$ . ./ml-glaucoma/gcloud_scripts/test.sh\r\n# Destroy network, firewall, compute, and tpu\r\n$ . ./ml-glaucoma/gcloud_scripts/teardown.sh\r\n```\r\n\r\nUnfortunately it fails in the `test.sh` phase, here's the relevant snippet:\r\n```\r\n2020-12-19 08:38:55.052102: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 192.168.0.2:8470}\r\n2020-12-19 08:38:55.052147: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:31480}\r\n2020-12-19 08:38:55.068584: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 192.168.0.2:8470}\r\n2020-12-19 08:38:55.068633: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:31480}\r\n2020-12-19 08:38:55.069112: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:31480\r\nAll devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU')]\r\nc device:  /job:worker/replica:0/task:0/device:TPU:0\r\ntf.Tensor(\r\n[[22. 28.]\r\n [49. 64.]], shape=(2, 2), dtype=float32)\r\nPerReplica:{\r\n  0: tf.Tensor(\r\n[[22. 28.]\r\n [49. 64.]], shape=(2, 2), dtype=float32),\r\n  1: tf.Tensor(\r\n[[22. 28.]\r\n [49. 64.]], shape=(2, 2), dtype=float32),\r\n  2: tf.Tensor(\r\n[[22. 28.]\r\n [49. 64.]], shape=(2, 2), dtype=float32),\r\n  3: tf.Tensor(\r\n[[22. 28.]\r\n [49. 64.]], shape=(2, 2), dtype=float32),\r\n  4: tf.Tensor(\r\n[[22. 28.]\r\n [49. 64.]], shape=(2, 2), dtype=float32),\r\n  5: tf.Tensor(\r\n[[22. 28.]\r\n [49. 64.]], shape=(2, 2), dtype=float32),\r\n  6: tf.Tensor(\r\n[[22. 28.]\r\n [49. 64.]], shape=(2, 2), dtype=float32),\r\n  7: tf.Tensor(\r\n[[22. 28.]\r\n [49. 64.]], shape=(2, 2), dtype=float32)\r\n}\r\nTraceback (most recent call last):\r\n  File \"tpu-tester.py\", line 85, in <module>\r\n    train_dataset = get_dataset(batch_size, is_training=True)\r\n  File \"tpu-tester.py\", line 52, in get_dataset\r\n    dataset, info = tfds.load(name='mnist', split=split, with_info=True,\r\n  File \"tensorflow_datasets/core/load.py\", line 356, in load\r\n    ds = dbuilder.as_dataset(**as_dataset_kwargs)\r\n  File \"tensorflow_datasets/core/dataset_builder.py\", line 552, in as_dataset\r\n    datasets = utils.map_nested(build_single_dataset, split, map_tuple=True)\r\n  File \"tensorflow_datasets/core/utils/py_utils.py\", line 183, in map_nested\r\n    return function(data_struct)\r\n  File \"tensorflow_datasets/core/dataset_builder.py\", line 582, in _build_single_dataset\r\n    ds = ds.cache()\r\n  File \"tensorflow/python/data/ops/dataset_ops.py\", line 1400, in cache\r\n    return CacheDataset(self, filename)\r\n  File \"tensorflow/python/data/ops/dataset_ops.py\", line 3779, in __init__\r\n    variant_tensor = gen_dataset_ops.cache_dataset_v2(\r\n  File \"tensorflow/python/ops/gen_dataset_ops.py\", line 782, in cache_dataset_v2\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"tensorflow/python/framework/ops.py\", line 6862, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'DecodeImage' in binary running on n-73dd04c3-w-0. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed. [Op:CacheDatasetV2]\r\n2020-12-19 08:39:12.439461: W tensorflow/core/distributed_runtime/eager/remote_tensor_handle_data.cc:76] Unable to destroy remote tensor handles. If you are running a tf.function, it usually indicates some op in the graph gets an error: Op type not registered 'DecodeImage' in binary running on n-73dd04c3-w-0. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\r\nError in atexit._run_exitfuncs:\r\nTraceback (most recent call last):\r\n  File \"tensorflow/python/distribute/tpu_strategy.py\", line 738, in async_wait\r\n    context.async_wait()\r\n  File \"tensorflow/python/eager/context.py\", line 2330, in async_wait\r\n    context().sync_executors()\r\n  File \"tensorflow/python/eager/context.py\", line 645, in sync_executors\r\n    pywrap_tfe.TFE_ContextSyncExecutors(self._context_handle)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'DecodeImage' in binary running on n-73dd04c3-w-0. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\r\n2020-12-19 08:39:12.516206: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 399, Output num: 0\r\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:\r\n:{\"created\":\"@1608367152.516107804\",\"description\":\"Error received from peer ipv4:192.168.0.2:8470\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Unable to find the relevant tensor remote_handle: Op ID: 399, Output num: 0\",\"grpc_status\":3}\r\n```", "comments": ["@SamuelMarks \r\nIs this still  an issue, could you please try on the latest version of tensorflow and let us know.", "Hi, could you please try with the latest Tensorflow version and let us know if this is still an issue. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 45873, "title": "Nightly", "body": "To clear the annotations warning in workflow, I would like to suggest this change.\r\n(Initial Warning obtained in summary part of workflow actions : Ubuntu-latest workflows will use Ubuntu-20.04 soon)\r\n\r\nChanged runs-on in update-nightly.yml from ubuntu-latest to Ubuntu-20.04", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45873) for more info**.\n\n<!-- need_sender_cla -->", "I have just made an entry to cla from : https://opensource.google/docs/cla/", "This PR seems to have been merged automatically as a side effect. However, from the merge commit I don't see anything related to the PR description."]}, {"number": 45872, "title": "Illegal instruction  CPUs under version 2.4.0", "body": "System information\r\n\r\nOS: Ubuntu 18.04\r\nTensorFlow binary installed using pip\r\nTensorFlow version 2.4.0rc0\r\nPython version 3.8.6\r\nCUDA/cuDNN version: 11.0/8.0.4\r\nGPU model: GTX 1080 ti\r\nDescribe the current behavior\r\n\r\nAttempting to import tensorflow produces an \"Illegal instruction\" error.\r\n\r\nDescribe the expected behavior\r\n\r\nImport tensorflow without error. (Illegal instruction)\r\n\r\npython -c \"import tensorflow as tf\"\r\n\r\n\r\n", "comments": ["@momen,\r\nCould you please provide the make and model of your CPU? Starting with v1.6, TensorFlow binaries use AVX instructions which may not run on older CPUs. For more information, please take a look at the [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements) for installing TensorFlow.\r\n\r\nAlso, please check similar issues [#33038](https://github.com/tensorflow/tensorflow/issues/33038#issuecomment-538470776), [#29788](https://github.com/tensorflow/tensorflow/issues/29788#issuecomment-505571104) and let us know if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45872\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45872\">No</a>\n"]}, {"number": 45871, "title": "Documentation for tf.keras.backend.max is missing", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/backend/max?hl=FA\r\n\r\n## Description of issue (what needs changing): \r\nGo to https://www.tensorflow.org/s/results?q=keras.backend.max and click on the first link (in fact, any link related to that API).\r\n\r\nThe link to the API, **`tf.keras.backend.max`** is not available. Is it intentional, because, in the [Source Code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L2272), it is set **not to generate the Docs**.\r\n\r\nIf it is intentional, can you please explain why? Is there a better alternative to this API, or you have plans to Deprecate it, etc..\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct? : No (Link for the API itself is missing)\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly? : No\r\n\r\n### Returns defined\r\n\r\nAre return values defined? : No\r\n\r\n### Raises listed and defined : No\r\n\r\n### Usage example\r\n\r\nIs there a usage example? : No\r\n\r\n### Submit a pull request? : \r\nI'm ready to submit a PR to remove this [line of code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L2272) if it is fine with you.", "comments": ["duplicate #45860"]}, {"number": 45870, "title": " Extract reference for operator FLOOR_DIV to standalone header", "body": "Move the reference implementation to its own header so that micro\r\ncan use it without the unrelated depedencies of reference_ops.h.\r\n\r\nThis PR is part of the work to port operator FLOOR_DIV from lite to micro,\r\nas tracked in Issue #45657.", "comments": ["@ddavis-2015 Can you please check @petewarden's comments and keep us posted ? Thanks!", "@ddavis-2015  Can you please resolve conflicts? Thanks!"]}, {"number": 45869, "title": " Extract a function for parsing operator FLOOR_DIV", "body": "Extract the parsing out of a switch statement case to create a\r\nstandalone function which can be called by the micro op resolver.\r\n\r\nThis PR is part of the work to port operator FLOOR_DIV from lite to micro,\r\nas tracked in Issue #45657.", "comments": []}, {"number": 45868, "title": " Extract reference for operator FLOOR_MOD to standalone header", "body": "Move the reference implementation to its own header so that micro\r\ncan use it without the unrelated depedencies of reference_ops.h.\r\n\r\nThis PR is part of the work to port operator FLOOR_MOD from lite to micro,\r\nas tracked in Issue #45749", "comments": ["The internal compilation error is:\r\n\r\n```\r\nthird_party/tensorflow/lite/kernels/internal/reference/floor_mod.h:28:14: error: no member named 'fmod' in namespace 'std'; did you mean simply 'fmod'?\r\n      return std::fmod(lhs, rhs);\r\n```", "With https://github.com/tensorflow/tensorflow/commit/2de0ae00494b8fe4d73aeb4575a8a20f52725be2, we expect that if the build is ok externally, it will also pass the internal checks.", "> With [2de0ae0](https://github.com/tensorflow/tensorflow/commit/2de0ae00494b8fe4d73aeb4575a8a20f52725be2), we expect that if the build is ok externally, it will also pass the internal checks.\r\n\r\nNeeded an additional fix with https://github.com/tensorflow/tensorflow/commit/b596693a772a7e3981fe9486229bcc311f4ed688. Submitting internally again."]}, {"number": 45867, "title": "Extract a function for parsing operator FLOOR_MOD", "body": "Extract the parsing out of a switch statement case to create a\r\nstandalone function which can be called by the micro op resolver.\r\n\r\nThis PR is part of the work to port operator FLOOR_MOD from lite to micro,\r\nas tracked in Issue #45749.\r\n\r\n", "comments": ["@ddavis-2015  Can you please resolve conflicts? Thanks!"]}, {"number": 45866, "title": "Illegal instruction on older CPUs under version 2.4.0 ", "body": "\r\n**System information**\r\n- Ubuntu 18.04 and 20.04, Scientific Linux 7\r\n- binary installed via pip\r\n- version 2.4.0\r\n- Python 3.8\r\n- installed via pip (either inside or not inside a Conda environment)\r\n- various CPU-only and GPU-hosting machines\r\n\r\n**Describe the problem**\r\n\r\n`import tensorflow` produces \"Illegal instruction (core dumped)\" on older machines (seemingly those that do not support AVX2 instructions). There is no problem on new machines (seemingly those that support AVX2 instructions). \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\npip install tensorflow\r\npython -c \"import tensorflow\"\r\n```\r\n\r\n**Any other info / logs**\r\n\r\nThe core dump occurs on various machines with various types of CPUs. The common thread seems to be that it occurs on machines that don't support AVX2 instructions. Most of the machines on which this occurs do support AVX instructions. \r\n\r\nThe issue does not occur with Tensorflow 2.3.1 nor with Tensorflow 2.5.0 installed via tf-nightly. \r\n\r\nAny chance Tensorflow 2.4.0 was built in a way (perhaps unintentionally) that requires AVX2 instructions or some other requirement that causes it to fail on somewhat older (but not really old) machines? Based on what I am seeing, it seems that using 2.4.0 on many machines will fail.\r\n\r\nThe same issue occurs when running in the official Tensorflow Docker container.\r\n\r\nThis seems related to issue #44668.\r\n", "comments": ["> Any chance Tensorflow 2.4.0 was built in a way (perhaps unintentionally) that requires AVX2 instructions\r\n\r\n@paciorek,\r\nYes, that is one of the requirements for TensorFlow. Starting with TensorFlow 1.6, the TensorFlow binaries use AVX instructions which may not run on older CPUs. For more information, please take a look at the [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\r\n\r\nAs an alternative, you can either [build](https://www.tensorflow.org/install/source) TensorFlow from source or use [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb). \r\n\r\nThanks!", "I don't think the AVX dependency explains the issue.\r\n\r\n1) We have various machines that support AVX (and therefore are listed in your hardware requirements as being supported, namely Ivy Bridge and Sandy Bridge) but not AVX2, and the issue occurs on these machines.\r\n\r\n2) If the requirement is for AVX2, it would be helpful if this were documented so that users can simply check /proc/cpuinfo. As far as I can see, only a dependency on AVX is indicated.\r\n\r\n3) Given the AVX dependency has been in place since TF 1.6, the AVX dependency does not explain why the problem occurs with Tensorflow 2.4.0 but not 2.3.1 or 2.5.0 from tf-nightly.\r\n\r\n", "Same issue here, 2.3.1 works on machine with AVX but lacking AVX2 (AMD A10-6700 ). \r\nVersion 2.4.0 fails instead ", "Same issue @ E5-2650 V2 (Supports AVX but not supports AVX2), CentOS7 Kernel 3.10.0-1127.10.1, Python 3.8.6.\r\n\r\nVersion 2.3.1 Works well.", "cc @mihaimaruseac seems AVX2 got accidentally built into the 2.4 release? Didn't see that in release notes", "This is interesting. Will raise the issue internally, but given the holiday season will probably not be able to do anything until January", "We can try bisecting nightlies to see which commit range introduced the AVX2 dependency", "> We can try bisecting nightlies to see which commit range introduced the AVX2 dependency\r\n\r\n@paciorek would you mind to try bisecting the nightlies? Not sure if a pre-AVX2 box is necessary for testing this as I don't have one myself...", "I won't be able to try until Thursday.\n\nOn Sun, Dec 27, 2020, 4:59 PM Bairen Yi <notifications@github.com> wrote:\n\n> We can try bisecting nightlies to see which commit range introduced the\n> AVX2 dependency\n>\n> @paciorek <https://github.com/paciorek> would you mind to try bisecting\n> the nightlies? Not sure if a pre-AVX2 box is necessary for testing this as\n> I don't have one myself...\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/45866#issuecomment-751539074>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AATCXXQQOKWQVQYZCZZLI5DSW7J6XANCNFSM4VB3TQ4Q>\n> .\n>\n", "I think this is the same of #45744 .", "Any updates on that topic?", "Same reply as in #45744.\r\n\r\nAlso, it seems we all have AVX2 enabled CPUs, so we cannot really bisect.", "I wonder if any of the Cloud machines available doesn't have it.", "Deduplicating to #45744 (randomly chosen among multiple duplicates)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45866\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45866\">No</a>\n"]}, {"number": 45865, "title": " micro: copy operator DIV kernel from lite ", "body": "This is a copy without modification of the kernel and test for\r\noperator DIV from tensorflow/lite/kernels at 635e8a0.\r\nAdaptations to micro and addition to the micro build to follow.\r\n\r\nPR step 3 for issue #45431", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@ddavis-2015  Can you please address Ubuntu Sanity errors? Thanks!"]}, {"number": 45864, "title": "Converting speech_embedding hub module to tflite results in `Encountered unresolved custom op: TensorArrayV3.Node`", "body": "**System information**\r\n- Colab default settings\r\n- TF version: 2.4.0\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nConverting speech_embedding module ''https://tfhub.dev/google/speech_embedding/1'' to tflite results in:\r\n\r\n`RuntimeError: Encountered unresolved custom op: TensorArrayV3.Node number 2 (TensorArrayV3) failed to prepare.`\r\n\r\nduring inference.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nHUB_URL = 'https://tfhub.dev/google/speech_embedding/1'\r\nTEST_PATH = '.'\r\n\r\nembedding_layer = hub.KerasLayer(HUB_URL, input_shape=(16000,), trainable=False)\r\n\r\nmodel = tf.keras.Sequential([\r\n    embedding_layer\r\n])\r\n\r\nmodel.save(TEST_PATH)\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(TEST_PATH)\r\n\r\n\r\nconverter.allow_custom_ops = True\r\ntflite_model = converter.convert()\r\ntflite_model_file = 'converted_model.tflite'\r\n\r\nwith open(tflite_model_file, \"wb\") as f:\r\n  f.write(tflite_model)\r\n\r\ninterpreter = tf.lite.Interpreter(model_path=tflite_model_file)\r\ninterpreter.allocate_tensors()\r\n```\r\n\r\n**Output**\r\n\r\n```---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-16-5f54d02787e4> in <module>()\r\n      1 interpreter = tf.lite.Interpreter(model_path=tflite_model_file)\r\n----> 2 interpreter.allocate_tensors()\r\n      3 \r\n      4 input_index = interpreter.get_input_details()[0][\"index\"]\r\n      5 output_index = interpreter.get_output_details()[0][\"index\"]\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py in allocate_tensors(self)\r\n    257   def allocate_tensors(self):\r\n    258     self._ensure_safe()\r\n--> 259     return self._interpreter.AllocateTensors()\r\n    260 \r\n    261   def _safe_to_run(self):\r\n\r\nRuntimeError: Encountered unresolved custom op: TensorArrayV3.Node number 2 (TensorArrayV3) failed to prepare.```\r\n\r\n\r\n\r\nI am aware that STFT may not be supported by tflite yet. If this is the issue, is there a quick workaround?\r\n\r\nThanks,\r\nBryan", "comments": ["Sorry for encountering this issue on my side. TensorArray is not supported via TensorFlow Lite currently.  We are working on supporting them in the future.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45864\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45864\">No</a>\n", "I am trying to perform transfer learning using tf hub speech embedding . I was able to train it in tf 1.15 but to convert it to tflite\r\nI used tf 2.4 as tf 1.15 was giving errors. I was successfully able to convert the model but I am not able to make predictions\r\nfor tf 2.4. my error is\r\nRuntimeError: Container __per_step_0 does not exist. (Could not find resource: __per_step_0/_tensor_arraysTensorArrayV3_0)\r\n(while executing 'TensorArrayScatterV3' via Eager)Node number 82 (TfLiteFlexDelegate) failed to invoke.\r\n\r\nI also tried using tf-nightly where also I encountered error\r\nRuntimeError: Tensor: tf.TensorArrayV3(56) buffer size mismatch 64(1) != 8(2)Node number 82 (TfLiteFlexDelegate) failed to invoke.", "@sonal-511,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!"]}, {"number": 45863, "title": "[Intel MKL] Enabling few optimizations with native format", "body": "This PR enables couple of optimizations that were exist with internal format but not available with native format:\r\nConv2DBackpropFilterWithBias Fusion.\r\nRemoval of redundant Transpose op.", "comments": []}, {"number": 45862, "title": "[TFTRT - Dynamic Shape Phase 3] Add Dynamic Shape Testing for ConvertArgMinMax", "body": "@bixia1 @tfeher for review\r\n\r\nFeature Tracker: #45481", "comments": ["@DEKHTIARJonathan  Can you please check @bixia1's comments and keep us posted ? Thanks!", "@bixia1 I found the issue:\r\n\r\n```\r\nDefaultLogger my_arg-topk: implementation limit for 4D and higher tensors -- reduceAxes must specify one of last three dimensions\r\n```\r\n\r\nI pushed the fix and now it passes on TRT5 & TRT7. Can you please run the CI ?", "@bixia1 we should be good to go ;)"]}, {"number": 45861, "title": "Tensorflow's hwloc build force-enables use of sys/sysctl.h, which breaks on recent Linux/glibc", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux, openSUSE Tumbleweed, tested on various snapshots up to 20201216 .\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not tested.\r\n- TensorFlow installed from (source or binary): Source.\r\n- TensorFlow version: 1.15.2\r\n- Python version: 3.6.9\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 9.3\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the problem**\r\n\r\nTensorflow vendors the hwloc library and heavily customizes the way in which this library is built. I am not familiar enough with Bazel to fully understand the details of what you are doing here and the reasons why you are doing it, but unfortunately, what I do know is that on my machine, the net result is a broken hwloc build...\r\n\r\nThe immediate symptom is that some hwloc source files do not compile because they are configured to include the `<sys/sysctl.h>` header, which [has been removed from glibc >=2.32](https://sourceware.org/pipermail/libc-announce/2020/000029.html) because the underlying system call has been removed from the Linux kernel since release 5.5.\r\n\r\nThis is not a hwloc bug/incompatibility however, as it would intuitively seem, because the hwloc build system is perfectly able to figure out that this header does not exist and the hwloc source code knows how to avoid using it when that happens.\r\n\r\nThe actual problem is this line of the tensorflow build system: https://github.com/tensorflow/tensorflow/blob/1cd185160a061a1213e8e8d05eb078e880ac9e46/third_party/hwloc/BUILD.bazel#L113\r\n\r\nFor some reason that I do not know, it is pretty clear that you force-set the `HAVE_SYS_SYSCTL_H` define, which would normally be unset by the hwloc build system after it correctly detects that there is no `sysctl.h` header...\r\n\r\nRemoving this line of the `BUILD.bazel` file fixes the build on my machine, but I can only assume that you added it for some reason (most likely to make the build work on an operating system that does use the `sysctl.h` header, but on which the hwloc build system does not correctly detect said header ?), which means that the actual tensorflow patch will need to be more nuanced and only perform this patch on the OS configurations where it is necessary.\r\n\r\nI am not able to easily share the build instructions that I followed because they are inside a mildly complicated build system within a closed-source project. But from my understanding of the problem, detailed reproducer instructions should not be necessary here, you should be able to easily replicate this issue just by trying to build tensorflow from source, through any method of your choosing, on any Linux distribution that uses glibc >=2.32. Although I personally observed this problem on openSUSE Tumbleweed, I would also expect it to reproduce identically on Gentoo, Arch, or Fedora 34...", "comments": ["Does this also happen on `master`? We no longer support the builds for the 1.15 branch, unfortunately. \r\n\r\nI couldn't find any reason for the flag to be set this way in our file history, so if it's still a problem on `master`, we can probably unset it.", "I did not try the master build yet, what I have checked so far is that...\r\n\r\n1. Removing this line of the hwloc BUILD.bazel file fixes the build of tensorflow 1.15 on my machines.\r\n2. This line is still present on the master version of hwloc BUILD.bazel file.\r\n\r\n...which gave me a reasonable suspicion that the build should still fail on master, and thus that the bug is worth reporting.", "I'd think so too, but it'll be good to have a confirmation. If it's easy to try the build at `master` for you, can you verify that it also fails?", "After a quick try, given the same build configuration flags that we use on 1.15, but on a different system with a very different set of installed packages (which happens to be the only one that I have at hand right now), the build succeeds. Which solves my immediate problem.\r\n\r\nHowever, I suspect that hwloc might not get built anymore. Which means that the build could still start failing again given a different system configuration or set of configuration flags that do lead the tensorflow build to use hwloc again.\r\n\r\nWhat leads me to believe this is that in the previous build configuration, I was able to find files associated with hwloc in `~/.cache/bazel`, and now I don't. Though I don't remember if I did search for these after I got my former build to succeed, so if bazel auto-cleans up this directory to some extent when the build succeed, that factor could also come into play.\r\n\r\nWould you, per chance, remember or be able to easily figure out what circumstances lead the tensorflow build to try to build hwloc, so that I don't have to reverse-engineer this information myself?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45861\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45861\">No</a>\n"]}, {"number": 45860, "title": "TF2.4 doc missing keras.backend methods", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/backend\r\n\r\n## Description of issue (what needs changing):\r\n\r\nMissing a lot of `keras.backend` method documentation\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\nIn TF2.4 doc, a lot of methods in keras.backend are missing. E.g., Fig1 is the screenshot of v2.4, while Fig2 is the screenshot of v2.3.\r\n\r\n![image](https://user-images.githubusercontent.com/5104719/102656451-bec4d500-4128-11eb-913e-cc4a3e27e58e.png)\r\n![image](https://user-images.githubusercontent.com/5104719/102656515-d8661c80-4128-11eb-95eb-d7ba74c5d1fc.png)\r\n\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct? N/A\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly? N/A\r\n\r\n### Returns defined\r\n\r\nAre return values defined? N/A\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\nN/A\r\n\r\n### Usage example\r\n\r\nIs there a usage example? N/A\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["In the [Source Code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L2272), it is set not to generate the Docs. ", "Just curious, is this intentional?", "Yes. @rmothukuru's link makes it clear that this is intentional.\r\n\r\n`Keras.backend` was important when there were multiple backends. But now that there's only TensorFlow backend is much less important/useful. They chose a limited set that they thought were worth documenting."]}, {"number": 45859, "title": "Fixes added to remove CorrectTensorEndianness in AllocateTensors", "body": "This PR request is raised with respect to bug #45858 .\r\nA previous discussion was done on a PR #45790 where it was suggested to raise a separate PR with only this fix.\r\n\r\nChanges done:\r\nCalling of CorrectTensorEndianness function is removed as the FlatBufferVectorToTfLiteTypeArray already converts flatbuffer tensor data from little endian to big endian during StartModelAllocation function call within AllocateTensors function.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@nkreeger Can you please review these changes when you some time?", "@njeffrie Thanks for looking into this. Yes, I can remove it but left it in case it is being used in other TF Projects. \r\n", "That would be great. I did a quick double check and these methods are only used from within MicroInterpreter, so they should be safe to remove.\r\n\r\nRemoving this code makes me happy, since is has previously been the source of substantial confusion.", "@njeffrie That's great then! I have removed these methods and pushed the commit. "]}, {"number": 45858, "title": "Need to remove CorrectTensorEndianness in AllocateTensors", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (use command below):2.3.1\r\n- Python version:3.6.9\r\n- Bazel version (if compiling from source):3.4.1\r\n- GCC/Compiler version (if compiling from source):Ubuntu 7.5.0-3ubuntu1~18.04\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n**Describe the current behavior**\r\nOn testing  ```//tensorflow/lite/micro:memory_arena_threshold_test``` on s390x, the TC was failing with a segmentation fault. \r\nOn debugging, it was found the `CorrectTensorDataEndianness` function was byte swapping a tensor which made it cross its data type limit. It looks like the `FlatBufferVectorToTfLiteTypeArray` already converts flatbuffer tensor data from little endian to big endian during `StartModelAllocation` function call within `AllocateTensors` function and ```CorrectTensorDataEndianness``` is not required anymore.\r\nTo make the test case pass, Allocation size values were also changed in `memory_arena_threshold_test.cc` file. Although as suggested in my last PR request #45790, I am raising this bug with a separate PR with removal of  `CorrectTensorDataEndianness` function call. \r\n\r\n**Describe the expected behavior**\r\n`CorrectTensorDataEndianness` is not required as the tensor data is already converted to BE format.\r\n\r\n**Standalone code to reproduce the issue**\r\nCode to reproduce the issue:\r\n```bazel --host_jvm_args=\"-Xms1024m\" --host_jvm_args=\"-Xmx2048m\" test --host_javabase=\"@local_jdk//:jdk\" --test_tag_filters=-gpu,-benchmark-test,-v1only,-no_oss,-oss_serial  -k --test_timeout 300,450,1200,3600 --build_tests_only --test_output=errors -- //tensorflow/lite/micro:memory_arena_threshold_test```\r\n\r\n**Other info / logs**\r\nI am creating a new PR request with only this change as suggested by @advaitjain. ", "comments": ["Closing as related PR is merged.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45858\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45858\">No</a>\n"]}, {"number": 45856, "title": "Fixing conv_ops_benchmark_test", "body": "This PR fixes the test //tensorflow/core/kernels:conv_ops_benchmark_test which was no longer runnable with below command due to recent changes to the test framework.\r\nbazel -c opt //tensorflow/core/kernels:conv_ops_benchmark_test -- --benchmarks=all", "comments": []}, {"number": 45855, "title": "[Intel MKL] Enabling quantized matmul with native format", "body": "", "comments": ["Made some refactoring changes based on review comments on similar changes in another PR.", "no worries, thank you for reviewing/approving this."]}, {"number": 45854, "title": "MultiHeadAttention masking mechanism", "body": "Hello!\r\n\r\nI wonder how we should apply masks (both padding and look-ahead) to the MultiHeadAttention layer, described in:\r\n\r\n[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/multi_head_attention.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/multi_head_attention.py)\r\n\r\nI have been trying to adapt the tutorial [https://www.tensorflow.org/tutorials/text/transformer](https://www.tensorflow.org/tutorials/text/transformer) with this layer but it seems like I'm having some masking problems (loss strangely ~100 times lower than usual and worse empirical results). The tutorial implementation works fine, but replacing the MultiHeadAttention with the one in tf.keras.layers just breaks it.\r\n\r\nI'm pretty sure I'm missing something, but I can't figure it out by reading the implementation. What about including an example for both padding and look-ahead masks? I think it would be easy and useful.\r\n\r\nAnother option would be updating the tutorial (as it happened with LayerNormalization) though I'm afraid that would have more impact.\r\n\r\ncc @tanzhenyu Sorry for the spam but I believe you are the one on charge about this.\r\n\r\nEDIT: I don't know if on charge, but I see you every time I come around issues/commits pages.", "comments": ["Okay, I have been researching and I think I just discovered that those implementations have the masks bits inverted.\r\n\r\ntf.keras.layers.MultiHeadAttention uses Softmax class, which can take a mask in the call function (1 being positions to attend):\r\n- https://github.com/tensorflow/tensorflow/blob/635e8a0749701f27928f766e651a0c07ff9c0fee/tensorflow/python/keras/layers/multi_head_attention.py#L388\r\n- https://github.com/tensorflow/tensorflow/blob/635e8a0749701f27928f766e651a0c07ff9c0fee/tensorflow/python/keras/layers/multi_head_attention.py#L401\r\n- https://github.com/tensorflow/tensorflow/blob/635e8a0749701f27928f766e651a0c07ff9c0fee/tensorflow/python/keras/layers/advanced_activations.py#L324\r\n\r\nOn the other hand, in the tutorial it is just the opposite:\r\n- https://www.tensorflow.org/tutorials/text/transformer#masking\r\n\r\nCan anyone in the team confirm this?\r\n\r\nIf this is the case, I think it would be great to add this information to the *attention_mask* call argument in MultiHeadAttention, as it is stated in tf addons [https://www.tensorflow.org/addons/api_docs/python/tfa/layers/MultiHeadAttention#arguments](https://www.tensorflow.org/addons/api_docs/python/tfa/layers/MultiHeadAttention#arguments).\r\n\r\nAlso, some consistency between a (pretty sure) super popular tutorial and a posterior official implementation would be great.\r\n\r\n\r\n", "Yes, the attention pattern is consistent with https://www.tensorflow.org/addons/api_docs/python/tfa/layers/MultiHeadAttention#arguments.\r\nWorking on a commit to update. Thanks for suggestions", "Update the docstring as commit: https://github.com/tensorflow/tensorflow/commit/5e90f547a295efeaff92a8c1ef2b4da568485124\r\n\r\n@ymodak Is it possible to update the transformer tutorial to use more core keras components?", "Thank you @saberkun . ", "@claverru I think the original issue was resolved. \r\n\r\nI am closing this issue as the original issue was resolved. Please feel free to reopen if I am mistaken. Thanks!", "Is it possible to get an example of mask using with this function ?"]}, {"number": 45853, "title": "Tensorflow 2.3.0 MKL Intel AVX Binary Issue", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): custom code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): tensorflow-mkl 2.3.0\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.7.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: \r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n2.3.0\r\n\r\n**Describe the current behavior**\r\nHere's the code:\r\n\r\n```\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\na = np.array([2 , 4, 5])\r\nap=tf.constant(a)\r\n\r\n```\r\n\r\nHere's the warning message: \r\n\r\n> \" I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\n> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\"\r\n\r\nMoreover, if I run system check mentioned on Intel's website (https://software.intel.com/content/www/us/en/develop/articles/intel-optimization-for-tensorflow-installation-guide.html),\r\n\r\nCode:\r\n\r\n```\r\nimport tensorflow as tf\r\nmajor_version = int(tf.__version__.split(\".\")[0])\r\nif major_version >= 2:\r\n   from tensorflow.python import _pywrap_util_port\r\n   print(\"MKL enabled:\", _pywrap_util_port.IsMklEnabled())\r\nelse:\r\n   print(\"MKL enabled:\", tf.pywrap_tensorflow.IsMklEnabled()) \r\n```\r\nI get \r\n\r\n> MKL enabled: False\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nI shouldn't get the warning about AVX because I am using Intel's MKL version. Moreover, surprisingly, if I downgrade tensorflow to 2.1, warning changes to the issue described https://github.com/tensorflow/tensorflow/issues/45632. \r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\na = np.array([2 , 4, 5])\r\nap=tf.constant(a)\r\n\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nCPU Version: Intel i7-10610U CPU @1.8GHz\r\nI have installed Tensorflow in a new environment to ensure that there is no issue with dependencies.\r\n\r\n\r\n", "comments": ["Please add label \"comp:mkl\", as requested at https://software.intel.com/content/www/us/en/develop/articles/intel-optimization-for-tensorflow-installation-guide.html", "\"I shouldn't get the warning about AVX because I am using Intel's MKL version.\". The warning is not complaining that AVX is not enabled. It is just an informative message saying *only* for some ops, Intel's oneDNN would be used and AVX or AVX2 would be turned on. ", "> \r\n> \r\n> \"I shouldn't get the warning about AVX because I am using Intel's MKL version.\". The warning is not complaining that AVX is not enabled. It is just an informative message saying _only_ for some ops, Intel's oneDNN would be used and AVX or AVX2 would be turned on.\r\n\r\nThanks for your help. My hypothesis is that if we use Intel's MKL version, we shouldn't get the warning because Tensorflow would be optimized for our Intel processor. So, I believe we shouldn't get the informative message (_This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX AVX2_). More importantly, given what you have said, we don't know whether we are getting full performance out of our CPU. Also, if we check whether mkl_enabled flag (as per documentation on Intel's website), it shows that MKL is not. I also tested with Tensorflow 2.1: the warning changes, but the flag is enabled. (https://github.com/tensorflow/tensorflow/issues/45632) _I think_ there is an issue with Intel's Tensorflow-mkl package. I am a beginner. So, much of what I have written could not be right.", "@gmatalongthewatchtower \r\n\r\nI run following code, get the result:\r\n`MKL enabled: True`\r\n\r\n```\r\nimport tensorflow as tf\r\nmajor_version = int(tf.__version__.split(\".\")[0])\r\nif major_version >= 2:\r\n   from tensorflow.python import _pywrap_util_port\r\n   print(\"MKL enabled:\", _pywrap_util_port.IsMklEnabled())\r\nelse:\r\n   print(\"MKL enabled:\", tf.pywrap_tensorflow.IsMklEnabled()) \r\n```\r\n\r\nCould you check your Tensorflow version?\r\n\r\nI install it by:\r\n`conda install tensorflow=2.3.0=mkl_py37h0481017_0`\r\n\r\nIf you want to confirm the MKL(oneDNN) is enabled, please set the ENV:\r\nexport MKLDNN_VERBOSE=1\r\n\r\nYou will see following log if Tensorflow-MKL is installed/built successfully.\r\n```\r\n2020-12-22 12:24:35.981441: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-12-22 12:24:36.014367: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz\r\n2020-12-22 12:24:36.020244: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55cdf9586130 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-12-22 12:24:36.020286: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-12-22 12:24:36.020435: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n\r\nInput image shape: (100, 224, 224, 3)\r\ndnnl_verbose,info,oneDNN v1.4.0 (commit N/A)\r\ndnnl_verbose,info,cpu,runtime:OpenMP\r\ndnnl_verbose,info,cpu,isa:Intel AVX-512 with Intel DL Boost\r\ndnnl_verbose,info,gpu,runtime:none\r\ndnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:acdb:f0 dst_f32::blocked:abcd:f0,,,32x3x224x224,49.5181\r\ndnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:cdba:f0 dst_f32::blocked:Acdb16a:f0,,,64x3x7x7,0.413818\r\ndnnl_verbose,exec,cpu,convolution,jit:avx512_common,forward_training,src_f32::blocked:abcd:f0 wei_f32::blocked:Acdb16a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:aBcd16b:f0,,alg:convolution_direct,mb32_ic3oc64_ih224oh112kh7sh2dh0ph3_iw224ow112kw7sw2dw0pw3,11.1809\r\ndnnl_verbose,exec,cpu,batch_normalization,bnorm_jit:avx512_common,forward_inference,data_f32::blocked:aBcd16b:f0 diff_undef::un\r\n```\r\n", "@NeoZhangJianyu \r\nHere's the output of tf version:\r\n\r\n```\r\ntf.__version__\r\n'2.3.0'\r\n```\r\n\r\nHere's the output from MKL code:\r\n\r\n```\r\nimport tensorflow as tf\r\nmajor_version = int(tf.__version__.split(\".\")[0])\r\nif major_version >= 2:\r\n    from tensorflow.python import _pywrap_util_port\r\n    print(\"MKL enabled:\", _pywrap_util_port.IsMklEnabled())\r\nelse:\r\n    print(\"MKL enabled:\", tf.pywrap_tensorflow.IsMklEnabled()) \r\n\r\n\r\nMKL enabled: False\r\n```\r\n\r\nHere's the package name after running ```conda list``` command:\r\n\r\n`tensorflow-mkl            2.3.0                h93d2e19_0`\r\n\r\nYou have told me to set `export MKLDNN_VERBOSE=1`. This looks like Linux environment variable. How can I set this on Windows 10? \r\n\r\nThanks so much for helping me with this issue. \r\n\r\nCan you please let me know if you have more questions?", "@gmatalongthewatchtower \r\nIn windows, `set MKLDNN_VERBOSE=1`", "@NeoZhangJianyu : Thanks again for your help. I did ```set MKLDNN_VERBOSE=1```. How do I verify whether this flag is set? I ran the following code:\r\n```\r\nimport tensorflow as tf\r\nmajor_version = int(tf.__version__.split(\".\")[0])\r\nif major_version >= 2:\r\n    from tensorflow.python import _pywrap_util_port\r\n    print(\"MKL enabled:\", _pywrap_util_port.IsMklEnabled())\r\nelse:\r\n    print(\"MKL enabled:\", tf.pywrap_tensorflow.IsMklEnabled()) \r\n```\r\n\r\nI am still getting ```MKL enabled: False```\r\n\r\nI ran the following sample code to test verbosity of mkl:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\ninput_A = keras.layers.Input(shape=[5], name=\"wide_input\")\r\ninput_B = keras.layers.Input(shape=[6], name=\"deep_input\")\r\nhidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\r\nhidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\r\nconcat = keras.layers.concatenate([input_A, hidden2])\r\noutput = keras.layers.Dense(1, name=\"output\")(concat)\r\nmodel = keras.models.Model(inputs=[input_A, input_B], outputs=[output])\r\n\r\n```\r\n\r\nHere's the output:\r\n\r\n> [2020-12-23 12:18:25.936282: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\n> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.]([url](url))", "@gmatalongthewatchtower \r\nAs explain of @wei-v-wang , the warning is not complaining that AVX is not enabled. \r\nCould paste more log?", "Hello @NeoZhangJianyu, I did respond to @wei-v-wang above. It is true that warning is not complaining, but there are two red flags. 1) We don't know whether we are getting full performance out of our CPU. 2) if you see mkl_enabled flag (as per documentation on Intel's website), it shows that MKL is not enabled. \r\n\r\nI am happy to generate log files. Can you please let me know the code and steps to generate these? I am using tf 2.3 on Windows 10.\r\n", "@gmatalongthewatchtower \r\n1) check MKL is working in your TF.\r\n\r\nWindows:\r\n`set MKLDNN_VERBOSE=1`\r\n\r\nLinux: \r\n`export MKLDNN_VERBOSE=1`\r\n\r\nExecute following python script based on TF.\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nx_in = np.array([[\r\n  [[2], [1], [2], [0], [1]],\r\n  [[1], [3], [2], [2], [3]],\r\n  [[1], [1], [3], [3], [0]],\r\n  [[2], [2], [0], [1], [1]],\r\n  [[0], [0], [3], [1], [2]], ]])\r\nkernel_in = np.array([\r\n [ [[2, 0.1]], [[3, 0.2]] ],\r\n [ [[0, 0.3]],[[1, 0.4]] ], ])\r\nx = tf.constant(x_in, dtype=tf.float32)\r\nkernel = tf.constant(kernel_in, dtype=tf.float32)\r\ntf.nn.conv2d(x, kernel, strides=[1, 1, 1, 1], padding='VALID')\r\n\r\n```\r\n\r\nyou will see the log which show oneDNN is enabled:\r\n```\r\n2020-12-25 09:20:59.557263: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-12-25 09:20:59.582017: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\ndnnl_verbose,info,oneDNN v1.4.0 (commit N/A)\r\ndnnl_verbose,info,cpu,runtime:OpenMP\r\ndnnl_verbose,info,cpu,isa:Intel AVX2\r\ndnnl_verbose,info,gpu,runtime:none\r\ndnnl_verbose,exec,cpu,reorder,simple:any,undef,src_f32::blocked:cdba:f0 dst_f32:p:blocked:Acdb8a:f0,,,2x1x2x2,0.0439\r\ndnnl_verbose,exec,cpu,convolution,jit:avx2,forward_training,src_f32::blocked:abcd:f0 wei_f32:p:blocked:Acdb8a:f0 bia_undef::undef::f0 dst_f32:p:blocked:aBcd8b:f0,,alg:convolution_direct,mb1_ic1oc2_ih5oh4kh2sh1dh0ph0_iw5ow4kw2sw1dw0pw0,0.0589\r\ndnnl_verbose,exec,cpu,reorder,simple:any,undef,src_f32:p:blocked:aBcd8b:f0 dst_f32::blocked:acdb:f0,,,1x2x4x4,0.0295\r\n```\r\n\r\n2.  MKL enabled issue\r\nI reproduce your issue by following code.\r\n```\r\nimport tensorflow as tf\r\nmajor_version = int(tf.__version__.split(\".\")[0])\r\nif major_version >= 2:\r\n    from tensorflow.python import _pywrap_util_port\r\n    print(\"MKL enabled:\", _pywrap_util_port.IsMklEnabled())\r\nelse:\r\n    print(\"MKL enabled:\", tf.pywrap_tensorflow.IsMklEnabled()) \r\n```\r\n\r\nThe root cause is tensorflow is MKL version and tensorflow-base is GPU version.\r\nRun `conda list`, I see:\r\n```\r\ntensorflow-2.3.0-mkl_py37h936c3e2_0\r\ntensorflow-base-2.3.0-gpu_py37h18d21e4_0\r\n```\r\n\r\nI fix it by remove them and reinstall by:\r\n```\r\nconda install tensorflow tensorflow-base=2.3.0=mkl_py37h7075554_0\r\nconda list\r\n...\r\ntensorflow                2.3.0           mkl_py37he70e3f7_0    defaults\r\ntensorflow-base           2.3.0           mkl_py37h7075554_0    defaults\r\n...\r\n```\r\n\r\n\r\n", "@NeoZhangJianyu: Thanks so much for your guidance. \r\n\r\n**1. Generate logs** \r\n\r\nI did try to run ```set mkldnn verbose=1``` in Windows Command Prompt, but I don't see verbose logs. I believe this ```set``` command is not being detected by python. Could you please guide me where should I run this command? I have tried running it by doing all of these below: \r\n1) on Anaconda Prompt that comes with Anaconda using Administrative privileges. \r\n2) I use PyCharm. I tried running it from the terminal using Admin privileges.\r\n3) Anaconda NAvigator -> Environment -> Right click terminal -> run command using Admin privileges. \r\n4) I googled this and found that we can write a code as well. \r\nI ran the following code and then the code that you gave me:\r\n```\r\nimport mkl\r\nmkl.verbose(1)\r\n```\r\n\r\nI ran the code you gave me, and here's the output:\r\n\r\n```\r\nMKL_VERBOSE Intel(R) MKL 2020.0 Update 2 Product build 20200624 for Intel(R) 64 architecture Intel(R) Advanced Vector Extensions 2 (Intel(R) AVX2) enabled processors, Win 1.80GHz lp64 intel_thread\r\nMKL_VERBOSE SDOT(2,0000020AB5779ED0,1,0000020AB5779ED0,1) 60.51us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\n2020-12-26 02:14:32.134646: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n\r\n<tf.Tensor: shape=(1, 4, 4, 2), dtype=float32, numpy= \r\n...\r\n```\r\n\r\nCould you please let me know your thoughts?\r\n\r\n**2. Version-check**\r\nI did run ```conda list```. \r\n\r\nHere are the *tensorflow* packages I have:\r\n\r\n```\r\ntensorboard               2.3.0              pyh4dce500_0\r\ntensorboard-plugin-wit    1.6.0                      py_0\r\ntensorflow                2.3.0           mkl_py37h3bad0a6_0\r\ntensorflow-base           2.3.0           eigen_py37h17acbac_0\r\ntensorflow-datasets       1.2.0                    py37_0\r\ntensorflow-estimator      2.3.0              pyheb71bc4_0\r\ntensorflow-metadata       0.14.0             pyhe6710b0_1\r\ntensorflow-mkl            2.3.0                h93d2e19_0\r\ntermcolor                 1.1.0                    py37_1\r\n```\r\n\r\nPlease note that I haven't done anything special to install `tensorflow` except following the instructions on Intel's website posted above. I am not sure whether `eigen-tensorflow-base` package is same as gpu-version.\r\n\r\nCould you please guide me? Thanks for your help. I look forward to hearing from you. \r\n", "@gmatalongthewatchtower \r\nI think your tensorflow release is with problem:\r\ntensoflow is for mkl, but tensorflow-base is for eigen.\r\n\r\nI guess it be the bug in conda to install the old tensorflow release.\r\nIn newer release installation,  I see it disappear.\r\n\r\nYes.  I have provided the guide in above comments.\r\n\r\n1. Please remove the tensorflow by conda command\r\n\r\n2. Reintall it by:\r\n`conda install tensorflow tensorflow-base=2.3.0=mkl_py37h7075554_0`\r\n\r\n3. check the tensorflow release.\r\n```\r\nconda list\r\n...\r\ntensorflow                2.3.0           mkl_py37he70e3f7_0    defaults\r\ntensorflow-base           2.3.0           mkl_py37h7075554_0    defaults\r\n...\r\n\r\n```\r\n\r\n4. test the MKL enable by my comment above.\r\n", "Thanks @NeoZhangJianyu.\r\n\r\nI installed tensorflow as per your suggestion:\r\n\r\nHere's ```conda list``` \r\nOutput:\r\n```\r\ntensorflow                2.3.0           mkl_py37he70e3f7_0\r\ntensorflow-base           2.3.0           mkl_py37h7075554_0\r\ntensorflow-estimator      2.3.0              pyheb71bc4_0\r\n```\r\n\r\n**Test for mkl**\r\n\r\nCode:\r\n```\r\nimport tensorflow as tf\r\nmajor_version = int(tf.__version__.split(\".\")[0])\r\nif major_version >= 2:\r\n    from tensorflow.python import _pywrap_util_port\r\n    print(\"MKL enabled:\", _pywrap_util_port.IsMklEnabled())\r\nelse:\r\n    print(\"MKL enabled:\", tf.pywrap_tensorflow.IsMklEnabled()) \r\n```\r\nOutput:\r\n```MKL enabled: True```\r\n\r\n\r\n**MKL vs. non-MKL test:**\r\nNow I test whether MKL is doing better than standard tensorflow 2.3 (```conda install tensorflow```).\r\n\r\nCode:\r\n```\r\nfrom tensorflow.keras.layers import Input, Dense, LSTM, Bidirectional, Conv1D\r\nfrom tensorflow.keras.layers import Flatten, Dropout\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.optimizers import Adam\r\nimport tensorflow.keras.backend as K\r\nimport numpy as np\r\nfrom time import time\r\n\r\n\r\ndef timeit(func, iterations, *args):\r\n    t0 = time()\r\n    for _ in range(iterations):\r\n        func(*args)\r\n    print(\"Time/iter: %.4f sec\" % ((time() - t0) / iterations))\r\n\r\ndef make_small_model(batch_shape):\r\n    ipt   = Input(batch_shape=batch_shape)\r\n    x     = Conv1D(128, 400, strides=4, padding='same')(ipt)\r\n    x     = Flatten()(x)\r\n    x     = Dropout(0.5)(x)\r\n    x     = Dense(64, activation='relu')(x)\r\n    out   = Dense(1,  activation='sigmoid')(x)\r\n    model = Model(ipt, out)\r\n    model.compile(Adam(lr=1e-4), 'binary_crossentropy')\r\n    return model\r\n\r\ndef make_medium_model(batch_shape):\r\n    ipt   = Input(batch_shape=batch_shape)\r\n    x     = Bidirectional(LSTM(512, activation='relu', return_sequences=True))(ipt)\r\n    x     = LSTM(512, activation='relu', return_sequences=True)(x)\r\n    x     = Conv1D(128, 400, strides=4, padding='same')(x)\r\n    x     = Flatten()(x)\r\n    x     = Dense(256, activation='relu')(x)\r\n    x     = Dropout(0.5)(x)\r\n    x     = Dense(128, activation='relu')(x)\r\n    x     = Dense(64,  activation='relu')(x)\r\n    out   = Dense(1,   activation='sigmoid')(x)\r\n    model = Model(ipt, out)\r\n    model.compile(Adam(lr=1e-4), 'binary_crossentropy')\r\n    return model\r\n\r\ndef make_data(batch_shape):\r\n    return np.random.randn(*batch_shape), np.random.randint(0, 2, (batch_shape[0], 1))\r\n\r\nbatch_shape = (32, 400, 16)\r\nX, y = make_data(batch_shape)\r\n\r\nmodel_small = make_small_model(batch_shape)\r\nmodel_small.train_on_batch(X, y)  # skip first iteration which builds graph\r\ntimeit(model_small.train_on_batch, 200, X, y)\r\n\r\nK.clear_session()  # in my testing, kernel was restarted instead\r\n\r\nmodel_medium = make_medium_model(batch_shape)\r\nmodel_medium.train_on_batch(X, y)  # skip first iteration which builds graph\r\ntimeit(model_medium.train_on_batch, 10, X, y)\r\n#endregion\r\n```\r\n**Without MKL**\r\n\r\nWarning message:\r\n```\r\n2020-12-31 19:06:04.281835: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n```\r\nOutput: \r\n```\r\nOut[2]: 0.6387945413589478\r\nTime/iter: 0.1114 sec\r\nOut[2]: 0.6915708780288696\r\nTime/iter: 20.7256 sec\r\n```\r\n\r\n**With MKL**\r\n\r\nWarning message:\r\n```\r\n2020-12-31 18:32:50.096360: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-12-31 18:32:50.097360: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n```\r\nOutput:\r\n```\r\nOut[2]: 0.6798648834228516\r\nTime/iter: 0.1222 sec\r\nOut[2]: 327.92877197265625\r\n<Timed out...I had to stop the kernel after ~50 minutes>\r\n```\r\n\r\n**Given above data, I have three questions, if you don't mind:**\r\n\r\n1) It seems base tensorflow is much faster than MKL version for the above code. Why so? At the time of installing tensorflow, I got the following question from Conda:\r\n```\r\nThe following packages will be DOWNGRADED:\r\n\r\n  intel-openmp                                   2020.2-254 --> 2019.4-245\r\n```\r\nIt seems TF is using older library. I am not too sure. Could this explain the slowness?\r\n\r\n\r\n2) While I was able to install MKL using the version you provided. Tensorflow  2.4 is already out. In the future, how do I know what version to install? \r\n\r\n3) I also noticed that MKL version doesn't use all 8 cores. Non-MKL version (please see package details below) uses all 8 cores. \r\n```\r\ntensorflow                2.3.0           mkl_py37h04bc1aa_0\r\ntensorflow-base           2.3.0           eigen_py37h17acbac_0\r\ntensorflow-estimator      2.3.0              pyheb71bc4_0\r\n```\r\n", "@gmatalongthewatchtower \r\n\r\n1. The TF with MKL needs to be set optimization setting.\r\nI try following setting in linux, got shorter time than stack TF in small_model (CNN).\r\nBut a little longer time than stack TF in medium model (LSTM).\r\n\r\nDifferent model need different setting to reach best performance on CPU.\r\nYou could try to adjust it based on your CPU.\r\n```\r\nset TF_ENABLE_MKL_NATIVE_FORMAT=1  \r\nset TF_NUM_INTEROP_THREADS=1\r\nset TF_NUM_INTRAOP_THREADS=4\r\nset  OMP_NUM_THREADS=4\r\nset KMP_BLOCKTIME=1\r\nset KMP_AFFINITY=granularity=fine,compact,1,0\r\n```\r\nAdditional, we recommend to set the physical CPU cores number in above setting, instead of threads number.\r\nfor example,  your CPU is  i7-10610U, 4 cores/8 threads.\r\nYou should use 4 in above setting.\r\n\r\nThe conda will upgrade/downgrade the depended package during installing tensorflow.\r\nSo,  intel-openmp is downgraded by conda.\r\n\r\n2. In conda, you could use following cmd to search which Tensorflow is supported by conda:\r\n```\r\nconda search tensorflow\r\n```\r\n\r\nCurrently, the latest release is 2.3\r\n\r\nBut you could build the TF2.4 from source code by bazel.\r\n\r\n3.  Yes, you find the cause.\r\nWe should try to let the CPU cores are busy to get better performance.\r\n\r\n\r\n", "Thanks @NeoZhangJianyu. \r\n\r\n#1: With above settings, Intel's MKL is much slower. \r\n\r\n#2: I searched for tensorflow, and here's what I got. How do I know which flavor of tensorflow to pick? I can decide between Python 3.7 and 3.8, but I am not sure about mkl versions. \r\n\r\n```\r\ntensorflow                     2.3.0 mkl_py37h04bc1aa_0  pkgs/main\r\ntensorflow                     2.3.0 mkl_py37h10aaca4_0  pkgs/main\r\ntensorflow                     2.3.0 mkl_py37h3bad0a6_0  pkgs/main\r\ntensorflow                     2.3.0 mkl_py37h48e11e3_0  pkgs/main\r\ntensorflow                     2.3.0 mkl_py37h856240d_0  pkgs/main\r\ntensorflow                     2.3.0 mkl_py37h936c3e2_0  pkgs/main\r\ntensorflow                     2.3.0 mkl_py37h952ae9f_0  pkgs/main\r\ntensorflow                     2.3.0 mkl_py37he40ee82_0  pkgs/main\r\ntensorflow                     2.3.0 mkl_py37he70e3f7_0  pkgs/main\r\ntensorflow                     2.3.0 mkl_py38h1fcfbd6_0  pkgs/main\r\ntensorflow                     2.3.0 mkl_py38h37f7ee5_0  pkgs/main\r\ntensorflow                     2.3.0 mkl_py38h3c6dea5_0  pkgs/main\r\ntensorflow                     2.3.0 mkl_py38h46e32b0_0  pkgs/main\r\ntensorflow                     2.3.0 mkl_py38h637f690_0  pkgs/main\r\ntensorflow                     2.3.0 mkl_py38h8557ec7_0  pkgs/main\r\ntensorflow                     2.3.0 mkl_py38h8c0d9a2_0  pkgs/main\r\ntensorflow                     2.3.0 mkl_py38ha39cb68_0  pkgs/main\r\ntensorflow                     2.3.0 mkl_py38hd19cc29_0  pkgs/main\r\n```\r\n\r\n3: Also, is there anyway to use Intel GPU?", "@gmatalongthewatchtower \r\n1.\r\nThe optimization setting is depended on your system (HW, OS, SW).\r\nI try the setting in Intel Xeon CPU, Linux. It maybe not adapt to your system. \r\nPlease tuning the values to get better performance.\r\nYou could check if the CPU is full when running.\r\n\r\n2. The newer is better.\r\n\r\n3.  Tensorflow support Intel GPU is coming.\r\nBy now, you could use OpenVINO for inference, accelerated by Intel Integrated GPU.\r\n", "same problem for me, but I tried all above methods but did not work :( ", "@Tlevi16 \r\nWhat's your CPU type? And what's the test case of your?\r\n\r\nCould you provide detailed info of your case? like, test script, optimization setting.", "by CPU, do u mean my processor \r\n??", "Yes.\r\nTensorflow MKL is optimized for Intel CPU.\r\nThis issue is about performance issue on Intel CPU only.", "if yes, it's \r\nINTEL(R) CORE(TM) i5- 8250U CPU @ 1.60 GHz 1.80 GHz ", "and 64 bit operating system, windows 10 home ", "Yes.  It's OK!\r\nCould you share your test script or more detailed info about your test?\r\n\r\nI try to reproduce your case in my PC. Then try to find the solution for your case.", "`import tensorflow as tf \r\nimport numpy as np \r\n\r\na = tf.constant(np.array([1., 2., 3.]))\r\nb = tf.constant(np.array([.4, .5, .6])) \r\nc = tf.tensordot(a, b, 1) \r\n\r\noutput = c.numpy() ` \r\n\r\nbut when I run this program, I get the error :- \r\n\r\n`I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: SSE4.2 AVX AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.` \r\n\r\n", "what should I do now ????????", "pleaseee help me ", "@Tlevi16 \r\nI believe you have met the performance issue in Tensorflow 2.3.0 MKL.\r\nI'd like to answer your question in this issue.\r\n\r\nFor above error in your simple case, it's another issue.\r\nIs it possible you create another github issue to track it?\r\n\r\nI guess your Tensorflow running environment is changed recently.", "I am new to tensorflow i just started it today, yeah I changed my tensorflow running environment ", "pls give a solution ", "to my problem ", "ya I'll create another github issue @NeoZhangJianyu , what should I name it as ??", "pls reply ", "@Tlevi16 \r\nWelcome to Tensorflow world!\r\n\r\nIs it possible to create a new issue of your case?\r\nWe want to keep this issue to focus on the original issue. \r\nThe discussion will help thousands of  developers for same issue.\r\n\r\nThank you!", "ok but what can I name it as ??", "I just test your case.\r\nIt's passed.\r\n\r\nThe warning is just warning.\r\nYour CPU supports AVX2. You could ignore it.\r\n\r\n", "ok thx ", "@gmatalongthewatchtower \r\nCould you please try on latest stable version of TF 2.5  and let us know if this is still an issue.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45853\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45853\">No</a>\n", "> > \r\n> \r\n> \u4eb2\uff0c\u6709\u6ca1\u6709\u5efa\u8bae\u7684\u6df1\u5ea6\u5b66\u4e60\u52a0\u901f\u5e93\uff0cstep-by\u2014step\u7684\u6307\u5357\uff1fondnn,openvino\u90fd\u6ca1\u5f04\u6210\uff0c\u5e38\u5e38\u53c2\u8003\u8d44\u6599\u8fc7\u65f6\u7f3a\u4e1c\u5c11\u897f\u3002(linux\u7cfb\u7edf)\u4e0d\u80dc\u611f\u6fc0\r\n\r\n\r\n\r\n> @gmatalongthewatchtower 1. The optimization setting is depended on your system (HW, OS, SW). I try the setting in Intel Xeon CPU, Linux. It maybe not adapt to your system. Please tuning the values to get better performance. You could check if the CPU is full when running.\r\n> \r\n> 2. The newer is better.\r\n> 3. Tensorflow support Intel GPU is coming.\r\n>    By now, you could use OpenVINO for inference, accelerated by Intel Integrated GPU.\r\n\r\n\u4eb2\uff0c\u6709\u6ca1\u6709\u5efa\u8bae\u7684\u6df1\u5ea6\u5b66\u4e60\u52a0\u901f\u5e93\uff0cstep-by\u2014step\u7684\u6307\u5357\uff1fondnn,openvino\u90fd\u6ca1\u5f04\u6210\uff0c\u5e38\u5e38\u53c2\u8003\u8d44\u6599\u8fc7\u65f6\u7f3a\u4e1c\u5c11\u897f\u3002(linux\u7cfb\u7edf)\u4e0d\u80dc\u611f\u6fc0", "@chuangzhidan \r\nCould you create a new github issue for your question? This issue is already closed.\r\nIt's OK to ask your question in Chinese. (topic should be in English)\r\n\r\nThank you!"]}, {"number": 45852, "title": "TPU compile fail using the new 2.4.0 version", "body": "After Colab update its tensorflow version to 2.4.0 recently, I have found that the code which I used to run the example https://colab.research.google.com/drive/1DvLiYBaolddDFok1FttwEkxHNhsRwK_R?usp=sharing has led to very peculiar behavior. In general, the reported bug is \r\n```\r\nCompilation failure: Expected element type in shape to be arithmetic type for operation subtract; got PRED.\r\n```\r\n\r\nI have not been able to pinpoint the root cause of the bug. However, it would seem that all the bug is caused by a substraction operation. When I remove the substraction operation in the code, another substraction issue pops up, albiet in a different location. It is impossible to remove all the substraction in the code. Therefore it would really be helpful if one can pinpoint the problem. \r\n\r\nPS: The code runs perfectly fine under CPU/GPU environment. ", "comments": ["With some experiments, I have found out that the problem is caused by sparsemax layer from tensorflow_addon. When replacing everything with softmax the situation resolves naturally. I will continue to find a way to fix the sparsemax problem. ", "@rwbfd,\r\nCan you please provide access to your [Colab File](https://colab.research.google.com/drive/1DvLiYBaolddDFok1FttwEkxHNhsRwK_R?usp=sharing). You can share the `Github Gist` (`File -> Save as Github Gist`). Thanks!", "@rmothukuru  Thank you for your timely response. This is the address. https://colab.research.google.com/gist/rwbfd/a3a55484cc742bbb674323d3b87b107e/tabnet_tf.ipynb\r\n\r\nHowever, when examing the source code, I find the backward pass for sparsemax activation seems to be missing. In my experiment with PyTorch, this can lead to disasterous bahevior. Is this the case? If so, I will be happy to provide a solution, but first the forward pass must be fixed.", "I think this looks like an internal bug, so we might need to release a new version of the TPU runtime to fix this. Will coordinate internally and let you know.\r\n\r\nInternal reference: b/173796434", "@rwbfd Can you try again next week. We have pushed out a new version of the TPU runtime, but it will take a day or two to propagate to all TPUs.", "@frankchn That would be great! Thanks!", "@frankchn Unfortunately, the same problem still persists. ", "Ah that's too bad. I'll have to investigate further.", "Was able to replicate the issue in TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/a6cfbc76cab948e5501f98974b6c7e4c/untitled187.ipynb)..Thanks !", "Now the error message for failure has been added to address the problem, please find the [gist](https://colab.research.google.com/gist/sachinprasadhs/27b052abefba96c18efa911dfabffd8a/untitled187.ipynb) for reference. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45852\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45852\">No</a>\n"]}, {"number": 45851, "title": "Unexpected behavior when a function which involves tf.reshape is run using strategy.run on a TPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- TensorFlow version (use command below): 2.4.0\r\n\r\n**Describe the current behavior**\r\nWhen computing a function which involves `tf.reshape` using `strategy.run` and a TPU, there is an unexpected behavior and errors are thrown. The function works fine if called directly.\r\nThis is the same error I get when I use that function as a loss function in a model.\r\n\r\n**Describe the expected behavior**\r\nI would expect the same behavior when the function is called directly and using strategy.run. Is there something I am missing out?\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/18yHVyEwbbfYXJ_r3XfP_NaStiVHqwkzN?usp=sharing\r\n\r\nI am not completely sure this is a bug of Tensorflow and not a bug in my code, if you could help me I would really appreciate it.\r\nThank you very much for your help\r\n", "comments": ["@aurelio-amerio,\r\nWas able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/c0f92ce5142c9a12b7ec9d883132bed9/45851-2-3.ipynb), [TF v2.4](https://colab.research.google.com/gist/amahendrakar/0b157e29d05cd9b67438827b5bc435d6/45851.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/2b099f98b5b9a95e00a978d32ba807f0/45851-2-5.ipynb). Please check the linked gist for reference. Thanks!", "I have solved the problem on tf 2.4 using `tf.map_fn`. It should work also on the other versions, though I have not tested it. Here is the working notebook for future reference:\r\nhttps://colab.research.google.com/gist/aurelio-amerio/a15e8646fbb7877120a2f9f33176fd95/test_amerio_26_12_2020.ipynb", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45851\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45851\">No</a>\n"]}, {"number": 45850, "title": "[INTEL MKL] Graph pattern matcher for grappler.", "body": "This PR is to help finding large pattern (e.g. tf.nn.gelu) in a the data flow graph for grappler graph optimization. This has been motivated from pattern matching that exists in the graph_transforms tools (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md)\r\n\r\nIntended use is for remapper optimizer. User can get an initial match for a pattern consisting of op types in the nodes with a simple grammar like \r\n\r\n```\r\nleaf_pattern ::= `{` op_type `}`\r\npattern ::= leaf_pattern |\r\n                  `{` op_type `,` `{` pattern `,` ... `,` pattern `}` `}`\r\n```\r\n\r\nHere a pattern syntax has a root and children (typically input ops feeding the root). Each child is a sub-pattern in a recursive manner. After an initial match has been found, additional constraint like device, shapes, constant values etc. will be used for determining a true match for a fusion.", "comments": ["@ezhulenev Thanks for approving this PR.\r\nI have a side question. Is it fine to swap the order of arithmetic optimizer with the remapper in grappler? This would simplify specifying a pattern that can be viewed from TF api level. Otherwise, a subgraph generated from TF apis would be modified by the arithmetic optimizer and a user needs to look at the pattern after arithmetic optimizer.", "That might be problematic, I remember that we had few performance regressions when I moved arithmetic optimizer, but I don't remember the details. Maybe it was not arithmetic optimizer :) Let's try, if it will lead to perf regressions or test failures I'll get a notification.", "@ezhulenev I have created a test PR for moving remapper before arithmetic_optimizer.\r\nhttps://github.com/tensorflow/tensorflow/pull/46097", "There are few build errors with this PR:\r\n\r\n```\r\nCppCompile action:\r\nthird_party/tensorflow/core/grappler/utils/pattern_utils_test.cc:23:10: error: module //third_party/tensorflow/core/grappler/utils:pattern_utils_test does not depend on a module exporting 'third_party/tensorflow/core/util/dump_graph.h'\r\nsee http://go/cpp-features#layering_check; to fix run:\r\nbuild_cleaner //third_party/tensorflow/core/grappler/utils:pattern_utils_test\r\n#include \"third_party/tensorflow/core/util/dump_graph.h\"\r\n         ^\r\nthird_party/tensorflow/core/grappler/utils/pattern_utils_test.cc:172:3: error: ignoring return value of function declared with 'warn_unused_result' attribute [-Werror,-Wunused-result]\r\n  graph_view.SortTopologically(/*ignore_cycles=*/false, {});\r\n  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~                   ~~~~~~~~~\r\nthird_party/tensorflow/core/grappler/utils/pattern_utils_test.cc:254:3: error: ignoring return value of function declared with 'warn_unused_result' attribute [-Werror,-Wunused-result]\r\n  graph_view.SortTopologically(/*ignore_cycles=*/false, {});\r\n  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~                   ~~~~~~~~~\r\nthird_party/tensorflow/core/grappler/utils/pattern_utils_test.cc:340:3: error: ignoring return value of function declared with 'warn_unused_result' attribute [-Werror,-Wunused-result]\r\n  graph_view.SortTopologically(/*ignore_cycles=*/false, {});\r\n  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~                   ~~~~~~~~~\r\nthird_party/tensorflow/core/grappler/utils/pattern_utils_test.cc:361:3: error: ignoring return value of function declared with 'warn_unused_result' attribute [-Werror,-Wunused-result]\r\n  graph_view.SortTopologically(/*ignore_cycles=*/false, {});\r\n  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~                   ~~~~~~~~~\r\nthird_party/tensorflow/core/grappler/utils/pattern_utils_test.cc:397:3: error: ignoring return value of function declared with 'warn_unused_result' attribute [-Werror,-Wunused-result]\r\n  graph_view.SortTopologically(/*ignore_cycles=*/false, {});\r\n  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~                   ~~~~~~~~~\r\nthird_party/tensorflow/core/grappler/utils/pattern_utils_test.cc:418:3: error: ignoring return value of function declared with 'warn_unused_result' attribute [-Werror,-Wunused-result]\r\n  graph_view.SortTopologically(/*ignore_cycles=*/false, {});\r\n  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~                   ~~~~~~~~~\r\nthird_party/tensorflow/core/grappler/utils/pattern_utils_test.cc:448:3: error: ignoring return value of function declared with 'warn_unused_result' attribute [-Werror,-Wunused-result]\r\n  mutation->Apply();\r\n  ^~~~~~~~~~~~~~~\r\nthird_party/tensorflow/core/grappler/utils/pattern_utils_test.cc:453:3: error: ignoring return value of function declared with 'warn_unused_result' attribute [-Werror,-Wunused-result]\r\n  mutation->Apply();\r\n  ^~~~~~~~~~~~~~~\r\n9 errors generated.\r\n```", "@ezhulenev I have addressed the errors. Please check.", "@ezhulenev Is there still error?", "It was approved internally just now, should be submitted soon."]}, {"number": 45848, "title": "Could not load dynamic library 'libcusolver.so.10' with official TF 2.4, even though it's installed", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04 LTS\r\n- TensorFlow installed from (source or binary): official 2.4 binary, installed via Python `pip`\r\n- TensorFlow version (use command below): 2.4.0 (git version v2.4.0-rc4-71-g582c8d236cb)\r\n- Python version: 3.8.5\r\n- CUDA/cuDNN version: 11.2.0-1 / 8.0.5.39-1 (both installed from Nvidia repo)\r\n- GPU model and memory: Tesla P100 (16 GB)\r\n\r\n**Describe the current behavior**\r\nAfter installing latest CUDA and cuDNN via the Nvidia repo I installed latest TF 2.4 via Python `pip`. Then I run the following:\r\n\r\n```python\r\nIn [1]: import tensorflow as tf\r\n2020-12-18 14:38:28.563109: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n\r\nIn [2]: tf.config.list_physical_devices('GPU')\r\n2020-12-18 14:38:30.711190: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-12-18 14:38:30.711736: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2020-12-18 14:38:30.738163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-12-18 14:38:30.738690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\r\ncoreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\r\n2020-12-18 14:38:30.738710: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-12-18 14:38:30.741099: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-12-18 14:38:30.741136: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2020-12-18 14:38:30.741931: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2020-12-18 14:38:30.742126: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2020-12-18 14:38:30.742245: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\r\n2020-12-18 14:38:30.742805: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2020-12-18 14:38:30.742917: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-12-18 14:38:30.742930: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n```\r\n\r\nSo apparently the library `libcusolver.so.10` cannot be found.\r\nHowever, this library file is installed on my machine:\r\n\r\n```shell\r\n$ locate libcusolver.so.10\r\n/usr/lib/x86_64-linux-gnu/libcusolver.so.10\r\n/usr/lib/x86_64-linux-gnu/libcusolver.so.10.2.0.243\r\n```\r\n\r\nAny ideas what I am missing here?", "comments": ["@haimat,\r\nTensorFlow 2.4 is built and tested against CUDA 11.0 (not 11.1) and cuDNN 8. Fore more information, please take a look at the [tested build configurations](https://www.tensorflow.org/install/source#gpu). \r\n\r\n\r\nCould you please install CUDA 11.0 with cuDNN 8 and check if you are facing the same issue. Thanks!", "@amahendrakar\r\nThanks, you're right - installing CUDA 11.0 works with TF 2.4 \ud83d\udc4d ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45848\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45848\">No</a>\n", "I also had this issue using a `nvidia/cuda` container with tensor flow 2.4. Switching from `nvidia/cuda:11.1.1-cudnn8-runtime-ubuntu20.04` to `nvidia/cuda:11.0.3-cudnn8-runtime-ubuntu20.04` fixed my issue.\r\n\r\n\r\nhttps://github.com/argosopentech/onmt-models/commit/d944572d2d9ef57febc7a2854b09e7827e0677ee", "Libcusolver.so.10 is renamed to .11, as CUDA updates from 11.0 to 11.1, so the following works.\r\n`sudo ln -s /usr/local/cuda-11.1/lib64/libcusolver.so.11 /usr/local/cuda-11.1/lib64/libcusolver.so.10`", "@Tingbopku It works! Thanks for the fix!"]}, {"number": 45847, "title": "TensorFlow Lite NNAPI with Quantisation: Invalid Zero Point", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy S10 5G\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5.0-dev20201210\r\n- Python version: 3.7.5\r\n- CUDA/cuDNN version: 11.1\r\n\r\n**Describe the current behavior**\r\nTensorFlow Lite raises an error when doing inference with NNAPI on a mobile device, with a custom model quantised to int8. Issue happens both when I use int8 quantisation with float fallback and strict int8 quantisation. If I take PReLU out of the model the error goes away.\r\n\r\nTerminal output below:\r\n```\r\n\r\n2020-12-18 12:01:37.452 6713-7171/org.tensorflow.benchmarking I/tflite: Created TensorFlow Lite delegate for NNAPI.\r\n2020-12-18 12:01:37.454 6713-7171/org.tensorflow.benchmarking I/tflite: Initialized TensorFlow Lite runtime.\r\n2020-12-18 12:01:37.454 6713-7171/org.tensorflow.benchmarking I/Manager: DeviceManager::DeviceManager\r\n2020-12-18 12:01:37.454 6713-7171/org.tensorflow.benchmarking I/Manager: findAvailableDevices\r\n2020-12-18 12:01:37.455 6713-7171/org.tensorflow.benchmarking I/Manager: Found interface armnn\r\n2020-12-18 12:01:37.458 6713-7171/org.tensorflow.benchmarking I/Manager: Capab {.relaxedFloat32toFloat16PerformanceScalar = {.execTime = 0.900000, .powerUsage = 0.000000}, .relaxedFloat32toFloat16PerformanceTensor = {.execTime = 0.000000, .powerUsage = 0.900000}, .operandPerformance = [16]{{.type = FLOAT32, .info = {.execTime = 0.400000, .powerUsage = 0.400000}}, {.type = INT32, .info = {.execTime = 0.600000, .powerUsage = 0.600000}}, {.type = UINT32, .info = {.execTime = 340282346638528859811704183484516925440.000000, .powerUsage = 340282346638528859811704183484516925440.000000}}, {.type = TENSOR_FLOAT32, .info = {.execTime = 0.400000, .powerUsage = 0.400000}}, {.type = TENSOR_INT32, .info = {.execTime = 0.600000, .powerUsage = 0.600000}}, {.type = TENSOR_QUANT8_ASYMM, .info = {.execTime = 0.600000, .powerUsage = 0.600000}}, {.type = BOOL, .info = {.execTime = 340282346638528859811704183484516925440.000000, .powerUsage = 340282346638528859811704183484516925440.000000}}, {.type = TENSOR_QUANT16_SYMM, .info = {.execTime = 0.600000, .powerUsage = 0.600000}}, {.type = TENSOR_FLOAT16\r\n2020-12-18 12:01:37.458 6713-7171/org.tensorflow.benchmarking I/Manager: Found interface liteadaptor\r\n2020-12-18 12:01:37.459 6713-6713/org.tensorflow.benchmarking D/RtgSchedIpcFile: setCommandByIoctl failed ret:-1, cmdid:32, errno:13\r\n2020-12-18 12:01:37.453 6713-6713/org.tensorflow.benchmarking W/ow.benchmarking: type=1400 audit(0.0:22623): avc: denied { ioctl } for pid=6713 path=\"/proc/6713/rtg\" dev=\"proc\" ino=478805 ioctlcmd=0xab20 scontext=u:r:untrusted_app:s0:c155,c256,c512,c768 tcontext=u:r:untrusted_app:s0:c155,c256,c512,c768 tclass=file permissive=0\r\n2020-12-18 12:01:37.460 6713-7171/org.tensorflow.benchmarking I/Manager: Capab {.relaxedFloat32toFloat16PerformanceScalar = {.execTime = 0.100000, .powerUsage = 0.100000}, .relaxedFloat32toFloat16PerformanceTensor = {.execTime = 0.100000, .powerUsage = 0.100000}, .operandPerformance = [14]{{.type = FLOAT32, .info = {.execTime = 1.000000, .powerUsage = 1.000000}}, {.type = INT32, .info = {.execTime = 0.500000, .powerUsage = 0.500000}}, {.type = UINT32, .info = {.execTime = 0.500000, .powerUsage = 0.500000}}, {.type = TENSOR_FLOAT32, .info = {.execTime = 1.000000, .powerUsage = 1.000000}}, {.type = TENSOR_INT32, .info = {.execTime = 0.500000, .powerUsage = 0.500000}}, {.type = TENSOR_QUANT8_ASYMM, .info = {.execTime = 0.200000, .powerUsage = 0.200000}}, {.type = BOOL, .info = {.execTime = 0.500000, .powerUsage = 0.500000}}, {.type = TENSOR_QUANT16_SYMM, .info = {.execTime = 1.000000, .powerUsage = 1.000000}}, {.type = TENSOR_FLOAT16, .info = {.execTime = 0.100000, .powerUsage = 0.100000}}, {.type = TENSOR_BOOL8, .info = {.execTime = 0.500000, .powerUsage = 0.500000}}, {.type = FLOA\r\n2020-12-18 12:01:37.460 6713-7171/org.tensorflow.benchmarking I/TypeManager: Failed to read /vendor/etc/nnapi_extensions_app_allowlist ; No app allowlisted for vendor extensions use.\r\n2020-12-18 12:01:37.461 6713-7171/org.tensorflow.benchmarking E/ExecutionBuilder: NN_RET_CHECK failed (frameworks/ml/nn/common/Utils.cpp:396): type.zeroPoint == 0 (type.zeroPoint = -128, 0 = 0) ANeuralNetworksModel_addOperand invalid zeroPoint: -128\r\n2020-12-18 12:01:37.461 6713-7171/org.tensorflow.benchmarking E/ExecutionBuilder: NN_RET_CHECK failed (frameworks/ml/nn/common/Utils.cpp:463): validateQuant8SymmParams(type, tag) \r\n2020-12-18 12:01:37.462 6713-7171/org.tensorflow.benchmarking E/org.tensorflow.benchmarking.MainActivity: Error initialising tflite interpreter\r\n    java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: NN API returned error ANEURALNETWORKS_BAD_DATA at line 1380 while adding operand for tensor 'model_tf_nlhd_nld/p_re_lu/add;model_tf_nlhd_nld/p_re_lu/Relu;model_tf_nlhd_nld/p_re_lu/Neg_1;model_tf_nlhd_nld/p_re_lu/Relu_1;model_tf_nlhd_nld/p_re_lu/mul'.\r\n    \r\n```\r\n\r\nI've also added the model file below.\r\n\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/5715611/model.zip)\r\n\r\n", "comments": ["Hello, any news?", "@Cy-r0 Are u using Android 10 ?\r\nAndroid 10 does not support ANEURALNETWORKS_TENSOR_QUANT8_ASYMM.\r\nWith ANEURALNETWORKS_TENSOR_QUANT8_SYMM, the zero point cannot be non-zero. But I don't think the code in nnapi_delegate.cc handle that case properly. currently \r\n", "Hey, yes I'm on Android 10. \r\n\r\nAs a temporary solution, is it possible to force the zero point to be 0? For now I'm only interested in the runtime speed of the model.", "@Cy-r0 mark all zeroPoint  to 0 instead of assign to the model value in tensorflow/lite/delegates/nnapi/nnapi_delegate.cc and build your own tensorflow lite.\r\nGood luck. As you might still encounter some layers only support ASYMM input or output .\r\n\r\nThere are just too many layers falling back to CPU before Android 1 due to nnapi 1.2 does not support ASYMM8, and  resize does not support align_corners (which  requires nnapi 1.3 Android 11)\r\n\r\n ", "Hi @Cy-r0, sorry for the delay. I've just merged [a patch](https://github.com/tensorflow/tensorflow/commit/696cb254a1eb60f4aa139d794fb3d9603ad4cbbe) to fix this issue. Could you try running the model now?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hey, thanks for the update, will try it out soon.", "Ok, downloaded the latest nightly version of tensorflow-lite from JCenter and put it in the app, and now PReLU works with int8 quantisation. Thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45847\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45847\">No</a>\n"]}, {"number": 45846, "title": " tf.keras.experimental.WideDeepModel saved model fails with 2 optimizers", "body": "tensorflow 2.4\r\n\r\n    \r\n```\r\n    wide_deep_model = tf.keras.experimental.WideDeepModel(linear_model, dnn_model, activation='sigmoid')\r\n    wide_deep_model.compile(optimizer= [linear_optimizer,dnn_optimizer]\r\n                            loss=tf.keras.losses.BinaryCrossentropy(),\r\n                            metrics=tf.keras.metrics.BinaryAccuracy())\r\n\r\n    ...\r\n    model.fit(dataset.batch(100).shuffle(100), epochs=2000, callbacks=[tensorboard_callback])\r\n    tf.saved_model.save(model, 'model/{}'.format(int(time.time())))\r\n```\r\n\r\nsaved_mode will fail if given 2 optimizer to  tf.keras.experimental.WideDeepModel, the error is straight forward(self.optimizer is a list, so has no get_config method):\r\n\r\nLib\\site-packages\\tensorflow\\python\\keras\\saving\\saving_utils.py\r\n\r\n```\r\n      else:\r\n        optimizer_config = {\r\n            'class_name':\r\n                generic_utils.get_registered_name(model.optimizer.__class__),\r\n            'config':\r\n                model.optimizer.get_config()\r\n        }\r\n      metadata['training_config']['optimizer_config'] = optimizer_config\r\n  return metadata\r\n```\r\n\r\n", "comments": ["@owenliang,\r\nOn running the code, I am facing an error stating `NameError: name 'linear_model' is not defined`.\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45846\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45846\">No</a>\n"]}, {"number": 45844, "title": "Risc-V Linux port for TensorFlow Lite", "body": "This PR has addition for riscv_makefile.inc\r\nPre-requistes:\r\n- Install RISC-V GNU Toolchain for Linux. Refer https://github.com/riscv/riscv-gnu-toolchain\r\n- Here we are using riscv64-unknown-linux-gnu- tools created for \"linux\" from above link.\r\nThe changes builds executables and static libs  in \"linux_riscv64\", in \"gen\" directory.\r\nTested on RISC-V HiFive Unleashed.\r\nTest Results on RISC-V HiFive Unleashed:\r\n$./label_image --tflite_model ./models/mobilenet_v1_1.0_224.tflite --labels labels.txt -i ./images/grace_hopper.bmp \r\nINFO: Loaded model ./models/mobilenet_v1_1.0_224.tflite\r\nINFO: resolved reporter\r\nINFO: invoked\r\nINFO: average time: 1553.19 ms\r\nINFO: 0.860174: 653 653:military uniform\r\nINFO: 0.0481021: 907 907:Windsor tie\r\nINFO: 0.00786705: 466 466:bulletproof vest\r\nINFO: 0.00644936: 514 514:cornet, horn, trumpet, trump\r\nINFO: 0.00608029: 543 543:drumstick\r\n", "comments": ["@terryheo Can you please tell me, during checks why it is failing for **\"import/copybara \u2014 An error happened while migrating the change\".**\r\n Is it because of outdated license? (tensorflow/lite/tools/make/build_riscv_lib.sh Outdated )", "I'll handle the copybara error. In the meantime, could you address my comment?", "> I'll handle the copybara error. In the meantime, could you address my comment?\r\n\r\nAs you suggested I had removed unwanted character(space) after #!/bin/bash"]}, {"number": 45843, "title": "Initial port of Transpose from lite to micro", "body": "An initial port of the Transpose kernel from lite to micro, without any changes to the code. Changes to make the code run in micro will be delivered in a later PR. See #45695 ", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}]