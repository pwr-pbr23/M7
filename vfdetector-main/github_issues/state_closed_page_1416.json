[{"number": 10498, "title": "TensorFlow logging configuration is non-standard", "body": "Ref https://github.com/tensorflow/tensorflow/issues/8023\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Docker image\r\n- **TensorFlow version (use command below)**: 1.1.0\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: Per Docker image\r\n- **GPU model and memory**: K80\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nThe logging configuration in `tf_logging` is odd and inconsistent with how things are generally done in Python.\r\n\r\nIn general, it's typical to attach handlers to the root logger in Python: https://docs.python.org/2/library/logging.html#logging.Logger.propagate\r\n\r\nHowever, because `tf_logging` sets up its own stream handler, and does not disable the propagate flag, anybody who does follow the standard Python convention of attaching handlers to the root logger gets duplicated log output from TensorFlow.\r\n\r\nPer https://github.com/tensorflow/tensorflow/issues/8023, while this isn't a bug per se, it's still wrong and undesirable.\r\n\r\nThe cleanest way forward is probably to just set `propagate=False` on the TF logger.", "comments": ["After browsing some of the code history, it looks like the current behavior was an explicit implementation choice made back in Nov 2015.\r\n\r\nHere's the git commit that brought it in:\r\nhttps://github.com/tensorflow/tensorflow/commit/d50565b35e886e7c3a201ea2f088790ed4b28de4\r\n\r\nFor reference, the internal tracking ID is cl/107710311 and the description is:\r\n```\r\nScope tensorflow logging to prevent namespace pollution for\r\nusers who import tf and configure their own loggers.\r\n```\r\n\r\n@dave-andersen and @mrry were involved in the original change, and may have more details.  I'm re-assigning to @mrry since he knows Python much better than I do, and may have some opinions on the suggestion of setting `propagate=False`", "To clarify, I think it's absolutely correct that TensorFlow logging is scoped under `tensorflow`. If anything, a more common pattern in Python libraries is for loggers to be scoped to each individual module, as this exposes more control over customizing log level on a more granular basis.\r\n\r\nWhat's suboptimal here is that the logger both has a handler _and_ propagates. This means that if I follow standard best practices, anything that TF tries to log hits both the handler on the `tensorflow` logger, and then my own handler on the root logger, so I end up getting two lines of log output.", "I think we'd welcome contributions to fix this!"]}, {"number": 10497, "title": "Merging rc2 back into master.", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 10496, "title": "Branch 158282834", "body": "", "comments": []}, {"number": 10495, "title": "Branch 158278922", "body": "", "comments": []}, {"number": 10494, "title": "incorrect documentation in _SliceHelper", "body": "The outputs shown for several of the examples in the `_SliceHelper` docstring are incorrect (based on some previous examples?).\r\n\r\nSee, for example, this line: https://github.com/tensorflow/tensorflow/blob/359d6f9716c0bb9bd8201ce600da98b0481a8049/tensorflow/python/ops/array_ops.py#L412\r\n", "comments": ["The first two section seem correct. The last two seem two be based on the output of the second example."]}, {"number": 10493, "title": "Typo in text", "body": "Typo at this address: https://www.tensorflow.org/api_docs/java/reference/org/tensorflow/Graph\r\nI thought it primes in our mind that TensorFlow for Java is not yet mature. Fixing it might help new comers!\r\n\r\nWARNING: Resources consumed by the Graph object **msut** be explicitly freed by invoking the close() method then the Graph object is no longer needed.\r\n\r\n", "comments": ["resolved in https://github.com/tensorflow/tensorflow/pull/9852, do we need to wait for a new release for the doc to be regenerated?", "The API docs point to the latest stable release. Fixes to the API docs are reflected the next time that docset is published, or for the default one, whenever the stable version switches. \r\n\r\nTo change this, we'd have to backport the fix to 1.1, and redeploy the 1.1 docs. \r\n\r\n@wolffg FYI -- I know of no plans to redeploy old docs regularly.  ", "Unfortunately it looks as if this didn't make the 1.2 docs, but it will be fixed in 1.3."]}, {"number": 10492, "title": "Read images with various sizes from TFRecord", "body": "I posted this issue in [StackOverflow](https://stackoverflow.com/questions/44152661/random-crop-a-patch-from-a-various-sized-image-read-in-tfrecord) and I got a response as well. However the replied answer would raise a **ValueError: All shapes must be fully defined** .\r\nI'm just wondering might it be a limitation or even a bug that it's impossible to read images with various sizes from TFRecord? \r\n", "comments": ["I think stackoverflow looks like the right discussion going on here, since there seem to be some workarounds proposed for you and no indication of a bug in TensorFlow so far.", "def distorted_inputs(filenames, batch_size, imagesize, img_channel):\r\n\r\n\tfilename_queue = tf.train.string_input_producer(filenames)\r\n\treader = tf.TFRecordReader()\r\n\t_, serialized_example = reader.read(filename_queue)\r\n\r\n\tfeatures = tf.parse_single_example(serialized_example, features={\r\n\t\t\t\t'abs_path': tf.FixedLenFeature([], tf.string),\r\n\t\t\t\t'height':tf.FixedLenFeature([], tf.int64),\r\n\t\t\t\t'width' :tf.FixedLenFeature([], tf.int64),\r\n\t\t\t\t'img_raw':tf.FixedLenFeature([], tf.string),\r\n\t\t\t\t'label':tf.FixedLenFeature([], tf.int64)})\r\n\theight = tf.cast(features['height'], tf.int32)\r\n\twidth = tf.cast(features['width'], tf.int32)\r\n\timages = tf.decode_raw(features['img_raw'], tf.float32)\r\n\timages = tf.reshape(images, [height, width, img_channel])\r\n\t\r\n\tdistorted_image = tf.random_crop(images, [530, 530, img_channel])\r\n\tdistorted_image = tf.image.random_flip_left_right(distorted_image)\r\n\tdistorted_image = tf.image.random_brightness(distorted_image, max_delta=63)\r\n\tdistorted_image = tf.image.random_contrast(distorted_image, lower=0.2, upper=1.8)\r\n\tdistorted_image = tf.image.resize_images(distorted_image, (imagesize,imagesize))\r\n\tfloat_image = tf.image.per_image_standardization(distorted_image)\r\n\t\r\n\tlabel = tf.cast(features['label'], tf.int64)\r\n\tlabel = tf.reshape(label, [1])\r\n\tmin_after_dequeue = 1000\r\n\tcapacity = min_after_dequeue + 3*batch_size\r\n\treturn tf.train.shuffle_batch(\r\n\t\t\t[float_image, label],\r\n\t\t\tbatch_size = batch_size,\r\n\t\t\tcapacity = capacity,\r\n\t\t\tmin_after_dequeue = min_after_dequeue)\r\n\r\nI alse meet this problem a month ago, I test my code line by line, this is code can works well.\r\nThe core problem is this : images = tf.reshape(images, [height, width, img_channel]).I think this can help you.Any question call me.", "baodingge, thanks for response so much.\r\nAfter reading and trying your codes, I just found what's the problem with my codes, \r\nMy modified codes are listed as below:\r\n\r\n    def read_and_decode(filename, img_size=-1, distorted=True, get_float_img=True):\r\n\r\n    filename_queue = tf.train.string_input_producer([filename])  # specifying parameter \"num_epochs\" would raised an error, which I don't know how to resolve yet\r\n\r\n    reader = tf.TFRecordReader()\r\n\r\n    _, serialized_example = reader.read(filename_queue)\r\n\r\n    features = tf.parse_single_example(serialized_example,\r\n                                       features={\r\n                                           'img_h': tf.FixedLenFeature([], tf.int64),\r\n                                           'img_w': tf.FixedLenFeature([], tf.int64),\r\n                                           'img_lbl': tf.FixedLenFeature([], tf.int64),\r\n                                           'img_raw': tf.FixedLenFeature([], tf.string)\r\n                                       })\r\n\r\n    img_h = tf.cast(features['img_h'], tf.int32)\r\n    img_w = tf.cast(features['img_w'], tf.int32)\r\n    img = tf.decode_raw(features['img_raw'], tf.uint8)\r\n\r\n    img = tf.reshape(img, [img_h, img_w, 3])  # reshape to a 2-D image\r\n    label = tf.cast(features['img_lbl'], tf.int32)\r\n\r\n    img = tf.random_crop(img, [20, 20, 3])  <==  After reshaping the tensor, I need to add this line to get images with specific size\r\n\r\n    return img, label\r\n\r\nThanks so much!!!", "# specifying parameter \"num_epochs\" would raised an error, which I don't know how to resolve yet\r\nthis question is not so difficult, I know the answer is quite.when you use num_epoch explicitly, string_input_producer will creat a variable. So you have to use tf.local_variables_initializer() alone.\r\nOr use init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()).\r\nthis may be the answer.", "I resolved another problem. Thanks for help so much, thanks!"]}, {"number": 10491, "title": "TF-Keras dont see TF variables", "body": "It is a feature or a bug?\r\n\r\nFor example, I have 2 models:\r\ntypical imports:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nfrom tensorflow.contrib.keras import layers as klayers\r\nfrom tensorflow.contrib.keras import activations as kacts\r\nfrom tensorflow.contrib.keras import models as kmodels\r\nfrom tensorflow.contrib.keras import optimizers as kopts\r\nfrom tensorflow.contrib.keras import losses as kloss\r\nfrom tensorflow.contrib.keras import backend as kback\r\n```\r\n\r\nKeras model\r\n```\r\n# keras model\r\nfeatures = klayers.Input(shape=(28, 28, 1))\r\nx = klayers.Conv2D(64, 3, strides=(2, 2))(features)\r\nx = klayers.MaxPool2D()(x)\r\nx = klayers.Conv2D(32, 3, strides=(2, 2))(x)\r\nx = klayers.Flatten()(x)\r\nprelogits = klayers.Dense(128, activation=kacts.elu)(x)\r\npred = klayers.Dense(10, activation=kacts.softmax)(prelogits)\r\n```\r\n\r\nand TF one:\r\n```\r\nfeatures = klayers.Input(shape=(28, 28, 1))\r\ndef convolution_network(\r\n        states, n_filters=None, kernels=None, strides=None,\r\n        activation_fn=tf.nn.elu, use_bn=False, dropout=-1):\r\n    n_filters = n_filters or [64, 32]\r\n    kernels = kernels or [3, 3]\r\n    strides = strides or [2, 2]\r\n    x = states\r\n    for n_filter, kernel, stride in zip(n_filters, kernels, strides):\r\n        x = tf.layers.conv2d(x, n_filter, kernel, stride, activation=None)\r\n        if use_bn:\r\n            x = tf.layers.batch_normalization(x, training=is_training)\r\n        x = tf.layers.max_pooling2d(x, 2, 2)\r\n        x = activation_fn(x)\r\n        if dropout > 0:\r\n            x = tf.layers.dropout(x, rate=dropout, training=is_training)\r\n    x = tf.contrib.layers.flatten(x)\r\n    return x\r\n\r\ndef simple_model(features):\r\n    features = convolution_network(features)\r\n    prelogits = tf.layers.dense(features, 128, activation=tf.nn.elu)\r\n    logits = tf.layers.dense(prelogits, 10, activation=tf.nn.softmax)\r\n    \r\n    return logits\r\n\r\npred = tf.contrib.keras.layers.Lambda(simple_model)(features)\r\n```\r\n\r\nBoth of them then made with `model = kmodels.Model(inputs=features, outputs=pred)`\r\n\r\nNevertheless, when I use `model.summary()` , Kesar model give me correct output:\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         (None, 28, 28, 1)         0         \r\n_________________________________________________________________\r\nconv2d_1 (Conv2D)            (None, 13, 13, 64)        640       \r\n_________________________________________________________________\r\nmax_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \r\n_________________________________________________________________\r\nconv2d_2 (Conv2D)            (None, 2, 2, 32)          18464     \r\n_________________________________________________________________\r\nflatten_1 (Flatten)          (None, 128)               0         \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 128)               16512     \r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 10)                1290      \r\n=================================================================\r\nTotal params: 36,906\r\nTrainable params: 36,906\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```\r\n**but** TF one, give only:\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         (None, 28, 28, 1)         0         \r\n_________________________________________________________________\r\nlambda_1 (Lambda)            (None, 10)                0         \r\n=================================================================\r\nTotal params: 0\r\nTrainable params: 0\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```\r\n\r\nIs it correct, that TF-Keras cannot see TF variables and optimize them? So we still have separated Tensorflow an Keras frameworks?\r\nWhy cannot I use TF as Keras backend?\r\nTF-versions: from 1.1.0 to 1.2.0rc1", "comments": ["Keras adds some metadata to Layers, that why model.summary can see them. Calling for example `klayers.MaxPool2D()(x)` will only call `tf.layers.max_pooling2d(x, 2, 2)` and adds some metadata to it. To add the metadata yourself, you should look into the `Lambda` layer.", "Personally I dont care about summary viev :)\r\nIts more important, that in this setup, Keras optimizer could not optimize tf-variables. It's strange, cause all of them (Keras and TF layers) was made with `tensorflow.python.layers`. And all of them have `add_weigth` method, and use it.\r\n\r\nWhen, what metadata does Keras need? Or where can something go wrong?", "For the time being, Keras models should be built with Keras layers, not with TensorFlow layers. We plan on allowing the use of TensorFlow layers in Keras models in the near future, though.\r\n\r\nThis is not a limitation since all TensorFlow layers are in Keras (albeit the reverse is not true).\r\n\r\n", "@fchollet \r\n> This is not a limitation since all TensorFlow layers are in Keras (albeit the reverse is not true).\r\n\r\nNo sure about this. TF have huge variety of RNN Cells and other important stuff (decoders) for seq2seq model, which I personally interested in. Nevertheless Keras is extremely helpful in prototyping feed-forward-like models with it's ConvLSTM2D and other primitives.\r\n\r\nPS. For the sake of interest: are there any best practices for TF-Keras seq2seq models other than [these one](https://github.com/farizrahman4u/seq2seq)?"]}, {"number": 10490, "title": "Recognize CPU core count in FreeBSD", "body": "At the moment FreeBSD can't recognize the CPU count, however FreeBSD uses the same method as OS X does so provided PR fixes that.\r\n\r\nIf this could be included in the 1.2.0 release that would be great.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "Makefile failure unrelated. Merging"]}, {"number": 10489, "title": "when logits is all zero, why in_top_k will return true?", "body": "\r\nx = tf.constant([[2.0,9.0],[7.0,5.0],[0.0,0.0]])\r\ny = tf.constant([1,0,1])\r\n tf.nn.in_top_k(x,y,1).eval()\r\nOut : array([ True,  True,  True], dtype=bool)\r\nwhy not output array([ True,  True,  False], dtype=bool)\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 10488, "title": "Changing the cache_size in Gemmlowp/meta/single_thread_gemm.h cause random error in Requantize nodes", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux ubuntu 16.04\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\n\r\nsource, NDK built Android ARM64 binary\r\n\r\n- **TensorFlow version (use command below)**:\r\n\r\ncommit id: f48673b5054b474fa1e51823edd075088cd16d5f\r\nAuthor: Luke Iwanski <luke@codeplay.com>\r\n\r\n- **Bazel version (if compiling from source)**:\r\n0.4.5\r\n\r\n- **CUDA/cuDNN version**:\r\nno\r\n\r\n- **GPU model and memory**:\r\nno\r\n\r\n- **Exact command to reproduce**:\r\n1. bazel --output_base=../out/armv8_benchmark_model/ build -s -c opt --jobs=1 --crosstool_top=//external:android/crosstool --cpu=arm64-v8a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain  tensorflow/tools/benchmark:benchmark_model 2>&1 |tee log.txt\r\n2.adb push benchmark_model /data/local/tmp/\r\n \r\n3. run_vgg1 on Nexus5X6P phone:\r\nscript:\r\ntime ./$1 --graph=$2   --input_layer=\"images:0\"   --input_layer_shape=\"1,224,224,3\"   --input_layer_type=\"float\"   --output_layer=\"prob:0\" --num_runs=1\r\n\r\nThe final command line that I use:\r\n./run_vgg1.sh benchmark_model vgg16.8bit.weightsnodes.model\r\n\r\n### My problem\r\nWhen I change the cache_size from 256*1024 to 128*1024 in the Gemmlowp/meta/single_thread_gemm.h, the benchmark_model randomly failed on the 8 bit quantized both node and weights vgg16 model.\r\n\r\n### Source code / logs\r\nMy Tensorflow commit-id:\r\ncommit f48673b5054b474fa1e51823edd075088cd16d5f\r\n\r\nMy modify for the Gemmlowp:\r\nin file  gemmlowp/meta/single_thread_gemm.h:\r\nchange all the \"int cache_size = 256 * 1024\" to \"int cache_size = 128 * 1024\".\r\n\r\nThe error log is :\r\nnative : benchmark_model.cc:381 Graph: [vgg16.8bit.weightsnodes.model]\r\nnative : benchmark_model.cc:382 Input layers: [images:0]\r\nnative : benchmark_model.cc:383 Input shapes: [1,224,224,3]\r\nnative : benchmark_model.cc:384 Input types: [float]\r\nnative : benchmark_model.cc:385 Output layers: [prob:0]\r\nnative : benchmark_model.cc:386 Num runs: [1]\r\nnative : benchmark_model.cc:387 Inter-run delay (seconds): [-1.0]\r\nnative : benchmark_model.cc:388 Num threads: [-1]\r\nnative : benchmark_model.cc:389 Benchmark name: []\r\nnative : benchmark_model.cc:390 Output prefix: []\r\nnative : benchmark_model.cc:391 Show sizes: [0]\r\nnative : benchmark_model.cc:392 Warmup runs: [2]\r\nnative : benchmark_model.cc:52 Loading TensorFlow.\r\nnative : benchmark_model.cc:59 Got config, 0 devices\r\ncan't determine number of CPU cores: assuming 4\r\ncan't determine number of CPU cores: assuming 4\r\nnative : benchmark_model.cc:257 Running benchmark for 2 iterations without detailed stat logging:\r\nnative : benchmark_model.cc:233 Error during inference: Invalid argument: requested_output_max must be >= requested_output_min, but got nan and 0\r\n\t [[Node: fc8/BiasAdd/eightbit/requantize = Requantize[Tinput=DT_QINT32, out_type=DT_QUINT8, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](fc8/BiasAdd/eightbit, fc8/BiasAdd/eightbit:1, fc8/BiasAdd/eightbit:2, fc8/BiasAdd/eightbit/requant_range, fc8/BiasAdd/eightbit/requant_range:1)]]\r\nnative : benchmark_model.cc:268 Failed on run 0\r\nnative : benchmark_model.cc:451 Timing failed with Invalid argument: requested_output_max must be >= requested_output_min, but got nan and 0\r\n\t [[Node: fc8/BiasAdd/eightbit/requantize = Requantize[Tinput=DT_QINT32, out_type=DT_QUINT8, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](fc8/BiasAdd/eightbit, fc8/BiasAdd/eightbit:1, fc8/BiasAdd/eightbit:2, fc8/BiasAdd/eightbit/requant_range, fc8/BiasAdd/eightbit/requant_range:1)]]\r\n    0m11.12s real     0m27.55s user     0m01.87s system\r\n\r\n\r\n\r\n", "comments": ["Maciek, can you take a look at this?", "The same error seems happens on the quantized MobileNet_v1_1.0_224 too.\r\nBy reference this page:\r\nhttps://github.com/tensorflow/models/tree/master/slim\r\n\r\n**Using the following command line, I get the mobilenet_v1_224.pb.**\r\n\r\n$ python export_inference_graph.py \\\r\n  --alsologtostderr \\\r\n  --model_name=mobilenet_v1 \\\r\n  --image_size=224 \\\r\n  --output_file=/tmp/mobilenet_v1_224.pb\r\n\r\n**Then I use the following command line to freeze it. The checkpoint file is downloaded from  [here](http://download.tensorflow.org/models/mobilenet_v1_1.0_224_2017_06_14.tar.gz)**\r\n\r\npython bazel-bin/tensorflow/python/tools/freeze_graph --input_graph=../model/models/slim/download_models/mobilenet/mobilenet_v1_224.pb  --input_checkpoint=../model/models/slim/download_models/mobilenet/mobilenet_v1_1.0_224.ckpt --input_binary=true --output_graph=../model/models/slim/download_models/mobilenet/freeze_mobilenet_v1_224.pb --output_node_names=MobilenetV1/Predictions/Reshape_1\r\n\r\n**Then quantized it using the \"trasnform_graph\" to quantize both the wights and node.**\r\n./transform_graph --in_graph=./freeze_mobilenet_v1_224.pb --out_graph=./quantized_freeze_mobilenet_v1_224.pb --inputs='input' --outputs=\"MobilenetV1/Predictions/Reshape_1\" --transforms='quantize_weights quantize_nodes' \r\n\r\n**After  I got the quantized model, I run it with benchmark_model then get the fault.**\r\ncommand line of benchmark_model:\r\n ./benchmark_model_armv8a_0628 --graph=quantized_freeze_mobilenet_v1_224.pb --input_layer=\"input\"   --input_layer_shape=\"1,224,224,3\"   --input_layer_type=\"float\"   --output_layer=\"MobilenetV1/Predictions/Reshape_1\" --num_runs=20\r\n\r\n**The** error log\r\nnative : benchmark_model.cc:382 Graph: [quantized_freeze_mobilenet_v1_224.pb]\r\nnative : benchmark_model.cc:383 Input layers: [input]\r\nnative : benchmark_model.cc:384 Input shapes: [1,224,224,3]\r\nnative : benchmark_model.cc:385 Input types: [float]\r\nnative : benchmark_model.cc:386 Output layers: [MobilenetV1/Predictions/Reshape_1]\r\nnative : benchmark_model.cc:387 Num runs: [20]\r\nnative : benchmark_model.cc:388 Inter-run delay (seconds): [-1.0]\r\nnative : benchmark_model.cc:389 Num threads: [-1]\r\nnative : benchmark_model.cc:390 Benchmark name: []\r\nnative : benchmark_model.cc:391 Output prefix: []\r\nnative : benchmark_model.cc:392 Show sizes: [0]\r\nnative : benchmark_model.cc:393 Warmup runs: [2]\r\nnative : benchmark_model.cc:53 Loading TensorFlow.\r\nnative : benchmark_model.cc:60 Got config, 0 devices\r\ncan't determine number of CPU cores: assuming 4\r\ncan't determine number of CPU cores: assuming 4\r\nnative : benchmark_model.cc:258 Running benchmark for 2 iterations without detailed stat logging:\r\nnative : benchmark_model.cc:234 Error during inference: Invalid argument: requested_output_max must be >= requested_output_min, but got nan and 0\r\n\t [[Node: MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit/requantize = Requantize[Tinput=DT_QINT32, out_type=DT_QUINT8, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit, MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit:1, MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit:2, MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit/requant_range, MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit/requant_range:1)]]\r\nnative : benchmark_model.cc:269 Failed on run 0\r\nnative : benchmark_model.cc:452 Timing failed with Invalid argument: requested_output_max must be >= requested_output_min, but got nan and 0\r\n\t [[Node: MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit/requantize = Requantize[Tinput=DT_QINT32, out_type=DT_QUINT8, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit, MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit:1, MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit:2, MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit/requant_range, MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit/requant_range:1)]]\r\n----\r\n", "@qjivy  Have u addressed the problem?", "@petewarden  I have came across similar problem. InvalidArgumentError (see above for traceback): requested_output_max must be >= requested_output_min, but got -33025.5117 and 0\r\n\t [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/ClipToWindow_2/Area/mul/eightbit/requantize = Requantize[Tinput=DT_QINT32, out_type=DT_QUINT8, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/ClipToWindow_2/Area/mul/eightbit, SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/ClipToWindow_2/Area/mul/eightbit:1, SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/ClipToWindow_2/Area/mul/eightbit:2, SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/ClipToWindow_2/Area/mul/eightbit/requant_range, SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/ClipToWindow_2/Area/mul/eightbit/requant_range:1)]]\r\n \r\n\r\nIn Faster RCNN, can you help to fix this problem?\r\n\r\n", "@snownus, I still can't solve this problem at the moment. ", "@petewarden, can you think of anyone else to look at this?", "@aselle , yes. we would appreciate it a lot if you can help to address the problem.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Ping @petewarden ", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 112 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 127 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 142 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 157 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'm closing this now, since we've moved away from this code path in favor of TensorFlow Lite. Sorry for the long delay!"]}, {"number": 10487, "title": "Variable \"weights\" does not exist with BasicLSTMcell or LSTMBlockCell ", "body": "### System information\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: via pip with GPU support\r\n- **TensorFlow version (use command below)**: 1.1\r\n- **GPU model and memory**: GPU Titan X\r\n\r\n### Describtion of the problem\r\n\r\nI am trying to use LSTM cell, but when I make the __call__ to it, I receive an ValueError message saying that the variable weights is does not exist or not created with get_variable().\r\n\r\nI looked into the tensorflow source code of the BasicLSTMCell or LSTMBlockCell and we can see that the weights matrix is getted by a call to tf.get_variable()... So it seems to be a bug, right ? \r\nDoes someone can tell me if I made a mistake of if it is a bug ?\r\n\r\nNB: Note that i tried several other solution with variable_scope instead of name_scope, with reuse = True, of False , with no success ... \r\n\r\n### Source code / logs : \r\n**here is the source code of BasicLSTMCell and LSTMBlockCell**\r\nthe call to get_variable into LSTMBlockCell __call_ funtion\r\n```\r\nw = vs.get_variable(self._names[\"W\"], [input_size + self._num_units,\r\n                                             self._num_units * 4])\r\n```\r\nand into the _linear function used by BasicLSTMCell\r\n```\r\nweights = vs.get_variable(\r\n_WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size], dtype=dtype)\r\n```\r\n\r\n**here is my source code**\r\n```\r\nlstm_cell = tf.contrib.rnn.LSTMBlockCell(num_units=self._state_size)\r\nfor t in range(self._rnn_step):\r\n    with tf.name_scope('lstm'):\r\n    _, (c, h) = lstm_cell(lstm_input, [c, h], scope=tf.get_variable_scope())\r\n```\r\n\r\n**here is the error message**\r\n```\r\n  File \"/home/toto/workspace/model.py\", line 441, in _rnn\r\n    _, (c, h) = lstm_cell(lstm_input, [c, h], scope=tf.get_variable_scope())\r\n  File \"/home/toto/PythonEnv/tensorflow/local/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/lstm_ops.py\", line 382, in __call__\r\n    self._num_units * 4])\r\n  File \"/home/toto/PythonEnv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1049, in get_variable\r\n    use_resource=use_resource, custom_getter=custom_getter)\r\n  File \"/home/toto/PythonEnv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 948, in get_variable\r\n    use_resource=use_resource, custom_getter=custom_getter)\r\n  File \"/home/toto/PythonEnv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 356, in get_variable\r\n    validate_shape=validate_shape, use_resource=use_resource)\r\n  File \"/home/toto/PythonEnv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 341, in _true_getter\r\n    use_resource=use_resource)\r\n  File \"/home/toto/PythonEnv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 671, in _get_single_variable\r\n    \"VarScope?\" % name)\r\nValueError: Variable weights does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n```", "comments": ["It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This is fixed in newer versions of TF."]}, {"number": 10486, "title": "[BUG] control_dependencies + sess.run(tf.global_variables_initializer())", "body": "### System information\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04 and mac osx\r\n- **TensorFlow installed from (source or binary)**: pip install tensorflow\r\n- **TensorFlow version (use command below)**: v1.1.0-rc0-61-g1ec6ed5 and 1.1.0\r\n\r\nHi TF team, I've stumbled upon a strange behaviour that I believe is a bug:\r\n\r\nIf I try to run the code below i end up with : `FailedPreconditionError (see above for traceback): Attempting to use uninitialized value timestep\r\n\t [[Node: AssignAdd = AssignAdd[T=DT_INT32, _class=[\"loc:@timestep\"], use_locking=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](timestep, AssignAdd/value)]]`\r\n\r\nIf I remove the `tf.zeros_like(Qs_t)` statement (as in the commented line), the problem disappears.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nwith tf.Graph().as_default():\r\n    Qs_t = tf.ones([3, 2], dtype=tf.float32)\r\n    timestep = tf.Variable(0, dtype=tf.int32, trainable=False, name=\"timestep\")\r\n    inc_t = tf.assign_add(timestep, 1)\r\n    with tf.control_dependencies([inc_t]):\r\n        Ns_t = tf.Variable(tf.zeros_like(Qs_t), dtype=tf.float32, name=\"N\", trainable=False)\r\n        # Ns_t = tf.Variable(0., dtype=tf.float32, name=\"N\", trainable=False)\r\n\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n```\r\n\r\n\r\n", "comments": ["I think the problem is that control dependencies and variable initializers are confusing. When you run the variable initializers, you only tell TensorFlow to run those initialization ops (i.e. compute zeros_like(Qs_t) and assign it to Ns_t), and it doesn't end up executing the precursors to the control dependency.\r\n\r\nThe more common use case would be something like:\r\n\r\n```\r\nwith tf.Graph().as_default():\r\n    Qs_t = tf.ones([3, 2], dtype=tf.float32)\r\n    timestep = tf.Variable(0, dtype=tf.int32, trainable=False, name=\"timestep\")\r\n    inc_t = tf.assign_add(timestep, 1)\r\n    with tf.control_dependencies([inc_t]):\r\n        Ns_t = tf.Variable(0., dtype=tf.float32, name=\"N\", trainable=False)\r\n        tf.Assign(Ns_t, inc_t)\r\n```\r\nwhere the control dependency was intended to apply to the Assign Op. In that case you probably want to be able to call the variable initializer outside the control dependency along with any other variable initializer. You should maybe think of variable initializers more like initializers for static variables in C++: the initializers are run once \"before any other code\".\r\n\r\nA judgment call was made in the TF design that more often than not, when you define the variable inside the control dependency you don't want to guard the initializer. In your case, you should just initialize based on the static shape of Qs_t, not zeros_like.\r\n\r\nI'm closing this for now, but if you have any followup questions, please post them on stackoverflow since github issues are only for TF bugs and feature requests. Thanks!"]}, {"number": 10485, "title": "Unable to install tensorflow-gpu==0.11.0", "body": "I am having issues with installing the v0.11.0 version of tensorflow-gpu. My code works on `tensorflow==0.11.0` but really slow. I had a `tensorflow-gpu==0.11.0` but i had lost it due to an upgrade. The pip repository doesn't have a v0.11 anymore and starts with v0.12 only. I got the v0.11 wheel from the TF_BINARY_URLs for v0.11. I am able to install `tensorflow==0.11.0` using the wheel at `export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl`\r\n\r\nbut I am unable to install the GPU version of `tensorflow-gpu==0.11.0` using the corresponding link at export `TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl`\r\n\r\nThe link looks same to me with just the folder different from `gpu/` and `cpu/`. Upon using the link for GPU, it installs `tensorflow==0.11.0` and not `tensorflow-gpu=0.11.0`. Where can I find a `tensorflow-gpu==0.11.0` wheel?", "comments": ["When we released 0.11, our GPU pip packages were not named `tensorflow-gpu`, it was just `tensorflow`.\r\nThe link you have will get you the tensorflow pip package with GPU support."]}, {"number": 10484, "title": "Can not execute hexagon_graph_execution on hexagon-sim", "body": "@satok16 I have followed the [build_and_run_inception_hexagon.sh](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/samples/build_and_run_inception_hexagon.sh) and generated the hexagon_graph_execution executable. Now instead of using a real device I would like to test inception model with \"hexagon-sim\" available at SDK 3.0. So there is no need to use adb push commands as the SDK can simulate HVX device with hexagon-sim. \r\nI have put the run-time libraries and the inception model plus the image at the same folder. After execution It gives me this error:\r\n\r\n```\r\n ~/Qualcomm/HEXAGON_Tools/7.2.12/Tools/bin/hexagon-sim ./hexagon_graph_execution \"/home/aashouri/Qualcomm/Hexagon_SDK/3.0/test/common/inception\"\r\nError: Unsupported machine type 0x0 in ELF image \"./hexagon_graph_execution\" - exiting.\r\n```\r\n\r\nCould you comment on this ? Thanks", "comments": ["Hi, sorry for the delay.\r\nI'm going to refactor this part (change has been already created, I'm cleaning up it now.) Will be checked in a week.", "Hi Amir,\r\nFor the time being, can you tell me command lines which you used to run hexagon SIM?", "Sorry for the delay @satok16 . I guess the problem is the fact that the `hexagon_graph_execution` binary was built for ARM and not meant to be used with Hexagon-sim. I guess I have to reproduce `libhexagon_controller.so` and `libhexagon_nn_skel.so` from scratch using their provided hexagon-clang compiler. Am I right ?", "@amirjamez , I am afraid hexagon_graph_execution doesn't get built with QURT libs support for obvious reasons to run on hexagon-sim, \r\n\r\nbest place to start would be to simulate standalone, testapp graph in hexagon nn lib.", "@amirjamez\r\n\r\n> Sorry for the delay @satok16 . I guess the problem is the fact that the hexagon_graph_execution binary was built for ARM and not meant to be used with Hexagon-sim. I guess I have to reproduce libhexagon_controller.so and libhexagon_nn_skel.so from scratch using their provided hexagon-clang compiler. Am I right ?\r\nYeah.  TF-HVX may be too large to build for Hexagon-sim.  If you find the way, that is great but that may not be a trivial work.\r\n\r\n> best place to start would be to simulate standalone, testapp graph in hexagon nn lib.\r\nThis is a good idea. If you want to run TF-HVX runtime on the simulator, we may need to extract some functionalities from TF framework to a standalone application and build it by SDK.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Closing due to lack of activity, but please reopen if this still needs attention."]}, {"number": 10483, "title": "How to reduce the package size?", "body": "Even if I only use arm64 size is 98.2 M, how to reduce the size?Please answer\r\n![snip20170607_1](https://user-images.githubusercontent.com/8908244/26865045-7de7b23a-4b8d-11e7-9503-6f07943645a8.png)\r\n", "comments": ["_Warning: As this doesn't appear to be a bug with Tensorflow, the devs may ask for this to be moved to Stack Ovefrlow._\r\n\r\nIt looks like you're using iOS. Have you tried reading through [these instructions](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/ios_examples#reducing-the-binary-size) to reduce the binary size? The idea is that you manually remove the kernels you are not using to reduce the size. This may help you.", "Thank you for your reply, I according to some of the size of the official document, just libtensorflow - the size of the core cannot be reduced, if support arvmc7 and arm64, release AppStore after increasing the size of the probably how much?", "I'm awfully sorry but i'm struggling to understand your reply. I don't think I can help you much more than pointing you to that documentation. It states that each binary should be around 11Mb so perhaps there is a problem with your binary compilation method. As I don't have experience with Tensorflow on iOS then i'm not sure whether you've come across a bug or not. Hopefully someone more qualified can come and help you. I know when I had issues with Android sometimes just starting fresh and running through the docs again sometimes helped. Good luck!\r\n\r\n", "I'm sorry I can't express clearly in English, but thank you very much for your reply!\"Libtensorflow - core. a\"\r\n![767ce499-260e-4951-84c3-c043b14d95c6](https://user-images.githubusercontent.com/8908244/26875754-dbd0a3ac-4bb5-11e7-94c8-27bea6a0ac84.png)\r\n is given by the official, and I'm not sure if it will  cause package very big, let me very worried", "@petewarden is this expected?", "The intermediate sizes of the library files on disk don't reflect the size that's added to the binary. The only impact of this file size is on your local drive, it shouldn't affect the size of the application package.", "Thank you for your reply.\r\nThe libtensorflow-core.a lib will be linded to my app which will Increase the package size ,so I want to reduce it.   how can I  modify makefile to remove files that I dont need .\r\n![snip20170611_1](https://user-images.githubusercontent.com/8908244/27010878-957f0536-4ee1-11e7-8bdb-3eb9eb4e8d85.png)\r\n", "What Pete is saying is that once your app is built it should compress. It's similar with the android app when you quantise. It shows as full size but once built in it compresses. Try putting it the app and then analysing for size to see if it's compressed as @petewarden suggested.", "Thank you for your reply. I tried, as you said,the package  is compressed,  an'd  the size of the library files can accept, thank you very much for your answer!!!!!!", "You're welcome. But thank @petewarden for his helpful comment that I totally forgot! Embarrassing as I only went through a similar phase of my project not too recently. If this is solved please close the issue. Good luck with your project!", "Thank @petewarden , also  thank you. can close this issue now!!", "facing the same issue , is any one would tell me how to reduce size ?\r\n the doc only said that config tf_op_files.txt .. \r\n\r\n1. need we also config other files such as  tf_pb_text_files.txt ,  proto_text_cc_files.txt  ... for reducing?\r\n2. is there any other thinking for reducing than configuring the txt files Exhaustive Attackly ?\r\n ", "@jakiechris are your library sizes not reduced once you compile or analyse your app? They appear bigger than on disk as @petewarden noted.", "@jubjamie you are right, final app increased in smaller size.\r\n   \r\nand i compeleted the reduce work several months before, only cut tf_op_files.txt is enough.\r\n\r\nhere's my reduce for android:\r\n[tf_op_files.txt](https://github.com/tensorflow/tensorflow/files/1255296/tf_op_files.txt)\r\n", "Hi @jubjamie @petewarden really appreciate your insights as I was also disturbed by the fact that the lib generated for Android is so large (more than 100MB). I am curiously what is the logic behind it? Why after compiled in Android Studio the lib size can be compressed so much? I think when I use selective registration I have already omit unnecessary ops?"]}, {"number": 10482, "title": "Support channel groups in convolutional layers", "body": "This PR implements the channel groups in convolutional layers (Conv1D, Conv2D, Conv3D, Conv2DTransposed).\r\n\r\nThe grouped convolution was firstly implemented in AlexNet as a way to share filter parameters across feature maps. A detailed discussion on this feature can be found on [here](https://github.com/BVLC/caffe/issues/778). This feature is supported by Caffe ([doc](http://caffe.berkeleyvision.org/tutorial/layers/convolution.html)) and used in its reference [caffenet](https://github.com/BVLC/caffe/blob/master/models/bvlc_reference_caffenet/train_val.prototxt#L128). Adding this feature to TensorFlow makes it easier to compare models on two different frameworks and migrate from Caffe to TensorFlow.\r\n", "comments": ["Can one of the admins verify this patch?", "Francois, mind commenting whether this would be useful to add?", "I don't think it's currently widely used enough to justify becoming part of the core layers. The proposed implementation would also not be useful in any practical setting, because it relies on manually splitting tensors, running independent convolutions ops, and concatenating the outputs. This will be very slow and inefficient.\r\n\r\nWhen we want to add support for convolution groups, it should happen at the op level, not be manually implemented as a graph of ops on the Python side. In theory, a convolution with groups (e.g. 8 groups in a 128-big channel space) should be significantly faster than a regular convolution, but with this setup it would be dramatically slower. Since the added speed / efficiency is the core reason for using them, that is clearly a big issue.", "Thanks for the pull request! If you'd like to add support for this feature in the meanwhile, consider adding it to contrib, or reopening the feature request and discussing it there.", "The lack of group convolution prevents migrating many pre-trained models from Caffe to TensorFlow. For example, 3 out of 5 example Caffe [models](https://github.com/BVLC/caffe/tree/master/models) use this feature. The users have to manually split the Caffe kernel to multiple TensorFlow conv layers, which is especially laborious and error-prone for large networks. This manual approach is also done in Python that is no faster, if not slower, than in conv layer.\r\n\r\nIn terms of performance, if the group number is set to 1 (as default), there is no performance difference from the current version. It would be ideal to implement this feature in the op. Supporting \r\nthis feature on the API can be a first step. It allows users to migrate to TensorFlow. Then we can profile and optimize.", "@bowang \r\nI think its much slower when group is large when using your codes.\r\nFor example, group=32 is used in resnext\r\ngroup convolution is more than 2 times slower than normal convolution(group=1)\r\n(total_ops is 128% and total_params is 34% for group=32 & group=1 in my case)\r\n\r\nI find there may be something to do with the device.\r\nThe results above are based on GPU running. When I run my network on CPU, it seems that group convolution is more than 3 times faster than normal convolution\r\n\r\nGPU is slower somehow when loop-intensive ?\r\n\r\nIs there any way for optimization?", "@sleepfin do you mean a 32-group convolution is 2x slower than 32 normal convolutions?\r\n\r\nIt is understandable that group convolution is slower than normal convolution since it decomposes a big convolution into multiple smaller convolutions. Batching usually helps performance.\r\n\r\nA fair comparison should be one 32-group 4-channel convolution vs 32 single-group 4-channel convolutions, rather than one single-group 128-channel convolution.\r\n", "Again, if we add it, it should be done at the op level and should be fast.\r\n\r\nIf we add a feature as part of the core API then users should have a reasonable expectation that the feature does what it claims to do. People's motivation for using convolution groups is that they should result in cheaper, faster convolutions. If we provide this option but the result is actually *slower* convolutions, we are not answering user expectations.\r\n", "@bowang \r\nBut in a real case, we usually replace \"one single-group 128-channel convolution\" with \"32-group 4-channel convolution\" like [ResNeXt](https://arxiv.org/pdf/1611.05431.pdf), which means the param:group =1 or 32.\r\nAnd notice that on CPU divece,  \"32-group 4-channel convolution\" is faster than \"one single-group 128-channel convolution\" but the opposite on GPU device\r\nMy point is it will be great to have a high performance group convolution on both CPU and GPU.", "@bowang\r\nAnother question:\r\nBN & Activation & Bias_add layer are applied to:\r\n1 -> The output of a single path of conv groups ?\r\n2 -> The output of final concat layer ?\r\nIn your codes, I think it will be the first case if I use `with arg_scope`\r\nI'm not sure if its correct and more efficent to apply those layers to the final concat output.", "@sleepfin Thanks for the performance measurement! My hypothesis on the GPU/CPU performance difference is caused by the change of per-op and total computation workload in group convolution.\r\n\r\nAs in the current implementation that a single large convolution is decomposed into a number of small convolutions, the workload for a single small convolution may not be able to fully utilize a GPU to its limit. Thus, the GPU utilization decrease may lead to the slowdown.\r\n\r\nMeanwhile, when comparing the total amount of workload, that of the group convolution is smaller than that of a single convolution. Since CPU has much lower computation power, it is always saturated. Thus, the speedup on CPU was due to the reduction of total workload.\r\n\r\nThis is just my hypothesis. Hope it may generate some ideas for further performance debugging.", "@sleepfin Bias and activation are applied on the concatenated results of conv groups as you may find on line 194 (concat), line 218 (bias add), and line 221 (activation). BN is implemented as a separate layer which I think is out of the scope of conv layers.", "A recent paper highlights the possible usefulness of group convolutions for mobile uses: https://arxiv.org/abs/1707.01083\r\n\r\nLooking forward to see a possibility of group convolution implemented in TF.", "> Again, if we add it, it should be done at the op level and should be fast.\r\n\r\n@fchollet Do you have a typical pathway to recommend for people interested in efficiently implementing a layer in TF (i.e. where in specific in the op level should one implement)? ", "@fchollet  #25818 Added support for Conv2D Group Convolutions using CUDNN on GPUs. Would you be open to revisit this addition for Keras? I'd be happy to send a PR.", "@lgeiger @fchollet It would be great to have group convolutions and their transposed version in Keras. There are at least depth-wise convolutions, but no `DepthwiseConv2DTranspose` to build image decoders."]}, {"number": 10481, "title": "how to download tensorflow history version=1.0.0.  please give me some tips", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["_Warning: As this doesn't appear to be a bug with Tensorflow, the devs may ask for this to be moved to Stack Ovefrlow._\r\n\r\nI don't understand the question. But if you are looking to install previous versions of Tensorflow, consider downloading an old branch from Github and compiling from source or using pip install tensorflow==1.0.0. Alternatively, [visit here for the wheels.](https://pypi.python.org/pypi/tensorflow/1.0.0)", "You should be able to find this information in our website, if you look at version 1.0 docs.\r\nAlso, you can tell pip to download a specific version.\r\n\r\nClosing as this is not a bug/issue in TF.\r\nAs @jubjamie suggested, you can easily get help for such issues through stackoverflow."]}, {"number": 10480, "title": "Branch 158212897", "body": "", "comments": ["Looks like batch_dataset_op_test failure is legitimate.\r\nI cannot find the culprit for the issue though, maybe a merge issue?", "Need to redo to pull in a fix for the Android failure. I have a fix for the batch_dataset_op_test."]}, {"number": 10479, "title": "Possible bug: LSTMCell with use_peephole=True breaks when using initializer=tf.orthogonal_initializer", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, below.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX Sierra\r\n- **TensorFlow installed from (source or binary)**: binary - pip\r\n- **TensorFlow version (use command below)**: 1.2rc0\r\n- **Bazel version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**:\r\n```\r\n\r\nclass ToyModel(object):\r\n\tdef __init__(self):\r\n\t\tx = tf.get_variable(\"x\", shape=[5, 3, 7],\r\n\t\t\t\t\t\t\tinitializer=tf.random_normal_initializer(),\r\n\t\t\t\t\t\t\ttrainable=False)\r\n\t\tcell = tf.contrib.rnn.LSTMCell(7, use_peepholes=True, initializer=tf.orthogonal_initializer)\r\n\t\tself.rnn_out, self.final_state = tf.nn.dynamic_rnn(cell=cell,\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t   inputs=x,\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t   parallel_iterations=8,\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t   time_major=True,\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t   dtype=tf.float32)\r\n\r\n\r\ngraph_context = tf.Graph()\r\nwith graph_context.as_default():\r\n\tm1 = ToyModel()\r\n\r\n\ttf_init = tf.global_variables_initializer()\r\n\tsave_dir = \"/Users/delkind/Desktop/whd/tf_checkpoints/unit_test\"\r\n\r\n\tsv = tf.train.Supervisor(logdir=save_dir)\r\n\twith sv.managed_session() as sess:\r\n\t\ty1 = m1.rnn_out.eval(session=sess)\r\n\r\n\t\tprint(y1)\r\n```\r\n### Describe the problem\r\nI believe this is a bug. When using this code, the following error is raised.\r\n```\r\n\r\nTraceback (most recent call last):\r\n  File \"src/tensorflow_unit_tests.py\", line 84, in <module>\r\n    m1 = ToyModel()\r\n  File \"src/tensorflow_unit_tests.py\", line 79, in __init__\r\n    dtype=tf.float32)\r\n  File \"/Users/delkind/Desktop/whd/venv_tf_1.2rc0/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 566, in dynamic_rnn\r\n    dtype=dtype)\r\n  File \"/Users/delkind/Desktop/whd/venv_tf_1.2rc0/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 729, in _dynamic_rnn_loop\r\n    swap_memory=swap_memory)\r\n  File \"/Users/delkind/Desktop/whd/venv_tf_1.2rc0/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2766, in while_loop\r\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n  File \"/Users/delkind/Desktop/whd/venv_tf_1.2rc0/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2595, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/Users/delkind/Desktop/whd/venv_tf_1.2rc0/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2545, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/Users/delkind/Desktop/whd/venv_tf_1.2rc0/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 712, in _time_step\r\n    skip_conditionals=True)\r\n  File \"/Users/delkind/Desktop/whd/venv_tf_1.2rc0/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 198, in _rnn_step\r\n    new_output, new_state = call_cell()\r\n  File \"/Users/delkind/Desktop/whd/venv_tf_1.2rc0/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 700, in <lambda>\r\n    call_cell = lambda: cell(input_t, state)\r\n  File \"/Users/delkind/Desktop/whd/venv_tf_1.2rc0/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 685, in __call__\r\n    output, new_state = self._cell(inputs, state, scope)\r\n  File \"/Users/delkind/Desktop/whd/venv_tf_1.2rc0/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 165, in __call__\r\n    return super(_RNNCell, self).__call__(inputs, state)\r\n  File \"/Users/delkind/Desktop/whd/venv_tf_1.2rc0/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 439, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/Users/delkind/Desktop/whd/venv_tf_1.2rc0/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 376, in call\r\n    \"w_f_diag\", shape=[self._num_units], dtype=dtype)\r\n  File \"/Users/delkind/Desktop/whd/venv_tf_1.2rc0/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1065, in get_variable\r\n    use_resource=use_resource, custom_getter=custom_getter)\r\n  File \"/Users/delkind/Desktop/whd/venv_tf_1.2rc0/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 962, in get_variable\r\n    use_resource=use_resource, custom_getter=custom_getter)\r\n  File \"/Users/delkind/Desktop/whd/venv_tf_1.2rc0/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 360, in get_variable\r\n    validate_shape=validate_shape, use_resource=use_resource)\r\n  File \"/Users/delkind/Desktop/whd/venv_tf_1.2rc0/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 168, in _rnn_get_variable\r\n    variable = getter(*args, **kwargs)\r\n  File \"/Users/delkind/Desktop/whd/venv_tf_1.2rc0/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 352, in _true_getter\r\n    use_resource=use_resource)\r\n  File \"/Users/delkind/Desktop/whd/venv_tf_1.2rc0/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 725, in _get_single_variable\r\n    validate_shape=validate_shape)\r\n  File \"/Users/delkind/Desktop/whd/venv_tf_1.2rc0/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 200, in __init__\r\n    expected_shape=expected_shape)\r\n  File \"/Users/delkind/Desktop/whd/venv_tf_1.2rc0/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 278, in _init_from_args\r\n    initial_value(), name=\"initial_value\", dtype=dtype)\r\n  File \"/Users/delkind/Desktop/whd/venv_tf_1.2rc0/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 701, in <lambda>\r\n    shape.as_list(), dtype=dtype, partition_info=partition_info)\r\n  File \"/Users/delkind/Desktop/whd/venv_tf_1.2rc0/lib/python2.7/site-packages/tensorflow/python/ops/init_ops.py\", line 481, in __call__\r\n    raise ValueError(\"The tensor to initialize must be \"\r\nValueError: The tensor to initialize must be at least two-dimensional\r\n\r\n```\r\nThis is unexpected behavior. If you omit either of ```initializer=tf.orthogonal_initializer``` or ```use_peephole=True```, the graph can be built and evaluated as expected. I'm not aware of a mathematical reason the weights in this model cannot be orthogonal.\r\n\r\n", "comments": ["This is probably caused by the peephole parameter variables being vectors,\na limitation of the orthogonal initializer not knowing whether to treat\nthese as row or column vectors.\n\nOn Jun 8, 2017 8:39 AM, \"Michael Isard\" <notifications@github.com> wrote:\n\n> Assigned #10479 <https://github.com/tensorflow/tensorflow/issues/10479>\n> to @ebrevdo <https://github.com/ebrevdo>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/10479#event-1115753906>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim4_8D08nnl1Yb0myjlTCbdi8NRu_ks5sCBWsgaJpZM4NyKQi>\n> .\n>\n", "@ebrevdo Still exists in release 1.2.", "The peephole connections are vectors that represent the diagonal of a\nmatrix; so an orthonormal initialization for these would be a vector of all\nones.  The solution is to fix the orthonormal initializer so you can decide\nhow to treat vectors.\n\nOn Sat, Jun 17, 2017 at 12:24 PM, David J. Elkind <notifications@github.com>\nwrote:\n\n> @ebrevdo <https://github.com/ebrevdo> Still exists in release 1.2.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/10479#issuecomment-309234929>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim5YE83NtY1Phx50aSU1rwPK-9PW5ks5sFCgHgaJpZM4NyKQi>\n> .\n>\n", "@ebrevdo \r\nTo be clear, my complaint is about the software interface, not the mathematics.\r\n\r\nFrom a software standpoint, there are two ways to look at my bug report.\r\n\r\nOne way to look at it is that the LSTM cell is defective for trying to pound a square peg (the peepholes) into a round hole (the same initializer used for the rest of the cell). Not allowing the peephole connections to be easily initialized _independently_ from the rest of the LSTM weights is the origin of the problem, to my mind. For example, in TF 1.2, the method `GRUCell.__init__` has no documentation, but it appears to have two weight initialization arguments, one for kernels, and one for biases. This approach to initialization seems perfectly compatible with the LSTM, with the added benefit that the GRU and LSTM classes are brought into closer alignment.\r\n\r\nThe other way is that which you point out: the orthogonal initializer needs to be smarter. But changing its behavior wholesale may not be uniformly desirable, e.g. when someone is initializing vector parameters other than peephole connections. So for this special case, I could write an initializer for this narrow corner case, and it would solve this immediate problem, but every other user would encounter the same issue when attempting to use the orthogonal initializer, or another initialization, because these initializers are not \"peephole-compatible.\"", "I encountered with this problem, too. \r\nThank you for your post, or I think it will take me a lot of time to find the reason of this error.\r\n\r\nBut now I still wonder what the simplest way is for me to use both peephole and orthogonal initializer, cuz peephole is the most popular variant of LSTM.\r\n", "You can copy the orthogonal initializer code and modify it to generate\ncorrect values when a vector is requested.\n\nOn Sep 4, 2017 9:37 AM, \"Tingbo Lu\" <notifications@github.com> wrote:\n\n> I encountered with this problem, too.\n> Thank you for your post, or I think it will take me a lot of time to find\n> the reason of this error.\n>\n> But now I still wonder what the simplest way is for me to use both\n> peephole and orthogonal initializer, cuz peephole is the most popular\n> variant of LSTM.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/10479#issuecomment-326999540>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim3HmI5DB-plxqfP_gFN7Yfd2WOEEks5sfCdZgaJpZM4NyKQi>\n> .\n>\n", "I have encountered this same problem in T2T.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@Sycor4x would you be interested in adding a peephole_initializer option to `LSTMCell` so that optimizer can be used for peephole connections if it's available?", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Which version of TF has the fix?", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Hi @Sycor4x! \r\nIt seems you are using older versions(1.x versions) of Tensorflow. We recommend that you upgrade  your code base to 2.x  versions as many features and bug fixes has been done in newer versions and  create a new  issue if it still persists in newer versions. Thanks!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 10478, "title": "tfdbg error when stepping through a graph that uses tf.train.shuffle_batch", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.04\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: v1.2.0-rc1-24-gce1d6ec 1.2.0-rc2\r\n(I also got the same error with tf 1.1.0)\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA 8.0, cuDNN 5.1\r\n- **GPU model and memory**: Nvidia GTX 1060\r\n- **Exact command to reproduce**:\r\nThe following code will start a tfdbg session. Inside this session, enter these commands to get the error:\r\ntfdbg> invoke_stepper\r\ntfdbg> s\r\ntfdbg> s\r\ntfdbg> s\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python import debug as tf_debug\r\n\r\ndef read_records(filename_queue, enqueue_many_size=1024):\r\n    reader = tf.TFRecordReader()\r\n    _, queue_batch = reader.read_up_to(filename_queue, enqueue_many_size)\r\n    return queue_batch\r\n\r\ndef input_batch(filenames, batch_size, num_epochs, min_after_dequeue=128, num_threads=8):\r\n    filename_queue = tf.train.string_input_producer(filenames, num_epochs=num_epochs)\r\n    queue_batch = read_records(filename_queue)\r\n\r\n    capacity = min_after_dequeue + (num_threads + 3) * batch_size\r\n    batch_serialized_example = tf.train.shuffle_batch(\r\n        [queue_batch],\r\n        batch_size=batch_size,\r\n        num_threads=num_threads,\r\n        capacity=capacity,\r\n        min_after_dequeue=min_after_dequeue,\r\n        enqueue_many=True)\r\n\r\n    return batch_serialized_example\r\n\r\nb = input_batch(['test_filename'], 32, 5)\r\nsess = tf_debug.LocalCLIDebugWrapperSession(tf.Session())\r\nsess.run(b)\r\n```\r\n\r\n### Describe the problem\r\ntfdbg seemingly cannot step through a graph that uses tf.train.shuffle_batch. I cannot work around it by \"stepping over\" the node using step -t either.\r\n\r\n### Source code / logs\r\nRunning the code provided above and stepping through the graph produces this stack trace. In a real graph, the error stops me from stepping any further. I worked out from looking at types_pb2.py that the value 20 is DT_RESOURCE. It seems like the error is triggered when tfdbg tries to convert this datatype to a numpy array.\r\n\r\n```\r\n--- Node Stepper: run #1: 1 fetch (shuffle_batch:0); 0 feeds ------\r\n| <-- --> | s\r\nError occurred during handling of command: step :\r\n<class 'KeyError'>: 20                                                                                                                                                                                    UP\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ed/.pyenv/versions/neda/lib/python3.6/site-packages/tensorflow/python/debug/cli/debugger_cli_common.py\", line 664, in dispatch_command\r\n    output = handler(argv, screen_info=screen_info)\r\n  File \"/home/ed/.pyenv/versions/neda/lib/python3.6/site-packages/tensorflow/python/debug/cli/stepper_cli.py\", line 487, in step\r\n    screen_output = self.cont([self._sorted_nodes[self._next]], screen_info)\r\n  File \"/home/ed/.pyenv/versions/neda/lib/python3.6/site-packages/tensorflow/python/debug/cli/stepper_cli.py\", line 397, in cont\r\n    restore_variable_values=parsed.restore_variable_values)\r\n  File \"/home/ed/.pyenv/versions/neda/lib/python3.6/site-packages/tensorflow/python/debug/lib/stepper.py\", line 679, in cont\r\n    options=run_options)\r\n  File \"/home/ed/.pyenv/versions/neda/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 789, in run\r\n    run_metadata_ptr)\r\n  File \"/home/ed/.pyenv/versions/neda/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 952, in _run\r\n    subfeed_dtype = subfeed_t.dtype.as_numpy_dtype\r\n  File \"/home/ed/.pyenv/versions/neda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py\", line 122, in as_numpy_dtype\r\n    return _TF_TO_NP[self._type_enum]\r\nKeyError: 20\r\n```\r\n", "comments": ["Same problem arises when using tf.train.batch", "Here is an even smaller example:\r\n\r\n```\r\n#!/usr/bin/python3\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python import debug as tf_debug\r\n\r\ndef problem_op():\r\n    q = tf.FIFOQueue(1, [ tf.string ])\r\n    return q.enqueue([ \"hello\" ])\r\n\r\nsess = tf_debug.LocalCLIDebugWrapperSession(tf.Session())\r\nsess.run(problem_op())\r\n```\r\nIt is specifically the enqueue operation that can't be stepped over.", "I looked at this for a bit to see if the fix was easy, or if there was a way to at least 'skip' the offending nodes in the debugger.\r\n\r\nThe problem appears to be a design constraint of the TensorFlow debugger: The NodeStepper is implemented by using feed_dict to replace the target node's dependencies with previously evaluated nodes, which are stored on disk. feed_dict requires its values to be numpy-compatible, and a queue object is not something Numpy has.\r\n\r\nIt looks like an implementation should:\r\n* Implement a way for an arbitrary resource to be serialized to disk(or at least queues)\r\n* Allow the feed_dict to replace queue nodes too.\r\n\r\nI wouldn't be comfortable making this kind of design-level change myself, and it doesn't seem possible to make a quick fix to get the debugger working.", "Nice detective work! Yeah, I assumed the lack of movement here meant it was probably not a trivial fix :/", "Sorry for the inconvenience caused by this bug and my delayed response. The reason for this delay is that instead of fixing the specific problem that @MichaelBurge pointed out, we are planning a more thorough revamping of the NodeStepper.\r\n\r\nThe current implementation of NodeStepper has several significant limitations, including inability to handle `tf.while_loop` and incompatibility with queues and other DT_RESOURCE-based ops. In fact, the NodeStepper has been treated as more of a minor feature of tfdbg (relative to its main value dumping and inspection features) and is less well documented. The new implementation being planned will overcome these difficulties. It will be based on the recently added grpc debugger protocol (e.g., https://github.com/tensorflow/tensorflow/commit/41803db36d4f4a3239bd81e5d460eb0e6e2eea88 and https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/debug/lib/grpc_debug_server.py). So please be patient and stay tuned.", "Is this problem fixed yet? If yes, what version of tensorflow is invoke_stepper available in?", "@km-sharad A new feature is being worked on in TensorBoard to replace tfdbg CLI's `stepper` mode (tfdbg CLI's `run` will remain). The ETA is end of 2017.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "I'd actually tried to debug on CPU and it did not work. Was not aware hat it will work n GPU only. Anyway, my model works now and I do not need this feature right now. Thanks for your responses!", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing this bug now that the TensorBoard Debugger Plugin has entered alpha release. Please see:\r\nhttps://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/debugger/README.md", "Just to be clear, the TensorBoard Debugger Plugin is not meant to replace the main features of the command-line interface of TensorFlow Debugger. Instead, it offers a GUI alternative, that is more robust for interactive stepping-based debugger on TF computation graphs."]}, {"number": 10477, "title": "Fix defect: shuffle_batch gives ZeroDivisionError when computing capacity stat", "body": "Fixes #1853.", "comments": ["Can one of the admins verify this patch?", "@jhseu this is the equivalent of PR #10431, and CLA passed", "@tensorflow-jenkins test this please."]}, {"number": 10475, "title": "Severin/configure without user interaction", "body": "configure step without user interaction using defaults and additional build target\r\n", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->"]}, {"number": 10474, "title": "No module named 'tensorflow'", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Please fill out the template or we cannot help and your issue will be closed.", "Try opening issues this way:\r\n[https://github.com/tensorflow/tensorflow/issues/6548](https://github.com/tensorflow/tensorflow/issues/6548)\r\nor:\r\n[https://github.com/tensorflow/tensorflow/issues/647](https://github.com/tensorflow/tensorflow/issues/647)\r\n\r\nPossible solutions on stackoverflow:\r\n[https://stackoverflow.com/questions/42970106/modulenotfounderror-no-module-named-tensorflow](https://stackoverflow.com/questions/42970106/modulenotfounderror-no-module-named-tensorflow)\r\n[https://stackoverflow.com/questions/42244198/importerror-no-module-named-tensorflow](https://stackoverflow.com/questions/42244198/importerror-no-module-named-tensorflow)\r\n[https://stackoverflow.com/questions/42970106/modulenotfounderror-no-module-named-tensorflow](https://stackoverflow.com/questions/42970106/modulenotfounderror-no-module-named-tensorflow)\r\n[https://stackoverflow.com/questions/37756452/no-module-named-tensor-flow-ipython-notebook](https://stackoverflow.com/questions/37756452/no-module-named-tensor-flow-ipython-notebook)\r\n[https://stackoverflow.com/questions/42528563/no-module-named-tensorflow-error-after-installation](https://stackoverflow.com/questions/42528563/no-module-named-tensorflow-error-after-installation)", "Closing for now but please reopen with the issue if it isn't a problem best addressed on stackoverflow."]}, {"number": 10473, "title": "Revert \"Map Staging Area (#9686)\"", "body": "This reverts commit 1705b099df0fd7c2ea4abf83cfee896febeb8309.", "comments": ["Jenkins, test this please", "@sjperkins fyi", "Abandoning for now. I'll assume it's fixed in the other pull request."]}, {"number": 10472, "title": "Revert \"Fix patching issue on Windows\"", "body": "Reverts tensorflow/tensorflow#10452\r\n\r\nBreaks windows bazel build.", "comments": ["Not sure why there is 5 commits, did this through the UI.\r\nWill double check to not revert anything unintended.", "stage_op_test failure known issue, merging."]}, {"number": 10471, "title": "[TF:XLA] XLA does not recognize symbol Polly.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: ('v1.0.0-1783-g4c3bb1a', '1.0.0')\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: - \r\n- **GPU model and memory**: -\r\n- **Exact command to reproduce**: -\r\n\r\n### Problem description\r\nI have written my custom build file to integrate Polly into XLA's LLVM build. I have included a dependency to ```@llvm//:polly``` in my [tensorflow/blob/master/tensorflow/compiler/xla/service/cpu/BUILD](https://gitlab.com/annanay25/tensorflow/blob/master/tensorflow/compiler/xla/service/cpu/BUILD#L294) file under the ```compiler_functor``` rule (Since this is the file with the LLVM PassManagerBuilder defined). \r\nI also added the header \r\n```\r\n#include \"external/llvm/tools/polly/include/polly/RegisterPasses.h\"\r\n```\r\nin this file.\r\nSomehow, this does not seem to be enough for XLA to recognize Polly. When I add - \r\n\r\n```\r\npolly::registerPollyPasses(...)\r\n```\r\nto this file, it fails to compile with \r\n```\r\nERROR: Symbol polly was not declared here\r\n```\r\n\r\nI am unable to fix this (possibly linking) issue.\r\n\r\n@hawkinsp - Could you please take a look at this?\r\n\r\nPlease refer to #10288 for more relevant information.", "comments": ["@hawkinsp please unassign if you don't have time to look into this.", "I got this working with   \r\n```\r\n::polly::registerPollyPasses(...)\r\n```"]}, {"number": 10470, "title": "Fix test failures on windows.", "body": "", "comments": ["@gunan Thanks for fixing these tests!\r\nAs for `tensorflow/python/kernel_tests:reader_ops_test`, it failed because having problem [reading data file](https://github.com/tensorflow/tensorflow/commit/e6f58186363279496c46563e6f065ce7ea16c501#diff-5f4bbe7d0a58188f2ab3766df90062aaR888). This is a bug in Bazel, I've filed https://github.com/bazelbuild/bazel/issues/3134. I think we'll have to disable this test temporarily on Windows until the bug is fixed in a new release."]}, {"number": 10469, "title": "Export C API symbols in _pywrap_tensorflow_internal.so", "body": "This PR exports C API symbols so that python extension libraries can use the C API.\r\n\r\nAs noted in #7541, there is currently a conflict between `_pywrap_tensorflow_internal.so` and `libtensorflow.so`. This conflict prevents use of the tensorflow C API and the python API in the same process. This PR attempts to implement a workaround suggested by @jhseu by exporting the C API symbols in the python library, so that the Python and C APIs are both available from the single `_pywrap_tensorflow_internal.so`.", "comments": ["Can one of the admins verify this patch?", "Seems fine to me. Adding Asim to see if he has any objections.", "Jenkins, test this please", "Thanks for reviewing, @jhseu!"]}, {"number": 10468, "title": "[feature] Use Core ML on iOS", "body": "Apple announced [Core ML](https://developer.apple.com/documentation/coreml) which may be useful for abstracting away the complexities of the hardware platform:\r\n\r\n![image](https://user-images.githubusercontent.com/51059/26841383-5226ef0c-4ab7-11e7-9ce9-61849a3c0cc9.png)\r\n\r\nRelated:\r\n* https://github.com/tensorflow/tensorflow/issues/7958 (MPS)\r\n* https://github.com/tensorflow/tensorflow/issues/3001 (BNNS)", "comments": ["Actually, all that you really need is to write the implementation of the python conversion from a TF graph to a CoreML (also protobuf) model file:\r\n[CoreML custom conversion tool](https://developer.apple.com/documentation/coreml/converting_trained_models_to_core_ml#2903105)", "Thanks for opening this. We we treat this as the tracking issue, assigning to @keveman.", "fyi - https://github.com/hwchong/MNIST_DRAW", "Does your tool support transformation from TensorFlow Inception v3 Model to CoreML Model? @johndpope ", "The link above is a vanilla implementation of coremltools Python conversion of Kees's models (with tensorflow backend) if you search inception v3 in Keras and swap in /out code above it should work. ", "Use this instead https://github.com/oktapodi/inception_ios_test?files=1", "FYI if that's helpful I've hosted the package code on github to make conversations easier: https://github.com/gsabran/coremltools", "Thanks @gsabran.\r\n\r\nIs that the official pip package source or just a clone. I see apple mentions it in their docs. https://developer.apple.com/machine-learning/\r\n\r\nI do like that Apple just didn't invent their own crazy format but built it on protobuf.\r\n\r\nAs far as I'm aware keras can load from 'json + h5'. \r\n\r\n```\r\n# serialize model to JSON\r\nmodel_json = model.to_json()\r\nwith open(\"model.json\", \"w\") as json_file:\r\n    json_file.write(model_json)\r\n# serialize weights to HDF5\r\nmodel.save_weights(\"model.h5\")\r\nprint(\"Saved model to disk\")\r\n \r\n# later...\r\n \r\n# load json and create model\r\njson_file = open('model.json', 'r')\r\nloaded_model_json = json_file.read()\r\njson_file.close()\r\nloaded_model = model_from_json(loaded_model_json)\r\n# load weights into new model\r\nloaded_model.load_weights(\"model.h5\")\r\n```\r\n\r\nThis should be relatively straightforward to achieve to write a converter for right?", "@nojvek it's just a clone since there's no official source on github to my knowledge", "how convert \u201cckpt\u201d to \"mlmodel\"? ", "I'd suggest someone use the [caffe to tensorflow](https://github.com/ethereon/caffe-tensorflow) project as a starting point, maybe combining both, and output keras since it uses tensorflow as a backend and is compatible with coremltools.Really once you get the weights it's not horrible to do by hand, but still annoying.", "Hi there. I'm novice (2 days)on tensorflow and all deep learning. I managed to train a Mobilenet model (I got a output_graph.pb and a output_labels.txt) in order to test it on iOS. But in fact the coremltools does not provide a converser. How could I manage to do it?\r\nOption 1: tensorflow => keras => core ML\r\nOption 2: caffe (Mobilenet seems to exist but not sure of the official compliance and I tried without to manage to install tools) => core ML\r\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "There's now an official TensorFlow to CoreML converter at https://github.com/tf-coreml/tf-coreml, so closing this as fixed.", "@petewarden it doesn't seem official. This seems though -> https://github.com/apple/coremltools.", "Hi @petewarden and other tensorflow devs,\r\nnew to tensoflow world so sorry I'm misinterpreting something but\r\njust seeing Google release MLKit this week with support for Android & IOS I was curious to learn underlying tech..\r\nseems it uses for it's custom model API tensorflow lite underneath which also supports also both Android & IOS..\r\non Android it uses Android NN API if avaiable to use HW accel. on phone..\r\nproblem is from quick search is that altough tensorflow lite supports IOS it doesn't uses CoreML underneath so only using CPU mode..\r\nyou say here for using CoreML there is a converter, but then you must use CoreML API and abandon Tensorflow lite API usage.. in other words \"only\" helps with model conversion issues..\r\nnot same inference API for both iOS & Android that is HW accel. on both OSes..\r\nI say that because also at Google I/O also ARCore 1.2 was released and there is IOS support using ARKit underneath..\r\nso just saying same for tensorflow lite..\r\nof course Tensorflow API should support only reading CoreML models in case compiling for IOS..\r\nMay I open a separate issue?\r\nthanks..\r\n\r\n"]}]