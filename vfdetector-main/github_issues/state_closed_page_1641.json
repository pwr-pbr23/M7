[{"number": 3678, "title": "New Feature:  Pascal, Cuda 8, Unified memory", "body": "Hi,\n\nAs Cuda 8 enables unified memory for Pascal GPU, combining CPU and GPU on the same address level, and enhancing the memory size available for GPU (with limited latency), using CPU RAM.\n\n1) Is there a possibility to have larger than GPU ram NN+data (lower than CPU ram) for training in tensor flow ? (it would help reducing distributed computing/network latency) ?\n\nie, using the idea of  Oversubscribe GPU memory for large dataset/models.\n\nhere, CUDA API :  \nhttp://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-simplifying\n\nExample of 64GB allocation on GPU:\n\n```\nvoid foo() {\n// Allocate 64 GB on GPU, using CPU RAM\nchar *data;\nsize_t size = 64*1024*1024*1024;\ncudaMallocManaged(&data, size);\n}\n```\n", "comments": ["UVM is implemented through page-fault between CPU and GPU. It makes GPU program easier to be adopted, but not necessarily as fast as possible. Without more investigation, we are not sure this is suitable for high-performance machine learning. Note that the typical bandwidth across PCIE is often about two orders of magnitude slower than accessing the GPU dram itself. \n\nIf the model is indeed too large and cannot fit into GPU memory, it may make sense to load parts of the model in parallel, instead of relying the page faults in the kernels to page in the data. \n", "Cuda 8 with Pascal has Nvlink which is 80Gb/sec, so Latency is very low between RAM.\nIt allows to create larger than GPU memory through single memory allocation,\nAt very low latency. Performance would be enhanced.\n\nSee the slides.\n", "My understanding was that 80GB/s was between devices. The CPU/GPU communication through PCIE used by UVM is only a small fraction of that. And they are both much smaller than the 720GB/s when GPU accessing its own memory.\n\nThis is an active research area, and we might still find a good use of UVM down the road. But the current belief is that it is better to page in/out memory with CPU in parallel, while the compute engine on GPU accesses its own memory in full speed. A good example is the \"swap_memory\" option in \"tf.while_loop\", which swaps the temporary memory created in the loop to the host, when the device memory is under pressure. \n", "initial support added in https://github.com/tensorflow/tensorflow/commit/cd4f5840", "UVM support throughout would be of enormous benefit. I would be very keen to see this implemented. Having ideal training throughput is often less important than coping with large input and large model sizes which simply fail to fit on a card. ", "Looks like another \u201cwe don\u2019t need distributed transactions in Google so we choose to let users implement crappy/wrong versions of their own\u201d for BigTable/MegaStore.\n\nNow we have Spanner :)", "+1 for \"UVM support throughout\"!", "+1 for \"UVM support throughout\"!", "Any movement on this? \r\n\r\nCheers", "Hey guys, any progress? Highly interested!", "UVM support was added recently in https://github.com/tensorflow/tensorflow/commit/b1139814.", "Unified memory in Power9/V100 avoids page faults and page migrations with NVLINK 2.0 connection:\r\n\r\nhttp://on-demand.gputechconf.com/gtc/2018/presentation/s8430-everything-you-need-to-know-about-unified-memory.pdf", "@smit-hinsu do you know how uvm can be used in Tensorflow 2.0?", "@donglinjy I don't think uvm configuration is exposed in TF 2.0. Please file a separate feature request for that.\r\n\r\ncc @aaroey @jaingaurav "]}, {"number": 3677, "title": "TFLearn estimators not passing \"save_summary_steps\"", "body": "I think this should be an easy fix (or it was to hack it in locally), but it looks like [supervisor.py:L560](https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/contrib/learn/python/learn/estimators/estimator.py#L560) for release **r0.10** is calling `graph_actions._supervised_train` without passing the `save_summary_steps` parameter now configurable in RunConfig.\n\nI was able to correct by adding `supervisor_save_summaries_steps=self._config.save_summary_steps` in the call to graph_actions._supervised_train and this worked as expected, with events showing in TensorBoard on a higher configured frequency than the default 100 steps.\n", "comments": ["@eric-czech Thanks for reporting! I have submitted a fix. Going forward you are welcome to submit a PR to fix it directly! \n"]}, {"number": 3676, "title": "BatchNorm on GPU become very slow in latest TF", "body": "Nightly built, python2 gpu, cuda 7.5, cudnn 4.0.7, archlinux. TitanX.\nI run [batch_norm_benchmark.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/batch_norm_benchmark.py) and got the following output:\n\n```\nForward convolution (lower layers).\ncpu shape:4/3 #layers:10 mode:op scale:True train:False - 0.043865 secs\ncpu shape:4/3 #layers:10 mode:py scale:True train:False - 0.053864 secs\ncpu shape:4/3 #layers:10 mode:slow scale:True train:False - 0.088060 secs\n=== op vs py: 22.8% ===\n=== py vs slow: 63.5% ===\ngpu shape:4/3 #layers:10 mode:op scale:True train:False - 0.287913 secs\ngpu shape:4/3 #layers:10 mode:py scale:True train:False - 0.284179 secs\ngpu shape:4/3 #layers:10 mode:slow scale:True train:False - 0.287409 secs\n=== op vs py: -1.3% ===\n=== py vs slow: 1.1% ===\nForward/backward convolution (lower layers).\ncpu shape:4/3 #layers:10 mode:op scale:True train:True - 0.220112 secs\ncpu shape:4/3 #layers:10 mode:py scale:True train:True - 0.201284 secs\ncpu shape:4/3 #layers:10 mode:slow scale:True train:True - 0.295103 secs\n=== op vs py: -8.6% ===\n=== py vs slow: 46.6% ===\ngpu shape:4/3 #layers:10 mode:op scale:True train:True - 2.108785 secs\ngpu shape:4/3 #layers:10 mode:py scale:True train:True - 0.578407 secs\ngpu shape:4/3 #layers:10 mode:slow scale:True train:True - 0.863753 secs\n=== op vs py: -59.0% ===\n=== py vs slow: -96.6% ===\nForward convolution (higher layers).\ncpu shape:4/3 #layers:10 mode:op scale:True train:False - 0.025443 secs\ncpu shape:4/3 #layers:10 mode:py scale:True train:False - 0.033344 secs\ncpu shape:4/3 #layers:10 mode:slow scale:True train:False - 0.048162 secs\n=== op vs py: 31.1% ===\n=== py vs slow: 44.4% ===\ngpu shape:4/3 #layers:10 mode:op scale:True train:False - 0.164241 secs\ngpu shape:4/3 #layers:10 mode:py scale:True train:False - 0.161473 secs\ngpu shape:4/3 #layers:10 mode:slow scale:True train:False - 0.163212 secs\n=== op vs py: -1.7% ===\n=== py vs slow: 1.1% ===\nForward/backward convolution (higher layers).\ncpu shape:4/3 #layers:10 mode:op scale:True train:True - 0.111931 secs\ncpu shape:4/3 #layers:10 mode:py scale:True train:True - 0.118556 secs\ncpu shape:4/3 #layers:10 mode:slow scale:True train:True - 0.163289 secs\n=== op vs py: 5.9% ===\n=== py vs slow: 37.7% ===\ngpu shape:4/3 #layers:10 mode:op scale:True train:True - 1.194288 secs\ngpu shape:4/3 #layers:10 mode:py scale:True train:True - 0.329403 secs\ngpu shape:4/3 #layers:10 mode:slow scale:True train:True - 0.491005 secs\n=== op vs py: -72.4% ===\n=== py vs slow: 49.1% ===\nForward fully-connected.\ncpu shape:2/1 #layers:10 mode:py scale:True train:False - 0.001866 secs\ncpu shape:2/1 #layers:10 mode:slow scale:True train:False - 0.002859 secs\n=== py vs slow: 53.2% ===\ngpu shape:2/1 #layers:10 mode:py scale:True train:False - 0.002420 secs\ngpu shape:2/1 #layers:10 mode:slow scale:True train:False - 0.002523 secs\n=== py vs slow: 4.3% ===\nForward/backward fully-connected.\ncpu shape:2/1 #layers:10 mode:py scale:True train:True - 0.004973 secs\ncpu shape:2/1 #layers:10 mode:slow scale:True train:True - 0.006781 secs\n=== py vs slow: 36.4% ===\ngpu shape:2/1 #layers:10 mode:py scale:True train:True - 0.006370 secs\ngpu shape:2/1 #layers:10 mode:slow scale:True train:True - 0.008182 secs\n=== py vs slow: 28.4% ===\n```\n\nAlthough `sess.run` in a loop might not be an accurate way to benchmark, I think the performance regression exists because running the same script with an earlier binary built (not sure which commit it is) is 10x-20x faster.\n", "comments": ["I'm assuming this is a duplicate of #3603\n", "Oh it seems so. Thanks. Will close due to duplication.\n"]}, {"number": 3675, "title": "Source-compiled 0.9.0 version performs worse than binary-installed one", "body": "I'm running my ResNet-32 benchmark model. My code looks quite similar with the code in TensorFlow GitHub:\n\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/models/image/cifar10\n\nbut the model is changed into my own ResNet-32 implementation. \n\nI'm measuring how long it takes to train a minibatch with size 128, and here are brief results. I used one GPU, 64bit Ubuntu 14.04 and Python 2.7 for all experiments.\n\n| TensorFlow Version | CUDA/cuDNN Version | GPU | Elapsed Time (ms) |\n| :-: | :-: | :-: | :-: |\n| 0.9.0 (pip installed) | 7.5 / 4.0.7 | GTX Titan X | 75 |\n| 0.9.0 (745f16f790d2f9abf2c6a70d6b383b152cca3cff) | 8.0 RC / 5.0.5 | GTX 1080 | 330 |\n| 0.8.0 (ea9e00a630f91a459dd5858cb22e8cd1a666ba4e) | 8.0 RC / 5.0.5 | GTX 1080 | 60 |\n\nThe 0.8.0 version commit contains the first support for CUDA 8.0 RC as far as I know and I found this to use my GTX 1080 without performance drop. Now I can use it with reasonable performance, but I want to figure out why the 0.9.0 compiled version is 4 times slower than before.\n", "comments": ["I found that batch norm becomes very slow recently (#3676). Could you try removing batchnorm from resnet and benchmark again? It would be very interesting to know if there are other performance regression as well.\n", "@ppwwyyxx I just tried it and I think you're right. If I remove all batch norm layers, the elapsed time becomes 330 -> 30 ms with 745f16f790d2f9abf2c6a70d6b383b152cca3cff. The same experiment done with pip installed 0.9.0 takes 34 ms.\n\nSo this means it's not a CUDA 8.0 issue.\n", "Marking as duplicate of #3676 \n"]}, {"number": 3674, "title": "PIP installation error due to libcuda.so.1 not found", "body": "I encountered a strange error after installing tensorflow through pip: the tensorflow says it cannot find libcuda.so.1, which should not exist. Does anyone know how to fix it? Thanks for help!\n### Environment info\n\nOperating System:\nCentOS 6\nInstalled version of CUDA and cuDNN: \nCuda 7.5 and cudnnv4\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0rc0-cp34-cp34m-linux_x86_64.whl\n### Logs or other output that would be helpful\n\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:102] Couldn't open CUDA library libcuda.so.1. LD_LIBRARY_PATH: /usr/local/GNU/gcc-4.9.2/lib64:/usr/local/cuda-7.5/lib64:/csproject/dygroup2/czeng/venv/lib:/usr/lib64/:/csproject/dygroup2/czeng/venv/cudnnv4/lib64\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:160] hostname: client108\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:185] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:356] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  352.93  Tue Apr  5 18:18:24 PDT 2016\nGCC version:  gcc version 4.4.7 20120313 (Red Hat 4.4.7-17) (GCC)\n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] kernel reported version is: 352.93.0\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1077] LD_LIBRARY_PATH: /usr/local/GNU/gcc-4.9.2/lib64:/usr/local/cuda-7.5/lib64:/csproject/dygroup2/czeng/venv/lib:/usr/lib64/:/csproject/dygroup2/czeng/venv/cudnnv4/lib64\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1078] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so.1; dlerror: libcuda.so.1: cannot enable executable stack as shared object requires: Operation not permitted\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nE tensorflow/stream_executor/cuda/cuda_driver.cc:491] failed call to cuInit: CUDA_ERROR_NO_DEVICE\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:153] retrieving CUDA diagnostic information for host: client108\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:160] hostname: client108\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:185] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:356] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  352.93  Tue Apr  5 18:18:24 PDT 2016\nGCC version:  gcc version 4.4.7 20120313 (Red Hat 4.4.7-17) (GCC)\n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] kernel reported version is: 352.93.0\n", "comments": ["When you say `libcuda.so.1` should not exist, does it exist?  Usually this kind of error is caused by a bad path or bad cuda installation.\n", "Also: CentOS isn't a fully supported platform, which may mean that our pip wheels don't work with it.\n", "Marking as community support since CentOS is not officially supported. There are definitely people monitoring the issues list who are experienced with CentOS that can better help you. Good luck!\n", "Is this still a problem?\r\n\r\nWhen you install CUDA, libcuda.so.1 should exist in your system.\r\nwhat do you see when you run \r\n```\r\nsudo find / -name libcuda.so.1\r\n```", "Closing issue due to inactivity."]}, {"number": 3673, "title": "Android demo crashes when package name is changed", "body": "### Environment info\n\nOperating System: Ubuntu 14.04 LTS 64-bit\nInstalled version of CUDA and cuDNN: none (not using GPU)\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`): fc9162975e52978d3af38549b570cc3cc5f0ab66\n2. The output of `bazel version`\n\n```\nBuild label: 0.3.0\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Jun 10 11:38:23 2016 (1465558703)\nBuild timestamp: 1465558703\nBuild timestamp as int: 1465558703\n```\n### Steps to reproduce\n1. Open Android demo in Android Studio (not imported)\n2. Change org/tensorflow/demo folder names to any other package name\n3. Use replace in path to replace org.tensorflow.demo with the new package name\n4. Run `bazel mobile-install //tensorflow/examples/android:tensorflow_demo --start_app`\n5. The demo crashes on startup with the message \"Unfortunately, TensorFlow Demo has stopped.\"\n### What have you tried?\n1. Importing the demo into Android Studio instead of simply opening it. Was unable to build/run, even without changing package name.\n2. Modifying the `tensorflow_jni.h` file to remove occurrences of `ORG_TENSORFLOW`, etc. with the changed package name in the same format, e.g. `COM_ME`.\n3. Looked through other possibly relevant files for references to the old package name, didn't find anything else, though I might've overlooked some.\n### Logs or other output that would be helpful\n\n(no output)\n\nAlso, as a general question, if I wanted to create my own Android app using TensorFlow, what sort of configuration would I need to do? I haven't found any explanation of how the demo works.\n", "comments": ["Turns out I missed a line in `imageutils_jni.cc`, and after changing that, the error was fixed. But at the moment, the only way I can work on an Android app with TensorFlow is by modifying the demo project. If I try to make a copy in another location, Bazel doesn't work, and creating a copy by importing it into Android Studio doesn't work either. Additionally, logcat seems to be unavailable, so I have no way to see any output/errors/printing. Is there a better way to handle all this?\n", "@cardshuffle If your final question is also covered by https://github.com/tensorflow/tensorflow/issues/3685, can I close this one?\n", "Yes, logcat is available now that the project is properly opened in Android Studio.\n"]}, {"number": 3672, "title": "added image patch grad", "body": "This solves: https://github.com/tensorflow/tensorflow/issues/2921\n\nI ran unit tests using tensorflow.python.ops.gradient_checker to verify that it works :D\n", "comments": ["Can one of the admins verify this patch?\n", "added the tests, let me know if there's anything else\n", "@tensorflow-jenkins test this please\n", "Jenkins, test this please.\n", "The tests are failing :(\n", "I'm looking at the logs, and the one that's failing is //tensorflow/tools/test:check_futures_test.\nI think it's because my test file \"tensorflow/python/kernel_tests/extract_image_patches_grad_test.py\" doesn't import \"absolute_import\"?\n\nShould we let the other two tests finish, and then I'll push the fix? or should I push it now?\n", "Push the fix, and I'll re-trigger the tests.\n", "@tensorflow-jenkins test this please\n", "@benoitsteiner Can you LGTM this now that tests were added?\n", "Looks like the BUILD file change for the test is missing from this PR.\nTherefore Jenkins did not run the new feature, and I suspect this is broken.\nI am investigating now.\n", "I am preparing a fix for the issue.\nIt will be pushed to github later today, or tomorrow.\n", "The current gradient seems to have a problem when `batches` is `None` i.e. when using a variable size batches using Keras. Simply added this line\n\n``` python\n  batch_size = array_ops.shape(op.inputs[0])[0]\n```\n\nafter this\n\n``` python\n  batch_size, rows_in, cols_in, channels = [\n    dim.value for dim in op.inputs[0].get_shape()\n  ]\n```\n\nand it seems to have fixed the problem. I am new to TensorFlow, so you probably have a better solution than this.\n"]}, {"number": 3671, "title": "Add layer_norm op to contrib.layers.", "body": "Addresses #3621 for python at least. The new op is modeled after batch_norm and borrows code from it heavily.\n", "comments": ["Can one of the admins verify this patch?\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "Added the proper email addresses to my account. Do I need to do anything else to resolve the CLA auto-reject?\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@xodus7 seems ok, have you use this op in any example such as seq2seq translate. the first two test func are still about batch_norm.\n", "@suiyuan2009 I've tested it with good results on a custom character RNN using a modified GRU cell (as described in the appendix of the paper). Training performance was ~50% slower due to the 4 extra layer normalizations per cell, but the network converged much quicker and with a lower perplexity overall.\n", "@suiyuan2009 Good catch on the test functions. I'll fix...\n", "Left some comments\n", "Jenkins test this please\n", "Jenkins, please test this.\n", "@martinwicke Why I cannot test this? and why I cannot merge it either?\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "@xodus7 Please update the last set of comments\n", "@sguada I've addressed the comments. Some of the changes were due to recent commits it seems (variable_scope) so I went ahead and re-based on master. That seems to have reset the googlebot. I hope that it didn't create a major inconvenience for you.\n", "@sguada can you confirm that this is ready to merge (pending tests)?\n", "@tensorflow-jenkins test this please\n", "@rmlarsen once test pass is ready for merge\n", "the test failure is a known flaky test. Merging.\n", "Thanks, on my experiment generalizes much better than batch norm.\nFor those who don't want to update, copy the file from master replacing the file in local python dir would work. (with one line minor change).\n", "Great stuff, Thank you! Just one question though, has anyone used this layer on recurrent networks and found some major slow down in terms of wall clock time? (about 3-5x slower compared to vanilla rnn without any form of normalization.) \n\nAlthough I think this is not the problem of this particular implementation, since I found [a thread on reddit ](https://www.reddit.com/r/MachineLearning/comments/4ufmxy/layer_normalization_implemented_in_tensorflow/), where other people have observed same slowdown with a different implementation. However, one of the co-authors from the original layer norm paper [commented in the same thread ](https://www.reddit.com/r/MachineLearning/comments/4ufmxy/layer_normalization_implemented_in_tensorflow/d5q95oe) stating that the implementation in Theano does not cause any slowdown with the help of CNMeM.\n\nI am really eager to solve this, since layer norm does help the model to converge faster in terms of iteration number, and if there is a way to incorporate it into rnn without any slowdown, I would love to add it to my future experiments more often. \n", "@MycChiu Yeah I'm also seeing a significant slowdown, though not quite that dramatic. I'm not too well versed on the Eigen library but it could be that my use of the batch_normalization op in this way is less than optimal.\n", "I found that this slowdown is not limited to rnn, it also happens to fully connected layers. Here's a quick benchmark:\n\n``` python\nfrom tensorflow.contrib.layers import layer_norm\nfrom timeit import default_timer\n\nimport tensorflow as tf\nimport numpy as np\nslim = tf.contrib.slim\n\nbatch_size = 64\nnb_units = 512\nnb_layers = 20\nnb_epoch = 3\nitrs = 100\n\n\ndef benchmark(norm_fn):\n    epo_times = np.zeros([nb_epoch])\n    with tf.Graph().as_default():\n        # fake data\n        x = tf.random_uniform([batch_size, 30])\n        y = tf.random_uniform([batch_size], maxval=10)\n        labels = tf.to_int32(y)\n        # define graph, need to pass scale argument since batch_norm's default\n        # option for scale is False while that of layer_norm is True.\n        out = slim.repeat(x, nb_layers, slim.fully_connected, nb_units,\n                          # normalizer_fn=layer_norm,\n                          normalizer_fn=norm_fn,\n                          normalizer_params={\"scale\": True},\n                          scope=\"fc\")\n        # calculate loss\n        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(out, labels)\n        # define optimizer\n        optimizer = tf.train.AdamOptimizer(1e-3)\n        # create train_op\n        train_op = optimizer.minimize(loss)\n        # initialization op\n        init = tf.initialize_all_variables()\n\n        with tf.Session(\"\") as sess:\n            sess.run(init)\n            # warm up\n            for i in range(itrs):\n                sess.run(train_op)\n            # start benchmark\n            for e in range(nb_epoch):\n                epo_start = default_timer()\n                for i in range(itrs):\n                    sess.run(train_op)\n                eT = (default_timer() - epo_start)\n                epo_times[e] = eT\n            return epo_times\n\n\n\nvanilla_t = benchmark(None)\nbatchNorm_t = benchmark(slim.batch_norm)\nlayerNorm_t = benchmark(layer_norm)\nprint(\"vanilla times in seconds:\")\nprint(vanilla_t)\nprint(\"batch_norm times in seconds:\")\nprint(batchNorm_t)\nprint(\"layer_norm times in seconds:\")\nprint(layerNorm_t)\nprint(\"batch_norm is %1.2fX slower\" %\n          (batchNorm_t.sum() / vanilla_t.sum()))\nprint(\"layer_norm is %1.2fX slower\" %\n          (layerNorm_t.sum() / vanilla_t.sum()))\n```\n\nHere's my output:\n\n```\nvanilla times in seconds:\n[ 0.42624067  0.42290944  0.43818542]\nbatch_norm times in seconds:\n[ 0.74915753  0.74786358  0.75310942]\nlayer_norm times in seconds:\n[ 1.13074295  1.12952367  1.13170891]\nbatch_norm is 1.75X slower\nlayer_norm is 2.63X slower\n```\n\nI just built tensorflow from source using the current master branch, so it's probably not the infamous batch_norm regression from few weeks ago. I wonder if this slowdown is inevitable for layer normalization or there are actually ways to improve the speed.\n\n@xodus7 I too can't think of a better way to implement this, I tried a naive approach using lower level ops, but it was even slower than yours.\n", "I'm no expert on low-level GPU architecture but I wonder if the slowdown is due to caching effects as we normalize across layer activations rather than across features.\n", "@MycChiu Just to add another data point, I ran your benchmark on my system and got very similar results:\n\nvanilla times in seconds:\n[ 0.392066    0.39233494  0.39980507]\nbatch_norm times in seconds:\n[ 0.68308187  0.68399501  0.68293214]\nlayer_norm times in seconds:\n[ 1.08977199  1.08793688  1.08768916]\nbatch_norm is 1.73X slower\nlayer_norm is 2.76X slower\n", "@xodus7 Yeah, I wonder the same thing, but too bad I have no idea how to improve this. Hopefully someone capable of improving this will see this thread soon. (and thanks for your benchmark result, at least now I know the slowdown is probably not caused my machine's configuration.)\n", "If anyone is still interested, I have written a [GPU kernel for layer normalization](https://github.com/MycChiu/fast-LayerNorm-TF) which reduce the slowdown dramatically. ", "@MycChiu would you send a PR with your implementation of layer norm? It probably can go where the batch_norm_fused kernel is. Thanks!\r\n\r\nPS, if you are at NIPS we can meet up.", "I think it would need a bit more work to make it more general but seems a good beginning.", "phew...after some hours of battling with bazel's build rules, I finally merged the codes into `tf.contrib.layers`. Here's the PR #6205.\r\n\r\nI didn't put them in `core/kernels`, because as @sguada suggested, it currently has quite a lot limitations, and since I also wrote the layer function mimicking current `tf.contrib.layers.layer_norm`, I decided to put them under `tf.contrb.layers`\r\n\r\n@ilblackdragon I would love to! Too bad my current financial state doesn't allow me to travel to Spain :(", "@xodus7 One question about the layer_normalization op in this PR:\r\nDid you really implement the layer_norm based on Ba et al 2016?\r\n\r\nAs proposed in  Ba et al 2016, each instance in a mini-batch has its individual beta/gamma, but there is no such option in your implementation. Instead, you implemented the beta/gamma following batch normalization-style.\r\n`params_shape = inputs_shape[-1:]`\r\n`gamma = variables.model_variable('gamma', shape=params_shape,dtype=dtype)`\r\n`outputs = nn.batch_normalization(inputs, mean, variance, offset=beta, scale=gamma, variance_epsilon=variance_epsilon)`\r\n\r\nThe authors of this paper should be able to comment on that: @jimmylba @ryankiros"]}, {"number": 3670, "title": "Update CUDA/cuDNN in Dockerfiles", "body": "Currently, the Dockerfiles are using CUDA 7.5 and cuDNN v4:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel-gpu#L1\n\nAny chance of updating the Dockerfile to cuDNN v5 as a first step? It might be just a single-character fix :) cuDNN v5 is RC but stable AFAIK.\n\nIdeally, it would be also useful for users with Pascal boards to also update to CUDA 8.0 as the next step (we now provide CUDA 8.0 RC on CentOS 7, ubuntu 14.04 and ubuntu 16.04). But I would understand if you prefer to wait until CUDA 8.0 is out of RC.\n\nRelated:\nhttps://github.com/NVIDIA/nvidia-docker/pull/131\n\ncc: @cbiffle @jendap @thatguymike\n", "comments": ["cuDNN v5 is now GA, so it should be safe to use.\n", "(Indeed, we should switch to v5 ASAP).  Do you know when CUDA 8.0 is planned to exit RC?\n", "The current estimate is September 12, but it might change.\n", "Solved with 60bb54e3111daf5a57008c97db05a91a93101fd7, closing.\n"]}, {"number": 3669, "title": "Update Docker instructions", "body": "Docker is supported as a way of using TensorFlow:\nhttps://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#docker-installation\n\n> We provide 4 Docker images:\n> \n> ```\n> gcr.io/tensorflow/tensorflow: TensorFlow CPU binary image.\n> gcr.io/tensorflow/tensorflow:latest-devel: CPU Binary image plus source code.\n> gcr.io/tensorflow/tensorflow:latest-gpu: TensorFlow GPU binary image.\n> gcr.io/tensorflow/tensorflow:latest-devel-gpu: GPU Binary image plus source code.\n> ```\n\nHowever, all those Docker images are 0.8, more than 3000 commits behind. An eon for TF ;)\n\n```\n$ nvidia-docker run -it --rm gcr.io/tensorflow/tensorflow:latest-devel-gpu python -c 'import tensorflow as tf; print tf.__version__'\n[...]\n0.8.0\n```\n\nJust below, you also mention the following:\n\n> We also have tags with latest replaced by a released version (e.g., 0.10.0rc0-gpu).\n\nHowever, I couldn't make it work for 0.10.0rc0 or 0.9.0:\n\n```\n$ docker run -it --rm gcr.io/tensorflow/tensorflow:0.10.0rc0-gpu\nUnable to find image 'gcr.io/tensorflow/tensorflow:0.10.0rc0-gpu' locally\nPulling repository gcr.io/tensorflow/tensorflow\ndocker: Tag 0.10.0rc0-gpu not found in repository gcr.io/tensorflow/tensorflow.\n\n$ docker run -it --rm gcr.io/tensorflow/tensorflow:0.9.0-gpu\nUnable to find image 'gcr.io/tensorflow/tensorflow:0.9.0-gpu' locally\nPulling repository gcr.io/tensorflow/tensorflow\ndocker: Tag 0.9.0-gpu not found in repository gcr.io/tensorflow/tensorflow.\n\n$ docker run -it --rm gcr.io/tensorflow/tensorflow:0.8.0-gpu\n[I 17:56:59.525 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret\n[... OK ...]\n```\n\nOn GitHub, you have another set of [instructions](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/README.md):\n\n> We currently maintain three Docker container images:\n> \n> ```\n> gcr.io/tensorflow/tensorflow - TensorFlow with all dependencies - CPU only!\n> \n> gcr.io/tensorflow/tensorflow:latest-gpu - TensorFlow with all dependencies and support for Nvidia Cuda\n> ```\n> \n> Note: We also publish the same containers into Docker Hub.\n\nSame problem, `gcr.io/tensorflow/tensorflow:latest-gpu` is `0.8.0`\nBut in this case, the link to the DockerHub is useful. `latest` is still `0.8.0`:\n\n```\n$ nvidia-docker run -it --rm tensorflow/tensorflow:latest-devel-gpu python -c 'import tensorflow as tf; print tf.__version__'\n0.8.0\n```\n\nBut you do have version tags, and even nightly builds:\n\n```\n$ nvidia-docker run -it --rm tensorflow/tensorflow:0.9.0-devel-gpu python -c 'import tensorflow as tf; print tf.__version__'\n0.9.0\n\n$ nvidia-docker run -it --rm tensorflow/tensorflow:0.10.0rc0-devel-gpu python -c 'import tensorflow as tf; print tf.__version__'\n0.10.0rc0\n\n$ nvidia-docker run -it --rm tensorflow/tensorflow:nightly-devel-gpu python -c 'import tensorflow as tf; print tf.__version__'\n0.10.0rc0\n```\n\nCan we fix `gcr.io`? Or the instructions?\n\nRelated:\nhttps://github.com/tensorflow/tensorflow/issues/3482#issuecomment-235314786\n\ncc @jendap @girving @thatguymike \n", "comments": ["Update: looks like `gcr.io` is fixed:\n\n```\n$ nvidia-docker run -it --rm gcr.io/tensorflow/tensorflow:latest-devel-gpu python -c 'import tensorflow as tf; print tf.__version__'\n[...]\n0.11.0rc0\n```\n\nAnd version tags are also available now on gcr.\n\n**However**,  tag `latest` on DockerHub is still `0.8.0`.\n", "The latest* tags should be up-to-date now. This should be the case for both images from Docker Hub (tensorflow/tensorflow) and gcr.io (gcr.io/tensorflow/tensorflow). \r\n\r\nPlease re-open this issue if there are remaining issues."]}, {"number": 3668, "title": "Fix cmake build", "body": " Fix cmake build\n\n```\n* Use dedicated Dockerflie.cmake for cmake: Ensure cmake version >=3.5.\n* Let cmake build protobuf 3.0.0-beta-2 from source. Version 3.0.0 cannot be used a.t.m. due to an integer type issue.\n* Fix a bug in tf_models.cmake.\n* Perform parallel make based on the number of cores.\n\nNew build command:\n\ntensorflow/tools/ci_build/ci_build.sh cmake\ntensorflow/tools/ci_build/builds/cmake.sh\n```\n", "comments": ["Test build passed: http://ci.tensorflow.org/job/experimental-cais-cpu-cmake/2/console (Jenkins login required to view)\n", "LGTM. Thanks for doing this, Shanqing!\n", "Make more changes to push the proto3 build into Dockerfile. New success run from test build: http://ci.tensorflow.org/view/Experimental/job/experimental-cais-cpu-cmake/11/console (Jenkins login required to view log)\n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins test this please.\n", "Very cool! LGTM.\n", "@zheng-xq @martinwicke ready to merge. \n"]}, {"number": 3667, "title": "convert tf.cond to constants failed by convert_variables_to_constants.", "body": "### Environment info\n\nOperating System:\nUbuntu 14.04.4 LTS\n\nIf installed from binary pip package, provide:\n0.9.0\n### Steps to reproduce\n\nimport tensorflow as tf\nfrom tensorflow.python.framework.graph_util import convert_variables_to_constants\n\ndef save_model(sess, output_node_names, path = \"predict.pb\"):\n    output_list =  output_node_names\n    output_graph_def = convert_variables_to_constants(sess, sess.graph_def, output_list)\n\nX1 = tf.Variable(1.)\nX2 = tf.Variable(1.)\n\ncond_value = tf.Variable(True)\n**cond_result = tf.cond(cond_value, lambda: tf.assign(X1, 2.), lambda: tf.assign(X2, 2.), name=\"cond_result\")**\nX3 = tf.add(X1, X2,\"X3\")\n\nwith tf.Session() as sesh:\n    sesh.run(tf.initialize_all_variables())\n    #sesh.run(cond_result)\n    save_model(sesh, [\"X3\",])\n### What have you tried?\n1. Test above code, convert_variables_to_constants failed.\n2.  If comment the line of tf.cond, call save model will be OK.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/graph_util.py\", line 226, in convert_variables_to_constants\n    returned_variables = sess.run(variable_names)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 382, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 598, in _run\n    processed_fetches = self._process_fetches(fetches)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 553, in _process_fetches\n    'Tensor. (%s)' % (subfetch, fetch, str(e)))\nValueError: Fetch argument u'cond_result/Assign/Switch:1:0' of u'cond_result/Assign/Switch:1:0' cannot be interpreted as a Tensor. (The name 'cond_result/Assign/Switch:1:0' looks a like a Tensor name, but is not a valid one. Tensor names must be of the form \"<op_name>:<output_index>\".)\n\nrefer to: [https://github.com/tensorflow/tensorflow/issues/3287](url)\n", "comments": ["Test from source aslo failed, the env infos are as follows;\n~tensorflow$ git rev-parse HEAD\n861644c0bcae5d56f7b3f439696eefa6df8580ec\nbazel version.\nBuild label: 0.3.1-2016-08-01 (@722fb2d)\n", "Hey, \nI am facing the exact same error in the same setting. Is there a work-around or solution?\nThx :) \n", "Could you try with the latest version?", "Closing due to lack of activity."]}, {"number": 3666, "title": "Fix analogy response of word2vec interactive when the response is unicode.", "body": "I run word2vec.py with interactive mode.\n\n```\n$ python word2vec.py --interactive --train_data=t --eval_data=q --save_path=save\n...\nIn [3]: model.analogy(b'\ub0a8\uc790', b'\uc5ec\uc790', b'\uc544\ube60')\nOut[3]: '\\xec\\x97\\x84\\xeb\\xa7\\x88'\n```\n\nThe response of analogy couldn't be readable.\n\nSo I fix the response and then rerun the code.\n\n```\nIn [3]: model.analogy(b'\ub0a8\uc790', b'\uc5ec\uc790', b'\uc544\ube60')\n\uc5c4\ub9c8\n```\n\nNow I can see the response.\n", "comments": ["Can one of the admins verify this patch?\n", "This doesn't seem quite right to me. Why should the analogy function sometimes return a string, but sometimes print a string and then return nothing? That is a strange contract for a function, and will confuse users of this code who are depending on the return value (i.e. not in interactive mode).\n\nInstead of changing the type signature of the function, can you instead make it handle unicode correctly? \n", "@danmane  I agree a strange contract for the analogy function. But IPython always prints an unreadable unicode output which is decoded or not.\n\nIPython examples:\n\n```\nIn [1]: '\ud55c\uae00'\n'\\xed\\x95\\x9c\\xea\\xb8\\x80'\n\nIn [2]: '\ud55c\uae00'.decode('UTF-8')\nu'\\ud55c\\uae00'\n```\n\nSo I fixed the function to be printing a result like the nearby function which is also a function for an interactive mode.\n", "@danmane please give this another look.\n", "Ok, @zffchen78 (who has a bit more context on this code) gave it LGTM.\n@tensorflow-jenkins, test this please.\n"]}, {"number": 3665, "title": "cannot generate visual studio solution file on Windows 7", "body": "### Environment info\n\nOperating System:\nWindows 7\n### What have you tried?\n\n1.on Windows 7 follow https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md\nwhen try to generate visual  studio sln file, error occurred.\n### Logs or other output that would be helpful\n\nCMakeError.log\n\nDetermining if the include file pthread.h exists failed with the following output:\nChange Dir: C:/tensorflow/tensorflow/contrib/cmake/build/solution/CMakeFiles/CMakeTmp\n\nRun Build Command:\"C:/Program Files (x86)/MSBuild/12.0/bin/MSBuild.exe\" \"cmTC_184bb.vcxproj\" \"/p:Configuration=Debug\" \"/p:VisualStudioVersion=12.0\"\nMicrosoft (R) Build Engine version 12.0.40629.0\n\n[Microsoft .NET Framework, version 4.0.30319.42000]\n\nCopyright (C) Microsoft Corporation. All rights reserved.\n\nBuild started 8/5/2016 8:36:31 PM.\n\nProject \"C:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\solution\\CMakeFiles\\CMakeTmp\\cmTC_184bb.vcxproj\" on node 1 (default targets).\n\nPrepareForBuild:\n\n  Creating directory \"cmTC_184bb.dir\\Debug\\\".\n\n  Creating directory \"C:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\solution\\CMakeFiles\\CMakeTmp\\Debug\\\".\n\n  Creating directory \"cmTC_184bb.dir\\Debug\\cmTC_184bb.tlog\\\".\n\nInitializeBuildStatus:\n\n  Creating \"cmTC_184bb.dir\\Debug\\cmTC_184bb.tlog\\unsuccessfulbuild\" because \"AlwaysCreate\" was specified.\n\nClCompile:\n\n  C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\bin\\x86_amd64\\CL.exe /c /Zi /W3 /WX- /Od /Ob0 /D WIN32 /D _WINDOWS /D _DEBUG /D \"CMAKE_INTDIR=\\\"Debug\\\"\" /D _MBCS /Gm- /RTC1 /MDd /GS /fp:precise /Zc:wchar_t /Zc:forScope /Fo\"cmTC_184bb.dir\\Debug\\\" /Fd\"cmTC_184bb.dir\\Debug\\vc120.pdb\" /Gd /TC /errorReport:queue C:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\solution\\CMakeFiles\\CMakeTmp\\CheckIncludeFile.c\n\n  Microsoft (R) C/C++ Optimizing Compiler Version 18.00.40629 for x64\n\n  Copyright (C) Microsoft Corporation.  All rights reserved.\n\n  cl /c /Zi /W3 /WX- /Od /Ob0 /D WIN32 /D _WINDOWS /D _DEBUG /D \"CMAKE_INTDIR=\\\"Debug\\\"\" /D _MBCS /Gm- /RTC1 /MDd /GS /fp:precise /Zc:wchar_t /Zc:forScope /Fo\"cmTC_184bb.dir\\Debug\\\" /Fd\"cmTC_184bb.dir\\Debug\\vc120.pdb\" /Gd /TC /errorReport:queue C:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\solution\\CMakeFiles\\CMakeTmp\\CheckIncludeFile.c\n\n  CheckIncludeFile.c\n\nC:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\solution\\CMakeFiles\\CMakeTmp\\CheckIncludeFile.c(1): fatal error C1083: Cannot open include file: 'pthread.h': No such file or directory [C:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\solution\\CMakeFiles\\CMakeTmp\\cmTC_184bb.vcxproj]\n\nDone Building Project \"C:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\solution\\CMakeFiles\\CMakeTmp\\cmTC_184bb.vcxproj\" (default targets) -- FAILED.\n\nBuild FAILED.\n\n\"C:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\solution\\CMakeFiles\\CMakeTmp\\cmTC_184bb.vcxproj\" (default target) (1) ->\n\n(ClCompile target) -> \n\n  C:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\solution\\CMakeFiles\\CMakeTmp\\CheckIncludeFile.c(1): fatal error C1083: Cannot open include file: 'pthread.h': No such file or directory [C:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\solution\\CMakeFiles\\CMakeTmp\\cmTC_184bb.vcxproj]\n\n```\n0 Warning(s)\n\n1 Error(s)\n```\n\nTime Elapsed 00:00:00.36\n", "comments": ["The CMake build in `tensorflow/contrib/cmake/...` doesn't currently support Windows. We are working on adding this support (see issue #17).\n\nFor running on Windows at present, the [best options](http://stackoverflow.com/q/33616094) are using Docker or installing the Linux version on the Windows 10 Subsystem for Linux.\n", "Is there any pre-build win10 tensorflow?\n"]}, {"number": 3664, "title": "Update jupyter_notebook_config.py", "body": "Adding the capability to set jupyter port via the PORT environment variable.\nThis is especially useful if this Docker container runs on clusters managers like Mesos, DCOS , etc.\n", "comments": ["Can one of the admins verify this patch?\n", "LGTM\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n"]}, {"number": 3663, "title": "Error when using TensorArray and variables in nested loops", "body": "Hi, I use r0.9 version but still have an error when use nested map_fn and a variable involved in calculations of inner function.\n\nThe error is \n\ntensorflow.python.framework.errors.InvalidArgumentError: Input 0 of node gradients/map/while/map/TensorArrayPack_grad/TensorArrayGrad/TensorArrayGrad was passed string from gradients/map/while/map/TensorArrayPack_grad/TensorArrayGrad/TensorArrayGrad/StackPop:0 incompatible with expected string_ref.\n\nThe code is\n\n```\nimport tensorflow as tf\n\ndef inner_loop(t):\n    def fn(n): return n + var # using var here leads to the error.\n    return tf.map_fn(fn=fn, elems=t, parallel_iterations=1)\n\ndef outer_loop(input):\n    def fn(n): return inner_loop(n) #if I would return inner_loop(n)+var here no errors appear.\n    return tf.map_fn(fn=fn, elems=input, parallel_iterations=1)\n\nwith tf.Session() as sess:\n    var = tf.Variable(tf.constant(1.0))\n    input = tf.to_float(tf.convert_to_tensor([[1,2],[3,4],[5,6]]))\n    res = outer_loop(input)\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n    trainOperation = optimizer.minimize(tf.reduce_mean(tf.square(res)))\n    sess.run(tf.initialize_all_variables())\n    sess.run(trainOperation)\n```\n", "comments": ["+1 I'm having the same issue with the scan function.\n", "+1. scan issues in nested condition throwing exception\n", "+1. Same issue when using map_fn inside dynamic_rnn. version=0.10.0rc0\n", "Update: the newest version fixed this issue. However, if parallel_iterations > 1, I got the following error:\n\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: TensorArray map/while/map/TensorArray_2_6@gradients: Could not read index 1 twice because it was cleared after a previous read (perhaps try setting clear_after_read = false?).\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: TensorArray map/while/map/TensorArray_2_2@gradients: Could not read index 0 twice because it was cleared after a previous read (perhaps try setting clear_after_read = false?).\n\nAny thoughts? @yuanbyu @ebrevdo \n", "Can you give a short code snippet that allows us to replicate the problem?\n\nOn Aug 20, 2016 12:17 PM, \"hhexiy\" notifications@github.com wrote:\n\n> Update: the newest version fixed this issue. However, if\n> parallel_iterations > 1, I got the following error:\n> \n> W tensorflow/core/framework/op_kernel.cc:968] Invalid argument:\n> TensorArray map/while/map/TensorArray_2_6@gradients: Could not read index\n> 1 twice because it was cleared after a previous read (perhaps try setting\n> clear_after_read = false?).\n> W tensorflow/core/framework/op_kernel.cc:968] Invalid argument:\n> TensorArray map/while/map/TensorArray_2_2@gradients: Could not read index\n> 0 twice because it was cleared after a previous read (perhaps try setting\n> clear_after_read = false?).\n> \n> Any thoughts? @yuanbyu https://github.com/yuanbyu @ebrevdo\n> https://github.com/ebrevdo\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3663#issuecomment-241218340,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim7uOz-uqefu43kr05OLMqFqG__Epks5qh1LEgaJpZM4JdjPE\n> .\n", "The same snippet---I just changed parallel_iterations to 10.\n\n```\nimport tensorflow as tf\n\ndef inner_loop(t):\n    def fn(n): return n + var # using var here leads to the error.\n    return tf.map_fn(fn=fn, elems=t, parallel_iterations=10)\n\ndef outer_loop(input):\n    def fn(n): return inner_loop(n) #if I would return inner_loop(n)+var here no errors appear.\n    return tf.map_fn(fn=fn, elems=input, parallel_iterations=10)\n\nwith tf.Session() as sess:\n    var = tf.Variable(tf.constant(1.0))\n    input = tf.to_float(tf.convert_to_tensor([[1,2],[3,4],[5,6]]))\n    res = outer_loop(input)\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n    trainOperation = optimizer.minimize(tf.reduce_mean(tf.square(res)))\n    sess.run(tf.initialize_all_variables())\n    sess.run(trainOperation)\n```\n", "@hhexiy, could you please tell the commit / branch where you found it working?\n", "Yes, this is probably a known bug. We are working on a fix.  In the meantime, for now could you please set `parallel_partitions` to 1 for the outer loop/map?\n", "@yuanbyu Thanks for the reply. Sure, that works for now.\n\n@stilda This is the version I'm using, from the master branch.\ncommit 647fb04b233f0476d46ce449125c7cc4087c8b81\nAuthor: Tijmen Tieleman tijmentieleman@gmail.com\nDate:   Thu Aug 18 14:27:05 2016 -0700\n\nOn Mon, Aug 22, 2016 at 1:14 AM, Alexander Korostilyov <\nnotifications@github.com> wrote:\n\n> @hhexiy https://github.com/hhexiy, could you please tell the commit /\n> branch where you found it working?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3663#issuecomment-241342215,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABJYppOn8mW-rtRTWmv2iTwPHFBQBwpKks5qiVqAgaJpZM4JdjPE\n> .\n", "A possible fix should be available in the next push.\n", "By the way, thank you so much for the simple test case!\n", "Hi, \n\nI am using rnn.row_rnn() with loop_fn which reads and writes a tensor_array of hidden states (e.g. I need more than one timestep memory) and I get the same error. Setting parallel_iterations =1 does not help me to move on. \n\nIn case of creating a python queue (or simply an array of cell_states) in the loop and passing it as a loop_state will still everything work fine in a graph while loop?\n", "Davit, can you post the minimal failing example?  This has worked for us\nfine.  You just have to ensure your updated TensorArray objects are fed\nback to the loop as outputs and you never access these temporary\nTensorArray objects outside the loop_fn.\n\nOn Oct 25, 2016 3:43 PM, \"Davit Buniatyan\" notifications@github.com wrote:\n\n> Hi,\n> \n> I am using rnn.row_rnn() with loop_fn which reads and writes a\n> tensor_array of hidden states (e.g. I need more than one timestep memory)\n> and I get the same error. Setting parallel_iterations =1 does not help me\n> to move on.\n> \n> In case of creating a python queue (or simply an array of cell_states) in\n> the loop and passing it as a loop_state will still everything work fine in\n> a Tensor graph while computing the gradients and the rest?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3663#issuecomment-256199623,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtimwmt3yX5_maaX8tGecRFq73aZh1wks5q3oYWgaJpZM4JdjPE\n> .\n", "The problem arises when I take one state from loop_state and concat with current state. Sorry if its not the minimal I just wanted to give the whole context.\n\n```\n        def loop_fn(time, cell_output, cell_state, loop_state):\n                emit_output = cell_output\n                if loop_state == None:\n                    loop_state = tf.TensorArray(dtype=tf.float32, size=args.width*args.height)\n                    for i in xrange(args.width):\n                         loop_state.write(i, cell.zero_state(args.batch_size, tf.float32))\n\n                if cell_state == None:\n                    state = cell.zero_state(args.batch_size, tf.float32)\n                else:  \n                    state = tf.cond(tf.reduce_all((time+1)%args.width == tf.constant(0)), \n                        lambda: cell.zero_state(args.batch_size, tf.float32), \n                        lambda: cell_state, name=None)\n\n                #State issues\n                prev_state_x = tf.slice(state, [0, 0], [-1, output_size*4])\n                prev_state_y = loop_state.read(time%args.width)\n                prev_state_y = tf.slice(prev_state_y, [0, output_size*4], [-1, output_size*2])\n                state = tf.concat(1,[prev_state_x, prev_state_y]) # <---- Problem\n\n                next_cell_state = state\n\n                elements_finished = (time >= sequence_length)\n                finished = tf.reduce_all(elements_finished)\n                next_input = tf.cond(\n                    finished,\n                    lambda: tf.zeros([args.batch_size, input_size], dtype=tf.float32),\n                    lambda: inputs_ta.read(time))\n\n                loop_state.write((time)%args.width, state)\n                next_loop_state = loop_state\n\n                return (elements_finished, next_input, next_cell_state,\n                        emit_output, next_loop_state)\n\n      outputs_ta, final_state, _ = rnn.raw_rnn(cell, loop_fn, parallel_iterations=1)\n```\n", "We'll try to make a minimal unit test out of this and see what's going on.\n", "Great thanks! I am implementing Multidim RNNs  and if I can be helpful in anyway let me know.\n", "I tried writing a unit test that mixes state & loop_state like yours, but was not able to get it to fail.  Are you using a TF nightly build?\n\nAlso, this line:\n\n```\nstate = tf.cond(tf.reduce_all((time+1)%args.width == tf.constant(0)), \n                        lambda: cell.zero_state(args.batch_size, tf.float32), \n                        lambda: cell_state, name=None)\n```\n\nwhat happens if you just set state = cell_state?\n\nI'm still waiting for Yuan's improved debugging to be pushed to github, guess it's gonna be another day or so?  In the meantime we don't have very useful error messages.\n", "I moved to nightly build on docker. I tried to set state = cell_state as well, but no difference. If I write instead of \n`prev_state_y = tf.slice(prev_state_y, [0, output_size*4], [-1, output_size*2])` \nthis \n`prev_state_y = tf.slice(state, [0, output_size*4], [-1, output_size*2])`\nThen it works like a usual RNN without giving any error on reading or writing the loop_state TensorArray\n", "I simplified the case as much as I could\n\n```\ninputs_ta = tf.TensorArray(dtype=tf.float32, size=args.width*args.height)\ninputs_ta = inputs_ta.unpack(inputs)\n```\n\n```\ndef loop_fn(time, cell_output, cell_state, loop_state):\n    emit_output = cell_output\n    if loop_state == None:\n        loop_state = tf.TensorArray(dtype=tf.float32, size=args.width*args.height)\n        for i in xrange(args.width*args.height):\n            loop_state.write(i, cell.zero_state(args.batch_size, tf.float32))\n\n    elements_finished = True\n    next_input = inputs_ta.read(time)\n\n    next_cell_state = loop_state.read(time)\n    next_loop_state = loop_state\n    return (elements_finished, next_input, next_cell_state,\n            emit_output, next_loop_state)\n```\n\nand this produces the following error\n\n```\n...\nInvalidArgumentError (see above for traceback): TensorArray content_vgg/gridrnn_1/RNN/TensorArray_0: Could not read from TensorArray index 0 because it has not yet been written to.\n     [[Node: content_vgg/gridrnn_1/RNN/TensorArrayRead_1 = TensorArrayRead[_class=[\"loc:@content_vgg/gridrnn_1/RNN/TensorArray\"], dtype=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](content_vgg/gridrnn_1/RNN/TensorArray, content_vgg/gridrnn_1/RNN/Const, content_vgg/gridrnn_1/RNN/TensorArray/Const)]]\n\n```\n", "Ok, problem is now resolved sorry for disturbance.  @ebrevdo I haven't noticed that TensorArray.write actually returns a new array so basically I was supposed to reassign loop_state on every write. Thank you very much.\n", "Ah OK.  yes i was about to say that :)  it's a structure that is as\nfunctional as possible; if you don't use the return value it skips the\nwrite.\n\nOn Thu, Oct 27, 2016 at 8:56 AM, Davit Buniatyan notifications@github.com\nwrote:\n\n> Ok, problem is now resolved sorry for disturbance. @ebrevdo\n> https://github.com/ebrevdo I haven't noticed that TensorArray.write\n> actually returns a new array so basically I was supposed to reassign\n> loop_state on every write. Thank you very much.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3663#issuecomment-256687358,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim_F8pRtVgidZQ8h5ts_gK-cvAHkAks5q4Mm5gaJpZM4JdjPE\n> .\n", "@ebrevdo - Hi , is this a Issue in the following code , I am not able to find out , what i am doing wrong with this following snippet\n\n```\n import tensorflow as tf\n from tensorflow.python.ops import tensor_array_ops\n import numpy as np\n\ndef body(time_var, attention_tracker):\n    a = tf.random_uniform(shape=[2, 2], dtype=tf.int32, maxval=100)\n    b = tf.constant(np.array([[1, 2], [3, 4]]), dtype=tf.int32)\n    c = a + b\n    attention_tracker.write(time_var, c)\n    return time_var+1 ,  attention_tracker\n\ndef condition(time_var, attention_tracker):\n    return time_var < 10\n\n x = tf.Variable(tf.constant(0, shape=[2, 2]))\n attention_tracker = tensor_array_ops.TensorArray(tf.float32, size=1, dynamic_size=True,     infer_shape=False)\n\n time = tf.constant(0)\n time_new , attention_tracker_result = tf.while_loop(condition, body, [time, attention_tracker])\n\n with tf.Session():\n     tf.initialize_all_variables().run()\n     print(attention_tracker_result.pack().eval())\n```\n\nThe error is InvalidArgumentError: TensorArray TensorArray: Could not read from TensorArray index 0 because it has not yet been written to.\n", "Closing this since it's based on old code. Feel free to open a new issue if the problem persists with new code."]}, {"number": 3662, "title": "Fix typo in error message formats", "body": "The parameters were not formatted properly.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it! \ud83d\udcdd \n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "Thanks for the pull request!\n"]}, {"number": 3661, "title": "Revert \"Fix prototype mismatch of ByteCount in env.cc\"", "body": "Reverts tensorflow/tensorflow#3614\n", "comments": []}, {"number": 3660, "title": "cannot run on x86 emulator android example", "body": "Linux Sabayon 4.3 on intel x86_64 platform \n- cannot build and run an apk for x86_64 cpu android emulator.\n- bazel release 0.3.1-2016-08-02 (@8b4df1b)\n\nI run this command in order to build the apk package that I would like to run on x86_64 emulator\n\nbazel build --android_cpu=x86_64 //tensorflow/examples/android:tensorflow_demo\n\nBut in \nbazel-bin/tensorflow/examples/android/_dx/tensorflow_demo/native_symlinks/MANIFEST\nI\n still see a link to an arm library...\n\nI've  built the library for x86_64 with this\n\nbazel build //tensorflow/examples/android:tensorflow_native_libs --crosstool_top=//external:android/crosstool --cpu=x86_64 --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\n\nI thought that specifying --android_cpu=x86_64 was enough to link the apk towards x86_64 native library\n\nKind regards.\nSeba\n", "comments": ["Assigned to @petewarden, but maybe @martinwicke would recommend following up with bazel project instead?\n", "We had many iterations on that code since Aug. Closing this. Feel free to open a new bug if the problem persists in recent versions."]}, {"number": 3659, "title": "Cross-import KMeans and GMM in tf.learn.estimators", "body": "- Cross-import KMeans and GMM in tf.learn.estimators\n", "comments": ["@agarwal-ashish do you prefer to keep this separate? We may just want to cross-import the estimator. That way it's clearer who the best contact for the code.\n", "Yeah we probably need some agreement going forward. I just saw GMM got merged in as well, which is a similar case. In my opinion, it's just easier to generate the docs and they will be in the same place for users to see. \n", "There's more of these coming. I'm leery of moving them around. However, we should cross-import them. Do you want to change the PR to do that? \n\nRegarding the documentation, we will reorganize the documentation more thoroughly, and hopefully that will address the issue of scattered docs. \n\nFYI @xmbrst @dr4b \n", "Sounds good. I'll see if I have some time this weekend. Thanks for clarification. \n", "@martinwicke I updated the PR to simply cross-import the others. This Linux GPU PIP test does not seem to finish, as seen in other PRs as well. Do you know why? \n", "@martinwicke Seems fixed now :-) \n", "@ilblackdragon: this is the right thing to do for users, but it is weird because there will be an expectation of API stability for the externally provided libraries. @terrytangyuan, I will hold off on merging this while we figure out policy and team responsibility questions.\n", "Sure that makes sense. \n", "@martinwicke any resolution on this yet?\n", "Let's not do this for now. Things will move out of contrib at their on pace, and I'd like to avoid more interdependencies.\n", "Sorry about the runaround -- I'll close this.\n", "Sure no problem. That makes sense. \n"]}, {"number": 3658, "title": "API for getting hidden layer state", "body": "I'm using `tf.contrib.learn.TensorFlowDNNRegressor` and I'd like to get the hidden layers state after forward propagation (calling predict()). There doesn't seem to be an API to do this. Will one be provided in the future, or would I need to avoid `tf.contrib.learn`?\n", "comments": ["@goncalopp We don't have anything planned at this point. One way around is to use custom `Estimator` class and return various tensors that you want to inspect in the `predictions` dictionary.\n\nWould this work for you?\n\nPS. you should switch to use `DNNRegressor` as `TensorFlowDNNRegressor` is deprecated.\n", "Thanks for looking into this!\nI've been reading the code, and writing a custom Estimator would involve essentially copy-pasting \n`_infer_model()`, `_get_predict_ops()` and `_logits()`, which would be very brittle, considering how fast this code is changing. Would you be interested in a pull request that modifies `BaseEstimator` to optionally return these? \n", "The better solution is to add hooks into predict method, to allow to pass `CaptureVariable` that would capture the values of hidden layers. \n- @ispirmustafa who is modifying underlaying layers to use `MonitoredSession` and can easily add this.\n", "We changed things around with the MonitoredSession if memory serves. @goncalopp is this still current?", "@drpngx I've moved to using a custom `Estimator`, as ilblackdragon suggested - it's not as easy, but works well. We can close this, if it's not a priority.\r\nI took a quick look on `MonitoredSession`, but it seems geared to persisting state, not accessing it? The feature request here is accessing the raw neuron weights, post-training - for visualization purposes, for example. Would `MonitoredSession` be able to do that?", "@ispirmustafa could probably help on the MonitoredSession.\r\n\r\nClosing this to tidy up. Please re-open if it becomes more important."]}, {"number": 3657, "title": "0.10.0rc0: Contrib distributions crash when sampling \"n\" is scalar", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System: \nmacOS 10.11\n\nInstalled version of CUDA and cuDNN: \n7.5 / 4\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed. \n   mac os / Python 3.5 / GPU / 0.10rc0\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   0.10.0rc0\n### Steps to reproduce\n\n``` python\nimport numpy as np\nimport tensorflow as tf\n# this works\ntf.contrib.distributions.MultivariateNormalDiag(np.ones(5), np.ones(5)).sample([1])\n# crashes; scalar case is not handled\ntf.contrib.distributions.MultivariateNormalDiag(np.ones(5), np.ones(5)).sample(1)\n```\n", "comments": ["@langmore Can you take a look?\n", "This is a documentation issue.  `.sample()` expects a rank 1 Tensor (or something convertible to that) specifying a shape.  So the scalar case was not intended to be handled.  I'm fixing the docs now.\n", "Doc change submitted.  Thanks for reporting this Jeremiah!\n", "Thanks @langmore, makes sense!\n"]}, {"number": 3656, "title": "Branch 129393964", "body": "Merging internal changes.\n", "comments": ["@tensorflow-jenkins, test this please\n", "Looks like that the old time out we have for the Python3 job (60 minutes) is a little too short, especially in the face of the low-level dockerfile changes in this push. I increased it to 90 minutes. Will kick off the test again.\n", "@tensorflow-jenkins, test this please\n"]}, {"number": 3655, "title": "Starting on a tf.contrib.learn notebook", "body": "I'd like to add an IPython notebook using tf.contrib.learn.\n\nI've started on this one. Content is similar to the MNIST For ML Beginners tutorial: https://www.tensorflow.org/versions/r0.10/tutorials/mnist/beginners/index.html\n\nCan continue fleshing it out down the road, and open to suggestions on how to improve.\n", "comments": ["Can one of the admins verify this patch?\n", "Awesome! One concern is that notebooks can get really large if we continue adding more in the future.  \n", "@martinwicke I'm not the right person for a `tf.learn` / docker PR.  Want to reroute?\n", "Jenkins, test this please.\n\nYeah, your assignment may have been a mis-click.\n", "Good point -- these are big. Can we clear the cells? Or would that defeat the purpose? Otherwise I would suggest we move all these tutorials into a separate repo.\n", "@craigcitro maybe you can opine? :)\n", "I think we should keep the content of the cells displayed, but open to\nhosting notebooks elsewhere to save on space. If we do host elsewhere,\nlet's put a single notebook in this directory that makes it clear how to\npull in the other notebooks pain-free.\n\nOn Mon, Aug 8, 2016 at 2:45 PM, Daniel W Mane notifications@github.com\nwrote:\n\n> @craigcitro https://github.com/craigcitro maybe you can opine? :)\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/3655#issuecomment-238336381,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ASyJIG5S7x566zQuTZPepFuHaxP42Csoks5qd3ldgaJpZM4JdOco\n> .\n", "I think a separate repo could be good, especially if it had a `requirements.txt` with an explicit version of tensorflow they were written against will also be a big win.\n\nNote that keeping the outputs in place has two big advantages:\n1. github does a great job displaying them (yay nbviewer)\n2. Makes it possible to write tests (at least internally)\n\nI think end-users will benefit a lot from (1).\n", "Separate repo it is.\n", "@martinwicke Please close this PR when you've set up another repo for the tutorials.\n", "@martinwicke what's the status here?\n", "Still working on someone to maintain the other repo. Any volunteers?\n\nOn Mon, Aug 22, 2016 at 10:12 AM, Rasmus Munk Larsen <\nnotifications@github.com> wrote:\n\n> @martinwicke https://github.com/martinwicke what's the status here?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/3655#issuecomment-241481788,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAjO_adSXIQLmNCgSqksLFGVpBvticn4ks5qidiCgaJpZM4JdOco\n> .\n", "I'm going to close this since it sounds like we're going to move this to another repo.  No value in having the PR in the open status\n"]}, {"number": 3654, "title": "archive.openswitch.net ssl cert path fails when installing syntaxnet.", "body": "Hey guys,\nWe've been using your default bazel shell script, `https://github.com/bazelbuild/bazel/releases/download/0.2.2b/bazel-0.2.2b-installer-linux-x86_64.sh` for a while to create a syntaxnet docker image for internal use, and it's been working great. However recently (past couple of days) we've tried a rebuild that started failing with the following error message:\n\n```\nERROR: /tmp/tensorflow/models/syntaxnet/tensorflow/tensorflow/workspace.bzl:84:3: no such package '@gmock_archive//': Error downloading from https://archive.open\nswitch.net/gmock-1.7.0.zip to /tmp/bazel/external/gmock_archive: Error downloading https://archive.openswitch.net/gmock-1.7.0.zip to /tmp/bazel/external/gmock_ar\nchive/gmock-1.7.0.zip: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable t\no find valid certification path to requested target and referenced by '//external:gtest'.\n```\n\nFrom quick inspection, it looks like the ssl cert path failed for https://archive.openswitch.net within the bazel build operation, IE it might have a self-signed certificate.\n\nThis is a brand new bug for us with no changes to our image or infrastructure, which makes us think that it's related specifically to the external file hosting you guys have designated in the shell script.\n\nLet me know if you need any other information to help\nThanks,\nJames\n", "comments": ["I am experiencing the same issue\n", "I have this problem as well.. \n", "Please re-file this issue to [tensorflow/models](github.com/tensorflow/models).\n\n@calberti: We switched away from the openswitch.net mirror because it isn't well maintained, you can look at the corresponding entry in the tensorflow WORKSPACE file, adapting yours to be the same should fix this.\n", "Thanks! I copied this whole part : native.new_http_archive(\n    name = \"gmock_archive\",\n    url = \"http://pkgs.fedoraproject.org/repo/pkgs/gmock/gmock-1.7.0.zip/073b984d8798ea1594f5e44d85b20d66/gmock-1.7.0.zip\",\n    sha256 = \"26fcbb5925b74ad5fc8c26b0495dfc96353f4d553492eb97e85a8a6d2f43095b\",\n    build_file = path_prefix + \"gmock.BUILD\",\n  )\nfrom original tensorflow and it worked! \n", "@hotomski : Can you please provide more thorough description of where you copied the code? I am fighting with this issue for a while and cannot make it work.\n", "@jiriker : You should place the code in the models/syntaxnet/tensorflow/tensorflow/workspace.bzl\nFind the section and just replace it with the new url to gmock-1.7.0.zip or copy paste the whole section as I did.  \n", "Btw, this seems to be because openswitch has put a Let's Encrypt CA certificate in place, but our JVM's don't trust the IdenTrust (\"Digital Signature Trust Co.\") signed issuer cert\n", "forked tensorflow as well as the models repo to solve this: \n\n**Tensorflow** - https://github.com/zeryx/tensorflow\n**Models** - https://github.com/zeryx/models\n\nI'll be actively maintaining & keeping everything in parity  until this is resolved internally.\n\n@hotomski I used your fix btw\n", "@zeryx Great! ;)\n", "It\u2019s because you do not have the certification! Your company do not give certifications to the relactive websites in tensorflow/workspace.bzl. So ask the IT to certificate it!"]}, {"number": 3653, "title": "Starting on a tf.contrib.learn notebook", "body": "I'd like to add an IPython notebook using tf.contrib.learn. \n\nI've started on this one. Content is similar to the MNIST For ML Beginners tutorial: https://www.tensorflow.org/versions/r0.10/tutorials/mnist/beginners/index.html \n\nCan continue fleshing it out down the road, and open to suggestions on how to improve.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n"]}, {"number": 3652, "title": "No such package '@iron_list (Device or resource busy)", "body": "### Environment info\n\n**Operating System:** Ubuntu 12.04\n\n**Installed from source:**\n**Commit Hash:** c5f94b10bbb30e525fa3ca313e7ccb173040c90a\n**Output of \"bazel version\":**\nWARNING: Output base '/home/user1/.cache/bazel/_bazel_user1/d49ce44fa50589aa9a9d880466ef1baf' is on NFS. This may lead to surprising failures and undetermined behavior.\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp as int: 0\n\nNote: Not sure why the actual version doesn't appear in the output, but it is 0.2.2b.\n\n**### Steps to reproduce**\n1. git clone https://github.com/tensorflow/tensorflow\n2. Installed Bazel\n3. Installed other dependencies\n4. ./configure\n5. /usr/bin/python\n6. N\n7. N (Note: After this, the configuration ended.)\n8. bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n\n**Error Message:**\nERROR: /home/user1/Documents/tensorflow/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_list//': /home/user1/.cache/bazel/_bazel_user1/d49ce44fa50589aa9a9d880466ef1baf/external/iron_list/.git/objects/pack/.nfs000000000189638500000904 (Device or resource busy) and referenced by '//tensorflow/tensorboard/bower:bower'.\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\n### What have you tried?\n1. Downloaded the necessary files locally and linked to them in tensorflow/WORKSPACE, as well as in tensorflow/tensorflow/tensorboard/bower/BUILD, with no success (Note: This is my first time using bazel, so I may not have done this correctly.).\n2. Googled the error; although many people seem to have bower-related errors with respect to downloading the files from GitHub, this seems to be a different type of error.\n3. Reran the command many times.\n4. Reinstalled nodejs, npm, and then bower on the computer and reran the command again.\n### Logs or other output that would be helpful\n\n**Full Log:**\n\n> bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n> WARNING: Output base '/home/user1.cache/bazel/_bazel_user1/d49ce44fa50589aa9a9d880466ef1baf' is on NFS. This may lead to surprising failures and undetermined behavior.\n> WARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\n> WARNING: /home/user1/Documents/tensorflow/util/python/BUILD:11:16: in includes attribute of cc_library rule //util/python:python_headers: 'python_include' resolves to 'util/python/python_include' not in 'third_party'. This will be an error in the future.\n> WARNING: /home/user1/.cache/bazel/_bazel_user1/d49ce44fa50589aa9a9d880466ef1baf/external/gemmlowp/BUILD:102:12: in hdrs attribute of cc_library rule @gemmlowp//:eight_bit_int_gemm: Artifact 'external/gemmlowp/public/bit_depth.h' is duplicated (through '@gemmlowp//:eight_bit_int_gemm_public_headers' and '@gemmlowp//:gemmlowp_headers').\n> WARNING: /home/user1/.cache/bazel/_bazel_user1/d49ce44fa50589aa9a9d880466ef1baf/external/gemmlowp/BUILD:102:12: in hdrs attribute of cc_library rule @gemmlowp//:eight_bit_int_gemm: Artifact 'external/gemmlowp/public/gemmlowp.h' is duplicated (through '@gemmlowp//:eight_bit_int_gemm_public_headers' and '@gemmlowp//:gemmlowp_headers').\n> WARNING: /home/user1/.cache/bazel/_bazel_user1/d49ce44fa50589aa9a9d880466ef1baf/external/gemmlowp/BUILD:102:12: in hdrs attribute of cc_library rule @gemmlowp//:eight_bit_int_gemm: Artifact 'external/gemmlowp/public/map.h' is duplicated (through '@gemmlowp//:eight_bit_int_gemm_public_headers' and '@gemmlowp//:gemmlowp_headers').\n> WARNING: /home/user1/.cache/bazel/_bazel_user1/d49ce44fa50589aa9a9d880466ef1baf/external/gemmlowp/BUILD:102:12: in hdrs attribute of cc_library rule @gemmlowp//:eight_bit_int_gemm: Artifact 'external/gemmlowp/public/output_stages.h' is duplicated (through '@gemmlowp//:eight_bit_int_gemm_public_headers' and '@gemmlowp//:gemmlowp_headers').\n> WARNING: /home/user1/.cache/bazel/_bazel_user1/d49ce44fa50589aa9a9d880466ef1baf/external/gemmlowp/BUILD:102:12: in hdrs attribute of cc_library rule @gemmlowp//:eight_bit_int_gemm: Artifact 'external/gemmlowp/profiling/instrumentation.h' is duplicated (through '@gemmlowp//:eight_bit_int_gemm_public_headers' and '@gemmlowp//:gemmlowp_headers').\n> WARNING: /home/user1/.cache/bazel/_bazel_user1/d49ce44fa50589aa9a9d880466ef1baf/external/gemmlowp/BUILD:102:12: in hdrs attribute of cc_library rule @gemmlowp//:eight_bit_int_gemm: Artifact 'external/gemmlowp/profiling/profiler.h' is duplicated (through '@gemmlowp//:eight_bit_int_gemm_public_headers' and '@gemmlowp//:gemmlowp_headers').\n> ERROR: /home/user1/Documents/tensorflow/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_list//': /home/user1/.cache/bazel/_bazel_user1/d49ce44fa50589aa9a9d880466ef1baf/external/iron_list/.git/objects/pack/.nfs000000000189638500000904 (Device or resource busy) and referenced by '//tensorflow/tensorboard/bower:bower'.\n> ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\n> INFO: Elapsed time: 0.374s\n\n**Additional Information:**\nThe package name in the error message rotates between iron_list, iron_icon, and iron_icons each new time the command is run.\n\nNot sure whether this is relevant, but I am able to run the following commands for syntaxnet:\ngit clone --recursive https://github.com/tensorflow/models.git\ncd models/syntaxnet/tensorflow\n./configure\ncd ..\nbazel test syntaxnet/... util/utf8/...\n\nwith all tests reported as being passed.  The only changes made to the included files in order to pass all tests successfully were to:\n1. Download gmock-1.7.0.zip manually, run the command: \"python -m SimpleHTTPServer 8000\", and change line 79 of workspace.bzl to: \"url = \"http://localhost:8000/gmock-1.7.0.zip\",\"\n2. Change line 25 of models/syntaxnet/syntaxnet/parser_trainer_test.sh to: \"BINDIR=$TEST_SRCDIR/$TEST_WORKSPACE/syntaxnet\"\n\nPlease let me know if you require any other information!  Thanks in advance for your help.\n\n-Natalie\n", "comments": ["This part of the log might have something to do with it:\n\n> WARNING: Output base '/home/user1/.cache/bazel/_bazel_user1/d49ce44fa50589aa9a9d880466ef1baf' is on NFS. This may lead to surprising failures and undetermined behavior.\n\nI think the appropriate resolution is to pass the flag `--output_base=/tmp/some/other/dir` (for example) to the `bazel build` command.\n", "That worked!  Thank you so much for your help; I hadn't thought to just try changing the output base.\n\nI appreciate your help!\n", "Glad to hear it!\n"]}, {"number": 3651, "title": "GTX 1070 source install bazel build -- Unsupported gpu architecture 'compute_61'", "body": "Here are my specs:\n- Ubuntu 15.10 (potential conflict with CUDA 7.5?)\n- NVIDIA GTX 1070 (compute capability 6.1)\n- CUDA 7.5\n- Cudnn v5.0\n\nError message:\n\n```\n\nERROR: /usr/local/lib/python2.7/dist-packages/tensorflow/tensorflow/core/kernels/BUILD:1496:1: error while parsing .d file: /home/volcart/.cache/bazel/_bazel_root/109ad80a732aaece8a87d1e3693889e7/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/batchtospace_op_gpu/tensorflow/core/kernels/batchtospace_op_gpu.cu.d (No such file or directory).\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nnvcc fatal   : Unsupported gpu architecture 'compute_61'\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 86.269s, Critical Path: 84.86s\n```\n\nIt is clearly complaining about the compute capability. Is a compute capability of 6.1 supported? Surely.  \n\nI am installing from source. \n\nSome dependency versions:\n\n```\nvolcart@volcart-Precision-Tower-7910:/usr/local/lib/python2.7/dist-packages/tensorflow_src$ bazel version\nExtracting Bazel installation...\n.\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp as int: 0\n\nvolcart@volcart-Precision-Tower-7910:/usr/local/lib/python2.7/dist-packages/tensorflow_src$ gcc -v\nUsing built-in specs.\nCOLLECT_GCC=gcc\nCOLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/4.9/lto-wrapper\nTarget: x86_64-linux-gnu\nConfigured with: ../src/configure -v --with-pkgversion='Ubuntu 4.9.3-5ubuntu1' --with-bugurl=file:///usr/share/doc/gcc-4.9/README.Bugs --enable-languages=c,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-4.9 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --with-gxx-include-dir=/usr/include/c++/4.9 --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --enable-gnu-unique-object --disable-vtable-verify --enable-plugin --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.9-amd64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.9-amd64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.9-amd64 --with-arch-directory=amd64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --enable-objc-gc --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu\nThread model: posix\ngcc version 4.9.3 (Ubuntu 4.9.3-5ubuntu1) \n\n\nvolcart@volcart-Precision-Tower-7910:/usr/local/lib/python2.7/dist-packages/tensorflow_src$ nvcc -V\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2015 NVIDIA Corporation\nBuilt on Tue_Aug_11_14:27:32_CDT_2015\nCuda compilation tools, release 7.5, V7.5.17\n```\n\nWhen I execute `sudo ./configure` I have tried different varients. Namely, the default for all options -- I only have one version of CUDA and Cudnn installed. \n\nI've also tried the following confirguration -- to the same end...\n\n```\nvolcart@volcart-Precision-Tower-7910:/usr/local/lib/python2.7/dist-packages/tensorflow_src$ sudo ./configure \nPlease specify the location of python. [Default is /usr/bin/python]: \nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] n\nNo Google Cloud Platform support will be enabled for TensorFlow\nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 7.5\nPlease specify the location where CUDA 7.5 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n[Default is: \"3.5,5.2\"]: 6.1\nSetting up Cuda include\nSetting up Cuda lib64\nSetting up Cuda bin\nSetting up Cuda nvvm\nSetting up CUPTI include\nSetting up CUPTI lib64\nConfiguration finished\n\n```\n\nI noticed CUDA 7.5 only supports Ubuntu 15.04 -- could this be a problem since I'm on Ubuntu 15.10?\n", "comments": ["I should note that a virtual_env install works totally fine, so maybe this isn't an issue?\n", "CUDA 7.5 doesn't support Pascal cards, so you'll need CUDA 8.0\nTensorFlow doesn't support CUDA 8.0, but you can make it work with extra\ntweaks like in https://github.com/tensorflow/tensorflow/issues/3431\n\nAlso CUDA 8.0 doesn't support Ubuntu 15.10, so you may need to upgrade to\n16.04, or fix a bunch of compilation errors by hand during install\n\nOn Thu, Aug 4, 2016 at 3:21 PM, Kendall Weihe notifications@github.com\nwrote:\n\n> I should note that a virtual_env install works totally fine, so maybe this\n> isn't an issue?\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3651#issuecomment-237701269,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHKYhD2RL5i0C8pEoGLbdr-R2H-ANks5qcmX5gaJpZM4JdLVk\n> .\n", "Do you know if the nightly wheels support CUDA 8.0?\n", "They do not\n\nOn Aug 5, 2016 9:04 AM, \"Kendall Weihe\" notifications@github.com wrote:\n\n> Do you know if the nightly wheels support CUDA 8.0?\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3651#issuecomment-237891477,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHPxAcLw5eXPeZFQHgG3dk7AOty-xks5qc18ngaJpZM4JdLVk\n> .\n", "Fort verification when I add\n\n`cxx_builtin_include_directory: \"/usr/local/cuda-8.0/include\"`\nto \n`third_party/gpus/crosstool/CROSSTOOL`\n\nAm I adding it to the first line of the `toolchain {...` block\n\nI'm having [this](http://stackoverflow.com/questions/38794497/tensorflow-bazel-build-cuda-8-0-gtx-1070-fails-w-gcc-error) issue now.\n", "Our wheel files now come built for cuda 8.0 and cudnn 5.1\r\nI think this issue is now obsolete, thus closing."]}, {"number": 3650, "title": "Fix chain rule to not imply second differentiation", "body": "Fixes part of #3629.\n", "comments": ["Jenkins, test this please.\n", "@poxvoculi Can you review?\n"]}, {"number": 3649, "title": "Boolean operations on GPU are extremely slow", "body": "ArchLinux. Cuda7.5. NIghtly TF built for Python2.\n\n``` python\nimport tensorflow as tf\nimport time\n\nv = tf.get_variable('test', shape=[100, 100, 100])\nvb = tf.get_variable('test2', shape=[100, 100, 100], dtype=tf.bool,\n        initializer=tf.constant_initializer(False))\n\nb1 = tf.reduce_sum(v)\nb2 = tf.reduce_all(vb)\nb3 = tf.reduce_all(tf.cast(v, tf.bool))\n\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\nwith sess.as_default():\n    start = time.time()\n    for k in range(100):\n        sess.run(b1)\n    print time.time() - start   # 0.02s\n\n    start = time.time()\n    for k in range(100):\n        sess.run(b2)\n    print time.time() - start   # 7s!\n\n    start = time.time()\n    for k in range(100):\n        sess.run(b3)\n    print time.time() - start   # 17s!\n```\n\nCPU version of the same operation is also much faster than this.\n", "comments": ["investigating\n", "First, thanks for reporting this issue.  There is something going on that looks like a possible bug.\n\nSecond, writing good performance benchmarks that measure what you intend, and not something else, is  hard.  I'm still not completely confident I've gotten it right.  However, I'm going to suggest an alternative benchmark program, then explain why.   It's not precisely what I run on my system, and you may need to mess with the python a bit to get it to run on yours.\n\n[benchmark.txt](https://github.com/tensorflow/tensorflow/files/404387/benchmark.txt)\n\nMy CPU is a dual-socket Haswell, and my GPU is a GTX Titan X.  Here's the output I get:\n\ndev cpu size 1024 logical and: 0.002729\ndev cpu size 1024 integer add: 0.002192\ndev cpu size 1024 float32 add: 0.002473\ndev cpu size 1024 logical red: 0.004275\ndev cpu size 1024 integer red: 0.005276\ndev cpu size 1024 float32 red: 0.004385\n\ndev gpu size 1024 logical and: 0.068111\ndev gpu size 1024 integer add: 0.004291\ndev gpu size 1024 float32 add: 0.003737\ndev gpu size 1024 logical red: 1.761069     <<<< ANOMALY\ndev gpu size 1024 integer red: 0.007658\ndev gpu size 1024 float32 red: 0.006458\n\ndev cpu size 1048576 logical and: 0.018126\ndev cpu size 1048576 integer add: 0.035838\ndev cpu size 1048576 float32 add: 0.039299\ndev cpu size 1048576 logical red: 0.027802\ndev cpu size 1048576 integer red: 0.048078\ndev cpu size 1048576 float32 red: 0.053308\n\ndev gpu size 1048576 logical and: 0.007452\ndev gpu size 1048576 integer add: 0.015339\ndev gpu size 1048576 float32 add: 0.010211\ndev gpu size 1048576 logical red: 0.009259\ndev gpu size 1048576 integer red: 0.021310\ndev gpu size 1048576 float32 red: 0.011330\n\ndev cpu size 10485760 logical and: 0.089549\ndev cpu size 10485760 integer add: 0.251854\ndev cpu size 10485760 float32 add: 0.292280\ndev cpu size 10485760 logical red: 0.089407\ndev cpu size 10485760 integer red: 0.270315\ndev cpu size 10485760 float32 red: 0.283756\n\ndev gpu size 10485760 logical and: 0.073739\ndev gpu size 10485760 integer add: 0.148312\ndev gpu size 10485760 float32 add: 0.051901\ndev gpu size 10485760 logical red: 0.026299\ndev gpu size 10485760 integer red: 0.169700\ndev gpu size 10485760 float32 red: 0.053469\n\nThe main issues I found with your program are:\n 1) Beware of session.run() overhead.  Instead of looping many times over session.run(), its better to run one graph that's expensive enough to get a significant time measurement.\n 2) Beware of data copying.  If you run a single Op on a GPU, you may be mostly measuring the time it takes to copy inputs to the GPU, and maybe the result back off.  \n 3) Beware of cache effects.  For some Ops, effective choreography of data through the cache is an important aspect of efficiency, so we really need to test cold cache performance, not just loop over execution while all data stays in cache.\n\nSo my approach is to build a test graph for each Op consisting of many similar instances of the Op application, where the inputs are nearly all device-local.  We still get Op-dispatch overhead in the measured time, which is going to be relatively more significant for small tensors.\n\nMy program compares elementwise Add and And, in addition to full-tensor reductions over boolean, int32, and float32 types, for both CPU and GPU.  What I'm seeing is that all these GPU operations are a bit faster than the CPU operations on large tensors, as expected, but on small tensors they are strangely slower, with a very large performance anomaly for boolean logical reduction of a small tensor.  \n\nI'm going to open up an internal bug ticket on this issue.  \n\nI would be interested if you see something significantly different on your system, or if you can find some flaw in the design of my benchmark program.\n", "Running your benchmark, I saw something very different regarding GPU reduction:\n\n```\ndev gpu size 1024 logical and: 0.004436\ndev gpu size 1024 integer add: 0.004852\ndev gpu size 1024 float32 add: 0.006709\ndev gpu size 1024 logical red: 0.010629\ndev gpu size 1024 integer red: 0.011083\ndev gpu size 1024 float32 red: 0.007468\n\ndev gpu size 1048576 logical and: 0.012983\ndev gpu size 1048576 integer add: 0.067125\ndev gpu size 1048576 float32 add: 0.022139\ndev gpu size 1048576 logical red: 3.732179\ndev gpu size 1048576 integer red: 0.043705\ndev gpu size 1048576 float32 red: 0.019991\n\ndev gpu size 10485760 logical and: 0.063713\ndev gpu size 10485760 integer add: 0.409385\ndev gpu size 10485760 float32 add: 0.124752\ndev gpu size 10485760 logical red: 36.839622\ndev gpu size 10485760 integer red: 0.384994\ndev gpu size 10485760 float32 red: 0.129313\n```\n\nThe above was on GTX 960M . It's even worse on TitanX.\n", "@ppwwyyxx  Oh my.  That's very interesting.  Will do some more analysis on this side.\n", "Closing this as we moved to `8.0`. Feel free to open a new issue if the problem persists with recent versions.", "The problem still exists (but better than before)\r\nTesting with tensorflow whl from today's nightly build. \r\ncuda 8.0.44, ubuntu 16.04. TitanX-Pascal\r\nThe output of the snippet I posted originally:\r\n```\r\n0.0230369567871\r\n5.01169514656\r\n5.04840922356\r\n```\r\n\r\nAlso I've tried the script posted by @poxvoculi , it also shows very bad performance of logical reduce on GPU.\r\n```\r\ndev gpu size 10485760 logical and: 0.076422\r\ndev gpu size 10485760 integer add: 0.403020\r\ndev gpu size 10485760 float32 add: 0.142583\r\ndev gpu size 10485760 logical red: 59.669889\r\ndev gpu size 10485760 integer red: 0.359660\r\ndev gpu size 10485760 float32 red: 0.172168\r\n```\r\n\r\n", "@rmlarsen maybe something you're interested in?", "@drpngx Sure, I'll take a look when I get a chance.", "Any updates here?", "I have also found reduce_all to be surprisingly slow on the GPU.\r\nAlthough my benchmark is not as correct as @ppwwyyxx I think it shows that a quick and dirty kernel performs orders of magnitude faster.\r\n\r\n### log output\r\n```\r\n2017-06-23 15:58:53.100418: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-23 15:58:53.100447: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-23 15:58:53.100452: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-23 15:58:53.100457: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-23 15:58:53.100461: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-23 15:58:53.473227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: \r\nname: TITAN X (Pascal)\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.531\r\npciBusID 0000:02:00.0\r\nTotal memory: 11.90GiB\r\nFree memory: 11.76GiB\r\n2017-06-23 15:58:53.473278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 \r\n2017-06-23 15:58:53.473284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y \r\n2017-06-23 15:58:53.473296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0)\r\ndifferent\r\ntf.reduce_all /gpu:0 16.2250158787\r\ntf.reduce_all_user_defined /gpu:0 0.0130350589752\r\ntf.reduce_all /gpu:0 16.276045084\r\ntf.reduce_all_user_defined /gpu:0 0.012246131897\r\ntf.reduce_all /gpu:0 16.2119369507\r\ntf.reduce_all_user_defined /gpu:0 0.0138440132141\r\nsame\r\ntf.reduce_all /gpu:0 16.3995730877\r\ntf.reduce_all_user_defined /gpu:0 0.00978088378906\r\ntf.reduce_all /gpu:0 16.5090448856\r\ntf.reduce_all_user_defined /gpu:0 0.0134818553925\r\ntf.reduce_all /gpu:0 16.4032981396\r\ntf.reduce_all_user_defined /gpu:0 0.0142848491669\r\n```\r\n### test.py\r\n```\r\nfrom __future__ import print_function\r\n\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport timeit\r\nimport functools\r\nfrom copy import deepcopy\r\n\r\nreduce_equal_module = tf.load_op_library('./reduce_all_user_defined.so')\r\ntf.reduce_all_user_defined = reduce_equal_module.reduce_all_user_defined\r\n\r\ndef compare(obj_0, obj_1, r):\r\n    assert tf.reduce_all(\r\n        tf.equal(obj_0, obj_1)).eval() == r\r\n\r\ndef compare_fast(obj_0, obj_1, r):\r\n    assert tf.reduce_all_user_defined(\r\n        tf.equal(obj_0, obj_1)).eval() == r \r\n\r\nwith tf.Session() as sess:\r\n\r\n    for device in ['/gpu:0']:\r\n        with tf.device(device):\r\n            for eq in [False, True]:\r\n\r\n                rand_0 = np.random.uniform(low=0., high=100., size=(100,100,100,100))\r\n                rand_1 = deepcopy(rand_0)\r\n                rand_1[-1] += 1.0\r\n\r\n                ph_0 = tf.placeholder(rand_0.dtype, shape=rand_0.shape)\r\n                var_0 = tf.Variable(ph_0, trainable=False, collections=[])\r\n\r\n                ph_1 = tf.placeholder(rand_1.dtype, shape=rand_1.shape)\r\n                var_1 = tf.Variable(ph_1, trainable=False, collections=[])\r\n                \r\n                if eq:\r\n                    print ('same')\r\n                    sess.run(var_0.initializer, feed_dict={ ph_0: rand_0 })\r\n                    sess.run(var_1.initializer, feed_dict={ ph_1: rand_0 })\r\n                else:\r\n                    print ('different')\r\n                    sess.run(var_0.initializer, feed_dict={ ph_0: rand_0 })\r\n                    sess.run(var_1.initializer, feed_dict={ ph_1: rand_1 })\r\n\r\n                for i in range(3):\r\n                    t = timeit.Timer(functools.partial(compare, var_0, var_1, eq)) \r\n                    print('tf.reduce_all '+ device + ' ' +  str(t.timeit(1)))\r\n                    \r\n                    t = timeit.Timer(functools.partial(compare_fast, var_0, var_1, eq)) \r\n                    print('tf.reduce_all_user_defined '+ device + ' ' +  str(t.timeit(1)))\r\n```\r\n### reduce_all_user_defined.cc\r\n```\r\n#include \"tensorflow/core/framework/op.h\"\r\n#include \"tensorflow/core/framework/shape_inference.h\"\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n\r\nusing namespace tensorflow;\r\n\r\nREGISTER_OP(\"ReduceAllUserDefined\")\r\n    .Input(\"input: bool\")\r\n    .Output(\"output: bool\")\r\n    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\r\n      c->set_output(0,  c->Scalar());\r\n      return Status::OK();\r\n    });\r\n\r\nvoid ReduceKernelLauncher(const bool* d_in, const unsigned int n, bool* d_out);\r\n\r\nclass ReduceAllUserDefinedOpGPU : public OpKernel {\r\n public:\r\n  explicit ReduceAllUserDefinedOpGPU(OpKernelConstruction* context) : OpKernel(context) {}\r\n\r\n  void Compute(OpKernelContext* c) override {\r\n\r\n    Tensor* output = nullptr;\r\n    OP_REQUIRES_OK(c, c->allocate_output(0, TensorShape({}), &output));\r\n\r\n    auto in =  c->input(0).flat<bool>();\r\n    ReduceKernelLauncher(in.data(),\r\n                         in.size(),\r\n                         output->flat<bool>().data());\r\n  }\r\n};\r\n\r\nREGISTER_KERNEL_BUILDER(Name(\"ReduceAllUserDefined\").Device(DEVICE_GPU), ReduceAllUserDefinedOpGPU);\r\n```\r\n### reduce_all_user_defined_gpu.cu.cc\r\n```\r\n#if GOOGLE_CUDA\r\n#define EIGEN_USE_GPU\r\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\r\n\r\ntemplate<class T>\r\nstruct SharedMemory\r\n{\r\n    __device__ inline operator       T *()\r\n    {\r\n        extern __shared__ int __smem[];\r\n        return (T *)__smem;\r\n    }\r\n\r\n    __device__ inline operator const T *() const\r\n    {\r\n        extern __shared__ int __smem[];\r\n        return (T *)__smem;\r\n    }\r\n};\r\n\r\n\r\ntemplate <unsigned int blockSize>\r\n__global__ void ReduceAll(const bool* g_idata, bool* g_odata,const unsigned int n)\r\n{\r\n    bool *sdata = SharedMemory<bool>();\r\n\r\n    // perform first level of reduction,\r\n    // reading from global memory, writing to shared memory\r\n    unsigned int tid = threadIdx.x;\r\n    unsigned int i = blockIdx.x*blockSize*2 + threadIdx.x;\r\n    unsigned int gridSize = blockSize*2*gridDim.x;\r\n\r\n    bool all = true;\r\n    // we reduce multiple elements per thread.  The number is determined by the\r\n    // number of active thread blocks (via gridDim).  More blocks will result\r\n    // in a larger gridSize and therefore fewer elements per thread\r\n    while (i < n)\r\n    {\r\n        all &= g_idata[i];\r\n\r\n        // ensure we don't read out of bounds -- this is optimized away for powerOf2 sized arrays\r\n        if (i + blockSize < n)\r\n            all &= g_idata[i+blockSize];\r\n        i += gridSize;\r\n    }\r\n\r\n    // each thread puts its local sum into shared memory\r\n    sdata[tid] = all;\r\n    __syncthreads();\r\n\r\n\r\n    // do reduction in shared mem\r\n    if ((blockSize >= 512) && (tid < 256))\r\n        sdata[tid] &= sdata[tid + 256];\r\n\r\n    __syncthreads();\r\n    if ((blockSize >= 256) &&(tid < 128))\r\n        sdata[tid] &= sdata[tid + 128];\r\n\r\n     __syncthreads();\r\n\r\n    if ((blockSize >= 128) && (tid <  64))\r\n       sdata[tid] &= sdata[tid +  64];\r\n\r\n    __syncthreads();\r\n    if ((blockSize >= 64) && (tid < 32))\r\n        sdata[tid] &= sdata[tid + 32];\r\n\r\n    __syncthreads();\r\n    if ((blockSize >= 32) && (tid < 16))\r\n        sdata[tid] &= sdata[tid + 16];\r\n\r\n    __syncthreads();\r\n    if ((blockSize >= 16) && (tid <  8))\r\n        sdata[tid] &= sdata[tid +  8];\r\n\r\n    __syncthreads();\r\n    if ((blockSize >= 8) && (tid <  4))\r\n        sdata[tid] &= sdata[tid +  4];\r\n\r\n    __syncthreads();\r\n    if ((blockSize >= 4) && (tid <  2))\r\n        sdata[tid] &= sdata[tid +  2];\r\n\r\n    __syncthreads();\r\n    if ((blockSize >= 2) && ( tid <  1))\r\n        sdata[tid] &= sdata[tid +  1];\r\n\r\n    __syncthreads();\r\n    // write result for this block to global mem\r\n    if (tid == 0) g_odata[blockIdx.x] = sdata[tid];\r\n}\r\n\r\nunsigned int ilog2(int n) {\r\n    // integer log 2\r\n    unsigned int l = 0;\r\n    while (n >>= 1) ++l;\r\n    return l;\r\n}\r\n\r\nstd::tuple<int, int> chooseBlockAndThreads(const unsigned int size) {\r\n    const int maxThreads = 256;\r\n    const int maxBlocks = 64;\r\n    int blocks = min(maxBlocks, size/(maxThreads*2) + 1);\r\n    int threads = 0; // number of threads per block\r\n    if (blocks == 1) {\r\n        threads = pow(2, ilog2(size));\r\n    } else {\r\n        threads = maxThreads;\r\n    }\r\n    return std::make_tuple(blocks, threads);\r\n}\r\n\r\nvoid reduce(int blocks, int threads, const bool* d_in, bool* d_odata,const unsigned int size) {\r\n    // when there is only one warp per block, we need to allocate two warps\r\n    // worth of shared memory so that we don't index shared memory out of bounds\r\n    dim3 dimBlock(threads, 1, 1);\r\n    dim3 dimGrid(blocks, 1, 1);\r\n\r\n    int smemSize = (threads <= 32) ? 2 * threads * sizeof(bool) : threads * sizeof(bool);\r\n    switch (threads)\r\n    {\r\n        case 256:\r\n            ReduceAll<256><<< dimGrid, dimBlock, smemSize >>>(d_in, d_odata, size);\r\n            break;\r\n\r\n        case 128:\r\n            ReduceAll<128><<< dimGrid, dimBlock, smemSize >>>(d_in, d_odata, size);\r\n            break;\r\n\r\n        case 64:\r\n            ReduceAll< 64><<< dimGrid, dimBlock, smemSize >>>(d_in, d_odata, size);\r\n            break;\r\n\r\n        case 32:\r\n            ReduceAll< 32><<< dimGrid, dimBlock, smemSize >>>(d_in, d_odata, size);\r\n            break;\r\n\r\n        case 16:\r\n            ReduceAll< 16><<< dimGrid, dimBlock, smemSize >>>(d_in, d_odata, size);\r\n            break;\r\n\r\n        case  8:\r\n            ReduceAll<  8><<< dimGrid, dimBlock, smemSize >>>(d_in, d_odata, size);\r\n            break;\r\n\r\n        case  4:\r\n            ReduceAll<  4><<< dimGrid, dimBlock, smemSize >>>(d_in, d_odata, size);\r\n            break;\r\n\r\n        case  2:\r\n            ReduceAll<  2><<< dimGrid, dimBlock, smemSize >>>(d_in, d_odata, size);\r\n            break;\r\n\r\n        case  1:\r\n            ReduceAll<  1><<< dimGrid, dimBlock, smemSize >>>(d_in, d_odata, size);\r\n            break;\r\n    }\r\n}\r\n\r\nvoid ReduceKernelLauncher(const bool* d_in, const unsigned int size, bool* d_out) {\r\n\r\n    auto t = chooseBlockAndThreads(size);\r\n    int threads = std::get<0>(t);\r\n    int blocks = std::get<1>(t);\r\n\r\n\r\n    bool* d_odata = nullptr;\r\n    cudaMalloc((void **) &d_odata, blocks*sizeof(bool));\r\n    cudaMemset(d_odata, 0, blocks*sizeof(bool));\r\n\r\n    reduce(blocks, threads, d_in, d_odata, size);\r\n    \r\n    bool* h_odata = (bool*) malloc(blocks*sizeof(bool));\r\n    cudaMemcpy(h_odata, d_odata, blocks*sizeof(bool), cudaMemcpyDeviceToHost);\r\n\r\n    bool h_out = true;\r\n    for(unsigned int b = 0; b < blocks; b++) {\r\n        h_out &= h_odata[b];\r\n    }\r\n    cudaMemcpy(d_out, &h_out, sizeof(bool), cudaMemcpyHostToDevice);\r\n\r\n    free(h_odata);\r\n    cudaFree(d_odata);\r\n}\r\n\r\n#endif\r\n```\r\n### compilation\r\n```\r\nTF_INC=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_include())') \r\n\r\nnvcc -std=c++11 -c -o reduce_all_user_defined_gpu.cu.o reduce_all_user_defined_gpu.cu.cc \\\r\n-I $TF_INC -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC -D_MWAITXINTRIN_H_INCLUDED --expt-relaxed-constexpr -Wno-deprecated-gpu-targets\r\ng++ -std=c++11 -shared -o reduce_all_user_defined.so reduce_all_user_defined.cc  \\\r\nreduce_all_user_defined_gpu.cu.o -I $TF_INC -fPIC -D_GLIBCXX_USE_CXX11_ABI=0\r\n```", "reductions of any kind for bool, complex, half types are currently extremely slow, there is work on fixing this, along with other improvements.", "This is now fixed internally.  Boolean reductions will be approximately 1000x faster now.", "Looks like this is the commit? d93a55b8\r\nSomehow it wasn't merged to master.", "It had to be rolled back, will be going in again shortly.", "It's in."]}]