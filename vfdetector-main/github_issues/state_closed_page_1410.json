[{"number": 10681, "title": "Build with Bazel on Windows: missing input file '@local_config_cuda//cuda:cuda-extras", "body": "### System information\r\n- **Have I written custom code**: No\r\n- **OS Platform**: `Windows 10`\r\n- **TensorFlow installed from**: `source`\r\n- **TensorFlow version**: `v1.2.0-rc2`\r\n- **Bazel version**: `git/master`\r\n- **CUDA/cuDNN version**: `8.0` / `5.1`\r\n- **GPU model**: `Nvidia GTX 1080`\r\n- **Exact command to reproduce**: `bazel build -c opt --config=win-cuda --cpu=x64_windows_msvc --host_cpu=x64_windows_msvc --copt=-w --host_copt=-w tensorflow/tools/pip_package:build_pip_package`\r\n\r\n### Describe the problem\r\n\r\nI try to compile TensorFlow on Windows with Bazel (without GPU support at first). But i run into a error (see down below).\r\n\r\nDoes anyone have a idea how i can fix this or what the problem is?\r\n\r\n### Error log\r\n```\r\nERROR: missing input file '@local_config_cuda//cuda:cuda-extras'.\r\nERROR: C:/users/spenh/appdata/local/temp/_bazel_spen/icnq02mb/external/highwayhash/BUILD.bazel:22:1: C++ compilation of rule '@highwayhash//:arch_specific' failed: msvc_cl.bat failed: error executing command external/local_config_cc/wrapper/bin/msvc_cl.bat /DOS_WINDOWS=OS_WINDOWS /DCOMPILER_MSVC /DNOGDI /DNOMINMAX /DPRAGMA_SUPPORTED /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS ... (remaining 26 argument(s) skipped): com.google.devtools.build.lib.shell.AbnormalTerminationException: Process terminated by signal 2.\r\nERROR: C:/users/spenh/appdata/local/temp/_bazel_spen/icnq02mb/external/local_config_cuda/cuda/BUILD:142:1: @local_config_cuda//cuda:cupti_headers: missing input file '@local_config_cuda//cuda:cuda-extras'.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: C:/users/spenh/appdata/local/temp/_bazel_spen/icnq02mb/external/local_config_cuda/cuda/BUILD:142:1 1 input file(s) do not exist.\r\n```\r\n", "comments": ["Oh, sorry. I just changed from with GPU build (that didn't work) to without and missed to remove the `--config=win-cuda` build option."]}, {"number": 10680, "title": "tf.estimator generator_input_fn no padding queue?", "body": "Hello again! \r\nWhile I was examining TF source code for [10597 issue](https://github.com/tensorflow/tensorflow/issues/10597) solution, I found another bug or lack of important feature, such as `PaddingFIFOQueue` for all of `tf.estimator` input pipelines. Such option is very important for seq2seq tasks with dynamic shapes.\r\n\r\nAs a solution I found a hacky way to generate `[1, batch_size, time]` batches and squeeze them to `[batch_size, time]` in the model. Source code: [tensorflow](https://github.com/Scitator/tensorflow), [model](https://github.com/Scitator/TF-seq2seq/tree/tf_master_seq2seq), [notebook](https://gist.github.com/Scitator/d72cef607d23200074dfbbc1cbce3b55).\r\n\r\nAs you can understand, I am not very happy with such solution, that's why I am asking for help to solve it in more correct and tensorflow way. Any suggestions?", "comments": ["Can you state the problem more clearly? What would the feature be? How would it work? Why is it needed? Thanks so much!", "The problem:\r\ntypically, `generator_input_fn` expects that python generator yields numpy array of features with same size for each training example. **But** for some cases, for example, seq2seq models, each training example has different size (just different lens, really). Dynamic shapes broke up current input pipeline:\r\n1. firstly, in `_enqueue_data` function: it use first yielded example for shape specification and remember it. For dynamic shapes it is completely wrong, cause...you know, shape are dynamic. [source](https://github.com/Scitator/tensorflow/blob/a827151b7a6ac152d63e2241a3d7a1d1aaf15aff/tensorflow/python/estimator/inputs/queues/feeding_functions.py#L365)\r\n2. secondly, at feed stage, when it converts list of numpy array to matrix: if shape are equal, all works correctly, but our shapes are not, so -> exception. [source](https://github.com/Scitator/tensorflow/blob/master/tensorflow/python/client/session.py#L1075)\r\n\r\nWhy is it needed?\r\nPersonally, the main reason why it needed - give simplified input pipeline interface for seq2seq models. It can be done through `PaddingFIFOQueue` and `placeholder_with_default`, but for `tf.estimator` usage we still need some input pipeline function\r\n\r\nWhat would the feature be? How would it work?\r\nThe feature is quite simple, it could be an additional flag, or `shuffle` replacement to allow user to use  `PaddingFIFOQueue` under the hood for batch preprocessing. \r\n\r\nFor now, I clearly understand how to do it for generator pipeline, although for other ones - dont think so. As for me, only generator one can handle dynamic shapes, other one, such as numpy source or pandas daaframe already want data to be same size.", "Thank you your new description is much more clear!\r\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tfboyd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tfboyd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tfboyd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tfboyd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tfboyd: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tfboyd: It has been 343 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tfboyd: It has been 358 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 10678, "title": "Add jni methods to support creating string Tensors from string bytes", "body": "Here are native methods to support creating string Tensors by string bytes. The implement of Tensor class and test will be soon available.", "comments": ["Can one of the admins verify this patch?", "Thanks for the PR @Moriadry , but I think I'm lacking some context here.\r\nWhat's the plan for these methods? Could you elaborate on what you intend to do with these methods?", "@asimshankar I'm trying to add this feature [string support](https://github.com/tensorflow/tensorflow/issues/8531) . And here is part of job I have done.\r\n\r\nI thought the problem was when calling Tensor.create(Object o), for example,  with byte[][]\r\n\r\n`byte[][] bytes = {{\u201cx\u201d, 1}, {\u201cy\u201d, \u201cz\"}, {\u201ca\u201d, \u201cb\u201d}, {\u201cm\u201d, 2}};`\r\n\r\nwe need to allocate the string in off-heap memory, and nativeHandle returns its address.\r\n\r\nDo I learn it correctly? Is there any thing different between string and primitive? Thank you for your help :)", "I will create a new pr when all code is finished, close it now, thanks"]}, {"number": 10677, "title": " No OpKernel was registered to support Op 'Maximum' with these attrs.---iOS", "body": "Consult: excuse me, write own model file, called collapse, iOS,\r\n\r\n![snip20170613_6](https://user-images.githubusercontent.com/8908244/27081496-407ad54a-5073-11e7-8464-faab2ca4836e.png)", "comments": ["It looks like you are invoking something involving a gradient. If you are training on device, you likely need to change the makefile to add more ops to the build. Correct @petewarden?\r\n", "Thank you for your reply. I'm not quite understand what you say, and how to change the makefile to add more ops to the build?please!!!", "My point is that you appear to trying to run the training part of your graph rather than the inference part? Is this correct? The set of ops included in ios is limited to training, so you'll have to play with the makefile to add more ops to the build. See:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/README.md\r\nand search for other issues. I think your issue is a duplicate of\r\nhttps://github.com/tensorflow/tensorflow/issues/2929\r\n\r\n", "Thank you for your reply, and I have read \"READEME\"many times. I don't know how to add more ops to the build.I wants to use his own model.My app built for Mobile will crash at runtime. I provide a screenshot.On the left side of the picture is the source code,and the path is \"tensorflow/core/ops/math_ops.cc\".On the right is my crash logs. I found that \"DT_FLOAT\" did not found in the source.How to solve? \r\n![27174808-ed5e7b54-51ef-11e7-8813-a94590972702](https://user-images.githubusercontent.com/8908244/27207657-283096da-5273-11e7-9315-9750a91c8f25.png)", "Are you trying to do inference or training? If you are just trying to do inference, then you should really find out why your session is invoking ops that are called \"gradients/...\". It is far easier to figure out why that is happening than to add ops, and it probably will be important for performance.", "@aselle  Item seems @StarRain-L is trying to use his own model which worked well before and do inference on the device, your referenced issue has been closed and related missed ops has already been added in tf_op_files.txt in TF folder. By the way, from StarRain-L's screenshot, the session invoked 'gradients/..' that might be used in training process, is that why you are wondering he is doing inference or training?", "@aselle  I am doing the inference , and do you mean the session should not invoking ops  \"gradients\" when doing the inference ? how can I remove these ops ? I just use the model file saved by python .\r\n", "Can you give example code that exhibits the problem. Essentially when you call session.run() you ask for a given op and every dependent of that has to be run. You have a dependency on something called \"Gradients/...\" (that's where the error is showing up). I feel like you might be pulling on a training op or something dependent on a training op. If not, it is somewhat strange. Either way you could work to modify the build to add ops, but it is quite difficult.", "Also check out this for more info... https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2/#0 \r\n", "@aselle Attach the code , and please have a look.How can I modify these ops ?\r\n![75c8e16c-9b93-42f6-8568-cd7f92558b23](https://user-images.githubusercontent.com/8908244/27252237-ad0a161a-538b-11e7-845c-c48b91fb59b6.png)\r\n\r\n\r\n", "Can you detail how you are creating your model, how you are constructing the model (what framework), the commands and programs you used to compact the model and freeze it. It seems what is happening is that a part of the training network is brought in. If all else fails, you could try to construct an inference only network (that uses the same variables so you can load your trained checkpoint in it).\r\n", "@aselle Very grateful for your reply. I understand a little bit about, I try again!!", "@aselle The problem is resolved.My colleague reference your instructions to modify the model code, I am not very clear what he modified.And  tf_op_files.txt increased \" random_op. cc\".I provide the reference link.Finally, thank you very much for your help, and if have any problem, I hope I can continue to ask you.\r\n> https://github.com/tensorflow/tensorflow/search?l=C%2B%2B&q=TruncatedNormal&type=&utf8=\u2713\r\n\r\n> https://stackoverflow.com/questions/40855271/no-opkernel-was-registered-to-support-op-switch-with-these-attrs-on-ios/43627334", "Closing this out, as it looks like the problem was resolved."]}, {"number": 10676, "title": "DSP process time investigation", "body": "Hello,\r\n\r\nI am evaluating the DSP process time by the bellows source code.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/hexagon/hexagon_graph_execution_test.cc\r\n\r\nAnd, The result was short process time.\r\nAs one factor, I think that the reason is quantization to input data of  DSP.\r\nCould you please tell me a change procedure for quantization ?\r\nAnd, Could I please have the pb file(NOT quantizate) before the bellows quantization pb file ?\r\n\r\ntensorflow_inception_v3_stripped_optimized_quantized.pb \r\n\r\nI want to compare between DSP process time(quantizate input data) and CPU process time(NOT quantizate input data)", "comments": ["@satok16, could you please take a look?\r\n", "The input data type to HVX is floating point not quantized data type for fairness purposes.\r\nThe quantization (of the graph) is a pre-process before running the inference.\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md for more details.\r\nIn this link, you can find a download link for non-quantized model."]}, {"number": 10675, "title": "Inner tf.device inherits device index when using wildcard index", "body": "TF version: v1.2.0-rc0-735-gf48673b (about one week ago)\r\n\r\n```python\r\nwith tf.device('gpu:7'):\r\n    with tf.device('cpu:*'):\r\n        print(tf.constant(0).device) # /device:CPU:7\r\n```\r\n\r\nworkaround:\r\n\r\n```python\r\nwith tf.device('gpu:7'):\r\n    with tf.device(None), tf.device('cpu:*'):\r\n        print(tf.constant(0).device) # /device:CPU:*\r\n```", "comments": ["@mrry, could you take a look please?", "Yes, this looks like a bug. There's no distinction in the [`DeviceSpec`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/device.py#L24) class between `*` and \"not present\", which are both represented using `None` for the `device_index` property.\r\n\r\nThe resulting behavior is not very intuitive, and it would be nice to fix this, but I'm not sure how heavily `*` is used in device strings, and it's possible that fixing it could break existing users.", "@asimshankar would you decide if this is worth fixing?", "@josh11b Would it make sense to consider this fix as part of the `tf.device()` changes in 2.0?", "This is a stale issue. Please check the issue with latest TensorFlow. If the issue still persists in the newer version of TF, please feel free to reopen it by providing details about the issue and a standalone code to reproduce the issue. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=10675\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=10675\">No</a>\n"]}, {"number": 10674, "title": "Tensor flow importing error please help very important", "body": "![error1](https://user-images.githubusercontent.com/29398681/27076888-0a944dde-504c-11e7-8549-6882eda7cff9.PNG)\r\n![error2](https://user-images.githubusercontent.com/29398681/27076889-0a947b9c-504c-11e7-8c59-4de236a9d328.PNG)\r\n", "comments": ["Did you read this section? https://www.tensorflow.org/install/install_windows#common_installation_problems\r\n\r\nIt might provide some useful information.", "@uttamgoli As for the first error, have you set the **environment variable** for Python?\r\nAnd for the second, following the advice of @caisq, you may want to read up on [this](https://stackoverflow.com/questions/42011070/on-windows-running-import-tensorflow-generates-no-module-named-pywrap-tenso).", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 10673, "title": "Public head of master is failing windows CMAKE tests (?)", "body": "The following tests are failing for several of the public pull requests.  I suggest that the current head of master would also fail in the same way.\r\n\r\n```\r\nThe following tests FAILED:\r\n\t171 - C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/python/kernel_tests/sparse_ops_test.py (Failed)\r\n\t173 - C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/python/kernel_tests/sparse_reshape_op_test.py (Failed)\r\n```\r\n\r\n", "comments": ["I'll revise that.  Looking at the history of the Windows CMAKE tests, I can see that this test has been failing on and off for quite a while (didn't go back to the origin to find out who actually broke it)", "@yifeif, could you take a look?", "Flake was due to a new numpy version. Should be fixed with #10709 and @gunan re-downloaded numpy afterward."]}, {"number": 10672, "title": "Strange performance: sparse tensor matmul in kernel_test", "body": "Sorry for the previous issue \r\nAccording to the document [HERE](https://www.tensorflow.org/api_docs/python/tf/sparse_tensor_dense_matmul)\r\n\r\n> tensorflow/python/sparse_tensor_dense_matmul_op_test --benchmarks\r\nA sparse [m, k] with % nonzero values between 1% and 80%\r\nB dense [k, n]\r\n\r\nWhen I run [sparse_tensor_dense_matmul_op_test.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/sparse_tensor_dense_matmul_op_test.py) with large n,m,k\r\nI get:\r\n```\r\n% nnz \t n \t gpu \t m \t k \t dt(dense) \t dt(sparse) \t dt(sparse)/dt(dense)\r\n0.5 \t 512 \t True \t 1822 \t 4608 \t 0.0222946 \t 0.0130646 \t 0.585998\r\n0.5 \t 512 \t True \t 1821 \t 4608 \t 0.0224976 \t 0.0130761 \t 0.581222\r\n0.5 \t 512 \t True \t 1820 \t 4608 \t 0.022529 \t 0.79513 \t 35.2936\r\n0.5 \t 512 \t True \t 1819 \t 4608 \t 0.0217343 \t 0.795709 \t 36.6107\r\n```\r\nIt is strange that `dt(sparse)/dt(dense)` are very different when m=1821 -> 1820\r\nBTY, I set the iterations to 10 for time saving...\r\n```\r\ndelta_dense = _timer(sess, ops_fn, 10)\r\ndelta_sparse = _timer(sess, ops_fn, 10)\r\n```\r\n\r\nHere's some information:\r\nOS: Ubuntu 14.04\r\ntf-version: 1.1.0  installed from source\r\nGPU: NVIDIA K80, 4 kernels (only '/gpu:0' is used)\r\nCUDA: 8.0\r\ncudnn: 5.1.5\r\n\r\nYou can run sparse_tensor_dense_matmul_op_test.py with my settings\r\n```\r\n  for thresh in (0.5,):\r\n    for n in (512,):\r\n      for use_gpu in (True,):\r\n        for m in (1822, 1821, 1820, 1819):\r\n          for k in (9*512,):\r\n            sparse_tensor_dense_vs_dense_matmul_benchmark(\r\n                thresh, m, k, n, False, False, use_gpu=use_gpu)\r\n```\r\n", "comments": ["@prb12, any thoughts on this?", "@prb12 - PTAL", "Is this issue resolved? Please update here If it was not resolved already. Thanks!", "Please check with the latest version of TensorFlow. Feel free to reopen if the issues still persists.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=10672\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=10672\">No</a>\n"]}, {"number": 10671, "title": "Linux CPU smoke tests failing due to merged API change", "body": "This commit appears to break the Linux CPU tests (at least for me).  It seems that when the smoke tests passed for this commit, the Linux CPU tests didn't include the api_compatibility_test, but now they do.  However, the golden API and the real API have already diverged, so there isn't any possibility of successful passing any more.\r\n\r\n```\r\ncommit e6f58186363279496c46563e6f065ce7ea16c501\r\nAuthor: Bo Wang <david.b.wang@gmail.com>\r\nDate:   Mon Jun 5 11:41:32 2017 -0700\r\n```\r\n", "comments": ["@gunan, please take a look.", "The goldens are supposed to be updated with each API change.\r\nThe test seems to be back to healthy now:\r\nhttp://ci.tensorflow.org/job/tensorflow-master-cpu/2449/"]}, {"number": 10670, "title": "Fix minor typos in lookup_ops code samples.", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 10669, "title": "Running session using c++ api is significantly slower than using python", "body": "### System information\r\n- **OS Platform and Distribution**: Linux 13.1 (Bottle)\r\n- **TensorFlow installed from**: source\r\n- **TensorFlow version**: 1.1.0 (git version: v1.1.0-1-g10ec24a, compiler version: v1.1.0-1-g10ec24a)\r\n- **Compiler**: c++ (SUSE Linux) 4.8.1 20130909 [gcc-4_8-branch revision 202388]\r\n- **Bazel version**: 0.4.5\r\n- **Packages**: numpy (1.11.3), numpydoc (0.6.0), protobuf (3.1.0)\r\n\r\nNo docker, no virtual environment.\r\nAll tests were done using CPU only. \r\nAll optimization flags are set, no warnings are shown.\r\ntcmalloc is also used.\r\nBatching also helps to increase the performance both for c++ and python, but the gap stays the same.\r\n\r\n### Describe the problem\r\nI have written a simple benchmark based on official Deep MNIST example (https://www.tensorflow.org/get_started/mnist/pros). I create a simple convolutional net, train it on MNIST (number of training steps is small as we are interested in speed, not accuracy), freeze the graph and load it to c++. Then I do tests using python and using c++ and measure average time that it takes to run a tensorflow session. My tests show that running session in python takes ~2ms, while doing the same using c++ api is slower: ~3ms.\r\nThe code of the benchmark can be found here:\r\nhttps://github.com/July-Morning/MNIST_convnet_tensorflow\r\nIt should also be mentioned that the same tests were done for the multilayer perceptron. In that case c++ api was significantly (~7-10x times) faster than python just as was expected.", "comments": ["Are you sure you build them the same way? I.e. were you running the python from a whl created from the same compilation you did for the C++ example? If you didn't run from the same, you should make sure you had optimization enabled for your c++ build \r\nhttps://stackoverflow.com/questions/27086145/what-is-the-default-build-configuration-of-cmake\r\nIt looks like not since you didn't do cmake -DCMAKE_BUILD_TYPE=Release (or whatever is the right thing for cmake). Let me know if this helps. Thanks.", "Thank you for the quick response!\r\n\r\nWell, I tried adding -DCMAKE_BUILD_TYPE=Release, but the results are the same. \r\nFor c++ I am using tensorflow headers built by bazel with all optimizations.", "@alextp and @skye, thought you'd be interested in this. Please redirect if others are better able  handle this.\r\n", "Is this related to using different clocks in the python and C++ code?", "Any idea how to compare running times in a better way? \r\nTried measuring time in Python using timeit.default_timer(), got the same results.\r\n\r\nIn fact, the reason I created this issue is as follows: after I trained my own net (more complicated that the one in benchmark) and started to test it I got real-time performance in Python but not in C++, and the bottleneck was running tensorflow session.", "I am more worried about the C++ usage of clock, which I don't know how to\ninterpret in multicore settings. Why not use\nstd::chrono::system_clock::now() which should track real time reasonably\nwell?\n\nOn Wed, Jun 14, 2017 at 1:26 AM, July-Morning <notifications@github.com>\nwrote:\n\n> Any idea how to compare running times in a better way?\n> Tried measuring time in Python using timeit.default_timer(), got the same\n> results.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/10669#issuecomment-308359389>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxTS2sfTHa567tMFK30WYl1lKd5aPks5sD5k4gaJpZM4N4KAA>\n> .\n>\n\n\n\n-- \n - Alex\n", "@alextp I did time measurements with std::chrono::system_clock::now() and got the same results.\r\n", "@alextp Oh, no, sorry, my mistake. Thank you a lot. For the benchmark I posted this solved the issue, but my original net is still slower in C++. Further investigation needed. Will close the issue for now and reopen if necessary. Is that OK?", "Sure, thanks!\n\nOn Wed, Jun 14, 2017 at 10:14 AM, July-Morning <notifications@github.com>\nwrote:\n\n> @alextp <https://github.com/alextp> Oh, no, sorry, my mistake. Thank you\n> a lot. For the benchmark I posted this solved the issue, but my original\n> net is still slower in C++. Further investigation needed. Will close the\n> issue for now and reopen if necessary. Is that OK?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/10669#issuecomment-308497350>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxe_H1mwCPWhPQE6slx9odVCnWRV8ks5sEBThgaJpZM4N4KAA>\n> .\n>\n\n\n\n-- \n - Alex\n", "For quite a time I was busy with another project, but finally I got back to that issue again.\r\nI found out that the reason was not moving from Python to C++ but freezing the graph. Even when I load frozen graph into Python code I get my model running 2-3 times slower.\r\nSame problem was reported [here](https://github.com/tensorflow/tensorflow/issues/3216). \r\nThe question is now: is it possible to load tensorflow model to C++ without freezing the graph? Or is it possible to speed up running session on a frozen graph?\r\n\r\ncPython prof files are attached in case they would be useful: [prof.zip](https://github.com/tensorflow/tensorflow/files/1173861/prof.zip)\r\nOnly tensorflow session.run was profiled.\r\ndemo_session.prof is for restoring from the checkpoint case,\r\ndemo_graph.prof is for loading the frozen graph case.", "Ah, I see. Are you using GPU variables? Maybe the graph freezing code isn't\nplacing them correctly.\n\nOn Tue, Jul 25, 2017 at 8:30 AM, July-Morning <notifications@github.com>\nwrote:\n\n> For quite a time I was busy with another project, but finally I got back\n> to that issue again.\n> I found out that the reason was not moving from Python to C++ but freezing\n> the graph. Even when I load frozen graph into Python code I get my model\n> running 2-3 times slower.\n> Same problem was reported here\n> <https://github.com/tensorflow/tensorflow/issues/3216>.\n> The question is now: is it possible to load tensorflow model to C++\n> without freezing the graph? Or is it possible to speed up running session\n> on a frozen graph?\n>\n> cPython prof files are attached in case they would be useful: prof.zip\n> <https://github.com/tensorflow/tensorflow/files/1173861/prof.zip>\n> Only tensorflow session.run was profiled.\n> demo_session.prof is for restoring from the checkpoint case,\n> demo_graph.prof is for loading the frozen graph case.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/10669#issuecomment-317774700>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxcDvCj1rOtqaUhjhHfYy4plA4E_uks5sRgongaJpZM4N4KAA>\n> .\n>\n\n\n\n-- \n - Alex\n", "I have no GPU on my computer (and tensorflow is built in cpu-only mode).\r\nAnd I tried to remove manually all the parts mentioning GPU from graph.pb - didn't help at all.", "Is it feasible for you to not freeze your graph, then?\n\nOn Tue, Jul 25, 2017 at 8:51 AM, July-Morning <notifications@github.com>\nwrote:\n\n> I have no GPU on my computer (and tensorflow is built in cpu-only mode).\n> And I tried to remove manually all the parts mentioning GPU from graph.pb\n> - didn't help at all.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/10669#issuecomment-317779295>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxfV8vNl3macwk8XPk-uPIcnlymofks5sRg8HgaJpZM4N4KAA>\n> .\n>\n\n\n\n-- \n - Alex\n", "Adding @petewarden as he might have a better understanding than I do about frozen graphs", "I need to use my trained model in C++ project. I couldn't find a way to load it without freezing. If it is possible, could you please give me a clue how to do that?", "Graph freezing turns all parameters into constants. Historically constants have been less performant than variables (ie, stored in the graph datastructure which has additional locking). ", "Got it, but is there any way to use model I have in C++ project without 2-3x loss in performance? Maybe it is possible to load model without freezing it? Any example or workaround would be really useful, because now I have real-time when I restore a model in Python and can't achieve anything close in C++.", "How about SavedModel?", "Thank you! \r\nI will try SavedModel and report the results here.", "To expand on Yaroslav's comments, there are some potential initialization overheads due to the way we copy GraphDefs, but we don't expect performance to be noticeably slower on real models (and can actually be faster if you use memmapping).\r\n\r\nWith that said, using MNIST as a benchmark probably isn't very useful, since the amount of computation involved is very small and so initialization and other overheads will predominate. Have you looked at https://www.tensorflow.org/performance/benchmarks for some more representative models?", "I am not using MNIST benchmark anymore, it was only provided as an example here. What I am working with is a more complicated net (modified SqueezeNet on a 800x450 image), so I don't think initialization is a main issue here. And I am comparing only performance of session.run itself on a set of consequent runs in a loop. \r\nSeems that memmapping and SavedModel are the next steps for me.\r\n\r\nAgain, thank you all for incredibly helpful and quick responses!", "More questions than answers for now:\r\n\r\n1) I tried memmapping of the frozen graph with convert_graphdef_memmapped_format, but when trying to load mapped frozen graph I get parse/reading errors both in C++ (ReadBinaryProto) and Python (import_graph_def). Should I do memmapping before or after freezing the graph? Haven't tried the second option cause I have initial graph.pb in text, not binary format.\r\n\r\n2) Is there a working example of saving model into SavedModel? I found [these](https://github.com/tensorflow/serving/tree/master/tensorflow_serving/example), but it is not really clear what to do with FasterRCNN-like architecture (which is my case), when one has also bounding boxes as output and may have more that one input. Specifically I am confused with tf.saved_model.signature_constants.", "Re (2) you can use a custom signature for your model instead of the\nstandard classification / regression ones. You can make a signaturedef by\nfilling out the proto in\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/meta_graph.proto#L293\nand pass it in the signature_def_map of your savedmodel as explained in\nhttps://tensorflow.github.io/serving/serving_basic.html\n\nOn Wed, Jul 26, 2017 at 8:13 AM, July-Morning <notifications@github.com>\nwrote:\n\n> More questions than answers for now:\n>\n>    1.\n>\n>    I tried memmapping of the frozen graph with convert_graphdef_memmapped_format,\n>    but when trying to load mapped frozen graph I get parse/reading errors both\n>    in C++ (ReadBinaryProto) and Python (import_graph_def). Should I do\n>    memmapping before or after freezing the graph? Haven't tried the second\n>    option cause I have initial graph.pb in text, not binary format.\n>    2.\n>\n>    Is there a working example of saving model into SavedModel? I found\n>    these\n>    <https://github.com/tensorflow/serving/tree/master/tensorflow_serving/example>,\n>    but it is not really clear what to do with FasterRCNN-like architecture\n>    (which is my case), when one has also bounding boxes as output and may have\n>    more that one input. Specifically I am confused with\n>    tf.saved_model.signature_constants.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/10669#issuecomment-318084041>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxdcxgjGyW5jg36VImVvLPqi86Zctks5sR1ehgaJpZM4N4KAA>\n> .\n>\n\n\n\n-- \n - Alex\n", "Great, I will try! Thank you!", "Well, I managed to save my model in SavedModel format and load it back with Python, but running time is as slow as when using a frozen graph.\r\n\r\nHere is how I save my model:\r\n`   export_path = './saved_model' + str(datetime.now()) + '/'\r\n    builder = tf.saved_model.builder.SavedModelBuilder(export_path)\r\n\r\n    tensor_info_im = tf.saved_model.utils.build_tensor_info(model.image_input)\r\n    tensor_info_pr = tf.saved_model.utils.build_tensor_info(model.keep_prob)\r\n    tensor_info_bb = tf.saved_model.utils.build_tensor_info(model.det_boxes)\r\n    tensor_info_sc = tf.saved_model.utils.build_tensor_info(model.det_probs)\r\n    \r\n    prediction_signature = (\r\n        tf.saved_model.signature_def_utils.build_signature_def(\r\n            inputs={'image_input:0': tensor_info_im, 'keep_prob:0':tensor_info_pr},\r\n            outputs={'bbox/trimming/bbox:0': tensor_info_bb, 'probability/score:0': tensor_info_sc},\r\n            method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))\r\n    builder.add_meta_graph_and_variables(\r\n        sess, [tf.saved_model.tag_constants.SERVING],\r\n        signature_def_map={'predict': prediction_signature}, clear_devices = True)\r\n\r\n    builder.save()\r\n`\r\n\r\nAnd this is how I load it:\r\n` tf.saved_model.loader.load(sess, [\"serve\"], export_dir)\r\n`\r\n\r\nIs something wrong here?\r\n\r\nSomething else that I noticed: when I am running a restored tf session in a loop, first two runs are approximately as slow as with frozen graph or SavedModel. However after that it gets much (2x-3x) faster. ", "In general the first run of a tensorflow session is slower as a lot of\nonce-only computation gets performed (graph pruning, GPU set up, etc)\n\nOn Thu, Jul 27, 2017 at 7:31 AM, July-Morning <notifications@github.com>\nwrote:\n\n> Well, I managed to save my model in SavedModel format and load it back in\n> Python, but running time is as slow as while using a frozen graph.\n>\n> Here is how I save my model:\n> ` export_path = './saved_model' + str(datetime.now()) + '/'\n> builder = tf.saved_model.builder.SavedModelBuilder(export_path)\n>\n> tensor_info_im = tf.saved_model.utils.build_tensor_info(model.image_input)\n> tensor_info_pr = tf.saved_model.utils.build_tensor_info(model.keep_prob)\n> tensor_info_bb = tf.saved_model.utils.build_tensor_info(model.det_boxes)\n> tensor_info_sc = tf.saved_model.utils.build_tensor_info(model.det_probs)\n>\n> prediction_signature = (\n>     tf.saved_model.signature_def_utils.build_signature_def(\n>         inputs={'image_input:0': tensor_info_im, 'keep_prob:0':tensor_info_pr},\n>         outputs={'bbox/trimming/bbox:0': tensor_info_bb, 'probability/score:0': tensor_info_sc},\n>         method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))\n> builder.add_meta_graph_and_variables(\n>     sess, [tf.saved_model.tag_constants.SERVING],\n>     signature_def_map={'predict': prediction_signature}, clear_devices = True)\n>\n> builder.save()\n>\n> `\n>\n> And this is how I load it:\n> tf.saved_model.loader.load(sess, [\"serve\"], export_dir)\n>\n> Is something wrong here?\n>\n> Something else that I noticed: when I am running a restored tf session in\n> a loop, first two runs are approximately as slow as with frozen graph or\n> SavedModel. However after that it gets much (2x-3x) faster.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/10669#issuecomment-318378089>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxWmtdA8CkqkSFd3zC_I4AEbbCEnOks5sSJ8qgaJpZM4N4KAA>\n> .\n>\n\n\n\n-- \n - Alex\n", "Yeah, I thought about it, too, but decided to mention just in case.", "Any ideas? \r\nI have uploaded my scripts and model files to github:\r\nhttps://github.com/July-Morning/Tensorflow_model_utils\r\nI have restore_model.py running 2 times faster than load_frozen_graph.py or load_savedmodel.py for this specific model.\r\nMay be that would help somehow...", "I have also made a test on tensorflow built with GPU support (run all the scripts above on the computer with TitanX), of course everything is faster there, but the difference still exists (restoring model ~2x faster than loading frozen graph or saved model), so I have to say it does not seem to be an operation system or a specific build issue. ", "This issue is a bit confusing to follow, is the problem with C++ API, or is the issue with SavedModel?", "Oh, I see. I will try to sum up shortly how it was and here we are now.\r\n\r\n1.  I noticed that when I was loading frozen graph with C++ API tf session ran twice slower than in Python. I created a MNIST benchmark as an example, but @alextp showed that there was a stupid mistake in time measurement. So, for MNIST everything was fine (however, for my initial more complicated net it was not) and I closed the issue for a while.\r\n\r\n**From this moment on MNIST benchmark was not used, all experiments were done on another net.**\r\n\r\n2. When I came back to this problem, I noticed that I have that slowdown even if I load frozen graph with standard Python API. Thus, **the issue seemed to be not in C++ API**.\r\n\r\n**From this moment on I was using Python only!**\r\n\r\n3. @yaroslavvb advised me to try using SavedModel instead of freezing the graph, and I did (not sure if perfectly correct, but it worked) and got the same deceleration.\r\n\r\n4. I have run my scripts on another computer (with GPU) and got the same difference in time, so it does not seem to be an operation system or a specific build issue. \r\n\r\n**So now the problem is as follows:\r\nI have real-time performance when I am using restored model, but I get a 2-3x slowdown when I am trying to load either frozen graph or SavedModel. The question is: why (or what am I doing wrong) and how to get rid of this effect?**\r\nModel files and python scripts can be found here:\r\nhttps://github.com/July-Morning/Tensorflow_model_utils\r\n\r\nPlease let me know if it would be better to open another issue after all this mess.", "Anything else I can do to make things more clear? Any tests or profiling? I would be really grateful for any tiny chance to solve that issue.", "I cloned your repo and tried to run it and get\r\n\r\n```\r\n  File \"load_savedmodel.py\", line 41\r\n    im = np.random.rand(IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH) \r\n                                                               ^\r\nTabError: inconsistent use of tabs and spaces in indentation\r\n```\r\n\r\nThe best bug reports are the ones that provide a line number of where the bug is :)\r\nIf I were you, I would first try reducing to a simplest possible example (ie, single variable, single op), maybe combined with profiling the code (TF timeline/snakeviz) to see where the slowness is. If you find the source of the problem, you could use git blame to find who added this line to the code, and cc them on the bug", "Thank you for the advice. I will do my best to follow it  and will definitely fix code in the repo :)\r\n\r\nBy the way, I have attached some profiling results with snakeviz earlier. Will do it again here: \r\n[prof.zip](https://github.com/tensorflow/tensorflow/files/1201094/prof.zip)\r\n", "I have updated my repo, hope now everything is alright with the indentations.\r\nhttps://github.com/July-Morning/Tensorflow_model_utils\r\nProfiling is also done here, it shows main difference in\r\n_pywrap_tensorflow_internal.TF_Run\r\nand\r\nsession.py(_run)", "Minor update here: the slowdown seems to happen only in case of cpu version of tensorflow and in case of gpu version of tensorflow running on several GPUs. In case of tensorflow-gpu running on one GPU the times for all three cases are nearly equal.", "Update: if I do training and evaluation with the same batch size (equal to 1), the slowdown disappears.", "Finally found the reason of the problem: it was variable batch size.\r\n\r\nThank you all, I'm closing the issue!", "Hi, guys\r\nOne thing i cannot understand, am I missing the frozen_graph.pb to successfully start the program?\r\nThanks a lot for answer.", "@July-Morning  Hi, I see you finally solved the problem, the reason is variable batch size. how did you solve this in your production environment? Do you mean  the batch size of training and evaluation has to be the same? But in my situation, we often train in large batch size like 4096, but inference with small batch like 20, thus I would be definitely suffer from this problem, is this the situation here?\r\n\r\nThanks very much.", "@July-Morning @yaroslavvb  Hi guys i have a model which was trained on gpu . This model with python code takes approx 4 sec on linux cpu system , when i use the same model on the cpu i get the timing of 11-16 sec can you point out the problem y i get this much timing difference and how can i solve this \r\n\r\nThanks in advance ", "All fast programs are alike, every slow program is slow in its own way --Tolstoy", "@yaroslavvb sorry i did not get you , are you quoting Tolstoy for me of giving me suggestion on it ", "> @July-Morning @yaroslavvb Hi guys i have a model which was trained on gpu . This model with python code takes approx 4 sec on linux cpu system , when i use the same model on the cpu i get the timing of 11-16 sec can you point out the problem y i get this much timing difference and how can i solve this\r\n> \r\n> I have the same problem and have not solved;\r\n\r\n", "> Adding @petewarden as he might have a better understanding than I do about frozen graphs\r\n\r\n@petewarden @yaroslavvb \r\nWhich will have good performance or speed improvements for **Inference**(only) with C++?\r\n1. Loading a frozen model\r\n2. Loading a graph/Checkpoints\r\n\r\nOr not C++ at all?\r\nwill loading with Python be actually faster?\r\nor it depends on training parameters if any?", "> Update: if I do training and evaluation with the same batch size (equal to 1), the slowdown disappears.\r\n\r\nDo you mean that the inference slowdown in tensorflow c++ disappears after you use the model which be trained and evaluated in python with the same batch size (equal to 1)? I have the same problem with you (Running session using c++ api is significantly slower than using python) and i have tried many solutions as the following shows:\r\n1.compile tensorflow c++ shared library with opmizition flags:AVX/AVX2/SSE4.1/SSE4.2/FMA/XLA\r\n2.MKL-DNN\r\n3.replace single image inference in tensorflow c++ with batch inference [https://stackoverflow.com/questions/57460782/batch-inference-is-as-slow-as-single-image-inference-in-tensorflow-c](url)\r\nThese above all are useless to me."]}, {"number": 10668, "title": "Get strange results by running sparse_tensor_dense_matmul_op_test.py", "body": "When I run sparse_tensor_dense_matmul_op_test.py with large n,m,k\r\nI get:\r\n```\r\n% nnz \t n \t gpu \t m \t k \t dt(dense) \t dt(sparse) \t dt(sparse)/dt(dense)\r\n0.5 \t 512 \t True \t 1822 \t 4608 \t 0.0222946 \t 0.0130646 \t 0.585998\r\n0.5 \t 512 \t True \t 1821 \t 4608 \t 0.0224976 \t 0.0130761 \t 0.581222\r\n0.5 \t 512 \t True \t 1820 \t 4608 \t 0.022529 \t 0.79513 \t 35.2936\r\n0.5 \t 512 \t True \t 1819 \t 4608 \t 0.0217343 \t 0.795709 \t 36.6107\r\n```\r\nIt is strange that `dt(sparse)/dt(dense)` are very different when m=1821 -> 1820\r\nBTY, I set the iterations to 10 for time saving...\r\n```\r\ndelta_dense = _timer(sess, ops_fn, 10)\r\ndelta_sparse = _timer(sess, ops_fn, 10)\r\n```", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the **issue template** (https://github.com/tensorflow/tensorflow/issues/new) . Please provide **all the information it asks**.  Thank you.\r\n"]}, {"number": 10667, "title": "Branch 158779483", "body": "Complex merge in FFT ops, Estimator tests, LMDB reader.", "comments": ["Jenkins, test this please.", "Ah, thanks, Jenkins. You're totally on top of it.", "Jenkins, test this please.", "@rryan @ispirmustafa LMK if my merge messed with your code.", "The tensorboard-related failures in Linux CPU Test (Python 3) are unrelated and pending fix internally. Merging PR."]}, {"number": 10666, "title": "Fix typos", "body": "This PR fixes some typos: `the the`, `Classs`, `classs`, `currrently`, and `apppropriate`.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 10665, "title": "The issue of compling from source code: undeclared inclusion(s) in rule '@nccl_archive//:nccl'", "body": "Hey guys,\r\n\r\nI can compile the cpu version of tensorflow 1.2.0-rc2 without any problem, however, i am blocked when i try to compile the gpu version. I spend almost the whole day to install tensorflow 1.2.0-rc2 gpu version in our cluster, however there is no lucky.\r\n\r\n### Here is the error from the terminal: \r\n\r\nERROR: /home/hpc-xin/.cache/bazel/_bazel_hpc-xin/c5e302e32d7acb9c3e4fce8142d1cd66/external/nccl_archive/BUILD:33:1: undeclared inclusion(s) in rule '@nccl_archive//:nccl':\r\nthis rule is missing dependency declarations for the following files included by 'external/nccl_archive/src/libwrap.cu.cc':\r\n  '/gpfs/gpfs1/apps2/gcc/5.4.0/lib/gcc/x86_64-unknown-linux-gnu/5.4.0/include-fixed/limits.h'\r\n  '/gpfs/gpfs1/apps2/gcc/5.4.0/lib/gcc/x86_64-unknown-linux-gnu/5.4.0/include-fixed/syslimits.h'\r\n  '/gpfs/gpfs1/apps2/gcc/5.4.0/lib/gcc/x86_64-unknown-linux-gnu/5.4.0/include/stddef.h'\r\n  '/gpfs/gpfs1/apps2/gcc/5.4.0/include/c++/5.4.0/new'\r\n  '/gpfs/gpfs1/apps2/gcc/5.4.0/include/c++/5.4.0/x86_64-unknown-linux-gnu/bits/c++config.h'\r\n  '/gpfs/gpfs1/apps2/gcc/5.4.0/include/c++/5.4.0/x86_64-unknown-linux-gnu/bits/os_defines.h'\r\n  '/gpfs/gpfs1/apps2/gcc/5.4.0/include/c++/5.4.0/x86_64-unknown-linux-gnu/bits/cpu_defines.h'\r\n  '/gpfs/gpfs1/apps2/gcc/5.4.0/include/c++/5.4.0/exception'\r\n  '/gpfs/gpfs1/apps2/gcc/5.4.0/include/c++/5.4.0/bits/atomic_lockfree_defines.h'\r\n  '/gpfs/gpfs1/apps2/gcc/5.4.0/include/c++/5.4.0/bits/exception_ptr.h'\r\n  '/gpfs/gpfs1/apps2/gcc/5.4.0/include/c++/5.4.0/bits/exception_defines.h'\r\n  '/gpfs/gpfs1/apps2/gcc/5.4.0/include/c++/5.4.0/bits/nested_exception.h'\r\n  '/gpfs/gpfs1/apps2/gcc/5.4.0/lib/gcc/x86_64-unknown-linux-gnu/5.4.0/include/stdarg.h'\r\n  '/gpfs/gpfs1/apps2/gcc/5.4.0/include/c++/5.4.0/cmath'\r\n  '/gpfs/gpfs1/apps2/gcc/5.4.0/include/c++/5.4.0/bits/cpp_type_traits.h'\r\n  '/gpfs/gpfs1/apps2/gcc/5.4.0/include/c++/5.4.0/ext/type_traits.h'\r\n  '/gpfs/gpfs1/apps2/gcc/5.4.0/include/c++/5.4.0/cstdlib'\r\n  '/gpfs/gpfs1/apps2/gcc/5.4.0/include/c++/5.4.0/cstdio'.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 22.226s, Critical Path: 5.20s\r\n\r\n### OS Information \r\nRed Hat Enterprise Linux Workstation release 6.7 (Santiago)\r\n\r\n### Modules I loaded\r\ngcc/5.4.0, cuda/8.0.61, cudnn/6.0, java/1.8.0_31, sqlite/3.18.0, tcl/8.6.6.8606, python/3.6.1\r\n\r\n### Here is how i make the configuration\r\ngcc (v5.4.0): /apps2/gcc/5.4.0 (customized path\uff09\r\nenabled cuda (v8.0): /apps2/cuda/8.0.61 (customized path\uff09\r\nenabled cudnn: /apps2/cudnn/6.0 (customized path\uff09\r\npython (v3.6.1, which is compiled with gcc 5.4.0): /apps2/python/3.6.1 (customized path)\r\nOther options are disabled (such as hadoop, google cloud). \r\n\r\n### Here are some files i modified manually before compiling\r\n(1) /tensorflow-1.2.0-rc2/third_party/gpus/crosstool/CROSSTOOL_clang.tpl\r\n     /tensorflow-1.2.0-rc2/third_party/gpus/crosstool/CROSSTOOL_nvcc.tpl\r\n    Add the following contents in all of \"toolchain\": \r\n    cxx_builtin_include_directory: \"/apps2/gcc/5.4.0/lib/gcc/x86_64-unknown-linux-gnu/5.4.0/include\"\r\n    cxx_builtin_include_directory: \"/apps2/gcc/5.4.0/lib/gcc/x86_64-unknown-linux-gnu/5.4.0/include-fixed\"\r\n    cxx_builtin_include_directory: \"/apps2/gcc/5.4.0/include/c++/5.4.0\"\r\n    cxx_builtin_include_directory: \"/apps2/gcc/5.4.0/include\"\r\n    cxx_builtin_include_directory: \"/apps2/cuda/8.0.61/include\"\r\n    cxx_builtin_include_directory: \"/apps2/cudnn/6.0/include\"\r\n\r\n(2) /home/hpc-xin/.cache/bazel/_bazel_hpc-xin/c5e302e32d7acb9c3e4fce8142d1cd66/external/protobuf/protobuf.bzl\r\n     Add the following content in \"ctx.action\"\r\n     env=ctx.configuration.default_shell_env\r\n\r\n(3) /home/hpc-xin/.cache/bazel/_bazel_hpc-xin/c5e302e32d7acb9c3e4fce8142d1cd66/external/nccl_archive/Makefile\r\n     Some modification as below:\r\n     CUDA_HOME ?= /usr/local/cuda --> CUDA_HOME ?= /apps2/cuda/8.0.61\r\n\r\nI noticed that some of people said this issue could be bypassed by delete the nccl dependence in \"/tensorflow-1.2.0-rc2/tensorflow/contrib/BUILD\". However, i do not think this is the correct way to solve this issue. I wanna keep this dependence.\r\n\r\nThanks so much for your help. Any comments are appreciated.\r\nBest Regards\r\nXin", "comments": ["Could you try to solve it like others have just to make sure you have everything working otherwise. Then put it back. Basically, it looks like you need to change the build file used for building nccl. @gunan, do you have any ideas?\r\n", "Dear Andrew:\n\nThanks so much for your kindly reply. I think the build file used for\nbuilding nccl you mentioned should be this one:\n\"/home/hpc-xin/.cache/bazel/_bazel_hpc-xin/c5e302e32d7acb9c3e4fce8142d1cd66/external/nccl_archive/BUILD\".\n\n\nActually, i spend sometime to checked this file yesterday, however, i have\nno idea about how to change it. Could you help me figure out how to change\nit?\n\nBTW, the compiling error shows that \"this rule is missing dependency\ndeclarations for the following files included by 'external/nccl_archive/src/\nlibwrap.cu.cc':\" and all of related files are listed afterwards. However, i\ncheck all of those files one by one, and all of them are exactly existed.\n\nThanks so much for your help.\nBest regards\nXin\n\n2017-06-13 4:27 GMT-04:00 Andrew Selle <notifications@github.com>:\n\n> Could you try to solve it like others have just to make sure you have\n> everything working otherwise. Then put it back. Basically, it looks like\n> you need to change the build file used for building nccl. @gunan\n> <https://github.com/gunan>, do you have any ideas?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/10665#issuecomment-308044303>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AKirbU2uZqbxGq8lQSKSVZ_MG32hd_dgks5sDkfbgaJpZM4N37-3>\n> .\n>\n", "I too have sees this when building for CentOS. For me when I reinstalled gcc/g++ this was resolved.\r\n\r\n", "Hi Gunan,\n\nThanks so much for your reply. May i know which version of gcc was used\nwhen you compile the gpu version of tensorflow v1.2.0-rc2?\n\nActually, for my case, the cpu version of tensorflow v1.2.0-rc2 could be\ncompiled without  any problem. Do you think this issue in gpu version is\nreally caused by my gcc compiler? Is there any way that just change some\nminor parts of certain *.bzl or BUILD file rather than re-compile the whole\nsource code of gcc?\n\nThanks again for your help.\nBest regards\nXin\n\n\n2017-06-13 14:00 GMT-04:00 gunan <notifications@github.com>:\n\n> I too have sees this when building for CentOS. For me when I reinstalled\n> gcc/g++ this was resolved.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/10665#issuecomment-308197611>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AKirbbIJ9hCw8h1gxnl_myasa0u_kuCzks5sDs5KgaJpZM4N37-3>\n> .\n>\n", "I did not build with 1.2, last time I experimented with centOS it was version 1.0.\r\nHere is the gcc version on my docker container:\r\n```\r\n# gcc --version\r\ngcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)\r\n```", "Thanks gunan, we already have the module gcc v4.8.5 in our cluster, i will try it right now. \r\nHowever, i still has a question: since the cpu version of tensorflow v1.2.0-rc2 could be compiled by my current gcc v5.4.0 successfully without any problem, i think my current gcc v5.4.0 should works fine. I am really confused why the gpu version failed to be compiled (it seems that the compiling stopped before compile the source code of cuda/cudnn)? Any comments are appreciated. I will let you know if gcc v4.8.5 works fine. \r\n\r\nThanks for your help\r\nBest \r\nXin", "I am not sure, our primary environment is ubuntu so I can only speculate.\r\nIt can be missing headers, or some confusion on bazel side when you have different gcc versions installed.\r\nIt may be as simple as setting some environment variables, or there may be bugs in Bazel itself.\r\n\r\nAs I mentioned before, our primary development environment is ubuntu, and we only provide official support on ubuntu. You may be able to get more information from the community on this issue.", "Thanks gunan,\r\nI tried gcc/4.8.5 with bazel 0.4.5, however, i still get the similar error as below: \r\n\r\n> ERROR: /home/hpc-xin/.cache/bazel/_bazel_hpc-xin/c5e302e32d7acb9c3e4fce8142d1cd66/external/nccl_archive/BUILD:33:1: undeclared inclusion(s) in rule '@nccl_archive//:nccl':\r\n> this rule is missing dependency declarations for the following files included by 'external/nccl_archive/src/libwrap.cu.cc':\r\n>   '/gpfs/gpfs1/apps2/gcc/4.8.5/lib/gcc/x86_64-unknown-linux-gnu/4.8.5/include-fixed/limits.h'\r\n>   '/gpfs/gpfs1/apps2/gcc/4.8.5/lib/gcc/x86_64-unknown-linux-gnu/4.8.5/include-fixed/syslimits.h'\r\n>   '/gpfs/gpfs1/apps2/gcc/4.8.5/lib/gcc/x86_64-unknown-linux-gnu/4.8.5/include/stddef.h'\r\n>   '/gpfs/gpfs1/apps2/gcc/4.8.5/include/c++/4.8.5/new'\r\n>   '/gpfs/gpfs1/apps2/gcc/4.8.5/include/c++/4.8.5/x86_64-unknown-linux-gnu/bits/c++config.h'\r\n>   '/gpfs/gpfs1/apps2/gcc/4.8.5/include/c++/4.8.5/x86_64-unknown-linux-gnu/bits/os_defines.h'\r\n>   '/gpfs/gpfs1/apps2/gcc/4.8.5/include/c++/4.8.5/x86_64-unknown-linux-gnu/bits/cpu_defines.h'\r\n>   '/gpfs/gpfs1/apps2/gcc/4.8.5/include/c++/4.8.5/exception'\r\n>   '/gpfs/gpfs1/apps2/gcc/4.8.5/include/c++/4.8.5/bits/atomic_lockfree_defines.h'\r\n>   '/gpfs/gpfs1/apps2/gcc/4.8.5/include/c++/4.8.5/bits/exception_ptr.h'\r\n>   '/gpfs/gpfs1/apps2/gcc/4.8.5/include/c++/4.8.5/bits/exception_defines.h'\r\n>   '/gpfs/gpfs1/apps2/gcc/4.8.5/include/c++/4.8.5/bits/nested_exception.h'\r\n>   '/gpfs/gpfs1/apps2/gcc/4.8.5/lib/gcc/x86_64-unknown-linux-gnu/4.8.5/include/stdarg.h'\r\n>   '/gpfs/gpfs1/apps2/gcc/4.8.5/include/c++/4.8.5/cmath'\r\n>   '/gpfs/gpfs1/apps2/gcc/4.8.5/include/c++/4.8.5/bits/cpp_type_traits.h'\r\n>   '/gpfs/gpfs1/apps2/gcc/4.8.5/include/c++/4.8.5/ext/type_traits.h'\r\n>   '/gpfs/gpfs1/apps2/gcc/4.8.5/include/c++/4.8.5/cstdlib'\r\n>   '/gpfs/gpfs1/apps2/gcc/4.8.5/include/c++/4.8.5/cstdio'.\r\n> Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n> Use --verbose_failures to see the command lines of failed build steps.\r\n\r\n\r\nI find there is a makefile in nccl_archive folder \"/home/hpc-xin/.cache/bazel/_bazel_hpc-xin/c5e302e32d7acb9c3e4fce8142d1cd66/external/nccl_archive\". Then, i try to make the nccl_archive source code manually by simply typing \"make\". What is surprise for me is that I did the \"make\" successfully ( libwrap.o be compiled successfully as well), please check below:\r\n\r\n> hpc-xin@cn02:/.cache/bazel/_bazel_hpc-xin/c5e302e32d7acb9c3e4fce8142d1cd66/external/nccl_archive$ make\r\n> Compiling src/libwrap.cu                      > /home/hpc-xin/.cache/bazel/_bazel_hpc-xin/c5e302e32d7acb9c3e4fce8142d1cd66/external/nccl_archive/build/obj/libwrap.o\r\n> Compiling src/core.cu                         > /home/hpc-xin/.cache/bazel/_bazel_hpc-xin/c5e302e32d7acb9c3e4fce8142d1cd66/external/nccl_archive/build/obj/core.o\r\n> Compiling src/all_gather.cu                   > /home/hpc-xin/.cache/bazel/_bazel_hpc-xin/c5e302e32d7acb9c3e4fce8142d1cd66/external/nccl_archive/build/obj/all_gather.o\r\n> Compiling src/all_reduce.cu                   > /home/hpc-xin/.cache/bazel/_bazel_hpc-xin/c5e302e32d7acb9c3e4fce8142d1cd66/external/nccl_archive/build/obj/all_reduce.o\r\n> Compiling src/broadcast.cu                    > /home/hpc-xin/.cache/bazel/_bazel_hpc-xin/c5e302e32d7acb9c3e4fce8142d1cd66/external/nccl_archive/build/obj/broadcast.o\r\n> Compiling src/reduce.cu                       > /home/hpc-xin/.cache/bazel/_bazel_hpc-xin/c5e302e32d7acb9c3e4fce8142d1cd66/external/nccl_archive/build/obj/reduce.o\r\n> Compiling src/reduce_scatter.cu               > /home/hpc-xin/.cache/bazel/_bazel_hpc-xin/c5e302e32d7acb9c3e4fce8142d1cd66/external/nccl_archive/build/obj/reduce_scatter.o\r\n> Linking   libnccl.so.1.3.4                    > /home/hpc-xin/.cache/bazel/_bazel_hpc-xin/c5e302e32d7acb9c3e4fce8142d1cd66/external/nccl_archive/build/lib/libnccl.so.1.3.4\r\n> Archiving libnccl_static.a                    > /home/hpc-xin/.cache/bazel/_bazel_hpc-xin/c5e302e32d7acb9c3e4fce8142d1cd66/external/nccl_archive/build/lib/libnccl_static.a\r\n> hpc-xin@cn02:~/.cache/bazel/_bazel_hpc-xin/c5e302e32d7acb9c3e4fce8142d1cd66/external/nccl_archive$ \r\n> \r\n\r\nIt seems that gcc works for nccl. I really have no idea why nccl-archive be failed while compiling by bazel. \r\nI will spend more time for investigation on Friday afternoon. Any additional comments of this issue is really appreciated. \r\n\r\nThanks again for your help\r\nBest regards\r\nXin", "Again, neither us nor bazel does not have official support for RedHat. So we do not have extensive experience with different error modes that you can see. Not sure why this is happening, so Ill mark this as Community support to see if anyone from the community has any information on building on RedHat.\r\n\r\nYou can also look through our previous issues. there seems to be a handful of people who ran into similar issues before:\r\nhttps://github.com/tensorflow/tensorflow/issues/3841\r\nhttps://github.com/tensorflow/tensorflow/issues/3431\r\nhttps://github.com/tensorflow/tensorflow/issues/4851\r\nhttps://github.com/tensorflow/tensorflow/issues/5182\r\nhttps://github.com/bazelbuild/bazel/issues/1317\r\nhttps://github.com/tensorflow/tensorflow/issues/1157\r\n", "I had this exact problem on CentOS 7.1, with gcc 4.8.3, bazel 0.4.5, CUDA 8.0, cuDNN 5.1. CUDA and cuDNN are installed in the default path (`/usr/local/cuda`).\r\n\r\nThe error messages are as follows:\r\n```\r\nERROR: /home/user/.cache/bazel/_bazel_sankuai/a124f28b2f66957c03d4d409b9638361/external/nccl_archive/BUILD:33:1: undeclared inclusion(s) in rule '@nccl_archive//:nccl':\r\nthis rule is missing dependency declarations for the following files included by 'external/nccl_archive/src/libwrap.cu.cc':\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.3/include/limits.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.3/include/syslimits.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.3/include/stddef.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.3/include/stdarg.h'.\r\n[...]\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 38.362s, Critical Path: 28.04s\r\n```\r\n\r\nAdding\r\n```\r\ncxx_builtin_include_directory: \"/usr/lib/gcc/x86_64-redhat-linux/4.8.3/include\"\r\n```\r\nto\r\n```\r\nthird_party/gpus/crosstool/CROSSTOOL_nvcc.tpl\r\n```\r\nseems fixed the problem.\r\n\r\n@lxwgcool I noticed that the file path in your errors starts with `/gpfs/gpfs1/`, while your added path in `CROSSTOOL_nvcc.tpl` starts with `/apps2`, could this be the problem?", "@RustingSword Thank you RustingSword . \"apps2\" is auto-mounted in our cluster, therefore, in most of the time, i just use \"/apps2\" rather than \"/gpfs/gpfs1/apps2\". I will try to replace it at afternnon today and let you know if it works fine. Thanks so much for your suggestion.\r\n\r\nBest regards\r\nXin", "@aselle @gunan @RustingSword \r\nHi All, \r\nGood news: I just compiled gpu version of tensorflow 1.2.0-rc2 successfully. I did several additional things as below:\r\n1: Thanks @RustingSword , I replaced all of \"/apps2\" by \"/gpfs/gpfs1/apps2\" to avoid the potential problem of auto-mount in our parallel file system. \r\n2: For the file: tensorflow/third_party/gpus/crosstool/CROSSTOOL_nvcc.tpl, replace: \r\n   \"tool_path { name: \"cpp\" path: \"/usr/bin//cpp\" }\" to \"tool_path { name: \"cpp\" path: \"/gpfs/gpfs1/apps2/gcc/5.4.0/bin/cpp\" }\"\r\n3: Go to \"/home/hpc-xin/.cache/bazel/_bazel_hpc-xin/c5e302e32d7acb9c3e4fce8142d1cd66/external/nccl_archive\". For the file \"MakeFile\", change \"CUDA_HOME ?= /usr/local/cuda\" to \"CUDA_HOME ?= /apps2/cuda/8.0.61\", then type \u201cmake\u201d to make the source code manually.\r\n4: Back to the main folder of tensorflow and type \"bazel clean\"\r\n5: bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n6: bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n\r\nI wrote some simple testing codes to test the gpu features of tensorflow 1.2.0-rc2  and all of those codes works fine now. The corresponding compiler and software I used as below: bazel v0.5.1, gcc v5.4.0, python v3.6.1, cuda v8.0, cudnn v6.0.\r\n\r\nI am really appreciated for you guys help. Thank you so much :)\r\n\r\nBest\r\nXin", "Looks like the issue was resolved.\r\nThanks for the update!", "I have this problem  with Centos 7.2. It is a pity as it is very close to working!\r\nThe corresponding compiler and software I used as below: bazel v0.5.2, gcc v4.8.5, python v2.7x, cuda v8.0, cudnn v6.0. The problem occurs when building the python package:\r\n\r\n___[1,783 / 4,162] Compiling external/protobuf/src/google/protobuf/descriptor_database.cc\r\nERROR: /root/.cache/bazel/_bazel_root/2608380002b7e82b3d07fef59e49e485/external/nccl_archive/BUILD:33:1: undeclared inclusion(s) in rule '@nccl_archive//:nccl':\r\nthis rule is missing dependency declarations for the following files included by 'external/nccl_archive/src/libwrap.cu.cc':\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/limits.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/syslimits.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/stddef.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/stdarg.h'.\r\nIn file included from /usr/local/cuda-8.0/bin/../targets/x86_64-linux/include/host_config.h:173:0,\r\n                 from /usr/local/cuda-8.0/bin/../targets/x86_64-linux/include/cuda_runtime.h:78,\r\n                 from <command-line>:0:\r\n", "you have to run bazel with this option: **--cxxopt=\"-I/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/*.h\"**", "Doesn't help me at all. I am running on CentOS7.2\r\n\r\n`bazel build --cxxopt=\"-I/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/*.h\" -c opt --copt=-mavx --copt=-mavx2 --copt=-msse4.2 --copt=-msse4.1 --copt=-msse3 --copt=-mfma //tensorflow/tools/pip_package:build_pip_package\r\n`\r\n\r\nbut it still fails with\r\n\r\n```\r\nERROR: /autofs/cluster/pubsw/1/pubsw/.cache/bazel/_bazel_pubsw/0afd619d2e7b272bb659b23ca414b60f/external/nccl_archive/BUILD:33:1: undeclared inclusion(s) in rule '@nccl_archive//:nccl':\r\nthis rule is missing dependency declarations for the following files included by 'external/nccl_archive/src/libwrap.cu.cc':\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/limits.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/syslimits.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/stddef.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/stdarg.h'.\r\n\r\n```\r\n", "Yes, I shared the same problem with @paulraines68 and have tried both\r\n1. edit the third_party/gpus/crosstool/CROSSTOOL_nvcc.tpl (@RustingSword )\r\n2. add --cxxopt=\"-I/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/*.h\" (@sea-man )\r\n\r\nPS I am using bazel (0.5.4), gcc (4.8.5) and compiling tf r1.4.\r\nAny helps are welcome!", "I think the problem is in the default crosstool bazel defines in version 0.5.4\r\nPlease try again with bazel version 0.8", "I have tried bazel version 0.8 and all the solutions above, but the \"nccl\" problem is still not fixed.\r\n\r\nSystem information:\r\n    redhat 7.2\r\n    gcc 4.8.5\r\n    bazel 0.5.4 or 0.8.0\r\n    tensorflow 1.4.1 \r\n    cuda 8.0\r\n    cudnn 6.0", "Thank you @gunan for checking - \r\nI  also got the same error with bazel 0.8.1 and tf r1.4.", "I have the same problem, can anyone help to check \" undeclared inclusion(s) in rule '@nccl_archive//:nccl' \" issue? \r\nit will be really appreciated. :)\r\n \r\n**System information:**\r\nCentOS 7.3\r\ngcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)\r\nBuild label: 0.8.1- (@non-git)\r\ncuda 9.0.176\r\ncudnn 7.0.5\r\ntensorflow 1.4.0\r\n\r\n**Build command:**\r\n bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Error message:**\r\nINFO: Found 1 target...\r\nERROR: /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/nccl_archive/BUILD:33:1: undeclared inclusion(s) in rule '@nccl_archive//:nccl':\r\nthis rule is missing dependency declarations for the following files included by 'external/nccl_archive/src/libwrap.cu.cc':\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/limits.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/syslimits.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/stddef.h'\r\n  '/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/stdarg.h'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 6.973s, Critical Path: 4.56s\r\nFAILED: Build did NOT complete successfully"]}, {"number": 10664, "title": "Forcing symmetric quantization", "body": "", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 10663, "title": "MemoryError when freezing large model", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.2.0-rc2\r\n- **Bazel version (if compiling from source)**: 0.5.1\r\n- **CUDA/cuDNN version**: CUDA8 / CuDNN6\r\n- **GPU model and memory**: Tesla K80, 11439MiB\r\n- **Exact command to reproduce**: ```~/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph --input_graph=fcn.pbtxt --output_graph=frozen.pb --input_checkpoint=ckpt --output_nodes_names=\"Softmax\"```\r\n\r\n### Describe the problem\r\nI'm attempting to freeze a large model with the `freeze_graph` tool and I get a `MemoryError`. This seems really weird to me since the machine should have more than enough memory. I also tried running with `CUDA_VISIBLE_DEVICES=\"\"` but got the same result.\r\n\r\n```\r\nubuntu@ip-172-31-33-208:~/code/segnet$ free -g\r\n              total        used        free      shared  buff/cache   available\r\nMem:             59           1          51           0           7          58\r\nSwap:             0           0           0\r\n```\r\n\r\n``` $ ls -larth\r\ndrwxr-xr-x 2 ubuntu ubuntu 4.0K Jun  7 22:37 variables\r\n-rw-r--r-- 1 ubuntu ubuntu 513M Jun  7 22:37 saved_model.pb\r\ndrwxrwxr-x 2 ubuntu ubuntu 4.0K Jun 12 20:45 data\r\ndrwxrwxr-x 3 ubuntu ubuntu 4.0K Jun 12 21:39 ..\r\n-rw-rw-r-- 1 ubuntu ubuntu 1.1K Jun 12 23:07 load.py\r\n-rw-rw-r-- 1 ubuntu ubuntu 1.5G Jun 12 23:09 fcn.pbtxt\r\n-rw-rw-r-- 1 ubuntu ubuntu 4.8K Jun 12 23:09 ckpt.index\r\n-rw-rw-r-- 1 ubuntu ubuntu 1.6G Jun 12 23:09 ckpt.data-00000-of-00001\r\n-rw-rw-r-- 1 ubuntu ubuntu   65 Jun 12 23:09 checkpoint\r\n-rw-rw-r-- 1 ubuntu ubuntu 513M Jun 12 23:09 ckpt.meta\r\ndrwxrwxr-x 4 ubuntu ubuntu 4.0K Jun 12 23:09 .\r\n```\r\n\r\nI first converted the `saved_model.pb` to a `.pbtxt`, `fcn.pbtxt` with a checkpoint `ckpt` with this script https://gist.github.com/domluna/ed477cb5698c787f29c7d56fba381fed. I couldn't get the freeze tool to work with `saved_model.pb` which was created based on `tf.saved_model`.\r\n\r\n### Source code / logs\r\n\r\n```\r\nubuntu@ip-172-31-33-208:~/code/segnet$ ~/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph --input_graph=fcn.pbtxt --output_graph=frozen.pb --output_nodes_names=\"Softmax\"\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 255, in <module>\r\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 187, in main\r\n    FLAGS.variable_names_blacklist)\r\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 165, in freeze_graph\r\n    input_graph_def = _parse_input_graph_proto(input_graph, input_binary)\r\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 134, in _parse_input_graph_proto\r\n    text_format.Merge(f.read(), input_graph_def)\r\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf/python/google/protobuf/text_format.py\", line 476, in Merge\r\n    descriptor_pool=descriptor_pool)\r\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf/python/google/protobuf/text_format.py\", line 526, in MergeLines\r\n    return parser.MergeLines(lines, message)\r\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf/python/google/protobuf/text_format.py\", line 559, in MergeLines\r\n    self._ParseOrMerge(lines, message)\r\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf/python/google/protobuf/text_format.py\", line 574, in _ParseOrMerge\r\n    self._MergeField(tokenizer, message)\r\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf/python/google/protobuf/text_format.py\", line 675, in _MergeField\r\n    merger(tokenizer, message, field)\r\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf/python/google/protobuf/text_format.py\", line 764, in _MergeMessageField\r\n    self._MergeField(tokenizer, sub_message)\r\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf/python/google/protobuf/text_format.py\", line 675, in _MergeField\r\n    merger(tokenizer, message, field)\r\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf/python/google/protobuf/text_format.py\", line 764, in _MergeMessageField\r\n    self._MergeField(tokenizer, sub_message)\r\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf/python/google/protobuf/text_format.py\", line 675, in _MergeField\r\n    merger(tokenizer, message, field)\r\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf/python/google/protobuf/text_format.py\", line 764, in _MergeMessageField\r\n    self._MergeField(tokenizer, sub_message)\r\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf/python/google/protobuf/text_format.py\", line 675, in _MergeField\r\n    merger(tokenizer, message, field)\r\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf/python/google/protobuf/text_format.py\", line 764, in _MergeMessageField\r\n    self._MergeField(tokenizer, sub_message)\r\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf/python/google/protobuf/text_format.py\", line 662, in _MergeField\r\n    tokenizer.Consume(':')\r\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf/python/google/protobuf/text_format.py\", line 1016, in Consume\r\n    if not self.TryConsume(token):\r\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf/python/google/protobuf/text_format.py\", line 1003, in TryConsume\r\n    self.NextToken()\r\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf/python/google/protobuf/text_format.py\", line 1236, in NextToken\r\n    match = self._TOKEN.match(self._current_line, self._column)\r\nMemoryError\r\n```\r\n", "comments": ["I encounter almost the same problem as you.  My OS is mac Sierra. I saved a model as pbtxt file of which size is 800M. However, when I want to freeze it with freeze_graph.py, the python process is always killed by the system and there are no useful logs printed by the system. I debug the python program and find the crash always when google.protobuf.text_format.Merge is executed. But I don't know why.", "A shot in the dark, but perhaps use a binary graphdef proto rather than pbtxt. @petewarden, do you have any other ideas?", "@aselle I'm saving it as pbtxt rather than pb because when I use a pb I get this error:\r\n\r\n```\r\nubuntu@ip-172-31-31-102:~/code/segnet$ CUDA_VISIBLE_DEVICES=\"\" ~/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph --input_graph=graph.pb --output_graph=frozen.pb --output_nodes_names=Softmax --input_checkpoint=ckpt\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 255, in <module>\r\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 187, in main\r\n    FLAGS.variable_names_blacklist)\r\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 165, in freeze_graph\r\n    input_graph_def = _parse_input_graph_proto(input_graph, input_binary)\r\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 134, in _parse_input_graph_proto\r\n    text_format.Merge(f.read(), input_graph_def)\r\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/lib/io/file_io.py\", line 125, in read\r\n    pywrap_tensorflow.ReadFromStream(self._read_buf, length, status))\r\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/lib/io/file_io.py\", line 93, in _prepare_value\r\n    return compat.as_str_any(val)\r\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/util/compat.py\", line 106, in as_str_any\r\n    return as_str(value)\r\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/util/compat.py\", line 84, in as_text\r\n    return bytes_or_text.decode(encoding)\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xc5 in position 439: invalid continuation byte\r\n```\r\n\r\nI've tried saving with\r\n\r\n```\r\ntf.train.write_graph(sess.graph_def, '.', 'graph.pb', as_text=False)\r\n```\r\n\r\nand\r\n\r\n```\r\nwith tf.gfile.GFile('graph.pb', 'rb') as f:\r\n    f.write(sess.graph_def.SerializeToString())\r\n```\r\n\r\nboth result in the error.\r\n\r\nAny suggestions?\r\n\r\nSome more info that might be helpful, the graph has 2165 operations in it and it's a fully convolutional network using VGG16 as the base.", "Can you try with python2 and see if the binary solves your problem?\r\n\r\n@petewarden, do you have any suggestions on this. It looks like the as_text=False may have a Python 3 incompatibility. ", "If you're loading a binary .pb graph, you'll need to add `--input_binary=false` to your freeze_graph command line. I believe that's what's causing the error you see in that case, since otherwise it tries to decode a binary file as UTF-8 text.", "@petewarden thanks! Adding ```--input_binary-true``` fixes the utf-8 error and using a binary fixes the memory error.", "Glad you got it to work, closing for now."]}, {"number": 10662, "title": "TF_BINARY_URL", "body": "Would be so grateful for some guidance.\r\n\r\nPlease advise on the appropriate TF_BINARY_URL for compiling Tensor Flow on an Odroid XU4 which uses a Samsung Exynos5422 Cortex\u2122-A15 2Ghz and Cortex\u2122-A7 Octa core CPUs and a Mali-T628 MP6(OpenGL ES 3.1/2.0/1.1 and OpenCL 1.2 Full profile).\r\n\r\nPlease advise on the TF_BINARY_URL to use for:\r\nUbuntu/Linux 64-bit, CPU only, Python 2.7\r\nUbuntu/Linux 64-bit, CPU only, Python 3.5\r\n\r\nIs there a GPU version that supports the Mali-T628 MP6(OpenGL ES 3.1/2.0/1.1 and OpenCL 1.2 Full profile) on the Odroid XU4\r\n\r\nThank you!\r\n", "comments": ["OpenCL support is currently quite primitive, so there aren't any binaries yet. You'd have to compile from source, and it will likely not be super smooth to use yet. Thanks!", "See #22 ", "Thanks Andrew, appreciate it!\n\nThe key part of the question was not so much about the GPU, but how to compile a version of tensor flow NOT using a GPU on the Odroid XU, any pointers please?\n\nThanks \n\nDavid\n\n\n\n\n\nSent from my iPhone\n\n> On Jun 12, 2017, at 5:33 PM, Andrew Selle <notifications@github.com> wrote:\n> \n> See #22\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n", "Perhaps this...\r\nhttps://forum.odroid.com/viewtopic.php?f=140&t=24385\r\nAlso it is probably similar to rasbperry pi or other android builds.", "Thanks, will take a look, appreciate it!\n\nSent from my iPhone\n\n> On Jun 12, 2017, at 6:25 PM, Andrew Selle <notifications@github.com> wrote:\n> \n> Perhaps this...\n> https://forum.odroid.com/viewtopic.php?f=140&t=24385\n> Also it is probably similar to rasbperry pi or other android builds.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n"]}, {"number": 10661, "title": "tf.train.Scaffold missing global_step attribute", "body": "According to the docstring of `tf.train.Scaffold`, there is a `global_step` attribute with the following description:\r\n\r\n* `global_step`: A tensor containing the global step counter.  Picked\r\n    from and stored into the `GLOBAL_STEP` collection in the graph by default.\r\n\r\n(see https://github.com/tensorflow/tensorflow/blob/f60b6bdcb59f5538f3301207eabc30c10a9b6d46/tensorflow/python/training/monitored_session.py#L84)\r\n\r\nThe problem is that no such attribute actually exists.\r\n", "comments": ["@ispirmustafa, could you take a look?", "Any updates @ispirmustafa?", "It's a wrong documentation. I'll fix the documentation. History: Global step was part of the Scaffold when we were prototyping the first versions. We removed it while it was getting mature."]}, {"number": 10660, "title": "Upgrade to protobuf 3.3.1", "body": "(Haven't tested, letting Jenkins try it out)", "comments": ["Jenkins, test this please", "Does this have implications on our installation instructions, the pip package dependencies, or other documentation?", "It should be ok to update the build dependency without updating the pip) runtime dependency. They're used for two different purposes right now.\r\n\r\npip tests would also catch any issues.", "different purposes? So using two different protobufs in different parts of the code isn't a problem?", "Yeah, the interaction points only test wire-compatibility, which is guaranteed by protobuf. The different versions only occur when serialization happens in the Python runtime (the pip version) and it's deserialized in TF core (bazel version in pywrap_tensorflow).", "Good catch @martinwicke. OOC is there any reason why we _wouldn't_ want to bump our setup.py protobuf dependency from 3.2.0 -> 3.3.1? Purely for the sake of consistency?\r\n\r\nOur Python code probably depends on `@com_google_protobuf//:python_srcs` somehow, for things like `from google.protobuf import json_format`. There's been changes to their Python API since 3.2.0 and probably other subtleties we haven't considered.", "Let's update the pip dependency as well. I think Jonathan is right and it\nwon't be a problem. But consistency is nice.\n", "Bumped all protobuf versions throughout. pypi doesn't have 3.3.1 yet, so depended on 3.3.0 instead.", "LGTM. Thanks @jhseu.", "Also had to downgrade some other spots to 3.3.0: only source is available for 3.3.1. The prebuilt protoc binary is not available.", "Jenkins, test this please."]}, {"number": 10659, "title": "seq2seq.dynamic_rnn_decoder not working for multilayered encoder in TF 1.0", "body": "I am trying to implement **Multilayer Bidirectional Attention based seq2seq** in **TF 1.0** using `**tf.contrib.seq2seq library**`. \r\n[This is my implementation.](https://github.com/adakum/seq2seq/blob/master/seq2seq_model.py)\r\nIt works perfectly fine for single layered encoder and decoder but gives error with multi layer. \r\n\r\n`Error : \"ValueError: Shape must be rank 2 but is rank 4 for 'Decoder/dynamic_rnn_decoder/Decoder/attention_decoder/concat' (op: 'ConcatV2') with input shapes: [?,10], [2,2,?,20], [].\"`\r\n\r\nThe problem is , when running single layer, the encoder just returns **LSTM state tuple**(for 1 layer) but in multi layer it returns an **array of LSTMStateTuple(i.e. one for each layer.)** which is fed into the decoder and creating problem I guess. \r\n\r\nSo isn't `seq2seq.dynamic_rnn_decoder` made to work `MultiRNNCell  `, i.e. multilayered decoder ? \r\n\r\nEncoder part is running perfectly fine for multi layers and **returning encoder states for each layer**. \r\n\r\n**_If u want to run my implementation : \r\n1.\tclone repo\r\n2.\tswitch to tf 1.0 env\r\n3.\tpython generate_questions.py \r\n\r\nThis will just create computation graph. In case of `num_layer=1`, graph is created is successfully but fails with `num_layer>1`._**\r\n", "comments": ["@adakum All works well, you just need to specify projection correctly (use same number of layers, or something like that) for encoder and decoder.\r\nFor code, you can look [here](https://github.com/Scitator/TF-seq2seq/tree/feature/seq2symbol).", "@Scitator  Your implementation is only for single layer encoder right ? \r\nBut yeah u are right. The problem was with concatenation. Right way to concatenate is as follows:\r\n```\r\nself.encoder_state = []\r\n\r\n    for i in range(self.num_layers):\r\n        if isinstance(encoder_fw_state[i], LSTMStateTuple):\r\n            encoder_state_c = tf.concat((encoder_fw_state[i].c, encoder_bw_state[i].c), 1, name='bidirectional_concat_c')\r\n            encoder_state_h = tf.concat((encoder_fw_state[i].h, encoder_bw_state[i].h), 1, name='bidirectional_concat_h')\r\n            encoder_state = LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\r\n        elif isinstance(encoder_fw_state[i], tf.Tensor):\r\n            encoder_state = tf.concat((encoder_fw_state[i], encoder_bw_state[i]), 1, name='bidirectional_concat')\r\n        self.encoder_state.append(encoder_state)\r\n\r\n    self.encoder_state = tuple(self.encoder_state)\r\n\r\n```\r\n\r\nCan you suggest me any way to bypass manually looping ? Thanks ", "@adakum Yeap, master branch is only for simple model. But in other one, I am using MultiRnnCell and projection looks like - [link](github.com/Scitator/TF-seq2seq/blob/tf_master_seq2seq/seq2seq/rnn_encoder.py#L101)", "@Scitator The link which u provided seems to be broken", "@adakum Sorry, have released the [repo](https://github.com/Scitator/YATS2S) yesterday :)\r\nThe code:\r\n```\r\nif isinstance(encoder_fw_state, rnn.LSTMStateTuple):  # LstmCell\r\n    state_c = tf.concat(\r\n        (encoder_fw_state.c, encoder_bw_state.c), 1, name=\"bidirectional_concat_c\")\r\n    state_h = tf.concat(\r\n        (encoder_fw_state.h, encoder_bw_state.h), 1, name=\"bidirectional_concat_h\")\r\n    self.state = rnn.LSTMStateTuple(c=state_c, h=state_h)\r\nelif isinstance(encoder_fw_state, tuple) \\\r\n        and isinstance(encoder_fw_state[0], rnn.LSTMStateTuple):  # MultiLstmCell\r\n    self.state = tuple(map(\r\n        lambda fw_state, bw_state: rnn.LSTMStateTuple(\r\n            c=tf.concat((fw_state.c, bw_state.c), 1,\r\n                        name=\"bidirectional_concat_c\"),\r\n            h=tf.concat((fw_state.h, bw_state.h), 1,\r\n                        name=\"bidirectional_concat_h\")),\r\n        encoder_fw_state, encoder_bw_state))\r\nelse:\r\n    self.state = tf.concat(\r\n        (encoder_fw_state, encoder_bw_state), 1,\r\n        name=\"bidirectional_state_concat\")\r\n```\r\n[working link](https://github.com/Scitator/YATS2S/blob/versions/tf_1.2/seq2seq/rnn_encoder.py#L91)"]}, {"number": 10658, "title": "Fix incorrect references of `tf.learn` -> `tf.contrib.learn`", "body": "This fix fixes the incorrect references in docs (`linear.md`) where `tf.learn` was used (should be `tf.contrib.learn`).\r\n\r\nThis fix fixes #8718.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Unrelated failures."]}, {"number": 10657, "title": "Lazily configure TensorFlow logger", "body": "Fixes #10498", "comments": ["Can one of the admins verify this patch?", "cc @tatatodd @mrry", "More correct, I think, would be to drop automatically attaching the handler, and instead just use the `warnings` library for deprecation warnings.\r\n\r\nIt's very odd to automatically opt-in on INFO-level logging. That should usually be a user-level concern.", "Reassigning this to @dave-andersen, since he wrote the affected code (and I have no sense of what consequences this change will have).", "To be concrete, the problem is that, currently:\r\n\r\n```\r\n>>> import logging, tensorflow as tf; logging.basicConfig(); tf.logging.info('foo')\r\nINFO:tensorflow:foo\r\nINFO:tensorflow:foo\r\n```\r\n\r\nThis is not really what a library should be doing.", "@dave-andersen any thoughts on that?", "Sorry for not seeing things.  Now aware and paying attention.\r\n\r\n@taion - do you see a solution that results in two properties:\r\n (a)  Users out-of-the-box see info warnings if they configure:\r\n  `tf.logging.set_verbosity(tf.logging.INFO)\r\n\r\nand\r\n(b)  We don't get this duplicate behavior if the user manually imports logging?  (@mrry, I didn't actually write all the code in question, I was just the last fool to touch it when trying to fix an earlier manifestation of this same problem. :)", "Also cc'ing @yilei who might care about this.", "@dave-andersen \r\n\r\nWhat about this bit? https://github.com/tensorflow/tensorflow/blob/v1.3.0-rc0/tensorflow/python/platform/tf_logging.py#L51-L57\r\n\r\nIn what cases is it desirable to you to automatically log things to the console, without explicit user setup?", "@taion - I'm not quite sure how to interpret your comment. :)  Are you saying that what's in the platform for _interactive is bad and we should change it, or are you pointing it out as a case in which it's desirable to default to verbose output?\r\n\r\nIn my view, there's a tricky thing here that TensorFlow is a library for some people, and kind of more an application for others, at least, when they're doing things like testing DNNs in Jupyter, etc.", "There's a few things going on here.\r\n\r\n- Using `logging` for _usage_ warning from a library is not ideal. It's much better to use the [`warnings`](https://docs.python.org/3.6/library/warnings.html) package from the stdlib. This is what e.g. numpy does for things like dividing by zero, and it sidesteps all the (sort of bizarre) Python logging configuration. `warnings` also exposes a bunch of hooks to users to do stuff like assert that code does not throw warnings, or to throw on hitting warnings, or to only log each warning once.\r\n- Setting up a log handler for a non-root logger is sort of unidiomatic; normally, it'd be possible to do things like set up my own logging format and handlers, and it's unusual to have to e.g. reach into a library's logging config to disable what that library does\r\n\r\nIn terms of options, I see:\r\n\r\n1. Use `warnings` for usage warnings, and drop the automatic logging setup. This is probably the most correct (assuming the things you want to show are usage warnings), and e.g. matches what numpy does, but it's a lot of work, and might require rethinking log levels.\r\n2. Disable propagation for the TF logger **(this PR)**. It makes it still be more of a hassle than I'd like, if I want to customize my log message format, but it's enough for me to be reasonably satisfied. This stops the double-logging if there are user-supplied root log handlers, and otherwise keeps working as things do currently.\r\n3. Defer the log handler setup until the first time logging runs. At that point, check to see if there are any configured root log handlers. If not, set up the `tensorflow` log handler. This is a little hacky, but it's sort of a best-of-both \u2013 by deferring logging setup to first use (given that TF doesn't log anything when you import it), users get a chance to configure their own logging in an idiomatic manner, and have things otherwise \"just work\".", "@vrv @dave-andersen can we come to a resolution about this. Reject this change?", "@rmlarsen I'm not really a decision maker here, so I will defer to others.  I like 3 as I think it's better hygiene to wait until users can set things up before making decisions about the loggers.\r\n\r\n@craigcitro might have some idea of whether this impacts stuff like Jupyter notebook logging in interesting ways.", "@dave-andersen Friendly ping. Any thoughts on what we should do with this change?", "I think proposal 3 from @taion's latest comment was the ideal since it requires the least amount of work and has the fewest implications for going wrong.  @taion would you like to give this a try?", "I realize I also failed to chime in: \r\n\r\n* logging doesn't show up by default in Jupyter unless a user calls something like `logging.basicConfig(level=logging.INFO)` themselves. This is the same \"users won't see info logs unless they opt in\" that matches @taion's expectations above.\r\n\r\n* @taion you're right that \"I see INFO logs by default\" is an unusual default in the outside world. As you can probably guess, it's the common default *inside* Google.\r\n\r\nI'm with @vrv that (3) is probably the best choice. I'm generally against (1) since I find the `warnings` module pretty cumbersome. ", "I'll take a look later this week or early next week.\r\n\r\nWe actually do mostly set log level to `INFO` as well, but we want _our_ log format, not yours :p (And we definitely don't want to see all the logging twice.)", "@taion Any chance to take a look?\r\n\r\n@vrv, @dave-andersen, @craigcitro . To recap, the recommendation is:\r\n\r\nDefer the log handler setup until the first time logging runs. At that point, check to see if there are any configured root log handlers. If not, set up the tensorflow log handler. This is a little hacky, but it's sort of a best-of-both \u2013 by deferring logging setup to first use (given that TF doesn't log anything when you import it), users get a chance to configure their own logging in an idiomatic manner, and have things otherwise \"just work\".\r\n", "Sorry, I've been a bit swamped lately. I'll try to look this week.", "(fwiw, i concur about #3, though I do worry that it may be fragile in a way we haven't expected -- but the examples that come to mind are weird, such as someone configuring logging *after* tf starts loading, which is already buggy, given that it would lose logging messages).", "@taion ping", "Updated.\r\n\r\nThe explicit lock is with the double-checked locking is meh, but the lock is unavoidable and is consistent with what upstream Python logging does.\r\n\r\nThe 100% semantically correct solution would be to use the same lock as `basicConfig`, but it's not exposed, and anyway if a user is racing `logging.basicConfig` against TensorFlow's logging on multiple threads, they deserve whatever weird stuff they end up getting.", "@dave-andersen triage ping? What should happen here?", "Any update here? I have implemented (3) as discussed above.", "Jenkins, test this please.", "@sb2nov This is ready to merge assuming it passes tests."]}, {"number": 10656, "title": "Ignoring visible gpu device", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux with linux kernel version 4.11\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:1.2.0-rc2\r\n- **Bazel version (if compiling from source)**:0.5.0-1\r\n- **CUDA/cuDNN version**:8.0.61-2/6.0.21-1\r\n- **GPU model and memory**:Quadro M1200, 32Gig of RAM\r\n- **Exact command to reproduce**:running ` sess = tf.session()` inside the python interactive shell.\r\n\r\n### Describe the problem\r\nI have followed the [official document](https://www.tensorflow.org/install/install_sources) to install from source. The build process and the install process shows no issues, but when I run the command:\r\n```\r\n>>> import tensorflow as tf\r\n>>> hello = tf.constant('Hello, TensorFlow!')\r\n>>> sess = tf.Session()\r\n```\r\nIt says:\r\n```\r\n>>> sess = tf.Session()\r\n2017-06-13 00:53:55.614764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-06-13 00:53:55.614999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: \r\nname: Quadro M1200\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.148\r\npciBusID 0000:01:00.0\r\nTotal memory: 3.95GiB\r\nFree memory: 3.92GiB\r\n2017-06-13 00:53:55.615013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 \r\n2017-06-13 00:53:55.615017: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y \r\n2017-06-13 00:53:55.615022: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] Ignoring visible gpu device (device: 0, name: Quadro M1200, pci bus id: 0000:01:00.0) with Cuda compute capability 5.0. The minimum required Cuda capability is 5.2.\r\n```\r\nBut when I visit the [nvidia website](https://developer.nvidia.com/cuda-gpus), it says that the Quadro M1200 have Compute Capability 5.2.\r\n", "comments": ["TensorFlow is querying the device for what CUDA version it supports. So, maybe your driver is too old of a version. You also can get it to work by recompiling from source for compute capability 5.0.\r\n", "I know that recompiling the tensorflow with capability 5.0 will work, but I am just wondering why tensorflow is saying that my GPU has only compute capability 5.0.\r\n\r\nI am using nvidia driver 375.66", "Please run this and reply with what it returns. \r\nhttps://gist.github.com/eyalroz/71ce52fa80acdd1c3b192e43a6c1d930", "It gives me the following result:\r\n$ ./cuda.sh \r\n50\r\nDoes it means that nvidia have made a mistake on their specs page?", "This seems to say it is 5, so whatever you were looking at probably had a mistake.\r\nhttp://www.nvidia.com/object/quadro-for-mobile-workstations.html\r\nIn any case, we just trust what the nvidia api returns, so this is not a TensorFlow issue for sure. Hopefully 5.0 is sufficient for your needs. Closing for now, Thanks! I am also told that the compute capacity really has minimal performance impact, certainly 5.2 to 5.0 hasn't made a different in our testing."]}, {"number": 10655, "title": "Feature Request: Add an optional command-line argument for a prefix URL", "body": "Would like to add a command-line argument that allows TensorBoard to run at a different URL location than the root domain.  So for example:\r\n* command-line: `--base_url runhere`\r\n* URL location: `http://<address>:6006/runhere/`\r\n\r\nMotivation for request: There are locations where only minimal ports are open and it would be great to use nginx (or similar) to route TensorBoard through port 80.\r\n\r\n", "comments": ["@dandelionmane, could you take a look at this feature request. Thanks!", "I've migrated this issue to our new repo over at tensorflow/tensorboard."]}, {"number": 10654, "title": "build from sources on amazon EC2 failed,  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 10653, "title": "Note that the cuDNN version must match exactly on Windows", "body": "Fixes #10594.", "comments": ["@StevenLShafer: Is that reasonable?  Happy to change it if you have better language.", "Jenkins, test this please.", "Sorry for the slow response. I'm in China, fighting the Great Firewall of Censorship... \r\n\r\nYour proposed change looks fine. I don't see the change here: https://www.tensorflow.org/install/install_windows, but that is probably in the works. \r\n\r\nYou might consider including checking the version of cuNDD.dll as an item in your your table of \"Common installation problems\". I originally did make a post in Stack Overflow, but was directed here since I'm trying to make a (marginally) helpful suggestion and don't really have a question.\r\n\r\nThanks!"]}, {"number": 10652, "title": "Undocumented change in sharing variables from version 1.0 to 1.1", "body": "\r\n### System information\r\n- **Custom code, a minimal reproducible example provided below**:\r\n- **Linux Ubuntu 14.04**:\r\n- **TensorFlow installed from binary using pip**:\r\n- **TensorFlow version 1.0.1 and 1.1.0**:\r\n- **CUDA 8.0/cuDNN 5.1**:\r\n- **GeForce GTX 1080**:\r\n\r\n\r\n### Problem\r\nVariable scopes and sharing works different in versions 1.0.1 and 1.1.0. I try to enter a non-reusing variable scope after executing tf.get_variable_scope().reuse_variables(). This results in different values of tf.get_variable_scope().reuse inside this scope. When using version 1.0.1 it is False while when using 1.1.0 it is True. [The sharing variable guide](https://www.tensorflow.org/versions/r1.0/programmers_guide/variable_scope) states that setting reuse = False inside a reusing scope is not the desired behavior, but the guide is completely the same for both versions. Moreover, no changes in handling variable scopes are mentioned in release notes for version 1.1. \r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\nprint tf.__version__\r\n\r\nwith tf.variable_scope('foo'):\r\n    assert tf.get_variable_scope().reuse == False, tf.get_variable_scope().reuse\r\n    tf.get_variable_scope().reuse_variables()\r\n    assert tf.get_variable_scope().reuse == True, tf.get_variable_scope().reuse\r\n    with tf.variable_scope(tf.get_variable_scope(), reuse=False):\r\n       print tf.get_variable_scope().reuse\r\n```\r\nOutput:\r\n\r\n- Version 1.0.1\r\n```\r\n1.0.1\r\nFalse\r\n```\r\n- Version 1.1.0\r\n```\r\n1.1.0\r\nTrue\r\n```", "comments": ["You are absolutely right: in 1.0.1 there was a bug that allowed \"False\" to override inheritance in a variable scope. We consider this a bug because variable scopes are meant to be nested inside functions and libraries. Imagine you're calling a sub-model defined in a library and you want to reuse all of it. If that model ever called \"False\" like in 1.0.1, you'd get a very wrong behavior that'd be potentially impossible to correct without changing the library. In 1.0.1, to get the correct behavior, you had to use \"None\" instead of \"False\" -- in fact we documented that you should always use only those two, but \"False\" worked like above. In 1.1.0 we changed this to avoid problems like the one above. But if you have a clear use-case for this, let us know!"]}, {"number": 10651, "title": "Different results using tf.train.batch and tf.train.shuffle_batch", "body": "### System information\r\n- **Custom code**:\r\nYes\r\n- **OS Platform and Distribution**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\nv1.1.0-rc0-61-g1ec6ed5\r\n- **CUDA/cuDNN version**:\r\nCUDA 8 and cuDNN 5.1\r\n- **GPU model and memory**:\r\nGTX Titan X (Maxwell) with 12GB of RAM\r\n\r\nHello,\r\n\r\nI am currently working on sequential data with variable length and would like to build a dynamic graph with respect to the \"time\" of the sequence. Because tf.train.shuffle_batch does not handle dynamic padding, the only official solution is to work with tf.train.batch. To assert the problem, I am preprocessing the data before batching to get sequences with constant length in order to be able to use shuffle_batch and tf.train.batch to compare them. So, **the only difference** between the two compared pipelines is the swap between tf.train.batch and tf.train.shuffle_batch. Problem : the results drop significantly at test time using tf.train.batch instead of tf.train.shuffle_batch.\r\n\r\n![tboard](https://user-images.githubusercontent.com/13334577/27040041-1c421dfc-4f90-11e7-8fea-ded0f5edc1a3.png)\r\n\r\nMy database is self made and I believe properly shuffled initially : the database is cross subject and all occurrences of a user are shuffled using Python's random.shuffle (the size of the lists is small enough for the pseudo random to be relevant), then added to the TFRecords. \r\n\r\nQueue randomization doesn't seem to be the problem either : using standy66's workaround showed in #5147 doesn't change the outcome of the tf.train.batch pipeline. \r\n\r\nShould we expect different behaviors from these two functions (except for the random queue shuffling) or is this behavior abnormal?\r\n\r\nThanks,\r\nQuentin", "comments": ["That looks like a bug in  your code to me, as I expect the two to be different. It's hard to say without  a reproducible test case.  Please try to to simplify your  problem to something you can post so we can help you further. You might also try on StackOverflow. @vrv, any thoughts?", "I am unfortunately not familiar with this code at all.  I think @ebrevdo might know more but is on vacation right now.  I could imagine that if you are making multiple passes over the data, then shuffling will present a different order of examples on the second epoch, and so a static shuffling of the dataset may still be different.  But I agree with @aselle that there is not enough information here to really explain why it might be different, and that shuffle_batch and batch shouldn't be different except for the shuffling property.  ", "Thank you for your answers, sorry to bother you but after some thoughts, as @vrv said, the only difference I see might be the fact that by using tf.train.batch, the database stays the same through the epochs. With a small database such as mine, do you think this can affect the model quality? About a bug in my code for that example, that would seem unlikely, as the swap between batch and shuffle_batch really is the only change between the two examples. As of now I don't see a way to condensate the problem ta make it reproducible, I will look into the two functions and come back if I find an explanation other than the \"static\" database. Thanks again.", "One way to validate this might be to manually replicate your database with shuffling several times (since the dataset is small this shouldn't be too bad), and see whether tf.train.batch is still worse than tf.train.shuffle_batch on the original dataset."]}]