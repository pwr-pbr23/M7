[{"number": 46464, "title": "Query: Convert Tensorflow NMT with attention model to tfjs", "body": "I'm following the TensorFlow NMT with attention (link) guide to build a character level transliteration model. I'm able to train the model and make inference calls in an .ipynb notebook. I'm able to save training checkpoints and load them later. However, my front end application is built on electron js. Therefore, I need to convert the encoder-decoder model to tfjs. Would anybody provide some guidance to convert my model to tfjs? I'd be happy to provide source code if needed.", "comments": ["@kabyanil \r\n\r\nPlease, refer this [link](https://github.com/tensorflow/tfjs-converter/tree/master/tfjs-converter) and see if it helps you.\r\nThis issue is more suitable for TFjs repo. Please post it on TFjs repo from [here](https://github.com/tensorflow/tfjs/issues/new). Thanks!", "@ravikyram \r\n\r\nThank you for replying. the TensorFlow tutorial builds an Encoder and Decoder using subclassing. It doesn't have a single Model. My confusion is how to save this Encoder-Decoder model when it doesn't have a single Model? For instance, when we build a sequential Model, we can call methods such as model.fit(), model.predict(), model.save() etc. But what do we do when we don't have a model to call? or in other words, how do we construct a model when using subclassing for the Encoder and the Decoder separately, as shown in the tutorial?\r\n\r\nThe provided tutorial works perfectly. I'm confused about how to construct a single model from the architecture. If I'm able to construct a model, then I'll be able to save the model and call the tensorflowjs_converter script to port the model to tfjs.\r\n\r\nwill you be able to guide me on that?", "@kabyanil \r\n\r\nCan you share colab link or simple standalone code to reproduce the issue. It helps in localizing the issue. Thanks!", "@ravikyram \r\n\r\nit's the exact same code as the tensorflow official tutorial, [nmt with attention](https://www.tensorflow.org/tutorials/text/nmt_with_attention). i was only experimenting with a different dataset.\r\n\r\nusing my own dataset, i am already able to load checkpoints and make inference calls within the .ipynb script. just seeking some guidance for saving the model in SavedModel format (any format works) and then calling tensorflowjs_converter script to port the model to tfjs, as my requirement is to make inferences within an electronjs app. it's totally ok with you take the official tutorial itself as reference code.", "Related question has been resolved in tfjs repo https://github.com/tensorflow/tfjs/issues/4566 , thank you ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 46463, "title": "Can't get Tensorflow 2.0 to properly distribute training across multiple GPUs", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Redhat Linux 7.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.0\r\n- Python version: 3.7.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: Cuda 10.1/cuDNN 7.6.5\r\n- GPU model and memory: 2x Titan RTX, 24gb each\r\n\r\n\r\n\r\n**Describe the problem**\r\nI have two GPUs, both of which are visible to Tensorflow (and Tensorflow confirms this). However, when I try to train a model using a mirrored strategy it only trains a tiny bit on the second GPU. If I make the first GPU invisible with `export CUDA_VISIBLE_DEVICES=1`, then it will actually train on the second GPU. So I know it is capable of training on both. Furthermore, if I start a model training on the first GPU and then do `export CUDA_VISIBLE_DEVICES=1` and try to train another model, it will train on the second GPU. So I know that Tensorflow can find both GPUs and can train on both GPUs and can have both GPUs training at the same time, but I cannot get it to properly train a single model on both GPUs. \r\n\r\nRunning `export CUDA_VISIBLE_DEVICES=0,1` does nothing; Tensorflow will still only train a tiny bit on the second GPU. \r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nAs stated above, I first ran `export CUDA_VISIBLE_DEVICES=0,1`. I have also tried training without running this command first. The exact code I am using to build my model is below. I have a convolutional neural network called FullModel():\r\n```python\r\nstrategy = tf.distribute.MirroredStrategy()\r\nwith strategy.scope():\r\n    net = FullModel()\r\n\r\nfor epoch in range(100):\r\n    # Custom training loop\r\n```\r\nI have also tried the following:\r\n```python\r\nstrategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\r\nwith strategy.scope():\r\n    net = FullModel()\r\n```\r\n\r\n\r\n**Any other info / logs**\r\nThe output of `nvidia-smi` right after beginning training:\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.82       Driver Version: 440.82       CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  TITAN RTX           Off  | 00000000:1B:00.0 Off |                  N/A |\r\n| 52%   70C    P2   199W / 280W |  23537MiB / 24220MiB |     51%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  TITAN RTX           Off  | 00000000:68:00.0 Off |                  N/A |\r\n| 41%   47C    P8     4W / 280W |    262MiB / 24219MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0     14382      C   python                                     23525MiB |\r\n|    1      3337      G   /usr/bin/X                                    27MiB |\r\n|    1      4783      G   /usr/bin/gnome-shell                          58MiB |\r\n|    1     14382      C   python                                       163MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\nNotice that job 14382 IS doing something on GPU 1. But it is only taking up 163mb on that GPU. I thought that maybe I just could train in larger batches or something to maximize my available GPU memory but that doesn't seem to change the situation. For some reason it just isn't using all of that GPU.\r\n\r\nThe output of `nvidia-smi` after I run `export CUDA_VISIBLE_DEVICES=1` and start another job training:\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.82       Driver Version: 440.82       CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  TITAN RTX           Off  | 00000000:1B:00.0 Off |                  N/A |\r\n| 53%   71C    P2   136W / 280W |  23537MiB / 24220MiB |     43%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  TITAN RTX           Off  | 00000000:68:00.0 Off |                  N/A |\r\n| 41%   62C    P2   246W / 280W |  23499MiB / 24219MiB |     75%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0     14382      C   python                                     23525MiB |\r\n|    1      3337      G   /usr/bin/X                                    27MiB |\r\n|    1      4783      G   /usr/bin/gnome-shell                          58MiB |\r\n|    1     14382      C   python                                       163MiB |\r\n|    1     19376      C   python                                     23237MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n", "comments": ["@rileypsmith,\r\nCould you please update TensorFlow to the latest stable version v2.4 and check if you are facing the same issue? Thanks!", "@amahendrakar \r\nI do not have permission on this machine to update cuda to the proper installation which is why I have refrained from updating my tensorflow. Are there any ways to solve this problem for tensorflow 2.0? I will try to get the proper version of cuda on the machine to update my tensorflow but I may not be able to", "@rileypsmith,\r\nCUDA 10.1 and cuDNN 7.6 is compatible with TensorFlow v2.3 as well. Is it possible for you to update the TensorFlow package?\r\n\r\nAlso in order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and the dataset you are using. Thanks!", "Apologies for the delayed response. I did not notice that Tensorflow v2.3 is also compatible with CUDA 10.1 and cuDNN 7.6. That is my mistake. I will try updating Tensorflow.\r\n\r\nMy complete code to reproduce this issue is incredibly lengthy and I cannot post all of it here. Below is my model and training loop (my real model is more complex and spans several files but is similar in principle to that which is shown below and the issue is robust to changes in model anyways).\r\n\r\nModel:\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import Model\r\nfrom tensorflow.keras.layers import Conv2D, UpSampling2D, ReLU\r\n\r\nclass FullModel(Model):\r\n\r\n    def __init__(self, use_bias=True, **kwargs):\r\n        super(AutoEncoder, self).__init__(**kwargs)\r\n\r\n        self.conv1 = Conv2D(128, kernel_size=(3, 3), strides=(2, 2), padding='same', use_bias=use_bias)\r\n        self.conv2 = Conv2D(128, kernel_size=(3, 3), strides=(2, 2), padding='same', use_bias=use_bias)\r\n        self.conv3 = Conv2D(128, kernel_size=(3, 3), strides=(2, 2), padding='same', use_bias=use_bias)\r\n        self.conv4 = Conv2D(128, kernel_size=(3, 3), strides=1, padding='same', use_bias=use_bias)\r\n        self.conv5 = Conv2D(128, kernel_size=(3, 3), strides=1, padding='same', use_bias=use_bias)\r\n        self.conv6 = Conv2D(3, kernel_size=(3, 3), strides=1, padding='same', use_bias=use_bias)\r\n\r\n        self.upsample = UpSampling2D()\r\n        self.relu = ReLU()\r\n\r\n    def call(self, x):\r\n        x = self.conv1(x)\r\n        x = self.relu(x)\r\n\r\n        x = self.conv2(x)\r\n        x = self.relu(x)\r\n\r\n        x = self.conv3(x)\r\n        x = self.relu(x)\r\n\r\n        x = self.upsample(x)\r\n        x = self.conv4(x)\r\n        x = self.relu(x)\r\n\r\n        x = self.upsample(x)\r\n        x = self.conv5(x)\r\n        x = self.relu(x)\r\n\r\n        x = self.upsample(x)\r\n        x = self.conv6(x)\r\n\r\n        return x\r\n```\r\n\r\nTraining:\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom tensorflow.keras.utils import Progbar\r\n\r\nfrom models import FullModel\r\n\r\ndef train_step(net, opt, batch):\r\n    # Split the batch into current and previous frames\r\n    curr, prev = split_batch(batch)\r\n\r\n    # Start tracking gradient\r\n    with tf.GradientTape() as tape:\r\n        # Put batch through network\r\n        pred = net(curr, prev)\r\n        # Compute loss as MSE\r\n        loss = tf.math.reduce_mean(tf.math.square(pred - curr))\r\n        # Setup gradient\r\n        gradient = tape.gradient(loss, net.trainable_variables)\r\n        # Apply gradients\r\n        opt.apply_gradients(zip(gradient, net.trainable_variables))\r\n\r\n    # Return loss so it can be tracked during training\r\n    return pred, loss\r\n\r\ndef train(epochs=100, steps_per_epoch_train=0, train_data=None, batch_size=16):\r\n    \"\"\"\r\n    Function to run through the dataset for the desired number of epochs\r\n    \"\"\"\r\n    assert train_data\r\n\r\n    # Build model\r\n    strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\r\n    with strategy.scope():\r\n        net = FullModel()\r\n\r\n    # Build optimizer\r\n    opt = Adam(learning_rate=3e-5)\r\n\r\n    for epoch in range(epochs):\r\n        print(f'Epoch {epoch+1}/{epochs}')\r\n\r\n        # Setup progress bar\r\n        pb = Progbar(steps_per_epoch_train)\r\n\r\n        # Set summary statistics\r\n        train_loss = 0\r\n        num_batches = 0\r\n\r\n        # Loop through training set\r\n        for i, batch in train_data.enumerate():\r\n            if i > steps_per_epoch_train:\r\n                break\r\n            # Do one step of SGD for each batch\r\n            loss = train_step(net, opt, batch)\r\n            # Update summary statistics\r\n            train_loss += loss\r\n            num_batches += 1\r\n            # Setup train loss for monitoring during training\r\n            values = [\r\n                ('Train Loss', train_loss / num_batches)\r\n            ]\r\n            pb.add(1, values=values)\r\n```\r\nVery simple. My dataset is just consecutive frames from a video. I split a video up into frames, reshape them, and then pass them as a batch. So each batch is a tensor of shape `(batch_size, 2, 256, 256, 3)`. `batch_size` is the number of different videos, size 2 along axis 1 is because I have the previous and current frame for each video, and then `256, 256, 3` is just the reshaped rgb image of each frame.\r\n\r\nHowever, this issue is robust to changes in dataset, model, and training. I have been able to observe the same issue when using the TensorFlow functional API to build a simple model and train with `.fit()`  on a regular dataset containing examples and labels.\r\n\r\n", "I updated Tensorflow to version 2.3 as suggested and this solved my issue! Thank you for your help. I do believe the issue I was experiencing in version 2.0 was a bug of sorts because there were no issues in my setup but I am closing this issue since I am now able to train on multiple GPUs in tversion 2.3.\r\n\r\nAlso, just for reference because this tripped me up for a second: I am running out of a conda virtual environment but I had to install tensorflow using `pip install tensorflow=2.3`. The current conda installation of tensorflow-gpu is version 2.2 for Linux.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46463\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46463\">No</a>\n"]}, {"number": 46461, "title": "Doc : keras.utils.plot_model prints the shape as (None, n) but outdated (?, n) is given in doc  ", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/guide/keras/functional\r\n\r\n## Description of issue (what needs changing):\r\n\r\nkeras.utils.plot_model prints the shape as (None, n) but given in doc as (?, n)\r\n\r\n### Clear description\r\n\r\nWatch the line `keras.utils.plot_model(model, \"my_first_model_with_shape_info.png\", show_shapes=True)` in the doc link provided above.The output shown there is outdated\r\n\r\n### Correct links\r\n\r\n[Gist](https://colab.research.google.com/drive/19dlb-Qcoo2EBDigpZ8E75cb63woYI_EI?usp=sharing) here to show the changes.\r\nTensorflow version: 2.4\r\n\r\n### Submit a pull request?\r\n Yes\r\n\r\n", "comments": ["A PR has to be merged with keras team but it was mentioned in the tensorflow documentation.so i opened it here. ", "Hello! I am new to the open source community and would like to contribute to this issue. ", "This is now fixed: https://www.tensorflow.org/guide/keras/functional"]}, {"number": 46460, "title": "tensorflow/core/kernels/sparse_matmul_op_test segfaults due to wrong alignment", "body": "Using TensorFlow 2.4.0 on an x86 system with GCC 8.3.0\r\n\r\n**Describe the current behavior**\r\n\r\nThe test code has an over-aligned class: https://github.com/tensorflow/tensorflow/blob/f18495306f26fa2e1d3351c03838cde265eb6690/tensorflow/core/kernels/sparse_matmul_op_test.cc#L265-L327\r\n\r\nAs this is used as a test fixture in https://github.com/tensorflow/tensorflow/blob/f18495306f26fa2e1d3351c03838cde265eb6690/tensorflow/core/kernels/sparse_matmul_op_test.cc#L329 this class is allocated via `new` by GoogleTest\r\n\r\nIn C++14 this is not supported, in C++17 it is. See the GCC manual for e.g.:\r\n```\r\n'-Waligned-new'\r\n     Warn about a new-expression of a type that requires greater\r\n     alignment than the 'alignof(std::max_align_t)' but uses an\r\n     allocation function without an explicit alignment parameter.  This\r\n     option is enabled by '-Wall'.\r\n```\r\n\r\nAlso compare the results at https://gcc.godbolt.org/z/PnnceM\r\n\r\nAs the result the class may not be aligned correctly and create a segfault when running https://github.com/tensorflow/tensorflow/blob/f18495306f26fa2e1d3351c03838cde265eb6690/tensorflow/core/kernels/sparse_matmul_op_test.cc#L330 (which GCC vectorizes assuming the alignment)\r\n\r\nI have found that the alignment of the class varies based on even the folder where the binary is run from. So reproducing requires a bit of experimenting but the above descriptions should make it clear that this is fragile.\r\n\r\n**Other info / logs**\r\n\r\nI suggest to enable the `-Waligned-new` as an error to catch such mistakes in the future.\r\nA workaround here is to enable the GCC flag `-faligned-new` which provides the C++17 `new` functions. Alternatively moving those fields out of the class will also work.", "comments": ["Nice catch!  We can fix this in the general case by using an Eigen macro to provide a custom aligned `new` operator.  Fix should propagate soon.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46460\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46460\">No</a>\n"]}, {"number": 46459, "title": "Sparse IndexedSlices warning due to tf.gather() and LossScaleOptimizer", "body": "**System information**\r\n\r\n    Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n    OS Platform and Distribution: Ubuntu 18.04.5 LTS\r\n    TensorFlow installed from: binary\r\n    TensorFlow version: 2.2.0\r\n    Python version: 3.6.9\r\n    CUDA/cuDNN version: 10.1.243 / 7.6.5\r\n    GPU model and memory: NVidia Tesla V100-SXM2-32GB\r\n\r\n**Describe the current behavior**\r\nUse `tf.gather()` with `LossScalerOptimizer` triggers this warning:\r\n\r\n`/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.`\r\n\r\nA fix (see colab notebook) using `tf.gather_nd()` removes the warning but breaks `saved_model_cli`.\r\n\r\n**Describe the expected behavior**\r\nThe warning is unexpected (why does it appear only when using `LossScaleOptimizer` ?)  \r\nThe fix is not expected to trigger a `saved_model_cli` error\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1vNX8ssp3nUTY_YOOml-ZfAa5yzPVHkl_?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\nPossibly related issues:\r\n- https://github.com/tensorflow/tensorflow/issues/38168\r\n- https://github.com/tensorflow/tensorflow/issues/36948\r\n", "comments": ["@thierryherrmann \r\nI ran the code shared on tf 2.2 and tf 2.4 and the mentioned issue does not occur on tf 2.4 please refer to the [gist here](https://colab.research.google.com/gist/Saduf2019/57f620adc81485c996c8b0eda7bf338b/untitled500.ipynb) and let us know.", "@Saduf2019  Ok I've just looked at the gist which is a copy of the notebook I wrote but with contents duplicated for TF2.4.\r\nIndeed the warning doesn't appear with TF2.4.\r\nAs mentioned in the **System Information**, I reported the issue against TF2.2 and I had not checked for any later version.\r\nWhile I was at it I also just checked for TF2.3 and the issue doesn't occur for TF2.3 as well.\r\nCan we hope for any fix for TF2.2 ?\r\nThanks", "@thierryherrmann, Sorry for late response.\r\n\r\nWe see that you're using old version of TF2.2 . Its unlikely for TF2.2 version to receive any bug fixes except when we have security patches.  \r\n\r\nAs per [this comment](https://github.com/tensorflow/tensorflow/issues/46459#issuecomment-762335517), your issue was resolved in TF2.3 and TF2.4. Please feel free to move this to closed status. Thanks!", "@chunduriv  yeah we no longer expected a response and switched to TF2.3 a long time ago. Thanks anyway.", "@thierryherrmann, Please feel free to move this to closed status. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46459\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46459\">No</a>\n"]}, {"number": 46458, "title": "TypeError: An op outside of the function building code is being passed", "body": "Following the [documentation](https://www.tensorflow.org/tutorials/reinforcement_learning/actor_critic) I re-arranged functions into a class, I changed the model and made a few other modifications, however the code fails to work if `tf.function` is enabled, otherwise it works perfectly fine. By commenting out line 97, the error is gone:\r\n\r\n    self.model.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\r\n\r\n**Error**\r\n\r\n    Traceback (most recent call last):\r\n      File \"/Users/emadboctor/Desktop/code/drl-algos/a2c.py\", line 109, in <module>\r\n        agn.fit()\r\n      File \"/Users/emadboctor/Desktop/code/drl-algos/a2c.py\", line 103, in fit\r\n        self.train_step()\r\n      File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\r\n        result = self._call(*args, **kwds)\r\n      File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 888, in _call\r\n        return self._stateless_fn(*args, **kwds)\r\n      File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2942, in __call__\r\n        return graph_function._call_flat(\r\n      File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1918, in _call_flat\r\n        return self._build_call_outputs(self._inference_function.call(\r\n      File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 555, in call\r\n        outputs = execute.execute(\r\n      File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 75, in quick_execute\r\n        raise e\r\n      File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\r\n        tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n    TypeError: An op outside of the function building code is being passed\r\n    a \"Graph\" tensor. It is possible to have Graph tensors\r\n    leak out of the function building context by including a\r\n    tf.init_scope in your function building code.\r\n    For example, the following function will fail:\r\n      @tf.function\r\n      def has_init_scope():\r\n        my_constant = tf.constant(1.)\r\n        with tf.init_scope():\r\n          added = my_constant * 2\r\n    The graph tensor has name: while:4\r\n\r\n**Code**\r\n\r\n    import gym\r\n    import numpy as np\r\n    import tensorflow as tf\r\n    from tensorflow.keras.layers import Conv2D, Dense, Flatten, Input\r\n    from tensorflow.keras.losses import Huber\r\n    from tensorflow.keras.models import Model\r\n    from tensorflow.keras.optimizers import Adam\r\n\r\n\r\n    class A2C:\r\n        def __init__(self, env, gamma=0.99, fc_units=512):\r\n            self.env = env\r\n            self.available_actions = env.action_space.n\r\n            self.model = self.create_model(fc_units)\r\n            self.state = tf.cast(self.env.reset(), tf.float32)\r\n            self.gamma = gamma\r\n            self.division_eps = np.finfo(np.float32).eps.item()\r\n            self.loss = Huber(reduction=tf.keras.losses.Reduction.SUM)\r\n    \r\n        def create_model(self, fc_units):\r\n            x0 = Input(self.env.observation_space.shape)\r\n            x = Conv2D(32, 8, 4, activation='relu')(x0)\r\n            x = Conv2D(64, 4, 2, activation='relu')(x)\r\n            x = Conv2D(32, 3, 1, activation='relu')(x)\r\n            x = Flatten()(x)\r\n            x = Dense(fc_units, activation='relu')(x)\r\n            actor = Dense(self.available_actions)(x)\r\n            critic = Dense(1)(actor)\r\n            model = Model(x0, [actor, critic])\r\n            model.call = tf.function(model.call)\r\n            return model\r\n    \r\n        def env_step(self, action):\r\n            state, reward, done, _ = self.env.step(action)\r\n            return (\r\n                state.astype(np.float32),\r\n                np.array(reward, np.int32),\r\n                np.array(done, np.int32),\r\n            )\r\n    \r\n        def tf_env_step(self, action):\r\n            return tf.numpy_function(\r\n                self.env_step, [action], [tf.float32, tf.int32, tf.int32]\r\n            )\r\n    \r\n        def get_returns(self, rewards, standardize=True):\r\n            n = tf.shape(rewards)[0]\r\n            returns = tf.TensorArray(dtype=tf.float32, size=n)\r\n            rewards = tf.cast(rewards[::-1], dtype=tf.float32)\r\n            discounted_sum = tf.constant(0.0)\r\n            discounted_sum_shape = discounted_sum.shape\r\n            for i in tf.range(n):\r\n                reward = rewards[i]\r\n                discounted_sum = reward + self.gamma * discounted_sum\r\n                discounted_sum.set_shape(discounted_sum_shape)\r\n                returns = returns.write(i, discounted_sum)\r\n            returns = returns.stack()[::-1]\r\n            if standardize:\r\n                returns = (returns - tf.math.reduce_mean(returns)) / (\r\n                    tf.math.reduce_std(returns) + self.division_eps\r\n                )\r\n            return returns\r\n    \r\n        def play_episode(self, max_steps=10000):\r\n            action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\r\n            values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\r\n            rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\r\n            initial_shape = self.state.shape\r\n            for i in tf.range(max_steps):\r\n                actor_out, value = self.model(tf.expand_dims(self.state, 0))\r\n                action = tf.random.categorical(actor_out, 1)[0, 0]\r\n                action_prob = tf.nn.softmax(actor_out)\r\n                self.state, reward, done = self.tf_env_step(action)\r\n                self.state.set_shape(initial_shape)\r\n                action_probs = action_probs.write(i, action_prob[0, action])\r\n                values = values.write(i, tf.squeeze(value))\r\n                rewards = rewards.write(i, reward)\r\n                if tf.cast(done, tf.bool):\r\n                    self.state = tf.cast(self.env.reset(), tf.float32)\r\n                    break\r\n            return [item.stack() for item in [action_probs, values, rewards]]\r\n    \r\n        def compute_loss(self, returns, values, action_probs):\r\n            advantage = returns - values\r\n            action_log_probs = tf.math.log(action_probs)\r\n            actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\r\n            critic_loss = self.loss(values, returns)\r\n            return actor_loss + critic_loss\r\n    \r\n        @tf.function\r\n        def train_step(self):\r\n            with tf.GradientTape() as tape:\r\n                action_probs, values, rewards = self.play_episode()\r\n                returns = self.get_returns(rewards)\r\n                loss = self.compute_loss(returns, values, action_probs)\r\n            grads = tape.gradient(loss, self.model.trainable_variables)\r\n            self.model.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\r\n            episode_reward = tf.math.reduce_sum(rewards)\r\n            return episode_reward\r\n    \r\n        def fit(self, learning_rate=7e-4):\r\n            self.model.compile(optimizer=Adam(learning_rate))\r\n            self.train_step()\r\n    \r\n    \r\n    if __name__ == '__main__':\r\n        gym_env = gym.make('PongNoFrameskip-v4')\r\n        agn = A2C(gym_env)\r\n        agn.fit()\r\n", "comments": ["I have tried in colab with TF version 2.4, nightly version(`2.5.0-dev20210114`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/0a2838dd44503c16dcb6cbb1a61af05a/untitled621.ipynb). Thanks!", "I closed the issue upon coming up with the fix below:\r\n\r\n    from collections import deque\r\n    from time import perf_counter\r\n    \r\n    import gym\r\n    import numpy as np\r\n    import tensorflow as tf\r\n    from tensorflow.keras.layers import Conv2D, Dense, Flatten, Input\r\n    from tensorflow.keras.losses import Huber\r\n    from tensorflow.keras.models import Model\r\n        \r\n    \r\n    class A2C:\r\n        def __init__(\r\n            self,\r\n            envs,\r\n            seed=None,\r\n            fc_units=512,\r\n            gamma=0.99,\r\n            reward_buffer_size=100,\r\n            max_episode_steps=10000,\r\n        ):\r\n            self.envs = envs\r\n            self.available_actions = envs[0].action_space.n\r\n            self.model = self.create_model(fc_units)\r\n            for env in self.envs:\r\n                env.seed(seed)\r\n            tf.random.set_seed(seed)\r\n            np.random.seed(seed)\r\n            self.total_rewards = deque(maxlen=reward_buffer_size)\r\n            self.mean_reward = -float('inf')\r\n            self.best_reward = -float('inf')\r\n            self.division_eps = np.finfo(np.float32).eps.item()\r\n            self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\r\n            self.gamma = gamma\r\n            self.steps = 0\r\n            self.games = 0\r\n            self.start_state = None\r\n            self.max_episode_steps = max_episode_steps\r\n    \r\n        def create_model(self, fc_units):\r\n            x0 = Input(self.envs[0].observation_space.shape)\r\n            x = Conv2D(32, 8, 4, activation='relu')(x0)\r\n            x = Conv2D(64, 4, 2, activation='relu')(x)\r\n            x = Conv2D(32, 3, 1, activation='relu')(x)\r\n            x = Flatten()(x)\r\n            x = Dense(fc_units, activation='relu')(x)\r\n            actor = Dense(self.available_actions)(x)\r\n            critic = Dense(1)(actor)\r\n            model = Model(x0, [actor, critic])\r\n            model.call = tf.function(model.call)\r\n            return model\r\n    \r\n        def env_step(self, action):\r\n            state, reward, done, _ = self.envs[0].step(action)\r\n            self.steps += 1\r\n            return (\r\n                state.astype(np.float32),\r\n                np.array(reward, np.int32),\r\n                np.array(done, np.int32),\r\n            )\r\n    \r\n        def tf_env_step(self, action):\r\n            return tf.numpy_function(\r\n                self.env_step, [action], [tf.float32, tf.int32, tf.int32]\r\n            )\r\n    \r\n        def play_episode(self):\r\n            action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\r\n            values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\r\n            rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\r\n            initial_state_shape = self.start_state.shape\r\n            state = self.start_state\r\n            for t in tf.range(self.max_episode_steps):\r\n                state = tf.expand_dims(state, 0)\r\n                action_logits_t, value = self.model(state)\r\n                action = tf.random.categorical(action_logits_t, 1)[0, 0]\r\n                action_probs_t = tf.nn.softmax(action_logits_t)\r\n                values = values.write(t, tf.squeeze(value))\r\n                action_probs = action_probs.write(t, action_probs_t[0, action])\r\n                state, reward, done = self.tf_env_step(action)\r\n                state.set_shape(initial_state_shape)\r\n                rewards = rewards.write(t, reward)\r\n                if tf.cast(done, tf.bool):\r\n                    break\r\n            return [item.stack() for item in [action_probs, values, rewards]]\r\n    \r\n        def get_returns(self, rewards, gamma, standardize=True):\r\n            n = tf.shape(rewards)[0]\r\n            returns = tf.TensorArray(dtype=tf.float32, size=n)\r\n            rewards = tf.cast(rewards[::-1], dtype=tf.float32)\r\n            discounted_sum = tf.constant(0.0)\r\n            discounted_sum_shape = discounted_sum.shape\r\n            for i in tf.range(n):\r\n                reward = rewards[i]\r\n                discounted_sum = reward + gamma * discounted_sum\r\n                discounted_sum.set_shape(discounted_sum_shape)\r\n                returns = returns.write(i, discounted_sum)\r\n            returns = returns.stack()[::-1]\r\n            if standardize:\r\n                returns = (returns - tf.math.reduce_mean(returns)) / (\r\n                    tf.math.reduce_std(returns) + self.division_eps\r\n                )\r\n            return returns\r\n    \r\n        @staticmethod\r\n        def compute_loss(action_probs, values, returns):\r\n            advantage = returns - values\r\n            action_log_probs = tf.math.log(action_probs)\r\n            actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\r\n            critic_loss = Huber(reduction=tf.keras.losses.Reduction.SUM)(values, returns)\r\n            return actor_loss + critic_loss\r\n    \r\n        @tf.function\r\n        def train_step(\r\n            self,\r\n        ):\r\n            with tf.GradientTape() as tape:\r\n                action_probs, values, rewards = self.play_episode()\r\n                returns = self.get_returns(rewards, self.gamma)\r\n                action_probs, values, returns = [\r\n                    tf.expand_dims(x, 1) for x in [action_probs, values, returns]\r\n                ]\r\n                loss = self.compute_loss(action_probs, values, returns)\r\n            grads = tape.gradient(loss, self.model.trainable_variables)\r\n            self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\r\n            return tf.math.reduce_sum(rewards)\r\n    \r\n        def fit(self, target_reward):\r\n            display_titles = (\r\n                'frame',\r\n                'games',\r\n                'speed',\r\n                'mean reward',\r\n                'best reward',\r\n                'episode reward',\r\n            )\r\n            while True:\r\n                start_steps = self.steps\r\n                t0 = perf_counter()\r\n                self.start_state = tf.constant(self.envs[0].reset(), dtype=tf.float32)\r\n                episode_reward = int(self.train_step())\r\n                self.games += 1\r\n                self.total_rewards.append(episode_reward)\r\n                self.mean_reward = np.around(np.mean(self.total_rewards), 2)\r\n                self.best_reward = max(episode_reward, self.best_reward)\r\n                speed = (self.steps - start_steps) // (perf_counter() - t0)\r\n                display_values = (\r\n                    self.steps,\r\n                    self.games,\r\n                    f'{speed} steps/s',\r\n                    self.mean_reward,\r\n                    self.best_reward,\r\n                    episode_reward,\r\n                )\r\n                display = (\r\n                    f'{title}: {value}'\r\n                    for title, value in zip(display_titles, display_values)\r\n                )\r\n                print(', '.join(display))\r\n                if self.mean_reward >= target_reward:\r\n                    break\r\n            print(f'\\nSolved in {self.steps} steps.')\r\n    \r\n    \r\n    if __name__ == '__main__':\r\n        en = gym.make('PongNoFrameskip-v4')\r\n        a2c = A2C([en], reward_buffer_size=10)\r\n        a2c.fit(18)\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46458\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46458\">No</a>\n"]}, {"number": 46457, "title": "[INTEL MKL] Refactor mkl_relu_ops files, and add accuracy and performace UT.", "body": "This PR contains two modifications:  \r\n\r\n- Refactor tensorflow/core/kernels/mkl/mkl_relu_op files.\r\n- Add benchmark  and accuracy files for MKL eltwise kernels UT.\r\n", "comments": ["@gyshi  Can you please resolve conflicts? Thanks!", "@gbaned  thanks, I have resolved conflict", "@gbaned , please review this MR, thanks very much, becaues some bug fix depend on this MR.", "@gyshi Can you please resolve conflicts? Thanks!", "@gbaned , i have resolved conflict. thanks for reviewing", "@gbaned , Please review", "@gyshi Can you please resolve conflicts? Thanks!", "@gyshi  Any update on this PR? Please. Thanks!", "hi, @gbaned   I remove  tensorflow/core/kernels/mkl/mkl_relu_op_test.cc this file, I do not know why it raises Conflicting. now, i have no update for this MR. thanks", "@gyshi Can you please resolve conflicts? then this PR will go for review. Thanks!", "hi, @gbaned  i have rebased this MR", "hi, @gbaned  please review this mr", "@gbaned @penpornk hi, please review this mr, some bug fix depend on this MR.  thanks very much", "@gbaned @penpornk hi, please review this mr, some bug fix depend on this MR. thanks very much", "@penpornk Can you please review this PR ? Thanks!", "@penpornk please review this MR\u3002 thanks very much, i think Gelu and other bug fix will depend on this mr", "@penpornk Can you please review this PR ? Thanks!", "@gyshi  Can you please resolve conflicts? Thanks!\r\n", "@gbaned  hi, after merge this mr: https://github.com/tensorflow/tensorflow/pull/52002, i will solve this conflict.  because these two change the same file's code."]}, {"number": 46456, "title": "Two SavedModel roundtrips lose track of a Resource's initializer and its assets", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): any\r\n- TensorFlow installed from (source or binary): either\r\n- TensorFlow version (use command below): 2.4.0, nightly\r\n- Python version: any\r\n\r\n**Describe the current behavior**\r\n 1) Creating `MyLookupModel` as below and doing tf.saved_model.save, load on it recreates the `._vocab_table._initializer` but does not track it as a trackable subobject of `._vocab_table`.\r\n 2) Doing tf.saved_model.save, load once more initializes the `._vocab_table` from the vocab file asset of the first SavedModel, not the second SavedModel - or crashes, if the first SavedModel is no longer available.\r\n\r\nItem 1 appears to be a general issue with restored resource objects (not peculiar to StaticHashTable), and item 2 appears to be a direct consequence of it.\r\n\r\nThe end-to-end effect of this was reported in issue https://github.com/tensorflow/hub/issues/719 for a first SavedModel stored on TF Hub and a second SavedModel built from it by the user (with more parts added, and using Keras, but that seems immaterial).\r\n\r\n**Describe the expected behavior**\r\n 1) Saving and restoring `MyLookupModel` recreates the `._vocab_table._initializer` and tracks it as a trackable subobject of `._vocab_table`.\r\n 2) Saving and restoring the model once more initializes the `._vocab_table` from the vocab file asset of the second SavedModel - with no need for the first SavedModel to be available in that time and place.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nSee also b/177326279\r\n\r\n```python\r\nclass MyLookupModel(tf.train.Checkpoint):\r\n  def __init__(self, vocab_file):\r\n    super().__init__()\r\n    vocab_initializer = tf.lookup.TextFileInitializer(\r\n        vocab_file,\r\n        key_dtype=tf.string, key_index=tf.lookup.TextFileIndex.WHOLE_LINE,\r\n        value_dtype=tf.int64, value_index=tf.lookup.TextFileIndex.LINE_NUMBER)\r\n    self._vocab_table = tf.lookup.StaticHashTable(vocab_initializer,\r\n                                                  default_value=-1)\r\n\r\n  @tf.function(input_signature=[tf.TensorSpec((None,), tf.string)])\r\n  def __call__(self, inputs):\r\n    return self._vocab_table.lookup(inputs)\r\n\r\nORIGINAL_VOCAB = \"/tmp/original/vocab.txt\"\r\ntf.io.gfile.makedirs(os.path.dirname(ORIGINAL_VOCAB))\r\nwith tf.io.gfile.GFile(ORIGINAL_VOCAB, \"w\") as f:\r\n  for x in [\"a\", \"b\", \"c\", \"d\"]:\r\n    f.write(x + \"\\n\")\r\n\r\nmodel0 = MyLookupModel(ORIGINAL_VOCAB)\r\ntf.saved_model.save(model0, \"/tmp/model1\")\r\nmodel1 = tf.saved_model.load(\"/tmp/model1\")\r\ntf.saved_model.save(model1, \"/tmp/model2\")\r\n# If \"/tmp/model1/assets/vocab.txt\" is deleted at this point, the next line crashes.\r\nmodel2 = tf.saved_model.load(\"/tmp/model2\") \r\n```\r\n", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/d10a3b25b824b58b198a4b8cf892d372/46456-tf-nightly.ipynb). Thanks!", "Is there any workaround?  I have try to copy assets to the specified directory, It works. But it is not suitable for distribution.", "Yes, there is a simple but tedious workaround: make the Asset (here: `._vocab_table._initializer._filename`) reachable by some other path of Trackable objects (for example, here in `MyLookupModel.__init__()`, change the local variable `vocab_initializer` to an attribute `self._vocab_initializer`, so that the asset is tracked as `modelX._vocab_initializer._filename` even if the path via `._vocab_table` is lost in `model1`).", "This has been fixed by https://github.com/tensorflow/tensorflow/commit/149691c4b1628ea67b06188503dc2fc4a6be3317, which has already been available from tf-nightly for a while and is expected to be released with TF2.5. Thank you, @k-w-w !\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46456\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46456\">No</a>\n"]}, {"number": 46455, "title": "Import Issue of cross-built tflite package", "body": "System information\r\n\r\nHost OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 x86_64 amd64 PC\r\nTensorFlow installed from (source or binary): v2.4\r\nTensorflow version (commit SHA if source): 582c8d2\r\nTarget platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): RK3399 Ubuntu 18.04 aarch64\r\n\r\n\r\nDescribe the problem:\r\n\r\nI built the tflite runtime whl on host PC and tried to import the whl package file in python 3.7.\r\nBut I met the following issue.\r\n\r\n![image](https://user-images.githubusercontent.com/47862419/104740652-264f5e00-5783-11eb-8add-5d517757adf2.png)\r\n\r\nHost PC environment:\r\nOS: Ubuntu 18.04 x86_64 AMD64\r\nnative gcc(g++): 7.5.0\r\nldd: 2.27 \r\ncross-compiler aarch64-linux-gnu-gcc(aarch64-linux-gnu-g++): 8.3.0 (This is downloaded and used automatically when building )\r\nbazel: 3.1.0\r\npython: virtual env python 3.7\r\n\r\nTarget device environment:\r\nOS: Ubuntu 18.04 LTS aarch64\r\nnative gcc(g++): 7.5.0\r\nldd: 2.27\r\npython: 3.7\r\n\r\nPlease provide the exact sequence of commands/steps when you ran into the problem\r\n\r\nI followed the following steps on my host PC.\r\n\r\nsudo apt update\r\nsudo apt-get install software-properties-common\r\nsudo apt update\r\nsudo apt install git curl\r\nsudo apt install python3.7 python3.7-dev python3.7-venv python3.7-distutils\r\nsudo apt install mesa-common-dev libegl1-mesa-dev libgles2-mesa-dev\r\n\r\ncd ~\r\npython3.7 -m venv py37\r\nsource ~/py37/bin/activate\r\npip install cython\r\npip install wheel\r\npip install numpy\r\n\r\ngit clone -b r2.4 https://github.com/tensorflow/tensorflow.git tensorflow_r2.4\r\ncd tensorflow_r2.4\r\npython configure.py\r\n\r\n![image](https://user-images.githubusercontent.com/47862419/104741092-b2fa1c00-5783-11eb-81a2-7315f1660250.png)\r\n\r\n./tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh aarch64\r\n\r\nThe whl file \"tflite_runtime-2.4.0-py3-none-linux_aarch64.whl\" is outputted in the folder \"tensorflow_r2.4/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/dist\".\r\n\r\nWhen importing the tflite run-time package on the target device platform, The issue occurs.\r\n\r\nfrom tflite_runtime.interpreter import Interpreter\r\n\r\n\r\n", "comments": ["Can you try building with docker, as mentioned in https://github.com/tensorflow/tensorflow/issues/46443?\r\n\r\nAlso you might want to check if the host's cross-compiler and target environments share the same glibc version.", "For GLIBC compatibility, please refer the following.\r\nhttps://www.tensorflow.org/lite/guide/build_cmake_arm#checking_glibc_version\r\n\r\nYou might need to use custom toolchain with CMake.\r\nhttps://www.tensorflow.org/lite/guide/build_cmake_pip", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46455\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46455\">No</a>\n"]}, {"number": 46454, "title": "No estimated time using model.fit() with TFRecord Dataset.", "body": "There is no estimated time when I call model.fit() with TFRecord Dataset.\r\n\r\n```\r\n   2448/Unknown - 894s 364ms/step - loss: 0.0703\r\n```", "comments": ["@BenjaminChoou \r\n\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nPlease,share colab link or simple standalone cose to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46454\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46454\">No</a>\n", "Sorry. I found the estimated time will appear after 1 epoch training. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46454\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46454\">No</a>\n"]}, {"number": 46453, "title": "TensorRT converter fails for CombinedNonMaxSuppression", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04**\r\n- TensorFlow installed from (source or binary): **Binary**\r\n- TensorFlow version (use command below): **TF:2.5.0-dev20210114**\r\n- Python version: **3.7**\r\n- CUDA/cuDNN version: **11.0, 8.0.4**\r\n- GPU model and memory: **1060**\r\n\r\n**Describe the current behavior**\r\nTensorRT converter crashes with a segmentation fault when I try to export my `saved_model`.\r\nInterestingly, if I set `minimum_segment_size=10`, it works because it skips \r\n\r\n*Replaced segment 5 consisting of 7 nodes by StatefulPartitionedCall/decode_predictions/TRTEngineOp_0_5.\r\n2021-01-15 15:21:38.915310: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:858] Segment consists of nodes: StatefulPartitionedCall/decode_predictions/combined_non_max_suppression/CombinedNonMaxSuppression, StatefulPartitionedCall/decode_predictions/combined_non_max_suppression/CombinedNonMaxSuppression/max_output_size_per_class, StatefulPartitionedCall/decode_predictions/combined_non_max_suppression/Const, StatefulPartitionedCall/decode_predictions/combined_non_max_suppression/iou_threshold, StatefulPartitionedCall/decode_predictions/combined_non_max_suppression/score_threshold, StatefulPartitionedCall/decode_predictions/transpose_1, StatefulPartitionedCall/decode_predictions/transpose_1/perm*\r\n\r\nI have attached the full log after running with these flags\r\n`TF_CPP_VMODULE=trt_engine_op=2,convert_nodes=2,convert_graph=2,segment=2,trt_shape_optimization_profiles=2,trt_engine_resource_ops=2 python trt.py`\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport os\r\n\r\nimport tensorflow as tf\r\n\r\n## Download and extract the zip \r\n## URL: https://drive.google.com/file/d/1Zxqdnm2iHpJGdUl17cAi-lV7wZ3UhMDA/view\r\n\r\nparams = tf.experimental.tensorrt.ConversionParams(\r\n    precision_mode='FP32',\r\n    maximum_cached_engines=1,\r\n    minimum_segment_size=5)\r\n\r\nconverter = tf.experimental.tensorrt.Converter(\r\n    input_saved_model_dir='retinanet-18-640-30x-64-tpu',\r\n    conversion_params=params)\r\nconverter.convert()\r\n\r\ndef input_fn(steps=1):\r\n    for i in range(steps):\r\n        yield (tf.random.uniform([640, 640, 3]), tf.constant(1, dtype=tf.int32))\r\n        \r\nconverter.build(input_fn=input_fn)\r\nconverter.save('trt')\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n[trt_log.txt](https://github.com/tensorflow/tensorflow/files/5819748/trt_log.txt)\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46453\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46453\">No</a>\n", "Was able to run the code without any issues on [TF v2.4](https://colab.research.google.com/gist/amahendrakar/7f400de432bddbf6b4e47a0feb33ed7a/46453.ipynb).\r\n\r\nHowever, session crashes on running the code with [TF-nightly](https://colab.research.google.com/gist/amahendrakar/be033a6c68b0028f5522924ef378e66b/46453-tf-nightly.ipynb#scrollTo=rdbO5RSdFM6a) (i.e. v2.5.0-dev20210114). Please check the linked gist for reference. Thanks!", "I can successfully convert, build, and then load back the model in `2.4.0`. But if I include `tf.image.combined_non_max_suppression` op for conversion, I get totally wrong predictions from the converted model. \r\n\r\nThe same code fails in nightly `dev20210114`.\r\n\r\n\r\nFor reference, this is how i call `tf.image.combined_non_max_suppression`\r\n\r\n```python\r\ndef call(self, predictions):\r\n    box_predictions, class_predictions = predictions\r\n\r\n    class_predictions = tf.cast(class_predictions, dtype=tf.float32)\r\n    box_predictions = tf.cast(box_predictions, dtype=tf.float32)\r\n\r\n    class_predictions = tf.nn.sigmoid(class_predictions)  #  [batch_size, num_anchors, num_classes]\r\n    boxes = self._decode_box_predictions(self._anchors.boxes[None, ...],\r\n                                         box_predictions)   #  [batch_size, num_anchors, 4]; (absolute coordinates)\r\n\r\n    if self.pre_nms_top_k > 0:  #  This condition return false because `pre_nms_top_k` is -1 always\r\n        top_k_class_predictions, top_k_boxes = self._filter_top_k(\r\n            class_predictions, boxes)\r\n\r\n    else:\r\n        top_k_boxes = tf.expand_dims(boxes, axis=2)   #  [batch_size, num_anchors, 1, 4]; (absolute coordinates)\r\n        top_k_class_predictions = class_predictions #  [batch_size, num_anchors, num_classes]\r\n\r\n    return tf.image.combined_non_max_suppression(\r\n        top_k_boxes,\r\n        top_k_class_predictions,\r\n        self.max_detections_per_class,  #  100\r\n        self.max_detections,  #  100\r\n        self.nms_iou_threshold,  #  0.5\r\n        self.confidence_threshold,  #  0.05\r\n        clip_boxes=False,\r\n    )\r\n```\r\n\r\n**EDIT 1**: I could fix the wrong predictions issue (after conversion) by using normalized coordinates (when x1, y1, x2, y2 lie in [0, 1])\r\nBut according to [the documentation](https://www.tensorflow.org/api_docs/python/tf/image/combined_non_max_suppression), both normalized and absolute coordinates should work. I think the TRT plugin only supports normalized coordinates.\r\n```\r\n... \"Bounding boxes are supplied as [y1, x1, y2, x2], where (y1, x1) and (y2, x2) \r\nare the coordinates of any diagonal pair of box corners and the coordinates can\r\nbe provided as normalized (i.e., lying in the interval [0, 1]) or absolute\"...\r\n```\r\n\r\n**EDIT 2**\r\nLooks like this is the reason why we always send normalized coordinates to TRT plugin\r\nhttps://github.com/tensorflow/tensorflow/blob/4fa4184a5a454eceb5b567c8b3c4fce46faf2de8/tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc#L5911-L5917", "@bixia1 what do you think?  Is [this](https://github.com/tensorflow/tensorflow/blob/4fa4184a5a454eceb5b567c8b3c4fce46faf2de8/tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc#L5914) correct?", "The [PR in question](https://github.com/tensorflow/tensorflow/pull/40062) (merged on 1/13/2021) doesn\u2019t make any change in terms of always use normalized coordinate in TF-TRT.\r\n\r\nThe [TF document](https://www.tensorflow.org/api_docs/python/tf/image/combined_non_max_suppression) says the bounding box coordinates can be normalized or absolute values. But I couldn't tell how the operation representation indicate whether the coordinates are normalized or absolute values.  I looked at [the implement of the compute method for the operation](https://github.com/tensorflow/tensorflow/blob/9045dcaf276cb7b24fde33da09165cb38d157a5e/tensorflow/core/kernels/image/non_max_suppression_op.cc#L916) and couldn't figure out this either. I am asking for information about this in an internal channel.\r\n\r\n", "@tfeher ", "I got an answer from the TensorFlow people which help me understand the situation. The TensorFlow implementation for the operation is the same regardless whether the coordinates are normalized or absolute values. On the other hand, the TensorRT implementation requires a parameter that specifies whether the coordinates are normalized or not, see code [here](https://github.com/tensorflow/tensorflow/blob/ab9eb3d10400c92b15bfae62b284bf9937cb1845/tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc#L5951-L5952). If this field is indeed used in the TensorRT implementation, then we have a problem here. Now the question is why it was working before the  [PR in question](https://github.com/tensorflow/tensorflow/pull/40062)? I will let @tfeher and @DEKHTIARJonathan take care of this.", "@bixia1 There are two issues here, \r\n - Conversion of `tf.image.combined_non_max_suppression` in nightly fails. But works in `2.4.0`\r\n - Reading the comments [here](https://github.com/tensorflow/tensorflow/blob/4fa4184a5a454eceb5b567c8b3c4fce46faf2de8/tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc#L5911-L5917), calculation of width/height differs in tensorrt vs tensorflow op. To avoid this, the converter always assumes the users are sending normalized coordinates [here](https://github.com/tensorflow/tensorflow/blob/4fa4184a5a454eceb5b567c8b3c4fce46faf2de8/tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc#L5918).\r\n\r\nHence, if the users want to export `tf.image.combined_non_max_suppression` to tensorrt, they should make sure that they are sending normalized coordinates. But the documentation fails to warn the user about this.", "> The TensorFlow implementation for the operation is the same regardless whether the coordinates are normalized or absolute values. On the other hand, the TensorRT implementation requires a parameter that specifies whether the coordinates are normalized or not\r\n\r\nTensorRT indeed has two implementation for the IOU calculation one for coordinates that should be interpreted as [pixels](https://github.com/NVIDIA/TensorRT/blob/183f891191f08fd016216fd0b94bc9c8c52d0ac2/plugin/common/kernels/allClassNMS.cu#L142-L143), and one otherwise (activated by[ isNormalized=true](https://github.com/NVIDIA/TensorRT/blob/183f891191f08fd016216fd0b94bc9c8c52d0ac2/plugin/common/kernels/allClassNMS.cu#L135-L139)). Note that `isNormalized` is an unfortunate name for the option, what it does is simply switching between these two modes. The latter implementation agrees with the [TF implementation](https://github.com/tensorflow/tensorflow/blob/508374a893df7999633d9ebbe55e94d92eef7280/tensorflow/core/kernels/image/non_max_suppression_op.cc#L123-L134), therefore the converter uses only that mode [mode of the TRT plugin](https://github.com/tensorflow/tensorflow/blob/ac4b2997b6c47e06a4f689355c08418856905594/tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc#L5911-L5918). We do have an unit test which checks that the converter works for non-normalized coordinates. \r\n\r\nTo summarize, the TF-TRT converted combinedNMS op should work with not-normalized coordinates. #40062 did not touch this. Among other things, the handling of the `clib_boxes` attribute was corrected in that PR, but not sure if that is relevant here.\r\n\r\nI will have a closer look at the problem and report back.\r\n\r\n\r\n\r\n", "I could reproduce the bug. The TRT engine throws a segfault when we call [enqueue](https://github.com/tensorflow/tensorflow/blob/3db793ee031f2abede180043b016c085f1d8c26b/tensorflow/compiler/tf2tensorrt/utils/trt_engine_utils.cc#L280). The problem indeed happens after the converter was changed in #40062, if I revert the changes then the segfault disappears. It still needs to be clarified whether we make an error on TF-TRT side while setting up the TRT NMS plugin parameters, or it is a bug in TRT.", "The issue is caused by the change in handling the `top_k` parameter for the plugin. The value that we are passing (5000) is larger than what TRT can handle (4096). There are two options to fix this:\r\n- Cap top_k to 4096. We have to check whether this leads to incompatibility between the TF and TRT results.\r\n- Mark node as incompatible, this is always safe but bad for performance.", "> > The TensorFlow implementation for the operation is the same regardless whether the coordinates are normalized or absolute values. On the other hand, the TensorRT implementation requires a parameter that specifies whether the coordinates are normalized or not\r\n> \r\n> TensorRT indeed has two implementation for the IOU calculation one for coordinates that should be interpreted as [pixels](https://github.com/NVIDIA/TensorRT/blob/183f891191f08fd016216fd0b94bc9c8c52d0ac2/plugin/common/kernels/allClassNMS.cu#L142-L143), and one otherwise (activated by[ isNormalized=true](https://github.com/NVIDIA/TensorRT/blob/183f891191f08fd016216fd0b94bc9c8c52d0ac2/plugin/common/kernels/allClassNMS.cu#L135-L139)). Note that `isNormalized` is an unfortunate name for the option, what it does is simply switching between these two modes. The latter implementation agrees with the [TF implementation](https://github.com/tensorflow/tensorflow/blob/508374a893df7999633d9ebbe55e94d92eef7280/tensorflow/core/kernels/image/non_max_suppression_op.cc#L123-L134), therefore the converter uses only that mode [mode of the TRT plugin](https://github.com/tensorflow/tensorflow/blob/ac4b2997b6c47e06a4f689355c08418856905594/tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc#L5911-L5918). We do have an unit test which checks that the converter works for non-normalized coordinates.\r\n> \r\n> To summarize, the TF-TRT converted combinedNMS op should work with not-normalized coordinates. #40062 did not touch this. Among other things, the handling of the `clib_boxes` attribute was corrected in that PR, but not sure if that is relevant here.\r\n> \r\n> I will have a closer look at the problem and report back.\r\n\r\n@tfeher \r\nhttps://github.com/tensorflow/tensorflow/blob/a6f927400dfa09f445faee953e5694512226eed5/tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc#L5980-L5987\r\nCan you please clarify: Since `is_normalized` explicitly being set to `true`, does this mean that the converter expects the coordinates to be normalized `[0, 1]`?. When I tried passing unnormalized coordinates, I did not get correct results.", "As I have described above, the combinedNMS op is expected to work with not normalized coordinates. And yes, we need to set `is_normalized = true` even if we have not normalied coordinates. TRT's naming of the argument is somewhat unfortunate. I have confirmed with a TRT engineer that the `isNormalized` arg only switches modes of IOU calculation, it does not require us to provide normalized coordinates.\r\n\r\nNote that TRT has an option `clipBoxes` with the following meaning:\r\n\r\n> Forcibly restrict bounding boxes to the normalized range [0,1]. Only applicable if isNormalized is also true. Defaults to true.\r\n\r\nIf it is set to false, then the conversion should work with unnormalized coordinates.\r\n\r\nNow the problem is, that before #40062, the `clipBoxes` arg was not set by the converter, so it defaulted to truncating the box coordinates. This is one of the things which was fixed by #40062. \r\n\r\nThe question is, which version of TF are you using  when you get the incorrect results?", "@tfeher, I guess this explains the behaviour. I was successfully able to convert the models in `2.4.0` but the output boxes were being clipped to 1 for the converted model. \r\n> clipBoxes arg was not set by the converter", "@tfeher, we encountered similar crash for a google internal customer and verified that setting top_k to 4096 fix the problem. For their case, num_boxes is 70K+. \r\n\r\nI think we probably misunderstand the relationship between num_boxes and top_k in [this code block](https://github.com/tensorflow/tensorflow/blob/ed22f400428a669c1c6e4553cd7f4900abeaf954/tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc#L5999-L6002).  top_k in the plugin is more like the num of top values we keep in the internal of the algorithm, in order to support the selection of keep_top_k as output, is it?\r\n\r\nI think we should fix this to something like this:\r\nif (keep_top_k > 4096) return \"tensorrt not support\" else top_k = 4096 (or top_k = keep_top_k?)\r\n\r\nThey also help me check the performance for these two ways of setting top_k:\r\n     top_k = keep_top_k\r\n     top_k = 4096\r\nand didn't see any perf diff for their app.\r\n", "See cloud_tpu is setting top_k to 5000 [here](https://github.com/tensorflow/tpu/blob/d4daff70a8a17625cb43386b2a564cb0e0e0e130/models/official/detection/ops/postprocess_ops.py#L60-L80)\r\nHere is their definition of top_k: \r\npre_nms_num_boxes: an int number of top candidate detections per class\r\n      before NMS", "@bixia1 pre_nms_top_k is used in models like RetinaNet, EfficientDet and other similar models. And in most of the cases, the literature asks us to pick the top 5000 boxes (depending on the score), this is where the number 5000 comes from.\r\n\r\nBut what troubles me is there is no way in which the user can set this value when calling `combined_non_max_suppression`. I feel that since tensorrt plugin already has this field, it would be beneficial to expose this param in `tf.image.combined_non_max_suppression`\r\n\r\n```python\r\ntf.image.combined_non_max_suppression(\r\n    boxes, scores, max_output_size_per_class, max_total_size, iou_threshold=0.5,\r\n    score_threshold=float('-inf'), pad_per_class=False, clip_boxes=True,\r\n    name=None\r\n)\r\n```", "Hi, Tama's PR is going to fix this by introducing an environment for you to overwrite the default behavior. With his PR, the default behavior won't be changed, that is, we still reject the case where top_k > 4096 as we do currently unless you explicit request to change this behavior through providing the environment variable. We need your feedback on this solution.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46453\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46453\">No</a>\n"]}, {"number": 46452, "title": "[BugFix] Fix bug when reduce dimension becomes empty in reduction_degenerate_dim_remover pass", "body": "I have encountered the following bug triggered by reduction_degenerate_dim_remover pass for HLO. Suppose we have the following two instructions.\r\n```\r\n%add.127 = f32[1,512,1024]{2,1,0} .......\r\n%reduce = f32[512,1024]{1,0} reduce(f32[1,512,1024]{2,1,0} %add.127, f32[] %constant_3), dimensions={0},\r\n```\r\n\r\nThen the reduction_degenerate_dim_remover pass helps to remove the only reduction dimension for ```%reduce```.\r\n```\r\n%add.127 = f32[1,512,1024]{2,1,0} .......\r\n%bitcast.292 = f32[512,1024]{1,0} bitcast(f32[1,512,1024]{2,1,0} %add.127)\r\n%reduce = f32[512,1024]{1,0} reduce(f32[512,1024]{1,0} %bitcast.292, f32[] %constant_3), dimensions={}, \r\n```\r\n\r\nThis leads to produce an illegal reduce instruction which causes core dump in llvm code generation phase.\r\n\r\nSo, I submit my bug fix for this scenario. The ```%reduce``` instruction is unnecessary and can be replaced by its operations (```%bitcast```) directly. \r\n\r\nPlease start a review for this PR, thanks!\r\n @cheshire ", "comments": ["Could you add a test case?", "A test case and comments has been added, please review again.  @cheshire ", "The only failed CI failed test case  is caused by OOM, which is not related to my code change~"]}, {"number": 46451, "title": "[BugFix] Fix bug when reduce dimension becomes emptry in reduction_degenerate_dim_remover pass", "body": "I have encountered the following bug triggered by reduction_degenerate_dim_remover pass for HLO. Suppose we have the following two instructions.\r\n```\r\n%add.127 = f32[1,512,1024]{2,1,0} .......\r\n%reduce = f32[512,1024]{1,0} reduce(f32[1,512,1024]{2,1,0} %add.127, f32[] %constant_3), dimensions={0},\r\n```\r\n\r\nThen the reduction_degenerate_dim_remover pass helps to remove the only reduction dimension for ```%reduce```.\r\n```\r\n%add.127 = f32[1,512,1024]{2,1,0} .......\r\n%bitcast.292 = f32[512,1024]{1,0} bitcast(f32[1,512,1024]{2,1,0} %add.127)\r\n%reduce = f32[512,1024]{1,0} reduce(f32[512,1024]{1,0} %bitcast.292, f32[] %constant_3), dimensions={}, \r\n```\r\n\r\nThis leads to produce an illegal reduce instruction which causes core dump in llvm code generation phase.\r\n\r\nSo, I submit my bug fix for this scenario. The ```%reduce``` instruction is unnecessary and can be replaced by its operations (```%bitcast```) directly. \r\n", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46451) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46451) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 46450, "title": "unable to load weights from directories other than the working directory", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):  binary\r\n- TensorFlow version (use command below): tf-nightly-gpu v2.5.0.dev20201214\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA v11.0\r\n- GPU model and memory: Tesla T4 15109MiB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nIf i use model.load_weights(dir) and specify a directory similarly to how it's done in the docs:\r\n\r\ncheckpoint_filepath = \"./checkpoints/training_checkpoints_tests/cp.ckpt\"\r\ncheckpoint_dir = os.path.dirname(checkpoint_filepath)\r\n\r\n if os.path.exists(checkpoint_dir):\r\n            model.load_weights(checkpoint_filepath)\r\n            print('Checkpoints loaded')\r\n        else:\r\n            !mkdir -p './checkpoints/training_checkpoints_tests'\r\n            print('No checkpoints found')\r\n\r\nhowever the script never actually reads into the directory i'm specifying. If i instead load checkpoints directly from the working directory:\r\n\r\ncheckpoint_filepath = \"/training_checkpoints_tests/cp.ckpt\"\r\ncheckpoint_dir = os.path.dirname(checkpoint_filepath)\r\n\r\n if os.path.exists(checkpoint_dir):\r\n            model.load_weights(checkpoint_filepath)\r\n            print('Checkpoints loaded')\r\n        else:\r\n            !mkdir -p '/training_checkpoints_tests'\r\n            print('No checkpoints found')\r\n\r\nit works no problem. This second approach is fine if you have few models or checkpoints, but the direcotry quickly fills with folders and folders of checkpoints of different models and runs and it'd be better to keep all checkpoint in a folder\r\n\r\n**Describe the expected behavior**\r\nload checkpoints from sub-directories\r\n\r\n**Standalone code to reproduce the issue**\r\nSee above\r\n", "comments": ["checkpoint_filepath = \"./checkpoints/training_checkpoints_tests/cp.ckpt\"\r\n\r\nwhat is the pwd here? Please write the entire path.", "\"$USER/(private github repository name)/MobilenetV3Large/checkpoints/training_checkpoints_tests/cp.ckpt\"", "@ghylander \r\nThere are too many indentation errors, can you please share your code on a colab gist with the error reported.", "While preparing the code to be linked here i think i found the issue\r\n\r\nin the docs [https://www.tensorflow.org/guide/keras/save_and_serialize](url) checkpoint filepaths are specified with double quotes \"\"\r\n\r\ni.e: `model.load_weights(\"pretrained_ckpt\")`\r\n\r\nthis is not the correct way to specify filepaths/directories or files in linux, Single quotes '' are to be used\r\n\r\nwhen specifying working directory paths, using double quotes works probably because python handles the paths as entities, while using os.path.exists may not correctly identify paths written inside double quotes.\r\n\r\n", "i ran a test and loading the checkpoints in a subdirectory, without using os.path.exists, will indeed load the checkpoints", "@ghylander \r\nPlease move the issue to closed status if resolved.", "should i post it to the docs repository?", "@ghylander\r\nplease create a pr in docs repo and move this to closed status.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46450\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46450\">No</a>\n"]}, {"number": 46449, "title": "no kernel image is available for execution on the device", "body": "\r\n**System information**\r\n- I have  written custom code to YOLO V4:\r\n- In Windows 10:\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.7.9\r\n\r\n- CUDA/cuDNN version: CUDA 10.1 with cuDNN == cudnn-10.1-windows10-x64-v7.6.4.38\r\n- GPU model and memory: NVIDIA GEFORCE 940MX [In acer E5 laptop]\r\n\r\nI have got error like this \r\n`2021-01-15 13:27:54.527065: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2021-01-15 13:28:41.927373: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n2021-01-15 13:28:42.898934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce 940MX computeCapability: 5.0\r\ncoreClock: 1.189GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 29.80GiB/s\r\n2021-01-15 13:28:42.899439: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2021-01-15 13:28:44.140267: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2021-01-15 13:28:45.049662: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2021-01-15 13:28:45.267345: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2021-01-15 13:28:48.207716: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2021-01-15 13:28:49.165815: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2021-01-15 13:28:52.588923: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2021-01-15 13:28:52.878248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\nGPUs [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\nLoading Darknet_weights from: ../model_data/yolov3.weights\r\n2021-01-15 13:28:53.793044: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-01-15 13:28:53.950440: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x11f09ecb740 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2021-01-15 13:28:53.950804: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2021-01-15 13:28:54.109305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce 940MX computeCapability: 5.0\r\ncoreClock: 1.189GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 29.80GiB/s\r\n2021-01-15 13:28:54.109697: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2021-01-15 13:28:54.109873: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2021-01-15 13:28:54.110285: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2021-01-15 13:28:54.110996: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2021-01-15 13:28:54.111835: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2021-01-15 13:28:54.112742: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2021-01-15 13:28:54.113099: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2021-01-15 13:28:54.113452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2021-01-15 13:28:54.927364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-01-15 13:28:54.927667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2021-01-15 13:28:54.927788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2021-01-15 13:28:54.928260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1464 MB memory) -> physical GPU (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n2021-01-15 13:28:54.953337: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x11f1afa1690 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2021-01-15 13:28:54.953588: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce 940MX, Compute Capability 5.0\r\n2021-01-15 13:28:54.979368: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-01-15 13:28:55.014682: F .\\tensorflow/core/kernels/random_op_gpu.h:232] Non-OK-status: GpuLaunchKernel(FillPhiloxRandomKernelLaunch<Distribution>, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: no kernel image is available for execution on the device\r\n\r\nProcess finished with exit code -1073740791 (0xC0000409)`\r\n\r\nI have mention about my hardware in above.I have states I have mention about my hardware in above. Any help would be greatly appreciated", "comments": ["@AnushangaWimalasena \r\n\r\nCan you please share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "I believe this error happens because TensorFlow 2.3.1 does not support GPUs with compute capability 5.0. See #46537. The GeForce 940MX is CC 5.0.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46449\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46449\">No</a>\n"]}, {"number": 46448, "title": "Add SkipLine to BufferedInputStream", "body": "This PR adds a `SkipLine` method to `BufferedInputStream` which will help implement the `SkipInternal` method of `TextLineDataset`, like #41222 to `TFRecordDataset`\r\n\r\nThank you for yor time on reviewing this PR.", "comments": ["@allenlavoie Thank you for your reviewing. There was a typo in the test file which resulted in the build errors. Could you take another look? :)", "The errors from TFLite are not induced by this PR. The relevant error log is:\r\n```\r\nINFO: Repository 'eigen_archive' used the following cache hits instead of downloading the corresponding file.\r\n * Hash 'd76992f1972e4ff270221c7ee8125610a8e02bb46708a7295ee646e99287083b' for https://storage.googleapis.com/mirror.tensorflow.org/gitlab.com/libeigen/eigen/-/archive/90ee821c563fa20db4d64d6991ddca256d5c52f2/eigen-90ee821c563fa20db4d64d6991ddca256d5c52f2.tar.gz\r\nIf the definition of 'eigen_archive' was updated, verify that the hashes were also updated.\r\nERROR: An error occurred during the fetch of repository 'eigen_archive':\r\n   Traceback (most recent call last):\r\n\tFile \"/workspace/third_party/repo.bzl\", line 119, column 21, in _tf_http_archive\r\n\t\tctx.template(\"BUILD.bazel\", build_file, executable = False)\r\nError in template: Not a regular file: /workspace/third_party/eigen.BUILD\r\nINFO: Repository clog instantiated at:\r\n  /workspace/WORKSPACE:20:10: in <toplevel>\r\n  /workspace/tensorflow/workspace.bzl:85:20: in workspace\r\n  /workspace/tensorflow/workspace.bzl:79:27: in tf_repositories\r\n  /workspace/tensorflow/workspace.bzl:13:9: in initialize_third_party\r\n  /workspace/third_party/clog/workspace.bzl:6:29: in repo\r\nRepository rule third_party_http_archive defined at:\r\n  /workspace/third_party/repo.bzl:226:43: in <toplevel>\r\nINFO: Repository remotejdk11_linux instantiated at:\r\n  /DEFAULT.WORKSPACE.SUFFIX:131:6: in <toplevel>\r\n  /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/bazel_tools/tools/build_defs/repo/utils.bzl:201:18: in maybe\r\nRepository rule http_archive defined at:\r\n  /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>\r\nINFO: Repository remote_java_tools_linux instantiated at:\r\n  /DEFAULT.WORKSPACE.SUFFIX:237:6: in <toplevel>\r\n  /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/bazel_tools/tools/build_defs/repo/utils.bzl:201:18: in maybe\r\nRepository rule http_archive defined at:\r\n  /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>\r\nERROR: /workspace/third_party/eigen3/BUILD:34:11: //third_party/eigen3:eigen3 depends on @eigen_archive//:eigen in repository @eigen_archive which failed to fetch. no such package '@eigen_archive//': Not a regular file: /workspace/third_party/eigen.BUILD\r\nERROR: Analysis of target '//tensorflow/lite/micro/kernels:fully_connected' failed; build aborted: Analysis failed\r\nINFO: Elapsed time: 7.455s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (57 packages loaded, 1038 targets configured)\r\nERROR: Couldn't start the build. Unable to run tests\r\nFAILED: Build did NOT complete successfully (57 packages loaded, 1038 targets configured)\r\n```", "@allenlavoie  Can you please take a look on above comments from @zhuzilin. Thanks!", "@allenlavoie Any update on this PR? Please. Thanks!"]}, {"number": 46446, "title": "Optimize RunLineHelper in BufferedInputStream", "body": "The old implementation of `ReadLineHelper` will append chars one at a time to the tstring result, which may cause multiply memcpy or reinitialization. This PR change from using\r\n```c++\r\ntstring& append(size_t n, char c)\r\n```\r\nto \r\n```c++\r\ntstring& append(const char* str, size_t len)\r\n```\r\nto append multiple chars once.\r\n\r\nThis optimziation will help `TextlineDataset` which iteratively calls `ReadLineHelper`.\r\n\r\nThank you for your time on reviewing this PR.", "comments": ["@zhuzilin  Can you please check @allenlavoie's comments and keep us posted ? Thanks!", "@gbaned Sorry... I missed the email alert for this...\r\n@allenlavoie \r\nI'm not sure how to use `result->reserve` to simplify the logic. The logic is:\r\n1. If we read to the end of the buffer, we need to append the remaining to `result` and refill the buffer.\r\n2. If the current character is `\\n` we need to append to it and return.\r\n3. skip `\\r`.\r\n\r\nI've removed all the `if (start_pos != pos_)` because I found that there is already an early return in `append` when the appending string is of zero length. Would this make the code a little better?"]}, {"number": 46445, "title": "Revert \"Change definition of tensorflow::int64 to std::int64_t.\"", "body": "This reverts commit 50c714cde2c67b4c326cbec21d9383620ffbd32d.\r\n\r\nIt breaks tf serving build.", "comments": []}, {"number": 46443, "title": "An issue on cross-building Tensorflow Lite for Python", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 x86_64 amd64 PC\r\n- TensorFlow installed from (source or binary): v2.4\r\n- Tensorflow version (commit SHA if source): 582c8d2\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): RK3399 Ubuntu 18.04 aarch64\r\n\r\n**Describe the problem**\r\nI am going to get tflite whl for Python3 on aarch64.\r\nSo I tried to cross-build tflite whl on Ubuntu 18.04 x86_64 host PC.\r\nBut while compiling the source I met the following issue.\r\n\r\n![image](https://user-images.githubusercontent.com/47862419/104678949-87494880-5727-11eb-9b5d-850b3e81bf9d.png)\r\n\r\nHost PC environment:\r\nOS: Ubuntu 18.04 x86_64 AMD64\r\nnative gcc(g++): 7.5.0\r\ncross-compiler aarch64-linux-gnu-gcc(aarch64-linux-gnu-g++): 7.5.0\r\nbazel: 3.1.0\r\npython: virtual env python 3.7\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\nI followed the following steps on my host PC.\r\n\r\nsudo apt update\r\nsudo apt-get install software-properties-common\r\nsudo apt update\r\nsudo apt install git curl\r\nsudo apt install python3.7 python3.7-dev python3.7-venv python3.7-distutils\r\nsudo apt install mesa-common-dev libegl1-mesa-dev libgles2-mesa-dev\r\n\r\ncd ~\r\npython3.7 -m venv py37\r\nsource ~/py37/bin/activate\r\npip install cython\r\npip install wheel\r\npip install numpy\r\n\r\ngit clone -b r2.4 https://github.com/tensorflow/tensorflow.git tensorflow_r2.4\r\ncd tensorflow_r2.4\r\npython configure.py\r\n![image](https://user-images.githubusercontent.com/47862419/104679377-81079c00-5728-11eb-9f23-a9bd0a3f8ff9.png)\r\n\r\n./tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh aarch64\r\n\r\n", "comments": ["Maybe your repository is corrupted", "No\r\nI tried to find the file 'pyconfig.h' but there is NOT the file.\r\n\r\n![image](https://user-images.githubusercontent.com/47862419/104688998-bd44f780-573c-11eb-8df2-ed72a2327956.png)\r\n\r\n", "Open the new terminal & enter the command sudo apt-get install python-dev to get the latest default python-dev. ", "No, it is NOT", "The command \"sudo apt-get install python-dev\" will install the corresponding files in the folder \"/usr/include/x86_64-linux-gnu\" rather than \"/usr/include/aarch64-linux-gnu\".", "You need to use tensorflow/tools/ci_build/ci_build.sh to use pre-configured Python Docker container for Python.\r\n\r\nYou can find example commands from this page.\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/pip_package\r\n", "I meant the issue on the host PC rather than docker.", "Hi @rose-jinyang ,\r\n\r\nAs @terryheo mentioned, building TFLite with docker (but still on the host machine) will handle most of compatibility issues by making an isolated environment. Can you try that workflow, instead of building with host's cross-complie toolchain?\r\n", "https://www.tensorflow.org/lite/guide/build_cmake_pip might be helpful.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46443\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46443\">No</a>\n"]}, {"number": 46442, "title": "Can't link against TensorFlowLiteC.framework (iOS) when built on recent TF versions (regression)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OSX 11.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): master\r\n- Python version: 3.8.2\r\n- Bazel version (if compiling from source): 3.7.2 and 3.1.0\r\n- GCC/Compiler version (if compiling from source): clang 12.0.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nIt looks like there might be a regression in the build system responsible for generating the `TensorFlowLiteC.framework` iOS library. I've bisected it to 3e9fccb0cb9a, which is a little odd since that's just a bazel version bump... I'm using [am15h/tflite_flutter_plugin](https://github.com/am15h/tflite_flutter_plugin) (which binds directly to the TFLite C API) and can build my own `TensorFlowLiteC.framework` successfully and use it with that plugin up until the aforementioned commit.\r\n\r\nFrom 3e9fccb0cb9a forward (bisection script provided below) I get the following link errors:\r\n\r\n<details>\r\n  <summary>Build output</summary>\r\n\r\n```\r\n$ flutter build ios\r\nWarning: You are using these overridden dependencies:\r\n! tflite_flutter 0.5.0 from path external/tflite_flutter_plugin\r\nRunning \"flutter pub get\" in RoutespotterApp...                     0.7s\r\nBuilding com.mgalgs.routespotter for device (ios-release)...\r\nAutomatically signing iOS for device deployment using specified development team in Xcode project: PH2D8HJC83\r\nRunning pod install...                                              2.3s\r\nRunning Xcode build...\r\nXcode build done.                                            4.3s\r\nFailed to build iOS app\r\nError output from Xcode build:\r\n\u21b3\r\n    ** BUILD FAILED **\r\n\r\n\r\nXcode's output:\r\n\u21b3\r\n    Undefined symbols for architecture arm64:\r\n      \"tflite::tensor_utils::MatrixBatchVectorMultiplyAccumulate(signed char const*, int, int, signed char const*, float const*, int, int*, float*,\r\n      tflite::CpuBackendContext*)\", referenced from:\r\n          l4840 in TensorFlowLiteC\r\n      \"tflite::ops::builtin::Register_FLOOR()\", referenced from:\r\n          l4485 in TensorFlowLiteC\r\n      \"tflite::ops::custom::Register_MFCC()\", referenced from:\r\n          l4485 in TensorFlowLiteC\r\n      \"_xnn_reallocate\", referenced from:\r\n          l4440 in TensorFlowLiteC\r\n      \"_xnn_aligned_allocate\", referenced from:\r\n          l4440 in TensorFlowLiteC\r\n      \"_xnn_aligned_deallocate\", referenced from:\r\n          l4440 in TensorFlowLiteC\r\n      \"flatbuffers::EnsureDirExists(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)\", referenced from:\r\n          l3532 in TensorFlowLiteC\r\n      \"flatbuffers::FileExists(char const*)\", referenced from:\r\n          l2367 in TensorFlowLiteC\r\n      \"flatbuffers::LoadFile(char const*, bool, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >*)\", referenced from:\r\n          l2367 in TensorFlowLiteC\r\n      \"flatbuffers::AbsolutePath(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)\", referenced from:\r\n          l2254 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::VectorBatchVectorCwiseProductAccumulate(short const*, int, short const*, int, int, int, short*)\", referenced from:\r\n          l1730 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::VectorScalarMultiply(signed char const*, int, float, float*)\", referenced from:\r\n          l1729 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::VectorBatchVectorAdd(float const*, int, int, float*)\", referenced from:\r\n          l1728 in TensorFlowLiteC\r\n          l1729 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::CwiseClipping(short*, int, short)\", referenced from:\r\n          l1727 in TensorFlowLiteC\r\n          l1731 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::MatrixBatchVectorMultiply(signed char const*, int, signed char const*, int, int, int, int, int, signed char*, signed char)\", referenced from:\r\n          l1727 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::TwoGateSaturatingAdd(signed char const*, signed char, signed char const*, signed char, int, int, int, int, int, int, short*)\", referenced from:\r\n          l1727 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::ApplySigmoidFloat(short const*, int, int, short*)\", referenced from:\r\n          l1727 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::CwiseMul(short const*, short const*, int, int, int, short*)\", referenced from:\r\n          l1727 in TensorFlowLiteC\r\n          l1731 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::ApplyTanhFloat(short const*, int, int, int, short*)\", referenced from:\r\n          l1727 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::ApplyTanh(int, short const*, int, int, short*)\", referenced from:\r\n          l1726 in TensorFlowLiteC\r\n          l1730 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::CwiseMul(short const*, short const*, int, int, int, int, int, signed char*)\", referenced from:\r\n          l1726 in TensorFlowLiteC\r\n      \"flatbuffers::StripExtension(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)\", referenced from:\r\n          l814 in TensorFlowLiteC\r\n          l1889 in TensorFlowLiteC\r\n          l3564 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::MatrixBatchVectorMultiplyAccumulate(signed char const*, int const*, signed char const*, int, int, int, int, int, int, int*, signed char*,\r\n      tflite::CpuBackendContext*)\", referenced from:\r\n          l1726 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::CwiseClipping(float*, int, float)\", referenced from:\r\n          l1722 in TensorFlowLiteC\r\n          l1724 in TensorFlowLiteC\r\n      \"tflite::Subgraph::HasDelegates()\", referenced from:\r\n          l1472 in TensorFlowLiteC\r\n      \"tflite::Subgraph::IsCancelled()\", referenced from:\r\n          l1469 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::AsymmetricQuantizeFloats(float const*, int, signed char*, float*, int*)\", referenced from:\r\n          l278 in TensorFlowLiteC\r\n          l290 in TensorFlowLiteC\r\n          l414 in TensorFlowLiteC\r\n          l1254 in TensorFlowLiteC\r\n          l1417 in TensorFlowLiteC\r\n          l1724 in TensorFlowLiteC\r\n          l4108 in TensorFlowLiteC\r\n          ...\r\n      \"tflite::Subgraph::SetExecutionPlan(std::__1::vector<int, std::__1::allocator<int> > const&)\", referenced from:\r\n          l1465 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::ApplyLayerNorm(short const*, short const*, int const*, int, int, int, int, int, short*)\", referenced from:\r\n          l1726 in TensorFlowLiteC\r\n          l1730 in TensorFlowLiteC\r\n      \"_xnn_deallocate\", referenced from:\r\n          l4440 in TensorFlowLiteC\r\n      \"tflite::Subgraph::SetTensorParametersReadWrite(int, TfLiteType, char const*, unsigned long, int const*, TfLiteQuantization, bool, unsigned long, int const*)\",\r\n      referenced from:\r\n          l1462 in TensorFlowLiteC\r\n          l1464 in TensorFlowLiteC\r\n          l3451 in TensorFlowLiteC\r\n      \"tflite::Subgraph::SetTensorParametersReadOnly(int, TfLiteType, char const*, unsigned long, int const*, TfLiteQuantization, char const*, unsigned long,\r\n      tflite::Allocation const*, TfLiteSparsity*)\", referenced from:\r\n          l1461 in TensorFlowLiteC\r\n          l1463 in TensorFlowLiteC\r\n          l3451 in TensorFlowLiteC\r\n      \"tflite::Subgraph::AddTensors(int, int*)\", referenced from:\r\n          l1459 in TensorFlowLiteC\r\n          l3454 in TensorFlowLiteC\r\n      \"flatbuffers::SaveFile(char const*, char const*, unsigned long, bool)\", referenced from:\r\n          l812 in TensorFlowLiteC\r\n          l1892 in TensorFlowLiteC\r\n          l3563 in TensorFlowLiteC\r\n      \"tflite::Subgraph::AddNodeWithParameters(std::__1::vector<int, std::__1::allocator<int> > const&, std::__1::vector<int, std::__1::allocator<int> > const&,\r\n      std::__1::vector<int, std::__1::allocator<int> > const&, char const*, unsigned long, void*, TfLiteRegistration const*, int*)\", referenced from:\r\n          l1454 in TensorFlowLiteC\r\n          l3443 in TensorFlowLiteC\r\n      \"tflite::Subgraph::Invoke()\", referenced from:\r\n          l1458 in TensorFlowLiteC\r\n          l3008 in TensorFlowLiteC\r\n          l4742 in TensorFlowLiteC\r\n          l5015 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::Sub1Vector(short const*, int, short*)\", referenced from:\r\n          l1727 in TensorFlowLiteC\r\n          l1731 in TensorFlowLiteC\r\n      \"tflite::Subgraph::ResetVariableTensors()\", referenced from:\r\n          l1460 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::ApplySigmoid(short const*, int, int, short*)\", referenced from:\r\n          l1730 in TensorFlowLiteC\r\n      \"flatbuffers::StripPath(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)\", referenced from:\r\n          l814 in TensorFlowLiteC\r\n          l1889 in TensorFlowLiteC\r\n          l3564 in TensorFlowLiteC\r\n      \"tflite::Subgraph::ReleaseNonPersistentMemory()\", referenced from:\r\n          l1457 in TensorFlowLiteC\r\n          l5015 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::SparseMatrixBatchVectorMultiplyAccumulate1x4(float const*, int const*, int const*, int, int, float const*, int, float*)\", referenced from:\r\n          l1282 in TensorFlowLiteC\r\n          l1286 in TensorFlowLiteC\r\n      \"tflite::Subgraph::ResizeInputTensorStrict(int, std::__1::vector<int, std::__1::allocator<int> > const&)\", referenced from:\r\n          l1456 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::SparseMatrixBatchVectorMultiplyAccumulate(signed char const*, unsigned char const*, int, int, signed char const*, float const*, int, float*)\",\r\n      referenced from:\r\n          l1724 in TensorFlowLiteC\r\n          l1729 in TensorFlowLiteC\r\n      \"tflite::Subgraph::ResizeInputTensor(int, std::__1::vector<int, std::__1::allocator<int> > const&)\", referenced from:\r\n          l1455 in TensorFlowLiteC\r\n          l3007 in TensorFlowLiteC\r\n          l3009 in TensorFlowLiteC\r\n          l4741 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::MatrixBatchVectorMultiply(short const*, signed char const*, int, int, int const*, int, int, int, int, signed char*)\", referenced from:\r\n          l1727 in TensorFlowLiteC\r\n      \"tflite::Subgraph::ReserveNodes(int)\", referenced from:\r\n          l1452 in TensorFlowLiteC\r\n          l3443 in TensorFlowLiteC\r\n      \"tflite::Subgraph::SetExternalContext(TfLiteExternalContextType, TfLiteExternalContext*)\", referenced from:\r\n          l1445 in TensorFlowLiteC\r\n      \"tflite::Subgraph::SetCustomAllocationForTensor(int, TfLiteCustomAllocation const&)\", referenced from:\r\n          l1446 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::MatrixBatchVectorMultiplyAccumulate(float const*, int, int, float const*, int, float*)\", referenced from:\r\n          l1252 in TensorFlowLiteC\r\n          l1416 in TensorFlowLiteC\r\n          l1722 in TensorFlowLiteC\r\n          l1728 in TensorFlowLiteC\r\n          l4106 in TensorFlowLiteC\r\n      \"tflite::Subgraph::RemoveAllDelegates()\", referenced from:\r\n          l1451 in TensorFlowLiteC\r\n          l1470 in TensorFlowLiteC\r\n          l1471 in TensorFlowLiteC\r\n      \"tflite::Subgraph::SetVariables(std::__1::vector<int, std::__1::allocator<int> >)\", referenced from:\r\n          l1449 in TensorFlowLiteC\r\n          l3454 in TensorFlowLiteC\r\n      \"flatbuffers::ConCatPathFileName(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char,\r\n      std::__1::char_traits<char>, std::__1::allocator<char> > const&)\", referenced from:\r\n          l2367 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::ReductionSumVector(signed char const*, int*, int, int)\", referenced from:\r\n          l1724 in TensorFlowLiteC\r\n          l4108 in TensorFlowLiteC\r\n          l4109 in TensorFlowLiteC\r\n          l4554 in TensorFlowLiteC\r\n          l4864 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::CwiseAdd(short const*, short const*, int, int, short*)\", referenced from:\r\n          l1727 in TensorFlowLiteC\r\n          l1731 in TensorFlowLiteC\r\n      \"tflite::Subgraph::SetOutputs(std::__1::vector<int, std::__1::allocator<int> >)\", referenced from:\r\n          l1448 in TensorFlowLiteC\r\n          l3454 in TensorFlowLiteC\r\n      \"tflite::Subgraph::Subgraph(tflite::ErrorReporter*, TfLiteExternalContext**, std::__1::vector<std::__1::unique_ptr<tflite::Subgraph,\r\n      std::__1::default_delete<tflite::Subgraph> >, std::__1::allocator<std::__1::unique_ptr<tflite::Subgraph, std::__1::default_delete<tflite::Subgraph> > > >*,\r\n      std::__1::unordered_map<int, std::__1::unique_ptr<tflite::resource::ResourceBase, std::__1::default_delete<tflite::resource::ResourceBase> >, std::__1::hash<int>,\r\n      std::__1::equal_to<int>, std::__1::allocator<std::__1::pair<int const, std::__1::unique_ptr<tflite::resource::ResourceBase,\r\n      std::__1::default_delete<tflite::resource::ResourceBase> > > > >*)\", referenced from:\r\n          l1441 in TensorFlowLiteC\r\n      \"flatbuffers::PosixPath(char const*)\", referenced from:\r\n          l2367 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::CwiseClipping(signed char*, int, signed char)\", referenced from:\r\n          l1726 in TensorFlowLiteC\r\n          l1727 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::Sub1Vector(float const*, int, float*)\", referenced from:\r\n          l1722 in TensorFlowLiteC\r\n          l1724 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::BatchVectorBatchVectorDotProduct(short const*, short const*, int, int, int*)\", referenced from:\r\n          l1418 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::MatrixBatchVectorMultiplyAccumulate(signed char const*, int, int, signed char const*, float const*, int, float*, float const*, int const*,\r\n      int*, int*, bool*, tflite::CpuBackendContext*)\", referenced from:\r\n          l1254 in TensorFlowLiteC\r\n          l1417 in TensorFlowLiteC\r\n          l1724 in TensorFlowLiteC\r\n          l1729 in TensorFlowLiteC\r\n          l4108 in TensorFlowLiteC\r\n      \"tflite::Subgraph::SetCancellationFunction(void*, bool (*)(void*))\", referenced from:\r\n          l1468 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::ReductionSumVector(int const*, int*, int, int)\", referenced from:\r\n          l1418 in TensorFlowLiteC\r\n      \"_xnn_allocate\", referenced from:\r\n          l4440 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::MeanStddevNormalization(float const*, float*, int, int)\", referenced from:\r\n          l1728 in TensorFlowLiteC\r\n          l1729 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::VectorVectorDotProduct(float const*, float const*, int)\", referenced from:\r\n          l1416 in TensorFlowLiteC\r\n          l1417 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::IsZeroVector(float const*, int)\", referenced from:\r\n          l1254 in TensorFlowLiteC\r\n          l1417 in TensorFlowLiteC\r\n          l1722 in TensorFlowLiteC\r\n          l1724 in TensorFlowLiteC\r\n          l4108 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::ApplyLayerNormFloat(short const*, short const*, int, int, int const*, int, int, short*)\", referenced from:\r\n          l1727 in TensorFlowLiteC\r\n      \"tflite::Subgraph::ModifyGraphWithDelegate(TfLiteDelegate*)\", referenced from:\r\n          l1451 in TensorFlowLiteC\r\n          l1470 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::MatrixScalarMultiplyAccumulate(signed char const*, int, int, int, int*)\", referenced from:\r\n          l764 in TensorFlowLiteC\r\n          l1981 in TensorFlowLiteC\r\n      \"tflite::Subgraph::SetInputs(std::__1::vector<int, std::__1::allocator<int> >)\", referenced from:\r\n          l1447 in TensorFlowLiteC\r\n          l3454 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::ReductionSumVector(float const*, float*, int, int)\", referenced from:\r\n          l1416 in TensorFlowLiteC\r\n          l1417 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::MatrixBatchVectorMultiplyAccumulate(signed char const*, int const*, signed char const*, int, int, int, int, int, int, int*, short*,\r\n      tflite::CpuBackendContext*)\", referenced from:\r\n          l1726 in TensorFlowLiteC\r\n          l1730 in TensorFlowLiteC\r\n      \"tflite::tensor_utils::SymmetricQuantizeFloats(float const*, int, signed char*, float*, float*, float*)\", referenced from:\r\n          l1254 in TensorFlowLiteC\r\n          l1417 in TensorFlowLiteC\r\n          l1724 in TensorFlowLiteC\r\n          l4108 in TensorFlowLiteC\r\n          l4542 in TensorFlowLiteC\r\n          l4551 in TensorFlowLiteC\r\n          l4837 in TensorFlowLiteC\r\n          ...\r\n      \"tflite::Subgraph::AllocateTensors()\", referenced from:\r\n          l1450 in TensorFlowLiteC\r\n          l3006 in TensorFlowLiteC\r\n          l3008 in TensorFlowLiteC\r\n          l4741 in TensorFlowLiteC\r\n          l5015 in TensorFlowLiteC\r\n    ld: symbol(s) not found for architecture arm64\r\n    clang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n    note: Using new build system\r\n    note: Building targets in parallel\r\n    note: Planning build\r\n    note: Constructing build description\r\n    warning: The iOS deployment target 'IPHONEOS_DEPLOYMENT_TARGET' is set to 8.0, but the range of supported deployment target versions is 9.0 to 14.3.99. (in target\r\n    'url_launcher' from project 'Pods')\r\n    warning: The iOS deployment target 'IPHONEOS_DEPLOYMENT_TARGET' is set to 8.0, but the range of supported deployment target versions is 9.0 to 14.3.99. (in target\r\n    'video_player' from project 'Pods')\r\n    warning: The iOS deployment target 'IPHONEOS_DEPLOYMENT_TARGET' is set to 8.0, but the range of supported deployment target versions is 9.0 to 14.3.99. (in target\r\n    'shared_preferences' from project 'Pods')\r\n    warning: The iOS deployment target 'IPHONEOS_DEPLOYMENT_TARGET' is set to 8.0, but the range of supported deployment target versions is 9.0 to 14.3.99. (in target\r\n    'path_provider' from project 'Pods')\r\n    warning: The iOS deployment target 'IPHONEOS_DEPLOYMENT_TARGET' is set to 8.0, but the range of supported deployment target versions is 9.0 to 14.3.99. (in target\r\n    'package_info' from project 'Pods')\r\n    warning: The iOS deployment target 'IPHONEOS_DEPLOYMENT_TARGET' is set to 8.0, but the range of supported deployment target versions is 9.0 to 14.3.99. (in target\r\n    'image_picker' from project 'Pods')\r\n    warning: The iOS deployment target 'IPHONEOS_DEPLOYMENT_TARGET' is set to 8.0, but the range of supported deployment target versions is 9.0 to 14.3.99. (in target\r\n    'flutter_isolate' from project 'Pods')\r\n    warning: The iOS deployment target 'IPHONEOS_DEPLOYMENT_TARGET' is set to 8.0, but the range of supported deployment target versions is 9.0 to 14.3.99. (in target\r\n    'camera' from project 'Pods')\r\n    warning: The iOS deployment target 'IPHONEOS_DEPLOYMENT_TARGET' is set to 8.0, but the range of supported deployment target versions is 9.0 to 14.3.99. (in target\r\n    'Flutter' from project 'Pods')\r\n\r\nEncountered error while building for device.\r\n```\r\n</details>\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect to be able to build a `TensorFlowLiteC.framework` and link against it from a Flutter application.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nThese steps must be run on an OSX machine.\r\n\r\nIf needed, install bazel versions 3.1.0 *and* 3.7.2:\r\n\r\n```\r\n(cd \"~/.bazel/bin\" && curl -fLO https://releases.bazel.build/3.1.0/release/bazel-3.1.0-darwin-x86_64 && chmod +x bazel-3.1.0-darwin-x86_64)\r\n(cd \"~/.bazel/bin\" && curl -fLO https://releases.bazel.build/3.7.2/release/bazel-3.7.2-darwin-x86_64 && chmod +x bazel-3.7.2-darwin-x86_64)\r\n```\r\n\r\nDownload [this gist](https://gist.github.com/mgalgs/3a79a6aa4b9ca237e0bea047d2326550) to a `$WORKSPACE` of your choosing, then:\r\n\r\n```\r\ncd $WORKSPACE\r\ngit clone --recurse-submodules https://github.com/mgalgs/object_detection_flutter.git\r\ncd <tensorflow_dir>\r\n./configure  # answer yes when it asks about iOS support\r\n\r\n# Checkout the breaker and you should see a build failure in our test app.\r\ngit checkout 3e9fccb0cb9a5f55a4e67e3011c20ff31c3cce67\r\nAPPDIR=$WORKSPACE/object_detection_flutter $WORKSPACE/tflite_ios_bisect.sh\r\n\r\n# Now checkout the parent of the breaking commit and build again. Should succeed.\r\ngit checkout 3e9fccb0cb9a5f55a4e67e3011c20ff31c3cce67~\r\nAPPDIR=$WORKSPACE/object_detection_flutter $WORKSPACE/tflite_ios_bisect.sh\r\n```\r\n\r\nYou can also run `tflite_ios_bisect.sh` with `git bisect` as documented at the top of the file. Theoretically you should land on the same breaking commit that I've indicated above...\r\n\r\n**Other info / logs**\r\n\r\nRelevant discussion at `tflite_flutter_plugin`: https://github.com/am15h/tflite_flutter_plugin/issues/64", "comments": ["I've identified the same issue and fixed it in dff3c8a47a24b27f3e490f15f4de90743efc7ecf.\r\nCan you sync your project after this commit and try again?", "@yyoon that appears to have fixed it. Thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46442\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46442\">No</a>\n"]}, {"number": 46441, "title": "explicit ValueError for built-in variations", "body": "The issue arises in the specific case when a string identifier is passed to the \"metrics\" parameter for model.compile(), which is callable but aimed at one of these metrics: `['accuracy', 'acc', 'crossentropy', 'ce']`. \r\n\r\n**Example : \"Accuracy\" instead of \"accuracy\" :**\r\n\r\nIt seems that when \"Accuracy\" is passed as a string (instead of \"accuracy\" lowercase) to the metric parameter, the compile deserializes it to get `tf.keras.metrics.Accuracy` directly instead of fetching `tf.keras.metrics.MeanMetricWrapper` as defined in `_get_metric_object()` from compile_utils. Therefore, the accuracy metric is still visible during Epochs but incorrectly calculated (always equal to 0) without throwing any warnings/errors. The ideal state should be to throw `ValueError` instead of failing silently.\r\n\r\nI have added an explicit condition that checks if `str.lower()` is present in the list `['accuracy', 'acc', 'crossentropy', 'ce']`. **This list is explicitly conditionally handled** in the `_get_metric_object()` from the [tf.python.keras.engine.compile_utils.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/compile_utils.py/#L441-L444).\r\n\r\n-----------\r\n\r\nA detailed description of the issue is available [here](https://github.com/tensorflow/tensorflow/issues/46436).", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46441) for more info**.\n\n<!-- need_sender_cla -->", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46441) for more info**.\n\n<!-- need_sender_cla -->", "Contributors, requesting a code review thanks!", "Hi, Is any more information needed from my side? Awaiting your response eagerly @gbaned, thanks!", "Sorry, forgot to press enter when I wrote up a response previously!  Thanks for bringing this to our attention! We're going to try taking a closer at this since it highlights some rough edges around our serialization mechanisms and it's not immediately obvious this PR is the right thing to do.", "@tomerk, thanks! I have detailed out the trace for the bug in this issue https://github.com/tensorflow/tensorflow/issues/46436.. This describes why and how the bug is occurring and I have primarily made a PR for handling the issue. I would love to discuss and help fix it at a deeper level and update the PR if required. Do feel free to involve me, would love to contribute! Thanks!", "@Akshaysehgal2005 Can you please resolve conflicts? Thanks!", "@Akshaysehgal2005 Any update on this PR? Please. Thanks!", "Apologies, will resolve it in a day, has been a bit crazy in my country due to the Covid situation. Apologies again @gbaned ", "@Akshaysehgal2005 , as was mentioned in the discussion earlier, we are investigating deeper solutions to this bug at the moment. If you are interested in a deeper fix at the root cause (serialization/deserialization) please start a new PR on https://github.com/keras-team/keras "]}, {"number": 46440, "title": "Remove 'Z' because it's a local time", "body": "Unless I'm mistaken, the 'Z' indicates 0-offset UTC time. Since this is using the local time, the Z is incorrect/misleading.", "comments": []}, {"number": 46439, "title": "[Cherrypick:r2.4] Install bazelisk in `docker_cpu_pip.sh`", "body": "Since caller scripts use Bazelisk, the docker job is broken if we want to use a bazel that is no longer the default one.\n\nPiperOrigin-RevId: 351891433\nChange-Id: I6caf0b5940934a3a90737f7fae288c42953a23f1", "comments": []}, {"number": 46438, "title": "Move schema version into micro_interpreter.h", "body": "This is a first (somewhat non-intuitive) step towards being able to use clang as part of the github CI system.\r\n\r\nThe benefits of this refactor are that we avoid a dependency into tensorflow/core which reduces the number of files that need to be downloaded as part of a bazel build from a TFLM CI docker image.\r\n\r\nThe tflite schema version has been unchanged since at-least Oct 2018 (when tflite was moved out of tensorflow/contrib).\r\n\r\nProgress towards #46465\r\nProgress towards http://b/177672856", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 46437, "title": "cudart64_101.dll not found", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.3\r\n- Python version: 3.8.5\r\n- Installed using virtualenv? pip? conda?: Conda\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: 10.2 and 7.6.5\r\n- GPU model and memory: RTX 2070 Super\r\n\r\n**Describe the problem**\r\nI am trying to install TensorFlow and be able to use it on my computer. I am only writing \"import tensorflow\" in my python file and getting several errors. I am using conda and created a new virtual environment and downloaded tensorflow and tensorboard. I also downloaded the 10.1 file and put it in my bin folder but still getting the same error.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\npython\r\nimport tensorflow\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nHere is the error that I am currently getting:\r\n tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n2021-01-14 14:04:25.725481: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n", "comments": [" Find the directory of dlls  and update your PATH to include that directory.\r\nIf that doesnt work try miniconda:\r\nconda create --name .conda_venv\r\nconda activate .conda_venv\r\nconda install tensorflow-gpu", "@JasonKramer525 \r\nHave you set in PATH\r\nSET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11\\bin;%PATH%\r\n\r\nPlease verify if you have refered to  https://www.tensorflow.org/install/source#gpu, also please check resolved issue with ame error: [link](https://stackoverflow.com/questions/59823283/could-not-load-dynamic-library-cudart64-101-dll-on-tensorflow-cpu-only-install), #43193, #36111", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46437\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46437\">No</a>\n", "I had the same issue, copying cudart64_102.dll and renaming it to cudart64_101.dll solved it for me. And now tf.test.is_gpu_available() returns True ."]}, {"number": 46436, "title": "Passing \"Accuracy\" to model.compile() parameter \"metrics\" (instead of \"accuracy\") returns 0 / epoch without throwing an error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Big Sur 11.0.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n- Python version: Python 3.8.3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Current Behavior**\r\nIt seems that when \"Accuracy\" is passed as a string (instead of \"accuracy\" lowercase) to the metric parameter, the compile deserializes it to get  `tf.keras.metrics.Accuracy` directly instead of fetching `tf.keras.metrics.MeanMetricWrapper` as defined in `_get_metric_object()` from compile_utils. Therefore, the accuracy metric is still visible during Epochs but incorrectly calculated  (always equal to 0) without throwing any warnings/errors.\r\n\r\n**Expected Behavior**\r\nPassing \"Accuracy\" instead of \"accuracy\" should result in a ValueError instead of failing silently and returning 0 value for each epoch. \r\n\r\n\r\n**Reproducable code**\r\nFor scenario 1 (correct case)- \r\n\r\n```python\r\n#Scenario 1: Lower case accuracy (accuracy displayed is proper)\r\n\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\nX_train = np.random.random((100,8))\r\ny_train = np.random.randint(0,2,(100,))\r\n\r\nmodel = keras.Sequential(\r\n    [\r\n     layers.Dense(10,activation=\"relu\",input_shape=(8,)),\r\n     layers.Dense(10,activation=\"relu\"),\r\n     layers.Dense(1,activation=\"sigmoid\")\r\n    ]\r\n)\r\n\r\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\r\nmodel.fit(X_train,y_train,batch_size=64,epochs=2,verbose=2)\r\nprint(model.metrics)\r\n\r\n# Epoch 1/2\r\n# 2/2 - 0s - loss: 0.6900 - accuracy: 0.4900\r\n# Epoch 2/2\r\n# 2/2 - 0s - loss: 0.6892 - accuracy: 0.5000   #<------\r\n        \r\n# [<tensorflow.python.keras.metrics.Mean object at 0x7f90e6626d90>, \r\n#  <tensorflow.python.keras.metrics.MeanMetricWrapper object at 0x7f90e5ac73a0>]  #<------\r\n```\r\n\r\nAnd scenario 2 (Incorrect case)- \r\n\r\n```python\r\n#Scenario 2: Uppercase case Accuracy (accuracy displayed / epoch always 0)\r\n\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\nX_train = np.random.random((100,8))\r\ny_train = np.random.randint(0,2,(100,))\r\n\r\nmodel = keras.Sequential(\r\n    [\r\n     layers.Dense(10,activation=\"relu\",input_shape=(8,)),\r\n     layers.Dense(10,activation=\"relu\"),\r\n     layers.Dense(1,activation=\"sigmoid\")\r\n    ]\r\n)\r\n\r\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['Accuracy'])\r\nmodel.fit(X_train,y_train,batch_size=64,epochs=2,verbose=2)\r\nprint(model.metrics)\r\n\r\n# Epoch 1/2\r\n# 2/2 - 0s - loss: 0.7021 - accuracy: 0.0000e+00\r\n# Epoch 2/2\r\n# 2/2 - 0s - loss: 0.6999 - accuracy: 0.0000e+00   #<------\r\n        \r\n# [<tensorflow.python.keras.metrics.Mean object at 0x7f90c80e4bb0>, \r\n#  <tensorflow.python.keras.metrics.Accuracy object at 0x7f90ca039a30>]  #<------\r\n```\r\n\r\nI don't think this was intentional, else it should have thrown a ValueError. I have described the details on this SO post.\r\n", "comments": ["(Please refer to my [SO answer](https://stackoverflow.com/questions/65722752/neural-network-accuracy-is-always-0-while-training-classification-problem-in-ker/65722998#65722998) for more details)\r\n\r\nI traced the implementation for compile to find out the reason for this issue. Here are the steps as per my understanding followed by the reason. (Code line links inline)\r\n\r\n1. During compile, when a metric is passed, the parameter is stored in a [MetricsContainer][2] object.\r\n2. This container class then calls a function called [_get_metric_object][3] from `compile_utils.py`. The job of this function is to take the input and return a metric object of that metric's class.\r\n3. One of the first things this function does is to [check][4] if the input belongs to list `['accuracy', 'acc', 'crossentropy', 'ce']` or not.\r\n    - If `YES`, then it directly fetches the classes from the metrics.py and [calls][5] the `MeanMetricWrapper` class. The [job of this class][6] is to wrap a stateless metric function with the Mean metric. This calculates the mean of the metric you have added.\r\n    - If `NO`, then it calls the [get function][7] from metrics.py. The `get` function further calls a [deserialize function][8] whose job is call a function call [deserialize_keras_object][9] function from `utils.generic_utils.py`. This function's job is to take the string and retrieve the actual object.\r\n\r\nLet's see the 2 scenarios now.\r\n\r\n### Scenario 1: \"accuracy\"\r\n\r\n```python\r\n#With lower case accuracy\r\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\r\nprint(model.metrics)\r\n```\r\n```\r\n[<tensorflow.python.keras.metrics.Mean at 0x7f90c7ea10d0>,\r\n <tensorflow.python.keras.metrics.MeanMetricWrapper at 0x7f90c7d07e20>]\r\n```\r\n\r\nSince the metric provided belongs to the `['accuracy', 'acc', 'crossentropy', 'ce']`, the `_get_metric_object` function fetches the `tf.keras.metrics.Accuracy` class and explicitly passes it to the `tf.keras.metrics.MeanMetricWrapper`. This calculates the mean accuracy as expected.\r\n\r\n### Scenario 2: \"Accuracy\"\r\n\r\n```python\r\n#With upper case Accuracy\r\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['Accuracy'])\r\nprint(model.metrics)\r\n```\r\n```\r\n[<tensorflow.python.keras.metrics.Mean at 0x7f90e7285e20>,\r\n <tensorflow.python.keras.metrics.Accuracy at 0x7f90e72fceb0>]\r\n```\r\n\r\nSince the \"Accuracy\" doesnt belong to the list, `_get_metric_object` calls the `metrics.get()` ->`metrics.deserialize()`-> `generic_utils.deserialize_keras_object()` function which simply pulls up the `tf.keras.metrics.Accuracy` and returns that directly, instead of calling `tf.keras.metrics.MeanMetricWrapper`.\r\n\r\nThis is why you get incorrect values for accuracy, but it does not throw an error.\r\n\r\n\r\n  [1]: https://github.com/tensorflow/tensorflow/issues/46436\r\n  [2]: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/compile_utils.py/#L283-L286\r\n  [3]: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/compile_utils.py/#L441-L444\r\n  [4]: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/compile_utils.py/#L458-L459\r\n  [5]: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/compile_utils.py/#L460-L497\r\n  [6]: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/metrics.py/#L599-L609\r\n  [7]: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/metrics.py#L3530-L3571\r\n  [8]: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/metrics.py#L3511-L3526\r\n  [9]: https://github.com/tensorflow/tensorflow/blob/9cbbc16a2ff969643bbbe474c93430917091ae9b/tensorflow/python/keras/utils/generic_utils.py#L340-L344", "I have tried in colab with TF 2.3, 2.4 gist [here](https://colab.research.google.com/gist/ravikyram/bed7c74d6a29378fe3947a08af3255f2/untitled619.ipynb) anf TF nightly version(`2.5.0-dev20210114` gist [here](https://colab.research.google.com/gist/ravikyram/8f76a13cf5ba44de0224993ca5d57722/untitled620.ipynb#scrollTo=hnEoHTmJbh02) and was able to reproduce the issue. Thanks!", "Hi, I have made a pull request with a potential solution to this. https://github.com/tensorflow/tensorflow/pull/46441. Do check. Thanks!", "Hi, is anything needed from my side? Awaiting your response @jvishnuvardhan eagerly. Thanks!", "@jvishnuvardhan any update on this? is anything needed from my side? thanks!", "Hi, Now it produces accuracy for both `metrics=['accuracy'] `and `metrics=['Accuracy']`, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/7e1091663e53f045096817b81461eab1/untitled619.ipynb#scrollTo=DkmS3e3uZOuc) and conform the same. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46436\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46436\">No</a>\n"]}, {"number": 46435, "title": "fix offset datatype for philox random kernel", "body": "The offset parameter in of one of \"FillPhiloxRandomKernel\" template specialization as well as the \"params_i\" in ScatterOpCustomKernel, data types have been set to int32. This is troublesome for tensors of size bigger than 2^32. The issue is significant since initialization of many layers by default end up in this particular implementation and as a result, we cannot have a Dense layer larger than 8GB of data. Simplest replication of the issue:\r\n`import tensorflow as tf;\r\ntf.random.uniform(shape=[256, 8493466], maxval=3, dtype=tf.float32, seed=10)`\r\n", "comments": []}, {"number": 46434, "title": "enable bazel builds on CI.", "body": "", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "closing this in favor of https://github.com/tensorflow/tensorflow/pull/46467"]}, {"number": 46433, "title": "[Cherrypick:r2.4] Pass cc_opt_flags to host_copt", "body": "", "comments": []}, {"number": 46432, "title": "Missing include files in libtensorflow-cpu-windows-x86_64-2.4.0.zip", "body": "In the page https://www.tensorflow.org/install/lang_c there are links to two zip files for the Windows installation of the TensorFlow C API. \r\n\r\nThese Windows zips: libtensorflow-cpu-windows-x86_64-2.4.0.zip and libtensorflow-gpu-windows-x86_64-2.4.0.zip are missing some of the includes:\r\n```\r\n.\\include\\tensorflow\\c\\c_api_experimental.h\r\n.\\include\\tensorflow\\c\\c_api_macros.h\r\n.\\include\\tensorflow\\c\\tensor_interface.h\r\n.\\include\\tensorflow\\c\\tf_file_statistics.h\r\n.\\include\\tensorflow\\c\\tf_tstring.h\r\n```\r\n\r\nThis means that using this zip files (and unlike with the zip files of 2.3.1) you cannot build.\r\n\r\nThanks!", "comments": ["I ran into this issue and figured out a workaround. I downloaded the source for TensorFlow 2.4.0 and copied missing header files to the C API folder.\r\n\r\nI copied these files:\r\n\r\n* tensorflow-2.4.0/tensorflow/c/c_api_macros.h\r\n* tensorflow-2.4.0/tensorflow/c/tf_tstring.h\r\n* tensorflow-2.4.0/tensorflow/core/platform/ctstring.h\r\n* tensorflow-2.4.0/tensorflow/core/platform/ctstring_internal.h\r\n\r\nAnd copied them here:\r\n\r\n* libtensorflow-gpu-windows-x86_64-2.4.0/include/tensorflow/c/c_api_macros.h\r\n* libtensorflow-gpu-windows-x86_64-2.4.0/include/tensorflow/c/tf_tstring.h\r\n* libtensorflow-gpu-windows-x86_64-2.4.0/include/tensorflow/core/platform/ctstring.h\r\n* libtensorflow-gpu-windows-x86_64-2.4.0/include/tensorflow/core/platform/ctstring_internal.h\r\n\r\nThe other header files listed in the original report don't seem to be necessary.", "@shoelzer - indeed. I have also created for myself a separate zip file as a workaround. However, obviously, it will be nice if it is fixed to include the correct files.\r\n\r\nAnother related issue, which I am not sure deserves a separate entry, is that in the page https://www.tensorflow.org/install/lang_c there is a reference to the nightly builts page. However, that page does not contain the nightly builts for Windows. This should also be fixed because in the README.md (https://github.com/tensorflow/tensorflow) there is a \"promise\" that the nightly builds are available also for Windows (last two rows in the table https://github.com/tensorflow/tensorflow#official-builds).\r\n", "@rani-pinchuk Yes, I think the missing Windows nightlies are worth a separate issue so I made one: #46538.", "You may try  TF 2.4.1 package, I see those files are covered in the package https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-2.4.1.zip\r\nAlso TF 2.5 was recently released which includes it as well.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46432\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46432\">No</a>\n"]}]