[{"number": 51906, "title": "Cannot find tensorflow 2.2.0rc4 as available option with pip install", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.9.5 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version: 2.2.0rc4\r\n- Python version: 3.6.4\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: \r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI am installing a programme which requires a conda env with tensorflow 2.2.0rc4. When I do this it says no matching distribution can be found.\r\n\r\nERROR: Could not find a version that satisfies the requirement tensorflow==2.2.0rc4 (from versions: 1.15.4, 1.15.5, 2.0.1, 2.0.3, 2.1.2, 2.2.1, 2.3.1)\r\nERROR: No matching distribution found for tensorflow==2.2.0rc4\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\npip install tensorflow==2.2.0rc4\r\n\r\n\r\nHow can I install this version?", "comments": ["Should mention pip is 21.2.4 (latest)\r\n", "Hi @bulgarianboy , In that  case ,select from suggested versions i.e  2.2.1 .But Minimum MacOS requirement is 10.12.6 or later . Detail instructions are here in this[ link](https://www.tensorflow.org/install/pip#macos) .", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51906\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51906\">No</a>\n"]}, {"number": 51905, "title": "fix Windows MSVC debug build", "body": "previous fix in #42676 using partial dummy template specializations (which are not needed but without optimization the compiler doesn't find out about it) stopped working, thus fix using `constexpr`, which is a better fix anyway", "comments": ["@jgehw  Can you please check @cantonios's comments and keep us posted ? Thanks!", "So now this MR looks like it essentially un-does the changes in #42676, part (2):\r\n- add partial dummy template specializations (which are not needed but without optimization the compiler doesn't find out about it)\r\n\r\nWas that never needed, or will removing these lead to issues again?", "> Was that never needed, or will removing these lead to issues again?\r\n\r\nIt was needed to build the debug build using MSVC because without `if constexpr` the MSVC compiler wasn't able to build the CPU-only (non-GPU) version of tensorflow. However, meanwhile this dummy template work-around doesn't help anymore because MSVC now generates linking errors related to the dummy template (i.e. some evolution either in tensorflow or MSVC broke this once-working work-around and made it useless; so we can only wait for C++17 to apply the `if constexpr` fix)."]}, {"number": 51904, "title": "Convert Tensorflow functions to Tensorflow Lite functions", "body": "### 1. System information\r\n\r\n- Linux\r\n- TensorFlow version: 2.5.0, also TFLite 2..5.0\r\n\r\n### 2. Code\r\n`    masks = tf.image.crop_and_resize(tf.cast(roi_masks, tf.float32), boxes, box_ids, config.MASK_SHAPE)`\r\n\r\nHi there,\r\nI am currently trying to convert the Mask R CNN model from Matterport/Leekunhee (https://github.com/leekunhee/Mask_RCNN) to Tensorflow Lite. I fixed the problem temporarily by allowing Select Ops for the tensorflow lite interpreter. Now I need to speed up the entire process of inference. If I remember correctly last time I got this error message:\r\n\r\n> RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 275 (FlexCropAndResize) failed to prepare.\r\n\r\nCan anybody help me translate this one line of code to something the tflite Interpreter and converter will accept? Any workaround is appreciated.\r\n\r\nThank you all for reading.", "comments": ["Okay, so I wanted to try on Tf 2.6 and tf nightly and this is what I found: \r\n\r\n```\r\nkeraModel = model.keras_model\r\n\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(keraModel)\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS#, # enable TensorFlow Lite ops.\r\n]\r\n\r\nconverter.optimizations = [ tf.lite.Optimize.DEFAULT ]\r\n\r\ntflite_model = converter.convert()\r\n\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\n```\r\nAnd this is the error message:\r\n\r\n> ConverterError: /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:552:0: error: 'tf.TensorListReserve' op requires element_shape to be 1D tensor during TF Lite transformation pass\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n/home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:552:0: error: failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: Lowering tensor list ops is failed. Please consider using Select TF ops and disabling `_experimental_lower_tensor_list_ops` flag in the TFLite converter object. For example, converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\\n converter._experimental_lower_tensor_list_ops = False\r\n\r\n", "And after I disabled \"converter._experimental_lower_tensor_list_ops = False\", the code looks like this:\r\n\r\n```\r\nkeraModel = model.keras_model\r\n\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(keraModel)\r\n#converter.allow_custom_ops = True\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS#, # enable TensorFlow Lite ops.\r\n    #tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n]\r\n\r\nconverter.optimizations = [ tf.lite.Optimize.DEFAULT ]\r\nconverter._experimental_lower_tensor_list_ops = False\r\n\r\ntflite_model = converter.convert()\r\n\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\n```\r\n\r\nMy new error message is this:\r\n\r\n> ConverterError                            Traceback (most recent call last)\r\n> <ipython-input-14-d2e1b8b44f0d> in <module>\r\n>      15 converter._experimental_lower_tensor_list_ops = False\r\n>      16 \r\n> ---> 17 tflite_model = converter.convert()\r\n>      18 \r\n>      19 interpreter = tf.lite.Interpreter(model_content=tflite_model)\r\n> \r\n> ~/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in wrapper(self, *args, **kwargs)\r\n>     770   def wrapper(self, *args, **kwargs):\r\n>     771     # pylint: disable=protected-access\r\n> --> 772     return self._convert_and_export_metrics(convert_func, *args, **kwargs)\r\n>     773     # pylint: enable=protected-access\r\n>     774 \r\n> \r\n> ~/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in _convert_and_export_metrics(self, convert_func, *args, **kwargs)\r\n>     756     self._save_conversion_params_metric()\r\n>     757     start_time = time.process_time()\r\n> --> 758     result = convert_func(self, *args, **kwargs)\r\n>     759     elapsed_time_ms = (time.process_time() - start_time) * 1000\r\n>     760     if result:\r\n> \r\n> ~/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in convert(self)\r\n>    1165         Invalid quantization parameters.\r\n>    1166     \"\"\"\r\n> -> 1167     saved_model_convert_result = self._convert_as_saved_model()\r\n>    1168     if saved_model_convert_result:\r\n>    1169       return saved_model_convert_result\r\n> \r\n> ~/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in _convert_as_saved_model(self)\r\n>    1148       if self.saved_model_dir:\r\n>    1149         return super(TFLiteKerasModelConverterV2,\r\n> -> 1150                      self).convert(graph_def, input_tensors, output_tensors)\r\n>    1151     finally:\r\n>    1152       shutil.rmtree(temp_dir, True)\r\n> \r\n> ~/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in convert(self, graph_def, input_tensors, output_tensors)\r\n>     944         input_tensors=input_tensors,\r\n>     945         output_tensors=output_tensors,\r\n> --> 946         **converter_kwargs)\r\n>     947 \r\n>     948     return self._optimize_tflite_model(\r\n> \r\n> ~/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/lite/python/convert_phase.py in wrapper(*args, **kwargs)\r\n>     221         else:\r\n>     222           report_error_message(str(converter_error))\r\n> --> 223         raise converter_error from None  # Re-throws the exception.\r\n>     224       except Exception as error:\r\n>     225         report_error_message(str(error))\r\n> \r\n> ~/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/lite/python/convert_phase.py in wrapper(*args, **kwargs)\r\n>     214     def wrapper(*args, **kwargs):\r\n>     215       try:\r\n> --> 216         return func(*args, **kwargs)\r\n>     217       except ConverterError as converter_error:\r\n>     218         if converter_error.errors:\r\n> \r\n> ~/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/lite/python/convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\r\n>     824       input_data.SerializeToString(),\r\n>     825       debug_info_str=debug_info_str,\r\n> --> 826       enable_mlir_converter=enable_mlir_converter)\r\n>     827   return data\r\n>     828 \r\n> \r\n> ~/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n>     313       for error_data in _metrics_wrapper.retrieve_collected_errors():\r\n>     314         converter_error.append_error(error_data)\r\n> --> 315       raise converter_error\r\n>     316 \r\n>     317   return _run_toco_binary(model_flags_str, toco_flags_str, input_data_str,\r\n> \r\n> ConverterError: /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1048:0: error: 'tf.CropAndResize' op is neither a custom op nor a flex op\r\n> <unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n> /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1048:0: note: Error code: ERROR_NEEDS_FLEX_OPS\r\n> /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1048:0: error: 'tf.CropAndResize' op is neither a custom op nor a flex op\r\n> <unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n> /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1048:0: note: Error code: ERROR_NEEDS_FLEX_OPS\r\n> /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1048:0: error: 'tf.CropAndResize' op is neither a custom op nor a flex op\r\n> <unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n> /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1048:0: note: Error code: ERROR_NEEDS_FLEX_OPS\r\n> /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1048:0: error: 'tf.CropAndResize' op is neither a custom op nor a flex op\r\n> <unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n> /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1048:0: note: Error code: ERROR_NEEDS_FLEX_OPS\r\n> /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1048:0: error: 'tf.DenseToDenseSetOperation' op is neither a custom op nor a flex op\r\n> <unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n> /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1048:0: note: Error code: ERROR_NEEDS_FLEX_OPS\r\n> /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:552:0: error: 'tf.TensorListReserve' op is neither a custom op nor a flex op\r\n> <unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n> /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:552:0: note: Error code: ERROR_NEEDS_FLEX_OPS\r\n> /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:552:0: error: 'tf.TensorListFromTensor' op is neither a custom op nor a flex op\r\n> <unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n> /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:552:0: note: Error code: ERROR_NEEDS_FLEX_OPS\r\n> /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:552:0: error: 'tf.TensorListStack' op is neither a custom op nor a flex op\r\n> <unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n> /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:552:0: note: Error code: ERROR_NEEDS_FLEX_OPS\r\n> /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1048:0: error: 'tf.DenseToDenseSetOperation' op is neither a custom op nor a flex op\r\n> <unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n> /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1048:0: note: Error code: ERROR_NEEDS_FLEX_OPS\r\n> /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1048:0: error: 'tf.CropAndResize' op is neither a custom op nor a flex op\r\n> <unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n> /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1048:0: note: Error code: ERROR_NEEDS_FLEX_OPS\r\n> /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1048:0: error: 'tf.CropAndResize' op is neither a custom op nor a flex op\r\n> <unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n> /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1048:0: note: Error code: ERROR_NEEDS_FLEX_OPS\r\n> /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1048:0: error: 'tf.CropAndResize' op is neither a custom op nor a flex op\r\n> <unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n> /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1048:0: note: Error code: ERROR_NEEDS_FLEX_OPS\r\n> /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1048:0: error: 'tf.CropAndResize' op is neither a custom op nor a flex op\r\n> <unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n> /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1048:0: note: Error code: ERROR_NEEDS_FLEX_OPS\r\n> /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:552:0: error: 'tf.TensorListGetItem' op is neither a custom op nor a flex op\r\n> /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:552:0: note: called from\r\n> <unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n> /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:552:0: note: Error code: ERROR_NEEDS_FLEX_OPS\r\n> /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:552:0: error: 'tf.TensorListSetItem' op is neither a custom op nor a flex op\r\n> /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:552:0: note: called from\r\n> <unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n> /home/codemonkey/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:552:0: note: Error code: ERROR_NEEDS_FLEX_OPS\r\n> <unknown>:0: error: failed while converting: 'main': \r\n> Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \r\n> TF Select ops: CropAndResize, DenseToDenseSetOperation, TensorListFromTensor, TensorListGetItem, TensorListReserve, TensorListSetItem, TensorListStack\r\n> Details:\r\n> \ttf.CropAndResize(tensor<?x?x?x256xf32>, tensor<?x4xf32>, tensor<?xi32>, tensor<2xi32>) -> (tensor<?x14x14x256xf32>) : {T = f32, device = \"\", extrapolation_value = 0.000000e+00 : f32, method = \"bilinear\"}\r\n> \ttf.CropAndResize(tensor<?x?x?x256xf32>, tensor<?x?xf32>, tensor<?xi32>, tensor<2xi32>) -> (tensor<?x7x7x256xf32>) : {T = f32, device = \"\", extrapolation_value = 0.000000e+00 : f32, method = \"bilinear\"}\r\n> \ttf.DenseToDenseSetOperation(tensor<1x?xi64>, tensor<1x?xi64>) -> (tensor<?x2xi64>, tensor<?xi64>, tensor<2xi64>) : {T = i64, device = \"\", set_operation = \"intersection\", validate_indices = true}\r\n> \ttf.TensorListFromTensor(tensor<?xi32>, tensor<0xi32>) -> (tensor<!tf_type.variant<tensor<i32>>>) : {device = \"\"}\r\n> \ttf.TensorListGetItem(tensor<!tf_type.variant<tensor<i32>>>, tensor<i32>, tensor<0xi32>) -> (tensor<i32>) : {device = \"\"}\r\n> \ttf.TensorListReserve(tensor<i32>, tensor<i32>) -> (tensor<!tf_type.variant<tensor<*xi64>>>) : {device = \"\"}\r\n> \ttf.TensorListSetItem(tensor<!tf_type.variant<tensor<*xi64>>>, tensor<i32>, tensor<?xi64>) -> (tensor<!tf_type.variant<tensor<*xi64>>>) : {device = \"\"}\r\n> \ttf.TensorListStack(tensor<!tf_type.variant<tensor<*xi64>>>, tensor<1xi32>) -> (tensor<?x100xi64>) : {device = \"\", num_elements = -1 : i64}\r\n\r\n\r\n", "@CodeMonkey3435 \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "Is it possible to upload the model and then just use the tf lite converter code to reproduce the error?", "> Is it possible to upload the model and then just use the tf lite converter code to reproduce the error?\r\n\r\nYes that will work to reproduce the error on our end."]}, {"number": 51903, "title": "[oneDNN] Keras LayerNormalization fusion with oneDNN CPU backend", "body": "Keras LayerNormalization API creates a set op smaller ops that can be realized by a single operation with oneDNN library on CPU. This PR fuses smaller ops into a single op using grappler remapper optimizer. Current fusion is restricted to the scenario when LayerNormalization uses FusedBatchNorm and the input tensor is 2D/3D.\r\n\r\nThis fusion improves performance for inference with Transformer based language models.", "comments": ["@qlzh727 This commit (https://github.com/tensorflow/tensorflow/commit/7172ebeac672d0bc44632edcba3504d2ddbd5ae4#diff-569c52f4e3da483a946de7b1d4a508853cd04ba1a80f653e6c709477f6be5b01) has recently removed layer-normalization from `tensorflow/python/keras/layers`. However, this PR was using this module with `from tensorflow.python.keras import layers`. I tried using `from keras import layers`, but failed. Could you please suggest me how should I change my BUILD file and dependencies?", "The keras code base has been moved to keras-team/keras repository, and the code in tensorflow/python/keras will be removed soon. TF code should not rely on Keras code (since the dependency direction is wrong). Are you using keras layer for testing?", "@qlzh727 Thanks for your comment. Yes, I am using a keras layer for testing. We are trying to enable a fusion of several smaller operations generated by keras layer-normalization. This helps good performance on language models.", "Thanks for the info. Since the keras code doesn't live in TF repo anymore, you might want to add the keras related test into keras repository. The grappler should have its own unit test in cpp, and the keras py test will be a e2e test.\r\n\r\nAlso cc @rohan100jain and @sun51 from tf core team. We are pushing towards using tf.function with jit_compile=True, and avoid using any specific ops/kernels from python code if possible. Your PR should be fine since it is purely graph level optimization.", "@penpornk Thanks for the comments. I have addressed those. Please check.", "@penpornk Mistakenly while I was clicking on comment, the click was on `Close with comment`. Reopened it. BTW, I have addressed the review comments.", "@penpornk Resolved merged conflict", "Seems auto-merge is not happening but the changes are merged into master now, so we can close this. Thank you for the PR.", "This PR has been rolled back because it broke the Windows build. Below are some of the error messages:\r\n```c++\r\ntensorflow/core/kernels/mkl/mkl_layer_norm_op.cc(95): error C2398: Element '1': conversion from 'size_t' to 'ptrdiff_t' requires a narrowing \r\ntensorflow/core/kernels/mkl/mkl_layer_norm_op.cc(99): error C2398: Element '1': conversion from 'size_t' to 'ptrdiff_t' requires a narrowing conversion\r\n\r\ntensorflow/core/kernels/mkl/mkl_layer_norm_op.cc(113): error C2036: 'void *': unknown size\r\n```\r\nPlease feel free to open a new PR with a fix to roll this forward. "]}, {"number": 51902, "title": "val_sample_weight not used in calculation of custom validation metrics", "body": "**System information**\r\n-  Custome code using standard keras and tfa.metrics\r\n-  Operating System: CentOS Linux 7 (Core), Kernel: Linux 3.10.0-1160.el7.x86_64\r\n- Tensorflow installed with pip install tf-nightly-gpu\r\n- v1.12.1-63317-g034300c177d 2.7.0-dev20210907\r\n- NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3\r\n- A100-40GB\r\n\r\n**Describe the current behavior**\r\nAll validation metrics other than loss (set in model.compile(metrics=['accuracy', 'another_metrics'])) and reported during model.fit(......,val_data=(x_val, y_val, val_sample_weight)) do not seem to take the val_sample_weight into consideration. The loss metrics works as expected. \r\n \r\n**Describe the expected behavior**\r\nThe val_sample_weight associated with the x_val and y_val should be used in the calculation of 'accuracy' and other metrics such as tfa.metrics.CohenKappa. This is important for early stopping of model based on validation metrics. \r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nimport os \r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow_addons as tfa\r\n\r\ninputs = tf.keras.Input(shape=(10))\r\nx = tf.keras.layers.Dense(100, activation='relu')(inputs)\r\nx = tf.keras.layers.Dense(10, activation='relu')(x)\r\noutputs = tf.keras.layers.Dense(3, activation='softmax')(x)\r\nmodel = tf.keras.models.Model(inputs=inputs, outputs=outputs)\r\nmodel.compile('adam', loss='sparse_categorical_crossentropy', metrics=['accuracy', tfa.metrics.CohenKappa(num_classes=3, sparse_labels=True)], sample_weight_mode='temporal')\r\nmodel.summary()\r\nX =tf.random.uniform(shape=[100,10], minval=0, maxval=1, dtype=tf.float32)\r\nY = tf.random.uniform(shape=[100,1], minval=0, maxval=3, dtype=tf.int64)\r\nW = tf.ones([100])\r\nW2 = tf.zeros([100])\r\nmodel.fit(X, Y, sample_weight=W, validation_data=(X,Y,W2), epochs=5, verbose=1)\r\nprobs = model.predict(X)\r\nkappa = tfa.metrics.CohenKappa(num_classes=3, sparse_labels=True)\r\nkappa.update_state(Y, probs)\r\nprint(\"Kappa Without Weights\", kappa.result())\r\nkappa = tfa.metrics.CohenKappa(num_classes=3, sparse_labels=True)\r\nkappa.update_state(Y, probs,W2)\r\nprint(\"Kappa With Weights\", kappa.result())\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n>>>W2 is all zeros. Notice that the val_loss is 0.000 as expected, but the val_accuracy and val_kappa are not. See prints the end showing kappa behavior with and without weights.   \r\n\r\nEpoch 1/5\r\n4/4 [==============================] - 1s 215ms/step - loss: 1.0857 - accuracy: 0.4000 - cohen_kappa: -0.0130 - val_loss: 0.0000e+00 - val_accuracy: 0.3900 - val_cohen_kappa: -0.0281\r\nEpoch 2/5\r\n4/4 [==============================] - 0s 31ms/step - loss: 1.0766 - accuracy: 0.4000 - cohen_kappa: -0.0096 - val_loss: 0.0000e+00 - val_accuracy: 0.4000 - val_cohen_kappa: -0.0052\r\nEpoch 3/5\r\n4/4 [==============================] - 0s 30ms/step - loss: 1.0736 - accuracy: 0.3900 - cohen_kappa: -0.0214 - val_loss: 0.0000e+00 - val_accuracy: 0.4000 - val_cohen_kappa: -0.0059\r\nEpoch 4/5\r\n4/4 [==============================] - 0s 31ms/step - loss: 1.0706 - accuracy: 0.4000 - cohen_kappa: -0.0037 - val_loss: 0.0000e+00 - val_accuracy: 0.4000 - val_cohen_kappa: -0.0059\r\nEpoch 5/5\r\n4/4 [==============================] - 0s 29ms/step - loss: 1.0666 - accuracy: 0.4000 - cohen_kappa: -0.0081 - val_loss: 0.0000e+00 - val_accuracy: 0.4200 - val_cohen_kappa: 0.0213\r\nKappa Without Weights tf.Tensor(0.021262169, shape=(), dtype=float32)\r\nKappa With Weights tf.Tensor(0.0, shape=(), dtype=float32)\r\n", "comments": ["@sanatmpa1 ,\r\nI was able to reproduce the issue in tf v2.5,v2.6 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/93668cb75a71299e9d335249a7450ca2/un51902.ipynb).", "@kevinkotzen This is intended behavior. Please check the response from @fchollet \r\n\r\n> \"Weighting\" in this context means \"loss weighting\", which the documentation is pretty explicit about. As a result, it only applies to the loss. If you need to monitor custom metrics that require weighting, you can implement your own.\r\n> \r\n> > Finally, can I have one question: what's the difference between Masking/Sample weights?\r\n> \r\n> \"Masking\" means masking specific timesteps in an input sequence. Sample weighting consist in weighting the impact of specific output samples in the loss function.\r\n\r\n\r\nhttps://github.com/keras-team/keras/issues/1642#issuecomment-179997624\r\n\r\nYou can notice that `val_loss` is 0. Thanks!", "@jvishnuvardhan thanks for your reply. \r\n\r\n[Keras documentation on Custom Metrics](https://keras.io/api/metrics/#creating-custom-metrics) specifically mentions that \"sample weighting is automatically supported for any such [custom] metric\". They even show sample weights being used in the example. \r\nTo reiterate, sample weights seem to work fine for all train metrics, but not validation metrics (other than loss).  \r\n\r\nI also see that Model.compile() has weighted_metrics which it describes as \"List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing\". This too however does not seem to do anything.  \r\n\r\nIs there a technical reason why **validation metrics** cannot or should not be weighted?\r\n \r\nAny suggestions as to a simple workaround? ", "@kevinkotzen Did check the webpage\r\n\r\n> Much like loss functions, any callable with signature metric_fn(y_true, y_pred) that returns an array of losses (one of sample in the input batch) can be passed to compile() as a metric. Note that sample weighting is automatically supported for any such metric.\r\n\r\nLooks like it is supported but not sure why the results are different.\r\n\r\nAs Keras team moved to [keras-team/keras](https://github.com/keras-team/keras/issues) repo, can you please open this issue in that repo? Thanks!", "Moved this to https://github.com/keras-team/keras/issues/15373.  ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51902\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51902\">No</a>\n"]}, {"number": 51901, "title": "TF 2.5 Asks for nightly build", "body": "\r\nNow that Keras 2.5 and 2.6 have been released, can we update the setup.py to a stable Keras version? It causes errors when trying to use package managers such as poetry since they rely on pip. Thanks!\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/tools/pip_package/setup.py#L107", "comments": ["Hi @diviramon ! We see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks", "There are no steps to reproduce, it is simply a line in the code under `pip_package/setup.py` for Tensorflow 2.5.", "Hi @diviramon, Could you please mention the OS then ?  Its already updated to keras 2.6  in TF \r\n[ 2.6](https://github.com/tensorflow/tensorflow/blob/r2.6/tensorflow/tools/pip_package/setup.py) though.", "Hi @mohantym, thanks for getting back to me. Yes, but TF 2.6 is not supported on Apple M1 chip (there is no arm distribution). Therefore we need to use 2.5. Could it be updated in 2.5 as well?", "Hi @sanatmpa1 ,Could you please look into this issue?", "There is no stable keras version that matches TF 2.5.\r\n\r\nSo TF 2.5 has to use the corresponding nightly. Given than there is no more keras nightly on the 2.5 timeline, this is still a frozen package, similar to a released one.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51901\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51901\">No</a>\n"]}, {"number": 51900, "title": "Multi gpus can not run parallel when using run_eagerly=True in model.fit() in MultiWorkerMirroredStrategy", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.6.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): None\r\n- CUDA/cuDNN version: 11.3.0/8.2.1\r\n- GPU model and memory: Tesla P100(16GB)\r\n\r\nMy code is very simple:\r\n```\r\n\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\nimport os\r\nimport json\r\n\r\ngpus = tf.config.experimental.list_physical_devices(device_type='GPU')\r\nfor gpu in gpus:\r\n    tf.config.experimental.set_memory_growth(gpu, True)\r\n\r\nnum_epochs = 50\r\nbatch_size_per_replica = 32\r\nlearning_rate = 0.001\r\n\r\nnum_workers = 1\r\nos.environ['TF_CONFIG'] = json.dumps({\r\n    'cluster': {\r\n        'worker': [\"172.37.2.20:8888\"]\r\n    },\r\n    'task': {'type': 'worker', 'index': 0}\r\n})\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\nprint('~~' * 10)\r\nprint('Number of devices: %d.' % strategy.num_replicas_in_sync)\r\nprint('~~' * 10)\r\nbatch_size = batch_size_per_replica * num_workers * strategy.num_replicas_in_sync\r\n\r\n\r\ndef resize(image, label):\r\n    image = tf.image.resize(image, [224, 224]) / 255.0\r\n    return image, label\r\n\r\n\r\ndataset = tfds.load(\"cats_vs_dogs\", split=tfds.Split.TRAIN, as_supervised=True)\r\ndataset = dataset.map(resize).shuffle(1024).batch(batch_size)\r\n\r\nwith strategy.scope():\r\n    model = tf.keras.applications.MobileNetV2(weights=None, classes=2)\r\n    model.compile(\r\n        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\r\n        loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n        metrics=[tf.keras.metrics.sparse_categorical_accuracy],\r\n        run_eagerly=True\r\n    )\r\n\r\nmodel.fit(dataset, epochs=num_epochs)\r\n```\r\n\r\n\r\n**Describe the current behavior**\r\nIf I remove the ` run_eagerly=True` in model.compile, the multi gpus will run parallel. But after add this kwarg to make the model is runing in eager mode, the multi gpus will run one by one.\r\n\r\n**Describe the expected behavior**\r\nThe parameter should not determine the gpu running mode.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@qq747688898 Could you please have a look at the [link](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras) for reference and please let us know if it helps ? Thanks! ", "\r\n\r\n\r\n> @qq747688898 Could you please have a look at the [link](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras) for reference and please let us know if it helps ? Thanks!\r\n\r\nThanks for your reply. This page introduces how to start a distributed training by using Keras and tf.distribute.Strategy, but it can not explain why the gpus does not run parallel when using `model.compile(run_eagerly=True)`", "@qq747688898 \r\nCan you please refer to [this issue](https://stackoverflow.com/questions/56062017/hot-to-fix-tensorflow-model-not-running-in-eager-mode-with-fit) and let us know if it helps.", "@Saduf2019 \r\nHello, thanks for your reply.\r\nI try to run my model with this issue's way as:\r\n`model.compile()`\r\n`model.run_eagerly = True`\r\nBut the gpus still doesn't run parallel.", "@qq747688898 \r\nYou dont have to necessitate eager execution  when using the distributed strategies, please follow the official doc [here](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile).\r\n\r\nFor any further queries i suggest you open an issue at the tf discussion forum as there is a larger community to support there.", "@Saduf2019 \r\nSo can i think that if I want to make my gpus run parallel when using distributed strategies, the model must run without eager mode?", "@qq747688898 \r\nrun_eagerly=True in model.fit is suggested only to debug\r\nonce debugging is over, it is always recommended to removed that flag\r\n\r\nAs requested before for any further queries i suggest you open an issue at the tf discussion forum as there is a larger community to support there, this is not a bug or a performance issue.\r\n\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51900\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51900\">No</a>\n"]}, {"number": 51899, "title": "BatchNormalization", "body": "I am trying to understand how BatchNormalization works. With code:\r\n`x=tf.constant([2.,3.,4.])`\r\n`x=keras.layers.BatchNormalization()(x)`\r\n`print(x)`\r\nI get response: \r\n`tf.Tensor([1.9990008 2.9985013 3.9980016], shape=(3,), dtype=float32)`\r\nWhich is not what i expect since it doesn't have mean 0 or variance 1. Can somebody explain?", "comments": ["@niesocja ,\r\nPlease refer this links [1](https://www.analyticsvidhya.com/blog/2021/03/introduction-to-batch-normalization/) and [2](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization) which provide the information on BatchNormalization.It helps.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51899\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51899\">No</a>\n"]}, {"number": 51898, "title": "the @tf.function annotation weird performance", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Ubuntu 18.04.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- binary\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Python 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\ntf version 2.6.0\r\n\r\n**Describe the current behavior**\r\n\r\ntf version 2.6\r\n\r\nwhen the model call function has no @tf.function annotation like this:\r\n![image](https://user-images.githubusercontent.com/7404433/132669008-2043ccca-c588-4809-bbd9-e8255d0b3342.png)\r\n\r\ntrain the model the loss and auc log is like this:\r\n\r\n![image](https://user-images.githubusercontent.com/7404433/132669307-b105084e-ec5a-4084-bf21-fe87a65feac2.png)\r\n\r\nas you can see from the picture,at epoch start the loss value is high, after epoch end the loss value is low,and auc >0.5\r\n![image](https://user-images.githubusercontent.com/7404433/132669640-23a22f16-ec0c-4920-8780-204eb859edc2.png)\r\n\r\n**All the code remains unchanged, just add @tf.function annotation like this:** \r\n![image](https://user-images.githubusercontent.com/7404433/132669988-e8725067-e4e0-40cd-b701-ae38c0b555d2.png)\r\n\r\ntrain the model the loss and auc log is like this:\r\n![image](https://user-images.githubusercontent.com/7404433/132670270-689f4e48-132a-4462-9c4d-ba891d91e125.png)\r\n\r\nas you can see from the picture,at epoch start the loss value is different ,Its value is much lower\uff0cand after epoch end the loss value **is optimizer much lower, but the auc is always 0.5**\r\n\r\n![image](https://user-images.githubusercontent.com/7404433/132670962-4b2c520b-6252-472a-a3ca-374dbb312c0d.png)\r\n\r\nit is too confusing!!!\r\n\r\n**Describe the expected behavior**\r\n\r\nI think their results should be exactly the same\uff0c\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nThe code needed to reproduce this problem\uff1a\r\n\r\nJust modify the @tf.function annotation (add or remove) reproduce this problem\r\n\r\n![image](https://user-images.githubusercontent.com/7404433/132793559-66cc46cc-1544-4330-9089-aef2d80ab6bb.png)\r\n\r\n\r\nhttps://colab.research.google.com/drive/1i6PnUrkvNszW03wnS6ZyQBsrI1HYnTpq?usp=sharing\r\n\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n\r\n\r\n", "comments": ["Ok. @gao8954,\r\nBut Could you please fill the issue template ?It helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks", "> Ok. @gao8954,\r\n> But Could you please fill the issue template ?It helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks\r\n\r\nOk,has been modified,and colab link provided", "Ok @gao8954 ,Could you please provide link to csv file too?", "> Ok @gao8954 ,Could you please provide link to csv file too?\r\n\r\ni have upload the csv file to the colab,but it always delete automatically,when exit the runtime", "[tmk_sample_50000.csv](https://github.com/tensorflow/tensorflow/files/7183704/tmk_sample_50000.csv)\r\n\r\ni have uploaded again,and upload a backup here,just in case", "Hi @gao8954, I was getting different error  in TF 2.6 though , Attaching [gist ](https://colab.research.google.com/gist/mohantym/7e304b7b98370a6fb18bca732383f800/untitled0.ipynb#scrollTo=W5VQnlisn0Y-)for reference. Can you try again after normalizing the data from CSV ( I saw some float point values among integer values) in input data.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51898\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51898\">No</a>\n"]}, {"number": 51897, "title": "official guide on developer.apple.com/metal/tensorflow-plugin fails to build grpcio", "body": "Apple Silicon steps it fails to install base tensorflow:\r\n\r\n````\r\npython -m pip install tensorflow-macos\r\n````\r\n\r\nwith the following error message:\r\n\r\n````\r\n\r\nBuilding wheels for collected packages: grpcio\r\n  Building wheel for grpcio (setup.py) ... error\r\n  ERROR: Command errored out with exit status 1:\r\n   command: /Users/sascha/miniforge3/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/bj/kl9gdt8145n_pgcjlm9j4_x00000gn/T/pip-install-b32jlj8u/grpcio_6d3ece3d284848398f436707643d6149/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/bj/kl9gdt8145n_pgcjlm9j4_x00000gn/T/pip-install-b32jlj8u/grpcio_6d3ece3d284848398f436707643d6149/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /private/var/folders/bj/kl9gdt8145n_pgcjlm9j4_x00000gn/T/pip-wheel-efohlhk7\r\n       cwd: /private/var/folders/bj/kl9gdt8145n_pgcjlm9j4_x00000gn/T/pip-install-b32jlj8u/grpcio_6d3ece3d284848398f436707643d6149/\r\n````\r\n\r\nWhat is the proposed solution?\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac M1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): https://developer.apple.com/metal/tensorflow-plugin/\r\n- TensorFlow version: unknown as they is no \r\n- Python version: https://pypi.org/project/tensorflow-macos/ 2.5\r\n- Installed using virtualenv? pip? conda?: miniforge3 https://developer.apple.com/metal/tensorflow-plugin/\r\n- Bazel version (if compiling from source): ?\r\n- GCC/Compiler version (if compiling from source): ?\r\n- CUDA/cuDNN version: `\r\n- GPU model and memory:?", "comments": ["[Apple developer forum](https://developer.apple.com/forums/tags/tensorflow-metal) is the right place to raise this issue since those builds are maintained by them.\r\n", "solution\r\nhttps://developer.apple.com/forums/thread/683757\r\n\r\nSYSTEM_VERSION_COMPAT=0 pip install tensorflow-macos tensorflow-metal", "Thanks for sharing your solution. I will close this thread now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51897\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51897\">No</a>\n", "I am also facing the above issue. What is the solution for it?"]}, {"number": 51894, "title": "merge 2.6", "body": "PiperOrigin-RevId: 391529518\r\nChange-Id: Ie3db4ae6d3c0f3dc88404e1dbdc22f7d03cbeb3b", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51894) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 51893, "title": "merging to 2.5", "body": null, "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51893) for more info**.\n\n<!-- need_sender_cla -->", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51893) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 51892, "title": "how to barrier when run distribution trainning by estimator api", "body": "When run distribution trainning by tf.estimator api \uff0cother workers will be hanging when one worker finished training.So i add sleep function below estimator funtion.But it seemed tricky this way.\r\n```\r\ntf.estimator.train_and_evaluate(self._tf_estimator, train_spec, eval_spec)\r\n time.sleep(300)\r\n\r\n```\r\nAny other good ways to fix this estimator issue?\r\nTF version is 1.14", "comments": ["@liumilan We see that you are using TF **`v1.14`** which is not actively supported, please try to upgrade the TF version to **`2.6.0`** which is the latest stable TF version. Please let us know if the issue still persists in newer versions? Thank you!", "> @liumilan We see that you are using TF **`v1.14`** which is not actively supported, please try to upgrade the TF version to **`2.6.0`** which is the latest stable TF version. Please let us know if the issue still persists in newer versions? Thank you!\r\n\r\nCan tf 2.6.0 support tf 1.14 api?", "@liumilan As per [TF official documentation](https://www.tensorflow.org/) ,There is a caution for using estimator api. To know more could you please have a look at the [link](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator) for reference.Please let us know if it helps ?Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51892\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51892\">No</a>\n"]}, {"number": 51891, "title": "Fix crash with tf.image.resize if size is large", "body": "This PR tries to address the issue raised in 46914 where\r\ntf.image.resize will crash if size is large, (implicitly\r\ncauses tf.keras.layers.UpSampling2D to crash).\r\n\r\nThis PR adds necessary shape overflow check to prevent\r\ncrash.\r\n\r\nThis PR fixes 46914.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n(cherry picked from commit 660ad5e76d8df64d855f77b5e2e39d8ddb40cab5)\r\nPiperOrigin-RevId: 391409572\r\nChange-Id: I027c4901c9717ae7ee8266e5f57baba950b3e1e3", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51891) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51891) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I signed it!", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51891) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I signed it!", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51891) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 51890, "title": "merging updated curl file", "body": null, "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51890) for more info**.\n\n<!-- need_sender_cla -->", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51890) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 51889, "title": "CUBLAS_STATUS_INVALID_VALUE when using `tf.matmal` to calculate large size of tensors", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Feroda 34\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.9.6\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 5.4\r\n- CUDA/cuDNN version: 11.3.1/8.2.1\r\n- GPU model and memory: 2080 Super 8GB\r\n\r\n**Describe the current behavior**\r\nWe are developers of [deepmd-kit](https://github.com/deepmodeling/deepmd-kit), a deep learning package based on TensorFlow in areas of computational chemistry and computational material. To give convenience to our users, we use `conda-build` to [build TensorFlow](https://github.com/deepmd-kit-recipes/tensorflow-base-feedstock/blob/master/recipe/build.sh) and our programs from source, and distribute them using an offline package, ensuring our users have the same library environment. As reported by our users in deepmodeling/deepmd-kit#1061 and deepmodeling/deepmd-kit#1062, when using `tf.matmal` to calculate large size of tensors, an error happened below. The original error should be ` failed to run cuBLAS routine: CUBLAS_STATUS_INVALID_VALUE`, and this causes `Blas xGEMV launch failed`. When we try to descrease the size of the tensor, and it works fine. However, I believe the error is not related to the memory, as some one also got this error in 40GB A100, where the memory is enough and there is no OOM message. And also, **it works fine with CUDA 10.1 and cuDNN 7**. I think it may be an internal error in TensorFlow or cuBLAS.\r\n\r\n```\r\n2021-09-08 17:04:15.418750: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\r\n2021-09-08 17:04:15.794003: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\r\n2021-09-08 17:04:15.839986: E tensorflow/stream_executor/cuda/cuda_blas.cc:564] failed to run cuBLAS routine: CUBLAS_STATUS_INVALID_VALUE\r\nTraceback (most recent call last):\r\n  File \"/home/jz748/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1375, in _do_call\r\n    return fn(*args)\r\n  File \"/home/jz748/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1359, in _run_fn\r\n    return self._call_tf_sessionrun(options, feed_dict, fetch_list,\r\n  File \"/home/jz748/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1451, in _call_tf_sessionrun\r\n    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\r\ntensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.\r\n  (0) Internal: Blas xGEMV launch failed : a.shape=[1,1228800,2], b.shape=[1,1,2], m=1228800, n=1, k=2\r\n\t [[{{node gradients/filter_type_all/MatMul_6_grad/MatMul}}]]\r\n\t [[l2_virial_test/_45]]\r\n  (1) Internal: Blas xGEMV launch failed : a.shape=[1,1228800,2], b.shape=[1,1,2], m=1228800, n=1, k=2\r\n\t [[{{node gradients/filter_type_all/MatMul_6_grad/MatMul}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/jz748/anaconda3/envs/dpdev113/bin/dp\", line 33, in <module>\r\n    sys.exit(load_entry_point('deepmd-kit', 'console_scripts', 'dp')())\r\n  File \"/home/jz748/codes/deepmd-kit/deepmd/entrypoints/main.py\", line 437, in main\r\n    train_dp(**dict_args)\r\n  File \"/home/jz748/codes/deepmd-kit/deepmd/entrypoints/train.py\", line 102, in train\r\n    _do_work(jdata, run_opt, is_compress)\r\n  File \"/home/jz748/codes/deepmd-kit/deepmd/entrypoints/train.py\", line 163, in _do_work\r\n    model.train(train_data, valid_data)\r\n  File \"/home/jz748/codes/deepmd-kit/deepmd/train/trainer.py\", line 497, in train\r\n    self.valid_on_the_fly(fp, [train_batch], valid_batches, print_header=True)\r\n  File \"/home/jz748/codes/deepmd-kit/deepmd/train/trainer.py\", line 591, in valid_on_the_fly\r\n    train_results = self.get_evaluation_results(train_batches)\r\n  File \"/home/jz748/codes/deepmd-kit/deepmd/train/trainer.py\", line 643, in get_evaluation_results\r\n    results = self.loss.eval(self.sess, feed_dict, natoms)\r\n  File \"/home/jz748/codes/deepmd-kit/deepmd/loss/ener.py\", line 140, in eval\r\n    error, error_e, error_f, error_v, error_ae, error_pf = run_sess(sess, run_data, feed_dict=feed_dict)\r\n  File \"/home/jz748/codes/deepmd-kit/deepmd/utils/sess.py\", line 20, in run_sess\r\n    return sess.run(*args, **kwargs)\r\n  File \"/home/jz748/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 967, in run\r\n    result = self._run(None, fetches, feed_dict, options_ptr,\r\n  File \"/home/jz748/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1190, in _run\r\n    results = self._do_run(handle, final_targets, final_fetches,\r\n  File \"/home/jz748/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1368, in _do_run\r\n    return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n  File \"/home/jz748/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1394, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.\r\n  (0) Internal: Blas xGEMV launch failed : a.shape=[1,1228800,2], b.shape=[1,1,2], m=1228800, n=1, k=2\r\n\t [[node gradients/filter_type_all/MatMul_6_grad/MatMul (defined at /codes/deepmd-kit/deepmd/descriptor/se_t.py:353) ]]\r\n\t [[l2_virial_test/_45]]\r\n  (1) Internal: Blas xGEMV launch failed : a.shape=[1,1228800,2], b.shape=[1,1,2], m=1228800, n=1, k=2\r\n\t [[node gradients/filter_type_all/MatMul_6_grad/MatMul (defined at /codes/deepmd-kit/deepmd/descriptor/se_t.py:353) ]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node gradients/filter_type_all/MatMul_6_grad/MatMul:\r\n filter_type_all/matrix_1_1_1/read (defined at /codes/deepmd-kit/deepmd/utils/network.py:174)\r\n\r\nInput Source operations connected to node gradients/filter_type_all/MatMul_6_grad/MatMul:\r\n filter_type_all/matrix_1_1_1/read (defined at /codes/deepmd-kit/deepmd/utils/network.py:174)\r\n\r\nOriginal stack trace for 'gradients/filter_type_all/MatMul_6_grad/MatMul':\r\n  File \"/anaconda3/envs/dpdev113/bin/dp\", line 33, in <module>\r\n    sys.exit(load_entry_point('deepmd-kit', 'console_scripts', 'dp')())\r\n  File \"/codes/deepmd-kit/deepmd/entrypoints/main.py\", line 437, in main\r\n    train_dp(**dict_args)\r\n  File \"/codes/deepmd-kit/deepmd/entrypoints/train.py\", line 102, in train\r\n    _do_work(jdata, run_opt, is_compress)\r\n  File \"/codes/deepmd-kit/deepmd/entrypoints/train.py\", line 158, in _do_work\r\n    model.build(train_data, stop_batch)\r\n  File \"/codes/deepmd-kit/deepmd/train/trainer.py\", line 329, in build\r\n    self._build_network(data)\r\n  File \"/codes/deepmd-kit/deepmd/train/trainer.py\", line 353, in _build_network\r\n    = self.model.build (self.place_holders['coord'],\r\n  File \"/codes/deepmd-kit/deepmd/model/ener.py\", line 229, in build\r\n    = self.descrpt.prod_force_virial (atom_ener, natoms)\r\n  File \"/codes/deepmd-kit/deepmd/descriptor/se_t.py\", line 353, in prod_force_virial\r\n    [net_deriv] = tf.gradients (atom_ener, self.descrpt_reshape)\r\n  File \"/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/ops/gradients_impl.py\", line 169, in gradients\r\n    return gradients_util._GradientsHelper(\r\n  File \"/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/ops/gradients_util.py\", line 681, in _GradientsHelper\r\n    in_grads = _MaybeCompile(grad_scope, op, func_call,\r\n  File \"/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/ops/gradients_util.py\", line 338, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/ops/gradients_util.py\", line 682, in <lambda>\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/ops/math_grad.py\", line 1733, in _MatMulGrad\r\n    grad_a = gen_math_ops.mat_mul(grad, b, transpose_b=True)\r\n  File \"/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 5716, in mat_mul\r\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\r\n  File \"/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py\", line 748, in _apply_op_helper\r\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\r\n  File \"/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\", line 3557, in _create_op_internal\r\n    ret = Operation(\r\n  File \"/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\", line 2045, in __init__\r\n    self._traceback = tf_stack.extract_stack_for_node(self._c_op)\r\n\r\n...which was originally created as op 'filter_type_all/MatMul_6', defined at:\r\n  File \"/anaconda3/envs/dpdev113/bin/dp\", line 33, in <module>\r\n    sys.exit(load_entry_point('deepmd-kit', 'console_scripts', 'dp')())\r\n[elided 4 identical lines from previous traceback]\r\n  File \"/codes/deepmd-kit/deepmd/train/trainer.py\", line 353, in _build_network\r\n    = self.model.build (self.place_holders['coord'],\r\n  File \"/codes/deepmd-kit/deepmd/model/ener.py\", line 159, in build\r\n    = self.descrpt.build(coord_,\r\n  File \"/codes/deepmd-kit/deepmd/descriptor/se_t.py\", line 316, in build\r\n    self.dout, self.qmat = self._pass_filter(self.descrpt_reshape,\r\n  File \"/codes/deepmd-kit/deepmd/descriptor/se_t.py\", line 388, in _pass_filter\r\n    layer, qmat = self._filter(tf.cast(inputs_i, self.filter_precision), type_i, name='filter_type_all'+suffix, natoms=natoms, reuse=reuse, trainable = trainable, activation_fn = self.filter_activation_fn)\r\n  File \"/codes/deepmd-kit/deepmd/descriptor/se_t.py\", line 499, in _filter\r\n    ebd_env_ij = embedding_net(ebd_env_ij,\r\n  File \"/codes/deepmd-kit/deepmd/utils/network.py\", line 188, in embedding_net\r\n    hidden = tf.reshape(activation_fn(tf.matmul(xx, w) + b), [-1, outputs_size[ii]])\r\n  File \"/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\", line 206, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py\", line 3489, in matmul\r\n    return gen_math_ops.mat_mul(\r\n  File \"/anaconda3/envs/dpdev113/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 5716, in mat_mul\r\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nIt works without errors.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no, I don't have the idea about the reason\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nI haven't successfully created a minimum code to reproduce the error. Maybe provide later...\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n`Blas xGEMV launch failed` is raised here:\r\nhttps://github.com/tensorflow/tensorflow/blob/0b6b491d21d6a4eb5fbab1cca565bc1e94ca9543/tensorflow/core/kernels/matmul_op_impl.h#L431-L436", "comments": ["@njzjz ,\r\nEvery TensorFlow release is compatible with a certain version, for more information please take a look at the [tested build](https://www.tensorflow.org/install/source#gpu) configurations.In this case, can you please try installing TensorFlow v2.5 with CUDA 11.2 and cuDNN 8 .1and check if you are facing the same error. Thanks!", "Hi @tilakrayal, CUDA 11.2 works for me. So, I guess there is still something incompatible with CUDA 11.3 in the source code, even if I built from source?", "@njzjz ,\r\nAs mentioned above please follow  the tested build configurations and try to install tf v2.5 with compatible cuda and cudnn and let us know if you are facing any issue. CUDA 11.3 is incompatible with v2.5 as every tensorflow release is compatible with certain version.Hope it r helps to resolve the issue.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51889\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51889\">No</a>\n"]}, {"number": 51888, "title": "change curl version to 7.78.0", "body": "PiperOrigin-RevId: 393777488\r\nChange-Id: Iaad20d85315b0bdd42bf7fb389df8c8f13179639", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51888) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51888) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I signed it!", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51888) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 51887, "title": "change curl version to 7.78.0", "body": "PiperOrigin-RevId: 393777488\r\nChange-Id: Iaad20d85315b0bdd42bf7fb389df8c8f13179639", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51887) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51887) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I signed it!", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51887) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 51886, "title": "GET returned 404 Not Found", "body": "\r\n**System information**\r\n- OS Platform and Distribution: manylinux2014\r\n- TensorFlow version: git HEAD\r\n- Python version: 3.6 - 3.9\r\n- Installed using virtualenv\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 10\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nFile missing on mirror?\r\n\r\n```\r\n20:49:09     Analyzing: target //tensorflow/tools/pip_package:build_pip_package (243 packages loaded, 3941 targets configured)\r\n20:49:09     Analyzing: target //tensorflow/tools/pip_package:build_pip_package (409 packages loaded, 16799 targets configured)\r\n20:49:09     WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/XNNPACK/archive/dfe763f462d3569323de6caa085d8b06ce38eb7b.zip failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\n20:49:09     INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (433 packages loaded, 25206 targets configured).\r\n```\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\n20:49:09     bazel clean --expunge\r\n20:49:09     export BAZEL_LINKLIBS=-l%:libstdc++.a\r\n20:49:09     bazel build --config=nonccl //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n20:49:09     mkdir tensorflow-pkg\r\n20:49:09     bazel-bin/tensorflow/tools/pip_package/build_pip_package --cpu --project_name tensorflow_aarch64 ./tensorflow-pkg\r\n```", "comments": ["Hi @Saduf2019 ! Could you look into this issue?", "@hrw \r\nCan you please confirm if this issue occurs with the stable version of tensorflow as the nightly could have daily updates, can you let us know if it works fine on stable tested version 3.4/2.5 or 2.6\r\n\r\nYou may refer to similar issue and let us nkow:[link](https://github.com/tensorflow/tensorflow/issues/45739#issuecomment-751489014), [link1](https://stackoverflow.com/questions/54319302/error-in-bazel-build-installation-of-tensorflow-from-source)", "This was on Linaro CI. Our 'manylinux tensorflow nightly' job builds Tensorflow in a loop for Python 3.[6789] versions.\r\n\r\nLog: https://ci.linaro.org/job/ldcg-python-manylinux-tensorflow-nightly/89/console\r\n\r\nIt looks like it built git HEAD for Python 3.6 and then started for 3.7 and failed.\r\n\r\nBuild of TF:\r\n```\r\nset -xe\r\nsource /tmp/workspace/venv-cp37-cp37m/bin/activate\r\nexpect configure_tensorflow\r\nbazel clean --expunge\r\nexport BAZEL_LINKLIBS=-l%:libstdc++.a\r\nbazel build --config=nonccl //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\nmkdir tensorflow-pkg\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package --cpu --project_name tensorflow_aarch64 ./tensorflow-pkg\r\n```\r\n\r\nOur Ansible playbooks/roles used for building: https://git.linaro.org/ci/job/configs.git/tree/ldcg-python-manylinux-tensorflow/ansible (parent dir has build.sh and build-manylinux2014-wheels.sh scripts which install and run Ansible).", "I am aware that build scripts I use may need some improvements. All comments are welcome.", "It built: https://snapshots.linaro.org/ldcg/python/tensorflow-manylinux-nightly/20210909-90/tensorflow-aarch64/\r\n\r\nClosing. Sorry for mess.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51886\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51886\">No</a>\n", "And another one:\r\n\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/f5b8f1247cd9d1b18b7b95f6f197d4d654597529.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nWARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/cf83c1cba92cb2ae97c08399531685fd9ccfe983.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\n\r\nCan't you first mirror and THEN commit? I have daily build so it fetches HEAD and do build. At some moment during 24h."]}, {"number": 51885, "title": "Fix EinsumHelper::ParseEquation to avoid uninitialized accesses.", "body": "EinsumHelper::ParseEquation is supposed to return true or false in\r\ninput_has_ellipsis and output_has_ellipsis to indicate whether there is\r\nellipsis in the inputs and output. Previously, when there is no ellipsis in the\r\ninputs or output, the routine doesn't assign false to the variables. This\r\nchange initializes the two variables with false to fix the problem.\r\nPiperOrigin-RevId: 391772004\r\nChange-Id: I17b6c88aadef4131470378e48cced054bf252e86", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51885) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 51884, "title": "Fix EinsumHelper::ParseEquation to avoid uninitialized accesses.", "body": "EinsumHelper::ParseEquation is supposed to return true or false in\r\ninput_has_ellipsis and output_has_ellipsis to indicate whether there is\r\nellipsis in the inputs and output. Previously, when there is no ellipsis in the\r\ninputs or output, the routine doesn't assign false to the variables. This\r\nchange initializes the two variables with false to fix the problem.\r\nPiperOrigin-RevId: 391772004\r\nChange-Id: I17b6c88aadef4131470378e48cced054bf252e86", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51884) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 51883, "title": "Fix EinsumHelper::ParseEquation to avoid uninitialized accesses.", "body": "EinsumHelper::ParseEquation is supposed to return true or false in\r\ninput_has_ellipsis and output_has_ellipsis to indicate whether there is\r\nellipsis in the inputs and output. Previously, when there is no ellipsis in the\r\ninputs or output, the routine doesn't assign false to the variables. This\r\nchange initializes the two variables with false to fix the problem.\r\nPiperOrigin-RevId: 391772004\r\nChange-Id: I17b6c88aadef4131470378e48cced054bf252e86", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51883) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 51882, "title": "Add TFL_CAPI_EXPORT to TfLiteXNNPACK*() functions", "body": "Without the `TFL_CAPI_EXPORT` annotations the functions don't get exported into the resulting `tensorflowlite_c.dll`.\r\n\r\nThis allows calling them on Windows using the C API, so you can explicitly create a XNNPACK delegate and apply it to an interpreter. I know using pure C API is discouraged, but we need to use it.\r\n\r\n`gl_delegate.h` already does this.\r\n\r\nCloses #47007.", "comments": ["Windows Bazel build failures are unrelated to the change judging by the logs."]}, {"number": 51881, "title": "tensorflow/compiler/mlir/tensorflow:gen_gen_mlir_passthrough_op_py_py_wrappers_cc' failed ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux, Kylin 4.4.131 (a system like Ubuntu16.04, which is based on the aarch64 architecture)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r2.6\r\n- Python version: 3.9.7\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n- CUDA/cuDNN version: no cuda, just with cpu\r\n- GPU model and memory: \r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. bazel clean\r\n2. ./configure\r\nYou have bazel 3.7.2 installed.\r\nPlease specify the location of python. [Default is /home/b517-120/miniforge3/envs/tensorflow-build/bin/python3]: \r\n\r\nFound possible Python library paths:\r\n  /home/b517-120/miniforge3/envs/tensorflow-build/lib/python3.9/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/b517-120/miniforge3/envs/tensorflow-build/lib/python3.9/site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: n\r\nClang will not be downloaded.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -Wno-sign-compare]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=mkl_aarch64 \t# Build with oneDNN and Compute Library for the Arm Architecture (ACL).\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=numa        \t# Build with NUMA support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n\t--config=v1          \t# Build with TensorFlow 1 API instead of TF 2 API.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n\r\n3.  bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (427 packages loaded, 25634 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/b517-120/tensorflow/tensorflow/compiler/mlir/tensorflow/BUILD:2037:21: Linking of rule '//tensorflow/compiler/mlir/tensorflow:gen_gen_mlir_passthrough_op_py_py_wrappers_cc' failed (Exit 1): gcc failed: error executing command /usr/bin/gcc @bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/tensorflow/gen_gen_mlir_passthrough_op_py_py_wrappers_cc-2.params\r\nbazel-out/aarch64-opt/bin/_solib_aarch64/_U_S_Stensorflow_Scompiler_Smlir_Stensorflow_Cgen_Ugen_Umlir_Upassthrough_Uop_Upy_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so.2: undefined reference to `std::allocator<absl::lts_20210324::string_view>::allocator()'\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 4758.311s, Critical Path: 2844.81s\r\nINFO: 12916 processes: 3232 internal, 9684 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\nThe above is all the errors I met.", "comments": ["@doubibobo Could you please have a look at the [link1](https://www.tensorflow.org/mlir),[link2](https://www.tensorflow.org/install/source), similar[ issue ](https://github.com/tensorflow/tensorflow/issues/46630) and try to install using latest stable TF version 2.6 from source ?Please let us know if it helps? Thanks!", "Thanks for your timely response. Following [issue-comment](https://github.com/tensorflow/tensorflow/issues/46630#issuecomment-885816590) , I set `TF_ENABLE_MLIR=0` and `TF_ENABLE_XLA=0`, but I still get the same error while compiling a new file.\r\nIt seems like that the `absl` package is not built correctly......\r\n\r\nCommands:\r\n```\r\n1. export TF_ENABLE_MLIR=0\r\n2. export TF_ENABLE_XLA=0\r\n3. bazel clean\r\n4. ./configure\r\n5. bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nERROR: /home/b517-120/tensorflow/tensorflow/cc/BUILD:621:22: Linking of rule '//tensorflow/cc:ops/experimental_dataset_ops_gen_cc' failed (Exit 1): gcc failed: error executing command /usr/bin/gcc @bazel-out/aarch64-opt/bin/tensorflow/cc/ops/experimental_dataset_ops_gen_cc-2.params\r\nbazel-out/aarch64-opt/bin/_solib_aarch64/_U_S_Stensorflow_Scc_Cops_Sexperimental_Udataset_Uops_Ugen_Ucc___Utensorflow/libtensorflow_framework.so.2: undefined reference to _**`std::allocator<absl::lts_20210324::string_view>::allocator()'**_\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /home/b517-120/tensorflow/tensorflow/lite/toco/python/BUILD:89:10 Linking of rule '//tensorflow/cc:ops/experimental_dataset_ops_gen_cc' failed (Exit 1): gcc failed: error executing command /usr/bin/gcc @bazel-out/aarch64-opt/bin/tensorflow/cc/ops/experimental_dataset_ops_gen_cc-2.params\r\nINFO: Elapsed time: 3776.873s, Critical Path: 2912.50s\r\nINFO: 11654 processes: 3053 internal, 8601 local.\r\nFAILED: Build did NOT complete successfully\r\n", "@doubibobo \r\nCan you try running a bazel clean --expunge before building again, try changed to gcc7.5.0 and let us know.", "Thank you so much!\r\nFollow your tips, I changed to gcc7.5.0 and tried running a `bazel clean --expunge` before built\r\nAnd then it succeeds! It really works!\r\nThank you again for solving such a confusing problem for me!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51881\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51881\">No</a>\n", "@doubibobo \r\nThank you for your update, glad the issue is resolved.", "> @doubibobo Can you try running a bazel clean --expunge before building again, try changed to gcc7.5.0 and let us know.\r\n\r\nI'm getting the exact same error compiling pytorch/xla, but according to pytorch/xla I can only use clang instead of gcc. Do you have a solution for this? thanks!"]}, {"number": 51880, "title": "IOS tflite model doesn't have ResetVariableTensors() attribute", "body": "- OS Platform and Distribution : Mac os(xcode)\r\n- TensorFlow installed from (source or binary): pod install\r\n- TensorFlow version : TensorFlowLiteSwift 2.6.0\r\n\r\nI am using model that contain two LSTM layers, if I don't use a function that has same functionality like reset_all_variables()(python), ResetVariableTensors()(java, and c++), the output will be different from what I want.\r\n\r\nI need the function using swift or objective-c, I just want a way to reset variables inside tflite model in IOS app\r\n\r\nI hope there is a way to do that, thanks\r\n\r\nLinks to the function that I want :\r\npython : https://github.com/tensorflow/tensorflow/blob/v2.6.0/tensorflow/lite/python/interpreter.py#L877-L878\r\njava : https://www.tensorflow.org/lite/api_docs/java/org/tensorflow/lite/Interpreter#resetVariableTensors()\r\nc++ : https://www.tensorflow.org/lite/api_docs/cc/class/tflite/interpreter#classtflite_1_1_interpreter_1a3f5386d1e3569a55cfe0990dda8c4d92", "comments": ["I did manage a way to include the feature within the pod lib for xcode until they release a newer version with it, because they already have it in the c files but still experimental, and not included in the swift nightly version.\r\n\r\nUsed the same command from this page (https://www.tensorflow.org/lite/guide/build_ios)\r\n\r\n```\r\nbazel build --config=ios_fat -c opt \\\r\n  //tensorflow/lite/ios:TensorFlowLiteC_framework\r\n```\r\n\r\nI edited three files before running the command :\r\n\r\nAdd function to tensorflow/lite/c/c_api.h \r\n\r\n```\r\nTFL_CAPI_EXPORT extern TfLiteStatus TfLiteInterpreterResetVariableTensors(\r\n    TfLiteInterpreter* interpreter);\r\n```\r\n\r\n\r\nAdd function to tensorflow/lite/c/c_api.cc \r\n```\r\n\r\nTfLiteStatus TfLiteInterpreterResetVariableTensors(\r\n    TfLiteInterpreter* interpreter) {\r\n  return interpreter->impl->ResetVariableTensors();\r\n}\r\n```\r\n\r\n\r\nInside xcode project add function to Pods/TensorFlowLiteSwift/tensorflow/lite/swift/Sources/Interpreter.swift \r\n\r\n```\r\n    public func ResetVariableTensors() throws {\r\n      guard TfLiteInterpreterResetVariableTensors(cInterpreter) == kTfLiteOk else {\r\n        throw InterpreterError.allocateTensorsRequired\r\n      }\r\n    }\r\n```\r\n\r\nI am not sure if I did something wrong, but I just wanted this feature to work, and it did work\r\n\r\nThanks anyway", "@karimkalimu is the issue resolved? Feel free to request a PR with these changes. \r\n\r\nFollow up: The`TensorFlowLiteC_framework` build target includes the `tensorflow/lite/c/c_api_experimental.h/cc` files which define the `TfLiteInterpreterResetVariableTensors(..)` function. Why did you add it to the `tensorflow/lite/c/c_api.h/cc` files as well?", "yes the issue resolved, using the way I mentioned\r\n\r\nI tried to include c_api_experimental.h inside this  file \"tensorflow_src/tensorflow/lite/ios/BUILD\", but it didn't work, and I didn't know where to put c_api_experimental.cc file to be included, I thought it will be compiled if .h file included. \r\n\r\nI am not familiar with bazel process, and which file to edit to make the change I want\r\n\r\nFROM :\r\n`# bazel build -c opt --config=ios_fat //tensorflow/lite/ios:TensorFlowLiteC_framework\r\ntflite_ios_framework(\r\n    name = \"TensorFlowLiteC_framework\",\r\n    hdrs = [\r\n        \":c_api.h\",\r\n        \":common.h\",\r\n        \":xnnpack_delegate.h\",\r\n        \"//tensorflow/lite/c:c_api_types.h\",\r\n    ],\r\n    allowlist_symbols_file = \":allowlist_TensorFlowLiteC.txt\",\r\n    bundle_name = \"TensorFlowLiteC\",\r\n    minimum_os_version = TFL_MINIMUM_OS_VERSION,\r\n    deps = [\r\n        \":tensorflow_lite_c\",\r\n    ],\r\n)\r\n`\r\n\r\n\r\nTO :\r\n\r\n`# bazel build -c opt --config=ios_fat //tensorflow/lite/ios:TensorFlowLiteC_framework\r\ntflite_ios_framework(\r\n   \u200bname = \"TensorFlowLiteC_framework\",\r\n   \u200bhdrs = [\r\n      \":builtin_ops.h\",\r\n      \":c_api_experimental.h\", \r\n      \u200b\":c_api.h\",\r\n       \u200b\":common.h\",\r\n       \u200b\":xnnpack_delegate.h\",\r\n       \u200b\"//tensorflow/lite/c:c_api_types.h\",\r\n   \u200b],\r\n   \u200ballowlist_symbols_file = \":allowlist_TensorFlowLiteC.txt\",\r\n   \u200bbundle_name = \"TensorFlowLiteC\",\r\n   \u200bminimum_os_version = TFL_MINIMUM_OS_VERSION,\r\n   \u200bdeps = [\r\n       \u200b\":tensorflow_lite_c\",\r\n   \u200b],\r\n)\r\n`\r\n\r\nAnyway the issue has been resolved, thank you for your time", "@karimkalimu Thank you for the information! Glad the issue is fixed on your end. "]}, {"number": 51879, "title": "Bug: Loading the  older versions of tfs and keras", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device: No\r\n![4 1](https://user-images.githubusercontent.com/26819449/132501060-50a9033b-f984-4828-9715-a77b50fa1c8a.JPG)\r\n![4 2](https://user-images.githubusercontent.com/26819449/132501064-9121e22f-bf76-4648-92fb-2b68456a500f.JPG)\r\n\r\nI wanted to run a Bioinformatics library, a deep learning model which supports particular versions.\r\nThe before model which I wanted to run showed error in TensorFlow also but this model also requirement can't be met.\r\nI was using google-collab.\r\nI think the issue is with integrating with google-collab or maybe with TensorFlow only.\r\nThanks!\r\n\r\n\r\n\r\n\r\n", "comments": ["Hi @starboyvarun! We see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks", "@mohantym Other template question was not important that's why I only filled important ones.\r\n and I have mentioned a screenshot that clearly says version. This is just the beginning of the code.\r\nThanks!", "Hi @starboyvarun , The above commands ran fine in Colab .Please  Find [Gist ](https://colab.research.google.com/gist/mohantym/973ccfa19402bedeb4aca8659b914a7c/github_51789.ipynb)for reference  . Please provide Full Error stack trace to proceed further. \r\nAlso,It seems a duplicate issue to [this issue](https://github.com/tensorflow/tensorflow/issues/51783) who response has have been updated  recently with example code as a [GIST.](https://colab.research.google.com/gist/mohantym/5cb3c61f95b40a50f82b470574d9375a/github_51789.ipynb)\r\nThanks!", "@mohantym its fine but see the version is 2.3 of tf ,check I wanted the version to be 2.2 of Tensorflow", "Yes , Its a bug. It has  been fixed in latest version though.I have updated the [Gist ](https://colab.research.google.com/gist/mohantym/973ccfa19402bedeb4aca8659b914a7c/github_51789.ipynb#scrollTo=itREjrDQFN79) for reference. Thanks!", "@mohantym \r\nNow, will anyone fix it?? \r\nFix with older versions as people of bioinformatics use older versions.\r\n", "Ok. @starboyvarun \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) . Thanks!", "@mohantym  @sanatmpa1  yes I have posted the issue where you have mentioned.\r\nPlease let me know when it gets fixed.\r\nThank you.", "Ok ! @starboyvarun ,Please mention that issue here and feel free to close this one. ", "@mohantym I have already mentioned it, please fix this ASAP. \r\nSure I will close the issue now. But do let me know in the comments when it gets fixed.\r\nThank you.", "Ok @starboyvarun ,but Can you please mention this issue in the respective issue in Keras repository so we can track and update you  just in time.", "@mohantym  I have already mentioned it in Keras repository already.", "ok, thank you. I am closing this issue now.\r\nHope it gets fixed soon.\r\nThanks!.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51879\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51879\">No</a>\n", "This seems to be WAI.", "@mihaimaruseac  WAI ?? ", "working as intended"]}, {"number": 51878, "title": "For the model trained on tensorflow 1. X, how to quickly infer on tensorflow 2. X", "body": "For the model trained on tensorflow 1. X, how to quickly infer on tensorflow 2. X\uff1f\r\n\r\nBased on the model trained on tensorflow 1. X, how to infer on tensorflow 2. X, and Quantization Compression Based on tflite 2. X\uff0cIs there any relevant example reference, thanks!\r\n\r\n", "comments": ["@van68 Could you please refer to the [link1](https://www.tensorflow.org/lite/performance/post_training_quantization), [link2](https://www.tensorflow.org/model_optimization/guide/quantization/training) ,[ link 3](https://www.tensorflow.org/hub/model_compatibility) and let us know if it helps ? Thanks!", "> @van68 Could you please refer to the [link1](https://www.tensorflow.org/lite/performance/post_training_quantization), [link2](https://www.tensorflow.org/model_optimization/guide/quantization/training) ,[ link 3](https://www.tensorflow.org/hub/model_compatibility) and let us know if it helps ? Thanks!\r\n\r\nthank you reply, but it is no using.\r\nOur problem is based on tensorflow 1. X training model, how it refer  under tensorflow 2. X\r\n", "@van68  In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 51876, "title": "M1 Mac TensorFlow metal", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): M1 MAC\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): mini forge\r\n- TensorFlow version: 2.5\r\n- Python version: 3.9\r\n- Installed using virtualenv? pip? conda?: condo\r\n- I HAVE INSTALLED THROUGH :  https://github.com/jeffheaton/t81_558_deep_learning/blob/master/install/tensorflow-install-mac-metal-jul-2021.ipynb\r\n\r\n\r\nCODE \r\nimport numpy as np\r\nfrom keras.preprocessing import image\r\nimport matplotlib.pyplot as plt\r\nfrom pathlib import Path\r\nimport tensorflow as tf\r\nimport sys\r\nimport tensorflow.keras\r\nimport pandas as pd\r\nimport sklearn as sk\r\n\r\n\r\n\r\n\r\nprint(tf.test.gpu_device_name())\r\nprint(tf.config.list_physical_devices('GPU'))\r\nprint(f\"Tensor Flow Version: {tf.__version__}\")\r\nprint(f\"Keras Version: {tensorflow.keras.__version__}\")\r\nprint()\r\nprint(f\"Python {sys.version}\")\r\nprint(f\"Pandas {pd.__version__}\")\r\nprint(f\"Scikit-Learn {sk.__version__}\")\r\ngpu = len(tf.config.list_physical_devices('GPU'))>0\r\nprint(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")\r\n\r\n\r\nOUTPUT\r\n\r\n/device:GPU:0\r\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\nTensor Flow Version: 2.5.0\r\nKeras Version: 2.5.0\r\n\r\nPython 3.9.7 | packaged by conda-forge | (default, Sep  2 2021, 17:55:16) \r\n[Clang 11.1.0 ]\r\nPandas 1.3.2\r\nScikit-Learn 0.24.2\r\nGPU is available\r\n2021-09-08 11:25:29.831941: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\r\n2021-09-08 11:25:29.832018: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\r\n\r\n\r\nI am not sure what above 2 warning means . Is there any problem with installation.\r\n\r\n", "comments": ["Hi @par1hsharma , Could you look into this [link ](https://developer.apple.com/forums/thread/684178)for answers ? It suggests upgrading to MacOS 12.0", "\r\n\r\n\r\n\r\n> Hi @par1hsharma , Could you look into this [link ](https://developer.apple.com/forums/thread/684178)for answers ? It suggests upgrading to MacOS 12.0\r\n\r\ncan you tell me if i followed correct tutorial to install TensorFlow for M1 Mac\r\nI followed this link  https://github.com/jeffheaton/t81_558_deep_learning/blob/master/install/tensorflow-install-mac-metal-jul-2021.ipynb\r\nthe above link contains  \"tensorflow-apple-metal.yml\" . is this yml file a correct way to install or should I have followed \r\nthis  https://developer.apple.com/metal/tensorflow-plugin/", "Both approach includes  required  packages and steps , But Could you test the second approach after and update whether issue is replicating or not?. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51876\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51876\">No</a>\n", "> Here is how to build TF on M1:\r\n> \r\n> [#51506 (comment)](https://github.com/tensorflow/tensorflow/issues/51506#issuecomment-901460541)\r\nis this not a correct way to install it on M1 . Can you please go through the link and share your response ?\r\n https://github.com/jeffheaton/t81_558_deep_learning/blob/master/install/tensorflow-install-mac-metal-jul-2021.ipynb\r\n", "Hi @par1hsharma ,Is the issue still replicating  with same error stack trace?", "> Hi @par1hsharma ,Is the issue still replicating with same error stack trace?\r\n\r\nYes it is still replicating. Can you suggest me come commands to check if my installation is successful especially for arm Mac (m1 Mac). how to check if I have everything M1 native installed (from python to TensorFlow)", "Ok @par1hsharma  \r\n**Commands to check python version .**\r\n```\r\npython --version (in terminal)\r\nimport sys         (in script)\r\nsys.version\r\n```\r\n**Commands to check Tensorflow version**\r\n```\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n```\r\n**Command to check successful Tensorflow installation .**\r\n```\r\npython3 -c \"import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"  \r\n#if a tensor is returned you have installed Tensorflow Sucessfully\r\n```\r\n\r\n\r\nPlease update once if the installation is verified.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51876\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51876\">No</a>\n"]}, {"number": 51875, "title": "Don't constant-fold DT_RESOURCE constants.", "body": "PiperOrigin-RevId: 391803952\r\nChange-Id: I0ea3ec31d3e7dfda0f03b4027a237f08d00a3091", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51875) for more info**.\n\n<!-- need_sender_cla -->", "@pranve  Can you please sign CLA. Thanks!", "@googlebot I signed it!"]}, {"number": 51874, "title": "[TF:TRT] Add static registration system for TF->TRT operation converters", "body": "This PR is the first stage of refactoring the TF-TRT op converters. It adds a static registration system for the conversion functions which map from Tensorflow operations to TensorRT. The end result is that converters can be defined outside of `convert_nodes.cc` and users can link in their own converter functions, overriding existing functions.\r\nThe following changes are made:\r\n1. TRT Weights-related classes defined in convert_nodes.h are moved into `convert/weights.[h|cc]`. This is required to capture correct class and include dependencies.\r\n2. Static OpConverter registration system is introduced.\r\n3. Changes in `convert_nodes.cc` and `convert_nodes_test.cc` are made in order to use the new registration system.\r\n\r\nMore easily reviewed by commit. The \"TRT Weights classes to seperate files\" commit is purely moving classes around. The main changes are in the final commit, which accomplishes points #2 and #3.", "comments": ["rebased", " BTW all your comments in `weights.h` are on lines that appeared due to code movement. I didn't intend to overburden your review workload by reviewing code that's just moved around, but happy to make any changes.", "Applied recommendations except for two unresolved. I think there is a misunderstanding.", "@christopherbate Can you please resolve conflicts? Thanks!", "rebased, resolved remaining comments"]}]