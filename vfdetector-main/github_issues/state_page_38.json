[{"number": 46840, "title": "TypeError: __array__() takes 1 positional argument but 2 were given", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOs Big Sur\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: 3.8.7\r\n\r\n**Describe the current behavior**\r\n\r\nI'm trying to assign the return value of `get_value()` to a numpy array within a method decorated using `tf.function`. It works only if I call the method directly. However, if I use the result of `tf.numpy_function()` (in `case3`), I get an error. \r\n\r\n**Describe the expected behavior**\r\n\r\nI don't know, but the error is not doing any good explaining why this is happening\r\n\r\n**Standalone code to reproduce the issue**\r\nPlease find the notebook [here](https://colab.research.google.com/drive/1npV2ZmDE-So0IlA63MpoKhIs7sL-N6Xu?usp=sharing) which contains the following:\r\n\r\n    import numpy as np\r\n    import tensorflow as tf\r\n    \r\n    \r\n    class Foo:\r\n        def __init__(self):\r\n            self.storage = np.zeros((10, 3, 1))\r\n    \r\n        @staticmethod\r\n        def get_value(): \r\n            return np.array([[1], [2], [3]])\r\n    \r\n        @tf.function\r\n        def case1(self):  # works\r\n            self.storage[0] = self.get_value()\r\n    \r\n        def case2(self):  # works\r\n            self.storage[0] = tf.numpy_function(self.get_value, inp=[], Tout=tf.float32)\r\n    \r\n        @tf.function  # fails\r\n        def case3(self):\r\n            self.storage[0] = tf.numpy_function(self.get_value, inp=[], Tout=tf.float32)\r\n    \r\n    \r\n    if __name__ == '__main__':\r\n        foo = Foo()\r\n        foo.case1()\r\n        foo.case2()\r\n        foo.case3()\r\n\r\nError:\r\n\r\n    2021-02-01 23:11:35.422101: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n    2021-02-01 23:11:35.422548: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\n    To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n    2021-02-01 23:11:35.439501: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n    Traceback (most recent call last):\r\n      File \"/Users/emadboctor/Library/Application Support/JetBrains/PyCharm2020.3/scratches/scratch.py\", line 29, in <module>\r\n        foo.case3()\r\n      File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\r\n        result = self._call(*args, **kwds)\r\n      File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 871, in _call\r\n        self._initialize(args, kwds, add_initializers_to=initializers)\r\n      File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 725, in _initialize\r\n        self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n      File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2969, in _get_concrete_function_internal_garbage_collected\r\n        graph_function, _ = self._maybe_define_function(args, kwargs)\r\n      File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3361, in _maybe_define_function\r\n        graph_function = self._create_graph_function(args, kwargs)\r\n      File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3196, in _create_graph_function\r\n        func_graph_module.func_graph_from_py_func(\r\n      File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 990, in func_graph_from_py_func\r\n        func_outputs = python_func(*func_args, **func_kwargs)\r\n      File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 634, in wrapped_fn\r\n        out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n      File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3887, in bound_method_wrapper\r\n        return wrapped_fn(*args, **kwargs)\r\n      File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 977, in wrapper\r\n        raise e.ag_error_metadata.to_exception(e)\r\n    TypeError: in user code:\r\n    \r\n        /Users/emadboctor/Library/Application Support/JetBrains/PyCharm2020.3/scratches/scratch.py:22 case3  *\r\n            self.storage[0] = tf.numpy_function(self.get_value, inp=[], Tout=tf.float32)\r\n    \r\n        TypeError: __array__() takes 1 positional argument but 2 were given\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["I have tried in colab with TF version 2.4, nightly version(`2.5.0-dev20210201`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/85e69a98c5573d6ff6ac367be6b8a3a7/untitled649.ipynb).Thanks!", "See also #46563 for a little more detail about what causes this error in general.", "@nfelt Any particular suggestions for fixing the issue?", "@emadboctorx Unfortunately not really. My best guess is that something in the wrapper logic around `numpy_function` is attempting to convert some sort of TF tensor object into a numpy ndarray to feed as an argument into your `numpy_function` and at that point is hitting the same conversion error that I hit in the other issue.  However, the actual original stacktrace seems to be obscured because the error message is being re-propagated up through some intermediate layers.\r\n\r\n", "Was able to reproduce the issue in TF 2.6.0-dev20210528,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/5c3c061fc9a60c88b64b71612c7280e0/untitled47.ipynb#scrollTo=COohR0nH3Jta)..Thanks ! ", "All Possible solution added [[Solved] TypeError: method() takes 1 positional argument but 2 were given](https://flutterq.com/typeerror-method-takes-1-positional-argument-but-2-were-given/)", "Able to reproduce the error for a similar case as shown below:\r\n\r\nfrom PIL import Image\r\nimg_data = np.random.random(size=(100, 100, 3))\r\nimg = tf.keras.preprocessing.image.array_to_img(img_data)\r\narray = tf.keras.preprocessing.image.img_to_array(img)\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-115-e13b4c9f83c3> in <module>\r\n      2 img_data = np.random.random(size=(100, 100, 3))\r\n      3 img = tf.keras.preprocessing.image.array_to_img(img_data)\r\n----> 4 array = tf.keras.preprocessing.image.img_to_array(img)\r\n\r\n~\\anaconda3\\envs\\tf-latest-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\preprocessing\\image.py in img_to_array(img, data_format, dtype)\r\n    226       dtype = backend.floatx()\r\n    227     kwargs['dtype'] = dtype\r\n--> 228   return image.img_to_array(img, data_format=data_format, **kwargs)\r\n    229 \r\n    230 \r\n\r\n~\\anaconda3\\envs\\tf-latest-gpu\\lib\\site-packages\\keras_preprocessing\\image\\utils.py in img_to_array(img, data_format, dtype)\r\n    307     # or (channel, height, width)\r\n    308     # but original PIL image has format (width, height, channel)\r\n--> 309     x = np.asarray(img, dtype=dtype)\r\n    310     if len(x.shape) == 3:\r\n    311         if data_format == 'channels_first':\r\n\r\nTypeError: __array__() takes 1 positional argument but 2 were given\r\n\r\n ", "I found that was caused by PIL lib.\r\nI found the numpy array function description here\r\n```\r\n An array, any object exposing the array interface, an object whose\r\n            __array__ method returns an array, or any (nested) sequence.\r\n```\r\n\r\nso, it will invoke the `__array__` of PIL object.\r\nI solved this problems by downgrade PIL from 8.3.0 to 8.2.0.\r\nMaybe this will help you.", "> I solved this problems by downgrade PIL from 8.3.0 to 8.2.0.\r\n\r\nThanks! I had suspected something like this, so I had done a `pip install --upgrade --force-reinstall tensorflow` already. Interestingly, the `pillow` package was unaffected by this change, as it is not even listed in the `tensorflow` dependency tree.\r\n\r\n`pip install \"pillow!=8.3.0\"` does indeed fix this for now, and the root issue is tracked in https://github.com/python-pillow/Pillow/issues/5571.", "Just came by to say that we too were seeing issues with this and a downgrade of pillow to 8.2.0 has worked around the issue for now.", "> I found that was caused by PIL lib.\r\n> I found the numpy array function description here\r\n> \r\n> ```\r\n>  An array, any object exposing the array interface, an object whose\r\n>             __array__ method returns an array, or any (nested) sequence.\r\n> ```\r\n> \r\n> so, it will invoke the `__array__` of PIL object.\r\n> I solved this problems by downgrade PIL from 8.3.0 to 8.2.0.\r\n> Maybe this will help you.\r\n\r\nThanks alooooot !!", "Upgrade to 8.3.1 is work too.", "[[already solved] TypeError: __array__() takes 1 positional argument but 2 were given](https://exerror.com/typeerror-array-takes-1-positional-argument-but-2-were-given/)", "\uff20schissmantics, Hi, have you solved this problem now, I've tried your code on tensorflow 2.5 and python 3.7 and Pillow 8.2.0(8.3.1), not solving the problem and same error.", "In some newer version of numpy, an extra parameter `dtype` may be passed to the the `__array__` attribute of an object, but in previous implementation of `Tensor` and some other structures, `__array__` is defined like\r\n```\r\n  def __array__(self):\r\n    raise NotImplementedError(\r\n        \"Cannot convert a symbolic Tensor ({}) to a numpy array.\"\r\n        \" This error may indicate that you're trying to pass a Tensor to\"\r\n        \" a NumPy call, which is not supported\".format(self.name))\r\n```\r\nIn that case, an error is thrown before the function is executed\r\n#51595 and fc0f0e61ca9fe3ca3b9b58f51bcf00e0643ed9e3 can solve this", "The changes mentioned by @collinzrj does fix the problem, but it raises another error:\r\n```\r\nNotImplementedError: Cannot convert a symbolic Tensor (PyFunc:0) to a numpy array. \r\nThis error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.\r\n```\r\n\r\nAlthough, I think this one is indeed expected. This is discussed at #32927 ", "@caio-davi yes, it is expected"]}, {"number": 46833, "title": "backing_device changes depending on the tensor's dtype", "body": "Hi!\r\n\r\nI have noticed that `backing_device` changes depending on the Tensor's `dtype`:\r\n\r\n```python\r\nimport tensorflow as tf\r\ntensor_1 = tf.math.add(tf.zeros([2, 2]), [[1, 2], [3, 4]])\r\ntensor_2 = tf.math.add(tf.zeros([2, 2], dtype=tf.dtypes.int32), [[1, 2], [3, 4]])\r\n \r\nprint(tensor_1.backing_device)\r\nprint(tensor_2.backing_device)\r\n```\r\n \r\n\r\n```\r\n/job:localhost/replica:0/task:0/device:GPU:0\r\n/job:localhost/replica:0/task:0/device:CPU:0\r\n```\r\n\r\nThe behaviour above has some impact when interacting with other libraries. For instance, when exporting the Tensor via DLPack, some libraries (i.e. CUDA or JAX), do not support DLPack object hosted in CPU memory.\r\n\r\nHope it helps!\r\nMiguel\r\n\r\n**System information**\r\n- TensorFlow version: 2.4.1\r\n- Ubuntu 20.04\r\n- Cuda 11.0\r\n- Python 3.8\r\n- Installed via pip.\r\n\r\n**Describe the current behavior**\r\nOne tensor backing device is CPU, while the other is GPU.\r\n\r\n**Describe the expected behavior**\r\nHaving GPU as backing device for both tensors.\r\n", "comments": ["Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/a63bda22f935bffd7c9715b07c3a93c5/46833-2-3.ipynb) and [TF v2.4](https://colab.research.google.com/gist/amahendrakar/442ebb80b1a53c219e6d34f622c00859/46833.ipynb#scrollTo=i2A5oY8bQUq-).\r\n\r\nWhereas Colab doesn't detect the GPU with latest [TF-nightly](https://colab.research.google.com/gist/amahendrakar/abba546e65072cb19f02f16ae77d7c22/46833-tf-nightly.ipynb). Please check the linked gist for reference. Thanks!", "This is the expected behavior, that all the int32 tensors are on cpu by default, no matter which context device is specified.", "You can force it on gpu by using `tf.identity` operator", "Hi @VoVAllen ,\r\n\r\nI understand that the current behavior is what you detailed above. Nevertheless, it looks it has some impact on some scenarios, as the one described in this issue.\r\n\r\nTherefore, could it be reconsidered the current behaviour? Nowadays, GPUs can efficiently deal with int32 tensors.\r\n\r\nRegards,\r\nMiguel ", "@miguelusque I'm not tf developer, but met the same issue before. Generally speaking this is a legacy issue from tensorflow 1. int32 tensors were usually used to represent the shape information, thus many operators assume it's a cpu tensor. If changed to gpu by default, many codes and op implementations will break.", "Some issues related to this https://github.com/tensorflow/tensorflow/issues/34071, https://github.com/tensorflow/tensorflow/issues/41307", "Hi @VoVAllen ,\r\n\r\nThank you for your detailed reply.\r\n\r\nit looks like we are in front of a great use case to challenge CI/CD system used in TensorFlow. \ud83d\ude00\r\n\r\nI hope the TF dev team may consider this issue.\r\n\r\nRegards,\r\nMiguel", "Hello,\r\n\r\nMany thanks to VoVAllen for clarifying where this behavior comes from.\r\n\r\nI am currently facing a practical issue due to this (as other have before): when I convert int Tensors to dlpack in order to use pre-compiled custom cuda kernels (using TVM), they end up on the CPU, which causes my custom code to fail.\r\n\r\nI am therefore wondering : **how can you force a given int Tensor to have a GPU as 'backing_device' (in addition to having it as 'device')?** Some GitHub Issues suggest using 'tf.identity', but I cannot seem to make it work.\r\n\r\nThanks in advance,\r\nPaul", "My bad, you mentioned a trick [here](https://github.com/tensorflow/tensorflow/issues/41307#issuecomment-657385914) by casting to and then from unsigned int. This is not optimal, but it works.", "Was able to reproduce the issue in TF 2.6.0-dev20210528,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/cbcca6a474a1a37a3f09fc52cbabf24a/untitled49.ipynb#scrollTo=hr6nP5jy59AQ) ..Thanks !", "Hi @miguelusque, you've hit a long-running issue internally known as \"the int32 problem\". Basically TF carves out int32 as \"the shape dtype\", to workaround some design problems of the device-placement system. We're designing potential alternatives but it's hard to fix. At the moment just don't use int32 for your normal computation (use e.g. int64 or uint32 instead)."]}, {"number": 46822, "title": "TFLite converter does not convert dilated conv to single conv op when spatial dimension is dynamic", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- TensorFlow installation (pip package or built from source): pip\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.4 and nightly\r\n\r\n### 2. Code\r\n\r\nhttps://colab.research.google.com/drive/1U_rbD_tlvRmHjHXXFGMch_s_-2vl_cSf?usp=sharing\r\n\r\nand visualize models with netron.\r\n\r\n### 3. Failure after conversion\r\n\r\n- Model converts successfully but the generated model does not have single dilated convolution op.\r\n\r\nAlthough\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/6b23dbc15988dd7bbfbdca7581f1cb2b85246e8c/tensorflow/compiler/mlir/lite/transforms/dilated_conv.h#L98-L102\r\n\r\nsays allow dynamic width and height, paddings in SpaceToBatchNDOp and crops in BatchToSpaceNDOp are not constant when input has dynamic spatial dim. Therefore,\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/6b23dbc15988dd7bbfbdca7581f1cb2b85246e8c/tensorflow/compiler/mlir/lite/transforms/dilated_conv.h#L239-L253\r\n\r\nfails to match this kind of pattern.\r\n\r\n", "comments": ["For dynamic dimensions, we can't make sure that the given inputs always are supported with the dilated conversion lowering. We have seen some cases that are not correctly performed with the certain inputs when there are dynamic dimensions. Once we can have a clear boundary between the supported and unsupported ranges even though the given inputs have partial dynamic dimensions, it will be possible to apply more cases. Sorry for encountering this issue.\r\n\r\nBTW, why is this dilated conversion important to your case?", "Thanks for clarification!\r\n\r\nOur use case: In time sequence modeling, the input has dynamic spatial dim (batch, sequence, feature), we do multiple dilated conv to capture larger receptive field (finally followed by fixed size resizing). We've observed it's bottlenecked by conv2d on the hardware so it would be great to have support on dynamic dilated conv patterns. We actually have a workaround to move fixed sized resizing in front of dilated conv with a small accuracy loss but better speed.\r\n\r\nMy thought is to follow the codepath that keras does and bring the whole pattern into dag rewrite. Probably lengthy but doable I think. Is it possible to share the inputs that doesn't work correctly?", "Here are some test cases related to that.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/a30d20b632b4ffbfd437ccf8ee205fef0917a3eb/tensorflow/compiler/mlir/lite/tests/dilated-conv.mlir#L329", "I am sorry to ask why this kind of inputs does not work correctly. It should work properly as least for dynamic batch dim.", "I am able to replicate the issue reported in tf 2.4 and nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/cf853efe2f0c63e3a176fe3bc8547e88/untitled528.ipynb).", "Was able to reproduce the issue in TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/98b22f0c014d5446a7e561453dc5ba72/untitled48.ipynb)..Thanks !"]}, {"number": 46821, "title": "ValueError: Failed to parse the model: pybind11::init(): factory function returned nullptr. when convert and quantize tf model", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: Yes\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**: source\r\n-   **TensorFlow version (use command below)**: `import tensorflow.compat.v1 as tf tf.__version__  2.4.1`\r\n-   **Python version**: Python 3.6.9\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI am trying to covert a Feature Extraction model that used in deepSort tracking to a int8 quantized tflite model, I am following the post-training quantization but failed with an error.  Actually, no matter what content in representative_data_gen() function, the error is same as in Traceback below:\r\n### Source code / logs\r\nHere is the code that converting the frozen-graph to tflite:\r\n    \r\n        import tensorflow.compat.v1 as tf\r\n        mnist = tf.keras.datasets.mnist\r\n        (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\r\n        train_images = train_images.astype(np.float32) / 255.0\r\n\r\n        def representative_data_gen():\r\n            for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\r\n                yield [input_value]\r\n        \r\n        converter = tf.lite.TFLiteConverter.from_frozen_graph(\"mars-small128.pb\",input_arrays=[\"Cast\"],output_arrays=[\"features\"],input_shapes={\"Cast\":[1, 128, 64, 3]})\r\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n        converter.inference_input_type = tf.uint8\r\n        converter.inference_output_type = tf.uint8\r\n        converter.representative_dataset = representative_data_gen\r\n        tflite_model = converter.convert()\r\n        open(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\nTraceback (most recent call last):\r\n  File \"/home/dev/.local/lib/python3.6/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 58, in __init__\r\n    _calibration_wrapper.CalibrationWrapper(model_content))\r\nTypeError: pybind11::init(): factory function returned nullptr\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"detect.py\", line 219, in <module>\r\n    main()\r\n  File \"detect.py\", line 109, in main\r\n    encoder = generate_detections.create_box_encoder(\"mars-small128.pb\", batch_size = 32)\r\n  File \"/home/dev/projects/coral/examples-camera/opencv/generate_detections.py\", line 191, in create_box_encoder\r\n    image_encoder = ImageEncoder(model_filename, input_name, output_name)\r\n  File \"/home/dev/projects/coral/examples-camera/opencv/generate_detections.py\", line 150, in __init__\r\n    tflite_model = converter.convert()\r\n  File \"/home/dev/.local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 1947, in convert\r\n    return super(TFLiteConverter, self).convert()\r\n  File \"/home/dev/.local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 1313, in convert\r\n    result = self._calibrate_quantize_model(result, **flags)\r\n  File \"/home/dev/.local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 449, in _calibrate_quantize_model\r\n    calibrate_quantize = _calibrator.Calibrator(result)\r\n  File \"/home/dev/.local/lib/python3.6/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 60, in __init__\r\n    raise ValueError(\"Failed to parse the model: %s.\" % e)\r\nValueError: Failed to parse the model: pybind11::init(): factory function returned nullptr.", "comments": ["@JiashuGuo \r\n\r\nPlease, share `mars-small128.pb` file to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "\r\n[mars-small128.pb.tar.gz](https://github.com/tensorflow/tensorflow/files/5902770/mars-small128.pb.tar.gz)\r\n\r\n", "@JiashuGuo \r\n\r\nI have tried in colab with TF version 2.4 and i am seeing the below error message.\r\n`UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 52: invalid start byte`. \r\nPlease, find the gist [here](https://colab.research.google.com/gist/ravikyram/be04157fa70cbdc11f0af50f252b9f09/untitled646.ipynb).\r\nPlease, help me in reproducing the issue. It helps me in localizing the issue faster. Thanks!", "> @JiashuGuo\r\n> \r\n> I have tried in colab with TF version 2.4 and i am seeing the below error message.\r\n> `UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 52: invalid start byte`.\r\n> Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/be04157fa70cbdc11f0af50f252b9f09/untitled646.ipynb).\r\n> Please, help me in reproducing the issue. It helps me in localizing the issue faster. Thanks!\r\n\r\nHi @ravikyram , I just ran cells again in order and error become this:\r\nValueError: Failed to parse the model: pybind11::init(): factory function returned nullptr.\r\n![gist](https://user-images.githubusercontent.com/25141355/106495270-b1fe0580-6478-11eb-8251-7bfaea7211c2.png)\r\n\r\nCan you please check it again?", "@jvishnuvardhan Have you get any ideas on this?", "@JiashuGuo As far i know `TypeError: pybind11::init(): factory function returned nullptr` is related to GPU. Thanks!\r\n\r\nI tried simple float model conversion, which works without any issues. We will check the root-cause of this error. Thanks!\r\n\r\n[gist](https://colab.research.google.com/gist/jvishnuvardhan/b05b95057bf6a17d046b7ee0e25d4e34/untitled646.ipynb) for our reference.", "Thanks for the updates! FYI I didn't use GPU to run this script. ", "@jianlijianli The model converts to float the call for calibration fails to load the model. Can you please take a look\r\n\r\nThanks", "Building `LoggingInterpreter` failed with following error.\r\n\r\n> ERROR: Only models with a single subgraph are supported, model had 3 subgraphs\r\n\r\nA fix would be desired to correctly pass the error message to Python world.\r\n", "@teijeong Will you going to make the converter to be able to convert the model that has more than one subgraph? ", "Hi Jiashu, apologies for the delay. This is a known limitation and TFLite's quantization framework currently lacks proper support of multi-graph models (and your model has 3 subgraphs). We are looking into the issue and will update this thread soon. Thanks.", "@jianlijianli, @JiashuGuo, I recently tried TFLite's quantization methods, I was able to follow Post-training float16 quantization and Post-training dynamic range quantization methods, convert my model and able to infer. However when I tried Post-training integer quantization, it gives:\r\nValueError: Failed to parse the model: pybind11::init(): factory function returned nullptr. \r\n\r\nHow do I confirm that my model has more than 1 subgraphs? Also is it normal that only Post-training integer quantization fails and gives this error even if my model has more than 1 subgraphs? Thanks.", "> @jianlijianli, @JiashuGuo, I recently tried TFLite's quantization methods, I was able to follow Post-training float16 quantization and Post-training dynamic range quantization methods, convert my model and able to infer. However when I tried Post-training integer quantization, it gives:\r\n> ValueError: Failed to parse the model: pybind11::init(): factory function returned nullptr.\r\n> \r\n> How do I confirm that my model has more than 1 subgraphs? Also is it normal that only Post-training integer quantization fails and gives this error even if my model has more than 1 subgraphs? Thanks.\r\n\r\nHi @dogacbasaran, you can view the structure and subgraphs of your model in Netron https://netron.app/. I also got this error during integer quantization but seems there is no fix for now.", "Hi @dogacbasaran, besides visualizing graph, if you try again with tf-nightly the error message should be visible. Please le me know if you still get nullptr error.", "I had the same issue. It dues to a version conflict between cuda and tensorRT, especially for my case, I use docker, it becomes more complicated to know the problem and to fix. I use latest tensorRT docker and update gpu driver of my pc to be the same as the one inside docker, the problem is solved.", "Now multiple subgraph calibration is supported with https://github.com/tensorflow/tensorflow/commit/cd5a9ad1338b01f5fd8ce64ed86ad09bf26468ee", "> Now multiple subgraph calibration is supported with [cd5a9ad](https://github.com/tensorflow/tensorflow/commit/cd5a9ad1338b01f5fd8ce64ed86ad09bf26468ee)\r\n\r\nI have installed tf-nightly and now get the following error:\r\n```\r\nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \r\nTF Select ops: TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3\r\n```\r\nPlease see the [gist](https://colab.research.google.com/drive/10CadH871KSf5Twklf0fI1rp3UGI0VhEB) in Colab. ", "As the error message says, some ops are not supported by native. I think you can try using flex delegate for those ops.", "@JiashuGuo ! I was getting Runtime error in [2.8](https://colab.sandbox.google.com/gist/mohantym/e60689239e39ee210952b90eaa401b3c/untitled646.ipynb#scrollTo=TouSYPZyuxPP). Thanks!"]}, {"number": 46806, "title": "edgetpu_compiler: \"Model not Quantized\" despite being quantized", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution: Google Colab; Ubuntu 20.04.1\r\n- TensorFlow installation (pip package or built from source): `pip`\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): Tested on `1.15.5`, `2.3.0`, and `2.4.1`\r\n\r\n### 2. Code\r\n\r\nI have followed these guides to the letter:\r\n\r\n- Creating a YOLOv3-tiny model, Converting it to Keras, then finally exporting as a tflite\r\n    - https://github.com/goruck/edge-tpu-train\r\n - Training YOLO models\r\n    - https://github.com/AlexeyAB/darknet#how-to-train-tiny-yolo-to-detect-your-custom-objects\r\n\r\nIt is as dead-stock a model as possible. I am trying to eliminate causes. I have also encountered the error while trying to convert EfficientDet-d0, YOLOv3, and YOLOv4-tiny.\r\n\r\n1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/drive/1p5TpbSZZgDVTTlrJPxfw-waAGH-HKFSy?usp=sharing): Download [this attached model](https://github.com/tensorflow/tensorflow/files/5898304/heehoo.zip). Simply drag and drop to `/content`, then run the notebook.\r\n\r\n### 3. Failure after conversion\r\n```\r\nEdge TPU Compiler version 15.0.340273435\r\nInvalid model: heehoo.tflite\r\nModel not quantized\r\n```\r\n\r\n### 5. (optional) Any other info / logs\r\nI encounter no errors during conversion from Keras to Tflite. I even verified the models using Netron, and there seems to be nothing wrong. All the layers seem to use `uint8`.\r\n\r\nThis is the worst warning I have seen, along with CUDA-related warnings:\r\n`2021-01-31 00:15:49.647787: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.`\r\n", "comments": ["@jingpu @daverim could you take a look?", "@pandalion98 The error is because tensor \"functional_1/up_sampling2d/strided_slice2\" has INT32 type. BTW, coral edge tpu doesn't support shape computation like \"Shape\" operator. I would suggest to try to avoid them."]}, {"number": 46803, "title": "Expose TString related C API", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n**System information**\r\n- TensorFlow v2.4.1:\r\n- Are you willing to contribute it (Yes):\r\n\r\n**Describe the feature and the current behavior/state.**\r\nSince v2.4.0 removed string related APIs like `TF_StringEncode`, we can't operate string in other language binding. What we need are exposing thoses APIs: \r\n* TF_CAPI_EXPORT extern TF_TString* TF_StringInit();\r\n* TF_CAPI_EXPORT extern void TF_StringCopy(TF_TString *dst, const char *src, size_t size);\r\n* TF_CAPI_EXPORT extern const char* TF_StringGetDataPointer(TF_TString* tstr);\r\n* TF_CAPI_EXPORT extern size_t TF_StringGetSize(TF_TString* tstr);\r\n* TF_CAPI_EXPORT extern void TF_StringDealloc(TF_TString* tstr);\r\n\r\n**Will this change the current api? How?**\r\nNo changing current api.\r\n\r\n**Who will benefit with this feature?**\r\nAll other language bindings.\r\n\r\n**Any Other info.**\r\nI'm willing to PR these changes.", "comments": ["I maintain github.com/tensorflow/rust, and upgrading to TensorFlow 2.4 is turning into a major headache.\r\n\r\nRemoving TF_StringEncode and friends breaks TensorFlow's compatibility guarantees.  https://www.tensorflow.org/guide/versions#what_is_covered states that the C API is covered, so a breaking change in a minor release like this should not have been allowed.  Not only does this create headaches for bindings maintainers, but it also creates headaches for their users, because the TensorFlow version is suddenly tightly coupled with the bindings library version, which was not previously the case.\r\n\r\nUsing inline functions in the C API is also hostile for other languages, because those other languages can no longer link to the symbols in the shared library, because there are no symbols to link to.  Requiring authors of other language bindings to write C is not trivial, because some of these projects (such as mine) have no C code, and suddenly needing to include it simply to wrap inline functions means manually changing the build to rely on a C compiler and all of the cross-platform headaches that come with that.", "My PR will make TFString works but in different API, still waiting for merging into master."]}, {"number": 46800, "title": "GPU Underutilized", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.5 LTS\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n- Python version: 3.7.9\r\n- CUDA/cuDNN version: CUDA Version: 11.0 \r\n- GPU model and memory: Tesla T4, 15gb\r\n\r\n**Describe the current behavior**\r\nSample code on a 1-GPU is utilizing 20% GPU.  Increasing the number of GPU's lowers utilization per GPU and increases the training total time.  There are a lot of copying from H2D and back.  Some functions, like Identity (which maybe expected) are executed on CPU. \r\n\r\n1-GPU:\r\nEpoch 2/2\r\n1000/1000 [==============================] - 4s 4ms/step\r\n\r\n8-GPU:\r\nEpoch 2/2\r\n1000/1000 [==============================] - 8s 8ms/step\r\n\r\n**Describe the expected behavior**\r\nGPU utilization should be much higher.  I am already using TF_GPU_THREAD_MODE=gpu_private option.  \r\nThe dataset is preprocessed and saved on to a snapshot, then loaded as a distributed dataset.  \r\nThe actual prod data is displaying similar characteristics to this sandbox test.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1C5gK_TshqO-fscJYIRDI6OX74b2l2Iw7\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\nhttps://github.com/iprovalo/tf/blob/main/input_pipeline_stats_benchmark.py\r\n\r\n", "comments": ["Updated the code with a more realistic scenario with more features, batch size 100, 100K samples.  GPU up to 20%, but similar patterns can be seen in the profiler.\r\n\r\n![Screen Shot 2021-01-30 at 9 04 38 AM](https://user-images.githubusercontent.com/1660997/106362998-b2f93080-62da-11eb-8bd0-60fa2f2374e3.png)\r\n![Screen Shot 2021-01-30 at 9 04 26 AM](https://user-images.githubusercontent.com/1660997/106363001-b55b8a80-62da-11eb-9dd9-78e15d38c119.png)\r\n![Screen Shot 2021-01-30 at 9 04 19 AM](https://user-images.githubusercontent.com/1660997/106363002-b55b8a80-62da-11eb-9859-15af1cace9dd.png)\r\n![Screen Shot 2021-01-30 at 9 03 57 AM](https://user-images.githubusercontent.com/1660997/106363003-b5f42100-62da-11eb-9dbe-2c65d49ce0c0.png)\r\n\r\n\r\n\r\n", "8-GPU:\r\n![Screen Shot 2021-01-30 at 9 21 56 AM](https://user-images.githubusercontent.com/1660997/106363343-d624df80-62dc-11eb-9e58-85c57545cd6a.png)\r\n![Screen Shot 2021-01-30 at 9 21 50 AM](https://user-images.githubusercontent.com/1660997/106363344-d6bd7600-62dc-11eb-9f20-48be9e5347f6.png)\r\n", "Increasing the batch size to 1K improves the GPU utilization slightly, maybe 5%.  The overall pattern seems to remain the same.", "@psobot I think this is possibly related to the issue you describe in [44194](https://github.com/tensorflow/tensorflow/issues/44194).\r\n\r\nI have tried your solution, but didn't notice any improvements for our app.  However, I am observing some ops on the CPU, which I am not sure are supposed to be there.  The TF ops on the timeline look a bit too spread out and there seems to be a lot of H2D copying.  \r\n\r\nI have a small [reference](https://github.com/iprovalo/tf/blob/main/input_pipeline_stats_benchmark.py) app to profile these issues.  I didn't include the Embeddings in my reference app.  ", "Hi @iprovalo, have you had a chance to look at the [Optimize GPU Performance](https://www.tensorflow.org/guide/gpu_performance_analysis) guide?", "@nikitamaia yes I have used the suggestions from this guide.\r\n\r\nThis is a bare bones benchmark, no metrics, preprocessed input is read from the snapshot.  \r\n\r\nMy main question is why there are still so many ops running on cpu?  I think it could be creating the kernel launching bottlenecks.\r\n\r\nDoes tf have something similar to this benchmark to demo an end-to-end optimized workflow described in the guide?", "Updated the benchmark code with embedding layer with generated data.", "I added two more configs - Dense layer dimension and a number of layers to make the model more complex.  https://github.com/iprovalo/tf/blob/main/input_pipeline_stats_benchmark.py#L32-L33\r\n\r\nWith Dense layer dim=1024 and using five layers instead of one, I can get close to 80% utilization (with 10 layers, for sure 100%).  \r\n\r\nWith more complex set up, the model's kernel launch time decreases relatively to the processing time, it's around 10% which sounds much better and a GPU profile in the tracer (see attached) looks much different, but at the same time, the absolute processing time per step is growing from 7.6ms to 48.2ms when I run it on:\r\n\r\n1-GPU (40% utilization, 7.6ms)\r\n2-GPU (50% utilization, 11.4ms)\r\n4-GPU (57% utilization, 23.3ms)\r\n8-GPU (80% utilization, 48.2ms)\r\n\r\nIf I try The MultiWorkerMirroredStrategy suggested as a work-around in https://github.com/tensorflow/tensorflow/issues/41898, I get 41.7ms per step (8-GPU).  It's not helpful since it still increases the time per step by 6X.  With tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce()) or ReductionToOneDevice the timing goes to ~52ms (8GPU). \r\n\r\nSo, the overall timing is getting worse with the addition of the GPU's (~7X for 1GPU->8GPU).  If I select top ~10 Kernels in the profiler, this reduce op takes 95%: ncclAllReduceRingLLKernel_sum_f32.  The long line in the attached tracer is this reducer.\r\n\r\nRelated issues:\r\nhttps://github.com/tensorflow/tensorflow/issues/39545\r\nhttps://github.com/tensorflow/tensorflow/issues/41898\r\n\r\nThe work-around suggested in the https://github.com/tensorflow/tensorflow/issues/41898 is not working for this case.\r\n\r\n![Screen Shot 2021-02-18 at 7 52 22 AM](https://user-images.githubusercontent.com/1660997/108391558-ae7fb380-71c6-11eb-9e0a-3779d1d6d20a.png)\r\n"]}, {"number": 46779, "title": "[Intel MKL] Fix shape inference for Quantized{Conv2D/DepthwiseConv2D}-like operations", "body": "", "comments": ["@CuiYifeng  Can you please resolve conflicts? Thanks!\r\n", "@gbaned Sorry for the slow response. The change is in the internal test.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@tensorflowbutler This PR is still in internal test.", "@gbaned @penpornk Changes done. Please continue to review, thanks!", "@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!"]}, {"number": 46766, "title": "C API to efficiently share weights and access tensor data during inferencing", "body": "**System information**\r\n- TensorFlow version (you are using): 2.3\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI believe the existing Session API is thread safe so that multiple inferences can be run simultaneously for the same model. It is not clear if doing so also shares the model weights (which are constants during inferencing) so that only a single copy of the weights is required.\r\n\r\nThe tensor input and output C APIs should have options to use input tensors in-place, from binary blobs, from either CPU/system memory or device (e.g. GPU) memory. This \"zero-copy\" API would remove any copy overhead and any overhead related to protobuf creation.\r\n\r\n**Will this change the current api? How?**\r\nYes, likely need new C API.\r\n\r\n**Who will benefit with this feature?**\r\nInference applications that need to integrate with TF library and get maximum performance.", "comments": []}, {"number": 46764, "title": "Add placement API for C interface - control subset of devices to make visible for model", "body": "**System information**\r\n- TensorFlow version: 2.3\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nDuring inference it may be necessary to control how a model is loaded onto the available devices (e.g. GPUs). Two cases that illustrate this are:\r\n- The model is created with explicit device assignments that use more/different devices then are available at inference time. The inference application should be able to query which devices are required by the model and somehow map those devices onto the set of devices that are currently available.\r\n- The model is created without explicit device assignments but the inference application wants to control which device the model should load onto (instead of loading onto the default, \"0\" device).\r\n\r\nThe existing GPU options has a visible_device_list setting that seems like it could provide at least part of a solution. Unfortunately, it seems all/most of the GPU options are actually global and not per-session / per-model and so visible_device_list is not actually a solution. This is discussed in #311 and #8136 (the second issue is closed but the underlying issue is not resolved).\r\n\r\n**Will this change the current api? How?**\r\nIt seems likely.\r\n\r\n**Who will benefit with this feature?**\r\nAny application that wants to control how a model is loaded, such as inference applications and servers.\r\n", "comments": []}, {"number": 46763, "title": "Performance and functional parity of C and Python API for the graph mode", "body": "**System information**\r\n- TF 2.3\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently the TF C/C++ API is incomplete (e.g. does not have API to save savedmodel, does not expose all optimization and configuration options) compared to the Python API. It also seems that the Python API enables or performs additional optimization passes that are not available when using the C/C++ API.\r\n\r\n**Will this change the current api? How?**\r\nYes. The C/C++ API will likely need to be enhanced significantly. Ideally this complete functionality will be implemented in a C API to ensure maximum ABI compatibility and portability.\r\n\r\n**Who will benefit with this feature?**\r\nApplications that want a more portable, high-performance solution that does not require using python. We are particularly interested in applications like model servers. A C API would be the most portable and performant and is relatively easy to integrate and wrap into other languages. Conceptually the python API (and any other language binding) should be a wrapper over the C API that presents the API in a language appropriate way but that does otherwise add (or remove) functionality.\r\n", "comments": []}, {"number": 46757, "title": "tf.linalg.triangular_solve adjoint option description is wrong", "body": "Link:\r\nhttps://www.tensorflow.org/api_docs/python/tf/linalg/triangular_solve\r\n\r\nThe following description about \"adjoint\" looks the other way around to me:\r\n\"If adjoint is True then the innermost matrices in output satisfy matrix equations sum_k matrix[..., i, k] * output[..., k, j] = rhs[..., i, j]. If adjoint is False then the innermost matrices in output satisfy matrix equations sum_k adjoint(matrix[..., i, k]) * output[..., k, j] = rhs[..., i, j].\"", "comments": ["@vicaws,\r\nCan you please elaborate your point with a code example? Thanks! ", "> @vicaws,\r\n> Can you please elaborate your point with a code example? Thanks!\r\n\r\nHey,\r\n\r\nIt is probably a documentation issue only. I'm using tf.linalg.solve and tf.linalg.triangular_solve, while both functions have an argument \"adjoint\". However, the descriptions of the use of adjoint are inconsistent for these two functions.\r\n\r\n**tf.linalg.solve:**\r\nhttps://www.tensorflow.org/api_docs/python/tf/linalg/solve\r\n\"If adjoint is **False** then each output matrix satisfies matrix[..., :, :] * output[..., :, :] = rhs[..., :, :]. If adjoint is **True** then each output matrix satisfies adjoint(matrix[..., :, :]) * output[..., :, :] = rhs[..., :, :].\"\r\n\r\n**tf.linalg.triangular_solve:**\r\nhttps://www.tensorflow.org/api_docs/python/tf/linalg/triangular_solve\r\n\"If adjoint is **True** then the innermost matrices in output satisfy matrix equations sum_k matrix[..., i, k] * output[..., k, j] = rhs[..., i, j]. If adjoint is **False** then the innermost matrices in output satisfy matrix equations sum_k adjoint(matrix[..., i, k]) * output[..., k, j] = rhs[..., i, j].\"\r\n\r\nI'm just wondering if this inconsistency is intended, and if both descriptions are correct - i.e. aligned with what the codes really do.\r\n\r\n", "Hey @vicaws  the description seems correct, can you please elaborate what the discrepancy you think is? "]}, {"number": 46753, "title": "Recommend replacement list for some incompatible ops in TFLite", "body": "Hi TensorFlow Lite Authors,  \r\n\r\nIt's glad to hear that there are some workaround for incompatible ops, such as ResizeOp, in TFLite. Would you mind share something like recommend replacement list for some of these bad ops? It will take us away from some troubles on early stage, the design stage, not the model deployment stage that things all are embarrassed to developers.\r\n\r\nLooking for your reply,\r\nSun Aries.", "comments": ["@liyunlu0618 ", "Some context. During the cn-sig group discussion about MOT, user asked about incompatible ops like ResizeNearestNeighbor and Renjie mentioned it could be replaced with other TFLite compatible ops. This request is for us to provide a list with such examples as guidance."]}, {"number": 46750, "title": "Implementation of Ordinary Differential Equations", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): The nightly build\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.** \r\nWe don't have a solution in Tensorflow for UNEVEN time series data. There are implementations using Torch for ODE (Ordinary Differential Equations) RNN.\r\n\r\n**Will this change the current api? How?**\r\nNot sure\r\n\r\n**Who will benefit with this feature?**\r\nI will be.... but this is a huge gap in Tensorflow right now. There are lots of data that are generated with uneven gap between them. Tensorflow does not have a viable solution right now.\r\n\r\n**Any Other info.**\r\nhttps://www.sciencedirect.com/science/article/abs/pii/S095219762030292X\r\nhttps://towardsdatascience.com/paper-summary-neural-ordinary-differential-equations-37c4e52df128\r\n\r\nI don't know, there are many papers on them.", "comments": ["Hello summa-code@: thanks for reporting the issue - would you be interested in making a contribution? ", "+1 to @rchao: we're open to contributions for such a feature. No plans on our side.", "And also, i heard that Tensorflow has PhasedLSTM implemented somewhere online. But i could not find it. Anyone know if it is the case ?"]}, {"number": 46745, "title": "Loading and stitching TF graphs has tf.Variable conflict even if using the name scope in loading", "body": "**System information**\r\n- OS Platform and Distribution: MacOS Catalina 10.15.6\r\n- TensorFlow installed: from binary\r\n- TensorFlow version: The issue could be reproduced by TF1.x (TF 1.15.2) and TF2.x (TF 2.4.1)\r\n- Python version: 3.6.5\r\n\r\n**Describe the current behavior**\r\nI have the following code `tf_stitch_keras_model_build_save.py` that simply creates a TF Keras model and saves it into a checkpoint. I ran the Python script twice and generated two checkpoints `./foo/model` with input shape 4 and output shape 3 (by running `$ python tf_stitch_keras_save.py -i 4 -o 3 -p ./foo/model`) and `./bar/model` with input shape 3 and output shape 2 (by running `$ python tf_stitch_keras_save.py -i 3 -o 2 -p ./bar/model`). Then I ran another Python script `python tf_stitch_keras_restore.py -p1 ./foo/model -p2 ./bar/model` that is supposed to load these two checkpoints into a new TF graph with model foo's output stitching to model bar's input.\r\n\r\n- tf_stitch_keras_model_build_save.py:\r\n```\r\nimport argparse\r\nfrom typing import Tuple\r\n\r\nimport numpy as np\r\nimport tensorflow.compat.v1 as tf\r\nfrom tensorflow.keras import models\r\nfrom tensorflow.keras import layers\r\n\r\ntf.disable_v2_behavior()\r\n\r\n\r\ndef build_model(input_shape: int, output_shape: int) -> Tuple[tf.Tensor, tf.Tensor]:\r\n    graph = tf.get_default_graph()\r\n    with graph.as_default():\r\n        model = models.Sequential()\r\n        model.add(layers.Dense(16, activation='relu', input_shape=(input_shape,), name=\"layer_0\"))\r\n        model.add(layers.Dense(16, activation='relu', name=\"layer_1\"))\r\n        model.add(layers.Dense(output_shape, activation='sigmoid', name=\"layer_2\"))\r\n\r\n        input_tensor = tf.placeholder(dtype=tf.float32, shape=[None, input_shape])\r\n        output_tensor = model(input_tensor)\r\n        graph.add_to_collection(\"INPUT\", input_tensor)\r\n        graph.add_to_collection(\"OUTPUT\", output_tensor)\r\n    return input_tensor, output_tensor\r\n\r\n\r\ndef save_model(sess: tf.Session, path: str) -> None:\r\n    saver = tf.train.Saver()\r\n    sess.run(tf.global_variables_initializer())\r\n    save_path = saver.save(sess, path)\r\n    print(f\"Saved the model in {save_path}\")\r\n\r\n\r\ndef parse_arguments() -> argparse.ArgumentParser:\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"-i\", \"--input_shape\", type=int)\r\n    parser.add_argument(\"-o\", \"--output_shape\", type=int)\r\n    parser.add_argument(\"-p\", \"--path\", type=str)\r\n    return parser\r\n\r\n\r\ndef main():\r\n    parser = parse_arguments()\r\n    args = parser.parse_args()\r\n    input_tensor, output_tensor = build_model(args.input_shape, args.output_shape)\r\n    print(f\"Finished building the model: Input tensor {input_tensor} and output tensor {output_tensor}.\")\r\n    sess = tf.Session()\r\n    sess.run(tf.global_variables_initializer())\r\n    res = sess.run(output_tensor, feed_dict={input_tensor: np.random.rand(1, args.input_shape)})\r\n    print(f\"The random output is {res}.\")\r\n    save_model(sess, args.path)\r\n\r\n\r\nmain()\r\n```\r\n-  tf_stitch_keras_restore.py:\r\n```\r\nimport argparse\r\nfrom typing import Dict, List\r\n\r\nimport tensorflow.compat.v1 as tf\r\n\r\n\r\ndef load_and_stitch_graph(\r\n    path: str,\r\n    sess: tf.Session,\r\n    input_map: Dict[str, tf.Tensor],\r\n    return_values: List[str],\r\n    scope: str = \"\",\r\n) -> List[tf.Tensor]:\r\n    \"\"\"\r\n    :param path: The path of the model to be loaded from.\r\n    :param sess: The tensorflow session for loading a model.\r\n    :param input_map: dict key is the tag name of the tensor for another Tensor as\r\n           dict value stitches to.\r\n    :param return_values: The tag name for the returned tensors.\r\n    :param scope: The name used for loading a model.\r\n    :return:\r\n    \"\"\"\r\n    # Load the checkpoint in the tmp graph.\r\n    tmp_graph = tf.Graph()\r\n    with tmp_graph.as_default():\r\n        saver = tf.train.import_meta_graph(path + \".meta\", import_scope=scope)\r\n\r\n    # Structure the input map for stitching the graph when loading.\r\n    input_map_for_import_graph_def = {\r\n        tmp_graph.get_collection(key)[0].name: val\r\n        for key, val in input_map.items()\r\n    }\r\n    return_values_for_import_graph_def: List[tf.Tensor] = [\r\n        tmp_graph.get_collection(val)[0].name\r\n        for val in return_values\r\n    ]\r\n\r\n    # Load and stitch.\r\n    with sess.graph.as_default():\r\n        return_tensors = tf.import_graph_def(\r\n            tmp_graph.as_graph_def(),\r\n            input_map=input_map_for_import_graph_def,\r\n            name=\"\",\r\n            return_elements=return_values_for_import_graph_def\r\n        )\r\n        # only restore if there's something to restore\r\n        if saver is not None:\r\n            saver.restore(sess, path)\r\n\r\n    return return_tensors\r\n\r\n\r\ndef parse_arguments() -> argparse.ArgumentParser:\r\n    \"\"\"\r\n    Stitch the model loaded from path1 to the model loaded from path2.\r\n    \"\"\"\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"-p1\", \"--path1\", type=str)\r\n    parser.add_argument(\"-p2\", \"--path2\", type=str)\r\n    return parser\r\n\r\n\r\ndef main():\r\n    parser = parse_arguments()\r\n    args = parser.parse_args()\r\n    graph = tf.get_default_graph()\r\n    sess = tf.Session(graph=graph)\r\n\r\n    with graph.as_default():\r\n        input1, output1 = load_and_stitch_graph(\r\n            path=args.path1,\r\n            sess=sess,\r\n            scope=\"foo\",\r\n            input_map={},\r\n            return_values=[\"INPUT\", \"OUTPUT\"]\r\n        )\r\n\r\n        out2 = load_and_stitch_graph(\r\n            path=args.path2,\r\n            sess=sess,\r\n            scope=\"bar\",\r\n            input_map={\"INPUT\": output1},\r\n            return_values=[\"OUTPUT\"]\r\n        )\r\n    sess.run(out2[0], feed_dict={input1: [[1, 2, 3, 4]]})\r\n\r\n\r\nmain()\r\n\r\n```\r\n\r\nHowever, when it tries to load model `bar` and stitches the graph with model `foo`, `tf.import_graph_def` crashes with the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/rukon/tf2/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1375, in _do_call\r\n    return fn(*args)\r\n  File \"/Users/rukon/tf2/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1360, in _run_fn\r\n    target_list, run_metadata)\r\n  File \"/Users/rukon/tf2/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1453, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Matrix size-incompatible: In[0]: [1,4], In[1]: [3,16]\r\n\t [[{{node foo/sequential/layer_0/Relu}}]]\r\n```\r\nAfter digging into the error, looks there is a tf.Variable conflict even though we added the name scope during loading model, and it tries to hook up the model \"bar\" first hidden layer to the model \"foo\" input, my best guess is internally somehow tf.Variables are messed up on reference. I understand Tensorflow would deprecate the usage of tf graph as well as its utilities, but currently wrt the backward compatibility with TF 1.x, I'd expect it should still work.\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nThe issue could be 100% reproduced by running the above code with the system info.\r\n\r\n", "comments": ["@RuofanKong \r\n\r\nI tried to run your code in colab and i am seeing the error as mentioned in the [gist](https://colab.research.google.com/gist/ravikyram/1a641d259acedfbfb5b2d9be8f4cfdab/untitled638.ipynb).\r\nCan you please share colab notebook or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "@ravikyram I ran it locally with provided sys info and command and that script `tf_stitch_keras_save.py` all consistently works. Are you sure you are using the exactly same Tensorflow and Python version? Also, I'm new to colab notebook, could you share more details?", "@ravikyram I see why yours don't work. When you run `tf_stitch_keras_save.py` and `tf_stitch_keras_restore.py` scripts, you need to pass the inline arguments as I shared in the reproduce steps above. Let me share it here again so you're clear on it:\r\n1. Run command: `$ python tf_stitch_keras_save.py -i 4 -o 3 -p ./foo/model` to build and save the model \"foo\".\r\n2. Run command: `$ python tf_stitch_keras_save.py -i 3 -o 2 -p ./bar/model` to build and save the model \"bar\".\r\n3. Run command: `$ python tf_stitch_keras_restore.py -p1 ./foo/model -p2 ./bar/model` to load models \"foo\" and \"bar\" and stitch them together, then you will hit the error that I showed above.\r\nThe Python version is 3.6.5, and Tensorflow version is 2.4.1.", "Thanks for the issue. I was able to repro the reported behavior with [gist](https://colab.research.google.com/gist/ymodak/72f524cdeb176eddee1fccc1059680f4/untitled638.ipynb).", "Any updates on this?"]}, {"number": 46740, "title": "Converting TF2 Object detection API model to frozen graph", "body": "Win10, Tensorflow 2.4, Object detection API bcc9ab69195489dae89ad784f882f81f90bf93e4.\r\n\r\n`ssd_resnet50_v1_fpn_640x640_coco17_tpu-8` model trained using Tensorflow object detection API using\r\nhttps://github.com/tensorflow/models/blob/master/research/object_detection/model_main_tf2.py\r\n\r\nAfter exported to `Save model`:\r\n `.\\exporter_main_v2.py --input_type image_tensor --pipeline_config_path .\\models\\my_ssd_resnet50_v1_fpn\\pipeline.config --trained_checkpoint_dir .\\models\\my_ssd_resnet50_v1_fpn\\ --output_directory .\\exported-models\\models\\Bel_model` using\r\nhttps://github.com/tensorflow/models/blob/master/research/object_detection/exporter_main_v2.py\r\n\r\nOn this step inference work fine with Tensorflow.\r\n\r\nInference from `Saved model`:\r\n<details>\r\n\r\n````\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    # Suppress TensorFlow logging (1)\r\nimport pathlib\r\nimport tensorflow as tf\r\n\r\ntf.get_logger().setLevel('ERROR')           # Suppress TensorFlow logging (2)\r\n\r\n# Enable GPU dynamic memory allocation\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nfor gpu in gpus:\r\n    tf.config.experimental.set_memory_growth(gpu, True)\r\n\r\n\r\nPATH_TO_LABELS = \"label_map.pbtxt\" \r\n\r\n# %%\r\n# Load the model\r\n# ~~~~~~~~~~~~~~\r\n# Next we load the downloaded model\r\nimport time\r\nfrom object_detection.utils import label_map_util\r\nfrom object_detection.utils import visualization_utils as viz_utils\r\n\r\nPATH_TO_SAVED_MODEL = \"models\\\\Bel_model\\\\saved_model\"\r\n\r\nprint('Loading model...', end='')\r\nstart_time = time.time()\r\n\r\n# Load saved model and build the detection function\r\ndetect_fn = tf.saved_model.load(PATH_TO_SAVED_MODEL)\r\n\r\nend_time = time.time()\r\nelapsed_time = end_time - start_time\r\nprint('Done! Took {} seconds'.format(elapsed_time))\r\n\r\n# %%\r\n# Load label map data (for plotting)\r\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n# Label maps correspond index numbers to category names, so that when our convolution network\r\n# predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility\r\n# functions, but anything that returns a dictionary mapping integers to appropriate string labels\r\n# would be fine.\r\n\r\ncategory_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS,\r\n                                                                    use_display_name=True)\r\n\r\n# %%\r\n# Putting everything together\r\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n# The code shown below loads an image, runs it through the detection model and visualizes the\r\n# detection results, including the keypoints.\r\n#\r\n# Note that this will take a long time (several minutes) the first time you run this code due to\r\n# tf.function's trace-compilation --- on subsequent runs (e.g. on new images), things will be\r\n# faster.\r\n#\r\n# Here are some simple things to try out if you are curious:\r\n#\r\n# * Modify some of the input images and see if detection still works. Some simple things to try out here (just uncomment the relevant portions of code) include flipping the image horizontally, or converting to grayscale (note that we still expect the input image to have 3 channels).\r\n# * Print out `detections['detection_boxes']` and try to match the box locations to the boxes in the image.  Notice that coordinates are given in normalized form (i.e., in the interval [0, 1]).\r\n# * Set ``min_score_thresh`` to other values (between 0 and 1) to allow more detections in or to filter out more detections.\r\nimport numpy as np\r\nfrom PIL import Image\r\nimport matplotlib.pyplot as plt\r\nimport warnings\r\nwarnings.filterwarnings('ignore')   # Suppress Matplotlib warnings\r\n\r\ndef load_image_into_numpy_array(path):\r\n    \"\"\"Load an image from file into a numpy array.\r\n\r\n    Puts image into numpy array to feed into tensorflow graph.\r\n    Note that by convention we put it into a numpy array with shape\r\n    (height, width, channels), where channels=3 for RGB.\r\n\r\n    Args:\r\n      path: the file path to the image\r\n\r\n    Returns:\r\n      uint8 numpy array with shape (img_height, img_width, 3)\r\n    \"\"\"\r\n    return np.array(Image.open(path))\r\n\r\nIMAGE_PATHS = ['1.png', '2.png']\r\n\r\nfor image_path in IMAGE_PATHS:\r\n\r\n    print('Running inference for {}... '.format(image_path), end='')\r\n\r\n    image_np = load_image_into_numpy_array(image_path)\r\n\r\n    # Things to try:\r\n    # Flip horizontally\r\n    # image_np = np.fliplr(image_np).copy()\r\n\r\n    # Convert image to grayscale\r\n    # image_np = np.tile(\r\n    #     np.mean(image_np, 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\r\n\r\n    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\r\n    input_tensor = tf.convert_to_tensor(image_np)\r\n    # The model expects a batch of images, so add an axis with `tf.newaxis`.\r\n    input_tensor = input_tensor[tf.newaxis, ...]\r\n\r\n    # input_tensor = np.expand_dims(image_np, 0)\r\n    detections = detect_fn(input_tensor)\r\n\r\n    # All outputs are batches tensors.\r\n    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\r\n    # We're only interested in the first num_detections.\r\n    num_detections = int(detections.pop('num_detections'))\r\n    detections = {key: value[0, :num_detections].numpy()\r\n                   for key, value in detections.items()}\r\n    detections['num_detections'] = num_detections\r\n\r\n    # detection_classes should be ints.\r\n    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\r\n\r\n    image_np_with_detections = image_np.copy()\r\n\r\n    viz_utils.visualize_boxes_and_labels_on_image_array(\r\n          image_np_with_detections,\r\n          detections['detection_boxes'],\r\n          detections['detection_classes'],\r\n          detections['detection_scores'],\r\n          category_index,\r\n          use_normalized_coordinates=True,\r\n          max_boxes_to_draw=200,\r\n          min_score_thresh=.30,\r\n          agnostic_mode=False)  \r\n    \r\n    plt.figure()\r\n    plt.imshow(image_np_with_detections)\r\n    \r\n    #***\r\n    newname =  image_path.replace('.', '_out.') \r\n    plt.savefig(newname)\r\n    \r\n    print('Done')\r\nplt.show()\r\n\r\n# sphinx_gallery_thumbnail_number = 2\r\n````\r\n</details>\r\n\r\nInference from `Checkpoint`:\r\n<details>\r\n\r\n````\r\n#!/usr/bin/env python\r\n# coding: utf-8\r\n\"\"\"\r\nObject Detection From TF2 Checkpoint\r\n====================================\r\n\"\"\"\r\n\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    # Suppress TensorFlow logging (1)\r\nimport pathlib\r\nimport tensorflow as tf\r\n\r\nPATH_TO_LABELS = \"label_map.pbtxt\" \r\nPATH_TO_MODEL_DIR = \"models/Bel_model\"\r\n\r\n# %%\r\n# Load the model\r\n# ~~~~~~~~~~~~~~\r\n# Next we load the downloaded model\r\nimport time\r\nfrom object_detection.utils import label_map_util\r\nfrom object_detection.utils import config_util\r\nfrom object_detection.utils import visualization_utils as viz_utils\r\nfrom object_detection.builders import model_builder\r\n\r\nPATH_TO_CFG = PATH_TO_MODEL_DIR + \"/pipeline.config\"\r\nPATH_TO_CKPT = PATH_TO_MODEL_DIR + \"/checkpoint\"\r\n\r\nprint('Loading model... ', end='')\r\nstart_time = time.time()\r\n\r\n# Load pipeline config and build a detection model\r\nconfigs = config_util.get_configs_from_pipeline_file(PATH_TO_CFG)\r\nmodel_config = configs['model']\r\ndetection_model = model_builder.build(model_config=model_config, is_training=False)\r\n\r\n# Restore checkpoint\r\nckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\r\nckpt.restore(os.path.join(PATH_TO_CKPT, 'ckpt-0')).expect_partial()\r\n\r\n@tf.function\r\ndef detect_fn(image):\r\n    \"\"\"Detect objects in image.\"\"\"\r\n\r\n    image, shapes = detection_model.preprocess(image)\r\n    prediction_dict = detection_model.predict(image, shapes)\r\n    detections = detection_model.postprocess(prediction_dict, shapes)\r\n\r\n    return detections\r\n\r\nend_time = time.time()\r\nelapsed_time = end_time - start_time\r\nprint('Done! Took {} seconds'.format(elapsed_time))\r\n\r\n# %%\r\n# Load label map data (for plotting)\r\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n# Label maps correspond index numbers to category names, so that when our convolution network\r\n# predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility\r\n# functions, but anything that returns a dictionary mapping integers to appropriate string labels\r\n# would be fine.\r\n\r\ncategory_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS,\r\n                                                                    use_display_name=True)\r\n\r\n# %%\r\n# Putting everything together\r\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n# The code shown below loads an image, runs it through the detection model and visualizes the\r\n# detection results, including the keypoints.\r\n#\r\n# Note that this will take a long time (several minutes) the first time you run this code due to\r\n# tf.function's trace-compilation --- on subsequent runs (e.g. on new images), things will be\r\n# faster.\r\n#\r\n# Here are some simple things to try out if you are curious:\r\n#\r\n# * Modify some of the input images and see if detection still works. Some simple things to try out here (just uncomment the relevant portions of code) include flipping the image horizontally, or converting to grayscale (note that we still expect the input image to have 3 channels).\r\n# * Print out `detections['detection_boxes']` and try to match the box locations to the boxes in the image.  Notice that coordinates are given in normalized form (i.e., in the interval [0, 1]).\r\n# * Set ``min_score_thresh`` to other values (between 0 and 1) to allow more detections in or to filter out more detections.\r\nimport numpy as np\r\nfrom PIL import Image\r\nimport matplotlib.pyplot as plt\r\nimport warnings\r\nwarnings.filterwarnings('ignore')   # Suppress Matplotlib warnings\r\n\r\ndef load_image_into_numpy_array(path):\r\n    \"\"\"Load an image from file into a numpy array.\r\n\r\n    Puts image into numpy array to feed into tensorflow graph.\r\n    Note that by convention we put it into a numpy array with shape\r\n    (height, width, channels), where channels=3 for RGB.\r\n\r\n    Args:\r\n      path: the file path to the image\r\n\r\n    Returns:\r\n      uint8 numpy array with shape (img_height, img_width, 3)\r\n    \"\"\"\r\n    return np.array(Image.open(path))\r\n\r\nIMAGE_PATHS = ['1.png', '2.png']\r\n\r\nfor image_path in IMAGE_PATHS:\r\n\r\n    print('Running inference for {}... '.format(image_path), end='')\r\n\r\n    image_np = load_image_into_numpy_array(image_path)        \r\n    \r\n    plt.figure()\r\n    plt.imshow(image_np)\r\n\r\n    # Things to try:\r\n    # Flip horizontally\r\n    # image_np = np.fliplr(image_np).copy()\r\n\r\n    # Convert image to grayscale\r\n    # image_np = np.tile(\r\n    #     np.mean(image_np, 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\r\n\r\n    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\r\n\r\n    detections = detect_fn(input_tensor)\r\n\r\n    # All outputs are batches tensors.\r\n    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\r\n    # We're only interested in the first num_detections.\r\n    num_detections = int(detections.pop('num_detections'))\r\n    detections = {key: value[0, :num_detections].numpy()\r\n                  for key, value in detections.items()}\r\n    detections['num_detections'] = num_detections\r\n\r\n    # detection_classes should be ints.\r\n    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\r\n\r\n    label_id_offset = 1\r\n    image_np_with_detections = image_np.copy()\r\n\r\n    viz_utils.visualize_boxes_and_labels_on_image_array(\r\n            image_np_with_detections,\r\n            detections['detection_boxes'],\r\n            detections['detection_classes']+label_id_offset,\r\n            detections['detection_scores'],\r\n            category_index,\r\n            use_normalized_coordinates=True,\r\n            max_boxes_to_draw=200,\r\n            min_score_thresh=.30,\r\n            agnostic_mode=False)\r\n\r\n    plt.figure()\r\n    plt.imshow(image_np_with_detections)  \r\n    \r\n    #***\r\n    newname =  image_path.replace('.', '_out_cp.') \r\n    plt.savefig(newname)\r\n    \r\n    print('Done')\r\nplt.show()\r\n\r\n# sphinx_gallery_thumbnail_number = 2\r\n````\r\n</details>\r\n\r\n\r\nBut when I try to convert `Saved model` to `Frozen graph` \r\n````\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\nfrom tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\r\n\r\nloaded = tf.saved_model.load('models/mnist_test')\r\ninfer = loaded.signatures['serving_default']\r\n\r\nf = tf.function(infer).get_concrete_function(flatten_input=tf.TensorSpec(shape=[None, 28, 28, 1], dtype=tf.float32))\r\nf2 = convert_variables_to_constants_v2(f)\r\ngraph_def = f2.graph.as_graph_def()\r\n\r\n# Export frozen graph\r\nwith tf.io.gfile.GFile('frozen_graph.pb', 'wb') as f:\r\n   f.write(graph_def.SerializeToString())\r\n````\r\nas suggested here\r\nhttps://github.com/opencv/opencv/issues/16879#issuecomment-603815872\r\nI receive error:\r\n<details>\r\n\r\n````\r\n2021-01-27 14:20:25.689687: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n2021-01-27 14:20:25.689761: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2021-01-27 14:20:33.819028: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-27 14:20:33.819694: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\r\n2021-01-27 14:20:33.821661: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\r\n2021-01-27 14:20:33.825189: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: Eclipse\r\n2021-01-27 14:20:33.825303: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: Eclipse\r\n2021-01-27 14:20:33.826001: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-01-27 14:20:33.826483: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\nTraceback (most recent call last):\r\n  File \".\\frozen_graph.py\", line 8, in <module>\r\n    f = tf.function(infer).get_concrete_function(input_1=tf.TensorSpec(shape=[None, 640, 640, 3], dtype=tf.float32))\r\n  File \"C:\\Users\\Bleach\\miniconda3\\envs\\TFstd\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 1299, in get_concrete_function\r\n    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)\r\n  File \"C:\\Users\\Bleach\\miniconda3\\envs\\TFstd\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 1205, in _get_concrete_function_garbage_collected\r\n    self._initialize(args, kwargs, add_initializers_to=initializers)\r\n  File \"C:\\Users\\Bleach\\miniconda3\\envs\\TFstd\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 725, in _initialize\r\n    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n  File \"C:\\Users\\Bleach\\miniconda3\\envs\\TFstd\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2969, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _ = self._maybe_define_function(args, kwargs)\r\n  File \"C:\\Users\\Bleach\\miniconda3\\envs\\TFstd\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3361, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"C:\\Users\\Bleach\\miniconda3\\envs\\TFstd\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3196, in _create_graph_function\r\n    func_graph_module.func_graph_from_py_func(\r\n  File \"C:\\Users\\Bleach\\miniconda3\\envs\\TFstd\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 990, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"C:\\Users\\Bleach\\miniconda3\\envs\\TFstd\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 634, in wrapped_fn\r\n    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"C:\\Users\\Bleach\\miniconda3\\envs\\TFstd\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 977, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nTypeError: in user code:\r\n\r\n    C:\\Users\\Bleach\\miniconda3\\envs\\TFstd\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1669 __call__  *\r\n        return self._call_impl(args, kwargs)\r\n    C:\\Users\\Bleach\\miniconda3\\envs\\TFstd\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1685 _call_impl  **\r\n        raise structured_err\r\n    C:\\Users\\Bleach\\miniconda3\\envs\\TFstd\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1678 _call_impl\r\n        return self._call_with_structured_signature(args, kwargs,\r\n    C:\\Users\\Bleach\\miniconda3\\envs\\TFstd\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1756 _call_with_structured_signature\r\n        self._structured_signature_check_missing_args(args, kwargs)\r\n    C:\\Users\\Bleach\\miniconda3\\envs\\TFstd\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1775 _structured_signature_check_missing_args\r\n        raise TypeError(\"{} missing required arguments: {}\".format(\r\n\r\n    TypeError: signature_wrapper(*, input_tensor) missing required arguments: input_tensor\r\n````\r\n</details>", "comments": ["First, it seems that you have a CUDA environment problem. The 1st line of error says that you don't have CUDA11 properly installed.\r\n\r\nIf you change the line\r\n`f = tf.function(infer).get_concrete_function(input_1=tf.TensorSpec(shape=[None, 640, 640, 3], dtype=tf.float32))`\r\nto \r\n`f = tf.function(infer).get_concrete_function(input=tf.TensorSpec(shape=[None, 640, 640, 3], dtype=tf.float32))`\r\nthen probably you are able to get the frozen model. But in my case, this frozen model has problems and can't be loaded.\r\n\r\nStill looking for a solution ...", "```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2_as_graph\r\nfrom tensorflow.lite.python.util import run_graph_optimizations, get_grappler_config\r\nimport numpy as np\r\n\r\ndef convert_saved_model_to_pb(output_node_names, input_saved_model_dir, output_graph_dir):\r\n    from tensorflow.python.tools import freeze_graph\r\n\r\n    output_node_names = ','.join(output_node_names)\r\n\r\n    freeze_graph.freeze_graph(input_graph=None, input_saver=None,\r\n                              input_binary=None,\r\n                              input_checkpoint=None,\r\n                              output_node_names=output_node_names,\r\n                              restore_op_name=None,\r\n                              filename_tensor_name=None,\r\n                              output_graph=output_graph_dir,\r\n                              clear_devices=None,\r\n                              initializer_nodes=None,\r\n                              input_saved_model_dir=input_saved_model_dir)\r\n\r\n\r\ndef save_output_tensor_to_pb():\r\n    output_names = ['StatefulPartitionedCall']\r\n    save_pb_model_path = 'model/freeze_graph.pb'\r\n    model_dir = 'graph/saved_model'\r\n    convert_saved_model_to_pb(output_names, model_dir, save_pb_model_path)\r\n\r\n\r\nsave_output_tensor_to_pb()\r\n```\r\n\r\nworks for me", "> ```\r\n> import tensorflow as tf\r\n> from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2_as_graph\r\n> from tensorflow.lite.python.util import run_graph_optimizations, get_grappler_config\r\n> import numpy as np\r\n> \r\n> def convert_saved_model_to_pb(output_node_names, input_saved_model_dir, output_graph_dir):\r\n>     from tensorflow.python.tools import freeze_graph\r\n> \r\n>     output_node_names = ','.join(output_node_names)\r\n> \r\n>     freeze_graph.freeze_graph(input_graph=None, input_saver=None,\r\n>                               input_binary=None,\r\n>                               input_checkpoint=None,\r\n>                               output_node_names=output_node_names,\r\n>                               restore_op_name=None,\r\n>                               filename_tensor_name=None,\r\n>                               output_graph=output_graph_dir,\r\n>                               clear_devices=None,\r\n>                               initializer_nodes=None,\r\n>                               input_saved_model_dir=input_saved_model_dir)\r\n> \r\n> \r\n> def save_output_tensor_to_pb():\r\n>     output_names = ['StatefulPartitionedCall']\r\n>     save_pb_model_path = 'model/freeze_graph.pb'\r\n>     model_dir = 'graph/saved_model'\r\n>     convert_saved_model_to_pb(output_names, model_dir, save_pb_model_path)\r\n> \r\n> \r\n> save_output_tensor_to_pb()\r\n> ```\r\n> \r\n> \u4e3a\u6211\u5de5\u4f5c\r\n\r\n\r\n\r\n> ```\r\n> import tensorflow as tf\r\n> from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2_as_graph\r\n> from tensorflow.lite.python.util import run_graph_optimizations, get_grappler_config\r\n> import numpy as np\r\n> \r\n> def convert_saved_model_to_pb(output_node_names, input_saved_model_dir, output_graph_dir):\r\n>     from tensorflow.python.tools import freeze_graph\r\n> \r\n>     output_node_names = ','.join(output_node_names)\r\n> \r\n>     freeze_graph.freeze_graph(input_graph=None, input_saver=None,\r\n>                               input_binary=None,\r\n>                               input_checkpoint=None,\r\n>                               output_node_names=output_node_names,\r\n>                               restore_op_name=None,\r\n>                               filename_tensor_name=None,\r\n>                               output_graph=output_graph_dir,\r\n>                               clear_devices=None,\r\n>                               initializer_nodes=None,\r\n>                               input_saved_model_dir=input_saved_model_dir)\r\n> \r\n> \r\n> def save_output_tensor_to_pb():\r\n>     output_names = ['StatefulPartitionedCall']\r\n>     save_pb_model_path = 'model/freeze_graph.pb'\r\n>     model_dir = 'graph/saved_model'\r\n>     convert_saved_model_to_pb(output_names, model_dir, save_pb_model_path)\r\n> \r\n> \r\n> save_output_tensor_to_pb()\r\n> ```\r\n> \r\n> works for me\r\n well,  I have succeeded according to your program,  but How to use it in OpenCV.\r\n\r\nI tried the following:\r\n\r\ncvNet = cv.dnn.readNetFromTensorflow(r\"D:\\Vs_Projects\\py_Project\\TF-Total\\ObjDetection\\Tensorflow\\workspace\\models\\my_ssd_mobnet\\export\\saved_model\\ssd_mobilenet_v2_320x320_coco17_tpu-8_freeze_graph.pb\")\r\n\r\nimg = cv.imread(r\"D:\\Vs_Projects\\py_Project\\TF-Total\\ObjDetection\\Tensorflow\\workspace\\images\\test\\eye_20210709_100249_915.jpg\")\r\nrows = img.shape[0]\r\ncols = img.shape[1]\r\ncvNet.setInput(cv.dnn.blobFromImage(img, size=(300, 300), swapRB=True, crop=False))\r\ncvOut = cvNet.forward('StatefulPartitionedCall')\r\n\r\nbut something was wrong:\r\ncvOut = cvNet.forward('StatefulPartitionedCall')\r\ncv2.error: OpenCV(4.5.2) C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-req-build-t9hleyt8\\opencv\\modules\\dnn\\src\\dnn.cpp:621: error: (-2:Unspecified error) Can't create layer \"StatefulPartitionedCall\" of type \"StatefulPartitionedCall\" in function 'cv::dnn::dnn4_v20210301::LayerData::getLayerInstance'\r\n\r\nThanks.\r\n\r\n", "> ```\r\n> import tensorflow as tf\r\n> from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2_as_graph\r\n> from tensorflow.lite.python.util import run_graph_optimizations, get_grappler_config\r\n> import numpy as np\r\n> \r\n> def convert_saved_model_to_pb(output_node_names, input_saved_model_dir, output_graph_dir):\r\n>     from tensorflow.python.tools import freeze_graph\r\n> \r\n>     output_node_names = ','.join(output_node_names)\r\n> \r\n>     freeze_graph.freeze_graph(input_graph=None, input_saver=None,\r\n>                               input_binary=None,\r\n>                               input_checkpoint=None,\r\n>                               output_node_names=output_node_names,\r\n>                               restore_op_name=None,\r\n>                               filename_tensor_name=None,\r\n>                               output_graph=output_graph_dir,\r\n>                               clear_devices=None,\r\n>                               initializer_nodes=None,\r\n>                               input_saved_model_dir=input_saved_model_dir)\r\n> \r\n> \r\n> def save_output_tensor_to_pb():\r\n>     output_names = ['StatefulPartitionedCall']\r\n>     save_pb_model_path = 'model/freeze_graph.pb'\r\n>     model_dir = 'graph/saved_model'\r\n>     convert_saved_model_to_pb(output_names, model_dir, save_pb_model_path)\r\n> \r\n> \r\n> save_output_tensor_to_pb()\r\n> ```\r\n> \r\n> works for me\r\n\r\nIt only works when you are able to load it with OpenCV dnn module... until then you have nothing. The exported frozen model from you function is not usable... hence not working", "Hi,\r\n\r\nThanks for checking ideas and work arounds so far.\r\n\r\nExporting to ONNX didn't work for me either in OpenCV 4.5.1.\r\nYou can export the frozen graph as a *.pb file via the ONNX export tool tf2onnx.convert() by adding the appropriate keyword. The OpenCV tf_text_graph_faster_rcnn.py seems not to work with this exported graph. Maybe one can load the frozen graph directly without tf_text_graph_faster_rcnn.py (not tested).\r\n\r\nHowever, you can load your models and do inference with the Tensorflow C-API of Tensorflow-2.7. These resources should help:\r\n\r\nhttps://github.com/AmirulOm/tensorflow_capi_sample\r\nhttps://danishshres.medium.com/tensorflow-1-vs-tensorflow-2-c-api-d94982a5eb16 https://github.com/danishshres/ssd_c_api/tree/tf_2\r\nhttps://danishshres.medium.com/tensorflow-1-vs-tensorflow-2-c-api-d94982a5eb16\r\n\r\nHope this helps.\r\n"]}, {"number": 46733, "title": "Support request for tanh activation function during quantization aware training (TensorFlow Model Optimization)", "body": "**System information**\r\n- TensorFlow version (you are using): 2.4.1\r\n- Are you willing to contribute it (Yes/No): Yes \r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nWhen using `tfmot.quantization.keras.quantize_model()` to convert a `tf.keras.model` that uses a `tanh` activation function to a `tf.keras.model prepared for quantization,` the following error message appears:\r\n\r\n**Case I:** ValueError: Only some Keras activations under `tf.keras.activations` are supported. For other activations, use `Quantizer` directly, and update layer config using `QuantizeConfig`.\r\n\r\nMinimal example to reproduce:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_model_optimization as tfmot\r\n\r\ni = tf.keras.layers.Input(shape=(24, 24, 3))\r\nx = tf.keras.layers.Conv2D(10, kernel_size=1, activation='tanh')(i)\r\nmodel = tf.keras.Model(inputs=i, outputs=x)\r\n\r\nquant_aware_model = tfmot.quantization.keras.quantize_model(model)\r\n```\r\n\r\n**Case II:** Even worse - when specifying the activation function explicitly as `tf.nn.tanh(x)`, one receives the following misleading error message:\r\n\r\nTypeError: in user code: TypeError: tf__call() got an unexpected keyword argument 'name'\r\n\r\nMinimal example to reproduce:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_model_optimization as tfmot\r\n\r\ni = tf.keras.layers.Input(shape=(24, 24, 3))\r\nx = tf.keras.layers.Conv2D(10, kernel_size=1)(i)\r\nx = tf.nn.tanh(x)\r\nmodel = tf.keras.Model(inputs=i, outputs=x)\r\n\r\nquant_aware_model = tfmot.quantization.keras.quantize_model(model)\r\n```\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo, I'd expect minor changes. Please have a look at the class [QuantizeAwareActivation](https://github.com/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/python/core/quantization/keras/quantize_aware_activation.py): in line 76-77 the author states:\r\n\r\n\" TODO(pulkitb): Other activations such as elu, tanh etc., should just work on inclusion. Verify in TFLite before enabling.\"\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAll users that are interested in building tiny applications beyond classification/ recognition, which mainly uses softmax/ sigmoid activations.\r\n\r\n**Any Other info.**\r\n\r\nalternatively: please provide me with some more information, on how \"to use the `Quantizer` directly\", to replace the tanh activation function. Thank you very much. ", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/fb7ff601eff3309556e56711d28cf264/46733.ipynb). Thanks!", "any updates @daverim ?", "@nutsiepully any suggestion? ", "any suggestion on this issue? I am encountering the same error. ", "Hi @raminmohammadi, \r\n\r\nAs a workaround, I manually insert fake quantization nodes into the computation graph:\r\n\r\n#### 1. Define regular tf.keras model\r\n```\r\ntiny_model = ...\r\n```\r\n\r\n#### 2. Build model without tail (=tanh activation function)\r\n```\r\nidx = -3 # adjust idx to your use case (probably -2, check it by yourself)\r\nmodel_no_tail = tf.keras.Model(inputs=tiny_model.input, outputs=tiny_model.layers[idx].output)\r\n```\r\n\r\n#### 3. Make model_no_tail quantization aware\r\n\r\n```\r\nquant_aware_model = tfmot.quantization.keras.quantize_model(model_no_tail)\r\n```\r\n\r\n#### 4. Add tail manually, by inserting a fake quantization node into the computation graph (simulates exact same behavior)\r\n```\r\ndef add_tail(quant_aware_model):\r\n    base_model=quant_aware_model\r\n    x = tf.nn.tanh(base_model.output)\r\n    x = tf.quantization.fake_quant_with_min_max_args(x, min=-1, max=1, num_bits=8, narrow_range=False, name=None)\r\n    model=tf.keras.Model(inputs=base_model.input, outputs=x)\r\n    return model\r\n\r\nquant_aware_model = add_tail(quant_aware_model)\r\nquant_aware_model.summary()\r\n```\r\n\r\n#### 5. Convert to fully-integer model (MCUs) after training:\r\n\r\n```\r\ntflite_models_dir = pathlib.Path(model_path)\r\ntflite_models_dir.mkdir(exist_ok=True, parents=True)\r\nquantized_tflite_encoder = tflite_models_dir/\"tiny_model.tflite\"\r\n\r\nif not os.path.exists(model_path + \"tiny_model.tflite\"):\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.inference_input_type = tf.int8\r\n    converter.inference_output_type = tf.int8\r\n\r\n    tflite_model_quant = converter.convert()\r\n    \r\n    # Write it out to a tflite file:\r\n    quantized_tflite_encoder.write_bytes(tflite_model_quant)\r\n```\r\n\r\nHope that helps, good luck with your work! In case you find a better solution, feel free to share it here :)"]}, {"number": 46727, "title": "FR for keras.preprocessing.image_dataset_from_directory label_mode parameter to accept enum as input", "body": "**System information**\r\n- TensorFlow version (you are using): 2.4.1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nWhen calling the `tf.keras.preprocessing.image_dataset_from_directory()` function, the parameter `label_mode=` accepts 3 values: `int`, `categorical` and `binary`. However, the string object is not deterministic: for that, I propose to add another option, which would be to specify the value using an enumeration which IS deterministic.\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\n\r\nUsers who want more structured code.\r\n\r\n**Any Other info.**\r\n\r\nThis behavior is, as far as I know, already available for some features in Keras.", "comments": ["> However, the string object is not deterministic: for that, I propose to add another option, which would be to specify the value using an enumeration which IS deterministic.\r\n\r\nWhat does that mean?", "> > However, the string object is not deterministic: for that, I propose to add another option, which would be to specify the value using an enumeration which IS deterministic.\r\n> \r\n> What does that mean?\r\n\r\nBy accepting a string as input, typos can be made and no autocompletion can be offered by the IDE, which is however the case when using an enum.\r\n\r\nMy proposition is to add support for an alternative input type using an enumeration, such as `tf.keras.preprocessing.label_mode.[INT|CATEGORICAL|BINARY]` so that autocompletion can be made, typos are avoided, and users would still be free to specify a string `\"categorical\"` instead of `tf.keras.preprocessing.label_mode.CATEGORICAL` if they want.\r\n\r\nWhat do you think?"]}, {"number": 46726, "title": "different result on dict of tensors by getting `_enable_dict_to_input_mapping` in tf.python.keras.engine.functional.py", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Linux Ubuntu 18.04 \r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.4.1, 2.3.2, 2.3.0, cpu, gpu both\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\n\r\n`_enable_dict_to_input_mapping` of same dict of tensors `{'text': (a, b)}` is `False` in tf 2.4.1, but `True` in tf 2.3.2\r\n\r\n**Describe the expected behavior**\r\n\r\nexpect same False or True\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nTensorflow 2.3.2\r\n```python\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'2.3.2'\r\n>>> # copy from https://github.com/tensorflow/tensorflow/blob/9edbe5075f79a4a95ed14a2be831f9b59e61f49d/tensorflow/python/keras/engine/functional.py#L136\r\n>>> from tensorflow.python.util import nest\r\n>>> def get_enable_dict_to_input_mapping_232(_nested_inputs):\r\n...     _enable_dict_to_input_mapping = (not nest.is_sequence(_nested_inputs) or (isinstance(_nested_inputs, (list, tuple, dict)) and not any(nest.is_sequence(t) for t in _nested_inputs)))\r\n...     return _enable_dict_to_input_mapping\r\n\r\n>>> a = tf.keras.Input(shape=(None,), dtype=tf.int32, name='a')\r\n>>> b = tf.keras.Input(shape=(None,), dtype=tf.int32, name='b')\r\n>>> c = {'text': (a, b) }\r\n\r\n>>> get_enable_dict_to_input_mapping_232(c)\r\nTrue\r\n```\r\n\r\n\r\nTensorflow 2.4.1\r\n```python\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'2.4.1'\r\n>>> # copy from https://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/keras/engine/functional.py#L142\r\n>>> from tensorflow.python.util import nest\r\n>>> def get_enable_dict_to_input_mapping_241(_nested_inputs):\r\n...     if not nest.is_nested(_nested_inputs):\r\n...         _enable_dict_to_input_mapping = True\r\n...     elif (isinstance(_nested_inputs, (list, tuple)) and not any(nest.is_nested(t) for t in _nested_inputs)):\r\n...         _enable_dict_to_input_mapping = True\r\n...     elif (isinstance(_nested_inputs, dict) and not any(nest.is_nested(t) for t in _nested_inputs.values())):\r\n...         _enable_dict_to_input_mapping = True\r\n...     else:\r\n...         _enable_dict_to_input_mapping = False\r\n...     return _enable_dict_to_input_mapping\r\n\r\n>>> a = tf.keras.Input(shape=(None,), dtype=tf.int32, name='a')\r\n>>> b = tf.keras.Input(shape=(None,), dtype=tf.int32, name='b')\r\n>>> c = {'text': (a, b) }\r\n\r\n>>> get_enable_dict_to_input_mapping(c)\r\nFalse\r\n```\r\n\r\n`_enable_dict_to_input_mapping` of same dict of tensors `{'text': (a, b)}` is `False` in tf 2.4.1 and `True` in tf 2.3.2\r\n\r\nThis leads to different result in `_flatten_to_reference_inputs` defined in [Functional._flatten_to_reference_inputs](https://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/keras/engine/functional.py#L574)\r\n\r\n- in tf 2.3.2, for `inputs = {'text': (a, b)}`, returns `[(a, b)]`, and then raise Exception  `AttributeError: 'tuple' object has no attribute '_keras_mask'` in [Functional._run_internal_graph](https://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/keras/engine/functional.py#L536) while assigning `input_t._keras_mask = mask`\r\n- in tf 2.4.1, for `inputs = {'text': (a, b)}`, returns `[a, b]`, and then all is okay while assigning `input_t._keras_mask = mask`\r\n\r\n\r\n**Refs**\r\n\r\ndifference impl of getting `_enable_dict_to_input_mapping` in `tf.python.keras.engine.functional.py` between tf 2.4.1-[_enable_dict_to_input_mapping](https://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/keras/engine/functional.py#L142) and tf 2.3.2 - [_enable_dict_to_input_mapping](https://github.com/tensorflow/tensorflow/blob/9edbe5075f79a4a95ed14a2be831f9b59e61f49d/tensorflow/python/keras/engine/functional.py#L136)\r\n\r\ntf 2.4.1\r\n```python\r\n    if not nest.is_nested(self._nested_inputs):\r\n      self._enable_dict_to_input_mapping = True\r\n    elif (isinstance(self._nested_inputs, (list, tuple)) and\r\n          not any(nest.is_nested(t) for t in self._nested_inputs)):\r\n      self._enable_dict_to_input_mapping = True\r\n    elif (isinstance(self._nested_inputs, dict) and\r\n          not any(nest.is_nested(t) for t in self._nested_inputs.values())):\r\n      self._enable_dict_to_input_mapping = True\r\n    else:\r\n      self._enable_dict_to_input_mapping = False\r\n\r\n    if not keras_tensor.keras_tensors_enabled():\r\n      if any(not hasattr(tensor, '_keras_history') for tensor in self.outputs):\r\n        base_layer_utils.create_keras_history(self._nested_outputs)\r\n```\r\n\r\ntf 2.3.2\r\n```\r\n  self._enable_dict_to_input_mapping = (\r\n        not nest.is_sequence(self._nested_inputs) or\r\n        (isinstance(self._nested_inputs, (list, tuple, dict)) and\r\n         not any(nest.is_sequence(t) for t in self._nested_inputs)))\r\n```\r\n\r\n**Additional**\r\nI cannot upgrade tensorflow to 2.4.1 on the production env, since the cuda version is 10.1, does not supported by tensorflow 2.4.1 (it requires cuda 11.0), If I install tensorflow-gpu 2.4.1, I  cannot get the GPUs using `tf.config.list_physical_devices('GPU')` (empty list will be returned)", "comments": ["@ericxsun,\r\nCorrect me if I am wrong but running the same code snippet on TF v2.3 and TF v2.4 produces identical results.\r\n\r\nPlease find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/62b7e3e1a5f188500114b3f167713066/46726.ipynb). Thanks!", "> @ericxsun,\r\n> Correct me if I am wrong but running the same code snippet on TF v2.3 and TF v2.4 produces identical results.\r\n> \r\n> Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/62b7e3e1a5f188500114b3f167713066/46726.ipynb). Thanks!\r\n\r\n\r\nYou made a mistake on what to compare.\r\n\r\n See the following. \r\n\r\n![image](https://user-images.githubusercontent.com/1772912/106142457-78917580-61ac-11eb-885c-6297342e178e.png)\r\n", "@amahendrakar available for checking ?", "Any Tensorflower could be available checking?", "Was able to reproduce the issue in TF v2.5,please check the gist [here](https://colab.research.google.com/gist/sushreebarsa/8c51b0cdf32c34e08ee0468239ad3d27/untitled54.ipynb)..Thanks ! "]}, {"number": 46719, "title": "tf wide and deep model save error", "body": "I am using TF 2.2, the `save` method raises error, \r\n\r\nvery brief code to reproduce, nearly all code are from TF official doc https://www.tensorflow.org/api_docs/python/tf/keras/experimental/WideDeepModel\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras.premade.linear import LinearModel\r\nfrom tensorflow.python.keras.premade.wide_deep import WideDeepModel\r\nimport numpy as np\r\n\r\nlinear_model = LinearModel()\r\ndnn_model = tf.keras.Sequential([tf.keras.layers.Dense(units=64),\r\n                              tf.keras.layers.Dense(units=1)])\r\ncombined_model = WideDeepModel(linear_model, dnn_model)\r\ncombined_model.compile(optimizer=['sgd', 'adam'], loss='mse', metrics=['mse'])\r\n# define dnn_inputs and linear_inputs as separate numpy arrays or\r\n# a single numpy array if dnn_inputs is same as linear_inputs.\r\nlinear_inputs = np.random.random((2, 3))\r\ndnn_inputs = np.random.random((2, 3))\r\ny = np.random.randint(0, 2, (2, 2))\r\n\r\ncombined_model.fit([linear_inputs, dnn_inputs], y, epochs=1)\r\ncombined_model.save('tf-wide-deep')\r\n```\r\nerror is \r\n```\r\n1/1 [==============================] - 0s 365us/step - loss: 0.4376 - mse: 0.4376\r\n2021-01-27 13:36:54.121411: W tensorflow/python/util/util.cc:329] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nWARNING:tensorflow:From /home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\n\r\nError\r\nTraceback (most recent call last):\r\n  File \"/home/litchy/rec-pro/tf-models/test_tf_models.py\", line 23, in test_wide_deep\r\n    combined_model.save('tf-wide-deep')\r\n  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 1052, in save\r\n    signatures, options)\r\n  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py\", line 138, in save_model\r\n    signatures, options)\r\n  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/save.py\", line 78, in save\r\n    save_lib.save(model, filepath, signatures, options)\r\n  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 951, in save\r\n    obj, export_dir, signatures, options, meta_graph_def)\r\n  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 1037, in _build_meta_graph\r\n    asset_info.asset_index)\r\n  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 697, in _serialize_object_graph\r\n    saveable_view.function_name_map)\r\n  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 737, in _write_object_proto\r\n    metadata=obj._tracking_metadata)\r\n  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 2742, in _tracking_metadata\r\n    return self._trackable_saved_model_saver.tracking_metadata\r\n  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/base_serialization.py\", line 54, in tracking_metadata\r\n    return json_utils.Encoder().encode(self.python_properties)\r\n  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py\", line 41, in python_properties\r\n    return self._python_properties_internal()\r\n  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/model_serialization.py\", line 38, in _python_properties_internal\r\n    self.obj, include_optimizer=True, require_config=False))\r\n  File \"/home/litchy/anaconda3/envs/rec/lib/python3.6/site-packages/tensorflow/python/keras/saving/saving_utils.py\", line 188, in model_metadata\r\n    model.optimizer.get_config()\r\nAttributeError: 'ListWrapper' object has no attribute 'get_config'\r\n\r\n```", "comments": ["I have tried in colab with TF version 2.2, nightly version (`2.5.0-dev20210126`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/db71114bed69f93906e38c5435d5ba56/untitled637.ipynb). Thanks!", "this is a bug that tf can't get optimizer from list. \r\nyou can save model with no optimizer like \"model.save(include_optimizer=False)\"\r\nthan feed a new optimizer after load the model , when you want to train it again.\r\n\r\nhere is my code :\r\nhttps://colab.research.google.com/gist/ravikyram/db71114bed69f93906e38c5435d5ba56/untitled637.ipynb", "Hello, is there any progress regarding this issue? Thanks.", "Still an issue in TF2.7 and tf-nightly(2.8.0-dev20211208). Please find the gist [here](https://colab.research.google.com/gist/chunduriv/5cf0ddd0630e1b459186e76f31eb5957/46719.ipynb).Thanks!", "@Litchilitchy,\r\nIssue with the optimizers list.\r\n`optimizer=['sgd', 'adam']`\r\nWhen we use multiple optimizers, we can use Multi Optimizer Wrapper `tfa.optimizers.MultiOptimizer`.\r\n\r\n**Working sample code**\r\n```\r\nimport tensorflow as tf\r\n#from tensorflow.keras.premade.linear import LinearModel\r\n#from tensorflow.keras.premade.wide_deep import WideDeepModel\r\nfrom tensorflow.keras.experimental import LinearModel\r\nfrom tensorflow.keras.experimental import WideDeepModel\r\nimport numpy as np\r\nimport tensorflow_addons as tfa\r\n\r\nlinear_model = LinearModel()\r\ndnn_model = tf.keras.Sequential([tf.keras.layers.Dense(units=64),\r\n                             tf.keras.layers.Dense(units=1)])\r\ncombined_model = WideDeepModel(linear_model, dnn_model)\r\n#combined_model.compile(optimizer=['sgd', 'adam'], 'mse', ['mse'])\r\noptimizers = [\r\n    tf.keras.optimizers.SGD(learning_rate=1e-4),\r\n    tf.keras.optimizers.Adam(learning_rate=1e-2)\r\n]\r\noptimizers_and_layers = [(optimizers[0], dnn_model.layers[0:]), (optimizers[1], dnn_model.layers[1:])]\r\noptimizer = tfa.optimizers.MultiOptimizer(optimizers_and_layers)\r\ncombined_model.compile(optimizer=optimizer, loss='mse', metrics=['mse'])\r\n\r\n# define dnn_inputs and linear_inputs as separate numpy arrays or\r\n# a single numpy array if dnn_inputs is same as linear_inputs.\r\n\r\nlinear_inputs = np.random.random((2, 3))\r\ndnn_inputs = np.random.random((2, 3))\r\ny = np.random.randint(0, 2, (2, 2))\r\n\r\ncombined_model.fit([linear_inputs, dnn_inputs], y, epochs=1)\r\n\r\ncombined_model.save('tf-wide-deep')\r\n```\r\n\r\n**Output**\r\n```\r\n1/1 [==============================] - 0s 314ms/step - loss: 0.3535 - mse: 0.3535\r\n<keras.callbacks.History at 0x7f4edc0442d0>\r\n\r\nINFO:tensorflow:Assets written to: tf-wide-deep/assets\r\n```\r\n\r\n\r\n", "@gadagashwini thanks, I would try. The original code is from official example so that it seems need some change if this works.", "@Litchilitchy, Official example code is not complete code snippet. If you make changes to optimizers, you can use the code given in the Tensorflow website. \r\nCan we move this issue to closure, since its resolved .Thanks!\r\n"]}, {"number": 46711, "title": "Allow reusing existing tensors for outputs of TF_SessionRun()", "body": "In the C API, `TF_SessionRun()` allocates new output tensors on every call. This leads to frequent allocation/deallocation and memory copies in some situations. It would be useful to be able to reuse existing tensors for output.\r\n\r\nThe current API takes `TF_Tensor** output_values` and will update that variable to point to newly allocated output tensors. I propose that if `output_values` points to existing tensors, those tensors should be used for output instead of allocating new tensors.", "comments": ["Hello, \r\n\r\nAny news on this? \r\n\r\nI also believe that the overhead of memory allocation should be avoided when calling `TF_SesssionRun()` multiple times. I'm using the Tensorflow C API (2.6) for inference in the context of real-time audio processing, and malloc is not a real-time safe operation. Having a way to avoid this memory allocation is crucial for I think numerous use cases.\r\n\r\nThis issue has been pointed more than 2 years ago for Tensorflow 1.13 (https://github.com/tensorflow/tensorflow/issues/29733). \r\n\r\nThe solution suggested by @shoelzer seems good to me. There might also be other ways to implement it.", "Same problem in C++ API, did anyone solve it ?"]}, {"number": 46708, "title": "Using the function signatures loaded from a SavedModel requires the original trackable object be kept in scope.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.7\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): `v2.4.0-rc4-71-g582c8d236cb 2.4.0`\r\n- Python version: 3.8.5\r\n\r\n**Describe the current behavior**\r\n\r\nLoading a SavedModel from disk and extracting a signature from it works only while the result of `tf.saved_model.load` has not be collected by the Python garbage collector. For example, the following code fails:\r\n\r\n```python\r\ndef load_model(path_to_saved_model):\r\n    saved_model = tf.saved_model.load(path_to_saved_model)\r\n    return saved_model.signatures[\"serving_default\"]\r\n\r\nload_model('./my_model.ckpt')(...) # throws \"Error while reading resource variable...\"\r\n```\r\n\r\nbut this code snippet runs successfully:\r\n```python\r\nsaved_model = tf.saved_model.load('./my_model.ckpt')\r\nmodel = saved_model.signatures[\"serving_default\"]\r\nmodel(...) # No exception is thrown\r\n```\r\n\r\nThe issue can be worked around by manually attaching the original trackable object to the return value of the function, preventing the Python garbage collector from collecting the object:\r\n```python\r\ndef load_model_with_backref(path_to_saved_model):\r\n    saved_model = tf.saved_model.load(path_to_saved_model)\r\n    model = saved_model.signatures[\"serving_default\"]\r\n    model._backref_to_saved_model = saved_model\r\n    return model\r\nload_model('./my_model.ckpt')(...) # No exception is thrown\r\n```\r\n\r\nThe exception thrown can, depending on the model and environment, sometimes be:\r\n> AssertionError: Called a function referencing variables which have been deleted. This likely means that function-local variables were created and not referenced elsewhere in the program. This is generally a mistake; consider storing variables in an object attribute on first call.\r\n\r\nThe exception can also manifest as:\r\n> FailedPreconditionError: Error while reading resource variable _AnonymousVar30 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar30/N10tensorflow3VarE does not exist.\r\n\t [[{{node StatefulPartitionedCall/model/layer/batch_normalization_3/FusedBatchNormV3/ReadVariableOp_1}}]] [Op:__inference_signature_wrapper_11671]\r\n\r\nThis may be a similar issue to https://github.com/tensorflow/tensorflow/issues/37615.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport os\r\n\r\n# Disable ultra-verbose TF logging\r\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"4\"  # noqa\r\nimport tensorflow as tf\r\n\r\n\r\ndef load_model(path_to_saved_model):\r\n    saved_model = tf.saved_model.load(path_to_saved_model)\r\n    return saved_model.signatures[\"serving_default\"]\r\n\r\n\r\ndef load_model_with_backref(path_to_saved_model):\r\n    saved_model = tf.saved_model.load(path_to_saved_model)\r\n    model = saved_model.signatures[\"serving_default\"]\r\n    model._backref_to_saved_model = saved_model\r\n    return model\r\n\r\n\r\ndef use_model(model):\r\n    input_tensors = [\r\n        tensor for tensor in model.inputs if tensor.dtype != tf.resource\r\n    ]\r\n    return model(\r\n        *[\r\n            tf.random.uniform(shape=[dim or 1 for dim in _input.shape])\r\n            for _input in input_tensors\r\n        ]\r\n    )\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    import sys\r\n\r\n    model_path = sys.argv[-1]\r\n    try:\r\n        use_model(load_model(model_path))\r\n        print(\r\n            f\"\u2705 TensorFlow {tf.__version__} loaded model {model_path} without \"\r\n            \"needing to set a back-reference to the SavedModel on the \"\r\n            \"ConcreteFunction.\"\r\n        )\r\n    except Exception as e:\r\n        print(\r\n            f\"\u274c TensorFlow {tf.__version__} failed to load model without\"\r\n            f\" setting back-reference {model_path}:\\n{e}\"\r\n        )\r\n\r\n    try:\r\n        use_model(load_model_with_backref(model_path))\r\n        print(\r\n            f\"\u2705 TensorFlow {tf.__version__} loaded model {model_path} by\"\r\n            \" setting a back-reference to the SavedModel on the\"\r\n            \" ConcreteFunction.\"\r\n        )\r\n    except Exception as e:\r\n        print(\r\n            f\"\u274c TensorFlow {tf.__version__} failed to load model when setting\"\r\n            f\" back-reference {model_path}:\\n{e}\"\r\n        )\r\n\r\n```\r\n\r\nWhen run on TensorFlow 2.4.0, the above script prints:\r\n```\r\n\u274c TensorFlow 2.4.0 failed to load model without setting back-reference ../model.ckpt:\r\n Error while reading resource variable _AnonymousVar46 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar46/N10tensorflow3VarE does not exist.\r\n\t [[{{node StatefulPartitionedCall/model/layer_name_here/batch_normalization_12/ReadVariableOp_1}}]] [Op:__inference_signature_wrapper_11671]\r\n\r\nFunction call stack:\r\nsignature_wrapper\r\n\r\n\u2705 TensorFlow 2.4.0 loaded model ../model.ckpt by setting a back-reference to the SavedModel on the ConcreteFunction.\r\n```", "comments": ["I am able to replicate the issue reported, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/6d6c3f08cb766c17b5384503b96f81d8/untitled505.ipynb)", "Interestingly, I'm observing this only since tensorflow 2.5.0 (and not since 2.4.0)\r\n(But maybe it is related to that my tf 2.4.0 installation uses python  3.7.6 but tf 2.5.0 uses  python 3.8.10?)", "@psobot Could you please have a look at this [gist](https://colab.research.google.com/gist/sushreebarsa/76d80574890e981d5ae970f1037b0c78/gist46708.ipynb) and let us know if it helps?Thanks!", "Hi @sushreebarsa - it looks like in that Gist, loading a SavedModel now works as expected in TensorFlow 2.8.0. Any idea what changed to fix this issue?", "@psobot I have provided the saved model path in the updated gist and the  code works fine for mnist model in TF v2.8.0.\r\nThanks!"]}, {"number": 46706, "title": "Tesnorflow Sparse operations are slow - Comparison between Dense, SparseTensor and raw_ops sparse ", "body": "**System information**\r\n- Tensorflow 2.4.0\r\n- colab - basic settings\r\n\r\n**Description**\r\n\r\nI am trying to convert my code to run with `Sparse Matrices` on Tensorflow but looks like the performance is quite bad. I tried using `SparseTensor` and `tf.raw_ops.Sparse...` functions. Below is a comparsion for some of the functions. Pleaese refer the [colab code](https://colab.research.google.com/drive/1urRVOW4hR3i_7koNtXOnUTa5bhl7WXPC?usp=sharing) to generate the results.  \r\n\r\nFollowing results are for a matrix with 75% zeros and time in seconds.  \r\n\r\n|Method | Dense | SparseTensor | Raw_ops|\r\n| --- | --- | --- | --- |\r\n|Element-wise Multiplication\t     | 0.0511\t   |  0.5765\t      |    0.5677|\r\n|Exponential\t                             |0.0438   \t   |   0.0121\t      |   0.0149 |\r\n|Devision\t                                     | 0.0625\t   |   0.5718\t      |   0.5614 |\r\n|Reduce Sum                                |0.0194\t   |    1.1918\t      |   1.1648 |\r\n|Matrix Multiplication\t              |0.0576\t   |   0.1589\t      |  0.1553  |\r\n\r\nFunctions used:\r\n|Method | Dense | SparseTensor | Raw_ops|\r\n| --- | --- | --- | --- |\r\n|Element-wise Multiplication\t     | `tf.multiply()`\t   | ` *`\t      |   `tf.raw_ops.SparseDenseCwiseMul()`|\r\n|Exponential\t                             |`tf.exp()` \t   |  ` tf.sparse.map_values(tf.exp,x)`\t      |` tf.exp(`)    |\r\n|Devision\t                                     | `tf.divide()`\t   |  `/`      |  ` tf.raw_ops.SparseDenseCwiseDiv()` |\r\n|Reduce Sum                                |`tf.reduce_sum()`   |    `tf.sparse.reduce_sum()`\t      |   `tf.raw_ops.SparseReduceSum()` |\r\n|Matrix Multiplication\t              |`tf.matmul()`\t   |  `tf.sparse.sparse_dense_matmul()`\t      |  `tf.raw_ops.SparseTensorDenseMatMul()`  |\r\n\r\n\r\nAs per the results on sparse tensors, elementvise operations take 10x while Reduce_sum has taken 100x time to run.  \r\n\r\nIs Tensorflow Sparse operations are written focusing on optimising the memory and not run time? Is there a fix for this? \r\n\r\n", "comments": ["Was able to reproduce the issue with [TF v2.4](https://colab.research.google.com/gist/amahendrakar/3298af6d440d21e42c7afc576290e040/46706.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/d4d9aaf0d4cb9215c11c9a2c15254dd6/46706-tf-nightly.ipynb). \r\n\r\nWhereas with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/a9026321cd14b9c59445ae9c18f6e837/46706-2-3.ipynb#scrollTo=H1GhEvwu2fzC), I am facing an error stating `AttributeError: module 'tensorflow._api.v2.sparse' has no attribute 'map_values'`. Please check the linked gist for reference. Thanks!", "I believe 'map_values' was introduced in the latest `Tensorflow 2.4.0`. ", "@rmothukuru hi,is there any update on this issue, because it has been half an year"]}, {"number": 46702, "title": "tf.data.Dataset.list_files is increadibly slow on Tensorflow 2.3.1", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 11.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nRunning the following snippet on Tensorflow 2.3.1 takes several minutes:\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.data.Dataset.list_files(\"s3://commoncrawl/*.txt\")\r\n```\r\n\r\nThe same snippet on Tensorflow 2.4.1 takes under a second. There is a single file in this path that matches. This makes this API unusable on 2.3.1.\r\n\r\nIf I run the command with `AWS_LOG_LEVEL=0` then I see there are a huge number of these messages:\r\n\r\n```\r\n2021-01-26 17:17:35.606039: I tensorflow/core/platform/s3/aws_logging.cc:71] Connection has been released. Continuing.\r\n2021-01-26 17:17:35.607345: I tensorflow/core/platform/s3/aws_logging.cc:71] Connection has been released. Continuing.\r\n2021-01-26 17:17:35.610330: I tensorflow/core/platform/s3/aws_logging.cc:71] Connection has been released. Continuing.\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe snippet should execute instantly\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nExport your environment variables:\r\n\r\n```\r\nexport AWS_REGION=us-east-1\r\nexport AWS_LOG_LEVEL=0\r\n```\r\n\r\nthen run the following code:\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.data.Dataset.list_files(\"s3://commoncrawl/*.txt\")\r\n```\r\n\r\n**Other info / logs**\r\n\r\nI've attached a complete log from running the above snippet. It's very large.\r\n\r\n[out.log](https://github.com/tensorflow/tensorflow/files/5874958/out.log)", "comments": ["I have tried in colab with TF version 2.3.1 and its taking some minutes please, find the gist [here](https://colab.research.google.com/gist/ravikyram/b97889aee2275a606f5acf6ec53f5098/untitled632.ipynb).However the code runs instantly with TF version 2.4.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/17243a783a997f1130184e0a17a7c0b9/untitled633.ipynb).Thanks!", "@orf,\r\nSince **`Tensorflow 2.4`** is the latest stable version, and it is working fine in this version, we suggest you to upgrade your Tensorflow Version to **`2.4`**. Thanks!", "I'm not sure if this is a good approach. Tensorflow 2.3 is still supported and receives updates.", "@orf,\r\nUsing the latest stable version is always the recommended approach. Is there any specific reason that you want to stick with TF Version 2.3.1?", "I guess not, but we are looking at using this more widely across many projects which may or may not have issues upgrading and would prefer to not have to foster that on them.\n\nOn top of that it's not really a backport or a minor incompatibility: the globbing mechanism just doesn't function on s3 at all on 2.3, which received a bugfix release only 22 days ago.\n\nBut it's your prerogative - if you wish to close this issue then that's up to you. But thanks for taking the time to investigate and respond.", "This issue happened to me in TF 2.4.1. The following line took 10+ minutes to complete, while there were only 1,500 images in the directory. Although the images were 4K and the size of the data was around 50GB, it doesn't seem reasonable to take a such long time. \r\n```\r\ntf.data.Dataset.list_files(div8k_save_path+'*.png')\r\n```", "The code using `os.listdir` is extremely fast(~3secs) while `tf.data.Dataset.list_files` takes around 15 minutes. This issue still seems to happen in tf `2.4.1`.\r\n```\r\ndiv8k_list=[os.path.join(div8k_save_path, x) for x in os.listdir(div8k_save_path)]\r\ntrain_path = tf.data.Dataset.from_tensor_slices(div8k_list)\r\n```", "@krenerd Does the slow listing happen only when the files are large, or does it happen with small/empty files as well? Also, does it still happen with the current `tensorflow==2.5.0rc3` release candidate?"]}, {"number": 46679, "title": "Integer quantization converts bias of 32-bit float type in conv2d to 8-bit int type on TFLite ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  **Ubuntu 16.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **RPI 3b+**\r\n- TensorFlow installed from (source or binary): **source**\r\n- TensorFlow version (use command below): **2.3.1**\r\n- Python version: **3.6**\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):  **5.4.0**\r\n- CUDA/cuDNN version: \r\n- GPU model and memory: \r\n\r\n**Describe the current behavior**\r\n![error](https://user-images.githubusercontent.com/47438617/105789041-06127100-5fc5-11eb-8430-249c2baf5b08.png)\r\n\r\nUsed network is a kind of resnet18 and this network is applied int8 quantization by using TFLite. But bias of one of the conv2d has int8 type. The other bias have int32 type. \r\n\r\nThen, inference with this model is not worked, like as below.\r\n![image](https://user-images.githubusercontent.com/47438617/105789822-740b6800-5fc6-11eb-8819-95f04868bc92.png)\r\n**Describe the expected behavior**\r\n\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nI share you kears and tflite versions of resnet18.\r\n[benchmark model.zip](https://github.com/tensorflow/tensorflow/files/5870712/benchmark.model.zip)\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["This seems to be resolved with nightly version of tensorflow. Can you try again?", "any news on this? still seems a problem with the latest TF2.5(RC) release"]}, {"number": 46672, "title": "softmax_cross_entropy_with_logits - wrong backprop for this operation", "body": "Hi,\r\n\r\nI've noticed that the arithmetic for the backward operation of softmax_cross_entropy_with_logits is lacking of an additional multiplier. As can be seen in the following Xent kernel, that the operation is using, the formula for the log_cross_entropy is:\r\n**_target - logits_**\r\nwhile it should be:\r\n_target*(1 - logits)_\r\nWhen the target is dense, and doesn't sum up to exactly one, the answer is wrong. \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/dec8e0b11f4f87693b67e125e67dfbc68d26c205/tensorflow/core/kernels/xent_op.h#L118\r\n\r\nHere is a numpy reference code for what I expect the backward should be:\r\n```\r\n## gradOutput is actually the target labels \r\ndef referenceLogSoftMaxBackward(logSoftMaxOutput, gradOutput):\r\n    '''\r\n    accreal sum = 0;\r\n    for (d = 0; d < dim; d++)\r\n      sum += gradOutput_data[d];\r\n\r\n    for (d = 0; d < dim; d++)\r\n      gradInput_data[d] = gradOutput_data[d] - exp(output_data[d])*sum;\r\n    '''\r\n    shape = gradOutput.shape\r\n    dims_to_normalize_tuple = tuple(np.arange(len(shape))[1:])\r\n\r\n    # Sum reduction over all dimensions except for batch\r\n    sum = np.sum(gradOutput, axis=dims_to_normalize_tuple, keepdims=True)\r\n\r\n    # Compound operation:\r\n    # Elementwise exponential and then multiplication by broadcasted tensor\r\n    # followed by element-wise subtraction\r\n    gradInput = gradOutput - np.exp(logSoftMaxOutput) * sum\r\n\r\n    return gradInput\r\n```\r\n\r\nCan you please check this out and comment?\r\n\r\nThanks.", "comments": []}, {"number": 46648, "title": "tf.repeat fails", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave 10.14.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n- Python version: 3.*\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: no GPU\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n```\r\nn = 50000\r\nmany_ones = tf.cast(tf.concat([tf.ones(n), [n,]], axis = 0), 'int32')\r\narray = tf.concat([tf.range(n), [n, ]], axis = 0)\r\ntf.repeat(array, many_ones)\r\n```\r\nreturns\r\n\r\n> InvalidArgumentError: Size 0 must be non-negative, not -1794917296 [Op:Reshape]\r\n\r\n**Describe the expected behavior**\r\n`np.repeat` returns\r\n> array([    0,     1,     2, ..., 50000, 50000, 50000], dtype=int32)\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nthe following code would work for small ns, but fail/be very slow for big ns\r\n```\r\nn = 50000\r\nmany_ones = tf.cast(tf.concat([tf.ones(n), [n,]], axis = 0), 'int32')\r\narray = tf.concat([tf.range(n), [n, ]], axis = 0)\r\ntf.repeat(array, many_ones)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@dh-g I was able  to execute the code  up to value n = 45000 but  after that the colab is crashing by utilizing the entire available  RAM. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/50427c688799dbcfbb20505b82e1efd1/untitled.ipynb).\r\n\r\nIs there any specific reason why do you want a tensor of such huge dimensions?", "`utilizing the entire available RAM.`\r\n\r\nyeah this is most likely a memory problem, but this shouldn't be, as numpy can do it without missing a beat, and so does pytorch, and will be done in less than a second.\r\n\r\n`Is there any specific reason why do you want a tensor of such huge dimensions?`\r\n\r\nthe reason is that i need to vectorize an operation \r\n\r\n[item1, ..., item1, ..., itemN, ..., itemN] - [item1_1, ..., item1_m, ..., itemN_1, ..., itemN_l]\r\n\r\nwhich if i used for loop with (the biggest repeat is actually way bigger than 45000 \u22482^15 btw), it would take forever to calculate.", "any updates? this is not a small issue by any means. ", "@dh-g,\r\nInstead of using all the APIs in one go, below code works without any issue, even for `n=100000`:\r\n\r\n```python\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\nn = 100000\r\n#many_ones = tf.cast(tf.concat([tf.ones(n), [n,]], axis = 0), 'int32')\r\nmany_ones = tf.ones(n)\r\nprint(many_ones.numpy())\r\nmany_ones = tf.concat([many_ones, [n,]], axis = 0)\r\nprint(many_ones.numpy())\r\nmany_ones = tf.cast(many_ones, 'int32')\r\nprint(many_ones.numpy())\r\n``` \r\nPlease find [the Gist](https://colab.research.google.com/gist/rmothukuru/0b3d68db66a7e76fd2b9de7b01211735/gh_tf_46648.ipynb) of working code.", "hey @rmothukuru please note the problem is about **`tf.repeat`**,  not on tf.ones\r\n\r\nthe tf.repeat part in the original code still fails in colab", "and I totally disagree with the change of tag from bug to performance, this IS a bug.", "hey any updates?", "@saikumarchalla @rmothukuru Hey guys, I just tried in the colab notebook again [here](https://colab.research.google.com/gist/saikumarchalla/50427c688799dbcfbb20505b82e1efd1/untitled.ipynb#scrollTo=pDt8nA5z_GQS). The code still fails for both 2.4 and the nightly version 2.6", "For not-very-large tensors(e.g. 5M elements), tf.repeat can also have some problems. \r\n```\r\na = tf.range(5000000)\r\nb = tf.concat([tf.zeros(5000000-1, dtype=tf.int32),tf.constant([5000000], dtype=tf.int32)], axis=0)\r\nc = tf.repeat(a,b)\r\n```\r\n\r\nThe above code results in an OOM error:\r\n\r\n> 2021-07-08 14:22:05.656061: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 25000000000000 exceeds 10% of system memory.\r\n2021-07-08 14:22:05.656150: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at cwise_ops_common.cc:82 : Resource exhausted: OOM when allocating tensor with shape[5000000,5000000] and type bool on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/tangfangshuang/.pyenv/versions/py3_venv/lib/python3.5/site-packages/tensorflow_core/python/ops/array_ops.py\", line 4927, in repeat\r\n    return repeat_with_axis(input, repeats, axis, name)\r\n  File \"/home/tangfangshuang/.pyenv/versions/py3_venv/lib/python3.5/site-packages/tensorflow_core/python/ops/array_ops.py\", line 4839, in repeat_with_axis\r\n    mask = sequence_mask(repeats, max_repeat)\r\n  File \"/home/tangfangshuang/.pyenv/versions/py3_venv/lib/python3.5/site-packages/tensorflow_core/python/ops/array_ops.py\", line 3591, in sequence_mask\r\n    result = row_vector < matrix\r\n  File \"/home/tangfangshuang/.pyenv/versions/py3_venv/lib/python3.5/site-packages/tensorflow_core/python/ops/gen_math_ops.py\", line 5376, in less\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[5000000,5000000] and type bool on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:Less]", "@rmothukuru @saikumarchalla hey guys any updates? there has been no update for 5 months now. And as you can see from @ttang235 's comment this is not a lone incidence."]}, {"number": 46636, "title": "Remove py_proto_library Macro from tensorflow/core/platform/default/build_config.bzl", "body": "tensorflow/core/platform/default/build_config.bzl contains a py_proto_library macro (Re-defined protocol buffer rule) with a comment that it should be removed once the protobuf dependency version is updated to include the commit shown in the comment.\r\n\r\nThe referenced protobuf commit (294b5758c373cbab4b72f35f4cb62dc1d8332b68) has been in protobuf since v3.6.0 from 2018-06-13.\r\n\r\nAlso, TensorFlow has the following protobuf version references:\r\n```\r\nmaster      tensorflow/workspace.bzl: protobuf 3.9.2, tensorflow/tools/pip_package/setup.py: protobuf >=3.9.2\r\n2.0 branch  tensorflow/workspace.bzl: protobuf 3.8.0, tensorflow/tools/pip_package/setup.py: protobuf >=3.6.1\r\n```\r\n", "comments": []}, {"number": 46635, "title": "Missing GPU op for zeros_like for RaggedTensorVariant, error occurs when Ragged Tensor fed thru tf.map_fn", "body": "**System information**\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, included below\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.10\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\nTensorFlow installed from (source or binary): pip binary\r\nTensorFlow version (use command below): v1.12.1-49539-g18d8bcbe72b 2.5.0-dev20210123\r\nPython version: '3.8.6 | packaged by conda-forge | (default, Nov 27 2020, 19:31:52) \\n[GCC 9.3.0]'\r\nBazel version (if compiling from source): n/a\r\nGCC/Compiler version (if compiling from source): n/a\r\nCUDA/cuDNN version: 11.0 / 8\r\nGPU model and memory: TITAN X (Pascal) computeCapability: 6.1\r\n\r\n**Describe the current behavior**\r\n\r\nI have a keras layer `RescaleB` that accepts a ragged tensor with shape [batch, (time), in_dim]. The layer calls `map_fn` to process each example in the batch separately, scaling the values along the inner dimension by a trainable gain vector. (The details of the operation aren't critical, but the ragged tensor going into map_fn is.)\r\n\r\nUsing this layer fails with `No unary variant unary_op function found for unary variant op enum: 1 Variant type_name: RaggedTensorVariant for device type: GPU` on a node whose name ends with `rescale_b/map/while/TensorArrayV2Write/TensorListSetItem_grad/zeros_like` which suggests that the zeros_like operation \r\nisn't defined for Ragged Tensors on GPU?\r\n\r\nIn this simple example, i also include `RescaleA`, which accomplishes the same task using `tf.ragged.map_flat_values`, although in my real use case I need `map_fn`. This is a simplified example.\r\n\r\n**Describe the expected behavior**\r\n\r\nI'd expect `RescaleB` and `RescaleA` to function identically.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nhttps://colab.research.google.com/drive/1mHycCXJL94VuCGkXIJ0bIXtbYamyZo78\r\n\r\nI've reproduced the issue locally with tf-nightly-gpu TF 2.5, but I can't seem to get the nightly version to see the GPU on Colab. The Colab notebook is using TF 2.4, but the issue remains in TF 2.5 nightly.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nThis may be the same issue as #44231 but hopefully the additional detail here is helpful. \r\n", "comments": ["I ran the code on tf 2.4 and and face a different error on nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/e6435e077a9cd287a043bed1389da761/untitled507.ipynb)", "As I mentioned, I don't think tf-nightly-gpu is running on the GPU in Colab. The issue is only present on GPU. In the gist you sent, the cell where it searches for the GPU returns:\r\n```\r\nTensorflow version ==  2.5.0-dev20210124\r\n\r\n---------------------------------------------------------------------------\r\nSystemError                               Traceback (most recent call last)\r\n<ipython-input-8-6f54130e32b7> in <module>()\r\n      4 print(device_name)\r\n      5 if device_name != '/device:GPU:0':\r\n----> 6   raise SystemError('GPU device not found')\r\n      7 print('Found GPU at: {}'.format(device_name))\r\n\r\nSystemError: GPU device not found\r\n```\r\n\r\nLocally, where it is running on GPU, TF nightly fails with the same error as TF2.4, so this issue is still present in nightly, at least on my local `pip install tf-nightly-gpu` installation.", "@djoshea this looks like a reasonable request, will you be able to send a PR for this?", "I could potentially try developing it, but I unfortunately don't know where to start. Is there a guide somewhere to implementing new ops for the GPU?", "> I could potentially try developing it, but I unfortunately don't know where to start. Is there a guide somewhere to implementing new ops for the GPU?\r\n\r\nI don't think we have a detailed guide for this, but if you're comfortable with C++ you could try to start [at the check that's failing](https://github.com/tensorflow/tensorflow/blob/5ecbbd87a4556e06cbb3dcf384d088dd27fce4e0/tensorflow/core/framework/variant_op_registry.h#L317).", "It would be nice if this could be implemented soon. Using map_fn with RaggedTensors is very convenient when working with data of different shapes. Unfortunately one must run these operations on CPU right now.", "I'm relying on map_fn to extract ragged image patches using bounding boxes so I can then produce ragged bounding-box centered feature maps via convolutions. I am encountering the same bug with the gradient step inside a tf.function (I can successfully run outside tf.function). The ragged image patches method is enabling a large speed up with a reduced memory footprint so it would be very useful to enable this in graph mode on GPUs.", "For me RaggedTensors + map_fn are quite an enabler because I am implementing some kind of link prediction in a graph neural network. Having TensorFlow handling the individual samples allows for pretty safe programming as I do not have to put all samples in a batch into a large, disconnected graph and make sure that no nodes of different graphs will be connected. Using TensorFlow this way results in nice code that is fast to write in contrast to solutions with masks which may need to be changed every time you change your computation. Having this in GPU would be awesome. :100: \r\n\r\n", "I would love to see this fixed, especially as iterating over RaggedTensors seems to not properly work in a lot of cases.", "I ran the code in TF v2.5 and face error ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/0bf4ae93db25542263e443bad439fb94/untitled155.ipynb?authuser=1)..Thanks !", "Hi @sanjoy any updates on this? This is a really needed feature.", "Hi, i will also very appreciate if this bug get fixed.\r\nI think it's somehow related to [No gradient defined for operation RaggedTensorFromVariant](https://github.com/tensorflow/tensorflow/issues/42189) and [No gradient defined for operation RaggedTensorFromVariant / or no gradients at all\r\n](https://github.com/tensorflow/tensorflow/issues/43107) which are fixed with commit [be6b1fd](https://github.com/tensorflow/tensorflow/commit/be6b1fdb0699d4000b70ad32cc23d1503e5c7511) by @edloper .\r\n\r\nThe part with `REGISTER_UNARY_VARIANT_UNARY_OP_FUNCTION(\r\n    ZEROS_LIKE_VARIANT_UNARY_OP, DEVICE_CPU, RaggedTensorVariant,\r\n    RaggedTensorVariantZerosLike<CPUDevice>);` just registers the zeros_like OP for CPU device but not for GPU.\r\n\r\nMaybe this will be a quick win to fix.\r\n@edloper ,it would be very kind, if you can have a look at this.\r\nThanks.", "Hi, thanks for raising the issue. According to @edloper \"Basically, RaggedTensorVariant objects should never be copied to GPU, because we can't do anything useful with them there. But Placer isn't currently smart enough to figure that out (it just sees a Variant tensor, and doesn't know what kind of value it contains).\" We have a project going on right now that hopefully will fix the issue. "]}, {"number": 46618, "title": "custom hardware support for tensorflow", "body": "Tensorflow currently has support for CPU and GPU devices, adding a new device seems possible but the documentation is out of date. The most recent activity on this seems to be from git hub issue: https://github.com/tensorflow/tensorflow/issues/4359\r\n\r\nIs there more current documentation or is there a best practice for adding a device? Has anyone been through the process of adding custom ML hardware under tensorflow?", "comments": ["See https://github.com/tensorflow/community/blob/master/rfcs/20200624-pluggable-device-for-tensorflow.md and https://github.com/tensorflow/community/blob/master/rfcs/20200612-stream-executor-c-api.md and https://github.com/tensorflow/community/blob/master/rfcs/20190305-modular-tensorflow.md", "Check also https://github.com/tensorflow/tensorflow/pull/45784\r\nhttps://github.com/tensorflow/tensorflow/pull/43611"]}]