[{"number": 45564, "title": "[INTEL MKL] DNN 0.x code cleanup - Fused Matmul ops", "body": "DNN 0.x cleanup of Fused Matmul op:\r\n\r\n(1) Remove all DNN 0.x related code;\r\n\r\n(2) Replace all DNN 1.x macro usages\r\n\r\nAnd minor change in quantized op (macro replacement in a couple of places)", "comments": ["There are merge conflicts, due to a recently merged PR which touches same changed source file. \r\n\r\nA new PR (rebase with current master branch) has been created to replace this one. \r\n\r\nSo I am closing this one. "]}, {"number": 45563, "title": "Adding Configurable Filesystem API part 1", "body": "This PR introduces necessary API changes needed for Configurable Filesystems as described in https://github.com/tensorflow/community/pull/277. This is the first part of a series of PRs to implement these changes.", "comments": ["@mihaimaruseac Can you please take a look on this PR ? Thanks!", "Sorry, this slipped again through GitHub marking notifications as read through the tracking pixel.\r\n\r\n@samikama the comments are still not following the same layout as the comments on the APIs above the new ones."]}, {"number": 45562, "title": "Dockerfiles: Pin pip version and fix apt-get", "body": null, "comments": []}, {"number": 45561, "title": "Dockerfiles: Pin pip version and fix apt-get", "body": null, "comments": []}, {"number": 45560, "title": "Dockerfiles: Pin pip version and fix apt-get", "body": null, "comments": []}, {"number": 45559, "title": "Update version numbers for TensorFlow 2.4.0", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 4 -> 4\nPatch: 0 -> 0\n\nNo lingering old version strings \"2.4.0-rc4\" found in source directory \n\"tensorflow/\". Good.\nNo lingering old version strings \"2.4.0rc4\" found in source directory \n\"tensorflow/\". Good.\n```", "comments": []}, {"number": 45558, "title": "Keras optimizer uses non-trainable variables instead of constants for hyperparameters", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 7.8\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nKeras OptimizerV2 converts hyperparameters which are python data types to non-trainable TF Variables. Is there a reason they are not converted to tf.constant instead? Here's the relevant piece of code from `optimizer_v2.py`:\r\n\r\n```python\r\ndef _create_hypers(self):\r\n    if self._hypers_created:\r\n      return\r\n    # Iterate hyper values deterministically.\r\n    for name, value in sorted(self._hyper.items()):\r\n      if isinstance(\r\n          value, (ops.Tensor, tf_variables.Variable)) or callable(value):\r\n        continue\r\n      else:\r\n        self._hyper[name] = self.add_weight(\r\n            name,\r\n            shape=[],\r\n            trainable=False,\r\n            initializer=value,\r\n            aggregation=tf_variables.VariableAggregation.ONLY_FIRST_REPLICA)\r\n    self._hypers_created = True\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nIt seems unnecessary to create variables for hyperparameters that are known to be constant.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nThis code creates a variable for the learning rate in the graph.\r\n\r\n```python\r\ntf.keras.optimizers.SGD(learning_rate=0.01)\r\n```", "comments": ["@behzad-a,\r\nOn instantiating the optimizer with TF v2.2, the type is a `tensorflow.python.keras.optimizer_v2.gradient_descent.SGD` object.\r\n> This code creates a variable for the learning rate in the graph.\r\n\r\nCould you please provide a minimal code to reproduce the issue? Thanks!", "@amahendrakar, I think you misunderstood my question. What I'm referring to are the ops in the TF graph (or the XLA graph for that matter) that are created by using these optimizers in a computation. Keras V2 optimizers, such as `tensorflow.python.keras.optimizer_v2.gradient_descent.SGD`, internally convert their **constant** hyperparameters (learning rate, momentum, etc.) into **non-trainable variables** in the TF graph as opposed pure constants (like tf.constant for example).", "Any updates on this issue? Thanks.", "@behzad-a,\r\nCan you please share the complete reproducible code or a Colab Gist so that we can take it forward? Thanks! ", "Here is a simple example of reproducing this:\r\n\r\n```python\r\nimport tensorflow as tf\r\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.1)  # opt with a constant hyperparameter (LR)\r\nlr_op = optimizer._get_hyper(\"learning_rate\")  # hyperparameter that's created in the TF graph\r\nprint(isinstance(lr_op, tf.Variable))  # Prints True\r\nprint(isinstance(lr_op, tf.Tensor))  # Prints False\r\n```", "@behzad-a,\r\n**`Learning Rate`** is set as **`Variable`** because some **`Optimizers`** change the value of **`Learning Rate`** dynamically during **`Training`**. Example of one such Optimizer is [Adagrad](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adagrad). \r\nHope this answers your question. Thanks!", "While that may be true for Adagrad, that's not always the case. For an SGD optimizer, for example, a constant learning rate is guaranteed to stay constant during training. Furthemore, please note that this odd behavior is only observed in V2 optimizers. V1 optimizers, such as the one pasted below, correctly use constants in the TF graph for learning rates that are explicitly specified to be constants by the user.\r\n\r\n```python\r\noptimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.1)\r\n```", "Any updates on this? Thanks.", "The reason is that these parameters are intended to be mutable without rebuilding the graph (graphs are immutable), e.g. `optimizer.learning_rate = 0.1` is a valid use case (independently of the optimizer considered).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45558\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45558\">No</a>\n"]}, {"number": 45556, "title": "lstm related tests are failing due to unprovided optional inputs are not read correctly as nullptr on s390x machine", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n`NoCifgNoPeepholeNoProjectionNoClippingLstmTest.LstmBlackBoxTest` from `nnapi_delegate_test` is failing due to seg fault on s390x. I looked into this issue a bit and found that it has this error because of this flag from `tensorflow/lite/kernels/lstm_eval.cc` is not set properly when `use_layer_norm` should be `false`.\r\n```\r\nconst bool use_layer_norm = (forget_layer_norm_coefficients_ptr != nullptr);\r\n```\r\n\r\nAs I checked further, I found that this is an interesting issue because these are the 4 optional inputs that come together:\r\n```\r\n// Layer norm coefficients of size 'n_cell', representing diagonal matrices.\r\n//   input_layer_norm_coefficients_ptr  - optional\r\n//   forget_layer_norm_coefficients_ptr - optional\r\n//   cell_layer_norm_coefficients_ptr   - optional\r\n//   output_layer_norm_coefficients_ptr - optional\r\n```\r\nSo it looks like they should be either all be used or all not be used. However, I found that when the test model does not have these 4 optional inputs, only one of them is `nullptr` while the other 3 are some random values. On an x86 machine this one input is `forget_layer_norm_coefficients_ptr` while on an s390x machine this one input is `cell_layer_norm_coefficients_ptr`. In other words, if I change this flag check into:\r\n```\r\nconst bool use_layer_norm = (cell_layer_norm_coefficients_ptr != nullptr);\r\n```\r\nthen the test will pass on s390x and fail with seg fault on x86. \r\nI just wonder is this flag check just randomly choose `forget_layer_norm_coefficients_ptr` or there are some rational behind it? It looks to me that the 4 optional inputs are always treated equally in the `LSTMOpModel` object in the test file, but there is this subtle difference between them. It will be very helpful to understand why only `forget_layer_norm_coefficients_ptr` is picked here so that I will be able to fix it for s390x rather than simply modifying the flag definition.\r\n\r\nAfter solving this issue, another lstm test case `NoCifgPeepholeProjectionClippingLstmTest.LstmBlackBoxTest` fails again in this test file. This time it is due to the `GetTensorData` function in `tensorflow/lite/kernels/internal/tensor_ctypes.h` does not return a `nullptr` for tensor of type `kTfLiteNoType`. I think all the tensor of type `kTfLiteNoType` should have it `data.raw` field points to a `nullptr`, but looks it is breaking for this test case only. I have to make the following change to fix it:\r\n```diff\r\ndiff --git a/tensorflow/lite/kernels/internal/tensor_ctypes.h b/tensorflow/lite/kernels/internal/tensor_ctypes.h\r\nindex f1d3e17fcb..f5165cc0f0 100644\r\n--- a/tensorflow/lite/kernels/internal/tensor_ctypes.h\r\n+++ b/tensorflow/lite/kernels/internal/tensor_ctypes.h\r\n@@ -27,8 +27,8 @@ inline T* GetTensorData(TfLiteTensor* tensor) {\r\n template <typename T>\r\n inline const T* GetTensorData(const TfLiteTensor* tensor) {\r\n-  return tensor != nullptr ? reinterpret_cast<const T*>(tensor->data.raw)\r\n-                           : nullptr;\r\n+  return tensor != nullptr ? (tensor->type != kTfLiteNoType ?\r\n+                 reinterpret_cast<const T*>(tensor->data.raw) : nullptr) : nullptr;\r\n }\r\n inline RuntimeShape GetTensorShape(const TfLiteTensor* tensor) {\r\n```\r\nI am not sure but it looks like these two issues are related. They all seem to be originated from the fact that the unprovided optional input is not set correctly. I am also suspecting that it has something to do with the model field initialization process or `lstm.cc` evaluation process, but all these errors are not straightforward so I would appreciate if any help could be provided, thanks.\r\n\r\n**Describe the expected behavior**\r\nThe unprovided optional input should be `nullptr`, and test case should pass\r\n\r\n**Standalone code to reproduce the issue**\r\nRunning the test case on s390x will reproduce the error.\r\n```\r\nbazel test --host_javabase=\"@local_jdk//:jdk\" --cache_test_results=no --build_tests_only --test_output=errors -- //tensorflow/lite/delegates/nnapi:nnapi_delegate_test\r\n```\r\n\r\n**Other info / logs** \r\n", "comments": ["@renjie-liu could you take a look at this?", "It looks like related to the nnapi_delegate_test setup? Miao, can you help take a look?", "To add on a bit more info, there are two other related test: `//tensorflow/lite/kernels:lstm_test` and `//tensorflow/lite/kernels:lstm_eval_test`.\r\nBefore any changes were made, `lstm_test` is failing due to seg fault on both my x86 machine and s390x machine. The first fix about `use_layer_norm` is actually making this test proceed further and then fail again.\r\nBefore any changes were made, `lstm_eval_test` will pass on both x86 and s390x. The second change I made about `GetTensorData` is actually breaking this test.\r\nIt looks to me that these tests are correlated and share some common issues here, still investigating.", "I haven't managed to reproduce the issue yet, but I can shed some light on why `forget_layer_norm_coefficients_ptr` is checked. The reason is because when the cell is used in the CIFG (coupled input and forget gates) mode, input layer norm coefficients will not be present but the rest of the layer norm weights still could be set. As the layer norm weights are either all present or all omitted, `forget_layer_norm_coefficients_ptr` is used as an indicator for all these 3 or 4 tensors.\r\n\r\nAlso, I also don't think that the problem is in the test setup since the test is [not adding the layer normalisation weights at all](https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/lite/delegates/nnapi/nnapi_delegate_test.cc#L3357-L3384). So the test should use the legacy path where the op has 20 inputs and do not touch layer norm coefficients after `use_layer_norm` is [set to false](https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/lite/kernels/lstm.cc#L1157).", "Hi @lev-prol, thanks a lot for the explanation! If I understand correctly, I think theoretically, in this particular nnapi test case where only 20 inputs are provided, all the 4 layer norm coefficient input should be `nullptr` and we could check `forget_layer_norm_coefficients_ptr` to indicate the rest ones.\r\nHowever, as I mentioned, if I change `const bool use_layer_norm = (forget_layer_norm_coefficients_ptr != nullptr);` into `const bool use_layer_norm = (cell_layer_norm_coefficients_ptr != nullptr);` at [lstm_eval.cc](https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/lite/kernels/lstm_eval.cc#L226), the test will fail on my x86 machine because `use_layer_norm` became true while it should be false. But I think theoretically, making this change should  not make it fail, is that right?\r\n\r\nI have some further info here when running this test case on my x86 machine, when the model [invokes](https://github.com/tensorflow/tensorflow/blob/fcc4b966f1265f466e82617020af93670141b009/tensorflow/lite/delegates/nnapi/nnapi_delegate_test.cc#L3281) itself and execute until this [line](https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/lite/core/subgraph.cc#L965), I pull out some info into gdb:\r\n```\r\n(gdb) p *node.inputs\r\n$1 = {size = 20, data = 0x55555567beb4}\r\n(gdb) p node.inputs->data[20]\r\n$2 = 0\r\n(gdb) p node.inputs->data[21]\r\n$3 = 33\r\n(gdb) p node.inputs->data[22]\r\n$4 = 0\r\n(gdb) p node.inputs->data[23]\r\n$5 = 1\r\n```\r\nThese \"out-of-range\" input will still be read as the 4 optional input later (because in `eval` [function](https://github.com/tensorflow/tensorflow/blob/fcc4b966f1265f466e82617020af93670141b009/tensorflow/lite/kernels/lstm.cc#L1518) the input is read regardless of its size), and you could notice only input 21 (kForgetLayerNormCoefficientsTensor) points to an invalid tensor index which will be retrieved as a `nullptr` later, the other 3 all points to a valid tensor index so their pointer will not be `nullptr`. So this should be the reason why the other 3 pointers are not `nullptr`, and I am very curious if this could be explained.\r\n(In comparison, this is the info I got for the same test case on s390x machine:)\r\n```\r\n(gdb) p *node.inputs\r\n$3 = {size = 20, data = 0x2aa001194e4}\r\n(gdb) p node.inputs->data[20]\r\n$4 = 0\r\n(gdb) p node.inputs->data[21]\r\n$5 = 0\r\n(gdb) p node.inputs->data[22]\r\n$6 = 33\r\n(gdb) p node.inputs->data[23]\r\n$7 = 1\r\n```\r\n\r\nAs for the `GetTensorData` issue, my fix seems to cause regression for other test cases but I found this [fix](https://github.com/tensorflow/tensorflow/commit/1970c2158b1ffa416d159d03c3370b9a462aee35) is merged already, so I guess it will be fixed in 2.4.0 release.", "Test case pass on latest release, closing the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45556\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45556\">No</a>\n"]}, {"number": 45555, "title": "Run apt-get update before installing jupyter dependencies", "body": "This is also required to fix some things with the new dockerfiles. I'll\nbackport this to the old branches alongside the pip downgrade.", "comments": ["I don't maintain the ppc64le images.", "> I don't maintain the ppc64le images.\n\nI meant this issue It Is also in your strings (this PR) for all the images.\n\nppc64le was Just a random sample.", "In that case, maybe I can take a look next time I have time -- this change\nfixed the builds we deploy.\n\nOn Thu, Dec 10, 2020 at 10:22 AM bhack <notifications@github.com> wrote:\n\n> I don't maintain the ppc64le images.\n>\n> I meant this issue It Is also in your strings (this PR) for all the images.\n>\n> ppc64le was Just a random sample.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/45555#issuecomment-742704823>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AHXWEQDF6SKOC5V4ML72UZDSUEGV7ANCNFSM4UUAVTTQ>\n> .\n>\n", "> In that case, maybe I can take a look next time I have time -- this change fixed the builds we deploy.\r\n> [\u2026](#)\r\n> On Thu, Dec 10, 2020 at 10:22 AM bhack ***@***.***> wrote: I don't maintain the ppc64le images. I meant this issue It Is also in your strings (this PR) for all the images. ppc64le was Just a random sample. \u2014 You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub <[#45555 (comment)](https://github.com/tensorflow/tensorflow/pull/45555#issuecomment-742704823)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AHXWEQDF6SKOC5V4ML72UZDSUEGV7ANCNFSM4UUAVTTQ> .\r\n\r\nAs you like it was just a review comment.", "I don't know if understand this correctly but this kind of issues could be caused by not removing the `apt cache`  at the end of apt commands. \r\nSo when you add something to a single layer you rebuild only that layer and the build will not fail cause you use a previous layer apt cache."]}, {"number": 45554, "title": " [INTEL MKL] Enable DNNL cpu dispatch control", "body": "Define DNNL_ENABLE_MAX_CPU_ISA when building oneDNN to allow TF env variable to pass through to OneDNN in order to select the desired ISA. Once this is enabled during build time then DNNL_MAX_CPU_ISA environment variable can be set to desired ISA such as AMX, VNNI etc, during runtime and will override ISA automatically detected by oneDNN. This is useful for debugging.", "comments": ["@shwetaoj  Can you please address Ubuntu Sanity errors? Thanks!", "@gbaned, @penpornk  could you please provide more details on what might be causing this issue? The error seems to be related to @llvm-project//llvm.", "@shwetaoj I agree that [Ubuntu Sanity](https://source.cloud.google.com/results/invocations/3521ec52-f69c-4067-837a-bd13e5a017b4/log) and [MacOS CPU Python3](https://source.cloud.google.com/results/invocations/fdce2b7a-a70c-4f0a-94ac-1061bf13759d/log) failures are unrelated. Our test dashboard at HEAD is green now so let me rerun all the tests just to confirm. ", "Thank you so much. Appreciate it.", "@shwetaoj @gbaned All tests passed now. :)", "Thanks a lot @penpornk. Appreciate your help."]}, {"number": 45553, "title": "Update PCRE library from 8.42 to 8.44", "body": "This PR updates PCRE library from 8.42 to 8.44.\r\n\r\nNote there is a CVS related to old 8.42 (https://nvd.nist.gov/vuln/detail/CVE-2019-20838#VulnChangeHistorySection)\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\nHandles [CVE-2019-20838](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-20838) and [CVE-2020-14155](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-14155)", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45553) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 45552, "title": "Update PCRE library from 8.42 to 8.44", "body": "This PR updates PCRE library from 8.42 to 8.44.\r\n\r\nNote there is a CVS related to old 8.42 (https://nvd.nist.gov/vuln/detail/CVE-2019-20838#VulnChangeHistorySection)\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\nHandles [CVE-2019-20838](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-20838) and [CVE-2020-14155](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-14155)", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45552) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 45551, "title": "Update PCRE library from 8.42 to 8.44", "body": "This PR updates PCRE library from 8.42 to 8.44.\r\n\r\nNote there is a CVS related to old 8.42 (https://nvd.nist.gov/vuln/detail/CVE-2019-20838#VulnChangeHistorySection)\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\nHandles [CVE-2019-20838](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-20838) and [CVE-2020-14155](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-14155)", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45551) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 45550, "title": "Update PCRE library from 8.42 to 8.44", "body": "This PR updates PCRE library from 8.42 to 8.44.\r\n\r\nNote there is a CVS related to old 8.42 (https://nvd.nist.gov/vuln/detail/CVE-2019-20838#VulnChangeHistorySection)\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\nHandles [CVE-2019-20838](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-20838) and [CVE-2020-14155](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-14155)", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45550) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 45549, "title": "Update PCRE library from 8.42 to 8.44", "body": "This PR updates PCRE library from 8.42 to 8.44.\r\n\r\nNote there is a CVS related to old 8.42 (https://nvd.nist.gov/vuln/detail/CVE-2019-20838#VulnChangeHistorySection)\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\nHandles [CVE-2019-20838](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-20838) and [CVE-2020-14155](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-14155)", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45549) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 45548, "title": "Update PCRE library from 8.42 to 8.44", "body": "This PR updates PCRE library from 8.42 to 8.44.\r\n\r\nNote there is a CVS related to old 8.42 (https://nvd.nist.gov/vuln/detail/CVE-2019-20838#VulnChangeHistorySection)\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\nHandles [CVE-2019-20838](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-20838) and [CVE-2020-14155](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-14155)", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45548) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 45547, "title": "Cleanup `tensorflow/c/experimental/gradients` Part 1", "body": "@saxenasaurabh \r\n\r\nOne thing I don't understand, if computing numerical gradients with TensorFloat-32 is numerically unstable, does disabling TensorFloat-32 inside `gradient_checker` make a lot more sense than disabling it inside the test, since we will have to disable TF-32 in all binaries depending on `gradient_checker` anyway ?", "comments": ["tensorflow/c/eager/unified_api_test.cc:124:76: error: invalid conversion from 'tensorflow::int64* {aka long long int*}' to 'int64_t* {aka long int*}' [-fpermissive]\r\n         TestTensorHandleWithDimsFloat(ctx.get(), data, dim_sizes, 2, &x_raw);\r\n\r\nPlease fix.", "> tensorflow/c/eager/unified_api_test.cc:124:76: error: invalid conversion from 'tensorflow::int64* {aka long long int*}' to 'int64_t* {aka long int*}' [-fpermissive]\r\nTestTensorHandleWithDimsFloat(ctx.get(), data, dim_sizes, 2, &x_raw);\r\n\r\nDone", "@vnvo2409 can you please resolve conflicts ?", "> @vnvo2409 can you please resolve conflicts ?\r\n\r\nDone", "Looks like `eager/BUILD` has conflicts. Rebase maybe? Also `nn_grad_test` is segfaulting internally. I will try to patch and see what's going on.", "@saxenasaurabh \r\n\r\n> `eager/BUILD` has conflicts.\r\n\r\nI've already fixed it.\r\n\r\n> Also `nn_grad_test` is segfaulting internally.\r\n\r\nThere are 3 memory problems with `nn_grad_test`. \r\n\r\n---\r\n\r\nOne is comming from `SoftMaxModel` and `RunAndMaybeSum`\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/bd1fd58c37b53426d0efca83bda64e49817da80d/tensorflow/c/eager/gradient_checker.cc#L68-L73\r\n\r\n`SoftMaxModel` returns 2 tensors but `model_outputs` take only 1 tensor so it causes a memory overflow. I intend to fix this issue in the next PR ( because running without `asan` is fine ), but we could fix in this PR if need though. WDYT ?\r\n\r\n---\r\n\r\n`gradient_checker` leaks a lot of `AbstractTensorHandle`. Will be fixed in the next PR too.\r\n\r\n---\r\n\r\nIt seems that `tape->ComputeGradients` leaks a tensor. The traceback points to the allocation of a new tensor in `BuildOnesLike`. However, I could not find a fix for it yet.", "Ah got it. Let's fix the SoftmaxModel. The leaks we can address in a follow-up.", "> Let's fix the SoftmaxModel.\r\n\r\nDone", "@saxenasaurabh \nThe copybara/feedback failed"]}, {"number": 45546, "title": "Tensorflow does not use the GPU during training with eager execution, despite manual activation", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux (Google colab)):\r\n- TensorFlow installed using `pip install tensorflow-gpu`:\r\n- TensorFlow version (2.3.1):\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1.243\r\n- GPU model and memory: Tesla T4 computeCapability: 7.5 coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI'm experiencing a very slow training time when running a third party [script](https://github.com/marload/DeepRL-TensorFlow2/blob/master/DQN/DQN_Discrete.py) which is the same(0% difference between GPU and CPU). I use the following command for activating the GPU on google colab after installing `tensorflow-gpu` using `pip`:\r\n\r\n```\r\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\r\nif len(physical_devices) > 0:\r\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n```\r\nI add the lines above in `main()` in the script I referred to earlier and I use [wandb](https://github.com/wandb/client) for monitoring the training. Here are the [graphs](https://drive.google.com/file/d/1Fgn7tlm6HBUyZIcPelVjxNz_RWvY7QU_/view?usp=sharing) within a few minutes of training showing 0% GPU utilization.\r\n\r\n**Describe the expected behavior**\r\n\r\nA fast performance which results in a remarkable difference in speeds (CPU vs GPU) and GPU utilization above 0% if the metrics are accurate and if they are not, I'm still experiencing the same speed when run on CPU or GPU.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport wandb\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Input, Dense\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\nimport gym\r\nimport argparse\r\nimport numpy as np\r\nfrom collections import deque\r\nimport random\r\n\r\ntf.keras.backend.set_floatx('float64')\r\nwandb.init(name='DQN', project=\"deep-rl-tf2\")\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument('--gamma', type=float, default=0.95)\r\nparser.add_argument('--lr', type=float, default=0.005)\r\nparser.add_argument('--batch_size', type=int, default=32)\r\nparser.add_argument('--eps', type=float, default=1.0)\r\nparser.add_argument('--eps_decay', type=float, default=0.995)\r\nparser.add_argument('--eps_min', type=float, default=0.01)\r\n\r\nargs = parser.parse_args()\r\n\r\nclass ReplayBuffer:\r\n    def __init__(self, capacity=10000):\r\n        self.buffer = deque(maxlen=capacity)\r\n    \r\n    def put(self, state, action, reward, next_state, done):\r\n        self.buffer.append([state, action, reward, next_state, done])\r\n    \r\n    def sample(self):\r\n        sample = random.sample(self.buffer, args.batch_size)\r\n        states, actions, rewards, next_states, done = map(np.asarray, zip(*sample))\r\n        states = np.array(states).reshape(args.batch_size, -1)\r\n        next_states = np.array(next_states).reshape(args.batch_size, -1)\r\n        return states, actions, rewards, next_states, done\r\n    \r\n    def size(self):\r\n        return len(self.buffer)\r\n\r\nclass ActionStateModel:\r\n    def __init__(self, state_dim, aciton_dim):\r\n        self.state_dim  = state_dim\r\n        self.action_dim = aciton_dim\r\n        self.epsilon = args.eps\r\n        \r\n        self.model = self.create_model()\r\n    \r\n    def create_model(self):\r\n        model = tf.keras.Sequential([\r\n            Input((self.state_dim,)),\r\n            Dense(32, activation='relu'),\r\n            Dense(16, activation='relu'),\r\n            Dense(self.action_dim)\r\n        ])\r\n        model.compile(loss='mse', optimizer=Adam(args.lr))\r\n        return model\r\n    \r\n    def predict(self, state):\r\n        return self.model.predict(state)\r\n    \r\n    def get_action(self, state):\r\n        state = np.reshape(state, [1, self.state_dim])\r\n        self.epsilon *= args.eps_decay\r\n        self.epsilon = max(self.epsilon, args.eps_min)\r\n        q_value = self.predict(state)[0]\r\n        if np.random.random() < self.epsilon:\r\n            return random.randint(0, self.action_dim-1)\r\n        return np.argmax(q_value)\r\n\r\n    def train(self, states, targets):\r\n        self.model.fit(states, targets, epochs=1, verbose=0)\r\n    \r\n\r\nclass Agent:\r\n    def __init__(self, env):\r\n        self.env = env\r\n        self.state_dim = self.env.observation_space.shape[0]\r\n        self.action_dim = self.env.action_space.n\r\n\r\n        self.model = ActionStateModel(self.state_dim, self.action_dim)\r\n        self.target_model = ActionStateModel(self.state_dim, self.action_dim)\r\n        self.target_update()\r\n\r\n        self.buffer = ReplayBuffer()\r\n\r\n    def target_update(self):\r\n        weights = self.model.model.get_weights()\r\n        self.target_model.model.set_weights(weights)\r\n    \r\n    def replay(self):\r\n        for _ in range(10):\r\n            states, actions, rewards, next_states, done = self.buffer.sample()\r\n            targets = self.target_model.predict(states)\r\n            next_q_values = self.target_model.predict(next_states).max(axis=1)\r\n            targets[range(args.batch_size), actions] = rewards + (1-done) * next_q_values * args.gamma\r\n            self.model.train(states, targets)\r\n    \r\n    def train(self, max_episodes=1000):\r\n        for ep in range(max_episodes):\r\n            done, total_reward = False, 0\r\n            state = self.env.reset()\r\n            while not done:\r\n                action = self.model.get_action(state)\r\n                next_state, reward, done, _ = self.env.step(action)\r\n                self.buffer.put(state, action, reward*0.01, next_state, done)\r\n                total_reward += reward\r\n                state = next_state\r\n            if self.buffer.size() >= args.batch_size:\r\n                self.replay()\r\n            self.target_update()\r\n            print('EP{} EpisodeReward={}'.format(ep, total_reward))\r\n            wandb.log({'Reward': total_reward})\r\n\r\n\r\ndef main():\r\n    physical_devices = tf.config.experimental.list_physical_devices('GPU')\r\n    if len(physical_devices) > 0:\r\n        tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n    env = gym.make('CartPole-v1')\r\n    agent = Agent(env)\r\n    agent.train(max_episodes=1000)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["I\u2019m having same issue. But in Kubernetes. ", "@joehoeller the problem is I can't tell whether this is a tensorflow or implementation specific problem. Clearly most of DRL algorithms are designed in a way that is not GPU friendly, a lot of computations are done sequentially using the CPU as this is not the first time that I experience this delay and GPU under-utilization. I never used kubernetes, so you can tell better than me if it's a similar issue.", "Was able to reproduce the issue with TF v2.2 and TF v2.3. GPU utilization shows 0%. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/e09b444c63660052f3f679a954cdd587/45546.ipynb). \r\n\r\n\r\n![gpu 2 3](https://user-images.githubusercontent.com/57165142/101768383-7e9f9a00-3b0b-11eb-91fc-fa51ef78d859.png)\r\nThanks!", "There is an update, I'm facing the same issue running the script below I wrote and I noticed that the issue is gone when I disable eager execution, which implies it would be gone for the script I used in the issue description above:\r\n\r\n    from collections import deque\r\n    from time import perf_counter\r\n    \r\n    import gym\r\n    import numpy as np\r\n    import tensorflow as tf\r\n    import wandb\r\n    from tensorflow.keras.layers import Conv2D, Dense, Flatten, Input\r\n    from tensorflow.keras.models import Model\r\n    from tensorflow.keras.optimizers import Adam\r\n    \r\n    from utils import activate_gpu_tf\r\n    \r\n    \r\n    class DQN:\r\n        def __init__(self, env, buffer_size=10000, batch_size=32):\r\n            self.env = gym.make(env)\r\n            self.input_shape = self.env.observation_space.shape\r\n            self.main_model = self.create_model()\r\n            self.buffer_size = buffer_size\r\n            self.buffer = deque(maxlen=buffer_size)\r\n            self.batch_size = batch_size\r\n    \r\n        def create_model(self):\r\n            x0 = Input(self.input_shape)\r\n            x = Conv2D(32, 8, 4, activation='relu')(x0)\r\n            x = Conv2D(64, 4, 2, activation='relu')(x)\r\n            x = Conv2D(64, 3, 1, activation='relu')(x)\r\n            x = Flatten()(x)\r\n            x = Dense(512, 'relu')(x)\r\n            x = Dense(self.env.action_space.n)(x)\r\n            return Model(x0, x)\r\n    \r\n        def get_action(self, epsilon, state):\r\n            if np.random.random() < epsilon:\r\n                return self.env.action_space.sample()\r\n            q_values = self.main_model.predict(np.expand_dims(state, 0))\r\n            return np.argmax(q_values)\r\n    \r\n        def get_buffer_sample(self):\r\n            indices = np.random.choice(len(self.buffer), self.batch_size, replace=False)\r\n            memories = [self.buffer[i] for i in indices]\r\n            batch = [np.array(item) for item in zip(*memories)]\r\n            return batch\r\n    \r\n        def update(self, batch, gamma):\r\n            states, actions, rewards, dones, new_states = batch\r\n            q_states = self.main_model.predict(states)\r\n            q_new_states = self.main_model.predict(new_states)\r\n            q_target = np.copy(q_states)\r\n            idx = np.arange(self.batch_size, dtype=np.uint8)\r\n            q_target[idx, actions] = rewards + gamma * np.max(q_new_states, axis=1) * dones\r\n            self.main_model.fit(states, q_target, verbose=0)\r\n    \r\n        def fit(\r\n            self,\r\n            epsilon_start=1,\r\n            epsilon_end=0.01,\r\n            decay_n_frames=150000,\r\n            lr=1e-4,\r\n            gamma=0.99,\r\n            target_reward=19,\r\n        ):\r\n            state = self.env.reset()\r\n            total_rewards = []\r\n            episode_reward = 0\r\n            frame_idx = 0\r\n            start_time = perf_counter()\r\n            optimizer = Adam(lr)\r\n            self.main_model.compile(optimizer, loss='mse')\r\n            mean_reward = 0\r\n            last_reset_frame = 0\r\n            while True:\r\n                frame_idx += 1\r\n                epsilon = max(epsilon_end, epsilon_start - frame_idx / decay_n_frames)\r\n                action = self.get_action(epsilon, state)\r\n                new_state, reward, done, info = self.env.step(action)\r\n                episode_reward += reward\r\n                self.buffer.append((state, action, reward, done, new_state))\r\n                state = new_state\r\n                if done:\r\n                    if mean_reward >= target_reward:\r\n                        print(f'Reward achieved in {frame_idx} frames!')\r\n                        break\r\n                    total_rewards.append(episode_reward)\r\n                    speed = (frame_idx - last_reset_frame) / (perf_counter() - start_time)\r\n                    last_reset_frame = frame_idx\r\n                    start_time = perf_counter()\r\n                    mean_reward = np.mean(total_rewards[-100:])\r\n                    titles = ('frame', 'games', 'reward', 'epsilon', 'speed')\r\n                    values = (\r\n                        frame_idx,\r\n                        len(total_rewards),\r\n                        np.around(mean_reward, 2),\r\n                        np.around(epsilon, 2),\r\n                        f'{round(speed)} frames/s',\r\n                    )\r\n                    display = (f'{title}: {value}' for title, value in zip(titles, values))\r\n                    print(*display, sep=', ')\r\n                    episode_reward = 0\r\n                    state = self.env.reset()\r\n                if len(self.buffer) < self.buffer_size:\r\n                    continue\r\n                batch = self.get_buffer_sample()\r\n                self.update(batch, gamma)\r\n    \r\n    \r\n    if __name__ == '__main__':\r\n        # tf.compat.v1.disable_eager_execution()\r\n        wandb.init(name='Eager execution')\r\n        activate_gpu_tf()\r\n        agn = DQN('PongNoFrameskip-v4', 10000)\r\n        agn.fit()", "I am having the same issue when using 2.3.0-gpu-jupyter Docker image. It was updated 13 days ago and we are unable to connect to GPU devices from the Jupyter Notebook. Looking at the logs for the docker container, the dso_loader is looking for the incorrect version of the cuda library (it is trying to load 11 instead of 10.1 per the documentation https://www.tensorflow.org/install/source ) This seems to be a build issue with TensorFlow. I would recommend checking your container logs to see if you are seeing the same issue.   \r\n\r\n```\r\n2020-12-22 01:34:00.776696: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n2020-12-22 01:34:00.776935: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n2020-12-22 01:34:00.777110: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n2020-12-22 01:34:00.777185: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2020-12-22 01:34:00.777235: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2020-12-22 01:34:00.777280: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2020-12-22 01:34:00.777408: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n2020-12-22 01:34:00.777568: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n2020-12-22 01:34:00.777580: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n```\r\n\r\n@sanjoy I think this is a build issue more than anything else", "@Bennyweasl Yes, I remember seeing these \"could not load ...\" prompts and then I reinstalled cuda based on tensorflow [docs](https://www.tensorflow.org/install/gpu) and the problem persists while on eager execution. \r\n", "@emadboctorx I went back through my docker cached imaged and saw that when I updated a requirements.txt file. One of the packages forced an update of tensorflow to 2.4.0 which requires cuda 11. That was my issue. ", "@Bennyweasl  yeah, I encountered an issue related to tensorflow 2.4 which is installed by default using pip, being the latest version. I get an error on google colab indicating that there is a problem with cuda driver, I just don't remember the exact error but most probably would be the same as yours.", "> @emadboctorx I went back through my docker cached imaged and saw that when I updated a requirements.txt file. One of the packages forced an update of tensorflow to 2.4.0 which requires cuda 11. That was my issue.\r\n\r\nClosing the issue based on this, please reopen if you disagree.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45546\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45546\">No</a>\n", "@sanjoy You should not close this issue @emadboctorx was just commenting on something I brought up not his direct issue. ", "@sanjoy I don't remember giving you permission to close my issue.", "Sorry for the confusion.\r\n\r\nI was not able to reproduce this though.  I tried both TF 2.3 and latest nightly and Titan-V and P100.  In all cases, I see some memory (400-500 MB) allocated on the GPU.  So clearly the program is doing _something_, but perhaps slowly.\r\n\r\nHave you tried using the [TensorBoard profiler](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras) to see if something interesting jumps out?", "@sanjoy, no problem, I closed the other issue, thanks. I have not tried the profiler no so I will try and see if it shows anything significant, but how it may differ from wandb? I'm asking because wandb graphs show enough proof that the gpu is not working (gpu utilization 0%)", "@emadboctorx Could you please try on latest stable version of tf 2.5 or 2.4.1 and let us know if this is still an issue.Thanks!", "The problem was solved by modifying the implementation", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45546\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45546\">No</a>\n"]}, {"number": 45545, "title": "Revert \"Polish TensorRT static linking a little.\"", "body": "DO NOT SUBMIT.\r\n\r\nThis reverts commit 1350613d6f8758d3fce2cec43228bd6ab8162641.\r\n\r\nBreaks TF Serving Docker GPU builds.", "comments": ["@netfs Can you please resolve conflicts? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on."]}, {"number": 45544, "title": "remove $ from readme for easy compatibility with Windows", "body": "Hi! \r\n`` $ `` only case specific to Linux/Unix based systems only. If we use ``$``in a Windows machine we get an error as its doesn't accepts ``$`` sign in the syntax. \r\nSo for solving that I am making this PR which will add easy compatibility to users getting started with TensorFlow on Windows.\r\nDo suggest me if I should have made this change in some other way. \r\nThanks! :) ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45544) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 45543, "title": "Windows Tensorflow load large than 4GB file will meet outOfRange error.", "body": "**System information**\r\n-OS Platform and Distribution: Windows\r\n-TensorFlow installed from: binary\r\n-TensorFlow version: 1.0.1, 1.15\r\n-Python version: 3.6\r\n\r\n**Describe the current behavior**\r\nTensorflow load large than 4GB file on Windows will meet **outOfRange** error.\r\n\r\n**Other info / logs** \r\nTensorflow Windows version's file stat function by default use **_wstat** which represents the file size as 32-bit integer, once load large than 4GB file on Windows will meet **outOfRange** error. So I change the default to use **_wstat64** which represents the file size as 64-bit integer to fix the issue.\r\n\r\nAttach [PR](https://github.com/tensorflow/tensorflow/pull/45542) here.\r\n\r\n\r\n\r\n\r\n", "comments": ["Does this manifest on master?", "The bug can be reproduced on TF 1.15 and I've tested the fix it worked. The related code has not changed for 3 years, so the master branch should have the same issue.", "@CathyCan0329 \r\n\r\nCan you please share a simple stand alone code with all dependencies, or if possible share a colab gist with the issue reported.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45543\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45543\">No</a>\n", "This is fixed now, as the PR landed", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45543\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45543\">No</a>\n"]}, {"number": 45542, "title": "Fix Windows platform file stat length out of range error", "body": "Tensorflow Windows version's file stat function by default use **_wstat** which represents the file size as 32-bit integer, once load large than 4GB file will meet **outOfRange** error. So I change the default to use **_wstat64** which represents the file size as 64-bit integer to fix the issue.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45542) for more info**.\n\n<!-- need_sender_cla -->", "You have to sign the CLA before we can review :)", "@googlebot I signed it!\r\n\r\n> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n> \r\n> \ud83d\udcdd **Please visit https://cla.developers.google.com/ to sign.**\r\n> \r\n> Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\r\n> \r\n> #### What to do if you already signed the CLA\r\n> ##### Individual signers\r\n> * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n> ##### Corporate signers\r\n> * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\r\n> * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\r\n> \r\n> \u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45542) for more info**.\r\n\r\n@googlebot I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45542) for more info**.\n\n<!-- need_author_cla -->", "> @googlebot I fixed it.\r\n\r\n\r\n\r\n> We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors. If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)? If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\r\n> In order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\r\n> \r\n> \u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45542) for more info**.\r\n\r\n@googlebot I fixed it.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45542) for more info**.\n\n<!-- need_author_cla -->", "> @googlebot I fixed it\r\n\r\n@googlebot I fixed it\r\n\r\n> We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors. If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)? If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\r\n> In order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\r\n> \r\n> \u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45542) for more info**.\r\n\r\n@googlebot I fixed it\r\n\r\n> We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors. If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)? If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\r\n> In order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\r\n> \r\n> \u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45542) for more info**.\r\n@googlebot I fixed it\r\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45542) for more info**.\n\n<!-- need_author_cla -->", "You need to sign the CLA with the Microsoft account since it seem that was used at one point for one of the commits. Looks like a different GitHub user?", "@googlebot  I fixed it", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45542) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent.", "> You need to sign the CLA with the Microsoft account since it seem that was used at one point for one of the commits. Looks like a different GitHub user?\r\n\r\nI've fixed the CLA problem. Sorry for confusing the github account."]}, {"number": 45541, "title": "Official Example For TensorFlow Lite Model Maker keeps crashing", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n **No**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n**Mac Book Pro 2017(2.3 GHz Dual-Core Intel Core i5, 8 GB 2133 MHz LPDDR3), MacOS Big Sur 11.0.1,\r\nBrowser: Version 87.0.4280.88 (Official Build) (x86_64),\r\nbut I guess the above info does not matter as I run the example in Google Colab [here](https://www.tensorflow.org/lite/tutorials/model_maker_question_answer)**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n**No**\r\n- TensorFlow installed from (source or binary):\r\n**TensorFlow is provided by Google Colab environment**\r\n- TensorFlow version (use command below):\r\n**2.4.0-dev20200902**\r\n- Python version:\r\nPython 3.6.9\r\n- Bazel version (if compiling from source):\r\n**N/A**\r\n- GCC/Compiler version (if compiling from source):\r\n**N/A**\r\n- CUDA/cuDNN version:\r\n**nvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2019 NVIDIA Corporation\r\nBuilt on Sun_Jul_28_19:07:16_PDT_2019\r\nCuda compilation tools, release 10.1, V10.1.243**\r\n- GPU model and memory:\r\n**I cannot check this as I run the Colab provided in official Tensorflow page on the provided Google Colab Environment, which you can see in the following url: https://www.tensorflow.org/lite/tutorials/model_maker_question_answer**\r\n\r\n\r\n**Describe the current behavior**\r\nWhen I try to run the officially provided Colab from TensorFlow Lite Model Maker Library for BERT Q&A\r\nnot changing anything, everything goes fine until the training model = `question_answer.create(train_data, model_spec=spec)`\r\nAs soon as training process started, after a while it begins to consume memory with big chunks (e.g. memory consumption increases by 2GB per 2-3 seconds) and shortly it crashes the **Colab** telling \"Your session crashed after using all available RAM\". I think this is some sort of a bug, because otherwise this will not be an official example, or it would be documented somewhere in the example that the free Colab environment provided by Google with 16GB RAM is not enough to run this example.\r\nI also tried to run the example with much smaller dataset, but the same problem in the same way is being reproduced. So I am almost sure that this is a bug in the library.\r\n\r\n\r\n**Describe the expected behavior**\r\nThe example project provided in Official website of TensorFlow should run without any problem on the officially provided Colab Environment.\r\n\r\n**Standalone code to reproduce the issue**\r\nPlease just go node by node through the official example [in the following Colab](https://www.tensorflow.org/lite/tutorials/model_maker_question_answer) with a basic free Colab plan, without changing anything in the example.\r\nwhen you reach \"Customize the TensorFlow Model\" point, and run the following code `model = question_answer.create(train_data, model_spec=spec)` wait for 2-3 minutes, and you will get and error that Colab environment was crashed telling \"Your session crashed after using all available RAM\".\r\n\r\n**Other info / logs** \r\nThere is the logs that I get after running `!cat /var/log/colab-jupyter.log` in Colab just after the crash\r\nhttps://gist.github.com/ando0689/c67406e124ce8180935f36fe5e95e835", "comments": ["The colab crashes on trying with a different model_spec as well.\r\n```python\r\nspec = model_spec.get('bert_qa')\r\n```", "@ando0689 Can you please try with model maker nightly package?\r\n```python\r\n!pip install tflite-model-maker-nightly\r\n```\r\nThe colab does not crash with the nightly package. Thanks!", "Hi. Thank you very much.\r\n\r\nThis specific problem is resolved on nightly build, but overall the example is not working, I will post the issues here and if needed I will open a separate issue for this.\r\n\r\nhere are the issues:\r\n\r\n1. When I do `!pip install tflite-model-maker-nightly` I see the following error in the log:\r\n```\r\nERROR: tensorflow 2.3.0 has requirement h5py<2.11.0,>=2.10.0, but you'll have h5py 3.1.0 which is incompatible.\r\nERROR: tf-nightly 2.5.0.dev20201215 has requirement numpy~=1.19.2, but you'll have numpy 1.18.5 which is incompatible.\r\n```\r\nThough this does not stop the execution and I can continue.\r\n\r\n2. When i try to use the created model with TensorFlow Lite Android v 2.3.0 I get the following error when I try to create interpreter with this model:\r\nhttps://gist.github.com/ando0689/1874b6881dc7af3a31519e1bb2813386\r\n\r\n3. And when I increase the version of TensorFlow Lite on Android to `0.0.0-nightly` I get the following error when I try to do inference:\r\nhttps://gist.github.com/ando0689/b25f00443249da833af08cb8a6235a95\r\n\r\n4. Just a note: The provided dataset with 8000 entities seems to be too much for this example, as running on the free Colab, it takes about 6 hours, and usually fails with some error.. I was only able to train it by reducing the dataset to be 1000 entities, I guess it's enough for example.\r\n\r\nThank you in advance,\r\nPlease tell me if I need to create separate issue for this.\r\n", "Thanks for your issue. Lets continue the discussion on your new thread. I will close this one. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45541\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45541\">No</a>\n"]}, {"number": 45540, "title": "TFLite java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter on Android SDK 29", "body": "Hello. I have a simple Android app connected to an accelerometer. I have a few tflite models in the app, one of which is throwing  a \"Failed to run on the given Interpreter\" error without any additional information. The code works fine on SDK 25.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android SDK 29\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy A20e, Android version 10\r\n- TFLite version: \"org.tensorflow:tensorflow-lite:1.13.1\"\r\n\r\n**Describe the current behavior**\r\nWhen the interpreter is called on the Android 10 phone, this error is thrown: \r\n```\r\njava.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: \r\n2020-12-09 13:08:10.985 30085-30561/com.specknet.respeckmodeltesting W/System.err:     at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n2020-12-09 13:08:10.985 30085-30561/com.specknet.respeckmodeltesting W/System.err:     at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:145)\r\n2020-12-09 13:08:10.985 30085-30561/com.specknet.respeckmodeltesting W/System.err:     at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:250)\r\n2020-12-09 13:08:10.985 30085-30561/com.specknet.respeckmodeltesting W/System.err:     at com.specknet.respeckmodeltesting.classification.Classifier.classifyActivityWithFeatures(Classifier.kt:111)\r\n2020-12-09 13:08:10.986 30085-30561/com.specknet.respeckmodeltesting W/System.err:     at com.specknet.respeckmodeltesting.live.LiveDataActivity$onCreate$2.onReceive(LiveDataActivity.kt:176)\r\n2020-12-09 13:08:10.986 30085-30561/com.specknet.respeckmodeltesting W/System.err:     at android.app.LoadedApk$ReceiverDispatcher$Args.lambda$getRunnable$0$LoadedApk$ReceiverDispatcher$Args(LoadedApk.java:1646)\r\n2020-12-09 13:08:10.986 30085-30561/com.specknet.respeckmodeltesting W/System.err:     at android.app.-$$Lambda$LoadedApk$ReceiverDispatcher$Args$_BumDX2UKsnxLVrE6UJsJZkotuA.run(Unknown Source:2)\r\n2020-12-09 13:08:10.986 30085-30561/com.specknet.respeckmodeltesting W/System.err:     at android.os.Handler.handleCallback(Handler.java:883)\r\n2020-12-09 13:08:10.986 30085-30561/com.specknet.respeckmodeltesting W/System.err:     at android.os.Handler.dispatchMessage(Handler.java:100)\r\n2020-12-09 13:08:10.987 30085-30561/com.specknet.respeckmodeltesting W/System.err:     at android.os.Looper.loop(Looper.java:237)\r\n2020-12-09 13:08:10.987 30085-30561/com.specknet.respeckmodeltesting W/System.err:     at android.os.HandlerThread.run(HandlerThread.java:67)\r\n\r\n```\r\n**Describe the expected behavior**\r\nThe code works fine and classifies correctly when run on an Android 7.1.2 phone.\r\nThere is no code change between these two phones. Furthermore, other tflite models work on both phones. The only difference I see between the working models and the ones that crash are the conversion types. The models that crash have been \"MLIR Converted\" (inspected with Netron), and the models that do not crash are TOCO converted. \r\n\r\nIs there something preventing MLIR converted models from running on more recent Android versions? \r\n\r\nThank you for your help.\r\n", "comments": ["The TFLite runtime version should be the same version with or higher version than the TFLite converter Python version.\r\n\r\nFor example, if you convert a model with TF 1.15, you need to use TFLite runtime version 1.15 at least.\r\n\r\nCould you make sure that your TFLite runtime version is aligned with the TFLite converter Python version, which is used for the TFLite model generation, especially regarding MLIR converted models?", "I do not have access to the original model unfortunately, but I tried importing the model as per the instructions here: https://www.tensorflow.org/lite/guide/android and this way it works. So it must be the way I was doing it before might be deprecated?\r\n\r\nThe approach I had before was like here https://github.com/tensorflow/examples/tree/a0ec947023c8c9c4a027fa887e954021d27cc1dd.\r\n\r\nAnyways thank you, I am happy I found a workaround. ", "@mynameisteodora can you share what the difference was for the other users?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45540\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45540\">No</a>\n"]}, {"number": 45539, "title": "Bug in tensorflow.python.ops.parallel_for.gradients.batch_jacobian()", "body": "There is a bug on `line 113` of the `batch_jacobian` function:\r\nhttps://github.com/tensorflow/tensorflow/blob/5681c179eff80bce00e526303950b67b23cad14c/tensorflow/python/ops/parallel_for/gradients.py#L83-L113\r\n\r\n## Explaining the bug\r\nOn `line 113` the `output_shape[0]` is not an instance of the class `tf.TensorShape()`.\r\nIt is either an `integer` or `None` and therefore has no `is_compatible_with()` method.\r\n\r\n## Suggesting a fix\r\n```python \r\nif not output_shape[0].is_compatible_with(inp.shape[0])\r\n``` \r\ncould be replaced with \r\n```python\r\nif not output_shape[0:1].is_compatible_with(inp.shape[0])\r\n```\r\nbecause `output_shape[0:1]` is an instance of `tf.TensorShape()`\r\n\r\n## Minimal Example - showing the bug and testing the fix\r\n\r\nThe example below has been evaluated in a docker container running the image: `tensorflow/tensorflow:2.3.0-gpu-jupyter`\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops.parallel_for.gradients import batch_jacobian\r\n\r\n# Define function that computes f(x) = x^2 and its derivative df/dx = 2*x\r\n@tf.function\r\ndef square(x):    \r\n    y = x**2\r\n    dydx = batch_jacobian(y, x)\r\n    return y, dydx\r\n\r\n# Create a model that uses the function\r\nx = tf.keras.backend.placeholder(shape=(None, 2), dtype=tf.float32)\r\ny, dydx = tf.keras.layers.Lambda(lambda c: square(c))(x)\r\nmodel = tf.keras.Model(x, [y, dydx])\r\n\r\n# Test case: evaluate model with dummy input\r\nx_input = tf.constant([[1., 2.], [3., 4.], [5., 6.]])\r\ny_output, dydx_output = model(x_input)\r\nprint(y_output)\r\nprint(dydx_output)\r\n```\r\n\r\nResults in the following error:\r\n```\r\nAttributeError: in user code:\r\n\r\n    <ipython-input-1-5f56962453df>:8 square  *\r\n        dydx = batch_jacobian(y, x)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/parallel_for/gradients.py:113 batch_jacobian  **\r\n        if not output_shape[0].is_compatible_with(inp.shape[0]):\r\n\r\n    AttributeError: 'NoneType' object has no attribute 'is_compatible_with'\r\n```\r\n\r\nAfter the suggested fix it outputs:\r\n```\r\ntf.Tensor(\r\n[[ 1.  4.]\r\n [ 9. 16.]\r\n [25. 36.]], shape=(3, 2), dtype=float32)\r\ntf.Tensor(\r\n[[[ 2.  0.]\r\n  [ 0.  4.]]\r\n\r\n [[ 6.  0.]\r\n  [ 0.  8.]]\r\n\r\n [[10.  0.]\r\n  [ 0. 12.]]], shape=(3, 2, 2), dtype=float32)\r\n```\r\n\r\n## Colab demonstrating the minimal example (mentioned above)\r\nhttps://colab.research.google.com/drive/1UNCyHc6d7mrZv7p7VD3GekI3YqAntQ9q?usp=sharing\r\n\r\n## Next steps\r\nIf you want I can make a pull request. But I first want to know whether this an intended way of using the batch_jacobian function.", "comments": ["My guess is that this worked with 1.x TensorShape, but in 2.x slicing a shape returns a literal value instead of a Dimension object. So one simple fix would be to use `tensor.shape.dims[index]` instead of `tensor.shape[index]`. A PR for that sounds fine, and it'd be good to have a unit test if so.\r\n\r\nThis function isn't part of the public API. Could you switch to [`GradientTape.batch_jacobian`](https://www.tensorflow.org/api_docs/python/tf/GradientTape#batch_jacobian)? They should be equivalent, but the public one supports eager execution and will be better tested (I doubt it has this problem).", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45539\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45539\">No</a>\n", "The problem is still there, I got this error \r\n\r\nAttributeError: 'int' object has no attribute 'is_compatible_with'"]}, {"number": 45538, "title": "tf.while_loop allow to vary shapes by default", "body": "I tried everything with tf.while_loop. In nested loops I always get ValueError due to the shape. \r\nI think the problem is with **shape_invariants**.\r\nIn my opinion, it's a **bad feature that a developer should think about the initial shape of the loop variable**. I think that **to allow the shape to vary across iterations must be by deafult**. \r\nSo one more parameter should be, for exmaple, **shape_variation**, that can be **True** or **False**. If **shape_variation = False** - we need to define **shape_invariant**. If **shape_variation = True** - shape has been allowed to vary across iterations, so **shape_invariant doesn't need to be defined**.\r\n\r\nSo in my opinion, it is necessary to correct this feature in the way I suggested\r\n", "comments": ["@vasilevskykv \r\n\r\nDo you have any use case that requires the feature you are interested in? Please feel free to submit a PR if you have use cases that supports that feature.Thanks!", "The case of every loop. The developer doesn't need to be concerned about the shape. It is impossible to make a correct nested loop because of the **shape_invariant**. Because the ValueError will be always:\r\n`ValueError: Input tensor 'while/while/Const:0' enters the loop with shape (), but has shape (3,) after one iteration. To allow the shape to vary across iterations, use the `shape_invariants` argument of tf.while_loop to specify a less-specific shape.`", "@vasilevskykv \r\n\r\nLooks like duplicate of #45337. Can we close the issue here and track in #45337. Thanks!", "The problem in 45337 didn't solve", "> The developer doesn't need to be concerned about the shape.\r\n\r\nYes, they do. Providing exact shapes yields optimized computation graphs, which is often critical in writing efficient operations, layers and models. Similarly, while `shape_invariants` allows to relax part of the loop variables' shapes, it also enables optimizing what can be (by partially or fully defining the shape of some of the variables). Having a coercive behavior as default can also be seen as a way to have developers offer some thoughts to what they are doing, and therefore push them towards writing better code - which is what they are supposed to do, in my humble opinion.\r\n\r\nIf you are truly annoyed about writing specs, you may try to write loops in a pythonic way and have Autograph take on part of the work.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 45537, "title": "save/load pair do not preserve `trainable` attribute", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Debian testing\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.8.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI have a model with some layers set as `trainable=False`. However, after saving this model and loading it back, all parameters are trainable again. \r\n\r\n**Describe the expected behavior**\r\nI would expect that save/load pair would reconstruct the model precisely in its original state. \r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nm1 = Sequential([\r\n    Input((100,)),\r\n    Dense(20),\r\n    Dense(10),\r\n])\r\n\r\nm1.trainable = False\r\n\r\nm2 = Sequential([\r\n    Input((100,)),\r\n    m1,\r\n    Dense(5),\r\n    Dense(3)\r\n])\r\n\r\nm2.summary()\r\n\r\nModel: \"sequential_7\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nsequential_6 (Sequential)    (None, 10)                2230      \r\n_________________________________________________________________\r\ndense_13 (Dense)             (None, 5)                 55        \r\n_________________________________________________________________\r\ndense_14 (Dense)             (None, 3)                 18        \r\n=================================================================\r\nTotal params: 2,303\r\nTrainable params: 73\r\nNon-trainable params: 2,230\r\n_________________________________________________________________\r\n```\r\n\r\nNote that only 73 parameters are trainable (as expected).\r\nNow let's save and load back the model:\r\n\r\n```python\r\ntf.keras.models.save_model(m2, \"/tmp/mymodel\")\r\nm2_reloaded = tf.keras.models.load_model(\"/tmp/mymodel\")\r\nm2_reloaded.summary()\r\n\r\nModel: \"sequential_7\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nsequential_6 (Sequential)    (None, 10)                2230      \r\n_________________________________________________________________\r\ndense_13 (Dense)             (None, 5)                 55        \r\n_________________________________________________________________\r\ndense_14 (Dense)             (None, 3)                 18        \r\n=================================================================\r\nTotal params: 2,303\r\nTrainable params: 2,303\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["tf-nightly behaves in the same way", "@eli-osherovich,\r\nPlease take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/37531#issuecomment-600439680) from similar issue #37531 and let us know if it helps. Thanks!", "Thanks @amahendrakar, It seems that the ticket is indeed related to this problem, however, I do not see how that particular comment is relevant. If I understand correctly the comment, It suggests to compile the model after setting `trainable=False`. This is probably necessary to use the model properly. However, it has nothing to do with the fact that reloaded model has all its parameters set to trainable.\r\n", "@amahendrakar , I have to correct myself, the issue you reference above (#37531) is completely unrelated to this bug. That thread is about the need to compile models after one changes `trainable` flag. ", "Any update on this issue?", "Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/8e1acab2e9e11b0bd03e05d0cc58bb9e/45537-2-3.ipynb), [TF v2.4](https://colab.research.google.com/gist/amahendrakar/170ece581b87cd904bef0385579ff8bf/45537-2-4.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/4d70cdf1c47c618f7510ca5292300a1f/45537-tf-nightly.ipynb). Please check the linked gist for reference. Thanks!", "@eli-osherovich,\r\nThe behavior that you are expecting can be achieved by Saving the Model in **`H5 format`**, instead of **`Tensorflow Saved Model Format`**. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/7aa9d3f874072c9995c0e5086b26539c/45537-2-4.ipynb) of working code. \r\n\r\nThanks!", "Thanks, @rmothukuru , saving in HDF5 files has its own limitations, as you surely know. \r\n", "Any update on this one?\r\n", "@eli-osherovich I think this was resolved in recent `tf-nightly`. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/6a2199d444d13e52f985cd3d9b67addf/45537-tf-nightly.ipynb).\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "Fixed indeed.\r\nThanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45537\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45537\">No</a>\n"]}, {"number": 45536, "title": "RMSprop fails if using mixed precision training and eager functions", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.0rc4\r\n- Python version: any\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n`v2.4.0-rc3-20-g97c3fef64ba 2.4.0-rc4`\r\n\r\n**Describe the current behavior**\r\n\r\nRMSprop failes if using mixed precision training and eager functions\r\n\r\n**Describe the expected behavior**\r\n\r\nsucceed without errors\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nhttps://colab.research.google.com/drive/1-ZrJyKtaWmOMtga44Efc8DOu1fnRo1wN?usp=sharing\r\n\r\nOriginated from addons https://github.com/tensorflow/addons/pull/2250", "comments": ["@WindQAQ Thanks for creating this issue. I can reproduce the issue with `tf-nightly` and `2.4rc4` but works as expected with `TF2.3`. We will look into it. Thanks!\r\n\r\nThe following gists are for our reference.\r\n\r\n[Here](https://colab.research.google.com/gist/jvishnuvardhan/980c1c17909d390528b4117a976b14ed/untitled115.ipynb) is a gist with `TF2.4rc4`.\r\n\r\n[Here](https://colab.research.google.com/gist/jvishnuvardhan/fbac121c81ac4b9e04d8b13552aebdd1/untitled115.ipynb) is a gist `TF2.3`. ", "Thank you for filing this issue. The error occurs when mixed precision is enabled and `RMSprop.apply_gradients` or `Nadam.apply_gradients` is called in Eager mode (outside any `tf.function`s). This occurs in `Model.fit` if you pass `run_eagerly=False` to `Model.compile`. The error that occurs is:\r\n\r\n> Tensor.op is meaningless when eager execution is enabled\r\n\r\nUnfortunately, TF 2.4 is about to be released, so this cannot be fixed for 2.4. It will be fixed in 2.5.\r\n\r\nAs a workaround, you can paste the following snippet of code into your program right after `import tensorflow as tf`. This snippet monkey patches the fix directly into Keras to avoid the error:\r\n\r\n```python\r\nif tf.__version__.startswith('2.4.'):\r\n  from tensorflow.python.keras.mixed_precision import autocast_variable\r\n\r\n  # Monkey patch AutoCastVariable.op to not raise AttributeError\r\n  @property\r\n  def op(self):\r\n    if self._op is not None:\r\n      return self._op\r\n    return getattr(self._variable, 'op', None)\r\n\r\n  autocast_variable.AutoCastVariable.op = op\r\n```\r\n\r\nAdmittedly, this monkey patching is hacky, but the version check for TF 2.4 means it is safe and won't break when run with TF 2.5 or other versions. Alternatively, you can ensure you do not call `apply_gradients` in Eager mode when using an `RMSprop` or `Nadam` optimizer. This can be done by not passing `run_eagerly=True` when using `Model.compile` and `Model.fit`, and by calling `apply_gradients` under a `tf.function` when using a custom training loop.\r\n\r\n", "@reedwm Thanks for clarification! Addons uses it in unittests only so it doesn't matter if we change the optimizer. Just out of curiosity. Do we really need to return `.op` of the variables in `_resource_apply_*`? Seems that returning the variables directly can work as expected.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/579ce3a2e1ec3a5392968f6c7703fc598b02d5d7/tensorflow/python/keras/optimizer_v2/rmsprop.py#L220", "When TF2 behavior is disabled with `tf.compat.v1.disable_v2_behavior()`, I think removing the `.op` might make `Optimizer.minimize` return a variable instead of an op. This probably won't break anyone, but [the documentation](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer?version=nightly#minimize) does state `Optimizer.minimize` returns an op. (The documentation should also be updated to state it returns None in Eager mode).\r\n\r\nA better implementation of that `.op` line would probably be to use `var.assign` instead of `state_ops.assign`, then pass in `read_value=False` to directly get the op instead of returning the variable:\r\n\r\n```python\r\nreturn var.assign(var_t, use_locking=self._use_locking, read_value=False)\r\n```\r\n\r\nIn any case, I plan on fixing this in AutoCastVariable regardless, since there may be other cases where the `.op` attribute is accessed.", "Thanks for the help. Actually, all optimizers in addons use the way you suggested. It's nice to see that we do not need to change anything. Thanks again for the clarification!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45536\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45536\">No</a>\n", "I've got a lot if custom layers. All my tests use tensorflow.python.keras.testing_utils.layer_test which internally use RMSProp as optimizer. So now i can't test my layers for mixed precision compatibility... :("]}, {"number": 45535, "title": "Refactor exp.h from reference_ops.h; 2nd step to port op EXP for TFLi\u2026", "body": "Issue #45415 PR2", "comments": ["This PR is failing internal checks which can be reproduced with:\r\n```\r\nbazel test tensorflow/lite/micro/...  --test_tag_filters=-no_oss --build_tag_filters=-no_oss --copt=-DTF_LITE_STATIC_MEMORY\r\n```\r\n\r\nunfortunately, the command above is also failing for unrelated reasons (because we do not have that build variant as part of our CI).\r\n\r\nI'm going to fix both of these issues and then you should be able to see what error is being caused by this particular PR.\r\n\r\nPlease wait for me to prepare and merge a PR to fix the underlying issue -- I'll let you know when that happens.", "> This PR is failing internal checks which can be reproduced with:\r\n> \r\n> ```\r\n> bazel test tensorflow/lite/micro/...  --test_tag_filters=-no_oss --build_tag_filters=-no_oss --copt=-DTF_LITE_STATIC_MEMORY\r\n> ```\r\n> \r\n> unfortunately, the command above is also failing for unrelated reasons (because we do not have that build variant as part of our CI).\r\n> \r\n> I'm going to fix both of these issues and then you should be able to see what error is being caused by this particular PR.\r\n> \r\n> Please wait for me to prepare and merge a PR to fix the underlying issue -- I'll let you know when that happens.\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/45679 should do the trick.", "Thanks @advaitjain for fixing the failure. Curious to know why the failure was triggered. I assume that refactoring <op>.h is a routine step in porting any TFL op to TFLM, and many ops have been ported by doing that. I am a little surprised that I encountered this failure.  ", "I should have been more explicit in my comment. My intent with #45679 was not to fix the error being caused by the current PR (which I believe you have done with the ruy header). Rather, it was intended to make the external CI better match the internal CI.\r\n\r\nFor the current pull request, the discrepancy between internal and external fixed with #45679 is a red-herring. What appears to have happened is that when I looked at the change internally, it did not have the ruy header (hence the error). But when I looked at the change on github you had already fixed the missing include (hence it was green here).\r\n\r\nImporting of a github pull request into the internal review system is not blocked by a failing external CI, which caused the confusion for me.", "@advaitjain Thanks for the explanation. It all makes sense now.", "@rsun-bdti  Can you please resolve conflicts? Thanks!", "@gbaned I merged tensorflow/lite/kernels/internal/reference/reference_ops.h and tensorflow/lite/kernels/internal/BUILD into my branch, but am now faced with two internal CI build failures: Windows Bazel, and Windows Bazel GPU.", "The interal CI build errors are unrelated to the current PR. You too can click through and see the logs (though its likely easier for a Tensorflow engineer to make a decision that the errors are unrelated).\r\n\r\nI think that this PR is good to go. I'll let Pete take a final pass an approve.", "\"Intel\u00ae oneDNN -- Community CI Build \u2014 oneDNN unit tests failed\" \r\nExecuted 1186 out of 1186 tests: 1185 tests pass and 1 fails locally.\r\n//tensorflow/core/kernels:quantize_op_test                               FAILED in 3 out of 3 in 0.7s\r\n\r\nThe failure doesn't seem to be related to my PR."]}, {"number": 45534, "title": "Dispatch reverse_sequence", "body": "The code snippet\r\n```python\r\ntf.reverse_sequence(x, seq_len, seq_axis=-1)\r\n```\r\nwill fail when inputs are keras symbolic tensor. Originated from addons when trying to test against TF2.4.\r\n\r\nhttps://github.com/tensorflow/addons/pull/2250\r\n\r\nhttps://colab.research.google.com/drive/1lGXNR5nm6FS-F_gzx-GretLJCrljU96t?usp=sharing", "comments": ["Do you plan to release a patch to version `2.4` containing this fix?", "It's unlikely. We're trying to minimize amount of code that goes into a patch since the Q&A for patch releases is different than Q&A for a normal release."]}]