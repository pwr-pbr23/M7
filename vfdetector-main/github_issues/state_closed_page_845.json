[{"number": 28169, "title": "Link to Images guide broken", "body": "**System information**\r\n- TensorFlow version: N/A\r\n- Doc Link: https://www.tensorflow.org/api_docs/python/tf/image\r\n\r\n\r\n**Describe the documentation issue**\r\n\r\nOn the page\r\nhttps://www.tensorflow.org/api_docs/python/tf/image\r\n\r\nThe link within the text \"See the Images guide.\" is broken.\r\n(It is currently directing to https://www.tensorflow.org/api_guides/python/image, but 404'd)\r\n\r\n\r\n_edit: Removed contents from template unrelated to the issue._", "comments": ["@SoloSynth1 I had accessed the page provided, it says 404 error. We will take a look. Thanks for bringing this to our notice.", "First two links are not working as shown in the [screenshot](https://screenshot.googleplex.com/Tv3SBDNQrmU). \r\nFirst link that is not working (bold_italic): Defined in tensorflow/_api/v1/image/**___init__.py_**\r\nSecond link that is not working (bold_italic): See the _**Images**_ guide . Thanks!", "I can add an updated API guide similar to [image.md](https://github.com/tensorflow/docs/blob/r1.11/site/en/api_guides/python/image.md)\r\n\r\n@muddham @jvishnuvardhan please share your views. Thank you", "Hi @vijayphoenix \r\n\r\nWe don't have an api-gides section anymore, but if you want to add that information back to the [docstring of the `tf.image` module](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/image_ops.py), that could be good.\r\n\r\nIf you take this on, remember that the generated page includes a list of all the symbols in the module.\r\n\r\nalso remember that if you include example code, please use [doctest format](https://docs.python.org/3/library/doctest.html), so the example gets tested.\r\n", "Hi @MarkDaoust,\r\nI will do the necessary changes and make a PR.\r\n\r\nThank you for your input.", "Hi @MarkDaoust,\r\nI made a PR https://github.com/tensorflow/tensorflow/pull/37171 addressing this issue. Please take a look.\r\nThank you", "LGTM, Thanks.\r\n"]}, {"number": 28168, "title": "Text classification doc Google Colab notebook issue", "body": "**System information**\r\n- TensorFlow version: 1.13.1\r\n- Doc Link: https://www.tensorflow.org/tutorials/keras/basic_text_classification\r\n\r\n\r\n**Describe the documentation issue**\r\nWhen running the Google Colab notebook from the documentation itself, I get an error saying \"Object arrays cannot be loaded when allow_pickle=False\". More details can be found in the attached image. Note that I ran the code directly from the Google colab notebook attached in the documentation.\r\n\r\nThank you!\r\n\r\n![bug](https://user-images.githubusercontent.com/20063012/56782705-deeca580-681a-11e9-8ca2-6f51eef0c733.png)\r\n\r\n", "comments": ["@viktordelacruz Please have look on this [#28102](https://github.com/tensorflow/tensorflow/issues/28102). Let us know how it progresses. Thanks!", " Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28168\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28168\">No</a>\n"]}, {"number": 28167, "title": "how", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": []}, {"number": 28166, "title": "fix a wrong example in slim README", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28166) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28166) for more info**.\n\n<!-- need_author_cla -->", "test CLA", "test CLA", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28166) for more info**.\n\n<!-- ok -->"]}, {"number": 28165, "title": "Issue defining tf.function input_signature on class methods", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version: tf-nightly-2.0-preview==2.0.0.dev20190426\r\n- Python version: 3.7.0\r\n\r\nI have defined the following Encoder class with a tf.function signature hoping to be able to save it using the SavedModel format.\r\n\r\n```python\r\nclass Encoder(tf.keras.Model):\r\n    # init and other functions...\r\n\r\n    @tf.function(input_signature=[\r\n        tf.TensorSpec([None, 50], tf.float32, name='x'),\r\n        tf.TensorSpec([None, 1024], tf.float32, name='hidden')\r\n    ])\r\n    def call(self, x, hidden):\r\n        x = self.embedding(x)\r\n        output, state = self.gru(x, initial_state = hidden)        \r\n        return output, state\r\n\r\n\r\n# Some more code...\r\n\r\ntf.saved_model.save(\r\n    encoder,  # instance of Encoder\r\n    './some/directory/',\r\n    signatures=Encoder.call\r\n)\r\n```\r\n\r\nI am not sure if this is the desired behavior, but I am getting this error when trying to save:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 244, in <module>\r\n    signatures=Encoder.call\r\n  File \"/home/marcio/Workspace/projects/traduki/traduki-pipel\r\nine/venv/lib/python3.7/site-packages/tensorflow/python/saved_\r\nmodel/save.py\", line 801, in save\r\n    signatures = signature_serialization.canonicalize_signatu\r\nres(signatures)\r\n  File \"/home/marcio/Workspace/projects/traduki/traduki-pipel\r\nine/venv/lib/python3.7/site-packages/tensorflow/python/saved_\r\nmodel/signature_serialization.py\", line 95, in canonicalize_s\r\nignatures\r\n    signature_function = _get_signature(function)\r\n  File \"/home/marcio/Workspace/projects/traduki/traduki-pipel\r\nine/venv/lib/python3.7/site-packages/tensorflow/python/saved_\r\nmodel/signature_serialization.py\", line 41, in _get_signature\r\n    function = function.get_concrete_function()\r\n  File \"/home/marcio/Workspace/projects/traduki/traduki-pipeline/venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 702, in get_concrete_function\r\n    self._initialize(args, kwargs, add_initializers_to=initializer_map)\r\n  File \"/home/marcio/Workspace/projects/traduki/traduki-pipeline/venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 379, in _initialize\r\n    *args, **kwds))\r\n  File \"/home/marcio/Workspace/projects/traduki/traduki-pipeline/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1331, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/marcio/Workspace/projects/traduki/traduki-pipeline/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1594, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/marcio/Workspace/projects/traduki/traduki-pipeline/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1527, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/marcio/Workspace/projects/traduki/traduki-pipeline/venv/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 713, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/marcio/Workspace/projects/traduki/traduki-pipeline/venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 329, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/marcio/Workspace/projects/traduki/traduki-pipeline/venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 436, in __call__\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \"/home/marcio/Workspace/projects/traduki/traduki-pipeline/venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 379, in _initialize\r\n    *args, **kwds))\r\n  File \"/home/marcio/Workspace/projects/traduki/traduki-pipeline/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1331, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/marcio/Workspace/projects/traduki/traduki-pipeline/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1594, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/marcio/Workspace/projects/traduki/traduki-pipeline/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1527, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/marcio/Workspace/projects/traduki/traduki-pipeline/venv/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 713, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/marcio/Workspace/projects/traduki/traduki-pipeline/venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 329, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/marcio/Workspace/projects/traduki/traduki-pipeline/venv/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 705, in wrapper\r\n    ), args, kwargs)\r\n  File \"/home/marcio/Workspace/projects/traduki/traduki-pipeline/venv/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 360, in converted_call\r\n    result = converted_f(*effective_args, **kwargs)\r\nTypeError: tf__call() missing 1 required positional argument: 'hidden'\r\n```\r\n\r\nWhen I remove the two TensorSpecs from the signature, I instead get a `TypeError: tf__call() missing 3 required positional arguments: 'self', 'x', and 'hidden'`, which means it's expecting me to provide a TensorSpec for `self`.\r\n\r\nBased on [this guide](https://www.tensorflow.org/alpha/guide/saved_model#identifying_a_signature_to_export) I wouldn't expect to have to provide signature information for `self` in the `call` function, which leads me to believe the current behavior should be changed.", "comments": ["This works if you pass `encoder.call` instead of `Encoder.call`. Is there something you could do passing the unbound method that you can't by passing the bound method?", "@allenlavoie Good catch! I guess I don't need the unbound method after all. Thanks!"]}, {"number": 28164, "title": "F tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/cc/saved_model/load_attempt_count", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n   EulerOS 4.8.5-4\r\n- TensorFlow installed from (source or binary):\r\n   source\r\n- TensorFlow version:\r\n  1.10.0\r\n- Bazel version (if compiling from source):\r\n  0.15.0\r\n- GCC/Compiler version (if compiling from source):\r\n  4.9.4\r\n\r\n**Describe the problem**\r\nDear TensorFlow contributors,\r\nI am using java to execute tensorflow model in tomcat in Linux with libtensorflow_jni.so and libtensorflow.jar , I have put libtensorflow_framework.so in LD_LIBRARY_PATH,  when the program started, the error happened:\r\n\r\nF tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/cc/saved_model/load_attempt_count\r\n\r\n\r\n**Any other info / logs**\r\nI guess maybe there are sth repeated in libtensorflow_jni.so and libtensorflow_framework.so and cause 'register 2 metrics'?\r\n\r\nI read https://github.com/tensorflow/tensorflow/issues/13522\uff0cbut I donnot know how to remove tensorflow-c dependency\uff1f\r\n\r\nneed your help\uff0cthanks\r\n\r\n", "comments": ["or maybe I need to remove tensorflow_cc dependency?", "I resolved it by building with \"--config=monolithic\", so it seems work OK now.", "Closing this out since I understand it to be resolved, but please let me know if I'm mistaken.", "yes, it has been resolved@jvishnuvardhan ", "> I resolved it by building with \"--config=monolithic\", so it seems work OK now.\r\n\r\nhello,  what did you rebuilding ?", "> I resolved it by building with \"--config=monolithic\", so it seems work OK now.\r\n\r\nWhat all flags have you enabled for bazel build  apart from opt and monolithic\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\nI'm also facing the same issue..Can you please tell that sir"]}, {"number": 28163, "title": "TFLite Interpreter fails to load quantized model on Android (stock ssd_mobilenet_v2)", "body": "**System information**\r\nAndroid 5.1.1 on LGL52VL, also tested on Android 9 Simulator (Nexus 5)\r\nLatest Tensorflow in build.gradle:\r\ncompile 'org.tensorflow:tensorflow-lite:+'\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\nhttp://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz\r\n\r\n**Any other info / logs**\r\n`\r\n// I convert the stock quantized mobilenet_v2 to TFLite using the following code:\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(graphDefPath, input_arrays, output_arrays, input_shapes)\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\nconverter.allow_custom_ops=True\r\nconverter.convert()    \r\n\r\n//The resulting .tflite file works fine in Python (including inference):\r\ninterpreter = tf.lite.Interpreter(model_path=modelFilePath)\r\ninterpreter.allocate_tensors()\r\n`\r\n`\r\n// When I try to load the same tflite model file on Android the Interpreter constructor gives me error \"Didn't find op for builtin opcode 'CONV_2D' version '2'\":\r\n\r\nMappedByteBuffer buffer = fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);\r\nInterpreter.Options options = new Interpreter.Options();\r\ntfLite = new Interpreter(buffer, options);\r\n\r\nE/flutter ( 7796): [ERROR:flutter/lib/ui/ui_dart_state.cc(148)] Unhandled Exception: PlatformException(error, Unsupported value: java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find op for builtin opcode 'CONV_2D' version '2'\r\nE/flutter ( 7796): Registration failed.\r\nE/flutter ( 7796): , null)\r\nE/flutter ( 7796): #0      StandardMethodCodec.decodeEnvelope (package:flutter/src/services/message_codecs.dart:564:7)\r\nE/flutter ( 7796): #1      MethodChannel.invokeMethod (package:flutter/src/services/platform_channel.dart:302:33)\r\nE/flutter ( 7796): <asynchronous suspension>\r\nE/flutter ( 7796): #2      Tflutter.loadModelAndLabels (package:tflutter/tflutter.dart:129:20)\r\nE/flutter ( 7796): <asynchronous suspension>\r\nE/flutter ( 7796): #3      Detector.initialize (package:AIM/services/detector.dart:51:26)\r\nE/flutter ( 7796): <asynchronous suspension>\r\nE/flutter ( 7796): #4      _MonitorState._initStateAsync (package:AIM/screens/Monitor.dart:47:20)\r\nE/flutter ( 7796): #5      _AsyncAwaitCompleter.start (dart:async-patch/async_patch.dart:49:6)\r\nE/flutter ( 7796): #6      _MonitorState._initStateAsync (package:AIM/screens/Monitor.dart:45:25)\r\nE/flutter ( 7796): #7      _MonitorState.initState (package:AIM/screens/Monitor.dart:42:5)\r\nE/flutter ( 7796): #8      StatefulElement._firstBuild (package:flutter/src/widgets/framework.dart:3853:58)\r\nE/flutter ( 7796): #9      ComponentElement.mount (package:flutter/src/widgets/framework.dart:3724:5)\r\nE/flutter ( 7796): #10     Element.inflateWidget (package:flutter/src/widgets/framework.dart:2968:14)\r\nE/flutter ( 7796): #11     Element.updateChild (package:flutter/src/widgets/framework.dart:2771:12)\r\nE/flutter ( 7796): #12     SingleChildRenderObjectElement.mount (package:flutter/src/widgets/framework.dart:4883:14)\r\nE/flutter ( 7796): #13     Element.inflateWidget (package:flutter/src/widgets/framework.dart:2968:14)\r\nE/flutter ( 7796): #14     Element.updateChild (package:flutter/src/widgets/framework.dart:2771:12)\r\nE/flutter ( 7796): #15     ComponentElement.performRebuild (package:flutter/src/widgets/framework.dart:3757:16)\r\nE/flutter ( 7796): #16     Element.rebuild (package:flutter/src/widgets/framework.dart:3572:5)\r\nE/flutter ( 7796): #17     ComponentElement._firstBuild (package:flutter/src/widgets/framework.dart:3729:5)\r\nE/flutter ( 7796): #18     ComponentElement.mount (package:flutter/src/widgets/framework.dart:3724:5)\r\nE/flutter ( 7796): #19     Element.inflateWidget (package:flutter/src/widgets/framework.dart:2968:14)\r\nE/flutter ( 7796): #20     Element.updateChild (package:flutter/src/widgets/framework.dart:2771:12)\r\nE/flutter ( 7796): #21     SingleChildRenderObjectElement.mount (package:flutter/src/widgets/framework.dart:4883:14)\r\nE/flutter ( 7796): #22     Element.inflateWidget (package:flutter/src/widgets/framework.dart:2968:14)\r\nE/flutter ( 7796): #23     Element.updateChild (package:flutter/src/widgets/framework.dart:2771:12)\r\nE/flutter ( 7796): #24     SingleChildRenderObjectElement.mount (package:flutter/src/widgets/framework.dart:4883:14)\r\nE/flutter ( 7796): #25     Element.inflateWidget (package:flutter/src/widgets/framework.dart:2968:14)\r\nE/flutter ( 7796): #26     Element.updateChild (package:flutter/src/widgets/framework.dart:2771:12)\r\nE/flutter ( 7796): #27     SingleChildRenderObjectElement.mount (package:flutter/src/widgets/framework.dart:4883:14)\r\nE/flutter ( 7796): #28     Element.inflateWidget (package:flutter/src/widgets/framework.dart:2968:14)\r\nE/flutter ( 7796): #29     Element.updateChild (package:flutter/src/widgets/framework.dart:2771:12)\r\nE/flutter ( 7796): #30     SingleChildRenderObjectElement.mount (package:flutter/src/widgets/framework.dart:4883:14)\r\nE/flutter ( 7796): #31     Element.inflateWidget (package:flutter/src/widgets/framework.dart:2968:14)\r\nE/flutter ( 7796): #32     Element.updateChild (package:flutter/src/widgets/framework.dart:2771:12)\r\nE/flutter ( 7796): #33     ComponentElement.performRebuild (package:flutter/src/widgets/framework.dart:3757:16)\r\nE/flutter ( 7796): #34     Element.rebuild (package:flutter/src/widgets/framework.dart:3572:5)\r\nE/flutter ( 7796): #35     ComponentElement._firstBuild (package:flutter/src/widgets/framework.dart:3729:5)\r\nE/flutter ( 7796): #36     StatefulElement._firstBuild (package:flutter/src/widgets/framework.dart:3871:11)\r\nE/flutter ( 7796): #37     ComponentElement.mount (package:flutter/src/widgets/framework.dart:3724:5)\r\nE/flutter ( 7796): #38     Element.inflateWidget (package:flutter/src/widgets/framework.dart:2968:14)\r\nE/flutter ( 7796): #39     Element.updateChild (package:flutter/src/widgets/framework.dart:2771:12)\r\nE/flutter ( 7796): #40     ComponentElement.performRebuild (package:flutter/src/widgets/framework.dart:3757:16)\r\nE/flutter ( 7796): #41     Element.rebuild (package:flutter/src/widgets/framework.dart:3572:5)\r\nE/flutter ( 7796): #42     ComponentElement._firstBuild (package:flutter/src/widgets/framework.dart:3729:5)\r\nE/flutter ( 7796): #43     ComponentElement.mount (package:flutter/src/widgets/fra\r\n`\r\nPS. The same Android code works fine for non-quantized tflite models that I converted, so I am confident that there should not be any silly bugs in my code. The problem seems to be triggered by Quantized + Android TFlite", "comments": ["i am facing the same error !", "yes, i'm also having problems with this the exact same problem. i'm using the next lines for convertion:\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\ntflite_model = converter.convert()\r\n```\r\n\r\nAlso.. \u00bfWhat is the correct way to preprocess an image in order to make a non-quantized model get the correct prediction in android?", "Use the`pip install -U tf-nightly` and it should solve the Op problem. Even while converting.\r\n\r\nThe tf nightly has the most latest build of tensorflow. And the most recent is two days ago from today.", "I have installed tf-nightly and converted model but it still would not load:\r\n```\r\nE/CustomCompatChecker(19664): The model is INCOMPATIBLE. It may contain unrecognized custom ops, or not FlatBuffer format: java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find op for builtin opcode 'CONV_2D' version '2'\r\nE/CustomCompatChecker(19664): Registration failed.\r\nW/System.err(19664): com.google.firebase.ml.common.FirebaseMLException: Remote model load failed with the model options: Local model name: unspecified. Remote model name: detector.\r\nW/System.err(19664): \tat com.google.android.gms.internal.firebase_ml.zzon.zza(Unknown Source:33)\r\nW/System.err(19664): \tat com.google.android.gms.internal.firebase_ml.zzpr.zza(Unknown Source:109)\r\nW/System.err(19664): \tat com.google.android.gms.internal.firebase_ml.zzpr.zzln(Unknown Source:105)\r\nW/System.err(19664): \tat com.google.android.gms.internal.firebase_ml.zzoa.zzf(Unknown Source:56)\r\nW/System.err(19664): \tat com.google.android.gms.internal.firebase_ml.zznt.call(Unknown Source:3)\r\nW/System.err(19664): \tat com.google.android.gms.internal.firebase_ml.zznn.zza(Unknown Source:30)\r\nW/System.err(19664): \tat com.google.android.gms.internal.firebase_ml.zznq.run(Unknown Source:2)\r\nW/System.err(19664): \tat android.os.Handler.handleCallback(Handler.java:873)\r\nW/System.err(19664): \tat android.os.Handler.dispatchMessage(Handler.java:99)\r\nW/System.err(19664): \tat com.google.android.gms.internal.firebase_ml.zzf.dispatchMessage(Unknown Source:6)\r\nW/System.err(19664): \tat android.os.Looper.loop(Looper.java:214)\r\nW/System.err(19664): \tat android.os.HandlerThread.run(HandlerThread.java:65)\r\nW/System.err(19664): Caused by: com.google.firebase.ml.common.FirebaseMLException: Model is not compatible with TFLite run time\r\nW/System.err(19664): \tat com.google.android.gms.internal.firebase_ml.zzpd.zza(Unknown Source:61)\r\nW/System.err(19664): \tat com.google.android.gms.internal.firebase_ml.zzpk.zzah(Unknown Source:52)\r\nW/System.err(19664): \tat com.google.android.gms.internal.firebase_ml.zzpk.load(Unknown Source:18)\r\nW/System.err(19664): \tat com.google.android.gms.internal.firebase_ml.zzon.zza(Unknown Source:41)\r\nW/System.err(19664): \tat com.google.android.gms.internal.firebase_ml.zzon.zza(Unknown Source:14)\r\nW/System.err(19664): \t... 11 more\r\nE/FirebaseModelInterpreter(19664): Remote model load failed with the model options: Local model name: unspecified. Remote model name: detector.\r\nW/System.err(19664): com.google.android.gms.tasks.RuntimeExecutionException: com.google.firebase.ml.common.FirebaseMLException: Remote model load failed with the model options: Local model name: unspecified. Remote model name: detector.\r\nW/System.err(19664): \tat com.google.android.gms.tasks.zzu.getResult(Unknown Source:15)\r\nW/System.err(19664): \tat com.azihsoyn.flutter.mlkit.MlkitPlugin$10.then(MlkitPlugin.java:358)\r\nW/System.err(19664): \tat com.azihsoyn.flutter.mlkit.MlkitPlugin$10.then(MlkitPlugin.java:350)\r\nW/System.err(19664): \tat com.google.android.gms.tasks.zzd.run(Unknown Source:5)\r\nW/System.err(19664): \tat android.os.Handler.handleCallback(Handler.java:873)\r\nW/System.err(19664): \tat android.os.Handler.dispatchMessage(Handler.java:99)\r\nW/System.err(19664): \tat android.os.Looper.loop(Looper.java:214)\r\nW/System.err(19664): \tat android.app.ActivityThread.main(ActivityThread.java:7032)\r\nW/System.err(19664): \tat java.lang.reflect.Method.invoke(Native Method)\r\nW/System.err(19664): \tat com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:494)\r\nW/System.err(19664): \tat com.android.internal.os.ZygoteInit.main(ZygoteInit.java:965)\r\nW/System.err(19664): Caused by: com.google.firebase.ml.common.FirebaseMLException: Remote model load failed with the model options: Local model name: unspecified. Remote model name: detector.\r\n```", "I am trying to load a custom trained quantized model and the android sample code of TensorFlow gives me the same error as above. But if I use the float model instead, it works perfectly. Does the android dependency `implementation 'org.tensorflow:tensorflow-lite:1.13.1'` not yet supports the CONV_2D conversion parameter?", "You'll need `org.tensorflow:tensorflow-lite:1.14.0`, which was just released this past week, or try the nightly.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28163\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28163\">No</a>\n", "I no longer face the above error but now if I use my custom quantized model and I get this error\r\n`java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 602112 bytes and a ByteBuffer with 150528 bytes.`\r\n\r\nIf I use a custom float model then there are no errors.\r\n\r\nMy code is as follows:\r\n```\r\n@Override\r\n public List<Recognition> recognizeImage(Bitmap bitmap) {\r\n        ByteBuffer byteBuffer = convertBitmapToByteBuffer(bitmap);\r\n        if(quant){\r\n            byte[][] result = new byte[1][labelList.size()]; //labelList.size() = 10;\r\n            interpreter.run(byteBuffer, result);\r\n            return getSortedResultByte(result);\r\n        } else {\r\n            float [][] result = new float[1][labelList.size()];\r\n            interpreter.run(byteBuffer, result);\r\n            return getSortedResultFloat(result);\r\n        }\r\n}\r\n\r\n  private ByteBuffer convertBitmapToByteBuffer(Bitmap bitmap) {\r\n        ByteBuffer byteBuffer;\r\n\r\n        if(quant) {\r\n            byteBuffer = ByteBuffer.allocateDirect(1 * 224 * 224 * 3);\r\n        } else {\r\n            byteBuffer = ByteBuffer.allocateDirect(4 * 1 * 224 * 224 * 3);\r\n        }\r\n\r\n        byteBuffer.order(ByteOrder.nativeOrder());\r\n        int[] intValues = new int[224 * 224];\r\n        bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());\r\n        int pixel = 0;\r\n        for (int i = 0; i < 224; ++i) {\r\n            for (int j = 0; j < 224; ++j) {\r\n                final int val = intValues[pixel++];\r\n                if(quant){\r\n                    byteBuffer.put((byte) ((val >> 16) & 0xFF));\r\n                    byteBuffer.put((byte) ((val >> 8) & 0xFF));\r\n                    byteBuffer.put((byte) (val & 0xFF));\r\n                } else {\r\n                    byteBuffer.putFloat((((val >> 16) & 0xFF))/255.0f);\r\n                    byteBuffer.putFloat((((val >> 8) & 0xFF))/255.0f);\r\n                    byteBuffer.putFloat((((val) & 0xFF))/255.0f);\r\n                }\r\n\r\n            }\r\n        }\r\n        return byteBuffer;\r\n    }\r\n\r\n @SuppressLint(\"DefaultLocale\")\r\n    private List<Recognition> getSortedResultByte(byte[][] labelProbArray) {\r\n\r\n        PriorityQueue<Recognition> pq =\r\n                new PriorityQueue<>(\r\n                        MAX_RESULTS,\r\n                        new Comparator<Recognition>() {\r\n                            @Override\r\n                            public int compare(Recognition lhs, Recognition rhs) {\r\n                                return Float.compare(rhs.getConfidence(), lhs.getConfidence());\r\n                            }\r\n                        });\r\n\r\n        for (int i = 0; i < labelList.size(); ++i) {\r\n            float confidence = (labelProbArray[0][i] & 0xff) / 255.0f;\r\n            if (confidence > THRESHOLD) {\r\n                pq.add(new Recognition(\"\" + i,\r\n                        labelList.size() > i ? labelList.size() : \"unknown\",\r\n                        confidence, quant));\r\n            }\r\n        }\r\n\r\n        final ArrayList<Recognition> recognitions = new ArrayList<>();\r\n        int recognitionsSize = Math.min(pq.size(), MAX_RESULTS);\r\n        for (int i = 0; i < recognitionsSize; ++i) {\r\n            recognitions.add(pq.poll());\r\n        }\r\n\r\n        return recognitions;\r\n    }\r\n\r\n```", "How did you quantize your model? Note that, with post-training quantization, the default approach is to preserve float inputs/outputs, implicitly quantizing/dequantizing as necessary. You can check the input tensor type using [`interpreter.getInputTensor(0).dataType()`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/src/main/java/org/tensorflow/lite/Tensor.java#L49), or by using the [visualization script](https://www.tensorflow.org/lite/guide/faq#how_do_i_inspect_a_tflite_file).", "My Model takes FLOAT32 as input, but when I use the float code mentioned above the code works fine but the time taken increases dramatically (2.3 seconds per image) as compared to my float model (0.8 seconds per image). Float Model file is which is of larger size ~80MB as compared to post-training Quantized Model size ~20MB.", "Just to confirm, you're saying that the quantized model (which is 4x smaller), takes 3x longer during inference? One thing to try is to explicitly\u00a0set the number of threads when executing your model (e.g., compare\u00a0single-threaded performance). We're still working on multi-threading support for the \"hybrid\" quantization path for post-training quantization.\u00a0\r\nWith the new [post-training integer quantization path](https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9ba), if you can provide a representative dataset at conversion\u00a0time, you should get both the size reduction and a substantial inference latency reduction (this includes multi-threading support).", "Yes, you understood it correctly. Currently, I am running the classifier class from a DB class as soon as media path gets inserted into my DB, I pass the media path to classifier methods which then runs respective code to get the confidence probabilities. So this whole thing is running in a background thread synchronously. One thing which I don't get is why the smaller model takes more time than the larger one. The quantized model is supposed to be faster. Is there any other specific way to train the model for quantization so that I can use the byte code instead of the float as used in the sample application?", "> One thing which I don't get is why the smaller model takes more time than the larger one. \r\n\r\nIn this case, it's because you're using a \"hybrid\" quantized model, which doesn't yet have multi-threading support. I would try using the new [post-training, integer quantization](https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9ba) path, which should get you both faster execution and a smaller model.\r\n\r\nIf you don't mind, could you attach the command you used to convert your model? And the generated .tflite model? Feel free to PM me directly if you prefer not to share openly. Thanks."]}, {"number": 28162, "title": "Merge pull request #28021 from avasid:master", "body": "PiperOrigin-RevId: 245066433\r\n\r\nNote: This cherrypick is necessary to compile with certain versions of CUDA, like CUDA 9.0", "comments": []}, {"number": 28161, "title": "MobileNet v2 model cannot be converted to TFLite (missing OP)", "body": "It is not possible to convert retrained model built on top of the feature vector [MobileNet v2](https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2) to TensorFlow Lite with **tf.lite.TFLiteConverter.from_concrete_function()** method.\r\n\r\nDuring the conversion, there are errors telling about not supported operations and data types (see below). \r\n\r\n**System information**\r\nTensorFlow Version:  2.0.0-alpha0\r\nEager mode:  True\r\nTensorFlow Hub version:  0.4.0\r\nIs GPU available:  True\r\n\r\n**Code to reproduce the issue**\r\nColab project with output: https://colab.research.google.com/drive/1fDWZfce2BJnU49xyrvwXYpX3EW3UK6QH#scrollTo=jyT0Zw6l8LE5\r\n\r\nStacktrace:\r\n```\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-48-cca93394edbc> in <module>()\r\n     11 # Convert the model.\r\n     12 converter = tf.lite.TFLiteConverter.from_concrete_function(concrete_func)\r\n---> 13 converted_tflite_model = converter.convert()\r\n     14 open(TFLITE_MODEL, \"wb\").write(converted_tflite_model)\r\n\r\n2 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str)\r\n    203       stderr = _try_convert_to_unicode(stderr)\r\n    204       raise ConverterError(\r\n--> 205           \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    206   finally:\r\n    207     # Must manually cleanup files.\r\n```\r\n\r\n```\r\nConverterError: TOCO failed. See console for info.\r\n2019-04-25 19:20:27.592164: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-04-25 19:20:27.592225: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n...\r\n2019-04-25 19:20:27.594555: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-04-25 19:20:27.594629: I tensorflow/lite/toco/import_tensorflow.cc:1335] Converting unsupported operation: StatefulPartitionedCall\r\n2019-04-25 19:20:27.604371: F tensorflow/lite/toco/import_tensorflow.cc:114] Check failed: attr.value_case() == AttrValue::kType (1 vs. 6)\r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x00007f9bd09c4780 (most recent call first):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33 in execute\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 251 in _run_main\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 300 in run\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 40 in run\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59 in main\r\n  File \"/usr/local/bin/toco_from_protos\", line 10 in <module>\r\nAborted (core dumped)\r\n```", "comments": ["Tried also with the model tf.keras.applications.MobileNetV2 from Transfer Learning Colab/Tutorial: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/images/transfer_learning.ipynb#scrollTo=19IQ2gqneqmS \r\n\r\nIt results with:\r\n```\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-37-763a1ccbfd3c> in <module>()\r\n     11 # Convert the model.\r\n     12 converter = tf.lite.TFLiteConverter.from_concrete_function(concrete_func)\r\n---> 13 converted_tflite_model = converter.convert()\r\n     14 open(TFLITE_MODEL, \"wb\").write(converted_tflite_model)\r\n\r\n1 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/convert_to_constants.py in convert_variables_to_constants_v2(func)\r\n    171       resource_placeholders[input_name] = {\r\n    172           \"dtype\": node.attr[\"dtype\"],\r\n--> 173           \"data\": tensor_data[input_name],\r\n    174       }\r\n    175 \r\n\r\nKeyError: 'sequential/mobilenetv2_1.00_160/bn_Conv1/FusedBatchNorm/ReadVariableOp/resource'\r\n```", "This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "Thanks for the response. I asked this question here assuming that if it is not possible to simply convert MobileNet to tflite (which which is designed for it I believe) this might be a bug. Esp when it works with no issues on TF1.x.\r\nOr maybe it should be reported to TF/Hub repo? \r\n\r\nIf I'm wrong here, I'm happy to move it to Stack Overflow then.", "There were some bugs in the alpha that have been resolved since. Can you try either `from_saved_model`, `from_keras_model` or `from_concrete_functions` in the 2.0 nightly (`tf-nightly-2.0-preview`)? The API is documented here: https://www.tensorflow.org/lite/r2/convert/python_api.", "I've just checked it with suggested changes, and now saving model works correctly.\r\n\r\nCode I needed to use:\r\n```\r\n!pip install tf-nightly-gpu-2.0-preview\r\n```\r\n\r\nInfo:\r\n```\r\nTensorFlow Version:  2.0.0-dev20190512\r\nEager mode:  True\r\nTensorFlow Hub version:  0.4.0\r\nIs GPU available:  True\r\n```\r\n\r\nUpdated module handle from version 2 to 3 (2 doesn't work):\r\n\r\n```\r\nMODULE_HANDLE =\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/3\"\r\n```\r\n\r\nAnd at the end:\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\n```\r\nInstead of `from_concrete_function`\r\n\r\nUpdated code: https://colab.research.google.com/drive/1fDWZfce2BJnU49xyrvwXYpX3EW3UK6QH#scrollTo=o5JPT0yOkiK6", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28161\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28161\">No</a>\n"]}, {"number": 28160, "title": "Fix multiline magic", "body": "Unfortunately, the feature for multiline introduced by another contributor in https://github.com/tensorflow/tensorflow/commit/93557712d0082a7e9fda3daacefddefb15e5d722#diff-db01673eb6e019b39689cd41bfae3d5e breaks notebook cell alignment. \r\n\r\nThe problem is that we skip the cell, during `_get_code `, but do not skip the same cells during `_update_notebook `. As a result, the diffs, that have such multiline magic, are aligned\r\n\r\nThis PR should fix the problem.", "comments": ["I have a couple of questions: \r\n- what is the good way to bring a small examples of notebooks to cover such cases with the tests? \r\n- what is a good way to run these tests locally? \r\n\r\nI tried specifying one test for bazel, but seems like bazel anyway tries to rebuild the universe:(", "cc @martinwicke ", "Something to be aware of: with implemented approach to multiline-magic, the python tensorflow code, that is used in `%%timeit` would not be considered as python code, so might have TF1.x code", "This is better than not handling multi-line properly. \r\n\r\nIs there a list of %% directives we should exclude? If timeit is the only real issue, we could special-case it."]}, {"number": 28159, "title": "Disable tests in tensorflow/examples/speech_commands for pip builds as they have a transitive dependency on kissfft (which is broken for Windows). ", "body": "", "comments": ["This closes #28129"]}, {"number": 28158, "title": "Keras ValueError stops autograph building", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- TensorFlow installed from (source or binary):\r\npip\r\n- TensorFlow version (use command below):\r\n2.0.0-dev20190424\r\n- Python version:\r\n3.7.1\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n  cudatoolkit-10.0.130-0                                                                                                  \r\n  cudnn-7.3.1-cuda10.0_0\r\n- GPU model and memory:\r\nGeForce RTX 2080 Ti\r\n\r\n**Describe the current behavior**\r\nCalling keras layer without calling `build()` automatically infers the shapes of the trainable variables. This works both in eager mode and graph mode in the current `2.0-alpha` version. However, running the provided code in `2.0.0-dev20190424` version, it gives the following error message:\r\n\r\n```\r\nW0425 12:08:40.775576 139922429134656 tf_logging.py:161] Entity <function update at 0x7f41dd7038c8> could not be transform\r\ned and will be executed as-is. Some features (e.g. tensor-dependent conditionals and loops) may not work as expected. Erro\r\nr details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the\r\nAutoGraph team. Cause: ValueError during conversion: Weights for model sequential have not yet been created. Weights are c\r\nreated when the Model is first called on inputs or `build()` is called with an `input_shape`.\r\n```\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport os\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers, models, optimizers\r\n\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n\r\nmodel = models.Sequential([layers.Dense(1, activation='relu')])\r\noptimizer = optimizers.SGD()\r\n\r\n# Is this line needed in graph mode?\r\n# model.build((None, 1))\r\n\r\n\r\n@tf.function\r\ndef update(batch):\r\n    with tf.GradientTape() as tape:\r\n        output = model(batch)\r\n    grads = tape.gradient(output, model.trainable_variables)\r\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    batch = tf.zeros((1, 1), dtype=tf.float32)\r\n    update(batch)\r\n\r\n```", "comments": ["I could reproduce the issue with tf-nightly. However, there is no error with TF2.0.0-alpha0. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28158\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28158\">No</a>\n", "Just submitted a fix that should allow your code to work without calling model.build."]}, {"number": 28157, "title": "Update 20-documentation-issue.md", "body": "", "comments": []}, {"number": 28156, "title": "Update 50-other-issues.md", "body": "", "comments": ["closing this as this for testing purpose for tensorflow bot"]}, {"number": 28155, "title": "cherry-pick 02badbc 'Add kissfft license back ...'", "body": "", "comments": ["Don't think we need this anymore: https://github.com/tensorflow/tensorflow/commit/0b19b8237a8bae784b33366c6912a2df5048a3a8"]}, {"number": 28154, "title": "[TF2.0] Bug or Feature? Keras model.summary() does not provide useful information for nested models", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.0.0-dev20190424\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0/7.5\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nFor the script below\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras.layers as nn\r\nfrom tensorflow.python.keras.backend import get_graph\r\nfrom tensorflow.python.keras.utils.generic_utils import to_snake_case\r\nfrom tensorflow.python.keras.engine.base_layer_utils import unique_layer_name\r\n\r\nModule = tf.keras.models.Model \r\n\r\n# class Module(tf.Module):\r\n#     def __init__(self, name=None, **kwargs):\r\n#         super(Module, self).__init__(name='dummy', **kwargs)\r\n#         name = name or to_snake_case(self.__class__.__name__)\r\n#         self._name = unique_layer_name(name, zero_based=True)\r\n#\r\n#     def __call__(self, *args, **kwargs):\r\n#         with get_graph().as_default(), tf.name_scope(self.name):\r\n#             return self.call(*args, **kwargs)\r\n\r\nclass Sequential(Module):\r\n    def __init__(self, *args):\r\n        super(Sequential, self).__init__()\r\n        self.module_list = list(args) if args else []\r\n\r\n    def call(self, x):\r\n        for module in self.module_list:\r\n            x = module(x)\r\n        return x\r\n\r\nclass Block(Module):\r\n    def __init__(self):\r\n        super(Block, self).__init__()\r\n        self.module = Sequential(\r\n                nn.Dense(10),\r\n                nn.Dense(10),)\r\n\r\n    def call(self, input_tensor):\r\n        x = self.module(input_tensor)\r\n        return x\r\n\r\nclass Base(Module):\r\n    def __init__(self):\r\n        super(Base, self).__init__()\r\n        self.module = Sequential(\r\n                Block(),\r\n                Block())\r\n\r\n    def call(self, input_tensor):\r\n        x = self.module(input_tensor)\r\n        y = self.module(x)\r\n        return x, y\r\n\r\nclass Network(Module):\r\n    def __init__(self):\r\n        super(Network, self).__init__()\r\n        self.child = Base()\r\n\r\n    def call(self, inputs):\r\n        return self.child(inputs)\r\n\r\nnet = Network()\r\n\r\ninputs = tf.keras.Input(shape=(10, ))\r\noutputs = net(inputs)\r\nmodel = tf.keras.models.Model(inputs=inputs, outputs=outputs)\r\n\r\nprint(model.summary(150))\r\n```\r\nThe current output, probably doesn't provide any useful information: \r\n```\r\nModel: \"model\"\r\n______________________________________________________________________________________________________________________________________________________\r\nLayer (type)                                                       Output Shape                                                Param #\r\n======================================================================================================================================================\r\ninput_1 (InputLayer)                                               [(None, 10)]                                                0\r\n______________________________________________________________________________________________________________________________________________________\r\nnetwork (Network)                                                  ((None, 10), (None, 10))                                    440\r\n======================================================================================================================================================\r\nTotal params: 440\r\nTrainable params: 440\r\nNon-trainable params: 0\r\n______________________________________________________________________________________________________________________________________________________\r\n\r\n```\r\n**Describe the expected behavior**\r\nIf we uncomment the new `Module` class, the output is much more informative. I think this should be the expected behavior:\r\n```\r\nModel: \"model\"\r\n______________________________________________________________________________________________________________________________________________________\r\nLayer (type)                                     Output Shape                     Param #           Connected to\r\n======================================================================================================================================================\r\ninput_1 (InputLayer)                             [(None, 10)]                     0\r\n______________________________________________________________________________________________________________________________________________________\r\ndense (Dense)                                    (None, 10)                       110               input_1[0][0]\r\n                                                                                                    dense_3[0][0]\r\n______________________________________________________________________________________________________________________________________________________\r\ndense_1 (Dense)                                  (None, 10)                       110               dense[0][0]\r\n                                                                                                    dense[1][0]\r\n______________________________________________________________________________________________________________________________________________________\r\ndense_2 (Dense)                                  (None, 10)                       110               dense_1[0][0]\r\n                                                                                                    dense_1[1][0]\r\n______________________________________________________________________________________________________________________________________________________\r\ndense_3 (Dense)                                  (None, 10)                       110               dense_2[0][0]\r\n                                                                                                    dense_2[1][0]\r\n======================================================================================================================================================\r\nTotal params: 440\r\nTrainable params: 440\r\nNon-trainable params: 0\r\n______________________________________________________________________________________________________________________________________________________\r\n```\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Note that the equivalent of this is done for plotting models https://github.com/keras-team/keras/pull/11431", "@thuyen Able to reproduce the issue with the code provided (tried the uncomment code and with comment code). We will take a look at it. Thanks!", "@muddham hi, I've run into this issue as well. Have you had a chance to take a look at it?", "Is there any update on this?", "hasnt this been fixed yer?\r\n", "Any update on this issue? `plot_model` supports this with `expand_nested = True` but `summary` has no such argument and defaults to no expansion.", "@thuyen \r\n\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28154\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28154\">No</a>\n"]}, {"number": 28153, "title": "BUG on `tf.saved_model.load` with large variable", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: none\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.alpha0\r\n- Python version:\r\n- Bazel version (if compiling from source): none\r\n- GCC/Compiler version (if compiling from source): none\r\n- CUDA/cuDNN version: none\r\n- GPU model and memory: none\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nIt works fine when run this in one console\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\nm = tf.Module()\r\nm.v = tf.Variable(np.random.normal(size=(200, 200)).astype(np.float64))\r\nm.f = tf.function(lambda x: x + m.v[0, 0])\r\n\r\ntf.saved_model.save(m, \"save/model\", signatures={\r\n    \"predict\": m.f.get_concrete_function(tf.constant(3., dtype=tf.float64))\r\n})\r\n```\r\nand then the other\r\n```\r\nimport tensorflow as tf\r\nm = tf.saved_model.load(\"save/model\")\r\n```\r\n\r\nIf you change 200 to 20000, you will see errors on load\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\nm = tf.Module()\r\nm.v = tf.Variable(np.random.normal(size=(20000, 20000)).astype(np.float64))\r\nm.f = tf.function(lambda x: x + m.v[0, 0])\r\n\r\ntf.saved_model.save(m, \"save/model\", signatures={\r\n    \"predict\": m.f.get_concrete_function(tf.constant(3., dtype=tf.float64))\r\n})\r\n```\r\n```\r\nimport tensorflow as tf\r\ntf.saved_model.load(\"save/model\")\r\n# tensorflow.python.framework.errors_impl.InvalidArgumentError: save/model/variables/variables.data-00000-of-00001; Invalid argument [Op:RestoreV2]\r\n```\r\n**Describe the expected behavior**\r\n\r\nShould be able to load regardless of the size of variable\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nsee above\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@allenlavoie what's wrong with `tf.saved_model.load`", "FWIW I can't reproduce on Linux (on a machine with plenty of memory). It's probably mac-specific; I think someone reported a similar issue for checkpointing that only affected macs. I haven't had a chance to find a mac and debug it yet.", "@allenlavoie I tested on ubuntu18.04 and there is no bug involved.", "If it affects macs it is a bug. We do support macs. But possibly a duplicate of https://github.com/tensorflow/tensorflow/issues/18769"]}, {"number": 28152, "title": "Add example for LR scheduler", "body": "A short and simple example, especially for new users. It helps for a general understanding on how to use this callback. Open for feedback and suggestions. ;)", "comments": ["@rchao integrated your suggestions ;)", "Resolved issue with line lengths ;)"]}, {"number": 28151, "title": "after quantization aware training, when I convert to tflite quant int8 model, still need to  provide min/max value for  add operation? why add operation need min/max value?", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):1.13\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:9.2/ cudnn 7\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nafter quantization aware training, why i'm still asked to provide min/max value for add operation when convert to tflite int8 model?\r\n\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n2019-04-25 20:18:08.606359: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 2360 operators, 3530 arrays (0 quantized)\r\n2019-04-25 20:18:08.663749: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 2360 operators, 3530 arrays (0 quantized)\r\n2019-04-25 20:18:08.993938: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 273 operators, 505 arrays (1 quantized)\r\n2019-04-25 20:18:08.998229: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 273 operators, 505 arrays (1 quantized)\r\n2019-04-25 20:18:09.000372: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 160 operators, 392 arrays (1 quantized)\r\n2019-04-25 20:18:09.002236: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 160 operators, 392 arrays (1 quantized)\r\n2019-04-25 20:18:09.004023: F tensorflow/lite/toco/tooling_util.cc:1708] Array slim_mobilenetv2/Add, which is an input to the Conv operator producing the output array slim_mobilenetv2/Conv_6/Relu6, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.\r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x00007fb011618740 (most recent call first):\r\n  File \"/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33 in execute\r\n  File \"/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/absl/app.py\", line 251 in _run_main\r\n  File \"/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/absl/app.py\", line 300 in run\r\n  File \"/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 40 in run\r\n  File \"/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59 in main\r\n  File \"/home/syshang/anaconda3/envs/tfnightly0410/bin/toco_from_protos\", line 10 in <module>\r\nAborted (core dumped)\r\n\r\n \r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["any ideas?\r\n", "any updates?", "a similar problem, do you solve it? bro", "I meet the same prblem\r\n", "Based on feedback that the contrib/quantize quantization-aware training tool is a bit brittle and hard to use on some model architectures, we have released a [post-training integer quantization tool](https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9ba), that requires a small calibration dataset. Please take a look at the [tutorial](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tutorials/post_training_integer_quant.ipynb) and give it a try, it should work much better! And let us know if you run into any issues.\r\n\r\nClosing this issue, since we are rethinking and working on an api to replace contrib/quantize quantization-aware-training (although post-training quantization above should be sufficient for the majority of use cases).\r\n\r\nThanks!\r\n-Suharsh\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28151\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28151\">No</a>\n"]}, {"number": 28150, "title": "TypeError: unsupported operand type(s) since TF 1.13", "body": "```\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\nimage_path = '6999_2_2047_690_44_93.png'\r\nimg_str = tf.read_file(image_path)\r\n\r\nimage = tf.image.decode_png(img_str)\r\nimage = image / 255.\r\n```\r\n\r\n**Describe the current behavior**\r\n\r\nWith tensorflow 1.12 this works fine. However with 1.13 this results in \r\n\r\n> Traceback (most recent call last):\r\n>   File \"test.py\", line 9, in <module>\r\n>     image = image / 255.\r\n> TypeError: unsupported operand type(s) for /: 'tensorflow.python.framework.ops.EagerTensor' and 'float'\r\n\r\nThis is the same for graph mode:\r\n\r\n> Traceback (most recent call last):\r\n>   File \"test.py\", line 9, in <module>\r\n>     image = image / 255.\r\n> TypeError: unsupported operand type(s) for /: 'Tensor' and 'float'\r\n\r\nThis looks like a bug?", "comments": ["@dmus With tf-nightly the error has been changed to:\r\n```\r\n>>> image = image / 255.\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\", line 897, in binary_op_wrapper\r\n    return func(x, y, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\", line 1009, in _div_python2\r\n    y = ops.convert_to_tensor(y, name=\"y\", dtype=x.dtype.base_dtype)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1080, in convert_to_tensor\r\n    return convert_to_tensor_v2(value, dtype, preferred_dtype, name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1138, in convert_to_tensor_v2\r\n    as_ref=False)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1160, in internal_convert_to_tensor\r\n    value = _TensorTensorConversionFunction(value, dtype=dtype)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1011, in _TensorTensorConversionFunction\r\n    (dtype.name, t.dtype.name, str(t)))\r\nValueError: Tensor conversion requested dtype uint8 for Tensor with dtype float32: 'tf.Tensor(255.0, shape=(), dtype=float32)'\r\n```\r\n\r\nI think the ValueError is relevant now. We could consider the issue has been fixed in tf-nightly?"]}, {"number": 28149, "title": "Multi-threaded check rectification", "body": "", "comments": ["@jdduke : Your comment is handled now, please check, Thanks!", "@ANSHUMAN87 Could you please resolve the conflicts? Thanks!", "@gbaned, the conflict is resolved now, thanks!", "Can one of the admins verify this patch?", "@ANSHUMAN87 Did you get a chance to look on reviewer comments? Please let us know on the update. Thanks! ", "@ANSHUMAN87 gentle ping to check the reviewer comments. Thanks!", "> @ANSHUMAN87 gentle ping to check the reviewer comments. Thanks!\r\n\r\n@jdduke , @gbaned : The comments are handled now, please check, Thanks!", "@ANSHUMAN87 Can you please resolve conflicts? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!", "Reopened!"]}, {"number": 28148, "title": "dataset object has no attribute 'output_shapes' in tensorflow 2.0 alpha version", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version:2.0.0-alpha0\r\n- Doc Link: https://tensorflow.google.cn/alpha/tutorials/text/text_classification_rnn\r\n\r\n\r\n**Describe the documentation issue**\r\nin the line \r\n\"train_dataset = train_dataset.padded_batch(BATCH_SIZE, train_dataset.output_shapes)\"\r\n\"train_dataset.output_shapes\" can not run, train_dataset of tensorflow2.0 has no attribute 'output_shapes'.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["@gandalflee I ran the code in google colab that was provided here, no error was shown. I ran this in cpu version. can you please check and let us know.", "I ran this on CPU and am seeing the same error. ", "> @gandalflee I ran the code in google colab that was provided here, no error was shown. I ran this in cpu version. can you please check and let us know.\r\n\r\nplease check the dataset api doc of version 2.0:\r\nhttps://tensorflow.google.cn/versions/r2.0/api_docs/python/tf/data/Dataset\r\nand version 1.13:\r\nhttps://tensorflow.google.cn/api_docs/python/tf/data/Dataset\r\n\r\nwe can not find all properties (output_shapes,output_types) of version 1.13  in version 2.0, and, also can not find method \"make_one_shot_iterator\" in version 2.0.\r\nis this a bug? or a new structure of dataset in tf2.0?", "> @gandalflee I ran the code in google colab that was provided here, no error was shown. I ran this in cpu version. can you please check and let us know.\r\n\r\nthis error only shown in tf2.0.\r\n in tf1.13, it is ok", "@gandalflee Executed the same in cpu version but I have not received any error,please see attached screen.As no error reproduced, closing this issue.\r\n![image](https://user-images.githubusercontent.com/48215502/57353957-c7e97400-7187-11e9-9461-9e2047e2280f.png)\r\n", "i think the output_shapes attribute is defined in tensorflow_datasets module\r\ntry first `import tensorflow_datasets as tfds`, and i get pass this error\r\ntensorflow official should solve this problem", "Try using [`tf.compat.v1.data.get_output_shapes`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/data/get_output_shapes)", "Thanks @SamuelMarks. This command works.\r\n", "> \r\n> \r\n> Try using [`tf.compat.v1.data.get_output_shapes`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/data/get_output_shapes)\r\n\r\n@SamuelMarks What do you mean by 'use' that link. Could you elaborate", "@NFeruch So what was required\u2014functionality wise\u2014is now in the `get_output_shapes` function."]}, {"number": 28147, "title": "Tensorflow 2.0 overwrites logging config", "body": "**System information**\r\n\r\n- Linux Ubuntu 16.04\r\n- Python 3.6.7\r\n- packages: logging 0.5.1.2, tensorflow 2.0.0-alpha0\r\n\r\nWhen importing `tensorflow` any logging user-defined config is ignored and tf config is used instead. Here is an example where logging format and level is changed. Note that the `info` isn't printed at all.\r\n\r\nCode:\r\n```\r\nimport logging\r\n\r\nimport tensorflow\r\n\r\nlogging.basicConfig(level=logging.INFO, format=\"%(levelname)s %(message)s\")\r\nlogger = logging.getLogger(__name__)\r\nlogger.info(\"Some info\")\r\nlogger.error(\"Some error\")\r\n```\r\n\r\nOutput:\r\n```\r\nWARNING: Logging before flag parsing goes to stderr.\r\nE0425 11:22:27.150884 140450657982208 temp.py:9] Some error\r\n```\r\n\r\nExpected output (as if you delete `import tensorflow` or with older version of `tf`):\r\n```\r\nINFO Some info\r\nERROR Some error\r\n```", "comments": ["@jankislinger Thanks for finding the issue. I could reproduce the issue with TF2.0.0-alpha0 and tf-nightly. However, with TF1.13.1, the above code outputs expected output. Thanks!", "I was able to reproduce this bug in tensorflow 1.14.0, which means it is now present in released code.\r\n\r\nEdited to add: this looks like the same bug as #29842", "Posten this in the issue you mentioned @jre21, but posting it here as well, as it is relevant:\r\n\r\nJust importing tensorflow intrusively adds a logging handler to the root logger:\r\n```python\r\nimport logging\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\n\r\nif __name__ == '__main__':\r\n    logger.warning(\"Hi!\")\r\n\r\n(stderr)>>>\r\nHi!\r\n```\r\nvs\r\n```python\r\nimport logging\r\n\r\nimport tensorflow as tf\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\n\r\nif __name__ == '__main__':\r\n    logger.warning(\"Hi!\")\r\n\r\n(stderr)>>>\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0628 20:51:13.387141 140355075146048 test.py:9] Hi!\r\n```\r\n\r\nThis is due to absl adding its own handler to the root logger when importing tensorflow.", "As for tf 1.14, the problem seems to stem for how absl is used in tf 2.0:\r\nhttps://github.com/tensorflow/tensorflow/blob/8e423e3d56390671f0d954c90f4fd163ab02a9c1/tensorflow/python/platform/app.py#L23\r\nThis results in an undesirable `ABSLHandler` being installed on the root logger. ", "@revan I think this is a duplicate of your assigned issue #26691. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28147\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28147\">No</a>\n"]}, {"number": 28146, "title": "Float 16 for training ,storage,and running on tflite CPU and GPU", "body": "Since the tensorflow lite supports the GPU on mobile phone. But, it only supports float32 and float 16, the model size with float32 is too large. So, will the float16 be available for training and storage, and for converting to .tflite.", "comments": ["Someone is actively working on this.  I don't have an exact ETA for this, but should land pretty soon.", "@holyhao This is a stale issue. If someone is looking for an example to implement `float16` quantization, [Here](https://www.tensorflow.org/lite/performance/post_training_float16_quant) is an example described in detail in TF website. \r\n\r\nI am closing this issue. Please feel free to reopen if there is any related issue. Thanks!"]}, {"number": 28145, "title": "Fix compile error for the mbed target for Tensorflow Lite Micro.", "body": "Add pooling.h to the list of files to be copied when building Tensorflow\r\nLite Micro for Mbed OS. Also add conv.h to the same list.", "comments": []}, {"number": 28144, "title": "pb file cannot run on my mobile devices with android demo,but it pass with python scripts", "body": "here is my log:\r\nNo OpKernel was registered to support Op 'Range' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n      device='CPU'; Tidx in [DT_INT32]\r\n      device='CPU'; Tidx in [DT_FLOAT]\r\n    \r\n    \t [[{{node unpool/range}} = Range[Tidx=DT_INT64](unpool/range/start, unpool/range/limit, unpool/range/delta)]]\r\n\r\nI don't know why the Range operation 's data_type is int64  ,how can I  solve this probelm? If this is the cause,why my pb files can run pass with python on pc\r\n", "comments": ["@sunzhe09 Could you provide more details about the issue and context? Also, it would be great if you can provide any commands you followed. Please provide as many details as possible to resolve the issue faster. Thanks!", "sorry for my lately\uff0chere is my test [model.pb](https://github.com/Joker316701882/Salient-Object-Detection)\uff0cI convert it \u3002the input node is Placeholder  \uff0coutput node is  sigmoid\u3002you can freeze it\uff01\r\n", "@sunzhe09 any updates or progress made? im stuck on a similar error. ", "> @azilaazman  \r\nNo\uff0cI just change my tensorflow to 1.12.2\uff0chope it helps\uff01", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28144\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28144\">No</a>\n"]}, {"number": 28143, "title": "[TF 2.0] Tf.keras.layers.Lambda does not fail if it does not support masking, but mask is passed", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **YES**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Debian Stable**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **CPU, both TF-2.0.0a0 and tf-nightly-2.0-preview-2.0.0.dev20190315**\r\n- Python version: **3.5.3**\r\n- Bazel version (if compiling from source): **N/A**\r\n- GCC/Compiler version (if compiling from source): **N/A**\r\n- CUDA/cuDNN version: **N/A**\r\n- GPU model and memory: **N/A**\r\n\r\n**Describe the current behavior**\r\n\r\nWhen `tf.keras.layers.Lambda` is created without `mask` argument, it does not support masking (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/core.py#L733). However, when a mask is passed, it is silently ignored and `None` is returned as the output mask (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/core.py#L797).\r\n\r\n**Describe the expected behavior**\r\n\r\nWhen `tf.keras.layers.Lambda` is created without `mask` argument, it does not support masking. Therefore, when a mask is passed, it should raise an exception similarly to what `Layer.compute_mask` does (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/base_layer.py#L494).\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nm = tf.keras.Sequential([\r\n    tf.keras.layers.Masking(.0),\r\n    tf.keras.layers.Lambda(tf.nn.sigmoid)])\r\n#     tf.keras.layers.GlobalMaxPool1D()])\r\nm(np.array([[[0.]]]))\r\n```\r\n\r\nThe above code does not fail and produces empty output mask. If the `Lambda` layer is replaced by `GlobalMaxPool1D` (which does not support masking), the model fails during construction.\r\n\r\n**Other info / logs**\r\n\r\nA fix is simple -- the `Lambda.compute_mask` should include the test present in `Layer.compute_mask`, namely: https://github.com/tensorflow/tensorflow/blob/9fc8d696b7019ad09d408933de0e4a4e7b81976f/tensorflow/python/keras/engine/base_layer.py#L505-L510\r\n", "comments": ["@foxik I could reproduce the issue with 2.0.0-alpha0 and tf-nightly. Here is the output(error) when `tf.keras.layers.GlobalMaxPool1D()])` enabled.\r\n\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-2-e24ba2c63d0f> in <module>()\r\n      5     #tf.keras.layers.Lambda(tf.nn.sigmoid)])\r\n      6      tf.keras.layers.GlobalMaxPool1D()])\r\n----> 7 m(np.array([[[0.]]]))\r\n\r\n4 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in compute_mask(self, inputs, mask)\r\n    503       if any(m is not None for m in nest.flatten(mask)):\r\n    504         raise TypeError('Layer ' + self.name + ' does not support masking, '\r\n--> 505                         'but was passed an input_mask: ' + str(mask))\r\n    506       # masking not explicitly supported: return None as mask.\r\n    507       return None\r\n\r\nTypeError: Layer global_max_pooling1d does not support masking, but was passed an input_mask: tf.Tensor([[False]], shape=(1, 1), dtype=bool)\r\n```\r\n\r\n\r\nWith TF1.13.1, I don't see the above error. Here is the output when `tf.keras.layers.GlobalMaxPool1D()])` enabled.\r\n<tf.Tensor 'sequential/Max:0' shape=(1, 1) dtype=float64>\r\n\r\nThanks!", "Note that the situation got even worse in TF 2.1, where `supports_masking` is not respected even for `GlobalMaxPool1D`, see #33260 :-( ", "Thanks for the report!\r\nLooking through the code, here's my understanding:\r\n1) The previous behavior (up until Tensorflow 2.0.0.alpha) is alway call self.compute_mask. Since GlobalMaxPooling doesn't override compute_mask, it will call `base_layer.compute_mask` and error out.\r\n2) The current behavior (since Tensorflow 2.0.0 stable) is simply to swallow the mask if self.supports_masking=False and `compute_mask` is not overridden. Specifically the implementation:\r\n```python\r\n    # Only compute the mask if the Layer explicitly supports masking or has\r\n    # overridden `compute_mask`.\r\n    should_compute_mask = (\r\n        hasattr(self, 'compute_mask') and\r\n        (self.supports_masking or\r\n         not getattr(self.compute_mask, '_is_default', False)))\r\n\r\n    if mask_already_computed:\r\n      flat_masks = [getattr(x, '_keras_mask', None) for x in flat_outputs]\r\n    elif not should_compute_mask:\r\n      flat_masks = [None for _ in flat_outputs]\r\n    else:\r\n      output_masks = self.compute_mask(inputs, previous_mask)\r\n      # `compute_mask` can return a single `None` even when a Layer\r\n      # has multiple outputs.\r\n      if output_masks is None:\r\n        flat_masks = [None for _ in flat_outputs]\r\n      else:\r\n        flat_masks = nest.flatten(output_masks)\r\n```\r\n\r\nThis is definitely an un-noted behavior change. But I'm not sure if this is a bad change?", "@foxik ,\r\nThe code could be run successfully when executed with **`Tensorflow Version 2.4.1`**. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/6eed6a584bc9eba2fbe801a2f2d81c1a/gh_28143.ipynb) of the working code. Thanks!", "@rmothukuru It seems (see also #33260 and especially comment https://github.com/tensorflow/tensorflow/issues/33260#issuecomment-609114823) that the semantics of masking changed since TF 2.0.0beta (since commit https://github.com/tensorflow/tensorflow/commit/8c65ee1de5ebcb85485cb0f7e5bc008134a394e7).\r\n\r\nTherefore, I am closing this issue, it is enough to track the problem in #33260. But given that this mask semantics is around since TF 2.0, it is very unlikely to change, and with RaggedTensors I have stopped using masking anyway.\r\n\r\nCheers!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28143\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28143\">No</a>\n"]}, {"number": 28142, "title": "ImportError: DLL load failed on Windows 10 using GPU", "body": "**System information**\r\n- OS Platform and Distribution : Windows 10 1809\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.7.3/3.6.8\r\n- Installed using virtualenv? pip? conda?: pip install tensorflow-gpu\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: tried 10.1/10.0/9.0, cudnn 7.5\r\n- GPU model and memory: GTX 1060 6GB\r\n\r\n**Describe the problem**\r\n\r\nI have tried several different versions of CUDA/cuDNN, and added the bin/include/lib folders to PATH, but that doesn't work for my situation. Reinstall python and tensorflow doesn't work either. I tried the binary version of tensorflow and installed the corresponding CUDA/cuDNN, this error also appeared. I can't figure out what's wrong.\r\n\r\n*VS 2015 C++ Redist is also installed\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nimport tensorflow as tf\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nTraceback (most recent call last):\r\n  File \"F:\\Python Environment\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"F:\\Python Environment\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"F:\\Python Environment\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"F:\\Python Environment\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"F:\\Python Environment\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6a21\u5757\u3002\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"F:\\Python Environment\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"F:\\Python Environment\\Python37\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"F:\\Python Environment\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"F:\\Python Environment\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"F:\\Python Environment\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"F:\\Python Environment\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"F:\\Python Environment\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"F:\\Python Environment\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6a21\u5757\u3002\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["That's a PATH error"]}, {"number": 28141, "title": "Cleanup and added a TC in the file.", "body": "Did some cleanup and added the TC for the transformation.", "comments": ["@amitsrivastava78 can you please check ubuntu sanity errors ? Thanks!", "> @amitsrivastava78 can you please check ubuntu sanity errors ? Thanks!\r\n@gbaned , thanks for pointing this out, i have fixed the problem.\r\n\r\nRegards\r\nAmit\r\n", "@gbaned , all the checks have passed for this PR, can you please help to merge this.\r\n\r\nRegards\r\nAmit", "@gbaned , all the checks have passed for this PR, can you please help to merge this.\r\n\r\nRegards\r\nAmit", "@gbaned , can you please help to merge this PR.\r\n\r\nRegards\r\nAmit", "@gbaned , can you please help to get this merged.\r\n\r\nRegards\r\nAmit", "@jianlijianli and @miaout17 I have resolved the merge conflicts , can you please have alook.\r\n\r\n\r\nRegards\r\nAmit", "@amitsrivastava78 Could you please resolve the conflicts? Thanks!", "Can one of the admins verify this patch?", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 28140, "title": "Error running TFLite Quantized model on NNAPI", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 9 PQ2A.190205.001\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel 3\r\n- TensorFlow installed from (source or binary):TensorFlow Lite from AAR\r\n- TensorFlow version (use command below):tensorflow-lite:0.0.0-nightly\r\n\r\n\r\n**Describe the current behavior**\r\nWe have an image classification model trained from scratch in frozen graph format and:\r\n\r\n1. .tflite model converted without post-training quantization works well on CPU and NNAPI.\r\n2. .tflite model converted with post-training quantization works well on CPU.\r\n3. .tflite model converted with post-training quantization fails on NNAPI with following error message:\r\n\r\n```\r\nE/Utils: Invalid input tensor type TENSOR_QUANT8_ASYMM for input 1, expected TENSOR_FLOAT32\r\nE/tflite: Returning error since NNAPI returned failure nnapi_delegate.cc:696.\r\n    Returning error since TFLite returned failure nnapi_delegate.cc:739.\r\n    Failed to build graph for NNAPI\r\n```\r\n\r\nWe share the same code across all cases.\r\n\r\n**Describe the expected behavior**\r\nthe quantized model should work on NNAPI\r\n\r\n**Code to reproduce the issue**\r\nAlmost same as example below:\r\nhttps://github.com/tensorflow/tensorflow/tree/eae92e9d58846f559561ae1c8934a025e740aeb2/tensorflow/lite/examples/android/app\r\n\r\nWe'd happy to attach our tflite model but GitHub says it's too large \ud83d\ude22 \r\nThe beginning of our model is like this:\r\n\r\n<img width=\"1136\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2019-04-25 16 43 52\" src=\"https://user-images.githubusercontent.com/3690310/56718223-58cc5280-6779-11e9-8573-222d7a156313.png\">\r\n", "comments": ["Just received the same error in similar conditions. +1 for this", "The NNAPI doesn't have post-training quantization tool support just yet, we are working on adding this as soon as we can, for now you will need to rely on tflite\u2019s cpu fallback operatinos.", "Hi @tamamachi !  \r\nWe are checking to see whether you still need help in this issue .  Have you checked [NNAPI](https://www.tensorflow.org/lite/performance/nnapi#quantization) documentation on post-quantization from 2.8 version now yet?", "Hi @mohantym ,\r\nThank you for letting me know. \r\nUnfortunately I've left from the team and I personally do not need help anymore. \r\nAccording to the document post-training quantization for NNAPI is ready (I haven't tested myself though) so feel free to close this issue. ", "Ok @tamamachi !Thanks for responding. Closing this issue for now. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28140\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28140\">No</a>\n"]}]