[{"number": 5784, "title": "Fix to ci_build.sh id command (fixes #5759)", "body": "Fixes #5759 ", "comments": ["Can one of the admins verify this patch?", "@abhitopia, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ebrevdo, @vrv and @caisq to be potential reviewers.", "@tensorflow-jenkins test this please", "The Linux GPU build looks like a flaky tools failure. Merging this PR now.", "Thanks, @abhitopia !"]}, {"number": 5783, "title": "My tensorboard appears many undefined events and charts", "body": "I only summary my loss as xentropy_mean in training ,but in tensorboard ,I had not find the xentropy_mean chart but many other charts I dont know. I don't know where I wrote wrong, and what's the matter indeed.\r\nI wrote the inference(), training() and loss() in this file\r\n`\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport math\r\n\r\nimport tensorflow.python.platform\r\nimport tensorflow as tf\r\n\r\n\r\nNUM_CLASSES = 16\r\n\r\n\r\nIMAGE_SIZE = 28\r\nIMAGE_PIXELS = 784\r\n\r\n\r\ndef inference(images, hidden1_units, hidden2_units):\r\n\r\n\r\n\r\n\r\n  with tf.name_scope('hidden1'):\r\n    weights = tf.Variable(\r\n        tf.truncated_normal([IMAGE_PIXELS, hidden1_units],\r\n                            stddev=1.0 / math.sqrt(float(IMAGE_PIXELS))),\r\n        name='weights')\r\n    biases = tf.Variable(tf.zeros([hidden1_units]),\r\n                         name='biases')\r\n    hidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)\r\n\r\n  with tf.name_scope('hidden2'):\r\n    weights = tf.Variable(\r\n        tf.truncated_normal([hidden1_units, hidden2_units],\r\n                            stddev=1.0 / math.sqrt(float(hidden1_units))),\r\n        name='weights')\r\n    biases = tf.Variable(tf.zeros([hidden2_units]),\r\n                         name='biases')\r\n    hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\r\n\r\n  with tf.name_scope('softmax_linear'):\r\n    weights = tf.Variable(\r\n        tf.truncated_normal([hidden2_units, NUM_CLASSES],\r\n                            stddev=1.0 / math.sqrt(float(hidden2_units))),\r\n        name='weights')\r\n    biases = tf.Variable(tf.zeros([NUM_CLASSES]),\r\n                         name='biases')\r\n    logits = tf.matmul(hidden2, weights) + biases\r\n  return logits\r\n\r\n\r\ndef loss(logits, labels):\r\n \r\n  batch_size = tf.size(labels)\r\n  labels = tf.expand_dims(labels, 1)\r\n  indices = tf.expand_dims(tf.range(0, batch_size), 1)\r\n  concated = tf.concat(1, [indices, labels])\r\n  print('Done2')\r\n  onehot_labels = tf.sparse_to_dense(\r\n      concated, tf.pack([batch_size, 16]), 1.0, 0.0)\r\n  print('Done1')\r\n  cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits,\r\n                                                          onehot_labels,\r\n                                                          name='xentropy')\r\n  loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\r\n  \r\n  return loss\r\n\r\n\r\ndef training(loss, learning_rate):\r\n  tf.summary.scalar(loss.op.name, loss)\r\n  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\r\n  global_step = tf.Variable(0, name='global_step', trainable=False)\r\n  train_op = optimizer.minimize(loss, global_step=global_step)\r\n\r\n  return train_op\r\n\r\n\r\ndef evaluation(logits, labels):\r\n  \r\n\r\n  correct = tf.nn.in_top_k(logits, labels, 1)\r\n  return tf.reduce_sum(tf.cast(correct, tf.int32))\r\n`\r\n\r\nand training process in this file\r\n\r\n`from __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport argparse\r\nimport os.path\r\nimport sys\r\nimport time\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\n\r\nimport mnist\r\n\r\n\r\n\r\n\r\n\r\nTRAIN_FILE = 'train.tfrecords'\r\nVALIDATION_FILE = 'validation.tfrecords'\r\nTEST_FILE='test.tfrecords'\r\n\r\n\r\nflags = tf.app.flags\r\nFLAGS = flags.FLAGS\r\n\r\nflags.DEFINE_string('train_dir', '/home/queenie/image2tfrecord/tfrecords-28-gray/', 'Directory to put the training data.')\r\nflags.DEFINE_string('filename', 'train.tfrecords', 'Directory to put the training data.')\r\nflags.DEFINE_integer('batch_size', 100, 'Batch size.  '\r\n                     'Must divide evenly into the dataset sizes.')\r\nflags.DEFINE_integer('num_epochs', None, 'Batch size.  '\r\n                     'Must divide evenly into the dataset sizes.')\r\nflags.DEFINE_integer('hidden1', 128,'balabala')\r\nflags.DEFINE_integer('hidden2', 32,'balabala')\r\nflags.DEFINE_integer('learning_rate', 0.01,'balabala')\r\nflags.DEFINE_integer('max_steps', 50000,'balabala')\r\n\r\n\r\ndef placeholder_inputs(batch_size):\r\n  images_placeholder=tf.placeholder(tf.float32,shape=(batch_size,mnist.IMAGE_PIXELS))\r\n  labels_placeholder=tf.placeholder(tf.int32,shape=(batch_size))\r\n  return images_placeholder,labels_placeholder\r\n\r\ndef fill_feed_dict(images_feed,labels_feed,images_pl,labels_pl):\r\n  \r\n  feed_dict={\r\n  images_pl:images_feed,\r\n  labels_pl:labels_feed,\r\n  }\r\n  return feed_dict\r\n\r\ndef read_and_decode(filename_queue):\r\n  reader = tf.TFRecordReader()\r\n  _, serialized_example = reader.read(filename_queue)\r\n  features = tf.parse_single_example(\r\n      serialized_example,\r\n      # Defaults are not specified since both keys are required.\r\n      features={\r\n          'image_raw': tf.FixedLenFeature([], tf.string),\r\n          'label': tf.FixedLenFeature([], tf.int64),\r\n      })\r\n\r\n\r\n  image = tf.decode_raw(features['image_raw'], tf.uint8)\r\n  image.set_shape([mnist.IMAGE_PIXELS])\r\n\r\n \r\n\r\n \r\n  image = tf.cast(image, tf.float32) * (1. / 255) - 0.5\r\n\r\n  label = tf.cast(features['label'], tf.int32)\r\n\r\n  return image, label\r\n\r\n\r\ndef do_eval(sess,eval_correct):\r\n\ttrue_count=0\r\n\tfor step in xrange(FLAGS.batch_size):\r\n\t\t#print(sess.run(eval_correct))\r\n\t\ttrue_count+=sess.run(eval_correct)\r\n\r\n\tprecision=float(true_count)/FLAGS.batch_size/FLAGS.batch_size\r\n\tprint('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' %\r\n(FLAGS.batch_size, true_count, precision))\r\n\treturn precision\r\n\r\n\r\ndef inputs(train, batch_size, num_epochs):\r\n\r\n  if not num_epochs: num_epochs = None\r\n  if train=='train':\r\n  \tfilename=os.path.join(FLAGS.train_dir,TRAIN_FILE)\r\n  elif train=='validation':\r\n  \tfilename=os.path.join(FLAGS.train_dir,VALIDATION_FILE)\r\n  else:\r\n  \tfilename=os.path.join(FLAGS.train_dir,TEST_FILE)\r\n\r\n\r\n \r\n  with tf.name_scope('input'):\r\n    filename_queue = tf.train.string_input_producer(\r\n        [filename], num_epochs=None)\r\n    image, label = read_and_decode(filename_queue)\r\n    images, sparse_labels = tf.train.shuffle_batch(\r\n        [image, label], batch_size=batch_size, num_threads=2,\r\n        capacity=1000 + 3 * batch_size,\r\n        \r\n        min_after_dequeue=1000)\r\n\r\n    return images, sparse_labels\r\n\r\n\r\ndef run_training():\r\n  with tf.Graph().as_default():\r\n    images, labels = inputs(train='train', batch_size=FLAGS.batch_size,\r\n                            num_epochs=FLAGS.num_epochs)\r\n\r\n    images_valid,labels_valid=inputs(train='validation', batch_size=FLAGS.batch_size,\r\n                             num_epochs=FLAGS.num_epochs)\r\n    \r\n    images_test,labels_test=inputs(train='test', batch_size=FLAGS.batch_size,\r\n                             num_epochs=FLAGS.num_epochs)\r\n\r\n    logits = mnist.inference(images,\r\n                             FLAGS.hidden1,\r\n                             FLAGS.hidden2)\r\n    \r\n\r\n    valid_prediction=mnist.inference(images_valid,FLAGS.hidden1,FLAGS.hidden2)\r\n\r\n    test_prediction=mnist.inference(images_test,FLAGS.hidden1,FLAGS.hidden2)\r\n\r\n\r\n\r\n    loss = mnist.loss(logits, labels)\r\n\r\n    \r\n    train_op = mnist.training(loss, FLAGS.learning_rate)\r\n\r\n    eval_correct=mnist.evaluation(logits,labels)\r\n\r\n    eval_correct_valid=mnist.evaluation(valid_prediction,labels_valid)\r\n\r\n    eval_correct_test=mnist.evaluation(test_prediction,labels_test)\r\n\r\n    summary_op=tf.merge_all_summaries()\r\n   \r\n    init_op = tf.group(tf.initialize_all_variables(),\r\n                       tf.initialize_local_variables())\r\n\r\n    saver = tf.train.Saver()\r\n    sess = tf.Session()\r\n\r\n    \r\n  \r\n    sess.run(init_op)\r\n\r\n    summary_writer = tf.train.SummaryWriter(FLAGS.train_dir, sess.graph)\r\n\r\n\r\n    coord = tf.train.Coordinator()\r\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n\r\n    try:\r\n      step = 0\r\n      train_precision=0\r\n      validation_precision=0\r\n      test_precision=0\r\n      #while not coord.should_stop():\r\n      while not coord.should_stop():\r\n        start_time = time.time()\r\n\r\n     \r\n        _, loss_value,images_see,labels_see = sess.run([train_op, loss,images,labels])\r\n\r\n        duration = time.time() - start_time\r\n\r\n\r\n       \r\n        if step % 100 == 0:\r\n          print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value,\r\n                                                     duration))\r\n          precision_tr=do_eval(sess,eval_correct)\r\n          summary_str=sess.run(summary_op)\r\n          summary_writer.add_summary(summary_str,step)\r\n    \r\n\r\n\r\n        if (step + 1) % 1000 == 0 or (step + 1) == FLAGS.max_steps:\r\n          checkpoint_file = os.path.join(FLAGS.train_dir, 'model.ckpt')\r\n          saver.save(sess, checkpoint_file, global_step=step)\r\n          print('Train:')\r\n          do_eval(sess,eval_correct)\r\n          print('Validation:')\r\n          do_eval(sess,eval_correct_valid)\r\n          print('Test:')\r\n          do_eval(sess,eval_correct_test)\r\n        \r\n\r\n        step += 1\r\n\r\n    except tf.errors.OutOfRangeError:\r\n      print('Done training for %d epochs, %d steps.' % (FLAGS.num_epochs, step))\r\n    finally:\r\n      coord.request_stop()\r\n\r\n   \r\n    coord.join(threads)\r\n    sess.close()\r\n\r\n\r\nrun_training()\r\n`\r\n\r\nand the tensorboard is as the picture\r\n\r\n![2016-11-22 20-43-03](https://cloud.githubusercontent.com/assets/11943769/20524207/53013746-b0f4-11e6-8f25-d8ae5d80b27f.png)\r\n\r\n", "comments": ["This is a general support question best suited to StakcOverflow.  Could you please re-ask your question there.", "@prb12 I think you should not close it, maybe it is best suited to StackOverflow, but the people who use tensorboard here is more than StackOverflow, people could get more suggests. Although I have re-asked in StackOverFlow ", "It is not simply a question of \"getting more suggestions here\".   Every \"issue\" reported here consumes a significant amount of developer time to track, triage, diagnose etc.\r\n\r\nTo quote from https://www.tensorflow.org/versions/master/resources/index.html:\r\n\r\n# Help / Support / How do I?\r\n\r\nFor help and support, technical or algorithmic questions, please submit your questions to Stack Overflow: https://stackoverflow.com/questions/tagged/tensorflow. You may also find answers in our FAQ, our glossary, or in the shapes, sizes and types guide. **Please do not use the mailing list or issue tracker for support.**\r\n\r\n#### Discussions\r\nFor general discussions, please join the TensorFlow discuss mailing list. This list is intended for general discussions about TensorFlow development and directions, not as a help forum. Instead, direct your questions to Stack Overflow, and report issues on GitHub.\r\n\r\n#### Report Issues\r\nPlease report bugs, feature requests and installation / compatibility issues on the TensorFlow issues tracker on GitHub. **If you need help with using TensorFlow, please do not use the issue tracker for that.** Instead, direct your questions to Stack Overflow.", "@prb12 Ok, fine, I got it. Thank you for noticing all these things to me"]}, {"number": 5782, "title": "model/rnn/translate reads the data model after creating network", "body": "The translate module reads the data files after creating the network, this leads to long waits if there are problems in the data files. \r\n\r\nI have moved the read file section forward in my branch \r\nhttps://github.com/h4ck3rm1k3/tensorflow/commit/62d46a3b8687e0b7d5e37320b545ffe63a9361a1#diff-4dada032ac312625f74be392b009cc00R152", "comments": ["This is a feature request, not a bug. Since the translation example will be going away (in favour of a better model in TF-models later), I'm closing this.", "alles klar...."]}, {"number": 5781, "title": "ImportError: cannot import name pywrap_tensorflow, not resolved by previous fixes", "body": "I'm getting an error similar to that in #3217, but it's not resolved by switching directories or adding the lines to my bash profile. I'm using a jupyter notebook. Any suggestions?\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-577-fd66eeda4fb7> in <module>()\r\n      2 import pandas as pd\r\n      3 import scipy as sp\r\n----> 4 import tensorflow as tf\r\n      5 import sklearn.preprocessing as Preprocessing\r\n      6 from sklearn.preprocessing import Imputer\r\n\r\n/Users/curr_user/anaconda/lib/python2.7/site-packages/tensorflow/__init__.py in <module>()\r\n     21 from __future__ import print_function\r\n     22 \r\n---> 23 from tensorflow.python import *\r\n\r\n/Users/curr_user/anaconda/lib/python2.7/site-packages/tensorflow/python/__init__.py in <module>()\r\n     46 _default_dlopen_flags = sys.getdlopenflags()\r\n     47 sys.setdlopenflags(_default_dlopen_flags | ctypes.RTLD_GLOBAL)\r\n---> 48 from tensorflow.python import pywrap_tensorflow\r\n     49 sys.setdlopenflags(_default_dlopen_flags)\r\n     50 \r\n\r\nImportError: cannot import name pywrap_tensorflow", "comments": ["This is a general support question best suited to StackOverflow.  Please could you re-ask your question there."]}, {"number": 5780, "title": "freeze_graph error", "body": "Hi there,\r\nI'm using Python 2.7.6, I have the following errors with freeze_graph:\r\n python tensorflow/python/tools/freeze_graph.py --input_graph=/tmp/imagenet/classify_image_graph_def.pb --input_checkpoint=/home/davide/stmp/flowers-models/inception_v3/all/model.ckpt-500 --output_graph=/tmp/frozen_graph.pb --output_node_names=softmax\r\nTraceback (most recent call last):\r\n  File \"tensorflow/python/tools/freeze_graph.py\", line 137, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"tensorflow/python/tools/freeze_graph.py\", line 134, in main\r\n    FLAGS.output_graph, FLAGS.clear_devices, FLAGS.initializer_nodes)\r\n  File \"tensorflow/python/tools/freeze_graph.py\", line 100, in freeze_graph\r\n    text_format.Merge(f.read().decode(\"utf-8\"), input_graph_def)\r\n  File \"/usr/lib/python2.7/encodings/utf_8.py\", line 16, in decode\r\n    return codecs.utf_8_decode(input, errors, True)\r\nUnicodeDecodeError: 'utf8' codec can't decode byte 0xbb in position 1: invalid start byte\r\n\r\n\r\nAny idea?\r\nThanks", "comments": ["Please can you supply the information requested in the issues reporting template.  (Platform/OS version, TF versions, installation method, exact sequence of commands to reproduce etc)\r\n\r\ne.g. how did you create the input graphdef file?  From the suffix it seems it might be a binary protobuf file, and is being read at ASCII.\r\n\r\n@petewarden  Can you spot anything obvious here?\r\n", "I would try using --input_binary=true as a command line argument, since it does look like you have a binary file.", "I have the same problem. I try exporting a protobuf file to Android using\r\n\r\n **bazel build tensorflow/python/tools:freeze_graph &&  bazel-bin/tensorflow/python/tools/freeze_graph --input_graph=trainGraph/train.pbtxt --input_checkpoint=trainGraph/checkpoint.ckpt  --output_graph=frozen_graph.pb --output_node_names=softmax.** \r\n\r\nTo do that, I generated a .pbtxt file using \"**tf.train.write_graph(sess.graph_def,'tarinGraph','train2.pbtxt',False)**\"\r\n to run freeze_graph on. I saved the protobuf file as binary and run it with freeze_graph, I get the following error:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/user/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.rnfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 135, in <mdule>\r\n    tf.app.run()\r\n  File \"/home/user/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.rnfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"/home/user/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.rnfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 132, in man\r\n    FLAGS.output_graph, FLAGS.clear_devices, FLAGS.initializer_nodes)\r\n  File \"/home/user/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.rnfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 98, in freze_graph\r\n    text_format.Merge(f.read().decode(\"utf-8\"), input_graph_def)\r\n  File \"/usr/lib/python2.7/encodings/utf_8.py\", line 16, in decode\r\n    return codecs.utf_8_decode(input, errors, True)\r\nUnicodeDecodeError: 'utf8' codec can't decode byte 0x80 in position 59: invalidstart byte\r\n\r\nEnvironment info\r\n\r\nOperating System:\r\nUbuntu 14.04\r\nTensorflow 0.11.0\r\nInstallation method : Installing from github sources\r\n\r\nThanks in advance", "@MikeChenfu As @petewarden already said, if you write your graph as a binary file using:\r\n```\r\ntf.train.write_graph(sess.graph_def, 'tarinGraph', 'train2.pbtxt', as_text=False)\r\n```\r\nthen you will need to pass the `--input_binary=true` flag to `freeze_graph`.\r\n\r\nPlease feel free to re-open this issue if the above solution doesn't work.", "@prb12 Thank you for your immediate response.  I use --input_binary=true as a command line argument and get below result. \r\n\r\n```\r\nConverted 0 variables to const ops.\r\n1 ops in the final graph.\r\n```\r\n\r\nThe generated frozen_graph.pb only contains 76 bytes. Any idea about this problem? \r\n\r\nThank you so much.", "@petewarden may know of some common failure modes, but now I think we're getting into general support territory.\r\n\r\nFrom https://www.tensorflow.org/versions/r0.11/resources/index.html:\r\n\r\n> For help and support, technical or algorithmic questions, please submit your questions to Stack Overflow: https://stackoverflow.com/questions/tagged/tensorflow. You may also find answers in our FAQ, our glossary, or in the shapes, sizes and types guide. Please do not use the mailing list or issue tracker for support.\r\n", "@MikeChenfu That problem might happen if you haven't specified the right output nodes to the freeze_graph script.", "In another project it has already worked okay for me with the command line argument --input_binary=true:\r\n\r\nConverted 2 variables to const ops.\r\n7 ops in the final graph.", "@petewarden  I also have got this issue. I got the checkpoints and the binary .pb file but the freeze_graph.py shows me a utf8 decoder error similar to what @MikeChenfu posted. I tested freeze_graph_test.py and it was working OK. \r\n\r\nWhat is the problem with the checkpoints / protobuf file. By the way I am getting my checkpoints through running inception V3 model. I validated the model as well and the validation process was OK. So this tells me that the checkpoints should be OK. So I don't understand what the problem is. Could you please help me with that ?", "Can you send me the checkpoint and graphdef file so I can reproduce the issue? If you don't want to publicly attach them to this issue, feel free to email me at petewarden at google.com instead.", "@petewarden I sent an email to you. Thank you very much. I appreciate your help. ", "Thanks @Pouyan-Jahangiri for the graph. I was able to give it a try, and it appears the utf-8 error happens when the input graph is binary format, but the script is expecting a text format protobuf.\r\nTo fix that, pass in `--input_binary=true` to the command line. For example:\r\n`bazel-bin/tensorflow/python/tools/freeze_graph --input_graph=/tmp/Cymax_graph.pb --input_checkpoint=/tmp/model.ckpt-50 --output_graph=/tmp/cymax_frozen.pb --output_node_names=MergeSummary/MergeSummary --input_binary=true`", "@petewarden Thank you so much for your time and consideration. I did that and I got a new error as following: \r\n\r\n\r\nINFO: Found 1 target...\r\nTarget //tensorflow/python/tools:freeze_graph up-to-date:\r\n  bazel-bin/tensorflow/python/tools/freeze_graph\r\nINFO: Elapsed time: 0.160s, Critical Path: 0.00s\r\nW tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nTraceback (most recent call last):\r\n  File \"/home/pouyanj/Desktop/tensorflow-master/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 148, in <module>\r\n    app.run()\r\n  File \"/home/pouyanj/Desktop/tensorflow-master/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/home/pouyanj/Desktop/tensorflow-master/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 144, in main\r\n    FLAGS.output_graph, FLAGS.clear_devices, FLAGS.initializer_nodes)\r\n  File \"/home/pouyanj/Desktop/tensorflow-master/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 110, in freeze_graph\r\n    _ = importer.import_graph_def(input_graph_def, name=\"\")\r\n  File \"/home/pouyanj/Desktop/tensorflow-master/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/importer.py\", line 259, in import_graph_def\r\n    raise ValueError('No op named %s in defined operations.' % node.op)\r\nValueError: No op named RandomShuffle in defined operations.", "@petewarden May I kindly ask you to look into this new error ? \r\n\r\nAt the end it says that there is a ValueError: \r\n    \"ValueError: No op named RandomShuffle in defined operations.\"", "@petewarden The same error appears when I run it with the text protobuf file (.pbtxt). ", "@petewarden I guess I am doing something wrong with passing --output_node_names. I am not really sure what I should pass to this flag. Currently, I am passing the default in the freeze_graph.py:\r\n--output_node_names=softmax\r\n\r\nis there anything else must be passed to this flag? ", "This error is unrelated to the flags, and just means you're running a version of the code that is out of sync with the one that created the graph. As a first step, try syncing to the latest top of tree, I was able to run the command line I pasted above successfully using that version of the code.", "@petewarden thank you very much for your help and sorry for posting unrelated issue to this flag. I didn't know that. I am using 0.11.0rc0 version and now I downloaded 0.11 version from git. The above error was solved but I still face a new error that is mentioned in the following flag:\r\n\r\nFreeze graph: node is not in graph (even though it's been named) #3986\r\n\r\nI will re-assign you to this flag and will appreciate if you can help me with that. \r\nCheers", "@petewarden Do you think it is possible to create Tensorbox freeze graphs ( *.pb) under TF 1.0 version? or I must to install 0.11 version?. \r\nI got \"Converted 0 variables to const ops\" & \"1 ops in the final graph\". \r\nI'm really not sure about this question.\r\nThanks", "I also get this output message:\r\n\"Converted 0 variables to const ops.\r\n47 ops in the final graph.\""]}, {"number": 5779, "title": "[TensorFlow Mechanics 101]: Add conv layers to this tutorial", "body": "It seems the tutorial in **TensorFlow Mechanics 101** has more beautiful structure in coding, and as it currently only contains fully connected layers. Is it a good addition to include conv layers ? If so, please leave a short comment and I'll submit a pull request soon. ", "comments": ["I think the point of that specific tutorial is to show general TensorFlow features and usage, rather than to teach people about different kinds of model?\r\n\r\n@xmbrst @dr4b  Would you like to comment on this?", "Yeah, I think that is correct.  We do have a convolutional neural network tutorial as well in /tensorflow/g3doc/tutorials/deep_cnn/index.md -- does that help for what you're looking for?", "Closing since I don't think that we want to complicate a beginner tutorial and there are plenty of other tutorials available on convnets. "]}, {"number": 5778, "title": "Add maximize method to Optimizer.", "body": "This PR adds the convenience method `maximize` to the `Optimizer` class.", "comments": ["Can one of the admins verify this patch?", "@tillahoffmann, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @vrv and @tensorflower-gardener to be potential reviewers.", "Sorry Till, but we had this discussion in https://github.com/tensorflow/tensorflow/pull/3229 and the result was that as an interface, it makes sense to only implement one of the two methods.\r\n\r\n(If python had the equivalent of a \"final\" keyword to prevent overriding, we'd be okay with it...)", "Makes sense. \ud83d\udc4d "]}, {"number": 5777, "title": "GPU becomes unavailable after computer wakes up ", "body": "I noticed many have issues with GPU being unavailable with message (e.g., [issue 394](https://github.com/tensorflow/tensorflow/issues/394))\r\n\r\n`E tensorflow/stream_executor/cuda/cuda_driver.cc:491] failed call to cuInit: CUDA_ERROR_UNKNOWN\r\n` \r\nsome suggested `sudo apt-get install nvidia-modprobe`  but it does not work for all including me. my GPU works until i put the computer to sleep/suspense, but after waking up the computer i always get the message above and the GPU (gtx 1070) is no longer available in execution of the code (only CPU is used) in nvidia docker.  I also noticed if prior to suspending the computer i exit the docker and then restart it when i wake the computer the GPU is still available in docker. So, the problem happens if i suspend the computer while the ipython-notebook session is up and running. \r\n\r\nI am using nvidia-docker \r\n\r\n`nvidia-docker run -it -p 8888:8888 -v /*..../Data/docker:/docker --name TensorFlow   gcr.io/tensorflow/tensorflow:latest-gpu /bin/bash`\r\n\r\nNvidia-smi and nvidia-debugdump -l both show the GPU is installed and driver is up to date within docker and in the host. \r\n\r\nwhen i run nvidia-smi in docker the output is \r\n\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1070    Off  | 0000:01:00.0      On |                  N/A |\r\n|  0%   41C    P0    39W / 180W |    450MiB /  8105MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n+-----------------------------------------------------------------------------+\r\n\r\n                                                                               \r\n\r\n\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:491] failed call to cuInit: CUDA_ERROR_UNKNOWN\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:153] retrieving CUDA diagnostic information for host: ca234sff235\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:160] hostname: ca234sff235\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:185] libcuda reported version is: 367.57.0\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:356] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.57  Mon Oct  3 20:37:01 PDT 2016\r\nGCC version:  gcc version 4.9.3 (Ubuntu 4.9.3-13ubuntu2) \r\n\"\"\"\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] kernel reported version is: 367.57.0\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:293] kernel version seems to match DSO: 367.57.0\r\n```\r\n\r\nSoftware specs:\r\nOS: Ubuntu 16.04 LTS - 64 bit\r\nGPU driver: nvidia 367.57\r\nCuda : 7.5", "comments": ["This seems most likely to be an issue with your NVidia driver / CUDA installation.\r\n @zheng-xq  I seem to remember that these cards need Cuda8.0 ?", "I'm having the exact same bug using the docker version of tensorflow.\r\nSpecs:\r\n+ __os__: Linux Mint 18 Cinnamon 64 bit\r\n+ __gpu__: Geforce 940M\r\n+ __driver__: 367.57\r\n+ __cuda__: 8.0\r\n+ __cudnn__: 5.1 ", "@Sadrpour, @aPere3, the best way to diagnose is to compile the vectorAdd samples from your Cuda SDK. If you can run it successfully, but TensorFlow cannot, it might be a TF issue. Otherwise, it is likely to be a problem with your local Cuda installation. ", "i have the same issue\r\n\r\nTitan X\r\nCUDA Version 8.0.44\r\nNVIDIA Driver Version: 367.57\r\nUbunut 16.04  4.4.0-64-generic\r\n", "I still have not found a fix either", "Same problem:\r\n\r\n* TensorFlow: tensorflow-gpu 1.0.0 installed via pip in Anaconda sandbox\r\n* GPU: GeForce GTX 680\r\n* cuda: 8.0\r\n* cudnn: 5.1.5\r\n* NVIDIA driver: 375.39\r\n* OS: Ubuntu 16.10 64 bit\r\n", "@zheng-xq, this is the test that I ran. My interpretation is that the issue is caused by TensorFlow:\r\n\r\n1. run vectorAdd -- OK\r\n2. sleep/wake up\r\n3. run vectorAdd -- OK\r\n4. sleep/wake up\r\n5. run vectorAdd -- OK\r\n6. run python and in there `import tensorflow as tf`\r\n7. run vectorAdd -- OK\r\n8. sleep/wake up\r\n9. run vectorAdd -- OK\r\n10. in the previously open python session: `tf.test.gpu_device_name()` -- OK\r\n11. run vectorAdd -- OK\r\n12. sleep/wake up\r\n13. run vectorAdd -- **FAILED**\r\n14. in the previously open python session: `tf.test.gpu_device_name()` -- **FAILED**\r\n15. run vectorAdd -- **FAILED**\r\n\r\n## Command output details\r\n\r\n### Successful vectorAdd\r\n\r\n```\r\n$ ./vectorAdd \r\n[Vector addition of 50000 elements]\r\nCopy input data from the host memory to the CUDA device\r\nCUDA kernel launch with 196 blocks of 256 threads\r\nCopy output data from the CUDA device to the host memory\r\nTest PASSED\r\nDone\r\n```\r\n\r\n### TensorFlow import\r\n\r\n```\r\n>>> import tensorflow as tf\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n```\r\n\r\n### Successful gpu_device_name\r\n\r\n```\r\n>>> tf.test.gpu_device_name()\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GTX 680\r\nmajor: 3 minor: 0 memoryClockRate (GHz) 1.15\r\npciBusID 0000:01:00.0\r\nTotal memory: 1.95GiB\r\nFree memory: 1.68GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 680, pci bus id: 0000:01:00.0)\r\n'/gpu:0'\r\n```\r\n\r\n### Failed vectorAdd\r\n\r\n```\r\n$ ./vectorAdd \r\n[Vector addition of 50000 elements]\r\nFailed to allocate device vector A (error code all CUDA-capable devices are busy or unavailable)!\r\n```\r\n\r\n### Failed gpu_device_name\r\n\r\n```\r\n>>> tf.test.gpu_device_name()\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 680, pci bus id: 0000:01:00.0)\r\n*** Error in `python': double free or corruption (fasttop): 0x0000000003e25870 ***\r\n======= Backtrace: =========\r\n/lib/x86_64-linux-gnu/libc.so.6(+0x790cb)[0x7f540c3d70cb]\r\n/lib/x86_64-linux-gnu/libc.so.6(+0x8275a)[0x7f540c3e075a]\r\n/lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7f540c3e418c]\r\n/usr/lib/x86_64-linux-gnu/libcuda.so.1(+0x1aa1df)[0x7f53cc6071df]\r\n/usr/lib/x86_64-linux-gnu/libcuda.so.1(+0xd051b)[0x7f53cc52d51b]\r\n/usr/lib/x86_64-linux-gnu/libcuda.so.1(cuStreamCreate+0x5b)[0x7f53cc64d29b]\r\n/home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN9perftools8gputools4cuda10CUDADriver12CreateStreamEPNS1_11CudaContextEPP11CUstream_st+0x4b)[0x7f53e05e7dbb]\r\n/home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN9perftools8gputools4cuda10CUDAStream4InitEv+0x21)[0x7f53e05fd461]\r\n/home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN9perftools8gputools14StreamExecutor14AllocateStreamEPNS0_6StreamE+0x29)[0x7f53e0678eb9]\r\n/home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN9perftools8gputools6Stream4InitEv+0x178)[0x7f53e0672a58]\r\n/home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN10tensorflow13BaseGPUDevice4InitERKNS_14SessionOptionsE+0x39a)[0x7f53e04da2ca]\r\n/home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN10tensorflow20BaseGPUDeviceFactory13CreateDevicesERKNS_14SessionOptionsERKSsPSt6vectorIPNS_6DeviceESaIS8_EE+0x48d)[0x7f53e04de6fd]\r\n/home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN10tensorflow13DeviceFactory10AddDevicesERKNS_14SessionOptionsERKSsPSt6vectorIPNS_6DeviceESaIS8_EE+0x12e)[0x7f53e050751e]\r\n/home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so(+0xdf8fc8)[0x7f53dea55fc8]\r\n/home/mmakowski/anaconda3/envs/tensorflow/bin/../lib/libpython3.5m.so.1.0(PyCFunction_Call+0xf9)[0x7f540d3175e9]\r\n/home/mmakowski/anaconda3/envs/tensorflow/bin/../lib/libpython3.5m.so.1.0(PyEval_EvalFrameEx+0x8fb5)[0x7f540d39ebd5]\r\n/home/mmakowski/anaconda3/envs/tensorflow/bin/../lib/libpython3.5m.so.1.0(PyEval_EvalFrameEx+0x9546)[0x7f540d39f166]\r\n/home/mmakowski/anaconda3/envs/tensorflow/bin/../lib/libpython3.5m.so.1.0(+0x144b49)[0x7f540d39fb49]\r\n/home/mmakowski/anaconda3/envs/tensorflow/bin/../lib/libpython3.5m.so.1.0(PyEval_EvalFrameEx+0x91d5)[0x7f540d39edf5]\r\n/home/mmakowski/anaconda3/envs/tensorflow/bin/../lib/libpython3.5m.so.1.0(PyEval_EvalFrameEx+0x9546)[0x7f540d39f166]\r\n/home/mmakowski/anaconda3/envs/tensorflow/bin/../lib/libpython3.5m.so.1.0(+0x144b49)[0x7f540d39fb49]\r\n/home/mmakowski/anaconda3/envs/tensorflow/bin/../lib/libpython3.5m.so.1.0(PyEval_EvalCodeEx+0x48)[0x7f540d39fcd8]\r\n/home/mmakowski/anaconda3/envs/tensorflow/bin/../lib/libpython3.5m.so.1.0(PyEval_EvalCode+0x3b)[0x7f540d39fd1b]\r\n/home/mmakowski/anaconda3/envs/tensorflow/bin/../lib/libpython3.5m.so.1.0(PyRun_InteractiveOneObject+0x1e6)[0x7f540d3c7276]\r\n/home/mmakowski/anaconda3/envs/tensorflow/bin/../lib/libpython3.5m.so.1.0(PyRun_InteractiveLoopFlags+0x5e)[0x7f540d3c75de]\r\n/home/mmakowski/anaconda3/envs/tensorflow/bin/../lib/libpython3.5m.so.1.0(PyRun_AnyFileExFlags+0x4c)[0x7f540d3c771c]\r\n/home/mmakowski/anaconda3/envs/tensorflow/bin/../lib/libpython3.5m.so.1.0(Py_Main+0xde2)[0x7f540d3e1a02]\r\npython(main+0x15d)[0x400add]\r\n/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf1)[0x7f540c37e3f1]\r\npython[0x4008b9]\r\n======= Memory map: ========\r\n00400000-00401000 r-xp 00000000 08:02 9311838                            /home/mmakowski/anaconda3/envs/tensorflow/bin/python3.5\r\n00601000-00602000 rw-p 00001000 08:02 9311838                            /home/mmakowski/anaconda3/envs/tensorflow/bin/python3.5\r\n024ea000-03e43000 rw-p 00000000 00:00 0                                  [heap]\r\n200000000-200100000 rw-s 2d7043000 00:06 454                             /dev/nvidiactl\r\n200100000-200104000 rw-s 2d7148000 00:06 454                             /dev/nvidiactl\r\n200104000-200120000 ---p 00000000 00:00 0 \r\n200120000-200520000 rw-s 2d7155000 00:06 454                             /dev/nvidiactl\r\n200520000-200524000 rw-s 2d656c000 00:06 454                             /dev/nvidiactl\r\n200524000-200540000 ---p 00000000 00:00 0 \r\n200540000-200940000 rw-s 2d6580000 00:06 454                             /dev/nvidiactl\r\n200940000-200a40000 rw-s 2d6198000 00:06 454                             /dev/nvidiactl\r\n200a40000-200b40000 rw-s 2d62c6000 00:06 454                             /dev/nvidiactl\r\n200b40000-200c40000 rw-s 2d63d1000 00:06 454                             /dev/nvidiactl\r\n200c40000-200d40000 rw-s 2d5ccc000 00:06 454                             /dev/nvidiactl\r\n200d40000-200e40000 rw-s 33dd2a000 00:06 454                             /dev/nvidiactl\r\n200e40000-200ec2000 rw-s 2d5eb1000 00:06 454                             /dev/nvidiactl\r\n200ec2000-900000000 ---p 00000000 00:00 0 \r\n7f5378000000-7f5378021000 rw-p 00000000 00:00 0 \r\n7f5378021000-7f537c000000 ---p 00000000 00:00 0 \r\n7f537c000000-7f537c021000 rw-p 00000000 00:00 0 \r\n7f537c021000-7f5380000000 ---p 00000000 00:00 0 \r\n7f5380000000-7f5380021000 rw-p 00000000 00:00 0 \r\n7f5380021000-7f5384000000 ---p 00000000 00:00 0 \r\n7f5384000000-7f5384021000 rw-p 00000000 00:00 0 \r\n7f5384021000-7f5388000000 ---p 00000000 00:00 0 \r\n7f5388000000-7f5388021000 rw-p 00000000 00:00 0 \r\n7f5388021000-7f538c000000 ---p 00000000 00:00 0 \r\n7f538c000000-7f538c021000 rw-p 00000000 00:00 0 \r\n7f538c021000-7f5390000000 ---p 00000000 00:00 0 \r\n7f5390000000-7f5390021000 rw-p 00000000 00:00 0 \r\n7f5390021000-7f5394000000 ---p 00000000 00:00 0 \r\n7f5394033000-7f53957fb000 rw-p 00000000 00:00 0 \r\n7f53957fb000-7f53957fc000 ---p 00000000 00:00 0 \r\n7f53957fc000-7f5395ffc000 rw-p 00000000 00:00 0 \r\n7f5395ffc000-7f5395ffd000 ---p 00000000 00:00 0 \r\n7f5395ffd000-7f53967fd000 rw-p 00000000 00:00 0 \r\n7f53967fd000-7f53967fe000 ---p 00000000 00:00 0 \r\n7f53967fe000-7f5396ffe000 rw-p 00000000 00:00 0 \r\n7f5396ffe000-7f5396fff000 ---p 00000000 00:00 0 \r\n7f5396fff000-7f53977ff000 rw-p 00000000 00:00 0 \r\n7f53977ff000-7f5397800000 ---p 00000000 00:00 0 \r\n7f5397800000-7f5398000000 rw-p 00000000 00:00 0 \r\n7f5398000000-7f5398021000 rw-p 00000000 00:00 0 \r\n7f5398021000-7f539c000000 ---p 00000000 00:00 0 \r\n7f539c000000-7f539c021000 rw-p 00000000 00:00 0 \r\n7f539c021000-7f53a0000000 ---p 00000000 00:00 0 \r\n7f53a0000000-7f53a0021000 rw-p 00000000 00:00 0 \r\n7f53a0021000-7f53a4000000 ---p 00000000 00:00 0 \r\n7f53a4000000-7f53a4021000 rw-p 00000000 00:00 0 \r\n7f53a4021000-7f53a8000000 ---p 00000000 00:00 0 \r\n7f53a8000000-7f53a8021000 rw-p 00000000 00:00 0 \r\n7f53a8021000-7f53ac000000 ---p 00000000 00:00 0 \r\n7f53ac000000-7f53ac021000 rw-p 00000000 00:00 0 \r\n7f53ac021000-7f53b0000000 ---p 00000000 00:00 0 \r\n7f53b0000000-7f53b0021000 rw-p 00000000 00:00 0 \r\n7f53b0021000-7f53b4000000 ---p 00000000 00:00 0 \r\n7f53b4000000-7f53b4021000 rw-p 00000000 00:00 0 \r\n7f53b4021000-7f53b8000000 ---p 00000000 00:00 0 \r\n7f53b8000000-7f53b8021000 rw-p 00000000 00:00 0 \r\n7f53b8021000-7f53bc000000 ---p 00000000 00:00 0 \r\n7f53bc000000-7f53bc021000 rw-p 00000000 00:00 0 \r\n7f53bc021000-7f53c0000000 ---p 00000000 00:00 0 \r\n7f53c0000000-7f53c0021000 rw-p 00000000 00:00 0 \r\n7f53c0021000-7f53c4000000 ---p 00000000 00:00 0 \r\n7f53c44c1000-7f53c44c2000 ---p 00000000 00:00 0 \r\n7f53c44c2000-7f53c4cc2000 rw-p 00000000 00:00 0 \r\n7f53c4cc2000-7f53c4cc3000 ---p 00000000 00:00 0 \r\n7f53c4cc3000-7f53c54c3000 rw-p 00000000 00:00 0 \r\n7f53c54c3000-7f53c54c4000 ---p 00000000 00:00 0 \r\n7f53c54c4000-7f53c5cc4000 rw-p 00000000 00:00 0 \r\n7f53c5cc4000-7f53c5cc5000 ---p 00000000 00:00 0 \r\n7f53c5cc5000-7f53c64c5000 rw-p 00000000 00:00 0 \r\n7f53c64c5000-7f53c64c6000 ---p 00000000 00:00 0 \r\n7f53c64c6000-7f53c6cc6000 rw-p 00000000 00:00 0 \r\n7f53c6cc6000-7f53c6cc7000 r-xp 00000000 08:02 9570451                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/site-packages/google/protobuf/internal/_api_implementation.cpython-35m-x86_64-linux-gnu.so\r\n7f53c6cc7000-7f53c6ec6000 ---p 00001000 08:02 9570451                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/site-packages/google/protobuf/internal/_api_implementation.cpython-35m-x86_64-linux-gnu.so\r\n7f53c6ec6000-7f53c6ec7000 rw-p 00000000 08:02 9570451                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/site-packages/google/protobuf/internal/_api_implementation.cpython-35m-x86_64-linux-gnu.so\r\n7f53c6ec7000-7f53c7107000 rw-p 00000000 00:00 0 \r\n7f53c7107000-7f53c7116000 r-xp 00000000 08:02 9972711                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_socket.so\r\n7f53c7116000-7f53c7316000 ---p 0000f000 08:02 9972711                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_socket.so\r\n7f53c7316000-7f53c731b000 rw-p 0000f000 08:02 9972711                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_socket.so\r\n7f53c731b000-7f53c735b000 rw-p 00000000 00:00 0 \r\n7f53c735b000-7f53c738f000 r-xp 00000000 08:02 9972731                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/pyexpat.so\r\n7f53c738f000-7f53c758f000 ---p 00034000 08:02 9972731                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/pyexpat.so\r\n7f53c758f000-7f53c7593000 rw-p 00034000 08:02 9972731                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/pyexpat.so\r\n7f53c7593000-7f53c75d3000 rw-p 00000000 00:00 0 \r\n7f53c75d3000-7f53c75d8000 r-xp 00000000 08:02 9972734                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/select.so\r\n7f53c75d8000-7f53c77d7000 ---p 00005000 08:02 9972734                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/select.so\r\n7f53c77d7000-7f53c77d9000 rw-p 00004000 08:02 9972734                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/select.so\r\n7f53c77d9000-7f53c77dc000 r-xp 00000000 08:02 9972706                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_posixsubprocess.so\r\n7f53c77dc000-7f53c79db000 ---p 00003000 08:02 9972706                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_posixsubprocess.so\r\n7f53c79db000-7f53c79dc000 rw-p 00002000 08:02 9972706                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_posixsubprocess.so\r\n7f53c79dc000-7f53c79e2000 r-xp 00000000 08:02 9972740                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/zlib.so\r\n7f53c79e2000-7f53c7be2000 ---p 00006000 08:02 9972740                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/zlib.so\r\n7f53c7be2000-7f53c7be4000 rw-p 00006000 08:02 9972740                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/zlib.so\r\n7f53c7be4000-7f53c7bfa000 r-xp 00000000 08:02 9054830                    /home/mmakowski/anaconda3/envs/tensorflow/lib/libz.so.1.2.8\r\n7f53c7bfa000-7f53c7df9000 ---p 00016000 08:02 9054830                    /home/mmakowski/anaconda3/envs/tensorflow/lib/libz.so.1.2.8\r\n7f53c7df9000-7f53c7dfa000 rw-p 00015000 08:02 9054830                    /home/mmakowski/anaconda3/envs/tensorflow/lib/libz.so.1.2.8\r\n7f53c7dfa000-7f53c7dff000 r-xp 00000000 08:02 9972722                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/binascii.so\r\n7f53c7dff000-7f53c7fff000 ---p 00005000 08:02 9972722                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/binascii.so\r\n7f53c7fff000-7f53c8000000 rw-p 00005000 08:02 9972722                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/binascii.so\r\n7f53c8000000-7f53c8200000 rw-p 00000000 00:00 0 \r\n7f53c8228000-7f53c82a8000 rw-p 00000000 00:00 0 \r\n7f53c82a8000-7f53ca736000 r-xp 00000000 08:02 4859634                    /usr/lib/x86_64-linux-gnu/libcurand.so.8.0.44\r\n7f53ca736000-7f53ca936000 ---p 0248e000 08:02 4859634                    /usr/lib/x86_64-linux-gnu/libcurand.so.8.0.44\r\n7f53ca936000-7f53cbd07000 rw-p 0248e000 08:02 4859634                    /usr/lib/x86_64-linux-gnu/libcurand.so.8.0.44\r\n7f53cbd07000-7f53cc211000 rw-p 00000000 00:00 0 \r\n7f53cc211000-7f53cc253000 r-xp 00000000 08:02 4861323                    /usr/lib/nvidia-375/libnvidia-fatbinaryloader.so.375.39\r\n7f53cc253000-7f53cc452000 ---p 00042000 08:02 4861323                    /usr/lib/nvidia-375/libnvidia-fatbinaryloader.so.375.39\r\n7f53cc452000-7f53cc45c000 rw-p 00041000 08:02 4861323                    /usr/lib/nvidia-375/libnvidia-fatbinaryloader.so.375.39\r\n7f53cc45c000-7f53cc45d000 rw-p 00000000 00:00 0 \r\n7f53cc45d000-7f53ccb2b000 r-xp 00000000 08:02 4850112                    /usr/lib/x86_64-linux-gnu/libcuda.so.375.39\r\n7f53ccb2b000-7f53ccd2a000 ---p 006ce000 08:02 4850112                    /usr/lib/x86_64-linux-gnu/libcuda.so.375.39\r\n7f53ccd2a000-7f53cce46000 rw-p 006cd000 08:02 4850112                    /usr/lib/x86_64-linux-gnu/libcuda.so.375.39\r\n7f53cce46000-7f53cce52000 rw-p 00000000 00:00 0 \r\n7f53cce52000-7f53d5a3a000 r-xp 00000000 08:02 4859540                    /usr/lib/x86_64-linux-gnu/libcufft.so.8.0.44\r\n7f53d5a3a000-7f53d5c39000 ---p 08be8000 08:02 4859540                    /usr/lib/x86_64-linux-gnu/libcufft.so.8.0.44\r\n7f53d5c39000-7f53d5c48000 rw-p 08be7000 08:02 4859540                    /usr/lib/x86_64-linux-gnu/libcufft.so.8.0.44\r\n7f53d5c48000-7f53d5ca0000 rw-p 00000000 00:00 0 \r\n7f53d5ca0000-7f53da82d000 r-xp 00000000 08:02 4862601                    /usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.5\r\n7f53da82d000-7f53daa2c000 ---p 04b8d000 08:02 4862601                    /usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.5\r\n7f53daa2c000-7f53daa4a000 rw-p 04b8c000 08:02 4862601                    /usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.5\r\n7f53daa4a000-7f53daa74000 rw-p 00000000 00:00 0 \r\n7f53daa74000-7f53dd1f9000 r-xp 00000000 08:02 4850666                    /usr/lib/x86_64-linux-gnu/libcublas.so.8.0.45\r\n7f53dd1f9000-7f53dd3f8000 ---p 02785000 08:02 4850666                    /usr/lib/x86_64-linux-gnu/libcublas.so.8.0.45\r\n7f53dd3f8000-7f53dd416000 rw-p 02784000 08:02 4850666                    /usr/lib/x86_64-linux-gnu/libcublas.so.8.0.45\r\n7f53dd416000-7f53dd424000 rw-p 00000000 00:00 0 \r\n7f53dd424000-7f53dd43a000 r-xp 00000000 08:02 10621433                   /home/mmakowski/anaconda3/envs/tensorflow/lib/libgcc_s.so.1\r\n7f53dd43a000-7f53dd639000 ---p 00016000 08:02 10621433                   /home/mmakowski/anaconda3/envs/tensorflow/lib/libgcc_s.so.1\r\n7f53dd639000-7f53dd63a000 rw-p 00015000 08:02 10621433                   /home/mmakowski/anaconda3/envs/tensorflow/lib/libgcc_s.so.1\r\n7f53dd63a000-7f53dd63b000 rw-p 00074000 08:02 10621433                   /home/mmakowski/anaconda3/envs/tensorflow/lib/libgcc_s.so.1\r\n7f53dd63b000-7f53dd7a6000 r-xp 00000000 08:02 10621922                   /home/mmakowski/anaconda3/envs/tensorflow/lib/libstdc++.so.6.0.21\r\n7f53dd7a6000-7f53dd9a6000 ---p 0016b000 08:02 10621922                   /home/mmakowski/anaconda3/envs/tensorflow/lib/libstdc++.so.6.0.21\r\n7f53dd9a6000-7f53dd9b0000 r--p 0016b000 08:02 10621922                   /home/mmakowski/anaconda3/envs/tensorflow/lib/libstdc++.so.6.0.21\r\n7f53dd9b0000-7f53dd9b2000 rw-p 00175000 08:02 10621922                   /home/mmakowski/anaconda3/envs/tensorflow/lib/libstdc++.so.6.0.21\r\n7f53dd9b2000-7f53dd9b6000 rw-p 00000000 00:00 0 \r\n7f53dd9b6000-7f53dd9f7000 rw-p 00178000 08:02 10621922                   /home/mmakowski/anaconda3/envs/tensorflow/lib/libstdc++.so.6.0.21\r\n7f53dd9f7000-7f53dda59000 r-xp 00000000 08:02 4859534                    /usr/lib/x86_64-linux-gnu/libcudart.so.8.0.44\r\n7f53dda59000-7f53ddc59000 ---p 00062000 08:02 4859534                    /usr/lib/x86_64-linux-gnu/libcudart.so.8.0.44\r\n7f53ddc59000-7f53ddc5c000 rw-p 00062000 08:02 4859534                    /usr/lib/x86_64-linux-gnu/libcudart.so.8.0.44\r\n7f53ddc5c000-7f53ddc5d000 rw-p 00000000 00:00 0 \r\n7f53ddc5d000-7f53e9b1a000 r-xp 00000000 08:02 9972949                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n7f53e9b1a000-7f53e9d1a000 ---p 0bebd000 08:02 9972949                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n7f53e9d1a000-7f53e9e0c000 r--p 0bebd000 08:02 9972949                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n7f53e9e0c000-7f53e9e13000 rw-p 0bfaf000 08:02 9972949                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n7f53e9e13000-7f53e9eb0000 rw-p 00000000 00:00 0 \r\n7f53e9eb0000-7f53e9f68000 r-xp 00000000 08:02 7623074                    /home/mmakowski/.local/lib/python3.5/site-packages/numpy/random/mtrand.cpython-35m-x86_64-linux-gnu.so\r\n7f53e9f68000-7f53ea167000 ---p 000b8000 08:02 7623074                    /home/mmakowski/.local/lib/python3.5/site-packages/numpy/random/mtrand.cpython-35m-x86_64-linux-gnu.so\r\n7f53ea167000-7f53ea18c000 rw-p 000b7000 08:02 7623074                    /home/mmakowski/.local/lib/python3.5/site-packages/numpy/random/mtrand.cpython-35m-x86_64-linux-gnu.so\r\n7f53ea18c000-7f53ea1ce000 rw-p 00000000 00:00 0 \r\n7f53ea1ce000-7f53ea1d7000 r-xp 00000000 08:02 7622830                    /home/mmakowski/.local/lib/python3.5/site-packages/numpy/fft/fftpack_lite.cpython-35m-x86_64-linux-gnu.so\r\n7f53ea1d7000-7f53ea3d6000 ---p 00009000 08:02 7622830                    /home/mmakowski/.local/lib/python3.5/site-packages/numpy/fft/fftpack_lite.cpython-35m-x86_64-linux-gnu.so\r\n7f53ea3d6000-7f53ea3d7000 rw-p 00008000 08:02 7622830                    /home/mmakowski/.local/lib/python3.5/site-packages/numpy/fft/fftpack_lite.cpython-35m-x86_64-linux-gnu.so\r\n7f53ea3d7000-7f53ea457000 rw-p 00000000 00:00 0 \r\n7f53ea457000-7f53ea480000 r-xp 00000000 08:02 7622130                    /home/mmakowski/.local/lib/python3.5/site-packages/numpy/linalg/_umath_linalg.cpython-35m-x86_64-linux-gnu.so\r\n7f53ea480000-7f53ea67f000 ---p 00029000 08:02 7622130                    /home/mmakowski/.local/lib/python3.5/site-packages/numpy/linalg/_umath_linalg.cpython-35m-x86_64-linux-gnu.so\r\n7f53ea67f000-7f53ea681000 rw-p 00028000 08:02 7622130                    /home/mmakowski/.local/lib/python3.5/site-packages/numpy/linalg/_umath_linalg.cpython-35m-x86_64-linux-gnu.so\r\n7f53ea681000-7f53ea683000 rw-p 000c3000 08:02 7622130                    /home/mmakowski/.local/lib/python3.5/site-packages/numpy/linalg/_umath_linalg.cpython-35m-x86_64-linux-gnu.so\r\n7f53ea683000-7f53ea687000 r-xp 00000000 08:02 7622119                    /home/mmakowski/.local/lib/python3.5/site-packages/numpy/linalg/lapack_lite.cpython-35m-x86_64-linux-gnu.so\r\n7f53ea687000-7f53ea887000 ---p 00004000 08:02 7622119                    /home/mmakowski/.local/lib/python3.5/site-packages/numpy/linalg/lapack_lite.cpython-35m-x86_64-linux-gnu.so\r\n7f53ea887000-7f53ea888000 rw-p 00004000 08:02 7622119                    /home/mmakowski/.local/lib/python3.5/site-packages/numpy/linalg/lapack_lite.cpython-35m-x86_64-linux-gnu.so\r\n7f53ea888000-7f53ea88a000 rw-p 00019000 08:02 7622119                    /home/mmakowski/.local/lib/python3.5/site-packages/numpy/linalg/lapack_lite.cpython-35m-x86_64-linux-gnu.so\r\n7f53ea94b000-7f53ea94e000 r-xp 00000000 08:02 9972707                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_random.so\r\n7f53ea94e000-7f53eab4d000 ---p 00003000 08:02 9972707                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_random.so\r\n7f53eab4d000-7f53eab4e000 rw-p 00002000 08:02 9972707                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_random.so\r\n7f53eab4e000-7f53ead5a000 r-xp 00000000 08:02 9311604                    /home/mmakowski/anaconda3/envs/tensorflow/lib/libcrypto.so.1.0.0\r\n7f53ead5a000-7f53eaf5a000 ---p 0020c000 08:02 9311604                    /home/mmakowski/anaconda3/envs/tensorflow/lib/libcrypto.so.1.0.0\r\n7f53eaf5a000-7f53eaf81000 rw-p 0020c000 08:02 9311604                    /home/mmakowski/anaconda3/envs/tensorflow/lib/libcrypto.so.1.0.0\r\n7f53eaf81000-7f53eaf85000 rw-p 00000000 00:00 0 \r\n7f53eaf85000-7f53eaff2000 r-xp 00000000 08:02 9055575                    /home/mmakowski/anaconda3/envs/tensorflow/lib/libssl.so.1.0.0\r\n7f53eaff2000-7f53eb1f1000 ---p 0006d000 08:02 9055575                    /home/mmakowski/anaconda3/envs/tensorflow/lib/libssl.so.1.0.0\r\n7f53eb1f1000-7f53eb1fc000 rw-p 0006c000 08:02 9055575                    /home/mmakowski/anaconda3/envs/tensorflow/lib/libssl.so.1.0.0\r\n7f53eb1fc000-7f53eb201000 r-xp 00000000 08:02 9972695                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_hashlib.so\r\n7f53eb201000-7f53eb401000 ---p 00005000 08:02 9972695                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_hashlib.so\r\n7f53eb401000-7f53eb402000 rw-p 00005000 08:02 9972695                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_hashlib.so\r\n7f53eb402000-7f53eb442000 rw-p 00000000 00:00 0 \r\n7f53eb442000-7f53eb466000 r-xp 00000000 08:02 9315096                    /home/mmakowski/anaconda3/envs/tensorflow/lib/liblzma.so.5.2.2\r\n7f53eb466000-7f53eb666000 ---p 00024000 08:02 9315096                    /home/mmakowski/anaconda3/envs/tensorflow/lib/liblzma.so.5.2.2\r\n7f53eb666000-7f53eb667000 rw-p 00024000 08:02 9315096                    /home/mmakowski/anaconda3/envs/tensorflow/lib/liblzma.so.5.2.2\r\n7f53eb667000-7f53eb66d000 r-xp 00000000 08:02 9972700                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_lzma.so\r\n7f53eb66d000-7f53eb86d000 ---p 00006000 08:02 9972700                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_lzma.so\r\n7f53eb86d000-7f53eb86f000 rw-p 00006000 08:02 9972700                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_lzma.so\r\n7f53eb86f000-7f53eb880000 r-xp 00000000 08:02 9972670                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_bz2.so\r\n7f53eb880000-7f53eba80000 ---p 00011000 08:02 9972670                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_bz2.so\r\n7f53eba80000-7f53eba82000 rw-p 00011000 08:02 9972670                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_bz2.so\r\n7f53eba82000-7f53eba84000 r-xp 00000000 08:02 9972725                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/grp.so\r\n7f53eba84000-7f53ebc84000 ---p 00002000 08:02 9972725                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/grp.so\r\n7f53ebc84000-7f53ebc85000 rw-p 00002000 08:02 9972725                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/grp.so\r\n7f53ebc85000-7f53ebe45000 rw-p 00000000 00:00 0 \r\n7f53ebe45000-7f53ebe5c000 r-xp 00000000 08:02 9972705                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_pickle.so\r\n7f53ebe5c000-7f53ec05c000 ---p 00017000 08:02 9972705                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_pickle.so\r\n7f53ec05c000-7f53ec060000 rw-p 00017000 08:02 9972705                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_pickle.so\r\n7f53ec060000-7f53ec0e0000 rw-p 00000000 00:00 0 \r\n7f53ec0e0000-7f53ec26e000 r-xp 00000000 08:02 7622228                    /home/mmakowski/.local/lib/python3.5/site-packages/numpy/core/umath.cpython-35m-x86_64-linux-gnu.so\r\n7f53ec26e000-7f53ec46d000 ---p 0018e000 08:02 7622228                    /home/mmakowski/.local/lib/python3.5/site-packages/numpy/core/umath.cpython-35m-x86_64-linux-gnu.so\r\n7f53ec46d000-7f53ec473000 rw-p 0018d000 08:02 7622228                    /home/mmakowski/.local/lib/python3.5/site-packages/numpy/core/umath.cpython-35m-x86_64-linux-gnu.so\r\n7f53ec473000-7f53ec475000 rw-p 00000000 00:00 0 \r\n7f53ec475000-7f53ec486000 r-xp 00000000 08:02 9972691                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_datetime.so\r\n7f53ec486000-7f53ec685000 ---p 00011000 08:02 9972691                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_datetime.so\r\n7f53ec685000-7f53ec688000 rw-p 00010000 08:02 9972691                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_datetime.so\r\n7f53ec688000-7f53ec6c8000 rw-p 00000000 00:00 0 \r\n7f53ec6c8000-7f53fa6c8000 rw-p 00000000 00:00 0 \r\n7f53fa6c8000-7f53fa6c9000 ---p 00000000 00:00 0 \r\n7f53fa6c9000-7f53faec9000 rw-p 00000000 00:00 0 \r\n7f53faec9000-7f53faeca000 ---p 00000000 00:00 0 \r\n7f53faeca000-7f53fb6ca000 rw-p 00000000 00:00 0 \r\n7f53fb6ca000-7f53fb6cb000 ---p 00000000 00:00 0 \r\n7f53fb6cb000-7f53fbecb000 rw-p 00000000 00:00 0 \r\n7f53fbecb000-7f53fbecc000 ---p 00000000 00:00 0 \r\n7f53fbecc000-7f53fc6cc000 rw-p 00000000 00:00 0 \r\n7f53fc6cc000-7f53fc6cd000 ---p 00000000 00:00 0 \r\n7f53fc6cd000-7f53fcecd000 rw-p 00000000 00:00 0 \r\n7f53fcecd000-7f53fcece000 ---p 00000000 00:00 0 \r\n7f53fcece000-7f53fd6ce000 rw-p 00000000 00:00 0 \r\n7f53fd6ce000-7f53ff6ce000 rw-p 00000000 00:00 0 \r\n7f53ff6ce000-7f53ff6cf000 ---p 00000000 00:00 0 \r\n7f53ff6cf000-7f53ffecf000 rw-p 00000000 00:00 0 \r\n7f53ffecf000-7f5405ecf000 rw-p 00000000 00:00 0 \r\n7f5406000000-7f5406200000 rw-p 00000000 00:00 0 \r\n7f5406360000-7f5406361000 ---p 00000000 00:00 0 \r\n7f5406361000-7f5406ca1000 rw-p 00000000 00:00 0 \r\n7f5406ca1000-7f5406caa000 r-xp 00000000 08:02 9972698                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_json.so\r\n7f5406caa000-7f5406eaa000 ---p 00009000 08:02 9972698                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_json.so\r\n7f5406eaa000-7f5406eab000 rw-p 00009000 08:02 9972698                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_json.so\r\n7f5406eab000-7f54078ec000 rw-p 00000000 00:00 0 \r\n7f54078ec000-7f5407abf000 r-xp 00000000 08:02 9570583                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/site-packages/google/protobuf/pyext/_message.cpython-35m-x86_64-linux-gnu.so\r\n7f5407abf000-7f5407cbe000 ---p 001d3000 08:02 9570583                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/site-packages/google/protobuf/pyext/_message.cpython-35m-x86_64-linux-gnu.so\r\n7f5407cbe000-7f5407ccd000 rw-p 001d2000 08:02 9570583                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/site-packages/google/protobuf/pyext/_message.cpython-35m-x86_64-linux-gnu.so\r\n7f5407ccd000-7f5407cce000 rw-p 00000000 00:00 0 \r\n7f5407cce000-7f5407cd2000 r-xp 00000000 08:02 10486480                   /lib/x86_64-linux-gnu/libuuid.so.1.3.0\r\n7f5407cd2000-7f5407ed1000 ---p 00004000 08:02 10486480                   /lib/x86_64-linux-gnu/libuuid.so.1.3.0\r\n7f5407ed1000-7f5407ed2000 r--p 00003000 08:02 10486480                   /lib/x86_64-linux-gnu/libuuid.so.1.3.0\r\n7f5407ed2000-7f5407ed3000 rw-p 00004000 08:02 10486480                   /lib/x86_64-linux-gnu/libuuid.so.1.3.0\r\n7f5407ed3000-7f5407fc3000 r-xp 00000000 08:02 7622696                    /home/mmakowski/.local/lib/python3.5/site-packages/numpy/.libs/libgfortran-ed201abd.so.3.0.0\r\n7f5407fc3000-7f54081c2000 ---p 000f0000 08:02 7622696                    /home/mmakowski/.local/lib/python3.5/site-packages/numpy/.libs/libgfortran-ed201abd.so.3.0.0\r\n7f54081c2000-7f54081c4000 rw-p 000ef000 08:02 7622696                    /home/mmakowski/.local/lib/python3.5/site-packages/numpy/.libs/libgfortran-ed201abd.so.3.0.0\r\n7f54081c4000-7f54081c5000 rw-p 00000000 00:00 0 \r\n7f54081c5000-7f54081cc000 rw-p 000f2000 08:02 7622696                    /home/mmakowski/.local/lib/python3.5/site-packages/numpy/.libs/libgfortran-ed201abd.so.3.0.0\r\n7f54081cc000-7f540a4e0000 r-xp 00000000 08:02 7622694                    /home/mmakowski/.local/lib/python3.5/site-packages/numpy/.libs/libopenblasp-r0-39a31c03.2.18.so\r\n7f540a4e0000-7f540a6df000 ---p 02314000 08:02 7622694                    /home/mmakowski/.local/lib/python3.5/site-packages/numpy/.libs/libopenblasp-r0-39a31c03.2.18.so\r\n7f540a6df000-7f540a6fe000 rw-p 02313000 08:02 7622694                    /home/mmakowski/.local/lib/python3.5/site-packages/numpy/.libs/libopenblasp-r0-39a31c03.2.18.so\r\n7f540a6fe000-7f540a761000 rw-p 00000000 00:00 0 \r\n7f540a761000-7f540a7dd000 rw-p 02425000 08:02 7622694                    /home/mmakowski/.local/lib/python3.5/site-packages/numpy/.libs/libopenblasp-r0-39a31c03.2.18.so\r\n7f540a7dd000-7f540a9ab000 r-xp 00000000 08:02 7622227                    /home/mmakowski/.local/lib/python3.5/site-packages/numpy/core/multiarray.cpython-35m-x86_64-linux-gnu.so\r\n7f540a9ab000-7f540abab000 ---p 001ce000 08:02 7622227                    /home/mmakowski/.local/lib/python3.5/site-packages/numpy/core/multiarray.cpython-35m-x86_64-linux-gnu.so\r\n7f540abab000-7f540abba000 rw-p 001ce000 08:02 7622227                    /home/mmakowski/.local/lib/python3.5/site-packages/numpy/core/multiarray.cpython-35m-x86_64-linux-gnu.so\r\n7f540abba000-7f540abcc000 rw-p 00000000 00:00 0 \r\n7f540abcc000-7f540abd2000 rw-p 0097f000 08:02 7622227                    /home/mmakowski/.local/lib/python3.5/site-packages/numpy/core/multiarray.cpython-35m-x86_64-linux-gnu.so\r\n7f540abd2000-7f540abda000 r-xp 00000000 08:02 9972726                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/math.so\r\n7f540abda000-7f540adda000 ---p 00008000 08:02 9972726                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/math.so\r\n7f540adda000-7f540addc000 rw-p 00008000 08:02 9972726                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/math.so\r\n7f540addc000-7f540ae1c000 rw-p 00000000 00:00 0 \r\n7f540ae3d000-7f540ae7d000 rw-p 00000000 00:00 0 \r\n7f540ae7d000-7f540ae7e000 r-xp 00000000 08:02 9972704                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_opcode.so\r\n7f540ae7e000-7f540b07d000 ---p 00001000 08:02 9972704                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_opcode.so\r\n7f540b07d000-7f540b07e000 rw-p 00000000 08:02 9972704                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_opcode.so\r\n7f540b07e000-7f540b0fe000 rw-p 00000000 00:00 0 \r\n7f540b0fe000-7f540b106000 r-xp 00000000 08:02 9972714                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_struct.so\r\n7f540b106000-7f540b305000 ---p 00008000 08:02 9972714                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_struct.so\r\n7f540b305000-7f540b308000 rw-p 00007000 08:02 9972714                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_struct.so\r\n7f540b308000-7f540b328000 r-xp 00000000 08:02 9972685                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_ctypes.so\r\n7f540b328000-7f540b527000 ---p 00020000 08:02 9972685                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_ctypes.so\r\n7f540b527000-7f540b52c000 rw-p 0001f000 08:02 9972685                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_ctypes.so\r\n7f540b52c000-7f540b551000 r-xp 00000000 08:02 10489680                   /lib/x86_64-linux-gnu/libtinfo.so.5.9\r\n7f540b551000-7f540b750000 ---p 00025000 08:02 10489680                   /lib/x86_64-linux-gnu/libtinfo.so.5.9\r\n7f540b750000-7f540b754000 r--p 00024000 08:02 10489680                   /lib/x86_64-linux-gnu/libtinfo.so.5.9\r\n7f540b754000-7f540b755000 rw-p 00028000 08:02 10489680                   /lib/x86_64-linux-gnu/libtinfo.so.5.9\r\n7f540b755000-7f540b782000 r-xp 00000000 08:02 10486924                   /lib/x86_64-linux-gnu/libncursesw.so.5.9\r\n7f540b782000-7f540b982000 ---p 0002d000 08:02 10486924                   /lib/x86_64-linux-gnu/libncursesw.so.5.9\r\n7f540b982000-7f540b983000 r--p 0002d000 08:02 10486924                   /lib/x86_64-linux-gnu/libncursesw.so.5.9\r\n7f540b983000-7f540b984000 rw-p 0002e000 08:02 10486924                   /lib/x86_64-linux-gnu/libncursesw.so.5.9\r\n7f540b9b9000-7f540b9f1000 r-xp 00000000 08:02 9048574                    /home/mmakowski/anaconda3/envs/tensorflow/lib/libreadline.so.6.2\r\n7f540b9f1000-7f540bbf1000 ---p 00038000 08:02 9048574                    /home/mmakowski/anaconda3/envs/tensorflow/lib/libreadline.so.6.2\r\n7f540bbf1000-7f540bbf9000 rw-p 00038000 08:02 9048574                    /home/mmakowski/anaconda3/envs/tensorflow/lib/libreadline.so.6.2\r\n7f540bbf9000-7f540bbfa000 rw-p 00000000 00:00 0 \r\n7f540bbfa000-7f540bbff000 r-xp 00000000 08:02 9972732                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/readline.so\r\n7f540bbff000-7f540bdff000 ---p 00005000 08:02 9972732                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/readline.so\r\n7f540bdff000-7f540be01000 rw-p 00005000 08:02 9972732                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/readline.so\r\n7f540be01000-7f540be41000 rw-p 00000000 00:00 0 \r\n7f540be41000-7f540be43000 r-xp 00000000 08:02 9972696                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_heapq.so\r\n7f540be43000-7f540c043000 ---p 00002000 08:02 9972696                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_heapq.so\r\n7f540c043000-7f540c045000 rw-p 00002000 08:02 9972696                    /home/mmakowski/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload/_heapq.so\r\n7f540c045000-7f540c085000 rw-p 00000000 00:00 0 \r\n7f540c085000-7f540c35e000 r--p 00000000 08:02 4852825                    /usr/lib/locale/locale-archive\r\n7f540c35e000-7f540c51b000 r-xp 00000000 08:02 10486940                   /lib/x86_64-linux-gnu/libc-2.24.so\r\n7f540c51b000-7f540c71b000 ---p 001bd000 08:02 10486940                   /lib/x86_64-linux-gnu/libc-2.24.so\r\n7f540c71b000-7f540c71f000 r--p 001bd000 08:02 10486940                   /lib/x86_64-linux-gnu/libc-2.24.so\r\n7f540c71f000-7f540c721000 rw-p 001c1000 08:02 10486940                   /lib/x86_64-linux-gnu/libc-2.24.so\r\n7f540c721000-7f540c725000 rw-p 00000000 00:00 0 \r\n7f540c725000-7f540c82d000 r-xp 00000000 08:02 10489734                   /lib/x86_64-linux-gnu/libm-2.24.so\r\n7f540c82d000-7f540ca2c000 ---p 00108000 08:02 10489734                   /lib/x86_64-linux-gnu/libm-2.24.so\r\n7f540ca2c000-7f540ca2d000 r--p 00107000 08:02 10489734                   /lib/x86_64-linux-gnu/libm-2.24.so\r\n7f540ca2d000-7f540ca2e000 rw-p 00108000 08:02 10489734                   /lib/x86_64-linux-gnu/libm-2.24.so\r\n7f540ca2e000-7f540ca35000 r-xp 00000000 08:02 10490209                   /lib/x86_64-linux-gnu/librt-2.24.so\r\n7f540ca35000-7f540cc34000 ---p 00007000 08:02 10490209                   /lib/x86_64-linux-gnu/librt-2.24.so\r\n7f540cc34000-7f540cc35000 r--p 00006000 08:02 10490209                   /lib/x86_64-linux-gnu/librt-2.24.so\r\n7f540cc35000-7f540cc36000 rw-p 00007000 08:02 10490209                   /lib/x86_64-linux-gnu/librt-2.24.so\r\n7f540cc36000-7f540cc38000 r-xp 00000000 08:02 10490217                   /lib/x86_64-linux-gnu/libutil-2.24.so\r\n7f540cc38000-7f540ce37000 ---p 00002000 08:02 10490217                   /lib/x86_64-linux-gnu/libutil-2.24.so\r\n7f540ce37000-7f540ce38000 r--p 00001000 08:02 10490217                   /lib/x86_64-linux-gnu/libutil-2.24.so\r\n7f540ce38000-7f540ce39000 rw-p 00002000 08:02 10490217                   /lib/x86_64-linux-gnu/libutil-2.24.so\r\n7f540ce39000-7f540ce3c000 r-xp 00000000 08:02 10489732                   /lib/x86_64-linux-gnu/libdl-2.24.so\r\n7f540ce3c000-7f540d03b000 ---p 00003000 08:02 10489732                   /lib/x86_64-linux-gnu/libdl-2.24.so\r\n7f540d03b000-7f540d03c000 r--p 00002000 08:02 10489732                   /lib/x86_64-linux-gnu/libdl-2.24.so\r\n7f540d03c000-7f540d03d000 rw-p 00003000 08:02 10489732                   /lib/x86_64-linux-gnu/libdl-2.24.so\r\n7f540d03d000-7f540d055000 r-xp 00000000 08:02 10490205                   /lib/x86_64-linux-gnu/libpthread-2.24.so\r\n7f540d055000-7f540d255000 ---p 00018000 08:02 10490205                   /lib/x86_64-linux-gnu/libpthread-2.24.so\r\n7f540d255000-7f540d256000 r--p 00018000 08:02 10490205                   /lib/x86_64-linux-gnu/libpthread-2.24.so\r\n7f540d256000-7f540d257000 rw-p 00019000 08:02 10490205                   /lib/x86_64-linux-gnu/libpthread-2.24.so\r\n7f540d257000-7f540d25b000 rw-p 00000000 00:00 0 \r\n7f540d25b000-7f540d4b5000 r-xp 00000000 08:02 9190569                    /home/mmakowski/anaconda3/envs/tensorflow/lib/libpython3.5m.so.1.0\r\n7f540d4b5000-7f540d6b4000 ---p 0025a000 08:02 9190569                    /home/mmakowski/anaconda3/envs/tensorflow/lib/libpython3.5m.so.1.0\r\n7f540d6b4000-7f540d71b000 rw-p 00259000 08:02 9190569                    /home/mmakowski/anaconda3/envs/tensorflow/lib/libpython3.5m.so.1.0\r\n7f540d71b000-7f540d74b000 rw-p 00000000 00:00 0 \r\n7f540d74b000-7f540d770000 r-xp 00000000 08:02 10486928                   /lib/x86_64-linux-gnu/ld-2.24.so\r\n7f540d773000-7f540d937000 rw-p 00000000 00:00 0 \r\n7f540d95f000-7f540d960000 rw-p 00000000 00:00 0 \r\n7f540d960000-7f540d961000 rw-s f7de4000 00:06 455                        /dev/nvidia0\r\n7f540d961000-7f540d962000 rw-s 2d6194000 00:06 454                       /dev/nvidiactl\r\n7f540d962000-7f540d963000 rw-s f7de4000 00:06 455                        /dev/nvidia0\r\n7f540d963000-7f540d964000 rw-s 2d6569000 00:06 454                       /dev/nvidiactl\r\n7f540d964000-7f540d965000 rwxp 00000000 00:00 0 \r\n7f540d965000-7f540d96c000 r--s 00000000 08:02 5132810                    /usr/lib/x86_64-linux-gnu/gconv/gconv-modules.cache\r\n7f540d96c000-7f540d96f000 rw-p 00000000 00:00 0 \r\n7f540d96f000-7f540d970000 r--p 00024000 08:02 10486928                   /lib/x86_64-linux-gnu/ld-2.24.so\r\n7f540d970000-7f540d971000 rw-p 00025000 08:02 10486928                   /lib/x86_64-linux-gnu/ld-2.24.so\r\n7f540d971000-7f540d972000 rw-p 00000000 00:00 0 \r\n7ffd6beec000-7ffd6bf0e000 rw-p 00000000 00:00 0                          [stack]\r\n7ffd6bfd4000-7ffd6bfd6000 r--p 00000000 00:00 0                          [vvar]\r\n7ffd6bfd6000-7ffd6bfd8000 r-xp 00000000 00:00 0                          [vdso]\r\nffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]\r\nAborted (core dumped)\r\n```\r\n", "Since both vectorAdd and tf crashed after wakeup, I consider this a Cuda driver issue. TensorFlow holds an active context the entire time. And it seems that the Cuda driver has a problem restoring the active context after the wakeup. ", "Thanks for the response, @zheng-xq. I confirmed that if the python session is shut down before the PC is put to sleep and then restarted after the wake up, the problem does not occur, so the issue indeed seems to have something to do with the cuda context being active when the system is suspended. My work-around for now is to restart Jupyter kernel (I use TF mostly from Jupyter notebooks) before suspending the PC. I will try to file a bug against cuda.", "Hello all!\r\nI was running in the same issue and finally managed to make it work after resume without rebooting the computer.\r\n\r\nYou just need to `rmmod` the `nvidia` module (and all dependent modules, in my case `nvidia_uvm`) and then `modprobe` them again (in reverse order):\r\n\r\n```\r\nsudo rmmod nvidia_uvm\r\nsudo rmmod nvidia\r\nsudo modprobe nvidia\r\nsudo modprobe nvidia_uvm\r\n```\r\n\r\nHope this helps :)", "Thanks @pierrekilly. Unfortunately this does not work for me (\"module is in use\") -- probably due to the GPU being the only graphics card in my system.", "@mmakowski I had this message because I didn't stop my Jupyter notebook server. Make sure you stop everything that could hold the GPU before doing this. I have an hybrid graphic card, so the `module is in use` problem might also be due to the GPU beeing the only graphic card, as you say, but it's worth a try :)", "I'm having this problem too, and I think it appeared after upgrading TF to 1.1... haven't touched nvidia drivers in a long time. I definitely didn't have that problem before.\r\n\r\nTrying `rmmod nvidia`, I get `rmmod: ERROR: Module nvidia is in use by: nvidia_modeset` so now I just have to reboot dammit.\r\n\r\nEDIT: also, restarting notebook kernel before sleep didn't help for me.", "@harpone Try like this:\r\n```\r\nsudo rmmod nvidia_modeset\r\nsudo rmmod nvidia\r\nsudo modprobe nvidia\r\nsudo modprobe nvidia_modeset\r\n```\r\nYou have to remove the modules that use the `nvidia` one before beeing able to remove it.", "Thanks @pierrekilly ! That worked for me.\r\nI was able to get around the issue by running the following script after my devbox wakes up.\r\n\r\n```\r\nsudo rmmod nvidia_uvm\r\nsudo rmmod nvidia_drm\r\nsudo rmmod nvidia_modeset\r\nsudo rmmod nvidia\r\nsudo modprobe nvidia\r\nsudo modprobe nvidia_modeset\r\nsudo modprobe nvidia_drm\r\nsudo modprobe nvidia_uvm\r\n```", "Is there a solution to this problem for people that only have one gpu and it is being used as a display?", "A solution that works for me is a systemd service that kills all CUDA consuming processes and restarts `nvidia-docker` (since I use tensorflow inside docker conteiner).\r\n\r\n```\r\n$ nvidia-smi -q | grep \"Driver Version\"\r\n\r\nDriver Version: 387.12\r\n```\r\n\r\nAnd the service itself:\r\n```\r\n[Unit]\r\nDescription=Kill processes with initialized CUDA before sleep\r\nBefore=sleep.target\r\nStopWhenUnneeded=yes\r\n\r\n[Service]\r\nType=oneshot\r\nRemainAfterExit=yes\r\nExecStart=/usr/bin/zsh -c 'nvidia-smi pmon -c 1 | awk \\'$3 == \"C\" { system(\"kill -9 \" $2) }\\''\r\nExecStart=/usr/bin/systemctl stop nvidia-docker.service\r\nExecStop=/usr/bin/systemctl start nvidia-docker.service\r\n\r\n[Install]\r\nWantedBy=sleep.target\r\n```", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Closing for now. Please reopen if @denzp's solution or the others mentioned in this thread don't work.", "I had the same problem after waking up the Ubuntu from sleep mode. \r\nSolution for me was to kill the compiz. \r\nPerhaps anything else which was left on before the sleep must be killed upon the wake up (as some others have also indicated on this issue). ", "Same issue for me (Linux Mint 18.3, Nvidia Quadro M1200, driver version 384.130, Cuda 9.0 and Keras/TF).\r\n\r\n> Thanks @pierrekilly ! That worked for me.\r\n> I was able to get around the issue by running the following script after my devbox wakes up.\r\n> \r\n> ```\r\n> sudo rmmod nvidia_uvm\r\n> sudo rmmod nvidia_drm\r\n> sudo rmmod nvidia_modeset\r\n> sudo rmmod nvidia\r\n> sudo modprobe nvidia\r\n> sudo modprobe nvidia_modeset\r\n> sudo modprobe nvidia_drm\r\n> sudo modprobe nvidia_uvm\r\n> ```\r\n\r\nThis does not work for me, I get stuck at `sudo rmmod nvidia_drm`:\r\n```\r\nrmmod: ERROR: Module nvidia_drm is in use\r\n```\r\nIt prevents other modules (nvidia_modeset and nvidia) from being stopped.", "> A solution that works for me is a systemd service that kills all CUDA consuming processes . . .\r\n\r\n```\r\n$ nvidia-smi pmon -c 1\r\n\r\n# gpu        pid  type    sm   mem   enc   dec   command\r\n# Idx          #   C/G     %     %     %     %   name\r\n    0       1323     G     0     0     0     0   Xorg\r\n.\r\n.\r\n.\r\n```\r\nIs it recommended to kill Xorg on every suspend?\r\n\r\n", "> Same issue for me (Linux Mint 18.3, Nvidia Quadro M1200, driver version 384.130, Cuda 9.0 and Keras/TF).\r\n> \r\n> > Thanks @pierrekilly ! That worked for me.\r\n> > I was able to get around the issue by running the following script after my devbox wakes up.\r\n> > ```\r\n> > sudo rmmod nvidia_uvm\r\n> > sudo rmmod nvidia_drm\r\n> > sudo rmmod nvidia_modeset\r\n> > sudo rmmod nvidia\r\n> > sudo modprobe nvidia\r\n> > sudo modprobe nvidia_modeset\r\n> > sudo modprobe nvidia_drm\r\n> > sudo modprobe nvidia_uvm\r\n> > ```\r\n> \r\n> This does not work for me, I get stuck at `sudo rmmod nvidia_drm`:\r\n> \r\n> ```\r\n> rmmod: ERROR: Module nvidia_drm is in use\r\n> ```\r\n> \r\n> It prevents other modules (nvidia_modeset and nvidia) from being stopped.\r\n\r\n\r\nFor what it's worth this is still a problem in 2021 for me on Fedora 33. I tried to follow this advise but I could also not unload `nvidia_drm` for the same reason. But after loading the first module in the list again, things worked for me again. So for me it was just:\r\n\r\n```\r\nsudo rmmod nvidia_uvm\r\nsudo modprobe nvidia_uvm\r\n```\r\n\r\nNot claiming this works for everyone though.", "If you meet\r\n\r\n`rmmod: ERROR: Module nvidia_drm is in use`\r\n\r\nwhen you are trying to do\r\n\r\n`sudo rmmod nvidia_uvm`\r\n\r\n`sudo rmmod nvidia_drm`\r\n\r\n`sudo rmmod nvidia_modeset`\r\n\r\n`sudo rmmod nvidia`\r\n\r\n`sudo modprobe nvidia`\r\n\r\n`sudo modprobe nvidia_modeset`\r\n\r\n`sudo modprobe nvidia_drm`\r\n\r\n`sudo modprobe nvidia_uvm`\r\n\r\n**Be careful to do the following, if you are unfamiliar with how to use the command line control of Linux**\r\n\r\nYou may want to try the following\r\n1. change to a text console (pressing Ctrl+Alt+F2)\r\n2. log in as Root\r\n3. `systemctl isolate multi-user.target` (your graphic interface will be offline)\r\n3. Type in the commands above one by one, or do the driver update-related things.\r\n4. After you finish, `systemctl start graphical.target` (your low-resolution interface is back)\r\n5. Now, you can try to restart your machine and check if CUDA device become available or not. \r\n\r\nI'd prefer to verify this by cd into the end of /samples within CUDA-XX folder, and run ./bandwidthTest, e.g., my folder is at /usr/local/cuda-11.2/samples/bin/x86_64/linux/release\r\nthen I run \r\n`./bandwidthTest`\r\nIf you can get this test passed, your device should be available now!", "For what it's worth, I got this error as well after suspending and waking up.  I didn't need to rmmod / modprobe anything.  Just needed to kill the jupyter notebook that had been running before the suspend.  Note sure if it'll work for others, but worth a try:\r\n\r\n[https://forums.developer.nvidia.com/t/reset-driver-without-rebooting-on-linux/40625](https://forums.developer.nvidia.com/t/reset-driver-without-rebooting-on-linux/40625)\r\n\r\n\"As stated elsewhere, the CUDA runtime should do a pretty good job of cleaning up without any of this as long as you kill any host processes associated with the crash session.\"\r\n", "Killing the jupyter notebook (or any of the processes using the GPU) never worked for me.\r\n\r\nHowever I did not have this problem anymore since I started using the recently introduced nvidia systemd services: `nvidia-suspend`, `nvidia-resume` and `nvidia-hybernate` mentioned e.g. [here](https://www.nvidia.com/Download/driverResults.aspx/171981/en-us).\r\n\r\nTo make it work I added a `nvidia-suspend.conf` to `/etc/modprobe.d/` which just contains:\r\n```\r\noptions nvidia NVreg_PreserveVideoMemoryAllocations=1\r\n```\r\nand then enabled these three services. So far I did not have any problems again after using this setup for about a month with several suspends.", "Ahh, interesting. That does sound like a solid solution.  Just for reference, what setup do you have?\r\n\r\nI have\r\nUbuntu 20.04\r\nGTX 1070 MaxQ\r\nDriver 465.19.01\r\nCUDA 11.3\r\n", "> Ahh, interesting. That does sound like a solid solution. Just for reference, what setup do you have?\r\n> \r\n> I have\r\n> Ubuntu 20.04\r\n> GTX 1070 MaxQ\r\n> Driver 465.19.01\r\n> CUDA 11.3\r\n\r\nI have\r\nFedora 34\r\nRTX 3080 (mobile)\r\nDriver 465.31\r\nCUDA 11.3", "I have: Ubuntu 20.04, RTX2060 (mobile) and I use the default nvidia drivers (no extra repository setup). First tried with `nvidia-driver-465` (11.3) but it does not have the service files (nvidia-suspend, nvidia-resume and nvidia-hybernate), then I found out that `nvidia-driver-460` (11.2) does:\r\n```\r\n$ apt-file list nvidia-driver-460\r\n....\r\nnvidia-driver-460: /usr/share/doc/nvidia-driver-460/nvidia\r\nnvidia-driver-460: /usr/share/doc/nvidia-driver-460/nvidia-hibernate.service\r\nnvidia-driver-460: /usr/share/doc/nvidia-driver-460/nvidia-resume.service\r\nnvidia-driver-460: /usr/share/doc/nvidia-driver-460/nvidia-sleep.sh\r\nnvidia-driver-460: /usr/share/doc/nvidia-driver-460/nvidia-suspend.service\r\n```\r\nSo I installed it with:  \r\n  \r\n`$ sudo apt install nvidia-driver-460`  \r\n  \r\nThen create the file `/etc/modprobe.d/nvidia.conf ` with `options nvidia NVreg_PreserveVideoMemoryAllocations=1` as @ruffson suggests. And restart.  \r\n  \r\nTo install the scripts I followed the official instructions from nvidia driver [readme](https://download.nvidia.com/XFree86/Linux-x86_64/460.80/README/powermanagement.html), specifically:  \r\n```\r\n$ sudo install /usr/share/doc/nvidia-driver-460/nvidia-suspend.service /etc/systemd/system/\r\n$ sudo install /usr/share/doc/nvidia-driver-460/nvidia-hibernate.service /etc/systemd/system/\r\n$ sudo install /usr/share/doc/nvidia-driver-460/nvidia-resume.service /etc/systemd/system/\r\n$ sudo install /usr/share/doc/nvidia-driver-460/nvidia /lib/systemd/system-sleep/\r\n$ sudo install /usr/share/doc/nvidia-driver-460/nvidia-sleep.sh /usr/bin/\r\n$ sudo systemctl enable nvidia-suspend.service\r\n$ sudo systemctl enable nvidia-hibernate.service \r\n$ sudo systemctl enable nvidia-resume.service \r\n```\r\nAnd problem solved. My docker containers (with gpus=all) keep working after suspend and resume my laptop.  ", "I tried a bunch of things from this thread without any success. \r\n\r\n* disabling `nvidia-suspend.service` and co services\r\n* enabling `nvidia-suspend.service` and co services\r\n* patching `/usr/bin/nvidia-sleep.sh`\r\n* config `options nvidia NVreg_PreserveVideoMemoryAllocations=1`\r\n\r\n-----\r\n\r\nWhat did work is **killing all CUDA-utilizing processes before suspending**. Most common scenario would be shutting down jupyter notebooks or terminating running console in an IDE. \r\n\r\n----\r\nPoC\r\n\r\n```bash\r\nnvidia-smi pmon -c 1 | grep 'python' | awk '{ print $2 }' | sudo xargs -n1 kill -9 2>/dev/null | systemctl suspend\r\n```\r\n^ this will always suspend, and if needed will kill any running python script/docker. Unfortunately it requires `sudo` and is generally a bit janky. Check out full service solution by  @denzp above. \r\n\r\nNote, **there's no need to kill Xorg**. At least on my dual-gpu laptop it works. \r\n\r\n", "My service file\r\n```service\r\n[Unit]\r\nDescription=Kill processes with initialized CUDA before sleep\r\nBefore=suspend.target\r\nStopWhenUnneeded=yes\r\n\r\n[Service]\r\nType=oneshot\r\nRemainAfterExit=yes\r\nExecStart=/usr/bin/bash -c \"nvidia-smi pmon -c 1 | grep 'python' | awk '{ print $2 }' | sudo xargs -n1 kill -9\"\r\n\r\n[Install]\r\nWantedBy=suspend.target\r\n```\r\n\r\nHow to enable: https://www.shubhamdipt.com/blog/how-to-create-a-systemd-service-in-linux/\r\n\r\n---\r\nI couldn't get @denzp `awk` stuff working, so I simplified it. "]}, {"number": 5776, "title": "Unknown events logged while profiling TensorFlow code using Timeline API", "body": "I am seeing Unknown events being logged in the Trace logs using APIs from Timeline.py module. Is this a known issue and any ideas on how this could be fixed? I could not find any references to this issue on GITHUB or StackOverFlow.\r\n\r\nHere is couple of examples of \"Unknown\" events:\r\n\r\n       {\r\n            \"name\": \"MEMCPYHtoD\",\r\n            \"ph\": \"X\",\r\n            \"ts\": 19926542594074346,\r\n            \"cat\": \"Op\",\r\n            \"args\": {\r\n                \"name\": \"unknown\",\r\n                \"op\": \"MEMCPYHtoD\"\r\n            },\r\n            \"pid\": 715,\r\n            \"tid\": 0,\r\n            \"dur\": 12535\r\n        },\r\n\r\n        {\r\n            \"tid\": 0,\r\n            \"ph\": \"X\",\r\n            \"name\": \"unknown\",\r\n            \"pid\": 63,\r\n            \"cat\": \"Op\",\r\n            \"args\": {\r\n                \"op\": \"unknown\",\r\n                \"name\": \"unknown\"\r\n            },\r\n            \"ts\": 19926135764459654,\r\n            \"dur\": 1147\r\n        },\r\n\r\n\r\nOperating System: CentOS\r\nInstalled version of CUDA and cuDNN: \r\nCuda 7.5 , cuDNN 5.1.3\r\nTensorFlow version 0.10 and 0.11 rc0\r\n", "comments": ["Hi agupta74,\r\n\r\ncan you please specify which API did you use from timeline.py and at when you will hit the issue, as in when this unknown event seen.. can you please provide few more information to have fix", "This is a known issue and is due to the current tracing format not being expressive enough to handle graphs which get rewritten during optimization.  \r\n\r\nPlease see the comments in #3717 "]}, {"number": 5775, "title": "Link to tutorial pages broken", "body": "Hi,\r\n\r\nIt seems every links to tutorial pages like http://tensorflow.org/tutorials/word2vec/index.md or http://tensorflow.org/tutorials/seq2seq/index.md are broken.\r\n\r\nSource: https://github.com/tensorflow/tensorflow/blob/754048a0453a04a761e112ae5d99c149eb9910dd/tensorflow/models/rnn/README.md#L5\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/754048a0453a04a761e112ae5d99c149eb9910dd/tensorflow/models/image/cifar10/README.md#L9", "comments": ["None of those links seem broken to me.  Could you please clarify the problem you are seeing?", "Sorry, it was the problem of local ISP."]}, {"number": 5774, "title": "per_image_standardization is not recognized", "body": "Hi, \r\nI installed Tensorflow with Python3 using the pip3 installation instructions from Tensorflow official site.\r\nThe version I installed is `0.11.0rc2` and I tried to use `per_image_standardization` tensor for normalization purposes. However it turned out that is not recognized, giving the following error:\r\n```\r\nAttributeError: module 'tensorflow.python.ops.image_ops' has no attribute 'per_image_standardization'\r\n```\r\nHas this error ever observed by someone else?", "comments": ["I believe it was recently renamed from `per_image_whitening` in this commit: https://github.com/tensorflow/tensorflow/commit/3adb3fd05f816bb6c68d13753bbdfa83516b26b6\r\n\r\n@martinwicke I think you reviewed this CL - the author doens't have a github account.   Any idea why this is not visible in the tf.image module?", "It really should be.\n", "I reproduced the problem with a 0.11 binary install.  Perhaps this is fixed at head though....?", "@skye, can you take a look?", "Yup. 3adb3fd is my commit btw, I'm looking into getting my github account associated with my commits.", "It looks like 3adb3fd is not included in 0.11, so the function is called `per_image_whitening`, not `per_image_standardization`. @hamidb, does the following work for you?\r\n```\r\nimport tensorflow as tf\r\ntf.image.per_image_whitening(...)\r\n```", "@skye `per_image_whitening` works for me. "]}, {"number": 5773, "title": "extract_image_patches zeros out data for large images", "body": "I'm reading a large image (7128x5097 pixels) and generating patches with extract_image_patches. Depending on the patch size, some or all of the resulting image is zeroed out. Is there a limit on tensor size that it hits?\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nnothing relevant\r\n\r\n### Environment info\r\nOperating System: macOS Sierra 10.12.1\r\n\r\nInstalled version of CUDA and cuDNN: none\r\n\r\nA link to the pip package you installed:\r\nhttps://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.11.0-py2-none-any.whl\r\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\".\r\n0.11.0\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nThe following code generates a random image, scales it up by a factor of 5, pulls a single pixel out of each patch, and writes an image with the result.\r\n\r\nExpected behavior: an image of random pixels.\r\nObserved behavior: for small images it works. But for the 5097x7129 image, N=3 works, N=5 gives a completely blank image, and N=7 is blank after the first 998 rows. (It's kind of strange that 7 works better than 5.)\r\n\r\n    from __future__ import absolute_import\r\n    from __future__ import print_function\r\n    import tensorflow as tf\r\n    N = 5 # Can try other numbers\r\n    def main(_):\r\n      img = tf.random_uniform([1, 5097, 7129, 3], minval=0, maxval=255, dtype=tf.int32)\r\n      img = tf.cast(img, tf.uint8)\r\n      patches = tf.extract_image_patches(img, [1, N, N, 1], [1, N, N, 1],\r\n        [1, 1, 1, 1], \"SAME\")\r\n      data = patches[0, :, :, 0:3]\r\n    \r\n      f = open('/tmp/img.png', 'w')\r\n      init_op = tf.initialize_all_variables()\r\n      with tf.Session() as sess:\r\n        sess.run(init_op)\r\n        f.write(tf.image.encode_png(data).eval())\r\n      f.close()\r\n    \r\n    if __name__ == \"__main__\":\r\n      tf.app.run()\r\n\r\n\r\n### What other attempted solutions have you tried?\r\nThis is a simplified version of a larger image learning system, and I've cut it down to the problematic code. I've checked the values to make sure the problem is in extract_image_patches and not image_encode_png. The problem seems to happen if the tensor is big, so there's probably some size limit somewhere.", "comments": ["@gpapan Could you please take a look at this?", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.", "Can confirm that this issue still exists.\r\n\r\n**Environment info**\r\n\r\nSystem 1: Ubuntu 16.04.3, CUDA 8.0.61, cuDNN 4.0.7, TF v1.1.0\r\nSystem 2: `tensorflow/tensorflow:latest-gpu` Docker container (with TF v1.3.0)\r\nSystem 3: Ubuntu 16.04.3, TF v1.2.0 on CPU\r\n\r\n**Minimal reproducible example**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nN = 13\r\nimg = tf.cast(tf.random_uniform([1, 5097, 7129, 1], minval=0, maxval=255, dtype=tf.int32), tf.uint8)\r\npatches = tf.extract_image_patches(img, ksizes=[1, N, N, 1], strides=[1, 1, 1, 1], rates=[1, 1, 1, 1], padding='SAME')\r\nencode_image = tf.image.encode_png(patches[0, :, :, 0:1])\r\nwrite_file = tf.write_file('output.png', encode_image)\r\ntf.Session().run(write_file)\r\n```\r\n\r\nOutput:\r\n\r\n![output](https://user-images.githubusercontent.com/2457311/29810001-811d25aa-8c9e-11e7-9afd-9a4403d5c785.png)\r\n\r\n\r\nAs @shirriff said the result should be an image with random pixel values but the result varies depending on the patch size `N` and the image size. @skye @gpapan Please reopen.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "@mzur isn't this expected behavior with padding type \"SAME\"? As I understand it, if you want no zeroed-out cells, you should set padding = \"VALID\".", "@sid-kap `SAME` padding may cause zeros at the borders (as described [here](https://stackoverflow.com/a/39371113/1796523)). But this issue is about zeroing out, i.e. the image is not \"finished\".\r\n\r\nLet me illustrate this with a [large picture](https://commons.wikimedia.org/wiki/File:101_Raven.jpg) (scaled down to 4876x7000 pixels). I run this code, which reads the picture, splits it into patches, then merges the patches back together and writes it to a file:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nN = 13\r\nimg = [tf.image.decode_jpeg(tf.read_file('101_Raven.jpg'), 1)]\r\npatches = tf.extract_image_patches(img, ksizes=[1, N, N, 1], strides=[1, 1, 1, 1], rates=[1, 1, 1, 1], padding='SAME')\r\nencode_image = tf.image.encode_jpeg(patches[0, :, :, 0:1])\r\nwrite_file = tf.write_file('output.jpg', encode_image)\r\ntf.Session().run(write_file)\r\n```\r\n\r\nThis results in the following image (note the black border due to `SAME` padding):\r\n\r\n![output](https://user-images.githubusercontent.com/2457311/34869128-42ce5172-f786-11e7-9c05-622a706832b4.jpg)\r\n\r\nThe bottom part of the image is missing. If I increase the stride to `[1, 2, 2, 1]`, I get the [correct result](https://user-images.githubusercontent.com/2457311/34869317-cee28070-f786-11e7-8d8c-085e6d0b0657.jpg). The behavior is the same for `VALID` padding.\r\n", "@mzur I guess I misunderstood, apologies!", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "@gpapan Any help?  This bug is pretty old.  Do you think this is a bug in the EigenTensor implementation of extract_image_patches?\r\n\r\nMy guess is that there's some indexing overflow happening.", "Actually, it looks like this was already fixed in https://github.com/tensorflow/tensorflow/commit/d75d5529d569d8f72cb215d3696db1feb1d9f033#diff-42af759a17331c263b5c57f04f130aa5 possibly, so it may no longer be valid.\r\n\r\nGiven the information in the commit and the related internal bug report that this fixes, I'm going to assume this is now fixed.  Please reopen if this is not true and you are running a recent version of TF.", "I've tried the code above with TF 1.5.0 and it's working correctly now. Thanks!"]}, {"number": 5772, "title": "could not set cudnn filter descriptor: CUDNN_STATUS_BAD_PARAM", "body": "The version of cuda and cudnn meets the requirement, but still cannot use cudnn properly.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\nLinux version 3.16.0-30-generic (buildd@kissel) (gcc version 4.8.2 (Ubuntu 4.8.2-19ubuntu1) ) #40~14.04.1-Ubuntu\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n-rw-r--r-- 1 root root   558720 Sep 15 07:02 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Sep 15 07:05 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 Sep 15 07:05 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   415432 Sep 15 07:02 /usr/local/cuda/lib64/libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   775162 Sep 15 07:02 /usr/local/cuda/lib64/libcudart_static.a\r\nlrwxrwxrwx 1 root root       13 Nov 22 10:55 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5\r\nlrwxrwxrwx 1 root root       17 Nov 22 10:55 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5\r\n-rw-r--r-- 1 root root 78065952 Nov 22 10:09 /usr/local/cuda/lib64/libcudnn.so.5.0.5\r\n-rw-r--r-- 1 root root 79337624 Nov 22 10:17 /usr/local/cuda/lib64/libcudnn.so.5.1.5\r\n-rw-r--r-- 1 root root 69756172 Nov 22 10:17 /usr/local/cuda/lib64/libcudnn_static.a\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\nexport TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\r\n0.11.0\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nwhen trying to call a function that is only supported by cudnn, for example conv2d\r\n", "comments": ["@yetionyo Could you please supply a minimal repro example?  \r\n@zheng-xq  Can you think of any reason why this might happen?", "It is proved to be irrelevant with conv2d itself, maybe it's related with the way I used conv2d, because I can run this demo without this problem.\r\nimport tensorflow as tf\r\n\r\nmy_data  = tf.random_normal([20,20,20,3])\r\nmy_filter = tf.random_normal([3,3,3,10])\r\nconv_result = tf.nn.conv2d(my_data, my_filter, strides=[1, 1, 1, 1], padding=\"VALID\")\r\nsess = tf.Session()\r\nresult = sess.run(conv_result)\r\nprint result\r\n\r\nBut it's a little strange that what kind of operation would lead to this problem (it's more like a failure of calling cudnn)", "Similar problem to #5476, #4909 and #4111 ?\r\n\r\nAll these seem to be mention passing an empty numpy array into TF.... @zheng-xq Is there perhaps some input validation missing on cuDNN ops?", "Yeah, these problems are similar to mine. Maybe empty numpy array is not main reason in this problem, but some improper ops indeed exist. Thanks :)", "I'd like to leave this open until we understand why an empty array causes a CUDA error, rather than a TensorFlow runtime InvalidArgument error status.", "Looks like this is still an issue on current `master`. It would be nice to get this fixed! The CUDA error is _quite_ mysterious when you run into it.", "This issue seems to affect TensorFlow Fold, which uses dynamic network structures and can often generate empty tensor if a path is not used in a dynamic batch", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "There is a pull request that should handle the issue. Please check after the pull request has been approved: https://github.com/tensorflow/tensorflow/pull/15264", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "#15264 has been merged, so I believe the issue should have been fixed by that. Please reopen if it still exists.", "Just found this page. I'm seeing this error with a fresh nightly tensorflow-gpu on Ubuntu. So, despite the merge, this doesn't look resolved. ", "Same here I get this error as well on ubuntu tf 1.4.1 not the nightly build.", "@drscotthawley you need to provide more details (logs, small repro code, etc) for people to tell whether it's the same problem (empty tensors into cudnn) or not. The fix above only adds support of empty tensor on certain ops, and very likely there are ops not covered.", "@ppwwyyxx Thanks for the comment! @drscotthawley and @kirk86 , could you provide more info so that I can take a closer look?", "@ppwwyyxx @yzhwang I had just downloaded a fresh CUDA from NVIDIA, which defaults to version 9.1, not realizing that TF didn't support that yet.  I resolved this problem by downgrading to CUDA 9.0.  You can close this issue again.  \r\n@kirk86, try using CUDA 9.0 instead. Also, I'm using CUDNN 7.0.5 and it's working. \r\n\r\n\r\nMight be worth noting: I've built TF from source before, but couldn't manage to do so using CUDA 9.1.  I don't recall the errors, just that downgrading to 9.0 finally enabled me to \"get back to work.\" ", "@drscotthawley Thanks for you answer but in my case I can't do that. It's a shared system and I'm not an admin."]}, {"number": 5771, "title": "tf.with_dependencies isn't exposed, but it is used in documentations", "body": "There are some examples that uses tf.with_dependencies (ex. [tf.Assert in r0.11](https://www.tensorflow.org/versions/r0.11/api_docs/python/control_flow_ops.html#Assert)) but tf.with_dependencies isn't exposed to tensorflow module.\r\n\r\n### Environment info\r\nbranch: master branch $ git rev-parse HEAD => c7edafccc\r\n\r\n### If possible, provide a minimal reproducible example\r\n```\r\n$ python3 -c \"import tensorflow as tf; tf.with_dependencies()\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute 'with_dependencies'\r\n```\r\nRelated: (https://github.com/tensorflow/tensorflow/blob/a4c8df209d7413068f4ed3e71c43eb798fbd5580/tensorflow/contrib/layers/python/layers/layers_test.py#L25)", "comments": ["Looks like it is maybe being removed by this: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/__init__.py#L241\r\n```\r\n# Remove all extra symbols that don't have a docstring or are not explicitly\r\n# referenced in the whitelist.\r\nremove_undocumented(__name__, _allowed_symbols,\r\n                    [framework_lib, array_ops, client_lib, check_ops,\r\n                     compat, constant_op, control_flow_ops, functional_ops,\r\n                     histogram_ops, io_ops, math_ops, nn, resource_loader,\r\n                     resources, script_ops, session_ops, sparse_ops, state_ops,\r\n                     string_ops, summary, tensor_array_ops, train])\r\n```\r\n@drpngx Is this correct?\r\n", "We have lots of internal uses for that, so I'm assuming that all commits on master work, but there might be some cherry-picks in 0.11 that don't work. Let's see if I can repro first.", "It looks like it was not part of our public API, even though it was documented. Looping internally for a fix. You'll have to upgrade.", "Actually, there `tf.control_dependencies` that provides the core functionality, and you should just use that. I'll update the doc.", "Submitted change internally to remove the doc example. It'll be pushed to the main repo soon."]}, {"number": 5770, "title": "Documentation of  tf.extract_image_patches is incomplete and slightly inaccurate", "body": "Hi,\r\n\r\nThe documentation of [tf.extract_image_patches](https://www.tensorflow.org/versions/r0.9/api_docs/python/array_ops.html#extract_image_patches) is unclear and slightly incorrect. \r\n\r\n1. The function call requires the first 5 parameters to be set and throws an error otherwise.The documentation, however, mentions that they are optional parameters.\r\n2. The \"out_rows\" and \"out_cols\" are not defined making it hard to make sense of what this function is actually doing. \r\n3. The definition of \"rates\" is not clear.. Is patch_size same as k_rows or k_cols?\r\n\r\nRunning\r\n`test_patches = tf.extract_image_patches(images, padding=\"SAME\",ksizes=[1, 32, 32, 1], strides=[1, 32, 32, 1], rates=[1,1, 1, 1])\r\n`\r\nwhere images = 1X299x299x3 returns me a tensor (test_patches) of shape 1x10x10x3072. \r\n\r\n1.Is test_patches[0,0,0,:] the first patch test_patches[0,0,1,:] the second patch and so on? \r\n2. If (1) is true, how was the dimension of a patch reduced from 32x32x3 to 3072, i.e., how exactly should this be reshaped? I tried reshaping test_patches[0,0,0,:] to (32,32,3) and (3,32,32) but the resulting image patch does not make sense. \r\n\r\nClarifying the documentation of this method would be immensely helpful!\r\n\r\n### Environment info\r\nUbuntu 14.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n-rw-r--r-- 1 root root   558720 Oct 22 01:37 /opt/cuda-8.0/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Oct 22 01:37 /opt/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 Oct 22 01:37 /opt/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rwxr-xr-x 1 root root   415432 Oct 22 01:37 /opt/cuda-8.0/lib64/libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   775162 Oct 22 01:37 /opt/cuda-8.0/lib64/libcudart_static.a\r\n-rwxr-xr-x 1 root root 79337624 Oct 22 01:48 /opt/cuda-8.0/lib64/libcudnn.so\r\n-rwxr-xr-x 1 root root 79337624 Oct 22 01:48 /opt/cuda-8.0/lib64/libcudnn.so.5\r\n-rwxr-xr-x 1 root root 79337624 Oct 22 01:48 /opt/cuda-8.0/lib64/libcudnn.so.5.1.5\r\n-rw-r--r-- 1 root root 69756172 Oct 22 01:48 /opt/cuda-8.0/lib64/libcudnn_static.a\r\n\r\nIf installed from source, provide \r\n1. The commit hash (`git rev-parse HEAD`)\r\n1f9e1eb186325dc789e7ea28aa5f7ef1e183f6b9\r\n2. The output of `bazel version`\r\n\r\nBuild label: 0.3.2\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Oct 7 17:25:10 2016 (1475861110)\r\nBuild timestamp: 1475861110\r\nBuild timestamp as int: 1475861110\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nJust calling this function would do.\r\n", "comments": ["I made a change to document these aspects internally, it should make its way externally in a couple of days. Thanks!"]}, {"number": 5769, "title": "Misformatted documentation after constant_initializer", "body": "The docs for constant_initializer fail to correctly close a code block in one of the examples.  As a result, many other sections are displayed in raw markdown.\r\n\r\nSee: https://www.tensorflow.org/versions/r0.11/api_docs/python/state_ops.html#constant_initializer", "comments": ["Is it fixed now? it looks ok to me."]}, {"number": 5768, "title": "Branch 139847768", "body": "", "comments": ["@caisq, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @martinwicke and @asimshankar to be potential reviewers.", "@tensorflow-jenkins test this please"]}, {"number": 5767, "title": "Disable the failing test case in conv_ops_3d_test.", "body": "", "comments": ["@gunan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener to be a potential reviewer."]}, {"number": 5766, "title": "Disable the failing test case in conv_ops_3d_test.", "body": "@mjanusz FYI", "comments": ["@gunan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener to be a potential reviewer.", "known flaky race condition in windows cmake test. merging."]}, {"number": 5765, "title": "Add missing line feed after totals in stat_summarizer", "body": "", "comments": ["@gar1t, thanks for your PR! By analyzing the history of the files in this pull request, we identified @andrewharp, @tensorflower-gardener and @petewarden to be potential reviewers.", "Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed the contributors agreement as gar1t on GitHub.", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins Test this please.", "Please advise as to how you'd like me to test this change. The test at tensorflow/contrib/stat_summarizer/python/stat_summarizer_test.py does not test line endings.\r\n\r\nYou are of course free to reject this PR for lack of explicit tests, but it's a trivial fix of an obvious bug.", "@gar1t This seems fine, and the test failures are unrelated. Thanks for the PR!"]}, {"number": 5764, "title": "No OpKernel for DepthwiseConv2dNative", "body": "When trying to run inference of tf graph by means of [benchmarking tool](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/benchmark), I'm getting error that indicates absence of [DepthwiseConv2dNative op](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/depthwise_conv_op.cc).\r\n\r\nI'm building and running benchmark tool as it described in [Tensorflow Makefile](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile)\r\n\r\nRun command:\r\n`adb shell '/data/local/tmp/benchmark --graph=/data/local/tmp/optimized_inference_graph.pb --input_layer=\"model/input_node:0\" --input_layer_shape=\"1,256,256,3\" --input_layer_type=\"float\" --output_layer=\"output:0\" '`\r\n\r\nI found this op as unsupported [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md). If it is related to this issue, I'm wondering if there is an upcoming patch to support this op? Or maybe there is a way to circumvent it?\r\n\r\nError log:\r\n```\r\nCould not create TensorFlow Session: Invalid argument: No OpKernel was registered to support Op 'DepthwiseConv2dNative' with these attrs.  Registered kernels:\r\n  <no registered kernels>\r\n\t [[Node: model/128to64/conv1/conv3x1x1/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, padding=\"SAME\", strides=[1, 1, 1, 1]](model/128to64/conv1/conv1x1xn/BatchNorm/batchnorm/add_1, model/128to64/conv1/conv3x1x1/depthwise_weights)]]\r\n```\r\n", "comments": ["@kostarion  I'm guessing that you are running on Android here?  (it would be helpful if you provided the information requested in the issues reporting template)\r\n\r\n@petewarden  I'm guessing there isn't an Android compatible kernel?", "There is an implementation in depthwise_conv_op.cc, but it looks like it hasn't been added to the makefile tf_op_files.txt list yet:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/tf_op_files.txt#L101\r\n\r\nIf adding tensorflow/core/kernels/tf_op_files.txt does help, it would be great if you could send us a pull request with that change in!", "Is this issue resolved? I tried building from make file after adding depthwise_conv_op.cc entry in t_op_files.txt. But no luck! Any ideas?", "we meet the same issue, we freeze inception_v2 and load in the iOS. \r\n\r\n\r\n\r\n2017-02-03 14:51:13.543432 CameraExample[28689:12736455] 0x17015dbc0 Copy matching assets reply: XPC_TYPE_DICTIONARY  <dictionary: 0x17015dbc0> { count = 2, transaction: 0, voucher = 0x0, contents =\r\n         \"Assets\" => <data: 0x170278c80>: { length = 1229 bytes, contents = 0x62706c6973743030d4010203040506646558247665727369... }\r\n         \"Result\" => <int64: 0x17042c000>: 0\r\n}\r\n2017-02-03 14:51:13.545767 CameraExample[28689:12736455] 0x17015e610 Copy assets attributes reply: XPC_TYPE_DICTIONARY  <dictionary: 0x17015e610> { count = 1, transaction: 0, voucher = 0x0, contents =\r\n         \"Result\" => <int64: 0x17042bcc0>: 1\r\n}\r\n2017-02-03 14:51:13.546079 CameraExample[28689:12736455] [MobileAssetError:1] Unable to copy asset attributes\r\n2017-02-03 14:51:13.546242 CameraExample[28689:12736455] Could not get attribute 'LocalURL': Error Domain=MobileAssetError Code=1 \"Unable to copy asset attributes\" UserInfo={NSDescription=Unable to copy asset attributes}\r\n2017-02-03 14:51:13.548111 CameraExample[28689:12736455] 0x17015dbc0 Copy matching assets reply: XPC_TYPE_DICTIONARY  <dictionary: 0x17015dbc0> { count = 2, transaction: 0, voucher = 0x0, contents =\r\n         \"Assets\" => <data: 0x1702796c0>: { length = 1237 bytes, contents = 0x62706c6973743030d4010203040506636458247665727369... }\r\n         \"Result\" => <int64: 0x17042bca0>: 0\r\n}\r\n2017-02-03 14:51:13.550122 CameraExample[28689:12736455] 0x174150f90 Copy assets attributes reply: XPC_TYPE_DICTIONARY  <dictionary: 0x174150f90> { count = 1, transaction: 0, voucher = 0x0, contents =\r\n         \"Result\" => <int64: 0x1742329c0>: 1\r\n}\r\n2017-02-03 14:51:13.550194 CameraExample[28689:12736455] [MobileAssetError:1] Unable to copy asset attributes\r\n2017-02-03 14:51:13.550345 CameraExample[28689:12736455] Could not get attribute 'LocalURL': Error Domain=MobileAssetError Code=1 \"Unable to copy asset attributes\" UserInfo={NSDescription=Unable to copy asset attributes}\r\n2017-02-03 14:51:13.826229: E /Users/Bonan1/tensorflow/tensorflow/contrib/ios_examples/camera/tensorflow_utils.mm:152] Could not create TensorFlow Graph: Invalid argument: No OpKernel was registered to support Op 'DepthwiseConv2dNative' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n         [[Node: InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, padding=\"SAME\", strides=[1, 2, 2, 1]](Placeholder, InceptionV2/Conv2d_1a_7x7/depthwise_weights)]]\r\n2017-02-03 14:51:13.828395: F /Users/Bonan1/tensorflow/tensorflow/contrib/ios_examples/camera/CameraExampleViewController.mm:392] Couldn't load model: Invalid argument: No OpKernel was registered to support Op 'DepthwiseConv2dNative' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n         [[Node: InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, padding=\"SAME\", strides=[1, 2, 2, 1]](Placeholder, InceptionV2/Conv2d_1a_7x7/depthwise_weights)]]\r\n(lldb) \r\n", "@civilman628 It looks like depthwise_conv_op.cc was never added to tf_op_files.txt. If you add it there, does that resolve the issue?\r\n\r\nI'll go ahead and send a change to add it since I can't think of any reason it shouldn't be there.", "we add this .cc file to tf_op_files.txt and works."]}, {"number": 5763, "title": "The TensorFlow operation concat_offset is undocumented", "body": "This shows up in some protocol buffers.  A vigorous search through the source code produced a short comment in some C++ code that gives some hints about its behaviour.  In particular, the word 'cumsum\" was used.  There appears to be, at times, a second output argument produced, although I have no idea what that could be except a copy of the first output.\r\n\r\nA lot of searching turned up nothing useful.\r\n\r\nTensorFlow version .11", "comments": ["Here is what I found in the source, the interface looks clear:\r\n```cpp\r\nREGISTER_OP(\"ConcatOffset\")                                                     \r\n    .Input(\"concat_dim: int32\")                                                 \r\n    .Input(\"shape: N * int32\")                                                  \r\n    .Output(\"offset: N * int32\")                                                \r\n    .Attr(\"N: int >= 2\")                                                        \r\n    .SetShapeFn([](InferenceContext* c) {                                       \r\n      for (int i = 1; i < c->num_inputs(); ++i) {                               \r\n        c->set_output(i - 1, c->input(i));                                      \r\n      }                                                                         \r\n      return Status::OK();                                                      \r\n    })                                                                          \r\n    .Doc(R\"doc(                                                                 \r\nComputes offsets of concat inputs within its output.                            \r\n                                                                                \r\nFor example:                                                                    \r\n                                                                                \r\nprettyprint                                                                  \r\n# 'x' is [2, 2, 7]                                                              \r\n# 'y' is [2, 3, 7]                                                              \r\n# 'z' is [2, 5, 7]                                                              \r\nconcat_offset(2, [x, y, z]) => [0, 0, 0], [0, 2, 0], [0, 5, 0]                  \r\n                                                                           \r\n                                                                                \r\nconcat_dim: The dimension along which to concatenate.                           \r\nshape: The `N` int32 vectors representing shape of tensors being concatenated.  \r\noffset: The `N` int32 vectors representing the starting offset                  \r\n        of input tensors within the concatenated output.                        \r\n                                                                                \r\nThis is typically used by gradient computations for a concat operation.         \r\n)doc\");\r\n```\r\n```cpp\r\n    // Suppose a Concat() op needs to Concatenate N tensors, each of            \r\n    // which has the same number of dimensions.  Their shapes match             \r\n    // except the concat dimension.                                             \r\n    //                                                                          \r\n    // E.g., say, we want to concatenate 3 tensors in the 2nd                   \r\n    // dimension, and their shapes are:                                         \r\n    //                                                                          \r\n    //  [2, 2, 5, 7]                                                            \r\n    //  [2, 3, 5, 7]                                                            \r\n    //  [2, 4, 5, 7]                                                            \r\n    //                                                                          \r\n    // Here, N=3, cdim=1, dims=4. The concatenated tensor has shape             \r\n    // [2,9,5,7]. We will compute the cumulative sum along the 2nd              \r\n    // dimension to figure out each input's offset in the concatenated          \r\n    // output:                                                                  \r\n    //  [0, 0, 0, 0]                                                            \r\n    //  [0, 2, 0, 0]                                                            \r\n    //  [0, 5, 0, 0]\r\n```\r\n\r\nHope this helps.", "Yes, I too was able to figure out what it does by looking at the test files.  But it seems to me if you have a website that says it has the API and describes TensorFlow, it should document ALL the ops, not exclude those that are \"internal\" or \"nobody cares about\".", "@yaccman this means this op is not part of TensorFlow client API (not suitable for public consumption - could have issues, or be subject to change or removal without warning)", "Well, if I want to make TensorFlow perform well on my new hardware, I guess I have 4 choices:\r\n1.  Use protocol buffer to capture the design.  But this means guessing \"the meaning of ops that may have issues or be subject to change or removal without warning\"\r\n2.  Use the mysterious \"coming soon\" XLA about which I cannot get any questions answered, even under NDA.\r\n3.  Parse the Python and use only documented ops to capture the design.  Painful, and works only for Python.\r\n4.  Use another front end that captures the programmer's intent in a stable, documented form.\r\n\r\nWhat would you do?", "I see, I guess you are interested in implementation-level API rather than client API. There's a list of all ops understood by TensorFlow runtime [here](https://github.com/tensorflow/tensorflow/blob/294442996b2aeff00b1bfdc7e7169f7cb35bbf3d/tensorflow/core/ops/ops.pbtxt) ([526](https://gist.github.com/yaroslavvb/08227f38f2fc3a92b32a7a0a6927500b) of them). That seems like too much to implement in hardware, maybe other hardware people can say what they are doing -- @aidan-plenert-macdonald @DavidNorman ", "We are doing macros to implement them quickly. A bunch of them really aren't that hard and can be macro'ed out. You can also often use existing implementations, but not that you need a patched version of TF to expose more headers. https://github.com/tensorflow/tensorflow/issues/1419", "We are using XLA, which significantly reduces the number of ops to implement", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 5762, "title": "BroadcastGradientArgs is used in protocol buffers but undocumented", "body": "Not too much to say about this.  The op, having two outputs, is far from trivial to figure out from its usage.  It is used in a number of the early TensorFlow tutorials.\r\n\r\nTensorFlow .11", "comments": ["This is an internal op used in autogenerated gradient code to support gradient computation for ops which support broadcasting.  You should not be using it directly.", "This is a very unsatisfying answer for those of us who are trying to persuade TensorFlow to run on new hardware.  It is not an internal op if it appears in the protocol buffer.  Please rethink this...", "@yaccman  Believe me, I came across the exact same problem about 9 months ago when working on compiling TF graphs for new hardware platforms. ;-)\r\n\r\nWorking directly from the GraphDef you are going to find an increasing number of problems like this, not least due to the ever increasing use of custom ops.  In the ideal case, constant folding removes much of this sort of code when you provide adequate shape information, but it is fragile to rely on this.\r\n\r\nFor this reason we are encouraging people (internally and externally) to standardize on XLA as a portable intermediate representation for those targeting new hardware and accelerator platforms.   The aim is to have a small and complete set of primitives with well-defined semantics that compiler writers can implement, and TF will provide the translation to XLA for connected subgraphs which are suitable for compilation/acceleration. \r\n\r\nIf you are working on something like this then please contact @wolffg or Zak Stone.", "Based on usage it looks like it returns the axis of reduction given two tensor shapes. Notice that the return values of this is always used for reduce_sum. ops that support broadcasting (an op involving a tensor of lesser rank or shape) needed to have a reduction function so that the resulting gradient has the same original size. It has the effect of summing individual gradients into one value."]}, {"number": 5761, "title": "Documentation for tf.strided_slice is nearly useless", "body": "The documentation for this op could almost be used as a teaching moment for how to not document something.  In order to understand what it does, it is necessary to not only understand in detail how the related numpy op works, but also understand the mapping between the inputs of that op and the inputs of theTensorFlow op, that are totally different -- something that is far from obvious.\r\n\r\nJust to ice the cake, the examples given in the document are for slice, not strided_slice...\r\n\r\nSearches didn't turn up anything.  TF version is .11.  The machine and OS are not applicable.", "comments": ["I apologize for the confusion and frustration. A few notes that might be of help.\r\n\r\n1. The examples actually are for strided_slice, and there has been a correction to the documentation just made to correct that.\r\n2. If operating in Python you almost always should use the tensor slice operator (i.e. getitem)\r\nhttps://www.tensorflow.org/versions/master/api_docs/python/framework.html#Tensor (and search for __getitem__)\r\n3. I will add a link referring to the the Tensor.__getitem__ documentation from here, because that is almost always what you want to be using.\r\n4. If you have a suggestion on how to restructure the documentation to be more intuitive, I would be happy to integrate it. Perhaps a section that gives some examples of what it can do first. I do believe that what is there is useful for those that do know what the NumPy slice does, which is probably many people.\r\n", "The example code has a typo:\r\ntf.slice -> tf.strided_slice\r\nAlso, the third example doesn't produce the given result:\r\ntf.slice(input, [1, 1, 0], [2, -1, 3], [1, -1, 1]) **!=**> [[[4, 4, 4], [3, 3, 3]]]", "As of version .12, `tf.strided_slice` has `[misleading documentation](https://www.tensorflow.org/versions/master/api_docs/python/array_ops/slicing_and_joining#strided_slice)`.  A fourth argument is mandatory, not a keyword argument, and it cannot be `None`."]}, {"number": 5760, "title": "Add deprecation warnings to tf.X_summary ops and tf.train.SummaryWriter", "body": "", "comments": ["@danmane, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @josh11b and @ilblackdragon to be potential reviewers.", "FYI, rc0 binaries are already built, looks like we missed this PR.\r\nthis will make into rc1, is that OK?\r\nJenkins, test this please."]}, {"number": 5759, "title": "Error running ci_build.sh on docker container mac.", "body": "When I run following command to test the ci_build on my mac\r\n\r\n`$ tensorflow/tools/ci_build/ci_build.sh CPU bazel test //tensorflow/learn/...`\r\n\r\nI get following error - \r\n\r\nWORKSPACE: /Users/<username>/tensorflow\r\nCI_DOCKER_EXTRA_PARAMS:\r\nCOMMAND: bazel test //tensorflow/learn/...\r\nCI_COMMAND_PREFIX: ./tensorflow/tools/ci_build/builds/with_the_same_user ./tensorflow/tools/ci_build/builds/configured cpu\r\nCONTAINER_TYPE: cpu\r\nBUILD_TAG: tf_ci\r\n  (docker container name will be tf_ci.cpu)\r\n\r\nBuilding container (tf_ci.cpu)...\r\nSending build context to Docker daemon 222.7 kB\r\nStep 1 : FROM ubuntu:14.04\r\n ---> 4d44acee901c\r\nStep 2 : MAINTAINER Jan Prach <jendap@google.com>\r\n ---> Using cache\r\n ---> 598ad93f9ec8\r\nStep 3 : COPY install/*.sh /install/\r\n ---> Using cache\r\n ---> 2e90718bcd83\r\nStep 4 : RUN /install/install_bootstrap_deb_packages.sh\r\n ---> Using cache\r\n ---> 785d8c4ecf96\r\nStep 5 : RUN add-apt-repository -y ppa:openjdk-r/ppa &&     add-apt-repository -y ppa:mc3man/trusty-media &&     add-apt-repository -y ppa:george-edison55/cmake-3.x\r\n ---> Using cache\r\n ---> 8bbcbdd9d0e3\r\nStep 6 : RUN /install/install_deb_packages.sh\r\n ---> Using cache\r\n ---> ddebcc57fb16\r\nStep 7 : RUN /install/install_pip_packages.sh\r\n ---> Using cache\r\n ---> 3f6d70765a8b\r\nStep 8 : RUN /install/install_bazel.sh\r\n ---> Using cache\r\n ---> c7e6bab7b915\r\nStep 9 : RUN /install/install_proto3.sh\r\n ---> Using cache\r\n ---> 5c754e68ad21\r\nStep 10 : RUN /install/install_buildifier.sh\r\n ---> Using cache\r\n ---> 348488be0193\r\nStep 11 : RUN /install/install_auditwheel.sh\r\n ---> Using cache\r\n ---> 70ee410ea1cb\r\nStep 12 : COPY install/.bazelrc /root/.bazelrc\r\n ---> Using cache\r\n ---> 75649c1f71be\r\nStep 13 : ENV BAZELRC /root/.bazelrc\r\n ---> Using cache\r\n ---> 79809615d28e\r\nSuccessfully built 79809615d28e\r\nRunning 'bazel test //tensorflow/learn/...' inside tf_ci.cpu...\r\nid: illegal option -- -\r\nusage: id [user]\r\n       id -A\r\n       id -F [user]\r\n       id -G [-n] [user]\r\n       id -M\r\n       id -P [user]\r\n       id -g [-nr] [user]\r\n       id -p [user]\r\n       id -u [-nr] [user]\r\nid: illegal option -- -\r\nusage: id [user]\r\n       id -A\r\n       id -F [user]\r\n       id -G [-n] [user]\r\n       id -M\r\n       id -P [user]\r\n       id -g [-nr] [user]\r\n       id -p [user]\r\n       id -u [-nr] [user]\r\ndialout:x:20:\r\nadduser: Only one or two names allowed", "comments": ["cc: @vrv, @ebrevdo ", "Found the culprit.\r\nIn this line.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/ci_build.sh#L151\r\n\r\n`--name` should be replaces by `-n` ( This also works on Ubuntu)\r\n\r\nShould I do a PR ?", "Does that also affect L153?", "@abhitopia A PR will be welcome!", "@caisq - PR created #5784 "]}, {"number": 5758, "title": "Documentation and error messages for tf.tile, tf.constant leave a lot to be desired", "body": "I noticed that a protocol buffer did the equivalent of\r\n    tf.tile( x, [] )\r\nand wondered what that actually produced (the documentation was completely unhelpful).  So I tried\r\n    x = tf.Variable([1.,2.,3.,4.])\r\n    y = tf.tile(x,tf.constant(2))\r\nto get started, and got the message \"ValueError: Shape () must have rank 1\".  So I tried\r\n    y = tf.tile(x,tf.constant([2]))\r\nand got the same message(!).  tf.rank(tf.constant([2])) of course returned the value 1, so the message made no sense at all to me.\r\nIn my struggle to get an acceptable empty tensor, I tried\r\n    x = tf.constant(shape=[],dtype=\"int32\")\r\nThis produced the memorable message \"TypeError: constant() takes at least 1 argument (2 given)\"\r\nI eventually discovered that tf.tile returns 1 copy of its tensor when the input is an empty list.  It would have been so helpful if the documentation would have just stated this fact...\r\n\r\nSearching for tf.tile bugs produced a number of references to the documentation and a lot of hits relating to roofs.\r\n\r\nMy OS is Ubuntu 64-bit, TF version .11\r\n", "comments": ["As a PS, one would expect that tiling with a multiple of 0 would give an empty tensor as result.  But in fact, tf.tile(1.,[0]) produces an error \"Shape () must have rank 1\".  And again, tf.rank([0]) returns 1.", "I'm struggling to follow your confusing issue report, but ... \r\n\r\nWith TF 0.11, I get the following output:\r\n```\r\n>>> x = tf.Variable([1.,2.,3.,4.])\r\n>>> y = tf.tile(x,tf.constant(2))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/google/home/pbar/tensorflow.0.11rc2/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2859, in tile\r\n    name=name)\r\n  File \"/usr/local/google/home/pbar/tensorflow.0.11rc2/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/google/home/pbar/tensorflow.0.11rc2/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2382, in create_op\r\n    set_shapes_for_outputs(ret)\r\n  File \"/usr/local/google/home/pbar/tensorflow.0.11rc2/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1783, in set_shapes_for_outputs\r\n    shapes = shape_func(op)\r\n  File \"/usr/local/google/home/pbar/tensorflow.0.11rc2/local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1789, in _TileShape\r\n    multiples_shape = op.inputs[1].get_shape().with_rank(1)\r\n  File \"/usr/local/google/home/pbar/tensorflow.0.11rc2/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 650, in with_rank\r\n    raise ValueError(\"Shape %s must have rank %d\" % (self, rank))\r\nValueError: Shape () must have rank 1\r\n```\r\n...as per the [documentation](https://www.tensorflow.org/versions/r0.11/api_docs/python/array_ops.html#tile).  You are passing a scalar and the documentation states that the `multiples` argument must be 1-D.\r\n```\r\n>>> y = tf.tile(x, tf.constant([2]))\r\n>>> \r\n```\r\nSeems to work for me?\r\n\r\n> x = tf.constant(shape=[],dtype=\"int32\")\r\nThis produced the memorable message \"TypeError: constant() takes at least 1 argument (2 given)\"\r\nThe signature of `tf.constant` is:\r\n```\r\ndef constant(value, dtype=None, shape=None, name=\"Const\"):\r\n```\r\nThe [documentation](https://www.tensorflow.org/versions/r0.11/api_docs/python/constant_op.html#constant) states:\r\n```\r\n  Args:\r\n    value:     A constant value (or list) of output type `dtype`.\r\n    dtype:     The type of the elements of the resulting tensor.\r\n    shape:     Optional dimensions of resulting tensor.\r\n    name:      Optional name for the tensor.\r\n  Returns:\r\n    A Constant Tensor.\r\n```\r\ni.e. you *must* pass an argument for the positional argument `value`, and the keyword argument `dtype`\r\n\r\nThis error message is lamentable, but not TensorFlow's fault. Consider the following simple Python program:\r\n```\r\n>>> def foo(value, dtype=None, shape=None): return value\r\n>>> foo\r\n<function foo at 0x7f2f7ae0bb90>\r\n>>> foo(dtype=2, shape=3)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: foo() takes at least 1 argument (2 given)\r\n```\r\n\r\n> tf.tile(1.,[0]) produces an error \"Shape () must have rank 1\".\r\n\r\nThe args to [tf.tile](https://www.tensorflow.org/versions/r0.11/api_docs/python/array_ops.html#tile) are as follows:\r\n```\r\nArgs:\r\ninput: A Tensor. 1-D or higher.\r\nmultiples: A Tensor. Must be one of the following types: int32, int64. 1-D. Length must be the same as the number of dimensions in input\r\nname: A name for the operation (optional).\r\n```\r\nYour error is because you passed a scalar as the input, which is required to be 1-D or higher.\r\n\r\n> As a PS, one would expect that tiling with a multiple of 0 would give an empty tensor as result.\r\n```\r\n>>> with tf.Session() as sess: tf.tile([1], [1]).eval()\r\n... \r\narray([1], dtype=int32)\r\n>>> with tf.Session() as sess: tf.tile([1], [0]).eval()\r\n... \r\narray([], dtype=int32)\r\n```\r\n...as it does?\r\n\r\nPlease feel free to reopen if this doesn't solve your problem.", "Ah, it makes more sense now.  How in the world am I supposed to look at that traceback and realize that the message relates to tf.constant rather than tf.tile?\r\nAnd I probably should have realized that the goofy message came from Python...\r\nThanks for your help...", "> Ah, it makes more sense now. How in the world am I supposed to look at that traceback and realize that the message relates to tf.constant rather than tf.tile?\r\n\r\nIn *this* case, the source line in the stack backtrace refers to 'multiples_shape' and 'op.inputs[1]' so it's relatively straightforward.... but I agree that in general, error messages like this are often hard to grok!   \r\n```\r\n  File \"/usr/local/google/home/pbar/tensorflow.0.11rc2/local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1789, in _TileShape\r\n    multiples_shape = op.inputs[1].get_shape().with_rank(1)\r\n  File \"/usr/local/google/home/pbar/tensorflow.0.11rc2/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 650, in with_rank\r\n    raise ValueError(\"Shape %s must have rank %d\" % (self, rank))\r\n```\r\nPersonally, my usual course of action would be to just head to `[tensorflow/python/ops/array_ops.y:1789](https://github.com/tensorflow/tensorflow/blob/r0.11/tensorflow/python/ops/array_ops.py#L1789)'\r\n"]}, {"number": 5757, "title": "CPU slowdown with Quantized Eight bit graphs", "body": "In attempts to highly optimize my Tensorflow client application (and TF install) for consumer desktop hardware i've noticed (and noticed in other bug reports) that quantized eight bit graphs appear to run very slow.  My goal is to match the realtime 1 batch (1 x 299 x 299 x3 ) iOS performance that the Camera Example gets, yet I can't get a Desktop CPU compile of TF to get lower than roughly 150ms per frame, where in reality close to 16ms per frame is needed for roughly 60hz, or 33ms for 30hz performance. It appears somehow the iOS / ArmV7 build is able to achieve this performance unless I am missing something!\r\n\r\nFrom the discussion group, I was asked by @petewarden to start a bug based on findings\r\n\r\nThread here: https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!msg/discuss/PJwgfoeNIKs/jiegynxLBAAJ\r\n\r\nBriefly, I've added Stat Tracing to the Image Label example.  Modified source is here:\r\n\r\nhttps://gist.github.com/vade/18d7e72f633f9479c5080a251661ebd9\r\n\r\nIve downloaded compiled tensor flow with the following bazel build commands, referenced from the makefile for iOS which speeds things up just a bit more than the standard compile:\r\n\r\n` bazel build -c opt --copt=-mavx --cxxopt=-fno-exceptions --cxxopt=--std=c++11 --cxxopt=-DNDEBUG --cxxopt=-DNOTFDBG --cxxopt=-O2 --cxxopt=-DUSE_GEMM_FOR_CONV //tensorflow:libtensorflow_cc.so `\r\n\r\nI then compiled Image Label via the standard bazel command:\r\n\r\n` bazel build tensorflow/examples/label_image/...`\r\n\r\nAnd ran it with 4 graphs:\r\n\r\n* Standard InceptionV3\r\n* InceptionV3 run through Inference Optimizer Script\r\n* InceptionV3 run through Inference Optimizer and Quantizer in Weighted Rounding mode\r\n* InceptionV3 run through Inference Optimizer and Quantizer in eight bit mode\r\n\r\nThe output of the runs are documented here, in order:\r\n\r\n* https://gist.github.com/vade/a7d95da155c25dc8134f7cda8168e540\r\n* https://gist.github.com/vade/71af1cbd38864cb176ce64bcafb934de\r\n* https://gist.github.com/vade/e1923d7e7a9abfe1d8c912cc5d36a763\r\n* https://gist.github.com/vade/1d9d5e102878cfb42f9c02a4200b3a50\r\n\r\nNote the time for the Quantized Eight bit mode is roughly 2x longer than previous runs.\r\n\r\nAs a second set of data, my custom C++ app which uses the same lib_tensorfow_cc.so nets similar results to the benchmark :\u00a0\r\n\r\n\r\n* InceptionV3 (no optimizations or quantizations) \r\n* 222 frames took 32.598143 seconds\r\n* https://gist.github.com/vade/77a9314a5c7a5bda9b4a2c90f691a98e\r\n\r\n\r\n* InceptionV3 (Inference Optimizations - no quantizations)\r\n* 222 frames took 28.129690 seconds\r\n* https://gist.github.com/vade/1c5dc51015f5a0fa24f4e0a7209cabf9\r\n\r\n\r\n* InceptionV3 (Inference Optimizations & Quantizations Rounded)\r\n* 222 frames took 25.201791 seconds\r\n* https://gist.github.com/vade/ad8a2c42c5fbcf9be9f074c95d2e95ae\r\n\r\n\r\n* InceptionV3 (Inference Optimizations & Quantizations Eightbit)\r\n* 222 frames took 63.174700 seconds\r\n* https://gist.github.com/vade/d6dcce06861bf8932446ae5ed33d93bb\r\n\r\n\r\nOperating System:\r\n\r\n`Mac OS X 10.12.1\r\n2.8 GHz Intel Core i7\r\n16GB Ram\r\nXcode 8.1 / Command Line Tools from 7.3.1 and enabled via xcselect\r\n`\r\nIf installed from source, provide \r\n\r\n1. The commit hash\r\n\r\n`41285cf7a11fa3a2c2ead6b6e9adcec4232b18ad`\r\n\r\n2. The output of\r\n\r\n`Build label: 0.4.0-homebrew\r\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Nov 2 19:18:00 2016 (1478114280)\r\nBuild timestamp: 1478114280\r\nBuild timestamp as int: 1478114280`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)`\r\n\r\nSee above for source code for minimally modified label image source.\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nAttempted to run cuda but am targeting consumer desktop systems and would like similar performance to iOS targets for realtime or better than realtime performance for InceptionV3 / pool_3 feature vector determination and possibly labelling / classification.\r\n\r\nFor my system, cuda compilation netted similar results to CPU, although admittedly I did not enable batch sizes larger than 1. However, I think this is moot because iOS appears to be able to get realtime labelling and desktop cant, (is roughly 10x slower)\r\n\r\n### Logs or other output that would be helpful\r\nLogs and links provided in preamble  / description", "comments": ["Note  - before any one asks, compiling libtensorflow_cc.so with simpler bazel build flags like :\r\n\r\n`bazel build -c opt --copt=-mavx //tensorflow:libtensorflow_cc.so`\r\n\r\nResults in similar delta's in performance albeit a few percentage points slower all around. In other words, the phenomena appears to exist regardless of my attempts at optimizing the build.", "I also have faced with this problem. Inference time after 8-bit quantisation is 2x times slower for my iOS app.\r\n\r\nAlso after quantisation the quality of my regression model became much worse. Perhaps this is not related to this issue. Obviously, quantisation should affect on quality. But didn't expect such degradation.", "Hi All. \r\n\r\nLooking at the new graph transform tool as documented here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#optimizing-for-deployment\r\n\r\nIt appears that the quantize_nodes pass has the same issue as discussed in this bug. I understand this is because 8 bit quantized operations on the X86/ 64 platform have likely not been optimized, but I'm just putting a public notice that transform graph tool does not fix the problem.\r\n\r\nThanks for all the work everyone, transform graph is very helpful in managing model size for deployment! Id love to eek some more speed from CPU builds on desktop if possible :) I understand that is likely not a priority use case however.\r\n\r\nThanks again!", "For me it is important to be able to run the 8 bit model on my CPU before deploying to iOS as I want to check the score on my test test to determine how much degradation in quality is caused by the quantization before shipping off the model.", "@petewarden Are there any plans to work on this?  Optimizing the 8 bit case for CPU seems good for testing and development purposes.", "We would like to see this too, but since x86 is not a priority for mobile, it hasn't been high enough on our task list to get done yet. We don't have plans to do this in the medium term.", "Removed off topic comment about GPUs on Darwin.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I saw @petewarden post something regarding additional quantization optimization research boing done at Google - https://arxiv.org/abs/1712.05877 - but it appears mobile only :)", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing this, since I think it's a dupe of #2807. We are still actively working on quantization, but with the goal of deploying it in TF Lite for mobile."]}, {"number": 5756, "title": "Import Plottable css file so that TensorBoard works in FF/Safari", "body": "This is the r0.12 fix for https://github.com/tensorflow/tensorflow/issues/4856", "comments": ["@danmane, thanks for your PR! By analyzing the history of the files in this pull request, we identified @jart, @keveman and @dsmilkov to be potential reviewers.", "This is also separately merged to master, right?", "Yeah, I separately merged it to master"]}, {"number": 5755, "title": "Fix Plottable css importing, so that TB works in FF/Safari.", "body": "Fixes https://github.com/tensorflow/tensorflow/issues/4856", "comments": ["@danmane, thanks for your PR! By analyzing the history of the files in this pull request, we identified @jart, @keveman and @dsmilkov to be potential reviewers.", "I recommend also putting \"Fixes #4856\" on the third line of the commit message so it gets closed automatically when this is merged.", "@tensorflow-jenkins Test this please."]}]