[{"number": 27369, "title": "TfLite floor_mod.cc compilation warning fix", "body": "Compilation warning fix.", "comments": ["@Dayananda-V can you please check failed build errors", "@gbaned \r\n\r\nAm not able to find failure test case, just one broken test suite able to seen. Can you force trigger build again?", "@Dayananda-V  Thank you. Yes, I can force trigger the build again."]}, {"number": 27368, "title": "TF Keras io_utils_test missing test cases add", "body": "1-test case added for ask_to_proceed_with_overwrite", "comments": ["Nagging Reviewers @fchollet, @pavithrasv: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 29 days with no activity and the `awaiting review` label has been applied."]}, {"number": 27367, "title": "Compilation warnings handled", "body": "", "comments": ["> What kind of compilation warning is it for?\r\n\r\nBelow is the warning message i got:\r\n>warning: comparison between signed and unsigned integer expressions [-Wsign-compare]", "@lu-wang-g : The code is updated now, please check and approve, Thanks!", "> For simplicity, please use \"size_t i = 0 \"instead\r\n\r\n@lu-wang-g : Updated as per your comment, please check and approve, Thanks!", "@lu-wang-g Could you PTAL and approve.", "@lu-wang-g Could you PTAL and approve."]}, {"number": 27366, "title": "Lite: Add Op Refatcored", "body": "1:> Optimized Macro usage to reduce code duplication.\r\n2:> Unnecessary Template usage removed.\r\n3:> Wrong error message corrected.", "comments": ["@ANSHUMAN87 can you please check failed build errors ", "> @ANSHUMAN87 can you please check failed build errors\r\n\r\n@gbaned : I have checked, the errors not related to my changes.", "@rthadur : The errors are not related to my changes,, would you please help proceed with this PR, Thanks!", "@lu-wang-g can you please approve this PR internally", "I took a second look at this PR, and found that the changes and benefit were not very obvious at this moment. The original version works well, and let's keep it for now.", "> I took a second look at this PR, and found that the changes and benefit were not very obvious at this moment. The original version works well, and let's keep it for now.\r\n\r\n@lu-wang-g : Thanks for your effort in reviewing, I am sorry i could only partially agree on your conclusion. I totally agree that this PR does not address any functionality issues. But as per benefit part \r\ni beg to differ from your conclusion by stating that, this kind of PR even though not necessary to have, but it is important to have, as it falls under one of the objectives of TFLite(\"Reduced binary size\").\r\nPlease correct me if i am wrong, Thanks a lot!", "Considering reviewers comments, I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!", "@gbaned : I think @lu-wang-g  has not yet went through my reply to his conclusion earlier, also i strongly feel this PR is important to consider, so lets have conclusion from @lu-wang-g."]}, {"number": 27365, "title": "Adding tensorflow as a default custom_object in keras.models.load_model", "body": "Adding tensorflow and tf as default custom objects. The commit makes it easier to include simple tensorflow operations as part of keras models (like `tf.reduce_sum` for example) and then reload the model without issues.\r\n- Closes https://github.com/tensorflow/tensorflow/issues/27364", "comments": ["Just as a note, I realize that in this specific case, running `K.sum(x, 1)` instead of `tf.reduce_sum(x, 1)` will work, but there are a number of tensorflow Ops that are not available in `keras.backend`", "Nagging Reviewers @k-w-w, @fchollet: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "Thanks for the PR. The issue above is readily resolved by using `tf.reduce_sum` without wrapping it in a Lambda layer. This will result in a serializable TensorFlowOp layer.", "@fchollet but using the unwrapped ```tf.reduce_sum``` means you cannot use the incredibly convenient hdf5 model saving feature of keras (you have to save it with tf tools)"]}, {"number": 27364, "title": "Include TF in keras.models.load_model for custom functions", "body": "**System information**\r\n- TensorFlow version (you are using): Latest\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n```\r\nfrom keras import models, layers\r\nimport tensorflow as tf\r\nin_tensor = layers.Input((50, 1), name='InSpeech')\r\ndef sum_tensor(x):\r\n  return tf.reduce_sum(x, 1)\r\nsum_out = layers.Lambda(sum_tensor, name='SumTensor')(in_tensor)\r\nk_model = models.Model(inputs=[in_tensor],\r\n            outputs=[sum_out])\r\nk_model.save('sum_model.h5')\r\n```\r\n\r\nLoading the model\r\n\r\n```\r\nmodels.load_model('sum_model.h5')\r\n```\r\n\r\nor \r\n\r\n```\r\ntf.lite.TFLiteConverter.from_keras_model_file('sum_model.h5')\r\n```\r\n\r\nResults in the error message:\r\n```\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py in sum_tensor(x)\r\n      3 in_tensor = layers.Input((50, 1), name='InSpeech')\r\n      4 def sum_tensor(x):\r\n----> 5   return tf.reduce_sum(x, 1)\r\n      6 sum_out = layers.Lambda(sum_tensor, name='SumTensor')(in_tensor)\r\n      7 k_model = models.Model(inputs=[in_tensor],\r\n\r\nNameError: name 'tf' is not defined\r\n```\r\n\r\nWhile running `models.load_model('sum_model.h5', custom_objects={'tf': tf})` works fine. Unfortunately with the tflite wrapper function to convert models you cannot specify custom objects.\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo, it would just make the default custom objects include tensorflow.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone using simple tensorflow-derived lambda functions inside of the HDF5 models in Keras. In particular people hoping to convert the models to tflite.\r\n\r\n", "comments": ["Closing this issue based on the explanation given in corresponding PR. Feel free to reopen if have any further questions. Thanks!", "@ymodak I have since commented on the PR, while the solution is correct it doesn't allow for saving models (as far as I can tell) with the keras hdf5 format. \r\n\r\nAnother use case for the issue is with EfficientNet's use of `tf.nn.swish` which isn't available in keras"]}, {"number": 27363, "title": "Correct Softmax Error messages", "body": "", "comments": ["Nagging Reviewers @lu-wang-g: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "@joyalbin can you please check build failures", "@joyalbin Could you please check build failures and resolve the conflicts? Thanks!", "@joyalbin Gentle ping to check build failures and resolve the conflicts? Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 27362, "title": "Porting the Conv kernel from Tensorflow Lite to Tensorflow Lite Micro", "body": "This PR ports the Conv kernel in Tensorflow Lite to Tensorflow Lite Micro.", "comments": ["I believe we have some engineers already working on this kernel. @petewarden can offer guidance.", "Nits: next time, better not to use force push so that the comment history and diff can be preserved.", "@jenselo can you please address Ubuntu Sanity build failures.", "> @jenselo can you please address Ubuntu Sanity build failures.\r\n\r\nYes, I'm looking into it today.", "\r\n\r\n> Nits: next time, better not to use force push so that the comment history and diff can be preserved.\r\n\r\nSorry, I didn't read this comment until after I did the force push. I'll think about that going forward. The only thing that changed in the last two pushes were the:\r\n  - \"2017\" > \"2019\" in conv_test.cc\r\n  - The BUILD file in which conv.cc to the top of the list of source-files after I ran buildifier on it.", "> Sry for the delay. This looks great! Thanks!\r\n> \r\n> Would love to sync with you on the next plan to avoid duplication of efforts. What are the operators missing in your use case that you would like the support next? Thanks\r\n\r\n\r\n\r\n> Sry for the delay. This looks great! Thanks!\r\n> \r\n> Would love to sync with you on the next plan to avoid duplication of efforts. What are the operators missing in your use case that you would like the support next? Thanks\r\n\r\nShort term it's only MaxPool, which I'm currently working on. In the long term it's more unclear.", "@jenselo please resolve conflicts.", "> @jenselo please resolve conflicts.\r\n\r\nDone, although it resulted in a merge commit since I did it through the Github web UI. Should I redo it in a way that doesn't leave a merge commit?\r\n", "@jenselo could you please fix build errors. Thanks !"]}, {"number": 27361, "title": "Sample code is having syntax error in TensorFlow nodejs code", "body": "**System information**\r\n- TensorFlow version:\r\n- Doc Link:\r\n\r\n\r\n**Syntax error in TensorFlow nodejs example**\r\n[See sample code for Node.js usage](https://www.tensorflow.org/js/tutorials/setup)\r\nExtra semicolon is causing the syntax error.\r\n\r\n`onEpochEnd: (epoch, log) => console.log(`Epoch ${epoch}: loss = ${log.loss}`);`\r\nIn the above line, semi-colon in the end should be removed.\r\n****\r\n", "comments": ["The link you have referenced does not refer to the sample code you are referring too. Can you please take a look and provide pointers? Thanks!", "Thanks for reporting. Closing since its  a duplicate #27237", "Thanks for reporting @varun-jalandery. @ymodak I think you can close the original issue the #27237"]}, {"number": 27360, "title": "TFLite Named Inputs and Outputs", "body": "Using models with multiple inputs and outputs it would be helpful to have the names of the input and output channels available inside the Java and C++ APIs for TFLite.\r\n\r\n**System information**\r\n- TensorFlow version (you are using): latest\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nThe model inputs and outputs can be listed and queried by size but not by name (no argument to get name of a `Tensor` or get input/output tensor names from an `Interpreter`. It would be very helpful for using models if you could get the names of the inputs and outputs particularly since many models have multiple of each (that are often not discriminated by size).\r\n\r\n**Will this change the current api? How?**\r\n\r\nIt would just at methods `.getInputNames` and `.getOutputNames` to the `org.tensorflow.lite.Interpreter` class or add `.name` to the `org.tensorflow.lite.Tensor` class \r\n\r\n**Who will benefit with this feature?**\r\n\r\nIt would make it a lot easier to have backward compatible models (newer versions predict more features and code doesn't always have to use all of the outputs a model provides). It would also allow reusing the same model for different applications and have the code select the output with the correct name rather than hard coding an output index to take or checking by output size (my current solutions).\r\n\r\n**Any Other info.**\r\n", "comments": ["With the interpreter class in python you can easily get names\r\n```\r\n# Load TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_path=str(tflite_path))\r\ninterpreter.allocate_tensors()\r\n\r\n# Get output tensors.\r\noutput_details = interpreter.get_output_details()\r\nfor c_output_details in output_details:\r\n    print(c_output_details['name'])\r\n```\r\n"]}, {"number": 27359, "title": "[INTEL MKL] Revert changes in mkl_concat_op. ", "body": "Temporarily reverting changes to workaround the squeeze op failure in SSD-Mobilenet.", "comments": []}, {"number": 27358, "title": "Fix an overflow issue in bfc_allocator.", "body": "A segmentation fault is raised when the total allocation amount exceeds 1024 MB, simply due to overflow.\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27358) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "I have received a mail informing the corporate CLA is completed.\r\nWhat should I do next to pass the CLA check?", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27358) for more info**.\n\n<!-- ok -->", "@rthadur Please take a look on this, thanks."]}, {"number": 27357, "title": "-D_GLIBCXX_DEBUG compiler flag causes prediction failure", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n\r\n\r\n- OS Platform and Distribution Linux Ubuntu 18.04\r\n- TensorFlow installed from source: v1.13.1\r\n- Bazel version 0.21\r\n- GCC/Compiler version: gcc version 7.3.0 (Ubuntu 7.3.0-27ubuntu1~18.04) \r\n\r\n**Describe the current behavior**\r\n\r\n-D_GLIBCXX_DEBUG compiled example code gives:\r\n\r\nInvalid argument: Must specify at least one target to fetch or execute.\r\n\r\nCompiled without this flag:\r\n\r\nRun session successfully\r\nTensor<type: float shape: [] values: 6>\r\noutput value: 6\r\n\r\n\r\n**Code to reproduce the issue**\r\n\r\n#include <tensorflow/core/platform/env.h>\r\n#include <tensorflow/core/public/session.h>\r\n#include <iostream>\r\n\r\nint main(int argc, char **argv)\r\n{\r\n\r\n    tensorflow::GraphDef GraphDef;\r\n    tensorflow::Session* Session = nullptr;\r\n    tensorflow::Status Status;\r\n\r\n    Status = tensorflow::ReadBinaryProto(tensorflow::Env::Default(), argv[1], &GraphDef);\r\n    if (!Status.ok())\r\n    {\r\n      printf(\"Error reading graph definition from %s: %s\\n\", argv[1], Status.ToString().c_str());\r\n      return false;\r\n    }\r\n\r\n    Session = tensorflow::NewSession(tensorflow::SessionOptions());\r\n    if (Session == nullptr)\r\n    {\r\n      printf(\"Could not create Tensorflow session.\\n\");\r\n      return false;\r\n    }\r\n\r\n    Status = Session->Create(GraphDef);\r\n    if (!Status.ok())\r\n    {\r\n      printf(\"Error creating graph: %s\\n\", Status.ToString().c_str());\r\n      return false;\r\n    }\r\n\r\n    //predict\r\n\r\n    tensorflow::Tensor a(tensorflow::DT_FLOAT, tensorflow::TensorShape());\r\n    a.scalar<float>()() = 3.0;\r\n\r\n    tensorflow::Tensor b(tensorflow::DT_FLOAT, tensorflow::TensorShape());\r\n    b.scalar<float>()() = 2.0;\r\n\r\n    std::vector<std::pair<std::string, tensorflow::Tensor>> inputs = {\r\n      { \"a\", a },\r\n      { \"b\", b },\r\n    };\r\n\r\n    std::vector<tensorflow::Tensor> outputs;\r\n\r\n    auto status = Session->Run(inputs, {\"c\"}, {}, &outputs);\r\n    if (!status.ok()) {\r\n      std::cerr << status.ToString() << std::endl;\r\n      return 1;\r\n    } else {\r\n      std::cout << \"Run session successfully\" << std::endl;\r\n    }\r\n\r\n    auto output_c = outputs[0].scalar<float>();\r\n    std::cout << outputs[0].DebugString() << std::endl;\r\n    std::cout << \"output value: \" << output_c() << std::endl;\r\n    Session->Close();\r\n\r\n    return 0;\r\n}\r\n\r\n**Other info / logs**\r\n\r\nThe simple graph file attached - supply its path as a command line argument\r\n[graph.pb.gz](https://github.com/tensorflow/tensorflow/files/3027851/graph.pb.gz)", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n", "Hi,\r\n\r\nCould you explain your comments please.\r\n\r\nDid you manage to reproduce the problem. At the very least  the error message generated (\"Invalid argument: Must specify at least one target to fetch or execute\") is incorrect as when compiled without the debug flag the sample application executes cleanly, meaning these arguments were supplied as they are hard coded. \r\n\r\nIf this is neither a bug or a feature request what is it ?\r\n\r\nThanks \r\n\r\nSean"]}, {"number": 27356, "title": "tft.sparse_tensor_to_dense_with_shape throwing indices out of bound error on large dataset", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 - Dataflow runtime\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 1.13.1\r\n- TensorFlow Transform: 0.12.0\r\n- Python version: 2.7.x\r\n\r\n**Describe the current behavior**\r\nTensorflow transform throwing error indices out of bound when processing large number of records (> 1mio) but running well on small records (< 10k).\r\nNote that this tensorflow transform run on Beam pipeline on Google Cloud Dataflow.\r\n\r\n**Describe the expected behavior**\r\nTransform should be able to process large number of records.\r\n\r\n**Code to reproduce the issue**\r\nTensorflow preprocessing function to convert sparse to dense tensor.\r\n\r\n```\r\ndef preprocessing_fn(inputs):\r\n        split = tf.string_split(\r\n            inputs['words'], delimiter=\"\", skip_empty=True)\r\n\r\n        token = tft.compute_and_apply_vocabulary(\r\n            split, vocab_filename='char_token')\r\n\r\n        token_pad = tft.sparse_tensor_to_dense_with_shape(\r\n            token, shape=(None, 100))\r\n\r\n        return {\r\n            'label': inputs['label'],\r\n            'token': token_pad,\r\n        }\r\n```\r\n\r\n**Other info / logs**\r\nDetailed logs: Captured from Stackdriver Dataflow logs.\r\n\r\n```\r\nindices[66312] = [4668,100] is out of bounds: need 0 <= index < [7710,100]\r\n\t [[node transform/transform/SparseToDense (defined at /usr/local/lib/python2.7/dist-packages/tensorflow_transform/saved/saved_transform_io.py:227) ]]\r\n\r\nCaused by op u'transform/transform/SparseToDense', defined at:\r\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\r\n    \"__main__\", fname, loader, pkg_name)\r\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\r\n    exec code in run_globals\r\n  File \"/usr/local/lib/python2.7/dist-packages/dataflow_worker/start.py\", line 86, in <module>\r\n    main()\r\n  File \"/usr/local/lib/python2.7/dist-packages/dataflow_worker/start.py\", line 82, in main\r\n    batchworker.BatchWorker(properties, sdk_pipeline_options).run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/dataflow_worker/batchworker.py\", line 840, in run\r\n    deferred_exception_details=deferred_exception_details)\r\n  File \"/usr/local/lib/python2.7/dist-packages/dataflow_worker/batchworker.py\", line 642, in do_work\r\n    work_executor.execute()\r\n  File \"/usr/local/lib/python2.7/dist-packages/dataflow_worker/executor.py\", line 172, in execute\r\n    op.start()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_transform/beam/impl.py\", line 436, in process\r\n    lambda: self._make_graph_state(saved_model_dir))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_transform/beam/shared.py\", line 221, in acquire\r\n    return _shared_map.acquire(self._key, constructor_fn)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_transform/beam/shared.py\", line 183, in acquire\r\n    result = control_block.acquire(constructor_fn)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_transform/beam/shared.py\", line 85, in acquire\r\n    result = constructor_fn()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_transform/beam/impl.py\", line 436, in <lambda>\r\n    lambda: self._make_graph_state(saved_model_dir))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_transform/beam/impl.py\", line 412, in _make_graph_state\r\n    self._exclude_outputs, tf_config)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_transform/beam/impl.py\", line 316, in __init__\r\n    saved_model_dir, {}))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_transform/saved/saved_transform_io.py\", line 370, in partially_apply_saved_transform_internal\r\n    saved_model_dir, logical_input_map, tensor_replacement_map)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_transform/saved/saved_transform_io.py\", line 227, in _partially_apply_saved_transform_impl\r\n    input_map=input_map)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1435, in import_meta_graph\r\n    meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1457, in _import_meta_graph_with_return_elements\r\n    **kwargs))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/meta_graph.py\", line 806, in import_scoped_meta_graph_with_return_elements\r\n    return_elements=return_elements)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 442, in import_graph_def\r\n    _ProcessNewOps(graph)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 235, in _ProcessNewOps\r\n    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3433, in _add_new_tf_operations\r\n    for c_op in c_api_util.new_tf_operations(self)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3325, in _create_op_from_tf_operation\r\n    ret = Operation(c_op, self)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): indices[66312] = [4668,100] is out of bounds: need 0 <= index < [7710,100]\r\n\t [[node transform/transform/SparseToDense (defined at /usr/local/lib/python2.7/dist-packages/tensorflow_transform/saved/saved_transform_io.py:227) ]]\r\n while applying transform function for tensors [u'label', u'token']\r\n```\r\n\r\n\r\n", "comments": ["False alarm. This issue caused by input shape tensor is greater than the desired output tensors in sparse_tensor_to_dense_with_shape"]}, {"number": 27355, "title": "Dataset c++ extend documentation is outdated for tf 2.0 & DatasetV2", "body": "**System information**\r\n- TensorFlow version: 2.0\r\n- Doc Link: https://www.tensorflow.org/guide/extend/formats\r\n\r\n**Describe the documentation issue**\r\n\r\nThe C++ code to extend Dataset (especially since DatasetV2) is outdated.\r\n\r\nHere is a working version of files needed in the documentation : https://github.com/vrince/tensorflow_addons/tree/master/tensorflow_addons/dataset\r\n\r\nNOTE: the only part I am not really sure about is this one : https://github.com/vrince/tensorflow_addons/blob/master/tensorflow_addons/dataset/cc/my_dataset.cpp#L76 ... basically let it as it was but I don't see the point.\r\n\r\nThere is also and external test running from python and bazel files.\r\n\r\nNot sure where or if I even can do a pull request to change the doc.", "comments": ["Thanks @vrince. Could you be little more specific and explain which part of the doc need to be changed? Thanks!", "Hi ! Sorry for the delay ... It a little hard for me to provide meaningful diff in issues comment. Can you point me to the source of the doc so I'll patch it and send you the difference ? ", "@vrince Do you want to modify the text in this [webpage](https://www.tensorflow.org/guide/extend/formats) ? or change codes like dataset_ops.py, etc.? Thanks!", "Basically what need to be changed is the webpage itself. Here is what I changed for the `dataset.cpp` file :\r\n\r\n```diff\r\n #include \"tensorflow/core/framework/op.h\"\r\n #include \"tensorflow/core/framework/shape_inference.h\"\r\n \r\n-namespace myproject\r\n-{\r\n-namespace\r\n-{\r\n-\r\n using ::tensorflow::DT_STRING;\r\n using ::tensorflow::PartialTensorShape;\r\n using ::tensorflow::Status;\r\n \r\n-class MyReaderDatasetOp : public tensorflow::DatasetOpKernel\r\n+class MyReaderDatasetOp : public tensorflow::data::DatasetOpKernel\r\n {\r\n   public:\r\n-    MyReaderDatasetOp(tensorflow::OpKernelConstruction *ctx)\r\n+    explicit MyReaderDatasetOp(tensorflow::OpKernelConstruction *ctx)\r\n         : DatasetOpKernel(ctx)\r\n     {\r\n         // Parse and validate any attrs that define the dataset using\r\n@@ -23,7 +18,7 @@ class MyReaderDatasetOp : public tensorflow::DatasetOpKernel\r\n     }\r\n \r\n     void MakeDataset(tensorflow::OpKernelContext *ctx,\r\n-                     tensorflow::DatasetBase **output) override\r\n+                     tensorflow::data::DatasetBase **output) override\r\n     {\r\n         // Parse and validate any input tensors that define the dataset using\r\n         // `ctx->input()` or the utility function\r\n@@ -35,13 +30,13 @@ class MyReaderDatasetOp : public tensorflow::DatasetOpKernel\r\n     }\r\n \r\n   private:\r\n-    class Dataset : public tensorflow::GraphDatasetBase\r\n+    class Dataset : public tensorflow::DatasetBase\r\n     {\r\n       public:\r\n-        Dataset(tensorflow::OpKernelContext *ctx) : GraphDatasetBase(ctx) {}\r\n+        Dataset(tensorflow::OpKernelContext *ctx) : tensorflow::data::DatasetBase(tensorflow::data::DatasetContext(ctx)) {}\r\n \r\n         std::unique_ptr<tensorflow::IteratorBase> MakeIteratorInternal(\r\n-            const string &prefix) const override\r\n+            const std::string &prefix) const\r\n         {\r\n             return std::unique_ptr<tensorflow::IteratorBase>(new Iterator(\r\n                 {this, tensorflow::strings::StrCat(prefix, \"::MyReader\")}));\r\n@@ -57,6 +52,7 @@ class MyReaderDatasetOp : public tensorflow::DatasetOpKernel\r\n             static auto *const dtypes = new tensorflow::DataTypeVector({DT_STRING});\r\n             return *dtypes;\r\n         }\r\n+\r\n         const std::vector<PartialTensorShape> &output_shapes() const override\r\n         {\r\n             static std::vector<PartialTensorShape> *shapes =\r\n@@ -64,15 +60,16 @@ class MyReaderDatasetOp : public tensorflow::DatasetOpKernel\r\n             return *shapes;\r\n         }\r\n \r\n-        string DebugString() const override { return \"MyReaderDatasetOp::Dataset\"; }\r\n+        std::string DebugString() const override { return \"MyReaderDatasetOp::Dataset\"; }\r\n \r\n       protected:\r\n         // Optional: Implementation of `GraphDef` serialization for this dataset.\r\n         //\r\n         // Implement this method if you want to be able to save and restore\r\n         // instances of this dataset (and any iterators over it).\r\n-        Status AsGraphDefInternal(DatasetGraphDefBuilder *b,\r\n-                                  tensorflow::Node **output) const override\r\n+        Status AsGraphDefInternal(tensorflow::SerializationContext *ctx,\r\n+                                  DatasetGraphDefBuilder *b,\r\n+                                  tensorflow::Node **output) const\r\n         {\r\n             // Construct nodes to represent any of the input tensors from this\r\n             // object's member variables using `b->AddScalar()` and `b->AddVector()`.\r\n@@ -85,8 +82,8 @@ class MyReaderDatasetOp : public tensorflow::DatasetOpKernel\r\n         class Iterator : public tensorflow::DatasetIterator<Dataset>\r\n         {\r\n           public:\r\n-            explicit Iterator(const Params &params)\r\n-                : DatasetIterator<Dataset>(params), i_(0) {}\r\n+            explicit Iterator(const Params &params) : DatasetIterator<Dataset>(params),\r\n+                                                      i_(0) {}\r\n \r\n             // Implementation of the reading logic.\r\n             //\r\n@@ -111,7 +108,7 @@ class MyReaderDatasetOp : public tensorflow::DatasetOpKernel\r\n                 {\r\n                     // Create a scalar string tensor and add it to the output.\r\n                     tensorflow::Tensor record_tensor(ctx->allocator({}), DT_STRING, {});\r\n-                    record_tensor.scalar<string>()() = \"MyReader!\";\r\n+                    record_tensor.scalar<std::string>()() = \"MyReader!\";\r\n                     out_tensors->emplace_back(std::move(record_tensor));\r\n                     ++i_;\r\n                     *end_of_sequence = false;\r\n@@ -145,7 +142,7 @@ class MyReaderDatasetOp : public tensorflow::DatasetOpKernel\r\n \r\n           private:\r\n             tensorflow::mutex mu_;\r\n-            int64 i_ GUARDED_BY(mu_);\r\n+            tensorflow::int64 i_ GUARDED_BY(mu_);\r\n         };\r\n     };\r\n };\r\n@@ -164,6 +161,3 @@ REGISTER_OP(\"MyReaderDataset\")\r\n // Register the kernel implementation for MyReaderDataset.\r\n REGISTER_KERNEL_BUILDER(Name(\"MyReaderDataset\").Device(tensorflow::DEVICE_CPU),\r\n                         MyReaderDatasetOp);\r\n-\r\n-} // namespace\r\n-} // namespace myproject\r\n```\r\n\r\nHere the `dataset_ops.py` : \r\n\r\n```diff\r\n@@ -1,46 +1,25 @@\r\n-import tensorflow as tf\r\n \r\n-# Assumes the file is in the current working directory.\r\n-my_reader_dataset_module = tf.load_op_library(\"./my_reader_dataset_op.so\")\r\n+\"\"\"Dataset ops.\"\"\"\r\n+from __future__ import absolute_import\r\n+from __future__ import division\r\n+from __future__ import print_function\r\n \r\n+import tensorflow as tf\r\n+from tensorflow.python.platform import resource_loader\r\n+from tensorflow.python.data.ops import dataset_ops\r\n+from tensorflow.python.data.util import structure\r\n+from tensorflow.python.framework import dtypes\r\n \r\n-class MyReaderDataset(tf.data.Dataset):\r\n+my_reader_dataset_module = tf.load_op_library(\r\n+    resource_loader.get_path_to_datafile(\"_dataset_ops.so\"))\r\n \r\n-    def __init__(self):\r\n-        super(MyReaderDataset, self).__init__()\r\n-        # Create any input attrs or tensors as members of this class.\r\n \r\n-    def _as_variant_tensor(self):\r\n-        # Actually construct the graph node for the dataset op.\r\n-        #\r\n-        # This method will be invoked when you create an iterator on this dataset\r\n-        # or a dataset derived from it.\r\n-        return my_reader_dataset_module.my_reader_dataset()\r\n-\r\n-    # The following properties define the structure of each element: a scalar\r\n-    # <a href=\"../../api_docs/python/tf#string\"><code>tf.string</code></a> tensor. Change these properties to match the `output_dtypes()`\r\n-    # and `output_shapes()` methods of `MyReaderDataset::Dataset` if you modify\r\n-    # the structure of each element.\r\n-    @property\r\n-    def output_types(self):\r\n-        return tf.string\r\n+class MyReaderDataset(dataset_ops.DatasetSource):\r\n \r\n-    @property\r\n-    def output_shapes(self):\r\n-        return tf.TensorShape([])\r\n+    def __init__(self):\r\n+        super(MyReaderDataset, self).__init__(\r\n+            my_reader_dataset_module.my_reader_dataset())\r\n \r\n     @property\r\n-    def output_classes(self):\r\n-        return tf.Tensor\r\n-\r\n-\r\n-if __name__ == \"__main__\":\r\n-    # Create a MyReaderDataset and print its elements.\r\n-    with tf.Session() as sess:\r\n-        iterator = MyReaderDataset().make_one_shot_iterator()\r\n-        next_element = iterator.get_next()\r\n-        try:\r\n-            while True:\r\n-                print(sess.run(next_element))  # Prints \"MyReader!\" ten times.\r\n-        except tf.errors.OutOfRangeError:\r\n-            pass\r\n+    def _element_structure(self):\r\n+        return structure.TensorStructure(dtypes.string, [])\r\n```\r\nTo follow the rest of the documentation I created two files one to test the thing one to build it with bazel : \r\n\r\n`dataset_ops_test.py`\r\n\r\n```py\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow_addons.utils.python import test_utils\r\n\r\nfrom tensorflow_addons.dataset import dataset_ops\r\n\r\n\r\nclass DatasetOpsTest(tf.test.TestCase):\r\n    def test_dataset(self):\r\n        dataset = dataset_ops.MyReaderDataset()\r\n        i = 0\r\n        for d in dataset:\r\n            self.assertAllEqual(d, tf.constant(\"MyReader!\"))\r\n            i += 1\r\n        self.assertEquals(i, 10)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    tf.test.main()\r\n```\r\n\r\n`BUILD` file\r\n\r\n```\r\nlicenses([\"notice\"])  # Apache 2.0\r\n\r\npackage(default_visibility = [\"//visibility:public\"])\r\n\r\ncc_binary(\r\n    name = \"_dataset_ops.so\",\r\n    srcs = [\r\n        \"cc/my_dataset.cpp\"\r\n    ],\r\n    linkshared = 1,\r\n    deps = [\r\n        \"@local_config_tf//:libtensorflow_framework\",\r\n        \"@local_config_tf//:tf_header_lib\",\r\n    ],\r\n    # see why -DNDEBUG https://github.com/tensorflow/tensorflow/issues/17316\r\n    copts = [\"-pthread\", \"-std=c++11\", \"-D_GLIBCXX_USE_CXX11_ABI=0\", \"-DNDEBUG\"]\r\n)\r\n\r\npy_library(\r\n    name = \"dataset_ops_py\",\r\n    srcs = ([\r\n        \"__init__.py\",\r\n        \"dataset_ops.py\",\r\n    ]),\r\n    data = [\r\n        \":_dataset_ops.so\",\r\n        \"//tensorflow_addons/utils:utils_py\",\r\n    ],\r\n    srcs_version = \"PY2AND3\",\r\n)\r\n\r\npy_test(\r\n    name = \"dataset_ops_test\",\r\n    size = \"small\",\r\n    srcs = [\r\n        \"dataset_ops_test.py\",\r\n    ],\r\n    main = \"dataset_ops_test.py\",\r\n    deps = [\r\n        \":dataset_ops_py\",\r\n    ],\r\n    srcs_version = \"PY2AND3\"\r\n)\r\n```", "This doc doesn't exist anymore."]}, {"number": 27354, "title": "Move LazyLoader warning to stderr", "body": "Commit d32dee99 add a warning message to LazyLoader class. The extra lines written to stdout breaks some of my scripts. I think that the warning message should be moved to stderr.", "comments": ["@cuihaoleo  please resolve conflicts", "Already fixed in 64e362bb910e9121480cbdf27162968496533bcd , no need for this PR."]}, {"number": 27353, "title": "tf.keras is ignoring specified step_per_epoch when keras doesn't", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):unknown\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):colab\r\n- TensorFlow version (use command below):1.13\r\n- Python version:python 3\r\n- Bazel version (if compiling from source):unknown\r\n- GCC/Compiler version (if compiling from source):unknown\r\n- CUDA/cuDNN version:unknown\r\n- GPU model and memory:unknown\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n`history = model.fit_generator(\r\n      train_generator,\r\n      steps_per_epoch=8,  \r\n      epochs=15,\r\n      verbose=1)`\r\nIf train_generator is a instance of Sequence.( in my case, coming from flow_from_directory), steps_per_epoch is overridden by len(train_generator) in this file [training_generator.py](https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/keras/engine/training_generator.py#L371)\r\nThis doesn't happen in keras. In keras, steps_per_epoch is kept.\r\n\r\n**Describe the expected behavior**\r\nUse specified steps_per_epoch instead of len(data) if specified.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nI find this when doing a coursera online course. The notebook i used can be found at [link](https://github.com/lmoroney/dlaicourse/blob/master/Course%201%20-%20Part%208%20-%20Lesson%202%20-%20Notebook.ipynb)\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "@jvishnuvardhan I understand what is happening. I don't need stackoverflown for clarification. \r\nThe problem I'm reporting is that the parameter is not working as expected in tf.keras, and is behaving differently in keras and tf.keras. I heard that you are trying to keep tf.keras api in agreement with keras. This could be fixed on either ends to make them behave the same.", "got the same issue with the last keras and tensorflow:\r\n\r\n```\r\ntf.__version__, tf.keras.__version__\r\n('1.13.1', '2.2.4-tf')\r\n```\r\n\r\n`model.fit_generator` just ignores `steps_per_epoch` parameter.", "Thanks for the issue! It seems reasonable to prefer `steps_per_epoch` (if provided) over the `Sequence` length, will look into this", "Looks like this has already been fixed in the TF 2.0 nightly", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27353\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27353\">No</a>\n"]}, {"number": 27352, "title": "[ROCM] Build failed due to libhip_hcc.so, but HIP and HCC are installed", "body": "**System information**\r\n- OS Platform and Distribution : Manjaro Linux x86_64\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 2.0.0-alpha0\r\n- Python version: Python 3.7.2\r\n- Installed using virtualenv? pip? conda?: No\r\n- Bazel version (if compiling from source): Build label: 0.22.0- (@non-git)\r\n- GCC/Compiler version : gcc 8.2.1 ; HCC clang version 9.0.0; clang version 7.0.1,\r\n- CUDA/cuDNN version: NA / ROCm 2.2.0-1 ; ROCm-opencl-runtime 2.2.0-3, HIP version 2.2.0-1\r\n- GPU model and memory: Vega 64, 8GB\r\n\r\n**Describe the problem**\r\n Building with ROCm support fails.\r\n\r\nInside  2.0.0-alpha0 folder: \r\n- ./configure\r\n- bazel build --config=opt --config=rocm //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\n**Any other info / logs**\r\n```\r\nINFO: Invocation ID: a35bccb9-a4af-40de-bede-9b7df9283be4\r\nDEBUG: Rule 'build_bazel_rules_swift' modified arguments {\"commit\": \"001736d056d7eae20f1f4da41bc9e6f036857296\", \"shallow_since\": \"1547844730 -0800\"} and dropped [\"tag\"]\r\nDEBUG: /home/auyer/.cache/bazel/_bazel_auyer/965b880f59234d9db802b221eaf21de8/external/build_bazel_rules_apple/apple/repositories.bzl:35:5:\r\nWARNING: `build_bazel_rules_apple` depends on `bazel_skylib` loaded from https://github.com/bazelbuild/bazel-skylib.git (tag 0.6.0), but we have detected it already loaded into your workspace from None (tag None). You may run into compatibility issues. To silence this warning, pass `ignore_version_differences = True` to `apple_rules_dependencies()`.\r\n\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': in /home/auyer/tensorflow/tensorflow-2.0.0-alpha0/tensorflow/tensorflow.bzl: Encountered error while reading extension file 'rocm/build_defs.bzl': no such package '@local_config_rocm//rocm': Traceback (most recent call last):\r\n\tFile \"/home/auyer/tensorflow/tensorflow-2.0.0-alpha0/third_party/gpus/rocm_configure.bzl\", line 755\r\n\t\t_create_local_rocm_repository(repository_ctx)\r\n\tFile \"/home/auyer/tensorflow/tensorflow-2.0.0-alpha0/third_party/gpus/rocm_configure.bzl\", line 631, in _create_local_rocm_repository\r\n\t\t_find_libs(repository_ctx, rocm_config)\r\n\tFile \"/home/auyer/tensorflow/tensorflow-2.0.0-alpha0/third_party/gpus/rocm_configure.bzl\", line 376, in _find_libs\r\n\t\t_find_rocm_lib(\"hip_hcc\", repository_ctx, cpu_value, ...)\r\n\tFile \"/home/auyer/tensorflow/tensorflow-2.0.0-alpha0/third_party/gpus/rocm_configure.bzl\", line 361, in _find_rocm_lib\r\n\t\tauto_configure_fail((\"Cannot find rocm library %s\" %...))\r\n\tFile \"/home/auyer/tensorflow/tensorflow-2.0.0-alpha0/third_party/gpus/rocm_configure.bzl\", line 121, in auto_configure_fail\r\n\t\tfail((\"\\n%sROCm Configuration Error:%...)))\r\n\r\nROCm Configuration Error: Cannot find rocm library libhip_hcc.so\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow/tools/pip_package': in /home/auyer/tensorflow/tensorflow-2.0.0-alpha0/tensorflow/tensorflow.bzl: Encountered error while reading extension file 'rocm/build_defs.bzl': no such package '@local_config_rocm//rocm': Traceback (most recent call last):\r\n\tFile \"/home/auyer/tensorflow/tensorflow-2.0.0-alpha0/third_party/gpus/rocm_configure.bzl\", line 755\r\n\t\t_create_local_rocm_repository(repository_ctx)\r\n\tFile \"/home/auyer/tensorflow/tensorflow-2.0.0-alpha0/third_party/gpus/rocm_configure.bzl\", line 631, in _create_local_rocm_repository\r\n\t\t_find_libs(repository_ctx, rocm_config)\r\n\tFile \"/home/auyer/tensorflow/tensorflow-2.0.0-alpha0/third_party/gpus/rocm_configure.bzl\", line 376, in _find_libs\r\n\t\t_find_rocm_lib(\"hip_hcc\", repository_ctx, cpu_value, ...)\r\n\tFile \"/home/auyer/tensorflow/tensorflow-2.0.0-alpha0/third_party/gpus/rocm_configure.bzl\", line 361, in _find_rocm_lib\r\n\t\tauto_configure_fail((\"Cannot find rocm library %s\" %...))\r\n\tFile \"/home/auyer/tensorflow/tensorflow-2.0.0-alpha0/third_party/gpus/rocm_configure.bzl\", line 121, in auto_configure_fail\r\n\t\tfail((\"\\n%sROCm Configuration Error:%...)))\r\n\r\nROCm Configuration Error: Cannot find rocm library libhip_hcc.so\r\nINFO: Elapsed time: 0.066s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package\r\n    Fetching @local_config_rocm; fetching\r\n```", "comments": ["Hi @auyer, Manjaro Linux x86_64 is not in ROCm official support list, [here](https://github.com/RadeonOpenCompute/ROCm#supported-operating-systems---new-operating-systems-available) is the document for reference.\r\nAlthernatively, you can install upstream kernels (4.18 or newer) and use the ROCm pre-build docker images to try tensorflow on ROCm stack.\r\n\r\nThe following is the TF-ROCm docker registry, instructions are provided there for docker configurations:\r\nhttps://hub.docker.com/r/rocm/tensorflow/\r\n", "I know Arch is not an officialy supported OS, but I\u2019ve built an older version of ROCm TF without problems before.\r\n\r\nAbout the Docker image, your URL requires authentication, and renders 404 when autjenticated.\r\nI managed to find this image on [docker hub](https://hub.docker.com/r/rocm/tensorflow/). If its the same, please edit your answer to help ones who decide to take this path. \r\n\r\nFeel free to close this issue if you fill fit. Thanks", "Thanks, @auyer , I've updated the link in my first comment.\r\nBesides, I'd remind that not all the required code has been upstreamed to the main tensorflow repo till now - we are actively working on it. \r\nYou can refer to our local fork at the moment for complete information and full support on ROCm-TF project:\r\nhttps://github.com/ROCmSoftwarePlatform/tensorflow-upstream\r\n", "Thanks!\r\nI also tried building with the release downloaded from their repository, and got the same results.\r\n\r\nShould I close this issue and post it in their repository then?", "@auyer Please post it in that repo. This will get some visibility and most probably will get resolved soon.  Close the issue as soon as you post it there. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "@auyer @jvishnuvardhan I've just tried under Arch and observed the same issue."]}, {"number": 27351, "title": "Broken link https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: API r1.13\r\n- Doc Link: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing\r\n\r\n\r\n**Describe the documentation issue**\r\nA _404 - Page not found_ error is displayed when clicking on the link in the paragraph below:\r\n\r\n> Additional documentation can be found [on the keras site](https://keras.io/preprocessing/), \r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["@miguelangel Thanks for bringing this to our notice. We will take a look.", "This link has been removed."]}, {"number": 27350, "title": "Rephrase comments", "body": "", "comments": ["> LGTM! One small tweak: could you please change `those` on line 108 back to `that`?\r\n> \r\n> ```\r\n>   // Raw memory buffer that is allocated for persistent tensors that are\r\n> ```\r\n> \r\n> Thank you for these fixes!\r\n\r\n@dynamicwebpaige : Your comment is handled now, please check and approve, Thanks!"]}, {"number": 27349, "title": "Tensorflow 2.0 BatchNorm not working", "body": "\r\n\r\n**System information**\r\n- I written custom code\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from: pip\r\n- TensorFlow version: 2.0\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 10.0/ 7.5\r\n- GPU model and memory: GTX 1070/ 8 gig\r\n\r\n\r\n\r\n\r\n So I wrote a simple model using tf.keras and I was using batchnorm, It was working just fine then sudddenly it stopped working.\r\n\r\n\r\n**logs**\r\nTraceback (most recent call last):\r\n  File \"/home/kislay/Documents/DeepQ/DQmodel.py\", line 30, in <module>\r\n    print(dqn.callback(np.random.rand(4,210,160,1),random_action(4)))\r\n  File \"/home/kislay/Documents/DeepQ/DQmodel.py\", line 18, in callback\r\n    frame_stack = self.BatchNorm(frame_stack)\r\n  File \"/home/kislay/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 660, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/home/kislay/.local/lib/python3.6/site-packages/tensorflow/python/keras/layers/normalization.py\", line 589, in call\r\n    outputs = self._fused_batch_norm(inputs, training=training)\r\n  File \"/home/kislay/.local/lib/python3.6/site-packages/tensorflow/python/keras/layers/normalization.py\", line 465, in _fused_batch_norm\r\n    training, _fused_batch_norm_training, _fused_batch_norm_inference)\r\n  File \"/home/kislay/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/tf_utils.py\", line 56, in smart_cond\r\n    pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n  File \"/home/kislay/.local/lib/python3.6/site-packages/tensorflow/python/framework/smart_cond.py\", line 56, in smart_cond\r\n    return false_fn()\r\n  File \"/home/kislay/.local/lib/python3.6/site-packages/tensorflow/python/keras/layers/normalization.py\", line 462, in _fused_batch_norm_inference\r\n    data_format=self._data_format)\r\n  File \"/home/kislay/.local/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py\", line 1206, in fused_batch_norm\r\n    name=name)\r\n  File \"/home/kislay/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 3931, in _fused_batch_norm\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: Could not find valid device for node.\r\nNode: {{node FusedBatchNorm}}\r\nAll kernels registered for op FusedBatchNorm :\r\n  device='XLA_CPU'; T in [DT_FLOAT]\r\n  device='XLA_GPU'; T in [DT_FLOAT]\r\n  device='XLA_CPU_JIT'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='XLA_GPU_JIT\r\n", "comments": ["Also if I comment out the tf.keras.layers.BatchNormalization()(input) line it starts working.\r\n", "@SinghKislay In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27349\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27349\">No</a>\n"]}, {"number": 27348, "title": "Fix incompatibility in \u2019tensorflow/core/kernels:cwise_op_gpu\u2019 with Clang", "body": "It is reported that Clang cannot handle \"cwise_ops_gpu_common.cu.h.\" in #27253 and #26159.  This PR is a patch to avoid the issue.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27348) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27348) for more info**.\n\n<!-- ok -->", "The right fix is to add missing functions to Eigen `Eigen/src/Core/arch/GPU/PacketMath.h`. /cc @rmlarsen ", "For a reviewer @ezhulenev and @ymodak  : Clang has a bug that -O2 and -D_FORTIFY_SOURCE=1 cannot coexist when running \"clang -x cuda\" , so you might have to modify 'bazel-tensorflow/external/local_config_download_clang/extra_tools/lib/clang/8.0.0/include/__clang_cuda_runtime_wrapper.h' like below after downloading clang. This page(https://bugs.llvm.org/show_bug.cgi?id=41390) might help. Thank you.\r\n\r\n```diff\r\n-#include <string.h>\r\n+#if defined(__fortify_function)\r\n+#define __tmp_value __fortify_function\r\n+#undef __fortify_function\r\n+#include <string.h>\r\n+#define __fortify_function __tmp_value\r\n+#undef __tmp_value\r\n+#else\r\n+#include <string.h>\r\n+#endif\r\n```\r\n\r\n", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 27347, "title": "Contradicting comments removed", "body": "", "comments": ["@talumbau : Your comment is handled now, Thanks!", "Can one of the admins verify this patch?", "@talumbau Can you please take a look on this PR? Thanks!"]}, {"number": 27346, "title": "benchmark_model is much slower than expected", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: MI 8\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): \r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.22.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n**Describe the current behavior**\r\nI use the benchmark_model to run the inference of models with tflite, but the speed of Mobilenet is rather slow on Snapdragon 845, which is nearly **166 ms**. (According to the [performance site](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/benchmarks.md), I expect that it could run faster than 100 ms.) I wonder whether it is built with NEON or not, and is there anything else I should do to make this problem fixed.\r\n\r\n```\r\nadb shell /data/local/tmp/benchmark_model  --graph=/data/local/tmp/tflite_model/mobilenet_v1_1.0_224.tflite   --num_threads=4 --warmup_runs=3 --num_runs=50 --use_nnapi=true\r\nSTARTING!\r\nMin num runs: [50]\r\nMin runs duration (seconds): [1]\r\nInter-run delay (seconds): [-1]\r\nNum threads: [4]\r\nBenchmark name: []\r\nOutput prefix: []\r\nMin warmup runs: [3]\r\nMin warmup runs duration (seconds): [0.5]\r\nGraph: [/data/local/tmp/tflite_model/mobilenet_v1_1.0_224.tflite]\r\nInput layers: []\r\nInput shapes: []\r\nUse nnapi : [1]\r\nAllow fp16 : [0]\r\nnnapi error: unable to open function ANeuralNetworksModel_relaxComputationFloat32toFloat16\r\nLoaded model /data/local/tmp/tflite_model/mobilenet_v1_1.0_224.tflite\r\nresolved reporter\r\nINFO: Initialized TensorFlow Lite runtime.\r\nInitialized session in 175.762ms\r\nRunning benchmark for at least 3 iterations and at least 0.5 seconds\r\ncount=4 first=157773 curr=166340 min=155120 max=170646 avg=162470 std=6283\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds\r\ncount=50 first=173787 curr=167988 min=151156 max=179750 avg=166446 std=6661\r\n\r\nAverage inference timings in us: Warmup: 162470, Init: 175762, no stats: 166446\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\nThe speed of Mobilenet's inference on Snapdragon 845 is expected faster than 100 ms. And how could I know whether the benchmark is built with NEON or not?\r\n\r\n\r\n**Code to reproduce the issue**\r\n1. `git clone --recurse-submodules https://github.com/tensorflow/tensorflow.git`\r\n2. \r\n```\r\n./configure \r\nYou have bazel 0.22.0 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\n\r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: n\r\nClang will not be downloaded.\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: n\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: -march='armv8-a' -mfpu='neon'\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: y\r\nSearching for NDK and SDK installations.\r\n\r\nPlease specify the home path of the Android NDK to use. [Default is /home/tfl/Android/Sdk/ndk-bundle]: \r\n\r\nPlease specify the home path of the Android SDK to use. [Default is /home/tfl/Android/Sdk]: \r\n\r\nPlease specify the Android SDK API level to use. [Available levels: ['26', '27', '28']] [Default is 28]: 27\r\n\r\nPlease specify an Android build tools version to use. [Available versions: ['26.0.3', '27.0.3', '28.0.3']] [Default is 28.0.3]: 27.0.3\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=gdr         \t# Build with GDR support.\r\n\t--config=verbs       \t# Build with libverbs support.\r\n\t--config=ngraph      \t# Build with Intel nGraph support.\r\n\t--config=numa        \t# Build with NUMA support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nohdfs      \t# Disable HDFS support.\r\n\t--config=noignite    \t# Disable Apache Ignite support.\r\n\t--config=nokafka     \t# Disable Apache Kafka support.\r\n\t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n```\r\n3. \r\n```\r\nbazel build -c opt \\\r\n  --config=android_arm64 \\\r\n  --cxxopt='--std=c++11' \\\r\n  tensorflow/lite/tools/benchmark:benchmark_model\r\n```\r\n4. \r\n```\r\nadb push bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model /data/local/tmp\r\nadb shell chmod +x /data/local/tmp/benchmark_model\r\nadb shell /data/local/tmp/benchmark_model  --graph=/data/local/tmp/tflite_model/mobilenet_v1_1.0_224.tflite   --num_threads=4 --warmup_runs=3 --num_runs=50 --use_nnapi=true\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@georgeokelly what do you want to measure? TFLite interpreter CPU time, just like the [performance page](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/benchmarks.md) you referred to? If yes, \r\n\r\n1. don't use `--use_nnapi=true`\r\n2. you may want to use `taskset` as suggested in the performance page", "Hi @freedomtan I am measuring the inference time with CPU only. I also tried with `--use_nnapi=true` or `taskset` but it didn't help. At last I do what described on [benchmark/android](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/android/README.md) and it works (which means the run time is about 44 ms rather than 166 ms using mobilenet v1). Now I am little confused that when use \r\n```\r\nbazel build -c opt \\\r\n  --config=android_arm64 \\\r\n  --cxxopt='--std=c++11' \\\r\n  tensorflow/lite/tools/benchmark/android:benchmark_model\r\n```\r\ninstead of \r\n```\r\nbazel build -c opt \\\r\n  --config=android_arm64 \\\r\n  --cxxopt='--std=c++11' \\\r\n  tensorflow/lite/tools/benchmark:benchmark_model\r\n```\r\nthe inference time of the same model is quite different, like one is built with NEON while the other is not.", "@georgeokelly I mean don't use `--use_nnapi=true`. Anyway, you are comparing two difference models. What described in the  [benchmark/android](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/android/README.md) page is to benchmark a quantized model (`mobilenet_quant_v1_224.tflite`) and what you described above is to benchmark floating point model (`mobilenet_v1_1.0_224.tflite`). ", "@freedomtan  I mean I have tried many commands with `--use_nnapi=true` or without that, but it makes little difference for me because the time is always more than 100 ms. BTW, to make it clear, here is my testing result.\r\n\r\nmodel|time1|time2\r\n:--:|:--:|:--:\r\nmobilenet|~160 ms|~40 ms\r\nmobilenet_quant|~60 ms|~20 ms\r\n\r\ntime1 is tested by benchmark:benchmark_model\r\ntime2 is tested by android:benchmark_model\r\n\r\nMy device is MI 8 with snapdragon 845 and time2 is exactly what I expect, and now you could see the difference.", "FYR, on Pixel 2 (snapdragon 835),\r\n- `taskset f0 ./benchmark_model --graph=mobilenet_v1_1.0_224_quant.tflite --num_threads=4`: ~ 20 ms\r\n- `taskset f0 ./benchmark_model --graph=mobilenet_v1_1.0_224_quant.tflite --num_threads=1`: ~ 60 ms\r\n\r\nThe `benchmark_model` is built with\r\n```\r\nbazel build --config=android_arm64 tensorflow/lite/tools/benchmark:benchmark_model\r\n```\r\n\r\nIf your results are much worse than what I got, either your system or something in TFLite is wrong (it seems there are some ongoing TFLite kernel rewriting/optimization recently).", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 27345, "title": "[TF2.0] EstimatorV2 uses non existing export_savedmodel method", "body": "Dear tensorflowers,\r\n\r\nas per the title, EstimatorV2's exporter.py calls a method that has been removed. The fix is pretty easy, but I'm not sure about the implications of the change. See more below. \r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: custom code, but using estimators\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 4.19.20-1rodete1-amd64 #1 SMP Debian 4.19.20-1rodete1 \r\n- **TensorFlow installed from (source or binary)**: pip install --upgrade tensorflow==2.0.0alpha0\r\n- **TensorFlow version (use command below)**: 2.0.0-alpha0\r\n- **Python version**: 3.7.1\r\n\r\n### Describe the problem\r\nusing `tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)` results in `AttributeError: 'EstimatorV2' object has no attribute 'export_savedmodel'` being thrown.\r\n\r\n2 small modifications to my local `tensorflow_estimator/python/estimator/exporter.py` file fix the issue. However, before submitting a PR i wanted to clarify that I had to:\r\n\r\n1. Rename the method call to `estimator.export_saved_model(...)` and this poses no problem.\r\n2. Remove the `strip_default_attrs=self._strip_default_attrs` [keyword argument](https://github.com/tensorflow/estimator/blob/d14b0dce35baea00f27e17d8a44690080abd7bce/tensorflow_estimator/python/estimator/exporter.py#L120), but I have no idea what this entails. I assume that, since you can't specify this argument anymore, it defaults to the default policy in TF 1.X of stripping the GraphDef default attributes. If that's the case, is there any action required?\r\n\r\n#### Logs\r\n\r\n```\r\n  File \"/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 473, in train_and_evaluate\r\n    return executor.run()\r\n  File \"/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 613, in run\r\n    return self.run_local()\r\n  File \"/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 714, in run_local\r\n    saving_listeners=saving_listeners)\r\n  File \"/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 359, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1139, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1173, in _train_model_default\r\n    saving_listeners)\r\n  File \"/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1452, in _train_with_estimator_spec\r\n    any_step_done = True\r\n  File \"/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 856, in __exit__\r\n    self._close_internal(exception_type)\r\n  File \"/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 889, in _close_internal\r\n    h.end(self._coordinated_creator.tf_sess)\r\n  File \"/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 588, in end\r\n    self._save(session, last_step)\r\n  File \"/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 607, in _save\r\n    if l.after_save(session, step):\r\n  File \"/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 519, in after_save\r\n    self._evaluate(global_step_value)  # updates self.eval_result\r\n  File \"/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 539, in _evaluate\r\n    self._evaluator.evaluate_and_export())\r\n  File \"/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 927, in evaluate_and_export\r\n    is_the_final_export)\r\n  File \"/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 960, in _export_eval_result\r\n    is_the_final_export=is_the_final_export))\r\n  File \"/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/exporter.py\", line 420, in export\r\n    is_the_final_export)\r\n  File \"/usr/local/google/home/msteiner/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/exporter.py\", line 120, in export\r\n    export_result = estimator.export_savedmodel(\r\nAttributeError: 'EstimatorV2' object has no attribute 'export_savedmodel'\r\n```\r\n##### Code\r\n\r\nI'm trying to adapt the [CMLE template](https://github.com/GoogleCloudPlatform/cloudml-samples/tree/master/cloudml-template) to TF 2.0 . I avoided putting the input_fns and model_fns since the model trains and evals correctly. The only problem is during the export.\r\n\r\n```python\r\n train_input_fn = model_input.generate_input_fn(\r\n      file_names_pattern=FLAGS.train_files,\r\n      mode=tf.estimator.ModeKeys.TRAIN,\r\n      num_epochs=FLAGS.num_epochs,\r\n      batch_size=FLAGS.train_batch_size)\r\n\r\n  eval_input_fn = model_input.generate_input_fn(\r\n      file_names_pattern=FLAGS.eval_files,\r\n      mode=tf.estimator.ModeKeys.EVAL,\r\n      batch_size=FLAGS.eval_batch_size)\r\n\r\n\r\n\r\n  if metadata.TASK_TYPE == \"classification\":\r\n    estimator = model.create_classifier(config=run_config)\r\n  elif metadata.TASK_TYPE == \"regression\":\r\n    estimator = model.create_regressor(config=run_config)\r\n  else:\r\n    estimator = model.create_estimator(config=run_config)\r\n\r\n  train_spec = tf.estimator.TrainSpec(\r\n      train_input_fn,\r\n      max_steps=int(train_steps),\r\n  )\r\n\r\n  # Export for the final prediction graph.\r\n  exporter_default = tf.estimator.FinalExporter(\r\n      'estimator',\r\n      model_input.SERVING_FUNCTIONS[FLAGS.export_format](),\r\n      as_text=False )\r\n\r\n  eval_spec = tf.estimator.EvalSpec(\r\n      eval_input_fn,\r\n      steps=FLAGS.eval_steps,\r\n      exporters=[exporter_default],  # This tells\r\n      throttle_secs=FLAGS.eval_every_secs,\r\n      start_delay_secs=0)\r\n\r\n  # Main train and evaluate loop.\r\n  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n\r\n```\r\n\r\n\r\nHappy to provide any additional information!", "comments": ["@toby this is the PR for estimator exporter.  He need 2 clarification before.", "We already have export_saved_model, is there anything else needed here?", "Are you saving it was already fixed in a recent nightly ? If yes, I will give a try. Thanks", "Yes, please let me know once you try it using master version.", "@tanzhenyu , I tried with the latest nightly tf-nightly-2.0-preview==2.0.0.dev20190426\r\nand I get another crash but still related to exporter:\r\n\r\n```\r\n~/anaconda3/envs/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/exporter.py in export(***failed resolving arguments***)\r\n    124         as_text=self._as_text,\r\n    125         checkpoint_path=checkpoint_path,\r\n--> 126         strip_default_attrs=self._strip_default_attrs)\r\n    127 \r\n    128     return export_result\r\n\r\nTypeError: '_HiddenTfApiAttribute' object is not callable\r\n```\r\n\r\nthe crash seems related to the 2) points mention above by @MMMarcy :\r\n\r\n\"Remove the strip_default_attrs=self._strip_default_attrs keyword argument, but I have no idea what this entails. I assume that, since you can't specify this argument anymore, it defaults to the default policy in TF 1.X of stripping the GraphDef default attributes. If that's the case, is there any action required?\"\r\n\r\nso 1) seems to be fixed but not 2). Thanks", "@tanzhenyu did you managed to run tf.estimator.LatestExporter with Estimator ? with nightly tf-nightly-2.0-preview==2.0.0.dev20190614 I have new errors:\r\n\r\n```\r\n~/anaconda3/envs/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)\r\n    567       raise TypeError(\"Failed to convert object of type %s to Tensor. \"\r\n    568                       \"Contents: %s. Consider casting elements to a \"\r\n--> 569                       \"supported type.\" % (type(values), values))\r\n    570     tensor_proto.string_val.extend(str_values)\r\n    571     return tensor_proto\r\n\r\nTypeError: Failed to convert object of type <class 'tensorflow.python.framework.dtypes.DType'> to Tensor. Contents: <dtype: 'float32'>. Consider casting elements to a supported type.\r\n```\r\nIs there something to change with respect to TF 1.1.2 ?\r\n\r\n```\r\n# Create serving input function\r\ndef serving_input_receiver_fn():\r\n    \"\"\"Serving input_fn that builds features from placeholders#\r\n\r\n    Returns\r\n    -------\r\n    tf.estimator.export.ServingInputReceiver\r\n    \"\"\"\r\n\r\n    input_images = tf.Variable(tf.float32, [None, 784])\r\n    features = {\r\n        'dense_input': input_images}  # this is the dict that is then passed as \"features\" parameter to your model_fn\r\n    receiver_tensors = {\r\n        'dense_input': input_images}  # As far as I understand this is needed to map the input to a name you can retrieve later\r\n\r\n    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\r\n```\r\n\r\nI am not sure now if the issue is with my code or if the issue is still on the TF side. I look at the official doc and I couldn't an a example with TF 2.0 beta", "@tarrade, I am assigning this to someone who's more familiar with this topic.", "Any status on this issue ? @tanzhenyu @MMMarcy @ymodak \r\nI am wondering what is the status of the migration of tf.estimator to Tensorfloe 2.0. It seems few things are not working yet. Probably tf.estimator is/will be the last part migrationg to Tensorflow 2.0. For example in the official documentation, the example on tf.estimator is very very limited, still use compat.v1 module for some ops and don't save the model:\r\nhttps://www.tensorflow.org/beta/tutorials/distribute/multi_worker_with_estimator\r\n\r\nI did a new test with the latest nightly tf-nightly-2.0-preview==2.0.0.dev20190725 and I still have issue:\r\n\r\n> I0726 12:01:23.221257 4583781824 estimator.py:2100] Saving 'checkpoint_path' summary for global step 10: results/Models/Mnist/tf_1_12/estimator/v3/ckpt/model.ckpt-10\r\n\r\n> ---------------------------------------------------------------------------\r\n> TypeError                                 Traceback (most recent call last)\r\n> ~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)\r\n>     534     try:\r\n> --> 535       str_values = [compat.as_bytes(x) for x in proto_values]\r\n>     536     except TypeError:\r\n> \r\n> ~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_util.py in <listcomp>(.0)\r\n>     534     try:\r\n> --> 535       str_values = [compat.as_bytes(x) for x in proto_values]\r\n>     536     except TypeError:\r\n> \r\n> ~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/util/compat.py in as_bytes(bytes_or_text, encoding)\r\n>      64     raise TypeError('Expected binary or unicode string, got %r' %\r\n> ---> 65                     (bytes_or_text,))\r\n>      66 \r\n> \r\n> TypeError: Expected binary or unicode string, got tf.float32\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n\r\nI managed to find where is the issue:\r\n\r\n```\r\n# Create serving input function\r\ndef serving_input_receiver_fn():\r\n    \"\"\"Serving input_fn that builds features from placeholders#\r\n\r\n    Returns\r\n    -------\r\n    tf.estimator.export.ServingInputReceiver\r\n    \"\"\"\r\n    input_images = tf.Variable(tf.float32, [None, 784]) #<- it is crashing here\r\n```\r\n\r\nnow I didn't find how this should be in TF 2.0. Normally we can still use gragh and tf.Variable. I couldn't find up to know anything in the doc of  tf.estimator.export.ServingInputReceiver or some official example.\r\n\r\nAny idea or expert that could know that. I have no idea if this is a bug or if I should update the problematic line. Thanks\r\n\r\n\r\n\r\n", "The issue in the OP with the wrong use of export_savedmodel should be fixed in 1.14.\r\n\r\n@tarrade That looks like a different bug, can you create a new issue with some code to reproduce the errors? Although from the code snippet, you should be creating a placeholder and not a variable. If you're using a variable, the initialization arguments are in a different order.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27345\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27345\">No</a>\n", "Excuse me @k-w-w but how do I create a placeholder in tensorflow 2.0 (module 'tensorflow' has no attribute 'placeholder') ?\r\nI am trying to update my serving_input_receiver_fn code where I need to create a tf.estimator.export.TensorServingInputReceiver. I don't find any example to update anywhere :(\r\n", "@laetitiaoist Estimator still relies on graph-based code, so you would have to use `tf.compat.v1.placeholder`.\r\n\r\n We're recommending that users switch to tf.Keras in the future, which has built-in support for eager mode.", "It would be great to have instructions on how to update from Estimator to Keras. I thought Estimator was supported in TF2...\r\nFor example how to save the top N models when evaluating? I only see \"save_best_only\" option . ", "Estimator is supported in TF2, but the APIs for building the graph have been removed from the V2 API to keep the new API compatible with eager. You can continue to use Estimators but you'll have to use the compat.v1 for certain APIs like placeholder.\r\n\r\nThere are instructions here for updating your model to use the Keras API: https://www.tensorflow.org/guide/migrate#converting_models\r\n\r\nDo you mean in Keras' `ModelCheckpoint` class? Yes, there're currently isn't a way to specify the keep the top N checkpoints. Feel free to create a new issue asking for a feature request."]}, {"number": 27344, "title": "q", "body": "", "comments": []}, {"number": 27343, "title": "j", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 27342, "title": "Building tensorflow wheel met error `clang: error: no input files`", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n Darwin Kernel Version 18.0.0, Mac OS 10.14\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from:\r\nbuild from source\r\n- TensorFlow version:\r\nMaster branch\r\n- Python version:\r\n2.7\r\n- Installed using virtualenv? pip? conda?:\r\nNo\r\n- Bazel version (if compiling from source):\r\n0.23.2\r\n- GCC/Compiler version (if compiling from source):\r\nConfigured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/c++/4.2.1\r\n- CUDA/cuDNN version:\r\nwithout it\r\n- GPU model and memory:\r\nwithout GPU.\r\n\r\n\r\n**Describe the problem**\r\nI want to build tensorflow from source, use command:\r\n```\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nAnd here is result:\r\n```\r\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\nINFO: Elapsed time: 5926.109s, Critical Path: 623.43s\r\nINFO: 10670 processes: 10670 local.\r\nINFO: Build completed successfully, 11295 total actions\r\n```\r\nThen I try to build the `wheel`, use command:\r\n```\r\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n```\r\nBut got error:\r\n```\r\nwarning: no files found matching '*.pyd' under directory '*'\r\nwarning: no files found matching '*.pd' under directory '*'\r\nwarning: no files found matching '*.dll' under directory '*'\r\nwarning: no files found matching '*.lib' under directory '*'\r\nwarning: no files found matching '*.h' under directory 'tensorflow/include/tensorflow'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/Eigen'\r\nwarning: no files found matching '*.h' under directory 'tensorflow/include/google'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/third_party'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/unsupported'\r\nclang: error: no input files\r\nerror: command 'clang' failed with exit status 1\r\n```\r\n", "comments": ["@a6802739 Apologies for the delay in response. Is this still an issue? Thanks!", "@ymodak, it's fixed now.", "That's great to hear. Can you please share your solution? It may help other users coming across this issue in the future. Thanks!"]}, {"number": 27341, "title": "evaluate", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 27340, "title": "Add a \"tf.contrib.*\" warning to tf_upgrade_v2 upgrade script", "body": "Tensorflow 1.13 and 2.0 come with the support of tf_upgrage_v2 script.\r\nThe script ignores imports and usage of parts of tf.contrib.* package that is no longer available in TF2.0. Due to that converted script fails with \"_ModuleNotFoundError: No module named 'tensorflow.contrib'_\". I suggest adding a user warning/error to the script to notify the user about incompatible library being used.", "comments": ["i opened tf_upgrade_v2.py and found that there is a warning already:\r\n contrib_warning = (\r\n        ast_edits.ERROR,\r\n        \"<function name> cannot be converted automatically. tf.contrib will not\"\r\n        \" be distributed with TensorFlow 2.0, please consider an alternative in\"\r\n        \" non-contrib TensorFlow, a community-maintained repository, or fork \"\r\n        \"the required code.\"\r\n)", "After reinstalling TF `pip install --upgrade tf-nightly-2.0-preview` just now and running` tf_upgrade_v2 --infile WhatWhereAutoencoder.py --outfile test.py` on a script containing lines like `import tensorflow.contrib.slim as slim` or  `slim.conv2d(input, 16, [5, 5])` I got no warning.\r\n\r\n@shashvatshahi1998, After examining the `tf_upgrade_v2.py` I could not find any actual usage of the warning you mention.", "Then we have to add that warning at its correct location from where that warning become effective.", "> i opened tf_upgrade_v2.py and found that there is a warning already:\r\n> contrib_warning = (\r\n> ast_edits.ERROR,\r\n> \" cannot be converted automatically. tf.contrib will not\"\r\n> \" be distributed with TensorFlow 2.0, please consider an alternative in\"\r\n> \" non-contrib TensorFlow, a community-maintained repository, or fork \"\r\n> \"the required code.\"\r\n> )\r\n\r\nFor the features moved to other repos such as `probability` or `addons`, the upgrade script can probably give more information regarding where the feature has been moved to.", "@yselivonchyk Apologies for the delay in response.  If you change ```slim.conv2d(input, 16, [5, 5])``` to ```tensorflow.contrib.slim.conv2d(input, 16, [5, 5])``` the required warning message will be triggered. \r\n"]}]