[{"number": 30305, "title": "load model.meta and checkpoint and model.data-00000-of-00001 and model.index", "body": "**Tensorflow version:1.12\r\nI have built libtensorflow_cc.so and test it with few examples successfully but when i wanted to transplate my python code below to c++ verison , i failed.**\r\n`import tensorflow as tf\r\nimport  numpy as np\r\nfrom skimage import io, transform\r\nimport cv2\r\n#import infer\r\nsess = tf.Session()\r\nnew_saver = tf.train.import_meta_graph(r'D:\\CNN_test_result_sizhuangzao_model\\v2_101_ziji_9\\model\\model.meta')\r\nnew_saver.restore(sess,tf.train.latest_checkpoint(r'D:\\CNN_test_result_sizhuangzao_model\\v2_101_ziji_9\\model'))\r\n\r\ngraph = tf.get_default_graph()\r\ninput_x = graph.get_tensor_by_name(\"input:0\")\r\nis_training_x=graph.get_tensor_by_name(\"is_training:0\")\r\n#print (sess.run(\"input:0\"))\r\n\r\njpg_path=r\"D:\\CNN_test_result_sizhuangzao\\chaodacesiji1\\pos.txt\"\r\nfile = open(jpg_path)\r\nlines = file.readlines()\r\nnum=0\r\nfor line in lines: \r\n    img0 = io.imread(line.strip())\r\n    l=img0.shape\r\n    k=l[0]\r\n    c=l[1]\r\n    num+=1\r\n    print(num)\r\n    if  c/k<5:\r\n        continue\r\n    img = transform.resize(img0, (48,160, 1))\r\n    feed_dict={input_x:np.reshape(img, [-1, 48, 160, 1]),is_training_x:False}\r\n\r\n    prob_op = graph.get_operation_by_name('output')\r\n    out_softmax = graph.get_tensor_by_name(\"output:0\")\r\n    img_out_softmax = sess.run(out_softmax,feed_dict)\r\n    prediction_labels = np.argmax(img_out_softmax,1)`\r\n\r\n**Here is my c++ version code below and the error information is 'ERROR: RUN failed...Invalid argument: Expects arg[1] to be bool but float is provided ' when i try to run.** \r\n`#include \"tensorflow/core/framework/graph.pb.h\"\r\n#include <tensorflow/core/public/session_options.h>\r\n#include <tensorflow/core/protobuf/meta_graph.pb.h>\r\n#include <fstream>\r\n#include <utility>\r\n#include <vector>\r\n#include <Eigen/Core>\r\n#include <Eigen/Dense>\r\n\r\n#include \"tensorflow/cc/ops/const_op.h\"\r\n#include \"tensorflow/cc/ops/image_ops.h\"\r\n#include \"tensorflow/cc/ops/standard_ops.h\"\r\n#include \"tensorflow/core/framework/graph.pb.h\"\r\n#include \"tensorflow/core/framework/tensor.h\"\r\n#include \"tensorflow/core/graph/default_device.h\"\r\n#include \"tensorflow/core/graph/graph_def_builder.h\"\r\n#include \"tensorflow/core/lib/core/errors.h\"\r\n#include \"tensorflow/core/lib/core/stringpiece.h\"\r\n#include \"tensorflow/core/lib/core/threadpool.h\"\r\n#include \"tensorflow/core/lib/io/path.h\"\r\n#include \"tensorflow/core/lib/strings/stringprintf.h\"\r\n#include \"tensorflow/core/platform/env.h\"\r\n#include \"tensorflow/core/platform/init_main.h\"\r\n#include \"tensorflow/core/platform/logging.h\"\r\n#include \"tensorflow/core/platform/types.h\"\r\n#include \"tensorflow/core/public/session.h\"\r\n#include \"tensorflow/core/util/command_line_flags.h\"\r\n\r\nusing namespace std;\r\nusing namespace tensorflow;\r\nusing namespace tensorflow::ops;\r\nusing tensorflow::Flag;\r\nusing tensorflow::Tensor;\r\nusing tensorflow::Status;\r\nusing tensorflow::string;\r\nusing tensorflow::int32;\r\n\r\ntypedef std::vector<std::pair<std::string, tensorflow::Tensor>> tensor_dict;\r\nusing tensorflow::Status;\r\n\r\nstatic Status ReadtheFile(tensorflow::Env* env, const string& filename,Tensor* output) {\r\n  tensorflow::uint64 file_size = 0;\r\n  TF_RETURN_IF_ERROR(env->GetFileSize(filename, &file_size));\r\n\r\n  string contents;\r\n  contents.resize(file_size);\r\n\r\n  std::unique_ptr<tensorflow::RandomAccessFile> file;\r\n  TF_RETURN_IF_ERROR(env->NewRandomAccessFile(filename, &file));\r\n\r\n  tensorflow::StringPiece data;\r\n  TF_RETURN_IF_ERROR(file->Read(0, file_size, &data, &(contents)[0]));\r\n  if (data.size() != file_size) {\r\n    return tensorflow::errors::DataLoss(\"Truncated read of '\", filename,\r\n                                        \"' expected \", file_size, \" got \",\r\n                                        data.size());\r\n  }\r\n//  output->scalar<string>()() = data.ToString();\r\n  output->scalar<string>()() = string(data);\r\n  return Status::OK();\r\n}\r\n\r\nStatus ReadImageFile(const string& file_name, const int input_height,\r\n                               const int input_width, const float input_mean,\r\n                               const float input_std,\r\n                               std::vector<Tensor>* out_tensors) {\r\n  auto root = tensorflow::Scope::NewRootScope();\r\n  using namespace ::tensorflow::ops;\r\n\r\n  string input_name = \"file_reader\";\r\n  string output_name = \"normalized\";\r\n\r\n  // read file_name into a tensor named input\r\n  Tensor input(tensorflow::DT_STRING, tensorflow::TensorShape());\r\n  TF_RETURN_IF_ERROR(ReadtheFile(tensorflow::Env::Default(), file_name, &input));\r\n\r\n  // use a placeholder to read input data\r\n  auto file_reader =Placeholder(root.WithOpName(\"input\"), tensorflow::DataType::DT_STRING);\r\n\r\n  std::vector<std::pair<string, tensorflow::Tensor>> inputs = {{\"input\", input},};\r\n\r\n  // Now try to figure out what kind of file it is and decode it.\r\n    const int wanted_channels = 1;\r\n    tensorflow::Output image_reader;\r\n\tif (tensorflow::str_util::EndsWith(file_name, \".png\"))\r\n\t{\r\n\t  image_reader = DecodePng(root.WithOpName(\"png_reader\"), file_reader,\r\n\t\t\t\t\t\t\t   DecodePng::Channels(wanted_channels));\r\n\t}\r\n\telse if (tensorflow::str_util::EndsWith(file_name, \".gif\"))\r\n\t{\r\n\t  // gif decoder returns 4-D tensor, remove the first dim\r\n\t  image_reader =\r\n\t\t  Squeeze(root.WithOpName(\"squeeze_first_dim\"),\r\n\t\t\t\t  DecodeGif(root.WithOpName(\"gif_reader\"), file_reader));\r\n\t}\r\n\telse if (tensorflow::str_util::EndsWith(file_name, \".bmp\"))\r\n\t{\r\n\t  image_reader = DecodeBmp(root.WithOpName(\"bmp_reader\"), file_reader);\r\n\t}\r\n\telse\r\n\t{\r\n\t  // Assume if it's neither a PNG nor a GIF then it must be a JPEG.\r\n\t  image_reader = DecodeJpeg(root.WithOpName(\"jpeg_reader\"), file_reader,\r\n\t\t\t\t\t\t\t\tDecodeJpeg::Channels(wanted_channels));\r\n\t}\r\n  // Now cast the image data to float so we can do normal math on it.\r\n  auto float_caster =Cast(root.WithOpName(\"float_caster\"), image_reader, tensorflow::DT_FLOAT);\r\n\r\n  auto dims_expander = ExpandDims(root.WithOpName(\"expand\"), float_caster, 0);\r\n\r\n  float input_max = 255;\r\n  Div(root.WithOpName(\"div\"),dims_expander,input_max);\r\n\r\n  tensorflow::GraphDef graph;\r\n  TF_RETURN_IF_ERROR(root.ToGraphDef(&graph));\r\n\r\n  std::unique_ptr<tensorflow::Session> session(\r\n      tensorflow::NewSession(tensorflow::SessionOptions()));\r\n  TF_RETURN_IF_ERROR(session->Create(graph));\r\n//  std::vector<Tensor> out_tensors;\r\n//  TF_RETURN_IF_ERROR(session->Run({}, {output_name + \":0\", output_name + \":1\"},\r\n//                                    {}, &out_tensors));\r\n  TF_RETURN_IF_ERROR(session->Run({inputs}, {\"div\"}, {}, out_tensors));\r\n  return Status::OK();\r\n}\r\n\r\nint main()\r\n{\r\n  Session* session;\r\n  Status status = NewSession(SessionOptions(), &session);\r\n\r\n  const std::string graph_fn = \"/media/root/Ubuntu311/projects/Ecology_projects/tensorflowtest/model-0617/model.meta\";\r\n  MetaGraphDef graphdef;\r\n  Status status_load = ReadBinaryProto(Env::Default(), graph_fn, &graphdef); //\u4ecemeta\u6587\u4ef6\u4e2d\u8bfb\u53d6\u56fe\u6a21\u578b;\r\n  if (!status_load.ok()) {\r\n        std::cout << \"ERROR: Loading model failed...\" << graph_fn << std::endl;\r\n        std::cout << status_load.ToString() << \"\\n\";\r\n        return -1;\r\n  }\r\n\r\n  Status status_create = session->Create(graphdef.graph_def()); //\u5c06\u6a21\u578b\u5bfc\u5165\u4f1a\u8bddSession\u4e2d;\r\n  if (!status_create.ok()) {\r\n        std::cout << \"ERROR: Creating graph in session failed...\" << status_create.ToString() << std::endl;\r\n        return -1;\r\n  }\r\n  cout << \"Session successfully created.Load model successfully!\"<< endl;\r\n\r\n  // \u8bfb\u5165\u9884\u5148\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u7684\u6743\u91cd\r\n  const std::string checkpointPath = \"/media/root/Ubuntu311/projects/Ecology_projects/tensorflowtest/model-0617/model\";\r\n  Tensor checkpointPathTensor(DT_STRING, TensorShape());\r\n  checkpointPathTensor.scalar<std::string>()() = checkpointPath;\r\n  status = session->Run(\r\n\t\t  {{ graphdef.saver_def().filename_tensor_name(), checkpointPathTensor },},\r\n\t\t  {},{graphdef.saver_def().restore_op_name()},nullptr);\r\n  if (!status.ok())\r\n  {\r\n\t  throw runtime_error(\"Error loading checkpoint from \" + checkpointPath + \": \" + status.ToString());\r\n  }\r\n  cout << \"Load weights successfully!\"<< endl;\r\n  //read image for prediction...\r\n  string image_path= \"/media/root/Ubuntu311/projects/Ecology_projects/copy/cnn-imgs/AABW.jpg\";\r\n   int input_height =48;\r\n   int input_width=160;\r\n   int input_mean=0;\r\n   int input_std=1;\r\n   std::vector<Tensor> resized_tensors;\r\n   Status read_tensor_status =\r\n       ReadImageFile(image_path, input_height, input_width, input_mean,\r\n                               input_std, &resized_tensors);\r\n   if (!read_tensor_status.ok()) {\r\n     LOG(ERROR) << read_tensor_status;\r\n     cout<<\"resing error\"<<endl;\r\n     return -1;\r\n   }\r\n\r\n   const Tensor& resized_tensor = resized_tensors[0];\r\n   std::cout <<\"Read image successfully: \"<< resized_tensor.DebugString()<<endl;\r\n\r\n   std::string Input1Name = \"input\";\r\n   std::string Input2Name = \"is_training\";\r\n   vector<std::pair<string, Tensor> > inputs;\r\n   inputs.push_back(std::make_pair(Input1Name, resized_tensor));\r\n   inputs.push_back(std::make_pair(Input2Name, resized_tensor));\r\n\r\n   vector<tensorflow::Tensor> outputs;\r\n   string output2=\"out_softmax\";\r\n   string output_ = \"output\";\r\n   Status status_run = session->Run(inputs, {output2}, {}, &outputs);\r\n   if (!status_run.ok()) {\r\n       std::cout << \"ERROR: RUN failed...\"  << std::endl;\r\n       std::cout << status_run.ToString() << \"\\n\";\r\n       return -1;\r\n   }\r\n   //Fetch output value\r\n   std::cout << \"Output tensor size:\" << outputs.size() << std::endl;\r\n   for (std::size_t i = 0; i < outputs.size(); i++) {\r\n       std::cout <<\"result: \"<<i<<\" :\"<< outputs[i].DebugString()<<endl;\r\n   }\r\n  cout << \"Prediction successfully!\"<< endl;`\r\n\r\n**Any help will be appreciated.** \r\n", "comments": ["This question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!", "Thanks,I have put the question here [https://stackoverflow.com/questions/56880358/model-pruner-failed-invalid-argument-invalid-input-graph-when-run-tensorflow-c](url)"]}, {"number": 30304, "title": "[BUG, iOS] Symbol not found: _clock_gettime ", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **iOS 9.3**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **TESTED on iPhone 6s, iPad Air 2, iPad Mini 4**\r\n- TensorFlow installed from (source or binary): **compiled for iOS using the provided Makefile**\r\n- TensorFlow version (use command below): **1.13**\r\n\r\n**Describe the current behavior**\r\n\r\nLoading a network using TensorFlow 1.13 compiled for iOS (using the provided Makefile, altered to use `ANDROID_TYPES_FULL` [variable is used in iOS as well, not only on Android!]) in an app running on iOS < 10.0 crashes the application with the following error:\r\n\r\n```\r\n\r\ndyld: lazy symbol binding failed: Symbol not found: _clock_gettime\r\n  Referenced from: /Users/rhcpfan/Library/Developer/CoreSimulator/Devices/4B733BDB-F1AB-49A4-87D5-3F23C507B599/data/Containers/Bundle/Application/58FB49D0-1330-442A-A46A-8ADADD34EAA2/MyApp.app/MyApp\r\n  Expected in: /Library/Developer/CoreSimulator/Profiles/Runtimes/iOS 9.3.simruntime/Contents/Resources/RuntimeRoot/usr/lib/libSystem.B.dylib\r\n\r\ndyld: Symbol not found: _clock_gettime\r\n  Referenced from: /Users/slowhand/Library/Developer/CoreSimulator/Devices/4B733BDB-F1AB-49A4-87D5-3F23C507B599/data/Containers/Bundle/Application/58FB49D0-1330-442A-A46A-8ADADD34EAA2/MyApp.app/MyApp\r\n  Expected in: /Library/Developer/CoreSimulator/Profiles/Runtimes/iOS 9.3.simruntime/Contents/Resources/RuntimeRoot/usr/lib/libSystem.B.dylib\r\n\r\n```\r\n", "comments": ["After more digging, every call to `tensorflow::EnvTime::NowMicros()` causes the app to crash on iOS 9.\r\n\r\nThe first call that produced a crash was in the logger, so I removed the time-related information from `LogMessage::GenerateLogMessage()` in `tensorflow/core/platform/default/logging.cc` and re-compiled. Now it crashes in `tensorflow::DirectSession::RunInterval(...)`.\r\n\r\nAny idea on how to provide a dummy implementation for `tensorflow::EnvTime::NowMicros()` until this issue is solved? If I'll always return 0, will this have any side effects?", "Is this still an issue for you in more recent versions, such as `1.15.0`, `2.0.0`?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 30303, "title": "INTERNAL ERROR: Failed to apply GPUdelegate to custom tflite model", "body": "**System information**\r\n- Have I written custom code: Yes, using a custom tflite model and Android app to run on my devices.\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- Mobile device if the issue happens on mobile device: Asus zenphone M2, Snapdragon 820, 605 dev-kits.\r\n- TensorFlow installed from: nightly AAR (org.tensorflow:tensorflow-lite:0.0.0-nightly)\r\n- TensorFlow-gpu installed from:  nightly AAR (org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly)\r\n- Python version: 3.6.7\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: GTX 1060 6GB\r\n\r\n**Describe the current behavior**\r\nI've written a simple CNN to count number of fingers, works perfectly on my laptop (both the .h5 and tflite version). When I load the .tflite model on Android it fails saying;\r\n\r\n` Caused by: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: TfLiteGpuDelegate Prepare: Shader compilation failed: ERROR: 0:6: 'unknown' : not a legal layout qualifier id`\r\n\r\nWhat exactly does this error mean? \r\n\r\n**Describe the expected behavior**\r\nGPUdelegate should load correctly and run inference on my Android devices.\r\n\r\n**Code to reproduce the issue**\r\nMy custom app is based on the TensorFlow Lite Android image classification example. The model loads and runs fine on CPU. The only difference in the code is the following lines:\r\n\r\n```\r\ntfliteOptions = new Interpreter.Options();\r\ngpuDelegate = new GpuDelegate();\r\ntfliteOptions.addDelegate(gpuDelegate);\r\n```\r\n\r\n**Other info / logs**\r\nGPUdelegate works fine if I try to run models like MobilenetV2, without any change in code. \r\nAfter much googling, [found this on StackOverflow](https://stackoverflow.com/questions/55794791/tensorflow-lite-gpu-delegate-failure)\r\n\r\nSo, is it an issue with the output layers in my model? I'll include the model graph. \r\nPlease help!\r\n\r\n\r\n**Traceback:**\r\n```\r\n2019-07-02 10:19:12.153 15591-15613/com.prat96.tflite I/Adreno: ERROR: 0:6: 'unknown' : not a legal layout qualifier id \r\n    ERROR: 0:6: 'unknown' : Syntax error:  syntax error\r\n    INTERNAL ERROR: no main() function!\r\n    ERROR: 2 compilation errors.  No code generated.\r\n2019-07-02 10:19:12.156 15591-15613/com.prat96.tflite E/AndroidRuntime: FATAL EXCEPTION: pool-1-thread-1\r\n    Process: com.prat96.tflite, PID: 15591\r\n    java.lang.RuntimeException: Error initializing TensorFlow!\r\n        at com.prat96.tflite.MainActivity$3.run(MainActivity.java:133)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)\r\n        at java.lang.Thread.run(Thread.java:764)\r\n     Caused by: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: TfLiteGpuDelegate Prepare: Shader compilation failed: ERROR: 0:6: 'unknown' : not a legal layout qualifier id \r\n    ERROR: 0:6: 'unknown' : Syntax error:  syntax error\r\n    INTERNAL ERROR: no main() function!\r\n    ERROR: 2 compilation errors.  No code generated.\r\n    \r\n    Node number 11 (TfLiteGpuDelegate) failed to prepare.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:83)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:60)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:224)\r\n        at com.prat96.tflite.TensorFlowImageClassifier.create(TensorFlowImageClassifier.java:57)\r\n        at com.prat96.tflite.MainActivity$3.run(MainActivity.java:126)\r\n        \t... 3 more\r\n2019-07-02 10:19:12.168 15591-15613/? I/Process: Sending signal. PID: 15591 SIG: 9\r\n```", "comments": ["![tf2_fingers](https://user-images.githubusercontent.com/19527450/60494011-aa7fe480-9cb6-11e9-885e-e8dad29c2daf.png)\r\n\r\nRequest help @impjdi ", "@prat96 \r\n\r\nIs it possible to share the network?  You don't need to share the weights/bias.  I only need a TFLite file with the correct network architecture.", "@impjdi Absolutely, I apologize for not putting that up in the first place!\r\n\r\n[Link to .h5 file, .tflite file and some test images](https://drive.google.com/open?id=1eb5LMDsZW_sgUSjljpCLiX1z_bGYfBEW)\r\n\r\nFYI the .h5 file was converted to .tflite using TF2-beta1", "@prat96 \r\n\r\nThanks.  Can you try to insert a reshape op right before the first FC, reshaping the tensor from [1, 6, 6, 32] to [1, 1152]?", "ok, I have done this by adding;\r\n``` \r\ntf.keras.layers.Reshape((1, 1152)),\r\ntf.keras.layers.Flatten(),\r\ntf.keras.layers.Dense(128, activation='relu'),\r\n....\r\n```\r\nAaaanndd now it works perfectly! \r\n@impjdi Could you please tell me in detail why this step is nescessary? I want to understand so that I can develop other models, keeping this in mind. \r\nThanks so much for your help!", "@prat96 \r\n\r\nAh, so the suspicion came from this:\r\n\r\n[ B H W I ]  -> FC with weights [ O H W I ] and bias [ O ]  -> [ B H W O ]\r\n\r\nI saw that your input and weights don't match, so I asked you to fix that.  On the CPU, [ 1 6 6 32 ] and [ 1 1 1 1152 ] have the same memory layout, so it may work just out of the box without a seg fault, but I think you got lucky that TFLite CPU op impl is not being strict.\r\n\r\nNow for the GPU, it is also true that the memory layout for [ 1 6 6 32 ] and [ 1 1 1 1152 ] is also the same, but it looked like the shader ops were expecting something that matches.  And then it was not able to find that, and that's why they got \"unknown\" data type instead of, e.g. \"rgba16f\" in `std::string ToImageLayoutQualifier(DataType type)` in `object_accessor.cc`.\r\n\r\nIn the future, I would try to stick to canonical input/output dimensions.  Ideally, the error log should be more descriptive, so that you can take action, but it's a problem we have.  Philosophically, we designed the shader parts to be agnostic of neural net: they are just a pipeline of executing GPU shader programs.  At that point, all the information of tensor names etc. got lost, and we were not able to pinpoint (in the error log), which tensor was bad =/  Maybe we have to revisit that decision, or find other ways to embed extra information for the error log...", "@rmothukuru \r\n\r\nI was thinking of closing this.  Do you have anything in mind as you have re-assigned this to yourself?", "@impjdi ,\r\nThank you for confirming it with me. I have assigned it on my name to label the issue. \r\n@prat96 ,\r\nCan you please confirm if your issue is resolved and if we can close this issue.", "@impjdi Aah, thats really interesting and makes a lot of sense...\r\nDo you think it would help if I had enabled some padding options in the first input layers? \r\n\r\nAgain, thanks a lot! :)\r\n\r\n@rmothukuru Yes my issue is resolved for now. ", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30303)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30303)\r\n"]}, {"number": 30302, "title": "installation isuuse ", "body": "Traceback (most recent call last):\r\n  File \"eg1.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 52, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\core\\framework\\graph_pb2.py\", line 7, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 47, in <module>\r\n    from google.protobuf.pyext import _message\r\nImportError: DLL load failed: The specified procedure could not be found.\r\n![error](https://user-images.githubusercontent.com/52444630/60493670-ebd8ba00-9cdf-11e9-95e5-c03f9210a8dd.png)\r\n\r\n", "comments": ["Raghvendra Dubey\n        \u60a8\u597d\uff01\u6211\u5df2\u6536\u5230\u60a8\u7684\u90ae\u4ef6\u3002\n        \n        \u6709\u4ec0\u4e48\u95ee\u9898\u968f\u65f6\u4e0e\u6211\u8054\u7cfb\u548c\u6c9f\u901a\uff0c\u795d\u597d\uff01\n\n\n2019-7-2\n| |\nxxx\n|\n|\nyonglonggeng@163.com\n|\n\u7b7e\u540d\u7531\u7f51\u6613\u90ae\u7bb1\u5927\u5e08\u5b9a\u5236\nOn 7/2/2019 15:50\uff0cRaghvendra Dubey<notifications@github.com> wrote\uff1a\n\nTraceback (most recent call last):\nFile \"eg1.py\", line 1, in\nimport tensorflow as tf\nFile \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow_init_.py\", line 28, in\nfrom tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import\nFile \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python_init_.py\", line 52, in\nfrom tensorflow.core.framework.graph_pb2 import *\nFile \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\core\\framework\\graph_pb2.py\", line 7, in\nfrom google.protobuf import descriptor as _descriptor\nFile \"C:\\Program Files\\Python36\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 47, in\nfrom google.protobuf.pyext import _message\nImportError: DLL load failed: The specified procedure could not be found.\n\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.", "> Raghvendra Dubey \u60a8\u597d\uff01\u6211\u5df2\u6536\u5230\u60a8\u7684\u90ae\u4ef6\u3002 \u6709\u4ec0\u4e48\u95ee\u9898\u968f\u65f6\u4e0e\u6211\u8054\u7cfb\u548c\u6c9f\u901a\uff0c\u795d\u597d\uff01 2019-7-2 | | xxx | | yonglonggeng@163.com | \u7b7e\u540d\u7531\u7f51\u6613\u90ae\u7bb1\u5927\u5e08\u5b9a\u5236 On 7/2/2019 15:50\uff0cRaghvendra Dubey<notifications@github.com> wrote\uff1a Traceback (most recent call last): File \"eg1.py\", line 1, in import tensorflow as tf File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow_init_.py\", line 28, in from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python_init_.py\", line 52, in from tensorflow.core.framework.graph_pb2 import * File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\core\\framework\\graph_pb2.py\", line 7, in from google.protobuf import descriptor as _descriptor File \"C:\\Program Files\\Python36\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 47, in from google.protobuf.pyext import _message ImportError: DLL load failed: The specified procedure could not be found. \u2014 You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread.\r\n\r\nhelp me to figure out what is the problem \r\n", "@RaghvendraDubey20  Try doing one of the things suggested here https://github.com/tensorflow/tensorflow/issues/22794 and here https://stackoverflow.com/questions/52092810/tensorflow-error-dll-load-failed-the-specified-procedure-could-not-be-found.\r\nIt's mostly a compatibility problem between your CUDA and tensorflow version. ", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\n If you are unclear what to include see the issue template displayed in the Github new issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose)\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 30301, "title": "[BUG] tf.layers.dropout not working", "body": "To reproduce the issue:\r\n```py\r\nsess = tf.Session()\r\nx = tf.ones([4, 4])\r\ny = tf.layers.dropout(x, 0.5)\r\nsess.run(y)\r\n```\r\n\r\nThis version works well:\r\n```py\r\nsess = tf.Session()\r\nx = tf.ones([4, 4])\r\ny = tf.nn.dropout(x, 0.5)\r\nsess.run(y)\r\n```\r\n\r\nTensorflow version: 1.13.3\r\nOS:  Ubuntu 18.04", "comments": ["I think this API tf.layers.dropout is already deprecated", "I think tf.layers.dropout would use keras.layers.Dropout\r\nAnd finally call this \"lambda: array_ops.identity(inputs)\" to return the exactly same tensor", "OK, but it would be more reasonable if it is wrapped to `tf.nn.dropout`", "@ghostplant I tried executing the code on Colab with Tensorflow 1.13.1\r\n```\r\nimport tensorflow as tf\r\nsess = tf.Session()\r\nx = tf.ones([4, 4])\r\ny = tf.layers.dropout(x, 0.5)\r\nsess.run(y)\r\n\r\n```\r\nIs this the expected output\r\narray([[1., 1., 1., 1.],\r\n       [1., 1., 1., 1.],\r\n       [1., 1., 1., 1.],\r\n       [1., 1., 1., 1.]], dtype=float32)\r\nThanks!", "@gadagashwini No, if you use `tf.nn.dropout` instead, you should get the expected output: some are zero and others are two.", "@ghostplant You are correct. With `tf.nn.dropout` i am able to get expected result. Thanks!", "The following will work:\r\n```\r\nimport tensorflow as tf\r\nsess = tf.Session()\r\nx = tf.ones([4, 4])\r\ny = tf.layers.dropout(x, 0.5, training=True)\r\nsess.run(y)\r\n```\r\n\r\nNote in the doctoring of `tf.layers.dropout`:\r\n```\r\n    training: Either a Python boolean, or a TensorFlow boolean scalar tensor\r\n      (e.g. a placeholder). Whether to return the output in training mode\r\n      (apply dropout) or in inference mode (return the input untouched).\r\n```\r\n", "@yongtang Thank you. It is a little weird that training is set to False by default, because it is just not what this operator describes to be, so I think it would be more reasonable to change the default value. I think many developers don't even notice this case when they just select to use a dropout operator.\r\nUsually, using dropout or not is most likely to be determined by changing the branch in user level in  many existing programs, such as:\r\n```sh\r\nif is_training:\r\n  out = tf.layers.dropout(out)\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30301\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30301\">No</a>\n"]}, {"number": 30300, "title": "[mlir] mlir stuff doesn't build after MLIR rev updated", "body": "mlir stuff doesn't build after 99d4a961210c7d800f4ccaf9a9b0a5c3bfc799b6 because of dependency problems", "comments": ["@jpienaar `BUILD` file has dependency problems.", "Thanks, Incoming change to address this and bump version.", "close this since this is addressed in e54c0bd"]}, {"number": 30299, "title": "[ROCm] ROCm stream executor implementation updates", "body": "This PR pushes out updates in the ROCm stream executor implementation from the ROCm TF fork. Most of the changes are cosmetic (`clang-format` related). The lone functional change is the addition of conv 3D support.\r\n\r\nPlease review and merge. thanks.\r\n\r\n---------------------------\r\n\r\n@tatianashp @whchung @chsigg ", "comments": ["@rthadur \r\n\r\nthis is the 2nd of 4 PRs that seem to be stuck in the merge pipeline. \r\nPlease let me know if there is anything that needs to be done on my end.\r\nthanks\r\n\r\ndeven", "> @rthadur\r\n> \r\n> this is the 2nd of 4 PRs that seem to be stuck in the merge pipeline.\r\n> Please let me know if there is anything that needs to be done on my end.\r\n> thanks\r\n> \r\n> deven\r\n\r\n@deven-amd thank you for your patience waiting for @chsigg review in order to pull this in."]}, {"number": 30298, "title": "Add mask optional arg to Batchnormalization.call()", "body": "Attempt to support masked values in BatchNormalization layer, so that mean and variance are correctly calculated ( #30161 )\r\n\r\nUses a modified version of nn.moments() which takes a `mask` argument.\r\n\r\n@karmel ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30298) for more info**.\n\n<!-- need_sender_cla -->", "@LukeBolly Please sign CLA in order to proceed with next steps. Thanks!", "Signed!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30298) for more info**.\n\n<!-- ok -->", "changes have been implemented", "Will these updates also support masking when `renorm=True`?", "Can one of the admins verify this patch?", "Any updates as to the status of this?", "@LukeBolly,  @qlzh727, @alextp  Any update on this PR, please. Thanks!", "@LukeBolly Could you please check alextp's comments and keep us posted. Thanks!", "I'll get to this shortly", "Fixed up the test and moved the mask handling into nn.moments, let me know if there is anything else needed.", "@LukeBolly Can you please resolve conflicts? Thanks!", "It has been 18 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Closing this PR since it is quite stale. @LukeBolly feel free to reopen it if you are back and resolve the merge conflict.", "This would be really useful! Hoping this gets merged in \ud83d\ude4f "]}, {"number": 30297, "title": "AttributeError: module 'tensorflow' has no attribute 'matrix_band_part'", "body": "Tensorflow 2.0.0-alpha0 AttributeError: module 'tensorflow' has no attribute 'matrix_band_part'", "comments": ["Can you provide a code sample, version you are using, how you installed it, operating platform, etc? There is an issue template with all the details that should be filled in to get a faster response.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 30296, "title": "Fixed a broken URL in TF Lite iOS Benchmark doc", "body": "", "comments": []}, {"number": 30295, "title": "Disable caching by default unless GCS_READ_CACHE_MAX_SIZE_MB is set.", "body": "Cherry-picked from master e43b94649d3e1ac5d538e4eca9166b899511d681\r\nAdd a buffered GCS reader ideal for sequential reads. This reader will preload up to 64MB (default GCS_READ_CACHE_BLOCK_SIZE_MB) to avoid reading small chunks of data from GCS. This buffer is per file handle and not shared with other readers of the same file. Use this new reader when caching is disabled.\r\n\r\nPiperOrigin-RevId: 255689024", "comments": []}, {"number": 30294, "title": " [Intel MKL] Fix klockworks issue", "body": "", "comments": ["@penpornk thanks.\r\nit's klocwork, a static code analysis tool.\r\nhttps://www.roguewave.com/products-services/klocwork/static-code-analysis\r\n", "@guizili0 Got it. Thank you! :)", "@rthadur Could you please help merge this?", "> @rthadur Could you please help merge this?\r\n\r\nsure pulling in this now again", "@rthadur Thank you! :)"]}, {"number": 30292, "title": "[ROCm] Adding support to depthwise_conv_op", "body": "This PR adds ROCm support for the depthwise_conv_ops\r\n\r\nThe changes in this PR are mostly trivial, please review and merge.\r\n\r\n------\r\nNotes:\r\n\r\n- CNN for double datatype is not yet supported in ROCm, therefore `dtypes.float64` is ignored\r\n- Test passed: //tensorflow/python/kernel_tests:depthwise_conv_op_test\r\n   - All but testDepthwiseConv2DInputGradFormat and testDepthwiseConv2DFilterGradFormat failing on CPU for `Depthwise convolution on CPU is only supported for NHWC format`\r\n\r\n@tatianashp @whchung @chsigg", "comments": ["In responding to @chsigg's question, \r\n\r\n- Everything run fine except `testDepthwiseConv2DFilterGrad` and `testDepthwiseConv2DFilterGradFormat`. Those two tests are failing due to format unsupported from CPU side. \r\n- Since ROCm does not satisfy [`#if CUDNN_VERSION >= 7000`](https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/blob/develop-upstream/tensorflow/core/kernels/depthwise_conv_op.cc#L479). `use_cudnn_grouped_conv_ ` is always false. I'm fairly confident that `LaunchDepthwiseConvOp` always runs the local TF kernels so test coverage is good.\r\n\r\nIt does look like though it is assuming 32 lanes per warp, eg [here](https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/blob/develop-upstream/tensorflow/core/kernels/depthwise_conv_op_gpu.h#L1510). @whchung I don't have enough background to tell what impact this assumption has on AMD Gpus, mind taking a look and remind me what I should do for the default 32 lanes per warp?", "> It does look like though it is assuming 32 lanes per warp, eg [here](https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/blob/develop-upstream/tensorflow/core/kernels/depthwise_conv_op_gpu.h#L1510). @whchung I don't have enough background to tell what impact this assumption has on AMD Gpus, mind taking a look and remind me what I should do for the default 32 lanes per warp?\r\n\r\nI think the reason is because `kCudaWarpAll` is defined as `0xffffffff` so warp voting function would also only activate at most 32 threads on AMD hardware.", "@chsigg I synced with @whchung offline and we agree that the assumption on 32 lanes per warp to be addressed al-together in future PR, together with `kGpuWarpAll` [here](https://github.com/tensorflow/tensorflow/blob/ce09f4ad03a5a97dae2db17b76b95288bbf4bdb1/tensorflow/core/util/gpu_device_functions.h#L137). Testing wise this kernel is doing well in my ROCm machine. Let me know if you have further feedbacks.", "@jerryyin \r\n\r\ncan you amend the PR to also enable ROCm support in the `tensorflow/core/kernels/depthwise_conv_grad_op.cc` file. :)\r\n\r\nthanks\r\ndeven", "A gentle ping @chsigg", "@rthadur can we merge this PR in?"]}, {"number": 30291, "title": "[ROCm] Adding ROCm support for the reduction ops", "body": "This PR adds ROCm support for the reduction ops\r\n\r\nThe changes in this PR are mostly trivial, please review and merge.\r\n\r\nNotes:\r\n* ROCm does not have support for complex datatype for some BLAS operations. As a consequence you will see #ifdef GOOGLE_CUDA around code related to complex datatype\r\n* This PR merge (https://github.com/tensorflow/tensorflow/commit/41c36223dcd265603ca6916fec0a88016eb9abaa#diff-dfb288ce4a5f0ff51ccd12df2c86edac) seems to have accidentally undone ROCm support for the where op. The second commit in this PR, re-enables the ROCm support for the where op. It is included as a part of this PR, because ROCm support for where op is required for the `--config=rocm` build to complete, when ROCm support for reduction ops is also enabled.\r\n\r\n-------------------------------------------\r\n\r\n@tatianashp @whchung @chsigg \r\n", "comments": ["@rthadur , I will push out a rebase shortly to resolve the merge conflict.\r\n", "rebase done.\r\n\r\n@chsigg , please re-approve...thanks", "@rthadur \r\n\r\nthis is the 3rd of 4 PRs that seem to be stuck in the merge pipeline. \r\nPlease let me know if there is anything that needs to be done on my end.\r\nthanks\r\n\r\ndeven", "> @rthadur\r\n> \r\n> this is the 3rd of 4 PRs that seem to be stuck in the merge pipeline.\r\n> Please let me know if there is anything that needs to be done on my end.\r\n> thanks\r\n> \r\n> deven\r\n\r\ntrying to pull this in again , thank you", "@deven-amd there are some internal test failures happening for this change , @chsigg can you please help to resolve those ?", "@rthadur @chsigg gentle ping. thanks.", "@chsigg while we have your attention, can you please also propose next steps for this PR?", "@chsigg gentle ping. "]}, {"number": 30290, "title": "@tf.function works the first time I run the code but it fails afterwords on Spyder IDE.", "body": "PROBLEM: the first time I run my custom code (that by the way works as a charm without @tf.function on my training function) on Spyder IDE everything goes smoothly. However if I attempt a second run I get the error reported below and a huge memory leak. I thought it was important for you to know. Thanks for your amazing work and best of lucks.\r\n\r\nTensorFlow version: 2.0.0-beta1\r\nEager execution: True\r\nMacOs\r\n\r\n**Code to reproduce the issue**\r\n\r\n    @tf.function\r\n    def train(self, X, Y, epochs, batch_size=None):\r\n\r\n        iter_data = self.bake_data(X, Y, epochs, batch_size)\r\n\r\n        for x, y in iter_data:\r\n\r\n            with tf.GradientTape() as tape:\r\n                Y_ = self.forward_step(x)\r\n                loss = self.loss_fun(y_true=y, y_pred=Y_)\r\n                cost = tf.reduce_mean(loss)\r\n\r\n            gradients = tape.gradient(cost, self.trainable_vars)\r\n            self.optimizer.apply_gradients(zip(gradients, self.trainable_vars))\r\n            self.loss_history_.append(cost)\r\n\r\n\r\n\r\n    def bake_data(self, X, Y, epochs, batch_size=None):\r\n        data = tf.data.Dataset.from_tensor_slices((X, Y))\r\n        if batch_size is not None:\r\n            data = data.batch(batch_size)\r\n        data = data.repeat(epochs)\r\n        data = data.shuffle(buffer_size=10000)\r\n        return iter(data)\r\n\r\n**Other info / logs**\r\nWARNING: Logging before flag parsing goes to stderr.\r\nE0701 21:48:20.707326 4585448896 ag_logging.py:132] Error converting <bound method Model.train of <tensorflow.python.eager.function.TfMethodTarget object at 0x1a3a5020b8>>\r\nTraceback (most recent call last):\r\n  File \"/Users/sm0037/anaconda/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 561, in to_graph\r\n    return conversion.convert(entity, program_ctx)\r\n  File \"/Users/sm0037/anaconda/lib/python3.7/site-packages/tensorflow/python/autograph/impl/conversion.py\", line 314, in convert\r\n    return _instantiate(entity, converted_entity_info, free_nonglobal_var_names)\r\n  File \"/Users/sm0037/anaconda/lib/python3.7/site-packages/tensorflow/python/autograph/impl/conversion.py\", line 256, in _instantiate\r\n    factory = converted_entity_info.get_factory()\r\n  File \"/Users/sm0037/anaconda/lib/python3.7/site-packages/tensorflow/python/autograph/impl/conversion.py\", line 92, in get_factory\r\n    assert self.module_name in sys.modules\r\nAssertionError\r\nW0701 21:48:20.709814 4585448896 ag_logging.py:145] Entity <bound method Model.train of <tensorflow.python.eager.function.TfMethodTarget object at 0x1a3a5020b8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Model.train of <tensorflow.python.eager.function.TfMethodTarget object at 0x1a3a5020b8>>: AssertionError: \r\n", "comments": ["In order to expedite the trouble-shooting process, please provide sample datasets and full minimal code snippet to reproduce the issue reported here. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Same problem. I have a simple reproduction here. It seems to be related with `reimport` process in the Spyder console.\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\n@tf.function\r\ndef mean(a):\r\n    return tf.reduce_mean(tf.abs(a))\r\n\r\n```\r\nOutput:\r\n```plain\r\nrunfile('D:/tfbug.py', wdir='D:/')\r\n\r\nmean([1, 2])\r\nOut[2]: <tf.Tensor: id=9, shape=(), dtype=int32, numpy=1>\r\n\r\nmean([1, 2])\r\nOut[3]: <tf.Tensor: id=10, shape=(), dtype=int32, numpy=1>\r\n\r\nrunfile('D:/tfbug.py', wdir='D:/')\r\nReloaded modules: tmpv1iph1dn\r\n\r\nmean([1, 2])\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0718 20:43:06.847428 10664 ag_logging.py:146] Entity <function mean at 0x000000001DB28B70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \r\nWARNING: Entity <function mean at 0x000000001DB28B70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \r\nOut[5]: <tf.Tensor: id=19, shape=(), dtype=int32, numpy=1>\r\n```\r\nSeems that when reloading modules, something generated from tensorflow has been destroyed, but tensorflow still tracks to it, causing the bug.", "After digging for a while, I finally found that it is related with Spyder User Module Reloader (UWR).\r\nNowhere documents controlling the UWR from script, and I dived into Spyder source code. I come up with an automatic process like this:\r\n```python\r\ntry:\r\n    import os\r\n    import sys\r\n    from spyder_kernels.customize import spydercustomize as spc\r\n    if spc.__umr__ is None:\r\n        namelist = os.environ.get(\"SPY_UMR_NAMELIST\", None)\r\n        if namelist is not None:\r\n            namelist = namelist.split(',')\r\n        spc.__umr__ = spc.UserModuleReloader(namelist=namelist)\r\n    spc.__umr__.namelist += \\\r\n        list(filter(lambda x: x.startswith(\"tmp\"), sys.modules.keys()))\r\nexcept ImportError:\r\n    pass\r\n```\r\nAppend this to the end of the calling script and everything goes on well. It looks quite hacky, and maybe things will change with the Spyder version.\r\nI think it is rather a Spyder issue than a TensorFlow issue. I'm going to report it to the Spyder."]}, {"number": 30289, "title": "Creating custom operations for Tensorflow Windows", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Microsoft Windows 10\r\n- ~~Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:~~\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 1.12.3\r\n- Python version: 3.5.0\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.17.2\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- ~~CUDA/cuDNN version: CPU, N/A~~\r\n- ~~GPU model and memory: N/A~~\r\n\r\n**Describe the problem**\r\n\r\nI've been trying to get a custom operation working with a Windows version of TensorFlow. I've followed the guide:\r\n\r\nhttps://www.tensorflow.org/guide/extend/op#build_a_pip_package_for_your_custom_op\r\n\r\nI was able to build correctly, but it seems my custom operation is not recognized still when I try to run a test on it.\r\n\r\nIt seems to recognize the built-in custom operation tf.user_ops.my_fact, but it does not recognize the user_ops I created: squared_out.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. Clone the repository:\r\n\r\n```powershell\r\ngit clone https://github.com/tensorflow/tensorflow.git \r\n```\r\n\r\nI put my C++ implementation of squared_out.cc in tensorflow\\tensorflow\\core\\user_ops\r\n\r\n```c++\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n#include \"tensorflow/core/framework/op.h\"\r\n#include \"tensorflow/core/framework/shape_inference.h\"\r\n\r\nusing namespace tensorflow;\r\n\r\nREGISTER_OP(\"SquaredOut\")\r\n    .Input(\"to_square: int32\")\r\n    .Output(\"squared: int32\")\r\n    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext *c) {\r\n        c->set_output(0, c->input(0));\r\n        return Status::OK();\r\n    });\r\n\r\n\r\nclass SquaredOutOp : public OpKernel {\r\n\r\n    public:\r\n        explicit SquaredOutOp(OpKernelConstruction* context) : OpKernel(context) {}\r\n\r\n        void Compute(OpKernelContext* context) override {\r\n\r\n            const Tensor& input_tensor = context->input(0);\r\n            auto input = input_tensor.flat<int32>();\r\n\r\n            Tensor* output_tensor = NULL;\r\n            OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(), &output_tensor));\r\n\r\n            auto output_flat = output_tensor->flat<int32>();\r\n\r\n            const int N = input.size();\r\n\r\n            for(int i = 0; i < N; ++i){\r\n\r\n                output_flat(i) = input(i) * input(i);\r\n\r\n            }\r\n        }\r\n};\r\n\r\nREGISTER_KERNEL_BUILDER(Name(\"SquaredOut\").Device(DEVICE_CPU), SquaredOutOp);\r\n```\r\n2. Afterwards I configure and built the tensorflow library by using bazel:\r\n\r\n```powershell\r\npython ./configure.py\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nbazel-bin\\tensorflow\\tools\\pip_package\\build_pip_package C:/tmp/tensorflow_pkg\r\n```\r\n\r\n3. I then installed the .whl that was produced by bazel:\r\n\r\n```powershell\r\npip install C:/tmp/tensorflow_pkg/tensorflow-version-cp35-cp35m-win_amd64.whl\r\n```\r\n4. Finally, I tried to test the newly imported squared_out from user_ops:\r\n```python\r\nimport tensorflow as tf\r\n#This works fine with build in fact function\r\ntf.user_ops.fact\r\n```\r\nThis returned:\r\n```python\r\n<function my_fact at 0x0000020CE5601048>\r\n```\r\nHowever, my custom operation doesn't work:\r\n```python\r\ntf.user_ops.squared_out\r\nAttributeError: module 'tensorflow._api.v1.user_ops' has no attribute 'squared_out'\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nDo I need to add a python binding for `squared_out` in tensorflow\\tensorflow\\python\\user_ops? \r\n\r\nIf so, I'm a little confused on how to do so. ", "comments": ["Hello alayu,\r\n\r\nIt would seem you missed a step, your operation is not automatically included in the build. You could add it to the build system if you're feeling adventurous or you can build it separately using bazel as explained in the tutorial.\r\n\r\nspecifically see: https://www.tensorflow.org/guide/extend/op#compile_the_op_using_bazel_tensorflow_source_installation\r\n\r\nFurthermore once you load tensorflow you will need to load the op library in the python code \r\nso somewhere in your python code you'll need to add \r\n```\r\nsquared_out_lib = tf.load_op_library('./squared_out.so')\r\n```\r\n\r\nTo summarize: \r\nYou need to build the library then load it in python code.\r\n", "@Scott-Thomas-Dev \r\n\r\nThanks Scott, \r\n\r\nIt seems like I was able to get the library compiled and loaded, I had an issue prior where .so file was not being read by Python. I believe I used Python 3.6.7 in a conda environment when compiling the .so file with bazel, but when I tried running it on my base environment the Python version was 3.5.0. \r\n\r\nThe error I'm referring to was this:\r\n\r\n```python\r\nImportError: DLL load failed: The specified module could not be found.\r\n```", "On Windows, I also have the same problem. Have you solved it?\r\nwhen I change the source code \u2018site-packages\\tensorflow\\_api\\v1\\user_ops\\__init__.py\u2019  to\r\n'from tensorflow.python.user_ops.user_ops import *', \r\nthe error changes to \u2018Op type not registered 'XXX' in binary running on DESKTOP-NXXXX\u2019"]}, {"number": 30288, "title": "Intel MKL : Fixed variable initialization, pass Clockworks and fixed Clang format", "body": "Minor edits.\r\nFixed variable initialization issue, thus now passed the Clockworks test. \r\nAlso fixed Clang format. ", "comments": ["@rthadur Could you please help merge this?", "> @rthadur Could you please help merge this?\r\n\r\nsure", "@rthadur Thank you!"]}, {"number": 30287, "title": "third_party/repo.bzl file silently failed to apply patch", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 16.04:\r\n- ~Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:~\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master branch\r\n- Python version: 3\r\n- ~Installed using virtualenv? pip? conda?:~\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- ~CUDA/cuDNN version:~\r\n- ~GPU model and memory:~\r\n\r\n\r\n\r\n**Describe the problem**\r\n[`git apply -v` in bazel scripts](https://github.com/tensorflow/tensorflow/blob/87f2f24f957abb76ad80e16b26b7221ddd277752/third_party/repo.bzl#L69) sliently failed to apply a patch. This cause tensorflow build to fail in an unexpected way.\r\n\r\nThe reason why `git apply -v` silently failed is due to [`git apply does not work from within the local checkout of an unrelated git repository`](https://stackoverflow.com/questions/24821431/git-apply-patch-fails-silently-no-errors-but-nothing-happens). In my setup, my home directory is a git directory used to sync up local development configuration files, and tensorflow build directory is under my home directory. This makes all tensorflow directories within the local checkout of a git directory (other than the one for project the patch is made for). Thus, at build time, the `_apply_patch()` subroutine will silently fail to apply any patch.\r\n\r\nTo fix, I will use the same patch command under either windows/linux to use the actual `patch -p1` command rather than `git apply`, ie, use [this line](https://github.com/tensorflow/tensorflow/blob/87f2f24f957abb76ad80e16b26b7221ddd277752/third_party/repo.bzl#L67) only. I didn't submit a PR because I'm not aware of why applying the patch need to branch on different OSes. If there is not a good reason to use `git apply`, I'd suggest to always use `patch -p1` instead.\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. Place tensorflow root directory into a/any git directory\r\n2. Compile tensorflow\r\n3. Tensorflow compilation failed due to related patch not applied.\r\n\r\n~**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.~\r\n", "comments": ["Just to verify did you get chance to follow instructions from [TensorFlow ](https://www.tensorflow.org/install/source)website .Please, let us know. Thanks", "@ravikyram Yes I do. Since official documentation never mentioned that it cannot be built from an existing git directory, it should not silently failing.", "I don't think this is a Tensorflow issue, seems to be caused by your special environment settings.\r\n\r\nGit inside git is quite a no-no, to be honest, unless you're working with git submodules.", "@mihaimaruseac While I agree it is very configuration related, what's the point of using different ways of apply patch on different OSes (knowing such a pitfall exists in `git apply`)?\r\n\r\nBTW, at the point when `git apply` silently fails, it is not in the \r\n\r\n> Git inside git \r\n\r\nsituation. It is in one level of git directory. It is due to bazel generated folders (symlink to `/root/.cache`) in a different git directory from where it is generated (TF). ", "The windows special casing was added in 87f2f24f957abb76ad80e16b26b7221ddd277752 Before that change, all code used `git apply`, since 8ec93550847eaf9289ff33403bb19fa78c097334 which changed from `patch -p1` to `git apply` because `patch -p1` on CentOS doesn't support renaming of files.\r\n\r\nAdding interested parties\r\n\r\n", "Sorry that this is causing trouble for you, and thanks Mihai for digging out the history and explaining the reason for the change. This was the best I could come up with to work around the limitations of `patch` on CentOS. Maybe there is a better way?", "@mihaimaruseac @chsigg Thanks for explaining. Good to know the background knowledge on issue under CentOS. Given what the issue it is, I'd say the safest way to run `git apply` is to run it under the system root `/` where it is least likely to have git enabled (indicated by second point under [this answer](https://stackoverflow.com/a/27283285/3002734)).\r\n\r\nI'm glad I'm able to figure it out, because it has choked me periodically for a while. But now I wouldn't mind if you guys leave the code as is, as it is caused by deficiencies of tooling in different platforms. Anyways I have automated my environment to get rid of git wrapping around bazel's generated files. ", "The patch command is always a headache on Windows because it's not a native tool. Looks like it is also a problem for Centos due to old versions. \r\nBazel is implementing a native patch in Java, we'll expose an API `repository_ctx.patch` to allow users do patch in repository rules. It supports the most common features of patch command line tools (renaming, adding, deleting files), but will be more strict on the patch file content. This is still an ongoing work: https://github.com/bazelbuild/bazel/issues/8316.", "@jerryyin  let me know if we can close this issue since it looks to be fixed. Thanks!", "@ravikyram I knew the fix from beginning, but I would leave it open until the bazel implementation from @meteorcloudy is ready and replaced the logic here.", "I am +1 on leaving open until Bazel version is in production", "@jerryyin We are checking to see if you still need help on this issue. Could you please try with TF 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. If it is resolved then please feel free to move this issue to close status ? Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30287\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30287\">No</a>\n"]}, {"number": 30286, "title": "[TF 2.0] tf.keras.preprocessing.image", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.0.0-beta1\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nKeras has neat data feeder in ```tf.keras.preprocessing.image.ImageDataGenerator``` that is basically working for Python 2 only because it uses ```PIL``` for images which currently is not available for Python 3.\r\n**Will this change the current api? How?**\r\nIt will not change the API, it just makes it work with python 3 (perhaps using pillow)\r\n**Who will benefit with this feature?**\r\nAnyone trying to use Python3 API of TF2.0 \r\n**Any Other info.**\r\n", "comments": ["Our focus has been pivoted to support this through Python3 via [this work](https://github.com/keras-team/governance/blob/master/rfcs/20190729-keras-preprocessing-redesign.md)."]}, {"number": 30285, "title": "Don't include xmmintrin or related tests on non-x86 systems.", "body": "Don't include xmmintrin or related tests on non-x86 systems. xmmintrin.h has x86 intrinsics\r\nShould resolve the latest issues seen in https://github.com/tensorflow/tensorflow/issues/28553.\r\n\r\n@ebrevdo Can you review this?", "comments": ["Can one of the admins verify this patch?", "@ebrevdo , could you please help in merging this PR ?"]}, {"number": 30284, "title": "CollectiveGatherOpKernel::ComputeAsync always crash when using MultiWorkerMirroredStrategy", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNone\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nCentos 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNone\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\n1.14\r\n- Python version:\r\n2.7\r\n- Bazel version (if compiling from source):\r\nNone\r\n- GCC/Compiler version (if compiling from source):\r\nNone\r\n- CUDA/cuDNN version:\r\nNone\r\n- GPU model and memory:\r\nNone\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n('v1.14.0-rc1-22-gaf24dc91b5', '1.14.0')\r\n\r\n**Describe the current behavior**\r\nWhen I use MultiWorkerMirroredStrategy to train model in multi-worker mode. I always got Error like this: \"check failed: nullptr == ctx->op_kernel().AsAsync() (nullptr vs. 0x7f8f74284010)Use OP_REQUIRES_ASYNC in AsyncOpKernel implementations.\"\r\nThen I use gdb to get stack of this crash. it shows that function CollectiveGatherOpKernel:: ComputeAsync use OP_REQUIRES instead of OP_REQUIRES_ASYNC. So I think this is a bug?\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```python\r\n# -*- coding: utf-8 -*-\r\nimport tensorflow as tf\r\nimport os\r\nimport shutil\r\nimport json\r\nfrom multiprocessing import Process\r\nclass Features(object):\r\n    @property\r\n    def names(self):\r\n        return ['a', 'b']\r\n\r\n    @property\r\n    def names_usedByCols(self):\r\n        return ['a', 'b']\r\n    \r\n    @property\r\n    def col_ids(self):\r\n        return [0, 1]\r\n\r\nclass Labels(object):\r\n    def __init__(self):\r\n        self._names = ('label',)\r\n        self._dtypes = [tf.int32]\r\n        self._col_ids = [2]\r\n\r\n    @property\r\n    def names(self):\r\n        return self._names\r\n\r\n    @property\r\n    def dtypes(self):\r\n        return self._dtypes\r\n\r\n    @property\r\n    def col_ids(self):\r\n        return self._col_ids\r\n\r\nclass Generator_RNN(object):\r\n    def __init__(self,\r\n            features,\r\n            labels):\r\n        self._feature_names = features.names_usedByCols\r\n        self._label_names = labels.names\r\n        self._feature_dtypes = {}\r\n        self._feature_shapes = {}\r\n        for i, name in enumerate(self._feature_names):\r\n            self._feature_dtypes[name] = tf.int32\r\n            self._feature_shapes[name] = [None, None]\r\n\r\n        self._label_dtypes = {}\r\n        self._label_shapes = {}\r\n        for i, name in enumerate(self._label_names):\r\n            self._label_dtypes[name] = labels.dtypes[i]\r\n            self._label_shapes[name] = [None]\r\n        self._i = 0\r\n        self._f_3 = {'a': [[1, 2, 3],[1, 2, 3],[1, 2, 3],[1, 2, 3]],\r\n                     'b': [[3, 1, 1],[3, 1, 1],[3, 1, 1],[3, 1, 1]]}\r\n        self._l_3 = {'label': [1, 1, 1, 1]}\r\n        self._f_2 = {'a': [[1, 2],[1, 2],[1, 2],[1, 2]],\r\n                     'b': [[3, 1],[3, 1],[3, 1],[3, 1]]}\r\n        self._l_2 = {'label': [0, 0, 0, 0]}\r\n\r\n    def _generate_data(self):\r\n        while True:\r\n            yield self._next_batch()\r\n\r\n    def _next_batch(self):\r\n        if self._i == 0:\r\n            self._i = 1\r\n            return (self._f_3, self._l_3)\r\n        else:\r\n            self._i = 0\r\n            return (self._f_2, self._l_2)\r\n\r\n    def _output_dtypes(self):\r\n        return (self._feature_dtypes, self._label_dtypes)\r\n\r\n    def _output_shapes(self):\r\n        return (self._feature_shapes, self._label_shapes)\r\n\r\n    def create_dataset(self):\r\n        return tf.data.Dataset.from_generator(\r\n                generator=self._generate_data,\r\n                output_types=self._output_dtypes(),\r\n                output_shapes=self._output_shapes(),\r\n                args=[])\r\n\r\ndef _dataset():\r\n    generator = Generator_RNN(\r\n                features=Features(),\r\n                labels=Labels())\r\n    return generator.create_dataset()\r\n\r\ndef _gen_feature_column(name):\r\n    vocabulary_list = [1,2,3]\r\n    feature_column = tf.feature_column.sequence_categorical_column_with_vocabulary_list(\r\n                    key=name,\r\n                    vocabulary_list=vocabulary_list)\r\n    ebeding_dimension = int(len(vocabulary_list) ** 0.25) * 3\r\n    feature_column = tf.feature_column.embedding_column(\r\n            feature_column,\r\n            ebeding_dimension)\r\n    return feature_column\r\n\r\ndef _my_model_fn(features, labels, mode, params):\r\n    learning_rate = params['learning_rate']\r\n    keep_prob = params['keep_prob']\r\n    n_classes = 2\r\n    dropout_rate = 1-keep_prob\r\n    feature_columns = [_gen_feature_column('a'), _gen_feature_column('b')]\r\n    sequence_input_layer = tf.keras.experimental.SequenceFeatures(feature_columns)\r\n    sequence_input, _ = sequence_input_layer(features)\r\n    sequence_input = tf.nn.dropout(sequence_input, rate=dropout_rate)\r\n\r\n    def gen_gru(units, keep_prob):\r\n        gru = tf.compat.v1.nn.rnn_cell.GRUCell(num_units=units, kernel_initializer=tf.compat.v1.orthogonal_initializer)\r\n        gru = tf.compat.v1.nn.rnn_cell.DropoutWrapper(gru, output_keep_prob=keep_prob)\r\n        return gru\r\n\r\n    gru_units = [64, 32]\r\n    cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell(\r\n            [gen_gru(units, keep_prob) for units in gru_units]\r\n        )\r\n    dynamic_rnn = tf.keras.layers.RNN(cell)\r\n    outputs = dynamic_rnn(inputs=sequence_input)\r\n\r\n    last_gru_units = gru_units[-1]\r\n    dense_layer = tf.keras.layers.Dense(\r\n            units=n_classes,\r\n            activation=tf.nn.relu,\r\n            kernel_initializer=tf.random_normal_initializer(stddev=tf.sqrt(2.0 / (last_gru_units * keep_prob))))\r\n    logits = dense_layer(inputs=outputs)\r\n    loss = tf.compat.v1.losses.sparse_softmax_cross_entropy(labels=labels['label'], logits=logits)\r\n    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\r\n    train_op = optimizer.minimize(loss, global_step=tf.compat.v1.train.get_global_step())\r\n    spec = tf.estimator.EstimatorSpec(tf.estimator.ModeKeys.TRAIN,\r\n            loss=loss,\r\n            train_op=train_op)\r\n    return spec\r\n\r\ndef train(task_index, model_dir, max_steps):\r\n    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(\r\n            communication=tf.distribute.experimental.CollectiveCommunication.RING)\r\n    cluster = {'worker': ['localhost:2001', 'localhost:2002']}\r\n    os.environ['TF_CONFIG'] = json.dumps({\r\n        'cluster': cluster,\r\n        'task': {'type': 'worker', 'index': task_index}\r\n    })\r\n    run_config = tf.estimator.RunConfig(\r\n            save_checkpoints_steps=500,\r\n            keep_checkpoint_max=1,\r\n            train_distribute=strategy)\r\n    learning_rate = 1e-6\r\n    keep_prob = 0.75\r\n    estimator_distribute = tf.estimator.Estimator(\r\n            model_fn=_my_model_fn,\r\n            model_dir=model_dir,\r\n            config=run_config,\r\n            params={\r\n                'learning_rate': learning_rate,\r\n                'keep_prob': keep_prob\r\n            })\r\n    train_input_fn = lambda : _dataset()\r\n    train_spec = tf.estimator.TrainSpec(\r\n            input_fn=train_input_fn,\r\n            max_steps=max_steps)\r\n    dummy_spec = tf.estimator.EvalSpec(input_fn=lambda : _dummy_dataset())\r\n    eval_input_fn = lambda : _dataset()\r\n    eval_spec = tf.estimator.EvalSpec(\r\n            input_fn=eval_input_fn)\r\n    print \"Start Training\"\r\n    tf.estimator.train_and_evaluate(estimator_distribute, train_spec, dummy_spec)\r\n    print \"Finish Training\"\r\n\r\ndef main(argv):\r\n    model_dir = '/root/test/tf2/model/'\r\n    max_steps = 600\r\n    shutil.rmtree(model_dir)\r\n    os.mkdir(model_dir)\r\n    print 'Clean model dir'\r\n    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\r\n    print 'Control process %s.' % os.getpid()\r\n    worker_list = []\r\n    for i in range(2):\r\n        worker = Process(target=train, args=(i, model_dir, max_steps))\r\n        print 'Training worker starting'\r\n        worker.start()\r\n        worker_list.append(worker)\r\n\r\n    for worker in worker_list:\r\n        worker.join()\r\n    print 'Train end.'\r\n\r\nif __name__ == '__main__':\r\n    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\r\n    tf.compat.v1.app.run()\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Thanks for filing the issue, @wudixiaotie.  Can you share the stack trace?", "This is all the log I have, @dubey \r\n\r\n\r\nClean model dir\r\nControl process 17907.\r\nTraining worker starting\r\nTraining worker starting\r\n2019-07-16 09:30:58.419077: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2019-07-16 09:30:58.422058: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2019-07-16 09:30:58.441691: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2494130000 Hz\r\n2019-07-16 09:30:58.441909: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x23df5d0 executing computations on platform Host. Devices:\r\n2019-07-16 09:30:58.441925: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-07-16 09:30:58.443555: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2494130000 Hz\r\n2019-07-16 09:30:58.443687: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x23df5d0 executing computations on platform Host. Devices:\r\n2019-07-16 09:30:58.443701: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\nI0716 09:30:58.447894 139695682893632 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_CPU:0\r\nW0716 09:30:58.448482 139695682893632 cross_device_ops.py:1182] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\nI0716 09:30:58.448693 139695682893632 collective_all_reduce_strategy.py:162] CollectiveAllReduceStrategy with local_devices = ('/device:CPU:0',)\r\nI0716 09:30:58.448306 139695682893632 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_CPU:0\r\nW0716 09:30:58.448853 139695682893632 cross_device_ops.py:1182] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\nI0716 09:30:58.449009 139695682893632 run_config.py:528] TF_CONFIG environment variable: {u'cluster': {u'worker': [u'localhost:2001', u'localhost:2002']}, u'task': {u'index': 0, u'type': u'worker'}}\r\nI0716 09:30:58.449038 139695682893632 collective_all_reduce_strategy.py:162] CollectiveAllReduceStrategy with local_devices = ('/device:CPU:0',)\r\nI0716 09:30:58.449275 139695682893632 run_config.py:558] Initializing RunConfig with distribution strategies.\r\nI0716 09:30:58.449331 139695682893632 run_config.py:528] TF_CONFIG environment variable: {u'cluster': {u'worker': [u'localhost:2001', u'localhost:2002']}, u'task': {u'index': 1, u'type': u'worker'}}\r\nI0716 09:30:58.449543 139695682893632 estimator_training.py:177] RunConfig initialized for Distribute Coordinator with INDEPENDENT_WORKER mode\r\nI0716 09:30:58.449605 139695682893632 run_config.py:558] Initializing RunConfig with distribution strategies.\r\nI0716 09:30:58.449843 139695682893632 estimator_training.py:177] RunConfig initialized for Distribute Coordinator with INDEPENDENT_WORKER mode\r\nI0716 09:30:58.451220 139695682893632 estimator.py:209] Using config: {'_save_checkpoints_secs': None, '_num_ps_replicas': 0, '_keep_checkpoint_max': 1, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7539b50>, '_model_dir': '/root/test/tf2/model/', '_protocol': None, '_save_checkpoints_steps': 500, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n  }\r\n}\r\n, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_experimental_max_worker_delay_secs': None, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': <tensorflow.python.distribute.collective_all_reduce_strategy.CollectiveAllReduceStrategyV1 object at 0x7539b10>, '_master': '', '_distribute_coordinator_mode': 'independent_worker'}\r\nI0716 09:30:58.451500 139695682893632 estimator.py:209] Using config: {'_save_checkpoints_secs': None, '_num_ps_replicas': 0, '_keep_checkpoint_max': 1, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7539bd0>, '_model_dir': '/root/test/tf2/model/', '_protocol': None, '_save_checkpoints_steps': 500, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n  }\r\n}\r\n, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_experimental_max_worker_delay_secs': None, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': <tensorflow.python.distribute.collective_all_reduce_strategy.CollectiveAllReduceStrategyV1 object at 0x7539b90>, '_master': '', '_distribute_coordinator_mode': 'independent_worker'}\r\nStart Training\r\nStart Training\r\nI0716 09:30:58.452205 139695682893632 training.py:462] Running `train_and_evaluate` with Distribute Coordinator.\r\nI0716 09:30:58.452683 139695682893632 training.py:462] Running `train_and_evaluate` with Distribute Coordinator.\r\nI0716 09:30:58.452801 139695682893632 distribute_coordinator.py:776] Running Distribute Coordinator with mode = 'independent_worker', cluster_spec = {u'worker': [u'localhost:2001', u'localhost:2002']}, task_type = u'worker', task_id = 0, environment = None, rpc_layer = 'grpc'\r\nW0716 09:30:58.452907 139695682893632 distribute_coordinator.py:829] `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nI0716 09:30:58.453319 139695682893632 distribute_coordinator.py:776] Running Distribute Coordinator with mode = 'independent_worker', cluster_spec = {u'worker': [u'localhost:2001', u'localhost:2002']}, task_type = u'worker', task_id = 1, environment = None, rpc_layer = 'grpc'\r\nW0716 09:30:58.453499 139695682893632 distribute_coordinator.py:829] `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nI0716 09:30:58.454328 139695682893632 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:CPU:0\r\nI0716 09:30:58.454574 139695682893632 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_CPU:0\r\nW0716 09:30:58.454673 139695682893632 cross_device_ops.py:1177] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\nI0716 09:30:58.454914 139695682893632 collective_all_reduce_strategy.py:226] Multi-worker CollectiveAllReduceStrategy with cluster_spec = {u'worker': [u'localhost:2001', u'localhost:2002']}, task_type = u'worker', task_id = 0, num_workers = 2, local_devices = (u'/job:worker/task:0',), communication = CollectiveCommunication.RING\r\nI0716 09:30:58.455830 139695682893632 distribute_coordinator.py:438] Starting standard TensorFlow server, target = u'grpc://localhost:2001', session_config= allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n    scoped_allocator_optimization: ON\r\n    scoped_allocator_opts {\r\n      enable_op: \"CollectiveReduce\"\r\n    }\r\n  }\r\n}\r\nexperimental {\r\n  collective_group_leader: \"/job:worker/replica:0/task:0\"\r\n}\r\n\r\nI0716 09:30:58.455884 139695682893632 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:CPU:0\r\nI0716 09:30:58.456100 139695682893632 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_CPU:0\r\nW0716 09:30:58.456176 139695682893632 cross_device_ops.py:1177] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\nI0716 09:30:58.456418 139695682893632 collective_all_reduce_strategy.py:226] Multi-worker CollectiveAllReduceStrategy with cluster_spec = {u'worker': [u'localhost:2001', u'localhost:2002']}, task_type = u'worker', task_id = 1, num_workers = 2, local_devices = (u'/job:worker/task:1',), communication = CollectiveCommunication.RING\r\nI0716 09:30:58.457274 139695682893632 distribute_coordinator.py:438] Starting standard TensorFlow server, target = u'grpc://localhost:2002', session_config= allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n    scoped_allocator_optimization: ON\r\n    scoped_allocator_opts {\r\n      enable_op: \"CollectiveReduce\"\r\n    }\r\n  }\r\n}\r\nexperimental {\r\n  collective_group_leader: \"/job:worker/replica:0/task:0\"\r\n}\r\n\r\n2019-07-16 09:30:58.457987: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2001, 1 -> localhost:2002}\r\n2019-07-16 09:30:58.459327: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2001, 1 -> localhost:2002}\r\n2019-07-16 09:30:58.460406: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:2001\r\n2019-07-16 09:30:58.460442: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:369] Server already started (target: grpc://localhost:2001)\r\n2019-07-16 09:30:58.461169: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:2002\r\n2019-07-16 09:30:58.461215: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:369] Server already started (target: grpc://localhost:2002)\r\nI0716 09:30:58.462960 139695682893632 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:CPU:0\r\nI0716 09:30:58.463197 139695682893632 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_CPU:0\r\nW0716 09:30:58.463289 139695682893632 cross_device_ops.py:1177] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\nI0716 09:30:58.463522 139695682893632 collective_all_reduce_strategy.py:226] Multi-worker CollectiveAllReduceStrategy with cluster_spec = {u'worker': [u'localhost:2001', u'localhost:2002']}, task_type = u'worker', task_id = 0, num_workers = 2, local_devices = (u'/job:worker/task:0',), communication = CollectiveCommunication.RING\r\nI0716 09:30:58.464776 139695682893632 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:CPU:0\r\nI0716 09:30:58.465043 139695682893632 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_CPU:0\r\nW0716 09:30:58.465148 139695682893632 cross_device_ops.py:1177] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\nI0716 09:30:58.465399 139695682893632 collective_all_reduce_strategy.py:226] Multi-worker CollectiveAllReduceStrategy with cluster_spec = {u'worker': [u'localhost:2001', u'localhost:2002']}, task_type = u'worker', task_id = 1, num_workers = 2, local_devices = (u'/job:worker/task:1',), communication = CollectiveCommunication.RING\r\nI0716 09:30:58.466485 139695682893632 estimator_training.py:228] Updated config: {'_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n  }\r\n}\r\n, '_keep_checkpoint_max': 1, '_task_type': u'worker', '_train_distribute': <tensorflow.python.distribute.collective_all_reduce_strategy.CollectiveAllReduceStrategyV1 object at 0x75431d0>, '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7543910>, '_model_dir': '/root/test/tf2/model/', '_protocol': None, '_save_checkpoints_steps': 500, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 2, '_task_id': 1, '_log_step_count_steps': 100, '_experimental_max_worker_delay_secs': None, '_evaluation_master': u'grpc://localhost:2002', '_eval_distribute': None, '_global_id_in_cluster': 1, '_master': u'grpc://localhost:2002', '_distribute_coordinator_mode': 'independent_worker'}\r\nI0716 09:30:58.466942 139695682893632 estimator_training.py:228] Updated config: {'_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n  }\r\n}\r\n, '_keep_checkpoint_max': 1, '_task_type': u'worker', '_train_distribute': <tensorflow.python.distribute.collective_all_reduce_strategy.CollectiveAllReduceStrategyV1 object at 0x7543150>, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7543890>, '_model_dir': '/root/test/tf2/model/', '_protocol': None, '_save_checkpoints_steps': 500, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 2, '_task_id': 0, '_log_step_count_steps': 100, '_experimental_max_worker_delay_secs': None, '_evaluation_master': u'grpc://localhost:2001', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': u'grpc://localhost:2001', '_distribute_coordinator_mode': 'independent_worker'}\r\nW0716 09:30:58.531770 139695682893632 deprecation.py:323] From /usr/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:494: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\ntf.py_func is deprecated in TF V2. Instead, there are two\r\n    options available in V2.\r\n    - tf.py_function takes a python function which manipulates tf eager\r\n    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\r\n    an ndarray (just call tensor.numpy()) but having access to eager tensors\r\n    means `tf.py_function`s can use accelerators such as GPUs as well as\r\n    being differentiable using a gradient tape.\r\n    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\r\n    (it is not differentiable, and manipulates numpy arrays). It drops the\r\n    stateful argument making all functions stateful.\r\n\r\nW0716 09:30:58.532164 139695682893632 deprecation.py:323] From /usr/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:494: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\ntf.py_func is deprecated in TF V2. Instead, there are two\r\n    options available in V2.\r\n    - tf.py_function takes a python function which manipulates tf eager\r\n    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\r\n    an ndarray (just call tensor.numpy()) but having access to eager tensors\r\n    means `tf.py_function`s can use accelerators such as GPUs as well as\r\n    being differentiable using a gradient tape.\r\n    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\r\n    (it is not differentiable, and manipulates numpy arrays). It drops the\r\n    stateful argument making all functions stateful.\r\n\r\nI0716 09:30:58.706667 139693526079232 estimator.py:1145] Calling model_fn.\r\nI0716 09:30:58.707686 139693517686528 estimator.py:1145] Calling model_fn.\r\nW0716 09:30:58.710247 139693526079232 deprecation.py:323] From /usr/lib/python2.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:3038: _num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\r\nW0716 09:30:58.710444 139693526079232 deprecation.py:323] From /usr/lib/python2.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4459: _num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\r\nW0716 09:30:58.711355 139693517686528 deprecation.py:323] From /usr/lib/python2.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:3038: _num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\r\nW0716 09:30:58.711641 139693517686528 deprecation.py:323] From /usr/lib/python2.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4459: _num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\r\nW0716 09:30:58.954670 139693526079232 deprecation.py:323] From /usr/lib/python2.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2655: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nW0716 09:30:58.957056 139693517686528 deprecation.py:323] From /usr/lib/python2.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2655: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nW0716 09:30:59.147145 139693526079232 deprecation.py:323] From test1.py:116: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\r\nW0716 09:30:59.150608 139693517686528 deprecation.py:323] From test1.py:116: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\r\nW0716 09:30:59.151688 139693526079232 deprecation.py:323] From test1.py:122: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\r\nW0716 09:30:59.155345 139693517686528 deprecation.py:323] From test1.py:122: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\r\nW0716 09:30:59.201013 139693526079232 deprecation.py:506] From /usr/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:564: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nW0716 09:30:59.206078 139693517686528 deprecation.py:506] From /usr/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:564: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nW0716 09:30:59.220520 139693526079232 deprecation.py:506] From /usr/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:574: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nW0716 09:30:59.227919 139693517686528 deprecation.py:506] From /usr/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:574: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nI0716 09:31:00.381159 139695682893632 cross_device_ops.py:1032] Collective batch_all_reduce: 12 all-reduces, num_workers = 2\r\nI0716 09:31:00.381464 139695682893632 cross_device_ops.py:1053] Collective batch_all_reduce: 10 all-reduces, num_workers = 2\r\nI0716 09:31:00.391840 139695682893632 cross_device_ops.py:1085] Collective batch_all_reduce for IndexedSlices: 2 all-reduces, num_workers = 2\r\nI0716 09:31:00.397902 139695682893632 cross_device_ops.py:1032] Collective batch_all_reduce: 12 all-reduces, num_workers = 2\r\nI0716 09:31:00.398242 139695682893632 cross_device_ops.py:1053] Collective batch_all_reduce: 10 all-reduces, num_workers = 2\r\nI0716 09:31:00.408795 139695682893632 cross_device_ops.py:1085] Collective batch_all_reduce for IndexedSlices: 2 all-reduces, num_workers = 2\r\nI0716 09:31:00.647948 139693526079232 estimator.py:1147] Done calling model_fn.\r\nI0716 09:31:00.648964 139695682893632 cross_device_ops.py:1032] Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\nI0716 09:31:00.649120 139695682893632 cross_device_ops.py:1053] Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\nI0716 09:31:00.686273 139693517686528 estimator.py:1147] Done calling model_fn.\r\nI0716 09:31:00.687391 139695682893632 cross_device_ops.py:1032] Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\nI0716 09:31:00.687582 139695682893632 cross_device_ops.py:1053] Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\nI0716 09:31:00.764759 139695682893632 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\r\nI0716 09:31:00.764977 139695682893632 monitored_session.py:408] all_hooks [<tensorflow_estimator.python.estimator.util.DistributedIteratorInitializerHook object at 0x7543fd0>, <tensorflow.python.training.basic_session_run_hooks.StopAtStepHook object at 0x7543990>, <tensorflow.python.training.basic_session_run_hooks.NanTensorHook object at 0x75307d0>, <tensorflow.python.training.basic_session_run_hooks.LoggingTensorHook object at 0x7f0cf8a518d0>, <tensorflow.python.training.basic_session_run_hooks.StepCounterHook object at 0x14dd7d0>, <tensorflow.python.training.basic_session_run_hooks.SummarySaverHook object at 0x7546210>, <tensorflow.python.training.basic_session_run_hooks.CheckpointSaverHook object at 0x2ee0750>]\r\nI0716 09:31:00.765131 139695682893632 distribute_coordinator.py:251] Creating chief session creator with config: device_filters: \"/job:worker/task:0\"\r\nallow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n    scoped_allocator_optimization: ON\r\n    scoped_allocator_opts {\r\n      enable_op: \"CollectiveReduce\"\r\n    }\r\n  }\r\n}\r\nexperimental {\r\n  collective_group_leader: \"/job:worker/replica:0/task:0\"\r\n}\r\n\r\nI0716 09:31:01.497885 139695682893632 monitored_session.py:240] Graph was finalized.\r\n2019-07-16 09:31:01.659481: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\nI0716 09:31:01.729172 139695682893632 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\r\nI0716 09:31:01.729402 139695682893632 monitored_session.py:408] all_hooks [<tensorflow_estimator.python.estimator.util.DistributedIteratorInitializerHook object at 0x7f0d6f730590>, <tensorflow.python.training.basic_session_run_hooks.StopAtStepHook object at 0x7543a10>, <tensorflow.python.training.basic_session_run_hooks.NanTensorHook object at 0x7530790>, <tensorflow.python.training.basic_session_run_hooks.LoggingTensorHook object at 0x7f0d6f730c50>, <tensorflow.python.training.basic_session_run_hooks.StepCounterHook object at 0x14dd790>, <tensorflow.python.training.basic_session_run_hooks.SummarySaverHook object at 0x7546310>, <tensorflow.python.training.basic_session_run_hooks.CheckpointSaverHook object at 0x75462d0>]\r\nI0716 09:31:01.729557 139695682893632 distribute_coordinator.py:251] Creating chief session creator with config: device_filters: \"/job:worker/task:1\"\r\nallow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n    scoped_allocator_optimization: ON\r\n    scoped_allocator_opts {\r\n      enable_op: \"CollectiveReduce\"\r\n    }\r\n  }\r\n}\r\nexperimental {\r\n  collective_group_leader: \"/job:worker/replica:0/task:0\"\r\n}\r\n\r\nI0716 09:31:02.443571 139695682893632 monitored_session.py:240] Graph was finalized.\r\n2019-07-16 09:31:02.568868: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\nI0716 09:31:02.710848 139695682893632 session_manager.py:500] Running local_init_op.\r\nI0716 09:31:02.714972 139695682893632 session_manager.py:500] Running local_init_op.\r\nI0716 09:31:02.749226 139695682893632 session_manager.py:502] Done running local_init_op.\r\nI0716 09:31:02.752799 139695682893632 session_manager.py:502] Done running local_init_op.\r\nI0716 09:31:04.219505 139695682893632 basic_session_run_hooks.py:606] Saving checkpoints for 0 into /root/test/tf2/model/tmp_worker_1/model.ckpt.\r\nI0716 09:31:04.228302 139695682893632 basic_session_run_hooks.py:606] Saving checkpoints for 0 into /root/test/tf2/model/model.ckpt.\r\nI0716 09:31:05.196074 139695682893632 basic_session_run_hooks.py:262] loss = 0.69606435, step = 0\r\nI0716 09:31:05.197033 139695682893632 basic_session_run_hooks.py:262] loss = 0.69606435, step = 0\r\n2019-07-16 09:31:05.725766: F tensorflow/core/framework/op_kernel.cc:1509] Check failed: nullptr == ctx->op_kernel().AsAsync() (nullptr vs. 0x7f0d043cecd0)Use OP_REQUIRES_ASYNC in AsyncOpKernel implementations.\r\n2019-07-16 09:31:05.812505: F tensorflow/core/framework/op_kernel.cc:1509] Check failed: nullptr == ctx->op_kernel().AsAsync() (nullptr vs. 0x7f0d204a03d0)Use OP_REQUIRES_ASYNC in AsyncOpKernel implementations.\r\nTrain end.", "Thanks, this should be fixed after 1c68152470836be8f3e4492c8cc909124f57b9d9.", "@dubey I see. Thank you."]}, {"number": 30283, "title": "[TF 2.0] Minor changes to the 'Load images with tf.data' tutorial + fix image links", "body": "_URL(s) with the issue:_\r\nhttps://www.tensorflow.org/beta/tutorials/load_data/images\r\n\r\n_Description of main issue:_\r\nPls take a look at JPG/JPEG links in the 'Load images with tf.data' tutorial in the 'Inspect the images' section. When you inspect the site elements they appear to be missing a letter 'g' as in `jpeg` or `jpg`\r\n```\r\nhttps://www.tensorflow.org/beta/tutorials/load_data/images_files/output_16_0.jpe\r\nhttps://www.tensorflow.org/beta/tutorials/load_data/images_files/output_16_2.jpe\r\nhttps://www.tensorflow.org/beta/tutorials/load_data/images_files/output_16_4.jpe\r\n```\r\n\r\n_Other minor suggestions:_\r\nRearrangement of words, fixing some grammar, addition/subtraction of commas/colons/spaces in English and Python for consistency - will submit a PR right away for your review @MarkDaoust @lamberta ", "comments": ["Thank you. We recently hit this filename issue ... something about nb_convert and mime types. @yashk2810 is looking in to it.", "@MarkDaoust has a fix on the way. This should be fixed soon.", "Thanks for the report.\r\n\r\nYes mimetypes\r\n\r\nhttps://github.com/jupyter/nbconvert/pull/175\r\nhttps://www.onebigfluke.com/2014/02/crazy-python-mimetypes-module.html\r\n\r\nThe fix is in, and the site-updater is running now. This will be resolved by EOD.\r\n"]}, {"number": 30282, "title": "ERROR: no such package 'tensorflow/lite/toco': Unable to determine the local repository for directory /home/devim/tensorflow/tensorflow/lite/toco", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version: 1.12.0\r\n- Python version: 2.7.15\r\n- Installed using virtualenv? pip? conda?: python-pip\r\n- Bazel version (if compiling from source): 0.26.0\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: Using CPU\r\n- GPU model and memory: N/A\r\n\r\nCode used:\r\n\r\n`bazel run -c opt tensorflow/lite/toco:toco -- --input_file=models/research/object_detection/tflite_graph.pb --output_file=models/research/object_detection/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_values=128 --change_concat_input_ranges=false --allow_custom_ops`\r\n\r\nI got this error\r\n\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Reading rc options for 'run' from /home/devim/tensorflow/.bazelrc:\r\n  Inherited 'build' options: --apple_platform_type=macos --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include\r\nINFO: Reading rc options for 'run' from /home/devim/tensorflow/.tf_configure.bazelrc:\r\n  Inherited 'build' options: --host_force_python=PY2 --action_env PYTHON_BIN_PATH=/home/devim/venv/bin/python --action_env PYTHON_LIB_PATH=/home/devim/venv/lib/python2.7/site-packages --python_path=/home/devim/venv/bin/python --action_env TF_CONFIGURE_IOS=0\r\nERROR: Skipping 'tensorflow/lite/toco:toco': no such package 'tensorflow/lite/toco': Unable to determine the local repository for directory /home/devim/tensorflow/tensorflow/lite/toco\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package 'tensorflow/lite/toco': Unable to determine the local repository for directory /home/devim/tensorflow/tensorflow/lite/toco\r\nINFO: Elapsed time: 0.092s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n\r\nThe directory tensorflow/lite/toco exists and here is the screenshot.\r\n![Capture](https://user-images.githubusercontent.com/48072621/60449936-b7d59a00-9bf6-11e9-8a12-b5318d93bef3.JPG)\r\n\r\n", "comments": ["@aselle @achandraa @jvishnuvardhan can you please look into this ?", "Toco converter is deprecated with TF 1.12. Can you please try [```tflite_convert```](https://www.tensorflow.org/lite/convert/cmdline_examples#command-line_tools_) instead?\r\nInstalling tf-nightly is recommended for using TensorFlow Lite Converter.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30282\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30282\">No</a>\n"]}, {"number": 30281, "title": "tf.read_file memory leak on python3.7", "body": "Hi,\r\ni noticed a memory leak in python3.7. When I used the dataset.map() function to load images, the process died after a few seconds with an OOM exception. After some testing, I could identify tf.read_file() as the source of this issue. Memory was never released, so all files accumulated to multiple GB of memory in seconds and the process crashes.\r\n```python\r\ndef load_img(filename: str, label):\r\n    image_byte = tf.read_file(filename)\r\n    ...\r\n    return processed_image, label\r\n\r\ndef dummy():\r\n     ...\r\n    load_fn = lambda f, l: load_img(f, l)\r\n    ...\r\n    dataset = tf.data.Dataset.from_tensor_slices(...)\r\n                ...\r\n                .map(load_fn, num_parallel_calls=4)\r\n                ...\r\n                \r\n\r\n```\r\nGoing back to python3.6, there was no memory leak. An attempt to update to TF1.14.0 on pyhton3.7 failed so I could not verify, if the issue is fixed.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): pip3\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory:\r\n", "comments": ["In order to expedite the trouble-shooting process, please provide a full code snippet to reproduce the issue reported here. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 30280, "title": "Is there any way to flush GPU memory in TF 2.0?", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): \r\n\r\n`\r\nBATCH_SIZE = 64\r\nfor j, (train_indices, val_indices) in enumerate(kf.split(datos_random)):\r\n    fold_dir = f'fold_{j}'\r\n    if not os.path.exists(fold_dir):\r\n        os.mkdir(fold_dir)\r\n    train = datos.iloc[train_indices]\r\n    val = datos.iloc[val_indices]   \r\n    metrics = ['accuracy']\r\n    model = build_model(DROPOUT, FC_LAYERS, num_classes=NUM_CLASSES, opt=OPT, metrics=metrics)\r\n\r\n    train_datagen =  keras.preprocessing.image.ImageDataGenerator(\r\n        preprocessing_function=preprocess_input,\r\n        horizontal_flip=True,\r\n        vertical_flip=True,\r\n        fill_mode='constant'\r\n        \r\n    )\r\n\r\n    val_datagen = keras.preprocessing.image.ImageDataGenerator(\r\n        preprocessing_function=preprocess_input,\r\n    )\r\n    \r\n    train_generator = train_datagen.flow_from_dataframe(\r\n                                          train,\r\n                                          None,\r\n                                          x_col='file',\r\n                                          target_size=(WIDTH, HEIGHT),\r\n                                          y_col=f'Class_cat_{NUM_CLASSES}', \r\n                                          batch_size=BATCH_SIZE, \r\n                                          seed=SEED,\r\n                                          has_ext=True,class_mode='categorical')\r\n\r\n    validation_generator = val_datagen.flow_from_dataframe(val, \r\n                                          None, \r\n                                          x_col='file',\r\n                                          target_size=(WIDTH, HEIGHT),\r\n                                          y_col=f'Class_cat_{NUM_CLASSES}', \r\n                                          batch_size=BATCH_SIZE,\r\n                                          seed=SEED,\r\n                                          has_ext=True,class_mode='categorical')\r\n\r\n    class_list = list(test_generator.class_indices.keys())\r\n    \r\n    history = model.fit_generator(train_generator, \r\n                        epochs=EPOCHS, \r\n                        workers=16, \r\n                        shuffle=True,  \r\n                        verbose=1, \r\n                        steps_per_epoch=len(train_indices) / BATCH_SIZE,\r\n                        validation_data=validation_generator, \r\n                        validation_steps=len(val_indices) / BATCH_SIZE\r\n`\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.0-9492-g2c319fb415 2.0.0-alpha0\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Titan V, Driver 418.67\r\n\r\n**Describe the current behavior**\r\nModel trains good for the first 3-5 folds but it eventually returns a out of memory error.\r\nI've tried deleting the model the end of each iteration also using  `keras.backend.clear_session()` and allow_growth with no avail. I'm also using my second GPU not the Titan V used for display, its alos weird that the error occurs in GPU:0 but I'm using GPU 1 ` os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\";`\r\n\r\n**Describe the expected behavior**\r\nNeed to flush GPU data at each iteration so it won't run out of memory\r\n\r\n**Other info / logs**\r\n`---------------------------------------------------------------------------\r\nResourceExhaustedError                    Traceback (most recent call last)\r\n<timed exec> in <module>\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\r\n   1513         shuffle=shuffle,\r\n   1514         initial_epoch=initial_epoch,\r\n-> 1515         steps_name='steps_per_epoch')\r\n   1516 \r\n   1517   def evaluate_generator(self,\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\r\n    255 \r\n    256       is_deferred = not model._is_compiled\r\n--> 257       batch_outs = batch_function(*batch_data)\r\n    258       if not isinstance(batch_outs, list):\r\n    259         batch_outs = [batch_outs]\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)\r\n   1257       else:\r\n   1258         self._make_fit_function()\r\n-> 1259         outputs = self._fit_function(ins)  # pylint: disable=not-callable\r\n   1260 \r\n   1261     if reset_metrics:\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/backend.py in __call__(self, inputs)\r\n   3215         value = math_ops.cast(value, tensor.dtype)\r\n   3216       converted_inputs.append(value)\r\n-> 3217     outputs = self._graph_fn(*converted_inputs)\r\n   3218     return nest.pack_sequence_as(self._outputs_structure,\r\n   3219                                  [x.numpy() for x in outputs])\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n    556       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\r\n    557           list(kwargs.keys()), list(self._arg_keywords)))\r\n--> 558     return self._call_flat(args)\r\n    559 \r\n    560   def _filtered_call(self, args, kwargs):\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args)\r\n    625     # Only need to override the gradient in graph mode and when we have outputs.\r\n    626     if context.executing_eagerly() or not self.outputs:\r\n--> 627       outputs = self._inference_function.call(ctx, args)\r\n    628     else:\r\n    629       self._register_gradient()\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args)\r\n    413             attrs=(\"executor_type\", executor_type,\r\n    414                    \"config_proto\", config),\r\n--> 415             ctx=ctx)\r\n    416       # Replace empty list with None\r\n    417       outputs = outputs or None\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     64     else:\r\n     65       message = e.message\r\n---> 66     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     67   except TypeError as e:\r\n     68     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/six.py in raise_from(value, from_value)\r\n\r\nResourceExhaustedError: OOM when allocating tensor with shape[64,64,256,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[{{node block1_conv2_4/Conv2D}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\t [[loss_4/dense_2_loss/categorical_crossentropy/weighted_loss/broadcast_weights/assert_broadcastable/is_valid_shape/else/_45/has_valid_nonscalar_shape/then/_141/has_invalid_dims_0/_44]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n [Op:__inference_keras_scratch_graph_2925097]`", "comments": ["@marcojulioarg Will it be possible to provide us the complete minimal code that replicates the issue. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 30279, "title": "Fix macos makefile build", "body": "- Adding `CoreFoundation` to `HOST_LDFLAGS` is necessary as the library target uses that variable\r\n- Adding `CoreFoundation` to `LIBS` is necessary as the benchmark target uses that variable", "comments": ["This PR is not ready; with the current state, I get weird errors such as \r\n```\r\n...\r\n/Users/rasmus/Downloads/tensorflow/tensorflow/contrib/makefile/gen/protobuf-host/bin/protoc  tensorflow/core/util/memmapped_file_system.proto --cpp_out /Users/rasmus/Downloads/tensorflow/tensorflow/contrib/makefile/gen/host_obj/\r\ntensorflow/tools/git/gen_git_source.sh tensorflow/core/util/version_info.cc\r\nifeq (x86_64,x86_64)\r\nifeq (x86_64,x86_64)\r\nifeq (x86_64,x86_64)\r\nifeq (x86_64,x86_64)\r\nifeq (x86_64,x86_64)\r\nifeq (x86_64,x86_64)\r\nifeq (x86_64,x86_64)\r\nifeq (x86_64,x86_64)\r\n/bin/bash: -c: line 0: syntax error near unexpected token `x86_64,x86_64'\r\n/bin/bash: -c: line 0: `ifeq (x86_64,x86_64)'\r\n/bin/bash: -c: line 0: syntax error near unexpected token `x86_64,x86_64'\r\n/bin/bash: -c: line 0: `ifeq (x86_64,x86_64)'\r\nifeq (x86_64,x86_64)\r\n/bin/bash: -c: line 0: syntax error near unexpected token `x86_64,x86_64'\r\n/bin/bash: -c: line 0: `ifeq (x86_64,x86_64)'\r\nmake: *** [tensorflow/contrib/makefile/Makefile:914: /Users/rasmus/Downloads/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/contrib/makefile/downloads/absl/absl/strings/substitute.o] Error 2\r\nmake: *** Waiting for unfinished jobs....\r\nmake: *** [tensorflow/contrib/makefile/Makefile:914: /Users/rasmus/Downloads/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/contrib/makefile/downloads/absl/absl/strings/str_split.o] Error 2\r\nmake: *** [tensorflow/contrib/makefile/Makefile:914: /Users/rasmus/Downloads/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/contrib/makefile/downloads/absl/absl/strings/charconv.o] Error 2\r\n/bin/bash: -c: line 0: syntax error near unexpected token `x86_64,x86_64'\r\n/bin/bash: -c: line 0: `ifeq (x86_64,x86_64)'\r\nmake: *** [tensorflow/contrib/makefile/Makefile:914: /Users/rasmus/Downloads/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/contrib/makefile/downloads/absl/absl/strings/numbers.o] Error 2\r\n/bin/bash: -c: line 0: syntax error near unexpected token `x86_64,x86_64'\r\n/bin/bash: -c: line 0: `ifeq (x86_64,x86_64)'\r\nmake: *** [tensorflow/contrib/makefile/Makefile:914: /Users/rasmus/Downloads/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/contrib/makefile/downloads/absl/absl/strings/string_view.o] Error 2\r\n/bin/bash: -c: line 0: syntax error near unexpected token `x86_64,x86_64'\r\n/bin/bash: -c: line 0: `ifeq (x86_64,x86_64)'\r\n```\r\n\r\nwhen running `build_all_linux.sh`, as if bash were somehow trying to parse the makefile. Anyone got an idea where this can come from? It seems to work only if I remove the indent from the nested `ifeq`s altogether.", "Apparently, make treats tab-indented lines as shell commands as part of a rule (see [answer here)](https://stackoverflow.com/a/4483467/2397253), so I'll push a version which does not indent with tabs, but consistently with spaces, same as the block for iOS. Testing now."]}, {"number": 30278, "title": "Fix erosion rate parameter name", "body": "More consistent variable name for erosion rate (was \"dilations\", changed to \"erosions\")", "comments": []}, {"number": 30277, "title": "Correct Tensor order for dilation2D", "body": "`gen_nn_ops.dilation2d` seems to be in `NHWC` while the parent function was asking for `NCHW`.\r\nI corrected the doc and the check.\r\n\r\nRELNOTES: tf.nn.dilation2d now correctly requires its data_format argument to be \"NHWC\".", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30277) for more info**.\n\n<!-- need_sender_cla -->", "I signed it...", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30277) for more info**.\n\n<!-- ok -->", "Thank you for the change, I think it's correct but let me follow up with more knowledgeable folks in the TF team before I approve it.", "@gabriel-vanzandycke can you also correct the converter tool to add the data_format arg? It should be parallel to the entry for erosion2d: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/compatibility/tf_upgrade_v2.py#L1525\r\n\r\n@tomerk The converter didn't do the right thing here, can you review the change to make sure it will once @gabriel-vanzandycke is done?\r\n\r\n@tensorflow/api-owners This is a bug affecting the API: the accepted string has changed. The old state is a bug, we need to fix it. I added relnotes."]}, {"number": 30275, "title": "why official tensorflow lite android demo faster than building library by myself", "body": "I use the official android demo [here](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android),and run the model I created by myself under device rk3288, the inference cost is about 200ms, but when I build `libtensorflowlite.so` by myself using the command below, it cost almost 500ms, I can't understand the gap. **Does any one know how to build tensorflow-lite c++  library by ourself, and get the same level of performance by official?**\r\n\r\nThe build command is here:\r\n\r\n`bazel build -c opt --crosstool_top=//external:android/crosstool  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=\"-std=c++11\" --fat_apk_cpu=armeabi-v7a  --config=android_arm //tensorflow/lite:libtensorflowlite.so` ", "comments": ["Try using:\r\n\r\n`--config=android_arm64` instead of `--fat_apk_cpu=armeabi-v7a --config=android_arm`", "> Try using:\r\n> \r\n> `--config=android_arm64` instead of `--fat_apk_cpu=armeabi-v7a --config=android_arm`\r\n\r\nUsing arm64 cup architecture, it will build with ruy, and I can get the same inference time cost. But the cpu architecture of rk3288 is 32 bit, only support armeabi-v7a. I test inference cost between official android demo and project created by myself under rk3288 of armeabi-v7a, the time cost vary greatly, so I wander if I missed some building definitions ", "I am using the same so, and write test code all by c++ native, then running under rk3288, it costs 100ms, so I think it maybe something wrong when embedding into android project", "I think I found the solution. When I open camera, the inference time cost suddenly increase.", "> I am using the same so, and write test code all by c++ native, then running under rk3288, it costs 100ms, so I think it maybe something wrong when embedding into android project\r\n\r\nhi,I have read your question and i need your help, would like to ask you some questions? I used your build command to build libtensorflowlite.so, but an error occurred :\r\n\r\n![image](https://user-images.githubusercontent.com/17593272/61166246-69f95400-a55d-11e9-91ce-217a843ecf8c.png)\r\n\r\nMy guess is that the relevant environment versions don't match, my tensorflow is release-1.14.0 and bazel-0.18.0 .Can you give me some advice please?", "You should configure tensorflow project before running bazel command"]}, {"number": 30274, "title": "TFSlim trained 'mobilenetv2' model is giving poor accuracy with is_training='False' (When save and restore the checkpoints)", "body": "## The steps I have followed\r\n### 1. I have trained the `MobilenetV2` model using `slim` framework with cifar10 dataset.\r\n#### Run training.\r\npython train_image_classifier.py \\\r\n  --train_dir=${TRAIN_DIR} \\\r\n  --dataset_name=cifar10 \\\r\n  --dataset_split_name=train \\\r\n  --dataset_dir=${DATASET_DIR} \\\r\n  --model_name=mobilenet_v2 \\\r\n  --preprocessing_name=mobilenet_v2 \\\r\n  --max_number_of_steps=100000 \\\r\n  --batch_size=48 \\\r\n  --save_interval_secs=120 \\\r\n  --save_summaries_secs=120 \\\r\n  --log_every_n_steps=100 \\\r\n  --optimizer=adagrad \\\r\n  --learning_rate=0.1 \\\r\n  --learning_rate_decay_factor=0.1 \\\r\n  --num_epochs_per_decay=200 \\\r\n  --weight_decay=0.004 \\\r\n  --moving_average_decay=0.9999\r\n\r\n### 2. After training, I could get a proper accuracy with below evaluation options:\r\n#### Run evaluation.\r\npython eval_image_classifier.py \\\r\n  --checkpoint_path=${TRAIN_DIR} \\\r\n  --eval_dir=${TRAIN_DIR} \\\r\n  --dataset_name=cifar10 \\\r\n  --dataset_split_name=test \\\r\n  --dataset_dir=${DATASET_DIR} \\\r\n  --model_name=mobilenet_v2\r\n\r\n### 3. I have saved the checkpoints with `is_training=False` from the `eval_image_classifier.py` itself. Converted the checkpoints to .pb file and then measured the evaluation accuracy. The accuracy is very less.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 18.04\r\n- TensorFlow installed from  binary\r\n- TensorFlow version (use command below):  1.13\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source): 24.1\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nThe modem accuracy changed after saving the evaluation graph with `'is_training=False'`\r\n\r\n**Describe the expected behavior**\r\nThe model should give same accuracy after saving with `'is_training=False'`\r\n\r\n**Code to reproduce the issue**\r\nTraining with slim framework\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Given `--max_number_of_steps=100000` you should probably lower the `--moving_average_decay=0.9999` to `--moving_average_decay=0.99` to make sure the moving average are updated.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30274\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30274\">No</a>\n"]}]