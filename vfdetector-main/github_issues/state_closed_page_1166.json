[{"number": 18220, "title": "Fix linux cmake compilation error caused by recent grpc update", "body": "compilation error:\r\n13:25:10 grpc/src/grpc/libgrpc_unsecure.a(dns_resolver_ares.cc.o): In function `grpc_resolver_dns_ares_init()':\r\n13:25:10 dns_resolver_ares.cc:(.text+0x11c1): undefined reference to `address_sorting_init'\r\n13:25:10 grpc/src/grpc/libgrpc_unsecure.a(dns_resolver_ares.cc.o): In function `grpc_resolver_dns_ares_shutdown()':\r\n13:25:10 dns_resolver_ares.cc:(.text+0x12a9): undefined reference to `address_sorting_shutdown'\r\n13:25:10 grpc/src/grpc/libgrpc_unsecure.a(grpc_ares_wrapper.cc.o): In function `grpc_cares_wrapper_address_sorting_sort(grpc_lb_addresses*)':\r\n13:25:10 grpc_ares_wrapper.cc:(.text+0x28d): undefined reference to `address_sorting_rfc_6724_sort'\r\n13:25:10 collect2: error: ld returned 1 exit status\r\n13:25:10 CMakeFiles/benchmark_model.dir/build.make:2069: recipe for target 'benchmark_model' failed\r\n13:25:10 make[2]: *** [benchmark_model] Error 1\r\n13:25:10 CMakeFiles/Makefile2:8340: recipe for target 'CMakeFiles/benchmark_model.dir/all' failed\r\n13:25:10 make[1]: *** [CMakeFiles/benchmark_model.dir/all] Error 2", "comments": ["some one already fixed. Closing...."]}, {"number": 18218, "title": "Feature Request: Named global_step", "body": "I have been developing GAN models for some time now and I find it useful to track the individual stages (pretraining of generator/discriminator, adversarial training) in different global_steps so I can save and restore my actual progress without doing a lot of calculations based on configured training steps and so on.\r\n\r\nTherefore, I have basically rebuild the functionality that [tf.train.get_or_create_global_step](https://www.tensorflow.org/versions/master/api_docs/python/tf/train/get_or_create_global_step) provides but with support for named global_steps.\r\n\r\nLooks something like `tf.train.get_or_create_global_step(graph, name)`.\r\n\r\nAm I the only one finding this particularly helpful?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I'm not sure if it would be appropriate to have something like this in TensorFlow core. I'm not sure if we'd have the resources to maintain something like that. There is already a lot of technical debt and complication surrounding the global step, which is a key concept in the many summary logging systems that exist. You might be able to reach out to framework authors like Keras and Estimator to see if they'd be interested. I'm going to close out this issue, but will happily reopen it, provided more community support."]}, {"number": 18217, "title": "TensorRT create exception when engine is created from sub-process", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nTensorRT works well when called in a single process however it breaks when trt.utils.uff_to_trt_engine () is called from sub-process. \r\n\r\n\r\n### Source code / logs\r\nHere is the summary of error and how to reproduce it.\r\n\r\nWorking TensorRT example Python code with a single process :\r\n\r\nimport pycuda.driver as cuda\r\nimport pycuda.autoinit\r\nimport argparse\r\nimport numpy as np\r\nimport time\r\nimport tensorrt as trt\r\nfrom tensorrt.parsers import uffparser\r\n\r\nuff_model = open('resnet_v2_50_dc.uff', 'rb').read()\r\n\r\nparser = uffparser.create_uff_parser()\r\nparser.register_input(\"input\", (3, 224, 224), 0)\r\nparser.register_output(\"resnet_v2_50/predictions/Reshape_1\")\r\n\r\n\r\ntrt_logger = trt.infer.ConsoleLogger(trt.infer.LogSeverity.INFO)\r\n\r\nengine = trt.utils.uff_to_trt_engine(logger=trt_logger,\r\n                                     stream=uff_model,\r\n                                     parser=parser,\r\n                                     max_batch_size=4,\r\n                                     max_workspace_size= 1 << 30,\r\n                                     datatype=trt.infer.DataType.FLOAT)\r\n\r\n\r\nNon-working TensorRT  example Python code  where  trt.utils.uff_to_trt_engine () is called from sub-process :\r\n\r\nimport pycuda.driver as cuda\r\nimport pycuda.autoinit\r\nimport argparse\r\nimport numpy as np\r\nimport time\r\nimport tensorrt as trt\r\nfrom tensorrt.parsers import uffparser\r\nimport multiprocessing\r\nfrom multiprocessing import sharedctypes, Queue\r\n\r\ndef inference_process():\r\n    uff_model = open('resnet_v2_50_dc.uff', 'rb').read()\r\n\r\n    parser = uffparser.create_uff_parser()\r\n    parser.register_input(\"input\", (3, 224, 224), 0)\r\n    parser.register_output(\"resnet_v2_50/predictions/Reshape_1\")\r\n\r\n    trt_logger = trt.infer.ConsoleLogger(trt.infer.LogSeverity.INFO)\r\n    engine = trt.utils.uff_to_trt_engine(logger=trt_logger,\r\n                                         stream=uff_model,\r\n                                         parser=parser,\r\n                                         max_batch_size=4,\r\n                                         max_workspace_size= 1 << 30,\r\n                                         datatype=trt.infer.DataType.FLOAT)\r\n\r\ninference_p = multiprocessing.Process(target=inference_process, args=( ))\r\ninference_p.start()\r\n\r\n\r\nConsole Error Message :\r\n[TensorRT] ERROR: cudnnLayerUtils.cpp (288) - Cuda Error in smVersion: 3\r\nterminate called after throwing an instance of 'nvinfer1::CudaError'\r\n  what():  std::exception\r\n\r\n", "comments": ["Sorry for the delay. I believe you should file this against TensorRT, as this doesn't appear to be a TensorFlow issue. Please let me know if I'm misunderstanding."]}, {"number": 18216, "title": " tf.space_to_depth alters input type if its quantized.  But why?", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  stock from the tf.space_to_depth() example as given on tensorflow.org, with output directed to tf.quantize()\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**:\r\n>>> tf.GIT_VERSION\r\n\"b'unknown'\"\r\n>>> tf.VERSION\r\n'1.6.0'\r\n- **Python version**: , Python 3.5.4 Tk 8.6.4 \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: dst = tf.dequantize(qstd,qmin,qmax)  see the script below please.\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\ntf.depth_to_space() doesnt return the input dtype when it is dtype=tf.qint8.  The type is changed to tf.int8.\r\n\r\n### Source code / logs\r\n>>> \r\nSource:\r\n\r\n#\r\nimport tensorflow as tf\r\nsess = tf.Session()\r\n\r\ndt=tf.float32\r\nbox=2\r\nslist = [(1,2,2,1),(1,2,2,3),(1,4,4,1),(2,2,4,1,)]\r\ndlist = [(1,1,1,4),(1,1,1,12),(1,2,2,4)]\r\nsshape=2\r\n\r\nspace_t=tf.random_uniform(slist[sshape],minval=-10, maxval=10,dtype=tf.float32)\r\n\r\nprint(sess.run(space_t))\r\n\r\nftin = tf.reshape(space_t,[-1])\r\nmax=ftin[tf.argmax(ftin)]\r\nmin=ftin[tf.argmin(ftin)]\r\n\r\naq_op = tf.quantize(space_t,min,max,tf.qint8,name=\"TFQuantize\")\r\naq_space_t = sess.run(aq_op)\r\n\r\n# The following qstd op should yield a type the same as aq_space_t.output, but does not, according to documentation.\r\nqstd = tf.space_to_depth(aq_space_t.output, box,name = 'TestOut')\r\nout1 = sess.run(qstd)\r\nqftin = tf.reshape(out1,[-1])\r\nqmax=ftin[tf.argmax(qftin)]\r\nqmin=ftin[tf.argmin(qftin)]\r\n\r\ndst = tf.dequantize(qstd,qmin,qmax)   ### This is the failing command\r\nout2 = sess.run(dqstd)\r\n\r\n# The NON quantized original space_to_depth \r\nstd = tf.space_to_depth(space_t, box,name = 'TestOut')\r\n\r\n>>> LOG:\r\n== RESTART: C:/Users/c_jrost/projects/tensorTest/py_tests/autoQuantize3.py ==\r\n[[[[-6.970806 ]\r\n   [-8.151844 ]\r\n   [ 1.0674906]\r\n   [-2.021196 ]]\r\n\r\n  [[-6.90624  ]\r\n   [ 7.7964478]\r\n   [ 8.286434 ]\r\n   [ 5.0095863]]\r\n\r\n  [[-0.5847721]\r\n   [ 7.9649334]\r\n   [-1.1554356]\r\n   [ 8.961611 ]]\r\n\r\n  [[ 1.2922192]\r\n   [-5.9919786]\r\n   [-5.741205 ]\r\n   [ 5.86102  ]]]]\r\nTraceback (most recent call last):\r\n  File \"C:/Users/c_jrost/projects/tensorTest/py_tests/autoQuantize3.py\", line 33, in <module>\r\n    dst = tf.dequantize(qstd,qmin,qmax)\r\n  File \"c:\\Users\\c_jrost\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 1319, in dequantize\r\n    mode=mode, name=name)\r\n  File \"c:\\Users\\c_jrost\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 609, in _apply_op_helper\r\n    param_name=input_name)\r\n  File \"c:\\Users\\c_jrost\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 60, in _SatisfiesTypeConstraint\r\n    \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\r\nTypeError: Value passed to parameter 'input' has DataType int8 not in list of allowed values: qint8, quint8, qint32, qint16, quint16\r\n\r\n", "comments": ["Not sure who is best to handle quantization issues. /CC @aselle, can you take a look?", "The point of my inquiry is because I want to use certain tf ops that pehaps only transpose the tensor shape and not necessarily operate on its data mathematically.  DepthSpace and BatchSpace are two such ops that supposedly return the input tensor dtype.  \r\n\r\nSince quatization is not specifically supported for most/any commands currently it would help if those commands that don't operate mathematically on the data would just pass the input tensor type to the output unaltered.  That is what the documentation says in the case of DepthSpace and BatchSpace.\r\n\r\nThanks in advance,", "Suharsh, please take a look.\r\n", "I believe the issue is that aq_space_t.output is a numpy type (since you call sess.run) that you are providing to the space_to_batch operation. The numpy type is just a int8 since qint8 is just a TF type (that is just a cosmetic thing for int8).\r\n\r\nIf you change:\r\nqstd = tf.space_to_depth(aq_space_t.output, box,name = 'TestOut')\r\nto:\r\nqstd = tf.space_to_depth(ac_op, box,name = 'TestOut')\r\n\r\nthings should work.\r\n\r\nPlease reopen if that doesn't fix this."]}, {"number": 18215, "title": "Add abseil-cpp dependency for cmake to fix windows build failure", "body": "Creating a tentative pull request to test this on a windows machine", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "abseil-cpp has a dependency on cctz, so the build still fails with the error cctz not found.\r\n\r\n    CMake Error at CMake/AbseilHelpers.cmake:153 (message):\r\n    ABSL: compiling absl requires a cctz CMake target in your project,\r\n                    see CMake/README.md for more details\r\n\r\nI tried to add ``cctz`` package in ``CMakeLists.txt`` as another external project. Though ``cctz`` compiled successfully, ``find_package`` in ``abseil-cpp`` still fails to find ``cctz`` package with the error above.\r\n", "Thanks for looking into this. yes, you are right about the cctz package dependency. I also ran into the same issue but found that abseil-cpp does not use find_package making it unfeasible to use ExternalProject in CMake. See discussion in https://github.com/abseil/abseil-cpp/issues/80\r\n\r\nI will close this PR and will wait for that issue to get resolved. For now, I have removed the dependency on absl in this commit. https://github.com/tensorflow/tensorflow/commit/e7ad6ec4267f1f79ee7d9f558c8a008746682959."]}, {"number": 18214, "title": "Documentation missing for tensorrt in Python API", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A\r\n- **TensorFlow installed from (source or binary)**: N/A\r\n- **TensorFlow version (use command below)**: 1.7\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\n Native TensorRT support in TensorFlow was announced by both [Google ](https://developers.googleblog.com/2018/03/tensorrt-integration-with-tensorflow.html) and Nvidia blog posts last week. But `tf.contrib.tensorrt` is missing from the Python API list. Currently, the only documentation is on readme file in the  [tensorflow/contrib/tensorrt](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/tensorrt) repository. But it doesn't given details about using the API.  Please include documentation for this important feature.", "comments": ["Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@wolffg, please take a look.", "There's an example pointed to by the README. I think that's the intended usage pattern. We can certainly export functionality under `tf.contrib.tensorrt` if that's useful.\r\n\r\n@samikama WDYT?", "AFAIK at present users need to export graphdef and rewrite it using a python api in ```trt_convert.py```, we are working on APIs to remove the rewrite process. But we probably need to document the current approach in the README.\r\n\r\n@aaroey", "@sibyjackgrove Do you mean updating documentation in tensorflow.org or in the README? if it is the first one, as @protoget mentioned, we are working heavily on the API and will probably simplify things thus documentation will be incorrect quite rapidly. If it is the second one I will try to update it in master but the test_tftrt.py should show some examples on how to use the api. Also python help should explain the parameters of the api.\r\n```python\r\nimport tensorflow.contrib.tensorrt as trt\r\nhelp(trt.trt_convert)\r\n```\r\nNevertheless you are right and we will try to update it soon.\r\n\r\nThanks,\r\nSami\r\n", "Hi @sibyjackgrove,\r\n\r\nThanks for the interest on TF-TRT integration. Currently the integration is still in progress and we're actively working on it. The API is not stable yet and may change in the future. You may want to take a look at the `test/test_tftrt.py` script mentioned in the README, which is in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensorrt/test/test_tftrt.py. I'll leave to @samikama about the documentation.", "@aaroey @samikama Thanks for the response and updates. If the API is not stable yet, it makes sense to wait until it is so before updating documentation on tensorflow.org. Closing the issue."]}, {"number": 18212, "title": "Fix `TypeError: 'dict_keys'` in contrib.distribute with python 3", "body": "This fix tries to fix the issue raised in #18205 where\r\n```\r\nTypeError: 'dict_keys' object does not support indexing\r\n```\r\nwas thrown when using contrib.distribute in python 3.\r\n\r\nThe issue is that DistributedValues.devices returned `self._index.keys()` which is a `dict_keys` and is not a list in python 3.\r\n\r\nThis fix converts the `dict_keys` to list for python 3 to fix the issue.\r\n\r\nThis fix fixes #18205.\r\nThis fix als fixes #18188.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 18211, "title": "freeze_graph.py generates error messages when run on sample model (census / wide_deep)", "body": "1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: ('v1.6.0-0-gd2e24b6039', '1.6.0')\r\n- **Python version**: Python 2.7.12 (default, Dec  4 2017, 14:50:18) [GCC 5.4.0 20160609] on linux2\r\n- **CUDA/cuDNN version**: Not using GPU\r\n- **GPU model and memory**: Not using GPU\r\n- **Exact command to reproduce**:\r\nFollow the \"wide_deep\" tutorial\r\nTry to freeze the generated model by running: \r\npython freeze_graph.py \r\n  --input_graph=/tmp/census_model/graph.pbtxt \r\n  --input_checkpoint=/tmp/census_model/model.ckpt-190 \r\n  --output_graph=/tmp/census_model/frozen_graph.pb\r\n  --output_node_names=softmax\r\n\r\nAn error is generated:\r\nTypeError: names_to_saveables must be a dict mapping string names to Tensors/Variables. Not a variable: Tensor(\"dnn/hiddenlayer_0/bias:0\", shape=(100,), dtype=float32)\r\n\r\n### Describe the problem\r\nI would think that the freeze_graph script should finish successfully, hence I believe the problem is that an error message is generated.\r\n\r\n### Source code / logs\r\nTraceback (most recent call last):\r\n  File \"freeze_graph.py\", line 380, in <module>\r\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/media/LinuxApps/home/karsten/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"freeze_graph.py\", line 274, in main\r\n    FLAGS.saved_model_tags, checkpoint_version)\r\n  File \"freeze_graph.py\", line 256, in freeze_graph\r\n    checkpoint_version=checkpoint_version)\r\n  File \"freeze_graph.py\", line 130, in freeze_graph_with_def_protos\r\n    var_list=var_list, write_version=checkpoint_version)\r\n  File \"/media/LinuxApps/home/karsten/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1293, in __init__\r\n    self.build()\r\n  File \"/media/LinuxApps/home/karsten/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1302, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"/media/LinuxApps/home/karsten/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1339, in _build\r\n    build_save=build_save, build_restore=build_restore)\r\n  File \"/media/LinuxApps/home/karsten/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 774, in _build_internal\r\n    saveables = self._ValidateAndSliceInputs(names_to_saveables)\r\n  File \"/media/LinuxApps/home/karsten/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 677, in _ValidateAndSliceInputs\r\n    variable)\r\nTypeError: names_to_saveables must be a dict mapping string names to Tensors/Variables. Not a variable: Tensor(\"dnn/hiddenlayer_0/bias:0\", shape=(100,), dtype=float32)\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version", "TensorFlow graph definition files are code and it's not clear to me if this is a bug in TensorFlow or an issue with the custom code. Consider trying [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow). There is a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!", "The issue occurs on the model which is generated by the census example. No custom code involved at all. Just running the tensorflow census example and then try to freeze it.\r\nBazel version is: 0.11.1", "I believe you. Let's reopen.\r\n\r\nI noticed you're using TF 1.6. The [tutorial](https://www.tensorflow.org/tutorials/wide_and_deep) points to [wide_deep.py](https://github.com/tensorflow/models/commits/master/official/wide_deep/wide_deep.py) which had E2E tests added in https://github.com/tensorflow/models/pull/3798 a few months after that release.\r\n\r\n@kjancke could you please try the tutorial at head and let us know if it works? If it doesn't, then could you include a short minimal little shell script that reproduces the error? Something along the lines of:\r\n\r\n```sh\r\nvirtualenv tf\r\nsource tf/bin/activate\r\npip install tensorflow\r\nwget https://raw.githubusercontent.com/tensorflow/models/master/official/wide_deep/wide_deep.py\r\npython wide_deep.py ...\r\n```", "Sure, will do. I have TF v1.8.0-2153-ge844159e63 on another machine, will this be current enough?", "Closing this issue since its [resolved](https://github.com/tensorflow/models/pull/3798). Feel free to reopen if the problem still persists. Thanks!"]}, {"number": 18210, "title": "Fix minor typo and math rendering format in linear operator related apis", "body": "This PR is to fix some minor typo and mathe rendering format in linear operator related doc strings.\r\n- Fix a minor typo format for below line in [tensor_rank](https://www.tensorflow.org/api_docs/python/tf/linalg/LinearOperator#tensor_rank);\r\n![image](https://user-images.githubusercontent.com/1680977/38263160-e696ea3e-37a1-11e8-9f63-9a27c847d05a.png)\r\n\r\n- Fix some math equation rendering format in linear operator related api doc strings, like the one example [LinearOperatorLowRankUpdate](https://www.tensorflow.org/api_docs/python/tf/linalg/LinearOperatorLowRankUpdate#base_operator) as below;\r\n![image](https://user-images.githubusercontent.com/1680977/38263690-54ee4a08-37a3-11e8-99fb-7a27676075eb.png)\r\n\r\n\r\n", "comments": ["Nagging Assignee @andrewharp: It has been 21 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@imsheridan please resolve conflicts. @andrewharp is this okay with you to merge?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 18209, "title": "[XLA] Don't run int64 tests for devices which do not support int64", "body": "This particular test assumes int64 support.  Add a test to prevent the test from running when a device has not advertized int64 support.\r\n", "comments": ["the offending test was eliminated"]}, {"number": 18208, "title": "[XLA] Allow for disabling of some tests in the XLA CC test suite", "body": "Allow for 2 XLA CC tests to be disabled in a manifest file.\r\n\r\n[some backends may not support features required for passing these tests]\r\n\r\n", "comments": ["Nagging Assignee @andrewharp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "ping - could we look at this? - it is a very simple change.", "@vrv Is it possible that this is assigned to a person who is no longer with the project?\r\n", "@tatatodd thanks for the review \ud83d\udc4d "]}, {"number": 18207, "title": "Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2", "body": "#When i try run code python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config\r\n\r\nIt tells me: \r\nWARNING:tensorflow:From C:\\Users\\kuba7\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\object_detection-0.1-py3.5.egg\\object_detection\\trainer.py:228: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.create_global_step\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:Summary name /clone_loss is illegal; using clone_loss instead.\r\nWARNING:tensorflow:From C:\\Users\\kuba7\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\slim\\python\\slim\\learning.py:736: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.MonitoredTrainingSession\r\n2018-04-03 17:15:26.363868: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows\\PY\\35\\tensorflow\\core\\platform\\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\nINFO:tensorflow:Restoring parameters from training/model.ckpt-0\r\nINFO:tensorflow:Starting Session.\r\nINFO:tensorflow:Saving checkpoint to path training/model.ckpt\r\nINFO:tensorflow:Starting Queues.\r\nINFO:tensorflow:global_step/sec: 0\r\n\r\nWhat should i do?", "comments": ["this is better question for stackoverflow"]}, {"number": 18206, "title": "Do you have plan to optimize memory for Tensorflow Lite?", "body": "Do you have plan to optimize memory for Tensorflow Lite? For a image or video related task, the memory occupation is too large.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code  N/A\r\nOS Platform and Distribution   Android\r\nTensorFlow installed from latest code\r\nTensorFlow version  1.7\r\nBazel version 5.4\r\nCUDA/cuDNN version N/A\r\nGPU model and memory N/A\r\nExact command to reproduce N/A", "Could you give us a sense of what you think is too high and what you think could be achievable?", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 18205, "title": "Distribution Strategy not working with tf-nightly-gpu for Python3.5", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, I am using the script https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/python/examples/simple_estimator_example.py provided as an example for DistributionStrategy API\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux  4.13.0-37-generic #42~16.04.1-Ubuntu SMP Wed Mar 7 16:03:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux \"16.04.4 LTS (Xenial Xerus)\"\r\n- **TensorFlow installed from (source or binary)**: binary through pip3 install tf-nightly-gpu\r\ntf.VERSION = 1.8.0-dev20180402\r\ntf.GIT_VERSION = v1.7.0-rc1-1091-gc7a04561fb\r\ntf.COMPILER_VERSION = v1.7.0-rc1-1091-gc7a04561fb\r\nSanity check: array([1], dtype=int32)\r\n- **TensorFlow version (use command below)**: v1.7.0-rc1-1091-gc7a04561fb 1.8.0-dev20180402\r\n- **Python version**: 3.5\r\n- **CUDA/cuDNN version**: \r\nCUDA version: 9.0, V9.0.176\r\n#define CUDNN_MAJOR 7\r\n#define CUDNN_MINOR 0\r\n#define CUDNN_PATCHLEVEL 5\r\n- **GPU model and memory**: Quadro M6000 24GB\r\n- **Exact command to reproduce**: python3 \r\n\r\n### Describe the problem\r\nI am trying to test the DistributionStrategy API. In order to do that I downloaded the tensorflow nightly build by executing  \r\npip3 install tf-nightly-gpu\r\nThen I tried to execute the example provided here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/python/examples/simple_estimator_example.py\r\nBut it is not working, there is an exception shortly after the script starts to run ( I am copying the stack trace in the next section).\r\nI have tried to do the same using the  tensorflow nightly build for python 2.7 downloaded by executing  \r\npip install tf-nightly-gpu and it works without any problem.\r\nThe issue here, is that I would like to integrate this API with my multi-gpu training and inference processes, which are complete written for python3. \r\nI would like to know, if DistributionStrategy API is already supported in for python 3.5 and the problem is that I am using a wrong example. Or, in case it is not supported yet, if there are plans to do it.\r\n\r\nThanks in advance.\r\n\r\n\r\n### Source code / logs\r\n(vtf_nightly_gpu) /ccrespo/mirrored_strategy/src$ python3 mirrored_strategy_test.py \r\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpmou3ft0t\r\n2018-04-03 16:17:32.334622: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-04-03 16:17:32.686755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \r\nname: Quadro M6000 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.114\r\npciBusID: 0000:03:00.0\r\ntotalMemory: 23.90GiB freeMemory: 23.35GiB\r\n2018-04-03 16:17:32.956384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 1 with properties: \r\nname: Quadro M6000 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.114\r\npciBusID: 0000:04:00.0\r\ntotalMemory: 23.90GiB freeMemory: 23.78GiB\r\n2018-04-03 16:17:33.215141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 2 with properties: \r\nname: Quadro M6000 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.114\r\npciBusID: 0000:a1:00.0\r\ntotalMemory: 23.90GiB freeMemory: 23.78GiB\r\n2018-04-03 16:17:33.215663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0, 1, 2\r\n2018-04-03 16:17:34.314104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-04-03 16:17:34.314172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 2 \r\n2018-04-03 16:17:34.314181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N Y N \r\n2018-04-03 16:17:34.314185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   Y N N \r\n2018-04-03 16:17:34.314190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 2:   N N N \r\n2018-04-03 16:17:34.315429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:0 with 22663 MB memory) -> physical GPU (device: 0, name: Quadro M6000 24GB, pci bus id: 0000:03:00.0, compute capability: 5.2)\r\n2018-04-03 16:17:34.802106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:1 with 23083 MB memory) -> physical GPU (device: 1, name: Quadro M6000 24GB, pci bus id: 0000:04:00.0, compute capability: 5.2)\r\n2018-04-03 16:17:35.250349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:2 with 23083 MB memory) -> physical GPU (device: 2, name: Quadro M6000 24GB, pci bus id: 0000:a1:00.0, compute capability: 5.2)\r\n2018-04-03 16:17:35.726606: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0, 1, 2\r\n2018-04-03 16:17:35.726819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-04-03 16:17:35.726838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 2 \r\n2018-04-03 16:17:35.726849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N Y N \r\n2018-04-03 16:17:35.726858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   Y N N \r\n2018-04-03 16:17:35.726867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 2:   N N N \r\n2018-04-03 16:17:35.727458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22663 MB memory) -> physical GPU (device: 0, name: Quadro M6000 24GB, pci bus id: 0000:03:00.0, compute capability: 5.2)\r\n2018-04-03 16:17:35.727626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 23083 MB memory) -> physical GPU (device: 1, name: Quadro M6000 24GB, pci bus id: 0000:04:00.0, compute capability: 5.2)\r\n2018-04-03 16:17:35.727819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 23083 MB memory) -> physical GPU (device: 2, name: Quadro M6000 24GB, pci bus id: 0000:a1:00.0, compute capability: 5.2)\r\nTraceback (most recent call last):\r\n  File \"mirrored_strategy_test.py\", line 86, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"mirrored_strategy_test.py\", line 70, in main\r\n    estimator.train(input_fn=input_fn, steps=10)\r\n  File \"/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\", line 363, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\", line 841, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\", line 884, in _train_model_distributed\r\n    self.config)\r\n  File \"/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/tensorflow/python/training/distribute.py\", line 751, in call_for_each_tower\r\n    return self._call_for_each_tower(fn, *args, **kwargs)\r\n  File \"/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 254, in _call_for_each_tower\r\n    coord.join(threads)\r\n  File \"/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 248, in _call_for_each_tower\r\n    self, *merge_args, **merge_kwargs)\r\n  File \"/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 667, in _distributed_apply\r\n    reduced_grads = distribution.batch_reduce(\"sum\", grads_and_vars)\r\n  File \"/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/tensorflow/python/training/distribute.py\", line 796, in batch_reduce\r\n    return self._batch_reduce(method_string, value_destination_pairs)\r\n  File \"/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 295, in _batch_reduce\r\n    value_destination_pairs)\r\n  File \"/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py\", line 175, in batch_reduce\r\n    return self._batch_reduce(method_string, value_destination_pairs)\r\n  File \"/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py\", line 462, in _batch_reduce\r\n    [v[0] for v in value_destination_pairs])\r\n  File \"/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py\", line 517, in _batch_all_reduce\r\n    method_string)\r\n  File \"/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py\", line 276, in _ungroup_and_make_mirrored\r\n    index[i][destinations[d]] = v\r\nTypeError: 'dict_keys' object does not support indexing\r\n", "comments": ["I think the issue is in:\r\nhttps://github.com/tensorflow/tensorflow/blob/28dec7f4669e8ed5af4a3abebf9888d3fdffe5fd/tensorflow/contrib/distribute/python/values.py#L75-L76\r\n\r\nwhere `self._index.keys()` returns a non-list in python 3. Added a PR #18212 for the fix.", "Thanks! "]}, {"number": 18204, "title": "Unable to compile from source using Bazel", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7\r\n- **TensorFlow installed from (source or binary)**: Attempting source. (Binary installation works)\r\n- **TensorFlow version (use command below)**: 1.7 (tried 1.5+)\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.11\r\n- **GCC/Compiler version (if compiling from source)**: 4.8\r\n- **CUDA/cuDNN version**: 9.0/7\r\n- **GPU model and memory**: K10, 12GB\r\n- **Exact command to reproduce**: bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\nI receive the following error:\r\n\r\n`INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (258 packages loaded).\r\nINFO: Found 1 target...\r\nUnhandled exception thrown during build; message: /home/dmallon@isd.csc.mrc.ac.uk/cardiac/dl_stuff/tensorflow/bazel-out (Operation not supported)\r\nINFO: Elapsed time: 6.481s\r\nFAILED: Build did NOT complete successfully\r\njava.lang.UnsupportedOperationException: /home/dmallon@isd.csc.mrc.ac.uk/cardiac/dl_stuff/tensorflow/bazel-out (Operation not supported)\r\n        at com.google.devtools.build.lib.unix.NativePosixFiles.symlink(Native Method)\r\n        at com.google.devtools.build.lib.unix.UnixFileSystem.createSymbolicLink(UnixFileSystem.java:329)\r\n        at com.google.devtools.build.lib.vfs.Path.createSymbolicLink(Path.java:883)\r\n        at com.google.devtools.build.lib.vfs.FileSystemUtils.ensureSymbolicLink(FileSystemUtils.java:369)\r\n        at com.google.devtools.build.lib.vfs.FileSystemUtils.ensureSymbolicLink(FileSystemUtils.java:320)\r\n        at com.google.devtools.build.lib.buildtool.OutputDirectoryLinksUtils.createLink(OutputDirectoryLinksUtils.java:251)\r\n        at com.google.devtools.build.lib.buildtool.OutputDirectoryLinksUtils.createOutputDirectoryLinks(OutputDirectoryLinksUtils.java:89)\r\n        at com.google.devtools.build.lib.buildtool.ExecutionTool.executeBuild(ExecutionTool.java:380)\r\n        at com.google.devtools.build.lib.buildtool.BuildTool.buildTargets(BuildTool.java:279)\r\n        at com.google.devtools.build.lib.buildtool.BuildTool.processRequest(BuildTool.java:383)\r\n        at com.google.devtools.build.lib.buildtool.BuildTool.processRequest(BuildTool.java:350)\r\n        at com.google.devtools.build.lib.runtime.commands.BuildCommand.exec(BuildCommand.java:74)\r\n        at com.google.devtools.build.lib.runtime.BlazeCommandDispatcher.execExclusively(BlazeCommandDispatcher.java:489)\r\n        at com.google.devtools.build.lib.runtime.BlazeCommandDispatcher.exec(BlazeCommandDispatcher.java:218)\r\n        at com.google.devtools.build.lib.runtime.CommandExecutor.exec(CommandExecutor.java:58)\r\n        at com.google.devtools.build.lib.server.GrpcServerImpl.executeCommand(GrpcServerImpl.java:851)\r\n        at com.google.devtools.build.lib.server.GrpcServerImpl.access$2100(GrpcServerImpl.java:109)\r\n        at com.google.devtools.build.lib.server.GrpcServerImpl$2.lambda$run$0(GrpcServerImpl.java:916)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\njava.lang.UnsupportedOperationException: /home/dmallon@isd.csc.mrc.ac.uk/cardiac/dl_stuff/tensorflow/bazel-out (Operation not supported)\r\n        at com.google.devtools.build.lib.unix.NativePosixFiles.symlink(Native Method)\r\n        at com.google.devtools.build.lib.unix.UnixFileSystem.createSymbolicLink(UnixFileSystem.java:329)\r\n        at com.google.devtools.build.lib.vfs.Path.createSymbolicLink(Path.java:883)\r\n        at com.google.devtools.build.lib.vfs.FileSystemUtils.ensureSymbolicLink(FileSystemUtils.java:369)\r\n        at com.google.devtools.build.lib.vfs.FileSystemUtils.ensureSymbolicLink(FileSystemUtils.java:320)\r\n        at com.google.devtools.build.lib.buildtool.OutputDirectoryLinksUtils.createLink(OutputDirectoryLinksUtils.java:251)\r\n        at com.google.devtools.build.lib.buildtool.OutputDirectoryLinksUtils.createOutputDirectoryLinks(OutputDirectoryLinksUtils.java:89)\r\n        at com.google.devtools.build.lib.buildtool.ExecutionTool.executeBuild(ExecutionTool.java:380)\r\n        at com.google.devtools.build.lib.buildtool.BuildTool.buildTargets(BuildTool.java:279)\r\n        at com.google.devtools.build.lib.buildtool.BuildTool.processRequest(BuildTool.java:383)\r\n        at com.google.devtools.build.lib.buildtool.BuildTool.processRequest(BuildTool.java:350)\r\n        at com.google.devtools.build.lib.runtime.commands.BuildCommand.exec(BuildCommand.java:74)\r\n        at com.google.devtools.build.lib.runtime.BlazeCommandDispatcher.execExclusively(BlazeCommandDispatcher.java:489)\r\n        at com.google.devtools.build.lib.runtime.BlazeCommandDispatcher.exec(BlazeCommandDispatcher.java:218)\r\n        at com.google.devtools.build.lib.runtime.CommandExecutor.exec(CommandExecutor.java:58)\r\n        at com.google.devtools.build.lib.server.GrpcServerImpl.executeCommand(GrpcServerImpl.java:851)\r\n        at com.google.devtools.build.lib.server.GrpcServerImpl.access$2100(GrpcServerImpl.java:109)\r\n        at com.google.devtools.build.lib.server.GrpcServerImpl$2.lambda$run$0(GrpcServerImpl.java:916)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n`\r\n\r\nDoes anyone have any ideas? I've tried multiple versions of Java (8-151, 8-161, 7-161). \r\n", "comments": ["Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I really have no clue, but it looks like symlink is not working which suggests you are using a filesystem that doesn't support symlinks. If you are using something like sshfs, you probably should not do that.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 30 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 18203, "title": "As per documentation Tensorflow 1.5 with Cuda -8 ", "body": "Hi,\r\n\r\nAs per documentation in the below link:\r\n\r\n[https://www.tensorflow.org/versions/r1.5/install/install_linux#InstallingVirtualenv](url)\r\n\r\nRequirement for tensorflow 1.5 is Cuda 8. I installed tensorflow using the following tf binary url https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.5.0rc1-cp27-none-linux_x86_64.whl\r\n\r\nHowever, still it is asking for cuda 9. Is there any .whl, which supports cuda 8 ( tensorflow 1.5).", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code - NA\r\nOS Platform and Distribution - Ubuntu 16.04\r\nTensorFlow installed from - Followed the instructions from https://www.tensorflow.org/versions/r1.5/install/install_linux#InstallingVirtualenv\r\nTensorFlow version - 1.5\r\nBazel version - NA\r\nCUDA/cuDNN version - Cuda-8\r\nGPU model and memory - Nvidia Geforce GTX 1070\r\nExact command to reproduce\r\nFollowed the tensorflow installation instructions in virtual env from the following link:\r\nhttps://www.tensorflow.org/versions/r1.5/install/install_linux\r\n\r\nUsed the following tf binary url to install:\r\n https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.5.0rc1-cp27-none-linux_x86_64.whl\r\n\r\nAfter, installing while importing tensorflow  I'm getting the following error:\r\n\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\nHowever, in the link  it requires only cuda-8.0. \r\n\r\n\r\n", "I meet the same question. Any solution?", "I have the same problem. I believe while TF devs upgraded to 1.7, they also added cuda 9.0 support for some of the prev versions, which lead to a backward compatibility issue. This problem should be solved easily, and quickly.", "@av8ramit Do you know how this may have happened?\r\nOr do we simply have a documentation issue?", "I believe this is a documentation issue. The [docs for 1.5](https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/docs_src/install/install_linux.md) as well as [release notes](https://github.com/tensorflow/tensorflow/blob/r1.5/RELEASE.md) advise the binaries are built for CUDA 9. @MarkDaoust mind updating the website for 1.5?", "Fixed.\r\n\r\nhttps://www.tensorflow.org/versions/r1.5/install/", "@MarkDaoust The link is dead.", "This info has moved: look for the v1.5 line in the gpu table:\r\n\r\n https://www.tensorflow.org/install/source#linux"]}, {"number": 18202, "title": "Submit Raspberry Pi releases to piwheels.org", "body": "I maintain the [piwheels](https://www.piwheels.org/) project where we build Arm platform Python wheels on Raspberry Pi, and provide them to Raspbian users (pre-configured in pip.conf) for speedy installs. As well as automating building most packages, we're able to manually import wheels built elsewhere.\r\n\r\nIt's great that you're now providing Raspberry Pi builds on Jenkins, but the current (undocumented) installation process is to `pip install` from the URL of the file on Jenkins. If you were able to upload these wheels to piwheels, users would just be able to `pip install tensorflow` and get it.\r\n\r\nHowever, you seem to provide wheels for Python 2.7 and 3.4 (but not 3.5, which is the version which ships with Raspbian Stretch). Would you be able to build for Python 3.5 as well?\r\n\r\nAlso you give your wheels a platform tag of `any` which is naughty. The platform reported on Pi 1/Zero is `linux_armv6l` and on Pi 2/3 is `linux_armv7l`.\r\n\r\nIf you can provide the following wheels for v1.7 (and future releases submitted to pypi - no need for nightlies):\r\n\r\n- Python 2.7 Armv6\r\n- Python 3.4 Armv6\r\n- Python 3.5 Armv6\r\n- Python 2.7 Armv7\r\n- Python 3.4 Armv7\r\n- Python 3.5 Armv7\r\n\r\nthen I can upload them to piwheels and Raspberry Pi users will have them available no hassle.\r\n\r\nThanks!\r\n\r\nP.S. If you want to get in touch I'm ben [at] raspberrypi [dot] org", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "N/A", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@petewarden is this something you can look into?", "Yes, thanks for the nudge, I think this is a great idea!\r\n\r\nMy biggest challenge has been figuring out the PyPi upload process, especially with the platform tags. The hints you give are helpful, and I'll talk to the team responsible for our main PyPi upload scripts.", "This comment is worth a read: https://github.com/pypa/warehouse/issues/3668#issuecomment-383369773", "Nagging Assignee @petewarden: It has been 18 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 33 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 48 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 63 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This is now done! There's documentation at https://www.tensorflow.org/install/install_raspbian\r\n\r\nThanks for all your help @bennuttall, closing.", "Thanks @petewarden! \\o/"]}, {"number": 18201, "title": "Cleanup the file structures in tensorflow/core/kernels", "body": "This is a feature request about the file structure in ```tensorflow/core/kernels``` .\r\n\r\n### The problem\r\n\r\nThere is a CPU/GPU mingled source codes in the OpKernel directory.\r\nThis file structure seems that it is complicated to search the CPU/GPU independent code.\r\nHow about to cleanup the file structures?\r\nI think the file structures (```tensorflow/compiler/xla/service```) on XLA is great approach because it is more pluggable and less dependency with another device.\r\n\r\nAnd I would like to contribute if you accept this idea.\r\n\r\nThanks", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 22 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 18200, "title": "fix problem: ld libgrpc symbols for MacOS", "body": "fix problem: ld libgrpc symbols for MacOS when cmake build", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "I signed it!"]}, {"number": 18199, "title": "[Question]TFLite gpu acceleration.", "body": "Current, my model's inference speed (mate 10) 300ms\r\nWe have applied all the structural transformations that can be done in the model, algorithm\r\nSo, I am looking for a way to improve the performance of the Android platform.\r\nExample of Render Script, OpenCL...\r\nBut, don't know if these ways really improve inference performance dramatically.\r\nI wonder if these methods are applied to Tensorflow Mobile(including TFLite) at present.\r\nI also wonder which method is the most appropriate.\r\n\r\nHave a nice day :)", "comments": ["There's standard channel for hw acceleration called NNAPI in Android 8.1.\r\nhttps://developer.android.com/ndk/guides/neuralnetworks/index.html\r\n\r\nTFL is gonna built on top of it and dispatch model to it.\r\nDepends on how SOC vendor implemented NN HAL, the dispatched model may or may not have performance gain.\r\n\r\nBefore doing that, make sure if your model can be converted to tflite format by TOCO.\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/toco\r\nIf yes, then you can run your converted tflite model with TFL.", "@VincentLin78 \r\nI try to convert pb to tflite, but encountered unsupported operation error ([here is my posting](https://github.com/tensorflow/tensorflow/issues/18191)).  I converted 5 pb  to tflite, but there was no dramatic effect. (mobilenet, inception, ssd mobilenet etc)\r\nalso\r\n[NNAPI document](https://developer.android.com/ndk/guides/neuralnetworks/index.html)\r\nI'm wondering if you need to write code that references the model(load graph, open session), or additionally needs to add other parts(model operation add, mul, activation function etc).", "@nanamare, we are working on various acceleration strategies (including GPU), and we will update this post when we can.\r\n\r\nThanks!", "@aselle @shashishekhar \r\nHi, I just wondering whether tflite will support CUDA gpu acceleration?\r\nI think tflite provides a more compatible quantization over difference types of nodes in tf.\r\n\r\nThanks, ", "@oscarriddle : CUDA may not be available on many mobile devices.  \r\nIn short term TFLite will support acceleration using OpenGL and other drivers that can target GPU. \r\n\r\nTechnically, it is certainly possible to write a [delegate](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/delegates) that uses CUDA for acceleration. ", "@aselle @shashishekhar Is it already known when acceleration using OpenGL is about to happen?\r\nJust FYI:\r\nI run a model on a different Android devices (Android < 8.0) and interference time takes 400-700ms using tflite. The same model on iOS takes ~30ms using CoreML.", "In short term TFLite will soon support acceleration using OpenGL and other drivers that can target GPU.", "Still would Firebase Mlkit supported the acceleration out of the box, it would be cool... But as long as it's scary!", "We are looking forward to the GPU OpenGLES delegate. But out benchmarks show that on same device Mace OpenCL is at least 30% faster than TfLite NNAPI with OpenGL NN driver.\r\n\r\nAny news of alternative delegate drivers?\r\n\r\nWill the delegate source be made public soon? We would like to implement additional ops such as resize nearest neighbor. ", "The binary preview of GPU OpenGLES delegate backend was just released. See also the [blog post](https://medium.com/tensorflow/tensorflow-lite-now-faster-with-mobile-gpus-developer-preview-e15797e6dee7?linkId=62443226) and [tutorial document](https://www.tensorflow.org/lite/performance/gpu)", "Since we released the GPU backend binary. I'm closing this issue. \r\nFeel free to create other issues if you have more specific questions. ", "> Since we released the GPU backend binary. I'm closing this issue.\r\n> Feel free to create other issues if you have more specific questions.\r\n\r\n@miaout17 Is the source code of the GPU delegate has been open source? I can not find the source code, just find the \"libtensorflowlite_gpu_jni.so\"", "I am working on TFLite using Mobile GPU. Just a followup question, \"does TFLite support mobile GPU for detector task (instead of classifier in the demo)?\" Also, \"does TFLite's mobile GPU only works with floating-point models not quantized ones?\" (I found this from code comments)"]}, {"number": 18198, "title": "Different scatter_nd behaviours with int and float", "body": "I am using tensorflow 1.4.1.\r\nscatter_nd seems not check the index validation with float, because following codes generate no error. \r\n\r\n```\r\na = tf.scatter_nd( [[0,99,0,0]], [[1.0 for i in range(16)]], [100, 8, 100, 100, 16])\r\nwith tf.Session() as sess:\r\n    print(sess.run(a))\r\n```\r\nIf the 1.0 is changed to 1, the codes generate errors:\r\n\r\n```\r\na = tf.scatter_nd( [[0,99,0,0]], [[1 for i in range(16)]], [100, 8, 100, 100, 16])\r\nwith tf.Session() as sess:\r\n    print(sess.run(a))\r\n```\r\nIs this designed for some purpose or a bug?\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I am glad to update the question with your suggestion : )\r\n\r\nOS Platform and Distribution: \r\nI am using ubuntu 16.04 with the kernel 4.13.0-37-generic\r\n\r\nTensorFlow installed from:\r\ntensorflow is installed from pip with the script.\r\n`pip3 install tensorflow-gpu==1.4.1`\r\n\r\nTensorFlow version\r\nI double checked it with the following script.\r\n` python3 -c 'import tensorflow; print (tensorflow.__version__)'`\r\nThe output is '1.4.1'\r\n\r\nExact command to reproduce\r\nTry following two code snippets.\r\n```\r\na = tf.scatter_nd( [[0,99,0,0]], [[1 for i in range(16)]], [100, 8, 100, 100, 16])\r\nwith tf.Session() as sess:\r\n    print(sess.run(a))\r\n```\r\n\r\n```\r\na = tf.scatter_nd( [[0,99,0,0]], [[1.0 for i in range(16)]], [100, 8, 100, 100, 16])\r\nwith tf.Session() as sess:\r\n    print(sess.run(a))\r\n```\r\n\r\n\r\n\r\n\r\n", "@worm004 Could you test it on the latest version 1.7? ", "Were you able to test it on a recent version of TF?", "@michaelisard @facaiy Sorry for replying late, I was so busy these days : ( Currently I am not able to test it on 1.7.", "Nagging Assignee @michaelisard: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing for now, but please reopen if the problem is still present on a recent version."]}, {"number": 18197, "title": "Fix std::out_of_range error when tensorrt.create_inference_graph", "body": "# Error \r\nwhen invoke ```trt.create_inference_graph(g,[\"detection_boxes:0\"])```\r\n```\r\nterminate called after throwing an instance of 'std::out_of_range'\r\n  what():  basic_string::substr\r\nAborted (core dumped)\r\n```\r\nError is from ParseTensorName() function, string ```name``` has been replaced as the substring, so the index of column is out ranged.", "comments": ["I encountered the exactly same problem.\r\nMy solution is to find the output node's name and assign to the outputs=.\r\nfor instance,` outputs=['InceptionV3/Logits/SpatialSqueeze']`", "@yjmade were you able to run a ssd on tensorrt ?\r\n", "I didn't test SSD, but I do test FasterRCNN, it worked", "@yjmade I tried to use the frozen graph of FasterRCNN but I'm greeted with this kind error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"tests/trt_od.py\", line 107, in <module>\r\n    minimum_segment_size = 2,\r\n  File \"/home/nvidia/Documents/tfinterface/tfinterface/estimator/getters.py\", line 38, in get\r\n    return cls(model_path, *args, **kwargs)\r\n  File \"tests/trt_od.py\", line 53, in __init__\r\n    **trt_ops\r\n  File \"/home/nvidia/.local/lib/python2.7/site-packages/tensorflow/contrib/tensorrt/python/trt_convert.py\", line 115, in create_inference_graph\r\n    int(msg[0]))\r\ntensorflow.python.framework.errors_impl.NotFoundError: No attr named 'index_type' in NodeDef:\r\n         [[Node: ones = Fill[T=DT_INT32](strided_slice_14, ones/Const)]]\r\n         [[Node: ones = Fill[T=DT_INT32](strided_slice_14, ones/Const)]]\r\n```\r\n\r\nDo you have any example that I could follow?\r\n\r\nSame thing with mobilnet or other detection model zoo", "Nagging Assignee @andrewharp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@yjmade, thanks for this PR. We also caught it. It has been fixed and we forgot about this PR. Sorry about that. I am going to close this but please let us know if you encounter with further such issues.\r\n\r\nSami\r\n"]}, {"number": 18196, "title": "Fix math equation rendering format in nn related doc strings", "body": "This PR is to fix mess-up math equation format in below NN related docstrings.\r\n\r\nThis PR is to fix this math equation issue according to the [Math in markdown guideline](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/community/documentation.md#math-in-markdown).\r\n\r\nTake [sparse_softmax_cross_entropy_with_logits](https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits) as an example below:\r\nBefore:\r\n![image](https://user-images.githubusercontent.com/1680977/38236128-c99ed41a-3755-11e8-948e-c18960964f68.png)\r\nAfter:\r\n![image](https://user-images.githubusercontent.com/1680977/38236836-fb3f7310-3757-11e8-93fc-a0e2a99a3794.png)\r\n\r\n", "comments": ["It has been 20 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 49 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 18195, "title": "Saving frozen model to tflite error", "body": "------------------------\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, I write own model define, loss and training codes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: binary, pip install\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**:  2.7\r\n- **Bazel version (if compiling from source)**: 0.10.0\r\n- **GCC/Compiler version (if compiling from source)**: 4.9.4\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: GeForce GTX 1060\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI have train some customized model with tensorflow and trying to make it a tensorflow lite model for mobile apps. I success in freezing the model and viewing it in tensorboard. I get error when I trying to save it to a tensorflow lite file.\r\n\r\nmy model defins like: \r\n\r\n```\r\n`def P_Net(inputs,label=None,bbox_target=None,landmark_target=None,training=True):\r\n    #define common param\r\n    with slim.arg_scope([slim.conv2d],\r\n                        activation_fn=prelu,\r\n                        weights_initializer=slim.xavier_initializer(),\r\n                        biases_initializer=tf.zeros_initializer(),\r\n                        weights_regularizer=slim.l2_regularizer(0.0005), \r\n                        padding='valid'):\r\n        print inputs.get_shape()\r\n        net = slim.conv2d(inputs, 28, 3, stride=1,scope='conv1')\r\n......\r\n        conv4_1 = slim.conv2d(net,num_outputs=2,kernel_size=[1,1],stride=1,scope='conv4_1',activation_fn=tf.nn.softmax)\r\n        #conv4_1 = slim.conv2d(net,num_outputs=1,kernel_size=[1,1],stride=1,scope='conv4_1',activation_fn=tf.nn.sigmoid)\r\n\r\n        print conv4_1.get_shape()\r\n        #batch*H*W*4\r\n        bbox_pred = slim.conv2d(net,num_outputs=4,kernel_size=[1,1],stride=1,scope='conv4_2',activation_fn=None)\r\n        print bbox_pred.get_shape()`\r\n```\r\n\r\nwhere conv4_1 and conv4_2 is the output layer.\r\n\r\nI freeze the model with:\r\n\r\n`freeze_graph.freeze_graph('out_put_model/model.pb', '', False, model_path, 'Squeeze,Squeeze_1', '', '', 'out_put_model/frozen_model.pb', '', '')`\r\n\r\nI use tensorboard to view the graph and even try to read them back and forward. it produces same result as reading from checkpoint.\r\n\r\n![screenshot from 2018-04-03 11 34 54](https://user-images.githubusercontent.com/4598153/38235205-d847d230-3752-11e8-98b4-8b9b235c517a.png)\r\n\r\n![screenshot from 2018-04-03 11 23 06](https://user-images.githubusercontent.com/4598153/38235214-dc606a26-3752-11e8-926e-7193e1174e47.png)\r\n\r\nSqueeze and Squeeze_1 is the output nodes. image_height, image_width, input_image is input nodes. all of the input nodes is not fixed shape, and resized in the graph.\r\n\r\ncode of input placeholder like:\r\n\r\n```\r\nwith graph.as_default():\r\n            #define tensor and op in graph(-1,1)\r\n            self.image_op = tf.placeholder(tf.float32, name='input_image')\r\n            self.width_op = tf.placeholder(tf.int32, name='image_width')\r\n            self.height_op = tf.placeholder(tf.int32, name='image_height')\r\n            image_reshape = tf.reshape(self.image_op, [1, self.height_op, self.width_op, 3])\r\n            #self.cls_prob batch*2\r\n            #self.bbox_pred batch*4\r\n            #construct model here\r\n            #self.cls_prob, self.bbox_pred = net_factory(image_reshape, training=False)\r\n            #contains landmark\r\n            self.cls_prob, self.bbox_pred, _ = net_factory(image_reshape, training=False)\r\n```\r\n\r\n\r\nI find the tensorflow 1.4.0 don't have tf lite module and checkout tensorflow master branch from github. and run bazel toco for a tflite model.\r\n\r\n` bazel run --config=opt   //tensorflow/contrib/lite/toco:toco --   --input_file='/home/sen/mtcnn_cat/MTCNN-Tensorflow/test/out_put_model/frozen_model.pb'    --output_file='/home/sen/mtcnn_cat/MTCNN-Tensorflow/test/out_put_model/pnet.tflite'    --inference_type=FLOAT   --input_shape=None,None,None   --input_array=image_height,image_width,input_image   --output_array=Squeeze,Squeeze_1  --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --dump_graphviz=/tmp`\r\n\r\n\r\nbut I got this error:\r\n```\r\nINFO: Analysed target //tensorflow/contrib/lite/toco:toco (0 packages loaded).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/contrib/lite/toco:toco up-to-date:\r\n  bazel-bin/tensorflow/contrib/lite/toco/toco\r\nINFO: Elapsed time: 0.288s, Critical Path: 0.00s\r\nINFO: Build completed successfully, 1 total action\r\n\r\nINFO: Running command line: bazel-bin/tensorflow/contrib/lite/toco/toco '--input_file=/home/sensetime/mtcnn_cat/MTCNN-Tensorflow/test/out_put_model/frozen_model.pb' '--output_file=/home/sensetime/mtcnn_cat/MTCNN-Tensorflow/test/out_put_model/pnet.tflite' '--inference_type=FLOAT' '--input_shape=None,None,None' '--input_array=image_height,image_width,input_image' '--output_array=Squeeze,Squeeze_1' '--input_format=TENSORFLOW_GRAPHDEF' '--output_format=TFLITE' '--dump_graphviz=/tmp'\r\nERROR: Non-zero return code '1' from command: Process exited with status 1\r\n```\r\n\r\nwhat's the problem? is the input_array, input_shape, output_array parameter right?\r\nThanks and any suggestion is welcome.\r\n\r\n\r\n\r\n\r\n", "comments": ["Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I suspect the problem is the input shape  `--input_shape=None,None,None` try an explicit value there.\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 30 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@aselle But this is a fully conv operation, input image and size is not fixed. How to set the input-shape?", "does this mean TF Lite only supports fixed input sizes?", "Nagging Assignee @aselle: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "> does this mean TF Lite only supports fixed input sizes?\r\n\r\nNo, it means that the conversion supports fixed sizes. You can resize inputs after you have converted. Of course, depending on your model, this may or may not work (this is true in tensorflow as well). You can often resize to accept different batch sizes, so if you make your input size 1,244,244,3 for an image input you can resize at runtime to 10,244,244,3 to allow a batch size fo 10.", "Please let me know if using an explicit size fixed your problem. If so, please close the plug.", "explicit size do fix this problem, I am encountering some operators not supported problem like:\r\n` Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.contrib.lite.toco_convert(). Here is a list of operators for which  you will need custom implementations: Abs.`\r\nI am closing this issue\r\n"]}, {"number": 18194, "title": "warnings building Tensorflow 1.7 from source", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nno, just tensorflow 1.7.0\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\ninstalled from source\r\n- **TensorFlow version (use command below)**:\r\n1.7.0\r\n- **Python version**:\r\n3.6.5 (Anaconda)\r\n- **Bazel version (if compiling from source)**:\r\n0.11.1\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\n- **CUDA/cuDNN version**:\r\n9.1 / 7.1.2\r\n- **GPU model and memory**:\r\nGeForce GTX 980, 4GB\r\n- **Exact command to reproduce**:\r\n```\r\nsource activate python36\r\n./configure\r\n\r\nPlease specify the location of python. [Default is /home/hunterwolf/anaconda3/envs/python36/bin/python]: \r\n\r\nFound possible Python library paths:\r\n  /home/hunterwolf/anaconda3/envs/python36/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/hunterwolf/anaconda3/envs/python36/lib/python3.6/site-packages]\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 9.0]: 9.1\r\n\r\nPlease specify the location where CUDA 9.1 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/local/cuda-9.1\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 7.1.2\r\n\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda-9.1]:\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 5.2]5.2\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\n\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n### Describe the problem\r\nfollowing https://www.tensorflow.org/install/install_sources gide, I get a lot of warnings while building Tensorflow 1.7 from source. the process is completed anyway and the whl file created.\r\n\r\n\r\n### Source code / logs\r\n```\r\nWARNING:\r\n/home/hunterwolf/.cache/bazel/_bazel_hunterwolf/52c81ed9b047b1a3afd28a55294d981e/external/protobuf_archive/WORKSPACE:1: Workspace name in /home/hunterwolf/.cache/bazel/_bazel_hunterwolf/52c81ed9b047b1a3afd28a55294d981e/external/protobuf_archive/WORKSPACE (@com_google_protobuf) does not match the name given in the repository's definition (@protobuf_archive); this will cause a build error in future versions\r\nWARNING: /home/hunterwolf/.cache/bazel/_bazel_hunterwolf/52c81ed9b047b1a3afd28a55294d981e/external/grpc/WORKSPACE:1: Workspace name in /home/hunterwolf/.cache/bazel/_bazel_hunterwolf/52c81ed9b047b1a3afd28a55294d981e/external/grpc/WORKSPACE (@com_github_grpc_grpc) does not match the name given in the repository's definition (@grpc); this will cause a build error in future versions\r\nWARNING: /home/hunterwolf/.cache/bazel/_bazel_hunterwolf/52c81ed9b047b1a3afd28a55294d981e/external/grpc/BUILD:1943:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/hunterwolf/.cache/bazel/_bazel_hunterwolf/52c81ed9b047b1a3afd28a55294d981e/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /home/hunterwolf/.cache/bazel/_bazel_hunterwolf/52c81ed9b047b1a3afd28a55294d981e/external/grpc/BUILD:1943:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/hunterwolf/.cache/bazel/_bazel_hunterwolf/52c81ed9b047b1a3afd28a55294d981e/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /home/hunterwolf/.cache/bazel/_bazel_hunterwolf/52c81ed9b047b1a3afd28a55294d981e/external/grpc/BUILD:1943:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/hunterwolf/.cache/bazel/_bazel_hunterwolf/52c81ed9b047b1a3afd28a55294d981e/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /home/hunterwolf/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/hunterwolf/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\n```\r\nWhat (eventually) I'm doing wrong?\r\nFirst time building Tensorflow from code and first time using Bazel, sorry for the noob question :)", "comments": ["@Hunterwolf88  The options I can think of to address these warnings:\r\n  1. Make sure your version is the latest master 1.7.0.\r\n  2. Following the Bazel install guide: https://docs.bazel.build/versions/master/install-ubuntu.html\r\n  3. Make Bazel point at com_google_protobuf by default for cc/java_proto_library rules.\r\nPlease make sure these three options.\r\n", "Nagging Assignee @tatianashp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "thanks @jyu25utk I somehow did something wrong in step 3 "]}, {"number": 18193, "title": "tf.keras.estimator.model_to_estimator fails with pre-trained models", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7 x64\r\n- **TensorFlow installed from (source or binary)**: binary (pip)\r\n- **TensorFlow version (use command below)**: 1.7.0 (cpu)\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: n.a.\r\n- **GCC/Compiler version (if compiling from source)**: n.a.\r\n- **CUDA/cuDNN version**: n.a.\r\n- **GPU model and memory**: n.a.\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nFollowing the example code for [Creating Estimators from Keras models](https://www.tensorflow.org/programmers_guide/estimators), I encountered a problem: the example code runs fine when `weights=None` is used in the initialization of the `InceptionV3` object. But when I change it to `weights=imagenet` the call to `model_to_estimator(...)` fails:\r\n\r\n> FailedPreconditionError: Attempting to use uninitialized value batch_normalization_100/beta\r\n> \t [[Node: _retval_batch_normalization_100/beta_0_0 = _Retval[T=DT_FLOAT, index=0, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](batch_normalization_100/beta)]]\r\n\r\n This looks like the weights of the model are not correctly initialized. But strangely I can use the keras model to obtain predictions, which should mean that the model is correctly initialized.\r\n\r\nAs far as I know there are no information in the documentation that pre-trained models (i.e. models with `weights` set to anything different than `None`) are not supported by this function, or that one has to change the workflow somehow. Therefore, this is either a bug in tensorflow or a case of a misleading documentation.\r\n\r\n### Source code / logs\r\n```python\r\n# Instantiate a Keras inception v3 model.\r\nkeras_inception_v3 = tf.keras.applications.inception_v3.InceptionV3(weights=\"imagenet\")\r\n# Compile model with the optimizer, loss, and metrics you'd like to train with.\r\nkeras_inception_v3.compile(optimizer=tf.keras.optimizers.SGD(lr=0.0001, momentum=0.9),\r\n                          loss='categorical_crossentropy',\r\n                          metric='accuracy')\r\n# Create an Estimator from the compiled Keras model. Note the initial model\r\n# state of the keras model is preserved in the created Estimator.\r\nest_inception_v3 = tf.keras.estimator.model_to_estimator(keras_model=keras_inception_v3)\r\n\r\n# Treat the derived Estimator as you would with any other Estimator.\r\n# First, recover the input name(s) of Keras model, so we can use them as the\r\n# feature column name(s) of the Estimator input function:\r\nkeras_inception_v3.input_names  # print out: ['input_1']\r\n# Once we have the input name(s), we can create the input function, for example,\r\n# for input(s) in the format of numpy ndarray:\r\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n    x={\"input_1\": train_data},\r\n    y=train_labels,\r\n    num_epochs=1,\r\n    shuffle=False)\r\n# To train, we call Estimator's train function:\r\nest_inception_v3.train(input_fn=train_input_fn, steps=2000)\r\n```", "comments": ["/cc @yifeif", "Seems a common issue across all pre-trained models: got similar error with RestNet50 and Xception pre-trained models. This seems a critical issue as keras pre-trained model is very widely used.", "@fchollet does this look like a bug or a documentation error?", "I checked the problem again using the nightly build on Windows and Linux, too, and both of them produce the same error. Furthermore, I've started to think, that this problem may also be the reason, why `tf.keras.utils.multi_gpu_model` produces a model which cannot be trained with the estimators eventhough no pre-trained weights are used - at least the error message looks very similar.", "My guess is this is due to how we set session in model_to_estimator. I'll take a look.", "@yifeif  Generally seems that we are having many problems with `tf.keras.estimator.model_to_estimator`. What is the problem? Is that is not used internally so frequently in Google? Poor test coverage? What else?\r\n", "I've just tested the example code from the documentation (see my initial post) again with the commit e606e9133e96caf00d60e2ac0eb3f308fd0a4758. I fear that this has not fixed the issue, as I now get this output:\r\n\r\n> WARNING:tensorflow:The Keras backend session has already been set. The _session_config passed to model_to_estimator is not used.\r\nTraceback (most recent call last):\r\n  File \"/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1330, in _do_call\r\n    return fn(*args)\r\n  File \"/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1315, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1423, in _call_tf_sessionrun\r\n    status, run_metadata)\r\n  File \"/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 516, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Error while reading resource variable SGD/lr from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/SGD/lr/N10tensorflow3VarE does not exist.\r\n         [[Node: SGD/lr/Read/ReadVariableOp = ReadVariableOp[_class=[\"loc:@SGD/lr\"], dtype=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](SGD/lr)]]\r\n         [[Node: SGD/lr/Read/ReadVariableOp/_3025 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5_SGD/lr/Read/ReadVariableOp\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\n", "@qlzh727 When this will be fixed I think that a point (1.7.1) release is required.", "Reopen the issue and wait for further comments from @yifeif", "I am facing the same issue but interestingly I trained using imagenet weights 2 days back and today the same script is not working using imagenet weights for my Resnet50 keras model. But when I initialized with weights = None the estimator model was working. @fchollet Can you please help out here ?", "It appears the problem might be that the model is not trained on the the layers from the tensorflow version of Keras. I found that when building a model using ```from keras.layers import ...``` then this flags an error. However, using layers in the following form:\r\n\r\n```\r\nfrom tensorflow import keras\r\n\r\ninpt = keras.layers.Input(shape = (...))\r\nx = keras.layers.Convolution2D(...)(inpt)\r\n```\r\nmakes it work.", "@JosephPB thanks yous answer\r\n\r\n` from tensorflow.keras.callbacks import TensorBoard, EarlyStopping`\r\n\r\nmakes it work.\r\n", "No problem @aojue1109 glad it worked. Just the same logic - to make sure the layers and callbacks etc. are imported from the eras implementation in tensorflow. I wrote an article about [Freezing a Keras model](https://towardsdatascience.com/freezing-a-keras-model-c2e26cb84a38), where one method uses the `model_to_estimator` method if you'd like more information.", "I had a similar problem, and was surprised that changing the import fixed it.\r\n\r\n> FailedPreconditionError: Error while reading resource variable dense_13/bias from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/dense_13/bias/class tensorflow::Var does not exist.\r\n> \t [[{{node training_5/SGD/ReadVariableOp_29}} = ReadVariableOp[dtype=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dense_13/bias)]]\r\n> \r\nswitching from \r\n`from keras.callbacks import TensorBoard` \r\nto \r\n`from tensorflow.keras.callbacks import TensorBoard, EarlyStopping`\r\nfixed it.  The \"EarlyStopping\" might not be required, as Spyder says it's unused, but tribal programming ftw.\r\n", "> `from tensorflow.keras.callbacks import TensorBoard, EarlyStopping`\r\n\r\nI imagine having the same problem but this did not fixed mine\r\n\r\nError:\r\n> FailedPreconditionError: Error while reading resource variable Adam_38/lr from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/Adam_38/lr/class tensorflow::Var does not exist.\r\n> \t [[{{node training_34/Adam/ReadVariableOp_1}} = ReadVariableOp[dtype=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Adam_38/lr)]]\r\n\r\nSource code:\r\n```\r\ntbCallBack =keras.callbacks.TensorBoard(log_dir=name, histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch')\r\n\r\n    model = tf.keras.models.Sequential([\r\n        tf.keras.layers.Flatten(input_shape=(60,60,)),\r\n        tf.keras.layers.Dropout(0.2)])\r\n    for dense in hiddenLayerStructure:\r\n        model.add(tf.keras.layers.Dense(dense, activation=tf.nn.relu))\r\n    model.add(tf.keras.layers.Dense(2, activation=tf.nn.softmax))\r\n    model.compile(optimizer=optimizer_,loss=loss_,metrics=['accuracy'])\r\n    tbCallBack.set_model(model)\r\n    model.fit(x, y,validation_data=([x], [y]), epochs=epochs_,verbose=2)\r\n```\r\n\r\nAlready tried :\r\n\r\n- Session variable initialization\r\n- Using `callbacks=[tbCallBack])` instead of  ` tbCallBack.set_model(model)`\r\n- Using other callbacks works perfectly\r\n-  tensorflow 1.12.0\r\n- keras 2.2.4\r\n- Reinstalling both tensorflow and keras using pip\r\n- win 10 64\r\n- python 3.6", "> It appears the problem might be that the model is not trained on the the layers from the tensorflow version of Keras. I found that when building a model using `from keras.layers import ...` then this flags an error. However, using layers in the following form:\r\n> \r\n> ```\r\n> from tensorflow import keras\r\n> \r\n> inpt = keras.layers.Input(shape = (...))\r\n> x = keras.layers.Convolution2D(...)(inpt)\r\n> ```\r\n> \r\n> makes it work.\r\n\r\n@JosephPB ... changing the import fixed mine too. Thanks a lot!!"]}, {"number": 18192, "title": "Generate Java ops wrappers", "body": "This pull request generates Java wrappers for TensorFlow operations. Generated source files are compiled with the main Java source code into the `libtensorflow.jar` file. The process is done in two steps:\r\n\r\n1- The op proto definition is parsed, with its api proto definitions if any, by the `op_parser`.\r\n2- The parsed information is used to generated a Java wrapper class by the `op_generator`.\r\n\r\nIt is important to do this in two steps (instead of writing the source code as we parse) because any aspect found in the proto definition can affect the very beginning of the op class.\r\n\r\nCC: @asimshankar ", "comments": ["Here @asimshankar , added your recommendations! BTW, I lost an incredible amount of time trying to implement a minimalist *markdown2javadoc* conversion utility, I think it would better go with a third party library, there is a few (e.g. `Discount`). What do you think?", "Here @asimshankar , your last suggestions have been applied.\r\n\r\nNow that we are trying to generate ops from all the libraries, I added a few Java api defs in the `core` package to exclude those that are not supported right now, which are those with a `func` attribute.\r\n\r\nI guess the next PR would be to populate more of those api defs to group all the other endpoints", "Here @asimshankar , just added an explicit check to skip all operations with a `func` attribute.\r\n\r\nI also removed the `java_api` folder for now, we'll populate all this at once in another PR, thanks!", "@asimshankar , can you give me a hand here? Seems like now it is complaining that it can\u2019t find my resource files but locally, I have no problem running it:\r\n```\r\n2018-05-05 17:22:45.430172: F tensorflow/java/src/gen/cc/source_writer.cc:70] Non-OK-status: ReadFileToString(env, fname, &data_) status: Not found: tensorflow/java/src/gen/resources/license.java.snippet; No such file or directory\r\n```\r\nAny clue how to fix this?", "Ah, that is because there is some missing data dependency in the BUILD rule (`bazel` aims for hermetic builds, so all the data and code dependencies need to be specified).\r\n\r\nI would suggest that we do away with the data file dependency altogether and instead embed that copyright notice as a string constant in the generator. That way the `java_gen_op_tool` binary is a standalone binary and you don't have to worry about access to the data file every time it is invoked.\r\n", "Here @asimshankar , try #3. (I don't know if you have any trick to test locally what will be the output of the build server?)"]}, {"number": 18191, "title": "How to add unsupported operation in TF-Lite?(ResizeNearestNeighbor)", "body": "Converting Hourglass model, I encountered an unsupported error.\r\nThis [document] is very simple. so I have to need more information(example code)(https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/custom_operators.md)\r\n\r\n[Custom operation](https://github.com/tensorflow/tensorflow/pull/17074) Is there only this way? It is a difficulty for me.\r\n\r\n**Error**\r\n```\r\n2018-04-03 12:16:05.786878: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1173] Converting unsupported operation: ResizeNearestNeighbor\r\n2018-04-03 12:16:05.787136: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1173] Converting unsupported operation: ResizeNearestNeighbor\r\n2018-04-03 12:16:05.787422: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1173] Converting unsupported operation: ResizeNearestNeighbor\r\n2018-04-03 12:16:05.787639: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1173] Converting unsupported operation: ResizeNearestNeighbor\r\n2018-04-03 12:16:05.792603: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 558 operators, 854 arrays (0 quantized)\r\n2018-04-03 12:16:05.798150: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 558 operators, 854 arrays (0 quantized)\r\n2018-04-03 12:16:05.807106: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 193 operators, 382 arrays (0 quantized)\r\n2018-04-03 12:16:05.809755: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 193 operators, 382 arrays (0 quantized)\r\n2018-04-03 12:16:05.811826: F tensorflow/contrib/lite/toco/toco_tooling.cc:46] Check failed: unsupported_ops.empty() These unsupported ops were not removed by graph transformations: ResizeNearestNeighbor\r\n```\r\n\r\n### System information\r\n-**OS Platform and Distribution : Ubuntu 16.04 LTS\r\n-**TensorFlow installed from : official\r\n-**TensorFlow version : 1.5 version\r\n-**Bazel version : 0.9.0\r\n-**CUDA/cuDNN version : V 8.0.61 (8.0)\r\n-**GPU model and memory : GeForce GTX 1060 / 3G\r\nExact command to reproduce :\r\n```\r\nbazel run -c opt --copt=-msse4.1 --copt=-msse4.2 --config=opt //tensorflow/contrib/lite/toco:toco --\r\n --input_file=/home/danshin/tensorflow_lite/final_sa_ri_mobile_192_1stack.pb \r\n--output_file=/home/danshin/tensorflow_lite/final_sa_ri_mobile_192_1stack.tflite \r\n--input_format=TENSORFLOW_GRAPHDEF \r\n--output_format=TENSORFLOW_GRAPHDEF\r\n--input_shape=1,192,144,3 --input_array=inference_images/Placeholder \r\n--output_array=Generator_1/stage1/confidence_map/Conv/convolution\r\n```\r\n\r\nHave a nice day.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code", "@angersson \r\nWhen will be offering ResizeNearestNeighbor operation in TFL?", "Have u solved this problem the unsupported operations?", "@aselle Can you take a look at this?", "@aselle \r\nDocuments relative to a custom operation are too simple for me.\r\n[Document Link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/custom_operators.md)\r\n[Document Link2](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/apis.md#writing-custom-operators)", "To help us prioritize implementation of new ops, what kind of network is this?", "Please provide more information for the question asked ", "Sorry, using custom op instructions is the only way to add new operations. If you would like to do nearest neighbor, you can do that, or you can remove the nearest neighbor from your model, and do it as a model preprocess.", "Can you provide more information in the [document](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/custom_operators.md)?\r\nI think it's still an issue. We need more explanation or example codes to understand how to do it. We even don't know where can we add our own codes.", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 18190, "title": "How to implement fine-tune training in blstm?", "body": "I just want a cnn network,blstm network,word embedding and implement them separately\r\nSave their params and use into a new network.\r\nI have tried tf.saver\\restore and tf.collection,it dosen't work.\r\nHow and I do that?\r\nthank u very much", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 18189, "title": "Merging 1.7.0 into master", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->"]}]