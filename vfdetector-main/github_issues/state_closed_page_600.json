[{"number": 35669, "title": "Add tests in TFLite micro for Max pooling Int8", "body": "I took some time to template and remove some duplicated code. Please let me know if that's ok", "comments": ["@giorgio-arenarm  Can you please resolve conflicts? Thanks!", "> @giorgio-arenarm Can you please resolve conflicts? Thanks!\r\n\r\nDone", "Thanks for the cleanup!"]}, {"number": 35668, "title": "the network is not learning anything - problem with Adam optimizer?", "body": "I customized this repository of Recurrent Models of Visual Attention (minimal) implementation on MNIST dataset to work with my custom RGB dataset, here is the forked repository: https://github.com/dusa2/RAM\r\n\r\nBasically I made minimal changes to make it work with RGB data, and I read in the data using the datagenerator, I have checked data input and shapes and it all seems okay.\r\n\r\nHowever, the network is not learning anything at all! I have a %50 %50 2 label dataset, and the network reward only moves around 0.5, the evaluation is around %50 and at first very little improvement is made (as it reads on the first epoch), but after that the network accuracy does not get better at all. I tried to lower the learning rate and nothing has changed. I suspect that the Adam optimizer does not update the learning rate either.\r\n\r\nIt all seems straightforward but I can not figure out why. I also tried another dataset (again custom) and it has the same behavior. How can I make sure that the optimizer is working as intended? What else might be the problem?\r\n\r\n", "comments": ["@dusa2 \r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary.\r\n\r\nRequest you to provide simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster.\r\n If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n\r\n\r\n\r\n", "@ravikyram I am using Ubuntu 16.04 python 3 tensorflow version 1.9.0 (binary install)\r\n\r\nwill try think of a way to add a standalone,  these could be useful as the source code is pretty minimal however:  \r\n\r\ntrain.txt format:\r\nfilename label\r\nvideo02_000000 0\r\nvideo03_000025 1\r\nvideo04_000050 0\r\n\r\nThe data is any a set of RGB images (jpeg format), the dataset is read in datagenerator lines 89-101 (path and folder structure needs to be customized)\r\n\r\nMine looks like:\r\n\r\n   \u251c\u2500 data\r\n   \u251c\u2500\u2500 images\r\n   \u2514\u2500\u2500\u2500\u2500video01\r\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500video01_000000.jpg\r\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500video01_000025.jpg\r\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500video01_000050.jpg\r\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500....\r\n   \u2514\u2500\u2500\u2500\u2500video02\r\n   \u2514\u2500\u2500\u2500\u2500video99\r\n....\r\n\r\n```\r\n    def _read_txt_file(self):\r\n        \"\"\"Read the content of the text file and store it into lists.\"\"\"\r\n        self.img_paths = []\r\n        self.labels = []\r\n        with open(self.txt_file, 'r') as f:\r\n            lines = f.readlines()\r\n            for line in lines:\r\n                items = line.split(' ')\r\n                print(items)\r\n                video = items[0].split('_')[0]\r\n                impath =  '/home/data/images/'+video+'/'+items[0] +'.jpg'\r\n                self.img_paths.append(impath)\r\n                self.labels.append(int(items[1]))\r\n\r\n```", "@dusa2 \r\nIs it possible for you to try latest TF version 1.15 and let us know whether the issue persists? There were lots of performance improvements in the latest versions. Thanks!", "@ravikyram I installed TF version 1.15 in a virtenv now I get this error:\r\n\r\n```\r\nimport tensorlayer as tl\r\nImportError: No module named 'tensorlayer'\r\n```\r\n\r\nso, I can't even import TF, I tried to look up for solutions but most suggests changing version?\r\n\r\nOne advised installing tensorlayer independently via `pip install tensorlayer`\r\nand this did not work because it seems to have installed TF layer 2.0:\r\n ```\r\n   \"TensorLayer does not support Tensorflow version older than 2.0.0.\\n\"\r\nRuntimeError: TensorLayer does not support Tensorflow version older than 2.0.0.\r\nPlease update Tensorflow with:\r\n - `pip install --upgrade tensorflow`\r\n - `pip install --upgrade tensorflow-gpu`\r\n```\r\n\r\nThis is how I installed TF btw:` pip install --pre \"tensorflow==1.15.*\"` (I also tried just `pip install \"tensorflow==1.15\"`)\r\n am I not getting the latest 1.15? what may be wrong?\r\n\r\nedit2: I can't install tensorlayer only with 1.15 either:\r\n```\r\npip install \"tensorlayer==1.15\"\r\nERROR: Could not find a version that satisfies the requirement tensorlayer==1.15 (from versions: 1.2, 1.2.1, 1.2.2, 1.2.4, 1.2.5, 1.2.6, 1.2.7, 1.2.8, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.3.4, 1.3.5, 1.3.6, 1.3.7, 1.3.8, 1.3.9, 1.3.10, 1.3.11, 1.4.1, 1.4.2, 1.4.3, 1.4.4, 1.4.5, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.6.0, 1.6.1, 1.6.2, 1.6.3, 1.6.4, 1.6.5, 1.6.6, 1.6.7, 1.7.0, 1.7.1, 1.7.2rc0, 1.7.2, 1.7.3, 1.7.4, 1.8.0rc0, 1.8.0, 1.8.1, 1.8.2, 1.8.3rc0, 1.8.3, 1.8.4rc0, 1.8.4rc1, 1.8.4, 1.8.5rc1, 1.8.5rc2, 1.8.5, 1.8.6rc0, 1.8.6rc1, 1.8.6rc2, 1.8.6rc3, 1.8.6rc4, 1.8.6rc5, 1.8.6rc6, 1.9.0, 1.9.1, 1.10.1rc0, 1.10.1, 1.11.0rc0, 1.11.0, 1.11.1, 2.0.0, 2.0.1, 2.0.2, 2.1.0)\r\nERROR: No matching distribution found for tensorlayer==1.15\r\n```\r\nSO I installed 1.11.1 (it warned me about some mismatch with dependent libraries)\r\nThe program runs (TF 1.15 and tensorlayer 1.11.1) but the problem remains!", "This is so curious, even for many epochs, it doesn't learn anything at all. Nothing. the loss / reward never changes", "Apologies for the delay in response.\r\nInstall pip version 19.0 or later for installing latest TF. See https://www.tensorflow.org/install/pip#system-requirements\r\nAlso this [article ]( https://stats.stackexchange.com/questions/352036/what-should-i-do-when-my-neural-network-doesnt-learn)may help."]}, {"number": 35667, "title": "assign() got an unexpected keyword argument 'validate_shape'", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n```\r\nimport tensorflow as tf\r\n\r\na = tf.Variable(2)\r\na.assign(5)\r\nassert a.numpy() == 5\r\n\r\n# ValueError: Shapes () and (2,) are incompatible\r\na.assign([1,2])  \r\n\r\n# TypeError: assign() got an unexpected keyword argument 'validate_shape'\r\na.assign([1,2], validate_shape=False)\r\n\r\n# ValueError: Shapes () and (2,) are incompatible\r\ntf.compat.v1.assign(a, [1,2], validate_shape=False)  \r\n\r\n```\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, Linux\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.0.0, 2.1.0\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\n\r\n`tf.assign` had a `validate_shape` parameter that `Variable.assign` seems to be missing.\r\n\r\nIn addition, the docs say:\r\n> If you want to change the shape of a variable later you have to use an `assign` Op with `validate_shape=False`.\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/Variable\r\n\r\nHow should one change the shape of a variable?\r\n\r\n**Code to reproduce the issue**\r\nSee above.\r\n", "comments": ["According to #30840, one now has to use `a = tf.Variable(2, Shape=tf.TensorShape(None))` (compare #26104 and https://github.com/tensorflow/tensorflow/commit/f9f2547b085969c67dc140aaa230551b6471a956).\r\n\r\nStill, this looks like a documentation issue: \r\n> you have to use an `assign` Op with `validate_shape=False`.\r\n\r\nis incorrect and should be replaced by\r\n\r\n> this is only possible if the variable has been initiliazed with `shape=TensorShape(None)` .", "> Still, this looks like a documentation issue:\r\n> \r\n>> you have to use an assign Op with validate_shape=False.\r\n> \r\n> is incorrect and should be replaced by\r\n> \r\n> >this is only possible if the variable has been initiliazed with shape=TensorShape(None) .\r\n> \r\n\r\n@MarkDaoust  Can I open a PR as my first contribution that makes this change?", "@bersbersbers,\r\nThe documentation for [tf.Variable](https://www.tensorflow.org/api_docs/python/tf/Variable#assign) has been updated and the above mentioned incorrect statement no longer exists.\r\n\r\nClosing this issue as it is resolved. Please feel free to re-open if mistaken. Thanks!"]}, {"number": 35666, "title": "[ROCm] Support for complex type BLAS operations", "body": "This PR enables complex-type BLAS operations GEMM, GEMV, and SCAL on the ROCm platform, and  does some housekeeping (e.g. corrects misspellings) in stream_executor/rocm/rocm_blas.cc.", "comments": ["@ekuznetsov139 , please fix the linter errors in `Ubuntu Sanity`", "@ekuznetsov139 Can you please resolve conflicts? Thanks!", "gentle ping"]}, {"number": 35665, "title": "tf.keras.models.load_model unable to use output layer names specified for loss_weights", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64-Bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v2 via conda install tensorflow-gpu cudnn=7.3.1=cuda9.0_0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: cudnn=7.3.1=cuda9.0_0\r\n- GPU model and memory: NVIDIA Quadro M1200\r\n\r\n**Describe the current behavior**\r\nI am using tf.keras functional APIs to create a multi-output model and to my compile call, I provide loss weights specific to my output names:\r\n\r\n```\r\nmy_model.compile(optimizer=optimizer, metrics=['accuracy']\r\n                      ,loss='categorical_crossentropy'\r\n                      ,loss_weights={'sec': 0.25, 'cls': 0.35, 'subcls': 0.4}\r\n                     )\r\n```\r\n\r\nModel compilation, fitment all work and I'm able to use model object to predict, evaluate etc. I'm also able to save model in 'tf' format. The save for 'hdf5' format doesn't work and gives OOM error, but that's ok for me now. The code to save is:\r\n\r\n`model.save(r'./models/hf_s100/')`\r\n\r\nThe problem I face is when I try to load it via:\r\n\r\n`hf_model = keras.models.load_model(r'./models/hf_s100/')`\r\n\r\nI get an error that loss weights found to be 'sec', 'cls' and 'subcls' and was expecting 'output_1', 'output_2', 'output_3'\r\n\r\n**Describe the expected behavior**\r\nGiven that we are allowed to name our output layers and use the names in loss_weights option, the load_model should also be able to pick the name specified as part of model save.\r\n\r\n**Code to reproduce the issue**\r\nSee code snippets provided above", "comments": ["@SinghB \r\n\r\nCan you please share simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@SinghB \r\n\r\nAny update on this issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 35664, "title": "Register a custom tensorflow operation that could be used in tensorflow C++, which itself uses some existing tensorflow operation", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Professional\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.14.0\r\n- Python version: 3.6.6\r\n- Installed using virtualenv? pip? conda?: -\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: - \r\n\r\n\r\n\r\n**Describe the problem**\r\nI\u2019m trying to register a custom tensorflow operation that I could later use in C++.\r\nThe operation is required to use an already existing tensorflow operation as-\r\n\r\n`Output output1 = tensorflow::ops::DeepCopy(scope, tensor);`\r\n\r\nI\u2019ve created custom_symbolic_gradients.cc file that contains following as shown in official documentation of tensorflow to create a custom op (https://www.tensorflow.org/guide/create_op )-\r\n1. Defining the op interface \r\n2. Implementing the kernel for the op\r\n3. Registering it with tensorflow kernel\r\n\r\nI\u2019ve placed `custom_symbolic_gradients.cc` in following directory:\r\n`//tensorflow/core/user_ops/custom_symbolic_gradients.cc`\r\n\r\nWhen I try to build this tensorflow package with bazel, the following problem arises-\r\n`custom_symbolic_gradients.cc` includes a header file `tensorflow/cc/ops/standard_ops.h` (I had to include it because `DeepCopy `is declared in this header file), which further contains more headers (`tensorlfow/cc/ops/array_ops.h`, `tensorflow/cc/ops/candidate_sampling_ops.h`, and so on...) but these headers are not pre existing in tensorflow package and are generated while building tensorflow with bazel.\r\n\r\nSo, while building tensorflow with bazel with the following command-\r\n`bazel build -c opt //tensorflow:libtensorflow_cc.so` \r\n\r\nI\u2019m getting the following error-\r\n`.\\tensorflow/cc/ops/standard_ops.h(19): fatal error C1083: Cannot open include file: 'tensorflow/cc/ops/array_ops.h': No such file or directory\r\n`\r\nI\u2019ve also tried it with different targets but still get the same error.\r\n\r\nLater, I tried it building as incremental build, I first built tensorflow using bazel successfully without adding any custom tensorflow operation, and then tried to build it again after adding custom_symbolic_gradients.cc file in the `tensorflow/core/user_ops/` directory, then I get the following error - \r\n```\r\nundeclared inclusion(s) in rule '//tensorflow/core:user_ops_op_lib':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/user_ops/custom_symbolic_gradients.cc':\r\n  'bazel-out/x64_windows-opt/genfiles/tensorflow/cc/ops/array_ops.h'\r\nTarget //tensorflow:libtensorflow_cc.so failed to build\r\n\r\n```\r\nAny idea that I can use to resolve this issue?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel build -c opt //tensorflow:libtensorflow_cc.so \r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n[command.log](https://github.com/tensorflow/tensorflow/files/4034281/command.log)\r\n", "comments": ["@mohitsaini-vgroup please post this question in stackoverflow as github is only meant for bug/performance, build/install, feature request or doc related issues. Thanks!"]}, {"number": 35663, "title": "ModuleNotFoundError: No module named 'tflite_runtime'", "body": "Hi,\r\n\r\nI am working a x86 Laptop and have installed tensorflow using https://www.tensorflow.org/lite/guide/python\r\nFollowing is the list of tflite  installed\r\n```\r\n\r\nankit@HP:~$ pip3 list | grep tflite\r\ntflite                        1.15.0                       \r\ntflite-runtime                1.14.0 \r\n```    \r\n\r\nmy aim is to get a Google coral example running from this link https://coral.ai/docs/accelerator/get-started/#3-run-a-model-using-the-tensorflow-lite-api\r\n\r\nWhen I execute the command for inference I get the folllowing error\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"classify_image.py\", line 36, in <module>\r\n    import tflite_runtime.interpreter as tflite\r\nModuleNotFoundError: No module named 'tflite_runtime'\r\n\r\n```\r\n\r\n\r\nIs there anything else that I need to install. I already have installed the libedgetpu1-std.\r\n", "comments": ["Maybe you can use `import tflite-runtime.interpreter as tflite` instead of ` import tflite_runtime.interpreter as tflite` .", "What exactly are you using to run the inference command?\r\nAre you sure that is the same version of python? i.e. if you are using pip3 but then using python you will probably be using python2 which doesn't have it installed. ", "> Maybe you can use `import tflite-runtime.interpreter as tflite` instead of ` import tflite_runtime.interpreter as tflite` .\r\n\r\nThis results in syntax error", "> What exactly are you using to run the inference command?\r\n> Are you sure that is the same version of python? i.e. if you are using pip3 but then using python you will probably be using python2 which doesn't have it installed.\r\n\r\nI am using python3 command as follows (same is mentioned on the example in website link given before)\r\npython3 classify_image.py --model models/mobilenet_v2_1.0_224_inat_bird_quant_edgetpu.tflite --labels models/inat_bird_labels.txt --input images/parrot.jpg\r\n\r\nI am not working with python2 and I am sure of that.\r\nActually the example was working initially (For this I had never installed anything related to python or tensorflow. It was a handover of laptop with all of this done).\r\nHowever I was perfoming a task to convert frozen model (.pb) to tflite model (.tflite) and ended up uninstalling and reinstalling different versions of tensorflow and messed up the system\r\nAs I am newbie to python and tensorflow I was not aware about conda environments and did everything on the base.\r\nHowever I have now set up a conda environment with python3.6 and tensorflow as mentioned. but the example faills at tflite_runtime.", "Try making a new conda environment and installing tf_runtime again. Show the whole log of commands you wrote and what happened. You can look inside the conda directories site-packages to see if tf_runtime files are there. You can run python3.6 interactively and look at sys.path to see what paths are searched for imports.", "Hi\r\nI have created a new conda environment with python=3.5\r\nI have also loaded the tflite as follows\r\n\r\n```\r\n(py35) ankit@HP:~/Desktop/coral/tflite/python/examples/classification$ sudo pip3 install /home/ankit/tflite_runtime-1.14.0-cp35-cp35m-linux_x86_64.whl \r\nWARNING: The directory '/home/ankit/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\r\nWARNING: The directory '/home/ankit/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\r\nProcessing /home/ankit/tflite_runtime-1.14.0-cp35-cp35m-linux_x86_64.whl\r\nInstalling collected packages: tflite-runtime\r\nSuccessfully installed tflite-runtime-1.14.0\r\n```\r\n\r\n\r\nhowever I am still getting the same error.\r\n```\r\n(py35) ankit@HP:~/Desktop/coral/tflite/python/examples/classification$ python3 classify_image.py --model models/mobilenet_v2_1.0_224_inat_bird_quant_edgetpu.tflite --labels models/inat_bird_labels.txt --input images/parrot.jpg\r\nTraceback (most recent call last):\r\n  File \"classify_image.py\", line 36, in <module>\r\n    import tflite_runtime.interpreter as tflite\r\nImportError: No module named 'tflite_runtime'\r\n```\r\n\r\nAfter careful investigation I observed that the tflite_runtime module was not getting loaded in the env lib path but the base /usr/local/lib.\r\n\r\nResolved it by copying the tflite_runtime folders in my conda env.\r\n\r\n\r\nI now have a query that why pip3 install command is not installing the module in the conda environment that I am working, instead its installed in the base environment.", "Ah, it makes sense from your commands what happened\r\nwhen you run with sudo pip install you install in /usr/local/lbi\r\n\r\n\r\nconda create --name myenv\r\nconda activate myenv\r\npip install tensorflow (or whatever)\r\n\r\nThe point is that you need to activate the conda environment. Then you don't need \"sudo pip\"\r\n", "I was working in the environment only. \r\n\r\nHowever if I do not use sudo it was giving me Permission denied error. Hence I had to use sudo with the pip install command", "So is tflite working only with conda and not venv?", "I confirm tflite 2.1.0 is working on venv. Tested on an RPI4.", "I am facing this issue on TFLite on Pi Zero.\r\n\r\nSystem information\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian GNU/Linux 10 (buster)\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Raspberry Pi Zero\r\nTensorFlow installed from (source or binary): Compile natively on Raspberry Pi using https://www.tensorflow.org/lite/guide/build_rpi\r\nTensorFlow version: Trying to install and Tensorflow lite as a standalone\r\nPython version: 3.7.3\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source): None\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\nDescribe the problem\r\nTrying to install Tensorflow Lite on RPI Zero as a standalone. However once installation is complete as per the following steps,\r\nsudo apt-get install build-essential\r\ngit clone https://github.com/tensorflow/tensorflow.git tensorflow_src\r\ncd tensorflow_src && ./tensorflow/lite/tools/make/download_dependencies.sh\r\n./tensorflow/lite/tools/make/build_rpi_lib.sh\r\n\r\nI can not import,\r\nimport tflite_runtime.interpreter as tflite\r\n\r\nError I am getting is\r\nModuleNotFoundError: No module named 'tflite_runtime'\r\n\r\nIn addition as per the instructions, I should see a static library. I do not see this either.\r\ntensorflow/lite/tools/make/gen/lib/rpi_armv6/libtensorflow-lite.a", "> \r\n> \r\n> I am facing this issue on TFLite on Pi Zero.\r\n> \r\n> System information\r\n> \r\n> OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian GNU/Linux 10 (buster)\r\n> Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Raspberry Pi Zero\r\n> TensorFlow installed from (source or binary): Compile natively on Raspberry Pi using https://www.tensorflow.org/lite/guide/build_rpi\r\n> TensorFlow version: Trying to install and Tensorflow lite as a standalone\r\n> Python version: 3.7.3\r\n> Installed using virtualenv? pip? conda?:\r\n> Bazel version (if compiling from source): None\r\n> GCC/Compiler version (if compiling from source):\r\n> CUDA/cuDNN version:\r\n> GPU model and memory:\r\n> Describe the problem\r\n> Trying to install Tensorflow Lite on RPI Zero as a standalone. However once installation is complete as per the following steps,\r\n> sudo apt-get install build-essential\r\n> git clone https://github.com/tensorflow/tensorflow.git tensorflow_src\r\n> cd tensorflow_src && ./tensorflow/lite/tools/make/download_dependencies.sh\r\n> ./tensorflow/lite/tools/make/build_rpi_lib.sh\r\n> \r\n> I can not import,\r\n> import tflite_runtime.interpreter as tflite\r\n> \r\n> Error I am getting is\r\n> ModuleNotFoundError: No module named 'tflite_runtime'\r\n> \r\n> In addition as per the instructions, I should see a static library. I do not see this either.\r\n> tensorflow/lite/tools/make/gen/lib/rpi_armv6/libtensorflow-lite.a\r\n\r\nSame Problem here, could someone help us? ", "\r\nDepending of your environment download the right **tflite** version from: [https://www.tensorflow.org/lite/guide/python](https://www.tensorflow.org/lite/guide/python)\r\nExample for Windows 10 64bits Python 3.6: https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp36-cp36m-win_amd64.whl\r\nAfter than just run (changing the path [D:\\Download] to your download directory):\r\n**pip install D:\\Downloads\\tflite_runtime-2.1.0.post1-cp36-cp36m-win_amd64.whl**", "> Depending of your environment download the right **tflite** version from: https://www.tensorflow.org/lite/guide/python\r\n> Example for Windows 10 64bits Python 3.6: https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp36-cp36m-win_amd64.whl\r\n> After than just run (changing the path [D:\\Download] to your download directory):\r\n> **pip install D:\\Downloads\\tflite_runtime-2.1.0.post1-cp36-cp36m-win_amd64.whl**\r\n\r\nAs I mentioned earlier, I am trying for TFLite standalone on RPi Zero. I have checked all links and they do not provide for RPi Zero which has arm6 architecture.", "> > Depending of your environment download the right **tflite** version from: https://www.tensorflow.org/lite/guide/python\r\n> > Example for Windows 10 64bits Python 3.6: https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp36-cp36m-win_amd64.whl\r\n> > After than just run (changing the path [D:\\Download] to your download directory):\r\n> > **pip install D:\\Downloads\\tflite_runtime-2.1.0.post1-cp36-cp36m-win_amd64.whl**\r\n> \r\n> As I mentioned earlier, I am trying for TFLite standalone on RPi Zero. I have checked all links and they do not provide for RPi Zero which has arm6 architecture.\r\n\r\nI'm answering the original question for a x86 architecture. If the help provided here doesn't matches your use case please open a new thread and post it there, just to avoid any misunderstanding."]}, {"number": 35662, "title": "Cannot build from source, get error: command succeeded, but there were errors parsing the target pattern", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0\r\n- Python version: 3.7.4\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version:CUDA 10.0/cuDNN7.4\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n I want to get rid of the message like \r\n\r\n>Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n\r\nTherefore, I build TF2.0 following the [guide](https://www.tensorflow.org/install/source) on the official site. Here's my configuration\r\n\r\n```\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: n\r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.0 in:\r\n    /usr/local/cuda/lib64\r\n    /usr/local/cuda/include\r\nFound cuDNN 7 in:\r\n    /usr/local/cuda/lib64\r\n    /usr/local/cuda/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 7.5]:\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: n\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:\r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: n\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]:\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=gdr            # Build with GDR support.\r\n        --config=verbs          # Build with libverbs support.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=noignite       # Disable Apache Ignite support.\r\n        --config=nokafka        # Disable Apache Kafka support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n```\r\nI use the following command to build\r\n```\r\nbazel build -c opt \u2014 copt=-mavx \u2014 copt=-mavx2 \u2014 copt=-mfma \u2014 copt=-mfpmath=both \u2014 copt=-msse4.2 -k //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:308:16:   required from \u2018void EigenForTFLite::internal::TensorContractionKernel<ResScalar, LhsScalar, RhsScalar, StorageIndex, OutputMapper, LhsMapper, RhsMappe\r\nr>::packRhs(RhsScalar**, const typename RhsMapper::SubMapper&, StorageIndex, StorageIndex) [with ResScalar = float; LhsScalar = float; RhsScalar = float; StorageIndex = long int; OutputMapper = EigenForTFLite::internal::blas_data_mapper<f\r\nloat, long int, 0, 0>; LhsMapper = EigenForTFLite::internal::TensorContractionInputMapper<float, long int, 1, EigenForTFLite::TensorEvaluator<const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenFo\r\nrTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, long int>, 16> >, EigenForTFLite::ThreadPoolDevice>, std::array<long int, 1>, std::array<long int, 1>, 4, true, false, 0, EigenForTFLite::MakePointer>; RhsMapper = EigenForTFLit\r\ne::internal::TensorContractionInputMapper<float, long int, 0, EigenForTFLite::TensorEvaluator<const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorImagePatchOp<-1, -1, const EigenF\r\norTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, long int>, 16> > >, EigenForTFLite::ThreadPoolDevice>, std::array<long int, 1>, std::array<long int, 1>, 4, true, false, 0, EigenForTFLite::MakePointer>; EigenForTFLite::intern\r\nal::TensorContractionKernel<ResScalar, LhsScalar, RhsScalar, StorageIndex, OutputMapper, LhsMapper, RhsMapper>::RhsBlock = float*; typename RhsMapper::SubMapper = EigenForTFLite::internal::TensorContractionSubMapper<float, long int, 0, Ei\r\ngenForTFLite::TensorEvaluator<const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorImagePatchOp<-1, -1, const EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, lo\r\nng int>, 16> > >, EigenForTFLite::ThreadPoolDevice>, std::array<long int, 1>, std::array<long int, 1>, 4, true, false, 0, EigenForTFLite::MakePointer>]\u2019\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:898:11:   required from \u2018void EigenForTFLite::TensorContractionEvaluatorBase<Derived>::evalGemmPartial(EigenForTFLite::TensorContractionEvaluatorBase<Derived>::\r\nScalar*, EigenForTFLite::TensorContractionEvaluatorBase<Derived>::Index, EigenForTFLite::TensorContractionEvaluatorBase<Derived>::Index, int) const [with bool lhs_inner_dim_contiguous = true; bool rhs_inner_dim_contiguous = true; bool rhs\r\n_inner_dim_reordered = false; int Alignment = 0; bool use_output_kernel = false; Derived = EigenForTFLite::TensorEvaluator<const EigenForTFLite::TensorContractionOp<const std::array<EigenForTFLite::IndexPair<long int>, 1>, const EigenForT\r\nFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorImagePatchOp<-1, -1, const EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, long int>, 16> > >, const EigenForTFLite::Tenso\r\nrReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, long int>, 16> >, const EigenForTFLite::NoOpOutputKernel>, EigenForTFLite::ThreadPoolDevice>; EigenForTFLite\r\n::TensorContractionEvaluatorBase<Derived>::Scalar = float; EigenForTFLite::TensorContractionEvaluatorBase<Derived>::Index = long int]\u2019\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:813:37:   [ skipping 7 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h:175:44:   required from \u2018bool EigenForTFLite::TensorEvaluator<const EigenForTFLite::TensorReshapingOp<NewDimensions, XprType>, Device>::evalSubExprsIfNeeded(EigenF\r\norTFLite::TensorEvaluator<const EigenForTFLite::TensorReshapingOp<NewDimensions, XprType>, Device>::EvaluatorPointerType) [with NewDimensions = const EigenForTFLite::DSizes<long int, 4>; ArgType = const EigenForTFLite::TensorContractionOp\r\n<const std::array<EigenForTFLite::IndexPair<long int>, 1>, const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorImagePatchOp<-1, -1, const EigenForTFLite::TensorMap<EigenForTFLite:\r\n:Tensor<const float, 4, 1, long int>, 16> > >, const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, long int>, 16> >, const EigenForTF\r\nLite::NoOpOutputKernel>; Device = EigenForTFLite::ThreadPoolDevice; EigenForTFLite::TensorEvaluator<const EigenForTFLite::TensorReshapingOp<NewDimensions, XprType>, Device>::EvaluatorPointerType = float*]\u2019\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:148:62:   required from \u2018bool EigenForTFLite::TensorEvaluator<const EigenForTFLite::TensorAssignOp<LhsXprType, RhsXprType>, Device>::evalSubExprsIfNeeded(EigenForTFL\r\nite::TensorEvaluator<const EigenForTFLite::TensorAssignOp<LhsXprType, RhsXprType>, Device>::EvaluatorPointerType) [with LeftArgType = EigenForTFLite::TensorMap<EigenForTFLite::Tensor<float, 4, 1, long int>, 16>; RightArgType = const Eigen\r\nForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 4>, const EigenForTFLite::TensorContractionOp<const std::array<EigenForTFLite::IndexPair<long int>, 1>, const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSiz\r\nes<long int, 2>, const EigenForTFLite::TensorImagePatchOp<-1, -1, const EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, long int>, 16> > >, const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int,\r\n 2>, const EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, long int>, 16> >, const EigenForTFLite::NoOpOutputKernel> >; Device = EigenForTFLite::ThreadPoolDevice; EigenForTFLite::TensorEvaluator<const EigenForTFLite::T\r\nensorAssignOp<LhsXprType, RhsXprType>, Device>::EvaluatorPointerType = float*]\u2019\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:277:16:   required from \u2018static void EigenForTFLite::internal::TensorExecutor<Expression, EigenForTFLite::ThreadPoolDevice, Vectorizable, Tileable>::run(const Expr\r\nession&, const EigenForTFLite::ThreadPoolDevice&) [with Expression = const EigenForTFLite::TensorAssignOp<EigenForTFLite::TensorMap<EigenForTFLite::Tensor<float, 4, 1, long int>, 16>, const EigenForTFLite::TensorReshapingOp<const EigenFor\r\nTFLite::DSizes<long int, 4>, const EigenForTFLite::TensorContractionOp<const std::array<EigenForTFLite::IndexPair<long int>, 1>, const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::Tens\r\norImagePatchOp<-1, -1, const EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, long int>, 16> > >, const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorMap<EigenF\r\norTFLite::Tensor<const float, 4, 1, long int>, 16> >, const EigenForTFLite::NoOpOutputKernel> > >; bool Vectorizable = true; bool Tileable = false]\u2019\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDevice.h:35:62:   required from \u2018EigenForTFLite::TensorDevice<ExpressionType, DeviceType>& EigenForTFLite::TensorDevice<ExpressionType, DeviceType>::operator=(const OtherDeri\r\nved&) [with OtherDerived = EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 4>, const EigenForTFLite::TensorContractionOp<const std::array<EigenForTFLite::IndexPair<long int>, 1>, const EigenForTFLite::TensorReshap\r\ningOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorImagePatchOp<-1, -1, const EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, long int>, 16> > >, const EigenForTFLite::TensorReshapingOp<const\r\nEigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, long int>, 16> >, const EigenForTFLite::NoOpOutputKernel> >; ExpressionType = EigenForTFLite::TensorMap<EigenForTFLite::Tensor<\r\nfloat, 4, 1, long int>, 16>; DeviceType = EigenForTFLite::ThreadPoolDevice]\u2019\r\n./tensorflow/lite/kernels/internal/optimized/multithreaded_conv.h:128:29:   required from \u2018void tflite::multithreaded_ops::EigenTensorConvFunctor<T>::operator()(const EigenForTFLite::ThreadPoolDevice&, const T*, int, int, int, int, const\r\nT*, int, int, int, int, int, int, int, tflite::PaddingType, T*, int, int) [with T = float]\u2019\r\n./tensorflow/lite/kernels/internal/optimized/multithreaded_conv.h:169:65:   required from here\r\n./tensorflow/core/kernels/eigen_spatial_convolutions-inl.h:600:15: warning: ignoring attributes on template argument \u2018EigenForTFLite::internal::packet_traits<float>::type {aka __vector(4) float}\u2019 [-Wignored-attributes]\r\n     const int packetSize = internal::unpacket_traits<Packet>::size;\r\n               ^~~~~~~~~~\r\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\nERROR: command succeeded, but there were errors parsing the target pattern\r\nINFO: Elapsed time: 6524.563s, Critical Path: 252.51s\r\nINFO: 18226 processes: 18226 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "comments": ["@xlnwel,\r\nIs this still an issue?\r\n\r\nCould you please check if you are facing the same error with the latest stable version of TensorFlow v2.4.1. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I've managed to build TF2.4. Thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35662\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35662\">No</a>\n"]}, {"number": 35661, "title": "Bug when convert to tflite models.", "body": "**System information**\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04, GPU GTX 1660\r\nTensorFlow installed from (source or binary):pip\r\nTensorFlow version (use command below):2.1.0\r\nPython version:3.7\r\n\r\n**Describe the current behavior**\r\nConvert to tflite failed\r\n\r\n**Code to reproduce the issue:**\r\n\r\n```\r\nfrom tensorflow.python.ops import gen_math_ops\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.framework import sparse_tensor\r\nfrom tensorflow.python.framework import constant_op\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass RepeatLayers(tf.keras.layers.Layer):\r\n    def __init__(self, axis=0):\r\n        super(RepeatLayers, self).__init__()\r\n        self.axis = axis\r\n\r\n    def _all_dimensions(self, x):\r\n        if isinstance(x, ops.Tensor) and x.get_shape().ndims is not None:\r\n          return constant_op.constant(\r\n              np.arange(x.get_shape().ndims), dtype=tf.int32)\r\n        if (isinstance(x, sparse_tensor.SparseTensor) and x.dense_shape.get_shape().is_fully_defined()):\r\n          r = x.dense_shape.get_shape().dims[0].value\r\n          return constant_op.constant(tf.arange(r), dtype=tf.int32)\r\n\r\n        return gen_math_ops._range(0, rank(x), 1)\r\n    \r\n    def _tile_one_dimension(self, data, axis, multiple):\r\n        if data.shape.ndims is not None:\r\n          multiples = [1] * data.shape.ndims\r\n          multiples[axis] = multiple\r\n        else:\r\n          ones_value = tf.ones(tf.rank(data), tf.int32)\r\n          multiples = tf.concat([ones_value[:axis], [multiple], ones_value[axis + 1:]],\r\n                           axis=0)\r\n      \r\n        return tf.tile(data, multiples)\r\n\r\n    def repeat_with_axis(self, data, repeats, axis):\r\n        data = tf.convert_to_tensor(data, name='data') # [B, max_len, d]\r\n        repeats = tf.cast(tf.convert_to_tensor(repeats, name='repeats'), tf.int32) # [B, max_len]\r\n\r\n        data_shape = tf.shape(data)\r\n\r\n        max_repeat = gen_math_ops.maximum(0, gen_math_ops._max(repeats, self._all_dimensions(repeats)))\r\n        mask = tf.sequence_mask(repeats, max_repeat) # [B, max_len, max_value_of_repeat]\r\n\r\n        expanded = tf.expand_dims(data, axis+1) # [B, max_len, 1, d]\r\n        tiled = self._tile_one_dimension(expanded, axis+1, max_repeat) # [B, max_len, max_value_of_repeat, d]\r\n\r\n        masked = tf.boolean_mask(tiled, mask) \r\n        result_shape = tf.concat([data_shape[:axis], [-1], data_shape[axis + 1:]], axis=0)\r\n        result = tf.reshape(masked, result_shape)\r\n\r\n        return result\r\n\r\n    def call(self, encoder_h, repeats):\r\n        return self.repeat_with_axis(data=encoder_h, repeats=repeats, axis=self.axis)\r\n\r\n\r\nrepeat = RepeatLayers(axis=1)\r\n\r\na = tf.keras.Input(shape=[35, 384], dtype=tf.float32)\r\nb = tf.keras.Input(shape=[35], dtype=tf.int32)\r\n\r\noutput = repeat(a, b)\r\n\r\nmodel = tf.keras.models.Model([a,b], outputs=output)\r\n\r\n\r\nmodel.summary()\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\r\ntflite_model = converter.convert()\r\n\r\n\r\n\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\ninterpreter.allocate_tensors()\r\n\r\n\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\n\r\ninput_shape_1 = input_details[0]['shape']\r\ninput_shape_2 = input_details[1]['shape']\r\ninput_data_1 = np.array(np.random.random_sample(input_shape_1), dtype=np.float32)\r\ninput_data_2 = np.array(np.random.random_sample(input_shape_2), dtype=np.int32)\r\n\r\n\r\ninterpreter.set_tensor(input_details[0]['index'], input_data_1)\r\ninterpreter.set_tensor(input_details[1]['index'], input_data_2)\r\n\r\n\r\n\r\ninterpreter.invoke()\r\n\r\ninterpreter.invoke()\r\n```\r\n\r\n**Other info / logs:**\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-13-7d35ed1dfe14> in <module>\r\n----> 1 interpreter.invoke()\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter.py in invoke(self)\r\n    491     \"\"\"\r\n    492     self._ensure_safe()\r\n--> 493     self._interpreter.Invoke()\r\n    494 \r\n    495   def reset_all_variables(self):\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py in Invoke(self)\r\n    111 \r\n    112     def Invoke(self):\r\n--> 113         return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_Invoke(self)\r\n    114 \r\n    115     def InputIndices(self):\r\n\r\nRuntimeError: tensorflow/lite/kernels/range.cc:39 (start > limit && delta < 0) || (start < limit && delta > 0) was not true.Node number 6 (RANGE) failed to invoke.\r\n```", "comments": ["Can you try setting `converter.experimental_new_converter = True` when converting the model and see if that fixes your error?", "> Can you try setting `converter.experimental_new_converter = True` when converting the model and see if that fixes your error?\r\n\r\nit's still error. @gargn ", "@renjie-liu I saw you implemented support for Range. Any idea where this error might be coming from?", "we don't allow things like\r\n\r\nrange(0, 0, 1), \r\n\r\ncan you change\r\n\r\n\"max_repeat = gen_math_ops.maximum(0, gen_math_ops._max(repeats, self._all_dimensions(repeats)))\"\r\n\r\nto \r\n\"max_repeat = gen_math_ops.maximum(1, gen_math_ops._max(repeats, self._all_dimensions(repeats)))\" ?", "> we don't allow things like\r\n> \r\n> range(0, 0, 1),\r\n> \r\n> can you change\r\n> \r\n> \"max_repeat = gen_math_ops.maximum(0, gen_math_ops._max(repeats, self._all_dimensions(repeats)))\"\r\n> \r\n> to\r\n> \"max_repeat = gen_math_ops.maximum(1, gen_math_ops._max(repeats, self._all_dimensions(repeats)))\" ?\r\n\r\nit occur another issue :D. @renjie-liu, seem because the last concat layers (tf.concat([data_shape[:axis], [-1], data_shape[axis + 1:]], axis=0))\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-7-c61c3de97313> in <module>\r\n     24 \r\n     25 \r\n---> 26 interpreter.invoke()\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter.py in invoke(self)\r\n    491     \"\"\"\r\n    492     self._ensure_safe()\r\n--> 493     self._interpreter.Invoke()\r\n    494 \r\n    495   def reset_all_variables(self):\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py in Invoke(self)\r\n    111 \r\n    112     def Invoke(self):\r\n--> 113         return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_Invoke(self)\r\n    114 \r\n    115     def InputIndices(self):\r\n\r\nRuntimeError: tensorflow/lite/kernels/concatenation.cc:85 output->type != input_type (1 != 2)Node number 20 (CONCATENATION) failed to prepare.\r\n```", "Do you think you can insert a cast op before the concat?\n\nThanks,\n\nOn Wed, Jan 15, 2020 at 12:04 PM dathudeptrai <notifications@github.com>\nwrote:\n\n> we don't allow things like\n>\n> range(0, 0, 1),\n>\n> can you change\n>\n> \"max_repeat = gen_math_ops.maximum(0, gen_math_ops._max(repeats,\n> self._all_dimensions(repeats)))\"\n>\n> to\n> \"max_repeat = gen_math_ops.maximum(1, gen_math_ops._max(repeats,\n> self._all_dimensions(repeats)))\" ?\n>\n> it occur another issue :D.\n>\n> ---------------------------------------------------------------------------\n> RuntimeError                              Traceback (most recent call last)\n> <ipython-input-7-c61c3de97313> in <module>\n>      24\n>      25\n> ---> 26 interpreter.invoke()\n>\n> ~/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter.py in invoke(self)\n>     491     \"\"\"\n>     492     self._ensure_safe()\n> --> 493     self._interpreter.Invoke()\n>     494\n>     495   def reset_all_variables(self):\n>\n> ~/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py in Invoke(self)\n>     111\n>     112     def Invoke(self):\n> --> 113         return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_Invoke(self)\n>     114\n>     115     def InputIndices(self):\n>\n> RuntimeError: tensorflow/lite/kernels/concatenation.cc:85 output->type != input_type (1 != 2)Node number 20 (CONCATENATION) failed to prepare.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35661?email_source=notifications&email_token=AIURNGME2VM3FDBT2JPGRCTQ52DLLA5CNFSM4KEDGWOKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEI67LOA#issuecomment-574485944>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AIURNGKVYCJH6WW6O5FQRPDQ52DLLANCNFSM4KEDGWOA>\n> .\n>\n\n\n-- \nRenjie Liu\n\nrenjieliu@google.com\n+1 (650) 253-4359\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35661\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35661\">No</a>\n", "Still same error @renjie-liu, i cast it to tf.int32. I just checked and the bug come from \"masked = tf.boolean_mask(tiled, mask)\"", "Can you try set\n\n\"converter.experimental_new_converter = True\" ?\n\nThanks,\n\nOn Wed, Jan 15, 2020 at 2:14 PM dathudeptrai <notifications@github.com>\nwrote:\n\n> Still same error @renjie-liu <https://github.com/renjie-liu>, i cast it\n> to tf.int32.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35661?email_source=notifications&email_token=AIURNGPENCS3GJT23M6IK33Q52SUVA5CNFSM4KEDGWOKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEI7F5CA#issuecomment-574512776>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AIURNGIW3GUSMRZ7FR2NQALQ52SUVANCNFSM4KEDGWOA>\n> .\n>\n\n\n-- \nRenjie Liu\n\nrenjieliu@google.com\n+1 (650) 253-4359\n", "Yeah @renjie-liu , that ok, the interpreter.invoke() run successfully. But when i try \"output_data = interpreter.get_tensor(output_details[0]['index'])\" i got error.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-9-799dc117df9e> in <module>\r\n----> 1 output_data = interpreter.get_tensor(output_details[0]['index'])\r\n      2 print(output_data)\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter.py in get_tensor(self, tensor_index)\r\n    426       a numpy array.\r\n    427     \"\"\"\r\n--> 428     return self._interpreter.GetTensor(tensor_index)\r\n    429 \r\n    430   def tensor(self, tensor_index):\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py in GetTensor(self, i)\r\n    141 \r\n    142     def GetTensor(self, i):\r\n--> 143         return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_GetTensor(self, i)\r\n    144 \r\n    145     def ResetVariableTensors(self):\r\n\r\nValueError: Invalid tensor size.\r\n```", "Ok i run successfully, the input_2 have all value 0. Thank for ur support @renjie-liu @gargn. I will close issue :D.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35661\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35661\">No</a>\n", "@dathudeptrai can you please specify what you changed to solve the error? I am also facing the same problem.", "@dathudeptrai I meet the same problem too. Could you share how do you solve this?"]}, {"number": 35660, "title": "adamax on tf.keras gpu 1.14.0 frequently crashes with this error", "body": "optimizer='adamax' with tf.keras 1.14.0 frequently crashes with this error (training on gpu), it also frequently produces NaNs in training when not producing this error. Other optimizers are fine (no crashes).\r\n\r\n  File \".../.conda/envs/tensorflow-gpu_1.14.0/lib/python3.7/site-packages/tensorflow/python/keras/wrappers/scikit_learn.py\", line 166, in fit\r\n    history = self.model.fit(x, y, **fit_args)\r\n  File \".../.conda/envs/tensorflow-gpu_1.14.0/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 780, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \".../.conda/envs/tensorflow-gpu_1.14.0/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 419, in model_iteration\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File \".../.conda/envs/tensorflow-gpu_1.14.0/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\", line 311, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File \".../.conda/envs/tensorflow-gpu_1.14.0/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\", line 1247, in on_epoch_end\r\n    self.model.set_weights(self.best_weights)\r\n  File \".../.conda/envs/tensorflow-gpu_1.14.0/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1105, in set_weights\r\n    if len(params) != len(weights):\r\nTypeError: object of type 'NoneType' has no len()", "comments": ["@oxqfsyef ,\r\nCan you please provide a code to reproduce the issue reported here. Thanks!", "@oxqfsyef ,\r\nAny update on the issue?Thanks!", "Sorry, it was a generated model and I didn't note it. I removed Adamax so I don't have more examples.", "@oxqfsyef Untill and unless we have a concrete model to reproduce, I am afraid we cannot go ahead with this issue.\r\n\r\nTried it on colab but didn't face any error. Please find the gist [here](https://colab.research.google.com/gist/gowthamkpr/e1d5c45bdb52b0c309e6f5907402f7ca/beginner.ipynb). ", "Closing this issue as the error is not reproducible. Please add additional comments and we can open the issue again. Thanks!"]}, {"number": 35659, "title": "Update api_def_StringUpper.pbtxt", "body": "In relation to issue #35652 updated stringupper docs", "comments": ["Can you please merge this and #35658 to minimize amount of CI time spent in testing?"]}, {"number": 35658, "title": "Update api_def_StringLower.pbtxt", "body": "#35652 Pull request in relation to the mentioned issue.", "comments": ["Please follow documentation guide at https://www.tensorflow.org/community/contribute/docs_ref and make them testable docstrings.\r\n\r\nNote also the merging comment in #35659", "Rohan, do you want to additionally combine this as additional commits in #35647, or at least base your branch off of that commit? As-is, the summary fields you're adding conflict with the ones I'm proposing.\r\n\r\nThanks for elaborating further on the missing docs!", "Should I close this pull request and make another one?", "Close this and move work onto #35659 or close that one and move work onto this. Either is fine.", "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_Sin.pbtxt \r\nI followed the syntax of this file in making the pull request, can I know what are the changes I should be making?", "There is a conflict that needs to be solved", "I just tried to resolve the conflicts, I edited the file and commited it, It was taking a long time so I reloaded the page now I cant see the `Resolve Conflicts` link (button).", "Did you delete the fork?", "Yep, so should I close this and open another one?", "Yes please"]}, {"number": 35657, "title": "'AttentionWrapper' object has no attribute 'zero_state' ", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["@Neel125 ,\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "@oanush \r\nI am using tensorflow 2.0 and os ubuntu 16.04", "> @oanush\r\n> I am using tensorflow 2.0 and os ubuntu 16.04\r\n\r\n@Neel125 ,\r\nPlease provide complete information like what exactly the issue is,code used before facing error log, stack trace of error. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 35656, "title": "Custom bidirectional LSTM cell throws unexpected keyword argument 'name'", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nyes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nN/A - tested on colab only\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\n2.1.0-rc1\r\n- Python version:\r\n3.6.9\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nGCC 8.3.0\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nthrows unexpected keyword argument 'name' error\r\n\r\n**Describe the expected behavior**\r\nshould not throw error\r\n\r\n**Code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/17B_MLss_dWjJCBvyMRCWFga31Im2eesc\r\n\r\n**Other info / logs**\r\n```\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\n%tensorflow_version 2.x\r\n\r\nimport tensorflow as tf\r\nimport sys\r\n\r\nprint(tf.version.GIT_VERSION, tf.version.VERSION)\r\nprint(sys.version)\r\n\r\nclass TestLSTMCell(tf.keras.layers.LSTMCell):\r\n    def __init__(self, units):\r\n        # what's missing here to prevent \"TypeError: __init__() got an unexpected keyword argument 'name'\"\r\n        super(TestLSTMCell, self).__init__(units)\r\n\r\ninputs = tf.keras.Input((128, 256))\r\ncell = TestLSTMCell(512)\r\nforward_layer = tf.keras.layers.RNN(cell, return_state=True)\r\n\r\noutputs = forward_layer(inputs)\r\nprint(\"works !!!\")\r\n\r\nbidirectional_rnn = tf.keras.layers.Bidirectional(forward_layer, merge_mode=\"concat\")\r\nprint(\"nope\")\r\n```\r\n\r\nSee colab link for stack trace.", "comments": ["Issue replicating for TF versions 2.1 and tf-nightly. Thanks!", "When you define a subclass of `tf.keras.layers.Layer`, you should have its `__init__` method respect the API defined at parent level, which means you need to accept some kwargs (and pass them to the parent constructor through the `super` call).\r\n\r\nThis is because a number of arguments ('trainable', 'name', 'dtype' and 'dynamic') are part of the basic API but take default values, so that they are not explicitly listed in the `__init__` signature (they are documented, though : run `help(tf.keras.layers.Layer)`).\r\n\r\nIn effect:\r\n```python\r\nclass TestLSTMCell(tf.keras.layers.LSTMCell):\r\n    def __init__(self, units, **kwargs):\r\n        super(TestLSTMCell, self).__init__(units, **kwargs)\r\n```", "> ```python\r\n> **kwargs\r\n> ```\r\n\r\nThanks. Passing **kwargs resolved the error.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35656\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35656\">No</a>\n"]}, {"number": 35655, "title": "tensorflow master window build fail", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version:master\r\n- Python version:3.6\r\n- Installed using virtualenv? pip? conda?:conda\r\n- Bazel version (if compiling from source):1.1.0\r\n- GCC/Compiler version (if compiling from source):vs 2015\r\n- CUDA/cuDNN version:no cuda \r\n- GPU model and memory:no\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nuse below compile command:\r\npython ./configure.py\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\nbuild fail\r\n\r\n**Any other info / logs**\r\n\r\nmaster window compile fail\uff0cERROR: E:/work/work/gitlab/github/tensorflow/tensorflow/core/framework/BUILD:450:1: C++ compilation of rule '//tensorflow/core/framework:allocator_registry_impl' failed (Exit 2)\r\nc:\\users\\yinxungong\\_bazel_yinxungong\\4pfkvgbi\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src/Tensor/TensorBlock.h(950): error C2061: \u8bed\u6cd5\u9519\u8bef: \u6807\u8bc6\u7b26\u201cKind\u201d\r\nc:\\users\\yinxungong\\_bazel_yinxungong\\4pfkvgbi\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src/Tensor/TensorBlock.h(1056): note: \u53c2\u89c1\u5bf9\u6b63\u5728\u7f16\u8bd1\u7684\u7c7b \u6a21\u677f \u5b9e\u4f8b\u5316\u201cEigen::internal::StridedLinearBufferCopy<Scalar,IndexType>\u201d\u7684\u5f15\u7528\r\nc:\\users\\yinxungong\\_bazel_yinxungong\\4pfkvgbi\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src/Tensor/TensorBlock.h(959): error C2061: \u8bed\u6cd5\u9519\u8bef: \u6807\u8bc6\u7b26\u201cKind\u201d\r\n", "comments": ["I think you need MSVC2019. Can you try from a fresh clone of the repo? And if the issue still reproduces please ensure the following:\r\n\r\n* the error logs use ` ``` ` before and after so that they are readable\r\n* the error logs use English throughout the names\r\n* the commit hash you cloned at is also included in the report", "@yinxungong \r\n\r\nAny update on this issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35655\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35655\">No</a>\n"]}, {"number": 35654, "title": "LSTM & LSTMCell with dropout are not working in Subclassed Keras Model.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave, no GPU\r\n- TensorFlow installed from (source or binary):pip\r\n- TensorFlow version (use command below):2.0.0\r\n- Python version:3.7\r\n\r\n**Describe the current behavior**\r\n\r\nLSTM with dropout are not working in customized keras model. If the dropout is set to 0 the codes work.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import *\r\nimport numpy\r\n\r\nassert tf.__version__==\"2.0.0\", f\"Expect TF-2.0.0 but get {tf.__version__}\"\r\n\r\nclass multiplicative_attention_model(tf.keras.Model):\r\n\t\"\"\"\r\n\tA Tensorflow 2.0 implementation of encoder-decoder attention model as illustrated in Effective Approaches to Attention-based Neural Machine Translation.\r\n\tThe following types of attentions are implemented:\r\n\t\u2022 Global attention with dot alignment\r\n\t\u2022 Local attention with monotonic alignment\r\n\t\r\n\t# Arguments:\r\n\r\n\t\tattn_mode: `global` or `local`\r\n\t\tinput_shape: shape of input data, should be a two-dimensional tuple (batch, n_step)\r\n\t\tinput_vocab_size: size of input vocabulary size excluding <EOS>\r\n\t\toutput_vocab_size: size of output vocabulary size excluding <EOS>\r\n\t\tinput_embed_dim: word embedding dimension for input\r\n\t\thidden_state_dim: hidden state dimension for encoder and decoder LSTM\r\n\t\tlocal_attn_window: the context vector will be extracted from window [current_position \u00b1 local_attn_window]\r\n\t\tname: name of the model\r\n\r\n\t# Paper:\r\n\r\n\t\thttps://www-nlp.stanford.edu/pubs/emnlp15_attn.pdf\r\n\r\n\t# Examples\r\n\r\n\t```python\r\n\t\t# Expected shape for input data and output data\r\n\t\tattn_mode = \"global\"\r\n\t\tinput_shape = (None,40)\r\n\t\tinput_vocab_size = 200\r\n\t\toutput_vocab_size = 300\r\n\r\n\t\t# Initialize Model\r\n\t\tmodel = multiplicative_attention_model(attn_mode, input_shape, input_vocab_size, output_vocab_size)\r\n\r\n\t\t# Compile Model\r\n\t\tlearning_rate = 1e-3\r\n\t\tmodel.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n\t\t\t\t\t  optimizer=tf.keras.optimizers.Adam(learning_rate),\r\n\t\t\t\t\t  metrics=[tf.keras.metrics.sparse_categorical_accuracy]\r\n\t\t\t\t\t  )\r\n\r\n\t\t# Call Model once to build the model\r\n\t\t_ = model(tf.ones(shape=(1,40)))\r\n\r\n\t\t# Take a look at model\r\n\t\tmodel.summary()\r\n\t```\r\n\t\"\"\"\r\n\tdef __init__(self, attn_mode, input_shape, input_vocab_size, output_vocab_size, name=\"Multiplicative-Attention\", input_embed_dim=200, hidden_state_dim=200, local_attn_window=None, **kwargs):\r\n\t\tsuper(multiplicative_attention_model, self).__init__(name=name, **kwargs)\r\n\r\n\t\t# Expect attention model to be either global or local\r\n\t\tassert attn_mode == \"global\" or attn_mode == \"local\", f\"Expect 'global' or 'local' for attn_mode, get {attn_mode} instead\"\r\n\t\tself.attn_mode = attn_mode\r\n\r\n\t\t# Expect input shape to be two-dimensional\r\n\t\tassert isinstance(input_shape, tuple), \"Expect tuple for input_shape, get {type(input_shape)} instead\"\r\n\t\tassert len(input_shape) == 2, \"Expect 2-dim tuple for input_shape, get {len(input_shape)}-dim instead\"\r\n\t\t_, self._n_step = input_shape\r\n\r\n\t\t# Embedding layer for input\r\n\t\tself._embedding_layer = Embedding(input_dim=input_vocab_size+1, output_dim=input_embed_dim, input_length=self._n_step)\r\n\r\n\t\t# LSTM encoder\r\n\t\tself._lstm_encoder_layer_1 = LSTM(units=hidden_state_dim, dropout=0.2, recurrent_dropout=0.2, activation=\"tanh\", recurrent_activation=\"sigmoid\", use_bias=True, return_sequences=True, return_state=True)\r\n\t\tself._lstm_encoder_layer_2 = LSTM(units=hidden_state_dim, dropout=0.2, recurrent_dropout=0.2, activation=\"tanh\", recurrent_activation=\"sigmoid\", use_bias=True, return_sequences=True, return_state=True)\r\n\r\n\t\t# LSTM decoder\r\n\t\tself._lstm_decoder_cell_1 = LSTMCell(units=hidden_state_dim, dropout=0.2, recurrent_dropout=0.2, activation=\"tanh\", recurrent_activation=\"sigmoid\", use_bias=True)\r\n\t\tself._lstm_decoder_cell_2 = LSTMCell(units=hidden_state_dim, dropout=0.2, recurrent_dropout=0.2, activation=\"tanh\", recurrent_activation=\"sigmoid\", use_bias=True)\r\n\t\tself._decoder_state_array = tf.TensorArray(dtype=tf.float32, size=self._n_step, clear_after_read=True)\r\n\r\n\t\t# Attention utility\r\n\t\tif self.attn_mode == \"global\":\r\n\t\t\tpass\r\n\t\tif self.attn_mode == \"local\":\r\n\t\t\tassert isinstance(local_attn_window, int), f\"Expect integer for local_attn_window, get {type(local_attn_window)} instead\"\r\n\t\t\tassert local_attn_window > 0, \"Expect positive value for local_attn_window\"\r\n\t\t\tself.D = local_attn_window\r\n\r\n\t\t# Utility layer\r\n\t\tself._concat_layer = Concatenate()\r\n\r\n\t\t# Output layer\r\n\t\tself._ffnn_decoder_sequence_layer = TimeDistributed(Dense(output_vocab_size+1))\r\n\t\r\n\tdef call(self, inputs, training=False):\r\n\t\t# Embedding\r\n\t\tinputs_embed = self._embedding_layer(inputs)\r\n\r\n\t\t# Encoder\r\n\t\tencoder_1_sequence, encoder_1_hidden_state, encoder_1_cell_state = self._lstm_encoder_layer_1(inputs=inputs_embed, training=training)\r\n\t\tencoder_2_sequence, encoder_2_hidden_state, encoder_2_cell_state = self._lstm_encoder_layer_2(inputs=encoder_1_sequence, initial_state=[encoder_1_hidden_state, encoder_1_cell_state], training=training)\r\n\t\tdecoder_1_input = tf.transpose(encoder_2_sequence, [1, 0, 2])\r\n\t\tdecoder_1_states = [encoder_1_hidden_state, encoder_1_cell_state]\r\n\t\tdecoder_2_states = [encoder_2_hidden_state, encoder_2_cell_state]\r\n\r\n\t\t# Decoder\r\n\t\tfor step in range(self._n_step):\r\n\t\t\tdecoder_1_hidden_state, decoder_1_states = self._lstm_decoder_cell_1(inputs=decoder_1_input[step],states=decoder_1_states, training=training)\r\n\t\t\tdecoder_2_hidden_state, decoder_2_states = self._lstm_decoder_cell_2(inputs=decoder_1_hidden_state, states=decoder_2_states, training=training)\r\n\t\t\tif self.attn_mode == \"global\":\r\n\t\t\t\tattention_score = tf.nn.softmax(tf.einsum(\"ijk,ik->ij\", encoder_2_sequence, decoder_2_hidden_state))\r\n\t\t\t\tcontext_vector = tf.einsum(\"ijk,ij->ik\", encoder_2_sequence, attention_score)\r\n\t\t\tif self.attn_mode == \"local\":\r\n\t\t\t\tlb, ub = max(0, step-self.D), min(self._n_step-1,step+self.D)\r\n\t\t\t\tencoder_2_sequence_part = encoder_2_sequence[:,lb:ub+1,:]\r\n\t\t\t\tattention_score = tf.nn.softmax(tf.einsum(\"ijk,ik->ij\", encoder_2_sequence_part, decoder_2_hidden_state))\r\n\t\t\t\tcontext_vector = tf.einsum(\"ijk,ij->ik\", encoder_2_sequence_part, attention_score)\r\n\t\t\toutput_vector = self._concat_layer([decoder_2_hidden_state, context_vector])\r\n\t\t\tself._decoder_state_array = self._decoder_state_array.write(step,output_vector)\r\n\t\tdecoder_sequence = tf.transpose(self._decoder_state_array.stack(), [1, 0, 2])\r\n\r\n\t\t# Softmax outputs\r\n\t\toutputs = tf.nn.softmax(self._ffnn_decoder_sequence_layer(decoder_sequence))\r\n\t\treturn outputs\r\n\r\nattn_mode = \"local\"\r\ninput_shape = (None,20)\r\ninput_vocab_size = 200\r\noutput_vocab_size = 300\r\n\r\n# Initialize Model\r\nmodel = multiplicative_attention_model(attn_mode, input_shape, input_vocab_size, output_vocab_size, local_attn_window=4)\r\n\r\n# Compile Model\r\nlearning_rate = 1e-2\r\nmodel.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n              optimizer=tf.keras.optimizers.Adam(learning_rate),\r\n              metrics=[tf.keras.metrics.sparse_categorical_accuracy]\r\n              )\r\n\r\nx = tf.constant(np.random.choice(range(201),size=(10000,20)))\r\ny = tf.constant(np.random.choice(range(301),size=(10000,20)))\r\n\r\nmodel.fit(x,y,batch_size=1024)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n~/.virtualenvs/python3env/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     60                                                op_name, inputs, attrs,\r\n---> 61                                                num_outputs)\r\n     62   except core._NotOkStatusException as e:\r\n\r\nTypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  @tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\nThe graph tensor has name: Multiplicative-Attention_6/lstm_cell_34/cond/Identity:0\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n_SymbolicException                        Traceback (most recent call last)\r\n<ipython-input-31-fab8afc8ed12> in <module>\r\n----> 1 model.fit(x,y,batch_size=64)\r\n\r\n~/.virtualenvs/python3env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    726         max_queue_size=max_queue_size,\r\n    727         workers=workers,\r\n--> 728         use_multiprocessing=use_multiprocessing)\r\n    729 \r\n    730   def evaluate(self,\r\n\r\n~/.virtualenvs/python3env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n    322                 mode=ModeKeys.TRAIN,\r\n    323                 training_context=training_context,\r\n--> 324                 total_epochs=epochs)\r\n    325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)\r\n    326 \r\n\r\n~/.virtualenvs/python3env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    121         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n    122       try:\r\n--> 123         batch_outs = execution_function(iterator)\r\n    124       except (StopIteration, errors.OutOfRangeError):\r\n    125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n\r\n~/.virtualenvs/python3env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)\r\n     84     # `numpy` translates Tensors to values in Eager mode.\r\n     85     return nest.map_structure(_non_none_constant_value,\r\n---> 86                               distributed_function(input_fn))\r\n     87 \r\n     88   return execution_function\r\n\r\n~/.virtualenvs/python3env/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    455 \r\n    456     tracing_count = self._get_tracing_count()\r\n--> 457     result = self._call(*args, **kwds)\r\n    458     if tracing_count == self._get_tracing_count():\r\n    459       self._call_counter.called_without_tracing()\r\n\r\n~/.virtualenvs/python3env/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    518         # Lifting succeeded, so variables are initialized and we can run the\r\n    519         # stateless function.\r\n--> 520         return self._stateless_fn(*args, **kwds)\r\n    521     else:\r\n    522       canon_args, canon_kwds = \\\r\n\r\n~/.virtualenvs/python3env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   1821     \"\"\"Calls a graph function specialized to the inputs.\"\"\"\r\n   1822     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 1823     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   1824 \r\n   1825   @property\r\n\r\n~/.virtualenvs/python3env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _filtered_call(self, args, kwargs)\r\n   1139          if isinstance(t, (ops.Tensor,\r\n   1140                            resource_variable_ops.BaseResourceVariable))),\r\n-> 1141         self.captured_inputs)\r\n   1142 \r\n   1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\n~/.virtualenvs/python3env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1222     if executing_eagerly:\r\n   1223       flat_outputs = forward_function.call(\r\n-> 1224           ctx, args, cancellation_manager=cancellation_manager)\r\n   1225     else:\r\n   1226       gradient_name = self._delayed_rewrite_functions.register()\r\n\r\n~/.virtualenvs/python3env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n    509               inputs=args,\r\n    510               attrs=(\"executor_type\", executor_type, \"config_proto\", config),\r\n--> 511               ctx=ctx)\r\n    512         else:\r\n    513           outputs = execute.execute_with_cancellation(\r\n\r\n~/.virtualenvs/python3env/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     73       raise core._SymbolicException(\r\n     74           \"Inputs to eager execution function cannot be Keras symbolic \"\r\n---> 75           \"tensors, but found {}\".format(keras_symbolic_tensors))\r\n     76     raise e\r\n     77   # pylint: enable=protected-access\r\n\r\n_SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'Multiplicative-Attention_6/lstm_cell_34/cond/Identity:0' shape=(None, 200) dtype=float32>, <tf.Tensor 'Multiplicative-Attention_6/lstm_cell_34/cond_4/Identity:0' shape=(None, 200) dtype=float32>, <tf.Tensor 'Multiplicative-Attention_6/lstm_cell_35/cond/Identity:0' shape=(None, 200) dtype=float32>, <tf.Tensor 'Multiplicative-Attention_6/lstm_cell_35/cond_4/Identity:0' shape=(None, 200) dtype=float32>]\r\n```", "comments": ["Issue replicating for TF-2.0.0-kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/b38d04d7bc3da967c0c8d89e318e225a/35654.ipynb) of colab.Thanks!", "Thanks for reporting the issue.\r\n\r\nThe problem happens when you use LSTMCell with dropouts. In tf2, LSTMcell cached the dropout mask to achieve the variational dropout for better accuracy. On the other hand, the cached dropout mask need to be reset for every batch, otherwise the following batch will try to use a mask from previous batch. We do the reset in the LSTM layer for user, but if you use the LSTMcell, then you have to reset them manually in the code. eg\r\n\r\n```\r\n# Decoder\r\nself._lstm_decoder_cell_1.reset_dropout_mask()\r\nself._lstm_decoder_cell_1.reset_recurrent_dropout_mask()\r\nself._lstm_decoder_cell_2.reset_dropout_mask()\r\nself._lstm_decoder_cell_2.reset_recurrent_dropout_mask()\r\n\r\nfor step in range(self._n_step):\r\n    .....\r\n```\r\nThen you code should run without issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35654\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35654\">No</a>\n"]}, {"number": 35653, "title": "tensorflow 2.1 build error", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary):  source 2.1\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.7.6\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 0.29.1\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version:  10.2/ 7.6.5\r\n- GPU model and memory: RTX2080Ti GDDR6 11GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nbazel build //tensorflow/tools/pip_package:build_pip_packagee\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nERROR: /home/wmind/repo/tensorflow/tensorflow/python/BUILD:1270:1: C++ compilation of rule '//tensorflow/python:_op_def_registry.so' failed (Exit 1)\r\nIn file included from bazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/pytypes.h:12:0,\r\n                 from bazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/cast.h:13,\r\n                 from bazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/attr.h:13,\r\n                 from bazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/pybind11.h:44,\r\n                 from ./tensorflow/python/lib/core/pybind11_status.h:21,\r\n                 from tensorflow/python/framework/op_def_registry.cc:20:\r\nbazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/detail/common.h:302:12: error: multiple definition of \u2018enum class pybind11::return_value_policy\u2019\r\n enum class return_value_policy : uint8_t {\r\n            ^~~~~~~~~~~~~~~~~~~\r\nIn file included from external/pybind11/include/pybind11/pytypes.h:12:0,\r\n                 from external/pybind11/include/pybind11/cast.h:13,\r\n                 from external/pybind11/include/pybind11/attr.h:13,\r\n                 from external/pybind11/include/pybind11/pybind11.h:49,\r\n                 from tensorflow/python/framework/op_def_registry.cc:16:\r\nexternal/pybind11/include/pybind11/detail/common.h:302:12: note: previous definition here\r\n enum class return_value_policy : uint8_t {\r\n            ^~~~~~~~~~~~~~~~~~~\r\nIn file included from bazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/pytypes.h:12:0,\r\n                 from bazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/cast.h:13,\r\n                 from bazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/attr.h:13,\r\n                 from bazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/pybind11.h:44,\r\n                 from ./tensorflow/python/lib/core/pybind11_status.h:21,\r\n                 from tensorflow/python/framework/op_def_registry.cc:20:\r\nbazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/detail/common.h: In function \u2018constexpr int pybind11::detail::log2(pybind11::size_t, int)\u2019:\r\nbazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/detail/common.h:355:29: error: redefinition of \u2018constexpr int pybind11::detail::log2(pybind11::size_t, int)\u2019\r\n inline static constexpr int log2(size_t n, int k = 0) { return (n <= 1) ? k : log2(n >> 1, k + 1); }\r\n                             ^~~~\r\nIn file included from external/pybind11/include/pybind11/pytypes.h:12:0,\r\n                 from external/pybind11/include/pybind11/cast.h:13,\r\n                 from external/pybind11/include/pybind11/attr.h:13,\r\n                 from external/pybind11/include/pybind11/pybind11.h:49,\r\n                 from tensorflow/python/framework/op_def_registry.cc:16:\r\nexternal/pybind11/include/pybind11/detail/common.h:355:29: note: \u2018constexpr int pybind11::detail::log2(pybind11::size_t, int)\u2019 previously defined here\r\n inline static constexpr int log2(size_t n, int k = 0) { return (n <= 1) ? k : log2(n >> 1, k + 1); }\r\n                             ^~~~\r\nIn file included from bazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/pytypes.h:12:0,\r\n                 from bazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/cast.h:13,\r\n                 from bazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/attr.h:13,\r\n                 from bazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/pybind11.h:44,\r\n                 from ./tensorflow/python/lib/core/pybind11_status.h:21,\r\n                 from tensorflow/python/framework/op_def_registry.cc:20:\r\nbazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/detail/common.h: In function \u2018constexpr pybind11::size_t pybind11::detail::size_in_ptrs(pybind11::size_t)\u2019:\r\nbazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/detail/common.h:358:32: error: redefinition of \u2018constexpr pybind11::size_t pybind11::detail::size_in_ptrs(pybind11::size_t)\u2019\r\n inline static constexpr size_t size_in_ptrs(size_t s) { return 1 + ((s - 1) >> log2(sizeof(void *))); }\r\n                                ^~~~~~~~~~~~\r\n```", "comments": ["error message is too long so I couldn't copy/paste all", "no problem with windows 10 build...", "is there any way I can get rid of k8-py2-opt compile? since I don't need py2....", "Just to confirm, did you run `configure.py` with python3? Can you also make sure that you are building from the latest commit on `r2.1` (the `v2.1.0` tag)?", "@mihaimaruseac \r\nof course, I've been doing this everytime rc come out...", "I am seeing a very similar error to the one reported here, this time on macOS 10.15 with Xcode 11.3 clang\r\n\r\n```\r\n  exec env - \\\r\n    APPLE_SDK_PLATFORM=MacOSX \\\r\n    APPLE_SDK_VERSION_OVERRIDE=10.15 \\\r\n    PATH=/opt/local/bin:/opt/local/sbin:/bin:/sbin:/usr/bin:/usr/sbin \\\r\n    XCODE_VERSION_OVERRIDE=11.3.0.11C29 \\\r\n  external/local_config_cc/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG '-std=c++11' -iquote . -iquote bazel-out/host/bin -iquote external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/bin/external/local_config_sycl -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/gif -iquote bazel-out/host/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/host/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/host/bin/external/com_google_protobuf -iquote external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/zlib_archive -iquote bazel-out/host/bin/external/zlib_archive -iquote external/local_config_python -iquote bazel-out/host/bin/external/local_config_python -iquote external/pybind11 -iquote bazel-out/host/bin/external/pybind11 -iquote external/mkl_darwin -iquote bazel-out/host/bin/external/mkl_darwin -iquote external/mkl_dnn -iquote bazel-out/host/bin/external/mkl_dnn -iquote external/bazel_tools -iquote bazel-out/host/bin/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem third_party/eigen3/mkl_include -isystem bazel-out/host/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/host/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/host/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/zlib_archive -isystem bazel-out/host/bin/external/zlib_archive -isystem external/local_config_python/python_include -isystem bazel-out/host/bin/external/local_config_python/python_include -isystem external/pybind11/include -isystem bazel-out/host/bin/external/pybind11/include -isystem external/mkl_darwin/include -isystem bazel-out/host/bin/external/mkl_darwin/include -isystem external/mkl_dnn/include -isystem bazel-out/host/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/host/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/host/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/host/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/host/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/host/bin/external/mkl_dnn/src/cpu/xbyak -MD -MF bazel-out/host/bin/tensorflow/python/_objs/_pywrap_transform_graph.so/transform_graph_wrapper.d -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' '-frandom-seed=bazel-out/host/bin/tensorflow/python/_objs/_pywrap_transform_graph.so/transform_graph_wrapper.o' -isysroot __BAZEL_XCODE_SDKROOT__ '-mmacosx-version-min=10.15' -g0 '-march=x86-64' -g0 '-std=c++14' -fexceptions '-fvisibility=hidden' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/python/util/transform_graph_wrapper.cc -o bazel-out/host/bin/tensorflow/python/_objs/_pywrap_transform_graph.so/transform_graph_wrapper.o)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nIn file included from tensorflow/python/util/transform_graph_wrapper.cc:22:\r\nIn file included from ./tensorflow/python/lib/core/pybind11_status.h:21:\r\nIn file included from bazel-out/host/bin/external/local_config_python/python_include/pybind11/pybind11.h:44:\r\nIn file included from bazel-out/host/bin/external/local_config_python/python_include/pybind11/attr.h:13:\r\nIn file included from bazel-out/host/bin/external/local_config_python/python_include/pybind11/cast.h:13:\r\nIn file included from bazel-out/host/bin/external/local_config_python/python_include/pybind11/pytypes.h:12:\r\nbazel-out/host/bin/external/local_config_python/python_include/pybind11/detail/common.h:302:12: error: redefinition of 'return_value_policy'\r\nenum class return_value_policy : uint8_t {\r\n           ^\r\nexternal/pybind11/include/pybind11/detail/common.h:302:12: note: previous definition is here\r\nenum class return_value_policy : uint8_t {\r\n           ^\r\nIn file included from tensorflow/python/util/transform_graph_wrapper.cc:22:\r\nIn file included from ./tensorflow/python/lib/core/pybind11_status.h:21:\r\nIn file included from bazel-out/host/bin/external/local_config_python/python_include/pybind11/pybind11.h:44:\r\nIn file included from bazel-out/host/bin/external/local_config_python/python_include/pybind11/attr.h:13:\r\nIn file included from bazel-out/host/bin/external/local_config_python/python_include/pybind11/cast.h:13:\r\nIn file included from bazel-out/host/bin/external/local_config_python/python_include/pybind11/pytypes.h:12:\r\nbazel-out/host/bin/external/local_config_python/python_include/pybind11/detail/common.h:355:48: error: redefinition of default argument\r\ninline static constexpr int log2(size_t n, int k = 0) { return (n <= 1) ? k : log2(n >> 1, k + 1); }\r\n                                               ^   ~\r\nexternal/pybind11/include/pybind11/detail/common.h:355:48: note: previous definition is here\r\ninline static constexpr int log2(size_t n, int k = 0) { return (n <= 1) ? k : log2(n >> 1, k + 1); }\r\n                                               ^   ~\r\nIn file included from tensorflow/python/util/transform_graph_wrapper.cc:22:\r\nIn file included from ./tensorflow/python/lib/core/pybind11_status.h:21:\r\nIn file included from bazel-out/host/bin/external/local_config_python/python_include/pybind11/pybind11.h:44:\r\nIn file included from bazel-out/host/bin/external/local_config_python/python_include/pybind11/attr.h:13:\r\nIn file included from bazel-out/host/bin/external/local_config_python/python_include/pybind11/cast.h:13:\r\nIn file included from bazel-out/host/bin/external/local_config_python/python_include/pybind11/pytypes.h:12:\r\nbazel-out/host/bin/external/local_config_python/python_include/pybind11/detail/common.h:358:32: error: redefinition of 'size_in_ptrs'\r\ninline static constexpr size_t size_in_ptrs(size_t s) { return 1 + ((s - 1) >> log2(sizeof(void *))); }\r\n                               ^\r\nexternal/pybind11/include/pybind11/detail/common.h:358:32: note: previous definition is here\r\ninline static constexpr size_t size_in_ptrs(size_t s) { return 1 + ((s - 1) >> log2(sizeof(void *))); }\r\n                               ^\r\nIn file included from tensorflow/python/util/transform_graph_wrapper.cc:22:\r\nIn file included from ./tensorflow/python/lib/core/pybind11_status.h:21:\r\nIn file included from bazel-out/host/bin/external/local_config_python/python_include/pybind11/pybind11.h:44:\r\nIn file included from bazel-out/host/bin/external/local_config_python/python_include/pybind11/attr.h:13:\r\nIn file included from bazel-out/host/bin/external/local_config_python/python_include/pybind11/cast.h:13:\r\nIn file included from bazel-out/host/bin/external/local_config_python/python_include/pybind11/pytypes.h:12:\r\nbazel-out/host/bin/external/local_config_python/python_include/pybind11/detail/common.h:366:18: error: redefinition of 'instance_simple_holder_in_ptrs'\r\nconstexpr size_t instance_simple_holder_in_ptrs() {\r\n                 ^\r\nexternal/pybind11/include/pybind11/detail/common.h:366:18: note: previous definition is here\r\nconstexpr size_t instance_simple_holder_in_ptrs() {\r\n                 ^\r\n<snip>\r\n```\r\n\r\nissue appears to be due to two different locations for `pybind11` being used.\r\n", "> issue appears to be due to two different locations for `pybind11` being used.\r\n\r\nAt least for me, the external/pybind11 copy is version 2.3.0 but the generated copy in ...local_config_python/python_include is version 2.4.3.\r\n", "Can you please test if the same happens on `master`?\r\n\r\nI think you found the root cause for a bug I am looking at since before Christmas. Which then might explain why you cannot build.", "I can't in the near future, I'm currently compiling v2.1.0 patched to expect pybind11 v2.4.3 to see if it completes when the versions match. \r\n\r\nWhat I think is happening here is that tensorflow/workspace.bzl specifies pybind11 v2.3.0 but some other part of the machinery just resolves a pybind11 dependency as latest. This would have stopped working reliably on Sep 20, 2019 when v2.4.0 became the latest.\r\n\r\nAs of this writing `master` still specifies pybind11 v2.3.0 so for it to work here such a dependency would have to have been fixed or removed. ", "@mihaimaruseac \r\n\r\nbuild succeeded without tensorrt in master", "So `tensorrt` brings a bad `pybind` into the workspace.\r\n\r\nI'm out of office for now, but will investigate this more tomorrow up to early next week.\r\n\r\nThank you @alanpurple and @jok-ts for investigating into this.", "I am unable to reproduce locally. For me the build succeeds (both on master and `r2.1` branch). I don't get `pybind11` generated in two different places:\r\n\r\n```\r\nroot@de706aa7b16f:/work/tensorflow# find -L . -name pybind11 -type d \r\nfind: File system loop detected; './bazel-tensorflow/external/org_tensorflow' is part of the same file system loop as '.'.\r\n./bazel-tensorflow/external/pybind11\r\n./bazel-tensorflow/external/pybind11/include/pybind11\r\n./bazel-tensorflow/external/pybind11/pybind11\r\nroot@de706aa7b16f:/work/tensorflow# find bazel-out/k8-opt/bin/external/local_config_python/python_include/ -name '*pybind*'\r\nroot@de706aa7b16f:/work/tensorflow# \r\n```\r\n\r\nCan you please try again on a fresh checkout and record all the commands you run? I'm curious what causes the other pybind to be generated.", "It's a bundled libs problem. If pybind11 is installed with pip first, then those headers get picked up by `@local_config_python//:python_headers` and the pybind headers are duplicated and things go wrong. I've unbundled pybind in gentoo and it fixes it, i'll send a PR upstream soon to fix it in TF too, it needs some reworking because the `#include` paths are off in TF", "successfully build image with latest master, closing", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35653\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35653\">No</a>\n"]}, {"number": 35652, "title": "Add doc summary for tf.strings.upper and tf.strings.lower.", "body": "I think this is the right way of fixing #35647.\r\n\r\nRecompiling and previewing the documentation is taking quite a while (following the instructions from [here](https://github.com/tensorflow/tensorflow/blob/f40a875355557483aeae60ffcf757fc9626c752b/tensorflow/g3doc/README.txt)), so I figured I'll just open the pull request now, and check out the preview tomorrow.", "comments": ["Sorry for the force-push, I noticed a typo in the commit description.", "It looks like the Bazel builds failed with \"exited with error code 1\" but no details besides that. Is there anything I can/should do to address this?", "There is an issue on the windows builds but we can ignore them at the moment.\r\n\r\nThis should soon get imported internally and merged from there, I don't think there's anything you need to do @pbaranay "]}, {"number": 35651, "title": "Tensorflow model.fit \"use_multiprocessing\" \"distribution_strategy\" \"adapter_cls\" \"failed to find data adapter that can handle\"", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- I have written custom code:\r\n- Windows 10 - Anaconda):\r\n-Tensor flow installed from Anaconda Repo, \r\n- TensorFlow version (use command below): unknown 2.0.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: GTX 1070 4 gb\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI have written two different files for an MRI image dataset i prepared (i am a doctor, not a software engineer)\r\n\r\n```\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport os\r\nimport cv2\r\nDATADIR = \"F:/MRI_data_final\"\r\nCATEGORIES = [\"DarKanal\", \"Disk\"]\r\n\r\nfor category in CATEGORIES:\r\n    path = os.path.join(DATADIR, category)\r\n    for img in os.listdir(path):\r\n        img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_GRAYSCALE)\r\n        plt.imshow(img_array, cmap=\"gray\")\r\n        plt.show()\r\n        break\r\n    break\r\n        \r\nprint(img_array)\r\nprint(img_array.shape)\r\n\r\nIMG_SIZE = 200\r\n\r\nnew_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\r\nplt.imshow(new_array, cmap = 'gray')\r\nplt.show()\r\n\r\ntraining_data =[]\r\n\r\ndef create_training_data():\r\n    for category in CATEGORIES:\r\n        path = os.path.join(DATADIR, category)\r\n        class_num = CATEGORIES.index(category)\r\n        for img in os.listdir(path):\r\n            try:\r\n                img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_GRAYSCALE)\r\n                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\r\n                training_data.append([new_array, class_num])\r\n            except Exception as e:\r\n                print(e)\r\n                \r\ncreate_training_data()\r\n\r\n    \r\nimport random\r\nrandom.shuffle(training_data)\r\n\r\nX = []\r\ny = []\r\n\r\nfor features, label in training_data:\r\n    X.append(features)\r\n    y.append(label)\r\n    \r\nX = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\r\n\r\nimport pickle\r\n\r\npickle_out = open(\"X.pickle\",\"wb\")\r\npickle.dump(X, pickle_out)\r\npickle_out.close()\r\n\r\npickle_out = open(\"y.pickle\", \"wb\")\r\npickle.dump(y, pickle_out)\r\npickle_out.close()\r\n```\r\n\r\n\r\nI load data to X.pickle and y.pickle files, and when i read data from them, it reads. \r\n\r\nThen I create the new file for Neural Network Model,\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\r\nimport pickle\r\n\r\nX = pickle.load(open(\"X.pickle\", \"rb\"))\r\ny = pickle.load(open(\"y.pickle\", \"rb\"))\r\n\r\nmodel = Sequential()\r\nmodel.add( Conv2D(64, (3,3), input_shape = X.shape[1:])   )\r\nmodel.add(Activation(\"relu\"))\r\nmodel.add(MaxPooling2D(pool_size=(2,2)))\r\n\r\nmodel.add(Conv2D(64, (3,3)))\r\nmodel.add(Activation(\"relu\"))\r\nmodel.add(MaxPooling2D(pool_size=(2,2)))\r\n\r\nmodel.add(Flatten())\r\nmodel.add(Dense(64))\r\n\r\nmodel.add(Dense(1))\r\nmodel.add(Activation('sigmoid'))\r\n\r\nmodel.compile(loss=\"binary_crossentropy\",\r\n              optimizer=\"adam\",\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(X, y, batch_size=32, validation_split=0.1)\r\n```\r\n\r\nI am using a Jupyter Notebook and the program works until the last line (model.fit) but after the last line it returns this:\r\n\r\nAlso note that i ran the code in both CPU and GPU environments... \r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-6-170db6ee9b5d> in <module>\r\n----> 1 model.fit(X, y, batch_size=32, validation_split=0.1)\r\n      2 time.sleep(0.1)\r\n\r\n~\\Anaconda3\\envs\\PythonGPU-MRI\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    726         max_queue_size=max_queue_size,\r\n    727         workers=workers,\r\n--> 728         use_multiprocessing=use_multiprocessing)\r\n    729 \r\n    730   def evaluate(self,\r\n\r\n~\\Anaconda3\\envs\\PythonGPU-MRI\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n    222           validation_data=validation_data,\r\n    223           validation_steps=validation_steps,\r\n--> 224           distribution_strategy=strategy)\r\n    225 \r\n    226       total_samples = _get_total_number_of_samples(training_data_adapter)\r\n\r\n~\\Anaconda3\\envs\\PythonGPU-MRI\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in _process_training_inputs(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\r\n    495                      'at same time.')\r\n    496 \r\n--> 497   adapter_cls = data_adapter.select_data_adapter(x, y)\r\n    498 \r\n    499   # Handle validation_split, we want to split the data and get the training\r\n\r\n~\\Anaconda3\\envs\\PythonGPU-MRI\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py in select_data_adapter(x, y)\r\n    651         \"Failed to find data adapter that can handle \"\r\n    652         \"input: {}, {}\".format(\r\n--> 653             _type_name(x), _type_name(y)))\r\n    654   elif len(adapter_cls) > 1:\r\n    655     raise RuntimeError(\r\n\r\nValueError: Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'int'>\"})\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Verify the shape and datatype of x and y before calling model.fit.\r\n```\r\nprint(x.shape, x.dtype)\r\nprint(y.shape, y.dtype)\r\n```\r\nAlso, you can try to convert Numpy Arrays to Tensors using the example here before passing to model.fit - https://www.tensorflow.org/tutorials/load_data/numpy\r\n\r\n", "@drsiyarb \r\n\r\nCan you please confirm if @sampathweb's suggestion is working for you.Thanks!", "Yes, it worked. Thanks a lot.", "For anyone facing a similar problem, you can simply convert the list y into a numpy array and it should do the trick. Something like this:\r\n\r\ny = np.array(y)\r\n", "@faridelaouadi That actually worked perfectly. Thank you buddy."]}, {"number": 35650, "title": "InvalidArgumentError if np.ndarray is registered as Sequence type", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip install\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: CPU\r\n\r\n**Describe the current behavior**\r\n\r\nFor reference, numpy is planning to register numpy ndarray as a Sequence:\r\nhttps://github.com/numpy/numpy/issues/2776\r\n\r\nThe sample ResNet50 code from https://keras.io/applications/ runs fine. But, if ndarray is registered as a sequence, then TF2 throws an InvalidArgumentError.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nfrom keras.applications.resnet50 import ResNet50\r\nfrom keras.preprocessing import image\r\nfrom keras.applications.resnet50 import preprocess_input, decode_predictions\r\nimport numpy as np\r\nimport typing\r\ntyping.Sequence.register(np.ndarray)\r\n\r\nmodel = ResNet50(weights='imagenet')\r\n\r\nimg_path = 'elephant.jpg'\r\nimg = image.load_img(img_path, target_size=(224, 224))\r\nx = image.img_to_array(img)\r\nx = np.expand_dims(x, axis=0)\r\nx = preprocess_input(x)\r\n\r\npreds = model.predict(x)\r\n# decode the results into a list of tuples (class, description, probability)\r\n# (one such list for each sample in the batch)\r\nprint('Predicted:', decode_predictions(preds, top=3)[0])\r\n# Predicted: [(u'n02504013', u'Indian_elephant', 0.82658225), (u'n01871265', u'tusker', 0.1122357), (u'n02504458', u'African_elephant', 0.061040461)]\r\n```\r\n\r\nTwo extra lines added to the sample ResNet50 code are:\r\n\r\n```\r\nimport typing\r\ntyping.Sequence.register(np.ndarray)\r\n```\r\n\r\nThe error is:\r\n\r\n```\r\n2020-01-07 13:48:16.421816: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: The first dimension of padding\\s must be the rank of inputs[4,2] []\r\n         [[{{node conv1_pad/Pad}}]]\r\nTraceback (most recent call last):\r\n  File \"test_d3m_imports.py\", line 16, in <module>\r\n    preds = model.predict(x)\r\n  File \"/data/dsbox/kyao/miniconda3/envs/dsbox-eval-2019-winter/lib/python3.6/site-packages/keras/engine/training.py\", line 1462, in predict\r\n    callbacks=callbacks)\r\n  File \"/data/dsbox/kyao/miniconda3/envs/dsbox-eval-2019-winter/lib/python3.6/site-packages/keras/engine/training_arrays.py\", line 324, in predict_loop\r\n    batch_outs = f(ins_batch)\r\n  File \"/data/dsbox/kyao/miniconda3/envs/dsbox-eval-2019-winter/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\", line 3740, in __call__\r\n    outputs = self._graph_fn(*converted_inputs)\r\n  File \"/data/dsbox/kyao/miniconda3/envs/dsbox-eval-2019-winter/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1081, in __call__\r\n    return self._call_impl(args, kwargs)\r\n  File \"/data/dsbox/kyao/miniconda3/envs/dsbox-eval-2019-winter/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1121, in _call_impl\r\n    return self._call_flat(args, self.captured_inputs, cancellation_manager)\r\n  File \"/data/dsbox/kyao/miniconda3/envs/dsbox-eval-2019-winter/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1224, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n  File \"/data/dsbox/kyao/miniconda3/envs/dsbox-eval-2019-winter/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 511, in call\r\n    ctx=ctx)\r\n  File \"/data/dsbox/kyao/miniconda3/envs/dsbox-eval-2019-winter/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError:  The first dimension of paddings must be the rank of inputs[4,2] []\r\n         [[node conv1_pad/Pad (defined at /data/dsbox/kyao/miniconda3/envs/dsbox-eval-2019-winter/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [O\\p:__inference_keras_scratch_graph_10370]\r\n\r\nFunction call stack:\r\nkeras_scratch_graph\r\n\r\n```\r\n", "comments": ["Hi,\r\nThere's a chance that TF2 doesn't support `Sequence` yet. I think we'll have to wait for the community to add such support", "@kyao,\r\nTried to reproduce the issue using keras and tensorflow.keras, but I am facing an error different from yours.\r\nPlease find the attached Gist for keras [here](https://colab.sandbox.google.com/gist/amahendrakar/75fd7c47f61a12133313d0f6b9ba0d11/35650_keras.ipynb) and tensorflow.keras [here](https://colab.sandbox.google.com/gist/amahendrakar/f5b3b3ca9979e6ceeb8a247436e811ef/35650_tf-keras.ipynb). Could you please provide a minimal reproducible code for the issue? Thanks!", "@amahendrakar ,\r\nThe correct expected behaviors are for keras [here ](https://colab.research.google.com/drive/11yS9C5Gi8zoL7JwxALvbZXKX-zwMh7hQ) and tensorflow.keras  [here](https://colab.research.google.com/drive/1VkDeuL1zY9VJecD6ApGANthGMH74NMuK).\r\n\r\nWith the Sequence.register statement added the unexpected behaviors  are for keras [here](https://colab.research.google.com/drive/1sSnoFdQjgZcNlFnNlpi_UeUfMtlo1UlH) and for tensorflow.keras [here](https://colab.research.google.com/drive/1bc7fSgeCpKQvO8wxImzeL-Opopa1qy1i#scrollTo=5KxvZjBXkJig). For tensorflow.keras I did get the same error as you.", "@kyao,\r\nI do not have the permission to view the files. Please try saving the Gist using the following method\r\n'File' -> 'Save a copy as Github Gist', and share the link of the new window. Thanks!", "@amahendrakar ,\r\nHere they are on Github Gist\r\n\r\nThe correct expected behaviors are for keras [here ](https://colab.research.google.com/gist/kyao/408f31e638a39371a35a8e8492619904/35650_keras-expected.ipynb)and tensorflow.keras [here](https://colab.research.google.com/gist/kyao/43e472c6b712935b956b8b6d5f4879df/35650_tf-keras-expected.ipynb).\r\n\r\nWith the Sequence.register statement added the unexpected behaviors are for keras [here ](https://colab.research.google.com/gist/kyao/42ced4cd80bc592e80829365ea6ab3f0/35650_keras_2.ipynb)and for tensorflow.keras [here](https://colab.research.google.com/gist/kyao/5626f3d7d8d326cd62147f3cb8cef448/35650_tf-keras_2.ipynb). For tensorflow.keras I did get the same error as you.", "Was able to replicate the issue, please find the attached Gist for [keras](https://colab.sandbox.google.com/gist/amahendrakar/ce359c15ec3fe768eb7eb05d34e1f3dc/35650_keras.ipynb) and [tf.keras](https://colab.sandbox.google.com/gist/amahendrakar/6a227202aac348654507c9914bd51005/35650_tf-keras.ipynb).", "@kyao -- Keras supports ndarray, but we have to check and handle a variety of types. It's likely that the Sequence registration is throwing off some of these checks. @tomerk will take a look, but it may be that you are best using the raw ndarray.", "@kyao, The issue is now fixed in TF Nightly version, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/43d5e0b89b76db855d57fb9fc1a81e6f/35650.ipynb).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35650\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35650\">No</a>\n"]}, {"number": 35649, "title": "Incorrect name_scope with tf.function decoration", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nIf a function is decorated with tf.function, the name_scope is lost\r\n\r\n**Describe the expected behavior**\r\nThe name_scope should be same with or without tf.function decoration\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef f():\r\n    with tf.name_scope(\"f\") as scope:\r\n        tf.print(scope)\r\n\r\ndef g():\r\n    with tf.name_scope(\"g\") as scope:\r\n        tf.print(scope)\r\n\r\ndef main():\r\n    with tf.name_scope(\"main\"):\r\n        f()   # expect to print \"main/f/\", actually get \"f/\"\r\n        g()\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\nThe output is\r\n```\r\nf/\r\nmain/g/\r\n```\r\n\r\n", "comments": ["Issue is replicating for 2.0 and also TF-nightly, please find the [gist](https://colab.sandbox.google.com/gist/oanush/40d1bf5f1cd626c48a9a55ff5b25527f/35649.ipynb) of colab.Thanks!", "This is working as intended. The name_scope in the function is rooted at the function graph, and not at the call site (otherwise we'd have to retrace the function every time it's called to make sure it's using the proper name_scope). At runtime you can tell the correct scoping by the nesting of the name of the call op and the ops inside the function graph.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35649\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35649\">No</a>\n", "I think that @tf.function should not change the behavior of the decorated function whenever possible. If a function behave differently after the decoration, it makes the code hard to write.\r\nI am sure there are ways to avoid retracing, similar to experimental_relax_shapes.\r\n\r\nThe reason I notice this is that I have function doing some tf.summary.scalar(), because of the name scope, the summary tag is different after @tf.function decoration."]}, {"number": 35648, "title": "Malformed `TRAINABLE_ATTRIBUTE_NOTE` in batch normalization documentation", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization?version=stable\r\n\r\n## Description of issue (what needs changing):\r\n\r\nAfter the references list there is a stray malformed tag:\r\n\r\n{ {TRAINABLE_ATTRIBUTE_NOTE}}\r\n\r\nI suspect that this is supposed to resolve to the note that `moving_mean` and `moving_variance` are placed in `UPDATE_OPS` and need to be executed alongside the training op.  (This note is present in the `tf.layers` doc: https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/layers/batch_normalization)  But without knowing what this tag refers to I can't really say for sure.  (The tag is present in only three places in the Tensorflow codebase and shows up malformed on the website in each case.)\r\n", "comments": ["Recent updates to the Tensorflow website seem to have resolved this issue on the link posted, but the issue continues to exist for the v1.15 docs: https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/keras/layers/BatchNormalization.", "@joe-antognini,\r\nAs **`Tensorflow 1.x`** is not actively supported and as we recommend `Developer Community` to **`upgrade`** to **`Tensorflow 2.x`**, can you please let us know if we can close this issue, as it has been resolved in the **`actively supported Version (2.x)`**? Thanks! ", "Sure, we can close this."]}, {"number": 35647, "title": "tf.strings.lower and tf.strings.upper have \"TODO\" docstrings", "body": "## URL(s) with the issue:\r\n\r\n* lower: https://www.tensorflow.org/api_docs/python/tf/strings/lower?version=stable\r\n* upper: https://www.tensorflow.org/api_docs/python/tf/strings/upper?version=stable\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nThe first line of the docstrings for these functions is the auto-generated \"TODO: add doc.\" instead of an actual summary.\r\n\r\n### Submit a pull request?\r\n\r\nYes, I'll be submitting one shortly.", "comments": ["Closing this issue since the associated PR has been merged. Thanks!"]}, {"number": 35646, "title": "ERROR: Could not find a version that satisfies the requirement tensorflow", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac Air, Moave (10.14.5 )\r\n\r\n- TensorFlow installed from (source or binary): installed using: pip3 install tensorflow\r\n- TensorFlow version: 2.0\r\n- Python version: 3.6.5 64Bit\r\n- Installed using: pip\r\n\r\n\r\n**Describe the problem**\r\nUsing in RStudio, I installed R packages: \r\n\r\ndevtools::install_github('rstudio/tensorflow')\r\n\r\ndevtools::install_github('rstudio/keras')\r\n\r\ntried:\r\n\r\ninstall_tensorflow()\r\ntensorflow::install_tensorflow()\r\n\r\nbut both gave me an error message:\r\n\r\nERROR: Could not find a version that satisfies the requirement tensorflow==2.0.0 (from versions: none)\r\nERROR: No matching distribution found for tensorflow==2.0.0\r\nError: Error installing package(s): 'tensorflow==2.0.0'\r\n\r\nI checked page after page and found that I needed 3.6.x version of python, so I reverted and it did nothing. I re-installed tensorflow and nothing changed. \r\n\r\nWhen I checked on python tensorflow worled but for some reason I can't get it to work on RStudio.\r\n\r\nAny help please?\r\n\r\nThanks\r\nMark", "comments": ["@MarkJThomas ,\r\nCan you check the [link](https://tensorflow.rstudio.com/installation/) and let us know if it was helpful?Thanks!", "@MarkJThomas ,\r\nAny update on the issue ?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35646\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35646\">No</a>\n", "I retrief after 5-10 mins, it worked"]}, {"number": 35645, "title": "Error trying to build for AVX2 on CPU", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.6.4\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10/7\r\n- GPU model and memory: NVIDIA GeForce MX150\r\n\r\n\r\n**Describe the problem**\r\nI am trying to build tensorflow with AVX2 instructions for my CPU to increase speeds, but bazel keeps failing when loading tensorflow\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. Cloned the tensorflow source from github and checked out the r2.0 branch.\r\n2. Ran python ./configure.py\r\n3. Proceded through steps, selecting no for all other builds and set the bazel config option to /arch:AVC2\r\n4. Ran bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nThe error text is attached here:\r\n[error.txt](https://github.com/tensorflow/tensorflow/files/4032375/error.txt)\r\n", "comments": ["@ajidevelop ,\r\nHello i see error `Stderr: /usr/bin/bash: patch: command not found` in log ,can you [install](https://github.com/bazelbuild/rules_go/issues/1811) `patch` in your `PATH`?Thanks!", "This seemed to fix the issue, but the only thing I'm confused about is why the build_pip_package command seem to target 16000+ packages", "Closing this issue since it's resolved. \r\nFeel free to reopen if still have problems. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35645\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35645\">No</a>\n"]}, {"number": 35644, "title": "Add usage example to tf.nn.conv2d()", "body": "", "comments": ["@mihaimaruseac please review this", "All builds failed and there are conflicts to be fixed", "The conflict still needs to be resolved.", "I did pylint, it did not give any complaints.", "Perfect. Then what's left is to fix the ubuntu cpu error.", "@boronhub please check the Ubuntu CPU error here \r\nhttps://source.cloud.google.com/results/invocations/bd11b04e-bca3-4721-950c-091d4e3022e2/targets/%2F%2Ftensorflow%2Ftools%2Fdocs:tf_doctest/tests\r\n", "@mihaimaruseac there are some doctest failures , probably that needs to be fixed.", "@boronhub it is still failing doctest , can you please check and run the doc test locally as mentioned [here](https://www.tensorflow.org/community/contribute/docs_ref#test_on_your_local_machine) ", "@mihaimaruseac builds are passing"]}, {"number": 35643, "title": "Separate the global NUMA thread pools from the default thread pool ", "body": "This patch allows devices configured with numa enabled to not share the thread pool with the default CPU device.\r\n\r\nThis has come around as I've been experimenting with NUMA aware datasets/iterators which share the same sessions, but we create a DeviceMgr and a dataset iterator with a CPU device per NUMA node. Without this change the CPU:0 from the new DeviceMgr might share a thread pool which was not created with NUMA awareness.", "comments": ["@georgepaw Can you please resolve conflicts? Thanks!", "This pull request was created in January, and it received no review. What's the point of fixing conflicts if it's not going be to be reviewed anyway? ", "@georgepaw sorry for the delay , we will try to get it review fast once the conflicts are resolved.", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 35642, "title": "mysql-client can not be installed FROM official tensorflow/tensorflow image", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): official tensorflow image from docker hub\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): n/a\r\n- TensorFlow version: 1.15.0\r\n- Python version: 3\r\n- Installed using virtualenv? pip? conda?: Docker\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: n/a\r\n\r\n\r\n\r\n**Describe the problem**\r\nSimple image built `FROM tensorflow/tensorflow:tag` fails to build when including `mysql-client`\r\n\r\n```dockerfile\r\nFROM tensorflow/tensorflow:1.15.0-gpu-py3-jupyter\r\n\r\n# Avoid ERROR: invoke-rc.d: policy-rc.d denied execution of start.\r\n# RUN sed -i \"s/^exit 101$/exit 0/\" /usr/sbin/policy-rc.d\r\n\r\n# --- Install any needed packages specified in requirements.apt\r\nCOPY requirements.apt .\r\nRUN apt-get update && xargs apt-get install -y < requirements.apt\r\n\r\n# --- Install any needed packages specified in requirements.pip\r\nCOPY requirements.pip .\r\nRUN pip install -U --trusted-host pypi.python.org -r requirements.pip\r\n\r\n# activate jupyter extensions\r\nRUN jupyter contrib nbextension install \\\r\n  && jupyter nbextension enable codefolding/main \\\r\n  && jupyter nbextension enable collapsible_headings/main\r\n```\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nA MWE [repo](https://gitlab.com/SumNeuron/mytf) contains the following files:\r\n\r\n- `Dockerfile.ai`: custom image built on top of official tensorflow/tensorflow\r\n- `docker-compose.ai.development.yml`: specifies `Dockerfile.ai` as build file and mounts `notebooks` directory\r\n- `requirements.pip`: pip requirements that may be used in images other than `Dockerfile.ai`\r\n- `requirements.apt`: packages needed to be installed via `apt-get`\r\n\r\nFor convenience a python script `docker.py` is provided, e.g. \r\n```\r\npython docker.py -c {build | up | down}\r\n```\r\ninstead of \r\n```\r\ndocker-compose -f docker-compose.ai.development.yml {build | up | down}\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35642\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35642\">No</a>\n"]}, {"number": 35641, "title": "Fix hwloc build for ppc64le", "body": "This commit: https://github.com/tensorflow/tensorflow/commit/41df105#diff-6fb2e55075204b47da0460ea2abbc32f\r\nbroke the CPU unit test build for ppc64le. The compiler error was:\r\n\r\n.../libexternal_Shwloc_Slibhwloc.so: error: undefined reference to 'hwloc_linux_component'\r\n.../libexternal_Shwloc_Slibhwloc.so: error: undefined reference to 'hwloc_linuxio_component'\r\n\r\nThese methods are defined in topology-linux.c, adding the necessary bazel select statement\r\nso they are built during a ppc64le build.", "comments": []}, {"number": 35640, "title": "Allow unbounded work queue to be spawned on a specific NUMA node", "body": "This patch allows us to create unbounded thread pools pinned to a specific node. This is useful when we want to pin workloads to a specific CPU node (for example NUMA aware dataset iterators, which are similar to `MultiDeviceIterator` where there is an iterator per NUMA node).\r\n\r\nThis should not break the public release but I assume the non-default internal implementation will need updating.", "comments": ["I don\u2019t think I can fix, as per my comment in the PR, that is an internal google implantation which has not been open sourced and it will require someone with the access to the internal source code to implement this.", "@georgepaw thank you, @mrry can you please help merge this changes internally ?"]}]