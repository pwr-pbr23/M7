[{"number": 17849, "title": "Link to gcc_s and gcc if compiler is GCC version 5", "body": "When using cmake and GCC 5.4 to build tensorflow in Ubuntu 16.04,\r\nthe following error message would show up when loading\r\n_pywrap_tensorflow_internal.so:\r\n```\r\n_pywrap_tensorflow_internal.so: undefined symbol: __cpu_model\r\n```\r\n\r\nThe root cause is the same to this issue:\r\nhttps://github.com/tensorflow/tensorflow/issues/9593\r\n\r\nSigned-off-by: Yihong Wang <yh.wang@ibm.com>", "comments": ["Nagging Assignee @benoitsteiner: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 17848, "title": "Disable kmeans test on mac.", "body": "", "comments": []}, {"number": 17847, "title": "Branch 189641833", "body": "", "comments": []}, {"number": 17846, "title": "Adds missing protobuf dep to tf.contrib.data ops. (#17840)", "body": "* Adds missing protobuf dep to tf.contrib.data ops.\r\n\r\nI think this will help resolve the following:\r\nhttps://github.com/tensorflow/serving/issues/421\r\nhttps://github.com/tensorflow/serving/issues/684\r\nhttps://github.com/tensorflow/tensorflow/issues/17619\r\n\r\nOr at least I was experiencing a similar issue and this change resolved it for me in my local repo.", "comments": ["@allenlavoie could you take a look?", "Note that this is already merged to master, and the PR is a cherrypick request for 1.7.0rc2.", "This seems problematic to me (but maybe I'm missing context). It looks like it's including a second copy of the protocol buffer implementation / arena allocator in the op library?\r\n\r\nI don't know much about the Raspberry Pi build, but the way the symbol in https://github.com/tensorflow/tensorflow/issues/17619 should be defined is either (1) via linking to libtensorflow_framework.so when building normally, or (2) from _pywrap_tensorflow_internal.so via RTLD_GLOBAL when building with --config=monolithic (which may be the default for Raspberry Pi).\r\n\r\nLinking in a second copy of protocol buffers for a custom/contrib op may work in some cases (if the same arena allocator happens to be used consistently), but I wouldn't rely on it.\r\n\r\nSo I think this should be wrapped in a select() so that it's Raspberry Pi-specific if it's necessary there. I don't see why it would be, though, since presumably we already have a copy of protocol buffers in that build too (but it'd need debugging to figure out what's going wrong with the linking, so I'm not necessarily opposed to this as a temporary hack on that platform).", "After discussing, this is not the right solution.  Reverting this; I'll also revert the change to tensorflow/master."]}, {"number": 17845, "title": "Latest Tensorflow-GPU installation crashes because of no found distributions for Tensorboad", "body": "Trying to install the latest Tensorflow-GPU version gives me the following error:\r\n\r\npip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.6.0-cp27-none-linux_x86_64.whl\r\nDownloading/unpacking https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.6.0-cp27-none-linux_x86_64.whl\r\n  Downloading tensorflow_gpu-1.6.0-cp27-none-linux_x86_64.whl (209.2MB): 209.2MB downloaded\r\nRequirement already up-to-date: termcolor>=1.1.0 in ./.local/lib/python2.7/site-packages (from tensorflow-gpu==1.6.0)\r\nDownloading/unpacking astor>=0.6.0 (from tensorflow-gpu==1.6.0)\r\n  Downloading astor-0.6.2.tar.gz\r\n  Running setup.py egg_info for package astor\r\n    \r\n    warning: no files found matching 'CHANGES'\r\nRequirement already up-to-date: wheel in ./.local/lib/python2.7/site-packages (from tensorflow-gpu==1.6.0)\r\nDownloading/unpacking absl-py>=0.1.6 (from tensorflow-gpu==1.6.0)\r\n  Downloading absl-py-0.1.11.tar.gz (80kB): 80kB downloaded\r\n  Running setup.py egg_info for package absl-py\r\n    \r\nRequirement already up-to-date: backports.weakref>=1.0rc1 in ./.local/lib/python2.7/site-packages (from tensorflow-gpu==1.6.0)\r\nDownloading/unpacking tensorboard>=1.6.0,<1.7.0 (from tensorflow-gpu==1.6.0)\r\n  Could not find any downloads that satisfy the requirement tensorboard>=1.6.0,<1.7.0 (from tensorflow-gpu==1.6.0)\r\nCleaning up...\r\nNo distributions at all found for tensorboard>=1.6.0,<1.7.0 (from tensorflow-gpu==1.6.0)\r\nStoring complete log in /home/user/.pip/pip.log\r\n\r\n\r\nIn addition, I also tried installing it with pip, still unsuccessfully:\r\n\r\npip install --upgrade tensorflow-gpu\r\nCould not find any downloads that satisfy the requirement tensorflow-gpu in ./.local/lib/python2.7/site-packages\r\nDownloading/unpacking tensorflow-gpu\r\nCleaning up...\r\nNo distributions at all found for tensorflow-gpu in ./.local/lib/python2.7/site-packages\r\nStoring complete log in /home/user/.pip/pip.log", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@jart Is this a transient release-related problem? No TB 1.6?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "yes i faced the same issue.", "I'm not able to reproduce this issue. What version of pip are you using? Does this happen after you run `pip install -U pip`?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Issue has been solved... Thank u"]}, {"number": 17844, "title": "Dropout training placeholder fails in tf.while_loop ", "body": " If I pass a placeholder to `training` parameter in `tf.layers.dropout`, then the model fails in `tf.while_loop`.  If I directly pass a boolean value, it works fine.  \r\n\r\n- OS Platform and Distribution: Linux Ubuntu 17.10\r\n- TensorFlow installed from: pip\r\n- TensorFlow version: v1.6.0:\r\n- Python version: 3.6\r\n- Have I written custom code: yes\r\n- Bazel version: N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n- Exact command to reproduce: N/A\r\n\r\nSee the following code.\r\n```py\r\nimport os\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n\r\n\r\nprint(tf.__version__)\r\n\r\n\r\nclass Config:\r\n    def __init__(self):\r\n        self.units = 10\r\n        self.n_classes = 2\r\n        self.drop_rate = 0.5\r\n\r\n\r\nclass Model:\r\n    def __init__(self, cfg):\r\n        self.cfg = cfg\r\n        self.mlp = tf.layers.Dense(cfg.units)\r\n        self.resize = tf.layers.Dense(cfg.n_classes)\r\n\r\n    def predict(self, x):\r\n        z = self.mlp(x)\r\n        z = tf.layers.dropout(z, rate=self.cfg.drop_rate,\r\n                              training=self.cfg.training)\r\n        z = self.resize(z)\r\n        return z\r\n\r\n\r\ncfg = Config()\r\n# training = tf.placeholder_with_default(False, (), 'mode')\r\ntraining = False\r\ncfg.training = training\r\n\r\nmodel = Model(cfg)\r\n\r\n\r\ndef _cond(x, i):\r\n    return tf.less(i, 20)\r\n\r\n\r\ndef _body(x, i):\r\n    y = model.predict(x)\r\n    dy_dx = tf.gradients(y, x)[0]\r\n    x = dy_dx\r\n    return x, i+1\r\n\r\n\r\nx = tf.placeholder(tf.float32, (None, 3))\r\ny = model.predict(x)\r\nxx, ind = tf.while_loop(_cond, _body, [x, 0])\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\n\r\nval = sess.run(xx, feed_dict={x: np.random.random((1, 3))})\r\nprint(val)\r\n\r\nsess.close()\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Could you try to add `tf.stop_gradient` here and there to see if it fixes things?", "@drpngx No. `tf.stop_gradient` does not fix the problem.  Similarly, adding `back_prob=False` does not fix the problem either.", "@gongzhitaao,\r\nSorry for the delayed response. The issue seems to be with [tf.placeholder](https://www.tensorflow.org/api_docs/python/tf/compat/v1/placeholder). \r\n\r\nAs per the [documentation of Migrating Guide](https://www.tensorflow.org/guide/migrate#after_converting), in **`Tensorflow 2.x`**\r\n\r\n> **There's no usage of sessions or placeholders.**\r\n\r\nAs **`Keras Input Layers`** replace **`tf.placeholders`** in **`Tensorflow 2.x`**,  can you please let us know if the issue is relevant now? If so, please migrate your code to **`2.x`** using this [Migration Guide](https://www.tensorflow.org/guide/migrate) and let us know if you face any issues. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/17844\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/17844\">No</a>\n"]}, {"number": 17843, "title": "[Feature Request] Add tf.extract_image_patches support on 3D inputs", "body": "currently, [extract_image_patches](https://www.tensorflow.org/api_docs/python/tf/extract_image_patches) only support 2d image input(4d tensor) [batch, in_rows, in_cols, depth], I think it would be helpful to add 3d input support (ie 5d tensor [batch, in_rows, in_cols, in_higs, depth]. \r\n\r\nIn recently months, 3D deep learning has become very popular. I think it's time to add 3D support on most tensorflow api.\r\n\r\nOS Platform and Distribution: Ubuntu 16.04\r\nTensorFlow installed from: N/A\r\nTensorFlow version: 1.6\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@kho Could you comment?", "@gpapan", "We currently don't have anybody working on this. If you or somebody else could help us by implementing this it'd be great.  The code will be similar to that in core/kernels/extract_image_patches_op.* except that it should call to Eigen with extract_volume_patches instead of extract_image_patches. Let us know if you need further clarification. Thanks!", "@tatianashp Is anyone working on this or has finished this? If not, I'd like to contribute. Sorry for bumping up a pretty old issue, and thanks in advance!", "@hsgkim As far as I know, nobody has implemented this for 3D and your contributions will be very helpful. Adding a few other folks to make certain.\r\n/cc @asimshankar @ezhulenev ", "Will be happy to review PR ;) I believe that will solve a problem in https://github.com/tensorflow/tensorflow/issues/19491", "Gentlepersons,\r\n  thanks a lot for looking into this. I would be very glad if the 3D performance could be improved. Greetings from CERN (http://www.cern.ch)", "Reopened the issue to keep track of the progress on PR.", "@tatianashp Sorry, this is my first time ever contributing to TensorFlow, so I'm not quite sure where to place my files; should they be under `tensorflow/core/kernels` so that my op becomes `tf.extract_volume_patches`, or somewhere under `contrib`?\r\n\r\nAnd it looks like Eigen for some reason designed `extract_volume_patches` in a way that does not let you pass the `rate`s (in a very straightforward way at least), i.e.\r\n```c++\r\nEIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE\r\nconst TensorVolumePatchOp<Dynamic, Dynamic, Dynamic, const Derived>\r\nextract_volume_patches(const Index patch_planes, const Index patch_rows, const Index patch_cols,\r\n                       const Index plane_stride = 1, const Index row_stride = 1, const Index col_stride = 1,\r\n                       const PaddingType padding_type = PADDING_SAME, const Scalar padding_value = Scalar(0)) const {\r\n  return TensorVolumePatchOp<Dynamic, Dynamic, Dynamic, const Derived>(derived(), patch_planes, patch_rows, patch_cols, plane_stride, row_stride, col_stride, 1, 1, 1, 1, 1, 1, padding_type, padding_value);\r\n}\r\n```\r\n\r\ncf) `extract_image_patches`\r\n```c++\r\nEIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE\r\nconst TensorImagePatchOp<Dynamic, Dynamic, const Derived>\r\nextract_image_patches(const Index patch_rows = 1, const Index patch_cols = 1,\r\n                      const Index row_stride = 1, const Index col_stride = 1,\r\n                      const Index in_row_stride = 1, const Index in_col_stride = 1,\r\n                      const PaddingType padding_type = PADDING_SAME, const Scalar padding_value = Scalar(0)) const {\r\n  return TensorImagePatchOp<Dynamic, Dynamic, const Derived>(derived(), patch_rows, patch_cols, row_stride, col_stride,\r\n                                                             in_row_stride, in_col_stride, 1, 1, padding_type, padding_value);\r\n}\r\n```\r\n\r\nIs it a must to keep the `rate` arguments or can I leave them out for volume patches?", "@tatianashp Sorry for pinging... I am the concerned user :-) Could someone answer Kim? Thanks!", "@hsgkim 1. That should be a new kernel under a core/kernels directory. 2. It would be nice to have api consistent with image patches, though I\u2019m not sure how important to have rates for volume patches. It seems that omitting parameter from the kernel and using default value of 1 for row/col rates is ok. Rates can be added later and it should be backward compatible.", "@tatianashp @ezhulenev Thanks for the comments! I just made a pull request and I would appreciate your feedback.\r\n/cc @fcacarminati ", "Hello, sorry again for pinging, I would be very interested to understand the progress on this matter. Sorry for the bother and best regards, ", "You can track the PR status here: https://github.com/tensorflow/tensorflow/pull/21715", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "@hsgkim I found a 2.5 improvement. When can we expect these changes in master? best ", "@fcacarminati The PR has already been merged to master. Thanks!", "@hsgkim Great!!! Checking now with the head.", "Close it as #21715 has been merged. Thank @hsgkim for you work!"]}, {"number": 17842, "title": "Fix deprecated api call in _NonAtrousConvolution", "body": "### System information\r\nI am using tf version `1.7.0-dev20180317, v1.6.0-rc1-1580-gc941c087a9` on OSX in standard non-eager mode.\r\n\r\n### Describe the problem\r\nLines `151-156` in class `_NonAtrousConvolution` in `tensorflow/python/ops/nn_ops.py` set `nhwc` as data format when `conv_dims` is 1.  \r\n\r\n``` python\r\n    151     if conv_dims == 1:\r\n    152       # conv1d uses the 2-d data format names\r\n    153       if data_format is None or data_format == \"NWC\":\r\n    154         data_format_2d = \"NHWC\"\r\n    155       elif data_format == \"NCW\":\r\n    156         data_format_2d = \"NCHW\"\r\n```\r\n\r\nBut this causes a warning later on at line `2384` when `conv1d` is called from inside the `_conv1d` method at line `189`\r\n\r\n```\r\nWARNING:tensorflow:From ~/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\n`NHWC` for data_format is deprecated, use `NWC` instead\r\n```", "comments": ["I encounter the same warning with tf  version `1.6.0` on ArchLinux\r\n```\r\nWARNING:tensorflow:From /usr/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\n`NHWC` for data_format is deprecated, use `NWC` instead\r\n```", "Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "This issue is assumed to be fixed by #17455 ", "Closing issue since #17455 was merged into master so it will be available in the nightly builds even if not in 1.7"]}, {"number": 17841, "title": "Hello, I am having a problem running the classify_image file from my command line. I have a recurring error and nothing I seem to do fixes it.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no, i was using script\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.6.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: python classify_image.py --image_file \"C:\\Users\\akilj\\Desktop\\cropped_panda.jpg\"\r\n\r\n\r\n### Describe the problem\r\n\r\nI Ran this line of from my command line:\r\n\r\npython classify_image.py --image_file \"C:\\Users\\akilj\\Desktop\\cropped_panda.jpg\"                                                                                           \r\nI was in this directory:\r\n~/Desktop/models-master/tutorials/image/imagenet\r\n\r\n### Source code / logs\r\n\r\nThe resultant error was:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\akilj\\AppData\\Local\\Programs\\Python\\Python36\\lib\\tarfile.py\", l                                                                                            ine 2294, in next\r\n    tarinfo = self.tarinfo.fromtarfile(self)\r\n  File \"C:\\Users\\akilj\\AppData\\Local\\Programs\\Python\\Python36\\lib\\tarfile.py\", l                                                                                            ine 1090, in fromtarfile\r\n    obj = cls.frombuf(buf, tarfile.encoding, tarfile.errors)\r\n  File \"C:\\Users\\akilj\\AppData\\Local\\Programs\\Python\\Python36\\lib\\tarfile.py\", l                                                                                            ine 1026, in frombuf\r\n    raise EmptyHeaderError(\"empty header\")\r\ntarfile.EmptyHeaderError: empty header\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"classify_image.py\", line 227, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"C:\\Users\\akilj\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\                                                                                            tensorflow\\python\\platform\\app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"classify_image.py\", line 190, in main\r\n    maybe_download_and_extract()\r\n  File \"classify_image.py\", line 186, in maybe_download_and_extract\r\n    tarfile.open(filepath, 'r:gz').extractall(dest_directory)\r\n  File \"C:\\Users\\akilj\\AppData\\Local\\Programs\\Python\\Python36\\lib\\tarfile.py\", l                                                                                            ine 1586, in open\r\n    return func(name, filemode, fileobj, **kwargs)\r\n  File \"C:\\Users\\akilj\\AppData\\Local\\Programs\\Python\\Python36\\lib\\tarfile.py\", l                                                                                            ine 1640, in gzopen\r\n    t = cls.taropen(name, mode, fileobj, **kwargs)\r\n  File \"C:\\Users\\akilj\\AppData\\Local\\Programs\\Python\\Python36\\lib\\tarfile.py\", l                                                                                            ine 1616, in taropen\r\n    return cls(name, mode, fileobj, **kwargs)\r\n  File \"C:\\Users\\akilj\\AppData\\Local\\Programs\\Python\\Python36\\lib\\tarfile.py\", l                                                                                            ine 1479, in __init__\r\n    self.firstmember = self.next()\r\n  File \"C:\\Users\\akilj\\AppData\\Local\\Programs\\Python\\Python36\\lib\\tarfile.py\", l                                                                                            ine 2309, in next\r\n    raise ReadError(\"empty file\")\r\ntarfile.ReadError: empty file\r\n\r\n##\r\nI tried changing up file location, checked instillation and found no issues but code wouldnt run. Please help.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 17840, "title": "Adds missing protobuf dep to tf.contrib.data ops.", "body": "I think this will help resolve the following:\r\nhttps://github.com/tensorflow/serving/issues/421\r\nhttps://github.com/tensorflow/serving/issues/684\r\nhttps://github.com/tensorflow/tensorflow/issues/17619\r\n\r\nOr at least I was experiencing a similar issue and this change resolved it for me in my local repo.  (See also cl/189580464).", "comments": ["Thanks for the review!  Checks look green, can you PTAL and merge?  (I don't have write access).", "I don't see this fix in r1.7. Is there a plan to fix on r1.7 or I should move to r1.8? "]}, {"number": 17839, "title": "Beam search terminology", "body": "(This issue is with terminology used in documentation and code. System information not applicable.)\r\n\r\nThe documentation and code in tf.contrib.seq2seq.BeamSearchDecoder and friends seems to use \"beam\" to mean \"search state\", whereas beam conventionally means \"a collection of search states\". This non-standard usage makes for confusing documentation!\r\n\r\nExamples in documentation at https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BeamSearchDecoder:\r\n\r\n\"The BeamSearchDecoder shuffles its beams\" \r\n\"beam_width: Python integer, the number of beams\" \r\n\r\nOr in code:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/838a8f54f92452a15e3bb62a23ad5cd67e86933f/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py#L144\r\n\r\nThe conventional search algorithm terminology as I understand it, and attested by wikipedia (https://en.wikipedia.org/wiki/Beam_search) and recent academic usage (http://www.ijcai.org/Proceedings/05/Papers/0596.pdf), is that beam search is called beam search because it keeps a beam of search states in memory. The beam refers to the collection of search states, not the individual search states (I presume by the analogy that a beam of light illuminates certain objects but not others that fall outside the beam).", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "As I mentioned, I'm looking at documentation and source code, so system information is not applicable. The TensorFlow version I'm looking at the documentation for is r1.6.", "PRs improving the documentation are welcome!", "Created a PR #18235 to fix this issue.", "Closing this issue as a PR is created for the same. ", "That PR doesn't fix the issue. It's just a straight-up replace of \"beam\" for \"search state\" which leaves the documentation ungrammatical or nonsensical in many places. (I'm afraid I don't have bandwidth at the moment to make the docs better though.)", "Reopening the issue. Can refer the PR number if one is created for the same.\r\n", "Contrib seq2seq has moved to tensorflow addons. If this is still an issue, please send a new Issue to that repository."]}, {"number": 17838, "title": "Fix related doc clarification request on tf.contrib.lookup.MutableHas\u2026", "body": "\u2026hTable insert operation #17835\r\n\r\nMake the doc example executable, and explicitly suggests that MutableDenseHashTable.insert is an operation rather than in-place computation.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to `go/cla#troubleshoot`.\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 17837, "title": "Fix internal breakage caused by #16659", "body": "Due to copybara modification, we cannot put\r\n`//tensorflow/contrib/ffmpeg:ffmpeg_ops_py` into if_not_windows()\r\n\r\n\r\n@gunan This is to address the breakage caused by https://github.com/tensorflow/tensorflow/pull/16659, I believe with this fix, we don't have to rollback it anymore.\r\n\r\nTested internally: cl/189559607 \r\n\r\n@gunan ", "comments": ["Hi, could you send this internally?\r\nIt should be easier for us to see all potential issues that way.", "This is based on #16659, since it is not merged into internal repo, I couldn't send this internally."]}, {"number": 17836, "title": "Fix the doc of tf.contrib.lookup.MutableHashTable insert operation", "body": "This PR is to fix #17835.\r\nDocument on [MutableHashTable](https://www.tensorflow.org/api_docs/python/tf/contrib/lookup/MutableDenseHashTable) is confusing, especially in this line:\r\n> table.insert(keys, values)\r\n\r\nThis code is unexecutable and misleading, it suggests insert is done inplace rather than an operation that need to be executed afterwards.\r\n> sess.run(table.insert(keys, values))\r\n\r\nReference: [tensorflow-mutablehashtable-not-updating](https://stackoverflow.com/questions/43373170/tensorflow-mutablehashtable-not-updating)", "comments": []}, {"number": 17835, "title": "[Doc] Clarification on tf.contrib.lookup.MutableHashTable insert operation", "body": "Document on [MutableHashTable](https://www.tensorflow.org/api_docs/python/tf/contrib/lookup/MutableDenseHashTable) is confusing, especially in this line:\r\n`table.insert(keys, values)`\r\nThis code is unexecutable and misleading, it suggests insert is done inplace rather than an operation that need to be executed afterwards.\r\n\r\nI think it shall be replaced by:\r\n`sess.run(table.insert(keys, values))`\r\n\r\nReference: [tensorflow-mutablehashtable-not-updating](https://stackoverflow.com/questions/43373170/tensorflow-mutablehashtable-not-updating)", "comments": ["Created PR #17836 to fix this.", "Create PR #17838 to fix related doc problem in tf.contrib.lookup.MutableDenseHashTable"]}, {"number": 17834, "title": "Very Large Dataset", "body": "I have very large dataset of 500GB csv single file. is there a way to batch or chunk process it?", "comments": ["This [Input Pipeline Performance Guide](https://www.tensorflow.org/versions/master/performance/datasets_performance) may helps. I think this kind of questions should be asked in [stackoverflow](stackoverflow.com).", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 17833, "title": "Create/Init a curl handler each time it's expensive. ", "body": "Hi,\r\nNice wrapper around libcurl but... initialize/clean_up a curl handler every time can be expensive especially if you need to perform multiple call on small data chunk. I'll suggest to update the \"LibCurl\" wrapper by adding a \"static\" cache where store the connection handlers once have been initialized (and reuse it). this will really impact the performances. \r\n\r\nBest,\r\nDiego", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "sure and sorry:\r\n\r\nHave I written custom code: YES\r\nOS Platform and Distribution: Linux Mint 18.3 Sylvia\r\nTensorFlow installed from: git\r\nTensorFlow version: 1.6.0\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A\r\n\r\nBest,\r\nDiego", "@saeta can you please take a look at?\r\nThanks\r\n", "Hi @diiiego83! We are checking to see if you still need help in this issue , Have you tried latest stable version TF 2.6  yet? Please create a new issue if the issue is replicating in newer version. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 17832, "title": "tf.contrib.estimator.add_metrics does not pass label_ids to tf.estimator.DNNClassifier evaluation", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nOSX 10.12.6\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n\r\n- **TensorFlow version (use command below)**:\r\n('v1.6.0-0-gd2e24b6039', '1.6.0')\r\n\r\n- **Python version**: \r\n2.7.13\r\n\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nUsing tf.estimator.DNNClassifier as a multi-class estimator with a string label_vocabulary, tf.contrib.estimator.add_metrics will enable additional metrics functions as designed, however the label set pass to the metrics function will be the string tensor of labels, not the internal integer 'label_ids' of  DNNClassifier.  \r\n\r\nThis means the metrics do not have access to the labels used in the prediction tensor 'class_ids' (the tensor 'classes' does have the string labels).  This is a particular problem when trying to construct a confusion_matrix metric, as the confusion matrix tries to cast the labels to int64.\r\n\r\nWith the canned metrics, DNNClassifier  will simply pass in label_ids/class_ids:\r\nhttps://github.com/tensorflow/tensorflow/blob/9054c9b2ac303cbd1538166d0821f389cbc75894/tensorflow/python/estimator/canned/head.py#L776\r\n\r\n\r\n", "comments": ["I believe this would also impact the auc metric as it uses confusion_matrix under the hood:\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/python/ops/metrics_impl.py#L661", "@jprosevear is this a bug or a feature request? ", "I believe it should be considered a bug because its not possible to use tf.contrib.estimator.add_metrics in this scenario.", "Nagging Assignee @cy89: It has been 33 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@ispirmustafa can you please take a look, and comment or reassign appropriately?", "label-ids kept internal. Current solution to get id version label is integerizing in your metric_fn. \r\nhaving said that I think outputing label-ids as part of prediction dictionary make sense to me. I label this as contribution welcome. ", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Hi @jprosevear ! \r\nIt seems you are using older versions(1.x versions) of Tensorflow which is not supported any more. Attaching relevant threads for reference. Ref [1](https://www.tensorflow.org/addons), [2](https://www.tensorflow.org/install). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 17831, "title": "Fix the incorrect link of building the op library in extend tutorials", "body": "This PR is to fix the incorrect link of building the op library in extend tutorials:\r\n- The link of building the op library in add_filesys.md should be @{$adding_an_op#build_the_op_library instead of @{$adding_an_op#build-the-op-library\r\n- The link of building the op library in new_data_formats.md should be @{$adding_an_op#build_the_op_library instead of @{$adding_an_op#building_the_op_library", "comments": []}, {"number": 17830, "title": "Fix set_difference doc", "body": "The example in [tf.sets.set_difference](https://www.tensorflow.org/api_docs/python/tf/sets/set_difference) has a mistake. The result of `tf.sets.set_difference(a, b)` should be \r\n\r\n```python\r\n  # collections.OrderedDict([\r\n  #     ((0, 0, 0), 2),\r\n  #     ((0, 1, 0), 3),\r\n  # ])\r\n```\r\n\r\ninstead of \r\n\r\n```python\r\n  # collections.OrderedDict([\r\n  #     ((0, 0, 0), 2),\r\n  #     ((0, 0, 1), 3),\r\n  # ])\r\n```\r\n\r\nAdditionally the following is the result of running the code on my machine:\r\n\r\n```python\r\nSparseTensorValue(indices=array([[0, 0, 0], [0, 1, 0]], dtype=int64), values=array([2, 3]), dense_shape=array([2, 2, 1], dtype=int64))\r\n```", "comments": []}, {"number": 17829, "title": "Fix typos in `resampling.py`.", "body": "- Correct `initial_dist` -> `target_dist`\r\n- `variabes` -> `variables`", "comments": []}, {"number": 17828, "title": "Bug in the TensorFlowLite pod, iOS, linker doesn't find GetSegmentPredictions function", "body": "### System information\r\n- **I have written custom code**:\r\n- **iOS 11.2.6**:\r\n- **TensorFlowLite iOS pod**:\r\n- **version: 0.0.2**:\r\n\r\n### Describe the problem\r\nI added the 0.0.2 version of TensorFlowLite pod to my project. I try to experiment with the Smart Reply demo. I used the Android demo code to start from. I can find the related function (GetSegmentPredictions) in one of the pod headers. but the linker doesn't find the appropriate code segment at build time.\r\n\r\nIs there any body who could try out the Smart Reply on iOS?\r\nWhen will this be added to the TensorFlowLite pod?\r\n\r\n### Source code / logs\r\nna\r\n\r\n### Have I written custom code\r\nno custom code, I implemented the Android code in objc.\r\n\r\n### OS Platform and Distribution\r\niOS 11.2.6., TensorFlowLite iOS pod, version 0.0.2\r\n\r\n### TensorFlow installed from\r\nTensorFlowLite iOS pod\r\n\r\n### TensorFlow version\r\nLite, 0.0.2\r\n\r\n### Bazel version\r\nna\r\n\r\n### CUDA/cuDNN version\r\nna\r\n\r\n### GPU model and memory\r\nna\r\n\r\n### Exact command to reproduce\r\nna\r\n\r\nthanks!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@qspi, would you please fill out the rest of the fields in the issue template?", "@cy89 , sorry for the delay, I updated my question just right now.", "@qspi in addition, could you please tell us the command line you used to exhibit the error, and copy all the relevant linker error information into the bug?", "Hi @cy89, sorry, I don't get it, I try to use TensorFlow Lite in my iOS App, built-in. As I described in my issue. So there is no command line here... could you please clarify your question?\r\n\r\nThanks!", "Nagging Assignee @miaout17: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @miaout17: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @miaout17: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'm closing this. Please reopen if there's enough information to reproduce the issue."]}, {"number": 17827, "title": "TensorFlowLite pod, iOS, linker doesn't find GetSegmentPredictions function", "body": "Hi,\r\n\r\nI added the 0.0.2 version of TensorFlowLite pod to my project. I try to experiment with the Smart Reply demo. I used the Android demo code to start from. I can find the related function (GetSegmentPredictions) in one of the pod headers. but the linker doesn't find the appropriate code segment at build time.\r\n\r\nIs there any body who could try out the Smart Reply on iOS?\r\nWhen will this be added to the TensorFlowLite pod?\r\n\r\nthanks!\r\n", "comments": []}, {"number": 17826, "title": "Tensorflow lite c++ shared library build steps.", "body": "Hi , \r\nI like to know steps to build shared/static  c++ tensorflow lite library+ headers **WITHOUT JNI** for Android armeabi-v7a/arm64-v8a.\r\nI search for this problem through all issues here , found couple relevant topics but all of them without complete answer. \r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\nMaster\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n0.10.1\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\nNone\r\n- **GPU model and memory**:\r\nNone\r\n\r\nThanks\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "updated", "@dimitryn any news on this?", "1. add this to BUILD file in contrib/lite\r\ncc_binary(\r\n    name = \"libtensorflowLite.so\",\r\n \r\nlinkopts=[\"-shared\", \"-Wl\"],\r\nlinkshared=1,\r\n\r\n\r\n    copts = tflite_copts(),\r\n    deps = [\r\n        \":framework\",\r\n    ],\r\n)\r\n\r\n2. bazel build  //tensorflow/contrib/lite:libtensorflowLite.so --crosstool_top=//external:android/crosstool --cpu=arm64-v8a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=\"-std=c++11\"", "Is this still functional for Tensorflow 2.2.0?"]}, {"number": 17825, "title": "fft wrong results with non-power of 2 input sizes", "body": "I have encounter problems with the tensorflow fft implementation when the input is not a power of two. If the input is real, the first entry of the output is the sum of the input and should therefore also be real. \r\nBut tf.fft output a non-zero imaginary part. An example is given below:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndata = np.random.random(100)\r\nsess = tf.Session()\r\nfft_tf = sess.run(tf.fft(data))\r\nprint(fft_tf[0])\r\nnp.sum(data.astype(np.float32))\r\n```\r\n\r\nworks as intended for inputs as power of two's including smaller errors:\r\n\r\n```\r\ndata = np.random.random(128)\r\nsess = tf.Session()\r\nfft_tf = sess.run(tf.fft(data))\r\nprint(fft_tf[0])\r\nnp.sum(data.astype(np.float32))\r\n```\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n    Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n    Windows 10\r\n- **TensorFlow installed from (source or binary)**:\r\n    Installed from Source\r\n- **TensorFlow version (use command below)**:\r\n    Version 1.4\r\n- **Python version**: \r\n    Python 3.6.2\r\n- **Bazel version (if compiling from source)**:\r\n    N/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\n    N/A\r\n- **CUDA/cuDNN version**:\r\n    CUDA Version 8.0, cuDNN64_6\r\n- **GPU model and memory**:\r\n    nvidia GTX 1050 mobile\r\n- **Exact command to reproduce**:\r\n    See above.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Sorry, I updated my post above!\r\nI also noticed that tensorflow 1.6 is released. I will check if this also happens with TF 1.6.", "Thanks @BinhBob and sorry for the trouble. This was a bug in Eigen and should be fixed in https://github.com/tensorflow/tensorflow/commit/67ff23036e093343a7ea02e17f36f2b02eaae740, which is included in TensorFlow 1.6.", "Running your example on TensorFlow 1.6:\r\n```\r\ndata = np.random.random(100)\r\nsess = tf.Session()\r\nfft_tf = sess.run(tf.fft(data))\r\nprint(fft_tf[0])\r\nnp.sum(data.astype(np.float32))\r\n(48.58312-2.2768974e-05j)\r\n48.58311\r\n```\r\n", "Sorry, I forgot to post my results for TF 1.6 but as you can see the imaginary part of the first entry is still non-zero in TF 1.6. In general the error using power of two inputs are smaller than non power of two's. If thats an issue in the cuFFT implementation one should mention it in the tf.fft() function description? So everybody would know this issue.", "Nagging Assignee @rryan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rryan: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rryan: It has been 53 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rryan: It has been 68 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'm learning that a lot of RFFT implementations simply zero the imaginary component of the first frequency bin instead of computing them. \r\n\r\nFor example, numpy.fft.rfft (FFTPack) does this:\r\nhttps://github.com/numpy/numpy/blob/master/numpy/fft/fftpack_litemodule.c#L193-L200\r\n\r\nDue to floating point error I think it's unlikely we'll reach true zero, so maybe we should follow suit.", "Nagging Assignee @rryan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rryan: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 17824, "title": "Feature request: Return true labels from `estimator.predict(...)`", "body": "Hello everyone,\r\n\r\nAs mentioned in the [Getting started with Tensorflow / Custom Estimators](https://www.tensorflow.org/get_started/custom_estimators#implement_training_evaluation_and_prediction) one has to know the expected label for the data, since the labels will be discarded during the `predict()` function.\r\n\r\n```python\r\n# Generate predictions from the model\r\nexpected = ['Setosa', 'Versicolor', 'Virginica']\r\npredict_x = {\r\n    'SepalLength': [5.1, 5.9, 6.9],\r\n    'SepalWidth': [3.3, 3.0, 3.1],\r\n    'PetalLength': [1.7, 4.2, 5.4],\r\n    'PetalWidth': [0.5, 1.5, 2.1],\r\n}\r\n\r\npredictions = classifier.predict(\r\n    input_fn=lambda:iris_data.eval_input_fn(predict_x,\r\n                                            batch_size=args.batch_size))\r\n```\r\n\r\nwhich can be seen [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L486)\r\n\r\n```python\r\n     features, input_hooks = self._get_features_from_input_fn(\r\n        input_fn, model_fn_lib.ModeKeys.PREDICT)\r\n    estimator_spec = self._call_model_fn(\r\n        features, None, model_fn_lib.ModeKeys.PREDICT, self.config)\r\n```\r\n\r\nI totally agree to discard the labels, and don't pass them to the `model_fn` function. However, it would be much easier to return them also from the `input_fn`-function, if they are provided. A simplified solution, without the case distinction of given/not given labels, could be:\r\n\r\n```python\r\n  def predict(self,\r\n              input_fn,\r\n              predict_keys=None,\r\n              hooks=None,\r\n              checkpoint_path=None):\r\n    \"\"\"Yields predictions for given features.\r\n\r\n    Args:\r\n      input_fn: A function that constructs the features. Prediction continues\r\n        until `input_fn` raises an end-of-input exception (`OutOfRangeError` or\r\n        `StopIteration`).\r\n        See @{$get_started/premade_estimators#create_input_functions} for more\r\n        information. The function should construct and return one of\r\n        the following:\r\n\r\n          * A 'tf.data.Dataset' object: Outputs of `Dataset` object must have\r\n            same constraints as below.\r\n          * features: A `Tensor` or a dictionary of string feature name to\r\n            `Tensor`. features are consumed by `model_fn`. They should satisfy\r\n            the expectation of `model_fn` from inputs.\r\n          * A tuple, in which case the first item is extracted as features.\r\n\r\n      predict_keys: list of `str`, name of the keys to predict. It is used if\r\n        the `EstimatorSpec.predictions` is a `dict`. If `predict_keys` is used\r\n        then rest of the predictions will be filtered from the dictionary. If\r\n        `None`, returns all.\r\n      hooks: List of `SessionRunHook` subclass instances. Used for callbacks\r\n        inside the prediction call.\r\n      checkpoint_path: Path of a specific checkpoint to predict. If `None`, the\r\n        latest checkpoint in `model_dir` is used.\r\n\r\n    Yields:\r\n      Evaluated values of `predictions` tensors.\r\n\r\n    Raises:\r\n      ValueError: Could not find a trained model in model_dir.\r\n      ValueError: if batch length of predictions are not same.\r\n      ValueError: If there is a conflict between `predict_keys` and\r\n        `predictions`. For example if `predict_keys` is not `None` but\r\n        `EstimatorSpec.predictions` is not a `dict`.\r\n    \"\"\"\r\n    hooks = _check_hooks_type(hooks)\r\n    # Check that model has been trained.\r\n    if not checkpoint_path:\r\n      checkpoint_path = saver.latest_checkpoint(self._model_dir)\r\n    if not checkpoint_path:\r\n      raise ValueError('Could not find trained model in model_dir: {}.'.format(\r\n          self._model_dir))\r\n\r\n    with ops.Graph().as_default() as g:\r\n      random_seed.set_random_seed(self._config.tf_random_seed)\r\n      self._create_and_assert_global_step(g)\r\n      features, labels, input_hooks = self._get_features_and_labels_from_input_fn(\r\n          input_fn, model_fn_lib.ModeKeys.PREDICT)\r\n      estimator_spec = self._call_model_fn(\r\n          features, None, model_fn_lib.ModeKeys.PREDICT, self.config)\r\n      predictions = self._extract_keys(estimator_spec.predictions, predict_keys)\r\n      all_hooks = list(input_hooks)\r\n      all_hooks.extend(hooks)\r\n      all_hooks.extend(list(estimator_spec.prediction_hooks or []))\r\n      with training.MonitoredSession(\r\n          session_creator=training.ChiefSessionCreator(\r\n              checkpoint_filename_with_path=checkpoint_path,\r\n              master=self._config.master,\r\n              scaffold=estimator_spec.scaffold,\r\n              config=self._session_config),\r\n          hooks=all_hooks) as mon_sess:\r\n        while not mon_sess.should_stop():\r\n          preds_evaluated, gt_labels = mon_sess.run([predictions, labels])\r\n          if not isinstance(predictions, dict):\r\n            for pred, true_label in zip(preds_evaluated, gt_labels):\r\n              yield pred, true_label\r\n          else:\r\n            for i in range(self._extract_batch_length(preds_evaluated)):\r\n              yield {\r\n                  key: value[i]\r\n                  for key, value in six.iteritems(preds_evaluated)\r\n              }, gt_labels[i]\r\n```\r\n\r\nOS Platform and Distribution\r\n> Ubuntu 16.04.3 LTS\r\n\r\nTensorFlow installed from\r\n> pip\r\n\r\nTensorFlow version\r\n>  tensorflow-gpu '1.6.0'\r\n\r\nBazel version\r\n> N/A\r\n\r\nCUDA/cuDNN version\r\n> N/A\r\n\r\nGPU model and memory\r\n> N/A\r\n\r\nExact command to reproduce\r\n> N/A\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@martinwicke Could you please comment on this suggestion?", "Hi @lhlmgr \r\nI think this should be handled as part of evaluation but not prediction. Do you have any suggestion if about how can we do it as evaluation?\r\nthanks", "Nagging Assignee @ispirmustafa: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi @ispirmustafa,\r\nSorry, I guess I don't understand exactly what you mean by \"should be handled as part of evaluation\". \r\n\r\nThe case I have in mind is the following: I want to predict e.g. a class `y_hat` for sample `x`, or I want to retrieve the generated image `x_hat` (in case of an auto-encoder). And to compare it with the true class / image I want access to the true labels without extracting `TFRecord`-Files. For this case, it seems, the evaluation method is the wrong method, or am I mistaken?\r\n\r\nCheers, and thanks!\r\n", "I mean if you're comparing predictions with the labels, which seems evaluation. Having said that what do you think about following solution: add label in to features dict. make it as one of the prediction if it exist in the features dictionary.", "Hi @ispirmustafa \r\nHere is a case:\r\n When using Siamese network ,  the loss only make sense when training,  So evaluation won't work because only  losses returned,  we want Euclidean distance or something like to actually evaluate on test data.\r\nI think a sample solution is just passing true labels to \"model_fn\u201c.\r\nIn estimator.py  line 492~495, if labels is not None, just pass to \"model_fn\u201c.\r\n\r\n", "Hi @ChuangLee \r\nwe use predict mode for exporting serving graph. providing labels to the `model_fn` in predict mode seems breaking the assumptions.\r\nYou can still have `labels` in your network by adding them to `features` dictionary. Does that work?", "Nagging Assignee @ispirmustafa: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ispirmustafa: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "hi @ispirmustafa, adding labels in features dictionary doesn't work, it receives labels as None values, and it raises `ValueError: None values not supported.` I'm also interested in getting the labels in prediction mode.", "@basque21 could you please provide a simple repo of this error?", "It appears that the `labels` arg in `model_fn` is always `None` during prediction even if they are provided by `input_fn`. @ispirmustafa's suggestion doesn't work here because `estimator.predict()` is tossing out labels regardless.\r\n\r\nI've run into similar situations where I'd like to get the input and ground truth labels if available alongside predictions to do further analysis (like view failure cases). This is easily done by calling `model_fn` and reading from `input_fn` manually, but it feels hacky to me. It would be nice if there was a high-level way to do this like there is with training and evaluation.", "I guess, what @ispirmustafa proposes is that you could pass the `labels` as additional `feature` in your `features`-`dict`.\r\nSomething along this lines:\r\n\r\n```python\r\n# Generate predictions from the model\r\ndef parse_input(x, y):\r\n   features = dict(\r\n      x=x,\r\n      labels=y\r\n   )\r\n   \r\n   return features, y\r\n```\r\n\r\nIt should definitely work (not tested), but I would also prefer another way.", "Oh! Yeah that would work, but that definitely feels like a hack.", "Nagging Assignee @ispirmustafa: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ispirmustafa: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing this since there is a workaround.", "@ispirmustafa  I consider this very basic functionality for a machine learning library so I am surprised the issue is closed because a workaround exists. ", "I agree, the solution is a very hacky workaround. A better solution would be e.g. to pass a parameter to predict so that it would get the labels when requested.", "I'm running into this exact issue, and also agree that this is an important limitation of the estimator API. ", "Agree, can't believe it has been a year and we're still waiting on this basic request.", "Bump, I would also like to see this done properly.", "Maybe you can take a look at 'bert' code, when define the EstimatorSpec, you can feed the relative values,\r\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\r\n          mode=mode,\r\n          predictions={\"probabilities\": probabilities,\r\n                       \"label\": label},\r\n          scaffold_fn=scaffold_fn)\r\nget the label after prediction:\r\n\r\nresult = estimator.predict(input_fn=my_predict_input_fn)\r\nfor (i, prediction) in enumerate(result):\r\n        probabilities = prediction[\"probabilities\"]\r\n        label = prediction[\"label\"]"]}, {"number": 17823, "title": "[Feature Request] GPU ops for strided_slice/pad on uint8, int8 and bool", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 and Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.6.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: GeForce GTX 1080 Ti\r\n- **Exact command to reproduce**: See below\r\n\r\n### Introduction\r\n\r\nWe are trying to move the preprocessing of our tensorflow network to GPU in order to gain performance. Unfortunately it does not allow to run `strided_slice` or `pad` on GPU, thus either memcpy from/to GPU/CPU is required or everything must be computed on CPU. Also we wish not to convert these to other datatypes, because the tensors are quite large (thus every operation very memory consuming).\r\n\r\n### Request\r\nImplement GPU ops for datatypes such as `DT_UIN8`, `DT_INT8`, `DT_BOOL` for `strided_slice` and `pad`.\r\n### Example\r\n\r\n    import tensorflow as tf\r\n\r\n    x = tf.constant(0, dtype=tf.uint8, shape=(1,))\r\n    with tf.device('GPU:0'):\r\n        y = x[0]\r\n        # Or for padding:\r\n        #y = tf.pad(x, [[0, 0]])\r\n\r\n    # This works, because the cpu op is taken as fallback.\r\n    #with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)).as_default():\r\n    # This doesn't work, because the GPU op doesn't exist.\r\n    with tf.Session().as_default():\r\n        print(y.eval())\r\n\r\n\r\nThis example fails with the following error for slicing (as defined in [strided_slice_op_gpu.cu.cc](https://github.com/tensorflow/tensorflow/blob/3c3c0481ec087aca4fa875d6d936f19b31191fc1/tensorflow/core/kernels/strided_slice_op_gpu.cu.cc) also via [TF_CALL_GPU_NUMBER_TYPES](https://github.com/tensorflow/tensorflow/blob/1752d9c8fac5f6cf85a41e77d92e2743adbfc446/tensorflow/core/framework/register_types.h#L188)):\r\n\r\n    InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'strided_slice': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n    Registered kernels:\r\n      device='CPU'; T in [DT_INT64]\r\n      device='CPU'; T in [DT_INT32]\r\n      device='CPU'; T in [DT_UINT16]\r\n      device='CPU'; T in [DT_INT16]\r\n      device='CPU'; T in [DT_UINT8]\r\n      device='CPU'; T in [DT_INT8]\r\n      device='CPU'; T in [DT_HALF]\r\n      device='CPU'; T in [DT_BFLOAT16]\r\n      device='CPU'; T in [DT_FLOAT]\r\n      device='CPU'; T in [DT_DOUBLE]\r\n      device='CPU'; T in [DT_COMPLEX64]\r\n      device='CPU'; T in [DT_COMPLEX128]\r\n      device='CPU'; T in [DT_BOOL]\r\n      device='CPU'; T in [DT_STRING]\r\n      device='CPU'; T in [DT_RESOURCE]\r\n      device='CPU'; T in [DT_VARIANT]\r\n      device='GPU'; T in [DT_HALF]\r\n      device='GPU'; T in [DT_FLOAT]\r\n      device='GPU'; T in [DT_DOUBLE]\r\n      device='GPU'; T in [DT_COMPLEX64]\r\n      device='GPU'; T in [DT_COMPLEX128]\r\n      device='GPU'; T in [DT_INT64]\r\n      device='GPU'; T in [DT_INT32]\r\n\r\n    \t [[Node: strided_slice = StridedSlice[Index=DT_INT32, T=DT_UINT8, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1, _device=\"/device:GPU:0\"](Const, strided_slice/stack, strided_slice/stack_1, strided_slice/stack_2)]]\r\n\r\nFor padding (as defined in [pad_op_gpu.cu.cc](https://github.com/tensorflow/tensorflow/blob/3c3c0481ec087aca4fa875d6d936f19b31191fc1/tensorflow/core/kernels/pad_op_gpu.cu.cc) via [TF_CALL_GPU_NUMBER_TYPES](https://github.com/tensorflow/tensorflow/blob/1752d9c8fac5f6cf85a41e77d92e2743adbfc446/tensorflow/core/framework/register_types.h#L188)):\r\n\r\n    InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'Pad': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n    Registered kernels:\r\n      device='CPU'; T in [DT_INT64]; Tpaddings in [DT_INT32]\r\n      device='CPU'; T in [DT_INT64]; Tpaddings in [DT_INT64]\r\n      device='CPU'; T in [DT_INT32]; Tpaddings in [DT_INT32]\r\n      device='CPU'; T in [DT_INT32]; Tpaddings in [DT_INT64]\r\n      device='CPU'; T in [DT_UINT16]; Tpaddings in [DT_INT32]\r\n      device='CPU'; T in [DT_UINT16]; Tpaddings in [DT_INT64]\r\n      device='CPU'; T in [DT_INT16]; Tpaddings in [DT_INT32]\r\n      device='CPU'; T in [DT_INT16]; Tpaddings in [DT_INT64]\r\n      device='CPU'; T in [DT_UINT8]; Tpaddings in [DT_INT32]\r\n      device='CPU'; T in [DT_UINT8]; Tpaddings in [DT_INT64]\r\n      device='CPU'; T in [DT_INT8]; Tpaddings in [DT_INT32]\r\n      device='CPU'; T in [DT_INT8]; Tpaddings in [DT_INT64]\r\n      device='CPU'; T in [DT_HALF]; Tpaddings in [DT_INT32]\r\n      device='CPU'; T in [DT_HALF]; Tpaddings in [DT_INT64]\r\n      device='CPU'; T in [DT_BFLOAT16]; Tpaddings in [DT_INT32]\r\n      device='CPU'; T in [DT_BFLOAT16]; Tpaddings in [DT_INT64]\r\n      device='CPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT32]\r\n      device='CPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT64]\r\n      device='CPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT32]\r\n      device='CPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT64]\r\n      device='CPU'; T in [DT_COMPLEX64]; Tpaddings in [DT_INT32]\r\n      device='CPU'; T in [DT_COMPLEX64]; Tpaddings in [DT_INT64]\r\n      device='CPU'; T in [DT_COMPLEX128]; Tpaddings in [DT_INT32]\r\n      device='CPU'; T in [DT_COMPLEX128]; Tpaddings in [DT_INT64]\r\n      device='CPU'; T in [DT_BOOL]; Tpaddings in [DT_INT32]\r\n      device='CPU'; T in [DT_BOOL]; Tpaddings in [DT_INT64]\r\n      device='GPU'; T in [DT_HALF]; Tpaddings in [DT_INT32]\r\n      device='GPU'; T in [DT_HALF]; Tpaddings in [DT_INT64]\r\n      device='GPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT32]\r\n      device='GPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT64]\r\n      device='GPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT32]\r\n      device='GPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT64]\r\n      device='GPU'; T in [DT_INT32]; Tpaddings in [DT_INT32]\r\n      device='GPU'; T in [DT_INT32]; Tpaddings in [DT_INT64]\r\n\r\n    \t [[Node: Pad = Pad[T=DT_UINT8, Tpaddings=DT_INT32, _device=\"/device:GPU:0\"](Const, Pad/paddings)]]", "comments": ["@andrehentz are there any plans to support strided_slice on GPU?\r\n", "Any updates? This is also something I am interested in, since `tf.slice` could be a way to do `tf.random_crop` on the GPU (e.g. as explained [on this blog](http://tech.donghao.org/2018/07/27/do-tf-random_crop-operation-on-gpu/)).", "Added PR #23103 for uint8 support for Pad with GPU.", "@voegtlel \r\nplease let us know if this is still an issue", "@Saduf2019 Well, the issue was fixed some time ago, so this can be closed I guess."]}, {"number": 17822, "title": "TF Lite: build instructions for RaspberryPi give build error", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Tested on Raspberry pi 3, Raspbian 9.3; Ubuntu 16.04 (cross-compile to arm); docker tensorflow/tensorflow:nightly-devel (cross-compile to arm)\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: latest (current master on git)\r\n- **Python version**: -\r\n- **Bazel version (if compiling from source)**: Build is done via makefiles\r\n- **GCC/Compiler version (if compiling from source)**: tested with 6.3 on Raspbian, 4.8 on Raspbian, 4.8 on Ubuntu (and whatever is in the docker image)\r\n- **CUDA/cuDNN version**: -\r\n- **GPU model and memory**: -\r\n- **Exact command to reproduce**: Follow instructions given in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/rpi.md\r\n\r\n### Describe the problem\r\nBuild fails with error: \r\n\r\n```\r\n+ set -e\r\n+++ dirname ./tensorflow/contrib/lite/build_rpi_lib.sh\r\n++ cd ./tensorflow/contrib/lite\r\n++ pwd\r\n+ SCRIPT_DIR=/tensorflow/tensorflow/contrib/lite\r\n+ cd /tensorflow/tensorflow/contrib/lite/../../..\r\n+ CC_PREFIX=arm-linux-gnueabihf-\r\n+ make -j 3 -f tensorflow/contrib/lite/Makefile TARGET=RPI TARGET_ARCH=armv7\r\n/bin/sh: 1: [[: not found\r\narm-linux-gnueabihf-gcc --std=c++11 -O3 -DNDEBUG -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -I. -I/tensorflow/tensorflow/contrib/lite/../../../ -I/tensorflow/tensorflow/contrib/lite/downloads/ -I/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src -I/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/tensorflow/tensorflow/contrib/lite/../../../bazel-genfiles/tensorflow/core/framework -I/tensorflow/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/tools/benchmark_model.cc -o /tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/tools/benchmark_model.o\r\nIn file included from ./tensorflow/core/lib/core/errors.h:21:0,\r\n                 from ./tensorflow/core/platform/env.h:24,\r\n                 from tensorflow/contrib/lite/tools/benchmark_model.cc:29:\r\n./tensorflow/core/lib/core/status.h:23:53: fatal error: tensorflow/core/lib/core/error_codes.pb.h: No such file or directory\r\ncompilation terminated.\r\ntensorflow/contrib/lite/Makefile:113: recipe for target '/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/tools/benchmark_model.o' failed\r\nmake: *** [/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/tools/benchmark_model.o] Error 1\r\n```\r\n\r\n**Note**: The build for the tflite static library target completes, it's the benchmark program that fails.\r\n", "comments": ["Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Actually this is not necessary part of this script.\r\nSolution in #17796. A quick fix here.\r\nModify ``tensorflow/contrib/lite/build_rpi_lib.sh``\r\n```\r\n-CC_PREFIX=arm-linux-gnueabihf- make -j 3 -f tensorflow/contrib/lite/Makefile TARGET=RPI TARGET_ARCH=armv7\r\n+CC_PREFIX=arm-linux-gnueabihf- make -j 3 -f tensorflow/contrib/lite/Makefile TARGET=RPI TARGET_ARCH=armv7 \\\r\n+$SCRIPT_DIR/gen/lib/rpi_armv7/libtensorflow-lite.a\r\n```", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "i met the same issue.", "The fix mentioned by chenzhiwo solves it for me. Make sure you get the latest version of the source code, <s>since the fix has been merged already</s>. I built the library yesterday and it worked. (The pi_examples are wrong, but I opened two issues too report those error and the related fixes)\r\n\r\nEdit: Just saw that the fix was not actually merged yet, but you can apply it manually to have the build succeed.", "This error dosen't matters. ``libtensorflow-lite.a`` still works fine.", "Nagging Assignee @petewarden: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 17821, "title": "Add IndRNN implementation", "body": "This PR adds the cell implementation of Independently Recurrent Neural Networks\r\nto contrib together with unit tests. \r\n\r\nThe implementation is based on [Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN](https://arxiv.org/abs/1803.04831) (Shuai Li et al., 2018).\r\n\r\nThe difference to the `BasicRNNCell` is that each unit only has one recurrent weight connected to its own last hidden state. This makes it independent from other units in the same layer. To prevent vanishing / exploding gradients over time steps, the paper recommends bounds on the absolute value of that recurrent weight. These can be specified via the `recurrent_min_abs` and `recurrent_max_abs` constructor arguments.\r\n\r\nAdditional arguments of the `IndRNNCell` are `recurrent_kernel_initializer` and `input_kernel_initializer`. The paper does not recommend default values for the recurrent and input weights, so the default values for these were taken from [\r\nA Simple Way to Initialize Recurrent Networks of Rectified Linear Units](https://arxiv.org/abs/1504.00941) (Quoc V. Le et al., 2015):\r\n\r\n- Recurrent weights are set to 1 initially\r\n- Input weights are initialized using a Gaussian with `mean=0` and `stddev=0.001`\r\n\r\nThe code is originally from [batzner/indrnn](https://github.com/batzner/indrnn/).", "comments": ["Please fix errors\r\n\r\n```\r\nensorflow/contrib/rnn/python/kernel_tests/rnn_cell_test.py:1695: [C0330(bad-continuation), ] Wrong continued indentation (remove 1 space).\r\n\r\ntensorflow/contrib/rnn/python/kernel_tests/rnn_cell_test.py:1720: [C0330(bad-continuation), ] Wrong continued indentation (remove 1 space).\r\n...\r\nFAIL: //tensorflow/python:contrib_test (see /home/kbuilder/.cache/bazel/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/contrib_test/test.log)\r\nINFO: From Testing //tensorflow/python:contrib_test:\r\nERROR:\r\n    output file name: /b/s/w/ir/run/bazel-out/k8-opt/bin/tensorflow/tools/docs/build_docs_test.runfiles/org_tensorflow/tensorflow/docs_src/api_guides/python/contrib.graph_editor.md\r\n    Cannot make link to \"tf.contrib.graph_editor.copy_with_input_replacements\": Not in index.\r\n\r\n\r\nERROR:\r\n    output file name: /b/s/w/ir/run/bazel-out/k8-opt/bin/tensorflow/tools/docs/build_docs_test.runfiles/org_tensorflow/tensorflow/docs_src/api_guides/python/contrib.graph_editor.md\r\n    Cannot make link to \"tf.contrib.graph_editor.graph_replace\": Not in index.\r\n```", "@drpngx Thanks for the hint. I fixed the indentation and the `//tensorflow/python:contrib_test` now passes on my machine. \r\n\r\nHow do TensorFlow's contributors check the indentation? I ran `pylint` with TensorFlow's [`pylintrc`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/pylintrc) on my code before committing but it didn't catch the wrong indentation. The [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html) also doesn't mention it.", "Nagging Reviewer @ebrevdo: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 121 days with no activity and the `awaiting review` label has been applied.", "I tried the IndyLSTMCell in TensorFlow 1.10. It works with the default activation (tanh), but it does not work with nn_ops.relu. If you try to set the activation to relu, the loss will become NAN.  IndyGRUCell has the same problem.\r\n\r\nThe relu activation does work with IndRNNCell, however, when I stack it to 4 or 6 layers, I did not see any improvement of the model capacity. \r\n\r\nThe cell was placed inside tf.contrib.rnn.MultiRNNCell to get multiple layers, then tf.nn.dynamic_rnn. I tried seq2seq model (Google's NMT model) based on the new cell type as well.\r\n\r\nI tried this with both GPU version and CPU version.\r\n\r\nAny suggestion to fix/address this issue would be very appreciated. Thanks.", "@batzner, seems that this PR has not been updated for a while. There is an existing implementation of IndRNNCell within tf.contrib. What's the difference between this PR and that? Assume they are based on the same paper, can we close this PR?\r\n \r\nhttps://github.com/tensorflow/tensorflow/blob/f40a875355557483aeae60ffcf757fc9626c752b/tensorflow/contrib/rnn/python/ops/rnn_cell.py#L3057", "@qlzh727 thank you for the update and the link to the `IndRNN` implementation! It seems like it was added to TF after this PR was opened, so this PR doesn't provide any benefit anymore. I am happy to close it!"]}, {"number": 17820, "title": "Add comment to examples to prevent resource leaks", "body": "Issue #17374", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to `go/cla#troubleshoot`.\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}]