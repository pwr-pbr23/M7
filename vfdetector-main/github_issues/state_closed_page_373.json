[{"number": 42849, "title": "[INTEL MKL] DNN 0.x cleanup - MKL quantize/dequantize ops", "body": "DNN 0.x cleanup of MKL quantize and dequantize ops:\r\n\r\n(1) Remove all DNN 0.x related code\r\n(2) Replace all DNN 1.x macro usages with actual APIs", "comments": []}, {"number": 42848, "title": "Add Transformer Layers to tf.keras.layers", "body": "Nowadays most of NLP researchers/practitioners uses Transformers instead of LSTMs for NLP, it would make sense to add some of the layers used in the transformer architecture to the Keras API to avoid third-party libs or a lot of rewriting of code, The API could be similar to this tutorial: https://medium.com/tensorflow/a-transformer-chatbot-tutorial-with-tensorflow-2-0-88bf59e66fe2\r\n\r\nSo the layers being EncoderLayer, DecoderLayer, Encoder, Decoder, MultiHeadAttention(which is already in beta), PositionalEncoding, LookAheadMask,  ScaledDotProduct, FeedForward and Transformer.\r\n\r\nThanks in advance.", "comments": ["cc @tanzhenyu who works on Keras NLP. ", "1) We already have `tf.keras.layers.MultiHeadAttention`\r\n2) We have PositionEmbedding in [keras_nlp](https://github.com/tensorflow/models/tree/master/official/nlp/keras_nlp)\r\n3) We are not gonna have FeedForward given this can be easily done with Keras.\r\n4) I am working on LookAheadMask\r\n5) ScaledDotProduct and PositionalEncoding can be supported\r\n\r\nMost importantly, please comment on keras_nlp [scoping doc](https://github.com/keras-team/governance/pull/22) if you're interested or wanted to contribute", "@tanzhenyu Thanks for the reply, I wasn't aware of the keras_nlp layer, but I'll look into it, thanks again.", "@tanzhenyu Hello, your answer helped me too. I've been using [this](https://www.tensorflow.org/tutorials/text/transformer) implementation until now that we already have MultiHeadAttention in a stable version (I'm looking for something more robust).\r\n\r\nHowever I have checked keras_nlp out and missed a DecoderBlock. Is there any plan on this? Also, seems like keras_nlp installation still requires tf-nightly even though MultiHeadAttention is stable.\r\n\r\nI wonder why Pytorch team has already these implementations for like over a year and TF is still missing this key pieces.", "> @tanzhenyu Hello, your answer helped me too. I've been using [this](https://www.tensorflow.org/tutorials/text/transformer) implementation until now that we already have MultiHeadAttention in a stable version (I'm looking for something more robust).\r\n> \r\n> However I have checked keras_nlp out and missed a DecoderBlock. Is there any plan on this? Also, seems like keras_nlp installation still requires tf-nightly even though MultiHeadAttention is stable.\r\n> \r\n> I wonder why Pytorch team has already these implementations for like over a year and TF is still missing this key pieces.\r\n\r\nYes DecoderBlock is still on the way. If you want to contribute, that'd be great as well!", "> > @tanzhenyu Hello, your answer helped me too. I've been using [this](https://www.tensorflow.org/tutorials/text/transformer) implementation until now that we already have MultiHeadAttention in a stable version (I'm looking for something more robust).\r\n> > However I have checked keras_nlp out and missed a DecoderBlock. Is there any plan on this? Also, seems like keras_nlp installation still requires tf-nightly even though MultiHeadAttention is stable.\r\n> > I wonder why Pytorch team has already these implementations for like over a year and TF is still missing this key pieces.\r\n> \r\n> Yes DecoderBlock is still on the way. If you want to contribute, that'd be great as well!\r\n\r\nWhat about [this](https://github.com/tensorflow/models/blob/master/official/nlp/modeling/layers/transformer.py)?"]}, {"number": 42847, "title": "Fixing typo in comment.", "body": "Changed mathemetical to mathematical.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42847) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42847) for more info**.\n\n<!-- ok -->", "We will not be encouraging one liner grammatical changes as this is expensive process, thank you for your interest.\r\nCC @mihaimaruseac ", "I understand. Anyway, just FYI, this is an orthographic issue that goes directly into this live [API documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Model#train_step).", "Can you try to fix all typos in that file/directory? If you fix more than just one/a handful we can get the PR accepted.\r\n\r\nIt's all a balance between amount of changes done and amount of CI that gets spent in merging the change.", "I just changed those three lines, I don't have enough time to go over all files in the directory. Don't worry, it can be fixed with some other changes in the future."]}, {"number": 42846, "title": "Add a warning when fitting models loaded from SavedModels", "body": "Add a warning when fitting models loaded from SaveModel having optimizers  with slots.\r\nRef: https://github.com/tensorflow/tensorflow/pull/42749#issuecomment-683964931", "comments": ["/cc @reedwm  @k-w-w ", "This is just an exploring proposal. If you have a better entry point let me know.", "@bhack @reedwm @k-w-w  This PR is in draft, any update on this? Please. Thanks!", "@gbaned I have any private channel so the thread it is what you see here.", "Can you give context for this PR? I think adding a warning in fit() for loaded saved models with slot variables is sensible, but I don't see any warnings added in this PR.", "@reedwm If you see is a draft as I have asked for comment on 1st September about the entry point I selected in the code to \"introduce\" the condition but as of 2nd December I got any feedback. \r\nI have any other chat channel to discuss with devs as Gitter SIG Keras-io/Lobby in the current status is useless for discussion about PRs.\r\n", "Unfortunately I am not very familiar with the SavedModel code so I'm not sure if this is a good place to do it. Sorry for not responding until now.\r\n\r\n@k-w-w, do you think it would be a good idea to add a warning if a SavedModel with slot variables is restored and Model.fit() is used? Currently slot variables are not restored in such a case, which could affect training and is why we should potentially add a warning. If this is a good idea, would the code modified in this PR be a good place to check for the use of slot variables?", "@reedwm Thank you. Just to note that I've already asked to @k-w-w on 23th Oct at https://github.com/tensorflow/community/pull/281#issuecomment-715596624", "The change looks good, although I think directly sending the warning when loading would make sense (see the warning message for the HDF5 format: https://github.com/tensorflow/tensorflow/blob/a6b7e4b94ceb3471a268a57741fe1f32a1472119/tensorflow/python/keras/saving/hdf5_format.py#L211)", "> The change looks good, although I think directly sending the warning when loading would make sense (see the warning message for the HDF5 format:\r\n> \r\n> https://github.com/tensorflow/tensorflow/blob/a6b7e4b94ceb3471a268a57741fe1f32a1472119/tensorflow/python/keras/saving/hdf5_format.py#L211\r\n> )\r\n\r\nDo you like the same HDF5 warning message or something else?", "Ok Windows is failing for unrelated MLIR issues and Macos CPU is failing probably for other unrelated stuff but has any visible details.\r\nThe other checks are passing.", "@k-w-w The sanity is temp broken by https://github.com/tensorflow/tensorflow/pull/48294#issuecomment-849923841", "> @k-w-w The sanity is temp broken by [#48294 (comment)](https://github.com/tensorflow/tensorflow/pull/48294#issuecomment-849923841)\r\n\r\nOh I see, good to know", "Can you re-trigger the build? I think that the CI Sanity it is working again.", "Do you need anything else here?", "@k-w-w If you want you can add a reference somewhere to https://github.com/tensorflow/tensorflow/issues/44670"]}, {"number": 42845, "title": "AttributeError: 'NoneType' object has no attribute 'python_grad_func' when using @tf.function()", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  dockerhub container 'latest' Digest: 7bc36fe0ca1a051a808122e87f5438614b371263515df4794abef9a78440af8b\r\n- TensorFlow version (use command below):\r\ntf.version.GIT_VERSION=v2.3.0-rc2-23-gb36436b087\r\ntf.version.VERSION=2.3.0\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\n\r\nGetting AttributeError: 'NoneType' object has no attribute 'python_grad_func' when decorating functions with @tf.function()\r\n\r\n**Describe the expected behavior**\r\nNo error\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nprint('tf.version.GIT_VERSION={}'.format(tf.version.GIT_VERSION))\r\nprint('tf.version.VERSION={}'.format(tf.version.VERSION))\r\n\r\n@tf.function()\r\ndef G(x, c):\r\n    xx = tf.transpose(a=x[:, :, None], perm=[0, 2, 1])\r\n    cc = tf.transpose(a=c[:, :, None], perm=[2, 0, 1])\r\n    G = tf.reduce_sum(input_tensor=tf.square(xx - cc), axis=2)\r\n    return G\r\n\r\n@tf.function()\r\ndef ff(x, c , v):\r\n    with tf.GradientTape(persistent=True) as t1:\r\n        t1.watch(x)\r\n        with tf.GradientTape(persistent=True) as t2:\r\n            t2.watch(x)\r\n            p = tf.reduce_sum(input_tensor=G(x,c) * v, axis=1, keepdims=True)\r\n        f = t2.batch_jacobian(p, x, experimental_use_pfor=False)[:,:,0]\r\n    f_w = t1.batch_jacobian(f, x, experimental_use_pfor=False)[:,:,0]\r\n    return f_w\r\n\r\nx = tf.constant([[1.],[2.]])\r\nc = tf.ones((2,1))\r\nv = tf.ones((1,2))\r\n\r\nwith tf.GradientTape() as tape:\r\n    tape.watch(v)\r\n    o = tf.reduce_sum(tf.square(ff(x, c, v)))\r\ng = tape.gradient(o, v)\r\nprint(g)\r\n\r\n\r\n```\r\n\r\n**Other info / logs** \r\n\r\n```\r\ntf.version.GIT_VERSION=v2.3.0-rc2-23-gb36436b087\r\ntf.version.VERSION=2.3.0\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 2486, in get_attr\r\n    pywrap_tf_session.TF_OperationGetAttrValueProto(self._c_op, name, buf)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Operation 'while_1' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py\", line 331, in _MaybeCompile\r\n    xla_compile = op.get_attr(\"_XlaCompile\")\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 2490, in get_attr\r\n    raise ValueError(str(e))\r\nValueError: Operation 'while_1' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 2486, in get_attr\r\n    pywrap_tf_session.TF_OperationGetAttrValueProto(self._c_op, name, buf)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Operation 'gradient_tape/while/while_grad' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py\", line 331, in _MaybeCompile\r\n    xla_compile = op.get_attr(\"_XlaCompile\")\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 2490, in get_attr\r\n    raise ValueError(str(e))\r\nValueError: Operation 'gradient_tape/while/while_grad' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py\", line 607, in _GradientsHelper\r\n    grad_fn = ops.get_gradient_function(op)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 2655, in get_gradient_function\r\n    return _gradient_registry.lookup(op_type)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/registry.py\", line 97, in lookup\r\n    \"%s registry has no entry for: %s\" % (self._name, name))\r\nLookupError: gradient registry has no entry for: PartitionedCall\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/mnt/MultiPhaseModel_PINN/tests/issue_xla.py\", line 29, in <module>\r\n    o = tf.reduce_sum(tf.square(ff(x, c, v)))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 846, in _call\r\n    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1848, in _filtered_call\r\n    cancellation_manager=cancellation_manager)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1929, in _call_flat\r\n    forward_function, args_with_tangents = forward_backward.forward()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1433, in forward\r\n    self._inference_args, self._input_tangents)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1189, in forward\r\n    self._forward_and_backward_functions(inference_args, input_tangents))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1341, in _forward_and_backward_functions\r\n    outputs, inference_args, input_tangents)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 899, in _build_functions_for_outputs\r\n    src_graph=self._func_graph)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py\", line 669, in _GradientsHelper\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py\", line 336, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py\", line 669, in <lambda>\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py\", line 354, in _WhileGrad\r\n    util.unique_grad_fn_name(body_graph.name), op, maximum_iterations)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py\", line 626, in _create_grad_func\r\n    body_graph_inputs, body_graph_outputs))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\", line 986, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py\", line 622, in <lambda>\r\n    lambda *args: _grad_fn(ys, xs, args, body_graph),\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py\", line 682, in _grad_fn\r\n    unconnected_gradients=\"zero\")\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py\", line 669, in _GradientsHelper\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py\", line 336, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py\", line 669, in <lambda>\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py\", line 354, in _WhileGrad\r\n    util.unique_grad_fn_name(body_graph.name), op, maximum_iterations)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py\", line 626, in _create_grad_func\r\n    body_graph_inputs, body_graph_outputs))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\", line 986, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py\", line 622, in <lambda>\r\n    lambda *args: _grad_fn(ys, xs, args, body_graph),\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py\", line 682, in _grad_fn\r\n    unconnected_gradients=\"zero\")\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py\", line 619, in _GradientsHelper\r\n    grad_fn = func_call.python_grad_func\r\nAttributeError: 'NoneType' object has no attribute 'python_grad_func'\r\n\r\nProcess finished with exit code 1\r\n\r\n```\r\n\r\n\r\n", "comments": ["I have tried in colab with TF version 2.2, 2.3 and was able to reproduce the issue. However in TF nightly (`2.4.0-dev20200831`) i am seeing different error message (`NotFoundError:  No gradient defined for op: AddN`).Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/775fd546727cdba54239b1077da77012/untitled288.ipynb).Thanks!", "Removing autograph label as the error message persists even when `autograph=False`", "It looks like this is working with experimental_use_pfor=True on both batch_jacobians. It's possible that this is due to recent vectorized_map improvements. Is using that an option for you?\r\n\r\nFixing the control flow gradients may be a bit more complicated, but that we should do too; thank you for the report.", "Hi @allenlavoie .  Thanks for taking a look.\r\n\r\n> It looks like this is working with experimental_use_pfor=True on both batch_jacobians. It's possible that this is due to recent vectorized_map improvements. Is using that an option for you?\r\n\r\nThis is a failing case I managed to reduce from a more complex code.  I shared it with the intention to get more understanding of why it happens, and to make developers aware of it in case they don't know.  I guess you might be tracking it somewhere else, otherwise you may close it.\r\n\r\nThe actual case I'm trying to optimize for running speed and memory allocation looks similar to what I shared in issue #43252, where you provided great help.  I will keep in mind `experimental_use_pfor=True` in my tests.", "Looks like the version without pfor is fixed after https://github.com/tensorflow/tensorflow/commit/07b75ffa453b1ec9189371244d21b6684503340b\r\n\r\nThank you for the report.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42845\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42845\">No</a>\n"]}, {"number": 42844, "title": "Can's Training by using the data on external SSD", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: macOS Catalina\r\n- Mobile device if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n### Bug Summary\r\nI am training an image classifier on the local machine, it works ok when the data is on the local machine, however, when I tried the data which on my external SSD, error pops out. The data on external SSD can be read and decoded separately by using the following code:\r\n```\r\nimport tensorflow as tf\r\nIMG_HEIGHT = 90\r\nIMG_WIDTH = 160\r\npath = \"/Volumes/External-SSD/dataset/labeled/gaming/Fresh_68799179400006397.bmp\"\r\n\r\ndef load_and_preprocess_image(path):\r\n    img = tf.io.read_file(path)\r\n    img = tf.image.decode_bmp(img, channels=3)\r\n    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\r\n    img /= 255.0  # normalize to [0,1] range\r\n    return img\r\n\r\nload_and_preprocess_image(path)\r\n```\r\n\r\n### My code\r\n\r\n- Dataloader\r\n```\r\nclass DataSetGenerator():\r\n    def __init__(self, IMG_HEIGHT, IMG_WIDTH):\r\n        self.IMG_HEIGHT = IMG_HEIGHT\r\n        self.IMG_WIDTH = IMG_WIDTH\r\n\r\n    def load_and_preprocess_image(self, path):\r\n        img = tf.io.read_file(path)\r\n        img = tf.image.decode_bmp(img, channels=3)\r\n        img = tf.image.resize(img, [self.IMG_HEIGHT, self.IMG_WIDTH])\r\n        img /= 255.0  # normalize to [0,1] range\r\n        return img\r\n\r\n    def prepare_for_training(self, ds, cache=True):\r\n        if cache:\r\n            if isinstance(cache, str):\r\n                ds = ds.cache(cache)\r\n        else:\r\n          ds = ds.cache()\r\n\r\n        ds = ds.shuffle(buffer_size=self.SHUFFLE_SIZE)\r\n\r\n        ds = ds.batch(self.BATCH_SIZE)\r\n\r\n        # `prefetch` lets the dataset fetch batches in the background while the model\r\n        # is training.\r\n        ds = ds.prefetch(buffer_size=self.AUTOTUNE)\r\n\r\n        return ds\r\n\r\n    def labeled_dataset(self, image_paths, labels):\r\n        # a dataset that returns image paths\r\n        path_ds = tf.data.Dataset.from_tensor_slices(image_paths)\r\n\r\n        # a dataset that returns images (loaded off disk, decoded, and preprocessed)\r\n        image_ds = path_ds.map(self.load_and_preprocess_image, num_parallel_calls=self.AUTOTUNE)\r\n\r\n        # a dataset that returns labels\r\n        label_ds = tf.data.Dataset.from_tensor_slices(tf.cast(labels, tf.int64))\r\n\r\n        # a dataset that returns images and labels\r\n        image_label_ds = tf.data.Dataset.zip((image_ds, label_ds))\r\n\r\n        return image_label_ds\r\n\r\n    def get_final_dataset(self, data_root, SHUFFLE_SIZE, BATCH_SIZE):\r\n        self.SHUFFLE_SIZE = SHUFFLE_SIZE\r\n        self.BATCH_SIZE = BATCH_SIZE\r\n        self.AUTOTUNE = tf.data.experimental.AUTOTUNE\r\n\r\n        # a dataset that returns image paths\r\n        data_root = pathlib.Path(data_root)\r\n        all_image_paths = list(data_root.glob('*/**/*'))\r\n        all_image_paths = [str(path) for path in all_image_paths if str(path).lower().endswith(\"bmp\")]\r\n                                                \r\n        random.shuffle(all_image_paths)\r\n\r\n        self.label_names = sorted(item.name for item in data_root.glob('*/') if item.is_dir())\r\n        label_to_index = dict((name, index) for index, name in enumerate(self.label_names))\r\n        all_labels = [label_to_index[path.split('/')[-2]]\r\n                        for path in all_image_paths]\r\n\r\n        # separate dataset\r\n        train_paths, test_paths, train_labels, test_labels = train_test_split(all_image_paths, all_labels)\r\n\r\n        train_ds = self.labeled_dataset(train_paths, train_labels)\r\n        train_ds = self.prepare_for_training(train_ds)\r\n\r\n        test_ds = self.labeled_dataset(test_paths, test_labels)\r\n        test_ds = self.prepare_for_training(test_ds)\r\n\r\n        return train_ds, test_ds\r\n```\r\n- Model:\r\n```\r\ndef GamingModel(IMG_HEIGHT, IMG_WIDTH, category_num):\r\n    model = models.Sequential()\r\n    model.add(layers.Conv2D(32, (3, 3), activation='relu', \r\n                            input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)))\r\n    model.add(layers.MaxPooling2D())\r\n    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\n    model.add(layers.MaxPooling2D())\r\n    model.add(layers.Flatten())\r\n    model.add(layers.Dropout(rate=0.2))\r\n    model.add(layers.Dense(category_num, activation='softmax'))\r\n    model.compile(optimizer='adam',\r\n                loss='sparse_categorical_crossentropy',\r\n                metrics=['accuracy'])\r\n    model.summary()\r\n\r\n    return model\r\n```\r\n- Train:\r\n```\r\nfrom dataloader import DataSetGenerator\r\nfrom models import GamingModel\r\nfrom datetime import datetime\r\n\r\nIMG_HEIGHT = 90\r\nIMG_WIDTH = 160\r\n\r\ndata_root = \"/Volumes/External-SSD/dataset/labeled\"\r\ndsg = DataSetGenerator(IMG_HEIGHT, IMG_WIDTH)\r\ntrain_ds, test_ds = dsg.get_final_dataset(data_root, SHUFFLE_SIZE=4000, BATCH_SIZE=32)\r\n\r\ncategory_num = len(dsg.label_names)\r\n\r\nmodel = GamingModel(IMG_HEIGHT, IMG_WIDTH, category_num)\r\nes_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2, mode='auto')\r\nhistory = model.fit(train_ds, validation_data=test_ds, epochs=10, callbacks=[es_callback])\r\n\r\nnow = datetime.now()\r\nnow = now.strftime(\"%Y%m%d-%H%M\")\r\nmodel.save(f\"../endpoints/game_state_{now}.h5\")\r\n```\r\n\r\n### Log\r\n```\r\n2020-08-31 17:33:39.847255: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-08-31 17:33:39.860848: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fd061d24690 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-08-31 17:33:39.860873: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\nconv2d (Conv2D)              (None, 88, 158, 32)       896\r\n_________________________________________________________________\r\nmax_pooling2d (MaxPooling2D) (None, 44, 79, 32)        0\r\n_________________________________________________________________\r\nconv2d_1 (Conv2D)            (None, 42, 77, 64)        18496\r\n_________________________________________________________________\r\nmax_pooling2d_1 (MaxPooling2 (None, 21, 38, 64)        0\r\n_________________________________________________________________\r\nflatten (Flatten)            (None, 51072)             0\r\n_________________________________________________________________\r\ndropout (Dropout)            (None, 51072)             0\r\n_________________________________________________________________\r\ndense (Dense)                (None, 12)                612876\r\n=================================================================\r\nTotal params: 632,268\r\nTrainable params: 632,268\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nEpoch 1/10\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 24, in <module>\r\n    history = model.fit(train_ds, validation_data=test_ds, epochs=10, callbacks=[es_callback])\r\n  File \"/Users/macuser/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 108, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/Users/macuser/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1098, in fit\r\n    tmp_logs = train_function(iterator)\r\n  File \"/Users/macuser/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/Users/macuser/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 840, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/Users/macuser/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2829, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/Users/macuser/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1848, in _filtered_call\r\n    cancellation_manager=cancellation_manager)\r\n  File \"/Users/macuser/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1924, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/Users/macuser/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 550, in call\r\n    ctx=ctx)\r\n  File \"/Users/macuser/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError:  channels attribute 3 does not match bits per pixel from file 288\r\n\t [[{{node DecodeBmp}}]]\r\n\t [[IteratorGetNext]] [Op:__inference_train_function_755]\r\n\r\nFunction call stack:\r\ntrain_function\r\n```", "comments": ["I see the problem is not from the SSD, is a part of data can't be decode, how I can solve that?", "I solved that, follow up for others need it. It was because of some broken images and hidden auto-generated images can't be decoded. Preprocess all images one by one using Tensorflow would tell you which image can't be decoded.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42844\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42844\">No</a>\n"]}, {"number": 42843, "title": "[INTEL MKL] DNN 0.x cleanup - softmax op", "body": "DNN 0.x cleanup of MKL softmax op:\r\n\r\n(1) Remove all DNN 0.x related code\r\n(2) Replace all DNN 1.x macro usages with actual APIs", "comments": []}, {"number": 42842, "title": "TFLM: added log_parser to vexriscv/utils", "body": "This PR adds `log_parser.py` for vexriscv in TFLM and a README with it.\r\n\r\nThe script is used to analyze and visualize function call log obtained from a proper script with GDB attached on Renode or Renode's built-in `LogFunctionNames` option.", "comments": ["Looks like we're getting some pylint errors. Can you follow the steps from [here](https://www.tensorflow.org/community/contribute/code_style#python_style) to fix those errors?", "Pylint didn't seem to catch the formatting errors, but yapf did. Fixed with [this commit](https://github.com/tensorflow/tensorflow/pull/42842/commits/8391515c2d52b0cdbdc4b0bc3e9c8eec1e8edf25).\r\n\r\n```\r\nyapf log_parser.py -i --style='{based_on_style: pep8, indent_width: 2}'\r\n```"]}, {"number": 42841, "title": "org.tensorflow:tensorflow:jar:2.3.0 not found via Maven", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OSX 10.15.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.15.0 (current), 2.3.0 (future)\r\n- Python version: N/A\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\nUnable to resolve `tensorflow` Java artifact via Maven at version 2.3.0, as per official [documentation](https://www.tensorflow.org/install/lang_java#tensorflow_with_apache_maven).\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nIn my project's pom.xml file, modified `${tensorflow.version}` value in following snippet from 1.15.0 to 2.3.0 as per official [documentation](https://www.tensorflow.org/install/lang_java#tensorflow_with_apache_maven) and re-ran my project's build (which works with 1.15.0, as expected):\r\n\r\n```\r\n    <dependency>\r\n      <groupId>org.tensorflow</groupId>\r\n      <artifactId>tensorflow</artifactId>\r\n      <version>${tensorflow.version}</version>\r\n    </dependency>\r\n```\r\n\r\n**Any other info / logs**\r\n\r\n```\r\nCould not find artifact org.tensorflow:tensorflow:jar:2.3.0 in maven-central (https://repo1.maven.org/maven2)\r\n```", "comments": ["https://github.com/tensorflow/tensorflow/issues/32943#issuecomment-684061320", "To expand on @pluradj 's response, it appears that:\r\n* The Java APIs of [tensorflow/tensorflow/java/](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java) are deprecated, unsupported, and incompatible with versions of TensorFlow later than 1.15.\r\n* There is a replacement Java API being developed by the SIG-Java community, which is **not** the same team that maintains the main TensorFlow project. None of the main contributors work for Google. The replacement Java API is under https://github.com/tensorflow/java\r\n* The replacement API has not yet had a stable release.\r\n* Instructions for compiling Java programs directly against an unstable nightly build of the new Java API can be found at https://github.com/tensorflow/java#using-maven-artifacts\r\n* The correct unstable version of the new Java API to use with TensorFlow 2.3.0 is `0.2.0-SNAPSHOT`\r\n* The Java/Maven installation documentation at https://www.tensorflow.org/install/lang_java is completely wrong.\r\n* The Java API documentation at https://www.tensorflow.org/api_docs/java is completely wrong.\r\n\r\n@angerson is this correct?\r\n", "To my limited knowledge, I believe you are correct. Thank you for your helpful summary.\r\n\r\n@lamberta @martinwicke If this is intentionally true, we'll need to remove or update these misleading documentation pages. What's the right step forward for that?", "@MarkDaoust was working on some javadocs recently, ideas?", "@trsudarshan, Could you please let us know if this issue still persists ? Does [this reference](https://github.com/tensorflow/java/#using-maven-artifacts) help you? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42841\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42841\">No</a>\n", "@chunduriv thank you, that reference helps."]}, {"number": 42840, "title": "Revert \"Fix sanity build on 1.15\"", "body": "Reverts tensorflow/tensorflow#42829", "comments": []}, {"number": 42839, "title": "Fix sanity build failures", "body": "", "comments": []}, {"number": 42838, "title": "Flags For activating AVX Bazel Build CPP Windows", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the problem**\r\n\r\nI successfully managed to build a TensorFlow library for CPP, Windows, but that was without AVX instructions. I want to enable the AVX instructions.  I want to take advantage of it and improve my inference speed.\r\n\r\n**Provide the exact sequence of commands/steps that you executed before running into the problem**\r\nI tried the following flags, but during the build, it doesn't recognize it. Looks like that is only valid for Python package.\r\n- bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 \r\n\r\n**Any other info / logs**: Error/Warning on the build\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx'\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@amitpandey2194 \r\n\r\nJust verify, did you the follow the instructions mentioned in this [link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/lib_package/README.md).\r\nPlease take a look at the [tested build configuration ](https://www.tensorflow.org/install/source_windows#cpu)of Tensorflow 1.12. Thanks!", "Yeah I verify I that I used above mentioned build steps. \r\nI have already build tensor flow library, but that is without avx avx2 instructions. I want to know how to build one for cpp with avx instructions enabled using bazel build. Which flags to use?", "@amitpandey2194 \r\n\r\nDoes your cpu support AVX instructions?\r\nPlease, see [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements).Thanks!", "Yeah, it does. Its intel core i5", "Afaik, windows compiler flags are with `/`, not with `-`. You should consult the flag documentation for the Windows compiler you are using. `-copt` options get passed verbatim to the compiler.", "@mihaimaruseac I am using bazel for building the library. Then what flags should I use for incorporating the avx and avx2 build?\r\nCan you please also point me to the documentation you are referring to? ", "Please track also https://github.com/tensorflow/tensorflow/issues/42960", "@amitpandey2194 To confirm, initial issue description mentions `conda`, although it's an issue about building from source. Is `conda` involved here to any capacity?\r\n\r\nIs the TF `1.12.0` version number correct, trying to build from a ~2 years old source? Doing this requires a rather old bazel (0.15) and MSVC (v2015). Might be worth considering the option of migrating to TF2.2 or 2.3 :) I've built successfully with AVX2 enabled under Windows since TF2.0 release, for every point release.\r\n\r\nUnless the `conda` is part of the build recipe, normally the configuration parameters are passed using `python configure.py`. For Windows, `/arch:AVX2` optimization flag enables AVX2.", "You should be able to use `--config` flags: https://cs.opensource.google/tensorflow/tensorflow/+/master:.bazelrc;l=368-369", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42838\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42838\">No</a>\n"]}, {"number": 42837, "title": "TFLite (tools) added reverse_xxd_dump_from_cc.py", "body": "This PR adds a Python script as a tool to convert C++ array source file dumped with command `xxd -i model_data.tflite > model_data.cc` back to its original binary file (`mode_data.tflite` in the example.)\r\n\r\nThe script is useful when developers have only the C++ source file in hand but would like to visualize the model with some visualizers (like [Netron](https://github.com/lutzroeder/netron)) or evaluate the model with TensorFlow Python API.\r\n\r\nAs pointed out by @advaitjain, this script is generic enough to be put under `tensorflow/lite/tools`.", "comments": []}, {"number": 42836, "title": "[INTEL MKL] DNN 0.x cleanup - lrn op", "body": "DNN 0.x cleanup of LRN op (core/kernels/mkl/mkl_lrn_op.cc) :\r\n\r\n(1) Remove all DNN 0.x related code blocks\r\n(2) Replace all DNN 1.x macro usages with actual APIs\r\n", "comments": []}, {"number": 42835, "title": "tf.GradientTape().jacobian() triggers retracing ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  dockerhub container 'latest' Digest: 7bc36fe0ca1a051a808122e87f5438614b371263515df4794abef9a78440af8b\r\n- TensorFlow version (use command below):\r\ntf.version.GIT_VERSION=v2.3.0-rc2-23-gb36436b087\r\ntf.version.VERSION=2.3.0\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\n\r\nThe use of tf.GradientTape().jacobian() triggers retracing\r\n\r\n**Describe the expected behavior**\r\nNo retracing\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nsimple example:\r\n\r\n```python\r\nimport tensorflow as tf\r\nprint('tf.version.GIT_VERSION={}'.format(tf.version.GIT_VERSION))\r\nprint('tf.version.VERSION={}'.format(tf.version.VERSION))\r\n\r\nx = tf.ones((1,1))\r\nfor k in range(0, 10):\r\n    with tf.GradientTape() as tape:\r\n        tape.watch(x)\r\n        y = 2*x\r\n    g = tape.jacobian(y, x)\r\n```\r\n\r\n**Other info / logs** \r\n\r\n```\r\ntf.version.GIT_VERSION=v2.3.0-rc2-23-gb36436b087\r\ntf.version.VERSION=2.3.0\r\nWARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x7f89839c8e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\r\n```\r\n\r\n\r\n", "comments": ["Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/c9eff0a2cb2a538cfb1fefc01a74d991/42835.ipynb#scrollTo=UtnyeZ10BjsV) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/24cb215026b64262e2734cf450f17e0f/42835-tf-nightly.ipynb). Please find the attached gist. Thanks!", "I am encountering the same issue, very curious to understand what is causing this. Using tf.GradientTape().gradient() on the same (x,y) variables works fine.", "This is because the jacobian implementation uses parallel for that creates a tf.function under the hood for each jacobian call. \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/parallel_for/control_flow_ops.py#L204\r\n\r\nYou're running the tape.jacobian 10 times and so we'll end up creating 10 tf.functions. \r\n\r\ntape.gradient doesn't create a tf.function underneath.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42835\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42835\">No</a>\n", "Hi @rohan100jain . Thanks for the explanation.  I guess this is a known issue then, and that is why you are closing this one?  How can I know when is this resolved?  Or should I never use tf.GradientTape().jacobian() within a loop?\r\n", "@andrescodas, sorry for the late response here but this issue was closed because it is intended behavior. You can wrap the whole code into another tf.function if you intend to avoid recompilation and vectorization rewrites on each call.", "@nikitamaia , thank you!.\r\n\r\n> You can wrap the whole code into another tf.function if you intend to avoid recompilation\r\n\r\nThis was not obvious, and that is why I came up the question.  I got the answer from issue #43710\r\n\r\nThis is something that could be documented better, i.e., indicating when `tf.function` is required to prevent retracing or for performance.  I tend to avoid decorating methods with `tf.function`.  [Here](https://github.com/tensorflow/tensorflow/issues/43252#issuecomment-696991585) I found and instance when wrapping a method with `tf.function` degrades the performance.", "Note that tf.function inside the jacobian is an implementation detail of the jacobian function, as is the retracing that happens internally. The logging message could in fact be suppressed in this case. User supplied code has no tf.function so they should not care about retracing here. \r\n\r\nGradietTape code is eagerly building and destroying the tape data structure. This is already inefficient and users should generally try tf.function around such code to get better performance. Jacobian has much bigger overheads than GradientTape, as the function also performs code vectorization eagerly. So the performance delta would be much larger. This could be better reflected in the documentation of jacobian as a performance optimization. This could not be enforced since the code in GradientTape scope may in fact be dynamic and may need re-evaluation of jacobian as well as re-vectorization.\r\n\r\n", "It may be helpful to users to have this message suppressed if it is intended (just spent a lil' bit of time trying to get rid of it to improve the performance of my code before stumbling upon this thread :))."]}, {"number": 42834, "title": "Nan Loss with Adam in MultiWorkerMirroredStrategy ", "body": "Hello \r\nI have a custom training loop that uses MultiWorkerMirroredStrategy. I use tf.keras.losses.SparseCategoricalCrossentropy as the loss function and tf.keras.optimizers.Adam as the optimizer. I noticed that when I train the network with 4 nodes (each has 1 P100 16GB GPU) using gRPC communication layer, loss becomes NaN after some steps. With the same code, this time I use only 1 node with 4 V100 32GB GPUs and training works smoothly without any issue. For example:\r\n```\r\n### with 4 nodes the lost sharply drops with few steps\r\nStep      10 ( 3.21%): loss = 3.98384 EER=0.38987 \r\nStep      20 ( 6.41%): loss = 3.71272 EER=0.30024 \r\nStep      30 ( 9.62%): loss = 3.50568 EER=0.25225 \r\nStep      40 (12.82%): loss = 3.32774 EER=0.21305 \r\nStep      50 (16.03%): loss = 3.19242 EER=0.19232 \r\nStep      60 (19.23%): loss = 3.09014 EER=0.17402 \r\nStep      80 (23.64%): loss = 0000nan EER=0.00000\r\n```\r\n```\r\nwith 1 node and 4 GPUs is working fine\r\nStep       0 ( 0.00%): loss = 4.00920 EER=0.40084 \r\nStep      10 ( 0.83%): loss = 3.92024 EER=0.38026 \r\nStep      20 ( 1.67%): loss = 3.78321 EER=0.34209 \r\nStep      30 ( 2.50%): loss = 3.77414 EER=0.33503 \r\nStep      40 ( 3.33%): loss = 3.70165 EER=0.31749 \r\nStep      50 ( 4.17%): loss = 3.65104 EER=0.30819 \r\nStep      60 ( 5.00%): loss = 3.64083 EER=0.30599 \r\nStep      70 ( 5.83%): loss = 3.58572 EER=0.29448 \r\nStep      80 ( 6.67%): loss = 3.57196 EER=0.29532 \r\n```\r\n\r\nCould the problem be the GPUs? or the communication layer?\r\nI also tested with low learning rate and did not help with Adam optimizer.\r\n\r\nThanks \r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution: 'Red Hat Enterprise Linux Server', '7.7'\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: v2.3.0-rc2-23-gb36436b087 2.3.0\r\n- Python version:  Python 3.7.7\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: P100 16 GB and V100 32GB\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@ha3an \r\nPlease provide with simple stand alone code for us to replicate the issue faced or if possible share a colab gist.", "It would take a while until I can create a stand alone code. I will close this for now. Thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42834\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42834\">No</a>\n"]}, {"number": 42833, "title": "[INTEL MKL] DNN 0.x code cleanup - mkl_util.h", "body": "DNN 0.x cleanup of MKL utility (core/util/mkl_util) :\r\n(1) Remove all DNN 0.x related code\r\n(2) Replace all DNN 1.x macro usages\r\n\r\nNote: mkl_types.h is still included, because some MKL kernel ops uses some \"macro's\" within it by only include \"mkl_util.h\" file.\r\nOnce we clean up all MKL ops, we will remove that included file.", "comments": []}, {"number": 42832, "title": "tflite(micro) resize_nearest_neightbour_test fails for BUILD_TYPE=debug", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (SHA-1 source): 44dbace063796ea99c5b3d27a3a5810048d5096c\r\n\r\n\r\n**Describe the current behavior**\r\nTest fails assertions in kernel implementation for BUILD_TYPE=debug build due to size tensor being provided as 2D non-constant tensor.\r\n\r\n**Describe the expected behavior**\r\n\r\nTest should pass.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nmake -f tensorflow/lite/micro/tools/make/Makefile BUILD_TYPE=debug test_kernel_resize_nearest_neighbor_test\r\n\r\nSmall unified diff patch to correctly setup the size input tensor as 1D constant to correct the issue is attached.\r\n\r\n[resize_nearest_neighbor_test.patch.txt](https://github.com/tensorflow/tensorflow/files/5151701/resize_nearest_neighbor_test.patch.txt)\r\n", "comments": ["Thanks, created a PR based on your patch.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42832\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42832\">No</a>\n"]}, {"number": 42831, "title": "Error while installing latest tensorflow version 2.3", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Nil\r\n- TensorFlow installed from (source or binary): pip install tensorflow\r\n- TensorFlow version: 2.3.0\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): Nil\r\n- GCC/Compiler version (if compiling from source): Anaconda prompt\r\n- CUDA/cuDNN version: Nil\r\n- GPU model and memory: 64 bit CPU\r\n\r\n\r\n\r\n<b>Python 3.7.8 | packaged by conda-forge | (default, Jul 31 2020, 01:53:57) [MSC v.1916 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\dell\\Anaconda3\\envs\\ox\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\dell\\Anaconda3\\envs\\ox\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\dell\\Anaconda3\\envs\\ox\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"C:\\Users\\dell\\Anaconda3\\envs\\ox\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n  File \"C:\\Users\\dell\\Anaconda3\\envs\\ox\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\dell\\Anaconda3\\envs\\ox\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\dell\\Anaconda3\\envs\\ox\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>></b>\r\n\r\n", "comments": ["@aditi1122000,\r\nInstallation issues within the Anaconda environment are tracked in the Anaconda repo.\r\n\r\nCould you please submit a new issue using [this link](https://github.com/ContinuumIO/anaconda-issues/issues) and fill in the template, so that the issue can be tracked there. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42831\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42831\">No</a>\n"]}, {"number": 42830, "title": "Better Instruction/Insight as to how models.load_model() custom objects are loaded (doc/feature request/bug?)", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n[https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model?hl=no]()\r\n\r\nIt may be better to have a separate URL for new documentation.\r\n\r\n## Description of issue (what needs changing):\r\nThis could be treated potentially as a feature request and/or bug as the issues encountered may or may not be a result of things out of my control. Better documentation is requested regardless.\r\n\r\nThe documentation on how to load models that contain custom objects is lacking as to prerequisites to make them loadable.\r\n\r\nThe only thing that is clear is that a dict of the objects of interest must be provided. It isn't clear what they keys are supposed to be. It is also unclear as to how the objects are loaded and what happens at load time.\r\n\r\nWhen running this code\r\n\r\n```\r\n    def load_model(self, model_name, model_path):\r\n        \"\"\"Reload tensorflow session for saved model. Called by djinn.load,\r\n\r\n        Args: \r\n            model_path (str, optional): Location of model if different than \r\n                       location set during training.\r\n            model_name (str, optional): Name of model if different than \r\n                       name set during training.\r\n        \r\n        Returns: \r\n            Object: djinn regressor model.\r\n            \r\n        \"\"\"\r\n        # self.__sess = {}\r\n        self.__models={}\r\n        for p in range(0, self.__n_trees):\r\n            tf.keras.backend.clear_session()\r\n            # tf.reset_default_graph()\r\n            # new_saver = \\\r\n            # tf.train.import_meta_graph('%s%s_tree%s.ckpt.meta'%(model_path,model_name,p))\r\n            # self.__sess[p] = tf.Session()\r\n            # new_saver.restore(self.__sess[p], '%s%s_tree%s.ckpt'%(model_path,model_name,p))\r\n            self.__models[p] = tf.keras.models.load_model('%s%s_tree%s.ckpt'%(model_path,model_name,p),custom_objects={'WB_Init': WB_Init})\r\n            # self.__models[p] = tf.keras.models.load_model('%s%s_tree%s.ckpt'%(model_path,model_name,p))\r\n            print(\"Model %s restored\"%p)\r\n```\r\n\r\nwhere WB_Init is defined in another file as\r\n\r\n```\r\nclass WB_Init(tf.keras.initializers.Initializer):\r\n    def __init__(self,dat=None,name=None):\r\n        self.dat = dat\r\n        self.name = name\r\n        # print(type(dat))\r\n        # print(dat)\r\n                \r\n    def __call__(self,shape,dtype):\r\n        \r\n        if not isinstance(self.dat,tf.Tensor):\r\n            a = tf.convert_to_tensor(self.dat,dtype=tf.float32,name=self.name)\r\n        else:\r\n            # a = self.dat.value()\r\n            a = self.dat\r\n                    \r\n        return a\r\n```\r\n\r\nand the file doing the calling is indeed loading the object with\r\n```\r\ntry:\r\n    from djinn_fns import tree_to_nn_weights, tf_dropout_regression, \\\r\n                    get_hyperparams, tf_continue_training, WB_Init\r\nexcept:\r\n    from djinn.djinn_fns import tree_to_nn_weights, tf_dropout_regression, \\\r\n                    get_hyperparams, tf_continue_training, WB_Init\r\n```\r\n\r\nWB_Init is used to initialize weights and biases that have been predefined by other functions and need to be trainable. I have avoided the constant initializer because there is no immediate documentation to suggest the values produced by it are capable of being trained (another documentation clarification maybe).\r\n\r\nWhen trying load in an already trained model I'm greeted with the error traceback:\r\n\r\n```\r\nTraceback (most recent call last):\r\n\r\n  File \"C:\\Users\\Michael\\Desktop\\Code Projects\\DJINN TF2\\tests\\djinn_regression_example.py\", line 81, in <module>\r\n    m=model.predict(x_test) #returns the median prediction if more than one tree\r\n\r\n  File \"..\\djinn\\djinn.py\", line 383, in predict\r\n    return self.bayesian_predict(x_test, None, random_state)\r\n\r\n  File \"..\\djinn\\djinn.py\", line 341, in bayesian_predict\r\n    if(self.__models == None): self.load_model(self.modelname, self.modelpath)\r\n\r\n  File \"..\\djinn\\djinn.py\", line 294, in load_model\r\n    self.__models[p] = tf.keras.models.load_model('%s%s_tree%s.ckpt'%(model_path,model_name,p),custom_objects={'WB_Init': WB_Init})\r\n\r\n  File \"C:\\Users\\Michael\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\", line 182, in load_model\r\n    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\r\n\r\n  File \"C:\\Users\\Michael\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\hdf5_format.py\", line 178, in load_model_from_hdf5\r\n    custom_objects=custom_objects)\r\n\r\n  File \"C:\\Users\\Michael\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\model_config.py\", line 55, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n\r\n  File \"C:\\Users\\Michael\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\serialization.py\", line 175, in deserialize\r\n    printable_module_name='layer')\r\n\r\n  File \"C:\\Users\\Michael\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\", line 358, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n\r\n  File \"C:\\Users\\Michael\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\", line 617, in from_config\r\n    config, custom_objects)\r\n\r\n  File \"C:\\Users\\Michael\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\", line 1214, in reconstruct_from_config\r\n    process_node(layer, node_data)\r\n\r\n  File \"C:\\Users\\Michael\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\", line 1162, in process_node\r\n    output_tensors = layer(input_tensors, **kwargs)\r\n\r\n  File \"C:\\Users\\Michael\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py\", line 757, in __call__\r\n    self._maybe_build(inputs)\r\n\r\n  File \"C:\\Users\\Michael\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py\", line 2098, in _maybe_build\r\n    self.build(input_shapes)\r\n\r\n  File \"C:\\Users\\Michael\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\", line 1178, in build\r\n    trainable=True)\r\n\r\n  File \"C:\\Users\\Michael\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py\", line 448, in add_weight\r\n    caching_device=caching_device)\r\n\r\n  File \"C:\\Users\\Michael\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 750, in _add_variable_with_custom_getter\r\n    **kwargs_for_getter)\r\n\r\n  File \"C:\\Users\\Michael\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_utils.py\", line 145, in make_variable\r\n    shape=variable_shape if variable_shape else None)\r\n\r\n  File \"C:\\Users\\Michael\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 260, in __call__\r\n    return cls._variable_v1_call(*args, **kwargs)\r\n\r\n  File \"C:\\Users\\Michael\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 221, in _variable_v1_call\r\n    shape=shape)\r\n\r\n  File \"C:\\Users\\Michael\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 199, in <lambda>\r\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\r\n\r\n  File \"C:\\Users\\Michael\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2597, in default_variable_creator\r\n    shape=shape)\r\n\r\n  File \"C:\\Users\\Michael\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 264, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n\r\n  File \"C:\\Users\\Michael\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 1518, in __init__\r\n    distribute_strategy=distribute_strategy)\r\n\r\n  File \"C:\\Users\\Michael\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 1651, in _init_from_args\r\n    initial_value() if init_from_fn else initial_value,\r\n\r\n  File \"..\\djinn\\djinn_fns.py\", line 873, in __call__\r\n    a = tf.convert_to_tensor(self.dat,dtype=tf.float32,name=self.name)\r\n\r\n  File \"C:\\Users\\Michael\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1382, in convert_to_tensor_v2\r\n    as_ref=False)\r\n\r\n  File \"C:\\Users\\Michael\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1499, in convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n\r\n  File \"C:\\Users\\Michael\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 338, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n\r\n  File \"C:\\Users\\Michael\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 264, in constant\r\n    allow_broadcast=True)\r\n\r\n  File \"C:\\Users\\Michael\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 282, in _constant_impl\r\n    allow_broadcast=allow_broadcast))\r\n\r\n  File \"C:\\Users\\Michael\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\", line 444, in make_tensor_proto\r\n    raise ValueError(\"None values not supported.\")\r\n\r\nValueError: None values not supported.\r\n```\r\nwhere it is clear as to the \"precise reason\" why load is failing. That reason is because the WB_Init.__init__ has a default `dat=None`\r\nwhich when WB_Init.__call__ is used returns a None value which isn't a support return type for the initialization process. This is what happens when saved to HDF5 format. The tensorflow format breaks in a different manner at save time with the final error being `FailedPreconditionError: ./reg_djinn_test_tree0.ckpt is not a directory`. \r\n\r\nHowever, it clearly demonstrates that a fresh call of a WB_Init object is made at load time which doesn't make sense to me because the model has already been trained. It should have weights to load. The initializer shouldn't be run as far as I know. Setting `compile=False` doesn't fix this problem. If I just wanted to load the model for inference, I could potentially try to completely reconstruct the model from some other pickled custom object that hold the layer topology and then just use load_weights, but this is a horribly dirty solution. Using this workaround also doesn't let me save the optimizer state so that the model training can be picked-up at the most recent optimizer state.\r\n\r\nThere is another hypothetical fix where WB_Init is fed the saved weights at load time, but I would have no idea of how to make this happen.\r\n\r\nThere is also the highly non-preferred way of initializing the network at construction by filling it the models with random numbers and loading the pre-calculated weights after construction (and compilation?) of the model. This defeats the purpose of providing an initializer option at construction time though.\r\n\r\n### Clear description\r\n\r\nPlease provide more a more clear description about how to make custom intializers (or any custom object ) and, more importantly, how to load custom objects (dict keys?) and the prerequisites for making the model and/or object loadable.\r\n\r\nThis problem and problems like this have been recurring. Some of them don't make sense to me, some are TF1 and don't apply, and some simply don't fix my issue. Some have simply been bugs.\r\n\r\n[https://github.com/tensorflow/tensorflow/issues/39752]() ( this is the one that actually demonstrated how to use the 'custom_object' option )\r\n[https://github.com/tensorflow/tensorflow/issues/42778]()\r\n[https://github.com/qqwweee/keras-yolo3/issues/48]()  (referenced in the link above)\r\n[https://github.com/tensorflow/tensorflow/issues/33646]()\r\n[https://github.com/titu1994/keras-efficientnets/issues/3]()\r\n[https://github.com/tensorflow/tensorflow/issues/26061]()\r\n\r\nIt clearly could use better documentation.\r\n\r\nI will provide a full script upon request, and if it is possible, I'd prefer to supply it privately. I'm currently trying to convert a TF1/(TF2 with compatibility) model to a TF2 model that uses Keras. It will (almost certainly) break even if this problem is fixed because the conversion is in progress. The break should be different in nature though if this problem is fixed.\r\n\r\n\r\n### Correct links\r\n\r\nN/A\r\n\r\n### Parameters defined\r\n\r\nN/A\r\n\r\n### Returns defined\r\n\r\nN/A\r\n\r\n### Raises listed and defined\r\n\r\nN/A\r\n\r\n### Usage example\r\n\r\nSee above, but it isn't usable without the rest of the files.\r\n\r\n\r\n### Submit a pull request?\r\nNo\r\n", "comments": ["@EnderWiggin14 \r\nCould you please let us know the tf version on which this issue is faced and share a  colab gist with the error reported.", "I'm using TF==2.30.\r\n\r\nOddly, the code broke where is wasn't breaking before because of something to do with metrics as I had passed \"callbacks=tf.keras.callbacks.BaseLogger()\" and it was choking on metrics. This doesn't happen with Spyder IDE.\r\n\r\nHere is colab gist. I couldn't figure out how to use regular .py files without giving access to my whole google drive, so this will have to do.\r\n\r\nhttps://colab.research.google.com/drive/1f-eQ22tKMc4o8j3gV81mS2ZIlk-_x34T?usp=sharing\r\n\r\nIt produces the same error as well.", "@EnderWiggin14 \r\nCan you please refer to these resolved issues with same error and let us know if it helps: #30120 [link](https://stackoverflow.com/questions/44280135/tensorflow-is-there-a-way-to-convert-a-list-with-none-type-to-a-tensor) [link1](https://blog.csdn.net/qq_32623363/article/details/106503695)", "@Saduf2019\r\nAre you referencing the possibility of using tf.TensorShape? I have added a bypass statement such that\r\n\r\n`\r\ndef __call__(self,shape,dtype):\r\n        \r\n    if self.dat is None:\r\n        a = tf.TensorShape(shape)            \r\n    else:\r\n        a = tf.convert_to_tensor(self.dat,dtype=tf.float32,name=self.name)\r\n                    \r\n    return a\r\n`\r\nIt didn't work. I'm met with the error `TypeError: Cannot convert a TensorShape to dtype: <dtype: 'float32'>` which, from what I read about TensorShape, this is more or less what I expected.\r\n \r\nOtherwise, I think you have misunderstood the problem. I'm aware of the reason as to why `convert_to_tensor` is failing. I'm well aware that `None` type isn't supported. The problem is that I'm trying to use a custom a initializer which because it tries to run the initializer at load time which fails. I have tried to default to a 2x2 array, but that causes an incompatibility with the first layer.\r\n\r\nTLDR: The error thrown is one of the kinds of bugs where the line with the problem isn't the actual problem. The problem is caused by unexpected behavior somewhere else in the code. That somewhere else is the process followed by models.load_model.\r\n\r\nThe two ways I can think of how to fix this are\r\n\r\n1. Somehow give a fully valid Tensor of correct shape and dtype without overwriting the existing weights\r\n2. Feed the correct weights as the input at load time.\r\n\r\nNeither of these things appear to be easily accomplished in Python (i.e. without messing with source code). My base assumption is that there is a known way to do what I need want with needing to modify source code or create new TF source code.\r\n\r\nI'm looking for how a person is supposed to be able to create a custom initializer, save the model that uses it, and re-load the model without having a problem.\r\n\r\nI'm not looking initialize with a supplied initializer and then overwriting the weights with the correct weights. This defeats the purpose of being able to create a custom initializer.", "@Saduf2019 \r\n\r\nWhat Tensorflow version does colab currently default to?\r\n\r\nAttached is a screen shot of the error I got from colab. It might be worded a bit differently, but it is the same error.\r\n\r\n![colab error](https://user-images.githubusercontent.com/18318520/91885267-7fe04180-ec55-11ea-832b-9eef73bd002d.PNG)\r\n", "@EnderWiggin14 It defaults to tensorflow 2.3.0 now", "Hi @EnderWiggin14 ,\r\n\r\nThe problem is here:\r\n\r\n```\r\n def __init__(self,dat=None,...):\r\n```\r\n\r\nTo make a non-trivial object keras-serializable it should define `get_config` and `from_config` methods, [as described here](https://www.tensorflow.org/guide/keras/save_and_serialize#custom_objects)\r\n\r\nSince you haven't defined those methods, when keras reconstructs your objects it calls `WB_Init` with no arguments, so `dat` takes the default value of `None`.\r\n\r\nAlso note in this case instead of defining an initializer, you should consider just setting the variable value after creating the layer (otherwise the initializer needs to remember the initial value, which seems unnecessary)."]}, {"number": 42829, "title": "Fix sanity build on 1.15", "body": "", "comments": []}, {"number": 42828, "title": "Make effective scale in quantize kernel consistent b/w Lite and Micro", "body": "There is no reason for the Micro implementation to do a division in single precision.\r\n\r\nFixes #42648", "comments": []}, {"number": 42827, "title": "tflite suppport for LSTM / RNN", "body": "Per [tflite roadmap](https://www.tensorflow.org/lite/guide/roadmap#usability) there is upcomming support forLSTM / RNN - when we can expect this for tflite?", "comments": ["Please take a look at the following guide document for RNN model support:\r\n\r\nhttps://www.tensorflow.org/lite/convert/rnn", "@peter197321 \r\n\r\nPlease, refer the below link and see if it helps you.\r\n\r\nhttps://groups.google.com/a/tensorflow.org/g/tflite/c/Ub4apUvblN8\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42825, "title": "Custom loss using intermediate tensorflow probability layer with Eager execution", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, a colab [here](https://colab.research.google.com/drive/1nRvN-edSeP1q1Q2lCCwXm3RV7Yunxh--?usp=sharing).\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Locally, Ubuntu 20.04, otherwise current colab.\r\n- TensorFlow installed from (source or binary): See colab.\r\n- TensorFlow version (use command below): v2.3.0-0-gb36436b087\r\n- Python version: Locally, 3.8.2\r\n\r\n**Describe the current behavior**\r\nWhen using a custom loss based on tensorflow probability `DistributionLambda`, or a distribution without `DistributionLambda`, I cannot move into Eager execution. In the first case, the gradients are not propagated properly, so variables say they have no gradients when training, and, in the second case, the layers are symbolic tensors when creating the network, so they cannot be used in the Eager execution loss.\r\n\r\n**Describe the expected behavior**\r\nI understand the problem. What I would like is a way to have the intermediate values within the network to be used in the loss. A regularization loss is not enough, as I need the outputs of the whole network, together with the intermediate layer values in order to compute the loss.\r\n\r\n**Standalone code to reproduce the issue**\r\nA [colab](https://colab.research.google.com/drive/1nRvN-edSeP1q1Q2lCCwXm3RV7Yunxh--?usp=sharing).\r\n\r\n**Other info / logs**\r\nPlease see the attached colab. A useful piece of information is that I am using a class to create the model because this is part of a bigger project, and the network is much bigger, so while in the example I showed the problem in one of the last layers, this could happen for a layer that is in the middle of the network, and I would not like to add all intermediate layers as outputs of the model.\r\n", "comments": ["The only workaround I have found is to add the intermediate layers as outputs of the model and access them in the loss function, although then I have to post process outputs before giving them to further code (this is part of a bigger project). I think that this solution works, but it is definitely what I wanted.", "@cserpell \r\nFor the error \" No gradients provided for any variable\" please refer to #27949 #41162 [link](https://stackoverflow.com/questions/42498876/tensorflow-no-gradients-provided-for-any-variable-and-partial-run/42504176)\r\n\r\nFor the error \" SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors\" #34944 [link](https://stackoverflow.com/questions/57704771/inputs-to-eager-execution-function-cannot-be-keras-symbolic-tensors) [link1](https://datascience.stackexchange.com/questions/64433/symbolicexception-inputs-to-eager-execution-function-cannot-be-keras-symbolic-t), and let us know.\r\n", "Reading many issues and questions like those mentioned, the only workaround was adding them as outputs. There is no bug, but it seems no easy way to do it without adding them as outputs.", "@cserpell \r\nPlease verify and let us know if it helps resolve the issue faced.", "As I mentioned, I added the intermediate layers as outputs to the model, and then read the outputs in the loss. Although not what I wanted, it works.", "@cserpell\r\nThank you for your update, can we move this to closed status as issue is resolved.", "@cserpell you had the same issue as mine, please take look at this gist [Here](https://colab.research.google.com/drive/1qGZYgWRZoYXIeMKIEVKLf4hTo24b0CHv?usp=sharing), i think you'll find what you're looking for.\r\n\r\n", "@cserpell \r\nPlease update as per above comment.", "My model has probabilistic random layers, so I needed the actual output instead of re evaluations of the layers, so I concatenated the internal values as outputs of the model, and that helps me at evaluation time as well, as I can print the actual internal layers activations. Thanks anyway, I think that also helps.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42825\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42825\">No</a>\n"]}, {"number": 42824, "title": "tf.compat.v1.data.get_output_shapes(datasets) gives Errors", "body": "## Iam getting different types of errors in different projects whenever I use tf.compat.v1.data.get_output_shapes(datasets)\r\nI have tried all the Codes in Colab only.\r\n</em>\r\n\r\n**System information**\r\n### COLAB\r\n\r\n1. Tensorflow Version :  Latest on Colab : 2.3.0\r\n2. Python 3\r\n3. COLAB GPU\r\n\r\n\r\n**Current behavior :**\r\nWhile using **tf.compat.v1.data.get_output_shapes(datasets), I got 2 types of errors in 2 projects:**\r\n1. ValueError: logits and labels must have the same shape ((None, 1) vs ())\r\n2. ValueError: Input 0 of layer global_average_pooling1d is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [None, None, None, 128]\r\n\r\n**As a temporary solution, I used datasets.output_shape** & I got the desired output.\r\n\r\n*But whenever I use datasets.output_shape , I get a warning as below:*\r\nWARNING:tensorflow:From <ipython-input-3-3cfb74cd1ccb>:3: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.data.get_output_shapes(dataset)`.\r\n\r\n\r\n**Request to the Tensorflow Team:**\r\nI kindly request the Tensorflow team to look into this issue and help me solve it.\r\n\r\n**Thank you Tensorflow Team in Advance...!!!**\r\n\r\n[Colab file Link](https://colab.research.google.com/drive/1m9yF3cXQGTc1xNLCgBVx0BdcHVnOctbz?usp=sharing)\r\n\r\n", "comments": ["I tried in colab with TF version 2.3, nightly(`2.4.0-dev20200831`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/460758e724e1b69840f66d3f1b037fbe/untitled290.ipynb). Thanks!", "Hi @iamsrilakshmi, I think your issue here is that you first define\r\n\r\n```\r\ntr = tr.shuffle(10000)\r\ntr = tr.padded_batch(64, tr.output_shapes)\r\n```\r\n\r\nAnd then when defining `tr2`, you are again padding `tr`, which is already padded, and that results in an extra dimension, hence the error message `expected ndim=3, found ndim=4. Full shape received: [None, None, None, 128]`\r\n\r\nIf you just remove the following lines from your colab notebook, it runs without error:\r\n```\r\ntr = tr.shuffle(10000)\r\ntr = tr.padded_batch(64, tr.output_shapes)\r\nval = val.padded_batch(64, val.output_shapes)\r\n```", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hi @nikitamaia ,\r\n\r\nCan you please check in this Colab code file... [Link to colab file](https://colab.research.google.com/drive/12V0MXjBVxZ7gEDYlKwW_QXZ2wb3UUbDa?usp=sharing)\r\n\r\nI am facing the same error with tf.compat.v1.data.get_output_shapes. This error is displayed only when training the model\r\n\r\nThe error I am talking about is as follows:\r\n\r\n**ValueError: logits and labels must have the same shape ((None, 1) vs ())**\r\n\r\nPlease look into the issues concerning this particular error\r\n\r\n", "The error you see is because you are passing in the wrong datasets to your `model.fit`\r\n\r\nYou are calling  `model.fit(train_data, epochs=num_epochs, validation_data=test_data)`\r\nbut `train_data` and `test_data` are the raw datasets from tfds, and not the ones you have applied  `tf.compat.v1.data.get_output_shapes` to.\r\n\r\nYou should be passing in `train_dataset` and `test_dataset` like so:\r\n`model.fit(train_dataset, epochs=num_epochs, validation_data=test_dataset)`\r\n\r\nFor future reference, Github is a place for filing bugs. These kinds of support requests are better for Stack Overflow where there is a wider community to troubleshoot.", "Closing issue since a solution was found and there is no bug here."]}, {"number": 42823, "title": "TFLu: Fix Ethos-U build issue", "body": "The issue is that a binary need to be built twice, since it\r\ndepends on recursive_find. The driver is not fully downloaded when\r\nrecursive_find is called. \r\nThe solution proposal is to call the download script\r\nimmediately via the make's shell function.\r\n\r\nAlso add missing exit to error case in download script.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\r\n\r\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\r\nRead the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\r\nCreated a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\r\nLinked to the issue from the PR description\r\n\r\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.", "I'll let @petewarden review this since he is most familiar with the Makefile logic and its repercussions.", "Looks good from my side.", "@petewarden Ping for review"]}, {"number": 42822, "title": "Gcs filesystem test", "body": "@mihaimaruseac \r\nThis PR adds tests for gcs plugins", "comments": []}, {"number": 42821, "title": "TFLu: update cmsis kernels", "body": "- Cast to double in mul.cc.\r\n- Add missing initializers in Register_MUL().\r\n- Fix compile issue in conv.cc reference (fall back) case.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\r\n\r\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\r\nRead the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\r\nCreated a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\r\nLinked to the issue from the PR description\r\n\r\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review."]}, {"number": 42820, "title": "Tensorflow only using a fraction of the GPU power available (NLP model, dual GPU, tf.data pipeline) ", "body": "Hi!\r\n\r\nSomewhat recently I got a new training server which is really fast, but I'm currently having trouble utilizing it's GPU and CPU to it's full potential when training my model.\r\n\r\nI'm training an NLP classification model with a string as input and a category as target. When I set the batch size to a reasonally small number, like 16 or 32, only around 10% of each of the 2 GPUs as well as of the CPU are used. Only when I size the batches up to 4096, CPU gets close to a 100% load but GPUs still only hit 7-8%. Training is really fast then but extremely inefficient because such batch sizes are b\\*\\*\\*s\\*\\*\\*, so the model converges only very slowly.\r\n\r\nI found a sweet spot around bs=256, where only around 20% CPU and 10% GPUs load is achieved and gradient descent is still somewhat efficient, which means I get the best results in terms of wall time.\r\n\r\nThe data pipeline is implemented with [tf.data](https://tf.data), reading the data from several CSVs in parallel from an SSD. I couldn't find any bottlenecks so far.\r\n\r\nThis is somewhat frustrating because I can only make use of a fraction of the full potential of my new machine. Any ideas on how to improve this?\r\n\r\nI'm grateful for any help.\r\n\r\n&#x200B;\r\n\r\n&#x200B;\r\n\r\nMy specs:\r\n\r\n\\- AMD Ryzen Threadripper 3960X 24-Core Processor\r\n\r\n\\- 64 GB RAM\r\n\r\n\\- two NVIDIA GeForce RTX 2070 SUPER with 8192MiB each\r\n\r\n\\- Win 10 (unfortunately, the ASrock Creator TRX40 motherboard we bought is currently incompatible with Linux, wtf...)\r\n\r\n\\- TF 2.1.0 installed from binary (anaconda)\r\n\r\n\\- Python 3.7.7\r\n\r\n\\- CUDA Version 10.2.89\r\n\r\n&#x200B;\r\n\r\nThe relevant part of my code:\r\n\r\n    class dataset_loader():\r\n        \r\n        def __init__(self, data_dir, csv_file, batch_size, cycle_length, tokenizer=None, n_threads=1, n_prefetch=1):\r\n            \r\n            self.batch_size = batch_size\r\n            self.cycle_length = cycle_length\r\n            self.n_threads = n_threads\r\n            self.n_prefetch = n_prefetch\r\n            \r\n            self.output_size = 3943     ## !!!!!!!!!!! TODO nur f\u00fcr testzwecke bitte nicht hardcoden!!!!!!!!!!!\r\n    \r\n            if(tokenizer is None):\r\n                self.tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\r\n                self.tokenizer.fit_on_texts(strat_search_words_with_beginnings)\r\n            else:\r\n                self.tokenizer = tokenizer\r\n                \r\n            char_dict = list(eval(self.tokenizer.get_config().get(\"word_index\")).keys())[:-1]  # hier vorletztes weglassen, da ein Out-Of-Vocabulary slot bei StaticVocabularyTable zwingend angegeben werden muss\r\n            char_index = list(eval(self.tokenizer.get_config().get(\"word_index\")).values())[:-1]\r\n            char_table_init = tf.lookup.KeyValueTensorInitializer(char_dict, char_index, value_dtype=tf.int64)\r\n            self.char_table = tf.lookup.StaticVocabularyTable(char_table_init, 1)\r\n            \r\n            self.max_id = len(self.tokenizer.word_index)\r\n            \r\n            assert self.max_id == len(char_index)+1\r\n    \r\n            self._filepath_ = os.path.join(data_dir, csv_file)\r\n    \r\n        \r\n        @tf.function\r\n        def preprocessing(self, line):\r\n            split_line = tf.strings.split(line, sep=\";\")\r\n            \r\n            search_string = split_line[0]\r\n            search_string = tf.strings.substr(search_string, 0, 50)      # zu lange strings abschneiden\r\n            while tf.strings.length(search_string) < 50:                 # zu kurze strings padden\r\n                search_string = search_string + tf.constant(\" \")\r\n    \r\n            chars = tf.strings.bytes_split(search_string)\r\n            indices = self.char_table.lookup(chars)\r\n            one_hot_string = tf.one_hot(indices, depth=self.max_id+1)\r\n            \r\n            icd_idx = tf.strings.to_number(split_line[1], out_type=tf.dtypes.int32)\r\n            one_hot_icd = tf.one_hot(icd_idx, self.output_size)\r\n            \r\n            return one_hot_string, one_hot_icd\r\n        \r\n        def interleaving(self, filepath):\r\n            return tf.data.TextLineDataset(filepath).skip(1)\r\n        \r\n        def get_dataset(self):\r\n            dataset = tf.data.Dataset.list_files(self._filepath_)\r\n            dataset = dataset.interleave(self.interleaving, cycle_length=self.cycle_length, num_parallel_calls=self.n_threads)\r\n            dataset = dataset.map(self.preprocessing, num_parallel_calls=self.n_threads)\r\n            return dataset.batch(self.batch_size).prefetch(self.n_prefetch)\r\n        \r\n        def get_tokenizer(self):\r\n            return self.tokenizer\r\n           \r\n    \r\n    \r\n    import pickle\r\n    f = open(os.path.join(data_dir, \"tokenizer.pkl\"), \"rb\")\r\n    tokenizer = pickle.load(f)\r\n    \r\n    train_data_loader = dataset_loader(data_dir,\r\n                                 \"train_*.csv\",\r\n                                 batch_size=batch_size,\r\n                                 tokenizer=tokenizer,\r\n                                 cycle_length=106,             #Anzahl der CSV-Dateien\r\n                                 n_threads=tf.data.experimental.AUTOTUNE,\r\n                                 n_prefetch=40)\r\n    \r\n    valid_data_loader = dataset_loader(data_dir,\r\n                                \"valid_*.csv\",\r\n                                batch_size=batch_size,\r\n                                tokenizer=tokenizer,\r\n                                cycle_length=13,\r\n                                n_threads=tf.data.experimental.AUTOTUNE,\r\n                                n_prefetch=40)\r\n    \r\n    \r\n    mirrored_strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\r\n    \r\n    with mirrored_strategy.scope():\r\n        model = keras.models.Sequential([\r\n        #    keras.layers.GRU(128, return_sequences=True, batch_input_shape=[batch_size, None, max_id+1]),\r\n            keras.layers.GRU(128, return_sequences=True, input_shape=[ None, train_data_loader.max_id+1], use_bias=False),\r\n            keras.layers.GRU(128, return_sequences=True, use_bias=False),\r\n            keras.layers.GRU(128, use_bias=False),\r\n            keras.layers.Flatten(),\r\n            keras.layers.Dense(train_data_loader.output_size, activation=\"softmax\")\r\n        ])\r\n        model.compile(loss=[focal_loss_umbertogriffo.categorical_focal_loss(alpha=.25, gamma=2)], optimizer=\"adam\", metrics=['accuracy'])\r\n    \r\n    callbacks = list()\r\n    callbacks.append(keras.callbacks.EarlyStopping(patience=2))\r\n    callbacks.append(keras.callbacks.ModelCheckpoint(filepath = os.path.join(data_dir, \"checkpoints\"), save_best_only=True))\r\n    \r\n    history = model.fit(train_data_loader.get_dataset(), validation_data=valid_data_loader.get_dataset(), epochs=25, callbacks = callbacks)", "comments": ["@philipp-ghtest We donot support Anaconda builds as we don't have complete knowledge on how they were built. May be you need to post it in Anaconda repository.\r\n\r\nGenerally, pip builds for 2.1 are built with CUDA 10.1 (you mentioned you have CUDA 10.2).Please check [here](https://www.tensorflow.org/install/source_windows#gpu) for more details.\r\n\r\n\r\nVersion | Python version | Compiler | Build tools | cuDNN | CUDA\r\n-- | -- | -- | -- | -- | --\r\ntensorflow_gpu-2.3.0 | 3.5-3.8 | MSVC 2019 | Bazel 3.1.0 | 7.4 | 10.1\r\ntensorflow_gpu-2.2.0 | 3.5-3.8 | MSVC 2019 | Bazel 2.0.0 | 7.4 | 10.1\r\ntensorflow_gpu-2.1.0 | 3.5-3.7 | MSVC 2019 | Bazel 0.27.1-0.29.1 | 7.4 | 10.1\r\n\r\nTwo options are \r\n(1) post it in Anaconda's repo where experts related to Anaconda will resolve your issue, or\r\n(2) unistall TF, CUDA drivers, clean up any remaining files related to CUDA/TF, restart, then reinstall pip version freshly.\r\n\r\nHope it helps. Please let us know how it progresses. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42819, "title": "TF r2.3 Bad Address build issue on windows", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n-Windows 10 Pro\r\n-GTX 1060 3gb\r\n-compute capabilities 6.1\r\n-Python 3.6\r\n-Tensorflow 2.3\r\n-bazel 3.1.0\r\n-CUDA 10.1\r\n-cuDNN SDK 7.6\r\n-TensorRT 6.0\r\n-using AVX2\r\n\r\n\r\n\r\n**Describe the problem**\r\nI know from testing that it is provided by using CUDA. Without it build without Problems. I already tried it with other Python versions like 3.8.\r\n\r\n    This is the Error Message\r\nERROR: C:/tenserflow/tensorflow/tensorflow/core/framework/BUILD:1107:1: Executing genrule //tensorflow/core/framework:attr_value_proto_text_srcs failed (Exit 126): bash.exe failed: error executing command\r\n  cd C:/users/benno/_bazel_benno/fdojh2ah/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\WINDOWS;C:\\WINDOWS\\System32;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\r\n    SET PYTHON_BIN_PATH=C:/Python36/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Python36/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_CUDA_PATHS=C:/tools/cuda,C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\r\n    SET TF_CUDA_VERSION=10.1\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_ENABLE_XLA=1\r\n    SET TF_NEED_CUDA=1\r\n  C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt-exec-50AE0418/bin/tensorflow/tools/proto_text/gen_proto_text_functions bazel-out/x64_windows-opt/bin/tensorflow/core/framework tensorflow/core/framework/ tensorflow/core/framework/attr_value.proto tensorflow/core/framework/resource_handle.proto tensorflow/core/framework/tensor.proto tensorflow/core/framework/tensor_shape.proto tensorflow/core/framework/types.proto tensorflow/tools/proto_text/placeholder.txt\r\nExecution platform: @local_execution_config_platform//:platform\r\n/usr/bin/bash: bazel-out/x64_windows-opt-exec-50AE0418/bin/tensorflow/tools/proto_text/gen_proto_text_functions: Bad address\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: C:/tenserflow/tensorflow/tensorflow/core/framework/BUILD:1107:1 Executing genrule //tensorflow/core/framework:attr_value_proto_text_srcs failed (Exit 126): bash.exe failed: error executing command\r\n  cd C:/users/benno/_bazel_benno/fdojh2ah/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\WINDOWS;C:\\WINDOWS\\System32;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\r\n    SET PYTHON_BIN_PATH=C:/Python36/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Python36/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_CUDA_PATHS=C:/tools/cuda,C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\r\n    SET TF_CUDA_VERSION=10.1\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_ENABLE_XLA=1\r\n    SET TF_NEED_CUDA=1\r\n  C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt-exec-50AE0418/bin/tensorflow/tools/proto_text/gen_proto_text_functions bazel-out/x64_windows-opt/bin/tensorflow/core/framework tensorflow/core/framework/ tensorflow/core/framework/attr_value.proto tensorflow/core/framework/resource_handle.proto tensorflow/core/framework/tensor.proto tensorflow/core/framework/tensor_shape.proto tensorflow/core/framework/types.proto tensorflow/tools/proto_text/placeholder.txt\r\nExecution platform: @local_execution_config_platform//:platform\r\n\r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI followed this Website https://www.tensorflow.org/install/source_windows#gpu\r\n\r\n     This is my .tf_configure.bazelrc\r\nbuild --action_env PYTHON_BIN_PATH=\"C:/Python36/python.exe\"\r\nbuild --action_env PYTHON_LIB_PATH=\"C:/Python36/lib/site-packages\"\r\nbuild --python_path=\"C:/Python36/python.exe\"\r\nbuild --config=xla\r\nbuild --action_env TF_CUDA_VERSION=\"10.1\"\r\nbuild --action_env TF_CUDNN_VERSION=\"7\"\r\nbuild --action_env TF_CUDA_PATHS=\"C:/tools/cuda,C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"6.1\"\r\nbuild --config=cuda\r\nbuild:opt --copt=/arch:AVX2\r\nbuild:opt --define with_default_optimizations=true\r\nbuild --define=override_eigen_strong_inline=true\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest:v1 --test_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu,-oss_serial\r\ntest:v1 --build_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu\r\ntest:v2 --test_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu,-oss_serial,-v1only\r\ntest:v2 --build_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu,-v1only\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@Ch33s3Burger \r\nCould you please check if you are able to build TensorFlow with CUDA 10.1 and cuDNN 7.4, and let us know if you are facing the same issue.\r\n\r\nPlease take a look at the [tested build configuration](https://www.tensorflow.org/install/source_windows#gpu) for reference. Thanks!", "Same Error\r\n\r\nERROR: C:/tenserflow/tensorflow/tensorflow/core/framework/BUILD:1107:1: Executing genrule //tensorflow/core/framework:attr_value_proto_text_srcs failed (Exit 126): bash.exe failed: error executing command\r\n  cd C:/users/benno/_bazel_benno/fdojh2ah/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\WINDOWS;C:\\WINDOWS\\System32;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\r\n    SET PYTHON_BIN_PATH=C:/Python36/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Python36/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_CUDA_PATHS=C:/tools/cuda,C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\r\n    SET TF_CUDA_VERSION=10.1\r\n    SET TF_CUDNN_VERSION=7.4\r\n    SET TF_ENABLE_XLA=1\r\n    SET TF_NEED_CUDA=1\r\n  C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt-exec-50AE0418/bin/tensorflow/tools/proto_text/gen_proto_text_functions bazel-out/x64_windows-opt/bin/tensorflow/core/framework tensorflow/core/framework/ tensorflow/core/framework/attr_value.proto tensorflow/core/framework/resource_handle.proto tensorflow/core/framework/tensor.proto tensorflow/core/framework/tensor_shape.proto tensorflow/core/framework/types.proto tensorflow/tools/proto_text/placeholder.txt\r\nExecution platform: @local_execution_config_platform//:platform\r\n/usr/bin/bash: bazel-out/x64_windows-opt-exec-50AE0418/bin/tensorflow/tools/proto_text/gen_proto_text_functions: Bad address\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: C:/tenserflow/tensorflow/tensorflow/core/framework/BUILD:1107:1 Executing genrule //tensorflow/core/framework:attr_value_proto_text_srcs failed (Exit 126): bash.exe failed: error executing command\r\n  cd C:/users/benno/_bazel_benno/fdojh2ah/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\WINDOWS;C:\\WINDOWS\\System32;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\r\n    SET PYTHON_BIN_PATH=C:/Python36/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Python36/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_CUDA_PATHS=C:/tools/cuda,C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\r\n    SET TF_CUDA_VERSION=10.1\r\n    SET TF_CUDNN_VERSION=7.4\r\n    SET TF_ENABLE_XLA=1\r\n    SET TF_NEED_CUDA=1\r\n  C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt-exec-50AE0418/bin/tensorflow/tools/proto_text/gen_proto_text_functions bazel-out/x64_windows-opt/bin/tensorflow/core/framework tensorflow/core/framework/ tensorflow/core/framework/attr_value.proto tensorflow/core/framework/resource_handle.proto tensorflow/core/framework/tensor.proto tensorflow/core/framework/tensor_shape.proto tensorflow/core/framework/types.proto tensorflow/tools/proto_text/placeholder.txt\r\nExecution platform: @local_execution_config_platform//:platform\r\n", "i also tried to run it in the git Bash same Error and with cudnn 10.0 but always the same Error", "Closing as duplicate of #41850", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42819\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42819\">No</a>\n"]}]