[{"number": 48937, "title": "Can't print RaggedTensor with dtype uint8", "body": "Following code block\r\n`>>> tf.print(tf.ragged.constant([[1,2,3],[1,2]], dtype=tf.uint8)`\r\nraises \r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Value for attr 'T' of uint8 is not in the list of allowed values: int8, int16, int32, int64, complex64, complex128, float, double, bool, variant\r\n\t; NodeDef: {{node AsString}}; Op<name=AsString; signature=input:T -> output:string; attr=T:type,allowed=[DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128, DT_FLOAT, DT_DOUBLE, DT_BOOL, DT_VARIANT]; attr=precision:int,default=-1; attr=scientific:bool,default=false; attr=shortest:bool,default=false; attr=width:int,default=-1; attr=fill:string,default=\"\"> [Op:AsString]\r\n```\r\nI'm filing this as a bug because I really don't think this error is thrown intentionally. If it is, apologies. \r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): tested in `2.4.1` and `2.6.0-dev20210506` (latest dev I could find)\r\n- Python version: 3.8\r\n\r\n**Describe the current behavior**\r\nIt raises an errror\r\n**Describe the expected behavior**\r\nThe Ragged tensor can be printed\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes): sure, I have absolutely no idea where to start though. \r\n\r\n**Standalone code to reproduce the issue**\r\n`tf.print(tf.ragged.constant([[1,2,3],[1,2]], dtype=tf.uint8) ` in an interactive python shell\r\n", "comments": ["I was able to reproduce your issue, please find the [gist](https://colab.research.google.com/gist/sachinprasadhs/c7853b066d1be5ac774da6022a759c90/untitled3.ipynb#scrollTo=FgJEmmVPm-3B) here.\r\nYou can try with normal `print` statement since` tf.print` is having issues.", "Normal `print` is not really an option, as I need to print my `RaggedTensor` every time the graph runs, not only at graph construction time.", "Thanks for pointing this out.  The underlying issue is that [`tf.strings.as_string`](https://www.tensorflow.org/api_docs/python/tf/strings/as_string) only supports a limited set of dtypes (int8, int16, int32, int64, complex64, complex128, float32, float64, bool); and tf.print uses tf.strings.as_string to print ragged tensors.\r\n\r\nA quick solution on the user side is to just cast the tensor to one of those types before printing.  In this case, the smallest type that would give the same string outputs would be int16, so you could do:\r\n\r\n```\r\nx = tf.ragged.constant([[1,2,3],[1,2]], dtype=tf.uint8)\r\ntf.print(tf.cast(x, tf.int16))\r\n```\r\n\r\nOn the TensorFlow side, we could do any of the following:\r\n\r\n1. Update `AsString` (`third_party/tensorflow/core/kernels/as_string_op.cc`) to properly handle more dtypes.\r\n2. Add a wrapper in `string_ops.py` that automatically casts `input` to an appropriate type when necessary.\r\n3. Update `ragged_tensor_to_string` to either cast values when necessary, or fall back on a less readable result (where flat_values and row_splits are listed separately) when an unsupported dtype is used.\r\n\r\nOption 1 seems best to me -- I don't see any particular downside to having AsString support uint8.  And presumably supporting all the other types (qint8, etc.) wouldn't be too difficult.  For tf.variant and tf.resource, we would probably still raise an exception.", "(Minor correction: it looks like AsString *does* support variant, and uses its DebugString method.  And ResourceBase also has a DebugString method, so maybe we could use that as well, and have tf.strings.as_string support all dtypes.)", "As Ed suggested, please feel free to send us a PR adding support for more dtypes for the AsString op in TF.", "This is fixed with latest tf-nightly version 2.6.0-dev20210618\r\nSee [gist](https://colab.research.google.com/gist/ymodak/d00a411d185b0fcac8ba7a814336c0f0/untitled3.ipynb) for your reference. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48937\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48937\">No</a>\n"]}, {"number": 48936, "title": "Use MultiHeadAttention in tf:2.3.1", "body": "I want to use `MultiHeadAttention` layer in tf:2.3.1 due to CUDA version limit.\r\n\r\nhere is the test code:\r\n```\r\nimport multi_head_attention  \r\n\r\ntest_layer = multi_head_attention.MultiHeadAttention(\r\n    num_heads=12, key_dim=64)\r\n# Create a 3-dimensional input (the first dimension is implicit).\r\nquery = keras.Input(shape=(40, 80))\r\noutput, coef = test_layer(query, query, return_attention_scores=True)\r\n\r\n```\r\n\r\nI copy this file : https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/layers/multi_head_attention.py \r\nto my current working path for import.\r\n\r\nHere is the error:\r\n\r\n> ---------------------------------------------------------------------------\r\n> TypeError                                 Traceback (most recent call last)\r\n> <ipython-input-6-8ef8346e0a54> in <module>\r\n>       1 query = keras.Input(shape=(40, 80))\r\n> ----> 2 output, coef = test_layer(query, query, return_attention_scores=True)\r\n> \r\n> ~/yanan/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n>     924     if _in_functional_construction_mode(self, inputs, args, kwargs, input_list):\r\n>     925       return self._functional_construction_call(inputs, args, kwargs,\r\n> --> 926                                                 input_list)\r\n>     927\r\n>     928     # Maintains info about the `Layer.call` stack.\r\n> \r\n> ~/yanan/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py in _functional_construction_call(self, inputs, args, kwargs, input_list)\r\n>    1115           try:\r\n>    1116             with ops.enable_auto_cast_variables(self._compute_dtype_object):\r\n> -> 1117               outputs = call_fn(cast_inputs, *args, **kwargs)\r\n>    1118\r\n>    1119           except errors.OperatorNotAllowedInGraphError as e:\r\n> \r\n> ~/yanan/env/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n>     256       except Exception as e:  # pylint:disable=broad-except\r\n>     257         if hasattr(e, 'ag_error_metadata'):\r\n> --> 258           raise e.ag_error_metadata.to_exception(e)\r\n>     259         else:\r\n>     260           raise\r\n> \r\n> TypeError: in user code:\r\n> \r\n>     /root/yanan/berts/topic_classification_augmentation/multi_head_attention.py:510 call  *\r\n>         attention_output, attention_scores = self._compute_attention(\r\n>     /root/yanan/berts/topic_classification_augmentation/multi_head_attention.py:475 _compute_attention  *\r\n>         attention_scores = self._masked_softmax(attention_scores, attention_mask)\r\n>     /root/yanan/berts/topic_classification_augmentation/multi_head_attention.py:438 _masked_softmax  *\r\n>         return self._softmax(attention_scores, attention_mask)\r\n>     /root/yanan/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py:926 __call__  **\r\n>         input_list)\r\n>     /root/yanan/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py:1117 _functional_construction_call\r\n>         outputs = call_fn(cast_inputs, *args, **kwargs)\r\n> \r\n>     TypeError: call() takes 2 positional arguments but 3 were given\r\n\r\n\r\nAny solution ?", "comments": ["In tf:2.4.1 everything is OK:\r\n\r\n`att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)`", "@yananchen1989 The file you are using referring to 2.4 version but the multi_head_attention.py  doesn't exist in TF 2.3.1.Thanks!", "> @yananchen1989 The file you are using referring to 2.4 version but the multi_head_attention.py doesn't exist in TF 2.3.1.Thanks!\r\n\r\nI copy this file to my working path manually, aiming to import it from current path by \r\n`import multi_head_attention ` , therefore  the initialization of `test_layer` works, but the call of it failed.", "@yananchen1989  Seems the multi head attention layer implemented from TF 2.4 version and that's why you are encountering the issues in 2.3. Thanks!", "@yananchen1989 Closing the issue since it was resolved. Please feel free to re-open  the issue if you have any further queries.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48936\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48936\">No</a>\n"]}, {"number": 48935, "title": "Compatibility with NumPy 1.20+", "body": "* Fix the one already visible failure with NumPy 1.20 (https://github.com/tensorflow/tensorflow/issues/47691#issuecomment-814255904)\r\n* Different approach than https://github.com/tensorflow/tensorflow/pull/48918: **Relax** upper bound instead of pinning to 1.20. Python 3.6 builds still need <1.20. (Per NEP 29, 1.20 dropped Python 3.6)", "comments": ["Just to notify, another user has submitted https://github.com/tensorflow/tensorflow/pull/49008", "@bnavigator  Can you please resolve conflicts? Thanks!", "@bnavigator Can you please resolve conflicts? Thanks!", "@gbaned: I resolved conflicts. But you developers clearly stick to the pinning strategy\r\n\r\nSee also\r\nhttps://github.com/tensorflow/tensorflow/commit/fe2fad5184f2ded836ccfeaa39b557472b69ce50#r52001636", "The mentioned commit is just a step in getting this handled.\r\n\r\nWe will keep pinning dependencies in our CI, but `setup.py` would have broader ranges", "We had already a rollback one month ago with https://github.com/tensorflow/tensorflow/pull/49008#issuecomment-841311202.\r\n\r\nWe have internal tests other then the public one in the CI.", "@bnavigator Can you please resolve conflicts? Thanks!", "Merged and updated.\r\n\r\nSo how do I read the logs of the \"public\" CI, specifically a build with Python >= 3.7, to see whether the correct numpy is used for the build and test?", "Unmarking myself as a reviewer here. I'm a Googler but I don't work on TensorFlow.", "@gbaned, please force run on internal again. I think the array_ops change also revealed a minor bug in an unrelated function test. (Possibly also through an updated numpy 1.19 in the internal py3.6 CI (?))", "From the MacOS CPU Python3 fail (Py3.7):\r\n\r\n```\r\n  File \"/Volumes/BuildData/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/kernel_tests/map_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2672, in _GetNdArray\r\n    return np.array(a)\r\n  File \"/Volumes/BuildData/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/kernel_tests/map_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py\", line 870, in __array__\r\n    \" a NumPy call, which is not supported\".format(self.name))\r\nNotImplementedError: Cannot convert a symbolic Tensor (gradient_tape/add/Reshape:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\r\n```\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/9c70f6b7aeabeb4eb0dc19a0275bdd17e67412f1/tensorflow/python/framework/test_util.py#L2664-L2673\r\n\r\nSomething goes wrong in the conversion here. The errors on Windows amount to the same problem.", "I have build TensorFlow 2.5.0 from sources against `numpy 1.20.3`. Then, I ran the TF test suite and ran into the issue described [here](https://github.com/tensorflow/tensorflow/issues/47691#issuecomment-814255904) (8 tests from the test suite were failing). I've applied the patches suggested in this PR, rebuilt TF 2.5.0 against the same numpy, and reran the TF test suite.\r\n\r\nThe tests that were failing on the `np.prod(...)` line are indeed fixed by the changes this PR makes to tensorflow/python/ops/array_ops.py. So far so good.\r\n\r\nNow, only 2 tests are still failing:\r\n\r\n```\r\nERROR: testLookupAddGrad (__main__.MapOpsTest)\r\nMapOpsTest.testLookupAddGrad\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/scratch/casparl/TensorFlow/2.5.0/foss-2021a-CUDA-11.3.1/tmplnPLAE-bazel-tf/828a44b36155f75b299651f28995fd3c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/map_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1213, in decorated\r\n    f(self, *args, **kwargs)\r\n  File \"/scratch/casparl/TensorFlow/2.5.0/foss-2021a-CUDA-11.3.1/tmplnPLAE-bazel-tf/828a44b36155f75b299651f28995fd3c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/map_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/map_ops_test.py\", line 300, in testLookupAddGrad\r\n    self.assertAllClose(g, [1, 1])\r\n  File \"/scratch/casparl/TensorFlow/2.5.0/foss-2021a-CUDA-11.3.1/tmplnPLAE-bazel-tf/828a44b36155f75b299651f28995fd3c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/map_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1253, in decorated\r\n    return f(*args, **kwds)\r\n  File \"/scratch/casparl/TensorFlow/2.5.0/foss-2021a-CUDA-11.3.1/tmplnPLAE-bazel-tf/828a44b36155f75b299651f28995fd3c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/map_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2743, in assertAllClose\r\n    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)\r\n  File \"/scratch/casparl/TensorFlow/2.5.0/foss-2021a-CUDA-11.3.1/tmplnPLAE-bazel-tf/828a44b36155f75b299651f28995fd3c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/map_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2675, in _assertAllCloseRecursive\r\n    a_as_ndarray = self._GetNdArray(a)\r\n  File \"/scratch/casparl/TensorFlow/2.5.0/foss-2021a-CUDA-11.3.1/tmplnPLAE-bazel-tf/828a44b36155f75b299651f28995fd3c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/map_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2586, in _GetNdArray\r\n    return np.array(a)\r\n  File \"/scratch/casparl/TensorFlow/2.5.0/foss-2021a-CUDA-11.3.1/tmplnPLAE-bazel-tf/828a44b36155f75b299651f28995fd3c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/map_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py\", line 867, in __array__\r\n    raise NotImplementedError(\r\nNotImplementedError: Cannot convert a symbolic Tensor (gradient_tape/add/Reshape:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\r\n\r\n======================================================================\r\nERROR: testLookupMultiplyGrad (__main__.MapOpsTest)\r\nMapOpsTest.testLookupMultiplyGrad\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n...\r\n    a_as_ndarray = self._GetNdArray(a)\r\n  File \"/scratch/casparl/TensorFlow/2.5.0/foss-2021a-CUDA-11.3.1/tmplnPLAE-bazel-tf/828a44b36155f75b299651f28995fd3c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/map_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2586, in _GetNdArray\r\n    return np.array(a)\r\n  File \"/scratch/casparl/TensorFlow/2.5.0/foss-2021a-CUDA-11.3.1/tmplnPLAE-bazel-tf/828a44b36155f75b299651f28995fd3c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/map_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py\", line 867, in __array__\r\n    raise NotImplementedError(\r\nNotImplementedError: Cannot convert a symbolic Tensor (gradient_tape/cond_1/Identity:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\r\n```\r\nEssentially, this is what @bnavigator also mentioned in [this comment](https://github.com/tensorflow/tensorflow/pull/48935#issuecomment-863264915)\r\n\r\nI'm wondering, @bnavigator , did you figure out what the solution is there? I'm not really deep enough into the TensorFlow & numpy codes to understand why this conversion would fail, so I'm not sure how to fix it...", "@bnavigator \r\n> Something goes wrong in the conversion here. The errors on Windows amount to the same problem.\r\n\r\nThe issue is due to this change in numpy 1.20: https://numpy.org/doc/stable/release/1.20.0-notes.html#arraylike-objects-which-do-not-define-len-and-getitem\r\n\r\nWhat we have here is a list of TF tensors: `[a, b]` where the Tensor has the `__array__` method which is (wrongly) called now and it also has a `__len__` but no `__getitem__` method. --> Fix would likely be to add the latter\r\n\r\nEdit: I read it the wrong way round: We would need to remove `__array__` or otherwise numpy will think it can call it and WILL call it.\r\nThe actual error is caused by TF 2.6 seemingly changing the type returned by `tensorflow.python.framework.constant_op.constant` from `tensorflow.python.framework.ops.EagerTensor` to `tensorflow.python.framework.ops.Tensor`, see output of `constant_op.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]]).__class__`\r\n\r\nI'm not sure that change was intended... Looks like for the tests the TF2 behavior (eager execution) is not enabled by default anymore...", "Both errors occur in a piece of code that tries to convert something into an `np.ndarray`. So you need to make sure that this piece actually results in `a` being an actual `ndarray`:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/573c905ff7b96d86d8deb5e3955ef04a8e738370/tensorflow/python/framework/test_util.py#L2679-L2683\r\n\r\n\r\nThat should be the case for both  `tensorflow.python.framework.ops.EagerTensor` and `tensorflow.python.framework.ops.Tensor`\r\n\r\n", "@bnavigator The problem is that `a` is a list which contains a `Tensor` and numpy 1.20+ recursively converts sequences into numpy arrays. I.e. you have `np.array([Tensor(), [1,2,3]])` due to https://github.com/tensorflow/tensorflow/blob/573c905ff7b96d86d8deb5e3955ef04a8e738370/tensorflow/python/framework/test_util.py#L2684-L2685\r\n\r\nI can confirm that removing the \"unimplemented\" `__array__` function fully solves all issues without changing anything else (even without this whole PR)", "> numpy 1.20+ recursively converts sequences into numpy arrays\r\n\r\nBefore that happens, you call `np.numpy(something)` and that `something` contains a Tensor. Which is not allowed.\r\n \r\n> I can confirm that removing the \"unimplemented\" `__array__` function fully solves all issues without changing anything else (even without this whole PR)\r\n\r\nAs I understand, removing it discards the safeguard **You shalt not pass a Tensor to a NumPy function**. Not as a single value nor as part of a sequence.\r\n\r\nInstead, you have to make sure that either `_GetNdArray(a)` is only called with Tensor-likes that are convertible or add extra logic to break down sequences and return them in a multidimensional `ndarray`.", "> Before that happens, you call np.numpy(something) and that something contains a Tensor. Which is not allowed.\r\n\r\nIt was in pre numpy 1.20 where it returns `np.array(Tensor, [1,2,3])`, i.e. non-recursive conversion\r\n\r\n> As I understand, removing it discards the safeguard You shalt not pass a Tensor to a NumPy function. Not as a single value nor as part of a sequence.\r\n\r\nMore or less true. At the moment (i.e. in current TF releases) passing it as a sequence is allowed de facto. With numpy 1.20 it is no longer allowed and TF code would need to adapt to that.\r\n\r\n>  add extra logic to break down sequences and return them in a multidimensional ndarray.\r\n\r\nThat is likely true. Or at least use a `TypeError` from `__array__` which is handled by the except clause.\r\n\r\nHowever to me adding a `__array__` function which (always) throws an error is a violation of the protocol. But not sure here...", "Could you please test with af47a19cd50477834bc54f8351c382f3a74e17ee?", "> However to me adding a `__array__` function which (always) throws an error is a violation of the protocol. But not sure here...\r\n\r\nIt's not, raising an error from `__array__` is a valid thing to do to prevent numpy from doing anything unwanted, such as treating a Tensor as a sequence, or converting it to a size-1 object array.", "> It's not, raising an error from __array__ is a valid thing to do to prevent numpy from doing anything unwanted, such as treating a Tensor as a sequence, or converting it to a size-1 object array.\r\n\r\nOk, yes you are right. Then I'd rather add the NotImplementedError to https://github.com/tensorflow/tensorflow/blob/573c905ff7b96d86d8deb5e3955ef04a8e738370/tensorflow/python/framework/test_util.py#L2793 or change it to a Type/ValueError in `__array__`", "> Ok, yes you are right. Then I'd rather add the NotImplementedError to\r\n> \r\n> https://github.com/tensorflow/tensorflow/blob/573c905ff7b96d86d8deb5e3955ef04a8e738370/tensorflow/python/framework/test_util.py#L2793\r\n> \r\n> or change it to a Type/ValueError in `__array__`\r\n\r\n~~I don't understand. `__array__` throws a `NotImplementedError` so I have to catch that. I cannot change one OR the other. Only both. Which would be an API change.~~ Edit: See below.", "> I don't understand __array__ throws a NotImplementedError so I have to catch that. I cannot change one OR the other. Only both. Which would be an API change.\r\n\r\nNo. You have to change exactly one.\r\n\r\nIMO The error here is not \"not implemented\" but rather: \"not valid on this type/value\" and hence should be a TypeError/ValueError in `__array__`. With that change the linked except will catch that and do the correct thing.\r\nIf one does not want to change the \"API\" (throw another error), then NotImplementedError should be added to the except list.", "`TypeError` seems correct to me.", "I didn't introduce the `NotImplementedError` in `Tensor.__array__()` and I don't want to override the developer's choice here. Looks like they might want to implement addionial functionality in the future:\r\n\r\nhttps://docs.python.org/3/library/exceptions.html#TypeError\r\n\r\n> This exception may be raised by user code to indicate that an attempted operation on an object is not supported, and is not meant to be. If an object is meant to support a given operation but has not yet provided an implementation, NotImplementedError is the proper exception to raise.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/305a8ba41978e7954c1a13baebc94bdb8ad16514/tensorflow/python/framework/ops.py#L898-L912", "Your quote supports the claim that TypeError should be the correct error: A symbolic Tensor (i.e. this type) cannot be converted, see e.g. `__len__`", "> Your quote supports the claim that TypeError should be the correct error: A symbolic Tensor (i.e. this type) cannot be converted, see e.g. `__len__`\r\n\r\nI agree that it would be correct, but `NotImplementedError` is not incorrect. The fact that `__len__` uses `TypeError` but `__array__` does not, is an indication that the choice was deliberate. I don't think it is appropriate to change this in a PR titled \"Compatibility with NumPy 1.20+\"", "> Ok, yes you are right. Then I'd rather add the NotImplementedError to\r\n> \r\n> https://github.com/tensorflow/tensorflow/blob/573c905ff7b96d86d8deb5e3955ef04a8e738370/tensorflow/python/framework/test_util.py#L2793\r\n> \r\n> or change it to a Type/ValueError in `__array__`\r\n\r\nOkay, after expanding the code snipped around the single referenced line, now I undestand.\r\n", "> I don't think it is appropriate to change this in a PR titled \"Compatibility with NumPy 1.20+\"\r\n\r\nAs mentioned: it is one way to fix this issue, unless that error really has to be a NotImplementedError, which I doubt. As you agreed the TypeError would be more suitable here, so fixing the (more or less) wrong error raised will also fix the numpy 1.20 issue.\r\nBut only the TF guys can tell that for sure.", "@robieta #30694 ebca088d52c135551268717b34d79c47d04e6794", "FWIW: A similar thing is happening for KerasTensor and there `TypeError` is used. I'd hence say changing the error is correct: https://github.com/tensorflow/tensorflow/blob/305a8ba41978e7954c1a13baebc94bdb8ad16514/tensorflow/python/keras/engine/keras_tensor.py#L253-L254", "I suggest, you make a PR and if it is merged before this one, I will happily revert https://github.com/tensorflow/tensorflow/pull/48935/commits/9a4642fb897e8ac7901253a4225d0d346715545e.", "Could someone please trigger a kokoro run? @bhack, @mihaimaruseac?", "Sorry, I was OOO. It seems now we have wide ranges of deps in setup.py and this causes conflicts.", "Merge conflict resolved.", "Can't this be closed now according to #52380?", "Yeah, https://github.com/tensorflow/tensorflow/pull/47957 \"fixed\" the `np.prod` error between TF 2.5 and 2.6 by catching the Exception correctly and not returning a constant. 0f8fde4 changes it to not run into the exception.\r\n\r\nThe changes in the requirements specifiers, the wrong comment about numpy install order and the wrong float shape tuples are still something to consider.", "Alright, fair enough.", "Please consider reproducers in https://github.com/tensorflow/tensorflow/issues/52657 closed this as this PR is supposed to address such issues. ", "We now merged most of this PR. Disabled tests will be fixed ahead in the near future."]}, {"number": 48934, "title": "Tflite Hexagon delegate performance drop when upgrading from TF 2.2 to 2.4", "body": "\r\n**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, Android.\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Snapdragon 855 dev platform\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.2 and 2.4\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): 3.7\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A \r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\n* A Keras model is converted to TfLite with full integer quantization and is executed on device using hexagon delegate.\r\nWhen converted using Tensorflow 2.2, 97 of 99 nodes are a executed on DSP and good performance is achieved - 12 ms per inference. When the same model is converted using Tensorflow 2.4, the output TfLite NN has 112 nodes of which only 81 are executed on DSP an runtime is 40 ms - nearly 4 times worse! \r\n\r\n* When converted using Tensorflow 2.2, a run time of 12 ms is observed in an offline test. When running online (in an android app), run time of inference degrades to 18 ms. \r\n\r\n**Describe the expected behavior**\r\n\r\n* Runtime (in an offline test) is expected to remain nearly the same when upgrading from Tensorflow 2.2 to Tensorflow 2.4\r\n* Runtime difference between online and offline execution is expected to be much less than 6ms (18 ms vs 12 ms) as most of the network is offloaded to DSP. \r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nSee [attached](https://drive.google.com/file/d/1yoPakbmxyhKXaXydn3RXLGMnsLdFl2t5/view?usp=sharing) a zip file with:\r\n1. The Keras model\r\n2. Conversion script\r\n3. representative dataset for conversion\r\n3. output TfLite model for TF version 2.2\r\n4. output Tflite model for TF version 2.4\r\n5. logs from TfLite benchmark utility for both TfLite model versions.\r\n\r\n_usage:_ python convertandquantize.py\r\nSwitch from TF 2.2 to TF 2.4 in your environment between runs\r\n\r\n\r\n", "comments": ["@karimnosseir could you take a look at this?", "Short answer:\r\nCan you please fix the batch size on the keras model before conversion.\r\n\r\nLonger answer:\r\nIt looks like your model has dynamic batch, older versions TFLite converter used to ignore the dynamic batch on the original model. This was fixed and now the model generated can properly supports dynamic dimensions.\r\nIf you didn't specify a static shape during conversion, you will be missing some optimizations which happens during conversion.\r\nThat means the model will be running extra ops and some might not be supported on DSP, which will make it even slower, since the model will switch between CPU and DSP.\r\nPlease fix the batch size to '1' on the original keras model before conversion and retry.\r\n\r\nSee \r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/46725#issuecomment-768534832\r\n\r\nand https://github.com/tensorflow/tensorflow/issues/43882#issuecomment-731636562\r\n", "Please let us know if you are having issues.\r\n\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48934\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48934\">No</a>\n"]}, {"number": 48933, "title": "Input depth should be equal to filter depth, conv2d, Tensorflow, Convolution operation, CNN", "body": "Tensorflow version 2.0\r\n\r\nPython version 3.7\r\n\r\n\r\n\r\n\r\nFollowing tensors extracted from TF Resnet50 model for experimentation on convolution.\r\nin1.shape = (1,56,56,64)\r\nweight_kernel.shape = (64,3,3,64)\r\n\r\nMethod1:-\r\nimage_conv = tensorflow.nn.conv2d(in1, weight_kernel, strides=(1,1), padding='SAME', dilations=None, data_format='NHWC')\r\n\r\nMethod2:-\r\nimage_conv = tensorflow.keras.backend.conv2d(in1, kernel=weight_kernel, strides=(1, 1), padding='same', dilation_rate=(1,1), data_format='channels_last')\r\n\r\n\r\n\r\nError:-\r\nInvalidArgumentError: input depth must be evenly divisible by filter depth: 64 vs 3 [Op:Conv2D]\r\n\r\nTried changing the padding; data_format, strides also.\r\n\r\n\r\nWhen the convolution has no issue happening in resnet model, why recreating outside is an issue with same tensor size; secondly  there is not depedency of filter depth as such in convolution, On what basis is 64 vs 3 coming up?\r\nIf i keep one tensor in channels first format and other one in channels last format, it starts giving error on 56 vs 3,  though I believe both should be in same format for the convolution operation.", "comments": ["Hi,\r\nI figured out that following is the format to pass conv data:-\r\n\r\nin1.shape = (1,56,56,64)\r\nweight_kernel.shape = (64,3,3,64)\r\n\r\nin1         ->     [1, height, width, channels]              -> (1,56,56,64)\r\nweight_kernel           ->      [height, width, in_channels, out_channels]           -> (3,3,64,64)\r\n\r\n\r\nApply:-\r\nimage_conv = tensorflow.nn.conv2d(in1, weight_kernel, strides=(1,1), padding='SAME', dilations=None)\r\n\r\n\r\nNow this gives me shape of output of conv, same as input shape         -> (1,56,56,64)\r\n\r\nIs this correct?\r\n\r\nOver this I have added the biases that were extracted from the resnet model , my_bias.shape (64,) - as shown in python\r\n\r\nthis is done by simply doing  image_conv + my_bias\r\n\r\nthe output dimension remains the same - (1,56,56,64)\r\n\r\n\r\n", "@manp-git ,\r\n\r\nPlease refer these issues with similar error log.It helps.#28777,[link1](https://stackoverflow.com/questions/60174964/invalidargumenterror-input-depth-must-be-evenly-divisible-by-filter-depth-4-v/60212961#60212961),[link2](https://github.com/llSourcell/deep_dream_challenge/issues/5)", "@manp-git ,\r\n\r\nLooks like this is duplicate of issue #48879.Can you please confirm if we close this issue, since it is already being tracked there? Thanks!", "https://github.com/tensorflow/tensorflow/issues/48933#issuecomment-834059960\r\n\r\nSure , @tilakrayal I have checked the similar error logs links that you have mentioned these did not help me resolve.\r\n\r\nHence I posted my resolve here to my own question to check if this is fine.\r\n\r\nIf it can be tracked in #48879 , I am happy with it.\r\n\r\n\r\n\r\n", "@manp-git ,\r\n\r\nPlease feel free to close this issue as it is being tracked #48879.\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48933\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48933\">No</a>\n"]}, {"number": 48932, "title": "Tflite from pytorch-onnx - seesaw facenet very slow loading.", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Xiaomi Redmi Note 5 Pro\r\n- TensorFlow installed from (source or binary): pip install tensorflow==2.4.0\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nTFlite loading time is approximately 10 seconds on Redmi Note 5 Pro. Pytorch for android is giving me lesser inference time and loading time on same mobile device.\r\n```\r\n\r\nAverage (of three) loading and running time (in ms) on 2 mobile devices!\r\n\r\n                               Inference Time                Model Loading Time\r\n                               Mobile 1 | Mobile 2            Mobile 1   | Mobile 2\r\nResnet-34 130 mb TFlite         768          1328               7                10\r\nSeesaw Facenet 35 mb Tflite     1669        3507               9332          15867\r\nSeesaw Facenet 35 mb Torch       922          2173               2925          9468\r\n\r\n```\r\n\r\n**Describe the expected behavior**\r\nTFlite loading time should be under 1 second as with other models like ResNets (34, 100 etc.)\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n''' Model code and pre-trained files here https://github.com/cvtower/seesawfacenet_pytorch/blob/master/src/seesaw_models/DW_seesawFaceNetv2.py ''''\r\n\r\n# IMPORTS\r\nfrom DW_seesawFaceNetv2 import *\r\nmodel = DW_seesawFaceNetv2(512)\r\nmodel = torch.nn.DataParallel(model)\r\nmodel.load_state_dict(torch.load('./trained-model.pth', map_location=torch.device('cpu') ))\r\nmodel.eval();\r\nmodel.to('cpu')\r\n\r\nprint(model(torch.zeros(1,3,112,112))) # for checking output after conversion\r\ninput_names = ['input'] # [\"Conv2d-1\"]\r\noutput_names = ['output'] # [\"DW_seesawFaceNetv2-826\"]\r\ninputs = torch.randn((1, 3, 112, 112), device = torch.device('cpu')) #to('cpu')\r\ndummy_input = Variable(inputs, requires_grad=True)\r\n\r\ntorch_out = torch.onnx.export(model.module.to('cpu'), dummy_input, 'output.onnx', export_params=True, verbose=True,\r\n                              input_names=input_names, output_names=output_names,\r\n                              opset_version = 12, do_constant_folding=False,\r\n                              enable_onnx_checker=True))\r\n\r\n# onnx.checker.check_model('./output.onnx') # to check the onnx conversion is correct\r\n\r\nmodel = onnx.load(\"output.onnx\")\r\nonnx.helper.printable_graph(model.graph) # print onnx graph!\r\n\r\nsess = nxrun.InferenceSession(\"./output.onnx\")\r\nximg = np.zeros((1, 3, 112, 112)).astype(np.float32)\r\n\r\nprint(\"The model expects input shape: \", sess.get_inputs()[0].shape)\r\nprint(\"The shape of the Image is: \", ximg.shape)\r\n\r\ninput_name = sess.get_inputs()[0].name\r\nlabel_name = sess.get_outputs()[0].name\r\n% time result = sess.run(None, {input_name: ximg})\r\nprob = result[0]\r\nprint(prob.ravel()) # to check if onnx outputs are same as torch model\r\n\r\n# Load ONNX model\r\nmodel_onnx = onnx.load('./output.onnx')\r\n\r\n# conv./output_onnxto TensorFlow format\r\ntf_rep = prepare(model_onnx)\r\n\r\n# Export model as .pb file\r\ntf_rep.export_graph('./onnx2tf')\r\n\r\ntf_model = tf.saved_model.load('./onnx2tf')\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('./onnx2tf') # path to the SavedModel directory\r\nconverter.allow_custom_ops = True\r\n\r\n# I have not used the below options!\r\n# converter.experimental_new_converter = True\r\n# converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n\r\ntflite_model = converter.convert() # takes a lot of time !!\r\n\r\n# Save the model.\r\nwith open('model.tflite', 'wb') as f:\r\n    f.write(tflite_model)\r\n```\r\n------------------------------------------------------------------------------------------------------\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nThe tflite model is the same size as the torch model approximately, the inference time is also in a good range. But the loadin time is coming out to be 10 seconds on some mobile devices, and consistently more than other models like ResNets (34,100 etc)\r\n\r\nA sample tflite model can be found here. There is a slight change in the number of blocks in original model and this model\r\nhttps://drive.google.com/file/d/1S52OXU0q9aFyk_IwGybyJ85WH1hEq3Hn/view?usp=sharing", "comments": ["I run the profiler with your model, here is what I got base on op type:\r\n```\r\nNumber of nodes executed: 1023\r\n============================== Summary by node type ==============================\r\n               [Node type]    [count]    [avg ms]      [avg %]      [cdf %]    [mem KB]  [times called]\r\n                   CONV_2D        857      95.729      68.921%      68.921%       0.000        857\r\n                 TRANSPOSE         47      11.771       8.475%      77.395%       0.000         47\r\n                  LOGISTIC         18       7.517       5.412%      82.807%       0.000         18\r\n                     SPLIT          5       6.587       4.742%      87.550%       0.000          5\r\n                      MEAN          3       6.388       4.599%      92.149%       0.000          3\r\n             CONCATENATION         11       4.572       3.292%      95.441%       0.000         11\r\n                       MUL         21       2.764       1.990%      97.430%       0.000         21\r\n                       PAD          7       1.560       1.123%      98.554%       0.000          7\r\n                   RESHAPE         14       0.665       0.479%      99.032%       0.000         14\r\n                       ADD         10       0.570       0.410%      99.443%       0.000         10\r\n             STRIDED_SLICE         14       0.492       0.354%      99.797%       0.000         14\r\n               MAX_POOL_2D          1       0.212       0.153%      99.950%       0.000          1\r\n           FULLY_CONNECTED          9       0.057       0.041%      99.991%       0.000          9\r\n                       ABS          3       0.007       0.005%      99.996%       0.000          3\r\n                       SUB          3       0.006       0.004%     100.000%       0.000          3\r\n```\r\n", "Confirmed that the loading time is high. Nettron and visualize tool both need very long time to process this model.\r\n\r\nFor runtime, @vizakshat Did you run all of them with single-thread?\r\nSince Conv_2D is the most expensive one in your model, you can try to use multi-thread or delegates like GPU with TFLite.\r\nAdditionally, you can try if quantize the model to int8 help the performance. A lot of Conv2d op has stride of (2, 2), if you can remove that, it will help speeding it up too.", "@karimnosseir do you know if there is a way to analyze why it has such high loading time?", "> Confirmed that the loading time is high. Nettron and visualize tool both need very long time to process this model.\r\n> \r\n> For runtime, @vizakshat Did you run all of them with single-thread?\r\n> Since Conv_2D is the most expensive one in your model, you can try to use multi-thread or delegates like GPU with TFLite.\r\n> Additionally, you can try if quantize the model to int8 help the performance. A lot of Conv2d op has stride of (2, 2), if you can remove that, it will help speeding it up too.\r\n\r\nRunning time is not an issue, Yes, I am using multi threads and using delegates for running. I need high accuracy for my application that's why I am not doing any quantizations. Does removing stride will help in loading time? I would like to be as close as possible to the author's code as I don't want to mess up the accuracies. I can't even see what's going on while loading. Any suggestions? @thaink @karimnosseir @rmothukuru ", "I pushed a change that should enhance the loading time. Can you retry past this [commit](https://github.com/tensorflow/tensorflow/commit/7957cde003aeb3284393cf960c1f2e369d9a8ddb)\r\n\r\nThanks"]}, {"number": 48931, "title": "in customized async op, should gpu allocate_output call be synchronized?", "body": "Hi, I am using tensorflow 1.15 with horovod in which cusomized asynchronous op kernel is involved.\r\nhorovod implements its collective op by enqueuing collective request to its own runtime message queue, and in nvidia gpu environment, horovod uses its own cuda stream other than tensorflow gpu stream.\r\ngenerally, horovod gpu allgather operation calls allocate_output and synchronizes on it (blocking host code). \r\nmy question is that, if allocate_output return valid memory pointer as soon as the call return, why calling BlockHostUntilDone on tensorflow stream is necessary?\r\nI checked tensorflow allocator implementation but got no clues. it appears to me that gpu memory allocation is done synchronously and actually not related to any stream.\r\na related discussion can be found [here](https://github.com/horovod/horovod/discussions/2877).\r\n", "comments": ["@woodlgz \r\nThank you for reporting your issue, we will update you at the earliest on this.", "@woodlgz \r\nCould you please upgrade to later versions of tf and let us know as tf 1.x is not actively supported on 2.x is.\r\nYou may refer to [this discussion](https://github.com/horovod/horovod/discussions).", "hi, Saduf2019, it 's not a bug. it's just a confusion.\r\ntensorflow 2.x should make no difference.", "BFC allocator is asynchronous; and this is best illustrated with an example.  Consider a program that does:\r\n\r\n```\r\n// All work is enqueued on the compute stream\r\na = bfc_allocate(size=100);\r\ndo_work(a)\r\nbfc_deallocate(a);\r\nb = bfc_allocate(size=100);\r\ndo_work(b)\r\nbfc_deallocate(b);\r\n```\r\n\r\nThe second allocation, `b`, can reuse the memory from `a` even though when we execute `b = bfc_allocate(size=100)`, `do_work(a)` is running on the GPU and using the memory.  The assumption is that any use of `b` will be ordered after `do_work(a)` so all is well.\r\n\r\nThis means you can think of the BFC allocate / deallocate operations as conceptually \"enqueued\" on the compute stream, and the pointers returned by allocate are legal to use only after the GPU execution has reached the point where the allocation was enqueued.\r\n\r\nFor this reason `BlockHostUntilDone` is necessary -- work enqueued on the Horovod stream will not be totally ordered after the BFC allocate operations otherwise.  However `BlockHostUntilDone` is a large hammer; it should be sufficient to create a CUDA event on the compute stream and wait on it on the Horovod stream.", "got it. thanks @sanjoy.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48931\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48931\">No</a>\n"]}, {"number": 48930, "title": "tensorflow go bazel visibility issue during bazel build", "body": "I'm using tensorflow 1.4.0 and  we are using bazel to build our service. During bazel build I'm getting the following error,\r\n\r\n`in go_library rule @com_github_tensorflow_tensorflow//tensorflow/go/op:op: target '@com_github_tensorflow_tensorflow//tensorflow/go:go' is not visible from target '@com_github_tensorflow_tensorflow//tensorflow/go/op:op'. Check the visibility declaration of the former target if you think the dependency is legitimate`\r\n\r\n\r\nI observed that the issue is existing because default package visibility is private.\r\nIs there any way to override this visibility behavior.\r\n\r\n", "comments": ["@b-entangled ,\r\n\r\nWe see that you are using tf version 1.4, there is no support for 1.x, please update to 2.x and let us know if you are using same issue.\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Thank you @tilakrayal .\r\nWe are creating git patches to change visibility. So I'm closing this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48930\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48930\">No</a>\n"]}, {"number": 48929, "title": "How the tf.linalg.eig operation is implemented and why it is  differentiable?", "body": "I used the tf.linalg.eig to compute the eigenvalues of an arbitrary real matrix. Although it is differentiable, I am concerned about how this operation is implemented and whether the differentiation is always correct.\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/linalg/eig", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nThanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48929\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48929\">No</a>\n"]}, {"number": 48928, "title": "compat.v1.resize_bilinear fails in 2.4.1", "body": "The following code works in 2.3.2 but fails in 2.4.1:\r\n\r\n`\r\nh, w = 3, 1\r\nimage = tf.constant(np.arange(h*w).reshape((h, w)), dtype=tf.float32)[tf.newaxis, ..., tf.newaxis]\r\n_input = tf.keras.layers.Input((h, w, 1))\r\ntf2_img = tf.image.resize(_input, [9, 1])\r\ntf1_img = tf.compat.v1.image.resize(_input, [9, 1], align_corners=True)\r\ntf1_img_bl = tf.compat.v1.image.resize_bilinear(_input, [9, 1], align_corners=True)  # <-- problematic line\r\nout = tf.concat([tf1_img, tf2_img, tf1_img_bl], 2)\r\n\r\nm = tf.keras.Model(inputs=_input, outputs=out)\r\n`\r\n", "comments": ["also fails in tf-nightly: 2.6.0-dev20210505", "@elad-c \r\nI Could  reproduce the code in tf2.4 and tf-nightly. Could you please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/0e42ef32964ec9f4205a1f99b67e6038/untitled41.ipynb).Thanks", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@UsharaniPagadala \r\nI checked the gist. Seems to validate the bug. Will this issue be fixed?", "@elad-c \r\n I could run the code in tf 2.4 as well.Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/0e42ef32964ec9f4205a1f99b67e6038/untitled41.ipynb).Thanks", "Hi @UsharaniPagadala ,\r\nMaybe we have some local differences, but running the code in the gist works well for 2.3 but fails for 2.4\r\ni'm not sure how local changes might affect the colab notebook, maybe you didn't do \"RESTART RUNTIME\"?\r\n\r\nThanks,\r\nElad", "@elad-c \r\nI did restart runtime  every time and worked for me and here is the [gist](https://colab.research.google.com/gist/UsharaniPagadala/0e42ef32964ec9f4205a1f99b67e6038/untitled41.ipynb).Thanks", "Hi @UsharaniPagadala , @elad-c . I tried the code in the gist and it fails for me as well with tf 2.4 (after pressing \"restart runtime\").\r\nYou can add a `print(tf.__version__)` to make sure the correct version is running. Thanks", "Hi,\r\nI tried the gist and I see a similar result (works well in 2.3 but fails in 2.4)...\r\n![screenshot](https://user-images.githubusercontent.com/44209964/120196859-00ee5880-c229-11eb-8699-28d4147d8fe7.png)\r\n", "It seems the nature of the compat.v1.resize_bilinear exporting and some odd deprecated decorator interactions means dispatching isn't being registered right now for some reason. We will look into this, but for now you can put the resize_bilinear call in a keras Lambda layer or a keras custom layer as a workaround.", "@elad-c Is this still an issue for you? Did you follow the above suggestion? I changed the last line as shown below. With that modification, it doesn't throw any error. [Here](https://colab.research.google.com/gist/jvishnuvardhan/338596f938330a20cd2bbcaae1a6e7af/untitled41.ipynb) is a gist for reference. Thanks!\r\n\r\n```\r\n# tf1_img_bl = tf.compat.v1.image.resize_bilinear(_input, [9, 1], align_corners=True,) # <-- problematic line\r\ntf1_img_bl = tf.image.resize(_input, size=[9, 1], method='bilinear')\r\n```", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48928\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48928\">No</a>\n"]}, {"number": 48927, "title": "Error loading maskrcnn  .pbtxt file", "body": "Error loading maskrcnn  .pbtxt file in opencv dnnn. i made a custom dataset and trained and created pb and ptxt  and i want to run the trained model in opencv dnn \r\ncreated .pbtxt using this code below \r\nlink to .pbtxt file https://drive.google.com/file/d/1HK86GRMg3LR8N9BPGZL5JyLdxVIxn3N3/view?usp=sharing\r\n```\r\nimport tensorflow as tf\r\nwith tf.gfile.FastGFile('data/gaze_model.pb', \"rb\") as f:\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n\r\n    for i in reversed(range(len(graph_def.node))):\r\n        if graph_def.node[i].op == 'Const':\r\n            del graph_def.node[i]\r\n\r\n    graph_def.library.Clear()\r\n\r\n    tf.train.write_graph(graph_def, \"\", 'data/gaze_model.pbtxt', as_text=True)\r\n```\r\n\r\ngenerated the  pb using trained .h5 file using this code \r\n\r\n```\r\n#!/usr/bin/env python3\r\n# -*- coding: utf-8 -*-\r\n\r\nimport os\r\nimport sys\r\nimport warnings\r\n\r\nimport keras.backend as K\r\nimport tensorflow as tf\r\n\r\nwarnings.filterwarnings('ignore', category=FutureWarning)\r\n# suppress warning and error message tf\r\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\r\n\r\n# Root directory of the project\r\nROOT_DIR = os.getcwd()\r\n# Import Mask RCNN\r\nsys.path.append(ROOT_DIR)  # To find local version of the library\r\nfrom mrcnn import model as modellib\r\nfrom mrcnn import utils\r\n\r\nimport coco \r\nK.clear_session()\r\nK.set_learning_phase(0)\r\n\r\n##############################################################################\r\n# Load model\r\n##############################################################################\r\n\r\n\r\n# Model Directory\r\nMODEL_DIR = (\"/logs\")\r\nDEFAULT_WEIGHTS =  (\"/content/drive/MyDrive/thirip2/mask_rcnn_coco.h5\")\r\n# Download COCO trained weights from Releases if needed\r\nif not os.path.exists(DEFAULT_WEIGHTS):\r\n    utils.download_trained_weights(DEFAULT_WEIGHTS)\r\n\r\n##############################################################################\r\n# Load configuration\r\n##############################################################################\r\n\r\n\r\n\r\nclass InferenceConfig(coco.CocoConfig):\r\n        # Set batch size to 1 since we'll be running inference on\r\n        # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\r\n        GPU_COUNT = 1\r\n        IMAGES_PER_GPU = 1\r\n        \r\n\r\n##############################################################################\r\n# Save entire model function\r\n##############################################################################\r\n\r\ndef h5_to_pb(h5_model, output_dir, model_name, out_prefix=\"output_\"):\r\n    out_nodes = []\r\n    for i in range(len(h5_model.outputs)):\r\n        out_nodes.append(out_prefix + str(i + 1))\r\n        tf.identity(h5_model.output[i], out_prefix + str(i + 1))\r\n    sess = K.get_session()\r\n    init_graph = sess.graph.as_graph_def()\r\n    main_graph = tf._api.v1.graph_util.convert_variables_to_constants(sess, init_graph, out_nodes)\r\n    with tf.gfile.GFile(os.path.join(output_dir, model_name), \"wb\") as filemodel:\r\n        filemodel.write(main_graph.SerializeToString())\r\n    print(\"pb model: \", {os.path.join(output_dir, model_name)})\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    config = InferenceConfig()\r\n    config.display()\r\n    # Create model in inference mode\r\n    model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\r\n\r\n    # Set path to model weights\r\n    weights_path = DEFAULT_WEIGHTS#model.find_last()\r\n    # Load weights\r\n    print(\"Loading weights \", weights_path)\r\n    model.load_weights(weights_path, by_name=True)\r\n    model.keras_model.summary()\r\n\r\n    # make folder for full model\r\n    model_dir = os.path.join(ROOT_DIR, \"Model\")\r\n    if not os.path.exists(model_dir):\r\n        os.makedirs(model_dir)\r\n\r\n    # save h5 full model\r\n    name_model = os.path.join(model_dir, \"/content/drive/MyDrive/thirip2/mask_rcnn_bottle_0040.h5\")\r\n    if not os.path.exists(name_model):\r\n        model.keras_model.save(name_model)\r\n        print(\"save model: \", name_model)\r\n\r\n    # export pb model\r\n    pb_name_model = \"mask_rcnn_landing.pb\"\r\n    h5_to_pb(model.keras_model, output_dir=model_dir, model_name=pb_name_model)\r\n    K.clear_session()\r\n    sys.exit()\r\n```\r\n\r\nlink to pb file https://drive.google.com/file/d/18B5_a7uKU6smvg05f6-H-JvsxDZYqGwY/view\r\n```?usp=sharing\r\n\r\n\r\nand loaded my .pb and generated pbtxt file to opencv dnn  got error  Failed to parse GraphDef file: E:/model.pbtxt in function 'cv::dnn::ReadTFNetParamsFromTextFileOrDie'\r\n\r\nwould be great if you guys help to get this done\r\nthank you ", "comments": ["@Jeffin21 ,\r\n\r\nPlease refer these links for similar error log [link1](https://stackoverflow.com/questions/55007656/cannot-parse-graphdef-file-in-function-readtfnetparamsfromtextfileordie-in-ope),[link2](https://github.com/opencv/opencv/issues/11805),[link3](https://github.com/opencv/opencv/issues/10043).Hope it helps.\r\n\r\nThanks!", "@tilakrayal thanks for your reply, already tried these. not working for me ", "any update??", "Hey, i got an update on this issue, i trained a custom model successfully converted to .pb file using .h5 file, this https://github.com/opencv/opencv/pull/13766 way i can run my custom model in opencv dnn to create the pbtxt from the pb file opencv got a module to do that ,this file https://github.com/opencv/opencv/blob/master/samples/dnn/tf_text_graph_mask_rcnn.py \r\nbut am facing the issues here is to convert to pbtxt i need pipeline.config i couldnt find that file and what i got is confi.py but when i train the model i got this text like this \r\n\r\n```\r\nConfigurations:\r\nBACKBONE                       resnet101\r\nBACKBONE_STRIDES               [4, 8, 16, 32, 64]\r\nBATCH_SIZE                     2\r\nBBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\r\nCOMPUTE_BACKBONE_SHAPE         None\r\nDETECTION_MAX_INSTANCES        100\r\nDETECTION_MIN_CONFIDENCE       0.9\r\nDETECTION_NMS_THRESHOLD        0.3\r\nFPN_CLASSIF_FC_LAYERS_SIZE     1024\r\nGPU_COUNT                      1\r\nGRADIENT_CLIP_NORM             5.0\r\nIMAGES_PER_GPU                 2\r\nIMAGE_CHANNEL_COUNT            3\r\nIMAGE_MAX_DIM                  1024\r\nIMAGE_META_SIZE                14\r\nIMAGE_MIN_DIM                  800\r\nIMAGE_MIN_SCALE                0\r\nIMAGE_RESIZE_MODE              square\r\nIMAGE_SHAPE                    [1024 1024    3]\r\nLEARNING_MOMENTUM              0.9\r\nLEARNING_RATE                  0.001\r\nLOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\r\nMASK_POOL_SIZE                 14\r\nMASK_SHAPE                     [28, 28]\r\nMAX_GT_INSTANCES               100\r\nMEAN_PIXEL                     [123.7 116.8 103.9]\r\nMINI_MASK_SHAPE                (56, 56)\r\nNAME                           bottle\r\nNUM_CLASSES                    2\r\nPOOL_SIZE                      7\r\nPOST_NMS_ROIS_INFERENCE        1000\r\nPOST_NMS_ROIS_TRAINING         2000\r\nPRE_NMS_LIMIT                  6000\r\nROI_POSITIVE_RATIO             0.33\r\nRPN_ANCHOR_RATIOS              [0.5, 1, 2]\r\nRPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\r\nRPN_ANCHOR_STRIDE              1\r\nRPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\r\nRPN_NMS_THRESHOLD              0.7\r\nRPN_TRAIN_ANCHORS_PER_IMAGE    256\r\nSTEPS_PER_EPOCH                100\r\nTOP_DOWN_PYRAMID_SIZE          256\r\nTRAIN_BN                       False\r\nTRAIN_ROIS_PER_IMAGE           200\r\nUSE_MINI_MASK                  True\r\nUSE_RPN_ROIS                   True\r\nVALIDATION_STEPS               50\r\nWEIGHT_DECAY                   0.0001\r\n\r\n```\r\nmy question is where can i find pipeline.config file \r\nthanks ", "@Jeffin21 You can find the `pipeline.config` file in the models directory in Tensorflow, path will be something like `object_detection/samples/configs/<model_name>.config`", "Hey,\nThanks for your reply, this is for maskrcnn I found only config.Py file not\npipeline.config\n\nOn Tue, 11 May 2021 at 9:59 AM sachinprasadhs ***@***.***>\nwrote:\n\n> @Jeffin21 <https://github.com/Jeffin21> You can find the pipeline.config\n> file in the models directory in Tensorflow, path will be something like\n> object_detection/samples/configs/<model_name>.config\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/48927#issuecomment-837783887>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AFZEXOJT2D6DNDKV2AXGWALTNCXDJANCNFSM44GLHDMQ>\n> .\n>\n-- \njeffin\n", "Sorry this file\nhttps://github.com/matterport/Mask_RCNN/tree/master/mrcnn\n\n\nOn Tue, 11 May 2021 at 10:04 AM jeffin george ***@***.***>\nwrote:\n\n> This file\n> https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/utils.py\n>\n> On Tue, 11 May 2021 at 10:02 AM jeffin george ***@***.***>\n> wrote:\n>\n>> Hey,\n>> Thanks for your reply, this is for maskrcnn I found only config.Py file\n>> not pipeline.config\n>>\n>> On Tue, 11 May 2021 at 9:59 AM sachinprasadhs ***@***.***>\n>> wrote:\n>>\n>>> @Jeffin21 <https://github.com/Jeffin21> You can find the pipeline.config\n>>> file in the models directory in Tensorflow, path will be something like\n>>> object_detection/samples/configs/<model_name>.config\n>>>\n>>> \u2014\n>>> You are receiving this because you were mentioned.\n>>> Reply to this email directly, view it on GitHub\n>>> <https://github.com/tensorflow/tensorflow/issues/48927#issuecomment-837783887>,\n>>> or unsubscribe\n>>> <https://github.com/notifications/unsubscribe-auth/AFZEXOJT2D6DNDKV2AXGWALTNCXDJANCNFSM44GLHDMQ>\n>>> .\n>>>\n>> --\n>> jeffin\n>>\n> --\n> jeffin\n>\n-- \njeffin\n", "@Jeffin21 You can find all the config files under this repository for your model. https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs.", "I tried most of the config files Maskrcnn particularly use resnet50 and resent 101 I tried both not working for me, I trained a custom model ,   does tensorflow generate pipeline config while training new model ? \n\n\n> On 11-May-2021, at 10:33 AM, sachinprasadhs ***@***.***> wrote:\n> \n> \ufeff\n> @Jeffin21 You can find all the config files under this repository for your model. https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n", "@Jeffin21, Yes it will be generated while training the model, you can find the below reference.\r\n\r\n![image](https://user-images.githubusercontent.com/73069040/117763367-50ec8780-b248-11eb-8da0-946ea9687bf0.png)\r\n", "@sachinprasadhs thanks, can you tell me the exact location? in tensor flow folder or something like that ?", "@Jeffin21, while training you might have specified output directory, you can check in that directory.", "@sachinprasadhs maskrcnn save .h5 file in log folder in that folder i didnt find files like  you mentioned above only just .h5 files ", "@sachinprasadhs what can see when the training get starts is this \r\n```\r\nConfigurations:\r\nBACKBONE                       resnet101\r\nBACKBONE_STRIDES               [4, 8, 16, 32, 64]\r\nBATCH_SIZE                     2\r\nBBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\r\nCOMPUTE_BACKBONE_SHAPE         None\r\nDETECTION_MAX_INSTANCES        100\r\nDETECTION_MIN_CONFIDENCE       0.9\r\nDETECTION_NMS_THRESHOLD        0.3\r\nFPN_CLASSIF_FC_LAYERS_SIZE     1024\r\nGPU_COUNT                      1\r\nGRADIENT_CLIP_NORM             5.0\r\nIMAGES_PER_GPU                 2\r\nIMAGE_CHANNEL_COUNT            3\r\nIMAGE_MAX_DIM                  1024\r\nIMAGE_META_SIZE                14\r\nIMAGE_MIN_DIM                  800\r\nIMAGE_MIN_SCALE                0\r\nIMAGE_RESIZE_MODE              square\r\nIMAGE_SHAPE                    [1024 1024    3]\r\nLEARNING_MOMENTUM              0.9\r\nLEARNING_RATE                  0.001\r\nLOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\r\nMASK_POOL_SIZE                 14\r\nMASK_SHAPE                     [28, 28]\r\nMAX_GT_INSTANCES               100\r\nMEAN_PIXEL                     [123.7 116.8 103.9]\r\nMINI_MASK_SHAPE                (56, 56)\r\nNAME                           bottle\r\nNUM_CLASSES                    2\r\nPOOL_SIZE                      7\r\nPOST_NMS_ROIS_INFERENCE        1000\r\nPOST_NMS_ROIS_TRAINING         2000\r\nPRE_NMS_LIMIT                  6000\r\nROI_POSITIVE_RATIO             0.33\r\nRPN_ANCHOR_RATIOS              [0.5, 1, 2]\r\nRPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\r\nRPN_ANCHOR_STRIDE              1\r\nRPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\r\nRPN_NMS_THRESHOLD              0.7\r\nRPN_TRAIN_ANCHORS_PER_IMAGE    256\r\nSTEPS_PER_EPOCH                100\r\nTOP_DOWN_PYRAMID_SIZE          256\r\nTRAIN_BN                       False\r\nTRAIN_ROIS_PER_IMAGE           200\r\nUSE_MINI_MASK                  True\r\nUSE_RPN_ROIS                   True\r\nVALIDATION_STEPS               50\r\nWEIGHT_DECAY                   0.0001\r\n```", "i set 40 epoches after that i convert the .h5 file to .pb, opencv dnn only take.pb files \r\nplease check this link https://github.com/opencv/opencv/pull/13766\r\n\r\nfor converting to h5 file to .pb file am using this code \r\n\r\n```\r\n#!/usr/bin/env python3\r\n# -*- coding: utf-8 -*-\r\n\r\nimport os\r\nimport sys\r\nimport warnings\r\n\r\nimport keras.backend as K\r\nimport tensorflow as tf\r\n\r\nwarnings.filterwarnings('ignore', category=FutureWarning)\r\n# suppress warning and error message tf\r\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\r\n\r\n# Root directory of the project\r\nROOT_DIR = os.getcwd()\r\n# Import Mask RCNN\r\nsys.path.append(ROOT_DIR)  # To find local version of the library\r\nfrom mrcnn import model as modellib\r\nfrom mrcnn import utils\r\n\r\nimport coco \r\nK.clear_session()\r\nK.set_learning_phase(0)\r\n\r\n##############################################################################\r\n# Load model\r\n##############################################################################\r\n\r\n\r\n# Model Directory\r\nMODEL_DIR = (\"/logs\")\r\nDEFAULT_WEIGHTS =  (\"/content/drive/MyDrive/thirip2/mask_rcnn_coco.h5\")\r\n# Download COCO trained weights from Releases if needed\r\nif not os.path.exists(DEFAULT_WEIGHTS):\r\n    utils.download_trained_weights(DEFAULT_WEIGHTS)\r\n\r\n##############################################################################\r\n# Load configuration\r\n##############################################################################\r\n\r\n\r\n\r\nclass InferenceConfig(coco.CocoConfig):\r\n        # Set batch size to 1 since we'll be running inference on\r\n        # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\r\n        GPU_COUNT = 1\r\n        IMAGES_PER_GPU = 1\r\n        \r\n\r\n##############################################################################\r\n# Save entire model function\r\n##############################################################################\r\n\r\ndef h5_to_pb(h5_model, output_dir, model_name, out_prefix=\"output_\"):\r\n    out_nodes = []\r\n    for i in range(len(h5_model.outputs)):\r\n        out_nodes.append(out_prefix + str(i + 1))\r\n        tf.identity(h5_model.output[i], out_prefix + str(i + 1))\r\n    sess = K.get_session()\r\n    init_graph = sess.graph.as_graph_def()\r\n    main_graph = tf._api.v1.graph_util.convert_variables_to_constants(sess, init_graph, out_nodes)\r\n    with tf.gfile.GFile(os.path.join(output_dir, model_name), \"wb\") as filemodel:\r\n        filemodel.write(main_graph.SerializeToString())\r\n    print(\"pb model: \", {os.path.join(output_dir, model_name)})\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    config = InferenceConfig()\r\n    config.display()\r\n    # Create model in inference mode\r\n    model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\r\n\r\n    # Set path to model weights\r\n    weights_path = DEFAULT_WEIGHTS#model.find_last()\r\n    # Load weights\r\n    print(\"Loading weights \", weights_path)\r\n    model.load_weights(weights_path, by_name=True)\r\n    model.keras_model.summary()\r\n\r\n    # make folder for full model\r\n    model_dir = os.path.join(ROOT_DIR, \"Model\")\r\n    if not os.path.exists(model_dir):\r\n        os.makedirs(model_dir)\r\n\r\n    # save h5 full model\r\n    name_model = os.path.join(model_dir, \"/content/drive/MyDrive/thirip2/mask_rcnn_bottle_0040.h5\")\r\n    if not os.path.exists(name_model):\r\n        model.keras_model.save(name_model)\r\n        print(\"save model: \", name_model)\r\n\r\n    # export pb model\r\n    pb_name_model = \"mask_rcnn_landing.pb\"\r\n    h5_to_pb(model.keras_model, output_dir=model_dir, model_name=pb_name_model)\r\n    K.clear_session()\r\n    sys.exit()\r\n```", "@sachinprasadhs this module by opencv to generate the pbtxt from .pb file. Opencv dnn need pb and pbtxt file to load\r\nplease check https://github.com/opencv/opencv/blob/master/samples/dnn/tf_text_graph_mask_rcnn.py\r\n\r\nAs the last procedure loading the pb and pbtxt files to opencv dnn this file \r\nhttps://github.com/opencv/opencv/blob/master/samples/dnn/mask_rcnn.py", "@Jeffin21  this issue seems to be on opencv, can you raise this issue in opencv repo.", "@Jeffin21 i think its not their issue, only need the pipeline.config file for this , they actuality done everything to run i successfully tried one example. they provided sample too.\r\nhttp://download.tensorflow.org/models/object_detection/mask_rcnn_resnet101_atrous_coco_2018_01_28.tar.gz\r\nfor making the pbtxt we need the config file, struggling to find that for 2 weeks, now posting everywhere no help :(", "an update on this issue here full trace\r\n\r\n```\r\nNumber of classes: 90\r\nScales:            [0.25, 0.5, 1.0, 2.0]\r\nAspect ratios:     [0.5, 1.0, 2.0]\r\nWidth stride:      16.000000\r\nHeight stride:     16.000000\r\nFeatures stride:   8.000000\r\n[libprotobuf ERROR C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-req-build-vi271kac\\opencv\\3rdparty\\protobuf\\src\\google\\protobuf\\wire_format_lite.cc:629] String field 'opencv_tensorflow.FunctionDef.Node.ret' contains invalid UTF-8 data when parsing a protocol buffer. Use the 'bytes' type if you intend to send raw bytes.\r\n2021-05-15 13:18:22.371302: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n2021-05-15 13:18:22.371450: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nTraceback (most recent call last):\r\n  File \"E:\\mask_rcnn_inception_v2_coco_2018_01_28\\opencv\\sources\\samples\\dnn\\tf_text_graph_common.py\", line 313, in writeTextGraph\r\n    cv.dnn.writeTextGraph(modelPath, outputPath)\r\ncv2.error: OpenCV(4.5.2) C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-req-build-vi271kac\\opencv\\modules\\dnn\\src\\tensorflow\\tf_io.cpp:42: error: (-2:Unspecified error) FAILED: ReadProtoFromBinaryFile(param_file, param). Failed to parse GraphDef file: E:\\mask_rcnn_inception_v2_coco_2018_01_28\\motabiju\\saved_model.pb in function 'cv::dnn::ReadTFNetParamsFromBinaryFileOrDie'\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"E:\\mask_rcnn_inception_v2_coco_2018_01_28\\opencv\\sources\\samples\\dnn\\tf_text_graph_mask_rcnn.py\", line 53, in <module>\r\n    writeTextGraph(args.input, args.output, ['num_detections', 'detection_scores', 'detection_boxes', 'detection_classes', 'detection_masks'])\r\n  File \"E:\\mask_rcnn_inception_v2_coco_2018_01_28\\opencv\\sources\\samples\\dnn\\tf_text_graph_common.py\", line 316, in writeTextGraph\r\n    from tensorflow.tools.graph_transforms import TransformGraph\r\nModuleNotFoundError: No module named 'tensorflow.tools.graph_transforms'\r\n```", "I don't think It is available in recent TF versions https://stackoverflow.com/questions/59825444/modulenotfounderror-no-module-named-tensorflow-tools-graph-transforms-when-us\n\nHonestly I think this whole ticket is more on the Opencv side and Its conversion/importing tools and we could close this.", "@bhack my project is in  tf 1.15 , not 2.0", "> @bhack my project is in  tf 1.15 , not 2.0\n\nYes but we don't support TF 1.x anymore here.", "Just as an extra side note that symbol is not exposed anymore. See https://github.com/tensorflow/tensorflow/issues/33352", "@bhack am able to train and and i want the pb file to work in in opencv dnn something like this https://github.com/opencv/opencv/wiki/TensorFlow-Object-Detection-API#use-existing-config-file-for-your-model \r\n\r\nwould be great if you can suggest me any other ways to achieve the instance segmentation in tensor flow that work in opencv dnn that would be great. tried the keras model and not worked \r\nthanks ", "Why you don't open a ticket there with a request of adding a column of the specific TF versions required/tested?\n\nI don't think that we could anything else here.", "@bhack okay i got it,  Thanks ", "I think you can close this.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48927\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48927\">No</a>\n"]}, {"number": 48925, "title": "Only some Keras activations under `tf.keras.activations` are supported", "body": "Hi,\r\n\r\nWhen adapting QAT(Quantization aware training) to get bbox regression with Keras, \r\nI've got following error:\r\n`ValueError: Only some Keras activations under `tf.keras.activations` are supported. For other activations, use `Quantizer` directly, and update layer config using `QuantizeConfig`.\r\n`\r\nEven I referred some Tensorflow documents, I cannot find out what's problem at all.\r\n[link](https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/quantization/keras/quantize_model?hl=ko): \r\n\r\nI attach my code block as below:\r\n```\r\nfrom tensorflow.keras.applications import MobileNetV2\r\nfrom tensorflow.keras.layers import AveragePooling2D\r\nfrom tensorflow.keras.layers import Dropout\r\nfrom tensorflow.keras.layers import Flatten\r\nfrom tensorflow.keras.layers import Dense\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\nINIT_LR = 1e-4\r\nNUM_EPOCHS = 1\r\nBATCH_SIZE = 8\r\n\r\nbasemodel = MobileNetV2(weights=\"imagenet\", include_top=False,\r\n        input_tensor=Input(shape=(224, 224, 3)))\r\nbasemodel.trainable = False\r\n\r\n# MobileNetV2\r\nflatten = basemodel.output\r\nbboxHead = Flatten(name=\"flatten\")(flatten)\r\nbboxHead = Dense(128, activation=\"relu\")(bboxHead)\r\nbboxHead = Dense(4, activation=\"sigmoid\")(bboxHead)\r\n\r\nmodel = Model(inputs=basemodel.input, outputs=bboxHead)\r\nopt = Adam(INIT_LR)\r\nmodel.compile(loss=\"mse\", optimizer=opt)\r\nH = model.fit(\r\n        trainImages, trainTargets, \r\n        validation_data=(testImages, testTargets),\r\n        batch_size=BATCH_SIZE,\r\n        epochs=NUM_EPOCHS,\r\n        verbose=1)\r\n\r\nimport tensorflow_model_optimization as tfmot\r\nquantize_model = tfmot.quantization.keras.quantize_model\r\n\r\nq_aware_model = quantize_model(model)\r\nq_aware_model.compile(loss=\"mse\", optimizer=opt, metrics=['accuracy'])\r\n```\r\n'**sigmoid**' of the last Dense layer may the error, because if I change this to the other activation, the error disappeared.\r\nJust disappeared.\r\n\r\n-----\r\n[Edit]\r\nTF: 2.4.1\r\n\r\nThanks,\r\n\r\n\r\n", "comments": ["@mipsan \r\nCould you please provide the dependencies as I am facing these [errors](https://colab.research.google.com/gist/UsharaniPagadala/82625cd04d3a942ed6a3d8f8482e358d/-48925.ipynb) .Thanks", "@UsharaniPagadala \r\n\r\nThanks, \r\nUpdated dependencies, then how about dataset?\r\n\r\n", "@mipsan \r\n\r\nI was able to reproduce the error.Could you please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/b2ac4f74f2b6e399ccc41ee415b4499c/-48925.ipynb#scrollTo=8DqfVFCf6LVB) here, also have a look at [this](https://github.com/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/python/core/quantization/keras/quantize_aware_activation.py) and let us know if it helps.Thanks", "@mipsan Just a quick question. Are you looking for 4 units in output layer with `sigmoid` activation or with `softmax` activation?\r\n\r\n`bboxHead = Dense(4, activation=\"sigmoid\")(bboxHead)` \r\nor\r\n`bboxHead = Dense(4, activation=\"softmax\")(bboxHead)`\r\n\r\nWhen I change to `softmax` activation in the final layer, it doesn't throw an error.  Thanks!", "@jvishnuvardhan \r\nSigmod for activation to get [0..1] outputs. \r\nsoftmax, as you said, does not make error but it's not what i want.\r\n\r\nActually, the purpose of this try is bbox regression.\r\nThanks", "@UsharaniPagadala \r\n\r\nI tested this [guide](https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide) as well, same errors as a result.\r\n", "@UsharaniPagadala \r\n\r\nYour [idea](https://github.com/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/python/core/quantization/keras/quantize_aware_activation.py) gave me a hint!\r\n\r\nI found that a version of `tensorflow-model-optimization` is **0.5.0** \r\nand `quantize_aware_activation.py` slightly differs to what I have. \r\n`_PRE_QUANT_ACTIVATIONS = frozenset({'softmax', 'sigmoid'})`\r\nBut \r\nI have `_PRE_QUANT_ACTIVATIONS = frozenset({'softmax'})`. \r\n\r\nAfter add `sigmoid` to this line, I am able to get what I want.\r\n\r\nThanks,\r\n", "@mipsan\r\n\r\nThank you for your update, glad its working fine for you, kindly move this issue to closed status as it is resolved.\r\n"]}, {"number": 48923, "title": " Failed to initialize TensorFlow context: subgraphs is null", "body": "I am trying to run this program; however I keep on getting the error listed below. \r\n```\r\npackage com.example.se;\r\n\r\nimport android.content.Context;\r\nimport android.content.Intent;\r\nimport android.content.SharedPreferences;\r\nimport android.content.res.AssetFileDescriptor;\r\nimport android.content.res.AssetManager;\r\nimport android.database.Cursor;\r\nimport android.media.AudioManager;\r\nimport android.net.Uri;\r\nimport android.net.rtp.AudioStream;\r\nimport android.os.Bundle;\r\nimport android.os.FileUtils;\r\nimport android.util.Log;\r\nimport android.view.LayoutInflater;\r\nimport android.view.View;\r\nimport android.view.ViewGroup;\r\nimport android.widget.Button;\r\nimport android.widget.Toast;\r\n\r\nimport androidx.annotation.NonNull;\r\nimport androidx.annotation.Nullable;\r\nimport androidx.fragment.app.Fragment;\r\nimport androidx.fragment.app.FragmentManager;\r\nimport androidx.fragment.app.FragmentPagerAdapter;\r\nimport androidx.fragment.app.FragmentTransaction;\r\nimport androidx.viewpager.widget.ViewPager;\r\n\r\nimport com.example.se.ui.main.SectionsPagerAdapter;\r\nimport com.google.android.material.tabs.TabLayout;\r\n\r\nimport org.tensorflow.lite.Interpreter;\r\nimport org.tensorflow.lite.flex.FlexDelegate;\r\nimport java.io.BufferedInputStream;\r\nimport java.io.BufferedReader;\r\nimport java.io.ByteArrayOutputStream;\r\nimport java.io.File;\r\nimport java.io.FileInputStream;\r\nimport java.io.IOException;\r\nimport java.io.InputStream;\r\nimport java.io.InputStreamReader;\r\nimport java.lang.Object;\r\nimport java.lang.reflect.Array;\r\nimport java.net.URISyntaxException;\r\nimport java.nio.ByteBuffer;\r\nimport java.nio.FloatBuffer;\r\nimport java.nio.MappedByteBuffer;\r\nimport java.nio.channels.FileChannel;\r\nimport java.util.ArrayList;\r\nimport java.util.Arrays;\r\nimport java.util.HashMap;\r\nimport java.util.List;\r\nimport java.util.Map;\r\n\r\npublic class Classify extends Fragment {\r\n    private Button choose_file_button;\r\n    public static final int PICKFILE_RESULT_CODE = 1;\r\n    private Uri fileUri;\r\n    private String filePath; // This is the final file path\r\n    View classify_view;\r\n\r\n    // To load from asset folder\r\n    private static final String LABEL_FILENAME = \"file:///android_asset/labels.txt\";\r\n    private static final String MODEL_FILENAME = \"file:///android_asset/soundclassifier.tflite\";\r\n    private static final String LOG_TAG = \"Log tagges is here\";\r\n\r\n    // For label and modelfile\r\n    private List<String> labels = new ArrayList<String>();\r\n    private List<String> displayedLabels = new ArrayList<>();\r\n\r\n    // For the audio file\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    InputStream in;\r\n    byte[] audioBytes;\r\n    Float[][] audioFile = new Float[1][44032];\r\n\r\n    // For machine learning\r\n    private MappedByteBuffer tfLiteModel;\r\n    private Interpreter tfLite;\r\n    private final Interpreter.Options ftliteOptions = new Interpreter.Options();\r\n    float[] outputs;\r\n    private RecognizeCommands recognizeCommands = null;\r\n    private int modelInputLength;\r\n    private int modelNumClasses;\r\n    private FloatBuffer inputBuffer;\r\n\r\n    @Nullable\r\n    @Override\r\n    public View onCreateView(@NonNull LayoutInflater inflater, @Nullable ViewGroup container, @Nullable Bundle savedInstanceState) {\r\n        classify_view = inflater.inflate(R.layout.classify,container,false);\r\n\r\n        // Both finds the classify and stop classify button\r\n        choose_file_button = (Button) classify_view.findViewById(R.id.classify_button);\r\n\r\n        // For labels file\r\n        String actualLabelFilename = LABEL_FILENAME.split(\"file:///android_asset/\",-1)[1];\r\n        Log.i(LOG_TAG,\"Reading labels from \" + actualLabelFilename);\r\n\r\n        BufferedReader br = null;\r\n        try{\r\n            br = new BufferedReader(new InputStreamReader(classify_view.getContext().getAssets().open(actualLabelFilename)));\r\n            String line;\r\n            while ((line = br.readLine()) != null){\r\n                labels.add(line);\r\n                if (line.charAt(0) != '_'){\r\n                    displayedLabels.add(line.substring(0,1).toUpperCase()+ line.substring(1));\r\n                }\r\n            }\r\n        } catch (IOException e){\r\n            throw new RuntimeException(\"Problem reading the label file!\",e);\r\n        }\r\n        // Creates the equal number of labels on the output file\r\n        Log.i(LOG_TAG,\"Labels file messages are :\"+ displayedLabels);\r\n        outputs = new float[displayedLabels.size()];\r\n\r\n        // ToDo : Implement Recognize Commands if not working\r\n\r\n        // Opening the model file\r\n        String actualModelFilename = MODEL_FILENAME.split(\"file:///android_asset/\",-1)[1];\r\n        try{\r\n            tfLiteModel = loadModelFile(classify_view.getContext().getAssets(), actualModelFilename);\r\n        } catch (Exception e){\r\n            throw new RuntimeException(e);\r\n        }\r\n\r\n        Log.i(LOG_TAG,\"The modal file is :\"+actualModelFilename);\r\n        Log.i(LOG_TAG,\"The actual content is :\"+tfLiteModel);\r\n\r\n        // ToDo : Model file opened here\r\n        try{\r\n            ftliteOptions.setNumThreads(1);\r\n            FlexDelegate flex = new FlexDelegate();\r\n            ftliteOptions.addDelegate(flex);\r\n            File openThis = new File(MODEL_FILENAME);\r\n            tfLite = new Interpreter(tfLiteModel,ftliteOptions);\r\n            // tfLite = new Interpreter(openThis);\r\n        } catch (Exception e){\r\n            throw new RuntimeException(e);\r\n        }\r\n        Log.i(LOG_TAG,\"TF lite file loaded. \");\r\n\r\n        // To load the metadata and verify it\r\n        int [] inputShape = tfLite.getInputTensor(0).shape();\r\n        modelInputLength = inputShape[1];\r\n\r\n        int [] outputShape  = tfLite.getOutputTensor(0).shape();\r\n        modelNumClasses = outputShape[1];\r\n\r\n        Log.i(LOG_TAG,\" \"+modelNumClasses);\r\n        if (modelNumClasses != displayedLabels.size()){\r\n            Log.e(LOG_TAG,\"The file's metadata is not the same\");\r\n        }else{\r\n            Log.i(LOG_TAG,\"The file's metadata is same\");\r\n        }\r\n        Log.i(LOG_TAG,\" \"+displayedLabels.size());\r\n        inputBuffer = FloatBuffer.allocate(modelInputLength);\r\n\r\n        choose_file_button.setOnClickListener(new View.OnClickListener() {\r\n            @Override\r\n            public void onClick(View v) {\r\n                Intent chooseFile = new Intent(Intent.ACTION_GET_CONTENT);\r\n                chooseFile.setType(\"*/*\");\r\n                chooseFile = Intent.createChooser(chooseFile, \"Choose a file\");\r\n                startActivityForResult(chooseFile, PICKFILE_RESULT_CODE);\r\n                // At this point we have the path of the file\r\n                // File path working.\r\n                /*\r\n                    For pie chart we can have : https://github.com/PhilJay/MPAndroidChart\r\n                 * */\r\n            }\r\n        });\r\n        return classify_view;\r\n    }\r\n    // This gets the file path\r\n    @Override\r\n    public void onActivityResult(int requestCode, int resultCode, Intent data) {\r\n        switch (requestCode) {\r\n            case PICKFILE_RESULT_CODE:\r\n                if (resultCode == -1) {\r\n                    fileUri = data.getData();\r\n                    filePath = fileUri.getPath();\r\n                    System.out.println(\"The selected file path is :\"+filePath);\r\n                    open_audio_file(fileUri);\r\n                    // Opens main audio file\r\n                    try{\r\n                        FloatBuffer outputBuffer = FloatBuffer.allocate(modelNumClasses);\r\n                        inputBuffer.rewind();\r\n                        outputBuffer.rewind();\r\n                        tfLite.run(inputBuffer,outputBuffer);\r\n                        Log.i(LOG_TAG,\"The output is :\"+ Arrays.toString(outputBuffer.array()));\r\n                        SharedPreferences sharedPref = classify_view.getContext().getSharedPreferences(getString(R.string.ml_values), Context.MODE_PRIVATE); // To open in private mode, can only be seen\r\n                        // by our application\r\n                        SharedPreferences.Editor editor = sharedPref.edit(); // Opening the file to edit\r\n                        float [] arr = outputBuffer.array();\r\n                        String str = \" \";\r\n                        for(int i=0;i<arr.length;i++){\r\n                            str = str + \", \"+ arr[i];\r\n                        }\r\n                        editor.putString(\"FLOAT_ARR\",str); // Putting in the string, Now Email keyword in SharedPref is associated with email entered by the user\r\n                        editor.apply(); // Applying the changes\r\n                        inputBuffer.clear();\r\n                        outputBuffer.clear();\r\n                        openHistoryPage(); // This opens history page after everything is done\r\n                    }catch(Exception e){\r\n                        throw new RuntimeException(e);\r\n                    }\r\n                }\r\n                break;\r\n        }\r\n    }\r\n\r\n    public void open_audio_file(Uri filePath){\r\n        try{\r\n            in = new BufferedInputStream(getContext().getContentResolver().openInputStream(filePath));\r\n\r\n            int read;\r\n            byte[] buff = new byte[1024];\r\n            while ((read = in.read(buff)) > 0)\r\n            {\r\n                out.write(buff, 0, read);\r\n            }\r\n            out.flush();\r\n        }catch(Exception e){\r\n            throw new RuntimeException(e);\r\n        }\r\n\r\n        //Todo : Change the audio file to a float pointer\r\n        audioBytes = out.toByteArray();\r\n        for (int i = 0;i < audioBytes.length;i++){\r\n            float val = (float) audioBytes[i];\r\n            inputBuffer.put(i,val);\r\n            audioFile[0][i] = val;\r\n        }\r\n    }\r\n\r\n    // This method loads the TF lite file\r\n    private static MappedByteBuffer loadModelFile(AssetManager assets, String modelFileName)\r\n            throws IOException{\r\n        AssetFileDescriptor fileDescriptor = assets.openFd(modelFileName);\r\n        FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());\r\n        FileChannel fileChannel = inputStream.getChannel();\r\n        long startOffset = fileDescriptor.getStartOffset();\r\n        long declaredLength = fileDescriptor.getDeclaredLength();\r\n        return fileChannel.map(FileChannel.MapMode.READ_ONLY,startOffset,declaredLength);\r\n    }\r\n    // To open history page\r\n    public void openHistoryPage(){\r\n        ViewPager viewPager = getActivity().findViewById(R.id.view_pager);\r\n        viewPager.setCurrentItem(2);\r\n    }\r\n}\r\n```\r\n```\r\nE/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: com.example.se, PID: 5777\r\n    java.lang.RuntimeException: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Failed to initialize TensorFlow context: subgraphs is null\r\n    Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\n    Node number 2 (FlexSize) failed to prepare.\r\n    \r\n        at com.example.se.Classify.onCreateView(Classify.java:138)\r\n        at androidx.fragment.app.Fragment.performCreateView(Fragment.java:2600)\r\n        at androidx.fragment.app.FragmentManagerImpl.moveToState(FragmentManagerImpl.java:881)\r\n        at androidx.fragment.app.FragmentManagerImpl.moveFragmentToExpectedState(FragmentManagerImpl.java:1238)\r\n        at androidx.fragment.app.FragmentManagerImpl.moveToState(FragmentManagerImpl.java:1303)\r\n        at androidx.fragment.app.BackStackRecord.executeOps(BackStackRecord.java:439)\r\n        at androidx.fragment.app.FragmentManagerImpl.executeOps(FragmentManagerImpl.java:2079)\r\n        at androidx.fragment.app.FragmentManagerImpl.executeOpsTogether(FragmentManagerImpl.java:1869)\r\n        at androidx.fragment.app.FragmentManagerImpl.removeRedundantOperationsAndExecute(FragmentManagerImpl.java:1824)\r\n        at androidx.fragment.app.FragmentManagerImpl.execSingleAction(FragmentManagerImpl.java:1696)\r\n        at androidx.fragment.app.BackStackRecord.commitNowAllowingStateLoss(BackStackRecord.java:299)\r\n        at androidx.fragment.app.FragmentPagerAdapter.finishUpdate(FragmentPagerAdapter.java:235)\r\n        at androidx.viewpager.widget.ViewPager.populate(ViewPager.java:1244)\r\n        at androidx.viewpager.widget.ViewPager.populate(ViewPager.java:1092)\r\n        at androidx.viewpager.widget.ViewPager.onMeasure(ViewPager.java:1622)\r\n        at android.view.View.measure(View.java:25466)\r\n        at android.view.ViewGroup.measureChildWithMargins(ViewGroup.java:6957)\r\n        at androidx.coordinatorlayout.widget.CoordinatorLayout.onMeasureChild(CoordinatorLayout.java:760)\r\n        at com.google.android.material.appbar.HeaderScrollingViewBehavior.onMeasureChild(HeaderScrollingViewBehavior.java:99)\r\n        at com.google.android.material.appbar.AppBarLayout$ScrollingViewBehavior.onMeasureChild(AppBarLayout.java:2003)\r\n        at androidx.coordinatorlayout.widget.CoordinatorLayout.onMeasure(CoordinatorLayout.java:831)\r\n        at android.view.View.measure(View.java:25466)\r\n        at android.view.ViewGroup.measureChildWithMargins(ViewGroup.java:6957)\r\n        at android.widget.FrameLayout.onMeasure(FrameLayout.java:194)\r\n        at androidx.appcompat.widget.ContentFrameLayout.onMeasure(ContentFrameLayout.java:146)\r\n        at android.view.View.measure(View.java:25466)\r\n        at android.view.ViewGroup.measureChildWithMargins(ViewGroup.java:6957)\r\n        at android.widget.LinearLayout.measureChildBeforeLayout(LinearLayout.java:1552)\r\n        at android.widget.LinearLayout.measureVertical(LinearLayout.java:842)\r\n        at android.widget.LinearLayout.onMeasure(LinearLayout.java:721)\r\n        at android.view.View.measure(View.java:25466)\r\n        at android.view.ViewGroup.measureChildWithMargins(ViewGroup.java:6957)\r\n        at android.widget.FrameLayout.onMeasure(FrameLayout.java:194)\r\n        at android.view.View.measure(View.java:25466)\r\n        at android.view.ViewGroup.measureChildWithMargins(ViewGroup.java:6957)\r\n        at android.widget.LinearLayout.measureChildBeforeLayout(LinearLayout.java:1552)\r\n        at android.widget.LinearLayout.measureVertical(LinearLayout.java:842)\r\n        at android.widget.LinearLayout.onMeasure(LinearLayout.java:721)\r\n        at android.view.View.measure(View.java:25466)\r\n        at android.view.ViewGroup.measureChildWithMargins(ViewGroup.java:6957)\r\n        at android.widget.FrameLayout.onMeasure(FrameLayout.java:194)\r\n        at com.android.internal.policy.DecorView.onMeasure(DecorView.java:747)\r\n        at android.view.View.measure(View.java:25466)\r\n        at android.view.ViewRootImpl.performMeasure(ViewRootImpl.java:3397)\r\n        at android.view.ViewRootImpl.measureHierarchy(ViewRootImpl.java:2228)\r\n        at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:2486)\r\n        at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:1952)\r\n        at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:8171)\r\nE/AndroidRuntime:     at android.view.Choreographer$CallbackRecord.run(Choreographer.java:972)\r\n        at android.view.Choreographer.doCallbacks(Choreographer.java:796)\r\n        at android.view.Choreographer.doFrame(Choreographer.java:731)\r\n        at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:957)\r\n        at android.os.Handler.handleCallback(Handler.java:938)\r\n        at android.os.Handler.dispatchMessage(Handler.java:99)\r\n        at android.os.Looper.loop(Looper.java:223)\r\n        at android.app.ActivityThread.main(ActivityThread.java:7656)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:592)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:947)\r\n     Caused by: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Failed to initialize TensorFlow context: subgraphs is null\r\n    Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\n    Node number 2 (FlexSize) failed to prepare.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegates(NativeInterpreterWrapper.java:367)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:85)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:63)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:277)\r\n        at com.example.se.Classify.onCreateView(Classify.java:135)\r\n        \t... 58 more\r\n```\r\n", "comments": ["The build.gradle file is: \r\n```\r\nplugins {\r\n    id 'com.android.application'\r\n    id 'com.google.gms.google-services'\r\n}\r\n\r\nandroid {\r\n    compileSdkVersion 30\r\n    buildToolsVersion \"30.0.3\"\r\n\r\n    aaptOptions{\r\n        noCompress \"tflite\"\r\n    }\r\n\r\n    defaultConfig {\r\n        applicationId \"com.example.se\"\r\n        minSdkVersion 25\r\n        targetSdkVersion 30\r\n        versionCode 1\r\n        versionName \"1.0\"\r\n\r\n        testInstrumentationRunner \"androidx.test.runner.AndroidJUnitRunner\"\r\n    }\r\n\r\n    buildTypes {\r\n        release {\r\n            minifyEnabled false\r\n            proguardFiles getDefaultProguardFile('proguard-android-optimize.txt'), 'proguard-rules.pro'\r\n        }\r\n    }\r\n    compileOptions {\r\n        sourceCompatibility JavaVersion.VERSION_1_8\r\n        targetCompatibility JavaVersion.VERSION_1_8\r\n    }\r\n    buildFeatures {\r\n        mlModelBinding true\r\n    }\r\n}\r\n\r\ndependencies {\r\n    implementation 'androidx.appcompat:appcompat:1.2.0'\r\n    implementation 'com.google.android.material:material:1.3.0'\r\n    implementation 'androidx.constraintlayout:constraintlayout:2.0.4'\r\n    implementation 'com.google.firebase:firebase-auth:20.0.3'\r\n    implementation 'com.google.firebase:firebase-database:19.7.0'\r\n    implementation 'androidx.lifecycle:lifecycle-livedata-ktx:2.2.0'\r\n    implementation 'androidx.lifecycle:lifecycle-viewmodel-ktx:2.2.0'\r\n    implementation \"org.tensorflow:tensorflow-lite:2.4.0\"\r\n    implementation 'com.google.firebase:firebase-ml-model-interpreter:22.0.3'\r\n    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly-SNAPSHOT'\r\n    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly-SNAPSHOT'\r\n    testImplementation 'junit:junit:4.+'\r\n    implementation 'com.github.PhilJay:MPAndroidChart:v3.1.0'\r\n    androidTestImplementation 'androidx.test.ext:junit:1.1.2'\r\n    androidTestImplementation 'androidx.test.espresso:espresso-core:3.3.0'\r\n}\r\n```", "Could you add the following static code in your Java class?\r\n\r\n```\r\nimport org.tensorflow.lite.flex.FlexDelegate;\r\n\r\npublic class Classify extends Fragment {\r\n  ...\r\n  static {\r\n    FlexDelegate.initTensorFlowForTesting();\r\n  }\r\n}\r\n```", "@abattery I added the static code, I get the same kind of error. \r\n```\r\nE/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: com.example.se, PID: 7472\r\n    java.lang.RuntimeException: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Failed to initialize TensorFlow context: subgraphs is null\r\n    Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\n    Node number 2 (FlexSize) failed to prepare.\r\n    \r\n        at com.example.se.Classify.onCreateView(Classify.java:141)\r\n        at androidx.fragment.app.Fragment.performCreateView(Fragment.java:2600)\r\n        at androidx.fragment.app.FragmentManagerImpl.moveToState(FragmentManagerImpl.java:881)\r\n        at androidx.fragment.app.FragmentManagerImpl.moveFragmentToExpectedState(FragmentManagerImpl.java:1238)\r\n        at androidx.fragment.app.FragmentManagerImpl.moveToState(FragmentManagerImpl.java:1303)\r\n        at androidx.fragment.app.BackStackRecord.executeOps(BackStackRecord.java:439)\r\n        at androidx.fragment.app.FragmentManagerImpl.executeOps(FragmentManagerImpl.java:2079)\r\n        at androidx.fragment.app.FragmentManagerImpl.executeOpsTogether(FragmentManagerImpl.java:1869)\r\n        at androidx.fragment.app.FragmentManagerImpl.removeRedundantOperationsAndExecute(FragmentManagerImpl.java:1824)\r\n        at androidx.fragment.app.FragmentManagerImpl.execSingleAction(FragmentManagerImpl.java:1696)\r\n        at androidx.fragment.app.BackStackRecord.commitNowAllowingStateLoss(BackStackRecord.java:299)\r\n        at androidx.fragment.app.FragmentPagerAdapter.finishUpdate(FragmentPagerAdapter.java:235)\r\n        at androidx.viewpager.widget.ViewPager.populate(ViewPager.java:1244)\r\n        at androidx.viewpager.widget.ViewPager.populate(ViewPager.java:1092)\r\n        at androidx.viewpager.widget.ViewPager.onMeasure(ViewPager.java:1622)\r\n        at android.view.View.measure(View.java:25466)\r\n        at android.view.ViewGroup.measureChildWithMargins(ViewGroup.java:6957)\r\n        at androidx.coordinatorlayout.widget.CoordinatorLayout.onMeasureChild(CoordinatorLayout.java:760)\r\n        at com.google.android.material.appbar.HeaderScrollingViewBehavior.onMeasureChild(HeaderScrollingViewBehavior.java:99)\r\n        at com.google.android.material.appbar.AppBarLayout$ScrollingViewBehavior.onMeasureChild(AppBarLayout.java:2003)\r\n        at androidx.coordinatorlayout.widget.CoordinatorLayout.onMeasure(CoordinatorLayout.java:831)\r\n        at android.view.View.measure(View.java:25466)\r\n        at android.view.ViewGroup.measureChildWithMargins(ViewGroup.java:6957)\r\n        at android.widget.FrameLayout.onMeasure(FrameLayout.java:194)\r\n        at androidx.appcompat.widget.ContentFrameLayout.onMeasure(ContentFrameLayout.java:146)\r\n        at android.view.View.measure(View.java:25466)\r\n        at android.view.ViewGroup.measureChildWithMargins(ViewGroup.java:6957)\r\n        at android.widget.LinearLayout.measureChildBeforeLayout(LinearLayout.java:1552)\r\n        at android.widget.LinearLayout.measureVertical(LinearLayout.java:842)\r\n        at android.widget.LinearLayout.onMeasure(LinearLayout.java:721)\r\n        at android.view.View.measure(View.java:25466)\r\n        at android.view.ViewGroup.measureChildWithMargins(ViewGroup.java:6957)\r\n        at android.widget.FrameLayout.onMeasure(FrameLayout.java:194)\r\n        at android.view.View.measure(View.java:25466)\r\n        at android.view.ViewGroup.measureChildWithMargins(ViewGroup.java:6957)\r\n        at android.widget.LinearLayout.measureChildBeforeLayout(LinearLayout.java:1552)\r\n        at android.widget.LinearLayout.measureVertical(LinearLayout.java:842)\r\n        at android.widget.LinearLayout.onMeasure(LinearLayout.java:721)\r\n        at android.view.View.measure(View.java:25466)\r\n        at android.view.ViewGroup.measureChildWithMargins(ViewGroup.java:6957)\r\n        at android.widget.FrameLayout.onMeasure(FrameLayout.java:194)\r\n        at com.android.internal.policy.DecorView.onMeasure(DecorView.java:747)\r\n        at android.view.View.measure(View.java:25466)\r\n        at android.view.ViewRootImpl.performMeasure(ViewRootImpl.java:3397)\r\n        at android.view.ViewRootImpl.measureHierarchy(ViewRootImpl.java:2228)\r\n        at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:2486)\r\n        at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:1952)\r\n        at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:8171)\r\nE/AndroidRuntime:     at android.view.Choreographer$CallbackRecord.run(Choreographer.java:972)\r\n        at android.view.Choreographer.doCallbacks(Choreographer.java:796)\r\n        at android.view.Choreographer.doFrame(Choreographer.java:731)\r\n        at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:957)\r\n        at android.os.Handler.handleCallback(Handler.java:938)\r\n        at android.os.Handler.dispatchMessage(Handler.java:99)\r\n        at android.os.Looper.loop(Looper.java:223)\r\n        at android.app.ActivityThread.main(ActivityThread.java:7656)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:592)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:947)\r\n     Caused by: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Failed to initialize TensorFlow context: subgraphs is null\r\n    Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\n    Node number 2 (FlexSize) failed to prepare.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegates(NativeInterpreterWrapper.java:367)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:85)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:63)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:277)\r\n        at com.example.se.Classify.onCreateView(Classify.java:138)\r\n        \t... 58 more\r\n\r\n```", "I see. I could confirm that this is a newly introduced bug in the Flex delegate. Thanks for reporting this bug! @karundawadi ", "@abattery Is there any other work around to this, I have a project due by tomorrow. If there are, what would be the appropriate workarounds?\r\n", "I think you can stick with the other TensorFlow versions, for example, TF 2.4 or TF 2.5 rc versions.", "@abattery  Thank you for the suggestion. I did try TF 2.4 but it says that :\r\n```\r\nerror: package org.tensorflow.lite.flex does not exist\r\nimport org.tensorflow.lite.flex.FlexDelegate;\r\n```", "I will fix this issue today, hopefully you can try tomorrow's nightly build to verify it. Sorry about that.", "@haozha111 @abattery  is there any other workaround to fix this issue? I did try TF 2.4 but I got the error saying that tensorflow.flex does not exist. I added various dependencies but I had no luck. \r\n\r\nIs there any way in which I can connect or use the previous versions of the nightly built in which the code was working?", "Could you install an earlier version tf-nightly and try?\r\n\r\nhttps://pypi.org/project/tf-nightly/#history", "@haozha111 @abattery Thank you for y'all help. I am using android and I believe it uses maven as repository management. I am trying to revert to an previous nightly built but unable to do so. I am using the following for my dependencies: \r\n```\r\ndependencies {\r\n    implementation 'androidx.appcompat:appcompat:1.2.0'\r\n    implementation 'com.google.android.material:material:1.3.0'\r\n    implementation 'androidx.constraintlayout:constraintlayout:2.0.4'\r\n    implementation 'com.google.firebase:firebase-auth:20.0.3'\r\n    implementation 'com.google.firebase:firebase-database:19.7.0'\r\n    implementation 'androidx.lifecycle:lifecycle-livedata-ktx:2.2.0'\r\n    implementation 'androidx.lifecycle:lifecycle-viewmodel-ktx:2.2.0'\r\n    implementation \"org.tensorflow:tensorflow-lite:2.4.0\"\r\n    implementation 'com.google.firebase:firebase-ml-model-interpreter:22.0.3'\r\n    implementation 'org.tensorflow:tensorflow-lite:2.6.0.dev20210427-nightly-SNAPSHOT'\r\n    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly-SNAPSHOT'\r\n    testImplementation 'junit:junit:4.+'\r\n    implementation 'com.github.PhilJay:MPAndroidChart:v3.1.0'\r\n    androidTestImplementation 'androidx.test.ext:junit:1.1.2'\r\n    androidTestImplementation 'androidx.test.espresso:espresso-core:3.3.0'\r\n}\r\n```\r\nThis is for my build.gradle file :\r\n```\r\n// Top-level build file where you can add configuration options common to all sub-projects/modules.\r\nbuildscript {\r\n    repositories {\r\n        google()\r\n        jcenter()\r\n    }\r\n    dependencies {\r\n        classpath 'com.android.tools.build:gradle:4.1.3'\r\n        classpath 'com.google.gms:google-services:4.3.5'\r\n\r\n        // NOTE: Do not place your application dependencies here; they belong\r\n        // in the individual module build.gradle files\r\n    }\r\n}\r\n\r\nallprojects {\r\n    repositories {\r\n        google()\r\n        mavenCentral ()  // mavenCentral() is for versioned stable release\r\n        jcenter()\r\n        maven {          // This maven with url is for snapshot release (nightly)\r\n            name 'ossrh-snapshot'\r\n            url 'http://oss.sonatype.org/content/repositories/snapshots'\r\n        }\r\n        maven{\r\n            url 'https://jitpack.io'\r\n        }\r\n    }\r\n}\r\n\r\ntask clean(type: Delete) {\r\n    delete rootProject.buildDir\r\n}\r\n```\r\n\r\nWhen I run this I am unable to import an previous version. What would be the ideal method to achieve the previous nightly build?", "@abattery could you provide some pointers here? ", "Everything works on the newer build, thank you. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48923\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48923\">No</a>\n"]}, {"number": 48922, "title": "When events are specified as root events by user, they should be treated as root events in trace view.", "body": "When events are created with _r by users like this https://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/keras/engine/training.py#L1098, [it indicates they are root events](https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/core/profiler/utils/xplane_schema.cc#L168).\r\nThus those user-specified root events should be processed as root events in the first place. \r\n\r\nHowever according to current processing order, [they are the last group of events to be processed](https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/core/profiler/utils/group_events.cc#L616). If any event fits into any of the preceding criteria of root events, they will be processed as root, [then the function will return](https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/core/profiler/utils/group_events.cc#L603), skipping the subsequent user-specified root events processing, which is undesirable.\r\n\r\nOne consequence is when events like [these](https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/engine/training.py#L1093) are not processed as root events, it breaks profiler's overview page as the profiler overview page relies on the root events and their step numbers to collect per-step time breakdown.", "comments": ["@jray319 @Terranlee would you mind taking a look? ", "_r is not only used by user defined root events, but also used by some legacy root events. See the IsLegacyRootEvent function in group_events.cc. Changing the order of root_events_ will affect existing users with legacy root events\r\n", "@Terranlee when _r is specified, should it override other root events? If so, why is current logic putting processing `_r` events at the last?  \r\n\r\nif processing legacy root events is a concern, then can we separate this part out from processing `root_events_` ?", "@jray319 @Terranlee let me know if you have any feedback on this change.", "Hi Cheng, Jiho and I are trying to understand your use case for event grouping.\r\nYou mentioned the user defined root events like [keras training loop](https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/engine/training.py#L1093)\r\n\r\nDoes this mean your job has both keras level loop and TF level loop? Right now the overview page is focused on the TF level loop and that is the only thing it displays, do you want to see the result at keras level loop?\r\n\r\nCan you share with us a profile so we can better understand your use case?\r\n\r\nThanks.", "@Terranlee thanks for your reply. What do you mean by TF level loop? Do you have some sample code for that?\r\n\r\nBased on my understanding,  in order to make overview work, trace and root events have to be defined like what keras(https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/engine/training.py#L1093) does, and this is also what doc says: https://www.tensorflow.org/guide/profiler#profiling_custom_training_loops.\r\n\r\nFor keras, it's not needed since keras already did this in the code as shown above,\r\nbut for custom training loop(https://www.tensorflow.org/guide/profiler#profiling_custom_training_loops) , users need to define the root event and trace themselves.\r\n\r\n", "@Terranlee @jray319 please let me know if you have any feedback for this.", "@burgerkingeater Can you please resolve conflicts? Thanks!", "@burgerkingeater Can you please resolve conflicts? Thanks!", "@burgerkingeater  Any update on this PR? Please. Thanks!", "@gbaned will do, thanks!", "@burgerkingeater Any update on this PR? Please. Thanks!", "@gbaned @Terranlee @jray319 updated the PR with latest master. Could you take a look when you have time? Thanks", "any feedback on this PR?  @Terranlee @jray319 ", "kindly ping again @Terranlee @jray319 ", "@Terranlee @jray319 mind taking a look when you have a chance?", "@gbaned If the assigned reviewers are not currently available, would you mind having others review this PR? Please let me know if there's anything needed from my side. Thanks.", "> @gbaned If the assigned reviewers are not currently available, would you mind having others review this PR? Please let me know if there's anything needed from my side. Thanks.\r\n\r\n@burgerkingeater Sorry for the late reply. I will assign other reviewer. Thank you.", "I pinged the team that is responsible for this code and they'll follow up here.", "Hi, sorry for the late reply to this issue.\r\nI tried this PR in our code base and some of the existing event grouping tests failed.\r\nThe issue happens when tf_loop_root_events_ and root_events both are not empty, the code we have right now will treat tf_loop_root_events_ as a special root event and ignores other root events.\r\nIn https://github.com/tensorflow/tensorflow/pull/48922#issuecomment-840933781 you mentioned you are using customized training loop(https://www.tensorflow.org/guide/profiler#profiling_custom_training_loops), in that case do you find tf_loop_root_events_ to be not empty?", "@Terranlee would you mind posting the details of the failed tests? \r\nIf both `tf_loop_root_events_` and `root_events` are not empty, in this PR, the logic becomes process root_events firstly and ignore `tf_loop_root_events_` regardless it's empty or not. In this case, do you think we should continue processing `tf_loop_root_events_` ?\r\n", "@Terranlee can we schedule a virtual meeting on this PR? It has been while and I really would like to merge it soon.", "I scheduled a meeting on Friday to talk about this PR.\r\nPlease change if the time does not work for you.\r\nThanks.", "@Terranlee thanks, can you send me the invite? My email is rencheng311@gmail.com", "@Terranlee thanks, saw the invite, the time LGTM. See you then!", "@Terranlee Can you please review this PR ? Thanks!", "I talked with burgerkingeater last time and it is not easy to make this change because it will break some google internal code.\r\nCan you send me an example of your profile (the XSpace file)? so I can look at why the events are not grouped as you expected?\r\nThanks.", "@burgerkingeater Can you please check above @Terranlee's comments and keep us posted ? Thanks!\r\n"]}, {"number": 48921, "title": "Temporarily delete TOSA concat legalizations for v0.22 spec update", "body": "- tosa.concat will be updated to take variadic input\r\n\r\nSigned-off-by: Suraj Sudhir <suraj.sudhir@arm.com>", "comments": ["@rsuderman and @stellaraccident , initiating a 2nd round of pushes for v0.22 changes. To be followed by a push to LLVM and then another to TF to restore and add new detailed legalizations. "]}, {"number": 48920, "title": "[CherryPick:r2.5]Run TensorFlow configure script in gpu_pip_on_cpu.sh", "body": "Our other build scripts run pip_new.sh which runs configure at some point. I'm not sure which critical flags this introduces, but without it, bazel still builds a package for Python 2.7.\n\nPiperOrigin-RevId: 370521377\nChange-Id: I36282f2487e4fb092bdd782fde7c8d8e0421a359", "comments": []}, {"number": 48919, "title": "cannot build TensorFLow with --config=dbg", "body": "when building opensource  TensorFlow with\r\n\r\nbazel build --config=dbg  --config=cuda --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package\r\n\r\n(for SM 7.0 only)\r\n\r\nThe build dies at link time with:\r\n\r\n`ERROR: /home/baarts/tensorflow-GH/tensorflow/python/BUILD:3373:24: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc @bazel-out/k8-dbg/bin/tensorflow/python/_pywrap_tensorflow_internal.so-2.params\r\nbazel-out/k8-dbg/bin/external/llvm-project/llvm/libScalar.a(AnnotationRemarks.pic.o):(.debug_aranges+0x6): relocation truncated to fit: R_X86_64_32 against `.debug_info'\r\nbazel-out/k8-dbg/bin/external/llvm-project/llvm/libScalar.a(BDCE.pic.o):(.debug_aranges+0x6): relocation truncated to fit: R_X86_64_32 against `.debug_info'\r\nbazel-out/k8-dbg/bin/external/llvm-project/llvm/libScalar.a(CallSiteSplitting.pic.o):(.debug_aranges+0x6): relocation truncated to fit: R_X86_64_32 against `.debug_info'\r\nbazel-out/k8-dbg/bin/external/llvm-project/llvm/libScalar.a(ConstantHoisting.pic.o):(.debug_aranges+0x6): relocation truncated to fit: R_X86_64_32 against `.debug_info'\r\nbazel-out/k8-dbg/bin/external/llvm-project/llvm/libScalar.a(ConstraintElimination.pic.o):(.debug_aranges+0x6): relocation truncated to fit: R_X86_64_32 against `.debug_info'\r\nbazel-out/k8-dbg/bin/external/llvm-project/llvm/libScalar.a(CorrelatedValuePropagation.pic.o):(.debug_aranges+0x6): relocation truncated to fit: R_X86_64_32 against `.debug_info'\r\nbazel-out/k8-dbg/bin/external/llvm-project/llvm/libScalar.a(DCE.pic.o):(.debug_aranges+0x6): relocation truncated to fit: R_X86_64_32 against `.debug_info'\r\nbazel-out/k8-dbg/bin/external/llvm-project/llvm/libScalar.a(DeadStoreElimination.pic.o):(.debug_aranges+0x6): relocation truncated to fit: R_X86_64_32 against `.debug_info'\r\nbazel-out/k8-dbg/bin/external/llvm-project/llvm/libScalar.a(DivRemPairs.pic.o):(.debug_aranges+0x6): relocation truncated to fit: R_X86_64_32 against `.debug_info'\r\nbazel-out/k8-dbg/bin/external/llvm-project/llvm/libScalar.a(EarlyCSE.pic.o):(.debug_aranges+0x6): relocation truncated to fit: R_X86_64_32 against `.debug_info'\r\nbazel-out/k8-dbg/bin/external/llvm-project/llvm/libScalar.a(FlattenCFGPass.pic.o):(.debug_aranges+0x6): additional relocation overflows omitted from the output\r\ncollect2: error: ld returned 1 exit status`\r\n\r\nAdding -mcmodel=large makes no difference, as the overflow is in a debug section.\r\nI tried -gdwarf64 which is not supported by gcc\r\n\r\nsome platform info:\r\n```\r\nroot@7fe23091cb5b:/opt/tensorflow# gcc --version\r\ngcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\nCopyright (C) 2019 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\nroot@7fe23091cb5b:/opt/tensorflow# uname -a\r\nLinux 7fe23091cb5b 4.15.0-72-generic #81-Ubuntu SMP Tue Nov 26 12:20:02 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux\r\n```", "comments": ["I am unable to reproduce by running:\r\n\r\n```bash\r\nyes '' | TF_NEED_CUDA=1  ./configure\r\nbazel build --config=dbg --config=cuda --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nI ran at commit a3d9f9be9ac2296615644061b40cefcee341dcc4 with Ubuntu 18.04. The compute capabilities I compiled with defaulted to 7.0.\r\n\r\nCan you give more information, like the commit and OS? Perhaps the issue was fixed at a later commit.\r\n\r\nYou can also try compiling only a subset of files with debugging symbols, which I highly recommend as it reduces build time and gdb startup time. This might help since the \"overflow\" in the error message might be fixed with less debugging symbols, although admittedly I have no idea how debugging symbols work at all. I use the following command to compile a subset of files:\r\n\r\n```\r\nbazel build --per_file_copt=+tensorflow.*,-tensorflow/compiler.*,-tensorflow/lite.*,-tensorflow/core/kernels.*@-O0,-g --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nStill, we should fix this even if the above command works for you.", "I'm at commit  9275e30a68f40c99b81a80bd55ec7d2567620cc7\r\n\r\nroot@7fe23091cb5b:/home/baarts/tensorflow-GH# cat /etc/issue\r\nUbuntu 20.04.1 LTS \\n \\l\r\n", "I can reproduce on Ubuntu 20.04. I am not familiar with how debugging symbols work, but based on what you said it seems they are exceeding 2 GB which is causing the issue? You mention `-gdwarf64`, which is in gcc 11.1 but not in gcc 9.3, which is what Ubuntu 20.04 uses.\r\n\r\n/CC @chsigg any ideas on what to do here? If it's impossible to support compiling all files with debug symbols, perhaps we should provide a config option and instructions on only building a subset of files with debugging symbols.", "@bas-aarts \r\nPlease take a look at above comment(reedwn) and let us know if you are still facing the same issue. Thanks!\r\n", "not sure what my action item is. @reedwm was able to reproduce, and asked @chsigg to chime in on an alternate way to build TF w/ debug symbols.\r\nYes, the issue is still there.", " @reedwm It could be really nice to have a practical solution for c++ contributors.", "Just a drive-by suggestion: could `-gsplit-dwarf` (aka. DWARF Fission) help here? \r\n\r\nBazel has a handy [--fision](https://docs.bazel.build/versions/4.0.0/user-manual.html#flag--fission), which according to the documentation should be on by default for `dbg` configs, although according to this [bug](https://github.com/bazelbuild/bazel/issues/11561) this is not quite the case, so maybe it's something worth looking into?\r\n\r\n\r\n\r\n", "See also https://github.com/tensorflow/tensorflow/issues/13295", "@bas-aarts \r\n\r\nCould you please confirm if the issue still persist.Thanks", "confirmed.\r\nIs there any reason to believe the problem would have been addressed?", "Other then @reedwm mentioned usability aspects that I think are quite important, or the feature request of precompiled release with debug symbols at https://github.com/tensorflow/tensorflow/issues/13295, it seems that someone else is compiling in debug mode https://github.com/tensorflow/tensorflow/issues/39521 or not?", "_All_ external developers have a need for this. It would be great if Google could document and support _the_ way for external developers to build TF with debug symbols.", "@sanjoy, I tried adding --copt=-O1 as well as --copt=-O2, which does not address the problem.\r\nAlso tried to use clang instead of nvcc/gcc, which results in:\r\n\r\n`ERROR: /root/.cache/bazel/_bazel_root/21773b1a37c97ed8ddda3e8be78ee764/external/libjpeg_turbo/BUILD.bazel:41:11: undeclared inclusion(s) in rule '@libjpeg_turbo//:jpeg':\r\nthis rule is missing dependency declarations for the following files included by 'libjpeg_turbo/jfdctflt.c':\r\n  '/usr/lib/clang/10.0.0/include/stddef.h'\r\n  '/usr/lib/clang/10.0.0/include/__stddef_max_align_t.h'\r\n  '/usr/lib/clang/10.0.0/include/stdarg.h'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 9.149s, Critical Path: 0.56s\r\nINFO: 60 processes: 53 internal, 7 local.\r\nFAILED: Build did NOT complete successfully`\r\n", "I think the best solution here is to only include debugging information for certain files when `--config=dbg` is specified. We should also document how to compile with debugging information, perhaps either on the [Build from source](https://www.tensorflow.org/install/source) guide or the [Contribute to TensorFlow](https://www.tensorflow.org/community/contribute)\r\n\r\nOn Ubuntu 18.04, `_pywrap_tensorflow_internal.so` still builds, but both the `.debug_info` and `.debug_str` sections are dangerously close to overflowing. When I run the following commands, the hexadecimal size of `.debug_info` in bytes is `0xfa5e36a5` and the size of `.debug_str` is `0xf940ec6a`, both which are close to the max value of `0xffffffff`.\r\n\r\n```\r\nbazel build --config=dbg --config=cuda //tensorflow/python:_pywrap_tensorflow_internal.so\r\nobjdump -h bazel-out/k8-dbg/bin/tensorflow/python/_pywrap_tensorflow_internal.so\r\n```\r\n\r\nI suggest that with `--config=dbg`, we only include debugging info in files under the `tensorflow` directory that are not also under `tensorflow/core/kernels`. This excludes TF dependencies and kernels, the former which takes up a lot of space in `.debug_info` and the latter which takes up a lot of space in `.debug_str` (including kernels causes a lot of long Eigen symbols to appear in `.debug_str`). Adding the following flags reduces the size of `.debug_info` to `5ae89488` and reduces the size of `.debug_str` to `474e9eda`\r\n\r\n```\r\n--copt=-g0 --per_file_copt=+tensorflow.*,-tensorflow/core/kernels.*@-g\r\n```\r\n\r\n@mihaimaruseac, @bas-aarts does adding the flags above to `--config=dbg` sound like a good plan? Not including debugging info in kernels is a shame, but they take up too much space in `.debug_str`. If one wants to have debugging symbols in a specific kernel, they can still do so with the flags `--config=dbg --per_file_copt=+tensorflow/core/kernels/my_favorite_kernel.*@-g`.\r\n", "This sounds good to me, we can go with this path for now.", "It could be really nice to have this already available with the standard compile option in a readonly bazel remote cache (GCS?).  \r\nThen we only dbg compile locally what we need as suggested with `--config=dbg --per_file_copt=+tensorflow/core/kernels/my_favorite_kernel.*@-g`", "@reedwm, it would be great if the per-file compilation would be part of the 'dbg' config in .bazelrc file.\r\nThat wy \"--config=dbg\" would be _the_ way to build a debug build. This could then easily be documented as well, such taht all external developers can benefit from this.\r\nFor what it's worths, I disabled kernel debugging with https://github.com/tensorflow/tensorflow/pull/34109 a long time ago. Debug kernels take many more resources (registers and stack), that will not allow them to load or run (lots of kernels fail).\r\n", "Yeah, I'll add it to the `.bazelrc` file. Then I'll document it (haven't decided where yet). Also thank you for disabling this in CUDA kernels, as I don't know if I would have been able to figure out what is wrong there. I plan on also disabling create debug info for TF kernels (in particular, the debugging info for CPU code since you already disabled it for device code).\r\n\r\n> It could be really nice to have this already available with the standard compile option in a readonly bazel remote cache (GCS?).\r\n\r\nI agree that would be really nice and would make development easier. But it is orthogonal to this issue. ", "Will the per file option be in conjunction  with '-c opt'? That would result in optimized debug, which it often a frustrating exercise. We should add -O0 -g to improve upon that", "I plan on adding the following to `.bazelrc`:\r\n\r\n```\r\nbuild:dbg --per_file_copt=+.*,-tensorflow.*@-g0\r\nbuild:dbg --per_file_copt=+tensorflow/core/kernels.*@-g0\r\n```\r\n\r\n`.bazelrc` already has the line `build:dbg -c dbg`, so I think adding `-O0 -g` is unnecessary.", "with the above diff, not all is well yet when compiling with --config=dbg . For mark_for_compilation_pass.cc, I see the following command line:\r\n\r\n`  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/compiler/jit/_objs/compilation_passes/mark_for_compilation_pass.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/compiler/jit/_objs/compilation_passes/mark_for_compilation_pass.pic.o' -DLLVM_ENABLE_STATS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -DLLVM_BUILD_GLOBAL_ISEL -DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL -DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DOPENSSL_IS_BORINGSSL -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -iquote . -iquote bazel-out/host/bin -iquote external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/gif -iquote bazel-out/host/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/host/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/host/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/host/bin/external/zlib -iquote external/double_conversion -iquote bazel-out/host/bin/external/double_conversion -iquote external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -iquote external/local_config_rocm -iquote bazel-out/host/bin/external/local_config_rocm -iquote external/local_config_tensorrt -iquote bazel-out/host/bin/external/local_config_tensorrt -iquote external/snappy -iquote bazel-out/host/bin/external/snappy -iquote external/curl -iquote bazel-out/host/bin/external/curl -iquote external/boringssl -iquote bazel-out/host/bin/external/boringssl -iquote external/jsoncpp_git -iquote bazel-out/host/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/host/bin/external/aws -iquote external/aws-c-common -iquote bazel-out/host/bin/external/aws-c-common -iquote external/aws-c-event-stream -iquote bazel-out/host/bin/external/aws-c-event-stream -iquote external/aws-checksums -iquote bazel-out/host/bin/external/aws-checksums -iquote external/llvm-project -iquote bazel-out/host/bin/external/llvm-project -iquote external/mkl_dnn_v1 -iquote bazel-out/host/bin/external/mkl_dnn_v1 -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/host/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/BuiltinDialectIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/BuiltinLocationAttributesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/BuiltinOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/BuiltinTypesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/CallOpInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/CastOpInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/InferTypeOpInterfaceIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/OpAsmInterfaceIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/RegionKindInterfaceIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SideEffectInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SymbolInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ControlFlowInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/DerivedAttributeOpInterfaceIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LoopLikeInterfaceIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ParserTokenKinds -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/StandardOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/TensorBaseIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/TensorOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/VectorInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ViewLikeInterfaceIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/AffineMemoryOpInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/AffineOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/MemRefBaseIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/MemRefOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/CopyOpInterfaceIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LinalgInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LinalgStructuredOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LinalgOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LinalgSparseOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SCFIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SCFPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ComplexBaseIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ComplexOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/PDLOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/PDLTypesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/PDLInterpOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ConversionPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/TransformsPassIncGen -Ibazel-out/host/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/canonicalize_inc_gen -Ibazel-out/host/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/chlo_ops_inc_gen -Ibazel-out/host/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/hlo_ops_base_inc_gen -Ibazel-out/host/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/hlo_ops_inc_gen -Ibazel-out/host/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/hlo_ops_pattern_gen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/MLIRShapeCanonicalizationIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ShapeOpsIncGen -Ibazel-out/host/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen -Ibazel-out/host/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_structs_inc_gen -Ibazel-out/host/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/chlo_legalize_to_hlo_inc_gen -Ibazel-out/host/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_gpu_ops_enums_inc_gen -Ibazel-out/host/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_gpu_ops_inc_gen -Ibazel-out/host/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_gpu_ops_structs_inc_gen -Ibazel-out/host/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/MhloPassIncGen -isystem external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem third_party/eigen3/mkl_include -isystem bazel-out/host/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/host/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/host/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/host/bin/external/zlib -isystem external/double_conversion -isystem bazel-out/host/bin/external/double_conversion -isystem external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_rocm/rocm -isystem bazel-out/host/bin/external/local_config_rocm/rocm -isystem external/local_config_rocm/rocm/rocm/include -isystem bazel-out/host/bin/external/local_config_rocm/rocm/rocm/include -isystem external/local_config_rocm/rocm/rocm/include/rocrand -isystem bazel-out/host/bin/external/local_config_rocm/rocm/rocm/include/rocrand -isystem external/local_config_rocm/rocm/rocm/include/roctracer -isystem bazel-out/host/bin/external/local_config_rocm/rocm/rocm/include/roctracer -isystem external/curl/include -isystem bazel-out/host/bin/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/host/bin/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/host/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-s3/include -isystem external/aws/aws-cpp-sdk-transfer/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-transfer/include -isystem external/aws-c-common/include -isystem bazel-out/host/bin/external/aws-c-common/include -isystem external/aws-c-event-stream/include -isystem bazel-out/host/bin/external/aws-c-event-stream/include -isystem external/aws-checksums/include -isystem bazel-out/host/bin/external/aws-checksums/include -isystem external/llvm-project/llvm/include -isystem bazel-out/host/bin/external/llvm-project/llvm/include -isystem external/mkl_dnn_v1/include -isystem bazel-out/host/bin/external/mkl_dnn_v1/include -isystem external/mkl_dnn_v1/src -isystem bazel-out/host/bin/external/mkl_dnn_v1/src -isystem external/mkl_dnn_v1/src/common -isystem bazel-out/host/bin/external/mkl_dnn_v1/src/common -isystem external/mkl_dnn_v1/src/common/ittnotify -isystem bazel-out/host/bin/external/mkl_dnn_v1/src/common/ittnotify -isystem external/mkl_dnn_v1/src/cpu -isystem bazel-out/host/bin/external/mkl_dnn_v1/src/cpu -isystem external/mkl_dnn_v1/src/cpu/gemm -isystem bazel-out/host/bin/external/mkl_dnn_v1/src/cpu/gemm -isystem external/mkl_dnn_v1/src/cpu/x64/xbyak -isystem bazel-out/host/bin/external/mkl_dnn_v1/src/cpu/x64/xbyak -isystem external/llvm-project/mlir/include -isystem bazel-out/host/bin/external/llvm-project/mlir/include -isystem tensorflow/compiler/mlir/tensorflow/include -isystem bazel-out/host/bin/tensorflow/compiler/mlir/tensorflow/include -isystem tensorflow/compiler/mlir/hlo/include -isystem bazel-out/host/bin/tensorflow/compiler/mlir/hlo/include -isystem tensorflow/compiler/mlir/xla/include -isystem bazel-out/host/bin/tensorflow/compiler/mlir/xla/include -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 -w -Wno-sign-compare -g0 '-std=c++14' -c tensorflow/compiler/jit/mark_for_compilation_pass.cc -o bazel-out/host/bin/tensorflow/compiler/jit/_objs/compilation_passes/mark_for_compilation_pass.pic.o)\r\n`\r\nnotice the -g0 -O2 -g0 -g0. This file is compiled with -O2, and has no debug information \r\n", "For me, it is compiled with debug info without optimizations. After adding the two lines in my previous post, when I run:\r\n\r\n```bash\r\nbazel build -s  --config=dbg --config=cuda //tensorflow/compiler/jit:compilation_passes\r\n```\r\n\r\nI get the command line:\r\n\r\n```\r\nexternal/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-dbg/bin/tensorflow/compiler/jit/_objs/compilation_passes/mark_for_compilation_pass.pic.d '-frandom\r\n-seed=bazel-out/k8-dbg/bin/tensorflow/compiler/jit/_objs/compilation_passes/mark_for_compilation_pass.pic.o' -DLLVM_ENABLE_STATS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -DLLV\r\nM_BUILD_GLOBAL_ISEL -DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL -DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY -DCURL_STATICLIB -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -\r\niquote . -iquote bazel-out/k8-dbg/bin -iquote external/com_google_absl -iquote bazel-out/k8-dbg/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/k8-dbg/bin/external/nsync -iquote exte\r\nrnal/eigen_archive -iquote bazel-out/k8-dbg/bin/external/eigen_archive -iquote external/gif -iquote bazel-out/k8-dbg/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/k8-dbg/bin/external/l\r\nibjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/k8-dbg/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-dbg/bin/external/com_googlesourc\r\ne_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-dbg/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-dbg/bin/external/fft2d -iquote external/highwayhash -iquote\r\nbazel-out/k8-dbg/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/k8-dbg/bin/external/zlib -iquote external/double_conversion -iquote bazel-out/k8-dbg/bin/external/double_conversion -iquot\r\ne external/local_config_cuda -iquote bazel-out/k8-dbg/bin/external/local_config_cuda -iquote external/local_config_rocm -iquote bazel-out/k8-dbg/bin/external/local_config_rocm -iquote external/local_confi\r\ng_tensorrt -iquote bazel-out/k8-dbg/bin/external/local_config_tensorrt -iquote external/snappy -iquote bazel-out/k8-dbg/bin/external/snappy -iquote external/curl -iquote bazel-out/k8-dbg/bin/external/curl\r\n -iquote external/boringssl -iquote bazel-out/k8-dbg/bin/external/boringssl -iquote external/jsoncpp_git -iquote bazel-out/k8-dbg/bin/external/jsoncpp_git -iquote external/llvm-project -iquote bazel-out/k\r\n8-dbg/bin/external/llvm-project -iquote external/mkl_dnn_v1 -iquote bazel-out/k8-dbg/bin/external/mkl_dnn_v1 -Ibazel-out/k8-dbg/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -\r\nIbazel-out/k8-dbg/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -Ibazel-out/k8-dbg/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header -Ibazel-out/k8-dbg/bin/externa\r\nl/llvm-project/mlir/_virtual_includes/BuiltinAttributesIncGen -Ibazel-out/k8-dbg/bin/external/llvm-project/mlir/_virtual_includes/BuiltinDialectIncGen -Ibazel-out/k8-dbg/bin/external/llvm-project/mlir/_vi\r\nrtual_includes/BuiltinLocationAttributesIncGen -Ibazel-out/k8-dbg/bin/external/llvm-project/mlir/_virtual_includes/BuiltinOpsIncGen -Ibazel-out/k8-dbg/bin/external/llvm-project/mlir/_virtual_includes/Buil\r\ntinTypesIncGen -Ibazel-out/k8-dbg/bin/external/llvm-project/mlir/_virtual_includes/CallOpInterfacesIncGen -Ibazel-out/k8-dbg/bin/external/llvm-project/mlir/_virtual_includes/CastOpInterfacesIncGen -Ibazel\r\n-out/k8-dbg/bin/external/llvm-project/mlir/_virtual_includes/InferTypeOpInterfaceIncGen -Ibazel-out/k8-dbg/bin/external/llvm-project/mlir/_virtual_includes/OpAsmInterfaceIncGen -Ibazel-out/k8-dbg/bin/exte\r\nrnal/llvm-project/mlir/_virtual_includes/RegionKindInterfaceIncGen -Ibazel-out/k8-dbg/bin/external/llvm-project/mlir/_virtual_includes/SideEffectInterfacesIncGen -Ibazel-out/k8-dbg/bin/external/llvm-proje\r\nct/mlir/_virtual_includes/SymbolInterfacesIncGen -Ibazel-out/k8-dbg/bin/external/llvm-project/mlir/_virtual_includes/TensorEncodingIncGen -Ibazel-out/k8-dbg/bin/external/llvm-project/mlir/_virtual_include\r\ns/ParserTokenKinds -Ibazel-out/k8-dbg/bin/external/llvm-project/mlir/_virtual_includes/ControlFlowInterfacesIncGen -Ibazel-out/k8-dbg/bin/external/llvm-project/mlir/_virtual_includes/DerivedAttributeOpInt\r\nerfaceIncGen -Ibazel-out/k8-dbg/bin/external/llvm-project/mlir/_virtual_includes/LoopLikeInterfaceIncGen -Ibazel-out/k8-dbg/bin/external/llvm-project/mlir/_virtual_includes/CopyOpInterfaceIncGen -Ibazel-o\r\nut/k8-dbg/bin/external/llvm-project/mlir/_virtual_includes/MemRefBaseIncGen -Ibazel-out/k8-dbg/bin/external/llvm-project/mlir/_virtual_includes/MemRefOpsIncGen -Ibazel-out/k8-dbg/bin/external/llvm-project\r\n/mlir/_virtual_includes/StandardOpsIncGen -Ibazel-out/k8-dbg/bin/external/llvm-project/mlir/_virtual_includes/TensorOpsIncGen -Ibazel-out/k8-dbg/bin/external/llvm-project/mlir/_virtual_includes/VectorInte\r\nrfacesIncGen -Ibazel-out/k8-dbg/bin/external/llvm-project/mlir/_virtual_includes/ViewLikeInterfaceIncGen -Ibazel-out/k8-dbg/bin/external/llvm-project/mlir/_virtual_includes/AffineMemoryOpInterfacesIncGen\r\n-Ibazel-out/k8-dbg/bin/external/llvm-project/mlir/_virtual_includes/AffineOpsIncGen -Ibazel-out/k8-dbg/bin/external/llvm-project/mlir/_virtual_includes/LinalgInterfacesIncGen -Ibazel-out/k8-dbg/bin/extern\r\nal/llvm-project/mlir/_virtual_includes/LinalgStructuredOpsIncGen -Ibazel-out/k8-dbg/bin/external/llvm-project/mlir/_virtual_includes/LinalgOpsIncGen -Ibazel-out/k8-dbg/bin/external/llvm-project/mlir/_virt\r\nual_includes/SCFIncGen -Ibazel-out/k8-dbg/bin/external/llvm-project/mlir/_virtual_includes/SCFPassIncGen -Ibazel-out/k8-dbg/bin/external/llvm-project/mlir/_virtual_includes/ComplexBaseIncGen -Ibazel-out/k\r\n8-dbg/bin/external/llvm-project/mlir/_virtual_includes/ComplexOpsIncGen -Ibazel-out/k8-dbg/bin/external/llvm-project/mlir/_virtual_includes/PDLOpsIncGen -Ibazel-out/k8-dbg/bin/external/llvm-project/mlir/_\r\nvirtual_includes/PDLTypesIncGen -Ibazel-out/k8-dbg/bin/external/llvm-project/mlir/_virtual_includes/PDLInterpOpsIncGen -Ibazel-out/k8-dbg/bin/external/llvm-project/mlir/_virtual_includes/ConversionPassInc\r\nGen -Ibazel-out/k8-dbg/bin/external/llvm-project/mlir/_virtual_includes/TransformsPassIncGen -Ibazel-out/k8-dbg/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/canonicalize_inc_gen -Ibazel-out/k8-dbg/b\r\nin/tensorflow/compiler/mlir/hlo/_virtual_includes/chlo_ops_inc_gen -Ibazel-out/k8-dbg/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/hlo_ops_base_inc_gen -Ibazel-out/k8-dbg/bin/tensorflow/compiler/mli\r\nr/hlo/_virtual_includes/hlo_ops_inc_gen -Ibazel-out/k8-dbg/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/hlo_ops_pattern_gen -Ibazel-out/k8-dbg/bin/external/llvm-project/mlir/_virtual_includes/MLIRSh\r\napeCanonicalizationIncGen -Ibazel-out/k8-dbg/bin/external/llvm-project/mlir/_virtual_includes/ShapeOpsIncGen -Ibazel-out/k8-dbg/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen -Ibazel-\r\nout/k8-dbg/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_structs_inc_gen -Ibazel-out/k8-dbg/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/chlo_legalize_to_hlo_inc_gen -Ibazel-out/k8-dbg\r\n/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_gpu_ops_enums_inc_gen -Ibazel-out/k8-dbg/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_gpu_ops_inc_gen -Ibazel-out/k8-dbg/bin/tensorflow/\r\ncompiler/mlir/hlo/_virtual_includes/lhlo_gpu_ops_structs_inc_gen -Ibazel-out/k8-dbg/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/MhloPassIncGen -isystem external/nsync/public -isystem bazel-out/k8-d\r\nbg/bin/external/nsync/public -isystem third_party/eigen3/mkl_include -isystem bazel-out/k8-dbg/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/k8-dbg/bin/external/eig\r\nen_archive -isystem external/gif -isystem bazel-out/k8-dbg/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/k8-dbg/bin/external/com_google_protobuf/src -isystem external/farmh\r\nash_archive/src -isystem bazel-out/k8-dbg/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/k8-dbg/bin/external/zlib -isystem external/double_conversion -isystem bazel-out/k8-dbg\r\n/bin/external/double_conversion -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-dbg/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-\r\nout/k8-dbg/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_rocm/rocm -isystem bazel-out/k8-dbg/bin/external/local_config_rocm/rocm -isystem external/local_config_rocm/rocm/\r\nrocm/include -isystem bazel-out/k8-dbg/bin/external/local_config_rocm/rocm/rocm/include -isystem external/local_config_rocm/rocm/rocm/include/rocrand -isystem bazel-out/k8-dbg/bin/external/local_config_ro\r\ncm/rocm/rocm/include/rocrand -isystem external/local_config_rocm/rocm/rocm/include/roctracer -isystem bazel-out/k8-dbg/bin/external/local_config_rocm/rocm/rocm/include/roctracer -isystem external/curl/inc\r\nlude -isystem bazel-out/k8-dbg/bin/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/k8-dbg/bin/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem\r\n bazel-out/k8-dbg/bin/external/jsoncpp_git/include -isystem external/llvm-project/llvm/include -isystem bazel-out/k8-dbg/bin/external/llvm-project/llvm/include -isystem external/mkl_dnn_v1/include -isyste\r\nm bazel-out/k8-dbg/bin/external/mkl_dnn_v1/include -isystem external/mkl_dnn_v1/src -isystem bazel-out/k8-dbg/bin/external/mkl_dnn_v1/src -isystem external/mkl_dnn_v1/src/common -isystem bazel-out/k8-dbg/\r\nbin/external/mkl_dnn_v1/src/common -isystem external/mkl_dnn_v1/src/common/ittnotify -isystem bazel-out/k8-dbg/bin/external/mkl_dnn_v1/src/common/ittnotify -isystem external/mkl_dnn_v1/src/cpu -isystem ba\r\nzel-out/k8-dbg/bin/external/mkl_dnn_v1/src/cpu -isystem external/mkl_dnn_v1/src/cpu/gemm -isystem bazel-out/k8-dbg/bin/external/mkl_dnn_v1/src/cpu/gemm -isystem external/mkl_dnn_v1/src/cpu/x64/xbyak -isys\r\ntem bazel-out/k8-dbg/bin/external/mkl_dnn_v1/src/cpu/x64/xbyak -isystem external/llvm-project/mlir/include -isystem bazel-out/k8-dbg/bin/external/llvm-project/mlir/include -isystem tensorflow/compiler/mli\r\nr/tensorflow/include -isystem bazel-out/k8-dbg/bin/tensorflow/compiler/mlir/tensorflow/include -isystem tensorflow/compiler/mlir/hlo/include -isystem bazel-out/k8-dbg/bin/tensorflow/compiler/mlir/hlo/incl\r\nude -isystem tensorflow/compiler/mlir/xla/include -isystem bazel-out/k8-dbg/bin/tensorflow/compiler/mlir/xla/include -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__\r\nTIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -g -w -DAUTOLOAD_DYNAMIC_KERNELS -DDEB\r\nUG_BUILD '-std=c++14' -DTF_LITE_DISABLE_X86_NEON -c tensorflow/compiler/jit/mark_for_compilation_pass.cc -o bazel-out/k8-dbg/bin/tensorflow/compiler/jit/_objs/compilation_passes/mark_for_compilation_pass.\r\npic.o\r\n```\r\n\r\nThere is a `-g` at the end, and no `-O2`. What bazel command are you using?", "My commandline shows up when building //tensorflow/tools/pip_package:build_pip_package\r\n`bazel build -s --config=dbg -j 24 --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package`\r\nWhen I build //tensorflow/compiler/jit:compilation_passes, it looks correct (although I still want to verify that -g == -O0 -g)", "I submitted d3bbd2ffc930220455e12a9269057f0e200147f9, which makes the changes to `.bazelrc`. Can you try again after that commit and check if you can set breakpoints in `mark_for_compilation_pass.cc`, or whatever other file you want to debug (except dependencies and kernels)? I was able to debug with the changes in that commit. I used the command:\r\n\r\n```\r\nbazel build  --config=dbg --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nYour command is slightly different, but it should still work.", "trying now", "Actually the build might be failing right now for an unrelated reason. So trying at d3bbd2ffc930220455e12a9269057f0e200147f9 itself, with or without debugging info, might not work.", "Building is working again, as of bf36815372b718a7e238dade77f29af94ce26cdf. I ran the command you tried:\r\n\r\n```\r\nbazel build -s --config=dbg -j 24 --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nThe subcommands for mark_for_compilation_pass.cc outputted by bazel are [here](https://gist.githubusercontent.com/reedwm/80555fac07b3984c0ffa0c1e4b164e7f/raw/2496c3c5b388c42c9f2cd3ebfa5a6f9a57a10199/bazel%2520gcc%2520commands). TensorFlow actually builds `mark_for_compiliation_pass.cc` twice: once with `-O2 -g` for the host platform and once with just `-g` for the target platform, the latter which has no optimizations enabled. In the subcommands, you can see the only the command with the string \"for host\" has `-O2`. I believe the pip package and any bazel tests/binaries you include should include the version of mark_for_compilation_pass with just `-g`, and so will be debuggable. I'm not familiar with bazel and so am not entirely sure why its being built for the host platform (maybe to run various genfile targets), but this shouldn't cause any issues debugging.", "I just came to the same conclusion. Many files are compiled twice. What is 'host' used for? Is it required?\r\nSeems like a lot of compilations if it's not strictly necessary.", "Debugging works fine. Fix looks good to me.\r\nWe should leave it open though until after it has been documented as well.\r\nThanks @reedwm ", "> I just came to the same conclusion. Many files are compiled twice. What is 'host' used for? Is it required?\r\n\r\nThe \"Build configurations and cross-compilation\" section of [this page](https://docs.bazel.build/versions/main/guide.html#build-configurations-and-cross-compilation) has details on the \"host\" vs \"target\" configuration (I used the word \"platform\" before but I think the right word is \"configuration\"). There is probably a genrule somewhere that uses `mark_for_compiliation_pass` somewhere along with other files, which causes it to be built for the host configuration in addition to the target configuration.", "You can use `--nodistinct_host_configuration` to avoid building these files twice, which should be fine for your local development.  Maybe this needs to be in `.bazelrc` as well (if putting this in bazelrc works as expected).", "fwiw, building _all_ of tensorflow/... (ie including tensorflow/core/kernels) with debug, still works as well.", "> fwiw, building all of tensorflow/... (ie including tensorflow/core/kernels) with debug, still works as well.\r\n\r\nI thought this is precisely what wasn't working right?  Or did I misunderstand the issue?", "> > fwiw, building all of tensorflow/... (ie including tensorflow/core/kernels) with debug, still works as well.\r\n> \r\n> I thought this is precisely what wasn't working right? Or did I misunderstand the issue?\r\n\r\nThis is still not _all_ , just the tensorflow directory. all external bits are still optimized", "The `.debug_str` section was very close to 4 GiB, so I removed the kernels to reduce it, although I'm not sure if the section must be below 4 GiB or not. Also, removing debug info in the kernels makes gdb start up considerably faster.", "> > I just came to the same conclusion. Many files are compiled twice. What is 'host' used for? Is it required?\r\n> \r\n> The \"Build configurations and cross-compilation\" section of [this page](https://docs.bazel.build/versions/main/guide.html#build-configurations-and-cross-compilation) has details on the \"host\" vs \"target\" configuration (I used the word \"platform\" before but I think the right word is \"configuration\"). There is probably a genrule somewhere that uses `mark_for_compiliation_pass` somewhere along with other files, which causes it to be built for the host configuration in addition to the target configuration.\r\n\r\nso I added --distinct_host_configuration=false to the bazel build just to see. Build was so much faster, as only the build is done. So far, seems like stuff is working.", "> so I added --distinct_host_configuration=false to the bazel build just to see. Build was so much faster, as only the build is done. So far, seems like stuff is working.\n\nIs this related to this debug build or it is going to impact also regular build?", "> > so I added --distinct_host_configuration=false to the bazel build just to see. Build was so much faster, as only the build is done. So far, seems like stuff is working.\r\n> \r\n> Is this related to this debug build or it is going to impact also regular build?\r\n\r\nWhile i have not verified, this should not be debug related", "> > > so I added --distinct_host_configuration=false to the bazel build just to see. Build was so much faster, as only the build is done. So far, seems like stuff is working.\n> > \n> > Is this related to this debug build or it is going to impact also regular build?\n> \n> While i have not verified, this should not be debug related\n\nThis could be interesting /cc @angerson @perfinion ", "@reedwm, just following up regarding the documentation part of this bug. Any updates? ", "No update yet, will try to do this this week.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48919\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48919\">No</a>\n", "This is fixed and [documented](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#debug-builds). But for some reason, using `--config=dbg` makes gdb take a lot longer to startup and start running the program compared to simply passing `-O0 -g` to the same set of files with the following:\r\n\r\n```\r\n--per_file_copt=+tensorflow.*,-tensorflow/core/kernels.*@-O0,-g\r\n```\r\n\r\nI would have thought passing `-c dbg` to bazel would simply make bazel pass `-O0 -g` to gcc, but perhaps it is doing something more that causes gdb to take longer to startup."]}, {"number": 48918, "title": "Update and align numpy versions in TensorFlow", "body": "This PR is just to explore CI error when we align and upgrade numpy in TF.\r\n\r\nWhen finalized it will fix https://github.com/tensorflow/tensorflow/issues/47691", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/48918\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n See visual diffs & provide feedback on Jupyter Notebooks. \n\n---\n\n <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>", "Many of the Docker images with python < 3.7 will fail cause we don't have numpy v1.20 for older python version.\r\nPython upgrade for the devel and devel derivated images is at https://github.com/tensorflow/tensorflow/pull/48371.", "I don't know what the TensorFlow team is planning, but according to [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html), \"all projects across the Scientific Python ecosystem\" were supposed to drop Python 3.6 last June.", "This is wrong. You don't have to specify a **minimum** Numpy 1.20. Just don't require a **maximum** numpy, by not relying on features deprecated long ago and finally removed in 1.19 or 1.20.", "After the next release, we can drop py3.6 and then revisit this.", "@bnavigator It is not the logic of this PR. This is just a CI exploration to expose public logs for all the users that are pushing for 1.20 in the ticket.  I am forcing 1.20.x to see the effect on our VMs, Community VM, Docker images, and TF tests on all the platforms.\r\n\r\nMore in general, but probably @mihaimaruseac can comment better then me, I suppose that currently as we are under release, we don't have the bandwidth to control all the python env matrix for all the numpy versions and probably it will be easier to evaluate python 3.6 removal and python 2.7 minimal version to work with an updated version of numpy. \r\nThen probably we could finetune what will be the minimal numpy version that we want to support considering also NEP29.", "I disagree. Comments like https://github.com/tensorflow/tensorflow/pull/48918#issuecomment-833004032 and https://github.com/tensorflow/tensorflow/pull/48918#issuecomment-833052500 show that this diverts energy into the wrong direction. Supporting NumPy 1.20 does have nothing to do with dropping 1.19 or python 3.6. Python 3.6 along with a compatible older NumPy (and SciPy and many more packages) are still the mainline on some major LTS Linux distros. Just relaxing the upper bounds on NumPy should suffice to let any reasonable resolver take the newest available version.\r\n\r\nAnd why would you want to drop python 3.6 support but still talk about python 2.7? That one is *really* EOL.", "`2.7` it was just a typo as it is EOL it was `3.7` of course. \r\n\r\nIf you think that you have a better solution for your scope and you have the best min/max numpy ranges, feel free to open a new Pull request we we will review that without any problems.\r\n\r\nHere I just want to verify CI VM and CI results with 1.20 and aligned numpy versions (they diverted in many places) as in the PR description:\r\n> This PR is just to explore CI error when we align and upgrade numpy in TF.\r\n\r\nThen I want to point out that for many of us, activities like this are only voluntary activities as the TF internal teams commit directly from the internal system and generally they don't use PRs.\r\n\r\nThe code is available for everyone and I believe it is appropriate to have a more collaborative approach when we comment on tickets as it is defined in our [community code of conduct](https://github.com/tensorflow/tensorflow/blob/master/CODE_OF_CONDUCT.md).\r\n", "@bnavigator: Given how deep TF integrates with numpy, having a wide range on the dependency results in broken CI in the current build system. Easiest fix at the numpy 1.20 release time has been to force TF to use older numpy. @bhack is currently trying to unblock this / see what are the remaining breakages.", "It's not because you still want to support python3.6 that your CI must run everything on 3.6 and then you are not able to instal NumPy's latest.\r\n\r\nHave a look at our CI for SciPy. We test a bunch of configurations and test the develop version of NumPy only in one particular workflow. This way we can ensure forward compatibility while still being able to test on older systems on other workflows. You could be fully compatible with NEP29 if wanted... Now you're just breaking everyones workflows.", "Most of Google's CI is still on Python 3.6. Since there's no numpy 1.20 for that version, we cannot use it.\r\n\r\nFurthermore, internal numpy at Google is much lower than that. While people are doing heroic efforts to update, as long as there are incompatibilities, we won't be able to upgrade", "@mihaimaruseac Let me know if it could be still useful to run this in TF CI in the future as a canary or we could close this now.", "I'd say let's keep this open with the understanding that it will take a while before merging and might need some rebasing from time to time. If PR author wants to close then we can close and use a different PR as canary", "@mihaimaruseac The PR author here is me :smile:", "Oh, I didn't check :) Your choice then", "I'll just note that it's also possible to test against the development version of upstream dependencies like NumPy. This is a pretty common practice for core projects in the scientific Python ecosystem and makes a huge difference, often identifying upstream bugs even before there's a release.", "> Most of Google's CI is still on Python 3.6. Since there's no numpy 1.20 for that version, we cannot use it.\n\nOC you can as you could have one workflow dedicated to forward compatibility check. You would test latest Python, NumPy, etc. Nothing complicated here, just willingness to do so... Again, cf SciPy's CI for example.", "> > Most of Google's CI is still on Python 3.6. Since there's no numpy 1.20 for that version, we cannot use it.\r\n> \r\n> OC you can as you could have one workflow dedicated to forward compatibility check. You would test latest Python, NumPy, etc. Nothing complicated here, just willingness to do so... Again, cf SciPy's CI for example.\r\n\r\nIf you are interested to contribute to the general discussion I've started a new thread in our forum: https://discuss.tensorflow.org/t/extend-the-collaboration-on-the-ci/1917", "> \r\n> If you are interested to contribute to the general discussion I've started a new thread in our forum:\r\n\r\nThanks for the initiative. I would comment there, but somehow the device 2FA for the google login does not succeed at them moment. :man_shrugging: (Plus, I actually don't want to use my personal gmail address for a public forum)\r\n\r\n> > > Most of Google's CI is still on Python 3.6. Since there's no numpy 1.20 for that version, we cannot use it.\r\n\r\nI must say I am a bit confused. In the release notes about TF 2.5.0 the first bullet point is that you now support Python 3.9. How do you do that if you don't have a CI for it?", "> Thanks for the initiative. I would comment there, but somehow the device 2FA for the google login does not succeed at them moment.\u00a0\ud83e\udd37\u200d\u2642\ufe0f\u00a0(Plus, I actually don't want to use my personal gmail address for a public forum)\n\nIf you need support with 2FA issue you could send a message to  forum-help@tensorflow.org \nAlso your gmail address will not be disclosed.\n\n/cc @theadactyl", "> I'll just note that it's also possible to test against the development version of upstream dependencies like NumPy. This is a pretty common practice for core projects in the scientific Python ecosystem and makes a huge difference, often identifying upstream bugs even before there's a release.\r\n\r\nYes, it is possible, but very costly. Our support matrix is already not manageable, unfortunately.", "\r\n> I must say I am a bit confused. In the release notes about TF 2.5.0 the first bullet point is that you now support Python 3.9. How do you do that if you don't have a CI for it?\r\n\r\nInternal CI is locked to py3.6. Google has a one version policy: all tools, libraries and binaries used anywhere inside Google are pinned to the exact same policy, regardless of team and product.\r\n\r\nExternally, we are building py3.6-py3.9, as you can see [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/ci_build/rel)", "> \n> \n> > I must say I am a bit confused. In the release notes about TF 2.5.0 the first bullet point is that you now support Python 3.9. How do you do that if you don't have a CI for it?\n> \n> \n> \n> Internal CI is locked to py3.6. Google has a one version policy: all tools, libraries and binaries used anywhere inside Google are pinned to the exact same policy, regardless of team and product.\n> \n> \n> \n> Externally, we are building py3.6-py3.9, as you can see [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/ci_build/rel)\n\nBuilding for something else than 3.6 is not the same as having a CI for something else than 3.6 \ud83e\udd14\nSounds like a weird policy to prevent you to have a better CI... Again, matrices are for that.\n", "> Again, matrices are for that.\r\n\r\nGranted, if a single matrix entry takes half a day to build, you want a small matrix. On the other hand, having \"internal\" and \"public\" CIs doing duplicate work is a disservice to the planet's climate, too.", "> > Again, matrices are for that.\r\n> \r\n> Granted, if a single matrix entry takes half a day to build, you want a small matrix. On the other hand, having \"internal\" and \"public\" CIs doing duplicate work is a disservice to the planet's climate, too.\r\n\r\nThis is why there are mechanism to have conditional CI or even skip some workflows (things like `[skip ci]` or  `[skip actions]` in the commit message). The classic example being, if you only push docs, you don't run the tests.\r\n\r\nIf there is no discipline with CI, of course it's intractable. But well executed, you can add lot of exotic plateformes or setups at no extra cost.", "Also caching of the C++ compilation, which I think @bhack was working on, would help a lot. In my experience, running the Python tests takes a fraction of the time of compilation (and even that could be sped up eg. by using pytest-xdist).", "> > > I must say I am a bit confused. In the release notes about TF 2.5.0 the first bullet point is that you now support Python 3.9. How do you do that if you don't have a CI for it?\r\n> > \r\n> > \r\n> > Internal CI is locked to py3.6. Google has a one version policy: all tools, libraries and binaries used anywhere inside Google are pinned to the exact same policy, regardless of team and product.\r\n> > Externally, we are building py3.6-py3.9, as you can see [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/ci_build/rel)\r\n> \r\n> Building for something else than 3.6 is not the same as having a CI for something else than 3.6 \ud83e\udd14\r\n> Sounds like a weird policy to prevent you to have a better CI... Again, matrices are for that.\r\n\r\nInternal CI only supports 3.6\r\nExternal CI runs on py3.6..py3.9\r\n\r\nMatrices work but also result in combinatorial explosion.\r\n\r\nInternal CI is needed due to differences between internal TF and OSS TF. Also because Google monorepo.", "> External CI runs on py3.6..py3.9\r\n\r\nI thought you said this is just for building. Which we all seem to interpret (thumbs up) as not running unit tests. \r\n\r\n> Matrices work but also result in combinatorial explosion.\r\n\r\nSure but nothing you cannot manage if conditionally skipped on some particular events. But I agree this adds some complexity.\r\n\r\nAs such a prominent package, I guess we all expect more rigour here. I mean, here we are just asking that you support latest NumPy and maybe check these in advance in your CI. Nothing esoteric which is not already done by all big packages. ", "> > External CI runs on py3.6..py3.9\r\n> \r\n> I thought you said this is just for building. Which we all seem to interpret (thumbs up) as not running unit tests.\r\n\r\nMy bad, I was not clear (though the linked CI scripts would have shown that we run tests). We have multiple CI types:\r\n\r\n1. PR presubmits: one single version of Python, just spreading on multiple OSes and configs (although not all of them are blocking)\r\n2. Continuous builds: whenever a new change lands and there is no continuous build running a new one is triggered at HEAD. This again covers only one version of Python and is spread across multiple OSes but is supposed to have buildcops monitoring these and immediately surfacing breakages.\r\n3. Nightly builds: 2 versions, all supported Python versions and OSes. Supposed to have buildcops to surface breakages too. The 2 versions are:\r\n    * `nonpip`: just `bazel test` of the code base, just like all the builds above\r\n    * `pip`: `bazel build` for the pip package, install the pip package, run tests against it\r\n4. Release builds: same as nightly builds but run on the release process, after branch cut (multiple times, at least ~2 times for each RC + final)\r\n\r\nBetween 1 and 2, during the PR import process there are presubmits builds inside Google that test the internal version of TF, on a single version of Python, single OS (Linux)\r\n\r\nStep 2 is actually duplicated: we build from HEAD of OSS and we also build from HEAD of internal code exposing that to a local OSS repo.\r\n\r\nSteps 3 and 4 have a few builds that are outside the `pip`/`nonpip` separation, but not relevant here. See [the build scrips linked before](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/ci_build/rel) for all details\r\n\r\n> \r\n> > Matrices work but also result in combinatorial explosion.\r\n> \r\n> Sure but nothing you cannot manage if conditionally skipped on some particular events. But I agree this adds some complexity.\r\n\r\nSome of these don't depend on our team. Changing the Google behemoth is very slow, but we're working on this.\r\n\r\n> \r\n> As such a prominent package, I guess we all expect more rigour here. I mean, here we are just asking that you support latest NumPy and maybe check these in advance in your CI. Nothing esoteric which is not already done by all big packages.\r\n\r\nThis is understood and we're working on making things easier for OSS contributors. But these are processes that take months given the whole CI structure and org setup\r\n", "We understand that Google is a large complex machine, and it is great that TF is even FOSS in the first place. I hope I speak for everyone in saying that we appreciate the work you do and just want to help however we can. As long as we keep the discussions going and things moving forward, we'll all benefit.", "@bhack  Can you please resolve conflicts? Thanks!", "By now this is just a monitor PR. I am waiting for @mihaimaruseac feedback on when I need to resync."]}, {"number": 48917, "title": "Add Support for TensorRT 8", "body": "This PR adds TRT 8 support to TensorFlow, removing deprecated API calls and updating TFTRT's SimpleTensor, FakeTensor, and ITensor handling with a new ITensorProxyPtr class. There will be a follow up PR to remove the use of ITensorProxyPtr class, see https://github.com/tensorflow/tensorflow/pull/48917#issuecomment-864244135.\r\n\r\nTagging @bixia1 @DEKHTIARJonathan @tfeher @jhalakpatel  for visibility", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48917) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48917) for more info**.\n\n<!-- need_author_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48917) for more info**.\n\n<!-- need_author_cla -->", "@MattConley Can you please sign CLA. Thanks!", "@googlebot I fixed it.", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48917) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I fixed it.", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48917) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent.", "FYI @bixia1 CLA issue fixed ;) ", "Thanks for the reviews, I'm putting together an update to address the comments, and will address any of the other questions in the reviews themselves.", "@MattConley  Can you please resolve conflicts? Thanks!", "@MattConley  Can you please check @bixia1's comments and keep us posted ? Thanks!", "Thanks for the help! I'll have one more set of pushes to address the remaining comments and then we should be ready for the next round of review.", "@MattConley  Can you please resolve conflicts? Thanks!", "## Daily Status Update - 06/15/2021\r\n\r\nCOMMIT ID TESTED: 59b148b6014\r\n\r\nCC: @mattconley @bixia1 @tfeher @jhalakpatel\r\n\r\n### Build Status:\r\n- **TRT 7.1.3** - Current Google Default: :heavy_check_mark: PASS\r\n- **TRT 7.2.3.4** - Current NVIDIA Default: :heavy_check_mark: PASS\r\n- **TRT 8.x** - Upcoming release: :heavy_check_mark: PASS\r\n\r\n### C++ Unittests Status:\r\n- **TRT 7.1.3** - Current Google Default: :x: FAIL 8/16\r\n  - //tensorflow/compiler/tf2tensorrt:convert_graph_test\r\n  - //tensorflow/compiler/tf2tensorrt:convert_graph_test_gpu\r\n  - //tensorflow/compiler/tf2tensorrt:convert_nodes_test\r\n  - //tensorflow/compiler/tf2tensorrt:convert_nodes_test_gpu\r\n  - //tensorflow/compiler/tf2tensorrt:trt_engine_op_test\r\n  - //tensorflow/compiler/tf2tensorrt:trt_engine_op_test_gpu\r\n  - //tensorflow/compiler/tf2tensorrt:trt_engine_resource_ops_test\r\n  - //tensorflow/compiler/tf2tensorrt:trt_engine_resource_ops_test_gpu\r\n\r\n- **TRT 7.2.3.4** - Current NVIDIA Default: :x: FAIL 8/16\r\n  - //tensorflow/compiler/tf2tensorrt:convert_graph_test\r\n  - //tensorflow/compiler/tf2tensorrt:convert_graph_test_gpu\r\n  - //tensorflow/compiler/tf2tensorrt:convert_nodes_test\r\n  - //tensorflow/compiler/tf2tensorrt:convert_nodes_test_gpu\r\n  - //tensorflow/compiler/tf2tensorrt:trt_engine_op_test\r\n  - //tensorflow/compiler/tf2tensorrt:trt_engine_op_test_gpu\r\n  - //tensorflow/compiler/tf2tensorrt:trt_engine_resource_ops_test\r\n  - //tensorflow/compiler/tf2tensorrt:trt_engine_resource_ops_test_gpu\r\n  - //tensorflow/compiler/tf2tensorrt:convert_graph_test\r\n\r\n- **TRT 8.x** - Upcoming release: :x: FAIL 8/16\r\n  - //tensorflow/compiler/tf2tensorrt:convert_graph_test\r\n  - //tensorflow/compiler/tf2tensorrt:convert_graph_test_gpu\r\n  - //tensorflow/compiler/tf2tensorrt:convert_nodes_test\r\n  - //tensorflow/compiler/tf2tensorrt:convert_nodes_test_gpu\r\n  - //tensorflow/compiler/tf2tensorrt:trt_engine_op_test\r\n  - //tensorflow/compiler/tf2tensorrt:trt_engine_op_test_gpu\r\n  - //tensorflow/compiler/tf2tensorrt:trt_engine_resource_ops_test\r\n  - //tensorflow/compiler/tf2tensorrt:trt_engine_resource_ops_test_gpu\r\n  - //tensorflow/compiler/tf2tensorrt:convert_graph_test\r\n\r\n### Python Unittests Status:\r\n- **TRT 7.1.3** - Current Google Default: :x: FAIL\r\n  - //tensorflow/python/compiler/tensorrt:trt_convert_test\r\n  - //tensorflow/python/compiler/tensorrt:trt_convert_test_gpu\r\n  \r\n- **TRT 7.2.3.4** - Current NVIDIA Default: :x: FAIL\r\n  - //tensorflow/python/compiler/tensorrt:trt_convert_test\r\n  - //tensorflow/python/compiler/tensorrt:trt_convert_test_gpu\r\n  \r\n- **TRT 8.x** - Upcoming release: :x: FAIL\r\n  - //tensorflow/python/compiler/tensorrt:trt_convert_test\r\n  - //tensorflow/python/compiler/tensorrt:trt_convert_test_gpu\r\n  \r\n  ----------------\r\n  \r\n  There seems to be a pretty simple bug:\r\n  \r\n```bash\r\n[ RUN      ] ConvertAfterShapesTest.DirectlyConnectedEngines\r\nfree(): double free detected in tcache 2                                                                                                                                                              [33/821]\r\n\r\n*** Received signal 6 ***                                                                                                                                                                                     \r\n*** BEGIN MANGLED STACK TRACE ***         \r\n[...]\r\n*** END MANGLED STACK TRACE ***\r\n*** Begin stack trace ***\r\n        tensorflow::CurrentStackTrace()\r\n        gsignal\r\n        abort\r\n        tensorflow::tensorrt::convert::TrtNodeValidator::ConvertToTensorOrWeights(tensorflow::NodeDef const&, int, tensorflow::tensorrt::convert::TRT_TensorOrWeights*)\r\n        tensorflow::tensorrt::convert::TrtNodeValidator::IsTensorRTCandidate(tensorflow::Node const*)\r\n        std::_Function_handler<tensorflow::Status (tensorflow::Node const*), std::_Bind<tensorflow::Status (tensorflow::tensorrt::convert::TrtNodeValidator::*(tensorflow::tensorrt::convert::TrtNodeValidator*, std::_Placeholder<1>))(tensorflow::Node const*)> >::_M_invoke(std::_Any_data const&, tensorflow::Node const*&&)\r\n        tensorflow::tensorrt::segment::SegmentGraph(tensorflow::Graph const*, tensorflow::grappler::GraphProperties const*, std::function<tensorflow::Status (tensorflow::Node const*)> const&, std::function<bool (tensorflow::Edge const*)> const&, std::function<bool (tensorflow::Edge const*)> const&, tensorflow::tensorrt::segment::SegmentOptions const&, std::vector<tensorflow::tensorrt::segment::Segment, std::allocator<tensorflow::tensorrt::segment::Segment> >*)\r\n        tensorflow::tensorrt::convert::ConvertAfterShapes(tensorflow::tensorrt::convert::ConversionParams const&)\r\n        void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*)\r\n        testing::Test::Run()\r\n        testing::TestInfo::Run()\r\n        testing::TestSuite::Run()\r\n        testing::internal::UnitTestImpl::RunAllTests()\r\n        bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*)\r\n        testing::UnitTest::Run()\r\n        main\r\n        __libc_start_main\r\n*** End stack trace ***\r\n/root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/tf2tensorrt/convert_graph_test.runfiles/org_tensorflow/tensorflow/tools/ci_b$ild/gpu_build/parallel_gpu_execute: line 69: 54991 Aborted                 (core dumped) \"$TEST_BINARY\" $@\r\n```", "@bixia1 Thanks for your continued review, I'm addressing the comments now.\r\n@DEKHTIARJonathan Thanks for your test; the shared pointer implementation I pushed earlier appears to have worsened the memory issues rather than fixing them, so I've reverted it for now and will push an updated version as soon as I confirm it passes the tests.", "## Daily Status Update - 06/15/2021\r\n\r\nCOMMIT ID TESTED: 22971b4eb07\r\n\r\nCC: @mattconley @bixia1 @tfeher @jhalakpatel\r\n\r\n### Build Status:\r\n- **TRT 7.1.3** - Current Google Default: :heavy_check_mark: PASS\r\n- **TRT 7.2.3.4** - Current NVIDIA Default: :heavy_check_mark: PASS\r\n- **TRT 8.x** - Upcoming release: :heavy_check_mark: PASS\r\n\r\n### C++ Unittests Status:\r\n- **TRT 7.1.3** - Current Google Default: :heavy_check_mark: PASS 14/16 | :x: FAIL 02/16\r\n  - //tensorflow/compiler/tf2tensorrt:convert_graph_test                                :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:convert_graph_test_gpu                            :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:convert_nodes_test                                :x: FAIL\r\n  - //tensorflow/compiler/tf2tensorrt:convert_nodes_test_gpu                            :x: FAIL\r\n  - //tensorflow/compiler/tf2tensorrt:segment_test                                      :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:segment_test_gpu                                  :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:tensorrt_test_cc                                  :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:tensorrt_test_cc_gpu                              :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:trt_allocator_test                                :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:trt_engine_op_test                                :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:trt_engine_op_test_gpu                            :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:trt_engine_resource_ops_test                      :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:trt_engine_resource_ops_test_gpu                  :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:trt_lru_cache_test                                :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:trt_shape_optimization_profiles_test              :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:trt_shape_optimization_profiles_test_gpu          :heavy_check_mark: PASS\r\n\r\n- **TRT 7.2.3.4** - Current NVIDIA Default: :heavy_check_mark: PASS 14/16 | :x: FAIL 02/16\r\n  - //tensorflow/compiler/tf2tensorrt:convert_graph_test                                :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:convert_graph_test_gpu                            :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:convert_nodes_test                                :x: FAIL\r\n  - //tensorflow/compiler/tf2tensorrt:convert_nodes_test_gpu                            :x: FAIL\r\n  - //tensorflow/compiler/tf2tensorrt:segment_test                                      :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:segment_test_gpu                                  :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:tensorrt_test_cc                                  :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:tensorrt_test_cc_gpu                              :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:trt_allocator_test                                :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:trt_engine_op_test                                :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:trt_engine_op_test_gpu                            :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:trt_engine_resource_ops_test                      :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:trt_engine_resource_ops_test_gpu                  :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:trt_lru_cache_test                                :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:trt_shape_optimization_profiles_test              :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:trt_shape_optimization_profiles_test_gpu          :heavy_check_mark: PASS\r\n\r\n\r\n- **TRT 8.x** - Upcoming release: :heavy_check_mark: PASS 16/16 | :x: FAIL 00/16\r\n  - //tensorflow/compiler/tf2tensorrt:convert_graph_test                                :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:convert_graph_test_gpu                            :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:convert_nodes_test                                :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:convert_nodes_test_gpu                            :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:segment_test                                      :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:segment_test_gpu                                  :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:tensorrt_test_cc                                  :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:tensorrt_test_cc_gpu                              :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:trt_allocator_test                                :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:trt_engine_op_test                                :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:trt_engine_op_test_gpu                            :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:trt_engine_resource_ops_test                      :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:trt_engine_resource_ops_test_gpu                  :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:trt_lru_cache_test                                :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:trt_shape_optimization_profiles_test              :heavy_check_mark: PASS\r\n  - //tensorflow/compiler/tf2tensorrt:trt_shape_optimization_profiles_test_gpu          :heavy_check_mark: PASS\r\n\r\n\r\n### Python Unittests Status:\r\n- **TRT 7.1.3** - Current Google Default: :heavy_check_mark: PASS 02/02 | :x: FAIL 00/02\r\n  - //tensorflow/python/compiler/tensorrt:trt_convert_test                              :heavy_check_mark: PASS\r\n  - //tensorflow/python/compiler/tensorrt:trt_convert_test_gpu                          :heavy_check_mark: PASS\r\n  \r\n- **TRT 7.2.3.4** - Current NVIDIA Default: :heavy_check_mark: PASS 02/02 | :x: FAIL 00/02\r\n  - //tensorflow/python/compiler/tensorrt:trt_convert_test                              :heavy_check_mark: PASS\r\n  - //tensorflow/python/compiler/tensorrt:trt_convert_test_gpu                          :heavy_check_mark: PASS\r\n  \r\n- **TRT 8.x** - Upcoming release: :heavy_check_mark: PASS 02/02 | :x: FAIL 00/02\r\n  - //tensorflow/python/compiler/tensorrt:trt_convert_test                              :heavy_check_mark: PASS\r\n  - //tensorflow/python/compiler/tensorrt:trt_convert_test_gpu                          :heavy_check_mark: PASS\r\n  \r\n## Python Test Files Status:\r\n- **TRT 7.1.3** - Current Google Default: :heavy_check_mark: PASS 23/27 | :x: FAIL 02/27 | :white_check_mark: IGNORED 02/27\r\n  - /tensorflow/python/compiler/tensorrt/test/annotate_max_batch_sizes_test.py          :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/base_test.py                              :x: FAIL `Cannot register 2 metrics with the same name`\r\n  - /tensorflow/python/compiler/tensorrt/test/batch_matmul_test.py                      :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/biasadd_matmul_test.py                    :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/binary_tensor_weight_broadcast_test.py    :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/cast_test.py                              :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/combined_nms_test.py                      :x: FAIL: `core dump => CombinedNmsTest.testTfTrtV2_OfflineConversion_DynamicEngine_FP16_NoCalibration`\r\n  - /tensorflow/python/compiler/tensorrt/test/concatenation_test.py                     :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/const_broadcast_test.py                   :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/conv2d_test.py                            :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/dynamic_input_shapes_test.py              :white_check_mark: IGNORED\r\n  - /tensorflow/python/compiler/tensorrt/test/identity_output_test.py                   :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/int32_test.py                             :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/lru_cache_test.py                         :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/memory_alignment_test.py                  :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/multi_connection_neighbor_engine_test.py  :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/neighboring_engine_test.py                :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/quantization_mnist_test.py                :white_check_mark: IGNORED\r\n  - /tensorflow/python/compiler/tensorrt/test/quantization_test.py                      :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/rank_two_test.py                          :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/reshape_transpose_test.py                 :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/tf_function_test.py                       :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/topk_test.py                              :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/trt_mode_test.py                          :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/unary_test.py                             :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/vgg_block_nchw_test.py                    :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/vgg_block_test.py                         :heavy_check_mark: PASS\r\n  \r\n- **TRT 7.2.3.4** - Current NVIDIA Default: :heavy_check_mark: PASS 24/27 | :x: FAIL 01/27 | :white_check_mark: IGNORED 02/27\r\n  - /tensorflow/python/compiler/tensorrt/test/annotate_max_batch_sizes_test.py          :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/base_test.py                              :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/batch_matmul_test.py                      :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/biasadd_matmul_test.py                    :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/binary_tensor_weight_broadcast_test.py    :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/cast_test.py                              :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/combined_nms_test.py                      :x: FAIL: `core dump => CombinedNmsTest.testTfTrtV2_OfflineConversion_DynamicEngine_FP16_NoCalibration`\r\n  - /tensorflow/python/compiler/tensorrt/test/concatenation_test.py                     :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/const_broadcast_test.py                   :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/conv2d_test.py                            :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/dynamic_input_shapes_test.py              :white_check_mark: IGNORED\r\n  - /tensorflow/python/compiler/tensorrt/test/identity_output_test.py                   :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/int32_test.py                             :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/lru_cache_test.py                         :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/memory_alignment_test.py                  :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/multi_connection_neighbor_engine_test.py  :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/neighboring_engine_test.py                :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/quantization_mnist_test.py                :white_check_mark: IGNORED\r\n  - /tensorflow/python/compiler/tensorrt/test/quantization_test.py                      :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/rank_two_test.py                          :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/reshape_transpose_test.py                 :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/tf_function_test.py                       :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/topk_test.py                              :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/trt_mode_test.py                          :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/unary_test.py                             :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/vgg_block_nchw_test.py                    :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/vgg_block_test.py                         :heavy_check_mark: PASS\r\n  \r\n- **TRT 8.x** - Upcoming release: :heavy_check_mark: PASS 25/27 | :x: FAIL 00/27 | :white_check_mark: IGNORED 02/27\r\n  - /tensorflow/python/compiler/tensorrt/test/annotate_max_batch_sizes_test.py          :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/base_test.py                              :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/batch_matmul_test.py                      :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/biasadd_matmul_test.py                    :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/binary_tensor_weight_broadcast_test.py    :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/cast_test.py                              :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/combined_nms_test.py                      :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/concatenation_test.py                     :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/const_broadcast_test.py                   :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/conv2d_test.py                            :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/dynamic_input_shapes_test.py              :white_check_mark: IGNORED\r\n  - /tensorflow/python/compiler/tensorrt/test/identity_output_test.py                   :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/int32_test.py                             :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/lru_cache_test.py                         :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/memory_alignment_test.py                  :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/multi_connection_neighbor_engine_test.py  :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/neighboring_engine_test.py                :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/quantization_mnist_test.py                :white_check_mark: IGNORED\r\n  - /tensorflow/python/compiler/tensorrt/test/quantization_test.py                      :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/rank_two_test.py                          :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/reshape_transpose_test.py                 :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/tf_function_test.py                       :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/topk_test.py                              :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/trt_mode_test.py                          :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/unary_test.py                             :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/vgg_block_nchw_test.py                    :heavy_check_mark: PASS\r\n  - /tensorflow/python/compiler/tensorrt/test/vgg_block_test.py                         :heavy_check_mark: PASS", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48917) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48917) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48917) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48917) for more info**.\n\n<!-- need_author_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48917) for more info**.\n\n<!-- need_author_cla -->", "This PR is the initial implementation of adding TensorRT 8 support to TensorFlow and TF-TRT.  After some discussion about the design of the newly added ITensorProxy and ITensorProxyPtr classes, we are planning on deferring some changes of this system to a second PR.  While the current implementation is functional, it does complicate the memory management of nvinfer1::ITensor and SimpleITensor objects, resulting in a less optimal and somewhat overly complex configuration.  We plan to remove the ITensorProxyPtr class in the follow up PR, simplifying the interactions to focus on the ITensorProxy class and using pointers thereto when necessary.  This change will also allow us to more easily redistribute ownership properly while avoiding memory leaks and an overabundance of shared and unique pointers.  One specific example mentioned earlier in the review of this PR which we will be addressing is moving the SimpleITensor object currently owned by ITensorProxy to TRT_TensorOrWeights.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48917) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48917) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.", "@bixia1 Thanks for your review!  Please let me know if there's any additional information I can provide / changes I should make to the code.  The CLA issue was due to me accidentally using a different email for some of the more recent commits, but after rebasing to amend the author (and also removing the reverted smart pointer commit) it looks good now.", "Working on an issue that just came up, fix incoming momentarily.", "Should be good to go now, thanks again", "I see convert_nodes_test has a correctness issue and fails:\r\nExpected equality of these values:\r\n  input->getDynamicRangeMax()\r\n    Which is: 0\r\n  5.0f\r\n    Which is: 5\r\n\r\nThere are a few other test with memory leak, see log using TRT8. Similar memory leak happens with TRT 7 also. See log.\r\n", "[fail4.log](https://github.com/tensorflow/tensorflow/files/6688593/fail4.log)\r\n[correctness.log](https://github.com/tensorflow/tensorflow/files/6688615/correctness.log)\r\n[fail3.log](https://github.com/tensorflow/tensorflow/files/6688594/fail3.log)\r\n[fail2.log](https://github.com/tensorflow/tensorflow/files/6688596/fail2.log)\r\n[fail1.log](https://github.com/tensorflow/tensorflow/files/6688599/fail1.log)\r\n\r\n", "Thanks for the continued review.  I've fixed the merge conflict and addressed the above reviews, but am still trying to reproduce and fix the DynamicRange issue you mention above.  I should have the changes for that pushed within the next few hours"]}, {"number": 48916, "title": "No gradients exist for a sub-classed model", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):  binary\r\n- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: using CPU\r\n- GPU model and memory: using CPU\r\n\r\n**Describe the current behavior**\r\nNo gradients provided for any variable of a simple subclassed model. The weights are not zero. \r\n\r\n**Describe the expected behavior**\r\nThe gradients for all the variables should exist\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): No - Briefly describe your candidate solution\r\n(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nThe full model is quite large/complex, in the full architecture, this basic block is the only part of the model that throws this issue. It is the most fundamental part of the architecture.\r\n\r\nFor google colab:\r\nAdd this to \"My Drive\" on google drive:\r\nhttps://drive.google.com/drive/folders/1epROVNfvO10Ksy8CwJQdamSK96JZnWW9?usp=sharing\r\nand then run the code: \r\nhttps://colab.research.google.com/drive/18sMqNn8IpTQLZBlInWSbX0ITd2GWDDkz?usp=sharing\r\n\r\n```\r\nclass BasicBlock(tf.keras.Model):\r\n        def __init__(self, ncIn, ncOut, batchNorm_type=0, strides=(1,1), dynamic=True, name=None):\r\n        super(BasicBlock, self).__init__(dynamic=True)\r\n        self.ncIn = ncIn\r\n        self.ncOut = ncOut\r\n        self.conv_1 = SeparableConv2D(self.ncOut, kernel_size=(1,1), strides=(1,1), padding=\"same\", use_bias=False, name=f'BB_conv_1_{name}')\r\n        self.conv_2 = SeparableConv2D(self.ncOut, kernel_size=(1,1), strides=(1,1), padding=\"same\", use_bias=False, name=f'BB_conv_2_{name}') \r\n        self.bn = BatchNormalization(name=f'BN_0_{name}')\r\n        def call(self, inputs):\r\n            x = inputs[0]\r\n            out = self.conv_1(x)\r\n            out = self.bn(out)\r\n            out = Activation('relu')(out) \r\n            out = self.conv_2(out)\r\n            out = self.bn(out) \r\n            out = Activation('relu')(out)\r\n            return out \r\n```\r\n\r\nFor the network architecture itself  and,\r\n\r\n```        \r\n        epoch = 0\r\n        optimizer = Adam()\r\n        bb = BasicBlock(1, 32)\r\n\r\n        while True:\r\n            real_samples = self.generate_real_samples(n_batch, n_patch)\r\n            [X_real_img, X_real_sh, X_gt_sh], y_real = real_samples\r\n            X_real_img = np.concatenate(X_real_img)\r\n            X_target_sh = np.concatenate(X_real_sh)\r\n            X_gt_sh = np.concatenate(X_gt_sh)\r\n            \r\n            with tf.GradientTape() as tape:\r\n                test = bb([X_real_img])\r\n                loss_value = MSE(X_gt_sh, X_target_sh) #dummy loss\r\n\r\n            #retrieve gradients of trainable variables w.r.t the loss\r\n            grads = tape.gradient(loss_value, bb.trainable_weights)\r\n            #Run SGD\r\n            optimizer.apply_gradients(zip(grads, bb.trainable_weights))\r\n```\r\n\r\nfor the training.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n\r\n`ValueError: No gradients provided for any variable: ['basic_block_8/BB_conv_1_None/depthwise_kernel:0', 'basic_block_8/BB_conv_1_None/pointwise_kernel:0', 'basic_block_8/BB_conv_2_None/depthwise_kernel:0', 'basic_block_8/BB_conv_2_None/pointwise_kernel:0', 'basic_block_8/BN_0_None/gamma:0', 'basic_block_8/BN_0_None/beta:0']`\r\n\r\n", "comments": ["@Zodaztream ,\r\n\r\nIn order to reproduce the issue reported here, could you please provide the complete code and the dataset  or colab link you are using. \r\nAlso please refer these issues for similar error log  #45604,#47445,#47288\r\n\r\n\r\nThanks!", "> @Zodaztream ,\r\n> \r\n> In order to reproduce the issue reported here, could you please provide the complete code and the dataset or colab link you are using.\r\n> Also please refer these issues for similar error log #45604,#47445,#47288\r\n> \r\n> Thanks!\r\n\r\nThanks, I have added a google colab and google drive link (for the necessary files", "@Zodaztream , This can be due to multiple reasons, but to bring numerical stability in your gradient you can use `tf.custom_gradient` decorator by following the documentation [here](https://www.tensorflow.org/api_docs/python/tf/custom_gradient).", "> @Zodaztream , This can be due to multiple reasons, but to bring numerical stability in your gradient you can use `tf.custom_gradient` decorator by following the documentation [here](https://www.tensorflow.org/api_docs/python/tf/custom_gradient).\r\n\r\nI have implemented a custom gradient with that wrapper and unfortunately it still throws the same warning\r\n\r\n```\r\n    @tf.custom_gradient\r\n    def loss_fn(self, L_hat, X_gt_sh):\r\n        x = tf.subtract(X_gt_sh, L_hat)\r\n        def grad(upstream):\r\n            dl_L_hat = tf.subtract(L_hat,X_gt_sh)\r\n            dl_L_hat = tf.reshape(dl_L_hat, (1,1,1,9))\r\n            dl_gt_sh = tf.subtract(X_gt_sh,L_hat)\r\n            dl_gt_sh = tf.reshape(dl_gt_sh, (1,1,1,9))\r\n            return upstream * dl_L_hat, upstream * dl_gt_sh\r\n        return tf.nn.l2_loss(x), grad\r\n```\r\n\r\nThe reshape is required because for some reason dl_L_hat spit out (1,1,1,1,1,1,1,1,9) in shape", "@Zodaztream you can also try with[ tape.watch](https://www.tensorflow.org/api_docs/python/tf/GradientTape#watch) to record and trace every operation that involves any tensors. \r\nThis can be useful in some cases, like if you want to implement a\r\nregularization loss that penalizes activations that vary a lot when the\r\ninputs vary little: the loss will be based on the gradient of the activations\r\nwith regard to the inputs. Since the inputs are not variables, you would\r\nneed to tell the tape to watch them.", "> @Zodaztream you can also try with[ tape.watch](https://www.tensorflow.org/api_docs/python/tf/GradientTape#watch) to record and trace every operation that involves any tensors.\r\n> This can be useful in some cases, like if you want to implement a\r\n> regularization loss that penalizes activations that vary a lot when the\r\n> inputs vary little: the loss will be based on the gradient of the activations\r\n> with regard to the inputs. Since the inputs are not variables, you would\r\n> need to tell the tape to watch them.\r\n\r\nHey. So, we decided to rework the entire network and remove all the subsubclassed models and thus only have one model with all the layers in sequential manner. This worked. No longer do we get that gradients do not exist. However, I still feel that the first network (which is essentially the same as this sequential one, just without the calls to other subclassed models) should work. \r\n\r\n```\r\nclass HourglasSequential(tf.keras.models.Model):\r\n    def __init__(self):\r\n        super(HourglasSequential, self).__init__()\r\n        self.nrSH_in = 27 #number of input spherical harmonics coeff.\r\n        self.baseFilter = 16\r\n        self.nrSH_out = 9 #if gray else 27 #nr output SH\r\n        self.ncPre = self.baseFilter #This is the amount required for the pre-convolution step\r\n\r\n        self.ncHG3 = self.baseFilter #this is the amount of output channels for the first and last hourglass block\r\n        self.ncHG2 = self.baseFilter * 2\r\n        self.ncHG1 = self.baseFilter * 4\r\n        self.ncHG0 = self.baseFilter * 8 + self.nrSH_in # Bottleneck layer. \r\n      \r\n        self.pre_conv = SeparableConv2D(self.ncPre, kernel_size=(5,5), strides=(1,1), padding=\"same\", name=\"pre_conv\")\r\n        self.pre_bn = BatchNormalization(name=\"pre_bn\")\r\n        self.relu_1 = ReLU()\r\n        self.HG3_BB_Upper_conv1 = SeparableConv2D(16, 3, 1, padding=\"same\", use_bias=False)\r\n        self.HG3_BN1 = BatchNormalization()\r\n        #self.HG3_upper_conv1 = SeparableConv2D(16, kernel_size=(3,3), strides=(1,1), padding=\"same\", use_bias=False) \r\n        #self.HG3_upper_conv2 = SeparableConv2D(16, kernel_size=(3,3), strides=(1,1), padding=\"same\", use_bias=False) \r\n        #self.HG3_upper_bn = BatchNormalization()\r\n        #self.HG3_upper_relu_1 = ReLU()\r\n        #self.HG3_upper_relu_2 = ReLU()\r\n    def compute_output_shape(self, input_shape):\r\n        return [tf.TensorShape((1,1,1,9))] #Must somehow add L_hat to the dimensions\r\n    \r\n    def setup_model(self):\r\n        x = tf.keras.Input(shape=(512,512,1))\r\n        return tf.keras.models.Model(inputs=[x], outputs=self.call(x))\r\n    \r\n    def call(self, inputs):\r\n        x = inputs[0]\r\n        feat = self.pre_conv(x)\r\n        feat = self.pre_bn(feat)\r\n        feat = self.relu_1(feat)\r\n        #HG3 tar in  16 som ncout, ncin = 16 som IN\r\n        #HG3 basic block upper som tar in 16 som input och 16 som output\r\n        feat = self.HG3_BB_Upper_conv1(feat)\r\n        feat = self.HG3_BN1(feat)\r\n        feat = ReLU()(feat)\r\n        feat = SeparableConv2D(16,3,1,padding=\"same\", use_bias=False)(feat)\r\n        feat = BatchNormalization()(feat)\r\n        feat = ReLU()(feat)\r\n        #HG3 downsample\r\n        feat = MaxPool2D(2,2)(feat)\r\n        #HG3 LOW1 basic block med ncIn 16 och 16 Ncout\r\n        feat = SeparableConv2D(16, 3, 1, padding=\"same\", use_bias=False)(feat)\r\n        feat = BatchNormalization()(feat)\r\n        feat = ReLU()(feat)\r\n        feat = SeparableConv2D(16,3,1,padding=\"same\", use_bias=False)(feat)\r\n        feat = BatchNormalization()(feat)\r\n        feat = ReLU()(feat)\r\n        #HG2 ncIn 16, ncout = 32\r\n        #HG2 bb upper \r\n        feat = SeparableConv2D(16,3,1, padding=\"same\", use_bias=False)(feat)\r\n        feat = BatchNormalization()(feat)\r\n        feat = ReLU()(feat)\r\n        feat = SeparableConv2D(16,3,1,padding=\"same\", use_bias=False)(feat)\r\n        feat = BatchNormalization()(feat)\r\n        feat = ReLU()(feat)\r\n        #HG2 downsample\r\n        feat = MaxPool2D(2,2)(feat)\r\n        #HG2 LOW1 basic block med ncIn 16 och 16 Ncout\r\n        feat = SeparableConv2D(32, 3, 1, padding=\"same\", use_bias=False)(feat)\r\n        feat = BatchNormalization()(feat)\r\n        feat = ReLU()(feat)\r\n        feat = SeparableConv2D(32,3,1,padding=\"same\", use_bias=False)(feat)\r\n        feat = BatchNormalization()(feat)\r\n        feat = ReLU()(feat)\r\n        #HG1 ncIn 32, ncout = 64\r\n        #HG1 bb upper \r\n        feat = SeparableConv2D(32,3,1, padding=\"same\", use_bias=False)(feat)\r\n        feat = BatchNormalization()(feat)\r\n        feat = ReLU()(feat)\r\n        feat = SeparableConv2D(32,3,1,padding=\"same\", use_bias=False)(feat)\r\n        feat = BatchNormalization()(feat)\r\n        feat = ReLU()(feat)\r\n        #HG1 downsample\r\n        feat = MaxPool2D(2,2)(feat)\r\n        #HG1 LOW1 basic block med ncIn 16 och 16 Ncout\r\n        feat = SeparableConv2D(64, 3, 1, padding=\"same\", use_bias=False)(feat)\r\n        feat = BatchNormalization()(feat)\r\n        feat = ReLU()(feat)\r\n        feat = SeparableConv2D(64,3,1,padding=\"same\", use_bias=False)(feat)\r\n        feat = BatchNormalization()(feat)\r\n        feat = ReLU()(feat)\r\n        #HG0 ncIn 64, ncout = 128 + 27 = 155\r\n        #HG0 bb upper \r\n        feat = SeparableConv2D(64,3,1, padding=\"same\", use_bias=False)(feat)\r\n        feat = BatchNormalization()(feat)\r\n        feat = ReLU()(feat)\r\n        feat = SeparableConv2D(64,3,1,padding=\"same\", use_bias=False)(feat)\r\n        feat = BatchNormalization()(feat)\r\n        feat = ReLU()(feat)\r\n        #HG0 downsample\r\n        feat = MaxPool2D(2,2)(feat)\r\n        #HG0 LOW1 basic block\r\n        feat = SeparableConv2D(155, 3, 1, padding=\"same\", use_bias=False)(feat)\r\n        feat = BatchNormalization()(feat)\r\n        feat = ReLU()(feat)\r\n        feat = SeparableConv2D(155,3,1,padding=\"same\", use_bias=False)(feat)\r\n        feat = BatchNormalization()(feat)\r\n        feat = ReLU()(feat)\r\n        #LightingNet ncIn 27, out 9, middle 128\r\n        feat = feat[:,:,:,0:27]\r\n        feat = tf.math.reduce_mean(feat, axis=(1,2), keepdims=True)\r\n        feat = SeparableConv2D(128, 1, 1, use_bias=False)(feat)\r\n        feat = PReLU()(feat)\r\n        #print(feat.shape)\r\n        L_hat = SeparableConv2D(9,1,1, use_bias=False)(feat)\r\n\r\n        return L_hat\r\n\r\n```", "@Zodaztream Please go ahead and close the issue if you don't have further queries. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48916\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48916\">No</a>\n"]}, {"number": 48915, "title": "Dynamic Input support for TFLite models", "body": "Hello Tensors\r\nI have a TF model with (None,None,100) input dims and its converted into TFLite too.\r\nBut when I do inference, it gives error-\r\nRuntimeError: tensorflow/lite/kernels/reshape.cc:58 stretch_dim != -1 (0 != -1)Node number 72 (RESHAPE) failed to prepare.\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.4.1.\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, 0th index can be dynamic(None or -1).\r\n\r\n**Will this change the current api? How?**\r\nOther indexs of model input can be dynamic(None or -1)\r\n\r\n**Who will benefit with this feature?**\r\nGenerally, Vision models have 0th index dynamic but NLP models have other dimensions as dynamic.\r\n", "comments": ["@neso613 Please share simple stand alone code such that we can replicate the issue faced or if possible share a colab gist.\r\n\r\nFYI, both TF and TFLite supports multiple dimensions but the Reshape operator does not only allow two dynamic dimensions in the shape argument.", "> @neso613 Please share simple stand alone code such that we can replicate the issue faced or if possible share a colab gist.\r\n> \r\n> FYI, both TF and TFLite supports multiple dimensions but the Reshape operator does not only allow two dynamic dimensions in the shape argument.\r\n\r\nThats the Question.\r\nTFLite supports dynamic input shape.\r\n`[{'name': 'input_1', 'index': 0, 'shape': array([ 1,  1, 40], dtype=int32), 'shape_signature': array([-1, -1, 40], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]`\r\n\r\nWhen I do inference, my input can be of any shape, so I have to resize using\r\n`interpreter.resize_tensor_input(input_details[0]['index'],data_test.shape)`\r\n", "That is an intended behavior since the input tensors should be fixed through the resize_tensor_input method when the given model has dynamic inpiuts. You can resize input tensors again when the model should handle a different size in the following tasks.", "> That is an intended behavior since the input tensors should be fixed through the resize_tensor_input method when the given model has dynamic inpiuts. You can resize input tensors again when the model should handle a different size in the following tasks.\r\n\r\nThis is the code, I am using\r\n`interpreter.resize_tensor_input(input_details[0]['index'],data_test.shape)\r\ninterpreter.allocate_tensors()\r\ninterpreter.set_tensor(input_details[0]['index'], data_test)\r\ninterpreter.invoke()\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])`", "@neso613 Please share simple stand alone code such that we can replicate the issue faced or if possible share a colab gist. It is really impossible to reproduce your problem at our end and please consider using the latest TF version from the conversion stage to the inference stage. Your issue might have been resolved already in the recent version.", "I have gotten a very similar error when converting models with TFLite and running with TensorFlow 2.4.  TensorFlow versions 2.3 and 2.5 pre-release work for me, I'd recommend trying these versions.", "I have tried TFv2.3 and TFnightly versions. None of them works for me. \r\n", "@neso613 Please share simple stand alone code such that we can replicate the issue faced or if possible share a colab gist. It is really impossible to reproduce your problem at our end", "It is running now, but with Flex dependecny. Any solution for this flex issue?", "Please file a feature request for the Select TF ops by adding a new issue. Thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48915\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48915\">No</a>\n"]}, {"number": 48914, "title": "MIRROR PAD result mixed", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 20\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (or github SHA if from source):2.4\r\n\r\n\r\n\r\nWhen I tested the mirror pad, I found that the result was a bit unexpected.\r\nThe input shape of pad is [1,1,2,2], paddings[[0, 0], [0, 0], [3, 3], [3, 3]],pad mode: SYMMETRIC.\r\nI know that my parameters may not be in line with the correct usage of pad, but I want to know whether his result is caused by a bug or if I run it incorrectly.\r\n\r\n### model [file](https://www.notion.so/tflite-pad-symmetric-mode-bb313fc05adc497cae4d719896da70f4)\r\n\r\n###The wrong result:\r\n![image](https://user-images.githubusercontent.com/39184746/117141626-588fd480-ade1-11eb-8bd3-4c57e341d761.png)\r\n\r\nIf i had took a mistake, please tell me how to fix it ! THX :p", "comments": ["Paddings must be no greater than the dimension size.\r\nHere, your paddings is bigger, that leads to incorrect results.", "@thaink Got it, I checked the documentation, it did mention this, but I ignored it.   THX !"]}, {"number": 48912, "title": "Question about tf.distribute.strategy", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch\r\n\r\n## Description of issue (what needs changing):\r\nConfused on how the losses get added up\r\n\r\n### Clear description\r\n\r\nI had one additional question for this line in documentation  (training_loss.update_state(loss * strategy.num_replicas_in_sync)\r\nWhy do we multiply the loss by number of replicas. I understand the gradients get added up but what happens to the loss. Does that get added up?\r\nIf not, s this randomly choosing the loss on one replica and multiplying by the number of replicas?\r\n\r\n```\r\n@tf.function\r\ndef train_multiple_steps(iterator, steps):\r\n  \"\"\"The step function for one training step\"\"\"\r\n\r\n  def step_fn(inputs):\r\n    \"\"\"The computation to run on each TPU device.\"\"\"\r\n    images, labels = inputs\r\n    with tf.GradientTape() as tape:\r\n      logits = model(images, training=True)\r\n      loss = tf.keras.losses.sparse_categorical_crossentropy(\r\n          labels, logits, from_logits=True)\r\n      loss = tf.nn.compute_average_loss(loss, global_batch_size=batch_size)\r\n    grads = tape.gradient(loss, model.trainable_variables)\r\n    optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))\r\n    training_loss.update_state(loss * strategy.num_replicas_in_sync)\r\n    training_accuracy.update_state(labels, logits)\r\n\r\n  for _ in tf.range(steps):\r\n```\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\nyes\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\nyes\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\nyes\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\nYes\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nYes\r\n\r\n### Request visuals, if applicable\r\nNot applicable\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["cc @MarkDaoust @lamberta (a docs-related question about https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch)", "@rohanmuplara,\r\nCan you please give reference to the **`Tensorflow Documentation`** where the code you specified, \r\n`training_loss.update_state(loss * strategy.num_replicas_in_sync ` is mentioned? Thanks!", "@rohanmuplara @rmothukuru The code snippet may be from the [Use TPUs](https://www.tensorflow.org/guide/tpu#improving_performance_by_multiple_steps_within_tffunction) guide instead of [Writing a training loop from scratch](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch). \r\n\r\nDo let us know if your question about `training_loss.update_state(loss * strategy.num_replicas_in_sync)` is about the code in the TPU guide. Thank you!", "@8bitmp3 @rmothukuru sry I had posted wrong link. https://www.tensorflow.org/tutorials/distribute/custom_training", "> @8bitmp3 @rmothukuru sry I had posted wrong link. https://www.tensorflow.org/tutorials/distribute/custom_training\r\n\r\n@rohanmuplara,\r\nCode is not present even in the link you specified,  https://www.tensorflow.org/tutorials/distribute/custom_training. Am I missing something?", "Hey sorry @rmothukuru I was completely wrong again and @8bitmp3 was right. I had another question on that link. This is the right link https://www.tensorflow.org/guide/tpu#distribution_strategies.  I have also attached a screen recording of my question.  https://share.descript.com/view/lh1VoY2ojQI", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", " @rmothukuru following up here", "Also feel free to ask this or any other question on the TensorFlow Forum - https://discuss.tensorflow.org/ - we have members of the TF community, core TF, and TF DevRel there \ud83d\udc4d ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48912\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48912\">No</a>\n"]}, {"number": 48911, "title": "Improve docstrings for strings.lower() and strings.upper()", "body": "I am adding a description of `input` and `encoding` parameters for strings.lower() and strings.upper() functions.\r\nThe set of allowed values for `encoding` seems to be limited to '' and 'utf-8' only.\r\n\r\nThis PR is related to this issue: https://github.com/tensorflow/tensorflow/issues/48551\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48911) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "Windows build on CI is failing. I briefly looked at the build logs. It does not look like a problem with my PR. How should I proceed?", "I agree it's unrelated. It won't block the merge, there's nothing you need to do. Merging can still take a bit, but if it's more than a week or so please do ping the thread."]}, {"number": 48910, "title": "How does the tf.raw_ops.Timestamp operator get unix time? Can we manipulate the timestamp by changing device time for tf.raw_ops.Timestamp operator?", "body": "According to tensorflow documentation, the tf.raw_ops.Timestamp operator returns the timestamp as a float64 for seconds since the Unix epoch. I got a question when using the tf.raw_ops.Timestamp operator: what if users change the device time when they do inference? Can we manipulate the timestamp it returns by change the system time? Specifically, if a user do inference on a phone and changes the time of his phone, can this operator still return the current time?\r\n\r\nA follow-up question: how does the tf.raw_ops.Timestamp operator get Unix time? From software (e.g, phone's operating system) or hardware?\r\n\r\nJaesung Chung from tflite team has kindly pointed the [source file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/default/env_time.cc#L25) of how tf gets the time. But I found it hard to answer my questions from the file. Code in the source file:\r\n\r\n```\r\n#include <sys/time.h>\r\n#include <time.h>\r\n\r\n#include \"tensorflow/core/platform/env_time.h\"\r\n\r\nnamespace tensorflow {\r\n\r\n/* static */\r\nuint64 EnvTime::NowNanos() {\r\n  struct timespec ts;\r\n  clock_gettime(CLOCK_REALTIME, &ts);\r\n  return (static_cast<uint64>(ts.tv_sec) * kSecondsToNanos +\r\n          static_cast<uint64>(ts.tv_nsec));\r\n}\r\n\r\n}  // namespace tensorflow\r\n```", "comments": ["`CLOCK_MONOTONIC` is not affected but `CLOCK_REALTIME`:\r\n```\r\nThis clock is affected by discontinuous jumps in the\r\n              system time (e.g., if the system administrator manually\r\n              changes the clock), and by the incremental adjustments per\u2010\r\n              formed by adjtime(3) and NTP.\r\n```\r\nSee more at https://man7.org/linux/man-pages/man3/clock_gettime.3.html", "Thanks for your answer. So the tf.raw_ops.Timestamp operator will be affected if users manually changed the system time.\r\n\r\nA follow-up question: is there any operator in tensorflow that acquires unix time by `CLOCK_MONOTONIC` which is not affected by user manual time changing?", "I don't think so, but you can `grep` in the source code.", "Thanks for your information!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48910\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48910\">No</a>\n"]}, {"number": 48909, "title": "[XLA/GPU] Improve h-loop-fusion by changing its traversal order.", "body": "Switch to traverse from use to def. Bitcasts are placed after h-fusions to\r\nresolve shape mismatch but bitcasts could prevent future h-fusion from\r\nhappening. So, a bottom-up, use-to-def order should be more favorable. It\r\nalso helps to save compiler iterations to reach the fixed point. See\r\nHorizontalLoopFusionTest::TraversalOrder for an example.\r\n\r\nWe see ~7% gain from the LAMB training optimizer in a real model.", "comments": ["@cheshire could you help to take a look of this? Thanks!"]}, {"number": 48908, "title": "tflite::Intepreter move constructor", "body": "`tflite::Interpeter` doesn't have a move constructor.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/interpreter.h\r\n\r\nBecause of this it's more difficult to make a resource pool of `Interpeter`s which is itself desirable because `Interpeter` is not itself threadsafe, and the critical piece which is needed once-per-thread, and is simultaneously expensive to build.\r\n\r\n**System information**\r\n- TensorFlow version (you are using): latest\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently move is not possible. Move would is desirable.\r\n\r\n**Will this change the current api? How?**\r\nAdds constructor to `tflite::Interpeter` so technically it's an API change, but it's backward compatible.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone attempting to use many `tflite::Interpeter`s in parallel.\r\n\r\n**Any Other info.**\r\nFirst pass glance at the code suggests that implementation likely be trivial.\r\n", "comments": ["@thaink could you take a look at this feature request?", "@catskul Interpreter is almost always created as std::unique_ptr<Interpreter>. How about using that for the pool?", "That certainly works, and is what I'm doing now, but the pool hands out handles which are effectively `unique_ptrs` and ends up creating an awkward double dereference, and causes generic code to need to special case `tflite::Intepreter` instead of treating it like all the other types.\r\n\r\nCertainly not the end of the world not to have this, but it would be a nice-to-have.\r\n\r\n", "@jdduke What do you think about this?", "Given that the `InterpreterBuilder` only allows creation of an Interpreter on the heap (with `unique_ptr`), it's not clear  how a move constructor would benefit this. That is, an `Interpreter` should always be held and passed around via `unique_ptr`, which also lets you move them if appropriate. Your pool of `Interpreter` instances could just be a pool of `unique_ptr<Interpreter>` instances. Does that make sense?", "It makes sense, and is currently what I'm doing. It just forces an extra layer of indirection: `(*interpreterResource)->AllocateTensors()` where the code normally just does `interpreterResource->AllocateTensors()` isn't too bad on it's own, but where it intersects with templates/generic code which is expecting a single layer of indirection, then I need to special case with SFINAE which I'd rather not do when I don't have to.\r\n\r\nNBD if this is seen as unnecessary maintenance, churn, or is otherwise undesirable though. I'll can make do the way it is certainly.\r\n\r\nWith regard to how it would be used given the builder's interface requiring a `unique_ptr`. I was planning on moving the `Interpeter` out of the `unique ptr` this way:\r\n\r\n```\r\n// pseudo-code\r\nauto creationFn = [&]() -> auto {\r\n    auto myInterpeterPtr = unique_ptr<Interpeter>{};\r\n    auto status = builder(&myInterpeterPtr);\r\n    assert(status == kTfLiteOk);\r\n    assert(myInterpeterPtr != nullptr);\r\n    return std::move(*myInterpreterPtr);\r\n}\r\nauto resourcePool = ResourcePool(defaultNumOfResources, creationFn);\r\n```", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "would a PR be welcome here? I'm don't currently have my dev env set up to make it easy to create/test appropriately to make a PR, but don't want to put in the effort if it's not wanted.", "> would a PR be welcome here? I'm don't currently have my dev env set up to make it easy to create/test appropriately to make a PR, but don't want to put in the effort if it's not wanted.\r\n\r\nI'm sympathetic, but at this point, probably not. In part as I don't think we event want to have an Interpreter object left in an indeterminate/undefined state, and we also probably wouldn't want to expose a move assignment operator, so the use-case here is fairly narrow. If you hit issues with your particular application that you can't work around, we can revisit. At a technical level, I don't think the implementation of the move operator should be all that difficult. Thanks for the feature suggestion though!"]}, {"number": 48907, "title": "Compact Multi-Class Boosted Trees", "body": "<em>The issue related to the performance of the [BoostedTree](https://github.com/tensorflow/estimator/blob/781c0d30c6bf100aa174591dd97cb70fc39d294d/tensorflow_estimator/python/estimator/canned/boosted_trees_test.py#L403) classifier in terms of accuracy.</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution: Linux Ubuntu 16.04, Windows 10\r\n- TensorFlow installed from (source or binary): pip install TensorFlow \r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.6\r\n\r\n**Describe the model**\r\nBoostedTree classifier is a model introduced by [N Ponomareva, T Colthurst, G Hendry.2017](https://ieeexplore.ieee.org/abstract/document/8257910).\r\nIt is built over the Xgboost idea and learning is done through building one layer of decision tree regressor over N boosting iteration.\r\n\r\n\r\n**Describe the expected behavior**\r\nAs the related paper discovered, for ten trees, the accuracy of the xgboost is about 60%, and for the TF model is 73%.\r\nThis experiment had done over the Cover type dataset.\r\n**Describe the current behavior**\r\nI did the same [experiments](https://github.com/samanemami/TFBoostedTree/blob/main/examples/Comparison.ipynb) over the same dataset and identical configuration, but the results have a huge difference.\r\nIn this experiment, the accuracy of xgboost is about 70%, and the TF model score is about 58 percent.\r\nIt is worthy to note the fact the logistic regression at least achieves 70 percent.\r\n\r\nAbout the training time, if you plot the training time over boosting iteration you will have a Neutral trend which should be linear as the gradient boosting is a linear ensemble mode.\r\n\r\n**Standalone code to reproduce the issue**\r\nI reproduced the model [here](https://github.com/samanemami/TFBoostedTree/blob/main/TFBT.py).\r\nAlso, you may find the related experiments [here](https://github.com/samanemami/TFBoostedTree/tree/main/examples).", "comments": ["@samanemami  I tried to reproduce the issue  but facing error while importing data. Please check the gist [here](https://colab.research.google.com/gist/saikumarchalla/848e49f1a2d1e1799c88c8183496599c/comparison.ipynb).", "@saikumarchalla Please find the modified gist [here](https://colab.research.google.com/gist/samanemami/b1a4797ad00e6c84c61a9a00fc363127/comparison.ipynb)", "Was able to reproduce the issue using TF version [2.4](https://colab.research.google.com/gist/saikumarchalla/449304216e9f97f6a14ecf22df52ef9f/comparison.ipynb#scrollTo=advanced-andrews) and [Nightly](https://colab.research.google.com/gist/saikumarchalla/305dda77526c4903f5b612105ee0466d/comparison.ipynb) version. Thanks!", "After running different experiments, I found that you reduced the tree number in XGBoost, hence, it reduced the XGBoost accuracy."]}, {"number": 48906, "title": "Avoid empty fallback list for cudnn v8.2", "body": "This PR bumps up the cudnn frontend repo commit to v0.2 to resolve the issue of returning empty fallback list when cudnn == 8.2. In addition, the patch file is reformatted to contain more info.\r\n\r\nhttps://github.com/NVIDIA/cudnn-frontend/releases/tag/v0.2\r\n\r\n\r\ncc. @nluehr ", "comments": ["Just noticed there are two lines of unnecessary changes in the patch file. Removed them. PTAL @timshen91 ", "I think this PR can be replaced by https://github.com/tensorflow/tensorflow/pull/49244 which upgrades the cudnn frontend to v0.3 and adds some related code change. So closing this one."]}, {"number": 48905, "title": "[determinism] Add GPU excepts, CPU d9m, and tests to crop_and_resize", "body": "This current PR adds the following functionality, plus associated tests:\r\n\r\nWhen `TF_DETERMINISTIC_OPS` is set to `\"true\"` or `\"1\"` (when op-determinism is expected),\r\n\r\n1. the gradient w.r.t `image` of `tf.image.crop_and_resize` when running on CPU will be bit-exactly reproducible (from run-to-run), and\r\n2. an attempt to use the GPU kernels for gradient w.r.t either `image` or `boxes` of `tf.image.crop_and_resize` will cause a `tf.errors.UnimplementedError` to be thrown along with an understandable message.\r\n\r\nThis current PR is associated with [RFC: Enabling Determinism in TensorFlow](https://github.com/tensorflow/community/blob/master/rfcs/20210119-determinism.md).\r\n\r\ncc @sanjoy, @reedwm, @nluehr ", "comments": []}]