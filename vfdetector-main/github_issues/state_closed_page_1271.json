[{"number": 15006, "title": "tf.contrib.data: tf-slim training pipeline  raise  GetNext() failed because the iterator has not been initialized.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n```\r\n        dataset = get_dataset()\r\n        iterator = tf.contrib.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)\r\n        train_init_op = iterator.make_initializer(dataset)\r\n\r\n        inputs, labels = iterator.get_next()\r\n        prob, end_points = model.nldf(inputs, labels)\r\n        total_loss = tf.losses.get_total_loss()\r\n        optimizer = tf.train.AdamOptimizer(1e-6)\r\n        train_op = slim.learning.create_train_op(\r\n                          total_loss,\r\n                          optimizer,\r\n                          clip_gradient_norm=FLAGS.max_grad_norm)\r\n        def init_fn(sess):\r\n            sess.run(train_init_op)\r\n        slim.learning.train(\r\n                        train_op,\r\n                        FLAGS.checkpoint_dir,\r\n                        init_fn=init_fn,\r\n                        number_of_steps=1000,\r\n                        save_summaries_secs=300,\r\n                        save_interval_secs=600)\r\n```\r\nis going to fail with  \r\n\r\n```\r\nFailedPreconditionError (see above for traceback): GetNext() failed because the iterator has not been initialized. Ensure that you have run the initializer operation for this iterator before getting the next element.\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,352,352,3], [?,176,176,?]], output_types=[DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Iterator)]]\r\n```\r\n\r\n== cat /etc/issue ===============================================\r\nDarwin MTL-PengYu 16.7.0 Darwin Kernel Version 16.7.0: Wed Oct  4 00:17:00 PDT 2017; root:xnu-3789.71.6~1/RELEASE_X86_64 x86_64\r\nMac OS X 10.12.6\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 8.0.0 (clang-800.0.42.1)\r\nTarget: x86_64-apple-darwin16.7.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n\r\n== uname -a =====================================================\r\nDarwin MTL-PengYu 16.7.0 Darwin Kernel Version 16.7.0: Wed Oct  4 00:17:00 PDT 2017; root:xnu-3789.71.6~1/RELEASE_X86_64 x86_64\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.3)\r\nprotobuf (3.4.0)\r\ntensorflow (1.3.0)\r\ntensorflow-serving-api (1.3.0)\r\ntensorflow-tensorboard (0.1.8)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.3.0\r\ntf.GIT_VERSION = v1.3.0-rc2-20-g0787eee\r\ntf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\ntf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n('v1.3.0-rc2-20-g0787eee', '1.3.0')\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n`init_fn` of `slim.learning.train` doesn't  init the `tf.contrib.data.Iterator` properly\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nhttps://github.com/yupbank/NLDF-tf/blob/make-multi-gpu/trainer/multi_gpu_task.py#L148", "comments": ["fixed by pass the train_init_op into local_init_op ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 15005, "title": "Make build rule for //tensorflow/contrib/lite/tools:benchmark_model", "body": "Resolve issue https://github.com/tensorflow/tensorflow/issues/14581\r\nNote that this benchmark_model.cc is not completed yet. It doesn't\r\nload and models.\r\n\r\n1. With proper Android SDK and NDK settings in WORKSPACE, we can\r\n   build armeabi-v7a or arm64-v8a binary, e.g.,\r\n\r\n   > bazel build --cxxopt='--std=c++11' \\\r\n     --crosstool_top=//external:android/crosstool \\\r\n     --cpu=arm64-v8a \\\r\n     --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n    //tensorflow/contrib/lite/tools:benchmark_model\r\n\r\n2. It's also possible to build this for Linux or OS X, e.g.,\r\n\r\n   >  bazel --config opt --cxxopt='--std=c++11' \\\r\n     //tensorflow/contrib/lite/tools:benchmark_model", "comments": ["Can one of the admins verify this patch?", "@aselle WDYT?", "Jenkins, test this please."]}, {"number": 15004, "title": "compile error with cmake for windows 10 and GPU enabled", "body": "Hi,\r\nI am trying to compile the tensorflow.dll with cmake for windows 10 and GPU enabled. However, the it failed because of the following error:\r\n```\r\n\" C:\\Users\\mcuevas\\bin\\tensorflow\\tensorflow\\tensorflow/stream_executor/stream.h(1921): error C2065: 'tf_shared_lock\r\n': undeclared identifier (compiling source file C:\\Users\\mcuevas\\bin\\tensorflow\\tensorflow\\tensorflow\\core\\grappler\\\r\ndevices.cc) [C:\\Users\\mcuevas\\bin\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build_GPU\\tf_core_cpu.vcxproj]\r\n  C:\\Users\\mcuevas\\bin\\tensorflow\\tensorflow\\tensorflow/stream_executor/stream.h(1921): error C2146: syntax error: m\r\nissing ';' before identifier 'lock' (compiling source file C:\\Users\\mcuevas\\bin\\tensorflow\\tensorflow\\tensorflow\\cor\r\ne\\grappler\\devices.cc) [C:\\Users\\mcuevas\\bin\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build_GPU\\tf_core_cpu.vc\r\nxproj]\r\n  C:\\Users\\mcuevas\\bin\\tensorflow\\tensorflow\\tensorflow/stream_executor/stream.h(1921): error C2065: 'lock': undecla\r\nred identifier (compiling source file C:\\Users\\mcuevas\\bin\\tensorflow\\tensorflow\\tensorflow\\core\\grappler\\devices.cc\r\n) [C:\\Users\\mcuevas\\bin\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build_GPU\\tf_core_cpu.vcxproj]\r\n  C:\\Users\\mcuevas\\bin\\tensorflow\\tensorflow\\tensorflow/stream_executor/stream.h(1921): error C2065: 'tf_shared_lock\r\n': undeclared identifier (compiling source file C:\\Users\\mcuevas\\bin\\tensorflow\\tensorflow\\tensorflow\\core\\common_ru\r\nntime\\gpu\\gpu_cudamalloc_allocator.cc) [C:\\Users\\mcuevas\\bin\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build_GP\r\nU\\tf_core_cpu.vcxproj]\r\n  C:\\Users\\mcuevas\\bin\\tensorflow\\tensorflow\\tensorflow/stream_executor/stream.h(1921): error C2146: syntax error: m\r\nissing ';' before identifier 'lock' (compiling source file C:\\Users\\mcuevas\\bin\\tensorflow\\tensorflow\\tensorflow\\cor\r\ne\\common_runtime\\gpu\\gpu_cudamalloc_allocator.cc) [C:\\Users\\mcuevas\\bin\\tensorflow\\tensorflow\\tensorflow\\contrib\\cma\r\nke\\build_GPU\\tf_core_cpu.vcxproj]\"\r\n```\r\nThese are the commands I use:\r\ncmake .. -A x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=C:\\Users\\mcuevas\\bin\\swigwin-3.0.12\\swigwin-3.0.12\\swig.exe -DPYTHON_EXECUTABLE=C:\\Users\\mcuevas\\AppData\\Local\\Continuum\\anaconda3\\envs\\acsis_cpu\\python.exe -DPYTHON_LIBRARIES=C:\\Users\\mcuevas\\AppData\\Local\\Continuum\\anaconda3\\pkgs\\python-3.5.2-0\\libs\\python35.lib -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX2 -Dtensorflow_ENABLE_GPU=ON -DCUDNN_HOME=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\"\r\n\r\nMSBuild /p:Configuration=Release tf_tutorials_example_trainer.vcxproj\r\n\r\nThanks in advance.\r\n\r\nMy system:\r\nWindows 10\r\nCuda 8.0\r\nCudnn 6\r\ncmake cmake-3.9.4-win64-x64\r\nPython 3.5.2\r\nVS2015", "comments": ["Which version of tensorflow are you compiling? `stream.h` of master branch doesn't match the error log. You can try to update to a newer version.", "I downloaded Release 1.4.0 and it worked just fine, thank you. "]}, {"number": 15003, "title": "msvcp140.dll missing on win 7", "body": "Hi everyone :)\r\n\r\nI installed Tensorflow on Windows 7 ultimate with Python 3.5.4, via the `pip3 install --upgrade tensorflow` command. It even stated it successfully installed version 1.4\r\n\r\nBut if I try to invoke `import tensorflow as tf` in the shell I get the following error:\r\n![grafik](https://user-images.githubusercontent.com/33151777/33433308-5b8fee18-d5db-11e7-8ddf-7037bae45e11.png)\r\n \r\nOf course I installed the recommended Visual C++ 2015 Redistributablle 64bit Update 3, but there is neither the .dll file in my system folder, nor does tensorflow work. I get the same error as before.\r\n\r\nAny help is appreciated, Thank you :)\r\n", "comments": ["@simonhuber95 This sounds like a problem that's specific to your machine or installation, rather than a problem with TensorFlow.  I'd suggest investigating online for `msvcp140.dll` problems.\r\n\r\nE.g. this StackOverflow issue suggests downloading `Visual C++ Redistributable for Visual Studio 2015`:\r\nhttps://stackoverflow.com/questions/32998902/msvcp140-dll-missing\r\n\r\nClosing this out, since there isn't much we can do on the TensorFlow side to fix this."]}, {"number": 15002, "title": "fatal error: cuda/include/cuda.h: No such file or directory", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes, I followed the official documentation for custom operations.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n\r\n```\r\nLinux pc 4.4.0-81-generic #104-Ubuntu SMP Wed Jun 14 08:17:06 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\n  tried both\r\n- **TensorFlow version (use command below)**:\r\n\r\n| version                               | status        | comment |\r\n| -------------                         | ------------- | -----|\r\n| ('v1.2.0-rc1-7529-g8a4d849', '1.4.0') | not working   | from pip or from source |\r\n| ('v1.2.0-rc2-21-g12f033d', '1.2.0')   | working       | from [pypip](https://pypi.python.org/packages/4b/d3/d4fe94f4f370fbb3790444b8f0a007298a795c447e195d88a066de04930d/tensorflow-1.2.0-cp27-cp27mu-manylinux1_x86_64.whl#md5=7e1640384dfd705e17b2f668c1b0427b) |\r\n\r\n- **Python version**: 2.7 (*irrelevant*)\r\n- **Bazel version (if compiling from source)**:\r\n\r\n```\r\nBuild label: 0.6.1\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Oct 5 21:54:59 2017 (1507240499)\r\nBuild timestamp: 1507240499\r\nBuild timestamp as int: 1507240499\r\n```\r\n\r\n- **GCC/Compiler version (if compiling from source)**: *irrelevant*\r\n  tried both\r\n  - g++4.8\r\n  - g++5.0\r\n- **CUDA/cuDNN version**: *irrelevant*\r\n\r\n```\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2016 NVIDIA Corporation\r\nBuilt on Sun_Sep__4_22:14:01_CDT_2016\r\nCuda compilation tools, release 8.0, V8.0.44\r\n```\r\n\r\n- **GPU model and memory**: irrelevant\r\n- **Exact command to reproduce**:\r\n\r\n```bash\r\ncd /tmp\r\nmkdir tf_issue\r\ncd tf_issue\r\nvirtualenv test\r\nsource test/bin/activate\r\npip install tensorflow # in some way: either tensorflow-gpu or from wheel package created by bazel\r\ngit clone https://github.com/cgtuebingen/tf_custom_op\r\ncd tf_custom_op\r\ncmake .\r\nmake\r\n```\r\n\r\n### Describe the problem\r\nCompiling custom ops with\r\n\r\n```cpp\r\n#include \"tensorflow/core/util/cuda_kernel_helper.h\"\r\n```\r\n\r\nfails due to missing files\r\n\r\n\r\n```\r\n/code/lib/python2.7/site-packages/tensorflow/include/tensorflow/core/util/cuda_kernel_helper.h:24:31: fatal error: cuda/include/cuda.h: No such file or directory\r\n```\r\n\r\nThis file `cuda/include/cuda.h` does not exists! Neither in the pip package nor in the git repository.\r\nRemoving `#include \"tensorflow/core/util/cuda_kernel_helper.h\"` \r\n\r\ngives plenty of other issues\r\n\r\n```\r\n/code/kernels/matrix_add_kernel.cu(13): error: namespace \"tensorflow\" has no member \"CudaLaunchConfig\"\r\n/code/kernels/matrix_add_kernel.cu(59): error: namespace \"tensorflow\" has no member \"CudaLaunchConfig\"\r\n/code/kernels/matrix_add_kernel.cu(59): error: expected a \";\"\r\n/code/kernels/matrix_add_kernel.cu(63): error: identifier \"cfg\" is undefined\r\n/code/kernels/matrix_add_kernel.cu(88): error: namespace \"tensorflow\" has no member \"CudaLaunchConfig\"\r\n/code/kernels/matrix_add_kernel.cu(88): error: expected a \";\"\r\n/code/kernels/matrix_add_kernel.cu(92): error: identifier \"cfg\" is undefined\r\n```\r\n\r\nAs I already wrote in a related issue #12860, the commit 2c598e874e6a7b6b3185846ce9bac97a7d5d0169 is causing this issue by changing\r\n\r\n```diff\r\n-#include \"third_party/gpus/cuda/include/cuda.h\"\r\n+#include \"cuda/include/cuda.h\"\r\n```\r\n\r\nCopying the old `cuda.h` gives\r\n\r\n```\r\n[...]/local/lib/python2.7/site-packages/tensorflow/include/tensorflow/core/platform/default/mutex.h:25:22: fatal error: nsync_cv.h: No such file or directory\r\n```\r\n\r\nwhich does not exist, too.\r\n\r\nThis problem is not related to custom code, it is related to ignore/omitting files in commit 2c598e874e6a7b6b3185846ce9bac97a7d5d0169\r\n\r\nAs mention in #12860, this affects many people. In fact, the entire way of writing customs ops with CUDA seems to be broken. Copying own source-code to the TensorFlow-repo was not necessary until TF1.3. Interestingly, even recent NIPS paper implementations state in their readme, they only support TFv1.2. I don't think the proposed workaround of downgrading to TFv1.2 should be the way to go.", "comments": ["@allenlavoie, could you please take a look at resolving this issue as you have commented on #12860.", "We should import some GPU headers in our custom op test once this gets fixed, but I have no idea how the CUDA headers should be packaged and unfortunately I don't have time to dig in to it.\r\n\r\n@flx42 @gunan know anyone who can work on our header packaging for CUDA custom ops? Not quite sure who broke it, but it would be good to have fixed.", "Amit worked on similar issues before. Could you take a look?", "I get the same error when compiling master (18c864cd) with the CMake build without support for CUDA.", "This is my bad. root cause is my change: https://github.com/tensorflow/tensorflow/pull/15298\r\nWill try to look into this once I have the nvcc bug workaround tests complete.\r\n", "Sorry for the noise, I got confused. My change broke this a little more, but there seems to be missing headers in our packages too.", "Thanks for info!", "Any update?", "Cmake build is community supported except for windows.\r\non windows, all seems to be working at master now, so there is nothing to do there for us.\r\nhttp://ci.tensorflow.org/job/tf-master-win-gpu-cmake/1373/\r\n\r\nThe initial reported issue, what happens if you add CUDA include paths to your compile command? I am not sure if TF should package cuda.h, so it may be better to make your command use the cuda headers installed on your system.", "@gunan: I have installed T.F 1.4 from pip3 in ubuntu 16.04, cuda 8.0 and I also have same error. Could you tell me how to fix it in ubuntu\r\n\r\n```/usr/local/lib/python3.5/dist-packages/tensorflow/include\r\nnvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\r\nIn file included from deform_conv.cu.cc:70:0:\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/util/cuda_kernel_helper.h:24:31: fatal error: cuda/include/cuda.h: No such file or directory\r\n #include \"cuda/include/cuda.h\"\r\n   ```                            \r\n", "What happens if you add `-I/usr` or wherever your cuda installation is to your build command?", "One of the related problems here is we seem to be missing `cuda_config.h` in our pip package.\r\n@av8ramit We should cuda_config.h to the pip package.\r\nThen we can revise the instructions on how to make sure compiler can locate cuda headers on user's system.", "Parts can be resolved by adding some additional include-paths\r\n\r\nSome updates on this issue:\r\n- adding `include_directories(\"/opt/path/to/cuda/toolkit_current/\")`\r\nresolves the missing `cuda.h` file-error, but gives `fatal error: nsync_cv.h: No such file or directory` which is related to issue #14620, #12482\r\n- adding `include_directories(\"$HOME/.local/lib/python2.7/site-packages/tensorflow/include/external/nsync/public/\")`\r\nresolves the missing `nsync_cv.h` file-error, but gives `fatal error: cuda/cuda_config.h: No such file or directory`\r\n\r\nAdding all these additional include-paths is not very comfortable. TF v1.2 was much easier to work with.\r\n\r\nAnd this `cuda/cuda_config.h` is now really missing.\r\n\r\n**edit:** resolved it by\r\n`include_directories(\"/opt/tensorflow/third_party/toolchains/gpus/cuda/\")`\r\nBut this requires an installation from source (or at least a git clone of the source).\r\nwhich now leads to an (unrelated) ABI issue. Still TF v1.2 was easier to work with.\r\n\r\nThis might be related to the output warnings\r\n```\r\nwarning: no files found matching '*.dll' under directory '*'\r\nwarning: no files found matching '*.lib' under directory '*'\r\nwarning: no files found matching '*.h' under directory 'tensorflow/include/tensorflow'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/Eigen'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/external'\r\nwarning: no files found matching '*.h' under directory 'tensorflow/include/google'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/third_party'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/unsupported'\r\n```\r\nduring the compilation of TensorFlow.", "CC @martinwicke for overall issues with building custom ops.\r\nWe are adding cherrypicks to 1.5 release to include cuda_config.h in our released pip packages. CC @av8ramit for that.\r\nfor nsync header files, I think we should have an easier way, or at least instructions to include the include direcotory? CC @m3bm3b  for that.\r\nFor cuda, I think we should add that to instructions that including CUDA path is necessary.", "> for nsync header files, I think we should have an easier way, or at least instructions to \r\n> include the include direcotory? CC @m3bm3b for that.\r\n\r\nFor the nsync header files:\r\n\r\nLooking at \r\n    https://github.com/cgtuebingen/tf_custom_op/blame/master/matrix_add/CMakeLists.txt\r\nit looks like that CMakeLists.txt has already been changed (a few hours ago) to do\r\n    include_directories(\"${TF_INC}/external/nsync/public/\")\r\nwhich helps, but only for uses of cgtuebingen/tf_custom_op\r\n\r\nWe could certainly add a sentence to the documentation for people\r\nusing other build descriptions.\r\nIt's unclear to me how to automate a process beyond that, \r\nbecause each build description will be different.   \r\n\r\nAt some cost, we could avoid the need to include the nsync headers.\r\n\r\nTo do it, we would change \r\n      tensorflow/core/platform/default/mutex.h\r\nand add a new\r\n      tensorflow/core/platform/default/mutex.cc\r\nAll the routines would be re-written to be out of line in the .cc file \r\n(not inline in the .h file), and the nsync_{mu,cv} fields in the .h file \r\nwould be replaced by an array of two pointer-sized values, e.g.:\r\n      void *mu_[2];\r\nThis would work because nsync's mutex and condition variable\r\nfit in that space and need no better than pointer alignment. \r\n(And the plan is that those facts will never change.)\r\n\r\nThe downsides are:\r\n    - by making the routines no longer be inline, the mutex fast path\r\n      will be a few nanoseconds slower.\r\n    - it's an abstraction violation for tensorflow code \r\n      to know that nsync's types have those sizes.\r\nBut those disadvantages must be weighed against the annoyance \r\nof having to include the nsync headers when using binary distributions.\r\n", "FYI everything works now fine using a [Makefile](https://github.com/cgtuebingen/tf_custom_op/blob/5072c836eae8e1cda5a0a1a4c64310c6b9386f8f/Backup/Makefile) or [CMake](https://github.com/cgtuebingen/tf_custom_op/blob/5072c836eae8e1cda5a0a1a4c64310c6b9386f8f/matrix_add/CMakeLists.txt) on the master branch at commit https://github.com/tensorflow/tensorflow/commit/4cb0c13c7779da536cac6c682180c5757611b384\r\n\r\nMost parts can be hidden from the user.\r\nThe only missing thing is indeed the `cuda_config.h` (see Line in [CMakeLists.txt](https://github.com/cgtuebingen/tf_custom_op/blob/5072c836eae8e1cda5a0a1a4c64310c6b9386f8f/matrix_add/CMakeLists.txt#L40)) which must be hard-coded until (#12860) is resolved.\r\n\r\nBtw, this missing file only contains:\r\n```cpp\r\n// DO NOT EDIT: automatically generated file\r\n#ifndef CUDA_CUDA_CONFIG_H_\r\n#define CUDA_CUDA_CONFIG_H_\r\n\r\n#define TF_CUDA_CAPABILITIES CudaVersion(\"3.0\")\r\n\r\n#define TF_CUDA_VERSION \"8.0\"\r\n#define TF_CUDNN_VERSION \"5\"\r\n\r\n#define TF_CUDA_TOOLKIT_PATH \"/usr/local/cuda-8.0\"\r\n\r\n#endif  // CUDA_CUDA_CONFIG_H_\r\n```\r\n\r\nAnd neither `TF_CUDNN_VERSION` nor `TF_CUDA_TOOLKIT_PATH` is correct, although TensorFlow works correctly and I set the configs correctly during the compilation.\r\n  \r\nNot sure, if this resolves *this* issue of missing files in the pip package.\r\n\r\n", "We now build pip packages starting from 1.5.0rc1 with cuda_config.h included.", "Please comment if there are any other issues.", "Looking at the documentation, it looks like we recommend setting `-DGOOGLE_CUDA=1` when building the ops. I am surprised we need that. That macro then triggers `PLATFORM_GOOGLE` which should only turn true internally, and that causes the error we see about `cuda_config.h` externally, I think it may be something we would like to avoid.\r\n@martinwicke any ideas on `GOOGLE_CUDA` macro?", "GOOGLE_CUDA definitely should not trigger PLATFORM_GOOGLE. That sounds like a bug. Can we avoid that and see whether it still builds?", "@gunan I think you find the solution. I have just tried the new command without  -D GOOGLE_CUDA=1. I compiled a custom op with tf rc1.5 installed from source and pip. No error occurred and the custom op just passed the test on both cup and gpu. :laughing::laughing::laughing:\r\nAnyone help to try this solution with other versions of tf?", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Want just mention that the master-branch 0464602ee99ff2a10336f2bea12440167e2d8a70 is broken again, while the tensorflow-gpu package works fine.\r\n\r\nIs this feedback you want to see in the issues, or should I wait if the issue goes into the next relase?\r\n- missing `#include<cassert>` in ` /tensorflow/include/tensorflow/core/util/cuda_device_functions.h(196): error: identifier \"assert\" is undefined`\r\n- stack-trace: `/tensorflow/include/tensorflow/core/framework/tensor.h:692] Check failed: IsAligned()`", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I cleaned up the dependency to `cuda_config.h` in our headers.\r\n@PatWie Would you like to contribute the fixes you mentioned above? Or are they already fixed?", "These issues are gone. I tested 49c20c5814dd80f81ced493d362d374be9ab0b3e", "I'm running into the same issue today when trying to compile an op:\r\nI need to use \"tensorflow/core/util/cuda_kernel_helper.h\", which recursively includes \"cuda/include/cuda.h\", which does not exist.\r\nOn my system, cuda is not installed under a directory named exactly \"cuda\", so I need to hack this by soft links.\r\nAny better solutions?", "Messing up the system with symbolic links is usually a bad idea.\r\nThe root of all evil is the path `#include \"cuda/include/cuda.h\"` itself. Usually, you use `#include \"cuda.h\"` or `#include \"cuda_runtime.h\"`  when doing CUDA programming. \r\n\r\n### Solution CMake\r\nUsually `FindCuda.cmake` sets `CUDA_INCLUDE_DIRS` properly which relies on the environment variables `CUDA_PATH` and `CUDA_INC_PATH` and they are pointing directly to the header files:\r\n\r\nAny simple CMakeLists.txt should detect the correct path\r\n\r\n```cmake\r\ncmake_minimum_required( VERSION 2.8 )\r\n\r\nfind_package(CUDA 9 REQUIRED)\r\nmessage(STATUS ${CUDA_INCLUDE_DIRS})\r\nmessage(STATUS ${CUDA_TOOLKIT_INCLUDE})\r\n\r\n\r\n```\r\ngives\r\n\r\n```\r\n/usr/local/cuda/include\r\n/usr/local/cuda/include\r\n```\r\n\r\nSo any [easy fix](https://github.com/PatWie/tensorflow-cmake/blob/203efca40fb4f91a8922626dd76ecd0f6a41e66d/custom_op/user_ops/CMakeLists.txt#L19) in CMake is:\r\n```cmake\r\nfind_package(CUDA REQUIRED)\r\ninclude_directories(SYSTEM \"${CUDA_INCLUDE_DIRS}/../../\")\r\n# or probably better\r\ninclude_directories(SYSTEM \"${CUDA_TOOLKIT_INCLUDE}/../../\")\r\n```\r\n\r\nIf you are using some other toolkit location (I do), setting the env-var `CUDA_PATH` seems to be sufficient (see below).\r\n\r\n### Solution Makefile\r\n\r\nWhen not relying on CMake and you are writing a shell-script or plain Makefile, it it a good idea to specify the environment variables [`CUDA_PATH, CUDA_INC_PATH`](https://github.com/Kitware/CMake/blob/1fea56c3bd99be6c7a6bfaa1454ba67e7a04da72/Modules/FindCUDA.cmake#L782-L783)\r\n\r\n\r\n```bash\r\nexport CUDA_PATH=/path/to/cuda/toolkit_9.0/cuda\r\nexport CUDA_INC_PATH=${CUDA_PATH}/include\r\n```\r\n\r\nNow, something like\r\n```\r\nnvcc -I${CUDA_INC_PATH}../../\r\n```\r\nis sufficient.", "Like I said my cuda is not installed under a directory named \"cuda\", so none of the \"../../\" trick will work for me.", "Fixed this by specifying paths using -I while running the nvcc command. Another error popped up which required a cuda_config.h file. Removing -D GOOGLE_CUDA=1 solved this.", "Removing the flag simply removes the entire CUDA part which is not most people want to do.", "I don't understand. Why remove the -D GOOGLE_CUDA=1 flag? imho, removes this flag, the cuda kernel can't be run. I tried to remove this flag, and built a .so file, but my custom op is not registered on the GPU device. "]}, {"number": 15001, "title": "How can i read csv file by file name and line number in tensorflow?", "body": "I read some csv files using queue like [this](https://www.tensorflow.org/api_guides/python/reading_data#creating_threads_to_prefetch_using_queuerunner_objects).  Then, i get `key` and `value` when reading from file queues. The `key` contains file name and line number.\r\n\r\n```\r\n    filename_queue = tf.train.string_input_producer([\"file0.csv\", \"file1.csv\"]) \r\n    reader = tf.TextLineReader()\r\n    key, value = reader.read(filename_queue)\r\n```\r\n\r\n\r\nI train model using Dynamic negative sampling like [IRGAN code](https://github.com/xf4fresh/irgan/blob/master/ltr-gan/ltr-gan-pointwise/ltr_dns_nn.py#L41). I need sample from large scale negative samples, so i want to save the example id such as filename:line rather than feature itself. \r\n\r\nHowever, i can not find a suitable function on document.\r\nThanks for your attention!", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 15000, "title": "WIP: Support \"causal\" padding in tf.layers.convolutional.Conv1D", "body": "Fix #14933.\r\n\r\nBecause we don't see causal padding in other use cases expect of NTC, we choose to modify code at Conv1D, instead of `tf.nn.convolution`.\r\nFor simplicity, we hack the Conv1D's `__call__` method, instead of `build`, `call` and `_compute_output_shape`.\r\n\r\n### How to test\r\n\r\n+ [x] add test case.\r\n+ [ ]  pass all tests.", "comments": ["Can one of the admins verify this patch?", "Lukasz, Francois, I don't know enough about needs in 1D convolutions to know if this is useful and correct. Could you take a look?", "Thanks, @martinwicke . As @szpssky mentioned in #14933, the PR cannot work for keras, so I mark the PR as WIP.", "Ok, thanks!", "In general, it'd be very very good to have causal=\"LEFT\" padding in TF. This is the one reason why both T2T and Sonet re-implement conv layers..."]}, {"number": 14999, "title": "fix clip weights tests", "body": "using learning rate 1; variable value after the first update becomes zero resulting in failed tests. ", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Ping @sguada ", "Jenkins, test this please."]}, {"number": 14997, "title": "Add broadcasting for rank 6 Tensors", "body": "As per @aselle's comment in #14924, this bumps up the broadcasting dimension limit of Tensors from 5 to 6.\r\n\r\nI'm making this pull request to make it easy to merge this change if it is wanted; perhaps it is not wanted.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed the CLA.", "CLAs look good, thanks!\n\n<!-- ok -->", "@aselle can you check whether we want this? I assume yes, but you may have more context.", "@aselle WDYT?", "@aselle WDYT?", "Nagging Reviewer @aselle: It has been 41 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Should we close this just to get the notifications to go away? It's probably a design choice that tensors sometimes only broadcast until 5 dimensions.\r\n\r\nI'm going to close this, and if you disagree just open it again."]}, {"number": 14996, "title": "How to read image_data and fill image_tensor by tensorflow c++ apis", "body": "I want to infer a image by a trained model.first read a image as Mat by opencv,then fill plane_tensor with Mat.Does it work ? Is there any better way to the following code? waiting for some warming person ! thank you very much.\r\n\r\nMat img = imread(file_name);\r\nstd::vector<Mat> bgr;\r\nsplit(img, bgr);\r\ntensorflow::Tensor four_dim_plane(DT_UINT8, tensorflow::TensorShape({ 1, img.rows, img.cols, 3 }));\r\nauto plane_tensor = four_dim_plane.tensor<unsigned char, 4>();\r\nfor (int  k = 0; k < 3; ++k)\r\n{\r\n        for (int  i = 0; i < img.rows; ++i)\r\n\t{\r\n\t\tfor (int j = 0; j < img.cols; ++j)\r\n\t\t{\r\n\t\t   plane_tensor(0, i, j, k) = bgr[k].at<unsigned char>(i, j);\r\n\t\t}\r\n\t}\r\n}\r\ntensorflow::Status status3 = session->Run({ { \"image_tensor\", plane_tensor } },{ output_node },\r\n          {}, &outputs);\r\n\r\n**System information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows10 \r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version (use command below): TF1.4\r\nPython version:python3.5.2\r\nGCC/Compiler version (if compiling from source): vs2015\r\nCUDA/cuDNN version:cuda8.0/cudnn6.0\r\nGPU model and memory:TXT1080TI/11G\r\nBazel version\uff1a\r\nExact command to reproduce\uff1a**", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Leave a comment"]}, {"number": 14994, "title": "Tensorflow", "body": " >>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/sovietsky/anaconda2/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/sovietsky/anaconda2/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/sovietsky/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/sovietsky/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/sovietsky/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/sovietsky/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: /home/sovietsky/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: cuDevicePrimaryCtxRetain\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["Please fill out the issue template with all information required or we won't be able to help you and your issue will be closed. You may find this helpful: https://github.com/NVIDIA/nvidia-docker/issues/262. Thank you.", "Did you try looking at the instructions here?  (the link the error had at the bottom!) https://www.tensorflow.org/install/install_sources#common_installation_problems", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 166 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 14993, "title": "Using tfdbg with errors.", "body": "VERSION: TF-1.4.0\r\n\r\nProblem:\r\n  I want to use tfdbg to debug NaN problem with `tf_debug.LocalCLIDebugHook()` , but encounter thie problem.\r\n  One more question, how to lower the size of dump files?\r\n\r\nLogging is below:\r\n```\r\nTraceback (most recent call last):\r\n  File \"./train.py\", line 166, in <module>\r\n    _, loss, l2_loss, predicted, labels, label_lengths, step = sess.run(ops)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 521, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 892, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 967, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 952, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1032, in run\r\n    run_metadata=run_metadata))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/wrappers/hooks.py\", line 157, in after_run\r\n    self._session_wrapper.on_run_end(on_run_end_request)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py\", line 321, in on_run_end\r\n    self._dump_root, partition_graphs=partition_graphs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/lib/debug_data.py\", line 495, in __init__\r\n    self._load_all_device_dumps(partition_graphs, validate)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/lib/debug_data.py\", line 517, in _load_all_device_dumps\r\n    self._load_partition_graphs(partition_graphs, validate)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/lib/debug_data.py\", line 798, in _load_partition_graphs\r\n    self._validate_dump_with_graphs(debug_graph.device_name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/lib/debug_data.py\", line 874, in _validate_dump_with_graphs\r\n    (node, datum.timestamp, repr(pending_inputs[node])))\r\nValueError: Causality violated in timing relations of debug dumps: model_train/model/model/decoder/while/BasicDecoderStep/TrainingHelperNextInputs/cond/Merge (1512027705071447): these input(s) are not satisfied: [(u'model_train/model/model/decoder/while/BasicDecoderStep/TrainingHelperNextInputs/cond/Switch_1', 1)]\r\n```\r\n\r\n", "comments": ["@zh794390558 Is it possible to share a minimum reproducible program, to help debug the problem?\r\n\r\n@caisq might have some suggestions here.", "@zh794390558 One way to reduce the size of the dump is to reduce your batch size.\r\n\r\nAlso, as @tatatodd suggested, a reproduction code would be helpful for me to diagnose the problem.", "@zh794390558 another question for you: what operating system are you using with TF?", "@caisq my OS  is centos, and the code is too large which is attention-based seq-2-seq model.   The https://github.com/tensorflow/nmt  repo is same to my. \r\n\r\n\r\nsnippet usage code is blew all other snippet request.\r\n```\r\nif hp.debug:\r\n     hooks.append(tf_debug.LocalCLIDebugHook(dump_root='/lustre/atlas/zhanghui/debug'))\r\n \r\n with tf.train.MonitoredTrainingSession(\r\n         hooks=hooks,\r\n         scaffold=scaffold,\r\n         checkpoint_dir=checkpoint_dir,\r\n         save_checkpoint_secs=600,\r\n         save_summaries_steps=50,\r\n         config=config) as sess:\r\n\r\n```", "I tried running tfdbg on the nmt model, so far I can't repro the issue on CPU yet. Will try again on a GPU machine soon.", "@caisq I met the same problem in a GPU machine, but when switched to CPU, the issue is gone.", "@zh794390558 @reset I tested the nmt model with tfdbg on a GPU machine\r\n* GeForce GTX 1050 Ti 4GB\r\n* Ubuntu 16.04\r\n* tf-nightly-gpu version: 1.5.0-dev20171206. (This is close to HEAD, installed with `pip install tf-nightly-gpu`)\r\n\r\nThe code I used to activate tfdbg is at: https://github.com/caisq/nmt/commit/a0591f3b7650f1be0d16ac681ec5df0686daaf65\r\n\r\nI couldn't reproduce the error. I ran a few training iterations and couldn't observe any crashes of the sort you mentioned. Am I missing any keys to reproducing this issue? ", "@caisq tensorflow version maybe? both @zh794390558 and I are working on tf 1.4.0. I installed it with `pip` directly. maybe you can give it a try.", "Nagging Assignee @caisq: It has been 289 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 14992, "title": "Update input_fn.md", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 14991, "title": "[Go] Adds Operations() method to Graph", "body": "There is currently no way to list all of the operations in a graph\r\nfrom the go api. This patch ads an Operations() method to retrieve the\r\nlist using the existing TF_GraphNextOperation c api. The graph_test\r\nwas modified to include testing this new method.\r\n\r\nSigned-off-by: Vishvananda Ishaya Abrams <vishvananda@gmail.com>", "comments": ["Can one of the admins verify this patch?", "Infra fail, retry. Jenkins, test this please.", "@tensorflow-jenkins test this please"]}, {"number": 14990, "title": "fcn network to tensorflow lite ?", "body": "Input:\r\n(u'input_image', (<tf.Tensor 'input_image:0' shape=<unknown> dtype=float32>,))\r\n(u'image_width', (<tf.Tensor 'image_width:0' shape=<unknown> dtype=int32>,))\r\n(u'image_height', (<tf.Tensor 'image_height:0' shape=<unknown> dtype=int32>,))\r\n\r\nOutput:\r\n(u'Squeeze', (<tf.Tensor 'Squeeze:0' shape=(?, ?, 2) dtype=float32>,))\r\n(u'Squeeze_1', (<tf.Tensor 'Squeeze_1:0' shape=(?, ?, 4) dtype=float32>,))\r\n(u'Squeeze_2', (<tf.Tensor 'Squeeze_2:0' shape=(?, ?, 10) dtype=float32>,))\r\n\r\nhow to do?\r\n bazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n  --input_file=PNet.pb \\\r\n  --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE \\\r\n  --output_file=PNet.float.lite --inference_type=FLOAT \\\r\n  --input_type=FLOAT --input_arrays=input_image,image_width,image_height \\\r\n  --output_arrays=Squeeze,Squeeze_1 --input_shapes=?\r\n\r\n\r\ntensorflow/contrib/lite/toco/tooling_util.cc:1547] Check failed: array.final_data_type == array.data_type", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "i have fix it ,thanks", "How did you solve it? I have met the same issue, thanks!\r\n\r\n> i have fix it ,thanks\r\n\r\n"]}, {"number": 14989, "title": "gen_image_ops.py", "body": "", "comments": []}, {"number": 14988, "title": "tf.image.crop_and_resize", "body": "- [ ] \r\n\r\n- [ ] #-\r\n\r\n **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 14.04LTS\r\n- **TensorFlow installed from (source or binary)**: yes\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 2.7.6\r\n- **Bazel version (if compiling from source)**:N/A\r\n- **GCC/Compiler version (if compiling from source)**:N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**:N/A\r\n- **Exact command to reproduce**:N/A\r\n\r\nAnyone know how the \r\ntf.image.crop_and_resize(images, boxes, batch_inds,\r\n                                         [pooled_height, pooled_width],\r\n                                         method='bilinear',\r\n                                         name='Crop')\r\nwork? i cannot find the \" tf.image.crop_and_resize\" in  tensorflow/python/ops/gen_image_ops.py. \r\nThank you", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "N/A", "It is written in C++ ... see e.g.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/image_ops.cc\r\n", "@aselle have the C++ code explains how the tf.image.crop_and_resize work?\r\nI am not sure the .Input() in given link can explain the function."]}, {"number": 14987, "title": "Wrong code in programmer's guide in Variable Section", "body": "In Programmer's guide Variable section\r\n\u2028the\u00a0assignment\u00a0variable is a\u00a0`tf.Tensor`\u00a0and should use\u00a0`assignment.op.run()` instead of `assignment.run()`\r\nOtherwise, this code would produce an error:\r\n```\r\nAttributeError: 'Tensor' object has no attribute 'run'\r\n```\r\nOr we can use `sess.run(assignment)` to finish this assignment operation", "comments": ["Can one of the admins verify this patch?"]}, {"number": 14986, "title": "improvement in the tf.nn.raw_rnn documentation", "body": "PR for #14963. This exposes the advanced use-case of the raw_rnn where the user can project the cell_output into logits as a part of the `loop_fn` computations. Earlier it mislead the users that it is mandatory for the `loop_fn` to return `emit_output` such that it matches the `cell.output_size`.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I have signed the CLA!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "CLAs look good, thanks!\n\n<!-- ok -->", "@ebrevdo WDYT?", "@ebrevdo: Does this look ok?", "This is more than a documentation improvement. There are code changes here.", "Update the pr title", "@ebrevdo there are no code changes in the PR. There are changes in the pseudocode given in the documentation of tf.nn.raw_rnn", "Thanks for clarifying; you're right.  The github context window was too small :(", "```\r\nFAIL: Found 5 non-whitelited pylint errors:\r\ntensorflow/python/ops/rnn.py:946: [C0301(line-too-long), ] Line too long (84/80)\r\ntensorflow/python/ops/rnn.py:947: [C0301(line-too-long), ] Line too long (92/80)\r\ntensorflow/python/ops/rnn.py:948: [C0301(line-too-long), ] Line too long (85/80)\r\ntensorflow/python/ops/rnn.py:949: [C0301(line-too-long), ] Line too long (86/80)\r\ntensorflow/python/ops/rnn.py:950: [C0301(line-too-long), ] Line too long (91/80)\r\n```", "@drpngx the lines were exceeding the 80 chars limit. I have fixed it in the subsequent commit.", "Thanks, testing again.", "Yay!", "Thank you for adding my contribution to the repo. :+1: \r\nCheers!"]}, {"number": 14985, "title": "tf.nn.fractional_max_pool output have same batch size when feed with different input batch size", "body": "\r\n### Describe the problem\r\ntf.nn.fractional_max_pool output have same batch size when feed with different input batch size.\r\nAttached is test code I write. 2 different input is feed in with different batch size , outputs get same batch size.\r\n[pool_test.py.txt](https://github.com/tensorflow/tensorflow/files/1516498/pool_test.py.txt)\r\n\r\n###code result\r\nshape of input_a (3, 32, 32, 3)\r\nshape of output_a (3, 21, 21, 3)\r\nshape of input_b **(4, 32, 32, 3)**\r\nshape of output_b **(3, 21, 21, 3)**\r\n\r\n### System information\r\n\r\n== cat /etc/issue ===============================================\r\nLinux c-1080u 4.10.0-40-generic #44~16.04.1-Ubuntu SMP Thu Nov 9 15:37:44 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.3 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux c-1080u 4.10.0-40-generic #44~16.04.1-Ubuntu SMP Thu Nov 9 15:37:44 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.3)\r\nnumpydoc (0.7.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensorflow'\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda-8.0/lib64:\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nThu Nov 30 11:55:40 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.90                 Driver Version: 384.90                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |\r\n|  0%   51C    P8    21W / 280W |    860MiB / 11169MiB |      9%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1060      G   /usr/lib/xorg/Xorg                           542MiB |\r\n|    0      1540      G   compiz                                       315MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61\r\n\r\n== cat /etc/issue ===============================================\r\nLinux c-1080u 4.10.0-40-generic #44~16.04.1-Ubuntu SMP Thu Nov 9 15:37:44 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.3 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux c-1080u 4.10.0-40-generic #44~16.04.1-Ubuntu SMP Thu Nov 9 15:37:44 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.3)\r\nprotobuf (3.5.0.post1)\r\ntensorflow-gpu (1.4.0)\r\ntensorflow-tensorboard (0.4.0rc3)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.4.0\r\ntf.GIT_VERSION = v1.4.0-rc1-11-g130a514\r\ntf.COMPILER_VERSION = v1.4.0-rc1-11-g130a514\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda-8.0/lib64:\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nThu Nov 30 11:56:18 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.90                 Driver Version: 384.90                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |\r\n|  0%   51C    P0    80W / 280W |    860MiB / 11169MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1060      G   /usr/lib/xorg/Xorg                           542MiB |\r\n|    0      1540      G   compiz                                       315MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61\r\n\r\n\r\n\r\n\r\n", "comments": ["@balconychy Thanks for the clear bug description!\r\n\r\nLooks like @weiranzhao wrote the code, and @josh11b might have reviewed it.  Can one of you take a look at this?\r\n\r\nFor convenience, here's the repro program that @balconychy created:\r\n```\r\nimport  cifar10\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.slim as slim\r\nimport numpy as np\r\n\r\ndef check_unequal_size():\r\n    input_holder = tf.placeholder(tf.float32, [None,32,32,3])\r\n    out=tf.nn.fractional_max_pool(input_holder,[1,1.5,1.5,1],name=\"low_fea_pool\")\r\n    sess=tf.Session()\r\n    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\n    x_train = x_train.astype('float32')\r\n    input_a=x_train[1:4]\r\n    input_b=x_train[1:5]\r\n    output_a,_,_=sess.run(out,\r\n               {input_holder:input_a\r\n                })\r\n    output_b,_,_= sess.run(out,\r\n                 {input_holder: input_b\r\n                 })\r\n    print(\"shape of input_a\",input_a.shape)\r\n    print(\"shape of output_a\", output_a.shape)\r\n    print(\"shape of input_b\", input_b.shape)\r\n    print(\"shape of output_b\", output_b.shape)\r\n    pass\r\n\r\ncheck_unequal_size()\r\n```", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Thanks for the detailed example. I am working on a fix. Will update the thread when it is checked in to master branch.", "@balconychy \r\nThis should have been fixed. I checked this with nightly build at tf-nightly 1.6.0.dev20180109.\r\n"]}, {"number": 14984, "title": "Add `AWS_REGION` env for S3 in TensorFlow", "body": "This fix tries to address the issue raised in #14951 where the region can only be specified with non-common `S3_REGION` environment variables.\r\n\r\nThis fix adds the support of `AWS_REGION` which takes precedence over `S3_REGION`.\r\n\r\nThis fix fixes #14951.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Jenkins, test this please.", "@tensorflow-jenkins test this please"]}, {"number": 14983, "title": "Switch over to max_pool_v2 in Python", "body": "This fix is a follow up to #11875 so that MaxPool in Python use v2 version. As 11875 has been merged several months ago, this fix conforms to the deprecation policy.\r\n\r\nThis fix is realted to #11875 and #4746.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @rmlarsen for the review. The PR has been updated.", "@rmlarsen WDYT?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "@yongtang sorry about dropping this. Please resolve the conflicts.", "@rmlarsen The PR has been rebased with conflict resolved. Please take a look.", "@yongtang this appears to break some tests in tensorflow/compiler: \r\n\r\nhttps://source.cloud.google.com/results/invocations/cc0169ef-2973-4bee-839e-94ba7b4e043b/targets\r\n\r\nplease take a look.", "@yongtang we are going to temporarily revert this change for the upcoming release to make sure this is also supported by the MKL accelerated code when enabled.", "Thanks @rmlarsen. I didn't know it caused the issue in MKL. Let me know if I can be of any help with migrating MKL's max_pool to v2."]}, {"number": 14982, "title": "Added ability to skip rescan bottleneks", "body": "", "comments": ["Can one of the admins verify this patch?", "@petewarden WDYT?"]}, {"number": 14981, "title": "Bump the GRPC version TF depends on.", "body": "To help fix #14039", "comments": ["@saeta fyi ", "Jenkins, test this please.", "FYI I'm bringing in this PR in as a CL. Please hold off on merging. Thanks!", "Can we close this then?", "It is merged internally. Closing."]}, {"number": 14980, "title": "tensorflow-gpu install from pip breaks conda on windows", "body": "Your instructions for installing tensorflow-gpu on windows often breaks peoples conda installations forcing a complete reinstall. This also forces a rebuilding of previous environments which no longer function.\r\n\r\nhttps://stackoverflow.com/questions/46356732/anaconda-prompt-corrupts-after-installation/46493533#46493533\r\n\r\nSome kind of warning or recommendation to check the above thread could really help. This seems like a problem with the tensorflow website instructions as opposed to a tensorflow issue.", "comments": ["The installation instructions linked from that issue are not tensorflows. They are another repository. That beintg said:\r\nIt seems like the problem is you probably didn't add the path properly you should always include %PATH% in any path you define in the windows environment so you inherit the previous path instead of replacing it.", "No. They are a possible fix for tensorflows. I\u2019ve been using the same few conda environments for months. After following the tensorflow web page installation instructions for the gpu install in a new conda environment all of my conda functionality suddenly crashed. Evidently the \u2014ignore-installed flag winds up installing an older conda binary that overrides the current installation. Took me three hours to figure this out and the rest of the Day eliminating my conda install, reinstalling and rebuilding my previous environments.\n\nSent from my iPhone\n\n> On Nov 29, 2017, at 1:45 PM, Andrew Selle <notifications@github.com> wrote:\n> \n> The installation instructions linked from that issue are not tensorflows. They are another repository. That beintg said:\n> It seems like the problem is you probably didn't add the path properly you should always include %PATH% in any path you define in the windows environment so you inherit the previous path instead of replacing it.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n", "Well that is not consistent with the stackoverflow question you linked. Could you please be precise about what instructions you followed by posting a link here, and tell me where you would like a warning included. I'm happy to help, but you need to provide more information.", "I was using the following instructions\n\nhttps://www.tensorflow.org/install/install_windows\n\n[image: Inline image 1]\nAfter creating a fresh environment, activating it and running the\n\n(tensorflow)C:> *pip install --ignore-installed --upgrade tensorflow-gpu*\n\ncommand, conda no longer responded to any of the current command line\nflags. I kept getting a choice error then a bunch of options. When I ran\nconda version (not conda --version because that raised an error) it said\n\"wheel-0.30\" or something to that effect. I've tracked this error back to a\nfew threads and all seem to be after attempting to install tensorflow-gpu.\n\nhttps://github.com/ContinuumIO/anaconda-issues/issues/6171\n\nhttps://groups.google.com/a/continuum.io/forum/#!msg/anaconda/5F_tTvy1Vvk/LpS2yyEoAQAJ\n\nhttps://github.com/antoniosehk/keras-tensorflow-windows-installation/issues/1\n\nhttps://stackoverflow.com/questions/46356732/anaconda-prompt-corrupts-after-installation\n\nhttps://stackoverflow.com/questions/47244290/anaconda-commands-stopped-working\n\n\n\nOn Wed, Nov 29, 2017 at 3:12 PM, Andrew Selle <notifications@github.com>\nwrote:\n\n> Well that is not consistent with the stackoverflow question you linked.\n> Could you please be precise about what instructions you followed by posting\n> a link here, and tell me where you would like a warning included. I'm happy\n> to help, but you need to provide more information.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/14980#issuecomment-348027598>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAs3JBTUMVBG8_AXpaUcj807OM9TVyrWks5s7eTEgaJpZM4QvjtP>\n> .\n>\n\n\n\n-- \nKen Pierce\nkpierce8@gmail.com\n", "/CC @mrry", "I got the same issue. Using `pip install --ignore-installed --upgrade tensorflow-gpu` on Windows broke Conda.\r\nThereafter, `conda -h` returns:\r\n```\r\nusage: conda [-h]\r\n             {keygen,sign,unsign,verify,unpack,install,install-scripts,convert,version,help}\r\n             ...\r\n\r\npositional arguments:\r\n  {keygen,sign,unsign,verify,unpack,install,install-scripts,convert,version,help\r\n}\r\n                        commands\r\n    keygen              Generate signing key\r\n    sign                Sign wheel\r\n    unsign              Remove RECORD.jws from a wheel by truncating the zip\r\n                        file. RECORD.jws must be at the end of the archive.\r\n                        The zip file must be an ordinary archive, with the\r\n                        compressed files and the directory in the same order,\r\n                        and without any non-zip content after the truncation\r\n                        point.\r\n    verify              Verify a wheel. The signature will be verified for\r\n                        internal consistency ONLY and printed. Wheel's own\r\n                        unpack/install commands verify the manifest against\r\n                        the signature and file contents.\r\n    unpack              Unpack wheel\r\n    install             Install wheels\r\n    install-scripts     Install console_scripts\r\n    convert             Convert egg or wininst to wheel\r\n    version             Print version and exit\r\n    help                Show this help\r\n\r\noptional arguments:\r\n  -h, --help            show this help message and exit\r\n```\r\n\r\nMost common Conda commands like `update`, `info`, `list `are missing.\r\n", "Interestingly on a different machine, I had \"conda install tensorflow\" disconnect jupyter from the conda environment. Exact same issue as this thread. So tensorflow installation has messed with conda now in two different ways on two different machines. (both win 10).\r\n\r\nhttps://stackoverflow.com/questions/39604271/conda-environments-not-showing-up-in-jupyter-notebook\r\n\r\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "OK. @kpierce8 do you have a suggestion on how to improve the docs?", "I had this problem too:\r\nhttps://github.com/tensorflow/tensorboard/issues/588\r\n\r\nI got around it by following these instructions:\r\nIf you broke the pip installation, you can fix it by downloading the whl file of html5lib from https://pypi.python.org/pypi/html5lib and then running pip install <path-to-whl-file>", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Could you explain the --ignore-installed flag is there. I've since\ninstalled it a few times without that flag and it seems to work fine,\n\nOn Mon, Apr 23, 2018 at 11:52 AM, Alfred Sorten Wolf <\nnotifications@github.com> wrote:\n\n> Nagging Assignee @aselle <https://github.com/aselle>: It has been 14 days\n> with no activity and this issue has an assignee. Please update the label\n> and/or status accordingly.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/14980#issuecomment-383681462>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAs3JOxvOxMhp0WTb3ZKEhnSwegJ6m-qks5triMAgaJpZM4QvjtP>\n> .\n>\n\n\n\n-- \nKen Pierce\nkpierce8@gmail.com\n", "Nagging Assignee @aselle: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 32 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 46 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 61 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 76 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 92 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 106 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing since there was no response on how to update the documents. Please re-open with a concrete suggestion. Also, windows should be improved by the new bazel work.\r\n", "Just had the same thing happen, really frustrating.  @aselle, @kpierce8 is suggesting that `--ignore-installed` be removed from the documents, but is first asking why it is needed..."]}, {"number": 14979, "title": "improved estimator.export_savedmodel exception", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 14978, "title": "Header mismatch when running 'Simple Audio Recognition'", "body": "tensorflow: 1.4, ubuntu 17.10, python3.6 anaconda, cuda 8.0, cudnn 6.0\r\n\r\nGoing with [Simple Audio Recognition](https://www.tensorflow.org/versions/master/tutorials/audio_recognition), I met an error when running \r\n```bash\r\npython tensorflow/examples/speech_commands/train.py --data_dir=tensorflow/examples/speech_commands/train/audio\r\n```\r\n\r\nerror:\r\n```\r\nTraceback (most recent call last):                                                                                                                            \r\n  File \"/home/jihao/.conda/envs/tf_cpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1323, in _do_call                               \r\n    return fn(*args)                                                                                                                                          \r\n  File \"/home/jihao/.conda/envs/tf_cpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn                                \r\n    status, run_metadata)                                                                                                                                     \r\n  File \"/home/jihao/.conda/envs/tf_cpu/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__                         \r\n    c_api.TF_GetCode(self.status.status))                                                                                                                     \r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Header mismatch: Expected RIFF but found \\udc87\\udc9b\\udc8f\\udc8c                               \r\n         [[Node: DecodeWav = DecodeWav[desired_channels=1, desired_samples=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ReadFile)]]\r\n```\r\n", "comments": ["Could you try Python 2? It looks like a unicode python 3 problem.", "@petewarden, did you test this on Python 3?", "I tried anaconda python2, and the error:\r\n```\r\n2017-11-30 21:26:39.194187: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Header mismatch: Expected RIFF but found \ufffd\ufffd\ufffd\ufffd\r\nTraceback (most recent call last):\r\n  File \"tensorflow/examples/speech_commands/train.py\", line 430, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/jihao/.conda/envs/python27/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"tensorflow/examples/speech_commands/train.py\", line 106, in main\r\n    FLAGS.testing_percentage, model_settings)\r\n  File \"/home/jihao/deep_learning/tensorflow/tensorflow/examples/speech_commands/input_data.py\", line 162, in __init__\r\n    self.prepare_background_data()\r\n  File \"/home/jihao/deep_learning/tensorflow/tensorflow/examples/speech_commands/input_data.py\", line 323, in prepare_background_data\r\n    feed_dict={wav_filename_placeholder: wav_path}).audio.flatten()\r\n  File \"/home/jihao/.conda/envs/python27/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"/home/jihao/.conda/envs/python27/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/jihao/.conda/envs/python27/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"/home/jihao/.conda/envs/python27/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\r\n    return fn(*args)\r\n  File \"/home/jihao/.conda/envs/python27/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\r\n    status, run_metadata)\r\n  File \"/home/jihao/.conda/envs/python27/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py\", line 472, in __exit__\r\n    compat.as_text(c_api.TF_Message(self.status.status)),\r\n  File \"/home/jihao/.conda/envs/python27/lib/python2.7/site-packages/tensorflow/python/util/compat.py\", line 84, in as_text\r\n    return bytes_or_text.decode(encoding)\r\n  File \"/home/jihao/.conda/envs/python27/lib/python2.7/encodings/utf_8.py\", line 16, in decode\r\n    return codecs.utf_8_decode(input, errors, True)\r\nUnicodeDecodeError: 'utf8' codec can't decode byte 0x87 in position 41: invalid start byte\r\n```\r\n", "I think this problem is caused by improper decompression.\r\nIf I use the default Ubuntu decompression tool to extract, there will be this problem.\r\nIf i use \r\n```bash \r\n7z x train.7z\r\n```\r\nI will not encounter this problem.\r\nMore in-depth reasons have yet to be studied.\r\nClosed.", "tensorflow.python.framework.errors_impl.InvalidArgumentError: Header mismatch: Expected RIFF but found \r\ni am getting this error while running my run file", "have any solution for this problem", "In my case, the problem was that in the folder containing the dataset there were system files left by OS. (e.g. on Mac OS .DS_Store file).", "In my case, this error happed due to the audio format. The TensorFlow audio could not decode the .mp3 audio format so I fixed this problem by inputting the .wav audio.  "]}, {"number": 14977, "title": "Wrong Code in example in Programmer's guide", "body": "In Programmer's guide Variable section\r\nthe `assignment` variable is a `tf.Tensor` and should use `.eval()` instead of  `.run()`\r\n\r\nOtherwise, this code would produce an error:\r\n```\r\nAttributeError: 'Tensor' object has no attribute 'run'\r\n```\r\n", "comments": ["Can one of the admins verify this patch?", "I found it is not right, It should be `sess.run(assignment)` or  `assignment.op.run()` to finish the assignment op and I will do a new pull request"]}, {"number": 14976, "title": "Add missing <stdio.h> include", "body": "Add missing <stdio.h> include in `tensorflow/contrib/lite/kernels/op_macros.h`. Otherwise, it fails to find `stderr`.\r\n\r\nTo reproduce it:\r\n\r\n```shell\r\nbazel build //tensorflow/contrib/lite/java:tflite_runtime\r\n```", "comments": ["Can one of the admins verify this patch?", "Actually this is already done.\r\nhttps://github.com/tensorflow/tensorflow/commit/c210009d0a8d40d458c21b1faf7b1adf8e4deaee\r\n"]}, {"number": 14975, "title": "master branch: compilation of llvm_gpu_backend failed", "body": "### System information\r\n- **OS Platform and Distribution**: slackware64-current\r\n- **Python version**: 3.6.3\r\n- **Bazel version**: 0.8.0\r\n- **GCC/Compiler version**: 5.4.0\r\n- **CUDA/cuDNN version**: 9 / 7\r\n- **build environment**:\r\nexport PYTHON_BIN_PATH=/usr/bin/python3\r\nexport USE_DEFAULT_PYTHON_LIB_PATH=1\r\nexport CC_OPT_FLAGS=\"-march=native\"\r\nexport TF_NEED_JEMALLOC=1\r\nexport TF_NEED_S3=1\r\nexport TF_NEED_GCP=0\r\nexport TF_NEED_HDFS=0\r\nexport TF_NEED_GDR=0\r\nexport TF_ENABLE_XLA=1\r\nexport TF_NEED_VERBS=0\r\nexport TF_NEED_OPENCL=0\r\nexport TF_NEED_MKL=1\r\nexport TF_NEED_MPI=0\r\nexport TF_NEED_CUDA=1\r\nexport GCC_HOST_COMPILER_PATH=/opt/gcc54/bin/gcc-5.4.0\r\nexport TF_CUDA_CLANG=0\r\nexport TF_NEED_OPENCL_SYCL=0\r\nexport CLANG_CUDA_COMPILER_PATH=/usr/bin/clang\r\nexport CUDA_TOOLKIT_PATH=/usr/share/cuda\r\nexport TF_CUDA_VERSION=$($CUDA_TOOLKIT_PATH/bin/nvcc --version | sed -n 's/^.*release \\(.*\\),.*/\\1/p')\r\nexport CUDNN_INSTALL_PATH=/usr/share/cuda\r\nexport TF_CUDNN_VERSION=$(sed -n 's/^#define CUDNN_MAJOR\\s*\\(.*\\).*/\\1/p' $CUDNN_INSTALL_PATH/include/cudnn.h)\r\nexport TF_CUDA_COMPUTE_CAPABILITIES=3.0\r\nsed -i 's|/lib\"|/lib64\"|g' third_party/mkl/build_defs.bzl\r\nsed -i 's|\"lib\"|\"lib64\"|g' third_party/mkl/build_defs.bzl\r\nsed -i 's|/lib|/lib64|g' tensorflow/c/generate-pc.sh\r\n- **build command**:\r\n./configure\r\nbazel build --config=opt --config=cuda --config=mkl //tensorflow:libtensorflow.so //tensorflow/tools/pip_package:build_pip_package\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package tmp`\r\n\r\n### Building from master fails\r\nERROR: /tmp/SBo-VCS/tensorflow_cuda-VCS/tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/BUILD:16:1: C++ compilation of rule '//tensorflow/compiler/xla/service/gpu/llvm_gpu_backend:llvm_gpu_backend' failed (Exit 1)\r\ntensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:37:41: fatal error: llvm/CodeGen/CommandFlags.def: No such file or directory\r\ncompilation terminated.\r\nINFO: Elapsed time: 1828.121s, Critical Path: 131.49s\r\nFAILED: Build did NOT complete successfully", "comments": ["This commit https://github.com/tensorflow/tensorflow/commit/4e9fa6dcce4912a4797c48f4cb55d3564961bfca changed CommandFlags.h to CommandFlags.def which is missing.", "Tensorflow compiles successfully now.", "Great, closing this out.", "the error is still showing even after switching to 4e9fa6d\r\n. what needs to be done .. m a nube.", "ERROR: /home/chaitanya/GitLibs/tensorflow/tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/BUILD:16:1: C++ compilation of rule '//tensorflow/compiler/xla/service/gpu/llvm_gpu_backend:llvm_gpu_backend' failed (Exit 1).\r\ntensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:37:41: fatal error: llvm/CodeGen/CommandFlags.def: No such file or directory\r\ncompilation terminated.\r\n", "The cause of the issue is https://github.com/tensorflow/tensorflow/commit/4e9fa6dcce4912a4797c48f4cb55d3564961bfca. To successfully compile tf you have to change CommandFlags.def back to CommanFlags.h  in tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc.\r\n\r\nI'll open another issue and address this.", "Never mind -- the issue seems to be fixed in master.", "no its not .. but I did that & it compiled properly .. sorry for bothering", "The CommandFlags.def file referenced is created / downloaded via bazel. The softlink for the bazel cache where the file lives is bazel-$TENSORFLOWBUILDDIR/external/llvm/CodeGen/CommandFlags.def in the tensorflow build directory.\r\n\r\nI'm a few minutes in the building process and CommandFlags.def shows up in the mentioned dir. Let's see if the whole program compiles.\r\n\r\nMaybe you have to clean / rm your bazel-cache directory?\r\n", "Compilation successful. In the above specified environment."]}]