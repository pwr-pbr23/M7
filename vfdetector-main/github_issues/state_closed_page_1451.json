[{"number": 9428, "title": "libhexagon_controller.so build failed [missing files: GRAPHINIT := /prj/dsp/qdsp6/arch/cnn/setup/inceptionv3_uint8in.c]", "body": "OS: Ubuntu 16.04 64bits\r\nAndroid Version: 7.1 (Nougat)\r\nNDK Version: android-ndk-r12b\r\nHEXAGON SDK: 3.1\r\nnnlib source: https://source.codeaurora.org/quic/hexagon_nn/nnlib\r\n\r\nI have been following the steps provided at below link, for using tensorflow on hexagon. \r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/hvx\r\n\r\nEnvironment and copying files properly as mentioned in the link is taken care of.\r\nHaving said that, I am facing 2 issues with the build,\r\n\r\n(1-  skel.so) while building skel.so for hexagon , error reported **missing file**\r\n\r\nGRAPHINIT := /prj/dsp/qdsp6/arch/cnn/setup/inceptionv3_uint8in.c in nnlib/Makefile\r\n\r\n\r\n(2 - stub+controller.so) while building libhexagon_controller.so, there is this error reported:\r\n\r\n    ------------------------------------------\r\n     --- V = android_Debug_aarch64\r\n     --- GLUE_DIR = glue\r\n     --- HEXAGON_SDK_ROOT = /home/zaheer.s/Qualcomm/Hexagon_SDK/3.1\r\n     ------------------------------------------\r\n\r\n making /home/zaheer.s/Qualcomm/Hexagon_SDK/3.1/test/common/test_util\r\n making /home/zaheer.s/Qualcomm/Hexagon_SDK/3.1/tools/qaic\r\n making /home/zaheer.s/Qualcomm/Hexagon_SDK/3.1/libs/common/atomic\r\n making /home/zaheer.s/Qualcomm/Hexagon_SDK/3.1/libs/common/rpcmem\r\n making .\r\n android_Debug_aarch64/hexagon_controller.o: In function                                         `hexagon_controller_InitInputNodeDataToInceptionDummyData':\r\n /home/zaheer.s/Qualcomm/Hexagon_SDK/3.1/examples/common/generated_hexagon_controller/src_impl/  hexagon_controller.c:74: undefined reference to `inception_dummy_int_data_224x224'\r\n /home/zaheer.s/Qualcomm/Hexagon_SDK/3.1/examples/common/generated_hexagon_controller/src_impl/  hexagon_controller.c:74: undefined reference to `inception_dummy_int_data_224x224'\r\n collect2: error: ld returned 1 exit status\r\n make[1]: *** [android_Debug_aarch64/libhexagon_controller.so] Error 1\r\n ERROR making .\r\n\r\nI am not sure if any work around is provided to fix the missing files causing build error. please suggest a fix.\r\n\r\nPS:- I am able to run sample examples provided with ${QUALCOMM_SDK} /examples/common/gemm.\r\n", "comments": ["@satok16: Can you comment?", "@girving , @satok16 : `undefined reference toinception_dummy_int_data_224x224'`\r\nthis error comes when debug option is enabled, \r\nI am checking how to get rid of 224x224 when debug is enabled.\r\nmeanwhile I am successful in running 229x229.\r\n\r\nCould you please check how to provide proper build as well runtime fix for debug option enabled.\r\n", "Hi @kzos.  Sorry for the delay.\r\nCurrently, the nnlib only supports Hexagon_SDK 3.0, not 3.1.  Actually these different SDKs are not compatible.  If you want to use SDK 3.1 you need to modify glue files a bit.\r\n\r\nCan you try using SDK3.0 first?\r\n", "Hi @satok16 , I tried in 3.0/3.1 as well, \r\nRelease build is working fine.\r\nwhen i tried `android_Debug_aarch64`,\r\n`debug` option gets set somehow and **it has side effects of inception version getting set to 1**\r\n\r\nunder version 1: toinception_dummy_int_data_224x224 gets used.\r\n\r\ni have added new changes in glue to build for android_Debug_aarch64 and pick proper adspmsgd in target/make/android.min.\r\n\r\nso could you help in checking below part:\r\n`debug` option gets set somehow and **it has side effects of inception version getting set to 1**\r\n\r\nthanks,", "I see.  That seems to be a bug caused by a difference of compile options.  \"inception_dummy_int_data_224x224\" is a dummy input data which can be arbitrary binary.  So if you just want to pass the build, please try just putting\r\n`uint8_t inception_dummy_int_data_224x224[224*224*3] = {};\r\nin tensorflow/contrib/hvx/hexagon_controller/src_dummy_data/inception_v3_dummy_int_data.c\r\n\r\nLet me know the compile result.", "oh, that should fix the build, thanks.\r\nbut i am not sure that will have its effect at run-time.\r\nI am going through src code, meanwhile if you have right fix for selecting inception v3, please do share.\r\n\r\nthanks,", "No worries, the change won't be a problem as long as you don't use this dummy input by modifying the code.  If you want to use dummy input, you need to put binary data here either way.", "that hack fixed the Debug enabled bug, will open another case if i face any discrepancies found in build between inception v1/v3\r\n\r\nthanks,", "@satok16 @kzos \r\nCould you please show me how to enable nnlib in Qualcomm hexagon 3.1 SDK because Hexagon 3.0 SDK installation package is out-of dated and cannot download remote toolchains?\r\n\r\nBest Regards", "@satok16 I am having the same issue on `SDK 3.0` and nnlib git v.(`721b2d58f`). I can compile the `make tree VERBOSE=1 V=hexagon_Release_dynamic_toolv72_v60`, but on the `make tree VERBOSE=1 V=android_Release` I ran into the mentioned problem. There is this missing file in the nnlib/Makefile and I changed it as per your comment to add a dummy integer ( I have now both 299 and 224 in `it/tensorflow/tensorflow/contrib/hvx/hexagon_controller/src_dummy_data/inception_v3_dummy_int_data.c:`\r\n\r\n```\r\n#include <stdint.h>\r\nuint8_t inception_dummy_int_data_299x299[299*299*3] = {};\r\nuint8_t inception_dummy_int_data_224x224[224*224*3] = {};\r\n```\r\n I passed the error and now have this other compilation error:\r\n\r\n```\r\nmake[1]: *** No rule to make target `adspmsgd.a', needed by `android_Release/graph_app'.  Stop.\r\nmake[1]: Leaving directory `~/Qualcomm/Hexagon_SDK/3.0/examples/common/nnlib'\r\n```\r\n"]}, {"number": 9427, "title": "Resource Exhausted and Initialization Errors on gpu", "body": "Hello,\r\nI have been facing a resource exhausted error on my gpu for a couple of days and can't seem to figure out how to fix it.\r\nSo, i have been training custom made conv net on a kaggle dataset for facial recognition with the 53% accuracy..it currently has four convolutional + maxpooling layers followed by 2 fully connected layers\r\nMy convolutional layers currently have a stride of (2,2) but since that results in loss of information i wanted to reduce the strides to (1,1). This is a snippet of the various error i'm facing \r\n\r\nTraining on the data\r\n---------------------------------------------------------------------------\r\nInternalError                             Traceback (most recent call last)\r\n/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1021     try:\r\n-> 1022       return fn(*args)\r\n   1023     except errors.OpError as e:\r\n\r\n/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1003                                  feed_dict, fetch_list, target_list,\r\n-> 1004                                  status, run_metadata)\r\n   1005 \r\n\r\n/home/carnd/anaconda3/envs/dl/lib/python3.5/contextlib.py in __exit__(self, type, value, traceback)\r\n     65             try:\r\n---> 66                 next(self.gen)\r\n     67             except StopIteration:\r\n\r\n/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py in raise_exception_on_not_ok_status()\r\n    468           compat.as_text(pywrap_tensorflow.TF_Message(status)),\r\n--> 469           pywrap_tensorflow.TF_GetCode(status))\r\n    470   finally:\r\n\r\nInternalError: Dst tensor is not initialized.\r\n\t [[Node: zeros_28 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [2304,1152] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInternalError                             Traceback (most recent call last)\r\n<ipython-input-37-ab81f6da0f51> in <module>()\r\n      2 print(\"Training on the data\")\r\n      3 with tf.Session() as sess:\r\n----> 4     sess.run(tf.global_variables_initializer())\r\n      5 \r\n      6     for epoch in range(epochs):\r\n\r\n/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    765     try:\r\n    766       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 767                          run_metadata_ptr)\r\n    768       if run_metadata:\r\n    769         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n    963     if final_fetches or final_targets:\r\n    964       results = self._do_run(handle, final_targets, final_fetches,\r\n--> 965                              feed_dict_string, options, run_metadata)\r\n    966     else:\r\n    967       results = []\r\n\r\n/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1013     if handle is None:\r\n   1014       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\r\n-> 1015                            target_list, options, run_metadata)\r\n   1016     else:\r\n   1017       return self._do_call(_prun_fn, self._session, handle, feed_dict,\r\n\r\n/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1033         except KeyError:\r\n   1034           pass\r\n-> 1035       raise type(e)(node_def, op, message)\r\n   1036 \r\n   1037   def _extend_graph(self):\r\n\r\nInternalError: Dst tensor is not initialized.\r\n\t [[Node: zeros_28 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [2304,1152] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n\r\nCaused by op 'zeros_28', defined at:\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\r\n    app.launch_new_instance()\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\r\n    ioloop.IOLoop.instance().start()\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\r\n    super(ZMQIOLoop, self).start()\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\r\n    handler_func(fd_obj, events)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\r\n    self._handle_recv()\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-34-12683d1a5f6b>\", line 47, in <module>\r\n    optimizer = tf.train.AdamOptimizer().minimize(cost)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 298, in minimize\r\n    name=name)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 412, in apply_gradients\r\n    self._create_slots(var_list)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/training/adam.py\", line 119, in _create_slots\r\n    self._zeros_slot(v, \"m\", self._name)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 656, in _zeros_slot\r\n    named_slots[var] = slot_creator.create_zeros_slot(var, op_name)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/training/slot_creator.py\", line 121, in create_zeros_slot\r\n    val = array_ops.zeros(primary.get_shape().as_list(), dtype=dtype)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1370, in zeros\r\n    output = constant(zero, shape=shape, dtype=dtype, name=name)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\", line 169, in constant\r\n    attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInternalError (see above for traceback): Dst tensor is not initialized.\r\n\t [[Node: zeros_28 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [2304,1152] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n\r\n### The error faced in the previous training run of the network was a resource exhausted error which i have commented on in another issue\r\nhttps://github.com/tensorflow/tensorflow/issues/9400#issuecomment-296718808\r\n\r\nI am using  a g2.2x large ec2 instance on aws amazon with a 32gb storage volume\r\n\r\nHere is a link to the original unbroken code \r\nhttps://github.com/vijpandaturtle/facial-expressions\r\nAnd here is the dataset of 48x48 pixel images(around 35000 images)\r\nhttps://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge\r\nThanks", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\nAlso, try smaller batch size."]}, {"number": 9426, "title": "Add link for Xavier initializer paper and update example for zero_faction function", "body": "On https://www.tensorflow.org/api_docs/python/tf/nn/zero_fraction:\r\n* The example for `zero_faction` uses an artificial function `tf.Relu` and deprecated one ` tf.contrib.deprecated.scalar_summary`.\r\n\r\nOn, https://www.tensorflow.org/api_docs/python/tf/contrib/layers/xavier_initializer:\r\n* Added a link to Xavier Glorot and Yoshua Bengio's paper for quick reference.\r\n* Fixed broken link to https://www.tensorflow.org/api_guides/python/constant_op#random-tensors.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 9425, "title": "Include refcount.h in grpc_call.h", "body": "Fix https://github.com/tensorflow/tensorflow/issues/9417", "comments": ["Can one of the admins verify this patch?", "@drpngx This fix works, can you take a look.", "Jenkins, test this please.", "Ignoring flaky `queue_runner` test on Mac."]}, {"number": 9424, "title": "[minor bug] tf1.0+ compatibility script doesn't handle old batch_matmul arguments", "body": "tf 1.0 merges `batch_matmul` into `matmul`, but the compatibility script forgot to rename the arguments for `batch_matmul`\r\n\r\nwhat is needed is: \r\n`adj_x` --> `transpose_a`\r\n`adj_y` --> `transpose_b`\r\n\r\nI would be happy to write a quick PR.\r\n\r\nReferences:\r\n\r\nhttps://www.tensorflow.org/versions/r0.12/api_docs/python/\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/matmul", "comments": ["Thanks for volunteering to improve TensorFlow. Please submit the PR, and reference it here!", "Apparently fixed in https://github.com/tensorflow/tensorflow/pull/9448."]}, {"number": 9423, "title": "Feature Request : Gradients for tf.contrib.image.rotate()", "body": "I wish to propagate gradients through image rotation. This is currently not supported:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nimages = tf.zeros(shape=[5, 10, 10, 1])\r\nangles = 2 * np.pi * np.random.random(5) - np.pi\r\nout = tf.contrib.image.rotate(images, angles)\r\nout_sum = tf.reduce_sum(out)\r\nimages_grad = tf.gradients(out_sum, [images])\r\n\r\nprint images_grad\r\n```\r\n\r\ngives\r\n\r\n```[None]```", "comments": ["I think the gradient with respect to `images` can be easily implemented by applying reversed rotation on the gradient coming from top. i.e. if `out_grad` is gradient with respect to `out`, then\r\n```\r\nimages_grad = tf.contrib.image.rotate(out_grad, -angles)\r\n```\r\n\r\nGradient with respect to `angles` might be trickier. However, I do not need this functionality.", "If you are not up to making a full patch (But please do if you can).. you can just define a gradient for the op ad-hoc in your code using the gradient_override_map... i.e.\r\n\r\n```python\r\n@tf.RegisterGradient(\u2018CustomSquare\u2019)\r\ndef custom_square_grad(op, grad):\r\n  # \u2026\r\n\r\nc = tf.constant(5.0)\r\ns1 = tf.square(c)  # Uses default gradient for square\r\nwith graph.gradient_override_map({\u2018Square\u2019: \u2018CustomSquare\u2019}):\r\n  s2 = tf.square(c)  # Uses _compute_square_grad\r\n```", "I gave this one a shot - as a first time contributor I guess some feedback would be really appreciated. Can put up a PR soon.", "I am interested in the gradients with respect to the angle(s) as well. Is this theoretically possible? As I am just getting started with TF, I'm not really sure how this could possibly be implemented...", "Can tf.contrib.image.rotate() be backpropagated at the moment? I have tried training with this op involved in my network architecture but failed because of it Is it possible to add a backprop feature to it? \r\n(Windows 7, gpu)"]}, {"number": 9422, "title": "Enable use of cuDNN RNNs with optimizers other than GradientDescentOptimizer", "body": "Currently cuDNN-based RNNs in TF are limited to GradientDescentOptimizer ([#6620](https://github.com/tensorflow/tensorflow/issues/6620)). This is a serious limitation given the widespread use of other optimizers. The claim that this cannot be supported in TF because cuDNN RNN don't have known shapes at static time seems overly pessimistic. Just like RNNParamsSaver provides a mechanism to convert between canonical shaped variables and the parameter buffer, a simple wrapper can be provided that does this automatically for cuDNN RNNs, so that the size of the parameter buffer is statically calculated and then used to define the variable, especially since the shape of the parameter buffer is just a 1D vector anyway.", "comments": ["@ebrevdo Can you comment?", "This is a duplicate of #6620.  That said, #6620 is a pretty serious issue.\n zheng-xq can probably say more about whether we can add static shape\ninformation to the parameter objects that cudnn-rnn builds.\n\nOn Mon, Apr 24, 2017 at 3:29 PM, Geoffrey Irving <notifications@github.com>\nwrote:\n\n> @ebrevdo <https://github.com/ebrevdo> Can you comment?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9422#issuecomment-296840255>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimy5HhUxDPJ_kG5ZO_SGLI8Ko-no2ks5rzSJSgaJpZM4NGw7o>\n> .\n>\n", "Let's continue discussion in #6620."]}, {"number": 9421, "title": "import tensorflow in python script using php", "body": "I try to execute a python script using php , the python script requires importing tensorflow , when i run the php script this error occur : \r\n\r\nTraining....string(1374) \"Traceback (most recent call last): File \"/opt/lampp/htdocs/gp/test.py\", line 1, in import tensorflow File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 24, in from tensorflow.python import * File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 72, in raise ImportError(msg) ImportError: Traceback (most recent call last): File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 61, in from tensorflow.python import pywrap_tensorflow File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in _pywrap_tensorflow = swig_import_helper() File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description) ImportError: /opt/lampp/lib/libstdc++.so.6: version `GLIBCXX_3.4.19' not found (required by /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so) Failed to load the native TensorFlow runtime. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error for some common reasons and solutions. Include the entire stack trace above this error message when asking for help. \"", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!"]}, {"number": 9419, "title": "1080P image by SRGAN automatically killed", "body": "When I use a small image for SRGAN training, it runs well, but when I use a 1080P image for generation, the process killed without any related log. The batch is 8 so I don't think it will run out all my GPU memory, also there's no fully connection layer in my SRGAN\r\n\r\ngpu device: GTX 1060 6G\r\ntf version: 1.0.0\r\npython version: 2.7\r\n\r\nif my code is required, pleasure to provide it", "comments": ["pdb info:\r\n\r\n```\r\n-> sess.run(tf.global_variables_initializer())\r\n(Pdb) n\r\nKilled\r\n```", "Are you running out of memory?\r\n\r\nUse `dmesg` to find out if you have been \"sacrificed\".", "@drpngx \r\n\r\nOK I will try it later as that PC is in my home\r\n\r\nHow can I know the memory required for training process? As the tensors & parameters for my network is stored in GPU memory, I don't think this training causes a big memory usage", "@ruiann, it is likely the intermediates for training and evaluation are using up your memory (i.e. the temporaries). a 1080P image is quite large 24 MB or so for a 3 channel float image version, and those sizes propagate and snowball quite easily.", "@aselle \r\nThx for your reply. I don't think the eight 1080P image can run out all my GPU memory. I've almost 6G free GPU memory for this training, and the SRGAN network has no fully connection layer, the GPU memory cost of parameters will not change with the image size, only tensors' size will grow, unless tensorflow copies the tensor many times or I don't think the GPU memory can be run out.\r\n\r\nI think I need to do the dmesg check first or use 1 pic to have a try", "@drpngx @aselle \r\n\r\nas you say memory run out, dmesg logs for 8 * 1080P images:\r\n\r\n[  687.293876] python invoked oom-killer: gfp_mask=0x24280ca, order=0, oom_score_adj=0\r\n[  687.293881] python cpuset=/ mems_allowed=0\r\n[  687.293888] CPU: 0 PID: 3557 Comm: python Tainted: P           OE   4.4.0-72-generic #93-Ubuntu\r\n[  687.293890] Hardware name: Gigabyte Technology Co., Ltd. B85M-D3V/B85M-D3V, BIOS FM 08/13/2015\r\n[  687.293892]  0000000000000286 000000003738f46c ffff88008ba47af8 ffffffff813f82b3\r\n[  687.293896]  ffff88008ba47cb0 ffff8801dc4a3800 ffff88008ba47b68 ffffffff8120b24e\r\n[  687.293899]  0000000000000015 0000000000000000 ffff880220b10240 ffff880222693800\r\n[  687.293903] Call Trace:\r\n[  687.293910]  [<ffffffff813f82b3>] dump_stack+0x63/0x90\r\n[  687.293915]  [<ffffffff8120b24e>] dump_header+0x5a/0x1c5\r\n[  687.293920]  [<ffffffff81391254>] ? apparmor_capable+0xc4/0x1b0\r\n[  687.293924]  [<ffffffff811928c2>] oom_kill_process+0x202/0x3c0\r\n[  687.293927]  [<ffffffff81192ce9>] out_of_memory+0x219/0x460\r\n[  687.293931]  [<ffffffff81198cd8>] __alloc_pages_slowpath.constprop.88+0x938/0xad0\r\n[  687.293935]  [<ffffffff811990f6>] __alloc_pages_nodemask+0x286/0x2a0\r\n[  687.293940]  [<ffffffff811e43cd>] alloc_pages_vma+0xad/0x250\r\n[  687.293946]  [<ffffffff811c2241>] handle_mm_fault+0x1491/0x1820\r\n[  687.293951]  [<ffffffff8106b527>] __do_page_fault+0x197/0x400\r\n[  687.293954]  [<ffffffff8106b7b2>] do_page_fault+0x22/0x30\r\n[  687.293957]  [<ffffffff8183e7f8>] page_fault+0x28/0x30\r\n[  687.293959] Mem-Info:\r\n[  687.293965] active_anon:1582299 inactive_anon:317922 isolated_anon:0\r\n                active_file:134 inactive_file:0 isolated_file:0\r\n                unevictable:922 dirty:2 writeback:286 unstable:0\r\n                slab_reclaimable:7856 slab_unreclaimable:10507\r\n                mapped:32359 shmem:904 pagetables:15915 bounce:0\r\n                free:25520 free_pcp:11 free_cma:0\r\n[  687.293970] Node 0 DMA free:15900kB min:132kB low:164kB high:196kB active_anon:0kB inactive_anon:0kB active_file:0kB inactive_file:0kB unevictable:0kB isolated(anon):0kB isolated(file):0kB present:15984kB managed:15900kB mlocked:0kB dirty:0kB writeback:0kB mapped:0kB shmem:0kB slab_reclaimable:0kB slab_unreclaimable:0kB kernel_stack:0kB pagetables:0kB unstable:0kB bounce:0kB free_pcp:0kB local_pcp:0kB free_cma:0kB writeback_tmp:0kB pages_scanned:0 all_unreclaimable? yes\r\n[  687.293976] lowmem_reserve[]: 0 3098 7823 7823 7823\r\n[  687.293980] Node 0 DMA32 free:45568kB min:26712kB low:33388kB high:40068kB active_anon:2419368kB inactive_anon:615432kB active_file:204kB inactive_file:0kB unevictable:280kB isolated(anon):0kB isolated(file):0kB present:3297140kB managed:3216484kB mlocked:280kB dirty:0kB writeback:592kB mapped:52056kB shmem:2648kB slab_reclaimable:11068kB slab_unreclaimable:14092kB kernel_stack:3536kB pagetables:23120kB unstable:0kB bounce:0kB free_pcp:0kB local_pcp:0kB free_cma:0kB writeback_tmp:0kB pages_scanned:5556 all_unreclaimable? yes\r\n[  687.293987] lowmem_reserve[]: 0 0 4724 4724 4724\r\n[  687.293990] Node 0 Normal free:40612kB min:40736kB low:50920kB high:61104kB active_anon:3909828kB inactive_anon:656256kB active_file:332kB inactive_file:0kB unevictable:3408kB isolated(anon):0kB isolated(file):0kB present:4970496kB managed:4838116kB mlocked:3408kB dirty:8kB writeback:552kB mapped:77380kB shmem:968kB slab_reclaimable:20356kB slab_unreclaimable:27936kB kernel_stack:5840kB pagetables:40540kB unstable:0kB bounce:0kB free_pcp:44kB local_pcp:0kB free_cma:0kB writeback_tmp:0kB pages_scanned:6552 all_unreclaimable? yes\r\n[  687.293996] lowmem_reserve[]: 0 0 0 0 0\r\n[  687.294000] Node 0 DMA: 1*4kB (U) 1*8kB (U) 1*16kB (U) 0*32kB 2*64kB (U) 1*128kB (U) 1*256kB (U) 0*512kB 1*1024kB (U) 1*2048kB (M) 3*4096kB (M) = 15900kB\r\n[  687.294014] Node 0 DMA32: 386*4kB (UME) 237*8kB (UME) 281*16kB (UME) 240*32kB (UE) 139*64kB (UME) 71*128kB (UME) 26*256kB (UE) 9*512kB (UME) 1*1024kB (M) 0*2048kB 0*4096kB = 45888kB\r\n[  687.294028] Node 0 Normal: 812*4kB (UME) 402*8kB (UME) 171*16kB (UME) 88*32kB (UME) 86*64kB (UME) 61*128kB (UME) 23*256kB (UME) 3*512kB (UE) 8*1024kB (M) 0*2048kB 0*4096kB = 40944kB\r\n[  687.294042] Node 0 hugepages_total=0 hugepages_free=0 hugepages_surp=0 hugepages_size=1048576kB\r\n[  687.294044] Node 0 hugepages_total=0 hugepages_free=0 hugepages_surp=0 hugepages_size=2048kB\r\n[  687.294046] 8293 total pagecache pages\r\n[  687.294047] 6620 pages in swap cache\r\n[  687.294049] Swap cache stats: add 2353174, delete 2346554, find 130887/234080\r\n[  687.294051] Free swap  = 0kB\r\n[  687.294052] Total swap = 8282108kB\r\n[  687.294053] 2070905 pages RAM\r\n[  687.294055] 0 pages HighMem/MovableOnly\r\n[  687.294056] 53280 pages reserved\r\n[  687.294057] 0 pages cma reserved\r\n[  687.294058] 0 pages hwpoisoned\r\n[  687.294059] [ pid ]   uid  tgid total_vm      rss nr_ptes nr_pmds swapents oom_score_adj name\r\n[  687.294067] [  382]     0   382     8847      781      20       3       27             0 systemd-journal\r\n[  687.294070] [  414]     0   414    23693      129      17       3       34             0 lvmetad\r\n[  687.294073] [  420]     0   420    11563      734      24       3      260         -1000 systemd-udevd\r\n[  687.294077] [  820]   100   820    25596      417      20       3        9             0 systemd-timesyn\r\n[  687.294079] [  997]     0   997    84333      467      67       3      294             0 ModemManager\r\n[  687.294082] [ 1004]   113  1004    11197      254      27       3       49             0 avahi-daemon\r\n[  687.294084] [ 1006]     0  1006     7155      305      18       3        5             0 systemd-logind\r\n[  687.294087] [ 1012]     0  1012     1100      283       8       3       22             0 acpid\r\n[  687.294090] [ 1016]   104  1016    64099      391      28       3     1167             0 rsyslogd\r\n[  687.294092] [ 1018]     0  1018    40226      225      15       4       51             0 lxcfs\r\n[  687.294095] [ 1024]   111  1024    93498      463      79       3      355             0 whoopsie\r\n[  687.294097] [ 1027]     0  1027     8124      412      21       3       37             0 cron\r\n[  687.294099] [ 1034]     0  1034     6511      385      17       3       24             0 atd\r\n[  687.294102] [ 1036]   107  1036    11057      659      27       3      155          -900 dbus-daemon\r\n[  687.294105] [ 1060]   113  1060    11197       15      25       3       67             0 avahi-daemon\r\n[  687.294107] [ 1084]     0  1084   113088      671      72       3      308             0 NetworkManager\r\n[  687.294110] [ 1091]     0  1091    69842      573      40       4        8             0 accounts-daemon\r\n[  687.294112] [ 1093]     0  1093    56460      475      31       5     1335             0 snapd\r\n[  687.294115] [ 1119]     0  1119    71359      527      42       4      539             0 polkitd\r\n[  687.294117] [ 1128]     0  1128     3344       25      11       4       13             0 mdadm\r\n[  687.294119] [ 1229]     0  1229     3999       97      11       3      115             0 dhclient\r\n[  687.294122] [ 1351]     0  1351     1306       20       9       3       10             0 iscsid\r\n[  687.294125] [ 1352]     0  1352     1431      878       9       3        0           -17 iscsid\r\n[  687.294127] [ 1421]     0  1421    87602      416      40       4      210             0 lightdm\r\n[  687.294130] [ 1423]     0  1423     4857      352      13       3        0             0 agetty\r\n[  687.294132] [ 1428]     0  1428     4892      354      14       3       44             0 irqbalance\r\n[  687.294135] [ 1434]     0  1434    59661     3249      90       3     4642             0 Xorg\r\n[  687.294137] [ 1533]     0  1533    57576      415      48       3      204             0 lightdm\r\n[  687.294140] [ 1590]   119  1590    45886      379      24       3       64             0 rtkit-daemon\r\n[  687.294142] [ 1623]  1000  1623    11312      324      26       3      221             0 systemd\r\n[  687.294145] [ 1625]  1000  1625    15863      110      34       4      416             0 (sd-pam)\r\n[  687.294147] [ 1630]  1000  1630    52176      306      34       3      732             0 gnome-keyring-d\r\n[  687.294149] [ 1632]  1000  1632    12526      399      31       4      324             0 upstart\r\n[  687.294152] [ 1713]  1000  1713     9087        0      19       3       73             0 upstart-udev-br\r\n[  687.294154] [ 1714]  1000  1714    10981      566      25       3      165             0 dbus-daemon\r\n[  687.294157] [ 1726]  1000  1726    22460       94      35       3      174             0 window-stack-br\r\n[  687.294159] [ 1773]  1000  1773   138779      356     183       4     8670             0 fcitx\r\n[  687.294162] [ 1780]  1000  1780     9071      183      17       3       68             0 upstart-dbus-br\r\n[  687.294164] [ 1783]  1000  1783     9071      201      18       3       65             0 upstart-dbus-br\r\n[  687.294167] [ 1787]  1000  1787    11227      177      22       3      114             0 upstart-file-br\r\n[  687.294169] [ 1788]  1000  1788    42507      299      17       3       41             0 gpg-agent\r\n[  687.294171] [ 1794]  1000  1794    10754      364      24       3      126             0 dbus-daemon\r\n[  687.294174] [ 1798]  1000  1798     6909        0      17       3       61             0 fcitx-dbus-watc\r\n[  687.294177] [ 1805]  1000  1805   130100      344     119       4     1533             0 bamfdaemon\r\n[  687.294179] [ 1815]  1000  1815   140755      489     130       3     1266             0 hud-service\r\n[  687.294182] [ 1817]  1000  1817   227177      391     166       4     2175             0 unity-settings-\r\n[  687.294184] [ 1824]  1000  1824    84503      320      34       4      161             0 at-spi-bus-laun\r\n[  687.294186] [ 1825]  1000  1825   174671      382     100       4      551             0 gnome-session-b\r\n[  687.294189] [ 1835]  1000  1835    69502      366      38       3      703             0 gvfsd\r\n[  687.294191] [ 1836]  1000  1836   195276     1249     132       3     1276             0 unity-panel-ser\r\n[  687.294194] [ 1841]  1000  1841    10756      444      26       3       99             0 dbus-daemon\r\n[  687.294196] [ 1846]  1000  1846   101715      337      31       3      184             0 gvfsd-fuse\r\n[  687.294199] [ 1863]     0  1863    87687      638      56       3       88             0 upowerd\r\n[  687.294201] [ 1899]  1000  1899    51717      290      36       3      144             0 at-spi2-registr\r\n[  687.294203] [ 1916]  1000  1916    44698      353      25       4      180             0 dconf-service\r\n[  687.294206] [ 1932]  1000  1932   107912      410      45       3      250             0 indicator-messa\r\n[  687.294208] [ 1933]  1000  1933   102654      223      35       3      168             0 indicator-bluet\r\n[  687.294211] [ 1934]  1000  1934   124770      371      44       4     1180             0 indicator-power\r\n[  687.294213] [ 1936]  1000  1936   325541      110     104       4      598             0 indicator-datet\r\n[  687.294215] [ 1943]  1000  1943   159335      240     126       4     1435             0 indicator-keybo\r\n[  687.294218] [ 1944]  1000  1944   185246      411      72       4      984             0 indicator-sound\r\n[  687.294220] [ 1947]  1000  1947   135871      315     124       3     1483             0 indicator-print\r\n[  687.294223] [ 1948]  1000  1948   239095      330      47       4      767             0 indicator-sessi\r\n[  687.294225] [ 1973]  1000  1973   128161      373      91       3      840             0 pulseaudio\r\n[  687.294228] [ 1986]   114  1986    76180      702      50       3      396             0 colord\r\n[  687.294230] [ 1987]  1000  1987   156202      351     150       4      950             0 evolution-sourc\r\n[  687.294232] [ 1990]  1000  1990    98985      370      90       3      385             0 indicator-appli\r\n[  687.294235] [ 2022]  1000  2022   108571      314     110       4     1351             0 notify-osd\r\n[  687.294237] [ 2030]  1000  2030   311474     9617     284       5    12818             0 compiz\r\n[  687.294240] [ 2048]  1000  2048   182844       93     146       4     1736             0 goa-daemon\r\n[  687.294242] [ 2055]  1000  2055   214844      416     192       4     9524             0 evolution-calen\r\n[  687.294245] [ 2064]  1000  2064   221385     1387     188       4     2660             0 nautilus\r\n[  687.294247] [ 2066]  1000  2066   106405        0     107       4      835             0 polkit-gnome-au\r\n[  687.294250] [ 2067]  1000  2067   326650      391     227       5    15687             0 gnome-software\r\n[  687.294252] [ 2072]  1000  2072   146486       98     142       3     2339             0 nm-applet\r\n[  687.294255] [ 2073]  1000  2073   143281       82     108       3      857             0 unity-fallback-\r\n[  687.294257] [ 2090]  1000  2090    90665      512      46       4      687             0 gvfs-udisks2-vo\r\n[  687.294260] [ 2096]     0  2096    91869     1403      45       4      101             0 udisksd\r\n[  687.294262] [ 2108]  1000  2108    65257      372      29       3      144             0 gvfs-goa-volume\r\n[  687.294265] [ 2122]     0  2122   157222      351      91       3     6161          -900 fwupd\r\n[  687.294267] [ 2131]  1000  2131    70778      224      41       3      231             0 goa-identity-se\r\n[  687.294269] [ 2136]  1000  2136    94046      285      52       4      838             0 mission-control\r\n[  687.294272] [ 2139]  1000  2139    65783      248      30       3      171             0 gvfs-mtp-volume\r\n[  687.294274] [ 2144]  1000  2144    68832      222      36       3      722             0 gvfs-gphoto2-vo\r\n[  687.294276] [ 2152]  1000  2152   101774      374      51       4      277             0 gvfs-afc-volume\r\n[  687.294279] [ 2186]  1000  2186    88514      349      40       3      245             0 gvfsd-trash\r\n[  687.294281] [ 2188]  1000  2188    16165      272      36       3       98             0 gconfd-2\r\n[  687.294284] [ 2225]  1000  2225   202835      349     111       4     9239             0 evolution-calen\r\n[  687.294286] [ 2234]  1000  2234   173528      374     115       3     1248             0 evolution-addre\r\n[  687.294289] [ 2236]  1000  2236   266515      372     113       4     9733             0 evolution-calen\r\n[  687.294291] [ 2267]  1000  2267   192971      344      94       4     1278             0 evolution-addre\r\n[  687.294293] [ 2270]  1000  2270   167547       70     173       4     1869             0 sogou-qimpanel-\r\n[  687.294296] [ 2271]  1000  2271   776899      186     222       5     5511             0 sogou-qimpanel\r\n[  687.294298] [ 2356]  1000  2356   147184       99     136       4     1843             0 gnome-terminal-\r\n[  687.294301] [ 2361]  1000  2361     7358      438      19       3     1314             0 bash\r\n[  687.294303] [ 2363]  1000  2363     1127      345       8       3       28             0 pycharm.sh\r\n[  687.294306] [ 2566]  1000  2566     1127      140       7       3       25             0 sh\r\n[  687.294308] [ 2577]  1000  2577   102549      353      35       3      248             0 zeitgeist-daemo\r\n[  687.294311] [ 2604]  1000  2604  1153069    49883     444       7    77463             0 java\r\n[  687.294313] [ 2625]  1000  2625    78156      339      44       4      630             0 zeitgeist-fts\r\n[  687.294315] [ 2627]  1000  2627   127804      346     104       4      559             0 zeitgeist-datah\r\n[  687.294318] [ 2686]  1000  2686     2217      343      10       3       98             0 fsnotifier64\r\n[  687.294320] [ 2716]  1000  2716   387246     2791     315      33    16611             0 gitkraken\r\n[  687.294323] [ 2730]  1000  2730    77895      386     130       4      751             0 gitkraken\r\n[  687.294325] [ 2731]  1000  2731   140923      358     127       4     1144             0 update-notifier\r\n[  687.294328] [ 2781]  1000  2781   119355     5491     206       5     8285             0 gitkraken\r\n[  687.294330] [ 2805]  1000  2805   243557      464     184      37     4273           300 gitkraken\r\n[  687.294333] [ 2809]  1000  2809   409919     9289     414     119    27622           300 gitkraken\r\n[  687.294335] [ 2811]  1000  2811   265132       75     205      43    11494           300 gitkraken\r\n[  687.294337] [ 3005]  1000  3005     7384      413      20       3     1340             0 bash\r\n[  687.294340] [ 3422]  1000  3422   109736      306      49       3      767             0 deja-dup-monito\r\n[  687.294342] [ 3486]  1000  3486    89109      314      43       3      236             0 gvfsd-network\r\n[  687.294345] [ 3520]     0  3520     4634      822      14       3      222             0 mount.ntfs\r\n[  687.294347] [ 3530]  1000  3530    47358      169      28       3      638             0 gvfsd-metadata\r\n[  687.294349] [ 3541]  1000  3541    91302      229      44       3      232             0 gvfsd-dnssd\r\n[  687.294352] [ 3557]  1000  3557 11300573  1836051    7255      21  1781592             0 python\r\n[  687.294355] Out of memory: Kill process 3557 (python) score 886 or sacrifice child\r\n[  687.294382] Killed process 3557 (python) total-vm:45202292kB, anon-rss:7263600kB, file-rss:80604kB\r\n", "for 2 1080P images, still run out the memory\r\n\r\n[ 1525.675820] python invoked oom-killer: gfp_mask=0x24280ca, order=0, oom_score_adj=0\r\n[ 1525.675823] python cpuset=/ mems_allowed=0\r\n[ 1525.675827] CPU: 1 PID: 3986 Comm: python Tainted: P           OE   4.4.0-72-generic #93-Ubuntu\r\n[ 1525.675828] Hardware name: Gigabyte Technology Co., Ltd. B85M-D3V/B85M-D3V, BIOS FM 08/13/2015\r\n[ 1525.675829]  0000000000000286 000000007e4c64a1 ffff88021f9afaf8 ffffffff813f82b3\r\n[ 1525.675831]  ffff88021f9afcb0 ffff8801ca4c0000 ffff88021f9afb68 ffffffff8120b24e\r\n[ 1525.675832]  0000000000000015 0000000000000000 ffff880220b10240 ffff880222693800\r\n[ 1525.675834] Call Trace:\r\n[ 1525.675838]  [<ffffffff813f82b3>] dump_stack+0x63/0x90\r\n[ 1525.675841]  [<ffffffff8120b24e>] dump_header+0x5a/0x1c5\r\n[ 1525.675843]  [<ffffffff81391254>] ? apparmor_capable+0xc4/0x1b0\r\n[ 1525.675845]  [<ffffffff811928c2>] oom_kill_process+0x202/0x3c0\r\n[ 1525.675847]  [<ffffffff81192ce9>] out_of_memory+0x219/0x460\r\n[ 1525.675849]  [<ffffffff81198cd8>] __alloc_pages_slowpath.constprop.88+0x938/0xad0\r\n[ 1525.675851]  [<ffffffff811990f6>] __alloc_pages_nodemask+0x286/0x2a0\r\n[ 1525.675853]  [<ffffffff811e43cd>] alloc_pages_vma+0xad/0x250\r\n[ 1525.675856]  [<ffffffff811c2241>] handle_mm_fault+0x1491/0x1820\r\n[ 1525.675859]  [<ffffffff8106b527>] __do_page_fault+0x197/0x400\r\n[ 1525.675861]  [<ffffffff8106b7b2>] do_page_fault+0x22/0x30\r\n[ 1525.675863]  [<ffffffff8183e7f8>] page_fault+0x28/0x30\r\n[ 1525.675864] Mem-Info:\r\n[ 1525.675867] active_anon:1574947 inactive_anon:315984 isolated_anon:72\r\n                active_file:174 inactive_file:119 isolated_file:0\r\n                unevictable:926 dirty:2 writeback:3417 unstable:0\r\n                slab_reclaimable:7894 slab_unreclaimable:11379\r\n                mapped:38164 shmem:983 pagetables:17316 bounce:0\r\n                free:25988 free_pcp:272 free_cma:0\r\n[ 1525.675869] Node 0 DMA free:15900kB min:132kB low:164kB high:196kB active_anon:0kB inactive_anon:0kB active_file:0kB inactive_file:0kB unevictable:0kB isolated(anon):0kB isolated(file):0kB present:15984kB managed:15900kB mlocked:0kB dirty:0kB writeback:0kB mapped:0kB shmem:0kB slab_reclaimable:0kB slab_unreclaimable:0kB kernel_stack:0kB pagetables:0kB unstable:0kB bounce:0kB free_pcp:0kB local_pcp:0kB free_cma:0kB writeback_tmp:0kB pages_scanned:0 all_unreclaimable? yes\r\n[ 1525.675872] lowmem_reserve[]: 0 3098 7823 7823 7823\r\n[ 1525.675874] Node 0 DMA32 free:45464kB min:26712kB low:33388kB high:40068kB active_anon:2406684kB inactive_anon:612436kB active_file:256kB inactive_file:232kB unevictable:280kB isolated(anon):128kB isolated(file):0kB present:3297140kB managed:3216484kB mlocked:280kB dirty:0kB writeback:3700kB mapped:60800kB shmem:3128kB slab_reclaimable:11032kB slab_unreclaimable:15740kB kernel_stack:4032kB pagetables:25644kB unstable:0kB bounce:0kB free_pcp:0kB local_pcp:0kB free_cma:0kB writeback_tmp:0kB pages_scanned:6688 all_unreclaimable? yes\r\n[ 1525.675877] lowmem_reserve[]: 0 0 4724 4724 4724\r\n[ 1525.675878] Node 0 Normal free:42588kB min:40736kB low:50920kB high:61104kB active_anon:3893104kB inactive_anon:651500kB active_file:440kB inactive_file:244kB unevictable:3424kB isolated(anon):160kB isolated(file):0kB present:4970496kB managed:4838116kB mlocked:3424kB dirty:8kB writeback:9968kB mapped:91856kB shmem:804kB slab_reclaimable:20544kB slab_unreclaimable:29776kB kernel_stack:6496kB pagetables:43620kB unstable:0kB bounce:0kB free_pcp:1088kB local_pcp:4kB free_cma:0kB writeback_tmp:0kB pages_scanned:58672 all_unreclaimable? yes\r\n[ 1525.675881] lowmem_reserve[]: 0 0 0 0 0\r\n[ 1525.675883] Node 0 DMA: 1*4kB (U) 1*8kB (U) 1*16kB (U) 0*32kB 2*64kB (U) 1*128kB (U) 1*256kB (U) 0*512kB 1*1024kB (U) 1*2048kB (M) 3*4096kB (M) = 15900kB\r\n[ 1525.675889] Node 0 DMA32: 182*4kB (UME) 168*8kB (UME) 109*16kB (UME) 99*32kB (UME) 49*64kB (UME) 20*128kB (UME) 9*256kB (UME) 6*512kB (UME) 27*1024kB (M) 0*2048kB 0*4096kB = 45704kB\r\n[ 1525.675896] Node 0 Normal: 258*4kB (UMEH) 354*8kB (UEH) 151*16kB (UEH) 84*32kB (UMEH) 38*64kB (UME) 11*128kB (UMEH) 3*256kB (UMH) 1*512kB (M) 28*1024kB (UM) 0*2048kB 0*4096kB = 42760kB\r\n[ 1525.675903] Node 0 hugepages_total=0 hugepages_free=0 hugepages_surp=0 hugepages_size=1048576kB\r\n[ 1525.675904] Node 0 hugepages_total=0 hugepages_free=0 hugepages_surp=0 hugepages_size=2048kB\r\n[ 1525.675904] 16829 total pagecache pages\r\n[ 1525.675905] 14906 pages in swap cache\r\n[ 1525.675906] Swap cache stats: add 4516454, delete 4501548, find 319017/564186\r\n[ 1525.675907] Free swap  = 0kB\r\n[ 1525.675907] Total swap = 8282108kB\r\n[ 1525.675908] 2070905 pages RAM\r\n[ 1525.675908] 0 pages HighMem/MovableOnly\r\n[ 1525.675909] 53280 pages reserved\r\n[ 1525.675909] 0 pages cma reserved\r\n[ 1525.675910] 0 pages hwpoisoned\r\n[ 1525.675911] [ pid ]   uid  tgid total_vm      rss nr_ptes nr_pmds swapents oom_score_adj name\r\n[ 1525.675918] [  382]     0   382    10997      578      24       3      685             0 systemd-journal\r\n[ 1525.675920] [  414]     0   414    23693      123      17       3       40             0 lvmetad\r\n[ 1525.675922] [  420]     0   420    11563      586      24       3      362         -1000 systemd-udevd\r\n[ 1525.675924] [  820]   100   820    25596      312      20       3       13             0 systemd-timesyn\r\n[ 1525.675925] [  997]     0   997    84333      457      67       3      304             0 ModemManager\r\n[ 1525.675927] [ 1004]   113  1004    11197       78      27       3       56             0 avahi-daemon\r\n[ 1525.675928] [ 1006]     0  1006     7155      462      18       3       14             0 systemd-logind\r\n[ 1525.675930] [ 1012]     0  1012     1100      272       8       3       32             0 acpid\r\n[ 1525.675931] [ 1016]   104  1016    64099      352      28       3     1204             0 rsyslogd\r\n[ 1525.675933] [ 1018]     0  1018    40226      225      15       4       60             0 lxcfs\r\n[ 1525.675934] [ 1024]   111  1024    93498      456      79       3      362             0 whoopsie\r\n[ 1525.675935] [ 1027]     0  1027     8124      408      21       3       47             0 cron\r\n[ 1525.675936] [ 1034]     0  1034     6511      385      17       3       24             0 atd\r\n[ 1525.675938] [ 1036]   107  1036    11057      563      27       3      251          -900 dbus-daemon\r\n[ 1525.675939] [ 1060]   113  1060    11197        6      25       3       76             0 avahi-daemon\r\n[ 1525.675941] [ 1084]     0  1084   113088      452      72       3      317             0 NetworkManager\r\n[ 1525.675942] [ 1091]     0  1091    69842      395      40       4        8             0 accounts-daemon\r\n[ 1525.675944] [ 1093]     0  1093    56460      489      31       5     1329             0 snapd\r\n[ 1525.675945] [ 1119]     0  1119    71359      532      42       4      511             0 polkitd\r\n[ 1525.675946] [ 1128]     0  1128     3344       25      11       4       13             0 mdadm\r\n[ 1525.675948] [ 1229]     0  1229     3999       50      11       3      162             0 dhclient\r\n[ 1525.675949] [ 1351]     0  1351     1306       14       9       3       16             0 iscsid\r\n[ 1525.675950] [ 1352]     0  1352     1431      878       9       3        0           -17 iscsid\r\n[ 1525.675952] [ 1421]     0  1421    87602      416      40       4      210             0 lightdm\r\n[ 1525.675953] [ 1423]     0  1423     4857      352      13       3        0             0 agetty\r\n[ 1525.675954] [ 1428]     0  1428     4892      296      14       3       50             0 irqbalance\r\n[ 1525.675956] [ 1434]     0  1434    59919     3110      90       3     4769             0 Xorg\r\n[ 1525.675957] [ 1533]     0  1533    57576      415      48       3      204             0 lightdm\r\n[ 1525.675959] [ 1590]   119  1590    45886      345      24       3       77             0 rtkit-daemon\r\n[ 1525.675960] [ 1623]  1000  1623    11312      324      26       3      221             0 systemd\r\n[ 1525.675962] [ 1625]  1000  1625    15863      108      34       4      418             0 (sd-pam)\r\n[ 1525.675963] [ 1630]  1000  1630    52209      301      34       3      650             0 gnome-keyring-d\r\n[ 1525.675964] [ 1632]  1000  1632    12526      462      31       4      261             0 upstart\r\n[ 1525.675966] [ 1713]  1000  1713     9087      275      19       3       68             0 upstart-udev-br\r\n[ 1525.675967] [ 1714]  1000  1714    10981      596      25       3      150             0 dbus-daemon\r\n[ 1525.675968] [ 1726]  1000  1726    22460      104      35       3      178             0 window-stack-br\r\n[ 1525.675970] [ 1773]  1000  1773   138779      432     183       4     8630             0 fcitx\r\n[ 1525.675971] [ 1780]  1000  1780     9071       44      17       3       77             0 upstart-dbus-br\r\n[ 1525.675973] [ 1783]  1000  1783     9071       39      18       3       92             0 upstart-dbus-br\r\n[ 1525.675974] [ 1787]  1000  1787    11227      183      22       3       90             0 upstart-file-br\r\n[ 1525.675975] [ 1788]  1000  1788    42507      216      17       3       42             0 gpg-agent\r\n[ 1525.675976] [ 1794]  1000  1794    10754      314      24       3       95             0 dbus-daemon\r\n[ 1525.675978] [ 1798]  1000  1798     6909      225      17       3       65             0 fcitx-dbus-watc\r\n[ 1525.675979] [ 1805]  1000  1805   130100      609     119       4     1248             0 bamfdaemon\r\n[ 1525.675980] [ 1815]  1000  1815   140953      537     131       3     1481             0 hud-service\r\n[ 1525.675982] [ 1817]  1000  1817   227177      473     166       4     2154             0 unity-settings-\r\n[ 1525.675983] [ 1824]  1000  1824    84503      320      34       4      161             0 at-spi-bus-laun\r\n[ 1525.675984] [ 1825]  1000  1825   174671      463     100       4      446             0 gnome-session-b\r\n[ 1525.675986] [ 1835]  1000  1835    69502      383      38       3      627             0 gvfsd\r\n[ 1525.675987] [ 1836]  1000  1836   195276     1212     132       3     1365             0 unity-panel-ser\r\n[ 1525.675988] [ 1841]  1000  1841    10756      364      26       3      156             0 dbus-daemon\r\n[ 1525.675990] [ 1846]  1000  1846   101715      337      31       3      184             0 gvfsd-fuse\r\n[ 1525.675991] [ 1863]     0  1863    87687      625      56       3      101             0 upowerd\r\n[ 1525.675992] [ 1899]  1000  1899    51717      255      36       3      175             0 at-spi2-registr\r\n[ 1525.675994] [ 1916]  1000  1916    44698      353      25       4      180             0 dconf-service\r\n[ 1525.675995] [ 1932]  1000  1932   107912      410      45       3      250             0 indicator-messa\r\n[ 1525.675996] [ 1933]  1000  1933   102654      223      35       3      168             0 indicator-bluet\r\n[ 1525.675998] [ 1934]  1000  1934   124770      371      44       4     1180             0 indicator-power\r\n[ 1525.675999] [ 1936]  1000  1936   325541      182     104       4      529             0 indicator-datet\r\n[ 1525.676001] [ 1943]  1000  1943   159335      263     126       4     1451             0 indicator-keybo\r\n[ 1525.676002] [ 1944]  1000  1944   185246      411      72       4      984             0 indicator-sound\r\n[ 1525.676003] [ 1947]  1000  1947   135871      265     124       3     1497             0 indicator-print\r\n[ 1525.676005] [ 1948]  1000  1948   239095      330      47       4      767             0 indicator-sessi\r\n[ 1525.676006] [ 1973]  1000  1973   128161      379      91       3      734             0 pulseaudio\r\n[ 1525.676008] [ 1986]   114  1986    76180      552      50       3      546             0 colord\r\n[ 1525.676009] [ 1987]  1000  1987   156202      351     150       4      950             0 evolution-sourc\r\n[ 1525.676010] [ 1990]  1000  1990    98985      195      90       3      430             0 indicator-appli\r\n[ 1525.676011] [ 2022]  1000  2022   108571      309     110       4     1356             0 notify-osd\r\n[ 1525.676013] [ 2030]  1000  2030   313546    10352     287       5    12466             0 compiz\r\n[ 1525.676014] [ 2048]  1000  2048   182844       93     146       4     1736             0 goa-daemon\r\n[ 1525.676016] [ 2055]  1000  2055   214844      416     192       4     9524             0 evolution-calen\r\n[ 1525.676017] [ 2064]  1000  2064   221385      677     188       4     3377             0 nautilus\r\n[ 1525.676018] [ 2066]  1000  2066   106405      235     107       4      847             0 polkit-gnome-au\r\n[ 1525.676020] [ 2067]  1000  2067   326650       92     227       5    15705             0 gnome-software\r\n[ 1525.676021] [ 2072]  1000  2072   146486      370     142       3     2344             0 nm-applet\r\n[ 1525.676022] [ 2073]  1000  2073   143281      199     108       3      872             0 unity-fallback-\r\n[ 1525.676024] [ 2090]  1000  2090    90665        0      46       4      925             0 gvfs-udisks2-vo\r\n[ 1525.676025] [ 2096]     0  2096    91869      987      45       4      175             0 udisksd\r\n[ 1525.676026] [ 2108]  1000  2108    65257      372      29       3      144             0 gvfs-goa-volume\r\n[ 1525.676028] [ 2122]     0  2122   157222      422      91       3     6114          -900 fwupd\r\n[ 1525.676029] [ 2131]  1000  2131    70778      181      41       3      232             0 goa-identity-se\r\n[ 1525.676030] [ 2136]  1000  2136    94046      361      52       4      767             0 mission-control\r\n[ 1525.676032] [ 2139]  1000  2139    65783      248      30       3      171             0 gvfs-mtp-volume\r\n[ 1525.676033] [ 2144]  1000  2144    68832      222      36       3      722             0 gvfs-gphoto2-vo\r\n[ 1525.676034] [ 2152]  1000  2152   101774      374      51       4      277             0 gvfs-afc-volume\r\n[ 1525.676036] [ 2186]  1000  2186    88514      349      40       3      245             0 gvfsd-trash\r\n[ 1525.676037] [ 2188]  1000  2188    16165      366      36       3      104             0 gconfd-2\r\n[ 1525.676038] [ 2225]  1000  2225   202835      349     111       4     9239             0 evolution-calen\r\n[ 1525.676040] [ 2234]  1000  2234   173528      374     115       3     1248             0 evolution-addre\r\n[ 1525.676041] [ 2236]  1000  2236   266515      372     113       4     9733             0 evolution-calen\r\n[ 1525.676042] [ 2267]  1000  2267   192971      344      94       4     1278             0 evolution-addre\r\n[ 1525.676044] [ 2270]  1000  2270   167547      427     173       4     1795             0 sogou-qimpanel-\r\n[ 1525.676045] [ 2271]  1000  2271   776899      257     222       5     5284             0 sogou-qimpanel\r\n[ 1525.676046] [ 2356]  1000  2356   147981      623     138       4     1878             0 gnome-terminal-\r\n[ 1525.676048] [ 2361]  1000  2361     7358      523      19       3     1229             0 bash\r\n[ 1525.676049] [ 2363]  1000  2363     1127      345       8       3       28             0 pycharm.sh\r\n[ 1525.676050] [ 2566]  1000  2566     1127      140       7       3       25             0 sh\r\n[ 1525.676052] [ 2577]  1000  2577   102549      229      35       3      290             0 zeitgeist-daemo\r\n[ 1525.676053] [ 2604]  1000  2604  1153069    39124     447       7    88918             0 java\r\n[ 1525.676054] [ 2625]  1000  2625    78156      303      44       4      539             0 zeitgeist-fts\r\n[ 1525.676056] [ 2627]  1000  2627   127804      149     104       4      571             0 zeitgeist-datah\r\n[ 1525.676057] [ 2686]  1000  2686     2217      329      10       3      106             0 fsnotifier64\r\n[ 1525.676058] [ 2716]  1000  2716   390063     3746     317      34    15830             0 gitkraken\r\n[ 1525.676060] [ 2730]  1000  2730    77895      379     130       4      758             0 gitkraken\r\n[ 1525.676061] [ 2731]  1000  2731   140923      226     127       4     1122             0 update-notifier\r\n[ 1525.676062] [ 2781]  1000  2781   119355     5359     206       5     8303             0 gitkraken\r\n[ 1525.676064] [ 2805]  1000  2805   243557      509     184      37     4193           300 gitkraken\r\n[ 1525.676065] [ 2809]  1000  2809   455706     9100     451     125    30161           300 gitkraken\r\n[ 1525.676066] [ 2811]  1000  2811   265132      127     205      43    11389           300 gitkraken\r\n[ 1525.676068] [ 3005]  1000  3005     7384      450      20       3     1287             0 bash\r\n[ 1525.676069] [ 3422]  1000  3422   109736      306      49       3      767             0 deja-dup-monito\r\n[ 1525.676070] [ 3486]  1000  3486    89109      314      43       3      236             0 gvfsd-network\r\n[ 1525.676072] [ 3520]     0  3520     4634      737      14       3      348             0 mount.ntfs\r\n[ 1525.676073] [ 3530]  1000  3530    47358      169      28       3      638             0 gvfsd-metadata\r\n[ 1525.676075] [ 3541]  1000  3541    91302      209      44       3      222             0 gvfsd-dnssd\r\n[ 1525.676076] [ 3711]  1000  3711   260792     1420     290       5     9465             0 chrome\r\n[ 1525.676077] [ 3718]  1000  3718     2734        0      11       3       24             0 cat\r\n[ 1525.676079] [ 3719]  1000  3719     2734        0      11       3       25             0 cat\r\n[ 1525.676080] [ 3726]  1000  3726    89362      474     138       5     1984             0 chrome\r\n[ 1525.676081] [ 3727]  1000  3727    20369      351      37       4      602             0 nacl_helper\r\n[ 1525.676083] [ 3731]  1000  3731    89362      262     105       5     1984             0 chrome\r\n[ 1525.676084] [ 3764]  1000  3764   135906     5974     228       6    19291           200 chrome\r\n[ 1525.676085] [ 3797]  1000  3797    98235     1438     124       4     3445           200 chrome\r\n[ 1525.676087] [ 3864]  1000  3864   228296     2603     333     103    18017           300 chrome\r\n[ 1525.676088] [ 3883]  1000  3883   186986      216     200      40     4663           300 chrome\r\n[ 1525.676090] [ 3901]  1000  3901    61267      300      56       3     3489             0 chrome-gnome-sh\r\n[ 1525.676091] [ 3986]  1000  3986 11281848  1822042    7091      21  1711900             0 python\r\n[ 1525.676092] Out of memory: Kill process 3986 (python) score 866 or sacrifice child\r\n[ 1525.676124] Killed process 3986 (python) total-vm:45127392kB, anon-rss:7207556kB, file-rss:80612kB\r\n\r\n**the total-vm looks the same**", "The same result for 2 720P images\r\n\r\nmy device info:\r\n\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GTX 1060 6GB\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7845\r\npciBusID 0000:01:00.0\r\nTotal memory: 5.92GiB\r\nFree memory: 5.46GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0)\r\nKilled\r\n\r\n\r\nSo what can I do for this @drpngx @aselle , thanks a lot", "I use 64 images with 96x96 size for training now, 2 720P images require 6 times more memory. This network has 16 layers for generation and 10 layers for discriminator, does each layer duplicates the image batch tensor? So I need to cut some layers or use a better GPU?", "You could reduce the layers or change the number channels in the layers so the intermediates require less memory. Of course this will affect accuracy of the model. I would suggest asking on stackoverflow how best to manage to reduce memory while doing SRGAN, because I think you'll find more people who have experience with it...\r\n\r\nYou can look at this under implementation I found by googling...\r\nhttps://github.com/buriburisuri/SRGAN\r\n\r\n@zheng-xq, could you give some strategies for optimizing memory debugging?", "@aselle thx for your suggestion. I will try to reduce the tensor scale.", "@ruiann why don't try a different approach starting to reduce the GPU allocated memory like:\r\n\r\n```python\r\nimport tensorflow as tf\r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.25)\r\nsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\r\n```", "@loretoparisi I don't think it can solve my problem.  The problem is that my network needs more than 6G GPU memory to train images with bigger size, but my GPU cannot stand it", "@ruiann having similar issues on `Nvidia Grid K520` on EC2, and one of the trick was that one, since TF was allocating all the memory available on the GPU with no reason.", "@loretoparisi thanks for your suggestion, I've tried 0.5 & 0.8, but doesn't work for my situation, maybe my network do need more memory than this GPU can offer, as you can see this network has 26 layers, the input image tensor's size will be duplicated many times I think. I guess the only way to support such a high-layer network is to create a good gc for each training step.", "@ruiann in this case I think yes, since you should use the opposite:\r\n\r\n```python\r\nsess = tf.Session(config=tf.GPUOptions(allow_growth=True))\r\n```\r\n\r\nbut with a good GC as you suggest.", "Closing since this seems to have generated some useful suggestions. Please let me know if there is still an outstanding issue."]}, {"number": 9418, "title": "Clarify scope argument documentation tf.get_collection. Fixes #9314.", "body": "cc: @drpngx ", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "Jenkins, test this please.\n\nOn Apr 24, 2017 8:45 AM, \"googlebot\" <notifications@github.com> wrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project. Before we can look at your\n> pull request, you'll need to sign a Contributor License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed, please reply here (e.g. I signed it!) and we'll\n> verify. Thanks.\n> ------------------------------\n>\n>    - If you've already signed a CLA, it's possible we don't have your\n>    GitHub username or you're using a different email address. Check your\n>    existing CLA data <https://cla.developers.google.com/clas> and verify\n>    that your email is set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - If you signed the CLA as a corporation, please let us know the\n>    company's name.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9418#issuecomment-296712514>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbVUl8RBbsQ99mFCI8JNNL9ykm4syks5rzMOZgaJpZM4NGWNY>\n> .\n>\n", "I signed it", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 9417, "title": "Faild to build a pip package for TensorFlow with GPU", "body": "TF master - 24/2/2017\r\n\r\nWhen I build a pip package for TensorFlow with GPU support by invoke the following command:\r\n$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\nI see following error;\r\n\r\nERROR: /root/tensorflow/tensorflow/contrib/verbs/BUILD:61:1: C++ compilation of rule '//tensorflow/contrib/verbs:grpc_verbs_service' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 151 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.                                                   \r\nIn file included from ./tensorflow/contrib/verbs/grpc_verbs_service.h:25:0,                                       \r\n                 from tensorflow/contrib/verbs/grpc_verbs_service.cc:22:                                          \r\n./tensorflow/core/distributed_runtime/rpc/grpc_call.h:74:28: error: 'core' has not been declared                  \r\n class UntypedCall : public core::RefCounted {                                                                    \r\n                            ^                                                                                     \r\n./tensorflow/core/distributed_runtime/rpc/grpc_call.h:74:34: error: expected '{' before 'RefCounted'              \r\n class UntypedCall : public core::RefCounted {                                                                    \r\n                                  ^                                                                               \r\n./tensorflow/core/distributed_runtime/rpc/grpc_call.h:75:2: error: expected primary-expression before 'public'    \r\n  public:                                                                                                         \r\n  ^                                                                                                               \r\n./tensorflow/core/distributed_runtime/rpc/grpc_call.h:75:2: error: expected '}' before 'public'", "comments": ["Could you give us more information about gcc versions etc?\r\n\r\n/CC @poxvoculi @junshi15 @caisq ", "Also, please specify what platform (and linux distribution)", "The date \"24/2/2017\" in @bkovalev 's description looks like a typo? contrib/verbs wasn't added until very recently.", "I installed two day ago same server without any problem.\r\nI am running on Ubuntu 16.04 with gcc (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609, Cuda compilation tools, release 8.0, V8.0.61, TF Master,nVidia Version: 375.51, Bazel 0.4.5.\r\n\r\nThanks\r\n", "I fixed the CROSSTOOL file ( know issue) by edit the text file tensorflow/third_party/gpus/crosstool/CROSSTOOL. \r\n$ vim ~/tensorflow/third_party/gpus/crosstool/CROSSTOOL.tpl \r\nb.\tAdd cxx_builtin_include_directory: \"%{cuda_include_path}\" (line 125) as below.\r\ncxx_builtin_include_directory: \"/usr/lib/gcc/\"\r\ncxx_builtin_include_directory: \"/usr/local/include\"\r\ncxx_builtin_include_directory: \"/usr/include\"\r\ncxx_builtin_include_directory: \"/usr/local/cuda-8.0/include\"\r\ntool_path { name: \"gcov\" path: \"/usr/bin/gcov\" }\r\n\r\nBut still have the error", "@jart, do you have any idea if anything we added recently could have caused this?", "For context, RDMA was introduced here: https://github.com/tensorflow/tensorflow/pull/8943\r\nso it's pretty recent. It worked on Jenkins, and it looks from @bkovalev 's comment that it should have also worked.\r\n\r\nIt looks like it's a simple include missing that we used to get from transitive includes.", "You should include `tensorflow/core/lib/core/refcount.h`.\r\n\r\n@bkovalev do you mind sending a PR for that?", "Would it work if you used GCC 4.x? It comes with Ubuntu. I've seen a lot of problems introduced by GCC 5.x here and there.", "I can reproduce the issue with gcc 4.8.4.  \r\n\r\nrefcount.h is here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/grpc_verbs_service.h#L26\r\n\r\nIt looks like something else is missing. Looking into it...", "@bkovalev it needs to be in `grpc_call.h`. That's an error in the code.", "I had the same problem when building RDMA support. Agree with @drpngx .\r\n```\r\ndiff --git a/tensorflow/core/distributed_runtime/rpc/grpc_call.h b/tensorflow/core/distributed_runtime/rpc/grpc_call.h\r\nindex 35f849c..3b45e7e 100644\r\n--- a/tensorflow/core/distributed_runtime/rpc/grpc_call.h\r\n+++ b/tensorflow/core/distributed_runtime/rpc/grpc_call.h\r\n@@ -16,6 +16,7 @@ limitations under the License.\r\n #ifndef THIRD_PARTY_TENSORFLOW_CORE_DISTRIBUTED_RUNTIME_RPC_GRPC_CALL_H_\r\n #define THIRD_PARTY_TENSORFLOW_CORE_DISTRIBUTED_RUNTIME_RPC_GRPC_CALL_H_\r\n \r\n+#include \"tensorflow/core/lib/core/refcount.h\"\r\n #include \"tensorflow/core/platform/macros.h\"\r\n \r\n #include \"grpc++/grpc++.h\"\r\n```", "The code compiled fine at commit dd40e9840da886c2a8b296146c709907c0dd3d85, where PR#8943 was merged. But it does not compile at the top of the branch as of now.", "@llhe did that work for you? If so, could you send a PR?", "@junshi15 This issue is not caused by PR#8934, but an error in `grpc_call.h` which should include `refcount.h`. However, `refcount.h` was luckly included before `grpc_call.h` in `grpc_verbs_service` target (and also other targets). And there should a recent change which removed this indirect include which leads to this compile error.", "@drpngx I made the change and I can submit a PR.", "@llhe that would be nice! If that fixes the problem, please put `Fixes #9417` in the description. Thanks!", "Thanks a lot!\r\n", "As @llhe said, refcount.h was included before grpc_call.h in #8493\r\n\r\n+#include \"tensorflow/core/lib/core/refcount.h\"\r\n+#include \"tensorflow/core/distributed_runtime/rpc/grpc_call.h\"\r\n\r\nAnd in #9384\r\n\r\n-#include \"tensorflow/core/lib/core/refcount.h\"\r\n  #include \"tensorflow/core/distributed_runtime/rpc/grpc_call.h\"\r\n +#include \"tensorflow/core/lib/core/refcount.h\"\r\n\r\nNot sure I understand why in #9384 the merge operation show conflicts of ~260 files, that were not relevant to the specific pull request.\r\nWorking with rebase would have the same effect here ?"]}, {"number": 9416, "title": "Android Demo: Rotation on some devices broken.", "body": "### System information\r\n- **Android**:\r\n- **TensorFlow installed from source**:\r\n- **TensorFlow version 1.0.0**:\r\nN/A\r\n\r\n### Describe the problem\r\nThe Android Demo code seems to be a bit buggy on our Pixel C. I wanted to confirm that this isn't an issue with the TF code but it's a strange one.\r\n\r\nThe Rotation in the demo apps on our Pixel C didn't seem to work correctly. It would give the value of 3 when it should have read 90. This meant that the input was basically on it's side. I found that for it to work that this line needed to be set to 0.\r\n\r\n```sensorOrientation = rotation + screenOrientation;``` needed to be\r\n```sensorOrientation = 0;```\r\nHowever, when used in landscape mode this didn't crop around the centre of the preview image but instead it created a square from the left edge.\r\n\r\n```\r\n|------------------| ---------|                         \r\n|                  |          |\r\n|       CROP       |          |\r\n|     SQUARE       |          |\r\n|__________________|__________|\r\n```\r\nWhen set to 0.\r\nThe solution seemed to be to set the Orientation to 360.  \r\n\r\n```\r\n|-----|------------------|----|                         \r\n|     |                  |    |\r\n|     |        CROP      |    |\r\n|     |      SQUARE      |    |\r\n|_____|__________________|____|\r\n```\r\nWhen set to 360.\r\n\r\nAt the time (a couple of weeks ago) I recalled that it forced some code to run that would centre it as required for some other operations however going through the code to find it I can't. I think it was related to this but it doesn't make sense as to why 360 solved the issue.\r\n```\r\nprivate void configureTransform(final int viewWidth, final int viewHeight) {\r\n    final Activity activity = getActivity();\r\n    if (null == textureView || null == previewSize || null == activity) {\r\n      return;\r\n    }\r\n    final int rotation = activity.getWindowManager().getDefaultDisplay().getRotation();\r\n    final Matrix matrix = new Matrix();\r\n    final RectF viewRect = new RectF(0, 0, viewWidth, viewHeight);\r\n    final RectF bufferRect = new RectF(0, 0, previewSize.getHeight(), previewSize.getWidth());\r\n    final float centerX = viewRect.centerX();\r\n    final float centerY = viewRect.centerY();\r\n    if (Surface.ROTATION_90 == rotation || Surface.ROTATION_270 == rotation) {\r\n      bufferRect.offset(centerX - bufferRect.centerX(), centerY - bufferRect.centerY());\r\n      matrix.setRectToRect(viewRect, bufferRect, Matrix.ScaleToFit.FILL);\r\n      final float scale =\r\n          Math.max(\r\n              (float) viewHeight / previewSize.getHeight(),\r\n              (float) viewWidth / previewSize.getWidth());\r\n      matrix.postScale(scale, scale, centerX, centerY);\r\n      matrix.postRotate(90 * (rotation - 2), centerX, centerY);\r\n    } else if (Surface.ROTATION_180 == rotation) {\r\n      matrix.postRotate(180, centerX, centerY);\r\n    }\r\n    textureView.setTransform(matrix);\r\n  }\r\n```\r\n\r\nSo I guess the question is that is this happening due to some strange hardware bug with the Pixel C or some coding error. I'm leaning towards the former as this is too bizarre to be caused by code but thought i'd open the discussion out as others trying to run the app in landscape may face similar issues.", "comments": ["Maybe @andrewharp knows something about this", "@drpngx You beat me by about 10 seconds. :)", "WFH, with fast internet :-) but I have some meeting now, will leave the rest to you :-)", "@jubjamie To clarify, is the demo broken on your Pixel C even when compiling with no edits?", "Yes. With no edits it had the rotation value as 3. So I went in to try and force it to be the correct value (which in portrait was 90).\r\nIn landscape I encountered the centring problem and had to set it to 360.  No problem on my 6P. Not sure if you guys have another Pixel C lying around to confirm it's not my hardware being funny?", "So `characteristics.get(CameraCharacteristics.SENSOR_ORIENTATION)` returns 3? That really shouldn't be happening... I suppose it'd be ok to assume 90 in general if the value returned % 90 != 0.\r\n\r\nRegarding landscape support, this is something that would be nice to have but hasn't been high priority enough to work through all the transformations to ensure correctness. Contributions definitely welcome.", "Yup. I've seen somewhere else on here where someone's Rotation was 1. I appreciate it's not really important in the grand scheme of things. Just mildly frustrating when this is the only sample code available. Are there any more detailed tutorials or specific documentation planned that could help with getting apps up and running correctly?", "We have more walkthroughs planned, but they aren't specifically meant to improve upon camera support themselves.\r\n\r\nYou've already found #9142, and #8736 is meant to add android.hardware.Camera support for devices with wonky Camera2 behavior (like the Pixel C apparently) -- we just have to find the right discriminating features to make misbehaving devices choose the original API.\r\n", "Walkthroughs for anything are very welcome! Fortunately my work doesn't really require too much android development work but I'm sure others will appreciate. I'll try and get back to this issue in a couple of days. Ta.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I solved this problem.", "Hi @jubjamie,\r\ndo you know if is it possible to make the crop square full screen?\r\nI would like to recognize full screen and not just a part.\r\nThanks", "@IgMart \r\nFor the **classifier** activity you can change [MAINTAIN_ASPECT_RATIO](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/ClassifierActivity.java#L80) to false, in which case it will stretch the entire screen to the dimensions of the model input.\r\n\r\nFor the **detector** activity it already stretches with the default SSD model. The YOLO detector changes the behavior to use a center-crop via a similar [MAINTAIN_ASPECT_RATIO](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/DetectorActivity.java#L93) constant.\r\n\r\nIn either case you can verify the image actually being sent to TensorFlow by turning on debug mode with the volume keys.\r\n\r\n@jubjamie This particular rotation issue should have been fixed last year -- I went through and corrected the behavior so landscape should work on most devices.\r\n\r\n", "@andrewharp landscape is not working. I have tried and tested all the workaround I found on multiple devices and nothing happened (sensorOrientation for example).\r\nEven with no edits, simply changing the orientation to landscape I get a cropped sceen just as @jubjamie described. Where can I try to force rotation?", "@nanamare Hey, may i ask how you were able to resolve the issue? I tried the 360 solution, however, it the cropped square is still staying towards left side of the screen. I am trying to stretch the preview square to fit the whole screen. ", "I commented out the portion in set fragment, which is calling the legacy connection fragment. Then landscape mode seems to work without any changes(except in the manifest file) - camera2api being used. (Incase, we want to retain the if..else block and want the legacy fragment to be called, then need to make changes in legacyconnectionfragment.java file corresponding to landscape mode.)"]}, {"number": 9415, "title": "XLA \"resnet 50\" style example crash during graph building", "body": "Note: The test code below was cobbled together from various un-credited sources.\r\n\r\nThe example crashes, during the building of the reduce_mean.   It fails with quite a large stack, maybe some sort of stack overflow?  Without the last two 'block's (marked with #*****) the compilation succeeds.\r\n\r\nPerhaps it is as simple as it failing on my laptop (OS/X, 16GB RAM).\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef _get_variable(name, shape, initializer, dtype=tf.float32):\r\n\r\n  return tf.get_variable(name,\r\n                         shape=shape,\r\n                         initializer=initializer,\r\n                         dtype=dtype)\r\ndef inference(x):\r\n\r\n  with tf.variable_scope('scale1', use_resource=True):\r\n    x = conv(x, 7, 2, 64)\r\n    x = tf.nn.relu(x)\r\n\r\n  with tf.variable_scope('max_pool', use_resource=True):\r\n    x = max_pool(x, ksize=3, stride=2)\r\n\r\n  with tf.variable_scope('scale2-1', use_resource=True):\r\n    x = block(x, 1, 64, 256)\r\n\r\n  with tf.variable_scope('scale2-2', use_resource=True):\r\n    x = block(x, 1, 64, 256)\r\n\r\n  with tf.variable_scope('scale2-3', use_resource=True):\r\n    x = block(x, 1, 64, 256)\r\n\r\n\r\n\r\n  with tf.variable_scope('scale3-1', use_resource=True):\r\n    x = block(x, 2, 128, 512)\r\n\r\n  with tf.variable_scope('scale3-2', use_resource=True):\r\n    x = block(x, 1, 128, 512)\r\n\r\n  with tf.variable_scope('scale3-3', use_resource=True):\r\n    x = block(x, 1, 128, 512)\r\n\r\n  with tf.variable_scope('scale3-4', use_resource=True):\r\n    x = block(x, 1, 128, 512)\r\n\r\n\r\n\r\n  with tf.variable_scope('scale4-1', use_resource=True):\r\n    x = block(x, 2, 256, 1024)\r\n\r\n  with tf.variable_scope('scale4-2', use_resource=True):\r\n    x = block(x, 1, 256, 1024)\r\n\r\n  with tf.variable_scope('scale4-3', use_resource=True):\r\n    x = block(x, 1, 256, 1024)\r\n\r\n  with tf.variable_scope('scale4-4', use_resource=True):\r\n    x = block(x, 1, 256, 1024)\r\n\r\n  with tf.variable_scope('scale4-5', use_resource=True):\r\n    x = block(x, 1, 256, 1024)\r\n\r\n  with tf.variable_scope('scale4-6', use_resource=True):\r\n    x = block(x, 1, 256, 1024)\r\n\r\n\r\n\r\n  with tf.variable_scope('scale5-1', use_resource=True):\r\n    x = block(x, 2, 512, 2048)\r\n\r\n  with tf.variable_scope('scale5-2', use_resource=True):  #*****\r\n    x = block(x, 1, 512, 2048)\r\n\r\n  with tf.variable_scope('scale5-3', use_resource=True): #*****\r\n    x = block(x, 1, 512, 2048)\r\n\r\n  x = tf.reduce_mean(x, reduction_indices=[1, 2])\r\n\r\n  with tf.variable_scope('fc', use_resource=True):\r\n    x = fc(x, 1000)\r\n\r\n  return x\r\n\r\n\r\ndef block(x, first_stride, internal_filters, final_filters):\r\n  shape_in = x.get_shape()\r\n\r\n  shortcut = x\r\n\r\n  with tf.variable_scope('a', use_resource=True):\r\n    x = conv(x, 1, first_stride, internal_filters)\r\n    x = tf.nn.relu(x)\r\n\r\n  with tf.variable_scope('b', use_resource=True):\r\n    x = conv(x, 3, 1, internal_filters)\r\n    x = tf.nn.relu(x)\r\n\r\n  with tf.variable_scope('c', use_resource=True):\r\n    x = conv(x, 1, 1, final_filters)\r\n\r\n  with tf.variable_scope('shortcut', use_resource=True):\r\n    pad = int(x.get_shape()[-1] - shape_in[-1])\r\n    kernel = np.reshape(np.concatenate((np.identity(shape_in[-1], dtype=np.float32),\r\n                                        np.zeros([pad, shape_in[-1]]))),\r\n                        [1, 1, shape_in[-1], final_filters])\r\n\r\n    shortcut = tf.nn.conv2d(shortcut,\r\n                            kernel,\r\n                            [1,first_stride,first_stride,1],\r\n                            padding='SAME')\r\n\r\n  return tf.nn.relu(x + shortcut)\r\n\r\n\r\ndef fc(x, num_units_out):\r\n  num_units_in = x.get_shape()[1]\r\n  weights_initializer = tf.truncated_normal_initializer(stddev=0.01)\r\n\r\n  weights = _get_variable('weights', shape=[num_units_in, num_units_out],\r\n                          initializer=weights_initializer)\r\n  biases = _get_variable('biases', shape=[num_units_out],\r\n                         initializer=tf.constant_initializer(0.0))\r\n\r\n  x = tf.nn.xw_plus_b(x, weights, biases)\r\n\r\n  return x\r\n\r\ndef conv(x, ksize, stride, filters_out):\r\n\r\n  filters_in = x.get_shape()[-1]\r\n  shape = [ksize, ksize, filters_in, filters_out]\r\n  initializer = tf.truncated_normal_initializer(stddev=0.1)\r\n\r\n  weights = _get_variable('weights', shape=shape, initializer=initializer)\r\n  return tf.nn.conv2d(x,\r\n                      weights,\r\n                      [1, stride, stride, 1],\r\n                      padding='SAME')\r\n\r\n\r\ndef max_pool(x, ksize=3, stride=2):\r\n  return tf.nn.max_pool(x,\r\n                        ksize=[1, ksize, ksize, 1],\r\n                        strides=[1, stride, stride, 1],\r\n                        padding='SAME')\r\n\r\n\r\n#\r\n# Main code\r\n#\r\n\r\nwith tf.device(\"/job:localhost/replica:0/task:0/device:XLA_CPU:0\"):\r\n  # Inputs\r\n  x = tf.placeholder(tf.float32, shape=[2, 224, 224, 4])\r\n\r\n  # Inference\r\n  logits = inference(x)\r\n\r\nsess = tf.InteractiveSession()\r\n\r\nsess.run(tf.global_variables_initializer())\r\n\r\ntraining_data = np.zeros([2, 224, 224, 4]);\r\n\r\nsess.run(logits, feed_dict={x: training_data})\r\n\r\nsess.close()\r\n\r\n```", "comments": ["I know we're actively working on XLA/resnet, @tatatodd might be able to add some color on this."]}, {"number": 9414, "title": "CNN pip installed error E tensorflow/stream_executor/cuda/cuda_dnn.cc:352]", "body": "MacOS pip installed tensorflow \r\nOnly cnn triggered this error \r\n```\r\n2017-04-24 04:14:57.699073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 \r\n2017-04-24 04:14:57.699094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y \r\n2017-04-24 04:14:57.701039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 650M, pci bus id: 0000:01:00.0)\r\n2017-04-24 04:15:04.233947: E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] Loaded runtime CuDNN library: 5005 (compatibility version 5000) but source was compiled with 5105 (compatibility version 5100).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.\r\n2017-04-24 04:15:04.234420: F tensorflow/core/kernels/conv_ops.cc:659] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) \r\nAbort trap: 6\r\n```\r\n```\r\nName: tensorflow-gpu\r\nVersion: 1.1.0\r\nSummary: TensorFlow helps the tensors flow\r\nHome-page: http://tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: opensource@google.com\r\nLicense: Apache 2.0\r\nLocation: /Users/andrey/tf/lib/python3.6/site-packages\r\nRequires: wheel, protobuf, six, numpy, werkzeug\r\n```\r\nCUDA Driver Version: 8.0.81\r\nGPU Driver Version: 10.16.34 355.10.05.35f05\r\n\r\n```\r\nDevice 0: \"GeForce GT 650M\"\r\n  CUDA Driver Version / Runtime Version          8.0 / 8.0\r\n  CUDA Capability Major/Minor version number:    3.0\r\n  Total amount of global memory:                 1024 MBytes (1073414144 bytes)\r\n  ( 2) Multiprocessors, (192) CUDA Cores/MP:     384 CUDA Cores\r\n  GPU Max Clock rate:                            900 MHz (0.90 GHz)\r\n  Memory Clock rate:                             2508 Mhz\r\n  Memory Bus Width:                              128-bit\r\n  L2 Cache Size:                                 262144 bytes\r\n  Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)\r\n  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers\r\n  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers\r\n  Total amount of constant memory:               65536 bytes\r\n  Total amount of shared memory per block:       49152 bytes\r\n  Total number of registers available per block: 65536\r\n  Warp size:                                     32\r\n  Maximum number of threads per multiprocessor:  2048\r\n  Maximum number of threads per block:           1024\r\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\r\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\r\n  Maximum memory pitch:                          2147483647 bytes\r\n  Texture alignment:                             512 bytes\r\n  Concurrent copy and kernel execution:          Yes with 1 copy engine(s)\r\n  Run time limit on kernels:                     Yes\r\n  Integrated GPU sharing Host Memory:            No\r\n  Support host page-locked memory mapping:       Yes\r\n  Alignment requirement for Surfaces:            Yes\r\n  Device has ECC support:                        Disabled\r\n  Device supports Unified Addressing (UVA):      Yes\r\n  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0\r\n  Compute Mode:\r\n     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\r\n\r\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 8.0, CUDA Runtime Version = 8.0, NumDevs = 1, Device0 = GeForce GT 650M\r\n```\r\n", "comments": ["It looks like you *have to* build from source.\r\n\r\n```\r\n2017-04-24 04:15:04.233947: E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] Loaded runtime CuDNN library: 5005 (compatibility version 5000) but source was compiled with 5105 (compatibility version 5100).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.\r\n```\r\n\r\n@yifeif maybe we an document this?", "Our binary was built with cuDNN v5.1 according to the [installation instruction](https://www.tensorflow.org/install/install_mac). @nazandr could you reinstall cuDNN5.1, or build from source as @drpngx suggested? ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 9413, "title": "Android ops - not supporting NCHW data format", "body": "Hi,\r\n\r\nI'm using my custom tensorflow v1.0.1 model freezed and exported to Android arm-ABI-v7a (I compiled tensorflow 1.0.1 with selective registration for the models ops).\r\n\r\nAs a training time optimization we tried to convert the data format to NCHW format (as written in the tensorflow performance tutorial).\r\n\r\nBut now, when I run inference on Android, I get the following error in the logcat:\r\n\r\n E/native: tensorflow_inference_jni.cc:233 Error during inference: Invalid argument: CPU BiasOp only supports NHWC. [[Node: conv_layer_1/BiasAdd = BiasAdd[T=DT_FLOAT, data_format=\"NCHW\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv_layer_1/conv_l11, bc11_init)]]\r\n\r\n\r\nAre you going to add NCHW support for android CPU ops in the future versions?\r\n\r\nThanks,\r\nEran\r\n\r\n\r\n### System information\r\n- **Have I written custom code: Yes - custom model*:\r\n- **OS Linux Ubuntu 16.04:\r\n- **TensorFlow installed from source:\r\n- **TensorFlow version 1.0.1:\r\n- **Bazel version 0.4.5*:\r\n- **CUDA/cuDNN version 5.1.10:\r\n- **GPU model and memory GTX 1080 TI:\r\n\r\n", "comments": ["@andrewharp can comment further, but unfortunately mobile TensorFlow is stripped down for code size reasons.  The easiest solution is to use NHWC ordering.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 9412, "title": "Android Demo DetectorActivity landscape orientation problem", "body": "I'm trying to change the orientation to landscape:\r\n\r\n`private static final Size DESIRED_PREVIEW_SIZE = new Size(1280, 720);`\r\n`private static final boolean USE_YOLO = true`\r\n\r\n```\r\n        <activity android:name=\"org.tensorflow.demo.DetectorActivity\"\r\n                  android:screenOrientation=\"landscape\"\r\n                  android:label=\"@string/activity_name_detection\">\r\n            <intent-filter>\r\n                <action android:name=\"android.intent.action.MAIN\" />\r\n                <category android:name=\"android.intent.category.LAUNCHER\" />\r\n            </intent-filter>\r\n        </activity>\r\n```\r\n\r\nBut it draws detected rects wrongly.\r\nHow to fix it?\r\n\r\n", "comments": ["By any chance are you referring to the fact that it draws them correctly in the debug preview but off to the side and squished in the main view?", "Actually in the debug preview there is also nothing.\r\nAnd in the main view, for example on my screenshot 2, we can see false (train) detected object (on the left)\r\n\r\nportrait \r\n![portrait](https://cloud.githubusercontent.com/assets/8851301/25341785/add555bc-2912-11e7-8f06-5a508ca782ff.png)\r\n\r\nlandscape\r\n![landscape](https://cloud.githubusercontent.com/assets/8851301/25341789/b13bdf0a-2912-11e7-988a-c4503406d60a.png)", "If you look at the preview screen bottom right you will see that your input is coming through on it's side. That debug preview screen is what goes through the model. As it is on it's side the model will not preform as expected. That's why in landscape you're getting nonsense predictions. Almost all image recognition problems are **NOT** rotation invariant.\r\n\r\nOn another note, you'll notice that the blue train box is being drawn on a part of the image that isn't in the preview that goes through the model. That's because the rectangle drawing functions don't work correctly when the app is changed to landscape. I am also struggling with this issue and hope @andrewharp is able to comment on that particular issue. (Probably not a bug but rather me being too rubbish at understanding the source code),", "I tried this:\r\n\r\n```\r\npublic class DetectorActivity extends CameraActivity implements OnImageAvailableListener {\r\n \r\n ...\r\n \r\n @Override\r\n  public void onPreviewSizeChosen(final Size size, final int rotation) {\r\n    \r\n\t...\r\n\t\r\n    sensorOrientation = screenOrientation;\r\n    //sensorOrientation = rotation + screenOrientation;\r\n```\r\n\r\n![screenshot_2017-04-24-19-53-20-430_org tensorflow demo](https://cloud.githubusercontent.com/assets/8851301/25348960/ada4678e-2928-11e7-8e13-fcdc4db598c5.png)\r\n\r\n\r\nSo debug preview looks fine now \r\n\r\nSo now it's only main view.\r\nI'm not sure where to look, but I guess here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowYoloDetector.java#L205\r\n", "Yes that is the way to force the rotation. I also found problems with the rotation. I guess this is kind of linked with #9416 that @andrewharp has been assisting with.\r\n\r\nRegarding the boxes on the main view, it is on my todo list to try and figure out but will be a few weeks away if anyone else has any ideas?", "I tried to force rotation in the different part of code.\r\nRects on the main view looks better now (I guess it shows normal width/height of every detected car now),\r\nbut they are still at the wrong position..\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/DetectorActivity.java#L297\r\n```\r\nrgbFrameBitmap.setPixels(rgbBytes, 0, previewWidth, 0, 0, previewWidth, previewHeight);\r\n    final Canvas canvas = new Canvas(croppedBitmap);\r\n    canvas.drawBitmap(rgbFrameBitmap, frameToCropTransform, null);\r\n    croppedBitmap = rotateBitmap(croppedBitmap, -90); // here forced rotation\r\n```\r\n\r\n```\r\npublic static Bitmap rotateBitmap(Bitmap source, float angle) {\r\n    Matrix matrix = new Matrix();\r\n    matrix.postRotate(angle);\r\n    return Bitmap.createBitmap(source, 0, 0, source.getWidth(), source.getHeight(), matrix, true);\r\n  }\r\n```\r\n![screenshot_2017-04-30-00-01-16-782_org tensorflow demo](https://cloud.githubusercontent.com/assets/8851301/25558969/4dc2dfec-2d3a-11e7-8f00-2349713fefa0.png)", "I am also struggling with landscape mode for drawing correct rectangles around objects. Debug looks totally valid only preview side is remaining. any progress on this? ", "I have the same issue. Any luck till recently?", "I solved this problem.", "@nanamare can you tell us how you solved the issue?", "@nanamare We'd love to have this as a contribution, if you'd be up for submitting a pull request!", "@nanamare great, if the problem was related to `List<Recognition> recognizeImage(final Bitmap bitmap)` method (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowYoloDetector.java#L134)\r\n\r\nthen maybe you understand code pretty well and could also help with Yolo detector in Java for Windows/Ubuntu\r\n\r\nI already made a project in IntelliJ IDEA, I posted a result and full java classes from the project in my question https://stackoverflow.com/questions/47077983/yolo-detector-in-java\r\nI got some negative votes, but I don't care:)\r\n\r\nI'm just doing this for fun, it's just cool that we can run Tensorflow just everywhere and using different programming frameworks, languages, devices\r\nand Java is powerful for end-products - works on Windows/Ubuntu/Mac\r\n\r\n\r\n", "I've made a fix for this internally, should show up in the next push. You'll still have to manually change the orientation of the Activity in AndroidManifest.xml, but everything else should just work afterwards.", "@gunan @andrewharp will this work with screenOrientation=\"sensor\" or screenOrientation=\"fullSensor\"?\r\n\r\nThanks.", "I downloaded the latest android example project and set `android:screenOrientation=\"landscape\"`\r\n\r\ndebug is ok, but still something wrong with main ui view and its orientation:\r\n\r\n**TF Detect (SSD-Mobilene**) Activity\r\n\r\n![1](https://user-images.githubusercontent.com/8851301/33552419-7cb0f784-d8fd-11e7-99d3-fd1d8e7a5cc5.png)\r\n\r\n\r\np.s. I fixed java project (windows/ubuntu) for tensorflow yolo2/tiny yolo detector:  https://stackoverflow.com/a/47632412/7767664", "Have you solved it, please?", "@andrewharp I changed the orientation but it stays just as @anonym24 described... Any idea on how to fix this?", "I am having some issue camera is opening only 70% to the screen Remaining 30% of the screen is black. Any suggestion or solution how to fix that?", "Hi \r\n\r\nI have integrated tensor flow with YOLO in android. and I want to detect the objects in landscape and I also want camera preview in full screen.  \r\nIf I change the camera preview size. Object detection not working. Please help me. \r\n\r\nI have changed to support in landscape mode by changing \r\n sensorOrientation = screenOrientation;\r\n\r\nChange the camera preview size by adding \r\nprivate static Size DESIRED_PREVIEW_SIZE = new Size(1280, 720);\r\nand I have commented that method  chooseOptimalSize() to show camera in full screen.\r\n\r\n\r\nPlease help me to show the preview of a camera in full screen and also support for a landscape to detect the objects\r\n\r\nThanks \r\nShruti\r\n\r\n\r\n\r\n", "@Shruti-Mahajan \r\nchange your mobile setting to auto rotate then it will detect the objects in any mode.", "Ok, Thanks ", "Hi \r\n\r\nI want to get the distance of detected objects. Please guide me how to get the distance of detected objects. Actually, I already searched on the internet and I didn't get any idea.\r\nPlease help me.\r\n\r\nThanks", "@Shruti-Mahajan you should not ask for help for such tasks here, it's not about TensorFlow at all. ", "Just pulled the latest demo, still cannott change to landscape mode by simply changing the screenOrientation in manifest.xml, and change to auto rotate didn`t work for me @prasanna532 \r\n\r\nHere`s my work around:\r\n\r\nIn DetectorActivity:\r\n`sensorOrientation = 0 - getScreenOrientation();//here change rotation to 0`\r\n\r\nIn MultiBoxTracker:\r\n\r\n    final boolean rotated = sensorOrientation % 180 != 90;//here change == to !=\r\n\r\n    frameToCanvasMatrix =\r\n    ImageUtils.getTransformationMatrix(\r\n            frameWidth,\r\n            frameHeight,\r\n            (int) (multiplier * (rotated ? frameHeight : frameWidth)),\r\n            (int) (multiplier * (rotated ? frameWidth : frameHeight)),\r\n            sensorOrientation+90,//here add 90 to degree\r\n            false);\r\n\r\n\r\n    borderedText.drawText(canvas, trackedPos.left + cornerSize, trackedPos.top, labelString,90);//here draw from top, and modify the drawText function in env/BorderedText\r\n\r\nIn env/BorderedText:\r\n\r\n    //Add a new drawText function\r\n    public void drawText(final Canvas canvas, final float posX, final float posY, final String text, float angle) \r\n    {\r\n        if(angle != 0){\r\n            canvas.rotate(angle, posX, posY);\r\n        }\r\n        canvas.drawText(text, posX, posY, exteriorPaint);\r\n        canvas.drawText(text, posX, posY, interiorPaint);\r\n        if(angle != 0){\r\n            canvas.rotate(-angle, posX, posY);\r\n        }\r\n        \r\n    }\r\n\r\n", "I commented out the portion in set fragment, which is calling the legacy connection fragment. Then landscape mode seems to work without any changes(except in the manifest file) - camera2api being used. (Incase, we want to retain the if..else block and want the legacy fragment to be called, then need to make changes in legacyconnectionfragment.java file corresponding to landscape mode.)", "I have the same problem with newest example file. Please guide me how to change the screen orientation to landscape. I have tried all suggested modification here, but it does not work.", "> I am having some issue camera is opening only 70% to the screen Remaining 30% of the screen is black. Any suggestion or solution on how to fix that?\r\n\r\nI've solved the landscape orientation problem thanks to @MICHAEL-ZENZ. However, the issue of 30% black rectangle @prasanna532 at the bottom stayed in both landscape and vertical orientations. Please, need help.\r\n\r\n![Screenshot_2020-12-29-13-13-37-219_org tensorflow lite examples detection](https://user-images.githubusercontent.com/30389608/103279965-d9743500-49d7-11eb-8169-fb66b2e2bd6d.png)"]}, {"number": 9411, "title": "Cifar10 Tutorial Link to Example Code 404's", "body": "Following any of the links to the code for the CIFAR10 tutorial 404's.\r\n\r\nExample link: https://www.tensorflow.org/versions/master/tutorials/deep_cnn#code_organization\r\n\r\nAny from above.", "comments": ["I've added this to our docs fixit event list. Thanks for reporting this. @wolffg, medium seems right, correct?", "Seems like the github has already fixed this issue, while the tensorflow.org does not, until next release.", "Closing this but there seems to be similar issues cropping up with the 1.1 branch on the website for example: #9774 "]}, {"number": 9410, "title": "Error Building from Source with CUDA 7.5 in docker.", "body": "Please go to Stack Overflow for help and support.\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues.\r\nWe want to focus on work that benefits the whole community, e.g., fixing\r\nbugs and adding features. Support only helps individuals. GitHub also notifies\r\nthousands of people when issues are filed. We want them to see you communicating\r\nan interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nI cloned r1.1 branch, and tried either leave repo as-is or modify `WORKSPACE.bzl` . Both comes out same error. \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:r1.1\r\n- **Bazel version (if compiling from source)**:0.4.5, compiled from sorce \r\n- **CUDA/cuDNN version**:CUDA 7.5.17 & cuDNNv5 (using nvidia/cuda:7.5-cudnn5-devel-ubuntu14.04\r\n- **GPU model and memory**:GPU is onls\r\n \r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can collect the TensorFlow version with\r\n```sh\r\npython -c \"import tensorflow as tf; print (tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nError when running `tensorflow/tools/ci_build/builds/configured GPU  bazel build -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package\r\n` .The error message is:\r\n\r\n> ERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/nccl_archive/BUILD:33:1: error while parsing .d file: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local_linux-opt/bin/external/nccl_archive/_objs/nccl/external/nccl_archive/src/all_reduce.cu.pic.d (No such file or directory). nvcc fatal : Unsupported gpu architecture 'compute_60'\r\nNote that I compile it in docker and have no Nvidia Pascal GPU, after looing for help on [StackOverflow](http://stackoverflow.com/questions/43525148/error-building-tensorflow-docker-image-with-cuda-7-5), I think there would be some unsupported configs. So I tried to alternate nccl package url in ./tensorflow/workspace.bzl. But still the issue occurs. I think it might be a bug that tensorflow installer not support CUDA 7.5 well, or should I add any options for nvcc when building with  bazel.\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached. Try to provide a reproducible test case that is the minimum\r\nnecessary to generate the problem.\r\n\r\n\r\n", "comments": ["I think it's because `nvcc` from CuDNN 7.5 is too old.\r\n\r\n@yifeif is 7.5 still supported?", "My understanding is that you will need cuda 8 for compute_60. @Vargnatt could you try upgrading your cuda?", "The assessment of @yifeif is correct:\r\nhttps://en.wikipedia.org/wiki/CUDA#GPUs_supported\r\nCuda 7.5 does not have support for compute capability 6.0 or higher.\r\n"]}, {"number": 9409, "title": "Add scaffolding for running XLA python tests on a plugin backend.", "body": "Copy the plugin_config.tpl file to plugin_config.py and configure\r\nthe two lines to name the plugin device and types that the device\r\nsupports.  The device can the be referred to as 'plugin', eg there\r\nwill be test targets called:\r\n\r\ntensorflow/compiler/tests:plugin_tests\r\ntensorflow/compiler/test:binary_ops_test_plugin\r\n\r\netc..", "comments": ["Can one of the admins verify this patch?", "I can see how the device name and type list can be pulled out of some bazel rule (using the 'native.' extensions).\r\n\r\nI'm not sure how to set up a bazel rule that depends on another rule that may not exist.  I wouldn't like to have to modify a file in the repo in order to enable plugin support.  if I was going to do that then I might as well just add my device in the same way as the GPU one is.\r\n\r\nI will investigate optional dependencies in bazel to see if that is a thing.\r\n", "ok - i see that the Skylark load function is more interesting than I thought.  I will have a go with that\r\n", "no - the load statement isn't acceptable within a skylark function.  it is something that is processed during the loading of the files, before the building of the rules begins.\r\n\r\nthere are no ways of loading configuration information before rule generation apart from through .bzl/BUILD files.  These can only be included unconditionally, and therefore have to be part of the repository, or generated by the ./configure command.  I don't think that either of those is nice.\r\n\r\nIf the bazel could accept rule generation time config options which have value (instead of being on/off), then it could be done.\r\n\r\nI think it is simpler to inject the config file into the runtime environment of the test and let the python script load it.\r\n", "I was thinking along the lines of unconditionally loading an dummy plugin .bzl file, and you can replace the contents of that file in your tree with something more meaningful. That way you don't need to conditionally load anything. Would something like that work?\r\n\r\n(I think the right \"real\" solution is to set this up at configure time, but that's probably a bit annoying to do.)", "I can see that would work.  The reason I don't like that is that both of its side effects are unpleasant:\r\n\r\n1) either that a dummy file has to be created before the repo will build.  if configure created it then that would be ok, but configure isn't very well designed or extensible at the moment.\r\n2) or that there will be a file in the repo that will be necessary to edit, but not to check in.  its messy and prone to error.\r\n\r\nI've never checked this out, but maybe you can check a file into the repo, and also include it in .gitignore.  if that works, then it is a good solution.", "No - unfortunately that isn't how git seems to work.  you can't check a file in, then include it in .gitignore so that any future changes are ignored.  however - apparently you can update the index to mark it as assume-unchanged.  this works ok.\r\n\r\ni have submitted a new diff\r\n", "Ok - i've squashed the commits -- now at:\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/9759\r\n\r\n", "Ok - i've squashed the commits -- now at:\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/9759\r\n\r\n"]}, {"number": 9408, "title": "Allocation tracking of pre-tracked buffer returned in a tuple should increment the buffer by 2", "body": "A buffer not pre-tracked has its ref-count set to 2 when it is part of a tuple. A buffer\r\npre-tracked has its value increased by 1, whether it is part of a tuple or not.  This is\r\nan error.\r\n\r\nThis fix increments a pre-tracked buffer by the amount of the 'initial_ref_count', which\r\nis really the number of references that a buffer acquires as part of being returned (1 for\r\na single buffer, but 2 when it is part of a tuple).\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 9407, "title": "Usage of float64 with version >0.11", "body": "### System information\r\n- Tensorflow is used as a backend for Keras library with setup \"floatx\":\"float64\", i.e. this is default float type\r\n- OS X El Capitan operational system\r\n- TensorFlow was installed via pip\r\n- Tensorflow version 0.12.1\r\n \r\nMy custom loss function includes exponent and it gets overflowed very easily, that is why I have to use float64. But after updating TensorFlow from 0.11 to 0.12 (I needed tensorboard) I started to get error while creating the model, when convolutional layer is added.\r\n\r\nTraceback of the error when I am adding a Conv layer to my model:\r\n```\r\nminiconda/lib/python2.7/site-packages/keras/models.pyc in add(self, layer)\r\n    330                  output_shapes=[self.outputs[0]._keras_shape])\r\n    331         else:\r\n--> 332             output_tensor = layer(self.outputs[0])\r\n    333             if isinstance(output_tensor, list):\r\n    334                 raise TypeError('All layers in a Sequential model '\r\n\r\nminiconda/lib/python2.7/site-packages/keras/engine/topology.pyc in __call__(self, x, mask)\r\n    570         if inbound_layers:\r\n    571             # This will call layer.build() if necessary.\r\n--> 572             self.add_inbound_node(inbound_layers, node_indices, tensor_indices)\r\n    573             # Outputs were already computed when calling self.add_inbound_node.\r\n    574             outputs = self.inbound_nodes[-1].output_tensors\r\n\r\nminiconda/lib/python2.7/site-packages/keras/engine/topology.pyc in add_inbound_node(self, inbound_layers, node_indices, tensor_indices)\r\n    633         # creating the node automatically updates self.inbound_nodes\r\n    634         # as well as outbound_nodes on inbound layers.\r\n--> 635         Node.create_node(self, inbound_layers, node_indices, tensor_indices)\r\n    636 \r\n    637     def get_output_shape_for(self, input_shape):\r\n\r\nminiconda/lib/python2.7/site-packages/keras/engine/topology.pyc in create_node(cls, outbound_layer, inbound_layers, node_indices, tensor_indices)\r\n    164 \r\n    165         if len(input_tensors) == 1:\r\n--> 166             output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))\r\n    167             output_masks = to_list(outbound_layer.compute_mask(input_tensors[0], input_masks[0]))\r\n    168             # TODO: try to auto-infer shape\r\n\r\nminiconda/lib/python2.7/site-packages/keras/layers/convolutional.pyc in call(self, x, mask)\r\n    161         output = K.conv2d(x, self.W, strides=self.subsample,\r\n    162                           border_mode=self.border_mode,\r\n--> 163                           dim_ordering='tf')\r\n    164         output = K.squeeze(output, 2)  # remove the dummy dimension\r\n    165         if self.bias:\r\n\r\nminiconda/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc in conv2d(x, kernel, strides, border_mode, dim_ordering, image_shape, filter_shape, filter_dilation)\r\n   2689     if filter_dilation == (1, 1):\r\n   2690         strides = (1,) + strides + (1,)\r\n-> 2691         x = tf.nn.conv2d(x, kernel, strides, padding=padding)\r\n   2692     else:\r\n   2693         assert filter_dilation[0] == filter_dilation[1]\r\n\r\nminiconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.pyc in conv2d(input, filter, strides, padding, use_cudnn_on_gpu, data_format, name)\r\n    394                                 strides=strides, padding=padding,\r\n    395                                 use_cudnn_on_gpu=use_cudnn_on_gpu,\r\n--> 396                                 data_format=data_format, name=name)\r\n    397   return result\r\n    398 \r\n\r\nminiconda/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)\r\n    519                   \"%s type %s of argument '%s'.\" %\r\n    520                   (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,\r\n--> 521                    inferred_from[input_arg.type_attr]))\r\n    522 \r\n    523           types = [values.dtype]\r\n\r\nTypeError: Input 'filter' of 'Conv2D' Op has type float64 that does not match type float32 of argument 'input'.\r\n```\r\nAs I understood, float64 is simply not supported as it is considered to be not needed to have that much of precision, but I need this format because of the numbers that are calculated.\r\nIs there any solution to this problem?\r\n", "comments": ["You need to use `tf.cast(.., dtype=tf.float32)` for `filter`.\r\n\r\nThis question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9406, "title": "gather_nd gradient / scatter_nd is broken", "body": "Test code:\r\n```\r\nimport tensorflow as tf\r\nsession = tf.InteractiveSession()\r\n\r\nn_base_time = 5\r\nn_in = 7\r\nn_beam = 3\r\nn_batch = 1\r\nbase = tf.ones((n_base_time, n_batch, n_in))  # (base_time,batch,n_in)\r\nidxs_exp = tf.constant(0, shape=(n_beam, n_batch, 2), name=\"idxs_exp\")  # (beam,batch,2), where the 2 stands for (base_time,batch)\r\n# Thus K == 2. gather_nd out will be idxs_exp.shape[:2] + params.shape[2:] = (beam,batch,n_in).\r\ngathered = tf.gather_nd(base, idxs_exp)  # (beam,batch,n_in)\r\ngathered_shape, _ = session.run([tf.shape(gathered), gathered])\r\nassert list(gathered_shape) == [n_beam, n_batch, n_in]\r\n\r\nbase_grad = tf.gradients(gathered, base)\r\nassert base_grad is not None\r\nsession.run(base_grad)\r\n```\r\n\r\nThe first `session.run` as well as the assert-check for the `gathered_shape` works fine but then the second `session.run` for the gradient breaks with the exception:\r\n```\r\nInvalidArgumentError (see above for traceback): Must have updates.shape = indices.shape[:IXDIM] + params_shape[IXDIM:], got updates.shape [3,1,7], indices.shape [3,1,2], params_shape [5,1,7]\r\n         [[Node: gradients/GatherNd_grad/ScatterNd = ScatterNd[T=DT_FLOAT, Tindices=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](idxs_exp, gradients/Fill/_39, gradients/GatherNd_grad/Shape)]]\r\n```\r\nThis exception message is also confusing because what it complains about what is not the case seems to be actually the case.\r\n", "comments": ["It basically results in this reduced test case which fails:\r\n```\r\n  n_base_time = 5\r\n  n_in = 7\r\n  n_beam = 3\r\n  n_batch = 1\r\n  ref_grad = tf.scatter_nd(\r\n    indices=tf.zeros((n_beam, n_batch, 2), dtype=tf.int32),\r\n    updates=tf.ones((n_beam, n_batch, n_in)),\r\n    shape=(n_base_time, n_batch, n_in))\r\n  session.run(ref_grad)\r\n```\r\n", "This was with TF 1.0.1. I guess 3b7b39ac fixes this, and I tested now with TF 1.1.0, and it seems to work fine.", "Thanks for reporting!\r\n\r\nWould you be willing to send a PR which adds your test case?\r\n\r\n@yifeif @av8ramit in case we want to cherry-pick.", "Closing since apparently it's fixed."]}, {"number": 9405, "title": "testAvgPoolSamePadding fails on ppc64le. Attempting to root cause, have some info need some help", "body": "Please go to Stack Overflow for help and support.\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues.\r\nWe want to focus on work that benefits the whole community, e.g., fixing\r\nbugs and adding features. Support only helps individuals. GitHub also notifies\r\nthousands of people when issues are filed. We want them to see you communicating\r\nan interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n  No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n  Ubuntu 16.04 ppc64le\r\n- **TensorFlow installed from (source or binary)**:\r\n  From source\r\n- **TensorFlow version (use command below)**:\r\n  'v1.0.1-0-ge895d5c-dirty'\r\n- **Bazel version (if compiling from source)**:\r\n  Build label: 0.4.4-2017-04-13 (@80a07b5)\r\n- **CUDA/cuDNN version**:\r\n  No GPU\r\n- **GPU model and memory**:\r\n  No GPU\r\n- **Exact command to reproduce**:\r\n  bazel test //tensorflow/compiler/tests:pooling_ops_test_cpu\r\nYou can collect some of this information using our environment capture script:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can collect the TensorFlow version with\r\n```sh\r\npython -c \"import tensorflow as tf; print (tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\npooling_ops_test_cpu **fails on ppc64le**, the test failing is **testAvgPoolSamePadding**\r\n\r\nI am making an attempt to root cause the issue, need some help with understanding the code flow (i am new to tensorflow code but interested/would-like-to debug & help :smile: )\r\n\r\nFirst i tried to understand the input parameters being passed to the avg_pool function (this is after understanding what avg_pool does)\r\n\r\nThe input tensor for this test case is \r\n[[[1 2 3],[4 5 6],[7 8 9],[10 11 12],[13 14 15],[16 17 18]]]   (4-d tensor)\r\n\r\nThe test expects avg_pool o/p as [  7. ,   8. ,   9. ,  11.5,  12.5,  13.5] but gets [  7. ,   8. ,   9. ,  11.5,   9. ,  13.5] (4th entry is different)\r\n\r\nThen i tried to trace the code to see why the entry differs (only for ppc64le, x86 the test passes)\r\n\r\nTest case calls avg_pool in python/ops/nn_ops.py, which ultimately drops into core/kernels/avgpooling_op.cc. The AvgPoolingOp class \"compute\" func calls SpatialAvgPool which i tried to find/grep, could find it in core/kernels/eigen_pooling.h. \r\n\r\nThis is where i am getting lost. I am suspecting an int overflow or sign type issue and my goal is to pinpoint the exact location where this could be happening, however seeing the code in avgpooling_op.cc (or eigen_pooling.h) i cant seem to find any good places where i can debug further\r\nAny help or pointers where i can go from here (or if i am following the right trail) would help\r\n\r\nThanks!\r\nVaibhav\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached. Try to provide a reproducible test case that is the minimum\r\nnecessary to generate the problem.\r\n\r\nFailure assertion snippet\r\n\r\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\r\n-----------------------------------------------------------------------------\r\nI tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 16 visible devices\r\nI tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:\r\nI tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>\r\nF.\r\n======================================================================\r\nFAIL: testAvgPoolSamePadding (__main__.PoolingTest)\r\n----------------------------------------------------------------------\r\n  File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/compiler/tests/pooling_ops_test_cpu.runfiles/org_tensorflow/tensorflow/compiler/tests/pooling_ops_test.py\", line 288, in testAvgPoolSamePadding\r\n    expected=expected_output)\r\n  File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/compiler/tests/pooling_ops_test_cpu.runfiles/org_tensorflow/tensorflow/compiler/tests/pooling_ops_test.py\", line 125, in _VerifyValues\r\n    data_format, expected)\r\n  File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/compiler/tests/pooling_ops_test_cpu.runfiles/org_tensorflow/tensorflow/compiler/tests/pooling_ops_test.py\", line 108, in _VerifyOneTest\r\n    self.assertAllClose(expected, actual.flatten(), rtol=1e-5, atol=1e-6)\r\n  File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/compiler/tests/pooling_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 485, in assertAllClose\r\n    np.testing.assert_allclose(a, b, rtol=rtol, atol=atol)\r\n  File \"/usr/lib64/python2.7/site-packages/numpy/testing/utils.py\", line 1411, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File \"/usr/lib64/python2.7/site-packages/numpy/testing/utils.py\", line 796, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nNot equal to tolerance rtol=1e-05, atol=1e-06\r\n\r\n(mismatch 16.6666666667%)\r\n x: array([  7. ,   8. ,   9. ,  11.5,  12.5,  13.5])\r\n y: array([  7. ,   8. ,   9. ,  11.5,   9. ,  13.5], dtype=float32)\r\n\r\n----------------------------------------------------------------------\r\nRan 2 tests in 0.514s\r\n", "comments": ["The definition is here: https://github.com/tensorflow/tensorflow/blob/2098b9abcf20d2c9694055bbfd6997bc00b73578/tensorflow/core/kernels/pooling_ops_common.h#L258", "Thanks! \r\n\r\nWith a little more investigation found that this fails on ppc64le only for the data_format \"NCHW\", for the other data format (\"NHWC\") it passes (data_formats as passed as input to tf.nn.avg_pool in test case file ./compiler/tests/pooling_ops_test.py). Investigating this further, if could get any suggestions/pointers to why this may be happening that would be great! ", "I would look at the numbers coming in and out of the operators to see where the difference is.", "Closing due to lack of recent activity. Please reopen if this is still an issue."]}, {"number": 9404, "title": "Add 3D operations for layers: conv3d, avg_pool3d, max_pool3d and conv3d_transpose.", "body": "1.Fix a TODO(jbms):\r\nchange rate parameter to dilation_rate for consistency with underlying op.\r\n\r\n2.Add 3D operations for layers:\r\n\r\n- conv3d\r\n- avg_pool3d\r\n- max_pool3d\r\n- conv3d_transpose\r\n", "comments": ["Can one of the admins verify this patch?", "This should be reviewed by someone with more experience with convolutional\noperators.  Perhaps Sergio?\n\nOn Tue, Apr 25, 2017 at 7:33 AM, Shanqing Cai <notifications@github.com>\nwrote:\n\n> Assigned #9404 <https://github.com/tensorflow/tensorflow/pull/9404> to\n> @ebrevdo <https://github.com/ebrevdo>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9404#event-1056773317>, or mute\n> the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim6fLAMPXvEJbVxaiLOfYYRzVjx2eks5rzgRAgaJpZM4NFzp9>\n> .\n>\n", "Conv3dTranspose exists in convolution.py now, can you rebase to use it?", "@Kongsea note that `Conv3DTranspose` has been introduced in `tf.layers` already: https://github.com/tensorflow/tensorflow/pull/8461\r\n\r\nSo this PR does not have to modify `tf.layers` at all.", "@martinwicke @fchollet I have seen it. However, you can ignore the commit [80a44db](https://github.com/tensorflow/tensorflow/pull/9404/commits/80a44db415fb74eab146c57dd6bf7d155720262a) now since this is similar with [#8461](https://github.com/tensorflow/tensorflow/pull/8461) and it has no influence with the other commits, so we don't need to do any modifications with them.", "@Kongsea please do rebase in any case.", "@fchollet Please check if I have rebased it? I don't know how to rebase and I do it following the instructions of [this](https://anavarre.net/how-to-rebase-a-github-pull-request/).", "@Kongsea I checked your recent two commits, they have similar work to #8461. If you look at **layers/convolution.py**, you shall see two definitions of `Conv3DTranspose` class, because you rebased on latest master (after my PR was merged). There were no merge conflicts as they were simply additions.\r\n\r\nTo avoid duplicate definitions, you may run `git rebase -i master` by staying on your branch. It may open a text editor in the terminal and you'll see a list of these four commits. You can remove the recent two commits from that file and save it, your first two commits shall be retained while the ones with `Conv3DTranspose` code should be gone by now.\r\n\r\nAlternatively, if you rebased just an hour ago and the master hasn't changed yet, you can run `git reset --hard HEAD~2` to remove your recent two commits and force push on this branch. But be cautious, do this step only if you haven't made a new commit locally after the rebase.", "Can one of the admins verify this patch?", "I am very sorry. I closed this pull request accidentally when I using git reset @karandesai-96 mentioned. I have to reopen a [new pull request](https://github.com/tensorflow/tensorflow/pull/9477). So please go there to discuss this features I added.", "@fchollet @martinwicke @karandesai-96 Very sorry for my accident.", "No problem at all. We'll continue on the new PR."]}, {"number": 9403, "title": "Add 3D operations for layers", "body": "1. Add 3D operations for layers:\r\n\r\n- conv3d\r\n- avg_pool3d\r\n- max_pool3d\r\n\r\n2.Fix a TODO(jbms):\r\nchange `rate` parameter to `dilation_rate` for consistency with underlying op.", "comments": ["Can one of the admins verify this patch?", "I found a bug, so I reopened a [new pull request.](https://github.com/tensorflow/tensorflow/pull/9404)"]}, {"number": 9402, "title": "tensorflow home page equation bug for section MNIST For ML Beginners", "body": "for section 2 of Get Started, https://www.tensorflow.org/get_started/mnist/beginners, there is a statement error.\r\n\r\n- **for screenshot 1, the equation is : wx + b**\r\n\r\n![\u56fe\u7247\u6ce8\u91ca](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/5b83df08-28be-11e7-9a94-0242ac140004)Please go to Stack Overflow for help and support.\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\n- **for screenshot 2, the equation is : xw + b**\r\n\r\n![\u56fe\u7247\u6ce8\u91ca](http://odqb0lggi.bkt.clouddn.com/5480622df9f06c8e773366f4/838d76da-28be-11e7-9a94-0242ac140004)\r\n\r\n\r\n\r\n", "comments": ["This is not a bug. \r\n\r\nThe explanation is right after your \"screenshot 2\":\r\n\r\n> This is flipped from when we multiplied them in our equation, where we had Wx, as a small trick to deal with x being a 2D tensor with multiple inputs."]}, {"number": 9401, "title": "ValueError: Attempt to reuse RNNCell with a different variable scope than its first use", "body": "OS: Ubuntu 14.04\r\nTF installed from source.\r\nTF version: 1.1.0-rc\r\n\r\nI want to reuse a RNNCell in two different variable scopes, which could simply  be like this:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import rnn\r\n\r\nx = tf.placeholder(tf.int32, [128, 20])\r\nembedding = tf.get_variable('embedding', [10000, 100])\r\ncells = []\r\nfor _ in range(10):\r\n    cell = rnn.BasicLSTMCell(100, state_is_tuple=True)\r\n    cells.append(cell)\r\ncell = rnn.MultiRNNCell(cells)\r\nzero_state = cell.zero_state(128, tf.float32)\r\n\r\ninputs = tf.nn.embedding_lookup(embedding, x)\r\nwith tf.variable_scope('rnn1'):\r\n    _, state = tf.nn.dynamic_rnn(cell, inputs=inputs, initial_state=zero_state)\r\nwith tf.variable_scope('rnn2'):\r\n    _, state = tf.nn.dynamic_rnn(cell, inputs=inputs, initial_state=zero_state)\r\n```\r\n\r\nBut I got following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 18, in <module>\r\n    _, state = tf.nn.dynamic_rnn(cell, inputs=inputs, initial_state=zero_state)\r\n  File \"/home/swp/test/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 582, in dynamic_rnn\r\n    dtype=dtype)\r\n  File \"/home/swp/test/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 745, in _dynamic_rnn_loop\r\n    swap_memory=swap_memory)\r\n  File \"/home/swp/test/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2623, in while_loop\r\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n  File \"/home/swp/test/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2456, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/home/swp/test/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2406, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/home/swp/test/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 730, in _time_step\r\n    (output, new_state) = call_cell()\r\n  File \"/home/swp/test/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 716, in <lambda>\r\n    call_cell = lambda: cell(input_t, state)\r\n  File \"/home/swp/test/local/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 968, in __call__\r\n    cur_inp, new_state = cell(cur_inp, cur_state)\r\n  File \"/home/swp/test/local/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 242, in __call__\r\n    with _checked_scope(self, scope or \"basic_lstm_cell\", reuse=self._reuse):\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\r\n    return self.gen.next()\r\n  File \"/home/swp/test/local/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 84, in _checked_scope\r\n    type(cell).__name__))\r\nValueError: Attempt to reuse RNNCell <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.BasicLSTMCell object at 0x7fb1c0aa1e90> with a different variable scope than its first use.  First use of cell was with scope 'rnn1/rnn/multi_rnn_cell/cell_0/basic_lstm_cell', this attempt is with scope 'rnn2/rnn/multi_rnn_cell/cell_0/basic_lstm_cell'.  Please create a new instance of the cell if you would like it to use a different set of weights.  If before you were using: MultiRNNCell([BasicLSTMCell(...)] * num_layers), change to: MultiRNNCell([BasicLSTMCell(...) for _ in range(num_layers)]).  If before you were using the same cell instance as both the forward and reverse cell of a bidirectional RNN, simply create two instances (one for forward, one for reverse).  In May 2017, we will start transitioning this cell's behavior to use existing stored weights, if any, when it is called with scope=None (which can lead to silent model degradation, so this error will remain until then.)\r\n\r\n```\r\nI googled this issue and found that https://github.com/tensorflow/tensorflow/issues/8191 is similar but not identical. That issue was caused by new arg \"reuse\" of LSTMCell.\r\nStrangely, when I use TF with version 1.0.1 builf from binary. the same code doesn't have this ValueError.\r\n\r\nSo, my questions is how to reuse a RNNCell within different variable scopes correctly in version 1.1.0-rc.\r\nThanks!", "comments": ["@ebrevdo do you know anything about that maybe?", "Update:\r\nActually with TF 1.0.1(built from binary),  in code:\r\n```\r\nwith tf.variable_scope('rnn1'):\r\n    _, state = tf.nn.dynamic_rnn(cell, inputs=inputs, initial_state=zero_state)\r\nwith tf.variable_scope('rnn2'):\r\n    _, state = tf.nn.dynamic_rnn(cell, inputs=inputs, initial_state=zero_state)\r\n```\r\nit's _doesn't reuse_ the RNNCell \"cell\", the weights and biases have different names as following(just first cell here).\r\n```\r\nrnn1/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0\r\nrnn2/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0\r\n```\r\nIn this simple case, we can put these two dynamic unrolling in the same variable scope if we wanna reuse RNNCell. Like:\r\n```\r\nwith tf.variable_scope('rnn1'):\r\n    _, state = tf.nn.dynamic_rnn(cell, inputs=inputs, initial_state=zero_state)\r\n    tf.get_variable_scope().reuse_variables()\r\n    _, state = tf.nn.dynamic_rnn(cell, inputs=inputs, initial_state=zero_state)\r\n```\r\nThere will be exactly one set of RNN parameters. But under some more complicated situations, where other wrappers may be added to original RNNCell, it's hard to reuse one BasicLSTMCell.\r\nFor example:\r\n\r\n- \"rnn\" will be added to variable names if we use \"dynamic_rnn\".\r\n\r\n- \"attention\" will be added to variable names if we use \"AttentionWrapper\".\r\n\r\n- \"decoder\" will be added to variable names if we use some instances of \"Decoder\".\r\n\r\n- ...\r\n\r\nLooking forward to more suggestions on this.", "as of this week, on master branch and in the nightlies, an rnncell carries\nits variables around with it.\n\nhttps://github.com/tensorflow/tensorflow/commit/e8482ab23bd0fce5c2941f6a190158bca2610a35\n\nOn Apr 24, 2017 6:46 PM, \"weipingpku\" <notifications@github.com> wrote:\n\n> Update:\n> Actually with TF 1.0.1(built from binary), in code:\n>\n> with tf.variable_scope('rnn1'):\n>     _, state = tf.nn.dynamic_rnn(cell, inputs=inputs, initial_state=zero_state)\n> with tf.variable_scope('rnn2'):\n>     _, state = tf.nn.dynamic_rnn(cell, inputs=inputs, initial_state=zero_state)\n>\n> it's *doesn't reuse* the RNNCell \"cell\", the weights and biases have\n> different names as following(just first cell here).\n>\n> rnn1/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0\n> rnn2/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0\n>\n> In this simple case, we can put these two dynamic unrolling in the same\n> variable scope if we wanna reuse RNNCell. Like:\n>\n> with tf.variable_scope('rnn1'):\n>     _, state = tf.nn.dynamic_rnn(cell, inputs=inputs, initial_state=zero_state)\n>     tf.get_variable_scope().reuse_variables()\n>     _, state = tf.nn.dynamic_rnn(cell, inputs=inputs, initial_state=zero_state)\n>\n> There will be exactly one set of RNN parameters. But under some more\n> complicated situations, where other wrappers may be added to original\n> RNNCell, it's hard to reuse one BasicLSTMCell.\n> For example:\n>\n>    -\n>\n>    \"rnn\" will be added to variable names if we use \"dynamic_rnn\".\n>    -\n>\n>    \"attention\" will be added to variable names if we use\n>    \"AttentionWrapper\".\n>    -\n>\n>    \"decoder\" will be added to variable names if we use some instances of\n>    \"Decoder\".\n>    -\n>\n>    ...\n>\n> Looking forward to more suggestions on this.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9401#issuecomment-296868157>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim9QvbPNlZAa3tG3U7wg8NM0P0NMjks5rzVB5gaJpZM4NFsxi>\n> .\n>\n", "@ebrevdo That sounds great. Thanks!\r\n", "Is it possible in 1.1 to create a *new* multicell while reusing old weights of every cell in the old one?\r\n\r\nMore explicitly here: http://stackoverflow.com/questions/43935609/how-to-reuse-weights-in-multirnncell\r\n", "I met the same problem. How to solve it?", "![error](https://user-images.githubusercontent.com/33259579/43353270-88ff3736-9251-11e8-8925-913c8b2917ea.png)\r\n \r\nI am getting this error while executing the file using tensorflow 1.5 in ubuntu 14.04\r\nThanks ......\r\n ", "What's in the line above? You may be missing a closing paren.", "@drpngx  sry for the late reply.....\r\nthese are the lines that are above\r\n![lsm](https://user-images.githubusercontent.com/33259579/43563344-5d3dad2a-963f-11e8-8438-5f4671e5e474.png)\r\n\r\nI have checked the parens of all the lines...I could not find it.\r\n     thanks...."]}, {"number": 9400, "title": "Tensorflow is still taking up all GPU memory despite allocation of memory", "body": "**System Information:**\r\n- OS Platform: Linux Ubuntu 16.04\r\n- TensorFlow installed from binary\r\n- TensorFlow version: 1.0.1\r\n- CUDA version: 8.0\r\n- cuDNN version: 5.1\r\n\r\noutput for print(tf.GIT_VERSION, tf.VERSION):\r\n('v1.0.0-65-g4763edf-dirty', '1.0.1')\r\n\r\n**Problem:**\r\nI have three codes running GPU-enabled TensorFlow. Currently, when the GPU is allocated for two of the processes, the GPU does get allocated. However, no matter how much GPU I allocate to the last process, it takes up all the GPU, disallowing other two processes to run simultaneously. \r\n\r\n**First process (the one with problem)**\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GTX 1060\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.6705\r\npciBusID 0000:01:00.0\r\nTotal memory: 5.93GiB\r\nFree memory: 5.68GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0)\r\n\r\n**Second process:**\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GTX 1060\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.6705\r\npciBusID 0000:01:00.0\r\nTotal memory: 5.93GiB\r\nFree memory: 237.00MiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0)\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 1.19G (1273049856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 1.07G (1145744896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 983.40M (1031170560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 885.06M (928053504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 796.55M (835248128 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 716.90M (751723264 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 645.21M (676550912 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 580.69M (608896000 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 522.62M (548006400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 470.36M (493205760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 423.32M (443885312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 380.99M (399496960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 342.89M (359547392 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 308.60M (323592704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 277.74M (291233536 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 249.97M (262110208 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 224.97M (235899392 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nWARNING:apscheduler.scheduler:Execution of job \"session (trigger: interval[0:00:01], next run at: 2017-04-24 11:24:48.702309)\" skipped: maximum number of running instances reached (1)\r\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:397] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:364] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\nF tensorflow/core/kernels/conv_ops.cc:605] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) \r\n\r\n**The code I am using to allocate GPU for the first process:**\r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3)\r\nsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\r\n\r\nI have also looked up stackoverflow but it does not seem to have the same problems raised.\r\nSome pages I referred to:\r\nhttp://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory \r\nhttps://www.tensorflow.org/tutorials/using_gpu \r\nhttps://github.com/tensorflow/tensorflow/issues/398 ", "comments": ["How much memory does your last process consume if you start this one on its own? CUDA_ERROR_OUT_OF_MEMORY usually states that a model is too big and doesn't fit into the pre-allocated memory.\r\n", "I am facing similar issues with memory allocation in tensorflow and i'm really unable to figure out what to do.. currently i am trying to train a cnn for a kaggle dataset of 48x48 pixel images and this is the code \r\nhttps://github.com/vijpandaturtle/facial-expressions\r\nEverytime i make even a small change in my layers, it keeps throwing the following error \r\n\r\nTraining on the data\r\n---------------------------------------------------------------------------\r\nResourceExhaustedError                    Traceback (most recent call last)\r\n/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1021     try:\r\n-> 1022       return fn(*args)\r\n   1023     except errors.OpError as e:\r\n\r\n/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1003                                  feed_dict, fetch_list, target_list,\r\n-> 1004                                  status, run_metadata)\r\n   1005 \r\n\r\n/home/carnd/anaconda3/envs/dl/lib/python3.5/contextlib.py in __exit__(self, type, value, traceback)\r\n     65             try:\r\n---> 66                 next(self.gen)\r\n     67             except StopIteration:\r\n\r\n/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py in raise_exception_on_not_ok_status()\r\n    468           compat.as_text(pywrap_tensorflow.TF_Message(status)),\r\n--> 469           pywrap_tensorflow.TF_GetCode(status))\r\n    470   finally:\r\n\r\nResourceExhaustedError: OOM when allocating tensor with shape[73728,2304]\r\n\t [[Node: truncated_normal_4/TruncatedNormal = TruncatedNormal[T=DT_INT32, dtype=DT_FLOAT, seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](truncated_normal_4/shape)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nResourceExhaustedError                    Traceback (most recent call last)\r\n<ipython-input-56-ab81f6da0f51> in <module>()\r\n      2 print(\"Training on the data\")\r\n      3 with tf.Session() as sess:\r\n----> 4     sess.run(tf.global_variables_initializer())\r\n      5 \r\n      6     for epoch in range(epochs):\r\n\r\n/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    765     try:\r\n    766       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 767                          run_metadata_ptr)\r\n    768       if run_metadata:\r\n    769         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n    963     if final_fetches or final_targets:\r\n    964       results = self._do_run(handle, final_targets, final_fetches,\r\n--> 965                              feed_dict_string, options, run_metadata)\r\n    966     else:\r\n    967       results = []\r\n\r\n/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1013     if handle is None:\r\n   1014       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\r\n-> 1015                            target_list, options, run_metadata)\r\n   1016     else:\r\n   1017       return self._do_call(_prun_fn, self._session, handle, feed_dict,\r\n\r\n/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1033         except KeyError:\r\n   1034           pass\r\n-> 1035       raise type(e)(node_def, op, message)\r\n   1036 \r\n   1037   def _extend_graph(self):\r\n\r\nResourceExhaustedError: OOM when allocating tensor with shape[73728,2304]\r\n\t [[Node: truncated_normal_4/TruncatedNormal = TruncatedNormal[T=DT_INT32, dtype=DT_FLOAT, seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](truncated_normal_4/shape)]]\r\n\r\nCaused by op 'truncated_normal_4/TruncatedNormal', defined at:\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\r\n    app.launch_new_instance()\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\r\n    ioloop.IOLoop.instance().start()\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\r\n    super(ZMQIOLoop, self).start()\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\r\n    handler_func(fd_obj, events)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\r\n    self._handle_recv()\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-53-742fe13e3ffc>\", line 40, in <module>\r\n    logits = conv_net(x, keep_prob)\r\n  File \"<ipython-input-53-742fe13e3ffc>\", line 18, in conv_net\r\n    neural_net = fully_conn(neural_net, 2304)\r\n  File \"<ipython-input-51-91b00ac8f4d7>\", line 10, in fully_conn\r\n    weights_full = tf.Variable(tf.truncated_normal([dimension, num_outputs], stddev = 0.05))\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/ops/random_ops.py\", line 174, in truncated_normal\r\n    seed2=seed2)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/ops/gen_random_ops.py\", line 293, in _truncated_normal\r\n    seed=seed, seed2=seed2, name=name)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[73728,2304]\r\n\t [[Node: truncated_normal_4/TruncatedNormal = TruncatedNormal[T=DT_INT32, dtype=DT_FLOAT, seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](truncated_normal_4/shape)]]\r\n\r\n\r\nI am using a g2.2x large aws instance to train my model on this dataset\r\nhttps://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge\r\nThanks !", "just checking: did you use `list_devices` before?\r\n(See https://github.com/tensorflow/tensorflow/issues/8021)", "No i don't think so ", "It might be another problem with initialization. Could you try to remove all preamble to see what makes it fail?", "Ah yes!! I'll post a snippet of a part of the code that kept throwing the error as i changed it \r\nSo basically the errors kept popping up the minute i change the stride of my convolutional layer from (2,2)\r\nto (1,1)\r\nI'm also posting a snippet of my network\r\n\r\n    # The first three convolutional + maxpooling layers\r\n    neural_net = conv2d_maxpool(x, 32, (5,5), (1,1), (2,2), (1,1))\r\n    neural_net = conv2d_maxpool(neural_net, 64, (1,1), (2,2), (4,4), (2,2))\r\n    neural_net = conv2d_maxpool(neural_net, 64, (2,2), (1,1), (2,2), (1,1))\r\n    neural_net = conv2d_maxpool(neural_net, 128, (2,2), (1,1), (2,2), (1,1))\r\n\r\n    # Layer to flatten the tensor from the convolutional layers\r\n    neural_net = flatten(neural_net)\r\n    \r\n    # Two fully connected layers with dropout\r\n    neural_net = fully_conn(neural_net, 2304)\r\n    neural_net = tf.nn.dropout(neural_net, keep_prob)\r\n    neural_net = fully_conn(neural_net, 1152)\r\n    neural_net = tf.nn.dropout(neural_net, keep_prob)\r\n    neural_net = fully_conn(neural_net, 576)\r\n    \r\n    # Layer for output\r\n    neural_net = output(neural_net, 7)\r\n\r\nHere i have defined  custom wrappers for each layer ..which you can find in my repo", "So, to be clear, if you use `(1, 1)` instead of `(2, 2)` in the code above, then it works?", "As far as I understood it, it works when he uses strides (2, 2), which makes more sense to me, as using strides of (1,1) doubles the memory necessary to store the activations compared to strides of 2. ", "@PhilJd The second process need less than 10% of the GPU to run. 10% of the GPU is 600MB. ", "I have just realized that the first process with the problem prints out **I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0)** twice. Other processes only prints out once. Does this mean that double the amount of GPU I specified is allocated to this process?", "@drpngx Using list_devices() prints out one extra **I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0)**\r\n", "@drpngx the network keeps throwing an error if i change the stride from (2,2) to (1,1)\r\nso i'm unable to figure out a way to get my network to improve its accuracy because the (2,2) stride is causing the network to lose lots of information. \r\n", "@vijpandaturtle you might be facing another problem. Could you open another issue?\r\n\r\n@yujulucy that's very informative (printing out twice). If you had a python/c++ stack trace for each of the print outs, that would be useful.", "Sure i'll do that ", "@yujulucy  I  tried before, and I found that if I run standalone training(not distribute training), the code you wrote: \r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3)\r\nsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\r\n works, but if run distribute training, and it didn't work. i also want to know how to solve it.\r\nfocus on it.", "@tbchj @yujulucy - when you write that this works in standalone training, but not with distributed training - can you post the example of how distributed training is set up?  Distributed training setup should also be able to take a 'config' parameter - you can try passing your config to that setup as well?", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!", "@cwhipkey the memory allocation happens right after you call `tf.train.Server` before distributed session is created.\r\n\r\n@tbchj have you found a solution? This issue is bombing our containers :(", "What does the tf.train.Server call look like?  Are you passing in the 'config' parameter?", "Yes, I also tried putting config there, didn't work. Tried both `config.gpu_options.per_process_gpu_memory_fraction = 0.2` and `config.gpu_options.allow_growth=True`\r\n\r\n``` 67         config = tf.ConfigProto()\r\n 68         config.gpu_options.per_process_gpu_memory_fraction = 0.2\r\n 69         server = tf.train.Server(tf_cluster_spec,\r\n 70                                  job_name=job_name,\r\n 71                                  task_index=task_index,\r\n 72                                  config=config)```", "@cwhipkey  above", "See [this question](https://stackoverflow.com/questions/47910681/tensorflow-setting-allow-growth-to-true-does-still-allocate-memory-of-all-my-gp)\r\n\r\nSet `os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'` in Python code."]}, {"number": 9399, "title": "Distributed Tensorflow model is no faster than standalone", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nI have taken the [Resnet code](https://github.com/tensorflow/models/tree/master/resnet) from the Tensorflow model zoo and distributed it as according to https://www.tensorflow.org/deploy/distributed\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04.1 LTS\r\n- **TensorFlow installed from (source or binary)**:\r\nOfficial Tensorflow-GPU docker image\r\n- **TensorFlow version (use command below)**:\r\n('v1.0.0-2378-g2259213', '1.1.0-rc0')\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nVersion 8\r\n- **GPU model and memory**:\r\nTested on multiple setups within a cluster\r\n4 machines with 2x Nvidia TitanX 12GB\r\n1 machine with 2x GeForce GTX 1080 8GB\r\n- **Exact command to reproduce**:\r\nThe commands differ slightly between machines depending on cluster configuration, but for 1 PS and 2 workers it looks like this:\r\n\r\n`CUDA_VISIBLE_DEVICES=\"\" python resnet_main.py --dataset=\"cifar10\" --train_data_path=\"/notebooks/cifar_data/data_batch*\" --train_dir=\"/notebooks/tmp/resnet_model/train\" --log_root=\"/notebooks/tmp/resnet_model\" --job_name=\"ps\" --task_index=0 --eval_data_path=\"/notebooks/cifar_data/test_batch.bin\" --eval_dir=\"/notebooks/tmp/resnet_model/test\"`\r\n\r\n`CUDA_VISIBLE_DEVICES=0 python resnet_main.py --dataset=\"cifar10\" --train_data_path=\"/notebooks/cifar_data/data_batch*\" --train_dir=\"/notebooks/tmp/resnet_model/train\" --log_root=\"/notebooks/tmp/resnet_model\" --job_name=\"worker\" --task_index=0 --eval_data_path=\"/notebooks/cifar_data/test_batch.bin\" --eval_dir=\"/notebooks/tmp/resnet_model/test\"`\r\n\r\n`CUDA_VISIBLE_DEVICES=0 python resnet_main.py --dataset=\"cifar10\" --train_data_path=\"/notebooks/cifar_data/data_batch*\" --train_dir=\"/notebooks/tmp/resnet_model/train\" --log_root=\"/notebooks/tmp/resnet_model\" --job_name=\"worker\" --task_index=1 --eval_data_path=\"/notebooks/cifar_data/test_batch.bin\" --eval_dir=\"/notebooks/tmp/resnet_model/test\"`\r\n\r\n\r\n### Describe the problem\r\nWhen running the Resnet model on CIFAR10 from the tf zoo in a distributed environment (asynchronous, between-graph replication), there is no performance gain. \r\n\r\nRunning Resnet 110 on a single (non-distributed) machine, single GPU resulted in approximately 3 iterations per second. This is considered the baseline for any tests on the following configurations - all of which resulted in approximately the same number of iterations per second irrespective of configuration.\r\n_Note: in all cases, each worker has their own dedicated GPU_\r\n1. 1 PS, 2 workers on one machine\r\n2. 1 PS with dedicated GPU & 1 worker on one machine, 1 worker on another machine\r\n3. 1 PS & 2 workers on one machine, 2 workers on another machine\r\n4. 1 PS & 2 workers on one machine, 3 more machines with 2 workers each\r\n5. 4 machines with 2 workers each, two of these with 1 PS each\r\n6. 4 machines, each with 1 PS & 2 workers\r\n\r\nWhen starting up the workers, each would begin training as soon as it's ready, printing information to the terminal during/after each iteration. The difference is speed is visually noticeable - when there is only 1 worker running, it's as fast as I would expect. As more workers spin up, all of the existing workers slow down. In this way, I can see (with 8 terminals on my screen) the entire cluster slowing as more workers begin.\r\n\r\nI monitored system stats during training to rule things out as the bottleneck and found the following for each machine:\r\n- CPU usage: CPU usage is not at 100% for any machine\r\n- GPU usage: GPU usage repeatedly flickers between 0% and ~80% - once per iteration\r\n- Network usage: Network bandwidth in/out for any given machine is never more than ~80% of its capacity. the network speed is limited to 1Gbps, and it doesn't reach this cap. We raised the limit to 2Gbps and saw no increased usage or performance.\r\n- HDD usage: Batches are loaded from disk multi-threaded. I printed to the screen during file-access and it's practically instantaneous.\r\n\r\nFor the input pipeline, I have also tried switching between `tf.RandomShuffleQueue` and `tf.train.shuffle_batch` and playing with the number of threads/min after deque etc for each of these batching methods to no avail.\r\n\r\n### Source code / logs\r\nSource code files attached below - `cifar_input.py` has small modifications from the original input pipeline. The original cifar input code is the file `cifar_input_orig.py`\r\n\r\n[resnet_distrib.zip](https://github.com/tensorflow/tensorflow/files/950254/resnet_distrib.zip)\r\n\r\n\r\n\r\n**Thank you in advance for any light you may be able to shed on this!**\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "i have meet this problem,too. is there  any clear resolution or question in StackOverflow?", "is this problem solved?", "I never found any good insight into the problem and ended up switching to MXNet to run my benchmarks (with better results). I have a suspicion that Tensorflow just doesn't scale well when using commodity network hardware.", "@ashwhall That\u2019s solely depending on your definition to \u201ccommodity\u201d hardware. If you only use GbE network, the bandwidth of a PCIe NV GPU to memory will be 128x larger than your network (128Gb/s vs 1Gb/s).\r\n\r\nYet from my perspective, an 100Gbps NIC shall be considered as commodity in 2018 :)"]}, {"number": 9398, "title": "Tf.gradients returning all zeroes when called on result of a tf.gradient call", "body": "I have the following code snippet:\r\n\r\ninterpolates = alpha*real_data + ((1-alpha)*fake_data)\r\ndisc_interpolates = Discriminator(interpolates)\r\ngradients = tf.gradients(disc_interpolates, [interpolates])[0]\r\nsecond_grad = tf.gradients(gradients[0], [interpolates])[0]\r\n\r\nWhere Discriminator corresponds to a neural network. The first call to tf.gradients is working correctly and I get back non-zero slope values in the gradients variable. However whenever I try to find the second derivative by applying tf.gradients to the gradients variable, my result is always a vector of zeroes. \r\n\r\nIs this expected behavior or should I be able to find the second derivatives of my neural net? ", "comments": ["What is discriminator?", "tnanks  my English is terrible,but Ilove AI so much.", "Here is the code for the discriminator:\r\n\r\n```\r\ndef Discriminator(inputs):\r\n    output = tf.reshape(inputs, [-1, 3, 32, 32])\r\n\r\n    output = lib.ops.conv2d.Conv2D('Discriminator.1', 3, DIM, 5, output, stride=2)\r\n    output = LeakyReLU(output)\r\n\r\n    output = lib.ops.conv2d.Conv2D('Discriminator.2', DIM, 2*DIM, 5, output, stride=2)\r\n    output = LeakyReLU(output)\r\n\r\n    output = lib.ops.conv2d.Conv2D('Discriminator.3', 2*DIM, 4*DIM, 5, output, stride=2)\r\n    output = LeakyReLU(output)\r\n\r\n    output = tf.reshape(output, [-1, 4*4*4*DIM])\r\n    output = lib.ops.linear.Linear('Discriminator.Output', 4*4*4*DIM, 1, output)\r\n\r\n    return tf.reshape(output, [-1])\r\n```\r\nI don't think the second derivative will be perfectly linear in all places, and I would be very surprised if that is the case because the first order derivates are not all exactly one and have variances greater than >.1\r\n", "Well if you are in the linear part, relu is linear and convolution is a linear combination of inputs, so you should get a zero gradient. Replace relu with, say, sigmoid and you should get something sigmoid-like.", "It's leaky relus, so zero gradients are unlikely.  However, this question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is likely not a bug or feature request. There is also a larger community that reads questions there. Thanks!"]}]