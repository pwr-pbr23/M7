[{"number": 23728, "title": "Unable to import tf.keras.optimizers", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): None\r\n- CUDA/cuDNN version: 9.0, 7.0\r\n- GPU model and memory: GTX 1080ti 11gb\r\n\r\n**Describe the current behavior**\r\nCannot import keras.optimizers when done like this\r\n```\r\nfrom tensorflow.keras.optimizers import *\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-17-ea1fa27bfa23> in <module>()\r\n----> 1 from tensorflow.keras.optimizers import *\r\n\r\n~/tf/lib/python3.6/site-packages/tensorflow/keras/__init__.py in <module>()\r\n     18 from tensorflow.keras import estimator\r\n     19 from tensorflow.keras import initializers\r\n---> 20 from tensorflow.keras import layers\r\n     21 from tensorflow.keras import losses\r\n     22 from tensorflow.keras import metrics\r\n\r\n~/tf/lib/python3.6/site-packages/tensorflow/keras/layers/__init__.py in <module>()\r\n      7 \r\n      8 from tensorflow.python.keras import Input\r\n----> 9 from tensorflow.python.keras.applications.densenet import Activation\r\n     10 from tensorflow.python.keras.applications.densenet import AveragePooling2D\r\n     11 from tensorflow.python.keras.applications.densenet import AveragePooling2D as AvgPool2D\r\n\r\nImportError: cannot import name 'Activation'\r\n```\r\nbut this seems to work fine\r\n```\r\nimport tensorflow as tf\r\noptim = tf.keras.optimizers.Adam()\r\n```\r\nwhat weird is that even this works\r\n```\r\nfrom tensorflow.python.keras.optimizers import *\r\n```", "comments": ["Have you solved this problem? I face the same problem, but still don't know how to solve it @srihari-humbarwadi ", "from tensorflow.python.keras.optimizers import RMSprop\r\nMy optimizers doesnt work can u help me \r\nHow can \u0131 fix this", "The mistake is;\r\nImportError: cannot import name 'RMSprop' from 'tensorflow.python.keras.optimizers' (C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizers.py)\r\n", "> The mistake is;\r\n> ImportError: cannot import name 'RMSprop' from 'tensorflow.python.keras.optimizers' (C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizers.py)\r\n\r\nI have the same problem.", "I have the same problem, but not only with optimizers:\r\n\r\n```\r\nfrom tensorflow.python.keras.utils import to_categorical\r\n\r\nfrom tensorflow.python.keras.utils import to_categorical\r\nImportError: cannot import name 'to_categorical' from 'tensorflow.python.keras.utils'\r\n```\r\n\r\nAnd with optimizers:\r\n\r\n```\r\nfrom tensorflow.python.keras.optimizers import SGD\r\n\r\nfrom tensorflow.python.keras.optimizers import SGD\r\nImportError: cannot import name 'SGD' from 'tensorflow.python.keras.optimizers'\r\n```\r\n", "I have the same problem\r\nfrom tensorflow_core.python.keras. import optimizers \r\nreplace by\r\nfrom tensorflow_core.python.keras.api._v2.keras import optimizers", "ImportError: cannot import name 'Adam' from 'keras.optimizers' (C:\\Programming\\Python39\\lib\\site-packages\\keras\\optimizers.py)\r\nwhile running Jupyter notebook in VSCode\r\n\r\nIt was solved by redusing repeating of importing keras and submodules", "@LazyDareDevil: I'm facing the same issue. Can you please explain a bit more about '_reducing the importing keas and submodules_'? It would be really great if you could also attach a code snippet. \r\nThanks!", "@Piyushagg19 I had importes like:\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom keras..optimizers import Adam\r\n\r\nthen I done:\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\nand it works for me\r\n\r\nI think, it's nessesary to add tensorflow before keras", "@LazyDareDevil: I did the exact same way but for some reason it is throwing an error.", "I had the same problem, and i solved it by : \r\n\r\nfrom tensorflow.keras.optimizers import RMSprop\r\n\r\ninstead of : \r\n\r\nfrom keras.optimizers import RMSprop\r\n\r\n", "As @LazyDareDevil mentioned:\r\n`from tensorflow.keras.optimizers import` works.\r\n\r\nThen I was able to add:\r\n`adam=Adam(learning_rate = 0.01)` and \r\n`model.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])`", "![s](https://user-images.githubusercontent.com/75624735/131151186-9e2d31e3-73a6-4700-b78a-68b5e13d5761.PNG)\r\n\r\nNot able to import  `from keras.optimizers import Adam`  in Google Colaboratory\r\n\r\n@Piyushagg19 is their any bug or something\r\n\r\n`from tensorflow.keras.optimizers import Adam`  works for me too!\r\nactually curious to know since its my first time working with CV", "  File \"F:\\python\\Project\\src\\train.py\", line 7, in <module>\r\n    from keras.optimizers import Adam\r\nImportError: cannot import name 'Adam' from 'keras.optimizers' (F:\\anaconda3\\lib\\site-packages\\keras\\optimizers.py)\r\n\r\nI'm facing the issue. anyone solved plz \r\n", "@ranewad even if you imported tensorflow or tensorflow.keras, you need to import by using full path for method, like:\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\nPlease, learn how to properly import libraries and functions", "@LazyDareDevil ok\r\n", "> I had the same problem, and i solved it by :\r\n> \r\n> from tensorflow.keras.optimizers import RMSprop\r\n> \r\n> instead of :\r\n> \r\n> from keras.optimizers import RMSprop\r\n\r\nTried this but not working either I use like `from tensorflow.keras.optimizers import Adam` it showing `Import \"tensorflow.keras.optimizers\" could not be resolved`\r\n\r\nCurrent version of tensorflow is `2.8.0` should I roll back to 1.x.x ?"]}, {"number": 23727, "title": "how to sample tfrecord data with probability like experience replay? does tfrecord support this?", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["how to sample tfrecord data with probability like experience replay? does tfrecord support this?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 23726, "title": "Win 10: Failed to load native runtime - DLL load failed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10\r\n- TensorFlow installed from (source or binary): pip install tensorflow\r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.6.5\r\n- Installed using virtualenv? pip? conda?:\r\n- CUDA/cuDNN version: 10.0 / 7\r\n- GPU model and memory: Geforce 1050 Ti 8gb\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI am unable to import tensorflow. It gives the error below.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. Updated my GPU drivers\r\n2. Installed CUDA 10.0\r\n3. Downloaded the cuDNN tool\r\n4. Added both CUDA and cuDNN bin to PATH\r\n5. pip installed tensorflow-gpu\r\n6. Tried to import\r\n\r\n7. did pip uninstall tensorflow-gpu\r\n8. tried pip install tensorflow-gpu==1.10.0 (same error)\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nError Shown:\r\n\r\n```\r\nC:\\Users\\Mom2\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\cloudpickle\\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n  import imp\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Mom2\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Mom2\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Mom2\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Mom2\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Mom2\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"ml.py\", line 2, in <module>\r\n    from tensorflow import keras\r\n  File \"C:\\Users\\Mom2\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Mom2\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Mom2\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Mom2\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Mom2\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Mom2\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Mom2\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Mom2\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n", "comments": ["TensorFlow officially supports CUDA 9.0. However it is compatible with CUDA 10.0 but not supported currently. For using TF with cuda 10, you have to build it from sources yourself. You can also take a look at [installations](https://medium.com/@amsokol.com/update-1-how-to-build-and-install-tensorflow-gpu-cpu-for-windows-from-source-code-using-bazel-and-c2e86fec9ef2) done by another users to make it work.", "No problem, thanks.\r\n\r\nJust glancing at the steps for building from source makes me believe simply installing CUDA with version 9.0 will be a faster and easier solution, then I can just (according to [this](https://devtalk.nvidia.com/default/topic/493290/multiple-cuda-versions-can-they-coexist-/)) change the path variables.\r\n\r\nAm I correct in that cudnn64_7.dll is the supported version for tensorflow? So I will not need to change that part of the installation?", "Yes you are correct, cudnn64_7.dll is the supported version for tensorflow. You can also refer the [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup) to update the environment variables correctly.", "Perfect, thank you :).", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)", "Hi guys, could you help me \r\nI'm getting the same error when I try to import keras: \"from keras.models import Sequential\"\r\nI installed Anaconda on my machine which has:\r\nWindows 10 64 bit\r\nAnaconda version 5.3.0\r\nand here's the installation command that I did:\r\n1- conda create -n tensorflow python=3.5 anaconda\r\n2- activate tensorflow\r\n3-conda install theano\r\n4-  conda install mingw libpython\r\n5- pip install tensorflow\r\n6- pip install keras\r\nthen I update all packeges:\r\nconda update --all\r\n\r\nthen I installed TensorFlow on my GPU:\r\npip install tensorflow-gpu\r\n\r\nand this is the error that I'm getting:\r\n\r\nfrom keras.models import Sequential\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-5-9c5e0a19b646>\", line 1, in <module>\r\n    from keras.models import Sequential\r\n\r\n  File \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from . import utils\r\n\r\n  File \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n\r\n  File \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n\r\n  File \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\keras\\backend\\__init__.py\", line 89, in <module>\r\n    from .tensorflow_backend import *\r\n\r\n  File \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n\r\n  File \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n\r\n  File \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\n  File \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\MHD\\Anaconda3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\MHD\\Anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.`", "@MohammadDabbas  Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n"]}, {"number": 23724, "title": "[Intel MKL] Quantized (int8) Pooling Support", "body": "This PR adds support for quantized pooling op with MKL-DNN. It also includes many clang-format changes.", "comments": ["Thank you @penpornk for your review. I have addressed your comments. Please let me know if anything else is needed. ", "@penpornk Thank you for the comments. I have run the newer version of clang-format and I added that as a new commit to the PR as you suggested. "]}, {"number": 23723, "title": "Upgrade to v1.11.0", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 23722, "title": "tf.keras - Saving model ignores layers that aren't fed into other layers", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**\r\n- TensorFlow installed from (source or binary): **Binary**\r\n- TensorFlow version (use command below): **b'v1.11.0-rc2-4-gc19e29306c' 1.11.0**\r\n- Python version: **3.6.7**\r\n- Bazel version (if compiling from source): **N/A**\r\n- GCC/Compiler version (if compiling from source): **N/A**\r\n- CUDA/cuDNN version: **N/A**\r\n- GPU model and memory: **N/A**\r\n\r\n**Describe the current behavior**\r\nWhen I save and reload a model that has multiple inputs, but some aren't fed into any layers (they are used in a custom loss function), not all inputs are created in the new model.\r\n\r\nOpening the HDF File of the saved model shows that the input layers aren't saved in the model configuration.\r\n\r\n**Describe the expected behavior**\r\nAll layers should be correctly created\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow.keras as keras\r\nimport numpy as np\r\n\r\n\r\ndef custom_loss(loss_input):\r\n    def loss(y_true, y_pred):\r\n        return y_true * y_pred * loss_input\r\n    return loss\r\n\r\n\r\n# Create simple model\r\ninput_layer = keras.Input(shape=(1,))\r\nloss_input_layer = keras.Input(shape=(1,))\r\n\r\noutput = keras.layers.Dense(1)(input_layer)\r\n\r\nmodel = keras.Model(inputs=[input_layer, loss_input_layer], outputs=[output])\r\n\r\nmodel.compile(optimizer=keras.optimizers.Adam(lr=1e4), loss=[custom_loss(loss_input_layer)])\r\n\r\n# Train the model with some example data\r\ninput_data = np.random.random_sample(512)\r\nloss_data = np.random.random_sample(512)\r\noutput_data = input_data * -1\r\n\r\nmodel.train_on_batch(x=[input_data, loss_data], y=[output_data])\r\n\r\n# Save and load model\r\nmodel.save(\"test.h5\")\r\nmodel = keras.models.load_model(\"test.h5\", custom_objects={\"loss\": custom_loss(loss_input_layer)})\r\n\r\n# Test model\r\nmodel.predict(x=[[np.random.random_sample(1)], [np.random.random_sample(1)]])\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:/Users/user/test/testTensorboard.py\", line 31, in <module>\r\n    model.predict(x=[[np.random.random_sample(1)], [np.random.random_sample(1)]])\r\n  File \"C:\\ProgramData\\Miniconda3\\envs\\SolitairePPO\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1752, in predict\r\n    x, check_steps=True, steps_name='steps', steps=steps)\r\n  File \"C:\\ProgramData\\Miniconda3\\envs\\SolitairePPO\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 993, in _standardize_user_data\r\n    class_weight, batch_size)\r\n  File \"C:\\ProgramData\\Miniconda3\\envs\\SolitairePPO\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1112, in _standardize_weights\r\n    exception_prefix='input')\r\n  File \"C:\\ProgramData\\Miniconda3\\envs\\SolitairePPO\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\", line 286, in standardize_input_data\r\n    str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\r\nValueError: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 2 arrays: [array([[0.08552315]]), array([[0.11069975]])]...\r\n```\r\n", "comments": ["It looks like a bad written code. If you call `model.summary()` before saving it, you'll not see no second input in it. It is ignored because disconnected from other layers. Check [keras guide for multi-input model](https://keras.io/getting-started/functional-api-guide/#multi-input-and-multi-output-models) for the right implementation. The basic idea that you use `keras.layers.concatenate` to merge two inputs and then split them in loss function. ", "In this example, the inputs can be merged, but that is not always the case, for example when the inputs different shapes.", "Apologies for the delay in response.\r\nI was able to execute your code snippet successfully using TF 1.13.rc0\r\nCan you please confirm? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 23721, "title": "Build error while building contrib/bigtable", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r1.12 and master (7561099fb65c18bd091751b60fc45550fc5d4805)\r\n- Python version: 3.6.7\r\n- Installed using virtualenv? pip? conda?: pyenv\r\n- Bazel version (if compiling from source): 0.19.1\r\n- GCC/Compiler version (if compiling from source): gcc-6 (Ubuntu 6.4.0-17ubuntu1) 6.4.0 20180424\r\n- CUDA/cuDNN version: 10.0 / 7.4\r\n- GPU model and memory: GTX 1080ti\r\n\r\n**Describe the problem**\r\n\r\nError message: ERROR: /home/jxstanford/src/tensorflow/tensorflow/contrib/bigtable/BUILD:53:1: Linking of rule '//tensorflow/contrib/bigtable:p\r\nython/ops/_bigtable.so' failed (Exit 1) gcc-6 failed: error executing command /usr/bin/gcc-6 -shared -o bazel-out/k8-opt/bin/tensorflow/contri\r\nb/bigtable/python/ops/_bigtable.so ... (remaining 64 argument(s) skipped)\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n`git checkout r1.12`\r\n`bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/lite/...`\r\n\r\n\r\n**Any other info / logs**\r\n\r\nBuild output attached.\r\n(https://github.com/tensorflow/tensorflow/files/2577363/TF_BUILD_OUTPUT.txt)", "comments": ["Try as per the warning message, add following line to **.bazelrc** file:\r\nimport /home/jxstanford/src/tensorflow/tools/bazel.rc", "your suggestion resulted in a viable test run, but I still had problems later in the build process. \r\n downgrading bazel to 18.1 resolved the problem. ", "Thanks for sharing your workaround."]}, {"number": 23720, "title": "tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected begin[1] in [0, 8], but got -2", "body": "**Error**\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1139, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1121, in _run_fn\r\n    status, run_metadata)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\contextlib.py\", line 88, in __exit__\r\n    next(self.gen)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Expected begin[1] in [0, 8], but got -2\r\n\t [[Node: Slice = Slice[Index=DT_INT32, T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](up_sampling2d/ResizeNearestNeighbor, Slice/begin, Slice/size)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"E:/VisionLearning/Segmentation/train.py\", line 71, in <module>\r\n    train(epochs,num_steps)\r\n  File \"E:/VisionLearning/Segmentation/train.py\", line 47, in train\r\n    sess.run(optimizer, feed_dict)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 789, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 997, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1132, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1152, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Expected begin[1] in [0, 8], but got -2\r\n\t [[Node: Slice = Slice[Index=DT_INT32, T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](up_sampling2d/ResizeNearestNeighbor, Slice/begin, Slice/size)]]\r\n\r\nCaused by op 'Slice', defined at:\r\n  File \"E:/VisionLearning/Segmentation/train.py\", line 71, in <module>\r\n    train(epochs,num_steps)\r\n  File \"E:/VisionLearning/Segmentation/train.py\", line 28, in train\r\n    logits = build_model(x, 0.5, 128)\r\n  File \"E:\\VisionLearning\\Segmentation\\Model.py\", line 153, in build_model\r\n    up1_0 = crop_and_concat(layers.UpSampling2D(size=(2,2))(conv5), conv4_1)\r\n  File \"E:\\VisionLearning\\Segmentation\\Model.py\", line 22, in crop_and_concat\r\n    x1_crop = tf.slice(x1, offsets, size)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 547, in slice\r\n    return gen_array_ops._slice(input_, begin, size, name=name)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 2896, in _slice\r\n    name=name)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Expected begin[1] in [0, 8], but got -2\r\n\t [[Node: Slice = Slice[Index=DT_INT32, T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](up_sampling2d/ResizeNearestNeighbor, Slice/begin, Slice/size)]]\r\n\r\n\r\nProcess finished with exit code 1\r\n```\r\n\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n\r\n```\r\ndef crop_and_concat(x1,x2):\r\n\r\n    x1_shape = tf.shape(x1)\r\n    x2_shape = tf.shape(x2)\r\n\r\n    x2_shape1 = x2.get_shape()\r\n    x1_shape1 = x1.get_shape()\r\n\r\n    # offsets for the top left corner of the crop\r\n    offsets = [0, (x1_shape[1] - x2_shape[1]) // 2, (x1_shape[2] - x2_shape[2]) // 2, 0]\r\n    print(\"offsets\",offsets)\r\n    print(\"x1\",x1)\r\n\r\n    #offsets = [0, (int(x1_shape1[1]) - int(x2_shape1[1])) // 2, (int(x1_shape1[2]) - int(x2_shape1[2])) // 2, 0]\r\n    size = [-1, int(x2_shape1[1]), int(x2_shape1[2]), -1]\r\n    print(\"size\",size)\r\n    #size = [-1, x2_shape[1], x2_shape[2], -1]\r\n    x1_crop = tf.slice(x1, offsets, size)\r\n    print(\"x1 crop\",x1_crop)\r\n    return tf.concat([x1_crop, x2], 3)\r\n\r\ndef build_model(x, keep_prob, batch_size):\r\n    print(x.get_shape())\r\n    conv1 = tf.layers.conv2d(\r\n        inputs= x,\r\n        filters=32,\r\n        kernel_size=(3,3),\r\n        padding='same',\r\n        activation = tf.nn.relu\r\n    )#256\r\n    print(\"conv1\", conv1)\r\n    conv1_2 = tf.layers.conv2d(\r\n        inputs=conv1,\r\n        filters=32,\r\n        kernel_size=(3,3),\r\n        padding='same',\r\n        activation=tf.nn.relu\r\n    )#252\r\n    print(\"conv1\", conv1_2)\r\n    pool1 = tf.layers.max_pooling2d(\r\n        inputs=conv1_2,\r\n        pool_size=(2,2),\r\n        padding='same',\r\n        strides = 2\r\n    )#126\r\n    print(\"pool1\", pool1)\r\n    conv2 = tf.layers.conv2d(\r\n        inputs=pool1,\r\n        filters=64,\r\n        kernel_size=(3, 3),\r\n        padding='same',\r\n        activation=tf.nn.relu\r\n    )#124\r\n    print(\"conv2\", conv2.get_shape())\r\n    conv2 = tf.layers.conv2d(\r\n        inputs=conv2,\r\n        filters=64,\r\n        kernel_size=3,\r\n        activation=tf.nn.relu\r\n    )#122\r\n    print(\"conv2\", conv2.get_shape())\r\n    pool2 = tf.layers.max_pooling2d(\r\n        inputs=conv2,\r\n        pool_size=2,\r\n        strides=2\r\n    )#61\r\n    print(\"pool2\", pool2.get_shape())\r\n    conv3 = tf.layers.conv2d(\r\n        inputs=pool2,\r\n        filters=128,\r\n        kernel_size=(3, 3),\r\n        padding='same',\r\n        activation=tf.nn.relu\r\n    )#59\r\n    print(\"conv3\", conv3.get_shape())\r\n    conv3 = tf.layers.conv2d(\r\n        inputs=conv3,\r\n        filters=128,\r\n        kernel_size=3,\r\n        activation=tf.nn.relu\r\n    )#57\r\n    print(\"conv3\", conv3.get_shape())\r\n    pool3 = tf.layers.max_pooling2d(\r\n        inputs=conv3,\r\n        pool_size=2,\r\n        strides=2\r\n    )\r\n    print(\"pool3\", pool3.get_shape())\r\n    conv4 = tf.layers.conv2d(\r\n        inputs=pool3,\r\n        filters=256,\r\n        kernel_size=(3, 3),\r\n        padding='same',\r\n        activation=tf.nn.relu\r\n    )\r\n    print(\"conv4\", conv4.get_shape())\r\n    conv4 = tf.layers.conv2d(\r\n        inputs=conv4,\r\n        filters=256,\r\n        kernel_size=3,\r\n        activation=tf.nn.relu\r\n    )\r\n    print(\"conv4\", conv4.get_shape())\r\n    pool4 = tf.layers.max_pooling2d(\r\n        inputs=conv4,\r\n        pool_size=(2,2),\r\n        strides=2\r\n    )\r\n    print(\"pool4\", pool4.get_shape())\r\n\r\n    conv4_1 = tf.layers.conv2d(\r\n        inputs=pool4,\r\n        filters=512,\r\n        kernel_size=(3, 3),\r\n        padding='same',\r\n        activation=tf.nn.relu\r\n    )\r\n    print(\"conv41\", conv4_1.get_shape())\r\n    conv4_1 = tf.layers.conv2d(\r\n        inputs=conv4_1,\r\n        filters=512,\r\n        kernel_size=3,\r\n        activation=tf.nn.relu\r\n    )\r\n    print(\"conv41\", conv4_1.get_shape())\r\n    pool4_1 = tf.layers.max_pooling2d(\r\n        inputs=conv4_1,\r\n        pool_size=(2, 2),\r\n        strides=2\r\n    )\r\n    print(\"pool41\", pool4_1.get_shape())\r\n    conv5 = tf.layers.conv2d(\r\n        inputs=pool4_1,\r\n        filters=1024,\r\n        kernel_size=(3, 3),\r\n        padding='same',\r\n        activation=tf.nn.relu\r\n    )\r\n    print(\"conv5\", conv5.get_shape())\r\n    conv5 = tf.layers.conv2d(\r\n        inputs=conv5,\r\n        filters=1024,\r\n        kernel_size=(3,3),\r\n        activation=tf.nn.relu\r\n    )\r\n    print(\"conv5\", conv5.get_shape())\r\n\r\n    #up1_0 = layers.concatenate([layers.UpSampling2D(size=(2,2))(conv5),conv4_1])\r\n    up1_0 = crop_and_concat(layers.UpSampling2D(size=(2,2))(conv5), conv4_1)\r\n\r\n    print(\"up10\", up1_0.get_shape())\r\n    conv6 = tf.layers.conv2d(\r\n        inputs=up1_0,\r\n        filters=512,\r\n        kernel_size=(3, 3),\r\n        padding='same',\r\n        activation=tf.nn.relu\r\n    )\r\n    print(\"conv6\", conv6.get_shape())\r\n    conv6 = tf.layers.conv2d(\r\n        inputs=conv6,\r\n        filters=512,\r\n        kernel_size=(3, 3),\r\n        padding='same',\r\n        activation=tf.nn.relu\r\n    )\r\n    print(\"conv6\", conv6.get_shape())\r\n    #up1 = layers.concatenate([layers.UpSampling2D(size=(2, 2))(conv6), conv4])\r\n    up1 = crop_and_concat(layers.UpSampling2D(size=(2,2))(conv6), conv4)\r\n    print(\"up1\", up1.get_shape())\r\n    conv6_1 = tf.layers.conv2d(\r\n        inputs=up1,\r\n        filters=256,\r\n        kernel_size=(3, 3),\r\n        padding='same',\r\n        activation=tf.nn.relu\r\n    )\r\n    print(\"conv61\", conv6_1.get_shape())\r\n    conv6_1 = tf.layers.conv2d(\r\n        inputs=conv6_1,\r\n        filters=256,\r\n        kernel_size=(3, 3),\r\n        padding='same',\r\n        activation=tf.nn.relu\r\n    )\r\n    print(\"conv61\", conv6_1.get_shape())\r\n    #up2  = layers.concatenate([layers.UpSampling2D(size=2)(conv6_1),conv3])\r\n    up2 = crop_and_concat(layers.UpSampling2D(size=(2,2))(conv6_1), conv3)\r\n    print(\"up2\", up2.get_shape())\r\n    conv7 = tf.layers.conv2d(\r\n        inputs=up2,\r\n        filters=128,\r\n        kernel_size=(3, 3),\r\n        padding='same',\r\n        activation=tf.nn.relu\r\n    )\r\n    print(\"conv7\", conv7.get_shape())\r\n    conv7 = tf.layers.conv2d(\r\n        inputs=conv7,\r\n        filters=128,\r\n        kernel_size=(3, 3),\r\n        padding='same',\r\n        activation=tf.nn.relu\r\n    )\r\n    print(\"conv7\", conv7.get_shape())\r\n    #up3 =  layers.concatenate([layers.UpSampling2D(size=2)(conv7),conv2])\r\n    up3 = crop_and_concat(layers.UpSampling2D(size=(2,2))(conv7), conv2)\r\n    print(\"up3\", up3.get_shape())\r\n    conv8 = tf.layers.conv2d(\r\n        inputs=up3,\r\n        filters=64,\r\n        kernel_size=(3, 3),\r\n        padding='same',\r\n        activation=tf.nn.relu\r\n    )\r\n    print(\"conv8\", conv8.get_shape())\r\n    conv8 = tf.layers.conv2d(\r\n        inputs=conv8,\r\n        filters=64,\r\n        kernel_size=(3, 3),\r\n        padding='same',\r\n        activation=tf.nn.relu\r\n    )\r\n    print(\"conv8\",conv8.get_shape())\r\n    #up4 = layers.concatenate([layers.UpSampling2D(size=2)(conv8), conv1])\r\n    up4 = crop_and_concat(layers.UpSampling2D(size=(4,4))(conv8), conv1)\r\n    print(\"up4\",up4.get_shape())\r\n    conv9 = tf.layers.conv2d(\r\n        inputs=up4,\r\n        filters=32,\r\n        kernel_size=(3, 3),\r\n        padding='same',\r\n        activation=tf.nn.relu\r\n    )\r\n    print(\"conv91\",conv9.get_shape())\r\n    conv9 = tf.layers.conv2d(\r\n        inputs=conv9,\r\n        filters=32,\r\n        kernel_size=(3, 3),\r\n        padding='same',\r\n        activation=tf.nn.relu\r\n    )\r\n    print(\"conv9\",conv9.get_shape())\r\n    conv10 = tf.layers.conv2d(\r\n        inputs=conv9,\r\n        filters=1,\r\n        kernel_size=1,\r\n        activation= tf.nn.sigmoid\r\n    )\r\n    print(\"conv10\",conv10.get_shape())\r\n    return conv10\r\n```\r\n\r\n```\r\ndef train(epochs, num_steps):\r\n    dir_images = \"Dataset/JPEGImages/480p\"\r\n    dir_annotations = \"Dataset/Annotations/480p\"\r\n\r\n    classes, classidx = find_classes(dir_images)\r\n    datas = make_dataset(dir_images, dir_annotations)\r\n    x = tf.placeholder(tf.float32, shape=[None, 256, 256, 3], name='X') #Need to define the shape of x\r\n    y = tf.placeholder(tf.float32, shape=[None, 256,256, 1], name='Y')\r\n    logits = build_model(x, 0.5, 128)\r\n    model = tf.identity(logits, name='logits')\r\n    Cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y))\r\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(Cost)\r\n\r\n    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\r\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\r\n\r\n    init = tf.global_variables_initializer()\r\n    sess = tf.InteractiveSession()\r\n    # Initialize all variables\r\n    sess.run(init)\r\n    saver = tf.train.Saver()\r\n    with tf.Session() as sess:\r\n        # variables need to be initialized before any sess.run() calls\r\n        tf.global_variables_initializer().run()\r\n\r\n        for X_batch, y_batch in generator(datas,32):\r\n            feed_dict = {x: X_batch, y: y_batch}\r\n            sess.run(optimizer, feed_dict)\r\n```\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA\r\n- TensorFlow installed from (source or binary):Source\r\n- TensorFlow version (use command below):1.2.1\r\n- Python version:3.6.4\r\n- Bazel version (if compiling from source):NA\r\n- GCC/Compiler version (if compiling from source):NA\r\n- CUDA/cuDNN version:NA\r\n- GPU model and memory:NA\r\n\r\n\r\n**Describe the current behavior** Getting this error when I am tring to start the training in images.\r\n\r\n**Describe the expected behavior**\r\nIt should start training\r\n**Code to reproduce the issue**\r\nGet Davis dataset and put it in respective folders\r\n\r\n", "comments": ["How to solve this problem?  Have you solved it?", "Its a really old question but i will give you an idea. Check the shape after each output. THis is a shape problem. "]}, {"number": 23719, "title": "Analysis of target '@local_config_cc//:cc-compiler-x64_windows' failed;", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source, both nightly and 1.12 tried\r\n- TensorFlow version: 1.12 and nightly\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: conda environment\r\n- Bazel version (if compiling from source): 0.19.1\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: 9.0 and 7.4\r\n- GPU model and memory: Geforce 770 4gb\r\n\r\n\r\n\r\n**Describe the problem**\r\nI'm trying to compile from source and I keep running into the same problem regardless of what release i try to build. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI followed the guide https://www.tensorflow.org/install/source_windows promptly. Below is what i get when i try to make the package builder by running 'bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package'\r\n\r\n**Any other info / logs**\r\n\r\n(venv) B:\\tensorflow>python ./configure.py\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\nb:\\tensorflow/tools/bazel.rc\r\nnul\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.19.1 installed.\r\nPlease specify the location of python. [Default is B:\\Program Files\\Anaconda\\envs\\venv\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  B:\\Program Files\\Anaconda\\envs\\venv\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [B:\\Program Files\\Anaconda\\envs\\venv\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]:\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]:\r\n\r\n\r\nPlease specify the location where CUDA 9.0 toolkit is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0]:\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]:\r\n\r\n\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0]:\r\n\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]: 3.0\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\r\nEigen strong inline overridden.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=gdr            # Build with GDR support.\r\n        --config=verbs          # Build with libverbs support.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=noignite       # Disable Apacha Ignite support.\r\n        --config=nokafka        # Disable Apache Kafka support.\r\n\r\n(venv) B:\\tensorflow>bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\nb:\\tensorflow/.bazelrc\r\nb:\\tensorflow/tools/bazel.rc\r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nWARNING: Option 'experimental_shortened_obj_file_path' is deprecated\r\nDEBUG: C:/users/nikolaj/_bazel_nikolaj/rm3fg6i6/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest Visual C++ installed.\r\nDEBUG: C:/users/nikolaj/_bazel_nikolaj/rm3fg6i6/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variables, eg. VS140COMNTOOLS\r\nDEBUG: C:/users/nikolaj/_bazel_nikolaj/rm3fg6i6/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: Visual C++ build tools found at C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\\r\nDEBUG: C:/users/nikolaj/_bazel_nikolaj/rm3fg6i6/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest Visual C++ installed.\r\nDEBUG: C:/users/nikolaj/_bazel_nikolaj/rm3fg6i6/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variables, eg. VS140COMNTOOLS\r\nDEBUG: C:/users/nikolaj/_bazel_nikolaj/rm3fg6i6/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: Visual C++ build tools found at C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\\r\nDEBUG: C:/users/nikolaj/_bazel_nikolaj/rm3fg6i6/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest Visual C++ installed.\r\nDEBUG: C:/users/nikolaj/_bazel_nikolaj/rm3fg6i6/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variables, eg. VS140COMNTOOLS\r\nDEBUG: C:/users/nikolaj/_bazel_nikolaj/rm3fg6i6/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: Visual C++ build tools found at C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\\r\nERROR: C:/users/nikolaj/_bazel_nikolaj/rm3fg6i6/external/local_config_cc/BUILD:106:1: in cc_toolchain rule @local_config_cc//:cc-compiler-x64_windows: Error while selecting cc_toolchain: Toolchain identifier 'msvc_x64' was not found, valid identifiers are [local_linux, local_darwin, local_windows]\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis of target '@local_config_cc//:cc-compiler-x64_windows' failed; build aborted\r\nINFO: Elapsed time: 29.408s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (24 packages loaded, 55 targets configured)\r\n    currently loading: tensorflow/python\r\n\r\n", "comments": ["Duplicate of #23674 \r\nCan you try by lowering your version to bazel 0.18.1 and build again?", "I don't think it's just the .rc files that have a problem\r\n\r\nI lowered bazel to 0.18.1, and I now get different errors. The full cmd output can be seen here https://pastebin.com/X8f8VZ5Z\r\nThe error on line 560 might be of interest, but I can't really make sense of all of it.\r\n\r\nI'm compiling the nightly build. I'll try and checkout r1.12 and post the results of that aswell\r\n", "Branch 1.12 fails much earlier. This is what I get:\r\n\r\nB:\\tensorflow>bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: Processed legacy workspace file b:\\tensorflow/tools/bazel.rc. This file will not be processed in the next release of Bazel. Please read https://github.com/bazelbuild/bazel/issues/6319 for further information, including how to upgrade.\r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nWARNING: Option 'experimental_shortened_obj_file_path' is deprecated\r\nDEBUG: C:/users/nikolaj/_bazel_nikolaj/rm3fg6i6/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest Visual C++ installed.\r\nDEBUG: C:/users/nikolaj/_bazel_nikolaj/rm3fg6i6/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variables, eg. VS140COMNTOOLS\r\nDEBUG: C:/users/nikolaj/_bazel_nikolaj/rm3fg6i6/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: Visual C++ build tools found at C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\\r\nDEBUG: C:/users/nikolaj/_bazel_nikolaj/rm3fg6i6/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest Visual C++ installed.\r\nDEBUG: C:/users/nikolaj/_bazel_nikolaj/rm3fg6i6/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variables, eg. VS140COMNTOOLS\r\nDEBUG: C:/users/nikolaj/_bazel_nikolaj/rm3fg6i6/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: Visual C++ build tools found at C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@png_archive//': Traceback (most recent call last):\r\n        File \"B:/tensorflow/third_party/repo.bzl\", line 106\r\n                _apply_patch(ctx, ctx.attr.patch_file)\r\n        File \"B:/tensorflow/third_party/repo.bzl\", line 73, in _apply_patch\r\n                _execute_and_check_ret_code(ctx, cmd)\r\n        File \"B:/tensorflow/third_party/repo.bzl\", line 52, in _execute_and_check_ret_code\r\n                fail(\"Non-zero return code({1}) when ...))\r\nNon-zero return code(256) when executing 'B:\\msys64\\usr\\bin\\bash.exe -l -c \"patch\" \"-p1\" \"-d\" \"C:/users/nikolaj/_bazel_nikolaj/rm3fg6i6/external/png_archive\" \"-i\" \"B:/tensorflow/third_party/png_fix_rpi.patch\"':\r\nStdout:\r\nStderr: Timed out\r\nINFO: Elapsed time: 55.283s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (295 packages loaded)", "@ymodak Met the same problem on Ubuntu Linux 16.04. I do not think it is a duplicate of #23674.\r\n\r\n```bash\r\n$ ./configure\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.19.1 installed.\r\nPlease specify the location of python. [Default is /home/byronyi/tf/bin/python]:\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nAttributeError: 'module' object has no attribute 'getsitepackages'\r\nFound possible Python library paths:\r\n  /home/byronyi/tf/lib/python2.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/byronyi/tf/lib/python2.7/site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]:\r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]:\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]:\r\n\r\n\r\nPlease specify the location where CUDA 9.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]:\r\n\r\n\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]:\r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nPlease specify the locally installed NCCL version you want to use. [Default is to use https://github.com/nvidia/nccl]:\r\n\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]: 7.0\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]:\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:\r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]:\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]:\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]:\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=gdr         \t# Build with GDR support.\r\n\t--config=verbs       \t# Build with libverbs support.\r\n\t--config=ngraph      \t# Build with Intel nGraph support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nohdfs      \t# Disable HDFS support.\r\n\t--config=noignite    \t# Disable Apacha Ignite support.\r\n\t--config=nokafka     \t# Disable Apache Kafka support.\r\nConfiguration finished\r\n$ bazel build -c opt --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nStarting local Bazel server and connecting to it...\r\nERROR: /srv/nfs4/home/byronyi/.cache/bazel/_bazel_byronyi/fe336f47afec8b033fb4b29a6628548f/external/local_config_cc/BUILD:57:1: in cc_toolchain rule @local_config_cc//:cc-compiler-k8: Error while selecting cc_toolchain: Toolchain identifier 'local' was not found, valid identifiers are [local_linux, local_darwin, local_windows]\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis of target '@local_config_cc//:cc-compiler-k8' failed; build aborted\r\nINFO: Elapsed time: 2.701s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (228 packages loaded, 1224 targets configured)\r\n    currently loading: tensorflow/core ... (6 packages)\r\n```", "A temporary workaround is changing the cc toolchain from local to local_linux in (your home)/.cache/bazel/_bazel_(your name)/(your hash)/external/local_config_cc/BUILD \r\n", "Unfortunately, this does not fix my issue, since I'm building on windows. I tried changing occurances of 'local' to 'local_windows' in the build file in C:\\Users\\<user name>\\_bazel_<user name>\\<hash>\\external\\local_config_cc, and it didn't help\r\nYour issue might be related, but I think it's worth its own thread perhaps, so other users can more easily find it.", "Nagging Assignee @meteorcloudy: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This might be a bug in 0.19.1, can you try upgrading Bazel to 0.19.2?\r\nRelated issue https://github.com/bazelbuild/bazel/issues/6662", "> This might be a bug in 0.19.1, can you try upgrading Bazel to 0.19.2?\r\n> Related issue [bazelbuild/bazel#6662](https://github.com/bazelbuild/bazel/issues/6662)\r\n\r\nUpgrading to 0.19.2 looks good to me.", "I ended up using Bazel 0.15.0 to compile tf r1.12. I did experience some timeout issues at first try, but a quick google search shows how to get around these, however I didn't need to do this, since it worked on the second try.\r\n\r\nE: I should add, that i used a clean install of python, not using conda, and a virtuel environment (using virtualenv)", "> \r\n> \r\n> I ended up using Bazel 0.15.0 to compile tf r1.12. I did experience some timeout issues at first try, but a quick google search shows how to get around these, however I didn't need to do this, since it worked on the second try.\r\n> \r\n> E: I should add, that i used a clean install of python, not using conda, and a virtuel environment (using virtualenv)\r\n\r\nso you just used original python to solve this problem?", "bazel build :hello_main\r\nINFO: Call stack for the definition of repository 'local_config_cc' which is a cc_autoconf (rule definition at C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/cc_configure.bzl:143:15):\r\n - <builtin>\r\n - C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/cc_configure.bzl:179:5\r\n - /DEFAULT.WORKSPACE.SUFFIX:317:1\r\nERROR: An error occurred during the fetch of repository 'local_config_cc':\r\n   Traceback (most recent call last):\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/cc_configure.bzl\", line 120\r\n                configure_windows_toolchain(<1 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 690, in configure_windows_toolchain\r\n                _get_msvc_vars(repository_ctx, <1 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 503, in _get_msvc_vars\r\n                _find_missing_vc_tools(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 387, in _find_missing_vc_tools\r\n                find_msvc_tool(repository_ctx, <2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 367, in find_msvc_tool\r\n                _get_vc_full_version(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 357, in _get_vc_full_version\r\n                _get_latest_subversion(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 336, in _get_latest_subversion\r\n                repository_ctx.path((vc_path + \"\\\\Tools\\\\MSVC\")).readdir()\r\nC:/Program Files (x86)/Microsoft Visual Studio/2019/Professional/VC/Tools/MSVC (No such file or directory)\r\nERROR: E:/tensorflow/02/BUILD:8:1: //:hello_main depends on @local_config_cc//:cc-compiler-x64_windows in repository @local_config_cc which failed to fetch. no such package '@local_config_cc//': Traceback (most recent call last):\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/cc_configure.bzl\", line 120\r\n                configure_windows_toolchain(<1 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 690, in configure_windows_toolchain\r\n                _get_msvc_vars(repository_ctx, <1 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 503, in _get_msvc_vars\r\n                _find_missing_vc_tools(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 387, in _find_missing_vc_tools\r\n                find_msvc_tool(repository_ctx, <2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 367, in find_msvc_tool\r\n                _get_vc_full_version(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 357, in _get_vc_full_version\r\n                _get_latest_subversion(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 336, in _get_latest_subversion\r\n                repository_ctx.path((vc_path + \"\\\\Tools\\\\MSVC\")).readdir()\r\nC:/Program Files (x86)/Microsoft Visual Studio/2019/Professional/VC/Tools/MSVC (No such file or directory)\r\nERROR: Analysis of target '//:hello_main' failed; build aborted: no such package '@local_config_cc//': Traceback (most recent call last):  \r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/cc_configure.bzl\", line 120\r\n                configure_windows_toolchain(<1 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 690, in configure_windows_toolchain\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 503, in _                _find_missing_vc_tools(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 387, in _find_missing_vc_tools\r\n                find_msvc_tool(repository_ctx, <2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 367, in find_msvc_tool\r\n                _get_vc_full_version(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 357, in _get_vc_full_version\r\n                _get_latest_subversion(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 336, in _get_latest_subversion\r\n                repository_ctx.path((vc_path + \"\\\\Tools\\\\MSVC\")).readdir()\r\nC:/Program Files (x86)/Microsoft Visual Studio/2019/Professional/VC/Tools/MSVC (No such file or directory)\r\nINFO: Elapsed time: 1.231s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)\r\nPS E:\\TensorFlow\\02> bazel clean --expunge\r\nINFO: Starting clean.\r\nPS E:\\TensorFlow\\02> bazel build :hello_main\r\nStarting local Bazel server and connecting to it...\r\nINFO: Call stack for the definition of repository 'local_config_cc' which is a cc_autoconf (rule definition at C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/cc_configure.bzl:143:15):\r\n - <builtin>\r\n - C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/cc_configure.bzl:179:5\r\n - /DEFAULT.WORKSPACE.SUFFIX:317:1\r\nERROR: An error occurred during the fetch of repository 'local_config_cc':\r\n   Traceback (most recent call last):\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/cc_configure.bzl\", line 120\r\n                configure_windows_toolchain(<1 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 690, in configure_windows_toolchain\r\n                _get_msvc_vars(repository_ctx, <1 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 503, in _get_msvc_vars\r\n                _find_missing_vc_tools(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 387, in _find_missing_vc_tools\r\n                find_msvc_tool(repository_ctx, <2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 367, in find_msvc_tool\r\n                _get_vc_full_version(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 357, in _get_vc_full_version\r\n                _get_latest_subversion(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 336, in _get_latest_subversion\r\n                repository_ctx.path((vc_path + \"\\\\Tools\\\\MSVC\")).readdir()\r\nC:/Program Files (x86)/Microsoft Visual Studio/2019/Professional/VC/Tools/MSVC (No such file or directory)\r\nERROR: E:/tensorflow/02/BUILD:8:1: //:hello_main depends on @local_config_cc//:cc-compiler-x64_windows in repository @local_config_cc which failed to fetch. no such package '@local_config_cc//': Traceback (most recent call last):\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/cc_configure.bzl\", line 120\r\n                configure_windows_toolchain(<1 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 690, in configure_windows_toolchain\r\n                _get_msvc_vars(repository_ctx, <1 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 503, in _get_msvc_vars\r\n                _find_missing_vc_tools(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 387, in _find_missing_vc_tools\r\n                find_msvc_tool(repository_ctx, <2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 367, in find_msvc_tool\r\n                _get_vc_full_version(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 357, in _get_vc_full_version\r\n                _get_latest_subversion(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 336, in _get_latest_subversion\r\n                repository_ctx.path((vc_path + \"\\\\Tools\\\\MSVC\")).readdir()\r\nC:/Program Files (x86)/Microsoft Visual Studio/2019/Professional/VC/Tools/MSVC (No such file or directory)\r\nERROR: Analysis of target '//:hello_main' failed; build aborted: no such package '@local_config_cc//': Traceback (most recent call last):  \r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/cc_configure.bzl\", line 120\r\n                configure_windows_toolchain(<1 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 690, in configure_windows_toolchain\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 503, in _get_msvc_vars\r\n                _find_missing_vc_tools(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 387, in _find_missing_vc_tools\r\n                find_msvc_tool(repository_ctx, <2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 367, in find_msvc_tool\r\n                _get_vc_full_version(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 357, in _get_vc_full_version\r\n                _get_latest_subversion(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 336, in _get_latest_subversion\r\n                repository_ctx.path((vc_path + \"\\\\Tools\\\\MSVC\")).readdir()\r\nC:/Program Files (x86)/Microsoft Visual Studio/2019/Professional/VC/Tools/MSVC (No such file or directory)\r\nINFO: Elapsed time: 7.744s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (12 packages loaded, 18 targets configured)\r\nPS E:\\TensorFlow\\02> bazel\r\n                                                           [bazel release 3.0.0]\r\nUsage: bazel <command> <options> ...\r\n\r\nAvailable commands:\r\n  analyze-profile     Analyzes build profile data.\r\n  aquery              Analyzes the given targets and queries the action graph.\r\n  build               Builds the specified targets.\r\n  canonicalize-flags  Canonicalizes a list of bazel options.\r\n  clean               Removes output files and optionally stops the server.\r\n  coverage            Generates code coverage report for specified test targets.\r\n  cquery              Loads, analyzes, and queries the specified targets w/ configurations.\r\n  dump                Dumps the internal state of the bazel server process.\r\n  fetch               Fetches external repositories that are prerequisites to the targets.\r\n  help                Prints help for commands, or the index.\r\n  license             Prints the license of this software.\r\n  mobile-install      Installs targets to mobile devices.\r\n  print_action        Prints the command line args for compiling a file.\r\n  query               Executes a dependency graph query.\r\n  run                 Runs the specified target.\r\n  shutdown            Stops the bazel server.\r\n  sync                Syncs all repositories specified in the workspace file\r\n  test                Builds and runs the specified test targets.\r\n  version             Prints version information for bazel.\r\n\r\nGetting more help:\r\n  bazel help <command>\r\n                   Prints help and options for <command>.\r\n  bazel help startup_options\r\n                   Options for the JVM hosting bazel.\r\n  bazel help target-syntax\r\n                   Explains the syntax for specifying targets.\r\n  bazel help info-keys\r\n                   Displays a list of keys used by the info command.\r\nPS E:\\TensorFlow\\02> bazel version\r\nBuild label: 3.0.0\r\nBuild target: bazel-out/x64_windows-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Mon Apr 6 12:56:01 2020 (1586177761)\r\nBuild timestamp: 1586177761\r\nBuild timestamp as int: 1586177761\r\nPS E:\\TensorFlow\\02> "]}, {"number": 23718, "title": " Failed to load the native TensorFlow runtime.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac os\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): using \"pip install tensorflow\"\r\n- TensorFlow version:\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:No GPU\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nInstalled tensorflow using pip install\r\nWhen i tried to load import tensorflow as tf, it throws a log\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n--------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in <module>()\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\n~/anaconda3/lib/python3.6/imp.py in load_module(name, file, filename, details)\r\n    242         else:\r\n--> 243             return load_dynamic(name, filename, file)\r\n    244     elif type_ == PKG_DIRECTORY:\r\n\r\n~/anaconda3/lib/python3.6/imp.py in load_dynamic(name, path, file)\r\n    342             name=name, loader=loader, origin=path)\r\n--> 343         return _load(spec)\r\n    344 \r\n\r\nImportError: dlopen(/Users/pritee/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _clock_gettime\r\n  Referenced from: /Users/pritee/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so (which was built for Mac OS X 10.12)\r\n  Expected in: /usr/lib/libSystem.B.dylib\r\n in /Users/pritee/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-3-41389fad42b5> in <module>()\r\n----> 1 import tensorflow as tf\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py in <module>()\r\n     22 \r\n     23 # pylint: disable=g-bad-import-order\r\n---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     25 \r\n     26 try:\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py in <module>()\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 from tensorflow.python.tools import component_api_helper\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"/Users/pritee/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/Users/pritee/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/Users/pritee/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/Users/pritee/anaconda3/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/Users/pritee/anaconda3/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: dlopen(/Users/pritee/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _clock_gettime\r\n  Referenced from: /Users/pritee/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so (which was built for Mac OS X 10.12)\r\n  Expected in: /usr/lib/libSystem.B.dylib\r\n in /Users/pritee/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["What is the OS version you are using? I suspect your operating system does not follow AVX instruction sets. TensorFlow official release binaries version 1.6 and higher are prebuilt with AVX instruction sets. In the case of macOS, TF is tested and supported against macOS 10.12.6 (Sierra) or later. To use Tensorflow on old CPUs you'll need to build a binary on your own.", "Thanks all.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 23717, "title": "Tensorflow Object detection -> How to get class name from trained model (.pb file)", "body": "Guys,\r\n\r\nTensorflow Object detection -> How to get class name from trained model (.pb file)\r\nTrained with four class, i want to know how to get class name based on input give.\r\nI checked with existing trained model and can able to get index value , but not able to get index/class name for my own trained mode\r\n\r\nAny help appreciated !\r\n\r\n", "comments": ["Any updates @wt-huang  ?\r\nI have used transfer training for my own dataset and also updated NUM_CLASSES=4 (for classes) in ssd mobilenet config \r\nPlease let me know if any information needed", "You can invoke model.predict() if using Keras, then call `probas.argmax(axis=-1)`\r\n\r\nA similar issue [2291](https://github.com/tensorflow/models/issues/2291) may help your case.\r\n\r\n\r\nHere is the issue template to fill out:\r\n\r\n**System information**\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version (use command below):\r\nPython version:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\nExact command to reproduce:\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nDescribe the problem\r\n\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nSource code / logs\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.", "Nagging Assignee @wt-huang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 23716, "title": "aarch64 support", "body": "\r\n**System information**\r\n- TensorFlow version (you are using): 1.12.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n-Target device - NVIDIA DrivePX2 and NVIDIA JetsonTX2\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nSupport aarch64 target architectures.\r\n\r\n**Will this change the current api? How?**\r\nNO, the current apis are not changed, this just enables bazel to find aarch64 tool chain during a native compilation on the aarch target.\r\n\r\n**Who will benefit with this feature?**\r\nPeople using NVIDIA Drive/Jetson platforms containing aarch64 devices.\r\n\r\n**Any Other info.**\r\n\r\nPlease add the following lines in the files to enable to support\r\n1. \r\nAdd -\r\n ```\r\nstatic int TryToReadNumaNode(const string &pci_bus_id, int device_ordinal) {\r\n-#if defined(__APPLE__)\r\n+#ifdef __aarch64__\r\n+    LOG(INFO) << \"ARM64 does not support NUMA - returning NUMA node zero\";\r\n+    return 0;\r\n+#elif defined(__APPLE__)\r\n   LOG(INFO) << \"OS X does not support NUMA - returning NUMA node zero\";\r\n   return 0;N\r\n #elif defined(PLATFORM_WINDOWS)\r\n```\r\nin the file -  /tensorflow/stream_executor/cuda/cuda_gpu_executor.cc \r\n\r\n2. https://github.com/tensorflow/tensorflow/issues/21852\r\nadd\r\n```\r\ndefault_toolchain {\r\n  cpu: \"aarch64\"\r\n  toolchain_identifier: \"local_linux\"\r\n}\r\n```\r\nin the file - ./third_party/gpus/crosstool/CROSSTOOL.tpl\r\n", "comments": ["Hi there, is there any update on this? \r\nI was able to compile TF on Nvidia Jetson tx2 but still see \"ARM64 does not support NUMA - returning NUMA node zero\". ", "@sanjaybharadwaj and @hoavt-54 Is this still an issue? If it is still an issue, please fill the fields in the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Thanks!", "@jvishnuvardhan The message is still there but I was able to use the GPUs so it's not my problem anymore. ", "Have you been looking at this NUMA area @poxvoculi?", "Yes, but not this part, where the StreamExecutor attempts to find the NUMA node of the GPU.  What's the problem?  Is it anything other than an error message in the log?  If the system is non-working unless a 0 is returned I suppose we should have more if-defs here.\r\n", "are you going to release whl for aarch64 or arm8v official?", "We also need the aarch64 support", "still no aarch64 support", "Need aarch64 support", "For aarch64, You need to compile Tensorflow from source, then it will be supported.\r\nHowever, we typically get following message that \r\n\"ARM64 does not support NUMA\"\r\n", "Need aarch64 support", "@sanjaybharadwaj,\r\n\r\nPlease use this [guide](https://www.tensorflow.org/lite/guide/build_arm#cross-compilation_for_arm_with_bazel) for building tensorflowlite in aarch64 architectures. Since the `aarch64` target architecture is already supported. I am closing this issue for now and feel free to open a new one if you still face any questions. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23716\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23716\">No</a>\n"]}, {"number": 23715, "title": "Cudnn version of the problem", "body": "Loaded runtime CuDNN library: 7.1.2 but source was compiled with: 7.3.1.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\r\n\r\n cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2\r\n#define CUDNN_MAJOR 7\r\n#define CUDNN_MINOR 4\r\n#define CUDNN_PATCHLEVEL 1\r\n--\r\n#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)\r\n\r\n#include \"driver_types.h\"\r\n\r\nNo 7.1.2 installed.\r\n\r\n tf-nightly-gpu  1.13.0.dev20181110\r\n\r\n tensorflow-gpu  1.12.0rc2\r\n\r\n\r\n\r\n", "comments": ["I have exactly the same error, do you have any progress yet? @wt-huang ", "amazing .... check carefully whether there is a 7.1.12 cudnn. Maybe you have only updated the cudnn.h but left the old libcudnn.so?", "I have a similar issue but with `tensorflow-gpu 1.12` stable, but instead with this error:\r\n```Loaded runtime CuDNN library: 7.0.5 but source was compiled with: 7.1.4.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.```\r\n\r\n**Versions:**\r\nUbuntu 16.04: `Ubuntu 16.04.2 LTS`\r\nTensorFlow 1.12: `tensorflow-gpu                    1.12.0`\r\nCUDA 9.0: `Cuda compilation tools, release 9.0, V9.0.176`\r\ncuDNN 7.1.4:\r\n```\r\n#define CUDNN_MAJOR 7\r\n#define CUDNN_MINOR 1\r\n#define CUDNN_PATCHLEVEL 4\r\n```\r\n\r\nThere's definitely no 7.0.5, and I even tried to re-install everything from scratch.. so there wouldn't be any leftover 7.0.5 `libcudnn.so`. Any ideas?\r\n", "@abdel In my case I had two installations of CUDA on different drives and I was copying cuDNN files to the wrong one initially, so it kepts loading old binaries and gave me the same error until I copied to the right one, make sure you do the same", "@zubairahmed-ai Thanks for the tip. There's definitely only one CUDA in our case on a single drive, and all PATHs seem to indicate it should be using that one, even CUDA/cuDNN test samples confirm that.. yet TensorFlow seems to be getting 7.0.5 from somewhere...?", "@abdel I am having the exact same problem.", "Installing cuDNN 7.1.4 seemed to fix my problem.", "I found a fix on my system! On my Windows computer using Anaconda, I found that using conda for some package (tensorflow? although I'm successfully using a custom tensorflow package) automatically grabbed another version of cudnn as a dependency--the version that was giving me troubles. Updating through conda and pip wouldn't give me a version more recent, so I fixed it through manually updating the cudnn package. I went to my install of Anaconda3 and went here:\r\n\r\nC:\\Anaconda3\\pkgs\\cudnn-7.1.4-cuda9.0_0\\Library\r\n\r\nIn here, you find a bin, include, and lib folder. Using the exact same process that was used installing cudnn with cuda, i replaced the cudnn files in these folders with the ones from the version I wanted. \r\nI also went to the library path of my environment\r\n\r\nC:\\Anaconda3\\envs\\ai1\\Library\\lib\\x64\r\n\r\nand replaced this cudnn.lib file as well.\r\n\r\nHopefully this makes sense and it helps somebody!", "I think it was resolved. I am closing the issue. Please open new ticket if you see similar issue again. Thanks!", "add `export LD_LIBRARY_PATH=/usr/local/cuda/lib64:${LD_LIBRARY_PATH:+${LD_LIBRARY_PATH}}` to the ~/.bashrc file, and then `source ~/.bashrc`. This solves my problem!\r\n\r\ntf:1.12, CUDA 9.0, 384, cuDNN.7.1.4", "I had a similar issue working with TF2. Turns out this was due to the fact that I was using an old conda environment that didn't have access to the same cuDNN.\r\n\r\nconda install -c anaconda cudnn saved my day", "I have exactly the same error, do you have any solution now?", "I also have this error - @jvishnuvardhan can we reopen? :(", "@mpodlasin Please open a new issue with your issue details and error trace. Thanks!", "I also had this error on Windows. On my case I had installed cupy-coda which also added de cudnn dll and the program always looked there for it.\r\n\r\nIn order to fix this I just removed the cupy-coda dll from the cupy-cuda install folder (..Python36\\Lib\\site-packages\\cupy\\cuda) and then the program loaded the correct cudnn dll from CUDA_PATH.", "I solve the same problem by follow method.\r\n\r\nI add the follow values into the System environment variable.\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\libnvvp\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\lib\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\include\r\n", "Downgrading to a compatible version (from 1.15 to 1.14) of TF for my CUDA version solved it for me.\r\n\r\nCompatility table : https://www.tensorflow.org/install/source#linux", "> I also had this error on Windows. On my case I had installed cupy-coda which also added de cudnn dll and the program always looked there for it.\r\n> \r\n> In order to fix this I just removed the cupy-coda dll from the cupy-cuda install folder (..Python36\\Lib\\site-packages\\cupy\\cuda) and then the program loaded the correct cudnn dll from CUDA_PATH.\r\n\r\nYou made my day !!!\r\nTensorflow did not find cudnn after installiong cupy.\r\n\r\n\r\n1) install tensorflow (using conda install keras-gpu is easy(\r\n2) install cupy-conda101 or another wheel as explains the cupy website\r\n3) uninstall cudnn.dll from your anaconda env\r\n4) make sure you have the correct version of cudnn installed and your windows path correctly entered", "I was having similar issue. I had an old version of cudnn (8.0.4) somewhere in `/usr/lib` and `LD_LIBRARY-PATH` was set in `.bashrc` and somehow still referring the old version of cudnn. If you re getting this error, check if you have an old version of cudnn somewhere.  So what I did was added a `conf` file in `/etc/ld.so.conf.d/ and did `sudo ldconfig` so that tf can load cudnn8.2.x . Surpringly, my code is running super fast."]}, {"number": 23714, "title": "How to remove NMS ops in mobilenet+ssd model?", "body": "I use mobilenet+ssd for detection with android NN. But  TFLite is not supported for NMS such customized ops.  So I want to remove NMS ops in the .pb model. Is there any method for that?\r\nOr any case of translating mobilnet+ssd to TFLite with android NN?", "comments": ["Are you following the steps in the [blog post](https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193)? If yes, which part is unclear?", "I fellow the steps on processing my own .pb file. But when using android NNAPI I got the following error. It seems that the tflite model has some custom operations. \r\n    E/tflite: Custom operations are not supported when using NNAPI.\r\n\r\nSo how to use NNAPI for accelerating mobile+ssd?", "If you run export_tflite_ssd_graph.py with the --add_postprocessing_op=false, then you can remove the custom op that will enable you to use NNAPI", "Hello, I exported an ssd model without the post processing op but it looks like the outputs are arrays of uint8 instead of a float32 as the comment block [here](https://github.com/tensorflow/models/blob/35f33abb8fa40053e8791b9144e894169e543869/research/object_detection/export_tflite_ssd_graph.py#L38) says. Is this a typo?", "For a quantized model, the outputs without NMS op are quantized. You will need to convert them to float using zero-point and scale.", "@achowdhery does TFLite have support for the dequantization and NMS ops for Java or would the best way to be to build a tflite shared library from the source and add it to Android Studio?", "What if I have custom object detection model (not SSD), but for example [FaceBoxes](https://github.com/TropComplique/FaceBoxes-tensorflow) which also use `tf.image.non_max_suppression` \r\nhttps://github.com/TropComplique/FaceBoxes-tensorflow/blob/545ec4f4f3c55c3592ee189ed56a11a3fd017194/src/utils/nms.py#L30\r\n\r\n@pkulzc @achowdhery can you eleborate on this?", "Try using this operation modifier while converting the model to TFLite.\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(\"_modelspath_\")\r\nconverter.post_training_quantize=True\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                        tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\nopen(\"litemodel.tflite\", \"wb\").write(tflite_model)\r\n```", "@mrgloom has made a good point. Why is NMS custom ops only supporting SSD model? Is there a plan to support non-SSD models that use `tf.image.non_max_suppression?`", "Marking issue as resolved due to inactivity. Feel free to re-open this if it's unresolved or file a [new issue](https://github.com/tensorflow/tensorflow/issues/new/choose)", "> @mrgloom has made a good point. Why is NMS custom ops only supporting SSD model? Is there a plan to support non-SSD models that use `tf.image.non_max_suppression?`\r\n\r\nIt can't be now. They are using `TFLite_Detection_PostProcess` in SSD instead of `tf.image.non_max_suppression?`. I want to apply this operation to other models too. If you have any answers, please let me know."]}, {"number": 23712, "title": "Keras with Tensorflow backend stooped working ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `CXXABI_1.3.9' not found", "body": "I am running Keras with Tensorflow backend and encountering a problem like \r\n\r\n```\r\n/home/sbhakat/miniconda2/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"script.py\", line 1, in <module>\r\n    from keras.callbacks import TensorBoard, ModelCheckpoint\r\n  File \"/home/sbhakat/miniconda2/envs/py36/lib/python3.6/site-packages/keras/__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"/home/sbhakat/miniconda2/envs/py36/lib/python3.6/site-packages/keras/utils/__init__.py\", line 27, in <module>\r\n    from .multi_gpu_utils import multi_gpu_model\r\n  File \"/home/sbhakat/miniconda2/envs/py36/lib/python3.6/site-packages/keras/utils/multi_gpu_utils.py\", line 7, in <module>\r\n    from ..layers.merge import concatenate\r\n  File \"/home/sbhakat/miniconda2/envs/py36/lib/python3.6/site-packages/keras/layers/__init__.py\", line 4, in <module>\r\n    from ..engine.base_layer import Layer\r\n  File \"/home/sbhakat/miniconda2/envs/py36/lib/python3.6/site-packages/keras/engine/__init__.py\", line 8, in <module>\r\n    from .training import Model\r\n  File \"/home/sbhakat/miniconda2/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\", line 21, in <module>\r\n    from . import training_arrays\r\n  File \"/home/sbhakat/miniconda2/envs/py36/lib/python3.6/site-packages/keras/engine/training_arrays.py\", line 8, in <module>\r\n    from scipy.sparse import issparse\r\n  File \"/home/sbhakat/miniconda2/envs/py36/lib/python3.6/site-packages/scipy/sparse/__init__.py\", line 229, in <module>\r\n    from .csr import *\r\n  File \"/home/sbhakat/miniconda2/envs/py36/lib/python3.6/site-packages/scipy/sparse/csr.py\", line 15, in <module>\r\n    from ._sparsetools import csr_tocsc, csr_tobsr, csr_count_blocks, \\\r\nImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `CXXABI_1.3.9' not found (required by /home/sbhakat/miniconda2/envs/py36/lib/python3.6/site-packages/scipy/sparse/_sparsetools.cpython-36m-x86_64-linux-gnu.so)\r\n```\r\n\r\nI think this problem was quite similar to https://github.com/tensorflow/tensorflow/issues/5017 hence I tried the proposed method which is following\r\n\r\n`cp /usr/lib/x86_64-linux-gnu/libstdc++.so.6 /home/sbhakat/miniconda2/envs/py36/lib/`\r\n\r\nBut now it seems the issue is similar to https://github.com/ContinuumIO/anaconda-issues/issues/5191\r\n\r\nAny quick help to fix this. Sorry I am new with this.", "comments": ["It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose) if it still exists. We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n"]}, {"number": 23711, "title": "please give an introduction of using quantize-aware training with tf.estimator", "body": "How to use  `tf.contrib.quantize.*` with tf.estimator or how to using  post_quantize_training model with inference type of int8.  Demo is perferred, thanks.", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n", "@zh794390558 Have you solved this problem?"]}, {"number": 23710, "title": "the performance of tensorflow distributed", "body": "**Describe the current behavior**\r\nI have trained a speech recognition network using tensorflow both on multi-gpu single machine version(multi-gpu) and multi-gpu multi-machine(distributed) version. the training speed was ok in multi-GPU but the speed on distributed is slow and accuracy is not same as multi-GPU version at same steps. I watched the timeline and found RecvTensor is waste so long time. i\u2018m wondering how to improve the performance of distributed training.\r\n![c2e456a6 17eda71d 1b93e65b](https://user-images.githubusercontent.com/16797858/48396541-c0f02480-e755-11e8-97fd-8dad532956fa.png)\r\n\r\n**log info**\r\nthe start script for each worker (3 workers in total and 3 ps) is:\r\n```\r\nCUDA_VISIBLE_DEVICES='' nohup python distributed.py --ps_hosts=gpu47-hca:2220,gpu42-hca:2221,gpu35-hca:2222 --worker_hosts=gpu47-hca:3330,gpu42-hca:3331,gpu35-hca:3332 --job_name=ps --task_index=0 &\r\n\r\nnohup python distributed.py --ps_hosts=gpu47-hca:2220,gpu42-hca:2221,gpu35-hca:2222 --worker_hosts=gpu47-hca:3330,gpu42-hca:3331,gpu35-hca:3332 --job_name=worker --task_index=0 --gpu_num=4 --learning_rate=0.00001 --batch_size=128&\r\n```\r\nthe training log:\r\nmulti-gpu:\r\n```\r\nINFO:tensorflow:Step #312: rate 0.000010, accuracy 0.08653%, cross entropy 7.359044(192.9 examples/sec; 0.166 sec/batch)\r\nINFO:tensorflow:Step #313: rate 0.000010, accuracy 0.09099%, cross entropy 7.384690(218.1 examples/sec; 0.147 sec/batch)\r\nINFO:tensorflow:Step #314: rate 0.000010, accuracy 0.09412%, cross entropy 7.325777(241.5 examples/sec; 0.132 sec/batch)\r\nINFO:tensorflow:Step #315: rate 0.000010, accuracy 0.08608%, cross entropy 7.381987(225.8 examples/sec; 0.142 sec/batch)\r\n```\r\ndistributed:\r\n```\r\nINFO:tensorflow:Step #768: learning_rate:0.00001, accuracy 0.49589%, cross entropy 6.968968 (62.8 examples/sec; 0.509 sec/batch)\r\nINFO:tensorflow:Step #769: learning_rate:0.00001, accuracy 0.60303%, cross entropy 7.082638 (87.0 examples/sec; 0.368 sec/batch)\r\nINFO:tensorflow:Step #770: learning_rate:0.00001, accuracy 0.77971%, cross entropy 7.041758 (54.9 examples/sec; 0.583 sec/batch)\r\nINFO:tensorflow:Step #771: learning_rate:0.00001, accuracy 1.00002%, cross entropy 6.996723 (41.4 examples/sec; 0.773 sec/batch)\r\n```\r\n\r\n\r\n**System information**\r\n- Have I written custom code: yes    (code is [here](https://github.com/dingevin/distributed-training/blob/master/distributed.py))\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS release 6.8\r\n- TensorFlow installed from (source or binary): binary \r\n- TensorFlow version (use command below): 1.10.1\r\n- Python version: 2.7.13\r\n- GPU model and memory: TITAN X 12G\r\n- Network bandwidt: 125M/s\r\n\r\n**What I have tried**\r\n1. the **RecvTensor** is so long time , so i increase the number of ps, the speed more fast than before but accuracy don't improve. i wonder how to confirm the number of ps and worker\uff1fif i increate the numer of ps the speed can be faster?  my network is 1CNN+5LSTM+1Fully Connected Layer\u3002\r\n2. the accuracy was 10% in multi-gpu 4000 steps; so i set 2000 steps to each worer(2 ps 2 worker), but the accuracy is 5%\uff0c it looks don't sync updata parameter? Is there something wrong with my [code](https://github.com/dingevin/distributed-training/blob/master/distributed.py)?", "comments": ["The accuracy is very low, the learning rate is also low. \r\nWhat is the accuracy number on a simple model using your setup?", "@wt-huang ,the accuracy can be reach 80% if training long time at multi-gpu. the accuracy is slow because only training a little steps and low learning rate; my questions is:\r\n1. when i training same steps on multi-gpu  and distributed, the accuracy not same approximately\uff1bi mean if training 4000 steps on multi-gpu, the accuracy can reach 10%\uff0c but the accuracy was 5% at distributed each worker training 2000 steps, it looks that  my code didn't share the parameters.\r\n2. and the training speeds is slow than multi-gpu\uff0c i don't know why.  "]}, {"number": 23709, "title": "image_ocr.py running error on tf.keras", "body": "I try to run image_ocr.py on tf.keras.\r\nimage_ocr.py is the example code from keras\r\nhttps://github.com/keras-team/keras/blob/master/examples/image_ocr.py\r\n\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'1.11.0'\r\n>>> tf.keras.__version__\r\n'2.1.6-tf'\r\n>>>\r\n\r\nI only modify some imports to use tf.keras.\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import backend as K\r\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\r\n...\r\n\r\nThe error code\r\n  File \"D:\\Anaconda2\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\data_utils.py\", line 689, in _data_generator_task\r\n    generator_output = next(self._generator)\r\n  File \"image_ocr.py\", line 312, in next_train\r\n    print(self.cur_train_index)\r\nAttributeError: 'TextImageGenerator' object has no attribute 'cur_train_index'\r\n\r\nThe problem is occurs in model.fit_generator().\r\nI found next_train() and build_word_list() run in different python thread.\r\n\r\nthe next_train() is called before the build_word_list(). So the initializeation of TextImageGenerator is incorrect.\r\n\r\nI also try to set  \"workers=0\" to use single thread. \r\n    model.fit_generator(\r\n        generator=img_gen.next_train(),\r\n        steps_per_epoch=(words_per_epoch - val_words) // minibatch_size,\r\n        epochs=stop_epoch,\r\n        validation_data=img_gen.next_val(),\r\n        validation_steps=val_words // minibatch_size,\r\n        callbacks=[viz_cb, img_gen],\r\n        initial_epoch=start_epoch,\r\n        workers=0)\r\n\r\nNow it can run. But the traing does not converge. 100% error result.\r\nBut there is no problem if use Keras intead of tf.keras.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["Apologies for the delay in response. Can you please build against latest version of TensorFlow and Keras and test again? Thanks!", "I tried with tf-nightly,  the issue\r\n```\r\nAttributeError: 'TextImageGenerator' object has no attribute 'cur_train_index'\r\n```\r\nis still there. Haven't look deep into where the issue comes from, though.", "@shijie001,\r\nCan you please let us know if the issue still persists? If so, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23709\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23709\">No</a>\n"]}, {"number": 23708, "title": "Can't build TensorRT 5.0.10 within Tensorflow 1.12, Tensorflow still search for libnvinfer.so.4 not 5", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version: r1.12\r\n- Python version: 2.7\r\n- Installed using virtualenv? pip? conda?: No\r\n- Bazel version (if compiling from source): 0.15.0\r\n- GCC/Compiler version (if compiling from source): 4.9.3\r\n- CUDA/cuDNN version: 9.0/7.0.5\r\n- GPU model and memory: TitanXP 12GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nSince the TensorRT 5 is released, but official latest tensorflow is unable to import TRT5, because it still searches TRT4 (libnvinfer.so.4), I manually compiled the tensorflow r1.12 with gpu and with TRT5.0.\r\n\r\nAfter setting the TRT5.0.10 location in ./configure, the compilation is successfully done. Then I generated the pip wheel file and pip installed it.\r\n\r\nThen I tried to import tensorflow and from tensorflow.contrib import tensorrt, still got below error:\r\n```\r\n$ python\r\nPython 2.7.13 (default, Jan  6 2017, 21:12:18) \r\n[GCC 4.8.5 20150623 (Red Hat 4.8.5-4)] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n/home/web_server/dlpy72/dlpy.tensor1.12/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n>>> tf.__version__\r\n'1.12.0'\r\n>>> from tensorflow.contrib import tensorrt\r\n**** Failed to initialize TensorRT. This is either because the TensorRT installation path is not in LD_LIBRARY_PATH, or because you do not have it installed. If not installed, please go to https://developer.nvidia.com/tensorrt to download and install TensorRT ****\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/web_server/dlpy72/dlpy.tensor1.12/lib/python2.7/site-packages/tensorflow/contrib/tensorrt/__init__.py\", line 34, in <module>\r\n    raise e\r\ntensorflow.python.framework.errors_impl.NotFoundError: libnvinfer.so.4: cannot open shared object file: No such file or directory\r\n>>> \r\n\r\n```\r\nHence the TensorRT 5 is not supported even if compiling it with tensorflow source.\r\n\r\nAny idea will be welcome.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nConfiguration log:\r\n```\r\n$ ./configure \r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.15.0 installed.\r\nDo you wish to build TensorFlow with Apache Ignite support? [Y/n]: n\r\nNo Apache Ignite support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: \r\n\r\n\r\nPlease specify the location where CUDA 9.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /home/web_server/xiaolun/cuda-9.0\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: \r\n\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: y\r\nTensorRT support will be enabled for TensorFlow.\r\n\r\nPlease specify the location where TensorRT is installed. [Default is /usr/lib/x86_64-linux-gnu]:/home/web_server/xiaolun/TensorRT-5.0.0.10\r\n\r\n\r\nPlease specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: 2.3\r\n\r\n\r\nAssuming NCCL header path is /media/disk1/fordata/web_server/project/xiaolun/nccl_2.3.7/lib/../include/nccl.h\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 6.1]: \r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: n\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /home/web_server/gcc-4.9.3/bin/gcc]: \r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: n\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See tools/bazel.rc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=gdr         \t# Build with GDR support.\r\n\t--config=verbs       \t# Build with libverbs support.\r\n\t--config=ngraph      \t# Build with Intel nGraph support.\r\nConfiguration finished\r\n\r\n```\r\nCompilation commands:\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package ./mnt\r\n", "comments": ["Maybe you forget to set TensorRT-5 lib path to the $LD_LIBRARY_PATH.", "@Jie-Fang I have set the $LD_LIBRARY_PATH already. Did you succeed in compiling it? I think we shall wait for the response from samikama.", "Nagging Assignee @samikama: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi @oscarriddle, would you please paste the content of your .tf_configure.bazelrc?", "@aaroey \r\nAs below:\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/home/web_server/dlpy72/dlpy.tensor1.12/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/home/web_server/dlpy72/dlpy.tensor1.12/lib64/python2.7/site-packages\"\r\nbuild --python_path=\"/home/web_server/dlpy72/dlpy.tensor1.12/bin/python\"\r\nbuild:ignite --define with_ignite_support=true\r\nbuild:xla --define with_xla_support=true\r\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\nbuild --action_env TF_NEED_ROCM=\"0\"\r\nbuild --action_env TF_NEED_CUDA=\"1\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/home/web_server/xiaolun/cuda-9.0\"\r\nbuild --action_env TF_CUDA_VERSION=\"9.0\"\r\nbuild --action_env CUDNN_INSTALL_PATH=\"/media/disk1/fordata/web_server/project/xiaolun/cuda-9.0\"\r\nbuild --action_env TF_CUDNN_VERSION=\"7\"\r\nbuild --action_env TENSORRT_INSTALL_PATH=\"/media/disk1/fordata/web_server/project/xiaolun/TensorRT-5.0.0.10/targets/x86_64-linux-gnu/lib\"\r\nbuild --action_env TF_TENSORRT_VERSION=\"5.0.0\"\r\nbuild --action_env NCCL_INSTALL_PATH=\"/media/disk1/fordata/web_server/project/xiaolun/nccl_2.3.7/lib\"\r\nbuild --action_env NCCL_HDR_PATH=\"/media/disk1/fordata/web_server/project/xiaolun/nccl_2.3.7/lib/../include\"\r\nbuild --action_env TF_NCCL_VERSION=\"2\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"6.1\"\r\nbuild --action_env LD_LIBRARY_PATH=\"/home/web_server/gcc-4.9.3/lib64:/home/web_server/xiaolun/cuda-9.0/lib64/:/usr/lib/:/home/web_server/xiaolun/nccl_2.3.7/lib/:/home/web_server/xiaolun/cudnn-7.0.5/lib6\\\r\n4:/data/web_server/lib:/home/web_server/xiaolun/caffe/build/lib/:/home/web_server/xiaolun/lib_so/:/home/web_server/xiaolun/TensorRT-5.0.0.10/lib:/home/web_server/xiaolun/cuda-9.2/extras/CUPTI/lib64\"\r\nbuild --action_env TF_CUDA_CLANG=\"0\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/home/web_server/gcc-4.9.3/bin/gcc\"\r\nbuild --config=cuda\r\ntest --config=cuda\r\nbuild:opt --copt=-march=native\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\nbuild:v2 --define=tf_api_version=2\r\n```", "I have same problem,when import tensorflow.contrib.tensorrt(tensorflow1.9.0+Tensort5.0)\r\nTraceback (most recent call last):\r\n  File \"classity.py\", line 6, in <module>\r\n    import tensorflow.contrib.tensorrt as trt\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tensorrt/__init__.py\", line 34, in <module>\r\n    raise e\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tensorrt/__init__.py\", line 25, in <module>\r\n    from tensorflow.contrib.tensorrt.python import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tensorrt/python/__init__.py\", line 22, in <module>\r\n    from tensorflow.contrib.tensorrt.python.ops import trt_engine_op\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tensorrt/python/ops/trt_engine_op.py\", line 32, in <module>\r\n    resource_loader.get_path_to_datafile(\"_trt_engine_op.so\"))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/util/loader.py\", line 56, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/load_library.py\", line 56, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.NotFoundError: libnvinfer.so.4: cannot open shared object file: No such file or directory\r\n\r\n", "@zcc720 You need to build tensorflow with TRT5 if you want to use it. You can't use pip installed tensorflow-gpu with trt5. Moreover, I would suggest you to upgrade a recent version of tensorflow since there are known bugs in TF1.9 that are fixed in recent releases.\r\n\r\n@oscarriddle Could you please make sure that TRT5 you configured is for cuda9 and compatible cudnn7 and not cuda10. Selected TRT version **must** be compatible with configured cudnn and cuda. Also please make sure that there is no other TF installation in the system. You can test this by installing generated pip package in a virtualenv.", "@samikama I used the version TRT5.0.0.10 with CUDA9.0 and cuDNN7.0.5 compatible. The official site seems providing new version 5.0.2.6 now, I will try it soon. For the second one, I will uninstall the previously pip-installed-tf before installing the source built whl file. ", "@oscarriddle pip installed TF will always look for trt4 until default becomes trt5. You can try printing TF systeminfo to see whether the imported package is your build old an earlier build installed in the system. I have been using trt5 since quite some time now and never had this issue so I can't reproduce what you observe. If you still have the same problems I would suggest running bazel tftrt tests or starting from a clean system.", "Thanks @samikama . I downloaded the newest version TensorRT 5.0.2.6, and re-built the tensorflow with it. Now I can import the tensorflow.contrib.tensorrt without any problem. \r\nHave a nice day.", "I installed TensorRT 5.1.5 and TensorFlow 1.12.0, when I use pip install the TensorRT module in tensorflow, It still can not work. Given the following error:\r\n\r\nimport tensorflow.contrib.tensorrt as trt\r\n**** Failed to initialize TensorRT. This is either because the TensorRT installation path is not in LD_LIBRARY_PATH, or because you do not have it installed. If not installed, please go to https://developer.nvidia.com/tensorrt to download and install TensorRT ****\r\nTraceback (most recent call last):\r\nFile \"\", line 1, in\r\nFile \"/media/huaxin/tcl1/asr/yujiangling/lib/python3.6/site-packages/tensorflow/contrib/tensorrt/init.py\", line 34, in\r\nraise e\r\nFile \"/media/huaxin/tcl1/asr/yujiangling/lib/python3.6/site-packages/tensorflow/contrib/tensorrt/init.py\", line 25, in\r\nfrom tensorflow.contrib.tensorrt.python import *\r\nFile \"/media/huaxin/tcl1/asr/yujiangling/lib/python3.6/site-packages/tensorflow/contrib/tensorrt/python/init.py\", line 22, in\r\nfrom tensorflow.contrib.tensorrt.python.ops import trt_engine_op\r\nFile \"/media/huaxin/tcl1/asr/yujiangling/lib/python3.6/site-packages/tensorflow/contrib/tensorrt/python/ops/trt_engine_op.py\", line 32, in\r\nresource_loader.get_path_to_datafile(\"_trt_engine_op.so\"))\r\nFile \"/media/huaxin/tcl1/asr/yujiangling/lib/python3.6/site-packages/tensorflow/contrib/util/loader.py\", line 56, in load_op_library\r\nret = load_library.load_op_library(path)\r\nFile \"/media/huaxin/tcl1/asr/yujiangling/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py\", line 60, in load_op_library\r\nlib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.NotFoundError: libnvinfer.so.4: cannot open shared object file: No such file or directory\r\n\r\nFinally, I have to use bazel to build the tensorflow source code and re-install it to solve this problem. But When I use TensorFlow 1.14.0 and TensorRT6.0, It works OK, doesn't need to build tensorflow source code. It's very strange!", "@yjiangling it seems you'll need to add tensorrt library path to LD_LIBRARY_PATH before running tensorflow."]}, {"number": 23707, "title": "fix spelling error", "body": "fix spelling error for documents.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "> I signed it!\r\n\r\nI signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Nagging Assignee @wt-huang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 23706, "title": "I am getting  the compilation error as 'tensorflow/lite/kernels/register.h' file not found in example cameraxcworkspace", "body": "I am trying to build the example camera from the tensorflow. But it shows the error as 'tensorflow/lite/kernels/register.h' file not found in my Xcode. \r\n![screen shot 2018-11-13 at 12 34 51 pm](https://user-images.githubusercontent.com/36992338/48396490-9c3d8200-e740-11e8-8cdd-008887ec5880.png)\r\n", "comments": ["Please provide more details by filling out the issue template.", "I have tried to run the example app of tensor flow by following the document. https://www.tensorflow.org/lite/demo_ios\r\n\r\n\r\nI have set up as mentioned all, while I am hitting the run button I am getting the error 'tensorflow/lite/kernels/register.h'\r\n\r\nKindly clarify as soon as possible.", "First check the file is indeed there, make sure location is set to \"Relative to Project\".\r\nGo to the menu bar then Product select Clean, hold option key then go to Product select Clean Build Folder.\r\n\r\nYou can fill out the issue template:\r\n\r\n**System information**\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version (use command below):\r\nPython version:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\n", "Hi, I am having the same issue of 'register.h' not being found. I don't know how in Xcode to set the file as relative to project when Xcode can't let me search outside of its own project folder. I am able to find the file, but am not able to set the path in Xcode. Is there another step I can take? I really want to try this demo out.", "@wt-huang man just checkout this project into a new folder and try to run the iOS project according to your own instructions:\r\nhttps://www.tensorflow.org/lite/demo_ios\r\nIt produces too much different errors. Do you really want to have a separate bug for each of them?", "I just sent PR #23907. It should solve the problem. ", "@miaout17 It works! thanks", "Thank you @miaout17 ! It also helped me a bit but I'm still struggling with [a related issue](https://stackoverflow.com/questions/53364001/tensorflow-lite-ios-camera-example-does-not-work-with-retrained-mobilenet-model). Could you please take a look? Thanks.", "@miaout17 I am getting Thread 1: signal SIGABRT while trying to run the tflite_camera_example.xcworkspace  project. \r\n\r\n@miaout17  Compilation error gone! Thanks", "Looks like the issue is resolved, closing this."]}, {"number": 23705, "title": "Install for Python 3.7 on Windows 64 bit", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Unable to install Tensorflow (pip install). Have Anaconda (Python 3.7). Can find any help on the TensorFlow website.", "@arcabhijeet  Hi, we are maintaining a single issue i.e  #20517 regarding Tensorflow support with Python 3.7. Request you to track that issue and post your comments there(if any). Thank you !", "Nagging Assignee @wt-huang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 23704, "title": "unable to detect multiple faces in an image/video", "body": "Hello,\r\ni am building a face detection system for study purposes.\r\nI installed tensorflow for python and developed a code in python for face detection.\r\n\r\nIf i have a single person in the image/video i am able to detect it.\r\nBut with multiple faces only one face is detected and others are ignored.\r\n\r\nIf anyone can help me i would be thankful to you.\r\n\r\nRegards,\r\nNapster", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 23703, "title": "import_meta_graph fails: NcclAllReduce", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v1.12.0-rc2-3-ga6d8ffa 1.12.0\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): 0.16.1\r\n- GCC/Compiler version (if compiling from source): gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10)\r\n- CUDA/cuDNN version: CUDA 9.1, cuDNN 7.1.3\r\n- GPU model and memory: dual 1080Ti (11178MiB)\r\n\r\nI trained a model with `MirroredStrategy` across two GPUs on a single machine using the `Estimator` API (custom `Estimator`). I'm unable to load the meta graph with the error:\r\n\r\n```\r\n$ python test.py\r\n2018-11-12 22:16:59.061730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:03:00.0\r\ntotalMemory: 10.92GiB freeMemory: 10.76GiB\r\n2018-11-12 22:16:59.271191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 1 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:04:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.76GiB\r\n2018-11-12 22:16:59.283574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1\r\n2018-11-12 22:17:01.209999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-12 22:17:01.210032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1\r\n2018-11-12 22:17:01.210038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y\r\n2018-11-12 22:17:01.210042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N\r\n2018-11-12 22:17:01.210423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10405 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1)\r\n2018-11-12 22:17:01.210742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10402 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 13, in <module>\r\n    tf.app.run()\r\n  File \"/home/sharvil/.virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"test.py\", line 6, in main\r\n    saver = tf.train.import_meta_graph('trained_models/student/3/model.ckpt-59112.meta')\r\n  File \"/home/sharvil/.virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1674, in import_meta_graph\r\n    meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]\r\n  File \"/home/sharvil/.virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1696, in _import_meta_graph_with_return_elements\r\n    **kwargs))\r\n  File \"/home/sharvil/.virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/meta_graph.py\", line 806, in import_scoped_meta_graph_with_return_elements\r\n    return_elements=return_elements)\r\n  File \"/home/sharvil/.virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/sharvil/.virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 391, in import_graph_def\r\n    _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)\r\n  File \"/home/sharvil/.virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 158, in _RemoveDefaultAttrs\r\n    op_def = op_dict[node.op]\r\nKeyError: 'NcclAllReduce'\r\n```\r\n\r\nI'm using the following code to load the meta graph and checkpoint:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\ndef main(args):\r\n  with tf.Session() as sess:\r\n    saver = tf.train.import_meta_graph('trained_models/student/3/model.ckpt-59112.meta')\r\n    saver.restore('trained_models/student/3/model.ckpt-59112')\r\n\r\nif __name__ == '__main__':\r\n  tf.app.run()\r\n```", "comments": ["Apologies for the delay in response. Is this still an issue?", "Yes, this is still an issue for me. Thanks for taking a look.", "I have the same problem. Have you solved this problem?", "@yuefengz can you help take a look?", "Same problem here.", "Looks like the nccl op is not loaded when the meta_graph is loaded. @azaks2, do you have any clue?", "You need to load contrib kernels  manually. It is a little bit involved for nccl kernels due to lazy loading (just call nccl_ops._maybe_load_nccl_ops_so())\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.nccl.python.ops import nccl_ops\r\n\r\nfrom tensorflow.python import pywrap_tensorflow as c_api\r\nfrom tensorflow.core.framework import op_def_pb2\r\n\r\nprint(tf.__version__)\r\nnccl_ops._maybe_load_nccl_ops_so()\r\nop_def_proto = op_def_pb2.OpList()\r\nbuf = c_api.TF_GetAllOpList()\r\nop_def_proto.ParseFromString(c_api.TF_GetBuffer(buf))\r\nprint op_def_proto\r\n\r\nNote since the nccl kernels moved to core at head the fix will need to be reverted going forward.", "@azaks2 should this issue happen if they built from source after nccl was moved to core? ", "No the issue should not be happening in head.", "I was having the same issue in the main branch. @azaks2 solution worked simple need to import nccl_ops and call to **_maybe_load_nccl_ops_so()** solved it", "you just need to add `*.so` compiled file generated by model implementation.\r\nas follows add this line before session and import meta graph.\r\n`tf.load_op_library('<compiled_filename>.so')`\r\nthis function add a new op in tensorflow runtime.\r\nfor example. i compiled a model that contains a `RoiPooling` layer and generated `roi_pooling.so` file. so that i added this line to my code\r\n`tf.load_op_library('op/roi_pooling.so')`\r\n\r\nand finally it works : )"]}, {"number": 23702, "title": "Weird Bug in Tf.keras.Model.Predict(x=tf.Dataset iterator)", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 1.9.0 and 1.12.0 (I am using 1.9.0 but the bug is present in 1.12.0 also)\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:M60 16 GB (two 8GB GPUs)\r\n\r\n\r\n**Describe the current behavior**\r\nwhen using tf.data.Dataset.Iterator in tf.keras.Model.predict(x=tf.data.Dataset.Iterator, steps). I am getting a weird value error:\r\n\r\n> Please provide data as a list or tuple of 2 elements  - input and target pair. Received Tensor(\"IteratorGetNext:0\", dtype=int64)\r\n\r\nThe above error is misleading. why does it need (X,Y) for prediction?\r\n\r\nMy Testing tf.Dataset iterator obviously does not give a (X,Y) tuple. It gives only X in batches. When I give a numpy array of X as input it works as intended. If I use the dataset iterator with **eager_execution** enabled: I get this error (my batch size is 2):\r\n\r\n> Please provide data as a list or tuple of 2 elements  - input and target pair. Received tf.Tensor(\r\n> [[     68       5     521 ...       0       0       0]\r\n>  [   6705 1235757    2411 ...    2804     147      13]], shape=(2, 5000), dtype=int64). We do not use the `target` value here. \r\n\r\nWhich makes it clear that when eager execution is enabled, Y is not used.\r\n\r\nMoreover, why does tf.Dataset iterator need to output a tuple of (X,Y) ? when using tf.keras.Model.predict() ? Is this the expected behaviour?\r\n\r\n\r\nNOTE: My model is a single input model not a multi input model\r\n\r\nEDIT:\r\nI worked around the error by providing (X,Y), But the keras progress bar doesn't seem to work with it.", "comments": ["@wt-huang were you able to reproduce this issue? Or am I missing something?", "@Abhijit-2592 You should be able to feed inputs to model.predict() method without getting any errors. \r\n\r\nMake sure your model is instantiated properly and invoke model.fit() to feed inputs and outputs. You can find more details on [Keras Documentation](https://keras.io).  \r\n\r\nYou can also post your code snippet here.", "Ok I'm lost about all of this I been reading a lot of this and to no end I\nstill don't know what or who it's for I hut like to type but now I want to\nknow what I need to do cab I make money dy doing this\n\nOn Fri, Nov 16, 2018, 9:04 PM wt-huang <notifications@github.com wrote:\n\n> @Abhijit-2592 <https://github.com/Abhijit-2592> You should be able to\n> feed inputs to model.predict() method without getting any errors.\n>\n> Make sure your model is instantiated properly and invoke model.fit() to\n> feed inputs and outputs. You can find more details on Keras Documentation\n> <https://keras.io>.\n>\n> You can also post your code snippet here.\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23702#issuecomment-439582666>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ArBbKtbA0bZaTW50ILg9ZGiG3_-x60m6ks5uv3zZgaJpZM4Ya_7j>\n> .\n>\n", "@wt-huang I am able to use model.fit() there isn't any issue there. But the problem is model.predict() requires (X,Y) When I am passing a tf.dataset.iterator But if I pass a numpy it takes only X and works as intended. ", "> @wt-huang I am able to use model.fit() there isn't any issue there. But the problem is model.predict() requires (X,Y) When I am passing a tf.dataset.iterator But if I pass a numpy it takes only X and works as intended.\r\n\r\nWhat if numpy X is too large to fit in memory when I call mode.predict()? I am trying to use tf.dataset to batch it. BUt it requires (X, Y)...", "@XYudong exactly the same issue", "@Abhijit-2592 and @XYudong Is this still an issue? Could you try loading newer TF version and check it? Please let me know the progress. Thanks!", "@jvishnuvardhan , I checked on Tensorflow 1.12.0 (pip released on 6th Nov 2018) and the bug still exists. You want me to check on the 1.13.0rc1 version? which was released on 8th Feb 2019?", "@Abhijit-2592 Yes. It would be great if you can check with the 1.13.0rc1 version. Thanks!", "@Abhijit-2592 If this still fails with 1.13.0rc1, please share a code snippet that reproduces the problem so that we can investigate further.", "@mrry  and @jvishnuvardhan. Thanks! This appears to be **fixed** in \r\n\r\n`tf-nightly-1.13.0-dev20190213`\r\n\r\nThe following code snippet threw the error I mentioned when running with <=1.12.0 (stable version)\r\n```python\r\nimport tensorflow as tf\r\n\r\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\nx_test = x_test.reshape((10000,28,28,1))\r\ny_test = tf.keras.utils.to_categorical(y=y_test)\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.Conv2D(8, kernel_size=3, activation='relu', input_shape=(28,28,1)))\r\nmodel.add(tf.keras.layers.Flatten())\r\nmodel.add(tf.keras.layers.Dense(10, activation='softmax'))\r\ndataset = tf.data.Dataset.from_tensor_slices((x_test)) # I need to provide y_test also until version 1.12.0\r\ndataset = dataset.batch(batch_size=10)\r\ndata = dataset.make_one_shot_iterator()\r\noutput = model.predict(x=data,steps=1000,verbose=True)\r\n```\r\n**This runs as intended in nightly version for both normal and eager mode**. \r\n\r\nOne more thing which I really liked was, It doesn't look like I need to create a `one_shot_iterator` or `initializable_iterator`.  I can directly pass the `dataset object` to `model.predict` which feels more natural :+1: . Before, while using `tf.data` with Embedding layers, I had to write a few boilerplate code to initialize lookup tables:\r\n\r\n```python\r\ntrain_iterator = train_data.make_initializable_iterator()\r\nvalid_iterator = valid_data.make_initializable_iterator()\r\n\r\n# initialize tables and iterators using sess from tf.keras\r\ninit_sess = tf.keras.backend.get_session()\r\ninit_sess.run(train_iterator.initializer)\r\ninit_sess.run(valid_iterator.initializer)\r\ninit_sess.run(tf.tables_initializer())\r\n```\r\nMy question is do I need to run this boilerplate code in upcoming versions of tensorflow? It will be really great if it is handled internally.", "i guess i am encountering this error in `tensorflow==1.14.0`, the `model.fit` is finished successfully with tf.data.Dataset input, but the `model.predict` keep giving error:\r\n```\r\nValueError: Please provide model inputs as a list or tuple of 2 or 3 elements: (input, target) or (input, target, sample_weights) Received (<tf.Tensor 'IteratorGetNext_1:0' shape=(224, 224, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext_1:1' shape=(224, 224, 3) dtype=float32>)\r\n```\r\n when i construct the dataset with X only, looks it is expecting Y also? ", "I am having the same issue with tensorflow version==1.12.0. Any solution?", "@lnshi @samra-irshad I don't see any issues with TF1.14.0 which is also a stable version or you could try with tf-nightly also. With recent versions, you need to pass `dataset` and not `dataset iterator`. Please check the code below. Please let us know whether it was resolved for you. Thanks!\r\n\r\n```\r\n!pip install tensorflow==1.14.0\r\nimport tensorflow as tf\r\n\r\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\nx_test = x_test.reshape((10000,28,28,1))\r\ny_test = tf.keras.utils.to_categorical(y=y_test)\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.Conv2D(8, kernel_size=3, activation='relu', input_shape=(28,28,1)))\r\nmodel.add(tf.keras.layers.Flatten())\r\nmodel.add(tf.keras.layers.Dense(10, activation='softmax'))\r\ndataset = tf.data.Dataset.from_tensor_slices((x_test)) # I need to provide y_test also until version 1.12.0\r\ndataset = dataset.batch(batch_size=10)\r\ndata = dataset.make_one_shot_iterator()\r\noutput = model.predict(x=dataset,steps=1000,verbose=True)\r\n```", "> @lnshi @samra-irshad I don't see any issues with TF1.14.0 which is also a stable version or you could try with tf-nightly also. With recent versions, you need to pass `dataset` and not `dataset iterator`. Please check the code below. Please let us know whether it was resolved for you. Thanks!\r\n> \r\n> ```\r\n> !pip install tensorflow==1.14.0\r\n> import tensorflow as tf\r\n> \r\n> (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\n> x_test = x_test.reshape((10000,28,28,1))\r\n> y_test = tf.keras.utils.to_categorical(y=y_test)\r\n> model = tf.keras.models.Sequential()\r\n> model.add(tf.keras.layers.Conv2D(8, kernel_size=3, activation='relu', input_shape=(28,28,1)))\r\n> model.add(tf.keras.layers.Flatten())\r\n> model.add(tf.keras.layers.Dense(10, activation='softmax'))\r\n> dataset = tf.data.Dataset.from_tensor_slices((x_test)) # I need to provide y_test also until version 1.12.0\r\n> dataset = dataset.batch(batch_size=10)\r\n> data = dataset.make_one_shot_iterator()\r\n> output = model.predict(x=dataset,steps=1000,verbose=True)\r\n> ```\r\n\r\n@jvishnuvardhan I was able to resolve this issue. I have tensorflow version 1.12.0. The predict function demands target labels in tensorflow version 1.12.0 where logically it should not. Anyhow I provided the targets (since it does not use them, so I guess it should not matter whether I provide it the labels or not)", "@samra-irshad Got it. There were lot of modifications and improvements between TF1.12.0 and current version. I would suggest you to upgrade to TF1.14.0 which is also a stable version. If you are familiar with TF2.0, then it is better to upgrade to TF2.0. Thanks.\r\n\r\nAutomatically closing this out since I understand it to be resolved in TF1.14.0, but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=23702\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=23702\">No</a>\n"]}, {"number": 23701, "title": "Tensorflow chatbot rnn model test issue", "body": "I am getting the following error while testing seq2seq rnn model for chatbot\r\n\r\n`Key embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/bias not found in checkpoint\r\n [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_HALF, DT_INT32, DT_HALF, DT_HALF, DT_HALF, ..., DT_HALF, DT_HALF, DT_HALF, DT_HALF, DT_HALF], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\r\n [[Node: save/RestoreV2/_1 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_6_save/RestoreV2\", tensor_type=DT_HALF, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]`", "comments": ["Please provide more details by filling out the issue template. \r\nDid you use a pre-trained model?", "Nagging Assignee @wt-huang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 23700, "title": "Builiding tensorflow C++ package on a new CPU platform.", "body": "**System information**\r\nIt's a cross compiling for tensorflow. The target platform is on Sunway Taihu Lake super computer. The CPU modle is SW26010 but it's hard for me to describe the instruction sets.  I'm building it through a cross compiler which is based on gcc5.3 with modified compiler backend and it runs on a normal x86 linux (I'm using ubuntu 18.04 through docker).  This compiler supports the C++11.\r\nThe bazel version is 0.18.0\r\n\r\n**Describe the problem**\r\nDuring the cross compiling, the boringssl tells me that #error \"Unknown target CPU\"\r\nHere is the CROSSTOOL for this cross-compiling:[CROSSTOOL](https://hastebin.com/idezoconuk.http)\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nAll configure options are \"no\" and the last -opt option is \"-O2\" .\r\nThe building commond is bazel build --sandbox_debug --verbose_failures --copt=-O2 --config=monolithic --crosstool_top=//tools/swgcc_compiler:toolchain --cpu=sw26010 //tensorflow:libtensorflow_cc.so\r\n\r\n**Any other info / logs**\r\nIn file included from external/boringssl/src/include/openssl/asn1.h:61:0,\r\n                 from external/boringssl/src/crypto/asn1/a_print.c:57:\r\nexternal/boringssl/src/include/openssl/base.h:118:2: error: #error \"Unknown target CPU\"\r\n #error \"Unknown target CPU\"\r\n", "comments": ["After checking the Cmakelists of boringssl, I found the message \"set(ARCH \"generic\")\" to avoid the \u201cunknown processor\u201d error.\r\nBut where can I set the generic in bazel?", "I tried to add these three macros, __mips__, __LP64__, OPENSSL_NO_ASM, into the CROSSTOOL file, the \"unknown processor error\" could be avoided but some more error will be encountered.\r\nexternal/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:30:18: fatal error: ares.h: No such file or directory"]}, {"number": 23699, "title": "[Bug fix]fix bug of reusing MirroredVariables failed in MirroredStrategy", "body": "The bug will be triggered in the following `else` branch when using `MirroredStrategy`. `create_zeros_slot` will pick up the `VarHandleOp` in current tower to create auxiliary variables. However, in the case of `replica_id > 0`, the `variable_scope` will set `reuse=True` in `MirroredStrategy`. It will use the `replica_id`th of `MirroredVaraible` to create auxiliary variable and this variable cannot be reused  because of its new name(`xxx/replica_1`) .\r\n```\r\n          if isinstance(var, variables.Variable):\r\n            avg = slot_creator.create_slot(var,\r\n                                           var.initialized_value(),\r\n                                           self.name,\r\n                                           colocate_with_primary=True)\r\n            # NOTE(mrry): We only add `tf.Variable` objects to the\r\n            # `MOVING_AVERAGE_VARIABLES` collection.\r\n            ops.add_to_collection(ops.GraphKeys.MOVING_AVERAGE_VARIABLES, var)\r\n          else:\r\n            avg = slot_creator.create_zeros_slot(\r\n                var,\r\n                self.name,\r\n                colocate_with_primary=(var.op.type in [\"Variable\",\r\n                                                       \"VariableV2\",\r\n                                                       \"VarHandleOp\"]))\r\n            if self._zero_debias:\r\n              zero_debias_true.add(avg)\r\n```\r\n\r\nCurrently, master branch even cannot apply moving average to `MirroredVariable`. The following solves this problem https://github.com/tensorflow/tensorflow/pull/23396  and my bug fix is building on that. \r\nPlease check it. Thanks very much. @yuefengz ", "comments": ["I noticed the code have been changed in master. This pr will be closed."]}, {"number": 23698, "title": "InvalidArgumentError: indices[26,0] = 5001 is not in [0, 5001)  ( at Training Error in Image Captioning with attention Google Colab)", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\nGoogle Colab\r\nimage_captioning_with_attention.ipynb\r\n\r\n\r\n**Describe the problem**\r\nInvalidArgumentError: indices[26,0] = 5001 is not in [0, 5001)  in Training phase.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nThis is all error message.\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-30-03bc9960ded7> in <module>()\r\n     19             for i in range(1, target.shape[1]):\r\n     20                 # passing the features through the decoder\r\n---> 21                 predictions, hidden, _ = decoder(dec_input, features, hidden)\r\n     22 \r\n     23                 loss += loss_function(target[:, i], predictions)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    755       if not in_deferred_mode:\r\n    756         self._in_call = True\r\n--> 757         outputs = self.call(inputs, *args, **kwargs)\r\n    758         self._in_call = False\r\n    759         if outputs is None:\r\n\r\n<ipython-input-23-b844d20e3fc2> in call(self, x, features, hidden)\r\n     16 \r\n     17     # x shape after passing through embedding == (batch_size, 1, embedding_dim)\r\n---> 18     x = self.embedding(x)\r\n     19 \r\n     20     # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    755       if not in_deferred_mode:\r\n    756         self._in_call = True\r\n--> 757         outputs = self.call(inputs, *args, **kwargs)\r\n    758         self._in_call = False\r\n    759         if outputs is None:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/embeddings.py in call(self, inputs)\r\n    175     if dtype != 'int32' and dtype != 'int64':\r\n    176       inputs = math_ops.cast(inputs, 'int32')\r\n--> 177     out = embedding_ops.embedding_lookup(self.embeddings, inputs)\r\n    178     return out\r\n    179 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py in embedding_lookup(params, ids, partition_strategy, name, validate_indices, max_norm)\r\n    311       name=name,\r\n    312       max_norm=max_norm,\r\n--> 313       transform_fn=None)\r\n    314 \r\n    315 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py in _embedding_lookup_and_transform(params, ids, partition_strategy, name, max_norm, transform_fn)\r\n    131     if np == 1 and (not transform_fn or ids.get_shape().ndims == 1):\r\n    132       with ops.colocate_with(params[0]):\r\n--> 133         result = _clip(array_ops.gather(params[0], ids, name=name),\r\n    134                        ids, max_norm)\r\n    135         if transform_fn:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in gather(***failed resolving arguments***)\r\n   2671     # TODO(apassos) find a less bad way of detecting resource variables without\r\n   2672     # introducing a circular dependency.\r\n-> 2673     return params.sparse_read(indices, name=name)\r\n   2674   except AttributeError:\r\n   2675     return gen_array_ops.gather_v2(params, indices, axis, name=name)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in sparse_read(self, indices, name)\r\n    756         tape.variable_accessed(self)\r\n    757       value = gen_resource_variable_ops.resource_gather(\r\n--> 758           self._handle, indices, dtype=self._dtype, name=name)\r\n    759     return array_ops.identity(value)\r\n    760 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py in resource_gather(resource, indices, dtype, validate_indices, name)\r\n    611       else:\r\n    612         message = e.message\r\n--> 613       _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n    614 \r\n    615 \r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: indices[26,0] = 5001 is not in [0, 5001) [Op:ResourceGather] name: rnn__decoder/embedding/embedding_lookup/\r\n", "comments": ["Same issue for me, with \"indices[6,0] = 5001 is not in [0, 5001)\"", "Answer found: increase the vocabulary size. \r\nIt is 5000 by default. This error is like an out of bound index error.  ", "**Your kindness is very much appreciated.**\r\n\r\nThat means that the number of \"vocab_size = len(tokenizer.word_index)\" is too big, Am I saying this correctly now? \r\nHowever, I used the same codes, and Dataset (MS COCO)\r\nTherefore, should I change the setting or dataset or codes about \"tokenizer\" or \"vocab\"?\r\n\r\nThank you for your continued support.\r\n", "@AKI-github \r\ncan you Provide the exact sequence of commands / steps that you executed before running into the problem. \r\nand i this this because you to let vocab_size = len(tokenizer.word_index)+1\r\n\r\n", "@AKI-github Is this resolved? Please close it if it was resolved by the suggestion of @aosman96 . Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!", "Hi All,\r\nI am getting a similar error, the snippets I have shared also. I tried to increase the Vocab size too but still got the same error. I have been trying to run it on my CPU only with no GPU installed. Can anyone help in this regard?\r\n<img width=\"554\" alt=\"Capture3\" src=\"https://user-images.githubusercontent.com/9390908/57506491-8f799f80-7319-11e9-9341-c53957cd4298.PNG\">\r\n\r\n\r\nInvalidArgumentError: indices[24,197] = 80000 is not in [0, 80000)\r\n\t [[{{node embedding_lookup}}]]\r\n\r\n", "set your embedding layer to len(words)+1, in your case 5001+1", "I was having a similar issue although I made sure that the vocab size is len(words) + 1 and there is no input larger than vocab size-1. I was having a dropout layer before the embedding layer and when I changed the order (embedding layer first) it worked. I am not sure why. ", "> Answer found: increase the vocabulary size.\r\n> It is 5000 by default. This error is like an out of bound index error.\r\n\r\nSolved my problem. Thanks!\r\n", "In my project I've changed: model.add(keras.layers.Embedding(10000, 16)) to model.add(keras.layers.Embedding(15000, 16)) and it fixed the problem. ", "When I pass class_weight to model.fit() the error comes and when I do not pass it goes? any Idea how to solve it?\r\n\r\n`\r\nTensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument:  indices[84] = 31 is not in [0, 31)\r\n\t [[{{node GatherV2}}]]\r\n\t [[IteratorGetNext]]\r\n\t [[ReverseSequence_1/_502]]\r\n  (1) Invalid argument:  indices[84] = 31 is not in [0, 31)\r\n\t [[{{node GatherV2}}]]\r\n\t [[IteratorGetNext]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_28314]\r\n\r\nFunction call stack:\r\ntrain_function -> train_function\r\n`\r\n\r\n`       model_input = tf.keras.layers.Input(shape=(timesteps,), dtype='int32')\r\n        self.createBertLayer()\r\n\r\n        bert = self.bert_layer(model_input)\r\n\r\n\r\n        self.d = tf.keras.layers.Dense(num_hiddens, activation=tf.nn.relu)(bert)\r\n        h = tf.keras.layers.Dropout(0.5)(self.d)\r\n        self.y1 = tf.keras.layers.Dense(classes, activation=tf.nn.softmax)(h)\r\n\r\n\r\n        self.crf = CRF()\r\n        output = self.crf(self.y1)\r\n\r\n        model = Model(model_input, output)\r\n        model.build(input_shape=(None, timesteps))\r\n        self.bert_layer.apply_adapter_freeze()\r\n        self.bert_layer.trainable = False\r\n\r\n        model.compile(loss=self.crf.loss, optimizer=tf.keras.optimizers.Adam(0.001), metrics=[self.crf.accuracy])\r\n\r\n        history = self.model.fit(\r\n            training_set,\r\n            training_label,\r\n            class_weight=class_weights,\r\n            epochs=n_epochs,\r\n            # validation_data=(testing_set, one_hotted2),\r\n            validation_split=0.1,\r\n            verbose=1,\r\n            batch_size=256\r\n            # callbacks=[cp_callback]\r\n        )\r\n`\r\n\r\nI checked class_weight keys and there was no problem with it.", "This issue is not limited to NLP but also for other types of DL problems. I have a multilabel classification problem with time series (24 labels - imbalanced). When I pass `class_weight` in fit method, I get this same error. Anyone can be kind and let us know who to solve it in this case? \r\n\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-1-84f69cedcd5b> in <module>\r\n     37 with timer('training models ================'):\r\n     38     hist = model.fit(x=training_generator, verbose=1, epochs=5, callbacks=callbacks, \r\n---> 39                      class_weight= get_class_wieght(),\r\n     40 #                     use_multiprocessing=True, workers=8\r\n     41                     )\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n     64   def _method_wrapper(self, *args, **kwargs):\r\n     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n---> 66       return method(self, *args, **kwargs)\r\n     67 \r\n     68     # Running inside `run_distribute_coordinator` already.\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n    846                 batch_size=batch_size):\r\n    847               callbacks.on_train_batch_begin(step)\r\n--> 848               tmp_logs = train_function(iterator)\r\n    849               # Catch OutOfRangeError for Datasets of unknown size.\r\n    850               # This blocks until the batch has finished executing.\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    578         xla_context.Exit()\r\n    579     else:\r\n--> 580       result = self._call(*args, **kwds)\r\n    581 \r\n    582     if tracing_count == self._get_tracing_count():\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    642         # Lifting succeeded, so variables are initialized and we can run the\r\n    643         # stateless function.\r\n--> 644         return self._stateless_fn(*args, **kwds)\r\n    645     else:\r\n    646       canon_args, canon_kwds = \\\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   2418     with self._lock:\r\n   2419       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 2420     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   2421 \r\n   2422   @property\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs)\r\n   1663          if isinstance(t, (ops.Tensor,\r\n   1664                            resource_variable_ops.BaseResourceVariable))),\r\n-> 1665         self.captured_inputs)\r\n   1666 \r\n   1667   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1744       # No tape is watching; skip to running the function.\r\n   1745       return self._build_call_outputs(self._inference_function.call(\r\n-> 1746           ctx, args, cancellation_manager=cancellation_manager))\r\n   1747     forward_backward = self._select_forward_and_backward_functions(\r\n   1748         args,\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n    596               inputs=args,\r\n    597               attrs=attrs,\r\n--> 598               ctx=ctx)\r\n    599         else:\r\n    600           outputs = execute.execute_with_cancellation(\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nInvalidArgumentError:  indices[3] = 24 is not in [0, 24)\r\n\t [[{{node GatherV2}}]]\r\n\t [[IteratorGetNext]] [Op:__inference_train_function_29319]\r\n\r\nFunction call stack:\r\ntrain_function\r\n```", "> Answer found: increase the vocabulary size.\r\n> It is 5000 by default. This error is like an out of bound index error.\r\n\r\nHi, how can i increase the vocab size? Which file should i modify? Thankyou", "Not completely sure why, but this error seems to happen when vocab_size <=  len(tokenizer.word_index)", "I am trying ragged tensors but Im not using text data so hod can i deal with this error `InvalidArgumentError:  indices[42355876] = -1 is not in [0, 6057585)\r\n\t [[node sequential_21/embedding_5/embedding_lookup_ragged/embedding_lookup (defined at <ipython-input-50-ded3c3b47b8d>:17) ]] [Op:__inference_train_function_9333]`.  This index crashes my colab session.", "Same issue cased by class_weight, worked fine without class_weight. Same issues everywhere on the web.\r\n\r\nhttps://datascience.stackexchange.com/questions/71774/setting-class-weights-for-imbalanced-dataset-in-tensorflow\r\n\r\nBye bye.", "I have a similar error, the model I'm working on is for text classification and word embeddings was used to transform the data. When trying to apply the exported model back on the training data on a separate occasion, the following error appears.\r\n```\r\nInvalidArgumentError:  indices[23,11] = 2489 is not in [0, 2489)\r\n```\r\nMy variables are `len(tokenizer.word_index) = 2488` and `len(embedding_matrix) = 2489`. I have looked into increasing the vocab_size in the Embedding layer by increasing it by 1 (and tried increasing it to 5000 as well) but the following error pops up.\r\n\r\n```\r\nValueError: Layer weight shape (2490, 200) not compatible with provided weight shape (2489, 200)\r\n```\r\n\r\nAnyone has any advice to tackle this issue? As my project is scheduled to end soon, I would really appreciate quick responses!", "InvalidArgumentError:  indices[12,21] = 20 is not in [0, 20)\r\n\t [[node embedding_2_9/embedding_lookup (defined at C:\\Users\\philp\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3009) ]] [Op:__inference_keras_scratch_graph_16422]\r\n\r\nFunction call stack:\r\nkeras_scratch_graph\r\n\r\nI am increasing vocabulary size to 21 lets see what will be the output ", "Universe of discourse \r\nalphabet = 'ARNDCQEGHILKMFPSTWYV-'\r\n\r\nVocabulary Size = 21  \r\n\r\n\ud83d\udc4e Wrong Approach:\r\nmodel = Sequential()\r\nembeddings = Embedding(20, 31, input_length=win_size)\r\n\r\n\ud83d\udc4d Right Approach:\r\nmodel = Sequential()\r\nembeddings = Embedding(21, 31, input_length=win_size)\r\n\r\nBecause I have 21 letters in universe of discourse or my vocabulary size is 21. \r\n\r\nThank You. \r\n", "> > Answer found: increase the vocabulary size.\r\n> > It is 5000 by default. This error is like an out of bound index error.\r\n> \r\n> Hi, how can i increase the vocab size? Which file should i modify? Thankyou\r\n\r\n\r\n\r\n> Answer found: increase the vocabulary size.\r\n> It is 5000 by default. This error is like an out of bound index error.\r\n\r\nthanks a lot, my problem solved ", "> set your embedding layer to len(words)+1, in your case 5001+1\r\n\r\ncould you explain the reason that you add one to embedding size?", "In my case, I was getting a large negative value like this: \r\nInvalidArgumentError: indices[15,28] = LARGE NEGATIVE VALUE\r\nSo it seemed that it was not really  an index out of bound issue. \r\nFound that this was because I was setting the embedding layer to be trainable, so fixed that part by doing \r\n`trainable=False`", "I have got the same error, and I solved it as follow (I have got the response from the responses in this issue):\r\n\r\nmodel.add(Embedding(input_dim=vocab_size+1, output_dim=10, input_length=len_of_one_input_example))"]}, {"number": 23697, "title": "Cannot assign a device for operation 'optimizer/dense_1/bias/RMSProp_1': Operation was explicitly assigned to /job:ps/task:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device. \t [[Node: optimizer/dense_1/bias/RMSProp_1 = VariableV2[_class=[\"loc:@dense_1/bias\"], container=\"\", dtype=DT_FLOAT, shape=[5], shared_name=\"\", _device=\"/job:ps/task:0\"]()]]", "body": "The error is when I am trying to run distributed training with edward. Below is the code : \r\n\r\n```\r\nparameter_servers = [\"localhost:2222\"]\r\nworkers = [\"localhost:2223\"]\r\ncluster = tf.train.ClusterSpec({\"ps\": parameter_servers, \"worker\": workers})\r\n\r\n# start a server for a specific task\r\nserver = tf.train.Server(\r\n     cluster,\r\n     job_name=FLAGS.job_name,\r\n     task_index=FLAGS.task_index)\r\n     \r\nclass VAE(object):\r\n   def __init__():\r\n       # MODEL\r\n       self.n_features = n_features\r\n       self.params = {\r\n                'M': 2048,\r\n                'd': 5,\r\n                'n_epoch': 2,\r\n                'hidden_d': [25, 5],\r\n                'learning_rate': 0.01\r\n            }\r\n       self.saved_dir_path = dir_path\r\n       self.ckpt_path = os.path.join(self.saved_dir_path,\r\n                                     'checkpointFiles/') + 'model.ckpt'\r\n       # distributed training\r\n       if FLAGS.job_name == \"ps\":\r\n           server.join()\r\n       elif FLAGS.job_name == \"worker\":\r\n           # Between-graph replication\r\n           with tf.device(tf.train.replica_device_setter(\r\n                   worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\r\n                   cluster=cluster)):\r\n               self.global_step = tf.get_variable(\r\n                                                    'global_step',\r\n                                                    [],\r\n                                                    dtype=tf.int64,\r\n                                                    initializer=tf.constant_initializer(0),\r\n                                                    trainable=False)\r\n               self.z = Normal(\r\n                   loc=tf.zeros([self.params['M'], self.params['d']]),\r\n                   scale=tf.ones([self.params['M'], self.params['d']]))\r\n               # self.hidden0 = tf.layers.dense(\r\n               self.hidden = tf.layers.dense(\r\n                   self.z, self.params['hidden_d'][0], activation=tf.nn.relu)\r\n               if len(self.params['hidden_d']) > 1:\r\n                   # for i in xrange(1, len(params[hidden_d])):\r\n                   #     self.__dict__['hidden' + str(i)] = \\\r\n                   #         tf.layers.dense(\r\n                   #             self.__dict__['hidden' + str(i-1)],\r\n                   #             self.params['hidden_d'][i],\r\n                   #             activation=tf.nn.relu)\r\n                   for i in xrange(1, len(params['hidden_d'])):\r\n                       self.hidden = \\\r\n                           tf.layers.dense(\r\n                               self.hidden,\r\n                               self.params['hidden_d'][i],\r\n                               activation=tf.nn.relu)\r\n               self.x = Bernoulli(\r\n                   logits=tf.layers.dense(\r\n                       # self.__dict__['hidden' + str(len(params['hidden_d'] - 1))],\r\n                       self.hidden,\r\n                       self.n_features), dtype=tf.float64)\r\n\r\n               # INFERENCE\r\n               self.x_ph = tf.placeholder(dtype=tf.float64,shape=[None, self.n_features])\r\n               self.hidden = tf.layers.dense(\r\n                   tf.cast(self.x_ph, tf.float32),\r\n                   self.params['hidden_d'][-1],\r\n                   activation=tf.nn.relu)\r\n               if len(self.params['hidden_d']) > 1:\r\n                   for i in xrange(1, len(params['hidden_d'])):\r\n                       j = -(1+i)\r\n                       self.hidden = \\\r\n                           tf.layers.dense(\r\n                               self.hidden,\r\n                               self.params['hidden_d'][j],\r\n                               activation=tf.nn.relu)\r\n               self.qz = Normal(\r\n                   loc=tf.layers.dense(self.hidden, self.params['d']),\r\n                   scale=tf.layers.dense(\r\n                       self.hidden, self.params['d'], activation=tf.nn.softplus))\r\n               self.x_avg = Bernoulli(\r\n                   logits=tf.reduce_mean(self.x.parameters['logits'], 0),\r\n                   name='x_avg')\r\n               self.log_likli = tf.reduce_mean(self.x_avg.log_prob(self.x_ph), 1)\r\n               self.optimizer = tf.train.RMSPropOptimizer(\r\n                   self.params['learning_rate'], epsilon=1.0)\r\n               # self.\r\n               self.inference = ed.KLqp({self.z: self.qz}, data={self.x: self.x_ph})\r\n               self.inference_init = self.inference.initialize(\r\n                   optimizer=self.optimizer, global_step = self.global_step, logdir='log')\r\n               self.init = tf.global_variables_initializer()\r\n               self.saver = tf.train.Saver()\r\n\r\n   def train(self, train_data):\r\n      #Generate x_batch\r\n      start = 0  # pointer to where we are in iteration\r\n      while True:\r\n         stop = start + self.params['M']\r\n         diff = stop - train_data.shape[0]\r\n         if diff <= 0:\r\n             batch = train_data[start:stop]\r\n             start += self.params['M']\r\n         else:\r\n             batch = np.concatenate((train_data[start:], train_data[:diff]))\r\n             start = diff\r\n         yield batch\r\n       train_data_generator = batch\r\n\r\n       saver_hook = tf.train.CheckpointSaverHook(\r\n                                                 checkpoint_dir=FLAGS.model_path,\r\n                                                 save_steps=100,\r\n                                                 saver=tf.train.Saver(),\r\n                                                 checkpoint_basename='model.ckpt',\r\n                                                 scaffold=None\r\n                                                 )\r\n\r\n       hooks = [saver_hook]\r\n\r\n       with tf.train.MonitoredTrainingSession(\r\n                                              master=server.target,\r\n                                              is_chief=(FLAGS.task_index == 0),\r\n                                              checkpoint_dir=FLAGS.model_path,\r\n                                              hooks=hooks,\r\n                                              config= tf.ConfigProto(allow_soft_placement=True,\r\n                                                                     log_device_placement=True),\r\n                                              save_summaries_steps=None,\r\n                                              save_summaries_secs=None\r\n                                              ) as sess:\r\n           sess.run(self.init)\r\n           # sess.run(self.inference_init)\r\n           # self.inference.initialize(optimizer=self.optimizer)\r\n           n_iter_per_epoch = np.ceil(\r\n               train_data.shape[0] / self.params['M']).astype(int)\r\n\r\n           for epoch in xrange(1, self.params['n_epoch'] + 1):\r\n               print \"Epoch: {0}\".format(epoch)\r\n               avg_loss = 0.0\r\n               pbar = Progbar(n_iter_per_epoch)\r\n               for t in xrange(1, n_iter_per_epoch + 1):\r\n                   pbar.update(t)\r\n                   x_batch = next(train_data_generator)\r\n                   info_dict = self.inference.update(\r\n                       feed_dict={self.x_ph: x_batch})\r\n                   avg_loss += info_dict['loss']\r\n               avg_loss /= n_iter_per_epoch\r\n               avg_loss /= self.params['M']\r\n               print \"-log p(x) <= {:0.3f}\".format(avg_loss)\r\n           print \"Done training the model.\"\r\n           \r\n if __name__ == \"__main__\":\r\n     vae = VAE()\r\n     vae.train(data)\r\n\r\n```\r\n\r\nThe error stack is as follows : \r\n\r\n```\r\n2018-11-12 15:46:11.525824: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-11-12 15:46:11.527300: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2222}\r\n2018-11-12 15:46:11.527310: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223}\r\n2018-11-12 15:46:11.527775: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2223\r\n51 features are used.\r\n2018-11-12 15:46:16.294466: I tensorflow/core/distributed_runtime/master_session.cc:1024] Start master session e39a9b7d7a1216dc with config:\r\nEpoch: 1\r\n 1/10 [ 10%] \u2588\u2588\u2588                            ETA: 0sTraceback (most recent call last):\r\n  File \"dist_vae.py\", line 357, in <module>\r\n    vae.train(data)\r\n  File \"dist_vae.py\", line 220, in train\r\n    feed_dict={self.x_ph: x_batch})\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/edward/inferences/variational_inference.py\", line 154, in update\r\n    _, t, loss = sess.run([self.train, self.increment_t, self.loss], feed_dict)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 905, in run\r\n    run_metadata_ptr)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1137, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1355, in _do_run\r\n    options, run_metadata)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1374, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'optimizer/dense_1/bias/RMSProp_1': Operation was explicitly assigned to /job:ps/task:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.\r\n\t [[Node: optimizer/dense_1/bias/RMSProp_1 = VariableV2[_class=[\"loc:@dense_1/bias\"], container=\"\", dtype=DT_FLOAT, shape=[5], shared_name=\"\", _device=\"/job:ps/task:0\"]()]]\r\n\r\nCaused by op u'optimizer/dense_1/bias/RMSProp_1', defined at:\r\n  File \"dist_vae.py\", line 356, in <module>\r\n    vae = VAE(filepath, params, n_features)\r\n  File \"dist_vae.py\", line 173, in __init__\r\n    optimizer=self.optimizer, global_step = self.global_step, logdir='log')\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/edward/inferences/klqp.py\", line 110, in initialize\r\n    return super(KLqp, self).initialize(*args, **kwargs)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/edward/inferences/variational_inference.py\", line 121, in initialize\r\n    global_step=global_step)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 520, in apply_gradients\r\n    self._create_slots([_get_variable_for(v) for v in var_list])\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/rmsprop.py\", line 115, in _create_slots\r\n    self._zeros_slot(v, \"momentum\", self._name)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 910, in _zeros_slot\r\n    named_slots[_var_key(var)] = slot_creator.create_zeros_slot(var, op_name)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py\", line 174, in create_zeros_slot\r\n    colocate_with_primary=colocate_with_primary)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py\", line 148, in create_slot_with_initializer\r\n    dtype)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py\", line 67, in _create_slot_var\r\n    validate_shape=validate_shape)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1297, in get_variable\r\n    constraint=constraint)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1093, in get_variable\r\n    constraint=constraint)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 439, in get_variable\r\n    constraint=constraint)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 408, in _true_getter\r\n    use_resource=use_resource, constraint=constraint)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 800, in _get_single_variable\r\n    use_resource=use_resource)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2157, in variable\r\n    use_resource=use_resource)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2147, in <lambda>\r\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2130, in default_variable_creator\r\n    constraint=constraint)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 233, in __init__\r\n    constraint=constraint)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 333, in _init_from_args\r\n    name=name)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/state_ops.py\", line 134, in variable_op_v2\r\n    shared_name=shared_name)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 1043, in _variable_v2\r\n    shared_name=shared_name, name=name)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\r\n    op_def=op_def)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'optimizer/dense_1/bias/RMSProp_1': Operation was explicitly assigned to /job:ps/task:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.\r\n\t [[Node: optimizer/dense_1/bias/RMSProp_1 = VariableV2[_class=[\"loc:@dense_1/bias\"], container=\"\", dtype=DT_FLOAT, shape=[5], shared_name=\"\", _device=\"/job:ps/task:0\"]()]]\r\n\r\n```\r\n\r\nWhen config= tf.ConfigProto(allow_soft_placement=True, log_device_placement=True) : \r\n\r\n```\r\noptimizer/RMSProp/value: (Const): /job:ps/replica:0/task:0/device:CPU:0\r\n2018-11-12 15:50:32.823393: I tensorflow/core/common_runtime/placer.cc:875] optimizer/RMSProp/value: (Const)/job:ps/replica:0/task:0/device:CPU:0\r\nglobal_step/Initializer/Const: (Const): /job:ps/replica:0/task:0/device:CPU:0\r\n\r\n```\r\n\r\nThanks for the help! :) ", "comments": ["This is a bug/missing feature in Edward, not TensorFlow. Please open an issue on that project's repository.\r\n\r\nThe issue is that when you run this line:\r\n\r\n```python\r\n                   info_dict = self.inference.update(\r\n                       feed_dict={self.x_ph: x_batch})\r\n```\r\n\r\n...it uses a *different* `tf.Session` at this line in the Edward codebase:\r\n\r\n```\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/edward/inferences/variational_inference.py\", line 154, in update\r\n    _, t, loss = sess.run([self.train, self.increment_t, self.loss], feed_dict)\r\n```\r\n\r\nIn particular, that `tf.Session` does not know about `server.target`, and so it does not have access to any of the distributed devices that you created. The error message \"available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]\" indicates that the `tf.Session` being used is a single-process session, presumably created in the Edward code.", "Thank you for the reply :)"]}]