[{"number": 30576, "title": "tf.function input signature error messages are not-helpful walls of text", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): yes\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2 b 1\r\n- Python version: 3.6.8\r\n\r\n**Describe the current behavior**\r\nif you make any errors with Input Signature on autograph / tf.function, the error message is essentially useless:\r\n\r\n```        \r\nneuromax.py:396 train  *\r\n        change, total_change, episode, episodes_this_dataset = run_episode(type, n_atoms, target_positions, positions, features, masses, quantum_target, target_features, change, total_change, episode, episodes_this_dataset, gif)\r\n    /home/bion/anaconda3/envs/tfg/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:404 __call__\r\n        return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n    /home/bion/anaconda3/envs/tfg/lib/python3.6/site-packages/tensorflow/python/eager/function.py:1334 __call__\r\n        graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n    /home/bion/anaconda3/envs/tfg/lib/python3.6/site-packages/tensorflow/python/eager/function.py:1612 _maybe_define_function\r\n        *args, **kwargs)\r\n    /home/bion/anaconda3/envs/tfg/lib/python3.6/site-packages/tensorflow/python/eager/function.py:1169 canonicalize_function_inputs\r\n        self._flat_input_signature)\r\n    /home/bion/anaconda3/envs/tfg/lib/python3.6/site-packages/tensorflow/python/eager/function.py:1229 _convert_inputs_to_signature\r\n        (str(inputs), str(input_signature)))\r\n\r\n    ValueError: Python inputs incompatible with input_signature: inputs ((<tf.Tensor 'args_5:0' shape=(None,) dtype=string>, <tf.Tensor 'args_7:0' shape=(None,) dtype=int32>, <tf.Tensor 'args_8:0' shape=(None, None, 3) dtype=float32>, <tf.Tensor 'args_9:0' shape=(None, None, 3) dtype=float32>, <tf.Tensor 'args_10:0' shape=(None, None, 7) dtype=float32>, <tf.Tensor 'args_11:0' shape=(None, None, 3) dtype=float32>, <tf.Tensor 'args_12:0' shape=<unknown> dtype=float32>, <tf.Tensor 'args_13:0' shape=(None, None, 7) dtype=float32>, <tf.Tensor 'args_0:0' shape=() dtype=float32>, <tf.Tensor 'args_4:0' shape=() dtype=float32>, <tf.Tensor 'args_1:0' shape=() dtype=int32>, <tf.Tensor 'args_2:0' shape=() dtype=int32>, False)), input_signature ((TensorSpec(shape=(1,), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None), TensorSpec(shape=(None, None, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, None, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, None, 7), dtype=tf.float32, name=None), TensorSpec(shape=(None, None, 3), dtype=tf.float32, name=None), TensorSpec(shape=(15,), dtype=tf.float32, name=None), TensorSpec(shape=(None, None, 7), dtype=tf.float32, name=None), TensorSpec(shape=(1,), dtype=tf.float32, name=None), TensorSpec(shape=(1,), dtype=tf.float32, name=None), TensorSpec(shape=(1,), dtype=tf.int32, name=None), TensorSpec(shape=(1,), dtype=tf.int32, name=None), TensorSpec(shape=(1,), dtype=tf.bool, name=None)))\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\nthis error message should be formatted and it should highlight exactly which input is incompatible. Otherwise this can become a needle in a haystack and gets annoying real quick\r\n\r\n**Code to reproduce the issue**\r\nN/A\r\n\r\n", "comments": ["@bionicles,\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "```\r\nimport tensorflow as tf\r\n\r\nDTYPE = tf.float32\r\n\r\n@tf.function(input_signature=[tf.TensorSpec(shape=(None, None, 1), dtype=DTYPE),\r\n                              tf.TensorSpec(shape=(None, None, 2), dtype=DTYPE),\r\n                              tf.TensorSpec(shape=(None, None, 3), dtype=DTYPE),\r\n                              tf.TensorSpec(shape=(None, None, 4), dtype=DTYPE),\r\n                              tf.TensorSpec(shape=(None, None, 5), dtype=DTYPE),\r\n                              tf.TensorSpec(shape=(None, None, 6), dtype=DTYPE),\r\n                              tf.TensorSpec(shape=(None, None, 7), dtype=DTYPE),\r\n                              tf.TensorSpec(shape=(None, None, 8), dtype=DTYPE),\r\n                              tf.TensorSpec(shape=(None, None, 9), dtype=DTYPE)])\r\ndef print_useless_wall_of_text(one, two, three, four, five, six, seven, eight, nine):\r\n    return one, two, three, four, five, six, seven, eight, nine\r\n\r\n\r\none = tf.random.normal((1, 123, 1), dtype=DTYPE)\r\ntwo = tf.random.normal((1, 123, 2), dtype=DTYPE)\r\nthree = tf.random.normal((1, 123, 3), dtype=DTYPE)\r\nfour = tf.random.normal((1, 123, 4), dtype=DTYPE)\r\nfive = tf.random.normal((1, 123, 5), dtype=DTYPE)\r\nsix = tf.random.normal((1, 123, 6), dtype=DTYPE)\r\nseven = tf.random.normal((1, 123, 7), dtype=DTYPE)\r\neight = tf.random.normal((1, 123, 8), dtype=DTYPE)\r\nwrong = tf.random.normal((1, 123, 1), dtype=DTYPE)\r\nstuff = print_useless_wall_of_text(one, two, three, four, five, six, seven, eight, wrong)\r\n```", "Reproduced the error with TF Version 2.0.beta1", "@jaingaurav This looks like a verification inside tf.function; improving it to point to the specific culprit sounds sensible to me.", "Added a PR #30902 for the fix."]}, {"number": 30575, "title": "register BatchMatMul op flops", "body": "this commits addresses issue #22071, and add flops calculation for BatchMatMul op. The only change from the MatMul op is that the intermediate k should be from the second to last dim of the a_shape.", "comments": ["Seems auto-merge is not happening but the changes are now committed so we can close this. Thank you for the PR."]}, {"number": 30574, "title": "Decode_wav sample rate output cannot be passed to tf.signal.linear_to_mel_weight_matrix", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Combined output of decode_wav with the sample in [signal/mfccs_from_log_mel_spectrograms](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/signal/mfccs_from_log_mel_spectrograms)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Conda binary\r\n- TensorFlow version (use command below): 2.0.0-dev20190702 (git version unknown)\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: \r\n- GPU model and memory: Surface Book Nvidia GPU\r\n\r\n**Describe the current behavior**\r\nThe output of decode_wav is tuple of (wav_data, sample_rate) \r\nSample_rate is int32 but linear_to_mel_weight_matrix expects a float32 sample_rate.\r\nIf the sample rate is cast using tf.cast(sample_rate, float32) and then a TypeError is thrown with the message:\r\n\r\nTypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.\r\n\r\n**Describe the expected behavior**\r\nSample rate output of decode_wav can be used as input to linear_to_mel_weight_matrix.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ndef load_and_mel_file(path_tensor):\r\n    #From: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/signal/mfccs_from_log_mel_spectrograms\r\n    pcm, sample_rate = tf.audio.decode_wav(path_tensor)\r\n    sr_f = tf.cast(sample_rate, tf.float32) #Mismatch in types between output of decode_wav and input to linear_to_mel_weight_matrix\r\n    print(pcm, sample_rate, sr_f)\r\n\r\n    # A 1024-point STFT with frames of 64 ms and 75% overlap.\r\n    stfts = tf.signal.stft(pcm, frame_length=1024, frame_step=256,\r\n                           fft_length=1024)\r\n    spectrograms = tf.abs(stfts)\r\n\r\n    # Warp the linear scale spectrograms into the mel-scale.\r\n    num_spectrogram_bins = stfts.shape[-1]\r\n    lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80\r\n    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\r\n      num_mel_bins, num_spectrogram_bins, sr_f, lower_edge_hertz,\r\n      upper_edge_hertz)\r\n    mel_spectrograms = tf.tensordot(\r\n      spectrograms, linear_to_mel_weight_matrix, 1)\r\n    mel_spectrograms.set_shape(spectrograms.shape[:-1].concatenate(\r\n      linear_to_mel_weight_matrix.shape[-1:]))\r\n\r\n    # Compute a stabilized log to get log-magnitude mel-scale spectrograms.\r\n    log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\r\n    print(log_mel_spectrograms)\r\n    \r\n    return log_mel_spectrograms\r\n\r\npath_ds = tf.data.Dataset.list_files(\"*.wav\")\r\nmel_ds = path_ds.map(load_and_mel_file)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n> > TypeError                                 Traceback (most recent call last)\r\n> <ipython-input-61-aaa96a9258a6> in <module>\r\n>       1 #Build datasets\r\n> ----> 2 train_ds = build_data_pairs_from_dir(train_dir)\r\n>       3 test_ds = build_data_pairs_from_dir(test_dir)\r\n> \r\n> <ipython-input-60-92023d8b5863> in build_data_pairs_from_dir(source_dir)\r\n>      54 \r\n>      55     #Convert to MEL\r\n> ---> 56     clean_mel_ds = clean_path_ds.map(load_and_mel_file, num_parallel_calls=AUTOTUNE)\r\n>      57     noisy_mel_ds = noisy_path_ds.map(load_and_mel_file, num_parallel_calls=AUTOTUNE)\r\n>      58 \r\n> \r\n> c:\\users\\benhe\\.conda\\envs\\homl2\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py in map(self, map_func, num_parallel_calls)\r\n>    1887       return DatasetV1Adapter(\r\n>    1888           ParallelMapDataset(\r\n> -> 1889               self, map_func, num_parallel_calls, preserve_cardinality=False))\r\n>    1890 \r\n>    1891   @deprecation.deprecated(None, \"Use `tf.data.Dataset.map()\")\r\n> \r\n> c:\\users\\benhe\\.conda\\envs\\homl2\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py in __init__(self, input_dataset, map_func, num_parallel_calls, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\r\n>    3333         self._transformation_name(),\r\n>    3334         dataset=input_dataset,\r\n> -> 3335         use_legacy_function=use_legacy_function)\r\n>    3336     self._num_parallel_calls = ops.convert_to_tensor(\r\n>    3337         num_parallel_calls, dtype=dtypes.int32, name=\"num_parallel_calls\")\r\n> \r\n> c:\\users\\benhe\\.conda\\envs\\homl2\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py in __init__(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\r\n>    2677       resource_tracker = tracking.ResourceTracker()\r\n>    2678       with tracking.resource_tracker_scope(resource_tracker):\r\n> -> 2679         self._function = wrapper_fn._get_concrete_function_internal()\r\n>    2680         if add_to_graph:\r\n>    2681           self._function.add_to_graph(ops.get_default_graph())\r\n> \r\n> c:\\users\\benhe\\.conda\\envs\\homl2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _get_concrete_function_internal(self, *args, **kwargs)\r\n>    1418     \"\"\"Bypasses error checking when getting a graph function.\"\"\"\r\n>    1419     graph_function = self._get_concrete_function_internal_garbage_collected(\r\n> -> 1420         *args, **kwargs)\r\n>    1421     # We're returning this concrete function to someone, and they may keep a\r\n>    1422     # reference to the FuncGraph without keeping a reference to the\r\n> \r\n> c:\\users\\benhe\\.conda\\envs\\homl2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n>    1412     if self.input_signature:\r\n>    1413       args, kwargs = None, None\r\n> -> 1414     graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n>    1415     return graph_function\r\n>    1416 \r\n> \r\n> c:\\users\\benhe\\.conda\\envs\\homl2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _maybe_define_function(self, args, kwargs)\r\n>    1716         graph_function = self._function_cache.primary.get(cache_key, None)\r\n>    1717         if graph_function is None:\r\n> -> 1718           graph_function = self._create_graph_function(args, kwargs)\r\n>    1719           self._function_cache.primary[cache_key] = graph_function\r\n>    1720         return graph_function, args, kwargs\r\n> \r\n> c:\\users\\benhe\\.conda\\envs\\homl2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n>    1602             arg_names=arg_names,\r\n>    1603             override_flat_arg_shapes=override_flat_arg_shapes,\r\n> -> 1604             capture_by_value=self._capture_by_value),\r\n>    1605         self._function_attributes)\r\n>    1606 \r\n> \r\n> c:\\users\\benhe\\.conda\\envs\\homl2\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n>     784                                           converted_func)\r\n>     785 \r\n> --> 786       func_outputs = python_func(*func_args, **func_kwargs)\r\n>     787 \r\n>     788       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n> \r\n> c:\\users\\benhe\\.conda\\envs\\homl2\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py in wrapper_fn(*args)\r\n>    2671           attributes=defun_kwargs)\r\n>    2672       def wrapper_fn(*args):  # pylint: disable=missing-docstring\r\n> -> 2673         ret = _wrapper_helper(*args)\r\n>    2674         ret = structure.to_tensor_list(self._output_structure, ret)\r\n>    2675         return [ops.convert_to_tensor(t) for t in ret]\r\n> \r\n> c:\\users\\benhe\\.conda\\envs\\homl2\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py in _wrapper_helper(*args)\r\n>    2616         nested_args = (nested_args,)\r\n>    2617 \r\n> -> 2618       ret = autograph.tf_convert(func, ag_ctx)(*nested_args)\r\n>    2619       # If `func` returns a list of tensors, `nest.flatten()` and\r\n>    2620       # `ops.convert_to_tensor()` would conspire to attempt to stack\r\n> \r\n> c:\\users\\benhe\\.conda\\envs\\homl2\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py in wrapper(*args, **kwargs)\r\n>     220         except Exception as e:  # pylint:disable=broad-except\r\n>     221           if hasattr(e, 'ag_error_metadata'):\r\n> --> 222             raise e.ag_error_metadata.to_exception(type(e))\r\n>     223           else:\r\n>     224             raise\r\n> \r\n> TypeError: in converted code:\r\n> \r\n>     <ipython-input-60-92023d8b5863>:27 load_and_mel_file  *\r\n>         linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\r\n>     c:\\users\\benhe\\.conda\\envs\\homl2\\lib\\site-packages\\tensorflow_core\\python\\ops\\signal\\mel_ops.py:155 linear_to_mel_weight_matrix\r\n>         lower_edge_hertz, upper_edge_hertz, dtype)\r\n>     c:\\users\\benhe\\.conda\\envs\\homl2\\lib\\site-packages\\tensorflow_core\\python\\ops\\signal\\mel_ops.py:74 _validate_arguments\r\n>         if sample_rate <= 0.0:\r\n>     c:\\users\\benhe\\.conda\\envs\\homl2\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:692 __bool__\r\n>         raise TypeError(\"Using a `tf.Tensor` as a Python `bool` is not allowed. \"\r\n> \r\n>     TypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.\r\n> \r\n\r\n\r\n", "comments": ["@jbgh2 I tried reproducing the issue on Colab with Tensorflow 2.0.0-dev20190709 but i got the below error `InvalidArgumentError: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: *.wav'` . Please help us to reproduce the issue. Thanks!", "@gadagashwini You need to have at least one .wav file in your working directory, otherwise it will obviously not run.", "@jbgh2 I tried executing the .wav file but I am getting the following error\r\n`\r\n\r\n> ValueError: `sample_rate` was a non-constant Tensor. Must be a Python float or a constant Tensor.\r\n\r\n`. Thanks!", "Bug repro in Colab using TF Versions: 2.0.0-beta1\r\nhttps://colab.research.google.com/drive/139qvpTBQH079_z-RFfeTcbJqixV2oL_G\r\n\r\nAs far as I can tell the bug is because _validate_arguments expects Python types not Tensors and decode_wav returns sample_rate as a Tensor.\r\n\r\nI've included code to repro both problems:\r\n1) Sample_Rate from decode_wav is an int32 tensor that is compared to a Python 0.0\r\n2) Cast sample_rate to float32 tensor causes bool comparison error", "Thanks for the report! I have a fix for this out for review.", "Fixed in 03ff87b.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30574\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30574\">No</a>\n"]}, {"number": 30573, "title": "Disable a bunch of tests to unblock patch release", "body": "", "comments": []}, {"number": 30572, "title": "register BatchMatMul op flops", "body": "this commit adds support for BatchMatMul flops calculation,  and it should solve #22071", "comments": []}, {"number": 30571, "title": "Trace/Wallclock Performance Discrepancy ", "body": "**Describe the bug**\r\n\r\nRunning a converted PyTorch model has some curious performance characteristics were running the `.pb` model directly in TensorFlow is ~2.5 times slower than the onnx runtime.\r\n\r\n```bash\r\n$ ./benchmark model\r\n> onnx runtime took 4.83842\r\n> tensorflow took 12.63854\r\n```\r\n\r\nLooking at the trace information shows that work is only being done for about 3.5 seconds so the question is what is `session.run` doing for the other ~9 seconds?  \r\n\r\n![onnx-tf-rnn-perf](https://user-images.githubusercontent.com/4310904/60820118-b547e700-a198-11e9-8c3f-f48c54e3952e.png)\r\n\r\n**To Reproduce**\r\n\r\n```python\r\n#!/usr/bin/env python\r\n\r\n\"\"\"\r\nBenchmark ONNX model\r\n\"\"\"\r\n\r\nimport time\r\nfrom argparse import ArgumentParser\r\n\r\nimport numpy as np\r\nimport numpy.testing\r\n\r\nimport onnxruntime as runtime\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.client import timeline\r\n\r\ndef main(args):\r\n\r\n    x = np.random.randn(args.chunksize, args.batchsize, 1).astype(np.float32)\r\n\r\n    # == ONNX Runtime ==\r\n\r\n    options = runtime.SessionOptions()\r\n    session = runtime.InferenceSession(args.model + \".onnx\", options)\r\n\r\n    input_name = session.get_inputs()[0].name\r\n    output_name = session.get_outputs()[0].name\r\n\r\n    t0 = time.time()\r\n    onnxrt_out = session.run([output_name], {input_name: x})\r\n    onnxrt_out = np.array(onnxrt_out[0])\r\n    t1 = time.time()\r\n    print(\"> onnx runtime took %.5f\" % (t1 - t0))\r\n \r\n    # == TensorFlow ==\r\n\r\n    with tf.compat.v1.Session() as session:\r\n        with tf.io.gfile.GFile(args.model + \".pb\", \"rb\") as graph:\r\n            g = tf.compat.v1.GraphDef()\r\n            g.ParseFromString(graph.read())\r\n            tf.import_graph_def(g)\r\n\r\n        graph = tf.get_default_graph()\r\n        first = graph.get_tensor_by_name(\"import/input:0\")\r\n        last = graph.get_operations()[-1].outputs[0]\r\n\r\n        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n        run_metadata = tf.RunMetadata()\r\n\r\n        t0 = time.time()\r\n        tensorflow_out = session.run(\r\n            last,\r\n            feed_dict={first: x},\r\n            options=run_options,\r\n            run_metadata=run_metadata\r\n        )\r\n        t1 = time.time()\r\n\r\n        tl = timeline.Timeline(run_metadata.step_stats)\r\n        ctf = tl.generate_chrome_trace_format()\r\n        with open('trace_file.json', 'w') as f:\r\n            f.write(ctf)\r\n\r\n    print(\"> tensorflow took %.5f\" % (t1 - t0))\r\n\r\n    numpy.testing.assert_allclose(\r\n        onnxrt_out,\r\n        tensorflow_out,\r\n        atol=1e-4\r\n    )\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    parser = ArgumentParser()\r\n    parser.add_argument(\"model\")\r\n    parser.add_argument('--batchsize', type=int, default=1000)\r\n    parser.add_argument('--chunksize', type=int, default=1000)\r\n    main(parser.parse_args())\r\n```\r\n\r\n**Model files**\r\n\r\n - ONNX https://www.dropbox.com/s/jv0hk9h62eymax2/model.onnx\r\n - TensorFlow https://www.dropbox.com/s/t2n4ut7kwfjq8j2/model.pb\r\n\r\n**Python, ONNX, Tensorflow version**\r\n\r\n - Python version: 3.5.3\r\n - ONNX version: 1.5.0\r\n - Tensorflow version: 1.14.0\r\n\r\n**Additional context**\r\n\r\n - Trace file https://www.dropbox.com/s/vhaq5ykt6vlu5ij/trace_file.json\r\n\r\n", "comments": ["@iiSeymour  \r\nIs this still an issue.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30571\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30571\">No</a>\n"]}, {"number": 30570, "title": "[INTEL MKL] Fixed quantized matmul unit test.", "body": "The test has been failing due to subtle mismatch in the computation between the kernels and unit test. This PR fixes it.", "comments": []}, {"number": 30569, "title": "Fix #30526: raise `ValueError` when using modular arithmetic with negative integers and `Dimension`", "body": "not show `raise ValueError` but `unsupported operand` error when using modular arithmetic as minus integer in TF 1.13.1, 1.14.0 #30526", "comments": []}, {"number": 30568, "title": "fixed data type inconsistency for tf.keras.datasets.cifar10.load_data", "body": "In current master branch, if user import cifar10 data using `tf.keras.datasets.cifar10.load_data` API,  the data types for `y_train` and `y_test` are inconsistent. This data type inconsistency may pose problem during training.\r\n```\r\nimport tensorflow as tf\r\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\r\nprint(y_train.dtype)  ##uint8\r\nprint(y_test.dtype) ##int64\r\n```\r\nThe root cause of the problem is because `x_train` and `y_train` are predefined with `uint8` dtype whereas `x_test` and `y_test` are created without data type check.\r\n\r\nThis pull request will fix the inconsistency by ensuring test data have the same dtype as train data.", "comments": []}, {"number": 30567, "title": "Deprecated tf.Session removed from examples", "body": "tf.Session -> tf.compat.v1.Session", "comments": []}, {"number": 30566, "title": "Missing Graph Editor Documentation", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/contrib/graph_editor\r\nhttps://www.tensorflow.org/api_guides/python/contrib.graph_editor\r\n\r\n## Description of issue (what needs changing):\r\nThe Graph Editor API Doc (first link above) links to the Graph Editor Guide (second link above) which is a 404 error.\r\n\r\n### Clear description\r\nThe broken link to the Graph Editor Guide needs to either be fixed or removed and replaced with some actual, comprehensive documentation on using the Graph Editor. At the moment, there is no documentation on Graph Editor usage available anywhere.\r\n", "comments": ["Thanks. That guide is gone so removed link in https://github.com/tensorflow/tensorflow/pull/30824", "@lamberta Out of curiosity, do you have any idea what happened to the original guide or where I could find it?", "@haximilian Yep, some of the older /api_guides are archived here in tensorflow/docs (r1.11 branch): https://github.com/tensorflow/docs/tree/r1.11/site/en/api_guides/python\r\nThough, as you've found, there are still some links that need to be removed.\r\nThese guides mostly existed pre-module\u2014and now modules have their own overview pages in the API docs.\r\n"]}, {"number": 30565, "title": "LogMatrix Gradients", "body": "Hi, \r\nI am trying to optimize a function that requires to compute tf.linalg.logmatrix but this operation doesn't seem to have a gradient as this error came up : LookupError: gradient registry has no entry for: MatrixLogarithm\r\n\r\nBest regards,\r\n\r\n", "comments": ["@paul-bd Please provide us the Tenosrflow version and also code snippet to reproduce the reported issue. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 30564, "title": "Output of sysconfig.get_link_flags does not seem to be suitable for Mac", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac High Sierra 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0\r\n- Python version: Python 3.6.6 (v3.6.6:4cf1f54eb7, Jun 26 2018, 19:50:54)\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): Apple LLVM version 9.0.0 (clang-900.0.39.2)\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nWhen running the example of creating a custom op from https://www.tensorflow.org/guide/extend/op on a Mac the compilation stage (see (https://www.tensorflow.org/guide/extend/op#build_the_op_library) fails with the following error message:\r\n\r\n```\r\nld: library not found for -l:libtensorflow_framework.1.dylib\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n```\r\n\r\nThis appears to be because the specified linker option `-l:libtensorflow_framework.1.dylib` (returned by `sysconfig.get_link_flags`) is not valid for `ld` on the Mac. The man page for `ld` says the following:\r\n\r\n```\r\n-lx         This option tells the linker to search for libx.dylib or libx.a in the library \r\nsearch path.  If string x is of the form y.o, then that file is searched for in the \r\nsame places, but without prepending `lib' or appending `.a' or `.dylib' to the \r\nfilename.\r\n```\r\n\r\nI believe that the correct format for this flag is `-ltensorflow_framework.1` (since the linker will prepend `lib` and append `.dylib`).\r\n\r\nA workaround for this is to replace this line\r\n\r\n```\r\nTF_LFLAGS=( $(python -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_link_flags()))') )\r\n```\r\n\r\nWith this:\r\n\r\n```\r\nTF_LFLAGS=( $(python -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_link_flags()).replace(\"-l:libtensorflow_framework.1.dylib\", \"-ltensorflow_framework.1\"))') )\r\n```\r\n\r\nAlternatively there is (I believe) a fix for the underlying cause here: https://github.com/pio-neil/tensorflow/pull/1/files\r\n\r\n**Describe the expected behavior**\r\n\r\nThe linking process should be performed successfully and produce a library file called zero_out.so.\r\n\r\n**Code to reproduce the issue**\r\n\r\nOn a Mac, create a file called zero_out.cc with the following code (copied from https://www.tensorflow.org/guide/extend/op):\r\n\r\n```\r\n#include \"tensorflow/core/framework/op.h\"\r\n#include \"tensorflow/core/framework/shape_inference.h\"\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n\r\n\r\nusing namespace tensorflow;\r\n\r\nREGISTER_OP(\"ZeroOut\")\r\n    .Input(\"to_zero: int32\")\r\n    .Output(\"zeroed: int32\")\r\n    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\r\n      c->set_output(0, c->input(0));\r\n      return Status::OK();\r\n    });\r\n\r\nclass ZeroOutOp : public OpKernel {\r\n public:\r\n  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}\r\n\r\n  void Compute(OpKernelContext* context) override {\r\n    // Grab the input tensor\r\n    const Tensor& input_tensor = context->input(0);\r\n    auto input = input_tensor.flat<int32>();\r\n\r\n    // Create an output tensor\r\n    Tensor* output_tensor = NULL;\r\n    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),\r\n                                                     &output_tensor));\r\n    auto output_flat = output_tensor->flat<int32>();\r\n\r\n    // Set all but the first element of the output tensor to 0.\r\n    const int N = input.size();\r\n    for (int i = 1; i < N; i++) {\r\n      output_flat(i) = 0;\r\n    }\r\n\r\n    // Preserve the first input value if possible.\r\n    if (N > 0) output_flat(0) = input(0);\r\n  }\r\n};\r\n\r\nREGISTER_KERNEL_BUILDER(Name(\"ZeroOut\").Device(DEVICE_CPU), ZeroOutOp);\r\n```\r\n\r\nThen run:\r\n\r\n```\r\nTF_CFLAGS=( $(python -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_compile_flags()))') )\r\nTF_LFLAGS=( $(python -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_link_flags()))') )\r\ng++ -std=c++11 -shared zero_out.cc -o zero_out.so -fPIC ${TF_CFLAGS[@]} ${TF_LFLAGS[@]} -O2\r\n```\r\n\r\nYou should see an error like this:\r\n\r\n```\r\nld: library not found for -l:libtensorflow_framework.1.dylib\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n```", "comments": ["I have seen the same issue. Suggested fix works for me", "The issue has been fixed through PR #30656", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30564\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30564\">No</a>\n"]}, {"number": 30563, "title": "Pretty print error message for mod op of Dimesion", "body": "This fix tries to address the issue raised in #30526 where the erorr message of mod with Dimension was not clear:\r\n```\r\nTraceback (most recent call last):\r\n  File \"v.py\", line 12, in <module>\r\n    print(z.shape[0] % (-3))\r\nTypeError: unsupported operand type(s) for %: 'Dimension' and 'int'\r\n```\r\n\r\nThe issue was that when TypeErorr or ValueError was thrown, they were rethrown as NotImplemented.\r\n\r\nThis fix expose the error message so that the error is clear:\r\n```\r\nTraceback (most recent call last):\r\n  File \"v.py\", line 12, in <module>\r\n    print(z.shape[0] % (-3))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/framework/tensor_shape.py\", line 579, in __mod__\r\n    other = as_dimension(other)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/framework/tensor_shape.py\", line 720, in as_dimension\r\n    return Dimension(value)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/framework/tensor_shape.py\", line 197, in __init__\r\n    raise ValueError(\"Dimension %d must be >= 0\" % self._value)\r\nValueError: Dimension -3 must be >= 0\r\n```\r\n\r\nThis fix fixes #30526.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Close this PR as it is covered by #30569. /cc @graykode FYI"]}, {"number": 30562, "title": "Fix build breaks in tf-nightly-2.0-preview", "body": "This fix tries to address the issue raised in #30560 where\r\ntf-nightly-2.0-preview throws error:\r\n```\r\nubuntu@ubuntu:/# python -c \"import tensorflow as tf\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 98, in <module>\r\n    from tensorflow_core import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/__init__.py\", line 363, in <module>\r\n    _API_MODULE = sys.modules[__name__].bitwise  # pylint: disable=undefined-variable\r\nNameError: name 'sys' is not defined\r\nubuntu@ubuntu:/#\r\n```\r\n\r\nThe issue was cause by the typo sys -> _sys in `tensorflow/api_template.__init__.py`\r\n\r\nThis fix fixes the issue.\r\n\r\nThis fix fixes #30560.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @mihaimaruseac for the review. Looks like  the issue has been fixed in  7c7c449. I am going to close this PR. Thanks for the help."]}, {"number": 30561, "title": "Eager execution drastically increases `tf.keras.Model.fit` runtimes", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Linux Mint 19.1\r\n- TensorFlow installed from: source\r\n- TensorFlow version: v2.0.0-beta1-0-g8e423e3d56\r\n- Python version: 3.6.8\r\n\r\n\r\n- Bazel version: 0.26.0\r\n- GCC version: 7.4.0 \r\n- CUDA/cuDNN version: 10.0 / 7.5\r\n- GPU model and memory: Nvidia Quadro P1000 - 4 GB GDDR5\r\n\r\n**Describe the current behavior**\r\n\r\nMy issue regards a performance degradation induced by enabling Eager execution, in a context when no Eager tensor should be created, apart from the model's weights (to which I do not need access). As of now, my installation (compiled from source based on yesterday's status of the r2.0 branch) does not have TF 2.0 behaviours enabled by default, thus I compared the run-times for a supervised learning task depending on whether I `enable_v2_behavior` or simply `enable_resource_variables`. It turns out the former yields a significant decrease in performance.\r\n\r\nOn my initial example (which I do not include as it relies on custom data and model layers), the run time went from two minutes to five. In Eager mode, the first epoch was really slow, while the following ones proved increasingly fast, stabilizing at 11 / 12 seconds (19 ms/step). By contrast, without eager, all epochs ran at the same speed, for 8 / 9 seconds (15 ms/step).\r\n\r\nI reproduced this issue on an example that uses mock data (i.e. properly-shaped randomly-generated values, since learning performance is not at stake here) and uses exclusively layers taken from `tensorflow.keras` (no custom bits), whose code I present below. As it happens, not enabling Eager execution results in a 13m49s runtime, while enabling it increases it to 19m17s, i.e. an increase of about 40 percent.\r\n\r\nThe task at hand consists in fitting a binary classifier that takes variable-length sequences of vectors as input. For this, I set up a `tensorflow.data.Dataset` instance (which, for simplification, contains random data in the provided code) based on pre-generated numpy arrays of data, and use the `padded_batches` method to format my inputs as desired. In the mock example, the model consists of a layer of 100 LSTM cells with inputs masking (due to length variability) and default tanh activation topped with a single feed-forward unit with sigmoid activation.\r\n\r\n**Describe the expected behavior**\r\n\r\nI can understand that Eager execution would create a slight overhead, however here it proves huge, while no mechanism whatsoever requires it. I do not know what can be done about it _per se_, but if such an overhead is to be expected, I think it would be nice to be able to disable Eager in 2.0. And I mean _properly_ disabling it, not using `tf.compat` instructions that are bound to be wiped out at some point.\r\n\r\nAlternatively, I believe that it could be great to enable using \"old-style\" (not Eager) tensors as layer weights through, _e.g._, a boolean option, so that in the settings when accessing those weights (as Eager tensors) is not required (which, I believe, is a majority of cases, especially when some keras methods allow to effectively pull out the weights as numpy arrays), no overhead would be implied by a (seemingly) useless Eager declaration.\r\n\r\nOf course, I might be missing an existing feature allowing to do so, in which case I would be most glad to be pointed to a way to optimize run times when Eager execution is enabled (maybe by enforcing the isolation of the operations in a compiled graph?).\r\n\r\n**Code to reproduce the issue**\r\n```\r\n# coding: utf-8\r\n\r\n\"\"\"Test script to measure runtime performances on mock data.\r\n\r\nSet up the EAGER constant on line 18 to toggle the use of\r\nEager execution (provided it is not enabled by default,\r\notherwise change the instructions on lines 31 and 33).\r\nThen, call `python3 <this_script.py>` to run it.\r\n\"\"\"\r\n\r\nimport os\r\nimport time\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nEAGER = False\r\n\r\n\r\ndef main(eager):\r\n    \"\"\"Set up and fit a model on mock data, with or without Eager execution.\r\n\r\n    The model consists of a layer of 100 LSTM cells topped with\r\n    a single dense unit with sigmoid activation. Its fitting is\r\n    bound not to yield actual accuracy improvements, as the data\r\n    used is purely random, but aims at measuring performances as\r\n    to runtime.\r\n    \"\"\"\r\n    if eager:\r\n        tf.enable_v2_behavior()\r\n    else:\r\n        tf.enable_resource_variables()\r\n    # Set up the classifier model using custom embedding units.\r\n    inputs = tf.keras.Input((None, 100), dtype=tf.float32)\r\n    lengths = tf.keras.Input((), dtype=tf.int64)\r\n    mask = tf.sequence_mask(lengths)\r\n    output = tf.keras.layers.LSTM(100)(inputs, mask=mask)\r\n    output = tf.keras.layers.Dense(1, activation='sigmoid')(output)\r\n    model = tf.keras.Model([inputs, lengths], output)\r\n    # Set up the training and validation data.\r\n    dataset = setup_mock_dataset()\r\n    train, valid = dataset.take(500), dataset.skip(500)\r\n    # Fit the model.\r\n    model.compile('adam', 'binary_crossentropy', ['accuracy'])\r\n    history = model.fit(\r\n        train.repeat(), steps_per_epoch=500, epochs=10,\r\n        validation_data=valid.repeat(), validation_steps=100\r\n    )\r\n\r\n\r\ndef setup_mock_dataset(n_batches=600, batch_size=32):\r\n    \"\"\"Return a tf.data.Dataset yielding batches of random data.\r\n\r\n    The input data consists of a couple of tensors, one with\r\n    zero-padded sequences of vectors of size 100, the other\r\n    with the true sequence lengths. The target data consists\r\n    of sequence-wise binary values.\r\n    \"\"\"\r\n    # Generate some random (mock) input and target data.\r\n    n_samples = n_batches * batch_size\r\n    lengths = 1 + np.random.choice(100, size=n_samples, replace=True)\r\n    inputs = np.random.normal(size=(lengths.sum(), 100))\r\n    targets = np.random.choice(2, size=(n_samples, 1), replace=True)\r\n    # Set up a generator yielding shuffled training samples.\r\n    def generator():\r\n        \"\"\"Yield individual training samples.\"\"\"\r\n        nonlocal inputs, targets, lengths, n_samples\r\n        start = 0\r\n        for i in range(n_samples):\r\n            end = start + lengths[i]\r\n            yield (inputs[start:end], lengths[i]), targets[i]\r\n            start = end\r\n    # Set up a tensorflow Dataset based on the previous.\r\n    output_shapes = (\r\n        (tf.TensorShape((None, 100)), tf.TensorShape(())), tf.TensorShape(1)\r\n    )\r\n    dataset = tf.data.Dataset.from_generator(\r\n        generator, ((tf.float32, tf.int64), tf.int64), output_shapes\r\n    )\r\n    # Have the dataset output data batches, and return it.\r\n    return dataset.padded_batch(batch_size, output_shapes)\r\n\r\n\r\nif __name__ == '__main__':\r\n    start = time.clock()\r\n    main(EAGER)\r\n    duration = time.clock() - start\r\n    min, sec = duration // 60, duration % 60\r\n    print('TOTAL DURATION: %im%i' % (min, sec))\r\n\r\n```\r\n\r\n**Additional logs**\r\n\r\n\r\nHere are the script's print outs:\r\n\r\n* With Eager execution enabled:\r\n\r\n```\r\nEpoch 1/10\r\n2019-07-10 15:58:59.009116: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n500/500 [==============================] - 83s 165ms/step - loss: 0.6983 - accuracy: 0.5009 - val_loss: 0.6990 - val_accuracy: 0.5034\r\nEpoch 2/10\r\n500/500 [==============================] - 72s 144ms/step - loss: 0.6712 - accuracy: 0.5904 - val_loss: 0.7125 - val_accuracy: 0.4931\r\nEpoch 3/10\r\n500/500 [==============================] - 72s 143ms/step - loss: 0.6056 - accuracy: 0.6841 - val_loss: 0.7604 - val_accuracy: 0.4947\r\nEpoch 4/10\r\n500/500 [==============================] - 72s 144ms/step - loss: 0.4445 - accuracy: 0.8073 - val_loss: 0.9025 - val_accuracy: 0.5031\r\nEpoch 5/10\r\n500/500 [==============================] - 72s 145ms/step - loss: 0.2502 - accuracy: 0.9169 - val_loss: 1.1817 - val_accuracy: 0.5069\r\nEpoch 6/10\r\n500/500 [==============================] - 72s 144ms/step - loss: 0.1250 - accuracy: 0.9699 - val_loss: 1.4860 - val_accuracy: 0.5091\r\nEpoch 7/10\r\n500/500 [==============================] - 72s 145ms/step - loss: 0.0724 - accuracy: 0.9859 - val_loss: 1.6498 - val_accuracy: 0.5047\r\nEpoch 8/10\r\n500/500 [==============================] - 72s 143ms/step - loss: 0.0537 - accuracy: 0.9891 - val_loss: 1.7936 - val_accuracy: 0.5038\r\nEpoch 9/10\r\n500/500 [==============================] - 72s 143ms/step - loss: 0.0351 - accuracy: 0.9948 - val_loss: 1.8995 - val_accuracy: 0.5066\r\nEpoch 10/10\r\n500/500 [==============================] - 72s 144ms/step - loss: 0.0297 - accuracy: 0.9949 - val_loss: 2.0393 - val_accuracy: 0.5041\r\nTOTAL DURATION: 19m17\r\n```\r\n\r\n* Without enabling Eager execution:\r\n\r\n```\r\nEpoch 1/10\r\n500/500 [==============================] - 46s 91ms/step - loss: 0.6989 - acc: 0.5006 - val_loss: 0.6999 - val_acc: 0.4894\r\nEpoch 2/10\r\n500/500 [==============================] - 45s 91ms/step - loss: 0.6708 - acc: 0.5932 - val_loss: 0.7138 - val_acc: 0.4875\r\nEpoch 3/10\r\n500/500 [==============================] - 45s 90ms/step - loss: 0.6048 - acc: 0.6849 - val_loss: 0.7705 - val_acc: 0.4975\r\nEpoch 4/10\r\n500/500 [==============================] - 45s 90ms/step - loss: 0.4453 - acc: 0.8104 - val_loss: 0.9408 - val_acc: 0.4950\r\nEpoch 5/10\r\n500/500 [==============================] - 46s 92ms/step - loss: 0.2518 - acc: 0.9169 - val_loss: 1.2647 - val_acc: 0.5009\r\nEpoch 6/10\r\n500/500 [==============================] - 46s 91ms/step - loss: 0.1206 - acc: 0.9726 - val_loss: 1.5996 - val_acc: 0.4994\r\nEpoch 7/10\r\n500/500 [==============================] - 45s 91ms/step - loss: 0.0766 - acc: 0.9839 - val_loss: 1.8580 - val_acc: 0.4975\r\nEpoch 8/10\r\n500/500 [==============================] - 46s 91ms/step - loss: 0.0569 - acc: 0.9884 - val_loss: 1.9623 - val_acc: 0.4975\r\nEpoch 9/10\r\n500/500 [==============================] - 46s 92ms/step - loss: 0.0476 - acc: 0.9897 - val_loss: 2.0733 - val_acc: 0.4966\r\nEpoch 10/10\r\n500/500 [==============================] - 46s 91ms/step - loss: 0.0316 - acc: 0.9946 - val_loss: 2.2490 - val_acc: 0.5044\r\nTOTAL DURATION: 13m49\r\n```", "comments": ["I ran some additional tests using a distinct TF installation (on the same system), namely version 2.0b1 installed from binary using pip. To do so, I marginally adjusted the script posted above, with the first few lines of `main` changed to:\r\n```\r\nif not eager:\r\n    tf.compat.v1.disable_eager_execution()\r\n```\r\n\r\nWhen I disable eager, I have, again, stable run-times at each epoch (which are, for some reason, much faster than those reported yesterday: 51 ms/step, 25s/epoch, total duration of 9m42s). When I enable eager... well, `constant folding failed: Invalid argument: Unsupported type: 21` warnings show up repeatedly during the first epoch (see issues #29525 #30263 #30533 etc.), and then all epochs are slow (406 ms/step at first epoch, around 380 ms/step at each of the following ones, so around 190s/epoch for a total duration of 33m39s). With the grappler issue, it seems that Eager is not the only mechanism at fault in this specific case; but:\r\n\r\nI also ran my \"true\" script (with custom data and network layers subclassing `tf.keras.layers.Layers` - which I unfortunately cannot share), and I can see that it also runs faster with this TF installation. When enabling Eager, epochs 2-10 have similar runtimes as when disabling it (which is nice), but the first epoch is way slower (180s against 10s).\r\n\r\nI guess this is because in Eager mode, a separate graph is create for each and every training batch, creating a huge overhead at the first epoch, while the use of those same batches in the following epochs allows the re-use of these graphs. This is, however, senseless since all batches have the same TensorSpec, and a single graph should be able to cover them all (as it does when Eager execution is disabled). Is there any way how I can force a similar behaviour when Eager is enabled? I know this is when `tf.function` is supposed to be useful, but I cannot enforce it within built-in keras layers, can I?", "I am able to reproduce the issue on Colab with tensorflow 2.0.0.beta1. Thanks!", "@robieta: Is this related to some of the recent performance investigations you're working on?", "Possibly. The fact that the first epoch is slower in eager is very surprising.", "The constant folding error has seemingly been fixed as of today's nightly build (see issue #29525), I was therefore able to re-run the mock example with TF 2.0b1 nightly (from july 12th). I decreased the number of epochs to 5 to gain time, as it appears that all following epochs run at the same speed as the fifth one.\r\n\r\nHere are the logs:\r\n* with Eager disabled:\r\n```\r\nEpoch 1/5\r\n2019-07-12 17:04:59.837656: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n500/500 [==============================] - 26s 52ms/step - loss: 0.6995 - accuracy: 0.4968 - val_loss: 0.6974 - val_accuracy: 0.4947\r\nEpoch 2/5\r\n500/500 [==============================] - 33s 66ms/step - loss: 0.6701 - accuracy: 0.6006 - val_loss: 0.7088 - val_accuracy: 0.4863\r\nEpoch 3/5\r\n500/500 [==============================] - 25s 49ms/step - loss: 0.6020 - accuracy: 0.6903 - val_loss: 0.7582 - val_accuracy: 0.4772\r\nEpoch 4/5\r\n500/500 [==============================] - 25s 50ms/step - loss: 0.4443 - accuracy: 0.8114 - val_loss: 0.8911 - val_accuracy: 0.4919\r\nEpoch 5/5\r\n500/500 [==============================] - 25s 50ms/step - loss: 0.2510 - accuracy: 0.9200 - val_loss: 1.1380 - val_accuracy: 0.5019\r\nTOTAL DURATION: 5m14\r\n```\r\n\r\n* with Eager enabled:\r\n```\r\nEpoch 1/5\r\nW0712 17:07:55.431549 140457053927232 deprecation.py:323] From /home/pandrey/Documents/tfnightly/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:460: BaseResourceVariable.constraint (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nApply a constraint manually following the optimizer update step.\r\n2019-07-12 17:07:56.198978: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n500/500 [==============================] - 72s 145ms/step - loss: 0.6988 - accuracy: 0.4942 - val_loss: 0.6949 - val_accuracy: 0.5069\r\nEpoch 2/5\r\n500/500 [==============================] - 58s 117ms/step - loss: 0.6697 - accuracy: 0.5954 - val_loss: 0.7051 - val_accuracy: 0.4941\r\nEpoch 3/5\r\n500/500 [==============================] - 59s 117ms/step - loss: 0.5952 - accuracy: 0.7014 - val_loss: 0.7517 - val_accuracy: 0.4959\r\nEpoch 4/5\r\n500/500 [==============================] - 59s 117ms/step - loss: 0.4256 - accuracy: 0.8236 - val_loss: 0.8976 - val_accuracy: 0.5072\r\nEpoch 5/5\r\n500/500 [==============================] - 58s 117ms/step - loss: 0.2332 - accuracy: 0.9242 - val_loss: 1.1507 - val_accuracy: 0.5016\r\nTOTAL DURATION: 9m56\r\n```\r\n\r\nAs you can see, the epochs' runtimes are still about twice as high with Eager enabled than with Eager disabled... I also have a deprecation warning regarding resource variables whose relatedness to the issue I cannot state.", "Just to give a quick update on this. (I know you've been waiting very patiently.) There are a bunch of tiny host to device transfers in the v2 path that seem to be blocking the main computation. I haven't been able to diagnose the underlying cause yet.\r\n\r\ncc @qlzh727 who is our resident RNN expert.", "Thank you for the update (and your work in general; Github issues are bound to be a complain space, but there are so many great things about tensorflow that it would be a shame not to mention it). I will keep waiting :-)", "Can you try with the latest nightly? This should be fixed by: https://github.com/tensorflow/tensorflow/commit/640b5f2513bc3c6537bd635cd9dcca4148cb5b26 (and an analogous fix for custom layers / autograph https://github.com/tensorflow/tensorflow/commit/f03540a63d0e03a210b03b8689e60d384d91833a was also added)\r\n\r\nTo briefly summarize what was going on, we were specifying the number of RNN steps twice: once in the while loop cond, and once in the max steps. while_loop reconciles those two by placing a logical_and op but in practice it would only ever see and(True, True) or and(False, False). So the GPU was blocking waiting for all of the info needed for the predicate to arrive and resulted in stalled training. We are working in parallel to make the v2 runtime interact better with control flow in general (Ideally it would handle the above optimally regardless), but the two changes above should do a reasonable job for both builtin and custom rnns until the broader runtime changes land.", "Hi,\r\n\r\nI installed the current nightly 2.0 build with GPU enabled using pip:\r\n```\r\n>>> tf.version.VERSION\r\n'2.0.0-dev20190731'\r\n>>> tf.version.GIT_VERSION\r\n'v1.12.1-7529-g3e0ad8a004'\r\n```\r\n\r\nAnd ran a series of test using the same scripts as before.\r\n\r\n**Mock script**\r\n\r\nThis is the script I shared, with mock data being processed by a LSTM.\r\n\r\nWith Eager enabled, I can indeed see a great performance improvement compared to previous tests: with GPU enabled, the first epoch runs in 23 seconds and each of the following ones in 8 seconds each, while with CPU only the first epoch takes 30 seconds to complete and the following ones 21 seconds each.\r\n\r\nI however have a warning message showing up repeatedly during the first epoch:\r\n`W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_618_2088_specialized_for_training_Adam_gradients_gradients_lstm_StatefulPartitionedCall_grad_StatefulPartitionedCall_at___inference_keras_scratch_graph_3555' and '__inference___backward_standard_lstm_2424_3034' both implement 'lstm_c1c4be15-b33a-462d-a312-bd8c7c49da68' but their signatures do not match.`\r\n\r\nWith Eager disabled, the script does not run, and I get the following error stack:\r\n```\r\nW0807 09:53:49.171155 140492685502272 deprecation.py:323] From /home/pandrey/Documents/tfnightly/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:468: BaseResourceVariable.constraint (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nApply a constraint manually following the optimizer update step.\r\n2019-08-07 09:53:49.183543: W tensorflow/c/c_api.cc:326] Operation '{name:'lstm/StatefulPartitionedCall' id:76 op device:{} def:{{{node lstm/StatefulPartitionedCall}} = StatefulPartitionedCall[Tin=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_RESOURCE, DT_RESOURCE, DT_RESOURCE, DT_BOOL], Tout=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_VARIANT, DT_VARIANT, DT_VARIANT, DT_FLOAT, DT_INT32], _gradient_op_type=\"PartitionedCall-252\", config=\"\", config_proto=\"\\n\\007\\n\\003GPU\\020\\001\\n\\007\\n\\003CPU\\020\\0012\\005*\\0010J\\0008\\001\", executor_type=\"\", f=__forward_standard_lstm_2916[]](input_1, lstm/zeros, lstm/zeros_1, lstm/kernel, lstm/recurrent_kernel, lstm/bias, SequenceMask/Less)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\r\nTraceback (most recent call last):\r\n  File \"/home/pandrey/Documents/tfnightly/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1356, in _do_call\r\n    return fn(*args)\r\n  File \"/home/pandrey/Documents/tfnightly/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1339, in _run_fn\r\n    self._extend_graph()\r\n  File \"/home/pandrey/Documents/tfnightly/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1379, in _extend_graph\r\n    tf_session.ExtendSession(self._session)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Node 'training/Adam/gradients/gradients/lstm/StatefulPartitionedCall_grad/StatefulPartitionedCall': Connecting to invalid output 5 of source node lstm/StatefulPartitionedCall which has 5 outputs\r\n```\r\n\r\n**Conclusion**: the performance with Eager is far better than before, but it would seem there still are issues with the LSTM implementation (which might be due to the interaction with Adam? I will run some more tests with another optimizer after posting this first wave of test reports).\r\n\r\n\r\n**Actual script and data**\r\n\r\nThis is the script I cannot share, with custom layers on a task whose specification is similar to the mock one. I first ran tets using an actual dataset (interfaced with a `tf.data.Dataset`), as I did in every previous performance report regarding that script.\r\n\r\nWith Eager enabled, the first epoch is still slow (102 ~ 104 seconds), while the following ones run at either 10 or 13 seconds per epoch depending on GPU availability. Compared to previous tests, the first epoch is nearly 80 seconds faster, while the rest runs at similar speed. With Eager disabled, all epochs run at either 9 or 13 seconds depending on GPU availability.\r\n\r\n**Conclusion**: the performance is similar to previous tests, save for a reduced but still important runtime overhead during the first epoch when Eager execution is enabled.\r\n\r\n\r\n**Actual script with mock data**\r\n\r\nAt this point, I came to wonder whether this was related to the actual network computations or to the dataset mechanisms. In order to partly disentangle those two aspects, I ran a third round of tests, using my actual custom model, but mock data (with the exact same function as in the shared mock script).\r\n\r\nIn this setting, all epochs run in 5~6 seconds (whether Eager is enabled or not, and with no significant change when GPU is available), save, again, for the first one with Eager enabled, which takes 11 seconds to complete.\r\n\r\n**Conclusion**: most of the overhead runtime during the first fitting epoch with Eager execution enabled seems to come from the handling of the Dataset. When using a mock-data in-memory Dataset, the overhead is greatly reduced although still existent.", "I tried running the mock script with various optimizers, it does not change a thing to the lstm-related warning and errors showing up (save for a reported faulty node names, obviously).", "The behavior that you're seeing is a mix of bugs and expected behavior. But in any case I think an explanation is in order. \r\n\r\nOne very minor point that I noticed is that `mask = tf.sequence_mask(lengths)` should actually be `mask = tf.keras.layers.Lambda(lambda x: tf.sequence_mask(x))(lengths)`. There is some fallback logic to auto wrap it in a layer, but it only works for builtin functions and can be less than ideal for complex transformations (since it makes a layer per op) so it's good to get in the habit of always using Layers. Interestingly, this revealed a different issue in how keras was handling connectivity which has since been fixed. Ok, on to the RNNs!\r\n\r\nIt appears that somewhere along the line you switched from the v1 to v2 API in tf. (I'm guessing moving from your home built version to the 2.0 nightly) For most layers it's a negligible difference, but LSTM and GRU are special. In tf 1.x, if you want CuDNN you have to use the `tf.keras.layers.CuDNNLSTM / CuDNNGRU` classes, while `tf.keras.layers.LSTM / GRU` gets you an rnn using raw tf ops. (This is the context of the initially reported slowdown, where the Non-CuDNN version was slow in v2.) On the other hand, the LSTM in v2 will look at the particular structure of your LSTM and automatically use CuDNN if possible. (The CuDNN LSTM is only applicable to a subset of LSTMs.) \r\n\r\nThe automatic switching only works in v2 due to some deep quirks with the v1 runtime, and we hadn't considered the case that someone would trigger it in v1 since it was only part of the v2 API. But of course comparing v1 and v2 is a very natural thing to try. The fix is that (except for Estimator which I won't go into) the v2 LSTM will not attempt the CuDNN swap if it detects that it is being run in a v1 environment.\r\n\r\nhttps://colab.sandbox.google.com/gist/robieta/7a00e418036fdc02821f29b96e3a5871/lstm_demo.ipynb\r\n\r\nThis means that the layer won't crash, but v2 will seem much faster than v1 simply because only v2 is using CuDNN. (you can always use `tf.compat.v1.keras.layers.CuDNNLSTM` for the more direct comparison, of course.)", "> One very minor point that I noticed is that mask = tf.sequence_mask(lengths) should actually be mask = tf.keras.layers.Lambda(lambda x: tf.sequence_mask(x))(lengths). There is some fallback logic to auto wrap it in a layer, but it only works for builtin functions and can be less than ideal for complex transformations (since it makes a layer per op) so it's good to get in the habit of always using Layers.\r\n\r\nThank you for the advice! I will make sure to be more careful about that in the future. Using lambdas has always felt counter-intuitive as it is rather unpythonic, but with your explanation I know understand why lambda layers should be used to wrap one-liners that do not require actual sub-classing :)\r\n\r\n> It appears that somewhere along the line you switched from the v1 to v2 API in tf. (I'm guessing moving from your home built version to the 2.0 nightly)\r\n\r\nActually, not in the context of this issue. The home built version I used to run the initial tests was based on the 2.0 github branch, which weirdly did not have Eager enabled by default (but did have the 2.0 API, including the version submodule stating it was a 2.0 installation). At any rate, each time I reported a Eager / non-Eager comparison, it was using one installation and using an Eager [en|dis]sabling command (`tf.compat.v1.disable_eager_execution()` in all recent posts).\r\n\r\n> In tf 1.x, if you want CuDNN you have to use the tf.keras.layers.CuDNNLSTM / CuDNNGRU classes, while tf.keras.layers.LSTM / GRU gets you an rnn using raw tf ops. (This is the context of the initially reported slowdown, where the Non-CuDNN version was slow in v2.) On the other hand, the LSTM in v2 will look at the particular structure of your LSTM and automatically use CuDNN if possible. (The CuDNN LSTM is only applicable to a subset of LSTMs.)\r\n\r\nBased on the previous point, I do not quite understand how this would apply in my case, but perhaps your point is that my home built may have been using that v1 behavior, explaining the apparent improvement related to using the nightly build? In any case, that is an interesting thing to know about :)\r\n\r\n> But of course comparing v1 and v2 is a very natural thing to try.\r\n\r\nI get the point. That being said, my issue is, at the moment, with the disabling of Eager execution in v2 improving run-times (and, in other contexts, memory usage, but this is a separate issue). I can see that, possibly due to a faulty versioning of my initial installation, the difference is not as drastic as initially reported, but I still encounter significant overheads, which partly seem to be related to the handling of Dataset objects.\r\n\r\nThis is just to, perhaps, clarify a bit where the issue appears to stand now - thank you for doing the same and providing me with a better understanding of some of the mechanisms at stake!", "Now that version 2.0 rc0 is out (congrats!), I re-ran the tests on the shared mock script (with the `tf.sequence_mask` line now wrapped with a Lambda layer) using it, and am overall very pleased with the results, although there might still be a few things to look at.\r\n\r\n**Mock script and data**\r\n\r\nWith **Eager execution enabled**, the training is faster than ever, with a first epoch running in 11 seconds and subsequent ones in 7 seconds. Is the slight initial over-time due to the model building mechanics? In other words, is it something to expect in general? At any rate, it is way less significant than in previous beta versions, which is really neat.\r\n\r\nI still get, however, the previously-reported grappler warning about signatures from LSTM backend functions not matching: \r\n```\r\nW tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_7463_8060' and '__inference___backward_cudnn_lstm_with_fallback_5855_7312_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_8216' both implement 'lstm_919b145d-75ed-47c1-a8cd-5f2737a5edbe' but their signatures do not match.\r\n```\r\n\r\nWith **Eager disabled**, the code now runs again (which it did not when I tested a previous nightly build), although there is an initial warning about an invalid lstm nodes modification through a while loop (see below). The times are about 25 seconds per epoch, as before - I am thus happy to see that execution with Eager enabled has not only closed the gap with non-Eager execution, but actually surpassed it as far as this example model is concerned, which I guess relies on the work done on LSTM layers. Many thanks and congratulations for that!\r\n\r\n```\r\nW tensorflow/c/c_api.cc:326] Operation '{name:'lstm/while' id:262 op device:{} def:{{{node lstm/while}} = While[T=[DT_INT32, DT_INT32, DT_INT32, DT_VARIANT, DT_FLOAT, ..., DT_VARIANT, DT_VARIANT, DT_VARIANT, DT_VARIANT, DT_VARIANT], _lower_using_switch_merge=true, _num_original_outputs=59, body=lstm_while_body_123[], cond=lstm_while_cond_122[], output_shapes=[[], [], [], [], [?,100], ..., [], [], [], [], []], parallel_iterations=32](lstm/while/loop_counter, lstm/while/maximum_iterations, lstm/time, lstm/TensorArrayV2_1, lstm/zeros_like, lstm/zeros, lstm/zeros_1, lstm/strided_slice_1, lstm/TensorArrayUnstack/TensorListFromTensor, lstm/TensorArrayUnstack_1/TensorListFromTensor, lstm/kernel, lstm/recurrent_kernel, lstm/bias, lstm/while/EmptyTensorList, lstm/while/EmptyTensorList_1, lstm/while/EmptyTensorList_2, lstm/while/EmptyTensorList_3, lstm/while/EmptyTensorList_4, lstm/while/EmptyTensorList_5, lstm/while/EmptyTensorList_6, lstm/while/EmptyTensorList_7, lstm/while/EmptyTensorList_8, lstm/while/EmptyTensorList_9, lstm/while/EmptyTensorList_10, lstm/while/EmptyTensorList_11, lstm/while/EmptyTensorList_12, lstm/while/EmptyTensorList_13, lstm/while/EmptyTensorList_14, lstm/while/EmptyTensorList_15, lstm/while/EmptyTensorList_16, lstm/while/EmptyTensorList_17, lstm/while/EmptyTensorList_18, lstm/while/EmptyTensorList_19, lstm/while/EmptyTensorList_20, lstm/while/EmptyTensorList_21, lstm/while/EmptyTensorList_22, lstm/while/EmptyTensorList_23, lstm/while/EmptyTensorList_24, lstm/while/EmptyTensorList_25, lstm/while/EmptyTensorList_26, lstm/while/EmptyTensorList_27, lstm/while/EmptyTensorList_28, lstm/while/EmptyTensorList_29, lstm/while/EmptyTensorList_30, lstm/while/EmptyTensorList_31, lstm/while/EmptyTensorList_32, lstm/while/EmptyTensorList_33, lstm/while/EmptyTensorList_34, lstm/while/EmptyTensorList_35, lstm/while/EmptyTensorList_36, lstm/while/EmptyTensorList_37, lstm/while/EmptyTensorList_38, lstm/while/EmptyTensorList_39, lstm/while/EmptyTensorList_40, lstm/while/EmptyTensorList_41, lstm/while/EmptyTensorList_42, lstm/while/EmptyTensorList_43, lstm/while/EmptyTensorList_44, lstm/while/EmptyTensorList_45)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\r\n```\r\n\r\n**Custom script and data**\r\n\r\nAs for my custom model and data, I ran my un-shared testing script, and now get only 1 seconds of overhead on the first training epoch with Eager enabled - otherwise, all epochs run in 9 seconds, whether Eager is disabled or not. If I use mock data instead of my custom one, the epochs run faster but the slight 2 seconds overhead is similar, which confirms Dataset handling issue have also been fixed in rc0.\r\n\r\n\r\n**Conclusion**\r\n\r\nGiven these latest test results, I am happy to say that the issue has been solved in 2.0rc0. Again, thank you to everyone involved in developing it, and special thanks to @robieta for your feedback on this issue, and work in general.\r\n\r\nI leave it to you to deem whether the couple of warnings I brought up in this post are worth investigating (more probably, you would already know about those), but as far as the core point of performance dropout is concerned, I am happy to close this issue!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30561\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30561\">No</a>\n", "I had the same problem, and seem to have found a good solution:\r\nwhen manually defining the training loop using the gradient type stuff like inidcated in the TensorFlow migration tutorial, training gets even sligthly faster than with disabled eager mode. \r\nJust adapting this code. to your situation:\r\n\r\n@tf.function\r\ndef train_step(inputs, labels):\r\n  with tf.GradientTape() as tape:\r\n    predictions = model(inputs, training=True)\r\n    if len(model.losses) > 0:\r\n        regularization_loss = tf.math.add_n(model.losses)\r\n    else:\r\n        regularization_loss = 0\r\n    pred_loss = loss_fn(labels, predictions)\r\n    total_loss = pred_loss + regularization_loss\r\n\r\n  gradients = tape.gradient(total_loss, model.trainable_variables)\r\n  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n\r\nfor epoch in range(NUM_EPOCHS):\r\n  for inputs, labels in train_data:\r\n    train_step(inputs, labels)\r\n  print(\"Finished epoch\", epoch)\r\n"]}, {"number": 30560, "title": "TF2 Nightly-20190710 broken import", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04/19.10\r\n- TensorFlow installed from (source or binary): Binary - **tf2-nightly-2.0-preview**\r\n- TensorFlow version (use command below): 2.0 preview\r\n- Python version: python3.6 (probably all others)\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\nAble to import TensorFlow\r\n\r\n**Code to reproduce the issue**\r\n```\r\npip install tf-nightly-2.0-preview\r\npython -c \"import tensorflow as tf\"\r\n```\r\n\r\nResults in:\r\n```\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/sean-morgan/miniconda3/envs/tf-broken/lib/python3.6/site-packages/tensorflow/__init__.py\", line 98, in <module>\r\n    from tensorflow_core import *\r\n  File \"/home/sean-morgan/miniconda3/envs/tf-broken/lib/python3.6/site-packages/tensorflow_core/__init__.py\", line 363, in <module>\r\n    _API_MODULE = sys.modules[__name__].bitwise  # pylint: disable=undefined-variable\r\nNameError: name 'sys' is not defined\r\n```", "comments": ["@seanpmorgan I added a PR #30562 for the fix.", "Fixed by 7c7c449f175db71cbcdf544bb4104f84f8cb91d9 closing as this should be fixed in tomorrow's nightly.", "Small correction. The live nightly update is working and the fix is in today's nightly \ud83d\udcaf "]}, {"number": 30559, "title": "Fix numpy warning with numpy 1.17.0+", "body": "This fix tries to address the issue raised in #30427 where\r\n`import tensorflow` caused the following warning with numpy 1.17.0+:\r\n```\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n```\r\n\r\nThe issue was cause by changes in numpy: https://github.com/numpy/numpy/commit/ad1e0600e45b9fa71096d0a0f10c1474e003f373\r\n\r\nThis fix fixes the warning.\r\n\r\nThis fix fixes #30427.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["what is the solution. Can u explain pls", "what is the solution then? I couldn't understand the procedure. What I understood is that to change the tensorflow version instead of numpy ? But how?", "Actually I downloaded numpy then It worked probably", "when does this land in a release? the warnings are a bit distracting atm.\r\n", "Cherry-picking this in #31333 for 1.14.1 patch release. It will also be included in a future main release shortly.", "@mihaimaruseac Thanks! \ud83d\udc4d "]}, {"number": 30558, "title": "[LITE]Deprecated tf.Session updated with tf.compat.v1.Session ", "body": "", "comments": []}, {"number": 30557, "title": "Padded_batch in TF2.0 throws DataLossError: Attempted to pad to a smaller size than the input element. [Op:IteratorGetNextSync]", "body": "I am following [this tutorial](https://www.tensorflow.org/beta/tutorials/load_data/text#split_the_dataset_into_text_and_train_batches) and exactly [here](https://www.tensorflow.org/beta/tutorials/load_data/text#split_the_dataset_into_text_and_train_batches) in\r\n`train_data = train_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]))`, it uses the length of the longest sentence in the dataset (which is 16). I want to pad the sentences further to length 8. So, after replacing this \r\n> **-1**  with **8**\r\nin the line above, it throws the error \r\n\r\n> DataLossError: Attempted to pad to a smaller size than the input element. [Op:IteratorGetNextSync]\r\non executing `sample_text, sample_labels = next(iter(test_data))\r\n\r\nsample_text, sample_labels or on fit method.\r\nI am not sure if this is a bug or a normal behaviour. I want to achieve sentences with shorter paddings. If anyone can help me, I will be very thankful. \r\nIt can be easily done with `tf.keras.preprocessing.sequence.pad_sequences()`but not sure about tf.Data in TF2.0. \r\n\r\n", "comments": ["Hi,\r\nYou simply need to trim your data before instructing padded-batching.\r\nI would recommend doing something in the vein of:\r\n```\r\ntrain_data = train_data.map(lambda x: x[:8])\r\ntrain_data = train_data.padded_batch(BATCH_SIZE, padded_shapes=([8], []))\r\n```\r\n\r\nI hope this is of help to you! By the way, no offence, but I believe this question would be more suited on a question-answering site like StackOverflow than in a GitHub Issues tracker.", "Hi @pandrey-fr , thank you for your reply. I tried your suggestion but it is giving me an error since lambda function only takes one argument. The `train_data` is a tf.Dataset type consisting **(data, labels)** tuples and I am not sure how to provide **data** from this tuple to lambda function for slicing. So, I am still stuck after trying many things for last 3 hours.\r\n\r\nThe reason I posted this question here was because it is a TF document related problem, since it is not clear if [padded_batch](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#padded_batch) function will raise an error if one tries to pad with smaller value than the lengths of sentences in the data. There are many documents missing in TF, and this was just one function. ", "> lambda function only takes one argument\r\n\r\nActually you can have multiple inputs: e.g. to sum two elements `lambda x, y: x + y`, so you should be able to work something around! Good luck :)\r\n\r\n> There are many documents missing in TF, and this was just one function.\r\n\r\nI strongly agree that the documentation is faulty... This is getting better with time (although the 2.0 release is creating a lot of new issues), and is mainly due to TF being such a big project (I guess it is hard to keep track of everything, even from the inside), but some effort should (in my humble opinion) be made in that direction.", "thank you again for your suggestion. I still could not do it using `lambda` function because it does not accept tensors and `map` function does not accept numpy objects ([source](https://stackoverflow.com/questions/56075037/how-to-convert-tensor-to-numpy-array-in-tensorflow)), so instead of slicing using `lambda` I edited `tf.py_function` ([here](https://www.tensorflow.org/beta/tutorials/load_data/text#encode_examples)) and returned back the values to `map` function.\r\n\r\nSince I am new to TF, I am sure there are other ways of doing this but I could not find them, if you (or anyone) have any inputs then please share them. I will be happy to add them to my knowledge.\r\nThank You :)", "Seems like a sensible solution! And you are welcome :)", "@rishabhsahrawat \r\n let me know if we can close this issue since it looks to be fixed. Thanks!"]}, {"number": 30556, "title": "Which Bazel versions are okay for Tensorflow source install?", "body": "\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/install/source_windows\r\n\r\n\r\n## Description of issue (what needs changing):\r\n\r\nIt is purely a logical issue:\r\nInstall Bazel 0.24.1, the build tool used to compile TensorFlow. Set up Bazel to build C++.\r\n[...] Ensure you install Bazel 0.23.0 or lower.\r\n\r\n0.24.1 > 0.23.0 \r\n", "comments": ["Hi, I completely agree with the documentation issue here. The thing is, it really depends on the version of TensorFlow you are trying to compile. You can find this information in the `configure.py` script once you have cloned the Git repo and checked out to the release branch you want to compile.\r\n\r\nNot knowing that, I personally had to download three distinct versions of Bazel while running through a compilation yesterday (first one was too recent, second one was okay for master but not for r2.0 which I eventually elected, third one did the trick), which, while not a major issue, was annoying.", "That's good to know, I think this should be stated in the documentation then or an appropriate intersection of the versions should be recommended.", "Due to the cadence of Bazel updates, currently there is no Bazel version in the intersection of all supported versions of TF:\r\n\r\nTF 1.13 wants bazel 0.19.0 to 0.21.0\r\nTF 1.14 wants bazel 0.24.1 to 0.25.2\r\nTF 2.0 wants bazel 0.24.1 to 0.26.0\r\nTF master wants bazel 0.24.1 to 0.26.1\r\n\r\nTF 1.12 just passed its end of support term. TF 1.13 will reach end of support at end of August at which point bazel 0.24.1 to 0.25.2 can be used for all of the other branches.", "@karkirowle Did you get a chance to look at the @mihaimaruseac's comment. Thanks!", "> Due to the cadence of Bazel updates, currently there is no Bazel version in the intersection of all supported versions of TF:\r\n> \r\n> TF 1.13 wants bazel 0.19.0 to 0.21.0\r\n> TF 1.14 wants bazel 0.24.1 to 0.25.2\r\n> TF 2.0 wants bazel 0.24.1 to 0.26.0\r\n> TF master wants bazel 0.24.1 to 0.26.1\r\n> \r\n> TF 1.12 just passed its end of support term. TF 1.13 will reach end of support at end of August at which point bazel 0.24.1 to 0.25.2 can be used for all of the other branches.\r\n\r\nHi Mihai, the note about support term is fairly interesting. Where can I find more information on the lifetime/support for different TF release versions? I suppose it's roughly half a year?", "I think we haven't added it on a public facing documentation, as it has something we talked about only recently. It's supposed to be 6 months.", "Closing this issue since its addressed. Feel free to reopen if the solution provided doesn't work for you. Thanks!"]}, {"number": 30555, "title": "Trying to create frozen graph with freeze_graph: IndexError: list index out of range", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): Docker image\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 2.7.15+\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GeForce RTX 2080 Ti\r\n\r\nI am trying to freeze a custom keras model (simple XOR) with the help of the freeze_graph script for later usage with the Coral TPU Stick. After a week of testing or so, I still can't get it to work because I am running into an error, caused by freeze_graph.\r\n\r\nI think I am following the instructions correctly on creating a graph, so the script should work and produce a frozen graph. I am using the official `tensorflow:tensorflow:gpu-latest` docker image.\r\n\r\n**Code**\r\n\r\nWith the following script, I create the graph in keras and save the quantized training graph.\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense\r\n\r\ntraining_data = np.array([[0,0],[0,1],[1,0],[1,1]], \"uint8\")\r\ntarget_data = np.array([[0],[1],[1],[0]], \"uint8\")\r\n\r\nmodel = Sequential()\r\nmodel.add(Dense(16, input_dim=2, use_bias=False, activation='relu'))\r\nmodel.add(Dense(1, use_bias=False, activation='sigmoid'))\r\n\r\nsession = tf.keras.backend.get_session()\r\ntf.contrib.quantize.create_training_graph(session.graph)\r\nsession.run(tf.global_variables_initializer())\r\n\r\nmodel.compile(loss='mean_squared_error',\r\n              optimizer='adam',\r\n              metrics=['binary_accuracy'])\r\n\r\nmodel.fit(training_data, target_data, nb_epoch=1000, verbose=2)\r\nprint(model.predict(training_data).round())\r\nmodel.summary()\r\n\r\nsaver = tf.train.Saver()\r\nsaver.save(keras.backend.get_session(), 'xor-keras.ckpt')\r\n\r\n```\r\n\r\nThen I load the graph and convert it into an eval graph with a second script:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense\r\n\r\nmodel = Sequential()\r\nmodel.add(Dense(16, input_dim=2, use_bias=False, activation='relu'))\r\nmodel.add(Dense(1, use_bias=False, activation='sigmoid')) \r\n\r\n\r\n #<Load the model into a new session>\r\n\r\nsession = tf.keras.backend.get_session()\r\n\r\nsaver = tf.train.Saver()\r\nsaver.restore(session, 'xor-keras.ckpt')\r\n\r\ntf.contrib.quantize.create_eval_graph(session.graph)\r\n\r\ntf.io.write_graph(session.graph, '.', 'xor-keras-eval.pb', as_text=False)\r\n```\r\n\r\nIn my opinion, this should be valid input for the script, which I execute with the command:\r\n\r\n`freeze_graph --input_graph='xor-keras-eval.pb' --input_checkpoint='xor-keras.ckpt' --output_graph='xor-keras-frozen.pb' --output_node_name='dense_1/Sigmoid' --input_binary=True`\r\n\r\nThis throws an error. Here the complete error traceback:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/freeze_graph\", line 10, in <module>\r\n    sys.exit(run_main())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/tools/freeze_graph.py\", line 487, in run_main\r\n    app.run(main=my_main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/tools/freeze_graph.py\", line 486, in <lambda>\r\n    my_main = lambda unused_args: main(unused_args, flags)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/tools/freeze_graph.py\", line 378, in main\r\n    flags.saved_model_tags, checkpoint_version)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/tools/freeze_graph.py\", line 361, in freeze_graph\r\n    checkpoint_version=checkpoint_version)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/tools/freeze_graph.py\", line 190, in freeze_graph_with_def_protos\r\n    var_list=var_list, write_version=checkpoint_version)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 825, in __init__\r\n    self.build()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 837, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 875, in _build\r\n    build_restore=build_restore)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 482, in _build_internal\r\n    names_to_saveables)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saving/saveable_object_util.py\", line 342, in validate_and_slice_inputs\r\n    for converted_saveable_object in saveable_objects_for_op(op, name):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saving/saveable_object_util.py\", line 205, in saveable_objects_for_op\r\n    variable, \"\", name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saving/saveable_object_util.py\", line 82, in __init__\r\n    self.handle_op = var.op.inputs[0]\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2410, in __getitem__\r\n    return self._inputs[i]\r\nIndexError: list index out of range\r\n```\r\n\r\nI have tried executing my scripts with a custom docker image with tensorflow 1.13.1 and Python 3.6.8, but with the same result. I also found other threads with similar/the same issue #20886 and #24591, but none of the proposed solutions worked. I am quite confident that this is a bug, and want you to verify that I am doing it correctly.", "comments": ["@DocDriven ,\r\nI have tried executing the second script and I am getting the error, \r\n\r\n```\r\nNotFoundError: Key dense_2/kernel not found in checkpoint\r\n\t [[{{node save_1/RestoreV2}}]]\r\n```.\r\n\r\nCan you please provide correct reproducible code snippet. Thanks.", "@rmothukuru : I have double checked my posted code and tested it with both docker images/Python versions, but I can't reproduce your error. The behavior I described earlier is still the same.\r\n\r\nHowever, I could share my images with you, if this helps. Just tell me where to put them.", "ckpt model can be transferred to GraphDef(pb)model by freeze_graph.py in tensorflow,\r\n[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py](url)\r\nor you can do like this depend by your code:\r\n1) load your ckpt model and generate graph\r\n2) use tf.saver to save your model [however, generating model by this way is made up of saved_model.pb and variables folder]", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 30554, "title": "Can't set an initial state of the tf.keras.layers.Bidirectional", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: macOS 10.14.15 Darwin-18.6.0-x86_64-i386-64bit\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 1.14.0\r\n- Python version: 3.6.0\r\n\r\n**Describe the current behavior**\r\n\r\n`tf.keras.layers.Bidirectional` wrapped around a `tf.keras.layers.LSTM` instance raises a `TypeError` when trying to pass it an initial state (through the `initial_state` argument of the `__call__`).\r\n\r\n**Describe the expected behavior**\r\n\r\nNo error is raised and the (joint) initial state is properly distributed by the `Bidirectional` between the underlying forward and backward `LSTM` instances.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nlstm = tf.keras.layers.LSTM(units=10)\r\nbidirectional = tf.keras.layers.Bidirectional(lstm)\r\n\r\ninputs = tf.placeholder(tf.float32, shape=[32, 15, 20])\r\n\r\nfw_state = [tf.zeros([32, 10]), tf.zeros([32, 10])]\r\nbw_state = [tf.zeros([32, 10]), tf.zeros([32, 10])]\r\ninitial_state = fw_state + bw_state\r\n\r\noutput = bidirectional(\r\n    inputs=inputs,\r\n    initial_state=initial_state)\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \".../test.py\", line 14, in <module>\r\n    initial_state=initial_state)\r\n  File \".../lib/python3.6/site-packages/tensorflow/python/keras/layers/wrappers.py\", line 592, in __call__\r\n    return super(Bidirectional, self).__call__(inputs, **kwargs)\r\n  File \".../lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 634, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \".../lib/python3.6/site-packages/tensorflow/python/keras/layers/wrappers.py\", line 629, in call\r\n    initial_state=forward_state, **kwargs)\r\n  File \".../lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 2533, in call\r\n    inputs, mask=mask, training=training, initial_state=initial_state)\r\n  File \".../lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 678, in call\r\n    inputs, initial_state, constants)\r\n  File \".../lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 787, in _process_inputs\r\n    if len(initial_state) != len(self.states):\r\nTypeError: object of type 'Tensor' has no len()\r\n```\r\n\r\nSimilar issue https://github.com/tensorflow/tensorflow/issues/28761 is resolved in the tensorflow 2.0 code. The corresponding code fix in https://github.com/tensorflow/tensorflow/commit/2a8f9b1ccfaaebd6f9cf5b5eb972c2dafded4f5e seems to be relevant for this issue in tensorflow 1.x, too.", "comments": ["I have tried on colab with TF version 1.14 and was able to reproduce the issue.Thanks!", "From basic (unexhaustive) testing, and as pointed out by @aakhundov, simply copy/pasting the definition of the `tf.keras.layers.Bidirectional.call` from branch r2.0 to branch r1.14 seems to fix the issue without breaking things around. I guess a PR should be opened to do so, and CI testing should validate this. @aakhundov, as you identified the issue, maybe you could / should do it?", "@pandrey-fr, I've tried the copy/paste that you'd suggested and it indeed fixed the issue, at least in my code snippet above. Regarding the PR, I see the changes in the `Bidirectional` code from https://github.com/tensorflow/tensorflow/commit/2a8f9b1ccfaaebd6f9cf5b5eb972c2dafded4f5e already reflected in the master branch. Does this mean that the fix will appear in the next 1.x release and we just have to wait until then?", "Hmm, I do not know, since the master branch seems to cover both 1.x and 2.x releases (its versioning feels quite blurry to me). I would have gone for a PR to the r1.14 branch, but I do not know whether that is the right behaviour. @jvishnuvardhan, any advice?", "@aakhundov I cannot reproduce the issue with tf-nightly. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/33931741ea0f5c161b043dbd0d0fab89/tf_keras_bidirectional_30554.ipynb). This might have been resolved. Thanks!", "@jvishnuvardhan it works with tf-nightly indeed, as the fix is already in the master branch. Can the fix also be incorporated into 1.14 somehow, or should we just wait for 1.15 to be released (and use tf-nightly in the mean time)? Thank you!", "@aakhundov I think I would use tf-nightly.\r\n@qlzh727 Any thoughts on incorporating this in 1.14? Thanks!", "I think the 1.14 release has already been published, and we probably won't change it unless there is a dramatic issue. Given the fact that its fixed in nightly and 1.15 is not far away, probably just wait for next release?", "@qlzh727 sure, will wait for 1.15. Thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30554\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30554\">No</a>\n"]}, {"number": 30553, "title": "Disallowing numpy 1.17 onwards as dependency", "body": "Numpy 1.17 comes with several FutureWarning messages that influence TF (see https://github.com/tensorflow/tensorflow/issues/30427 ). This PR limits the range of Numpy versions to max 16.x.", "comments": ["@habernal @mihaimaruseac instead of disallowing numpy 1.17+, I think another approach is to update tensorflow to make it compatible with 1.17+ with warning removed. Created a PR #30559 for that. Please take a look.", "Thank you for the PR. However, upper limits on dependencies should be avoided as much as possible. I'm going to approve #30559 instead, since that seems to work too.", "#30559 merged. Thank you again for the PR"]}, {"number": 30552, "title": "VS linkage fails with \"unresolved external symbol\"", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: git tag v2.0.0_beta1\r\n- Python version: 3.6.7\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): bazel release 0.24.1\r\n- GCC/Compiler version (if compiling from source): \"C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC//bin/amd64/cl.exe\"\r\n- Microsoft (R) C/C++ Optimizing Compiler Version 19.00.24234.1 for x64\r\n- CUDA/cuDNN version: 10.1/7.6.1\r\n- GPU model and memory: GeForce GTX 1060 (6GB)\r\n\r\n\r\n\r\n**Describe the problem**\r\nCan't link the code. I get this error:\r\n1>a.obj : error LNK2001: unresolved external symbol \"public: int __cdecl google::protobuf::RepeatedPtrField<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::size(void)const \" (?size@?$RepeatedPtrField@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@protobuf@google@@QEBAHXZ)\r\n1>a.obj : error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::SessionOptions::SessionOptions(void)\" (??0SessionOptions@tensorflow@@QEAA@XZ)\r\n1>a.obj : error LNK2001: unresolved external symbol \"class tensorflow::Session * __cdecl tensorflow::NewSession(struct tensorflow::SessionOptions const &)\" (?NewSession@tensorflow@@YAPEAVSession@1@AEBUSessionOptions@1@@Z)\r\n1>b.obj : error LNK2001: unresolved external symbol \"public: bool __cdecl google::protobuf::MessageLite::ParseFromArray(void const *,int)\" (?ParseFromArray@MessageLite@protobuf@google@@QEAA_NPEBXH@Z)\r\n\r\nI checked with dumpbin and didn't find this symbols/exports.\r\nI also tried to patch missing symbols, and it didn't help (https://github.com/guikarist/tensorflow-windows-build-script/blob/master/patches/tf_exported_symbols_msvc.lds)\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI built a tensorflow.dll & tensorflow.lib using this command:\r\nbazel build --config=opt //tensorflow:tensorflow_dll_import_lib\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@goldiegadde who ran the 2.0 builds.\r\nGoldie, was the GPU windows build successfil for 2.0 release?\r\nHave you ran into this problem yourself?", "I encountered the same issue in version 1.14 version. But I think the solution probably works for 2.0 as well. \r\n\r\nThe solution is to modify tensorflow\\tools\\def_file_filter\\def_file_filter.py.tpl to add those missing symbols. For instance around line 130.\r\n\r\nThe long term solution is to modify the rules in this file to include more useful symbols.", "The solution proposed at https://github.com/guikarist/tensorflow-windows-build-script/blob/master/patches/tf_exported_symbols_msvc.lds only works for versions earlier than 1.14. ", "I edited the file to include missing symbols in v1.14, but it didn't work.\r\n\r\n    def_fp.write(\"\\t ??1OpDef@tensorflow@@UEAA@XZ\\n\")\r\n    def_fp.write(\"\\t ?size@?$RepeatedPtrField@V?$basic_string@DU?$char_traits@D@std@@v?$allocator@D@2@@std@@@protobuf@google@@QEBAHXZ\\n\")\r\n    def_fp.write(\"\\t ??0SessionOptions@tensorflow@@QEAA@XZ\\n\")\r\n    def_fp.write(\"\\t ?NewSession@tensorflow@@YAPEAVSession@1@AEBUSessionOptions@1@@z\\n\")\r\n    def_fp.write(\"\\t ?ParseFromArray@MessageLite@protobuf@google@@QEAA_NPEBXH@Z\\n\")\r\n\r\nActually TF dll even didn't link.\r\n\r\n\tExecution platform: @bazel_tools//platforms:host_platform\r\n\t   Creating library bazel-out/x64_windows-opt/bin/tensorflow/tensorflow.dll.if.lib and object bazel-out/x64_windows-opt/bin/tensorflow/tensorflow.dll.if.exp\r\n\tmemory_optimizer.lib(memory_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported\r\n\tpin_to_host_optimizer.lib(pin_to_host_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported\r\n\tutils.lib(utils.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported\r\n\tcollective_param_resolver_distributed.lib(collective_param_resolver_distributed.obj) : warning LNK4217: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported in function \"public: __cdecl tensorflow::CollGroupParams::CollGroupParams(void)\" (??0CollGroupParams@tensorflow@@QEAA@XZ)\r\n\tbatch_kernels.lo.lib(batch_kernels.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported\r\n\tcaptured_function.lib(captured_function.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported\r\n\tarithmetic_optimizer.lib(arithmetic_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported\r\n\tpin_to_host_optimizer.lib(pin_to_host_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported\r\n\tutils.lib(utils.obj) : warning LNK4049: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported\r\n\tarithmetic_optimizer.lib(arithmetic_optimizer.obj) : warning LNK4217: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported in function \"private: bool __cdecl tensorflow::grappler::`anonymous namespace'::ReorderCastLikeAndValuePreserving::NodeIsOnCpuOrGpu(class tensorflow::NodeDef const *)const \" (?NodeIsOnCpuOrGpu@ReorderCastLikeAndValuePreserving@?A0xbdf9375e@grappler@tensorflow@@AEBA_NPEBVNodeDef@4@@Z)\r\n\tauto_mixed_precision.lib(auto_mixed_precision.obj) : warning LNK4049: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported\r\n\tlayout_optimizer.lib(layout_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported\r\n\tmemory_optimizer.lib(memory_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported\r\n\r\n\ttensorflow.dll.if.exp : error LNK2001: unresolved external symbol \"?NewSession@tensorflow@@YAPEAVSession@1@AEBUSessionOptions@1@@z\" (?NewSession@tensorflow@@YAPEAVSession@1@AEBUSessionOptions@1@@z)\r\n\ttensorflow.dll.if.exp : error LNK2001: unresolved external symbol \"public: int __cdecl google::protobuf::RepeatedPtrField<class std::basic_string<char,struct std::char_traits<char>,std::allocator<char> > >::size(void)const \" (?size@?$RepeatedPtrField@V?$basic_string@DU?$char_traits@D@std@@v?$allocator@D@2@@std@@@protobuf@google@@QEBAHXZ)\r\n\r\n\tbazel-out/x64_windows-opt/bin/tensorflow/tensorflow.dll : fatal error LNK1120: 2 unresolved externals\r\n\tTarget //tensorflow:tensorflow_dll_import_lib failed to build", "Looks like you are build the C version. You should build the cpp version of tensorflow, which is tensorflow_cc.dll and tensorflow_cc.lib.\r\nPlease try to use to use following command\r\nbazel build -c opt  //tensorflow:tensorflow_cc\r\n", "Switching to the target you proposed (the exact same command you wrote) didn't help as well.\r\n\r\nAdding missing symbols in def_file_filter.py.tpl throws the exception above.\r\nWithout editing this file, dll/lib compiled ok, but linking with my code fails with same missing definitions.", "I see missing symbols in these 2 files:\r\ncore_cpu_impl.lo.lib\r\nprotobuf_lite.lib\r\nBut they were not added to dll for some reason.\r\n", "I added these libs manually, and got other error of double definition. Probably part of symbol definitions from libs were added into dll.\r\nI temporary resolved it by adding /FORCE:MULTIPLE in my program linking stage.\r\n\r\nRunning the code gave me other error: \r\n\r\n> 2019-07-18 10:09:26.269905: E tensorflow/core/common_runtime/session.cc:60] Not found: No session factory registered for the given session options: {target: \"\"}\r\n> Registered factories are {}.", "Currently I have these 3 libs linked in my project:\r\n\r\n> tensorflow_cc.dll.if.lib\r\n> core_cpu_impl.lo.lib\r\n> protobuf_lite.lib\r\n\r\nand tensorflow_cc.dll near project exe.\r\n\r\nMaybe I have to link more libs to overcome \"Registered factories are {}.\" ?", "This is similar to https://github.com/tensorflow/tensorflow/issues/30647\r\nLet me try to see who I can escalate libtensorflow issues, as I am not very experienced with it.", "May somebody share the place, where writes are to be added in tf_exported_symbols_msvc.lds?", "> Currently I have these 3 libs linked in my project:\r\n> \r\n> > tensorflow_cc.dll.if.lib\r\n> > core_cpu_impl.lo.lib\r\n> > protobuf_lite.lib\r\n> \r\n> and tensorflow_cc.dll near project exe.\r\n> \r\n> Maybe I have to link more libs to overcome \"Registered factories are {}.\" ?\r\n\r\nHello,\r\n\r\nHave you solved your problem ?\r\nI have exactly the same issue\r\n\r\nthank you very much\r\n", "No =(\r\n\r\n> > Currently I have these 3 libs linked in my project:\r\n> > > tensorflow_cc.dll.if.lib\r\n> > > core_cpu_impl.lo.lib\r\n> > > protobuf_lite.lib\r\n> > \r\n> > \r\n> > and tensorflow_cc.dll near project exe.\r\n> > Maybe I have to link more libs to overcome \"Registered factories are {}.\" ?\r\n> \r\n> Hello,\r\n> \r\n> Have you solved your problem ?\r\n> I have exactly the same issue\r\n> \r\n> thank you very much\r\n\r\n", "Hi @AndreyPlotkinOr ! \r\nWe are checking to see whether you still need help in this issue . Have your tried instructions in this [thread](https://www.tensorflow.org/install/source_windows) yet?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30552\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30552\">No</a>\n"]}, {"number": 30551, "title": "RuntimeError: Collective ops must be configured at program startup", "body": "Hi,\r\n\r\nI am trying to run mnist example using distributed training tf 2.0.I am getting below similar error. I request to help me with workaround for this error.\r\n\r\ntf.distribute.experimental.CollectiveCommunication.NCCL\r\n\r\nFile \"/home/administrator/venv/lib/python3.5/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 80, in init\r\ncommunication=communication))\r\nFile \"/home/administrator/venv/lib/python3.5/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 110, in init\r\nself._initialize_strategy(cluster_resolver)\r\nFile \"/home/administrator/venv/lib/python3.5/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 116, in _initialize_strategy\r\nself._initialize_multi_worker(cluster_resolver)\r\nFile \"/home/administrator/venv/lib/python3.5/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 208, in _initialize_multi_worker\r\ndevice_filters=(\"/job:%s/task:%d\" % (task_type, task_id),))\r\nFile \"/home/administrator/venv/lib/python3.5/site-packages/tensorflow/python/eager/context.py\", line 541, in configure_collective_ops\r\nraise RuntimeError(\"Collective ops must be configured at program startup\")\r\nRuntimeError: Collective ops must be configured at program startup", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the code snippet to reproduce the issue. If you are unclear what to include see the issue template displayed in the [Github](https://github.com/tensorflow/tensorflow/issues/new/choose) new issue template.\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "NOTE: I was encountering this error too, and was getting all ready to post a long description here--when I tried pulling down the latest tf2-nightly. The error disappeared for me. So...thought I'd mention there's a possibility this has been taken care of somewhere along the line.", "OS:\r\nRed Hat Enterprise Linux 7.6\r\n\r\nHi @CJMenart,\r\n\r\nThanks, indeed pulling latest tf2-nightly helped resolving `RuntimeError: Collective ops must be configured at program startup`.\r\n\r\nHowever now the script gets stuck like below, and won't start training, any idea?:\r\n\r\n```\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\nI0802 10:09:39.379140 47982668778688 distribute_coordinator.py:776] Running Distribute Coordinator with mode = 'independent_worker', cluster_spec = {'worker': ['gcn29:2222', 'gcn32:2222', 'gcn60:2222'], 'ps': {}}, task_type = 'worker', task_id = 0, environment = None, rpc_layer = 'grpc'\r\nW0802 10:09:39.379293 47982668778688 distribute_coordinator.py:825] `eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nW0802 10:09:39.379407 47982668778688 distribute_coordinator.py:829] `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nI0802 10:09:39.380918 47982668778688 collective_all_reduce_strategy.py:311] Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'worker': ['gcn29:2222', 'gcn32:2222', 'gcn60:2222'], 'ps': {}}, task_type = 'worker', task_id = 0, num_workers = 3, local_devices = ('/job:worker/task:0/device:GPU:0', '/job:worker/task:0/device:GPU:1'), communication = CollectiveCommunication.AUTO\r\nI0802 10:09:39.381378 47982668778688 distribute_coordinator.py:438] Starting standard TensorFlow server, target = 'grpc://gcn29:2222', session_config= allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    scoped_allocator_optimization: ON\r\n    scoped_allocator_opts {\r\n      enable_op: \"CollectiveReduce\"\r\n    }\r\n  }\r\n}\r\nexperimental {\r\n  collective_group_leader: \"/job:worker/replica:0/task:0\"\r\n}\r\n\r\n2019-08-02 10:09:39.383500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: Tesla K40m major: 3 minor: 5 memoryClockRate(GHz): 0.745\r\npciBusID: 0000:02:00.0\r\n2019-08-02 10:09:39.384651: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: \r\nname: Tesla K40m major: 3 minor: 5 memoryClockRate(GHz): 0.745\r\npciBusID: 0000:82:00.0\r\n2019-08-02 10:09:39.384697: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-08-02 10:09:39.384731: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-08-02 10:09:39.384761: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-08-02 10:09:39.384791: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-08-02 10:09:39.384821: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-08-02 10:09:39.384850: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-08-02 10:09:39.384880: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-08-02 10:09:39.389346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1\r\n2019-08-02 10:09:39.389411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-08-02 10:09:39.389428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 1 \r\n2019-08-02 10:09:39.389439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N N \r\n2019-08-02 10:09:39.389448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1:   N N \r\n2019-08-02 10:09:39.394302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 10798 MB memory) -> physical GPU (device: 0, name: Tesla K40m, pci bus id: 0000:02:00.0, compute capability: 3.5)\r\n2019-08-02 10:09:39.395495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:1 with 10798 MB memory) -> physical GPU (device: 1, name: Tesla K40m, pci bus id: 0000:82:00.0, compute capability: 3.5)\r\n2019-08-02 10:09:39.398090: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:254] Initialize GrpcChannelCache for job ps -> {}\r\n2019-08-02 10:09:39.398116: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:254] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2222, 1 -> gcn32:2222, 2 -> gcn60:2222}\r\n2019-08-02 10:09:39.400709: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:2222\r\n2019-08-02 10:09:39.400749: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:369] Server already started (target: grpc://localhost:2222)\r\nI0802 10:09:39.402244 47982668778688 collective_all_reduce_strategy.py:311] Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'worker': ['gcn29:2222', 'gcn32:2222', 'gcn60:2222'], 'ps': {}}, task_type = 'worker', task_id = 0, num_workers = 3, local_devices = ('/job:worker/task:0/device:GPU:0', '/job:worker/task:0/device:GPU:1'), communication = CollectiveCommunication.AUTO\r\nW0802 10:09:39.402778 47982668778688 backend.py:523] OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.\r\nI0802 10:09:39.496106 47982668778688 cross_device_ops.py:1105] Collective batch_all_reduce: 1 all-reduces, num_workers = 3\r\nI0802 10:09:39.504189 47982668778688 cross_device_ops.py:1105] Collective batch_all_reduce: 1 all-reduces, num_workers = 3\r\nI0802 10:09:39.511809 47982668778688 cross_device_ops.py:1105] Collective batch_all_reduce: 1 all-reduces, num_workers = 3\r\nI0802 10:09:39.519497 47982668778688 cross_device_ops.py:1105] Collective batch_all_reduce: 1 all-reduces, num_workers = 3\r\nI0802 10:09:39.568049 47982668778688 distribute_coordinator.py:776] Running Distribute Coordinator with mode = 'independent_worker', cluster_spec = {'worker': ['gcn29:2222', 'gcn32:2222', 'gcn60:2222'], 'ps': {}}, task_type = 'worker', task_id = 0, environment = None, rpc_layer = 'grpc'\r\nW0802 10:09:39.568199 47982668778688 distribute_coordinator.py:825] `eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nW0802 10:09:39.568313 47982668778688 distribute_coordinator.py:829] `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nI0802 10:09:39.569651 47982668778688 collective_all_reduce_strategy.py:311] Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'worker': ['gcn29:2222', 'gcn32:2222', 'gcn60:2222'], 'ps': {}}, task_type = 'worker', task_id = 0, num_workers = 3, local_devices = ('/job:worker/task:0/device:GPU:0', '/job:worker/task:0/device:GPU:1'), communication = CollectiveCommunication.AUTO\r\nI0802 10:09:39.571154 47982668778688 collective_all_reduce_strategy.py:311] Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'worker': ['gcn29:2222', 'gcn32:2222', 'gcn60:2222'], 'ps': {}}, task_type = 'worker', task_id = 0, num_workers = 3, local_devices = ('/job:worker/task:0/device:GPU:0', '/job:worker/task:0/device:GPU:1'), communication = CollectiveCommunication.AUTO\r\n```\r\n", "Sadly, no. I'm really behind on debugging and raising issues about distributed training...I think I have seen that error before. Seems like every time I change something in my distributed training script I get a new message I've never seen before. So far it's kind of a mystery.", "I had the same problem, you need to make sure that the environment variable is set on all cluster members and readable from the python process before is tf.distribute.experimental.MultiWorkerMirroredStrategy() called\r\n\r\nI've found this one a much better documentation than the official one, have fun!\r\nhttps://lambdalabs.com/blog/tensorflow-2-0-tutorial-05-distributed-training-multi-node/", "like  @romeokienzler , this [link ](https://lambdalabs.com/blog/tensorflow-2-0-tutorial-05-distributed-training-multi-node/) solved my problem.\r\nyou must declare the distribute strategy before the dataset and models. because the distribute strategy use of the eager's context.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport os\r\nimport json\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" \r\nos.environ['TF_CONFIG'] =  json.dumps({\r\n    'cluster': {\r\n        'worker': [\"localhost:12345\", \"localhost:23456\"]\r\n    },\r\n    'task': {'type': 'worker', 'index': 0}\r\n})\r\n\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n```\r\n\r\noutput is `INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0']\r\nINFO:tensorflow:Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'worker': ['localhost:12345', 'localhost:23456']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0',), communication = CollectiveCommunication.AUTO`"]}, {"number": 30550, "title": "[ROCm] Adding ROCm support for the LRN op", "body": "This PR adds ROCm support for the LRN op\r\n\r\nThe kernel implementation for the LRN op is different on the ROCm (than on the CUDA platform).\r\n \r\nThe difference between the two is that on ROCm platform, the underlying miopen (ROCm equivalent of cudnn) API expects the input tensor to be in NCHW format, and hence the ROCm implementation needs to convert the input tensor from NHWC format to NCHW format and vice-versa for the output tensor.\r\n\r\nThe CUDA implementation seems to be able to handle the NHWC format fine...\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/lrn_op.cc#L199\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/lrn_op.cc#L425\r\n\r\n...even though the documentation for the underlying cudnn API calls seems to indicate that only NCHW tensors are supported!\r\n\r\nhttps://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#cudnnLRNCrossChannelForward\r\nhttps://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#cudnnLRNCrossChannelBackward\r\n\r\n------------------------------------------------------------------\r\n\r\n@tatianashp @whchung @chsigg \r\n", "comments": ["@chsigg gentle ping...please re-approve. thanks", "@chsigg please re-approve. I pushed out a fix for the buildifier failure in \"Ubuntu Sanity\". thanks."]}, {"number": 30549, "title": "[INTEL MKL] Enabled Conv2D fprop for MKL-DNN v1.0.", "body": "", "comments": ["It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@tensorflowbutler yes, this PR is still being worked on. I'll push the requested changes within a couple of days. Thanks!", "@penpornk I have addressed your review comments. Please take a look. Thanks!", "@penpornk I have addressed your latest review comments. Please let me know if it looks okay. Thank you!"]}, {"number": 30548, "title": "init_from_checkpoint performing incorrectly", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: GTX Titan X\r\n\r\n**Describe the current behavior**\r\nI have a variable stored in a ckpt called prsr/prior/conv1/weights and i want to do a map assignment to a variable called prsr/prior/conv1/weights_1:0. When I execute the init_from_checkpoint execution it gives the following error \r\n\r\n> ValueError: Assignment map with scope only name prsr/prior/conv1 should map to scope only prsr/prior/conv1/weights. Should be 'scope/': 'other_scope/'.\r\n\r\n**Describe the expected behavior**\r\nI think it should assigned correctly the variable from the .ckpt prsr/prior/conv1/weights into the current variable prsr/prior/conv1/weights_1:0 as indicated in the API official documentation:\r\n\r\n```\r\n# Initialize partitioned variables using variable's name\r\ninit_from_checkpoint('/tmp/model.ckpt',\r\n                     {'old_scope_2/var3': 'new_scope_2/var3'})\r\n```\r\n", "comments": ["@manurare Can you share a standalone code to reproduce the issue? Thanks!", "Sorry for the delay. I think I found the issue myself and it was my fault. Thanks for the response nonetheless.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30548\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30548\">No</a>\n"]}, {"number": 30547, "title": "[INTEL MKL] Added support for common utility functions for MKL-DNN v1.0.", "body": "", "comments": ["@penpornk Thanks for reviewing the PR! I have addressed your review comments. Please let me know if it looks okay.", "@penpornk I have addressed your review comments. Please take a look. Thanks!", "@penpornk I have changed `CHECK_NE` to `DCHECK_NE`."]}]