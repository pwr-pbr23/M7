[{"number": 44101, "title": "Remove Python 2 instructions from TF Lite demo README", "body": "Python 2 is not supported in TensorFlow after TF 2.1, so having Python 2 instructions in a demo's README in 2.4.0 is misleading.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44101) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 44100, "title": "Add tflite_runtime 2.3.1 wheels to Python quickstart", "body": "## URL(s) with the issue: \r\nhttps://www.tensorflow.org/lite/guide/python\r\n\r\n## Description of issue (what needs changing): \r\nThe [TensorFlow Lite Python quickstart guide](https://www.tensorflow.org/lite/guide/python) currently only contains links to prebuilt wheels for tflite_runtime 2.1.0. TensorFlow 2.1.0 is missing several new features and key [security fixes](https://github.com/tensorflow/tensorflow/releases/tag/v2.3.1) that would be valuable for developers. As such, would it be possible to update this page with download links for a fresh (>= 2.3.1) version of TensorFlow? Updating the wheels to a newer version should not require any other changes to the quickstart guide.\r\n\r\n\r\nThe source for this page is located at https://github.com/tensorflow/tensorflow/blob/1e87951747af11b61206dd19f9363b309fc8e6f5/tensorflow/lite/g3doc/guide/python.md .\r\n\r\n### Submit a pull request?\r\n\r\nI don't have the ability to host wheels at https://dl.google.com, so I believe a TensorFlower will need to build all the wheels and then upload them. Public instructions for building the wheels are hosted at https://github.com/tensorflow/tensorflow/blob/f1a3160b04dda71d50130d6a03727486bed2f5e1/tensorflow/lite/tools/pip_package/README.md\r\n", "comments": ["As TensorFlow has [announced](https://groups.google.com/a/tensorflow.org/forum/#!topic/announce/Up9cjJY_KYw) it is dropping future support for Python 3.5, updating this page will likely also entail dropping the Python 3.5 wheels (if providing 2.4.0 wheels).", "There is a team who is working on this. It'll take some time.\r\n\r\nBTW, can't you just use tf.lite package in TF PIP?\r\nhttps://www.tensorflow.org/api_docs/python/tf/lite\r\n\r\nWhat's the reason you need tflite_runtime package?", "Hi Terry,\r\n\r\nIt's great to hear that the team is working on this. I worked around my issue by compiling copies of both the tflite_runtime and full TensorFlow from source, but a prebuilt `.whl` is definitely more convenient.\r\n\r\nAs for your question, there were two main reasons I was looking for a `tflite_runtime` package.\r\n\r\n1. I was looking to do inference on a Raspberry Pi 4B running 64-bit Ubuntu, and while there isn't a [prebuilt TensorFlow wheel for AArch64](https://www.tensorflow.org/install/pip#package-location), there is a prebuilt (2.1.0) tflite_runtime wheel\r\n2. I was interested in minimizing memory consumption, and I was under the (potentially flawed) assumption that accessing TF Lite as part of the full package would incur higher memory usage than using the stripped down runtime package.\r\n\r\nThe reason I wanted a 2.3.X version of TF Lite was so that I could configure the number of threads the interpreter is using via the Python API (https://github.com/tensorflow/tensorflow/pull/25748).\r\n\r\nThank you for your help, I really appreciate it.", "tflite_runtime 2.5 wheels are available https://github.com/google-coral/pycoral/releases", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44098, "title": "Custom function adding before augmentation in ImageDataGenerator.", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests, and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.3.1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behaviour/state.**\r\ntensorflow.keras.preprocessing.image.ImageDataGenerator class takes an argument to implement custom function after augmentation, which it is 'preproccessing_function'. Allowing to use a custom function before augmentation or rescale/resizing will be a great advantage for images that need to be cropped into bounding boxes.\r\n\r\n**Will this change the current API? How?**\r\nI cannot comment on this due to my inexperience.\r\n\r\n**Who will benefit with this feature?**\r\nWho has bounding box coordinates in *.csv file. But not applied yet.\r\n \r\n**Any Other info.**\r\nNo.", "comments": ["ImageDataGenerator is legacy. We recommend that you do image data augmentation via layers, like this: https://keras.io/examples/vision/image_classification_from_scratch/#using-image-data-augmentation\r\n\r\nIn this setup you can add transformation layers either before or after the data augmentation layers.", "Thank you, I didn't know that ImageDataGenerator is legacy."]}, {"number": 44097, "title": "Fix transpose op legalization patterns", "body": "The second transpose legalization pattern originally handled both types but the first one handled 64 bit, so with this change the second one will only handle 32 bit so they operate disjointly.", "comments": ["@smit-hinsu ", "> Could you add a test for this which would have failed before this change?\r\n> \r\n> Also, add a TODO note to combine these two patterns as we had discussed.\r\n\r\nWould the test be a transpose with a 64 bit non constant second argument?", "> > Could you add a test for this which would have failed before this change?\r\n> > Also, add a TODO note to combine these two patterns as we had discussed.\r\n> \r\n> Would the test be a transpose with a 64 bit non constant second argument?\r\n\r\nYes, that's correct. That should give an illegal IR before this change."]}, {"number": 44096, "title": "Add mul gradients", "body": "@saxenasaurabh \r\n\r\nPart of #42668 \r\n\r\nI would like to ask something:\r\n- Should we file an issue to track the progress of this project ?\r\n- Is there any timeline for this project in general ?\r\n- The test is quite large. Should we seperate it into multiple files ( something like `math_grad_test.cc` and `unified_api_math_test.py` )", "comments": ["@saxenasaurabh \nCould you review this PR please ?", "Sorry for the hold up, I will review this today.", "> Should we file an issue to track the progress of this project ?\r\n\r\nThat's an interesting idea and something I would definitely like to try. For now I have been tracking issues and progress using Google internal tools. Let me know if you have any suggestions on how we can manage this on github. @dynamicwebpaige may have pointers on managing OSS projects.\r\n\r\n> Is there any timeline for this project in general ?\r\n\r\nI think this project overall will span into Q1'21. In Q4, I want to focus on fleshing out the C++ infrastructure. E.g. Accidental memory leaks are a big problem right now. We also need to make sure that C++ gradient functions are as easy to write as python gradients.\r\n\r\nAlso getting ResNet training using C++ gradients would be a great milestone! I had done some [benchmarking](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/python/framework/experimental/unified_api_test.py;l=260;drc=e9204018c310f1ab53323f468c76a40b43ce92e8) for simple MLPs on MNIST and those were 2x faster in eager mode. If we can get similar gains on ResNet, that would be amazing I think.\r\n\r\nI also want to figure out how these gradient functions can be used from python alongside other python gradients. I have this and other things written down in a doc. I will try to publish it externally asap.\r\n\r\n> The test is quite large. Should we seperate it into multiple files ( something like math_grad_test.cc and unified_api_math_test.py )\r\n\r\nThat's a great idea. We should try to colocate gradients tests with the gradients code.", "> Let me know if you have any suggestions on how we can manage this on github\r\n\r\nI don't know any tool honestly. I am just thinking that we could use a list of checkboxes to maintain the list of gradients but I did not think any further."]}, {"number": 44095, "title": "installation for Torch", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@viraj123456789 \r\nPlease refer to [this link](https://medium.com/datadriveninvestor/installing-pytorch-and-tensorflow-with-cuda-enabled-gpu-f747e6924779), we see that you have not filled in the issue template, please explain the issue.", "Looks like spam issue", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44095\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44095\">No</a>\n"]}, {"number": 44094, "title": "Use movi NEON instruction to zero out registers", "body": "Currently `dup` is used to zero our NEON registers in the packing and AArch64 kernel code. According to the [Cortex A72 optimization guide](https://developer.arm.com/documentation/uan0016/a/) which is used in the Raspberry PI 4, `dup` has an execution latency of 8 cycles and a throughput of 1 when copying from a general purpose register to a NEON register.\r\n\r\nThis PR changes the code to use `movi` which has a latency of 3 cycles and a throughput of 2. This is also used in [LLVM for zeroing out registers](https://github.com/llvm/llvm-project/blob/master/llvm/test/CodeGen/AArch64/arm64-zero-cycle-zeroing.ll), but please let me know if I am missing something here.", "comments": ["@abattery it looks fine to me. What do you think?", "Thank you for your contribution. \r\n\r\nAccording to the optimization guide, the dup instruction has the same latency and throughput with the movi instruction.\r\n\r\nIt would be better to keep the current code as is.\r\n\r\nPlease see the page 30 of the guide pdf file .\r\n\r\n\r\n| Instruction Group        | AArch64 Instructions | Exec latency | Execution Throughput | Utilized Pipelines | Notes |\r\n|--------------------------|----------------------|--------------|----------------------|--------------------|-------|\r\n| ASIMD duplicate, element | DUP                  | 3            | 2                    | F0/F1              |       |", "Thanks a lot for the fast response.\r\n\r\n> According to the optimization guide, the dup instruction has the same latency and throughput with the movi instruction.\r\n\r\nI think this is only true for the case when both source registers are already NEON vector registers.\r\nSince `wzr` is a general purpose register I think the numbers on the bottom of page 29 need to be used:\r\n\r\nInstruction Group | AArch64 Instructions | Exec latency | Execution Throughput | Utilized Pipelines\r\n-- | -- | -- | -- | --\r\nASIMD duplicate, gen reg | DUP | 8 | 1 | L, F0/F1\r\n\r\nBut please let me know if I am missing something.", "Thanks @lgeiger for correcting me on this. LLVM community also has a similar optimization like this one in https://reviews.llvm.org/D41515", "> LLVM community also has a similar optimization like this one in https://reviews.llvm.org/D41515\r\n\r\nGreat find!\r\n\r\nThanks for the fast review."]}, {"number": 44093, "title": "tf.distribute.MirroredStrategy using TF_CONFIG and non-eager execution assigns ops to nonexistent device names", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google AI Platform\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: reproducible on 2.2 or 2.3\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: Unknown\r\n- GPU model and memory: 2x NVIDIA Tesla K80\r\n\r\n---\r\n\r\nWhen running [the example code from _Distributed training with Keras_](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/distribute/keras.ipynb) on AI Platform with multiple GPUs, **eager execution disabled**, and using `tf.keras.MirroredStrategy` to utilize all GPUs, TensorFlow raises an `InvalidArgumentError`:\r\n\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError:\r\n    Cannot assign a device for operation conv2d/kernel/Initializer/random_uniform/RandomUniform:\r\n    Could not satisfy explicit device specification '' because the node\r\n    {{colocation_node conv2d/kernel/Initializer/random_uniform/RandomUniform}}\r\n    was colocated with a group of nodes that required incompatible device \r\n    '/job:chief/replica:0/task:0/device:GPU:0'. All available devices [\r\n        /job:localhost/replica:0/task:0/device:CPU:0,\r\n        /job:localhost/replica:0/task:0/device:XLA_CPU:0,\r\n        /job:localhost/replica:0/task:0/device:XLA_GPU:0,\r\n        /job:localhost/replica:0/task:0/device:XLA_GPU:1,\r\n        /job:localhost/replica:0/task:0/device:GPU:0,\r\n        /job:localhost/replica:0/task:0/device:GPU:1\r\n    ]. \r\n    \r\n    Colocation Debug Info:\r\n    Colocation group had the following types and supported devices: \r\n        Root Member(\r\n            assigned_device_name_index_=-1\r\n            requested_device_name_='/job:chief/replica:0/task:0/device:GPU:0'\r\n            assigned_device_name_=''\r\n            resource_device_name_='/job:chief/replica:0/task:0/device:GPU:0'\r\n            supported_device_types_=[GPU, CPU]\r\n            possible_devices_=[]\r\n        AssignVariableOp: GPU CPU XLA_CPU XLA_GPU \r\n        RandomUniform: GPU CPU XLA_CPU XLA_GPU \r\n        VarIsInitializedOp: GPU CPU XLA_CPU XLA_GPU \r\n        Const: GPU CPU XLA_CPU XLA_GPU \r\n        Mul: GPU CPU XLA_CPU XLA_GPU \r\n        ReadVariableOp: GPU CPU XLA_CPU XLA_GPU \r\n        Sub: GPU CPU XLA_CPU XLA_GPU \r\n        VarHandleOp: GPU CPU XLA_CPU XLA_GPU \r\n        Add: GPU CPU XLA_CPU XLA_GPU \r\n\r\n```\r\n\r\nOddly, the list of available devices includes the GPUs I want to use, but their names include `job:localhost` rather than `job:chief`.\r\n\r\n\r\nThe logs used to launch the job show that only one worker is being used, and it's being assigned the type `chief`:\r\n```json\r\nRunning task with arguments:\r\n  --cluster={\"chief\": [\"127.0.0.1:2222\"]}\r\n  --task={\"type\": \"chief\", \"index\": 0}\r\n  --job={\r\n    \"scale_tier\": \"CUSTOM\", \r\n    \"master_type\": \"n1-standard-16\",\r\n    \"package_uris\": [...snip...], \r\n    \"python_module\": \"psobot.mirrored_strategy_test\", \r\n    \"region\": \"europe-west1\", \r\n    \"runtime_version\": \"2.2\", \r\n    \"run_on_raw_vm\": true,\r\n    \"python_version\": \"3.7\", \r\n    \"master_config\": {\r\n      \"accelerator_config\": {\r\n        \"count\": \"2\",\r\n        \"type\": \"NVIDIA_TESLA_K80\"\r\n      }\r\n}}\r\n```\r\n\r\n...but also show that the device naming mismatch seems to happen earlier:\r\n```\r\nCreated TensorFlow device (/device:GPU:0 with 10634 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\"\r\nsuccessful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\"\r\nCreated TensorFlow device (/device:GPU:1 with 10634 MB memory) -> physical GPU (device: 1, name: Tesla K80, pci bus id: 0000:00:05.0, compute capability: 3.7)\"\r\nSome requested devices in `tf.distribute.Strategy` are not visible to TensorFlow: /job:chief/replica:0/task:0/device:GPU:0,/job:chief/replica:0/task:0/device:GPU:1\"\r\nUsing MirroredStrategy with devices ('/job:chief/replica:0/task:0/device:GPU:0', '/job:chief/replica:0/task:0/device:GPU:1')\"\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nTensorFlow should identify that `/job:chief` and `/job:localhost` refer to the same machine (the current machine) and should be able to place ops there, and it should be possible to train on multiple GPUs.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\n\r\n# This is the only functional change from the example code.\r\ntf.compat.v1.disable_eager_execution()\r\n\r\n# Copied from the example code at:\r\n# https://github.com/tensorflow/docs/blob/master/site/en/tutorials/distribute/keras.ipynb\r\ndatasets, info = tfds.load(name=\"mnist\", with_info=True, as_supervised=True)\r\nmnist_train, mnist_test = datasets[\"train\"], datasets[\"test\"]\r\nstrategy = tf.distribute.MirroredStrategy()\r\nprint(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\r\n\r\ntrain_dataset = mnist_train.map(lambda im, l: ((tf.cast(im, tf.float32) / 255), l)).batch(64)\r\n\r\nwith strategy.scope():\r\n    model = tf.keras.Sequential(\r\n        [\r\n            tf.keras.layers.Conv2D(32, 3, activation=\"relu\", input_shape=(28, 28, 1)),\r\n            tf.keras.layers.MaxPooling2D(),\r\n            tf.keras.layers.Flatten(),\r\n            tf.keras.layers.Dense(64, activation=\"relu\"),\r\n            tf.keras.layers.Dense(10),\r\n        ]\r\n    )\r\n\r\n    model.compile(\r\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n        optimizer=tf.keras.optimizers.Adam(),\r\n        metrics=[\"accuracy\"],\r\n    )\r\n\r\nmodel.fit(train_dataset, epochs=12)\r\n```\r\n\r\nRunning the above code on AI Platform with the following:\r\n```bash\r\ngcloud ai-platform jobs submit training psobot_mirrored_dummy_$(date +%s) \\\r\n  --runtime-version 2.2\r\n  --python-version 3.7\r\n  --project [YOUR GCP PROJECT HERE]\r\n  --region europe-west1\r\n  --module-name YOUR_MODULE_NAME_HERE.mirrored_strategy_test\r\n  --package-path YOUR_PACKAGE_NAME_HERE\r\n  --scale-tier custom\r\n  --master-machine-type n1-standard-16\r\n  --master-accelerator count=2,type=nvidia-tesla-k80 \r\n  --staging-bucket YOUR_STAGING_BUCKET_HERE\r\n```\r\n", "comments": ["Hi @psobot, is there a reason you have `tf.compat.v1.disable_eager_execution()` ? And do you still see this error if you comment out that line?", "Hi @nikitamaia! Yes - we're disabling eager execution (in a similar model to the one shown above) for performance reasons, similar to those outlined in [the Eager Execution documentation](https://www.tensorflow.org/guide/eager#benchmarks). Re-enabling eager execution does allow us to train, but reduces performance significantly. If `MirroredStrategy` doesn't support graph execution, I'd also expect it to return an error message saying as such, rather than with the error message that we've seen above.", "You should only use `tf.compat.v1.disable_eager_execution()` if you have some specific TF1 code that is incompatible with the TF2 default eager runtime and needs to be run in legacy graph mode. You should not explicitly disable eager execution for performance reasons. Disabling eager execution has additional drawbacks like preventing use of custom loss functions. To get improved performance in TF2 with graph mode, you should make use of [tf.function](https://www.tensorflow.org/guide/function) instead. This is a common confusion, but the gist is that in TF2 the way to get better performance is not to _disable_ eager execution, but instead to _enable_ graph execution.\r\n\r\nIn the MNIST example you have provided, the Keras API's `model.fit` takes care of running the code in TF2 graph mode with the use of `tf.function` under the hood. When you compile the model, there's an option to provide `run_eagerly=True` (it's set to False by defualt) and that will run model training in eager mode. You'll see a dramatic slow down in training, and also if training with `MirroredStrategy` I believe you will also see a warning that `MirroredStrategy` works better with graph mode. Hope this helps to clear up any confusion. ", "Thanks @nikitamaia! That's helpful, but still doesn't solve this issue - am I right to assume then that `MirroredStrategy` cannot be expected to work correctly with eager execution disabled, and that the `InvalidArgumentError` is to be expected? (another way to put it: is disabling eager execution such an anti-pattern that bugs aren't worth fixing if they only occur with eager disabled?)", "Yes that's correct. I agree that unfortunately this error message is not really helpful in helping you to figure out what is going on...But unless you have a very specific compat use case that absolutely requires legacy graph mode, you shouldn't disable eager execution.", "Makes sense - I'll close this issue. The performance issues that I was disabling eager execution for in the first place seem to be the result of another bug (see: https://github.com/tensorflow/tensorflow/issues/44194). Thanks for your help, @nikitamaia !", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44093\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44093\">No</a>\n"]}, {"number": 44092, "title": "Loss of shape information when using dilation_rate != 1 in Conv layers", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nRunning TF 2.3.0 in colab I have come across an issue very similar to the one fixed in #29542 . The difference however being that I have an extra dimension that is undefined at layer construction time. With a dilation rate = 1 the Conv2D layer in the following code produces an output shape as expected (None, None, 512, 64). However, for layers with a dilation rate > 1, the \"width\" dimension shape information is lost and the result is (None, None, None, 64).\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nclass DenseBlock(tf.keras.Model):\r\n  def __init__(self, input_size, depth=5, in_channels=64):\r\n    super(DenseBlock, self).__init__(name='')\r\n    self.depth = depth\r\n    self.in_channels = in_channels\r\n    self.pad = tf.keras.layers.ZeroPadding2D(((1, 0), (1, 1)))\r\n    self.twidth = 2\r\n    self.kernel_size = (self.twidth, 3)\r\n    for i in range(self.depth):\r\n      dil = 2**i\r\n      pad_length = self.twidth + (dil-1)*(self.twidth-1)-1\r\n      setattr(self, 'pad{}'.format(i+1), tf.keras.layers.ZeroPadding2D(((pad_length, 0), (1, 1))))\r\n      setattr(self, 'conv{}'.format(i+1), tf.keras.layers.Conv2D(filters=self.in_channels, kernel_size=self.kernel_size, dilation_rate=(dil, 1)))\r\n      setattr(self, 'norm{}'.format(i+1), tf.keras.layers.LayerNormalization())\r\n      setattr(self, 'prelu{}'.format(i+1), tf.keras.layers.PReLU(shared_axes=[1, 2]))\r\n\r\n  def call(self, input_tensor):\r\n    skip = input_tensor\r\n    for i in range(self.depth):\r\n      print('Dilation rate', 2**i)\r\n      x = getattr(self, 'pad{}'.format(i+1))(skip)\r\n      print(x.shape)\r\n      x = getattr(self, 'conv{}'.format(i+1))(x)\r\n      print(x.shape)\r\n      x = getattr(self, 'norm{}'.format(i+1))(x)\r\n      x = getattr(self, 'prelu{}'.format(i+1))(x)\r\n      skip = tf.concat((x, skip), axis=3)\r\n    return x\r\n\r\ninput = tf.keras.layers.Input(shape=(None, 512, 64))\r\nx = DenseBlock(512, 5, 64)(input)\r\n```\r\n\r\n**Describe the expected behavior**\r\nI believe that the behaviour for dilation rate > 1 should match that of the case where dilation rate = 1.\r\n\r\n**Standalone code to reproduce the issue**\r\nHere is a [link](https://colab.research.google.com/drive/11wATlQbeahUHQE63RFsMFl6EV598j1sm?usp=sharing) to a colab notebook that reproduces the issue \r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nThe cell output is:\r\n```\r\n2.3.0\r\nDilation rate 1\r\n(None, None, 514, 64)\r\n(None, None, 512, 64)\r\nDilation rate 2\r\n(None, None, 514, 128)\r\n(None, None, None, 64)\r\nDilation rate 4\r\n(None, None, 514, 192)\r\n(None, None, None, 64)\r\nDilation rate 8\r\n(None, None, 514, 256)\r\n(None, None, None, 64)\r\nDilation rate 16\r\n(None, None, 514, 320)\r\n(None, None, None, 64)\r\n```\r\n", "comments": ["Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/8dd49ffe9df3f1551d4f3f7a09941b7f/44092.ipynb) and [TF-nightly](dilation_rate). Please find the attached gist. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44092\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44092\">No</a>\n"]}, {"number": 44091, "title": "ValueError: Failed to parse the model: pybind11::init(): factory function returned nullptr", "body": "### System information\r\n-   **OS Platform and Distribution  - Linux Ubuntu 20.04 LTS, 64bit**:\r\n-   **TensorFlow installed from conda**:\r\n-   **TensorFlow version 2.2.0**:\r\n-   **Python version 3.8.5**:\r\n-   **GCC/Compiler version 9.3.0**:\r\n-   **CUDA/cuDNN version 11.0**:\r\n-   **GPU model and memory GTX 1080, 12GB**:\r\n\r\n\r\n**Command to export model**\r\n```\r\npython export_tflite_graph_tf2.py --pipeline_config_path trening_demo/pre-trained-models/ssd_resnet50_v1_fpn_1024x1024_coco17_tpu-8/pipeline.config \\ \r\n--trained_checkpoint_dir trening_demo/models/my_ssd_resnet50_v1_fpn_1024x1024_coco17_tpu-8/ \\\r\n--output_directory trening_demo/exported-models-tflite/ex_my_ssd_resnet50_v1_fpn_1024x1024_coco17_tpu-8\r\n```\r\n\r\n**Command used to run the converter**\r\n\r\n```\r\nimport tensorflow as tf\r\nimport importlib.util\r\nimport multiprocessing\r\n\r\ndef representative_data_gen():\r\n  dataset_list = tf.data.Dataset.list_files ('/content/model_temp/ed/*')\r\n  for i in range(100):\r\n    image = next(iter(dataset_list))\r\n    image = tf.io.read_file(image)\r\n    image = tf.io.decode_jpeg(image, channels=3)\r\n    image = tf.image.resize(image, [1024, 1024])\r\n    image = tf.cast(image / 255., tf.float32)\r\n    image = tf.expand_dims(image, 0)\r\n    yield [image]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('/content/model_temp/exported-models-tflite/ex_my_ssd_resnet50_v1_fpn_1024x1024_coco17_tpu-8/saved_model')\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nconverter.representative_dataset = representative_data_gen\r\n\r\n\r\ntflite_model = converter.convert()\r\nwith open('/content/model_temp/c-tflite/model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n\r\nlen(tflite_model)\r\n```\r\nIn the above function I am using **decode_jpeg** however, my photos have the extension *** .jpg.** I assume it doesn't matter\r\n\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/calibrator.py in __init__(self, model_content)\r\n     51       self._calibrator = (\r\n---> 52           _calibration_wrapper.CalibrationWrapper(model_content))\r\n     53     except Exception as e:\r\n\r\nTypeError: pybind11::init(): factory function returned nullptr\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n4 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/calibrator.py in __init__(self, model_content)\r\n     52           _calibration_wrapper.CalibrationWrapper(model_content))\r\n     53     except Exception as e:\r\n---> 54       raise ValueError(\"Failed to parse the model: %s.\" % e)\r\n     55     if not self._calibrator:\r\n     56       raise ValueError(\"Failed to parse the model.\")\r\n\r\nValueError: Failed to parse the model: pybind11::init(): factory function returned nullptr.\r\n```\r\n\r\n**Link to the saved model** -- ex_my_ssd_resnet50_v1_fpn_1024x1024_coco17_tpu-8\r\n[https://drive.google.com/drive/folders/1Hz4BdMw2xg5t1rRMhl0smiz17GnL-Pz5?usp=sharing](url)\r\n\r\nI read that this may be a problem with the TensorRT version. Right? I use google colab.\r\n", "comments": ["@Rariusz,\r\nI do not have access to the link you have provided. Could you please provide the required permissions to view the files. \r\n\r\nAlso in order to reproduce the issue reported here, please provide the contents of the directory `/content/model_temp/ed/`. Thanks!", "@amahendrakar Thank you for your answers !\r\nLink for model (ex_my_ssd_resnet50_v1_fpn_1024x1024_coco17_tpu-8) that I need to convert to TFLite:\r\nhttps://drive.google.com/drive/folders/1Hz4BdMw2xg5t1rRMhl0smiz17GnL-Pz5?usp=sharing\r\n\r\nContents of the directory ```/content/model_temp/ed/ ``` \r\nhttps://drive.google.com/drive/folders/1y5AIh86wdA8ZoVxQBuOSh8ZbGGdAmIXl?usp=sharing\r\n\r\nThese are sample photos. Currently, I am mainly interested in solving the problem of converting the model to tflite.\r\n\r\nThank you for your help and time !!!", "@Rariusz,\r\nI was able to run the code without any issues using TF v2.2, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/cba1317f936aafe5590119f410dd6791/44091-2-2.ipynb). Thanks!", "Was able to reproduce the error with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/c00131d159d591ade4e8290c7ae5fee8/44091.ipynb#scrollTo=IVtxr4FUI5vo).\r\n\r\nHowever, on running the code with [TF-nightly](https://colab.research.google.com/gist/amahendrakar/394d9b7ba2fb7d04b55deb0d99e405da/44091-tf-nightly.ipynb#scrollTo=IVtxr4FUI5vo), the error changes to `ValueError: Model input is not quantized`. Please find the attached gist. Thanks!", "@amahendrakar \r\nThank you for your help and time !!!. I run code with the solution was to install tensorflow==2.2 as You propose, **but, now I'm back to my starting point.** Please see this issue: https://github.com/tensorflow/tensorflow/issues/43959\r\n\r\nAs You will see the problem is that model is reduced to 432 bytes and that creates a problem. Thanks!\r\n\r\n\r\n\r\n", "@amahendrakar \r\nFollowing my previous comment, if you have an idea of how to solve my problem or a suggestion that I can use, I will be very grateful!.\r\n", "@Rariusz What happens if you don't perform any optimizations on the model and convert without quantization?", "@ymodak Following this issue https://github.com/tensorflow/tensorflow/issues/44561  without quantization everything work. If I run the detector for ```ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8``` the result is \r\n![prediction](https://user-images.githubusercontent.com/28406311/98166790-44c5ed00-1ee8-11eb-9565-b16ddef58339.jpg)\r\n\r\nAfter running this command:\r\n```\r\n!python object_detection/export_tflite_graph_tf2.py \\\r\n    --pipeline_config_path \"/content/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/pipeline.config\" \\\r\n    --trained_checkpoint_dir \"/content/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint\"\\\r\n    --output_directory \"/content/tflite_graph\"\r\n``` \r\nThe results is the same.  Now I will try to run this model on google coral. Then I will try to run this model for my data set. But there will be problems for sure...I wonder so far why the image has a different color in the result ..\r\n\r\n"]}, {"number": 44089, "title": "Small style changes and fixes", "body": "Fixed some typos and added some info.", "comments": ["Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/44089\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n See visual diffs & provide feedback on Jupyter Notebooks. \n\n---\n\n <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44089) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "@lu-wang-g could you review this?", "Reassigned to Yuqi as she should know this better."]}, {"number": 44088, "title": "Add Linaro Aarch64 builds to community", "body": "As per - https://github.com/tensorflow/build/issues/15 and discussion in SigBuild.\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44088) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "@angerson Hello, just wondering if there's an update about this pull request for the Community builds please", "@neuralmimicry Thanks for the ping. Sorry this got lost!\r\n\r\nThis looks good to me."]}, {"number": 44087, "title": "<tf.Operation 'PrintV2_2' type=PrintV2>", "body": "i can NOT run this code!\r\nwhat is the solution?\r\n```\r\n@tf.function\r\ndef squaredDiff(x,y):\r\n    linearRegressionModel(x)\r\n    return tf.square(LinearRegressionModel -y )\r\n\r\n@tf.function\r\ndef totalLoss(x,y):\r\n    return tf.reduce_sum(squaredDiff(x,y))\r\n\r\ntf.print(totalLoss([0,1,2,3,4,5] , [9,7,5,3,1,-1]))\r\n```\r\nthe output:\r\n`<tf.Operation 'PrintV2_2' type=PrintV2>`\r\n", "comments": ["You need to enable eager execution. This is printing the tensor op.\r\n\r\nhttps://www.tensorflow.org/guide/eager", "@StudentUsingGithup \r\n\r\nI have tried in colab with TF version 2.3 and i am seeing the error message (`NameError: name 'linearRegressionModel' is not defined`).\r\nPlease, share the complete code snippet with supporting files to reproduce the issue in our environment.\r\n\r\nAs @maximus12793  suggested, please enable eager execution and see if the issue still persists. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44087\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44087\">No</a>\n"]}, {"number": 44086, "title": "Adapted MLIR-BufferPlacement to the latest MLIR-API-changes.", "body": "Due to the refactoring of [BufferPlacement](https://reviews.llvm.org/D87756) into two parts, this companion PR fixes the issues on the Tensorflow side.", "comments": ["This has landed as part of an llvm version change."]}, {"number": 44085, "title": "ImportError: libcuda.so.1: cannot open shared object file: No such file or directory", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Scientific Linux 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary):  pip install tensorflow-gpu==1.13.1\r\n- TensorFlow version:Desktop\r\n- Python version:3.6.2\r\n- Installed using virtualenv? pip? conda?: virtualenv and pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nI have set up a virtual environment, to run ImageAI on a custom detection task (https://github.com/OlafenwaMoses/ImageAI/blob/master/imageai/Detection/Custom/CUSTOMDETECTIONTRAINING.md) . ImageAI requires now outdated packages, that i have installed in the virtualenv.\r\n\r\npip install tensorflow-gpu==1.13.1 for gpu\r\n or pip install tensorflow==1.13.1 for cpu\r\npip install opencv-python\r\npip install keras==2.2.4\r\npip install numpy==1.16.1\r\npip install imageai --upgrade\r\n\r\nmodule load cuda/9.0\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\ni have installed the packages with the version recommended.\r\n\r\nthe code itself its just the one from the webpage to retrain yolov3 over a test dataset\r\n\r\nfrom imageai.Detection.Custom import DetectionModelTrainer\r\nimport os\r\n\r\nINPUT_DIR = \"/zhome/94/5/101974/data/hololens/\"\r\n\r\ntrainer = DetectionModelTrainer()\r\ntrainer.setModelTypeAsYOLOv3()\r\ntrainer.setDataDirectory(data_directory=INPUT_DIR)\r\ntrainer.setTrainConfig(object_names_array=[\"hololens\"], batch_size=4, num_experiments=10, train_from_pretrained_model=\"pretrained-yolov3.h5\", show_network_summary=True)\r\ntrainer.trainModel()\r\n\r\n\"\"\"## evaluate performance\"\"\"\r\ntrainer = DetectionModelTrainer()\r\ntrainer.setModelTypeAsYOLOv3()\r\ntrainer.setDataDirectory(data_directory=INPUT_DIR)\r\nmetrics = trainer.evaluateModel(model_path=INPUT_DIR+\"models\", json_path=\"hololens/json/detection_config.json\", iou_threshold=0.5, object_threshold=0.3, nms_threshold=0.5)\r\nprint(metrics)\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib64/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib64/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcuda.so.1: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"retraining_YOLO_new_data.py\", line 22, in <module>\r\n    from imageai.Detection.Custom import DetectionModelTrainer\r\n  File \"/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/imageai/Detection/__init__.py\", line 2, in <module>\r\n    from imageai.Detection.keras_retinanet.models.resnet import resnet50_retinanet\r\n  File \"/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/imageai/Detection/keras_retinanet/models/resnet.py\", line 19, in <module>\r\n    import keras\r\n  File \"/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/keras/__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/keras/utils/__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n  File \"/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/keras/utils/conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n  File \"/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/keras/backend/__init__.py\", line 89, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  File \"/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib64/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib64/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcuda.so.1: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "Hello again, \r\n i really would love to use colab, but after last deeplearning project i got my quota of gpu very much limited. Also its quite unstable, so it finds a device once and 9 times not. \r\n\r\nPlus I am connected to a linux remote server of my uni, which is used by students to run on gpu, so i really cant believe the hardware they are using is not up to the task. \r\n\r\n i changed from CUDA9.0 to CUDA10.0 which is the one compatible with TF 1.13.1 (that i need to use).\r\nI added the installation directory of cuda10 to the LD library path.\r\n\r\nI cant find anywhere the CUPTI and cuDNN directories (i guess as a user and not adm i dont have visibilities on these? )\r\n\r\nI uninstalled TF1.13.1 and TF-GPU 1.13.1, uninstalled protobuf, reinstalled TF and TF-GPU. \r\n\r\nThe error persists, now its just with a different version:\r\n----------------------------------------------------------------------------------------------------------------------------------------\r\nsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib64/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib64/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"retraining_YOLO_new_data.py\", line 22, in <module>\r\n    from imageai.Detection.Custom import DetectionModelTrainer\r\n  File \"/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/imageai/Detection/__init__.py\", line 2, in <module>\r\n    from imageai.Detection.keras_retinanet.models.resnet import resnet50_retinanet\r\n  File \"/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/imageai/Detection/keras_retinanet/models/resnet.py\", line 19, in <module>\r\n    import keras\r\n  File \"/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/keras/__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/keras/utils/__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n  File \"/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/keras/utils/conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n  File \"/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/keras/backend/__init__.py\", line 89, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  File \"/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib64/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib64/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\n**ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory**\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\n", "@arita89,\r\nTensorFlow 1.x is not actively supported. Could you please update TensorFlow to v2.3 and check if you are facing the same issue. Thanks!", "well i can try, just the package that i am trying to make work (imageAI) was updated lastly in Dec2019 and goes with these versions of tensorflow and keras. \r\n\r\nif you tell me that there is no way to resolve this issue that i can just try my way with the latest packages and probably a lot of more debugging ", "@arita89,\r\nCould you please uninstall all the packages and check if a fresh installation of CUDA 10 with cuDNN 7.4 works? Thanks!", "hei, i basically updated the package to tf 2.3 and now it works, maybe minor error messages here and there, but trains and all.\r\nthanks for your support, in times of panic its something valuable :) ", "@arita89,\r\nHappy to help :)\r\n\r\nMarking the issue as closed since it is resolved. Please feel free to re-open if necessary. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44085\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44085\">No</a>\n"]}, {"number": 44084, "title": "Add SelectV2 to list of format agnostic ops", "body": "Since 7ea9c329266e365f667cfb75965543c08bac7476 grappler treats `Select` and `SelectV2` the same.\r\nThis PR adds `SelectV2` to `GetOpsFormatAgnostic` in order to also support [layout optimizations](https://github.com/tensorflow/tensorflow/blob/dc843fad65fb4eb1b173a7577d0d932b2af94951/tensorflow/core/grappler/optimizers/layout_optimizer.cc#L2142-L2143) . I can't think of a reason why this pass shouldn't be  enabled for `SelectV2`, but let me know if I am missing something here.", "comments": []}, {"number": 44083, "title": "Training BERT model gives Error :  \"[_Derived_]RecvAsync is cancelled. [[{{node loss_3/mul}}]]\"", "body": "**Describe the current behaviour**\r\nError: \r\n```\r\n Epoch 1/10\r\n ---------------------------------------------------------------------------\r\n CancelledError                            Traceback (most recent call last)\r\n<ipython-input-16-402ab8549e42> in <module>\r\n----> 1 model.fit(trainGen, epochs = 10, steps_per_epoch = trainGen.getLen(),verbose = 1)\r\n\r\nD:\\Anaconda\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    726         max_queue_size=max_queue_size,\r\n    727         workers=workers,\r\n--> 728         use_multiprocessing=use_multiprocessing)\r\n    729 \r\n    730   def evaluate(self,\r\n\r\nD:\\Anaconda\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_generator.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n    601         shuffle=shuffle,\r\n    602         initial_epoch=initial_epoch,\r\n--> 603         steps_name='steps_per_epoch')\r\n    604 \r\n    605   def evaluate(self,\r\n\r\nD:\\Anaconda\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\r\n    263 \r\n    264       is_deferred = not model._is_compiled\r\n--> 265       batch_outs = batch_function(*batch_data)\r\n    266       if not isinstance(batch_outs, list):\r\n    267         batch_outs = [batch_outs]\r\n\r\nD:\\Anaconda\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)\r\n   1016       self._update_sample_weight_modes(sample_weights=sample_weights)\r\n   1017       self._make_train_function()\r\n-> 1018       outputs = self.train_function(ins)  # pylint: disable=not-callable\r\n   1019 \r\n   1020     if reset_metrics:\r\n\r\nD:\\Anaconda\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py in __call__(self, inputs)\r\n   3578 \r\n   3579     fetched = self._callable_fn(*array_vals,\r\n-> 3580                                 run_metadata=self.run_metadata)\r\n   3581     self._call_fetch_callbacks(fetched[-len(self._fetches):])\r\n   3582     output_structure = nest.pack_sequence_as(\r\n\r\nD:\\Anaconda\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py in __call__(self, *args, **kwargs)\r\n   1470         ret = tf_session.TF_SessionRunCallable(self._session._session,\r\n   1471                                                self._handle, args,\r\n-> 1472                                                run_metadata_ptr)\r\n   1473         if run_metadata:\r\n   1474           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\nCancelledError: [_Derived_]RecvAsync is cancelled.\r\n\t [[{{node loss_3/mul}}]]\r\n\r\n```\r\n\r\n**Describe the expected behavior**\r\nI am new to NLP with BERT but I need to train this model using BERT. I have used the pre-trained BERT using tensorflow hub but getting errors.\r\n\r\n**Standalone code to reproduce the issue**\r\nModel \r\n`   \r\n\r\n    def createModel_YesNo(vocab_size, batchSize, maxlen):\r\n    bLayer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\", trainable=True)\r\n    \r\n    #Document\r\n    input_ids_Document = tf.keras.layers.Input(shape = (maxlen, ), dtype = tf.int32)\r\n    token_type_ids_Document = tf.keras.layers.Input(shape=(maxlen,), dtype=tf.int32)\r\n    attention_mask_Document = tf.keras.layers.Input(shape=(maxlen,), dtype=tf.int32)\r\n    \r\n    bertInputs_Document = [input_ids_Document, token_type_ids_Document, attention_mask_Document]\r\n    \r\n    #bertOutput_Document = bLayer(n_fine_tune_layers = 3)(bertInputs_Document)\r\n    bertOutput_Document, _ = bLayer(bertInputs_Document)\r\n    \r\n    #Question\r\n    input_ids_Question = tf.keras.layers.Input(shape = (maxlen, ), dtype = tf.int32)\r\n    token_type_ids_Question = tf.keras.layers.Input(shape=(maxlen,), dtype=tf.int32)\r\n    attention_mask_Question = tf.keras.layers.Input(shape=(maxlen,), dtype=tf.int32)\r\n    \r\n    bertInputs_Question = [input_ids_Question, token_type_ids_Question, attention_mask_Question]\r\n    \r\n    #bertInputs_Question = bLayer(n_fine_tune_layers = 3)(bertInputs_Question)\r\n    bertInputs_Question, _ = bLayer(bertInputs_Question)\r\n    \r\n    #Concat Layer\r\n    concat = tf.keras.layers.Concatenate()([bertOutput_Document, bertInputs_Question])\r\n    \r\n    denseLayer = tf.keras.layers.Dense(128, activation = 'relu')(concat)\r\n    denseLayer = tf.keras.layers.Dense(256, activation = 'relu')(denseLayer)\r\n    denseLayer = tf.keras.layers.Dense(256, activation = 'relu')(denseLayer)\r\n    denseLayer = tf.keras.layers.Dense(128, activation = 'relu')(denseLayer)\r\n    denseLayer = tf.keras.layers.Flatten()(denseLayer)\r\n    denseLayer = tf.keras.layers.Dense(2)(denseLayer)\r\n    \r\n    model = tf.keras.Model(inputs = [input_ids_Document, token_type_ids_Document, attention_mask_Document, input_ids_Question, token_type_ids_Question, attention_mask_Question],\r\n                           outputs = [denseLayer])\r\n    return model\r\n    `\r\nGenerator\r\n\r\n```\r\n\r\n    class trainGenSeq_short_YesNo(tf.keras.utils.Sequence, ):\r\n    def __init__(self, batchSize, sentenceLength):\r\n        self.batchSize = batchSize\r\n        self.trainFiles = os.listdir('D:/Python/Datasets/v1.0/train/')\r\n        self.trainingSamples = 307372 * 2\r\n        self.sentenceLength = sentenceLength\r\n        \r\n        #Load Vocab\r\n        slow_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\r\n        save_path = \"bert_base_uncased/\"\r\n        if not os.path.exists(save_path):\r\n            os.makedirs(save_path)\r\n        slow_tokenizer.save_pretrained(save_path)\r\n        self.tokenizer = BertTokenizer('vocab.txt', lowercase = True)\r\n        self.vocabSize = len(self.tokenizer.vocab)\r\n        \r\n    \r\n    def __len__(self):\r\n        return int(self.trainingSamples // self.batchSize)\r\n    \r\n    def getLen(self):\r\n        return int(self.trainingSamples // self.batchSize)\r\n    \r\n    def attentionMasks(self,input_dims):\r\n        return [int(id > 0) for id in input_dims]\r\n        \r\n    def inputDims(self, dims):\r\n        return pad_sequences([dims], maxlen = self.sentenceLength, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")[0]\r\n    \r\n    def encode_sentence(self, sentence):\r\n        sentence = sent_tokenize(sentence)\r\n        ans = []\r\n        for i in range(len(sentence)):\r\n            encode_sent = self.tokenizer.encode(sentence[i],add_special_tokens = True)\r\n            ans += encode_sent\r\n\r\n        ans = pad_sequences([ans], maxlen = self.sentenceLength, dtype = \"long\", value = 0, truncating = \"post\", padding = \"post\")\r\n        return ans[0]\r\n    \r\n    def __getitem__(self, _):\r\n        documentStack = np.array([])\r\n        questionStack = np.array([])\r\n        answerStack = np.array([])\r\n        \r\n        document_AttStack = np.array([])\r\n        question_AttStack = np.array([])\r\n\r\n        document_SegStack = np.array([])\r\n        question_SegStack = np.array([])\r\n\r\n        First = True\r\n        \r\n        for file in self.trainFiles:\r\n            for line in open('D:/Python/Datasets/v1.0/train/' + file):\r\n                file = json.loads(line)\r\n                #annotations\r\n                if file.get('annotations')[0].get('short_answers'):\r\n                    s_Start = file.get('annotations')[0].get('short_answers')[0].get('start_token')\r\n                    s_End = file.get('annotations')[0].get('short_answers')[0].get('end_token')\r\n                    l_Start = file.get('annotations')[0].get('long_answer').get('start_token')\r\n                    l_End = file.get('annotations')[0].get('long_answer').get('end_token')\r\n\r\n                    #Question and Title\r\n                    question = file.get('question_text')\r\n\r\n                    #document\r\n                    document = []\r\n                    for indexs in file.get('document_tokens')[l_Start:l_End]:\r\n                        if indexs.get('html_token') == False:\r\n                            document.append(indexs.get('token'))\r\n                    \r\n                    #Fake Document OR No document\r\n                    fake = []\r\n                    randomNumber = random.randint(7500, 9000)\r\n                    front = random.choice([True, False])\r\n                    \r\n                    if front:\r\n                        try:\r\n                            for indexs in range(max(0, l_Start - randomNumber), min(len(file.get('document_tokens')),l_End - randomNumber)):\r\n                                if file.get('document_tokens')[indexs].get('html_token') == False:\r\n                                    fake.append(file.get('document_tokens')[indexs].get('token'))\r\n                                else:\r\n                                    indexs -= 1\r\n                        except:\r\n                            for indexs in range(max(0, l_Start + randomNumber), min(len(file.get('document_tokens')),l_End + randomNumber)):\r\n                                if file.get('document_tokens')[indexs].get('html_token') == False:\r\n                                    fake.append(file.get('document_tokens')[indexs].get('token'))\r\n                                else:\r\n                                    indexs -= 1\r\n                    else:\r\n                        try:\r\n                            for indexs in range(max(0, l_Start + randomNumber), min(len(file.get('document_tokens')),l_End + randomNumber)):\r\n                                if file.get('document_tokens')[indexs].get('html_token') == False:\r\n                                    fake.append(file.get('document_tokens')[indexs].get('token'))\r\n                                else:\r\n                                    indexs -= 1\r\n                        except:\r\n                            for indexs in range(max(0, l_Start - randomNumber), min(len(file.get('document_tokens')),l_End - randomNumber)):\r\n                                if file.get('document_tokens')[indexs].get('html_token') == False:\r\n                                    fake.append(file.get('document_tokens')[indexs].get('token'))\r\n                                else:\r\n                                    indexs -= 1\r\n                    \r\n                    document = ' '.join(document)\r\n                    fake = ' '.join(document)\r\n\r\n                    document = self.encode_sentence(document)\r\n                    fake = self.encode_sentence(fake)\r\n                    question = self.encode_sentence(question)\r\n                    \r\n                    fake_AttentionMask = self.attentionMasks(fake)\r\n                    document_AttentionMask = self.attentionMasks(document)\r\n                    question_AttentionMask = self.attentionMasks(question)\r\n                    \r\n                    fake_SegID = [0 for _ in range(len(fake))]\r\n                    document_SegID = [0 for _ in range(len(document))]\r\n                    question_SegID = [0 for _ in range(len(question))]\r\n\r\n                    if First:\r\n                        #Document\r\n                        documentStack = np.array([document])\r\n                        documentStack = np.append(documentStack, np.array([fake]), axis = 0)\r\n                        document_AttStack = np.array([document_AttentionMask])\r\n                        document_AttStack = np.append(document_AttStack, np.array([fake_AttentionMask]), axis = 0)\r\n                        document_SegStack = np.array([document_SegID])\r\n                        document_SegStack = np.append(document_SegStack, np.array([fake_AttentionMask]), axis = 0)\r\n                        \r\n                        #Add Question Again\r\n                        questionStack = np.array([question])\r\n                        questionStack = np.append(questionStack, np.array([question]), axis = 0)\r\n                        \r\n                        question_AttStack = np.array([question_AttentionMask])\r\n                        question_AttStack = np.append(question_AttStack, np.array([question_AttentionMask]), axis = 0)\r\n                        question_SegStack = np.array([question_SegID])\r\n                        question_SegStack = np.append(question_SegStack, np.array([question_SegID]), axis = 0)\r\n                        \r\n                        #Add Answer\r\n                        answerStack = np.array([np.array([1,0])])\r\n                        answerStack = np.append(answerStack, np.array([np.array([0,1])]), axis = 0)\r\n                        \r\n                        First = False\r\n                    else:\r\n                        documentStack = np.append(documentStack, np.array([document]), axis = 0)\r\n                        documentStack = np.append(documentStack, np.array([fake]), axis = 0)\r\n                        questionStack = np.append(questionStack, np.array([question]), axis = 0)\r\n                        questionStack = np.append(questionStack, np.array([question]), axis = 0)\r\n                        answerStack = np.append(answerStack, np.array([np.array([1,0])]), axis = 0)\r\n                        answerStack = np.append(answerStack, np.array([np.array([0,1])]), axis = 0)\r\n                        \r\n                        #Attention Mask\r\n                        document_AttStack = np.append(document_AttStack, np.array([document_AttentionMask]), axis = 0)\r\n                        document_AttStack = np.append(document_AttStack, np.array([fake_AttentionMask]), axis = 0)\r\n                        question_AttStack = np.append(question_AttStack, np.array([question_AttentionMask]), axis = 0)\r\n                        question_AttStack = np.append(question_AttStack, np.array([question_AttentionMask]), axis = 0)\r\n                        \r\n                        #SegmentIDs\r\n                        document_SegStack = np.append(document_SegStack, np.array([document_AttentionMask]), axis = 0)\r\n                        document_SegStack = np.append(document_SegStack, np.array([fake_AttentionMask]), axis = 0)\r\n                        question_SegStack = np.append(question_SegStack, np.array([question_SegID]), axis = 0)\r\n                        question_SegStack = np.append(question_SegStack, np.array([question_SegID]), axis = 0)\r\n                \r\n                if documentStack.shape[0] == self.batchSize:\r\n                    documentStack = np.reshape(documentStack, (documentStack.shape[0], 1, documentStack.shape[1]))\r\n                    questionStack = np.reshape(questionStack, (questionStack.shape[0], 1, questionStack.shape[1]))\r\n                    answerStack = np.reshape(answerStack, (answerStack.shape[0], 1, answerStack.shape[1]))\r\n                    First = True\r\n\r\n                    #print(type(documentStack), type(questionStack), type(answerStack))\r\n                    return [np.squeeze(documentStack), np.squeeze(document_AttStack), np.squeeze(document_SegStack), \r\n                            np.squeeze(questionStack), np.squeeze(question_AttStack), np.squeeze(question_SegStack)], np.squeeze(answerStack)\r\n                    \r\n                    documentStack = None\r\n                    titleStack = None\r\n                    questionStack = None\r\n                    answerStack = None\r\n\r\ntrainGen = trainGenSeq_short_YesNo(BatchSize, SeqLength)\r\n```\r\n\r\n\r\n\r\nFitting and Compiling\r\n\r\n\r\n```\r\nmodel = createModel_YesNo(trainGen.vocabSize, BatchSize, SeqLength)\r\nmodel.summary()\r\nmodel.compile(optimizer = 'adam', loss = 'categorical_crossentropy')\r\nmodel.fit(trainGen, epochs = 10, steps_per_epoch = trainGen.getLen(),verbose = 1)\r\n```\r\n\r\n\r\n\r\nTF Version = 2.0.0\r\nCUDA = 10.0\r\nCUDDN = 7.6.5\r\nGPU = GTX 1060\r\nOperating System: Windows 10\r\n\r\n**Other info / logs** \r\nI have tried turning on GPU growth that did not help.\r\n\r\n", "comments": ["@NiksanJP \r\nThere are to many indentation issues to replicate the code, please share a colab gist if possible with the error reported.", "> @NiksanJP\r\n> There are to many indentation issues to replicate the code, please share a colab gist if possible with the error reported.\r\n\r\nhttps://colab.research.google.com/drive/1V58QbrI1PQikHRizxiBpQeNcQJYOhp3j?usp=sharing", "@NiksanJP \r\nPlease share \"/Python/Datasets/v1.0/train/\" file for us to replicate.", "Thanks for the comment, the file is about 42gb big. The file is from Google natural questions dataset which can be found online, but I have checked the generator it seems to run normally without any flaw. The main reason for error is the model.\n\nThe generator will read about 10000 word paragraph and turn to tokens, attention mask and segment masks thats it.", "use this [link](https://ai.google.com/research/NaturalQuestions/).\r\n\r\nThe top div has:\r\n\r\n- dev\r\n- sample\r\n- train\r\n\r\nOn train there are 50 files of about 1 GB of name nq-train-00.jsonl.\r\n\r\n", "@gowthamkpr anything u can help me with?", "> @gowthamkpr anything u can help me with?\r\n\r\nHello?", "@NiksanJP Please post your issue [here](https://github.com/google-research/bert/issues) as this issue is related to Bert. Also please post this issue in stackoverflow as I think this might be a user generated issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44082, "title": "Input shape for convolution1D ", "body": "PI have sparse coefficients array of shape 1D. Each value in the 1D array corresponds to one class. For example, I just have an array\r\na = [1,2,3,4,4,5,6,78,8543,35,878,.............]\r\nclass_label = [1,1,1,2,2,3,3,1,2,......]\r\n\r\nHow should I input this to Convolution1D? It always gives me an error of input dimensions.", "comments": ["@gharshini \r\n\r\nThis is the duplicate of #44065.Please, close the issue here and can track the issue in #44065. Thanks!\r\n\r\n\r\n\r\n\r\n"]}, {"number": 44081, "title": "Add Person Detection demo for Zephyr RISC-V", "body": "This PR ports the Person Detection demo to the Zephyr RISC-V platform.\r\n\r\nIt has been tested on LiteX SoC with VexRiscv core running on Arty A7 FPGA development board with the Arducam Mini 2MP Plus camera module. Details on building and runinng the demo are included in README.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@petewarden, could you review this?", "I'm going to close this PR for now. We will revisit this work as we take a second look at how we want to support TFLM examples across multiple platforms."]}, {"number": 44080, "title": "Replace dockerized Renode with portable one", "body": "This PR:\r\n* adds script for downloading portable version of Renode\r\n* replaces dockerized Renode with portable one\r\n* updates robot tests to work with latest version of Renode\r\n* adds installation of additional dependencies to `Dockerfile.micro`\r\n* adds docs/renode.md as a central location for all TFLM+Renode documentation.\r\n\r\nRunning all the tests with the following command takes around 5 mins on a i7-7820HQ. Running Renode tests as part of CI is disabled while we figure out how to reduce this time.\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=bluepill build\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=bluepill test\r\n```\r\n\r\nAddtionally, only the Bluepill platform is modified. Other renode platforms in this repo (STM32F4) will be updated once the use of portable Renode for TFLM is turned on as part of CI.\r\n\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44080) for more info**.\n\n<!-- need_sender_cla -->", "@JakubJatczak Can you please sign CLA. Thanks!", "@googlebot I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44080) for more info**.\n\n<!-- need_author_cla -->", "Merged your PR and reverted changes as requested. \r\nI allowed myself to made small change in the `renode.md`, now it uses `renode/tests/requirements.txt` file instead of installing requirements explicitly."]}, {"number": 44079, "title": "Getting errors when converting a simple convs model to INT8 TFLite", "body": "**System information**\r\n- OS Platform and Distribution: **Ubuntu 18.04**\r\n- TensorFlow installed from binary\r\n- TensorFlow version: **tf-nightly (2.4.0-dev20201015)** and **tf-2.3.0**\r\n- TensorFlow Model Optimization version: **0.5.0**\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\nFP32 TFLite is work, but INT8 TFLite is not work with tf-nightly.\r\n\r\nPlease check the [gist](https://colab.research.google.com/drive/14glMvUN1H2PZHB2VPOwX9sZX9E2RNu0J?usp=sharing) \\(tf-nightly\\).\r\n\r\n1. Can not convert quantization aware model to INT8 TFLite.\r\n\r\n2. `converter.inference_input_type = tf.int8` and `converter.inference_output_type = tf.int8` are not work.\r\n\r\n3. FP32 TFLite is work.\r\n\r\nItem 2 has been fixed in 2.3.0 \\(#36024\\), but still has another error.\r\n\r\nPlease check the [gist](https://colab.research.google.com/drive/1ejLyom6qeGJlxkk0jL62OF7ybrzzX3Wu?usp=sharing) \\(tf-2.3.0\\)\r\n\r\n**The output from the converter invocation**\r\n\r\nItem 1:\r\n\r\n```\r\nRuntimeError: tensorflow/lite/kernels/quantize.cc:113 affine_quantization->scale->size == 1 was not true.Node number 0 (QUANTIZE) failed to prepare.\r\n```\r\n\r\nItem 2:\r\n\r\n```\r\nValueError: The inference_input_type and inference_output_type must be tf.float32.\r\n```\r\n\r\n**Failure details**\r\n\r\nGet errors in tensorflow\r\n\r\nThanks in advance.", "comments": ["Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/c9fe5970e900520fadff7c515c18a13e/44079_2.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/26edd3e78722dfa86b4bccf93d21a8fc/44079_2-nightly.ipynb). Please find the attached gist. Thanks!", "**System information**\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from binary\r\n- TensorFlow version: tf-nightly (2.5.0.dev20201029)\r\n- TensorFlow Model Optimization version: 0.5.0.dev20201103 (installed from github)\r\n\r\n**Command used to run the converter or code**\r\n\r\nIt can convert model to INT8 TFLite successfully with latest tf-nightly,\r\n\r\nbut fail to convert quantized model with tensorflow_model_optimization.\r\n\r\nPlease check the [gist](https://colab.research.google.com/drive/1UeEnd2Hh3Xc2fLxuBEsy2G_qBRrP0hdN?usp=sharing).\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nRuntimeError: Quantization not yet supported for op: 'DEQUANTIZE'.\r\n```\r\n\r\nMight be similar to issue #42082\r\n\r\nThanks in advance.", "#42082", "Issue of the last comments was solved in #44882 \r\n@MeghnaNatraj , is there a rough estimate at which point `converter.inference_output_type = tf.int8` will work for quantization-aware models?", "@hhass It is supported in all 2.4 release candidates and the final yet-to-be released versions (2.4.0-rc0 .... 2.4). For details refer to this [comment](https://github.com/tensorflow/tensorflow/issues/42082#issue-673921045)\r\n\r\nThe changes were made in this [commit](https://github.com/tensorflow/tensorflow/commit/5832fa2) \r\n\r\n", "> **System information**\r\n> \r\n>     * OS Platform and Distribution: Ubuntu 18.04\r\n> \r\n>     * TensorFlow installed from binary\r\n> \r\n>     * TensorFlow version: tf-nightly (2.5.0.dev20201029)\r\n> \r\n>     * TensorFlow Model Optimization version: 0.5.0.dev20201103 (installed from github)\r\n> \r\n> \r\n> **Command used to run the converter or code**\r\n> \r\n> It can convert model to INT8 TFLite successfully with latest tf-nightly,\r\n> \r\n> but fail to convert quantized model with tensorflow_model_optimization.\r\n> \r\n> Please check the [gist](https://colab.research.google.com/drive/1UeEnd2Hh3Xc2fLxuBEsy2G_qBRrP0hdN?usp=sharing).\r\n> \r\n> **The output from the converter invocation**\r\n> \r\n> ```\r\n> RuntimeError: Quantization not yet supported for op: 'DEQUANTIZE'.\r\n> ```\r\n> \r\n> Might be similar to issue #42082\r\n> \r\n> Thanks in advance.\r\n\r\nClose the issue, because the error is duplicate of 42082.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44079\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44079\">No</a>\n", "Thanks for  MeghnaNatraj's [example](https://github.com/tensorflow/tensorflow/issues/42082#issuecomment-745541910).\r\n\r\nI can convert QAT models successfully. My [gist](https://colab.research.google.com/drive/1P4tnoZ4Sb9Q_uwRr2CxwcAtCM8rtzlFo?usp=sharing)"]}, {"number": 44078, "title": "Tensorflow lite object detection  andriod examples app run on my phone but nothing detection ", "body": "[GitHub Policy](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android),\r\n\r\n\r\n**System information**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: xiaomi mi 6x\r\n\r\n**Describe the current behavior**\r\nbuild is OK\r\nrun is OK \r\nbut  can not detection erverthing object : \r\n![WeChat Image_20201016154631](https://user-images.githubusercontent.com/36255328/96235722-b8ea3080-0fcd-11eb-96e6-d8a11b6bf88f.jpg)\r\n", "comments": ["@mingx9527 \r\nPlease open this issue in models repo.", "@Saduf2019  thank you response! \r\nI add the question in model repo:https://github.com/tensorflow/models/issues/9389 \r\nwaiting response! ", "@mingx9527 \r\nPlease move this issue to closed status as it is tracked at the models repo.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44077, "title": "Fix zephyr_riscv build process", "body": "This adds additional compiler flags for Zephyr applications\r\nin order to eliminate compilation warnings that are treated as errors.\r\n\r\nAdditionally, the `-fno-threadsafe-statics` switch can be applied to C++ files only.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 44076, "title": "Fix zephyr_riscv hello_world demo", "body": "This disables networking in the project's configuration,\r\nas the target platform does not support any network controller.", "comments": []}, {"number": 44075, "title": "How do we know the compatibility of flatbuffers version in SchemaGenerated.h ", "body": "After building the flatbuffers latest release and combining with the latest SchemaGenerated.h, there's an error as below\r\n\r\n> Error\t1\terror LNK2001: unresolved external symbol \"private: static class flatbuffers::ClassicLocale flatbuffers::ClassicLocale::instance_\" (?instance_@ClassicLocale@flatbuffers@@0V12@A)\r\n\r\nThis error is mostly due to incompatibility between flatbuffers version in my system and version used for SchemaGenerated.h\r\nFLatbuffers version used: 1.12.0\r\n\r\n\r\n", "comments": ["@tsindhuja \r\n\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nProvide the exact sequence of commands / steps that you executed before running into the problem.Thanks!", "1. Downloaded flatbuffers source code from https://github.com/google/flatbuffers/releases\r\n2. Built using CMAKE GUI on windows 10[By using configuration-x64]\r\n3. Opened the project in Visual Studio 13 and ran 'Build' for 'ALL BUILD' project for both Debug and Release\r\n4. Extracted SchemaGenerated.h from https://github.com/tensorflow/tensorflow/tree/ec59a624ac3c20c800ef7dfb759daf1e5bd11efa/tensorflow/lite/schema\r\n5. While trying to build my cpp project in Visual Studio 13 the above error occurred. \r\n\r\nNote: The respective libraries and include files were mentioned in the Project Properties\r\n", "Generally you can find the appropriate flatbuffer version in the [TensorFlow deps spec](https://github.com/tensorflow/tensorflow/blob/master/third_party/flatbuffers/workspace.bzl#L10). In this case, it should be 1.12. This is more likely an issue with your compiler and/or the CMake configuration.\r\n\r\nHave you looked at the experimental [CMake support](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt) for TensorFlow Lite? Is there a reason you're downloading and building the Flatbuffers library manually?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44075\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44075\">No</a>\n"]}, {"number": 44074, "title": "NaN occurs when building with GaussianNoise, GaussianNoise and Dense", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):a GNU/Linux system with Linux kernel 4.15.0 on 1 6-core 3.60GHz Intel Core CPU i7-6850K with 64 GB RAM equipped with a NVIDIA Corporation GP102 GPUs\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (use command below):tensorflow2.1.0-GPU\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI got the NaN as loss when I tried to train my model with GaussianNoise, GaussianNoise and Dense\r\n**Describe the expected behavior**\r\nThe loss should be a number\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\nbatch_size = 110\r\nepochs = 128\r\nnum_classes = 10\r\nimport os\r\nmodel_name = 'model.h5'\r\nimport tensorflow.keras as keras\r\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\nimg_rows, img_cols = x_train.shape[1], x_train.shape[2]\r\n\r\nx_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\nx_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\ninput_shape = (img_rows, img_cols, 1)\r\nx_train = x_train.astype('float32')\r\nx_test = x_test.astype('float32')\r\nx_train /= 255\r\nx_test /= 255\r\ny_train = keras.utils.to_categorical(y_train, num_classes)\r\ny_test = keras.utils.to_categorical(y_test, num_classes)\r\nmodel = keras.models.Sequential()\r\nmodel.add(keras.layers.GaussianNoise(stddev=0.7498748441096037))\r\n\r\nmodel.add(keras.layers.Flatten())\r\nmodel.add(keras.layers.Dense(num_classes, activation='relu'))\r\nmodel.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\r\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\r\nmodel.save(model_name)\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\nEpoch 1/128\r\n\r\n  110/60000 [..............................] - ETA: 43:48 - loss: 9.9702 - accuracy: 0.0636\r\n  660/60000 [..............................] - ETA: 7:18 - loss: 9.6680 - accuracy: 0.0848 \r\n 1100/60000 [..............................] - ETA: 4:24 - loss: 9.4182 - accuracy: 0.0991\r\n 1540/60000 [..............................] - ETA: 3:10 - loss: nan - accuracy: 0.1019   \r\n 1760/60000 [..............................] - ETA: 2:47 - loss: nan - accuracy: 0.1000\r\n 2200/60000 [>.............................] - ETA: 2:14 - loss: nan - accuracy: 0.0995\r\n 2530/60000 [>.............................] - ETA: 1:58 - loss: nan - accuracy: 0.0992\r\n 3080/60000 [>.............................] - ETA: 1:37 - loss: nan - accuracy: 0.1042\r\n 3630/60000 [>.............................] - ETA: 1:23 - loss: nan - accuracy: 0.1047\r\n```", "comments": ["Was able to reproduce the issue with TF v2.1, TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/518d78ca0ecfb6c09939051d94a751d9/44074.ipynb). Thanks!", "Hi @bugreporter450, I think this is because you're using `relu` as the activation on the output layer. Try this instead:\r\n`model.add(keras.layers.Dense(num_classes, activation='softmax'))`\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44074\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44074\">No</a>\n"]}, {"number": 44072, "title": "CUDNN_STATUS_INTERNAL_ERROR with MirroredStrategy under graph mode when using tf.summary", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 9\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below):2.3.1\r\n- Python version:3.7.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):6.3.0\r\n- CUDA/cuDNN version: CUDA Version: 10.1 \r\n- GPU model and memory: Tesla T4 * 2\r\n\r\n\r\n**Describe the current behavior**\r\n**When I remove the \u201ctf.summary.write(tag=\"prediction_\" + key, tensor=prediction_concat, step=batch)\u201d, it works well without error.**\r\n\r\n**But With \u201ctf.summary.write(tag=\"prediction_\" + key, tensor=prediction_concat, step=batch)\u201d.** It has the error: \r\ntensorflow.python.framework.errors_impl.UnknownError: 3 root error(s) found.\r\n  (0) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[{{node while/body/_17/while/StatefulPartitionedCall/replica_1/conv2d_1/Conv2D}}]]\r\n         [[DeleteIterator/_102]]\r\n  (1) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[{{node while/body/_17/while/StatefulPartitionedCall/replica_1/conv2d_1/Conv2D}}]]\r\n  (2) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[{{node while/body/_17/while/StatefulPartitionedCall/replica_1/conv2d_1/Conv2D}}]]\r\n         [[while/body/_17/while/StatefulPartitionedCall/replica_1/conv2d_1/BiasAdd/_109]]\r\n0 successful operations.\r\n\r\n\r\n**Describe the expected behavior**\r\nin the infer() function, the result from serving func will be written as tfevents by tf.summary.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf  \r\nfrom tensorflow.keras import layers  \r\n\r\nclass Evaluation:\r\n\r\n    def __init__(self, strategy=None):  \r\n        self.strategy = strategy\r\n        H, W, C = 10, 10, 3\r\n        imgs = tf.zeros([8, H, W, C])\r\n        self.dataset = tf.data.Dataset.from_tensor_slices(imgs)\r\n        self.dataset = self.dataset.batch(4)\r\n        self.dataset = self.strategy.experimental_distribute_dataset(self.dataset)\r\n        with self.strategy.scope():\r\n            self.conv1 = layers.Conv2D(32, (4, 4), strides=(2, 2), padding='same')\r\n            self.conv2 = layers.Conv2D(32, (4, 4), strides=(2, 2), padding='same')\r\n\r\n    @tf.function\r\n    def serving(self, img):\r\n        prediction1 = self.conv1(img)\r\n        prediction2 = self.conv2(img)\r\n        return {\r\n            'pre1': prediction1,\r\n            'pre2': prediction2,\r\n        }\r\n\r\n    @tf.function\r\n    def infer(self, serve_summary_writer, key_list):\r\n        with serve_summary_writer.as_default():\r\n            batch = tf.cast(0, tf.int64)\r\n            for img in self.dataset:\r\n                prediction_perReplica = strategy.run(self.serving, args=(img,))\r\n                tf.print(\"prediction_perReplica:\", prediction_perReplica)\r\n                for key in key_list:\r\n                    prediction_tensor = prediction_perReplica[key].values\r\n                    prediction_concat = tf.concat(prediction_tensor, axis = 0)\r\n                    tf.summary.write(tag=\"prediction_\" + key, tensor=prediction_concat, step=batch)\r\n                batch += 1\r\n                \r\n    def eval(self):\r\n        serve_summary_writer = tf.summary.create_file_writer('tmp', max_queue=100000, flush_millis=100000)\r\n        key_list = [\"pre1\", \"pre2\"]\r\n        self.infer(serve_summary_writer, key_list)\r\n        serve_summary_writer.close()\r\n        tf.io.gfile.rmtree('tmp')  \r\n\r\nif __name__ == \"__main__\":\r\n\r\n    strategy = tf.distribute.MirroredStrategy()\r\n    e = Evaluation(strategy)   \r\n    e.eval()\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n2020-10-16 04:24:03.428115: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-10-16 04:24:04.441402: E tensorflow/stream_executor/cuda/cuda_dnn.cc:328] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2020-10-16 04:24:05.457372: E tensorflow/stream_executor/cuda/cuda_dnn.cc:328] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2020-10-16 04:24:05.476782: E tensorflow/stream_executor/cuda/cuda_dnn.cc:328] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2020-10-16 04:24:05.489398: E tensorflow/stream_executor/cuda/cuda_dnn.cc:328] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2020-10-16 04:24:05.491341: E tensorflow/stream_executor/cuda/cuda_dnn.cc:328] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2020-10-16 04:24:05.495273: E tensorflow/stream_executor/cuda/cuda_dnn.cc:328] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2020-10-16 04:24:05.497372: E tensorflow/stream_executor/cuda/cuda_dnn.cc:328] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2020-10-16 04:24:05.501205: E tensorflow/stream_executor/cuda/cuda_dnn.cc:328] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 53, in <module>\r\n    e.eval()\r\n  File \"test.py\", line 41, in eval\r\n    self.infer(serve_summary_writer, key_list)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 846, in _call\r\n    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1848, in _filtered_call\r\n    cancellation_manager=cancellation_manager)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1924, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 550, in call\r\n    ctx=ctx)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.UnknownError: 3 root error(s) found.\r\n  (0) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[{{node while/body/_17/while/StatefulPartitionedCall/replica_1/conv2d_1/Conv2D}}]]\r\n         [[DeleteIterator/_102]]\r\n  (1) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[{{node while/body/_17/while/StatefulPartitionedCall/replica_1/conv2d_1/Conv2D}}]]\r\n  (2) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[{{node while/body/_17/while/StatefulPartitionedCall/replica_1/conv2d_1/Conv2D}}]]\r\n         [[while/body/_17/while/StatefulPartitionedCall/replica_1/conv2d_1/BiasAdd/_109]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_infer_436]\r\n\r\nFunction call stack:\r\ninfer -> infer -> infer", "comments": ["@ymodak any insights for this issue? ran into a same issue.", "Hi @ZhixinLai, I was able to reproduce this issue on GCP with two T4 GPUs. While I'm not quite sure what the problem here is, it's always a good idea to make sure your code works first without distribution before adding MirroredStrategy. I replaced the line `strategy = tf.distribute.MirroredStrategy()` with `strategy = tf.distribute.get_strategy()`, which will run your code with the default strategy (ie no distribution). I'm seeing a different error now:\r\n\r\n```\r\nprediction_tensor = prediction_perReplica[key].values\r\nAttributeError: 'Tensor' object has no attribute 'values'\r\n```\r\n\r\nTry this out and let me know if this helps you debug.", "> Hi @ZhixinLai, I was able to reproduce this issue on GCP with two T4 GPUs. While I'm not quite sure what the problem here is, it's always a good idea to make sure your code works first without distribution before adding MirroredStrategy. I replaced the line `strategy = tf.distribute.MirroredStrategy()` with `strategy = tf.distribute.get_strategy()`, which will run your code with the default strategy (ie no distribution). I'm seeing a different error now:\r\n> \r\n> ```\r\n> prediction_tensor = prediction_perReplica[key].values\r\n> AttributeError: 'Tensor' object has no attribute 'values'\r\n> ```\r\n> \r\n> Try this out and let me know if this helps you debug.\r\n\r\nThanks for your help @nikitamaia . With tf.distribute.MirroredStrategy(), I want to get the result from different devices, and the return result is perReplica tensor type, which is similar to a dict. In order to get the values, I use the .values. I got the idea from the discussion:\r\nhttps://stackoverflow.com/questions/57549448/how-to-convert-perreplica-to-tensor\r\n\r\nYou are right, in order to make it run successfully with single GPU, we'd better write like:\r\n```\r\nif strategy.num_replicas_in_sync > 1:\r\n    predicition_tensors = prediction_perReplica[key].values\r\nelse:\r\n    predicition_tensors = prediction_perReplica[key]\r\n```\r\n\r\nBut the code needs to be run with multi GPUs eventually, and thus we still need tf.distribute.MirroredStrategy() and get value from perReplica tensor with .values.", "Ah okay. It sounds like what you're looking for is `gather` ? This has been added to nightly. Please [have a look at the nightly docs](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy?version=nightly#gather) and examples for `tf.distribute.Strategy.gather` and let me know if this achieves the desired functionally. ", "Hi @ZhixinLai, I'm taking a second look at this code and I am able to run it without error. I ran it with 2.3 using two T4s, and also tried running in nightly. Are you still facing problems? Have you tried running in nightly?\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44072\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44072\">No</a>\n"]}, {"number": 44071, "title": "Tensorflow lite application execution on CPU only", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n\r\nHello,\r\nWe are trying to execute TFLite application (label_image) on RPi CPU only. We tried cross compilation for aarch64(RaspberryPi). Here are the steps\r\n1. Installed bazel\r\n2. git clone https://github.com/tensorflow/tensorflow.git\r\n    cd tensorflow\r\n3. ./configure\r\n\r\n> You have bazel 3.1.0 installed.\r\n> Please specify the location of python. [Default is /usr/bin/python3]: \r\n> Found possible Python library paths:\r\n>   /usr/local/lib/python3.6/dist-packages\r\n>   /usr/lib/python3/dist-packages\r\n> Please input the desired Python library path to use.  Default is [/usr/local/lib/python3.6/dist-packages]\r\n> \r\n> Do you wish to build TensorFlow with ROCm support? [y/N]: n\r\n> No ROCm support will be enabled for TensorFlow.\r\n> \r\n> Do you wish to build TensorFlow with CUDA support? [y/N]: n\r\n> No CUDA support will be enabled for TensorFlow.\r\n> \r\n> Do you wish to download a fresh release of clang? (Experimental) [y/N]: n\r\n> Clang will not be downloaded.\r\n> \r\n> Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n> \r\n> \r\n> Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\n> Not configuring the WORKSPACE for Android builds.\r\n> \r\n> Preconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n> \t--config=mkl         \t# Build with MKL support.\r\n> \t--config=monolithic  \t# Config for mostly static monolithic build.\r\n> \t--config=ngraph      \t# Build with Intel nGraph support.\r\n> \t--config=numa        \t# Build with NUMA support.\r\n> \t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n> \t--config=v2          \t# Build TensorFlow 2.x instead of 1.x.\r\n> Preconfigured Bazel build configs to DISABLE default on features:\r\n> \t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n> \t--config=nogcp       \t# Disable GCP support.\r\n> \t--config=nohdfs      \t# Disable HDFS support.\r\n> \t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\n> Configuration finished\r\n\r\n4.  bazel build --config=elinux_aarch64 //tensorflow/lite/examples/label_image:label_image\r\n\r\nBuild was successful, but not sure that application executed on CPU only or it is using GPU as well.\r\n1. Can you please help us with running tensorflow_lite application on CPU only(disable GPU)?\r\n2. How to check whether application using both CPU & GPU or only CPU?\r\n\r\nThanks\r\n\r\n", "comments": ["That's actually configuration for TensorFlow not TensorFlow Lite.\r\nTensorFlow Lite is using different GPU path which is enabled with GPU delegate.\r\nhttps://www.tensorflow.org/lite/performance/gpu\r\nYou'll use CPU only execution until you enabled GPU delegate.\r\n\r\nBTW, GPU delegate on RPI isn't available for now.", "> That's actually configuration for TensorFlow not TensorFlow Lite.\r\n> TensorFlow Lite is using different GPU path which is enabled with GPU delegate.\r\n> https://www.tensorflow.org/lite/performance/gpu\r\n> You'll use CPU only execution until you enabled GPU delegate.\r\n> \r\n> BTW, GPU delegate on RPI isn't available for now.\r\n\r\n\r\n\r\n> That's actually configuration for TensorFlow not TensorFlow Lite.\r\n> TensorFlow Lite is using different GPU path which is enabled with GPU delegate.\r\n> https://www.tensorflow.org/lite/performance/gpu\r\n> You'll use CPU only execution until you enabled GPU delegate.\r\n> \r\n> BTW, GPU delegate on RPI isn't available for now.\r\n\r\nExcuse me, is GPU delegate available on the server for TFLite?\r\n\r\nMy server is:\r\n\r\nDISTRIB_ID=Ubuntu\r\nDISTRIB_RELEASE=18.04\r\nDISTRIB_CODENAME=bionic\r\nDISTRIB_DESCRIPTION=\"Ubuntu 18.04.1 LTS\"\r\n\r\nI've tried to compile TFLite with --copt=-DCL_DELEGATE_NO_GL (bazel build -c opt --copt=-DCL_DELEGATE_NO_GL --copt=-DEGL_NO_X11 tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification:run_eval)\r\n\r\nBut it still tell me like this:\r\n\r\nThe GPU delegate compile options are only supported on Android or iOS platforms or when the tool was built with -DCL_DELEGATE_NO_GL.\r\nGPU acceleration is unsupported on this platform.\r\n\r\nThanks a lot!"]}, {"number": 44070, "title": "The weights data in tflite model is still in float32 format", "body": "**System information**\r\n- Windows 10\r\n- wheel\r\n- 1.14.0 CPU\r\n\r\n\r\n **I convert the tflite from a saved_model as the follow API, and got a tflite model, then I use Netron to open it, why the weights\r\ndata in the tflite model is still float32, shouldn't it be int8?**\r\n\r\n```\r\n# tf.lite.TFLiteConverter.from_saved_model(model_path)\r\n```\r\n", "comments": ["It seems like that the model is converted to tflite, but the weights is not quantified.", "@Harleytu \r\nCan you please let us know if it is saved as keras model.save or saved in Savedmodel format, also verify if you have specified the optimization option correctly.\r\nPlease refer to [link](https://www.tensorflow.org/lite/performance/post_training_integer_quant).\r\n**You can use post training weight quantization, but it will only quantize weights activation remain as they are :** \r\n`converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.experimental_new_converter = True\r\n\r\n--Post training quantization\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\ntflite_quant_model = converter.convert()\r\ntflite_model_quant_file = \"model.tflite\"\r\ntflite_model_quant_file.write_bytes(tflite_quant_model)`\r\n\r\n\r\n**Or you may use full integer quantization, quantizining weights ans activations:**\r\n`converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\n\r\ntflite_model_quant = converter.convert()\r\ntflite_model_quant_file = \"model.tflite\"\r\ntflite_model_quant_file.write_bytes(tflite_model_quant)`\r\n", "@Saduf2019 Thank you, your answer helped me a lot."]}, {"number": 44069, "title": "tf.keras.layers.experimental.preprocessing.TextVectorization doesn't raise error when  len(vocabulary_list)<=max_tokens<len(vocabulary_list) + 2", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): tf2.3\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nIt doesn't raise error :\r\n```\r\nvocabulary_list = [\"a\", \"b\", \"c\"]\r\ntext_dataset = tf.data.Dataset.from_tensor_slices(vocabulary_list)\r\nmax_features = 3 # Maximum vocab size.\r\n\r\n# Create the layer.\r\nvectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\r\n max_tokens=max_features)\r\n\r\n# Now that the vocab layer has been created, call `adapt` on the text-only\r\n# dataset to create the vocabulary. You don't have to batch, but for large\r\n# datasets this means we're not keeping spare copies of the dataset.\r\nvectorize_layer.adapt(text_dataset.batch(64))\r\n```\r\nThis will lead to wrong result : \r\n```\r\nvectorize_layer([\"a b c d e\"])\r\n# output :\r\n# <tf.Tensor: shape=(1, 5), dtype=int64, numpy=array([[1, 1, 2, 1, 1]])>\r\n```\r\n\r\n**Describe the expected behavior**\r\nRaise error :\r\n```\r\nValueError: Attempted to set a vocabulary larger than the maximum vocab size. Passed vocab size is 5, max vocab size is 3.\r\n```\r\n", "comments": ["When ```output_mode = \"binary\"``` , It doen't raise error since  ```max_features = 2```\r\n\r\n**Describe the current behavior**\r\nIt doesn't raise error :\r\n```\r\nvocabulary_list = [\"a\", \"b\", \"c\"]\r\ntext_dataset = tf.data.Dataset.from_tensor_slices(vocabulary_list)\r\nmax_features = 2 # Maximum vocab size.\r\n\r\n# Create the layer.\r\nvectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\r\n max_tokens=max_features,output_mode = \"binary\")\r\n\r\n# Now that the vocab layer has been created, call `adapt` on the text-only\r\n# dataset to create the vocabulary. You don't have to batch, but for large\r\n# datasets this means we're not keeping spare copies of the dataset.\r\nvectorize_layer.adapt(text_dataset.batch(64))\r\n```\r\n\r\n**Describe the expected behavior**\r\nRaise error :\r\n```\r\nValueError: Attempted to set a vocabulary larger than the maximum vocab size. Passed vocab size is 5, max vocab size is 2.\r\n```\r\n", "Maybe [https://github.com/tensorflow/tensorflow/issues/41552](https://github.com/tensorflow/tensorflow/issues/41552) is the same problem . ", "I have tried in colab with TF version 2.3, nightly version(`2.4.0-dev20201015`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/786ed1b63277236baea66cec80ee7c9e/untitled458.ipynb). Thanks!", "Was able to reproduce your issue in Tensorflow 2.5, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/7987a78a23fa3801df1d15207ad244cb/44069.ipynb). Thanks!", "Hi @DachuanZhao! I was able to resolve the issue by using `tf.keras.layers.TextVectorization` instead of `tf.keras.layers.experimental.preprocessing.TextVectorization` in TF 2.7 . Attaching [Gist ](https://colab.research.google.com/gist/mohantym/92eedc0747b3af556493a73aa1e6c35c/44069.ipynb#scrollTo=pyKTWxg-7yRC)for reference . Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44069\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44069\">No</a>\n"]}]