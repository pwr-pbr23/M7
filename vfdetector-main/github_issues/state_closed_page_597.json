[{"number": 35762, "title": "_ = dataset.cache() accelerates data pipeline", "body": "**System information**\r\n- Have I written custom code\r\n- Linux Ubuntu 18.04\r\n- TensorFlow ROCm installed from PyPI\r\n- TensorFlow version: v2.0.0-rocm-3-g0826c3a 2.0.2\r\n- Python version: Python 3.7.5\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: 2 x Radeon Vega 64\r\n\r\n**Describe the current behavior**\r\n\r\nTF.Data is quicker when one caches the whole dataset to an unused name.\r\n\r\n-    With `dataset_*.cache(...)`:\r\n    -   With `_= dataset.cache(...)`: 10870 samples/s stdev 27 samples/s\r\n    -   Without `_= dataset.cache(...)`: 10563 samples/s, stdev 50 samples/s\r\n    -   (10870 - 27) / (10563 + 50) = 1.02167153491\r\n-    Without `dataset_*.cache(...)`:\r\n    -   With `_= dataset.cache(...)`: 2902 samples/s stdev 14 samples/s\r\n    -   Without `_= dataset.cache(...)`: 2732 samples/s, stdev 9 samples/s\r\n    -   (10870 - 27) / (10563 + 50) = 1.05363006202\r\n\r\n**Describe the expected behavior**\r\n\r\nTF.Data is as quick as possible by default.\r\n\r\n**Code to reproduce the issue**\r\n\r\nMy input data pipeline looks something like this: \r\n\r\n```python\r\ndata_frame_valid = pd.read_csv(...)\r\ndata_frame_invalid = pd.read_csv(...)\r\ndataset_valid: tf.data.Dataset = tf.data.Dataset.from_tensor_slices(...)\r\ndataset_invalid: tf.data.Dataset = tf.data.Dataset.from_tensor_slices(...)\r\ndataset: tf.data.Dataset = dataset_valid.concatenate(dataset_invalid)\r\ndataset = dataset.shuffle(...)\r\n_ = dataset.cache()  # This speeds up iterating the data by 2 - 5 %\r\ndataset_training = dataset.take(data_set_size_training)\r\ndataset_testing = dataset.skip(data_set_size_training)\r\ndataset_validation = dataset_testing.skip(data_set_size_validation)\r\ndataset_testing = dataset_testing.take(data_set_size_testing)\r\n# dataset_training = dataset_training.cache()  # \"_ speed up\" can be observed with and without\r\n# dataset_validation = dataset_validation.cache()  # \"_ speed up\" can be observed with and without\r\n# dataset_testing = dataset_testing.cache()  # \"_ speed up\" can be observed with and without\r\n```\r\n\r\nMy benchmark:\r\n\r\n```python\r\nimport statistics\r\n\r\nimport time\r\nimport tensorflow as tf\r\n\r\nfrom pfasdr.neural.ze_discriminate_pd_np.get_data_sets_module import \\\r\n    get_data_sets\r\nfrom pfasdr.neural.ze_discriminate_pd_np.path_templates_module import \\\r\n    valid_file_path_template, invalid_file_path_template\r\n\r\n\r\ndef benchmark_dataset(dataset, num_epochs=2):\r\n    tf.print('Iterating data set ...')\r\n\r\n    throughput_history = []\r\n    for index_epoch in tf.data.Dataset.range(num_epochs):\r\n        tf.print(f'Iterating for epoch {index_epoch}')\r\n        index = 0\r\n\r\n        # The actual benchmark\r\n        tine_start = time.perf_counter()\r\n        for index, _ in enumerate(dataset):\r\n            pass\r\n            # Uncomment for progress reporting\r\n            # if not index % 100:\r\n            #     print('\\r' + str(index), end='')\r\n        time_end = time.perf_counter()\r\n\r\n        print('\\r' + str(index))\r\n        duration = time_end - tine_start\r\n        throughput = index / duration\r\n\r\n        tf.print(\r\n            f'Iterating data set took: '\r\n            f'{round(duration, 2)} s in epoch number {index_epoch}. '\r\n            f'This makes for a throughput of {round(throughput)} 1/s'\r\n        )\r\n\r\n        if not index_epoch:\r\n            # First round uses cold caches.\r\n            # So do not record it.\r\n            continue\r\n        throughput_history.append(throughput)\r\n\r\n    throughput_average = statistics.mean(throughput_history)\r\n    throughput_deviation = statistics.stdev(throughput_history)\r\n    throughput_upper = throughput_average + throughput_deviation\r\n    throughput_lower = throughput_average - throughput_deviation\r\n    tf.print(f'Average dataset entry throughput {round(throughput_average)} '\r\n             f'with a variation of +/- {round(throughput_deviation)}, '\r\n             f'which means {round(throughput_upper)} (+), '\r\n             f'or {round(throughput_lower)} (-).')\r\n\r\n\r\ndef main():\r\n    batch_size_training = 32\r\n    node_count = 15\r\n    valid_file_path = valid_file_path_template.substitute(\r\n        length=node_count,\r\n    )\r\n    invalid_file_path = invalid_file_path_template.substitute(\r\n        length=node_count,\r\n    )\r\n\r\n    dataset_training, dataset_validation, dataset_testing, \\\r\n        batch_size_training, dataset_size_validation, dataset_size_evaluate,\\\r\n        dataset_size_training \\\r\n        = get_data_sets(\r\n            batch_size=batch_size_training,\r\n            names=list(range(node_count)),\r\n            invalid_file_path=invalid_file_path,\r\n            valid_file_path=valid_file_path,\r\n        )\r\n\r\n    benchmark_dataset(dataset=dataset_training, num_epochs=5)\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```", "comments": ["Why `_ = dataset.cache()`? \r\n\r\nTry `dataset = dataset.cache()` instead.", "Yes, I know assigning to `_` does not make sense. I stumbled upon this behavior by mistake. I tried to invoke `.cache()` on one of the `dataset_*` subsets, which was not visible from the function handling the `dataset` creation. Therefore, the name assignment should have had no effect, which makes it equivalent to assigning to `_`. Yet, I still measured an improvement, which obviously should not be the case. Using `dataset = dataset.cache()` performs the same, but throws warnings, because this `.cache()` call is before `take()` calls. `dataset.cache()` also speeds up the pipeline, so assigning itself is optional.", "The cache bahavior is designed for accelerate repeated reads from disk, not arrays in memory.", "Yes, I know `.cache()`is not supposed to offer speed ups here at all. However, it does so in both instances (`dataset_*.cache()` and `dataset.cache()`).", "Wait a minute. It seems like I got the standard deviations of the epochs down far enough to measure a difference by iterating over a dataset for long enough. However, I still have a hard time reproducing my results. Could it be that the throughput performance of an input pipeline is just not reliable up to a few percent and that I have been measuring luck of the draw by running my benchmark script a few times?", "Here are some results of consecutive manual runs:\r\n\r\n-   without `dataset_*.cache`:\r\n    -   2895 with a variation of +/- 8\r\n    -   2893 with a variation of +/- 13\r\n    -   2876 with a variation of +/- 17\r\n    -   2844 with a variation of +/- 7\r\n    -   2842 with a variation of +/- 9\r\n\r\nWhich makes for a maximum difference of the means up to 2.5 % and thus **does explain** the measured speed up.\r\n\r\n-   with `dataset_*.cache`:\r\n    -   10744 with a variation of +/- 29\r\n    -   10711 with a variation of +/- 26\r\n    -   10702 with a variation of +/- 27\r\n    -   10625 with a variation of +/- 16\r\n    -   10601 with a variation of +/- 45\r\n\r\nWhich makes for a maximum difference of the means of about 2.05 % and thus **does not explain** the measured speed up.\r\n\r\nSo it seems like I was at least possibly just measuring run-to-run differences in the case of no caching at all. The effect of the speed up seems to be really caused by the seemingly useless `dataset.cache()` call under the condition of data-sub-sets being cached.", "Putting `_ = dataset.cache()` will have no effect on the input pipeline graph that ends up being created and executed, so your performance difference must be due to noise.\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35762\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35762\">No</a>\n"]}, {"number": 35761, "title": "[TF2.1] k parameter is ignored in tf.linalg.diag_part", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64bit (also happens in WSL)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 10.1 (also happens in tensorflow-cpu)\r\n- GPU model and memory: GTX 1050 2GB\r\n\r\n**Describe the current behavior**\r\nUsing the `k` parameter of `tf.linalg.diag_part` does not affect anything. The result is still the same.\r\n\r\n**Describe the expected behavior**\r\nIt should select the super or subdiagonal dependent on `k`.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\ninput = np.array([[[1, 2, 3, 4],  # Input shape: (2, 3, 4)\r\n                   [5, 6, 7, 8],\r\n                   [9, 8, 7, 6]],\r\n                  [[5, 4, 3, 2],\r\n                   [1, 2, 3, 4],\r\n                   [5, 6, 7, 8]]])\r\n# this works as expected\r\ntf.linalg.diag_part(input) ==> [[1, 6, 7],  # Output shape: (2, 3)\r\n                                [5, 2, 7]]\r\n# this does not\r\ntf.linalg.diag_part(input, k = 1) ==> [[1, 6, 7], [5, 2, 7]] # still returns the same output\r\n  # [[2, 7, 6], [4, 3, 8]]  Is the expected output\r\n```\r\nThis example is taken from the [documentation](https://www.tensorflow.org/api_docs/python/tf/linalg/diag_part?version=nightly), which is also incorrect see #35760.", "comments": ["I could replicate the issue with Tf 2.1.\r\nPlease find the attached [gist](https://colab.sandbox.google.com/gist/gadagashwini/86984446b8f07eeb590c3bd724a1d556/untitled343.ipynb?authuser=1). Thanks!", "@n-gao I can reproduce the issue with `TF2.1`. However, the document you are referring is related `tf-nightly`. When I used `tf-nightly`, the output is as you expected. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/025c3ba3c089f6b4a580072f599239fa/untitled749.ipynb).\r\n\r\nOutput from the gist\r\n```\r\n<tf.Tensor: shape=(2, 3), dtype=int64, numpy=\r\narray([[2, 7, 6],\r\n       [4, 3, 8]])>\r\n```\r\n\r\nPlease close the issue if you agree that it was resolved by using `tf-nightly`. Thanks!", "I can confirm that this is fixed in `tf-nightly`. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35761\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35761\">No</a>\n"]}, {"number": 35760, "title": "diag_part documentation outdated", "body": "## URL(s) with the issue: \r\nhttps://www.tensorflow.org/api_docs/python/tf/linalg/diag_part?version=nightly\r\n\r\n## Description of issue (what needs changing):\r\nThe `diag_part` documentation contains old examples of non-existing APIs, it uses `tf.matrix_diag_part` in the examples which does not exist.\r\n", "comments": ["@n-gao Thanks for reporting the issue. Agree. It is defined correctly in the [`tf-nightly`](https://www.tensorflow.org/api_docs/python/tf/linalg/diag_part?version=nightly) but in the examples `tf.matrix_diag_part` should have been replaced by `tf.linalg.diag_part`. Thanks!\r\n\r\nAre you interested in creating PR to update the docs. Thanks!", "It's still the old documentation: https://www.tensorflow.org/api_docs/python/tf/linalg/diag_part\r\n"]}, {"number": 35759, "title": "Error when running gmake for Tensorflow Lite Micro", "body": "Hello.\u00a0\r\nI have an issue similar to\u00a0#29524.\r\n\r\nI'm trying to generate a Keil project for the microcontroller I'm developing.\r\nWhen I was following procedures in the following link, I got the error:\r\n\r\nhttps://www.tensorflow.org/lite/microcontrollers/library\r\n\r\nSystem informationOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\u00a0macOS Catalina 10.15.2 (19C57)\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version:\u00a0e689e24\r\nGCC/Compiler version (if compiling from source):\u00a0Apple clang version 11.0.0 (clang-1100.0.33.16)\r\n\r\nDescribe the problem\r\nWhen running$ gmake -f tensorflow/lite/experimental/micro/tools/Makefile generate_projects results in the following error:\r\n\r\ngmake: *** No rule to make target 'tensorflow/lite/micro/tools/make/gen/osx_x86_64/prj/micro_speech/make/tensorflow/lite/experimental/micro/examples/micro_speech/simple_features/simple_model_settings.h', needed by 'generate_micro_speech_make_project'.\u00a0 Stop.\r\n", "comments": ["I can confirm I'm seeing this exact behavior trying to build an mBed project.  I've tried on both Windows and a clean Ubuntu 19.10 VM", "@Shusuke-O  It looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version  2.5 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35759\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35759\">No</a>\n"]}, {"number": 35758, "title": "Fix build breakage due to missing static member definitions", "body": "The static constexpr datamember declarations in the various ToDataType\r\nspecialiations (in stream_executor/dnn.h) do not have a corresponding\r\ndefinition outside of the class.\r\nThis results in compilation failures in debug mode", "comments": ["I'm not sure if this should be necessary or points to a problem somewhere else.\r\n\r\n@timshen91 do you know what to expect here?", "It is necessary per [C++ standard](https://www.ibm.com/support/knowledgecenter/SS3KZ4_9.0.0/com.ibm.xlcpp9.bg.doc/language_ref/cplr038.htm)\r\nIn optimized mode, all uses get replaced by the definition, which is why the linker does not report the error. ", "Thanks.  This isn't needed in c++17, but is needed in c++14 so your fix is correct."]}, {"number": 35757, "title": "[INTEL MKL]  Refactor the  implementation of forward conv ops w MKL-DNN v1.0 integration -", "body": "This PR is intended to refactor the current implementation of forward conv ops with MKL-DNN\r\nv1.0 integration:\r\n(1) Abstract macro definitions from individual source files to a common header file (mkl_types)\r\nfor better code reuse (in coming PR's)\r\n(2) Refactor based on Google coding style. For instance,  replace CHECK_NOTNULL to DCHECK. \r\n\r\nBackground: \r\nWe (Intel) will submit a sequence of PR's with MKL-DNN v1.0 integration (around 24 MKL ops). \r\nAnd this PR will be fundamental for the near future PR's.\r\n\r\nReference of previously merged PR: \r\n   https://github.com/tensorflow/tensorflow/pull/30549\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35757) for more info**.\n\n<!-- need_sender_cla -->", "@gzmkl thank you for your contribution, please sign CLA.", "I am having CLA problem and will ask teammate to create a new one and close this one", "Will create a new one", "Reopen to test CLA access", "Hi there, \r\nI should have CLA access now with the test of https://cla.developers.google.com/clas\r\nPlease help to check this. \r\nMany thanks!", "@googlebot ", "@googlebot I signed it! ", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35757) for more info**.\n\n<!-- ok -->", "@gzmkl  Can you please resolve conflicts? Thanks!", "Thanks for the reminder. I have addressed the merge conflict. \r\n\r\nBy the way, Amin (@mdfaijul) will cover me during my vacation (next two weeks). ", "@Penporn We are preparing a series of PRs for DNNL1.0 upgrade, which all depends on this PR.  Merging of this PR will immensely help with other PRs. Thanks! ", "@Jianhui-Li @gzmkl Sorry for the delay! I'll review this later today. ", "@penpornk you can tag me for any comments. I will be addressing the comments. The author of this PR is currently on vacation.", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35757) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35757) for more info**.\n\n<!-- ok -->", "@penpornk Fixed typos in comments. Please check. ", "@penpornk I removed some unoptimized code in `tensorflow/core/util/mkl_util.h` and minor changes in other files. Please check. Also addressed your earlier comments.", "@gbaned Our subsequent PRs for integrating MKLDNN 1.x version depends on this current PR. We can submit those PR once this PR is merged. Can we expect this PR to be merged soon?"]}, {"number": 35756, "title": "Build error when building //tensorflow/compiler/aot:tfcompile", "body": "when building the aot compiler with:\r\n`bazel build --config=opt  -j 24 //tensorflow/compiler/aot:tfcompile`\r\nthe following error is triggered:\r\n`ERROR: /root/.cache/bazel/_bazel_root/e7f9511d00737e63bb3d33a961e2b58a/external/llvm-project/llvm/BUILD:741:1: C++ compilation of rule '@llvm-project//llvm:aarch64_info' failed (Exit 1)\r\nexternal/llvm-project/llvm/lib/Target/AArch64/TargetInfo/AArch64TargetInfo.cpp:9:10: fatal error: TargetInfo/AArch64TargetInfo.h: No such file or directory\r\n #include \"TargetInfo/AArch64TargetInfo.h\"\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\nTarget //tensorflow/compiler/aot:tfcompile failed to build`\r\nthe failing compilation is:\r\n` (cd /root/.cache/bazel/_bazel_root/e7f9511d00737e63bb3d33a961e2b58a/execroot/org_tensorflow &&   exec env -     CUDA_TOOLKIT_PATH=/usr/local/cuda     GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-7     LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64     PATH=/usr/local/nvm/versions/node/v13.3.0/bin:/usr/local/mpi/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/tensorrt/bin     PWD=/proc/self/cwd     PYTHON_BIN_PATH=/usr/bin/python     PYTHON_LIB_PATH=/usr/lib/python3.6/dist-packages     TF2_BEHAVIOR=1     TF_CONFIGURE_IOS=0     TF_CUDA_COMPUTE_CAPABILITIES=7.0     TF_ENABLE_XLA=1     TF_NEED_CUDA=1   external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/llvm-project/llvm/_objs/aarch64_info/AArch64TargetInfo.d '-frandom-seed=bazel-out/k8-opt/bin/external/llvm-project/llvm/_objs/aarch64_info/AArch64TargetInfo.o' -DLLVM_ENABLE_STATS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -DLLVM_BUILD_GLOBAL_ISEL -iquote external/llvm-project -iquote bazel-out/k8-opt/bin/external/llvm-project -iquote external/zlib_archive -iquote bazel-out/k8-opt/bin/external/zlib_archive -isystem external/llvm-project/llvm/include -isystem bazel-out/k8-opt/bin/external/llvm-project/llvm/include -isystem external/zlib_archive -isystem bazel-out/k8-opt/bin/external/zlib_archive -isystem external/llvm-project/llvm/include/llvm/IR -isystem bazel-out/k8-opt/bin/external/llvm-project/llvm/include/llvm/IR -isystem external/llvm-project/llvm/lib/IR -isystem bazel-out/k8-opt/bin/external/llvm-project/llvm/lib/IR -isystem external/llvm-project/llvm/lib/Transforms/InstCombine -isystem bazel-out/k8-opt/bin/external/llvm-project/llvm/lib/Transforms/InstCombine '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIE -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -w -DAUTOLOAD_DYNAMIC_KERNELS '-march=native' -Wno-sign-compare '-std=c++14' -Iexternal/llvm/lib/Target/AArch64 -c external/llvm-project/llvm/lib/Target/AArch64/TargetInfo/AArch64TargetInfo.cpp -o bazel-out/k8-opt/bin/external/llvm-project/llvm/_objs/aarch64_info/AArch64TargetInfo.o)`\r\n\r\nThis is because an incorrect include path: `-Iexternal/llvm/lib/Target/AArch64`\r\nThe include comes from the file `third_party/llvm/llvm.autogenerated.BUILD`\r\nalong with 90 other incorrect include paths to a non-existing directory. All paths of the form:\r\n`    copts = llvm_copts + [\"-Iexternal/llvm/lib/Target/<SomeTarget>\"],`\r\nshould be:\r\n`    copts = llvm_copts + [\"-Iexternal/llvm-project/llvm/lib/Target/<SomeTarget>\"],`\r\nSince this is a generated file, I can not submit a PR with the proposed fix.\r\n\r\n", "comments": []}, {"number": 35755, "title": " Upgrade tflite micro softmax op version", "body": "Fix for #35748", "comments": ["I landed a patch to fix this, along with a handful of other ops before I saw your change.  Since the fix is already in, I'll close this.  Thanks for your change, and sorry to have duplicated it."]}, {"number": 35754, "title": "Make Tensorflow 2.1 available through conda", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): conda\r\n- Python version: 3.7\r\n\r\n**Describe the problem**\r\n\r\n`conda install tensorflow` installs version 2.0 instead of 2.1.\r\nPlease, make tensorflow 2.1 available through conda.", "comments": ["I think conda support is community driven, not provided by us. Adding @gunan to confirm/infirm", "Maybe there is another repo where this issue can be raised?", "@jjhelmus may be able to help.\r\nWe can redirect conda availability issues to Jonathan, I think?", "The team at Anaconda is looking to make Tensorflow 2.1 packages available.  This process may take a few weeks.  ", "Having `pip` and `conda` together is a pain. Why tensorflow is released on pip by default?", "Umm, because that's where the python packages are?", "Sorry, I meant why is a conda release not automatic on the pipeline? PyTorch has a conda channel where the package appears very soon after release.", "We don't have infrastructure for that and I don't think it is prioritized", "I prefer to install tensorflow with conda, because conda does not require us to consider GPU support. Hope tensorflow 2.1 can be installed through conda as soon as possible.", "Frameworks like Pytorch even have their own Conda channel. Shipping the latest version via Conda can bring the easiest installation experience for our users (So we don't need consider about the GPU enviorment setup etc.). In Conda, we only need to \"conda install tensorflow-gpu\", everything will be setup and ready to use.", "> We don't have infrastructure for that and I don't think it is prioritized\r\n\r\nMaybe that's one of many reasons why PyTorch is more and more popular than TensorFlow. ", "From https://www.anaconda.com/tensorflow-in-anaconda/:\r\n\r\n\"Anaconda is proud of our efforts to deliver a simpler, faster experience using the excellent TensorFlow library. It takes significant time and effort to add support for the many platforms used in production, and to ensure that the accelerated code is still stable and mathematically correct. As a result, **our TensorFlow packages may not be available concurrently with the official TensorFlow wheels**. We are, however, committed to maintaining our TensorFlow packages, and work to have updates available as soon as we can.\"", "> Maybe that's one of many reasons why PyTorch is more and more popular than TensorFlow.\r\n\r\nEase of installing is definitely a plus.", "@woodknight @cossio please stay on topic as per community guidelines. Thank you! ", "@joanafilipa which part of those comments isn't on topic? It's all well and good for @mihaimaruseac to say this isn't a priority, but a community shouldn't be discouraged from expressing frustration when those priorities push them away. Judging by reactions to posts - which certainly isn't an exhaustive survey, but better than nothing - @cossio isn't the only one who feels this way.", "At risk of continuing a conversation that's already off-topic,\r\nPlease recall that expressing frustration is not productive and it's something that a lot of us who subscribe to the issue just looking for updates get emails about. It's okay to *be* frustrated, but please do it somewhere else so the github issues are not full of chatter. ", "Agree. However, the result of the \"productive \" comment is \r\n\r\n> We don't have infrastructure for that and I don't think it is prioritized.\r\n\r\n\r\n\r\nFrustrated or not, I think those users only want the Tensorflow become better.\r\n\r\nAlso, I believe the Pytorch Conda is supported by offical [1, 2]. We did not off-topic. We are only requesting the offical Tensorflow to support the Conda distribution without rely too much on the community.\r\n\r\n\r\n> Maybe that's one of many reasons why PyTorch is more and more popular than TensorFlow.\r\n\r\nUnfortunately, this is the truth, especially in Research [3]. And for my personal experience, most of researchers around me are moved to Pytorch.\r\n\r\nI understand the Tensorflow Developers are working very hard to deliver the new features and fix the bugs. Thank you very much. \r\nHowever, installation of Tensorflow enviorment is the most important things since this is the entrance to use Tensorflow. And I can tell you lots of Researchers or Users have zero enginner or developer background, which means they only know how to design the algorithm, but they can very easy in trouble with installation.\r\n\r\n\r\n[1] https://anaconda.org/pytorch\r\n[2] https://pytorch.org/get-started/locally/\r\n[3] https://towardsdatascience.com/is-pytorch-catching-tensorflow-ca88f9128304", "@jjhelmus Anything we could help to provide future TF releases on Anaconda in a more timely manner? \r\n\r\nTF 2.2 is upcoming and @edwardyehuang @cossio you might consider joining the effort to make the release on conda faster.", "Thanks @edwardyehuang for the context. \r\n\r\nWe hear that this is a point of frustration for conda users. @gunan could add the appropriate label (feature? or build/install?) to this so we can better track this issue going forward? We'll circle round with the TF and community folks who add conda support and see what the blockers are to \r\n getting packages ready sooner after new TF releases.\r\n\r\nA general reminder to keep things constructive and on-topic. If you have additional data points from the TF ecosystem you'd like to mention, please do.\r\n\r\n\r\n", "I think Tensorflow 2.1.0 is now available on Anaconda.\r\n\r\nI reinstalled it today and installed tensorflow, here it automatically installed Tf 2.1.0\r\n\r\nMy steps:\r\nOpen anaconda prompt\r\n-> conda create -n tf Tensorflow // here it downloaded Tensorflow 2.1.0 \r\n\r\nI dont know if there is the possibility to update it by any Command. But this way works.", "> I think Tensorflow 2.1.0 is now available on Anaconda.\r\n> \r\n> I reinstalled it today and installed tensorflow, here it automatically installed Tf 2.1.0\r\n> \r\n> My steps:\r\n> Open anaconda prompt\r\n> -> conda create -n tf Tensorflow // here it downloaded Tensorflow 2.1.0\r\n> \r\n> I dont know if there is the possibility to update it by any Command. But this way works.\r\n\r\n@FeVo96 Is your reinstalled TF2.1 GPU version?\r\n\r\nI just looked at https://repo.anaconda.com/pkgs/main/linux-64/ and found tensorflow-2.1 CPU version, but no GPU version.", "> > I think Tensorflow 2.1.0 is now available on Anaconda.\r\n> > I reinstalled it today and installed tensorflow, here it automatically installed Tf 2.1.0\r\n> > My steps:\r\n> > Open anaconda prompt\r\n> > -> conda create -n tf Tensorflow // here it downloaded Tensorflow 2.1.0\r\n> > I dont know if there is the possibility to update it by any Command. But this way works.\r\n> \r\n> @FeVo96 Is your reinstalled TF2.1 GPU version?\r\n> \r\n> I just looked at https://repo.anaconda.com/pkgs/main/linux-64/ and found tensorflow-2.1 CPU version, but no GPU version.\r\n\r\nIt is also the gpu version available.\r\nI just opened up the conda environment in the prompt and typed conda Upgrade tensorflow-gpu. And it works", "> \r\n> \r\n> > > I think Tensorflow 2.1.0 is now available on Anaconda.\r\n> > > I reinstalled it today and installed tensorflow, here it automatically installed Tf 2.1.0\r\n> > > My steps:\r\n> > > Open anaconda prompt\r\n> > > -> conda create -n tf Tensorflow // here it downloaded Tensorflow 2.1.0\r\n> > > I dont know if there is the possibility to update it by any Command. But this way works.\r\n> > \r\n> > \r\n> > @FeVo96 Is your reinstalled TF2.1 GPU version?\r\n> > I just looked at https://repo.anaconda.com/pkgs/main/linux-64/ and found tensorflow-2.1 CPU version, but no GPU version.\r\n> \r\n> It is also the gpu version available.\r\n> I just opened up the conda environment in the prompt and typed conda Upgrade tensorflow-gpu. And it works\r\n\r\nTensorflow 2.1 should included GPU", "tensorflow 2.1.0 and tensorflow-gpu 2.1.0 both of windows\r\n\r\nCurrently just tensorflow 2.1.0 on Linux (Ubuntu 18.04), no gpu yet", "I don't think you need a separate gpu package for 2.1. Can someone confirm?", "I can confirm that conda's tensorflow 2.1 defaults to CPU (and with tensorflow-gpu 2.0 the same code ran on GPU). But the 'official' tensorflow 2.1 (not tensorflow-gpu) [release](https://github.com/tensorflow/tensorflow/releases/tag/v2.1.0) does state that, from now on, GPU is always included and is default...", "For windows: if you conda install tensorflow-gpu==2.1.0 you get the current version.", "Conda's tensorflow-gpu 2.1.0 has also become available for Linux. Issue can be closed, I guess.", "Closing per request", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35754\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35754\">No</a>\n", "What about the macOS version?", "Any update regarding conda and Tensorflow 2.1 on macOS? It is still installing 2.0", "I'd also like to see Mac support for TensorFlow 2.1 - I've filed https://github.com/ContinuumIO/anaconda-issues/issues/11697 since this issue has been closed.", "There is no CUDA support for macOS anyway, so why don't you guys simply do `pip3 install tensorflow==2.1.0`?", "because\r\n\r\npip3 install tensorflow==2.1.0\r\nERROR: Could not find a version that satisfies the requirement tensorflow==2.1.0 (from versions: none)\r\nERROR: No matching distribution found for tensorflow==2.1.0", "@acegilz  , You likely need to update pip\r\n```\r\npip3 install --upgrade pip\r\npip3 install tensorflow==2.1.0\r\n```", "`pip3 install --upgrade pip`\r\nRequirement already up-to-date: pip in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (20.0.2)\r\n\r\n`pip3 install tensorflow==2.1.0`\r\nERROR: Could not find a version that satisfies the requirement tensorflow==2.1.0 (from versions: none)\r\nERROR: No matching distribution found for tensorflow==2.1.0", "@acegilz TensorFlow 2.1 did not publish Py3.8 package on macOS. Luckily we got 2.2 soon and it should come with Py3.8 support.", "@acelitz: Py3.8 support #33374\r\n\r\nYou can always check the pypi package page to see which pips are available.", "@cossio I am trying to install `tf 2.x` via `conda`, but I am receiving a different error for both versions: `tf 2.0.0` and `tf 2.1.0`.\r\n\r\nI saw that there is another possible way of solving this, by installing `tf 2.1.0` using `pip` as:\r\n```\r\nconda create --name tf2\r\npip install tensorflow\r\n```\r\n\r\nIs this a proper way of doing it?\r\n\r\nThank you!\r\n", "@cciprianmihai Use `conda install` instead of `pip install`, and you should set the version of TF, like `conda install tensorflow-gpu=2.1` for TF 2.1 GPU version.", "Will tensorflow-2.2.0 be on conda soon as well? Thanks.", "We haven't yet released the final 2.2 pips. Once that happens, @jjhelmus can trigger the conda builds, afaik", "@jjhelmus any eta on the tensorflow-gpu 2.2.0 conda build? Thanks.", "Typically, it takes anywhere from 2 to 4 weeks for us (Anaconda, Inc) to build Tensorflow conda packages.  I expect that the 2.2.0 release will take a similar amount of time.", "@jjhelmus Thanks a lot for the feedback!", "Any updates on TF 2.2?", "Please file conda related issue through anaconda issue tracker, located at:\r\nhttps://github.com/ContinuumIO/anaconda-issues/issues\r\n\r\nAnaconda manages the TF releases on their platform, and they do not monitor this issue tracker.", "Any updates on TensorFlow >2.1 for Conda users on Linux and macOS ? thanks.", "> \r\n> \r\n> Any updates on TensorFlow >2.1 for Conda users on Linux and macOS ? thanks.\r\n\r\nI think Tensorflow 2.2 has already been releated at conda long time ago.\r\n[https://anaconda.org/anaconda/tensorflow](https://anaconda.org/anaconda/tensorflow)", "macOS is still stuck at 2.0.0:\r\nhttps://anaconda.org/anaconda/tensorflow\r\n<img width=\"202\" alt=\"Screen Shot 2021-02-12 at 10 15 22\" src=\"https://user-images.githubusercontent.com/73581880/107749654-396e3300-6d1b-11eb-85be-02a0472dc40a.png\">\r\n\r\nAny update on this?", "You should really direct this question to anaconda. "]}, {"number": 35753, "title": "Making a copy of the the input dict in Wrapper.from_config", "body": "Proposed fix for #35683\r\n\r\nDuplicate of #35684 but for the master branch", "comments": []}, {"number": 35752, "title": "[ROCm] Fix the ROCm CSB breakage - 200110", "body": "PR # (which enabled MIOpen Immediate Mode API for convolutions) was merged yesterday, but it causes the following test to fail\r\n\r\n```\r\n//tensorflow/python/keras:convolutional_test_gpu\r\n```\r\n\r\nThe cause of the failure is because the above test calls the convolution API with cudnn_use_autotune set to false. (as a consequence of this receent commit : https://github.com/tensorflow/tensorflow/commit/fc761aa9b1ec32500b762dfd699f254d97ce08c6 )\r\n\r\nWith the new MIOpen Immediate Mode API, we need to have an explicit call to GetMIOpenConvolveAlgorithms, when convolution kernel is called for the first time for a given conv_paramereters set. That call gets skipped if cudnn_use_autotune is set to\r\nfalse, and hence the test failure.\r\n\r\nThis fix (workaround really) essentially disables the cudnn_use_autotune functionality for convolution kernel calls in the ROCm flow.\r\n\r\n----\r\n\r\n/cc @whchung @chsigg ", "comments": ["We want the ROCm flow to check the autotune cache irrespective of whether or not the `cudnn_use_autotune` bool is set. Given how the if condition is currently coded, setting `cudnn_use_autotune = true` forces that check to happen. ", "@chsigg, gentle ping", "@chsigg, gentle ping", "@deven-amd thanks for your patience , right now reviewers are busy and will get back to it early next week.", "Approved, but I still find it quite confusing. Please consider improving how the condition is coded so that we don't have `cudnn_use_autotune = true` while autotuning is disabled.", "@rthadur , gentle ping", "@chsigg can you please help import this change manually ? Thank you"]}, {"number": 35751, "title": "[XLA] Update the path for the LLVM FileCheck executable", "body": "The path for the `FileCheck` executbale needs to be updated as a consequence of the following commit.\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/d6ba353dd974f3d883734e96653cdee7fe6abfbc\r\n\r\nAfter that commit the following test (and many others) start failing with the following error\r\n\r\n```\r\nbazel test //tensorflow/compiler/xla/service/cpu/tests:cpu_intrinsic_test\r\n...\r\n...\r\n2020-01-10 19:48:15.756796: W tensorflow/compiler/xla/tests/filecheck.cc:72]\r\nTried to execute FileCheck at /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/xla/service/cpu/tests/cpu_intrinsic_test.runfiles/org_tensorflow/external/llvm/FileCheck\r\n\r\n2020-01-10 19:48:15.756884: W tensorflow/compiler/xla/tests/filecheck.cc:74]\r\nNOTE: FileCheck binary does not exist!\r\n...\r\n...\r\n```\r\n\r\nThis fix updates the path of the LLVM FileCheck executable to the correct one.\r\n\r\n\r\n------------------\r\n\r\n/cc @cheshire @whchung \r\n\r\n@cheshire , the `bazel test` command above was failing in the upstream repo as of a few minutes ago. Note that I was running without `--config=rocm` (i.e. with a CPU only TF build)", "comments": ["@deven-amd I think removing `[ROCm]` prefix would be better as the proposed fix applies on every XLA-support platform.", "@jpienaar Do you think this commit is necessary? Apparently, the previous path has stopped working after https://github.com/tensorflow/tensorflow/commit/d6ba353dd974f3d883734e96653cdee7fe6abfbc . I wonder then how come our OSS bots are still passing.", "I believe an internal change is in the works (look at my review queue\ninternally) that may intersect with this one [AFK] and should resolved this\ntoo. I believe it is being submitted as we speak.\n\nNot sure why it didn't trigger on the bots.\n\nOn Fri, Jan 10, 2020, 12:09 PM George Karpenkov <notifications@github.com>\nwrote:\n\n> @jpienaar <https://github.com/jpienaar> Do you think this commit is\n> necessary? Apparently, the previous path has stopped working after d6ba353\n> <https://github.com/tensorflow/tensorflow/commit/d6ba353dd974f3d883734e96653cdee7fe6abfbc>\n> . I wonder then how come our OSS bots are still passing.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/35751?email_source=notifications&email_token=AAFMRTXXHH7NTCW6CQMUOB3Q5DIXHA5CNFSM4KFMRPNKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEIVCHGY#issuecomment-573186971>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAFMRTRSJR6ROTN66EGONNLQ5DIXHANCNFSM4KFMRPNA>\n> .\n>\n", "@jpienaar Any objections on submitting this? Or should we wait?", "@cheshire the other changes don't overlap with this, I expected that they would and might still be pending, but we do need to update this too, so LG.", "Seems you have found a sizeable whole in our OSS testing, thanks!\r\nThis should be fixed in a different layer, I'll push out a fix shortly."]}, {"number": 35750, "title": "TensorFlow 2.0 Unknown entries in loss dictionary when recovering saved model with tf.keras.models.load_model", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): based on an official example, but modified\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): docker image `tensorflow/tensorflow:2.0.0-gpu-py3-jupyter`\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): docker\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: n/a\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI have a `tf.keras` model initialized via the \"functional\" API, e.g.\r\n\r\n```python\r\ninput = tf.keras.Input(...)\r\ny = tf.keras.layers.Dense(...)(input)\r\n...\r\nlogits = tf.keras.layers.Dense(..., name=\"logit_layer\")(y)\r\n\r\nmodel = tf.keras.models.Model(inputs, logits)\r\n```\r\n\r\nI then compile it with a custom loss:\r\n\r\n```python\r\ndef my_loss(labels, logits):\r\n    # here just wrapping a known loss to remove errors that could come from me\r\n    # however there may be additional functionality here\r\n    loss = tf.nn.softmax_cross_entropy_with_logits(\r\n        labels=labels,\r\n        logits=logits\r\n    )\r\n    return loss\r\n\r\nmodel.compile(\r\n    ...,\r\n    loss ={\r\n        'logit_layer': my_loss\r\n    },\r\n)\r\n\r\n```\r\n\r\nI then proceed to train this model with `ModelCheckpoint` callbacks, and after training save it in a myriad of ways:\r\n\r\n\r\n```python\r\nmodel.save(...)\r\ntf.keras.experimental.export_saved_model(model, ...)\r\ntf.keras.models.save_model(model, ...)\r\nmodel.save_weights(...)\r\n```\r\n\r\nNow I try to load my saved model:\r\n\r\n```\r\ncustom = {\r\n    'my_loss': my_loss\r\n}\r\nmodel_file = # one of the files from above\r\nmodel = tf.keras.models.load_model(model_file, custom)\r\n```\r\n\r\nEach and every one of these models gives me the following error:\r\n\r\n```\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-7-52082d68b682> in <module>\r\n      1 model_file = os.path.join(model_to_use)\r\n----> 2 model = tf.keras.models.load_model(model_file, custom)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/save.py in load_model(filepath, custom_objects, compile)\r\n    145   if isinstance(filepath, six.string_types):\r\n    146     loader_impl.parse_saved_model(filepath)\r\n--> 147     return saved_model_load.load(filepath, compile)\r\n    148 \r\n    149   raise IOError(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/saved_model/load.py in load(path, compile)\r\n     91     if model._training_config is not None:  # pylint: disable=protected-access\r\n     92       model.compile(**saving_utils.compile_args_from_training_config(\r\n---> 93           model._training_config))  # pylint: disable=protected-access\r\n     94 \r\n     95   return model\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\r\n    334     # Prepare list of loss functions, same size of model outputs.\r\n    335     self.loss_functions = training_utils.prepare_loss_functions(\r\n--> 336         self.loss, self.output_names)\r\n    337 \r\n    338     target_tensors = self._process_target_tensor_for_compile(target_tensors)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_utils.py in prepare_loss_functions(loss, output_names)\r\n   1337   \"\"\"\r\n   1338   if isinstance(loss, collections_abc.Mapping):\r\n-> 1339     generic_utils.check_for_unexpected_keys('loss', loss, output_names)\r\n   1340     loss_functions = []\r\n   1341     for name in output_names:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/utils/generic_utils.py in check_for_unexpected_keys(name, input_dict, expected_values)\r\n    589     raise ValueError('Unknown entries in {} dictionary: {}. Only expected '\r\n    590                      'following keys: {}'.format(name, list(unknown),\r\n--> 591                                                  expected_values))\r\n    592 \r\n    593 \r\n\r\nValueError: Unknown entries in loss dictionary: ['logit_layer']. Only expected following keys: ['output_1']\r\n```\r\n\r\n\r\nI have found several github issues related to this:\r\n\r\n1. https://github.com/qqwweee/keras-yolo3/issues/481\r\n2. https://github.com/tensorflow/tensorflow/issues/28059\r\n3. https://github.com/tensorflow/tensorflow/issues/25938\r\n\r\nThe second issue claims to be duplicate of the third and seems different from what I do above.\r\nThe first issue, uses an actual layer to be the loss, and passes a permissive lambda function as the loss:\r\n\r\n```python\r\nlambda y_pred, y_true: y_pred\r\n``` \r\nThis does not seem the same as what I am doing.\r\n\r\n\r\n**Describe the expected behavior**\r\nThat providing the `customs` object is sufficient. \r\n\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nA MWE can be found here: https://gitlab.com/SumNeuron/neumf\r\nClone it then run\r\n```\r\npython docker.py -w ai -c build\r\npython docker.py -w ai -c up\r\n```\r\n\r\nThe notebook to run is: https://gitlab.com/SumNeuron/neumf/blob/master/notebooks/NeuMF.ipynb\r\n\r\nThis is based off of the official recommendations model\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@SumNeuron Can you please provide simple standalone code to reproduce the issue? Thanks!", "@SumNeuron Can you please provide simple standalone code to reproduce the issue? Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "i meet same issue too", "@Lu1352 Can you please open a new issue with a simple standalone code to reproduce the error? Thanks!"]}, {"number": 35749, "title": "TensorFlow 2.1: ImportError: DLL load failed: The specified module could not be found.", "body": "I am using \r\n\r\nPython 3.76\r\nTensorFlow 2.1\r\nInstalled using: pip install tensorflow\r\nProcessor:  Intel(R) Core(TM) i7-6500U CPU @ 2.50GHz, 2601 Mhz, 2 Core(s), 4 Logical Processor(s)\r\nLaptop System Model:  HP Spectre x360 Convertible\r\nReproduce: All I have to type is  \"import tensorflow as tf\" and it fails.\r\n\r\n**Note:** I also tried using tensorflow-cpu and still got the same issue.\r\n\r\nStack Trace:\r\n```\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Development\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Development\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Development\\Python\\Python37\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Development\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Development\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n<class 'ImportError'>, ImportError('Traceback (most recent call last):\\n  File \r\n\"C:\\\\Development\\\\Python\\\\Python37\\\\lib\\\\site-packages\\\\tensorflow_core\\\\python\\\\pywrap_tensorflow.py\", line 58, in <module>\\n    from \r\ntensorflow.python.pywrap_tensorflow_internal import *\\n  File \r\n\"C:\\\\Development\\\\Python\\\\Python37\\\\lib\\\\site-\r\npackages\\\\tensorflow_core\\\\python\\\\pywrap_tensorflow_internal.py\", line 28, in <module>\\n    _pywrap_tensorflow_internal = swig_import_helper()\\n  File \r\n\"C:\\\\Development\\\\Python\\\\Python37\\\\lib\\\\site-\r\npackages\\\\tensorflow_core\\\\python\\\\pywrap_tensorflow_internal.py\", line 24, in\r\n swig_import_helper\\n    _mod = imp.load_module(\\'_pywrap_tensorflow_internal\\', fp, pathname,\r\n description)\\n  File \"C:\\\\Development\\\\Python\\\\Python37\\\\lib\\\\imp.py\", line 242, in load_module\\n\r\n    return load_dynamic(name, filename, file)\\n  File \r\n\"C:\\\\Development\\\\Python\\\\Python37\\\\lib\\\\imp.py\", line 342, in load_dynamic\\n    return \r\n_load(spec)\\nImportError: DLL load failed: The specified module could not be found.\\n\\n\\nFailed to\r\n load the native TensorFlow runtime.\\n\\nSee https://www.tensorflow.org/install/errors\\n\\nfor some\r\n common reasons and solutions.  Include the entire stack trace\\nabove this error message when\r\n asking for help.'), <traceback object at 0x000001E0E43DCA48>\r\n\r\n\r\n\r\n<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n```\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I solved it today downloading and installing visual studio 2015-2019 x86 and x64 from here: \r\n[https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads](url)\r\n\r\nOther solution is downgrading tensorflow to 2.0:\r\n`pip install tensorflow==2.0`\r\n\r\nI have Python 3.7.6, CUDA 10.1 and cuDNN for CUDA 10.1 and I have followed this requeriments:\r\n[https://www.tensorflow.org/install/gpu](url)\r\n\r\nI hope it works for you also!", "Thanks! That worked for me too!", "> https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads\r\n\r\nWrong URL in markdown", "Best solution is just downgrading your tensorflow using\r\npip install tensorflow==2.0\r\n ", "It's work for me\r\n**pip install tensorflow==2.0**\r\nif you are using tensoflow-gpu\r\n**pip install --upgrade tensorflow-gpu==2.0**", "> I solved it today downloading and installing visual studio 2015-2019 x86 and x64 from here:\r\n> [https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads](url)\r\n> \r\n> Other solution is downgrading tensorflow to 2.0:\r\n> `pip install tensorflow==2.0`\r\n> \r\n> I have Python 3.7.6, CUDA 10.1 and cuDNN for CUDA 10.1 and I have followed this requeriments:\r\n> [https://www.tensorflow.org/install/gpu](url)\r\n> \r\n> I hope it works for you also!\r\n\r\nWorked! :) ", "1.https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads\r\n2. download  x64: vc_redist.x64.exe\r\n3.install it.\r\n\r\nIt works.", "> 1.https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads\r\n> 2. download x64: vc_redist.x64.exe\r\n> 3.install it.\r\n> \r\n> It works.\r\n\r\nI will try  this solution ", "> 1.https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads\r\n> 2. download x64: vc_redist.x64.exe\r\n> 3.install it.\r\n> \r\n> It works.\r\n\r\nThanks! It works for me! ", "> 1.https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads\r\n> 2. download x64: vc_redist.x64.exe\r\n> 3.install it.\r\n> \r\n> It works.\r\n\r\nThanks!!! it works....", "Guys, please stop spamming with \"it works\" messages.", "i have python 3.6.0 , CUDA 10.2 and cuDNN and this fixed my problem :\r\n- downgrade to tensorflow-gpu 2.0 , and downgrade to protobuf 3.6.0  ,  install Microsoft Visual C++ Redistributable 2015-2019 x86 /  x64", "@AhmedSheha0 that's a terrible advice.\r\n\r\nSolution is given in https://github.com/tensorflow/tensorflow/issues/35749#issuecomment-588425399 (among other places). Let's not hide the solution with fake solutions and spam", "> @AhmedSheha0 that's a terrible advice.\r\n> \r\n> Solution is given in [#35749 (comment)](https://github.com/tensorflow/tensorflow/issues/35749#issuecomment-588425399) (among other places). Let's not hide the solution with fake solutions and spam\r\n\r\nIm sharing what worked for me .. You can dislike my advice , or claim it didnt work for you , that's your right but it's not too nice of you to say it's a \"terrible\" advice and that I'm spamming and giving out fake solutions.", "Tensorflow-gpu 2.1\r\nCuda 10.1\r\nPython 3.6.5\r\ncuDNN 7.6.4\r\nHad the same problem, and upgrading Visual C totally worked for me.", "@AhmedSheha0 no, it is a terrible advice because 2.0 is not a LTS release hence after a year it will no longer receive security updates (just one example of why downgrading is not a solution)\r\n\r\nAlso, think how many people are notified and how many confusing \"solutions\" are in these threads. Also, notice how many people replied with the same spammy advice of \"it works for me if you downgrade\"\r\n\r\nWe could just lock thread to contributors to prevent such spam, but we rely on people who comment here to think before they write.\r\n\r\nTo conclude, the real solution is to install the latest MSVC redistributable from https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads **and not** to downgrade to 2.0"]}, {"number": 35748, "title": "tflite micro softmax op is still version 1", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from: Source\r\n- Tensorflow version: 4b3c1199a97cb36b8866d98e7036f4ec3e70abd6\r\n- Target platform: Apollo3\r\n\r\n**Describe the problem**\r\n\r\nThe tflite micro softmax op in [tensorflow/lite/micro/kernels/softmax.cc](https://github.com/tensorflow/tensorflow/blob/4b3c1199a97cb36b8866d98e7036f4ec3e70abd6/tensorflow/lite/micro/kernels/softmax.cc) already has int8 input support.\r\nFrom what I understand this should be version 2 in [tensorflow/lite/micro/kernels/all_ops_resolver.cc](https://github.com/tensorflow/tensorflow/blob/4b3c1199a97cb36b8866d98e7036f4ec3e70abd6/tensorflow/lite/micro/kernels/all_ops_resolver.cc#L26)\r\n\r\n", "comments": ["Thanks for reporting this bug.  I have landed a patch to fix this operator as well as a handful of others internally and it should show up on Github soon.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35748\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35748\">No</a>\n"]}, {"number": 35747, "title": "Tensorflow Lite download_dependencies.sh doesn't work", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 10 (buster)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not a mobile device, it's a FreeWave ZumIQ edge computer. CPU is the same CPU as in a BeagleBone Black.\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: See description\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): gcc (Debian 8.3.0-6) 8.3.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI'm trying to build a custom TF Lite wheel for this hardware. As a shortcut, I tried using the Raspberry Pi wheel, but the processor is different and so I got an \"Illegal instruction\" error. Therefore, I have to build my own.\r\n\r\nI did this successfully last summer, with r1.14. It's really easy to do: I run the **download_dependencies.sh** script, then I do a couple of minor edits to the **build_pip_package.sh** script and run that script. \r\n\r\nNow it doesn't work anymore. It's the **download_dependencies.sh** that is causing the problem.\r\n\r\nBy the way, my shell is **/bin/bash** and my **uname-a** output is\r\n\r\n`Linux fwt1311tp-min-jail-agent 4.4.0-170-generic #199-Ubuntu SMP Thu Nov 14 01:45:04 UTC 2019 armv7l GNU/Linux`\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\n$ ./tensorflow/lite/tools/make/download_dependencies.sh\r\ndownloading https://gitlab.com/libeigen/eigen/-/archive/e6fcee995b0083e5652c79957090684a47a727c3/eigen-e6fcee995b0083e5652c79957090684a47a727c3.tar.gz\r\n\r\ngzip: stdin: unexpected end of file\r\n/bin/tar: Child returned status 1\r\n/bin/tar: Error is not recoverable: exiting now\r\n```\r\n \r\nSo I edit **download_dependencies.sh**. Lines 63 and 65 are:\r\n\r\n```\r\n63   if [[ \"${url}\" == *gz ]]; then\r\n64  ...\r\n55   elif [[ \"${url}\" == *zip ]]; then\r\n```\r\n\r\nI end up wrapping the **star gz** and **star zip** in double quotes.\r\n\r\nThis time, I get this far:\r\n\r\n```\r\n$ ./tensorflow/lite/tools/make/download_dependencies.sh\r\ndownloading https://gitlab.com/libeigen/eigen/-/archive/e6fcee995b0083e5652c79957090684a47a727c3/eigen-e6fcee995b0083e5652c79957090684a47a727c3.tar.gz\r\ndownloading https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/gemmlowp/archive/12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3.zip\r\ndownloading https://github.com/google/googletest/archive/release-1.8.0.tar.gz\r\ndownloading https://github.com/abseil/abseil-cpp/archive/43ef2148c0936ebf7cb4be6b19927a9d9d145b8f.tar.gz\r\ndownloading https://github.com/intel/ARM_NEON_2_x86_SSE/archive/master.zip\r\ndownloading https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/farmhash/archive/816a4ae622e964763ca0862d9dbd19324a1eaf45.tar.gz\r\ndownloading https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/flatbuffers/archive/v1.11.0.tar.gz\r\ndownloading https://storage.googleapis.com/mirror.tensorflow.org/www.kurims.kyoto-u.ac.jp/~ooura/fft2d.tgz\r\n/bin/sed: can't read tensorflow/lite/tools/make/downloads/eigen/Eigen/src/Core/arch/NEON/Complex.h: No such file or directory\r\n```\r\nAnd, in fact, the directory **./tensorflow/lite/tools/make/downloads/eigen** is empty. All of the **downloads** directories are empty. Nothing was downloaded.\r\n\r\nI `git checkout r1.14` instead, to see if it makes any difference. It doesn't. I get the same failures.\r\n\r\n**Any other info / logs**\r\n\r\nRather than repeat the source code, I will point you to **tensorflow/lite/tools/make/download_dependencies.sh**.\r\n", "comments": ["More information: I tried it on a different Linux system and it worked. Back to my hardware. I took the **curl** statement in line 64, omitted the **-s** option, and ran it from the command line:\r\n```\r\ncurl -L https://gitlab.com/libeigen/eigen/-/archive/e6fcee995b0083e5652c79957090684a47a727c3/eigen-e6fcee995b0083e5652c79957090684a47a727c3.tar.gz | tar -xz\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r\ncurl: (60) SSL certificate problem: unable to get local issuer certificate\r\nMore details here: https://curl.haxx.se/docs/sslcerts.html\r\n\r\ncurl failed to verify the legitimacy of the server and therefore could not\r\nestablish a secure connection to it. To learn more about this situation and\r\nhow to fix it, please visit the web page mentioned above.\r\n\r\ngzip: stdin: unexpected end of file\r\n/bin/tar: Child returned status 1\r\n/bin/tar: Error is not recoverable: exiting now\r\n```\r\nSo the \"silent\" operation was masking the SSL certificate problem. After reading the page at https://curl.haxx.se/docs/sslcerts.html, I added the **-k** option to the **curl** commands on lines 64 and 69. That's a bandaid. I'll get the SSL certificate later.\r\n\r\nThis time, the files downloaded successfully, but immediately after downloading I got this error:\r\n```\r\n/bin/sed: can't read tensorflow/lite/tools/make/downloads/eigen/Eigen/src/Core/arch/NEON/Complex.h: Permission denied\r\n```\r\nI'm not sure what it's complaining about. I'm logged in as **devuser**, and **devuser** owns the file in question, and the directory as well:\r\n```\r\n$ whoami\r\ndevuser\r\n$ ll tensorflow/lite/tools/make/downloads/eigen/Eigen/src/Core/arch/NEON\r\ntotal 60\r\n-rw-r--r-- 1 devuser devuser 19035 Jan 10 18:59 Complex.h\r\n-rw-r--r-- 1 devuser devuser  1298 Jan  9 00:04 MathFunctions.h\r\n-rw-r--r-- 1 devuser devuser 31476 Jan  9 00:04 PacketMath.h\r\n-rw-r--r-- 1 devuser devuser  1338 Jan  9 00:04 TypeCasting.h\r\n$ ll -d tensorflow/lite/tools/make/downloads/eigen/Eigen/src/Core/arch/NEON\r\ndrwxr-xr-x 2 devuser devuser 4096 Jan 10 18:59 tensorflow/lite/tools/make/downloads/eigen/Eigen/src/Core/arch/NEON\r\n```\r\n", "I think I fixed it -- but it's a patch to a patch, and nothing to be proud of.\r\n\r\nWhile examining the **download_dependencies.sh** code, I noticed that `replace_by_sed()` is called three times at the end of the script.  According to the comments for `replace_by_sed()` (_cheers to the person who put those comments there!_), the `replace_by_sed()` calls are a patch to make **download_dependencies.sh** work with iOS arm64. Since I'm compiling for Linux and armv7l, I concluded that this patch does not apply to my project, and so I commented out the calls to `replace_by_sed()`. This time, **download_dependencies.sh** completed successfully.\r\n\r\nI know that a patch to a patch is not sustainable in the long term, but it worked for me.", "Thanks for the detailed report and your self investigation. I see 3 separate issues in your report.\r\n\r\n1. `*gz` and `*zip` not quoted - We could fix this issue in the script itself.\r\n2. SSL certificate issue - Sounds like this is something you should fix on your machine with the correct certificates installed.\r\n3. permission issue with sed - I'm not sure why this is happening, and this would be difficult to reproduce without the actual hardware / OS setup you're using.\r\n\r\nSo, in summary I think we could fix 1, but not 2 and 3. Will send a patch soon.", "Actually, I think 1 should be quoted like `*\"gz\"` and `*\"zip\"`, with the wildcard outside of the double quotes. Did you also do this? or did you quote the entire string with the `*` character?\r\n\r\nIn the latter case, it is expected to not work correctly, because the `*` character is interpreted as a literal character, and not a wildcard.", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I also had the same issue and I solved by replacing the following line - \r\n`EIGEN_URL=\"$(grep -o 'http.*bitbucket.org/eigen/eigen/get/.*tar\\.gz' \"${BZL_FILE_PATH}\" | grep -v mirror.tensorflow | head -n1)\"`\r\nwith \r\n`EIGEN_URL=\"$(grep -o 'https.*bitbucket.org/eigen/eigen/get/.*tar\\.gz' \"${BZL_FILE_PATH}\" | grep -v mirror.bazel | head -n1)\"`"]}, {"number": 35746, "title": "Cannot train the person detection test on Tensorflow 2.0", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- Tensorflow version (commit SHA if source):\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):\r\n\r\n**Describe the problem**\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\n", "comments": ["This is currently a work in progress. We'll update this issue once we have something we can share.\r\n\r\n", "Update:  Due to the lack of of support for full int-8 quantization in TF2.0, we are currently only supporting model training in TF1.x.  "]}, {"number": 35745, "title": "Add a warning that tfds.load can not be used for own Datasets", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/datasets/api_docs/python/tfds/load\r\n\r\n## Description of issue (what needs changing):\r\nAdd a warning that `tfds.load()` can not be used for the users own Datasets, i.e. that he creates himself. To a new user trying to to load a Dataset from a set of files it is not obvious that this method is only for pre-made, immutable Datasets.\r\n\r\nAlthough it does say\r\n> Loads the named dataset into a tf.data.Dataset.\r\n\r\ni initially interpreted it such that my own Dataset can be assigned a name. \r\n\r\nI was looking for a way to split a Dataset into train and validation subsets and stumbled upon this documentation. I was redirected from https://www.tensorflow.org/datasets/splits which comes up as one of the most prominent search results when searching for \"tensorflow Dataset splits\" .\r\n\r\n## Result\r\nA user who visits https://www.tensorflow.org/datasets/api_docs/python/tfds/load will not spend 1 h of trying to understand all the documentation but will immediately realize that this is only for immutable pre-made Datasets.", "comments": ["Hey, @PanCakeConnaisseur Tensorflow Datasets (tfds) is used to load the datasets which are present on the library. For knowing which datasets are included in this library you can\r\nUse ```tfds.list_builder()``` . For loading your custom dataset you can use ```tf.data```.\r\nSee tf.data documention : https://www.tensorflow.org/guide/data\r\nIf your query has solved then kindly close this issue. ", "Hi everyone, i have a query, I have made a customized dataset by using different images available online. Can i add that dataset in the tfds directory ?\r\n\r\nLooking forward to your replies \r\n ", "Thanks for the issue we can only load datasets (pre-made) mentioned in the catalog [here](https://www.tensorflow.org/datasets/catalog/overview).\r\nPerhaps we can add this to the function documentation. Thanks!", "Sorry for the late answer.\r\n\r\nThis is not true. `tfds.load` can be used to load any datasets (including user defined) as long as they have been imported. Otherwise TFDS has no way to know where the datasets is declared.\r\n\r\n```python\r\nimport tensorflow_datasets as tfds\r\n\r\nimport my_custom_dataset  # Register `MyCustomDataset`\r\n\r\ntfds.load('my_custom_dataset')\r\n```\r\nTo create a TFDS dataset, you can use the following guide: https://www.tensorflow.org/datasets/add_dataset", "I only see this issue now. In the future, please post the `tfds` issues in https://github.com/tensorflow/datasets as TFDS library is developed independently from TensorFlow.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 35744, "title": "tf.shape shape mismatched in custom loss function", "body": "Running on tf2.1.0-rc and python 3.6.8/ I wrote a custom loss function, but somehow I always get tf.shape error message\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom scipy.stats import rankdata\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow import keras\r\n\r\nBATCH_SIZE = 4\r\nnum_targets =9\r\n\r\n# Define custom py_func which takes also a grad op as argument:\r\ndef py_func(func, inp, Tout, name=None, grad=None):\r\n    \r\n    rnd_name = 'PyFuncGrad' + str(np.random.randint(0, 1E+8))\r\n    \r\n    tf.RegisterGradient(rnd_name)(grad)  \r\n    g = tf.compat.v1.get_default_graph()\r\n    with g.gradient_override_map({\"PyFunc\": rnd_name}):\r\n        return tf.py_function(func, inp, Tout, name=name)\r\n\r\n# Def custom square function using np.square instead of tf.square:\r\ndef myrankdata(x, name=None):\r\n    \r\n    with ops.op_scope([x], name, \"MyGrad\") as name:\r\n        rank = py_func(rankdata,\r\n                        [x],\r\n                        [tf.float32],\r\n                        name=name,\r\n                        grad=_MyGrad)  \r\n        return rank\r\n\r\n# Actual gradient:\r\ndef _MyGrad(op, grad):\r\n    return grad * 20 * op.inputs[0]  \r\n\r\ndef custom_loss(num_targets):\r\n    def custom_rhos(y_true, y_pred):\r\n        rhos = []\r\n        y_true, y_pred = tf.reshape(y_true, [-1, num_targets]), tf.reshape(y_pred, [-1, num_targets])\r\n        for ind in range(num_targets):\r\n            a = tf.slice(y_true, [0, ind], [-1, 1])\r\n            a = tf.reshape(a, [-1])\r\n            b = tf.slice(y_pred, [0, ind], [-1, 1])\r\n            b = tf.reshape(b, [-1])\r\n            rank_a, rank_b = myrankdata(a)[0], myrankdata(b)[0]\r\n            rho = 1 - 6 * tf.reduce_sum((rank_a - rank_b) ** 2) / (BATCH_SIZE ** 3 - BATCH_SIZE)\r\n            rhos.append(rho)\r\n        return -tf.reduce_sum(rhos)\r\n    return custom_rhos\r\n\r\n```\r\n\r\nMy model summary is\r\n```\r\nModel: \"model_3\"\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_category (InputLayer)     [(None, 1)]          0                                            \r\n__________________________________________________________________________________________________\r\ninput_host (InputLayer)         [(None, 1)]          0                                            \r\n__________________________________________________________________________________________________\r\nq_input_word_ids (InputLayer)   [(None, None)]       0                                            \r\n__________________________________________________________________________________________________\r\nq_input_masks (InputLayer)      [(None, None)]       0                                            \r\n__________________________________________________________________________________________________\r\nq_segment_ids (InputLayer)      [(None, None)]       0                                            \r\n__________________________________________________________________________________________________\r\nembedding_6 (Embedding)         (None, 1, 32)        192         input_category[0][0]             \r\n__________________________________________________________________________________________________\r\nembedding_7 (Embedding)         (None, 1, 32)        2080        input_host[0][0]                 \r\n__________________________________________________________________________________________________\r\nkeras_layer_3 (KerasLayer)      [(None, 768), (None, 109482241   q_input_word_ids[0][0]           \r\n                                                                 q_input_masks[0][0]              \r\n                                                                 q_segment_ids[0][0]              \r\n__________________________________________________________________________________________________\r\nspatial_dropout1d_6 (SpatialDro (None, 1, 32)        0           embedding_6[0][0]                \r\n__________________________________________________________________________________________________\r\nspatial_dropout1d_7 (SpatialDro (None, 1, 32)        0           embedding_7[0][0]                \r\n__________________________________________________________________________________________________\r\nglobal_average_pooling1d_3 (Glo (None, 768)          0           keras_layer_3[0][1]              \r\n__________________________________________________________________________________________________\r\nconcatenate_6 (Concatenate)     (None, 2, 32)        0           spatial_dropout1d_6[0][0]        \r\n                                                                 spatial_dropout1d_7[0][0]        \r\n__________________________________________________________________________________________________\r\ndropout_3 (Dropout)             (None, 768)          0           global_average_pooling1d_3[0][0] \r\n__________________________________________________________________________________________________\r\nflatten_3 (Flatten)             (None, 64)           0           concatenate_6[0][0]              \r\n__________________________________________________________________________________________________\r\nconcatenate_7 (Concatenate)     (None, 832)          0           dropout_3[0][0]                  \r\n                                                                 flatten_3[0][0]                  \r\n__________________________________________________________________________________________________\r\ndense_3 (Dense)                 (None, 9)            7497        concatenate_7[0][0]              \r\n==================================================================================================\r\nTotal params: 109,492,010\r\nTrainable params: 109,492,009\r\nNon-trainable params: 1\r\n__________________________________________________________________________________________________\r\n```\r\n\r\nHere is my error message\r\n\r\n```\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument:  Input to reshape is a tensor with 1 values, but the requested shape has 4\r\n\t [[node Reshape_41 (defined at <ipython-input-147-9b8e1e62f3d2>:10) ]]\r\n\t [[Reshape_74/_626]]\r\n  (1) Invalid argument:  Input to reshape is a tensor with 1 values, but the requested shape has 4\r\n\t [[node Reshape_41 (defined at <ipython-input-147-9b8e1e62f3d2>:10) ]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_distributed_function_868236]\r\n\r\nFunction call stack:\r\ndistributed_function -> distributed_function\r\n```", "comments": ["@acmilannesta \r\n\r\nLooks like code is incomplete. Request you to provide colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "@acmilannesta \r\nAny update on this issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 35743, "title": "Multi-GPU training issue, tensorflow 2.0.0", "body": "I am trying to train my code on multiple-GPUs and have followed the tutorials online on using MirroredStrategy using MNIST dataset. Below is the error... \r\n\r\nValueError: `handle` is not available outside the replica context or a `tf.distribute.Strategy.update()` call.\r\n\r\nI have a feeling that the issue is due to the fact that when I call on my GPUs. They are named XLA and not :/device:GPU. This leads to me to believe it is a bug? Below is my full code and following that the full error. I am currently using tensorflow 2.0.0, CUDA 10.1, and CentOS 7.6.1810\r\n\r\nMy code is below...\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.applications import Xception\r\nfrom tensorflow.keras.utils import multi_gpu_model\r\nimport numpy as np\r\n\r\nnum_samples = 1000\r\nheight = 224\r\nwidth = 224\r\nnum_classes = 1000\r\n\r\n#this puts the models weights on the CPU \r\nwith tf.device(device_names[1]):\r\n    model = Xception(weights = None, \r\n                     input_shape = (height, width, 3), \r\n                     classes = num_classes)\r\n\r\n#this splits up the training amongst multiple GPUs\r\nmirrored_strategy = tf.distribute.MirroredStrategy(devices = [device_names[3], device_names[4]])\r\n\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\nimport tensorflow_datasets as tfds \r\nimport tensorflow as tf\r\ntfds.disable_progress_bar()\r\nimport os\r\nprint(tf.__version__)\r\n\r\n2.0.0\r\n\r\ndatasets, info = tfds.load(name = 'mnist', with_info = True, as_supervised = True)\r\nmnist_train, mnist_test = datasets['train'], datasets['test']\r\n#this splits up the training amongst multiple specific GPUs\r\nstrategy = tf.distribute.MirroredStrategy(devices = [device_names[2], device_names[3]])\r\nprint('Number of devices: {}'.format(strategy.num_replicas_in_sync))\r\nprint('Name of devices are: ' + device_names[2] + ' and ' + device_names[3])\r\n\r\nNumber of devices: 2\r\nName of devices are: /device:XLA_GPU:0 and /device:XLA_GPU:1\r\n\r\n#The benefit of using multiple GPUs is that you can train with the largest batchsize\r\n#then you can just tweak the learning_rate accordingly\r\n\r\nnum_train_examples = info.splits['train'].num_examples\r\nnum_test_examples = info.splits['test'].num_examples\r\n\r\nBUFFER_SIZE = 10000\r\n\r\nBATCH_SIZE_PER_REPLICA = 64\r\ndef scale(image, label):\r\n    image = tf.cast(image, tf.float32)\r\n    image /=255\r\n    \r\n    return image, label\r\n\r\ntrain_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\r\neval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)\r\n\r\nwith strategy.scope(): #the scope portion indicates which parts of the code will be distributed\r\n    model = tf.keras.Sequential([\r\n        tf.keras.layers.Conv2D(32, 3, activation = 'relu', input_shape = (28,28,1)),\r\n        tf.keras.layers.MaxPooling2D(), \r\n        tf.keras.layers.Flatten(), \r\n        tf.keras.layers.Dense(64, activation = 'relu'),\r\n        tf.keras.layers.Dense(10, activation = 'softmax')\r\n    ])\r\n\r\nimport datetime\r\nlog_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir, histogram_freq = 1)\r\nmodel.compile(loss = 'sparse_categorical_crossentropy', \r\n              optimizer = tf.keras.optimizers.Adam(), \r\n              metrics = ['accuracy'])\r\n\r\ndef decay(epoch):\r\n    if epoch < 3:\r\n        return 1e-3\r\n    elif epoch >= 3 and epoch < 7:\r\n        return 1e-4\r\n    else:\r\n        return 1e-5\r\ncheckpoint_dir = './training_checkpoints'\r\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\r\n\r\nclass PrintLR(tf.keras.callbacks.Callback):\r\n    def on_epoch_end(self, epoch, logs = None):\r\n        print('\\nLearning rate for epoch {} is {}'.format(epoch + 1, model.optimizer.lr.numpy()))\r\n\r\ncallbacks = [tensorboard_callback, \r\n             tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_prefix, \r\n                                                save_weights_only = True), \r\n             tf.keras.callbacks.LearningRateScheduler(decay), \r\n             PrintLR()\r\n            ]\r\n\r\n\r\nmodel.fit(train_dataset, epochs = 30, callbacks = callbacks)\r\n\r\nFULL ERROR: \r\n-------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-12-1c5af51f2d91> in <module>\r\n----> 1 model.fit(train_dataset, epochs = 30, callbacks = callbacks)\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    726         max_queue_size=max_queue_size,\r\n    727         workers=workers,\r\n--> 728         use_multiprocessing=use_multiprocessing)\r\n    729 \r\n    730   def evaluate(self,\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n    322                 mode=ModeKeys.TRAIN,\r\n    323                 training_context=training_context,\r\n--> 324                 total_epochs=epochs)\r\n    325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)\r\n    326 \r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    121         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n    122       try:\r\n--> 123         batch_outs = execution_function(iterator)\r\n    124       except (StopIteration, errors.OutOfRangeError):\r\n    125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)\r\n     84     # `numpy` translates Tensors to values in Eager mode.\r\n     85     return nest.map_structure(_non_none_constant_value,\r\n---> 86                               distributed_function(input_fn))\r\n     87 \r\n     88   return execution_function\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    455 \r\n    456     tracing_count = self._get_tracing_count()\r\n--> 457     result = self._call(*args, **kwds)\r\n    458     if tracing_count == self._get_tracing_count():\r\n    459       self._call_counter.called_without_tracing()\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    501       # This is the first call of __call__, so we have to initialize.\r\n    502       initializer_map = object_identity.ObjectIdentityDictionary()\r\n--> 503       self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n    504     finally:\r\n    505       # At this point we know that the initialization is complete (or less\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    406     self._concrete_stateful_fn = (\r\n    407         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 408             *args, **kwds))\r\n    409 \r\n    410     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   1846     if self.input_signature:\r\n   1847       args, kwargs = None, None\r\n-> 1848     graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   1849     return graph_function\r\n   1850 \r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   2148         graph_function = self._function_cache.primary.get(cache_key, None)\r\n   2149         if graph_function is None:\r\n-> 2150           graph_function = self._create_graph_function(args, kwargs)\r\n   2151           self._function_cache.primary[cache_key] = graph_function\r\n   2152         return graph_function, args, kwargs\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2039             arg_names=arg_names,\r\n   2040             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 2041             capture_by_value=self._capture_by_value),\r\n   2042         self._function_attributes,\r\n   2043         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    913                                           converted_func)\r\n    914 \r\n--> 915       func_outputs = python_func(*func_args, **func_kwargs)\r\n    916 \r\n    917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    356         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    357         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 358         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    359     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    360 \r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in distributed_function(input_iterator)\r\n     71     strategy = distribution_strategy_context.get_strategy()\r\n     72     outputs = strategy.experimental_run_v2(\r\n---> 73         per_replica_function, args=(model, x, y, sample_weights))\r\n     74     # Out of PerReplica outputs reduce or pick values to return.\r\n     75     all_outputs = dist_utils.unwrap_output_dict(\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py in experimental_run_v2(self, fn, args, kwargs)\r\n    758       fn = autograph.tf_convert(fn, ag_ctx.control_status_ctx(),\r\n    759                                 convert_by_default=False)\r\n--> 760       return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    761 \r\n    762   def reduce(self, reduce_op, value, axis):\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py in call_for_each_replica(self, fn, args, kwargs)\r\n   1785       kwargs = {}\r\n   1786     with self._container_strategy().scope():\r\n-> 1787       return self._call_for_each_replica(fn, args, kwargs)\r\n   1788 \r\n   1789   def _call_for_each_replica(self, fn, args, kwargs):\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py in _call_for_each_replica(self, fn, args, kwargs)\r\n   2130         self._container_strategy(),\r\n   2131         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\r\n-> 2132       return fn(*args, **kwargs)\r\n   2133 \r\n   2134   def _reduce_to(self, reduce_op, value, destinations):\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    290   def wrapper(*args, **kwargs):\r\n    291     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\r\n--> 292       return func(*args, **kwargs)\r\n    293 \r\n    294   if inspect.isfunction(func) or inspect.ismethod(func):\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in train_on_batch(model, x, y, sample_weight, class_weight, reset_metrics)\r\n    262       y,\r\n    263       sample_weights=sample_weights,\r\n--> 264       output_loss_metrics=model._output_loss_metrics)\r\n    265 \r\n    266   if reset_metrics:\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorfl<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\now_core/python/keras/engine/training_eager.py in train_on_batch(model, inputs, targets, sample_weights, output_loss_metrics)\r\n    309           sample_weights=sample_weights,\r\n    310           training=True,\r\n--> 311           output_loss_metrics=output_loss_metrics))\r\n    312   if not isinstance(outs, list):\r\n    313     outs = [outs]\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py in _process_single_batch(model, inputs, targets, output_loss_metrics, sample_weights, training)\r\n    270                         loss_scale_optimizer.LossScaleOptimizer):\r\n    271             grads = model.optimizer.get_unscaled_gradients(grads)\r\n--> 272           model.optimizer.apply_gradients(zip(grads, trainable_weights))\r\n    273       else:\r\n    274         logging.warning('The list of trainable weights is empty. Make sure that'\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py in apply_gradients(self, grads_and_vars, name)\r\n    439           functools.partial(self._distributed_apply, apply_state=apply_state),\r\n    440           args=(grads_and_vars,),\r\n--> 441           kwargs={\"name\": name})\r\n    442 \r\n    443   def _distributed_apply(self, distribution, grads_and_vars, name, apply_state):\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py in merge_call(self, merge_fn, args, kwargs)\r\n   1915     if kwargs is None:\r\n   1916       kwargs = {}\r\n-> 1917     return self._merge_call(merge_fn, args, kwargs)\r\n   1918 \r\n   1919   def _merge_call(self, merge_fn, args, kwargs):\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py in _merge_call(self, merge_fn, args, kwargs)\r\n   1922         distribution_strategy_context._CrossReplicaThreadMode(self._strategy))  # pylint: disable=protected-access\r\n   1923     try:\r\n-> 1924       return merge_fn(self._strategy, *args, **kwargs)\r\n   1925     finally:\r\n   1926       _pop_per_thread_mode()\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py in _distributed_apply(self, distribution, grads_and_vars, name, apply_state)\r\n    480         # delays. See b/136304694.\r\n    481         with backend.name_scope(\r\n--> 482             scope_name), distribution.extended.colocate_vars_with(var):\r\n    483           update_ops.extend(\r\n    484               distribution.extended.update(\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/contextlib.py in __enter__(self)\r\n     79     def __enter__(self):\r\n     80         try:\r\n---> 81             return next(self.gen)\r\n     82         except StopIteration:\r\n     83             raise RuntimeError(\"generator didn't yield\") from None\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py in _colocate_with_for_gradient(self, op, gradient_uid, ignore_existing)\r\n   4218   def _colocate_with_for_gradient(self, op, gradient_uid,\r\n   4219                                   ignore_existing=False):\r\n-> 4220     with self.colocate_with(op, ignore_existing):\r\n   4221       if gradient_uid is not None and self._control_flow_context is not None:\r\n   4222         self._control_flow_context.EnterGradientColocation(op, gradient_uid)\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/contextlib.py in __enter__(self)\r\n     79     def __enter__(self):\r\n     80         try:\r\n---> 81             return next(self.gen)\r\n     82         except StopIteration:\r\n     83             raise RuntimeError(\"generator didn't yield\") from None\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py in colocate_with(self, op, ignore_existing)\r\n   4267       raise ValueError(\"Trying to reset colocation (op is None) but \"\r\n   4268                        \"ignore_existing is not True\")\r\n-> 4269     op = _op_to_colocate_with(op, self)\r\n   4270 \r\n   4271     # By default, colocate_with resets the device function stack,\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py in _op_to_colocate_with(v, graph)\r\n   6601   # happen soon, perhaps this hack to work around the circular\r\n   6602   # import dependency is acceptable.\r\n-> 6603   if hasattr(v, \"handle\") and hasattr(v.handle, \"op\") and isinstance(\r\n   6604       v.handle.op, Operation):\r\n   6605     if graph.building_function:\r\n\r\n~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/distribute/values.py in handle(self)\r\n    715       device = distribute_lib.get_update_device()\r\n    716       if device is None:\r\n--> 717         raise ValueError(\"`handle` is not available outside the replica context\"\r\n    718                          \" or a `tf.distribute.Strategy.update()` call.\")\r\n    719     return self.get(device=device).handle\r\n\r\nValueError: `handle` is not available outside the replica context or a `tf.distribute.Strategy.update()` call.", "comments": ["do you have NCCL? If not, you need to use `tf.distribute.HierarchicalCopyAllReduce()` but not default function 'tf.distribute.NcclAllReduce()'", "@HLSS-Hen Yeah, I just checked. I have NCCL 2. Either way, I tried inputting tf.distribute.HierarchicalCopyAllReduce() as follows...\r\n\r\ntf.distribute.MirroredStrategy(cross_device_ops = tf.distribute.HierarchicalCopyAllReduce(), \r\ndevices = [device_name[0], device_name[1]])\r\n\r\nBut, sadly, I get the same error", "@etorrjr10 Can you repaste the code snippet to clearly show which parts of the code are under strategy scope? For example, are you calling model.compile() under strategy scope? Also why do you need to specify a device scope for your model?\r\n```\r\nwith tf.device(device_names[1]):\r\n```\r\n", "Please reopen if you are still having issues. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35743\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35743\">No</a>\n", "I am also getting same error while handling gradient tape.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"train_model.py\", line 211, in <module>\r\n    main()\r\n  File \"train_model.py\", line 194, in main\r\n    batch_loss = train_step(inp, targ, enc_hidden, targ_lang, encoder, decoder)\r\n  File \"train_model.py\", line 115, in train_step\r\n    gradients = tape.gradient(loss, variables)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/eager/backprop.py\", line 1011, in gradient\r\n    flat_sources = [_handle_or_self(x) for x in flat_sources]\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/eager/backprop.py\", line 697, in _handle_or_self\r\n    x = x.handle\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/distribute/values.py\", line 720, in handle\r\n    raise ValueError(\"`handle` is not available outside the replica context\"\r\nValueError: `handle` is not available outside the replica context or a `tf.distribute.Strategy.update()` call.\r\n``` \r\n\r\nMy code:\r\n```\r\nstrategy = tf.distribute.MirroredStrategy()\r\nwith strategy.scope():\r\n    LOSS_OBJECT = tf.keras.losses.SparseCategoricalCrossentropy(\r\n    from_logits=True, reduction='none')\r\n\r\n    OPTIMIZER = tf.keras.optimizers.Adam()\r\n\r\n\r\nwith strategy.scope():\r\n    #@tf.function\r\n    def train_step(inp, targ, enc_hidden, targ_lang, encoder, decoder):\r\n        loss = 0\r\n\r\n        with tf.GradientTape(persistent=True) as tape:\r\n            enc_output, enc_hidden = encoder(inp, enc_hidden)\r\n\r\n            dec_hidden = enc_hidden\r\n\r\n            dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\r\n\r\n            # Teacher forcing - feeding the target as the next input\r\n            for t in range(1, targ.shape[1]):\r\n                # passing enc_output to the decoder\r\n                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\r\n\r\n                loss += loss_function(targ[:, t], predictions)\r\n\r\n                # using teacher forcing\r\n                dec_input = tf.expand_dims(targ[:, t], 1)\r\n\r\n            batch_loss = (loss / int(targ.shape[1]))\r\n\r\n            variables = encoder.trainable_variables + decoder.trainable_variables\r\n\r\n            gradients = tape.gradient(loss, variables)\r\n\r\n            OPTIMIZER.apply_gradients(zip(gradients, variables))\r\n\r\n        return batch_loss\r\n```\r\n\r\nI am using ```Tensorflow==2.1.0``` and ```cuda 10.1```"]}, {"number": 35742, "title": "How to add a new op to a pb model", "body": "This problem is rather embarassing: I have a pb model at hand but not ready for deployment, because of a missing op. So, I doubt if I could add the required op to the pb and export a new pb for deployment?\r\n\r\nCould anyone shed some light please?", "comments": ["@stoneyang,\r\nHi, Please provide more information on the issue,share a standalone code to reproduce the error reported here,also mention the TF version being used. Thanks!", "@stoneyang ,\r\nAny update on the issue ?Thanks!"]}, {"number": 35741, "title": "fake_quant_with_min_max_vars innefficiencies", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\nTensorFlow installed from (source or binary): Binary\r\nTensorFlow version (use command below): v1.12.1-21401-gd908b50 2.1.0-dev20191230\r\nPython version: 3.6.9\r\nBazel version (if compiling from source): NA\r\nGCC/Compiler version (if compiling from source): NA\r\nCUDA/cuDNN version: CUDA Version 10.1.243 / cuDNN 7.6.4.38-1\r\nGPU model and memory: TITAN V, 12GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nA call to fake_quant_with_min_max_vars consistently results in a couple of D2H transfers before the quantization kernel is executed. I believe these transfers are part of ValidateInputTypeAndPlacement and largely dominate the operation cost. This is slowing down to unbearable levels the training of large NNs with fake quantization nodes.\r\n\r\nAn image of a profile resulting from back to back dependent quantization calls:\r\n\r\n![image](https://user-images.githubusercontent.com/24900898/72160140-d7aaff00-338b-11ea-9215-d44440bb65fa.png)\r\n\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nI would expect little to zero overhead before the actual quantization kernel.\r\n\r\n**Code to reproduce the issue**\r\n\r\n`import tensorflow as tf\r\nimport numpy as np\r\nimport time\r\nfrom tensorflow.python import eager\r\nimport os\r\n\r\nx = tf.random.uniform(shape=[10000,1000])\r\nxmax = tf.Variable(0.5)\r\n\r\neager.profiler.start()\r\n\r\nxmax_val = xmax.value()\r\n\r\nfor n in range(10):\r\n    x = tf.quantization.fake_quant_with_min_max_vars(inputs=x,\r\n                                                    min=xmax_val,\r\n                                                    max=xmax_val,\r\n                                                    num_bits=8)\r\n\r\nprofiler_result = eager.profiler.stop()\r\neager.profiler.save(os.path.join('quant','log'), profiler_result)\r\n\r\n`\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hi, I dug into this and the issue is that the FakeQuant operation seems to do some computation using the min and max values that must happen on the CPU host. The issue can be resolved  by forcing the min/max variable to be placed on the CPU. I see ValidateInputTypeAndPlacement disappear from my profiler when I do this.\r\n\r\nwith tf.device('/cpu:0'):\r\n  xmax = ...\r\n  xmax_val = \r\n\r\n...\r\n\r\nHope that helps!"]}, {"number": 35740, "title": "I just want to sync the branch, but don't know how", "body": "I just want to sync the branch, but don't know how", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35740) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 35739, "title": "disable_eager_execution resets random seeds set before", "body": "This is on TF2.1 from pip on Windows 10.\r\n\r\n**Describe the current behavior**\r\n```\r\nimport tensorflow.compat.v1 as tf1\r\n\r\ntf1.random.set_random_seed(0)\r\ntf1.disable_eager_execution()\r\n\r\nprint(tf1.keras.backend.get_session().run(tf1.random.uniform((), 0, 1)))\r\n```\r\n\r\nThis prints a different number every time. I have a similar example with more TF2-relevant code, where the same thing happens:\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.compat.v1 as tf1\r\n\r\ntf.random.set_seed(0)\r\ntf1.disable_eager_execution()\r\n\r\nprint(tf1.keras.backend.get_session().run(tf.random.uniform((), 0, 1)))\r\n```\r\n\r\nA workaround is to set the seed after `disable_eager_execution`.", "comments": ["@bersbersbers \r\nCan you please try with \r\n```\r\nimport tensorflow as tf \r\ntf.random.set_seed(0)\r\nprint(tf.random.uniform((), 0, 1))\r\n```\r\nI am not seeing any issue with the above code. Thanks!", "@ravikyram you are right, the issue does not appear without `tf1.disable_eager_execution()`. This is why the title of this issue is \"*disable_eager_execution resets random seeds* ...\".", "@bersbersbers \r\nI have tried on colab with TF version 2.1 and i am getting the same number after disabling the eager execution.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/1f9c382caa98ab177d990c925f77a42d/untitled559.ipynb). Thanks!", "@ravikyram you are right, the issue does not appear when you set the seed *after* `tf1.disable_eager_execution()`. This is why the title of this issue is \"... resets random seeds *set before*\".\r\n\r\nSetting the seed later is a possible workaround as I have noted in in the original post. But the other way round, I consider it a bug. ", "@bersbersbers \r\n\r\nI tried other way around also. Earlier i missed in attaching the file. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/9a093ecaa2305acc5cb3a9a8b2de6e2d/untitled562.ipynb).Please, let me know if i am missing something.Thanks!", "@ravikyram could you please stop changing the repro code and simply run the code examples that I have posted? In the latest gist, you entered `tf.compat.v1.disable_eager_execution` instead of `tf.compat.v1.disable_eager_execution()` - you are not calling this function. Also, the final line in the gist, `print(tf.random.uniform((), 0, 1))`, is not from my example code, either: in fact, it will fail once you correctly call `disable_eager_execution()`. Please, very simply, copy and paste this code and run it without modification, it will reproduce the issue:\r\n```\r\nimport tensorflow.compat.v1 as tf1\r\ntf1.random.set_random_seed(0)\r\ntf1.disable_eager_execution()\r\nprint(tf1.keras.backend.get_session().run(tf1.random.uniform((), 0, 1)))\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/12128514/72334707-10194880-36be-11ea-97dd-dc46a6676c7d.png)\r\n(Funny coincidence with the three very similar random numbers, though.)", "I have tried on colab with TF version 2.1 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/5637084fda4a8c4c84ea700e5a396d34/untitled565.ipynb). Thanks!", "Hi @bersbersbers, I think this is the intended behavior. Disabling eager execution is a very big change that will destroy many previous settings, including the global seed. `disable_eager_execution` should be the first thing you call before doing any TF-related stuff. \r\n\r\nBTW please see https://www.tensorflow.org/guide/random_numbers for the recommended ways to generate random numbers in TF2 eager mode.", "Closing (intended behavior).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35739\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35739\">No</a>\n"]}, {"number": 35738, "title": "build aborted analysis of target build_tbb failed", "body": "I am trying to build `tensorflow` from source for my `Mac` as it supports `SSE4.1`, `SSE4.2`, `AVX` and `FMA` with the following the official documentation (https://www.tensorflow.org/install/source). \r\nI am using the `master` branch and the following command:\r\n\r\n    bazel build -c opt --copt=-mavx --copt=-mfma --copt=-msse4.2 --copt=-msse4.1 --copt=-mfpmath=sse --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --config=mkl --config=ngraph --config=numa --config=noaws --config=nogcp --config=nohdfs --config=nonccl -k //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\nHere goes my `bazel` configuration:\r\n\r\n    (base) IKA-XK0X8JG5J:tensorflow_build sardarmrinal$ ./configure \r\n    WARNING: Running Bazel server needs to be killed, because the startup options are different.\r\n    WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\n    You have bazel 1.2.1 installed.\r\n    Please specify the location of python. [Default is /opt/anaconda3/bin/python]: \r\n\r\n\r\n    Found possible Python library paths:\r\n      /opt/anaconda3/lib/python3.7/site-packages\r\n    Please input the desired Python library path to use.  Default is [/opt/anaconda3/lib/python3.7/site-packages]\r\n\r\n    Do you wish to build TensorFlow with XLA JIT support? [Y/n]: Y\r\n    XLA JIT support will be enabled for TensorFlow.\r\n\r\n    Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: N\r\n    No OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\n    Do you wish to build TensorFlow with ROCm support? [y/N]: N\r\n    No ROCm support will be enabled for TensorFlow.\r\n\r\n    Do you wish to build TensorFlow with CUDA support? [y/N]: N\r\n    No CUDA support will be enabled for TensorFlow.\r\n\r\n    Do you wish to download a fresh release of clang? (Experimental) [y/N]: N\r\n    Clang will not be downloaded.\r\n\r\n    Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n\r\n\r\n    Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N\r\n    Not configuring the WORKSPACE for Android builds.\r\n\r\n    Do you wish to build TensorFlow with iOS support? [y/N]: N\r\n    No iOS support will be enabled for TensorFlow.\r\n\r\n    Preconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n    \t--config=mkl         \t# Build with MKL support.\r\n    \t--config=monolithic  \t# Config for mostly static monolithic build.\r\n    \t--config=ngraph      \t# Build with Intel nGraph support.\r\n    \t--config=numa        \t# Build with NUMA support.\r\n    \t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n    \t--config=v2          \t# Build TensorFlow 2.x instead of 1.x.\r\n    Preconfigured Bazel build configs to DISABLE default on features:\r\n    \t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n    \t--config=nogcp       \t# Disable GCP support.\r\n    \t--config=nohdfs      \t# Disable HDFS support.\r\n    \t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\n    Configuration finished\r\n\r\nAnd I am getting the following error:\r\n\r\n    ERROR: /private/var/tmp/_bazel_sardarmrinal/841c8c9a203505f6ff7b50ea63697e9e/external/tbb/BUILD.bazel:12:1: in cmd attribute of genrule rule @tbb//:build_tbb: $(AR) not defined\r\n    ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis of target '@tbb//:build_tbb' failed; build aborted\r\n\r\nAny idea how I can get past this?\r\n\r\nFYI: I built one successfully yesterday wihtout `--cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"` and I wanted to use this option this time. All I did was pulling the latest `master` branch.\r\n\r\n\r\n\r\nOutput from `tf_env_collect.sh`:\r\n\r\n    == check python ===================================================\r\n        python version: 3.7.4\r\n        python branch: \r\n        python build version: ('default', 'Aug 13 2019 15:17:50')\r\n        python compiler version: Clang 4.0.1 (tags/RELEASE_401/final)\r\n        python implementation: CPython\r\n\r\n\r\n    == check os platform ===============================================\r\n    os: Darwin\r\n    os kernel version: Darwin Kernel Version 18.7.0: Sun Dec  1 18:59:03 PST 2019; root:xnu-4903.278.19~1/RELEASE_X86_64\r\n    os release version: 18.7.0\r\n    os platform: Darwin-18.7.0-x86_64-i386-64bit\r\n    linux distribution: ('', '', '')\r\n    linux os distribution: ('', '', '')\r\n    mac version: ('10.14.6', ('', '', ''), 'x86_64')\r\n    uname: uname_result(system='Darwin', node='IKA-XK0X8JG5J', release='18.7.0', version='Darwin Kernel Version 18.7.0: Sun Dec  1 18:59:03 PST 2019; root:xnu-4903.278.19~1/RELEASE_X86_64', machine='x86_64', processor='i386')\r\n    architecture: ('64bit', '')\r\n    machine: x86_64\r\n\r\n\r\n    == are we in docker =============================================\r\n    No\r\n\r\n    == compiler =====================================================\r\n    Apple LLVM version 10.0.1 (clang-1001.0.46.4)\r\n    Target: x86_64-apple-darwin18.7.0\r\n    Thread model: posix\r\n    InstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n\r\n    == check pips ===================================================\r\n    numpy                              1.17.2   \r\n    numpydoc                           0.9.1    \r\n    protobuf                           3.11.2   \r\n    tensorflow                         2.0.0    \r\n    tensorflow-estimator               2.0.1    \r\n\r\n    == check for virtualenv =========================================\r\n    False\r\n\r\n    == tensorflow import ============================================\r\n    tf.version.VERSION = 2.0.0\r\n    tf.version.GIT_VERSION = unknown\r\n    tf.version.COMPILER_VERSION = 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)\r\n\r\n    == env ==========================================================\r\n    LD_LIBRARY_PATH is unset\r\n    DYLD_LIBRARY_PATH is unset\r\n\r\n    == nvidia-smi ===================================================\r\n    tf_env_cololect.sh: line 147: nvidia-smi: command not found\r\n\r\n    == cuda libs  ===================================================\r\n\r\n    == tensorflow installed from info ==================\r\n    Name: tensorflow\r\n    Version: 2.0.0\r\n    Summary: TensorFlow is an open source machine learning framework for everyone.\r\n    Home-page: https://www.tensorflow.org/\r\n    Author-email: packages@tensorflow.org\r\n    License: Apache 2.0\r\n    Location: /opt/anaconda3/lib/python3.7/site-packages\r\n    Required-by: \r\n\r\n    == python version  ==============================================\r\n    (major, minor, micro, releaselevel, serial)\r\n    (3, 7, 4, 'final', 0)\r\n\r\n    == bazel version  ===============================================\r\n    Build label: 1.2.1\r\n    Build time: Tue Nov 26 15:27:31 2019 (1574782051)\r\n    Build timestamp: 1574782051\r\n    Build timestamp as int: 1574782051\r\n\r\n", "comments": ["I have met the same error with Tensorflow 1.15.0 with XLA and CUDA support enabled (using bazel 0.26.1 which is indicated by the `configure`).\r\n\r\nBut the problem is solved by not building nGraph support.", "@mrinalsardar, did you get a chance to take a look at @004307ec's comment. Thanks! ", "@gadagashwini Yes I removed all extra options ans used the default setting and that worked for me as well. But I used all combinations of options from the ones I have mentioned and it seems only the default build without any option works and nothing else. Anyways, I think we can close this issue for now. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35738\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35738\">No</a>\n"]}, {"number": 35737, "title": "TFlite compilation failing for tf 2.1.0", "body": "<em>\r\nI am trying to compile tflite library for x86 machine, I have tried it using the following script\r\nhttps://github.com/sourcecode369/tensorflow-1/blob/master/tensorflow/lite/tools/make/build_lib.sh\r\n\r\nbefore this, I have also installed the required dependencies using\r\nhttps://github.com/sourcecode369/tensorflow-1/blob/master/tensorflow/lite/tools/make/download_dependencies.sh\r\n</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution : Linux Ubuntu 18.04\r\n- TensorFlow installed from : source\r\n- TensorFlow version : 2.1.0\r\n- GCC/Compiler version :7.4.0\n- Bazel version : 2.0.0\r\n\r\n**Describe the current behavior**\r\n/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/obj/tensorflow/lite/tools/benchmark/benchmark_performance_options.o /home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/obj/tensorflow/lite/tools/benchmark/benchmark_utils.o /home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/obj/tensorflow/lite/tools/benchmark/benchmark_params.o /home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/obj/tensorflow/lite/profiling/profile_summarizer.o /home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/obj/tensorflow/core/util/stats_calculator.o /home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/obj/tensorflow/lite/tools/command_line_flags.o /home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/obj/tensorflow/lite/tools/evaluation/utils.o\r\nar: creating /home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/lib/benchmark-lib.a\r\ng++ -O3 -DNDEBUG -fPIC  --std=c++11 -fPIC -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -pthread -I. -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include \\\r\n-o /home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/bin/benchmark_model /home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/obj/tensorflow/lite/tools/benchmark/benchmark_main.o \\\r\n /home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/lib/benchmark-lib.a  -lstdc++ -lpthread -lm -lz -ldl\r\ng++ -O3 -DNDEBUG -fPIC  --std=c++11 -fPIC -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -pthread -I. -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include \\\r\n-o /home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/bin/benchmark_model_performance_options /home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/obj/tensorflow/lite/tools/benchmark/benchmark_tflite_performance_options_main.o \\\r\n /home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/lib/benchmark-lib.a  -lstdc++ -lpthread -lm -lz -ldl\r\n/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/lib/benchmark-lib.a(command_line_flags.o): In function `tflite::Flags::Parse(int*, char const**, std::vector<tflite::Flag, std::allocator<tflite::Flag> > const&)':\r\ncommand_line_flags.cc:(.text+0x57c2): undefined reference to `tensorflow::internal::LogMessage::LogMessage(char const*, int, int)'\r\ncommand_line_flags.cc:(.text+0x57ed): undefined reference to `tensorflow::internal::LogMessage::~LogMessage()'\r\ncommand_line_flags.cc:(.text+0x5997): undefined reference to `tensorflow::internal::LogMessage::LogMessage(char const*, int, int)'\r\ncommand_line_flags.cc:(.text+0x59c8): undefined reference to `tensorflow::internal::LogMessage::~LogMessage()'\r\ncommand_line_flags.cc:(.text+0x5b1d): undefined reference to `tensorflow::internal::LogMessage::LogMessage(char const*, int, int)'\r\ncommand_line_flags.cc:(.text+0x5b48): undefined reference to `tensorflow::internal::LogMessage::~LogMessage()'\r\ncommand_line_flags.cc:(.text+0x5db0): undefined reference to `tensorflow::internal::LogMessage::LogMessage(char const*, int, int)'\r\ncommand_line_flags.cc:(.text+0x5e24): undefined reference to `tensorflow::internal::LogMessage::~LogMessage()'\r\ncommand_line_flags.cc:(.text+0x5ea5): undefined reference to `tensorflow::internal::LogMessage::~LogMessage()'\r\ncollect2: error: ld returned 1 exit status\r\ntensorflow/lite/tools/make/Makefile:295: recipe for target '/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/bin/benchmark_model' failed\r\nmake: *** [/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/bin/benchmark_model] Error 1\r\nmake: *** Waiting for unfinished jobs....\r\n/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/lib/benchmark-lib.a(command_line_flags.o): In function `tflite::Flags::Parse(int*, char const**, std::vector<tflite::Flag, std::allocator<tflite::Flag> > const&)':\r\ncommand_line_flags.cc:(.text+0x57c2): undefined reference to `tensorflow::internal::LogMessage::LogMessage(char const*, int, int)'\r\ncommand_line_flags.cc:(.text+0x57ed): undefined reference to `tensorflow::internal::LogMessage::~LogMessage()'\r\ncommand_line_flags.cc:(.text+0x5997): undefined reference to `tensorflow::internal::LogMessage::LogMessage(char const*, int, int)'\r\ncommand_line_flags.cc:(.text+0x59c8): undefined reference to `tensorflow::internal::LogMessage::~LogMessage()'\r\ncommand_line_flags.cc:(.text+0x5b1d): undefined reference to `tensorflow::internal::LogMessage::LogMessage(char const*, int, int)'\r\ncommand_line_flags.cc:(.text+0x5b48): undefined reference to `tensorflow::internal::LogMessage::~LogMessage()'\r\ncommand_line_flags.cc:(.text+0x5db0): undefined reference to `tensorflow::internal::LogMessage::LogMessage(char const*, int, int)'\r\ncommand_line_flags.cc:(.text+0x5e24): undefined reference to `tensorflow::internal::LogMessage::~LogMessage()'\r\ncommand_line_flags.cc:(.text+0x5ea5): undefined reference to `tensorflow::internal::LogMessage::~LogMessage()'\r\ncollect2: error: ld returned 1 exit status\r\ntensorflow/lite/tools/make/Makefile:301: recipe for target '/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/bin/benchmark_model_performance_options' failed\r\nmake: *** [/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/bin/benchmark_model_performance_options] Error 1\r\nmake: Leaving directory '/home/swati/git_workspace/tensorflow'\r\n\r\n\r\n**Describe the expected behavior**\r\nIt should successfully compile and build the library\r\n\r\n\r\n**Code to reproduce the issue**\r\ngit clone https://github.com/tensorflow/tensorflow\r\ncd tensorflow\r\n./tensorflow/lite/tools/download_dependencies.sh \r\n./tensorflow/lite/tools/make/build_lib.sh\r\n\r\nI am new to source compilation, currently unable to understand why this is not working.\r\n\r\n", "comments": ["This was a regression. Fixed in master with this commit.\r\nhttps://github.com/tensorflow/tensorflow/commit/35095ee07fd63b4722d2b87b4de928c89c5a4845"]}, {"number": 35736, "title": "Converting saved_model to TFLite model using TF 2.0", "body": "**System information**\r\n- Google colab:\r\n- TensorFlow 2.0.0\r\n\r\nI am working on converting custom object detection model (trained using SSD and inception network) to quantized TFLite model. I can able to convert custom object detection model from frozen graph to quantized TFLite model using the following code snippet (using **Tensorflow 1.4**):\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(args[\"model\"],input_shapes = {'normalized_input_image_tensor':[1,300,300,3]},\r\ninput_arrays = ['normalized_input_image_tensor'],output_arrays = ['TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1',\r\n'TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'])\r\n\r\nconverter.allow_custom_ops=True\r\nconverter.post_training_quantize=True \r\ntflite_model = converter.convert()\r\nopen(args[\"output\"], \"wb\").write(tflite_model)\r\n```\r\nHowever ```tf.lite.TFLiteConverter.from_frozen_graph``` class method is not available for **Tensorflow 2.0** (refer [this link](https://www.tensorflow.org/lite/convert/python_api#exporting_a_savedmodel_)). So I tried to convert the model using ```tf.lite.TFLiteConverter.from_saved_model``` class method. The code snippet is shown below:\r\n\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(\"/content/\") # Path to saved_model directory\r\nconverter.optimizations =  [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\n```\r\n\r\n**The above code snippet throws the following error:**\r\n\r\n```\r\nValueError: None is only supported in the 1st dimension. Tensor 'image_tensor' has invalid shape '[None, None, None, 3]'.\r\n```\r\n\r\n**I tried to pass ```input_shapes``` as argument**\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(\"/content/\",input_shapes={\"image_tensor\" : [1,300,300,3]})\r\n```\r\n\r\n**but it throws the following error:**\r\n\r\n```\r\nTypeError: from_saved_model() got an unexpected keyword argument 'input_shapes'\r\n```\r\n\r\nAm I missing something? Please feel free to correct me!", "comments": ["The easiest way to override the signature is to load the saved model back into tensorflow and then edit the concrete function signature to specify the shape.\r\n\r\nGuide to concrete functions here\r\nhttps://www.tensorflow.org/guide/concrete_function\r\n```\r\nreloaded = tf.saved_model.load(export_dir)\r\ncf = reloaded.signatures['/content/']\r\n```\r\nThen you can change cf.inputs to provide the shape.\r\n\r\nFinally make use TF Lite converter function from_concrete_function.\r\n\r\n", "Thanks for the kind response. Could you please provide more information?\r\n\r\nI tried the following code so far:\r\n```\r\nreloaded = tf.saved_model.load(export_dir=\"/content/\")\r\ncf = reloaded.signatures\r\ncf.input_shapes = {'image_tensor':[1,300,300,3]}\r\nconverter =  tf.lite.TFLiteConverter.from_concrete_functions(reloaded)\r\nconverter.optimizations =  [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\n```\r\n\r\nGot the following error:\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-13-86ab3b34a7ae> in <module>()\r\n----> 1 converter =  tf.lite.TFLiteConverter.from_concrete_functions(reloaded)\r\n      2 \r\n      3 converter.optimizations =  [tf.lite.Optimize.DEFAULT]\r\n      4 tflite_model = converter.convert()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py in from_concrete_functions(cls, funcs)\r\n    326       Invalid input type.\r\n    327     \"\"\"\r\n--> 328     for func in funcs:\r\n    329       if not isinstance(func, _function.ConcreteFunction):\r\n    330         message = \"This function takes in a list of ConcreteFunction.\"\r\n\r\nTypeError: 'AutoTrackable' object is not iterable\r\n```\r\nI am very new to this tensorflow 2.0. Could you please guide me?", "Here is a slightly more comprehensive example that shows how to use the SavedModel structure. It is based on the first example in the concrete functions documentation:\r\n```\r\n# Load the SavedModel.\r\nsaved_model_obj = tf.saved_model.load(export_dir=saved_model_dir)\r\n\r\n# Load the specific concrete function from the SavedModel.\r\nconcrete_func = saved_model_obj.signatures['serving_default']\r\n\r\n# Set the shape of the input in the concrete function.\r\nconcrete_func.inputs[0].set_shape([])\r\n\r\n# Convert the model to a TFLite model.\r\nconverter =  tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\nconverter.optimizations =  [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\n```\r\n\r\n`serving_default` is the default key for signatures in a SavedModels. However, your signature key might be different.\r\n\r\nJust for complete context, this is the model generation code for the model above:\r\n```\r\nimport tensorflow as tf\r\n\r\nclass Pow(tf.Module):\r\n  def __init__(self, exponent):\r\n    self.exponent = tf.Variable(exponent, dtype=tf.float32, name='Pow/exponent')\r\n\r\n  @tf.function\r\n  def __call__(self, x):\r\n    return x ** self.exponent\r\n\r\n# Generate concrete function.\r\nroot = Pow(3)\r\nconcrete_func = root.__call__.get_concrete_function(tf.constant(2.))\r\n\r\n# Save the generated concrete function as a SavedModel.\r\nsaved_model_dir = '/tmp/pow'\r\ntf.saved_model.save(root, saved_model_dir, signatures=concrete_func)\r\n```", "@gargn : Thanks for the detailed explanation. I followed the explanation given by you. However, I am getting the error:\r\n```\r\nsaved_model_dir='/content/'\r\nsaved_model_obj = tf.saved_model.load(export_dir=saved_model_dir)\r\nconcrete_func = saved_model_obj.signatures['serving_default']\r\nprint(concrete_func.structured_outputs)\r\n```\r\n\r\nThe output of the above ```print``` seems to be fine:\r\n```\r\n{'detection_boxes': <tf.Tensor 'detection_boxes:0' shape=(None, 100, 4) dtype=float32>, 'raw_detection_boxes': <tf.Tensor 'raw_detection_boxes:0' shape=(None, None, 4) dtype=float32>, 'detection_scores': <tf.Tensor 'detection_scores:0' shape=(None, 100) dtype=float32>, 'raw_detection_scores': <tf.Tensor 'raw_detection_scores:0' shape=(None, None, 4) dtype=float32>, 'detection_multiclass_scores': <tf.Tensor 'detection_multiclass_scores:0' shape=(None, 100, 4) dtype=float32>, 'detection_classes': <tf.Tensor 'detection_classes:0' shape=(None, 100) dtype=float32>, 'num_detections': <tf.Tensor 'num_detections:0' shape=(None,) dtype=float32>}\r\n```\r\nSetting the shape:\r\n```\r\nconcrete_func.inputs[0].set_shape([None,300,300,3]) # I also tried with [1,300,300,3]\r\n```\r\nAnd finally converted the model to tflite model:\r\n```\r\nconverter =  tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\nconverter.optimizations =  [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\n```\r\nIt throws the following error:\r\n```\r\n---------------------------------------------------------------------------\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-31-70c5da23dc1d> in <module>()\r\n      1 converter =  tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\n      2 converter.optimizations =  [tf.lite.Optimize.DEFAULT]\r\n----> 3 tflite_model = converter.convert()\r\n\r\n2 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    198       stdout = _try_convert_to_unicode(stdout)\r\n    199       stderr = _try_convert_to_unicode(stderr)\r\n--> 200       raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    201   finally:\r\n    202     # Must manually cleanup files.\r\n\r\nConverterError: See console for info.\r\n2020-01-14 08:34:56.050487: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-01-14 08:34:56.050574: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-01-14 08:34:56.050592: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-01-14 08:34:56.050611: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-01-14 08:34:56.050684: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3\r\n2020-01-14 08:34:56.050710: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-01-14 08:34:56.050723: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3\r\n2020-01-14 08:34:56.050735: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-01-14 08:34:56.050746: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-01-14 08:34:56.050759: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3\r\n2020-01-14 08:34:56.050769: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-01-14 08:34:56.050780: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-01-14 08:34:56.050795: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-01-14 08:34:56.050804: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-01-14 08:34:56.050814: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-01-14 08:34:56.050823: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-01-14 08:34:56.050833: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3\r\n2020-01-14 08:34:56.050847: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-01-14 08:34:56.050859: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-01-14 08:34:56.050868: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-01-14 08:34:56.050891: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-01-14 08:34:56.050906: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: LoopCond\r\n2020-01-14 08:34:56.050927: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit\r\n2020-01-14 08:34:56.050942: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit\r\n2020-01-14 08:34:56.050952: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3\r\n2020-01-14 08:34:56.050963: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3\r\n2020-01-14 08:34:56.050988: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3\r\n2020-01-14 08:34:56.051008: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3\r\n2020-01-14 08:34:56.051022: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3\r\n2020-01-14 08:34:56.051043: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3\r\n2020-01-14 08:34:56.051066: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3\r\n2020-01-14 08:34:56.051760: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3\r\n2020-01-14 08:34:56.051792: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-01-14 08:34:56.051806: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3\r\n2020-01-14 08:34:56.051820: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-01-14 08:34:56.051832: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3\r\n2020-01-14 08:34:56.051843: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-01-14 08:34:56.051855: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3\r\n2020-01-14 08:34:56.051866: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-01-14 08:34:56.051903: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3\r\n2020-01-14 08:34:56.051915: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-01-14 08:34:56.051927: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3\r\n2020-01-14 08:34:56.051937: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-01-14 08:34:56.051948: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-01-14 08:34:56.051962: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3\r\n2020-01-14 08:34:56.051972: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-01-14 08:34:56.051984: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3\r\n2020-01-14 08:34:56.051994: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-01-14 08:34:56.052005: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3\r\n2020-01-14 08:34:56.052015: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-01-14 08:34:56.052026: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3\r\n2020-01-14 08:34:56.052036: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-01-14 08:34:56.052052: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-01-14 08:34:56.052070: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-01-14 08:34:56.052080: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-01-14 08:34:56.052090: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-01-14 08:34:56.052100: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-01-14 08:34:56.052111: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3\r\n2020-01-14 08:34:56.052123: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-01-14 08:34:56.052133: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-01-14 08:34:56.052144: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-01-14 08:34:56.052155: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-01-14 08:34:56.052165: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3\r\n2020-01-14 08:34:56.052177: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-01-14 08:34:56.052188: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-01-14 08:34:56.052198: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-01-14 08:34:56.052208: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-01-14 08:34:56.052219: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3\r\n2020-01-14 08:34:56.052243: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-01-14 08:34:56.052256: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-01-14 08:34:56.052268: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-01-14 08:34:56.052278: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-01-14 08:34:56.052290: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-01-14 08:34:56.052300: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-01-14 08:34:56.052311: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-01-14 08:34:56.052322: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-01-14 08:34:56.052332: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-01-14 08:34:56.052343: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-01-14 08:34:56.052354: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-01-14 08:34:56.052364: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-01-14 08:34:56.052376: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3\r\n2020-01-14 08:34:56.052393: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-01-14 08:34:56.052411: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-01-14 08:34:56.052437: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-01-14 08:34:56.052469: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-01-14 08:34:56.052488: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: LoopCond\r\n2020-01-14 08:34:56.052553: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit\r\n2020-01-14 08:34:56.052571: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit\r\n2020-01-14 08:34:56.052585: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit\r\n2020-01-14 08:34:56.052597: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit\r\n2020-01-14 08:34:56.052610: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit\r\n2020-01-14 08:34:56.052629: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3\r\n2020-01-14 08:34:56.052643: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3\r\n2020-01-14 08:34:56.052656: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3\r\n2020-01-14 08:34:56.052668: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3\r\n2020-01-14 08:34:56.053081: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3\r\n2020-01-14 08:34:56.053100: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3\r\n2020-01-14 08:34:56.053113: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3\r\n2020-01-14 08:34:56.053124: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3\r\n2020-01-14 08:34:56.053134: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3\r\n2020-01-14 08:34:56.053207: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3\r\n2020-01-14 08:34:56.053234: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3\r\n2020-01-14 08:34:56.053248: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3\r\n2020-01-14 08:34:56.053261: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3\r\n2020-01-14 08:34:56.053273: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3\r\n2020-01-14 08:34:56.053285: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3\r\n2020-01-14 08:34:56.053298: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-01-14 08:34:56.053319: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3\r\n2020-01-14 08:34:56.053405: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: NonMaxSuppressionV3\r\n2020-01-14 08:34:56.053428: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: NonMaxSuppressionV3\r\n2020-01-14 08:34:56.053441: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: NonMaxSuppressionV3\r\n2020-01-14 08:34:56.053529: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size\r\n2020-01-14 08:34:56.053627: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size\r\n2020-01-14 08:34:56.053674: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3\r\n2020-01-14 08:34:56.053782: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3\r\n2020-01-14 08:34:56.053797: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3\r\n2020-01-14 08:34:56.053814: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3\r\n2020-01-14 08:34:56.053825: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3\r\n2020-01-14 08:34:56.070797: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 816 operators, 1530 arrays (0 quantized)\r\n2020-01-14 08:34:56.106869: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 809 operators, 1514 arrays (0 quantized)\r\n2020-01-14 08:34:56.146313: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 809 operators, 1514 arrays (0 quantized)\r\n2020-01-14 08:34:56.201644: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 548 operators, 1070 arrays (0 quantized)\r\n2020-01-14 08:34:56.216716: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 548 operators, 1070 arrays (0 quantized)\r\n2020-01-14 08:34:56.230809: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 548 operators, 1070 arrays (0 quantized)\r\n2020-01-14 08:34:56.253590: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 1080704 bytes, theoretical optimal value: 1080704 bytes.\r\n2020-01-14 08:34:56.256625: F tensorflow/lite/toco/tooling_util.cc:2275] Check failed: array.data_type == array.final_data_type Array \"image_tensor\" has mis-matching actual and final data types (data_type=uint8, final_data_type=float).\r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x00007fbd1dc0f780 (most recent call first):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 52 in execute\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250 in _run_main\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299 in run\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py\", line 40 in run\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 89 in main\r\n  File \"/usr/local/bin/toco_from_protos\", line 8 in <module>\r\nAborted (core dumped)\r\n```\r\nCould you please guide me?", "Reading your original post, one option is to use `tf.compat.v1.TFLiteConverter.from_frozen_graph`.\r\n\r\nHowever, regarding the type error - @alanchiao any insight on it?", "As mentioned in the question, I can able to convert a tensorflow object detection model to tflite model using  ```tf.lite.TFLiteConverter.from_frozen_graph``` class method but this class method is not available for ```Tensorflow 2.0```. I want to use ```tensorflow 2.0``` for this.\r\n", "@chauhansaurabhb As @gargn mentioned, You could you use `compat.v1` as follows. This `compat.v1` brings the functionality of `TF1.x` into `TF2.x`.\r\n\r\n`tf.compat.v1.lite.TFLiteConverter.from_frozen_graph`\r\n", "Thanks for the pointer! I got it using ```tf.compat.v1.lite.TFLiteConverter.from_frozen_graph```.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35736\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35736\">No</a>\n", "i got an error converting Model to Tensorflow lite.\r\n\r\nPlease guide me how get from error\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model_file(keras_file)\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\nopen(\"brainmodel.tflite\",\"wb\").write(tflite_model)\r\n\r\n\r\nINFO:tensorflow:Froze 10 variables.\r\nINFO:tensorflow:Converted 10 variables to const ops.\r\n---------------------------------------------------------------------------\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-34-6d52cc1bd9a7> in <module>\r\n      2 converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n      3                                        tf.lite.OpsSet.SELECT_TF_OPS]\r\n**----> 4 tflite_model = converter.convert()**\r\n      5 open(\"brainmodel.tflite\",\"wb\").write(tflite_model)\r\n\r\n~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\lite\\python\\lite.py in convert(self)\r\n    981           input_tensors=self._input_tensors,\r\n    982           output_tensors=self._output_tensors,\r\n--> 983           **converter_kwargs)\r\n    984     else:\r\n    985       result = _toco_convert_graph_def(\r\n\r\n~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\lite\\python\\convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\r\n    447       input_data.SerializeToString(),\r\n    448       debug_info_str=debug_info_str,\r\n**--> 449       enable_mlir_converter=enable_mlir_converter**)\r\n    450   return data\r\n    451 \r\n\r\n~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\lite\\python\\convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    198       stdout = _try_convert_to_unicode(stdout)\r\n    199       stderr = _try_convert_to_unicode(stderr)\r\n**--> 200       raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))**\r\n    201   finally:\r\n    202     **# Must manually cleanup files.**\r\n\r\n**ConverterError: See console for info.**\r\n2020-03-28 14:16:04.520483: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-03-28 14:16:04.521064: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-03-28 14:16:04.521543: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-03-28 14:16:04.522307: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-03-28 14:16:04.522799: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-03-28 14:16:04.523293: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-03-28 14:16:04.523781: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-03-28 14:16:04.524333: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-03-28 14:16:04.525351: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3\r\n2020-03-28 14:16:04.525837: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-03-28 14:16:04.526261: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3\r\n2020-03-28 14:16:04.526684: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-03-28 14:16:04.527134: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-03-28 14:16:04.527659: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-03-28 14:16:04.528087: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-03-28 14:16:04.528402: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-03-28 14:16:04.528864: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-03-28 14:16:04.529348: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-03-28 14:16:04.529874: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-03-28 14:16:04.530306: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-03-28 14:16:04.530986: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3\r\n2020-03-28 14:16:04.531854: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: LoopCond\r\n2020-03-28 14:16:04.532648: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-03-28 14:16:04.533486: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit\r\n2020-03-28 14:16:04.534252: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3\r\n2020-03-28 14:16:04.535040: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3\r\n2020-03-28 14:16:04.535725: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3\r\n2020-03-28 14:16:04.536374: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3\r\n2020-03-28 14:16:04.536802: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-03-28 14:16:04.537209: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3\r\n2020-03-28 14:16:04.537637: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-03-28 14:16:04.538061: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-03-28 14:16:04.538512: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-03-28 14:16:04.538915: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-03-28 14:16:04.539317: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-03-28 14:16:04.539625: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-03-28 14:16:04.539888: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-03-28 14:16:04.540567: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-03-28 14:16:04.540896: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2020-03-28 14:16:04.541161: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3\r\n2020-03-28 14:16:04.541854: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-03-28 14:16:04.542234: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: LoopCond\r\n2020-03-28 14:16:04.542505: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-03-28 14:16:04.543155: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2020-03-28 14:16:04.543663: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit\r\n2020-03-28 14:16:04.544069: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3\r\n2020-03-28 14:16:04.544806: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3\r\n2020-03-28 14:16:04.545361: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3\r\n2020-03-28 14:16:04.545969: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3\r\n2020-03-28 14:16:04.546588: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3\r\n2020-03-28 14:16:04.562446: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 213 operators, 357 arrays (0 quantized)\r\n2020-03-28 14:16:04.576774: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 213 operators, 357 arrays (0 quantized)\r\n2020-03-28 14:16:04.618435: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 187 operators, 336 arrays (0 quantized)\r\n2020-03-28 14:16:04.627775: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 187 operators, 336 arrays (0 quantized)\r\n2020-03-28 14:16:04.633599: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 187 operators, 336 arrays (0 quantized)\r\n2020-03-28 14:16:04.658481: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 512 bytes, theoretical optimal value: 512 bytes.\r\n2020-03-28 14:16:04.660512: I tensorflow/lite/toco/toco_tooling.cc:454] Number of parameters: 133558\r\n2020-03-28 14:16:04.673790: E tensorflow/lite/toco/toco_tooling.cc:481] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nTensorFlow Lite currently doesn't support control flow ops: Merge, Switch. We are working on supporting control flow ops, please see github issue at https://github.com/tensorflow/tensorflow/issues/28485.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Noraiz\\Anaconda3\\envs\\tensorflow\\Scripts\\toco_from_protos-script.py\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"C:\\Users\\Noraiz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\lite\\toco\\python\\toco_from_protos.py\", line 89, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"C:\\Users\\Noraiz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"C:\\Users\\Noraiz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\absl\\app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"C:\\Users\\Noraiz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\absl\\app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"C:\\Users\\Noraiz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\lite\\toco\\python\\toco_from_protos.py\", line 52, in execute\r\n    enable_mlir_converter)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nTensorFlow Lite currently doesn't support control flow ops: Merge, Switch. We are working on supporting control flow ops, please see github issue at https://github.com/tensorflow/tensorflow/issues/28485.\r\n\r\n\r\n\r\n", "Is there someway like set_shape to change the datatype of input tensor. I'm trying to convert model to tflite but facing issues with float64 datatype. "]}, {"number": 35735, "title": "Cherry pick 32758: Explicit python version dependency into r1.15", "body": "As per this comment https://github.com/tensorflow/tensorflow/pull/32758#issuecomment-569429209\r\n\r\n", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35735) for more info**.\n\n<!-- need_author_consent -->", "@mihaimaruseac I cannot assign this to you as the comment asks. But here it is", "@aflc I don't suppose you could leave your consent here, as the CLA bot requests?", "Closing in favor of #34994"]}, {"number": 35734, "title": "Hexagon Delegate requirements needs to be clarified?", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: SD 820 device\r\n- TensorFlow installed from (source or binary): source, 4b7d5117de4a193bd895ff357dc5286de847c632\r\n- TensorFlow version (use command below): -\r\n- Python version: -\r\n- Bazel version (if compiling from source): 1.2.1\r\n- GCC/Compiler version (if compiling from source): Android NDK clang\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n**Describe the current behavior**\r\n\r\nWhen using Hexagon delegate on 32bit ARM device:\r\n\r\n```\r\nCANNOT LINK EXECUTABLE: cannot locate symbol \"remote_handle64_open\" referenced by \"/data/package/lib/libhexagon_interface.so\"...\r\n```\r\n\r\nAs far as I know 820 supports Hexagon acceleration, but it seems libhexagon_interface.so requires extra symbol that is not present on my device system libraries (As far as I know it is part of qualcomm code)\r\n\r\nOn our 64bit device I see that this function is properly used, there are even some logs from `vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c`\r\n\r\nSo it is more of a question: what is required to run hexagon delegate?\r\n\r\nI know that this is pre-built library from vendor, but I wasn't able to find it in Hexagon SDK itself.\r\nSo I'm not sure where exactly it is coming from.\r\nIf possible I'd appreciate some guidance here\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Thanks for the report. Can you provide more details on the specific make/model of your phone, e.g., `adb shell getprop ro.product.device`? We've tested execution on 820, so this could be device-specific. ", "Thanks for the report. We saw similar issue, we have a fix for the issue and will be pushed soon.\r\n\r\nSorry for the trouble.", "Thanks for fix!"]}, {"number": 35733, "title": "Docker images with tags '2.1.0-gpu-py3' and '2.1.0-gpu-py3-jupyter' are missing on DockerHub", "body": "Probably the CI has failed to upload those images, because all other combinations of tensorflow:2.1.0[-gpu][-py3][-jupyter] are available.\r\n\r\n", "comments": ["Yeah seems like a recurring problem. Even many `devel` docker images are stale. According to this issue, I created [here](https://github.com/tensorflow/tensorflow/issues/35492), this problem should have been fixed by the 31st of December but, unfortunately, hasn't.", "@angerson is OOO for a week more but then I think this would get fixed.\r\n\r\nApologies for the delay, as mentioned on the other issue, our docker person has left and we have to catch up.", "These images were uploaded some time ago, so this issue can be closed. Thanks for your understanding! Our old Docker maintainer left and we're still hard-pressed to keep up-to-date."]}]