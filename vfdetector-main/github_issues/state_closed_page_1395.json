[{"number": 11209, "title": "fallback to find if locate is not used", "body": "Related to #11029 and discussion in #9580\r\n\r\nLocate is non-standard, and not installed on every system. It is much faster than 'find', but 'find' should be on every system. Find is even in busybox. On extremely large systems, it can potentially run for a long period of time.\r\n\r\nThis fix will first attempt to use the locate command, piping any error messages to /dev/null. If locate did not generate output, the script will warn that it will attempt to use 'find', and then use 'find'.\r\n\r\nWhen using find, piping stderr to /dev/null is required since 'find' will generate errors when it cannot access a file (for instance if permissions are enforced). Piping the output from 'find' into head -n1 will complete the line of code, and thus stop 'find' from running longer.", "comments": ["Can one of the admins verify this patch?", "Closing due to other work on the section for now. It looks like this won't be needed in the future."]}, {"number": 11207, "title": "Branch 160731226", "body": "", "comments": ["Jenkins, test this please.", "Not sure what that is:\r\n\r\n```\r\nERROR: /var/lib/jenkins/workspace/tensorflow-pull-requests-gpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/external/local_config_cuda/cuda/BUILD:1305:1: declared output 'external/local_config_cuda/cuda/lib/libcublas.so.8.0' is a dangling symbolic link.\r\n```", "@gunan @annarev Any idea where the libcublas-related error quoted by @drpngx above comes from?", "Not sure what it is, but looks bad.\r\nInternal and external builds are both healthy, so I am not sure what is happening with this one.", "Maybe some cache corruption?\r\n\r\nJenkins, test this please.", "@drpngx I was thinking of the same thing. I wiped the cache of the \"tensorflow-pull-requests-gpu\" build on all Jenkins GPU instances. Let's see how the new builds go.", "Looks like wiping the cache gets rid of the previous error. But it appears that the cache of \"tensorflow-pull-requests-xla\" also needs to be wiped. Doing that right now.", "/CC: @jart @davidzchen @rmlarsen in case they know something.", "@tensorflow-jenkins test this please\r\nhopefully this time all tests will pass", "@tensorflow-jenkins test this please\r\n", "Both the GPU and XLA builds passed after clearing the caches. So it does look like a cache problem related to libcublas. @davidzchen @meteorcloudy , if you have thoughts, please let us know. \r\n\r\nI'm merging this PR now.", "Looks like another bazel bug related to caching of external build rules?\n@damienmg fyi\n\nOn Jul 1, 2017 4:01 PM, \"Shanqing Cai\" <notifications@github.com> wrote:\n\n> Both the GPU and XLA builds passed after clearing the caches. So it does\n> look like a cache problem related to libcublas. @davidzchen\n> <https://github.com/davidzchen> @meteorcloudy\n> <https://github.com/meteorcloudy> , if you have thoughts, please let us\n> know.\n>\n> I'm merging this PR now.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/11207#issuecomment-312460160>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOdclprqgOegWtSbFNj0SzXIX6-Mmks5sJs_agaJpZM4OLV4h>\n> .\n>\n", "That's because we do not track the binary to which we point the symlink too, so if you change the install of cuda, this will fail and needs a reconfigure. We are discussing how to do it correctly in the future already. FTR the same fix that the previous case is feasible (force a reconfiguration of the repo).", "Thank you very much for the clarification Damien!"]}, {"number": 11206, "title": "fix_typo of map_structure doc", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please", "Yes! My first contribution to Tensorflow. I should do this more often.", "Congrats! By all means, contributions are welcome :)"]}, {"number": 11205, "title": "How to create our own dataset", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 11204, "title": "fix typo: SYC -> SYCL", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 11203, "title": "L-Op and R-Op in Tensorflow?", "body": "Are there currently equivalents of [theano.gradient.Lop](http://deeplearning.net/software/theano/tutorial/gradients.html#l-operator) and [theano.gradient.Rop](http://deeplearning.net/software/theano/tutorial/gradients.html#r-operator) implemented in Tensorflow? If not, is this a feature that is in the pipeline? I'm trying to port some code over but can't seem to find a way to do this yet. Thank you!", "comments": ["`Lop` is equivalent to `tf.gradients`, but tensorflow does not explicitly have `Rop` built in.\r\n\r\nWith that said, [you can use some tricks to get it done](https://www.reddit.com/r/MachineLearning/comments/6gvv0o/r_first_blog_post_a_new_trick_for_calculating/).", "Thanks so much! Closing now.", "`Rop` can be implemented with `tf.gradients` as well. See, e.g., https://gist.github.com/yang-song/07392ed7d57a92a87968e774aef96762"]}, {"number": 11202, "title": "quantify the mobilenet", "body": "I try to quantify the mobilenet, but I don't decide the out_node .", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 11201, "title": "bazel build causes Kernel Panic on clean Ubuntu 16.04", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Trying for source\r\n- **TensorFlow version (use command below)**: 1.2\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.5.2\r\n- **CUDA/cuDNN version**: 5.1\r\n- **GPU model and memory**: Nvidia GTX 750 Ti\r\n- **Exact command to reproduce**: bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package \r\n\r\n### Describe the problem\r\nI get through a fair chunk of the build process then I get hit with: \r\n\r\nKernel panic - not syncing: Timeout: Not all cpus entered broadcast exception handler\r\n\r\nThe computer then reboots after about 30 seconds.  This is on a fresh install of Ubuntu.\r\n\r\n### Source code / logs\r\n\r\n~/Development/tensorflow$ ./configure\r\n.........\r\nYou have bazel 0.5.2 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\nUsing python library path: /usr/local/lib/python2.7/dist-packages\r\nDo you wish to build TensorFlow with MKL support? [y/N] \r\nNo MKL support will be enabled for TensorFlow\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\nDo you wish to use jemalloc as the malloc implementation? [Y/n] \r\njemalloc enabled\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] Y\r\nGoogle Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] y\r\nHadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] \r\nNo XLA support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with VERBS support? [y/N] y\r\nVERBS support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] \r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nDo you want to use clang as CUDA compiler? [y/N] \r\nnvcc will be used as CUDA compiler\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: \r\nPlease specify the location where CUDA  toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]: 5\r\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"5.0\"]: \"5.0,6.1\"\r\nDo you wish to build TensorFlow with MPI support? [y/N] \r\nMPI support will not be enabled for TensorFlow\r\nConfiguration finished\r\n\r\n\r\n", "comments": ["Try to fix the problem by unplugging all drives and devices from the pc. ", "My fault. It was a bad overclock that I was unaware of. \n\nThanks\nAarron Younan\n102576786\n\n> On Jul 1, 2017, at 12:06 AM, Dhruv <notifications@github.com> wrote:\n> \n> Try to fix the problem by unplugging all drives and devices from the pc.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n"]}, {"number": 11200, "title": "How can I cmpile the Tensorflow library to use CPU Instructions", "body": "My CPU is AMD Ryzen 5 1400\r\nHere is my Instructions sets given by CPU-Z\r\n\tMMX (+), SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2, SSE4A, x86-64, AMD-V, AES, AVX, AVX2, FMA3, SHA", "comments": ["Following suggestions might be helpful,\r\n1. I guess you have to build tensor flow from its source in order to use those instructions and make faster computations.\r\n2. Visit this link as well for same issue [https://github.com/tensorflow/tensorflow/issues/7778](https://github.com/tensorflow/tensorflow/issues/7778)\r\n3. Please review guidelines before [https://github.com/tensorflow/tensorflow/blob/master/ISSUE_TEMPLATE.md](https://github.com/tensorflow/tensorflow/blob/master/ISSUE_TEMPLATE.md) posting an issue. \r\n", "When you run `./configure` before calling `bazel build` you are asked to provide compiler flags in one of the steps. Do NOT provide any compile options at that point as the default is `-march=native` which enables all the CPU specific instructions.", "@LuminousXLB ,  @lakshayg's  suggestion should solve your problem. Let us know if it doesn't.", "@aselle I'm sorry but I got a huge amount of warnings when Building the pip package. And then it gives as below:\r\n`external/snappy/snappy.cc: In member function 'void snappy::SnappySinkAllocator::Flush(size_t)':\r\nexternal/snappy/snappy.cc:1403:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     for (int i = 0; i < blocks_.size(); ++i) {\r\n                       ^\r\nIn file included from external/snappy/snappy-internal.h:34:0,\r\n                 from external/snappy/snappy.cc:30:\r\nexternal/snappy/snappy.cc: In instantiation of 'bool snappy::SnappyScatteredWriter<Allocator>::AppendFromSelf(size_t, size_t) [with Allocator = snappy::SnappySinkAllocator; size_t = long unsigned int]':\r\nexternal/snappy/snappy.cc:715:13:   required from 'void snappy::SnappyDecompressor::DecompressAllTags(Writer*) [with Writer = snappy::SnappyScatteredWriter<snappy::SnappySinkAllocator>]'\r\nexternal/snappy/snappy.cc:799:3:   required from 'bool snappy::InternalUncompressAllTags(snappy::SnappyDecompressor*, Writer*, snappy::uint32) [with Writer = snappy::SnappyScatteredWriter<snappy::SnappySinkAllocator>; snappy::uint32 = unsigned int]'\r\nexternal/snappy/snappy.cc:1460:78:   required from here\r\nexternal/snappy/snappy.cc:1316:34: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     if (PREDICT_TRUE(offset - 1u < op_ptr_ - op_base_ && op_end <= op_limit_)) {\r\n                                  ^\r\nexternal/snappy/snappy-stubs-internal.h:80:25: note: in definition of macro 'PREDICT_TRUE'\r\n #define PREDICT_TRUE(x) x\r\n                         ^\r\nAt global scope:\r\ncc1plus: warning: unrecognized command line option '-Wno-shift-negative-value'\r\nINFO: From Compiling external/snappy/snappy-stubs-internal.cc:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\ncc1plus: warning: unrecognized command line option '-Wno-shift-negative-value'\r\nINFO: From Compiling external/snappy/snappy-c.cc:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\ncc1plus: warning: unrecognized command line option '-Wno-shift-negative-value'\r\nINFO: From Compiling external/snappy/snappy-sinksource.cc:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\ncc1plus: warning: unrecognized command line option '-Wno-shift-negative-value'\r\nINFO: From Compiling external/protobuf/python/google/protobuf/pyext/message.cc:\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc: In function 'PyObject* google::protobuf::python::message_meta::New(PyTypeObject*, PyObject*, PyObject*)':\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc:200:54: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\n   static char *kwlist[] = {\"name\", \"bases\", \"dict\", 0};\r\n                                                      ^\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc:200:54: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc:200:54: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc: At global scope:\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc:398:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\n };\r\n ^\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc:398:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc: In function 'bool google::protobuf::python::CheckAndGetDouble(PyObject*, double*)':\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc:702:44: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\n     FormatTypeError(arg, \"int, long, float\");\r\n                                            ^\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc: In function 'bool google::protobuf::python::CheckAndGetBool(PyObject*, bool*)':\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc:720:43: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\n     FormatTypeError(arg, \"int, long, bool\");\r\n                                           ^\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc: In function 'PyObject* google::protobuf::python::CheckString(PyObject*, const google::protobuf::FieldDescriptor*)':\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc:756:44: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\n       FormatTypeError(arg, \"bytes, unicode\");\r\n                                            ^\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc:771:33: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\n     FormatTypeError(arg, \"bytes\");\r\n                                 ^\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc:782:24: warning: 'PyObject* PyUnicode_AsEncodedObject(PyObject*, const char*, const char*)' is deprecated [-Wdeprecated-declarations]\r\n       encoded_string = PyUnicode_AsEncodedObject(arg, \"utf-8\", NULL);\r\n                        ^\r\nIn file included from bazel-out/local-py3-opt/genfiles/external/local_config_python/python_include/Python.h:77:0,\r\n                 from external/protobuf/python/google/protobuf/pyext/message.h:37,\r\n                 from external/protobuf/python/google/protobuf/pyext/message.cc:34:\r\nbazel-out/local-py3-opt/genfiles/external/local_config_python/python_include/unicodeobject.h:1227:23: note: declared here\r\n PyAPI_FUNC(PyObject*) PyUnicode_AsEncodedObject(\r\n                       ^\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc:782:24: warning: 'PyObject* PyUnicode_AsEncodedObject(PyObject*, const char*, const char*)' is deprecated [-Wdeprecated-declarations]\r\n       encoded_string = PyUnicode_AsEncodedObject(arg, \"utf-8\", NULL);\r\n                        ^\r\nIn file included from bazel-out/local-py3-opt/genfiles/external/local_config_python/python_include/Python.h:77:0,\r\n                 from external/protobuf/python/google/protobuf/pyext/message.h:37,\r\n                 from external/protobuf/python/google/protobuf/pyext/message.cc:34:\r\nbazel-out/local-py3-opt/genfiles/external/local_config_python/python_include/unicodeobject.h:1227:23: note: declared here\r\n PyAPI_FUNC(PyObject*) PyUnicode_AsEncodedObject(\r\n                       ^\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc:782:68: warning: 'PyObject* PyUnicode_AsEncodedObject(PyObject*, const char*, const char*)' is deprecated [-Wdeprecated-declarations]\r\n       encoded_string = PyUnicode_AsEncodedObject(arg, \"utf-8\", NULL);\r\n                                                                    ^\r\nIn file included from bazel-out/local-py3-opt/genfiles/external/local_config_python/python_include/Python.h:77:0,\r\n                 from external/protobuf/python/google/protobuf/pyext/message.h:37,\r\n                 from external/protobuf/python/google/protobuf/pyext/message.cc:34:\r\nbazel-out/local-py3-opt/genfiles/external/local_config_python/python_include/unicodeobject.h:1227:23: note: declared here\r\n PyAPI_FUNC(PyObject*) PyUnicode_AsEncodedObject(\r\n                       ^\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc: At global scope:\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc:2613:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\n };\r\n ^\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc:2613:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc:2613:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc:2613:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc: In instantiation of 'bool google::protobuf::python::CheckAndGetInteger(PyObject*, T*) [with T = int; PyObject = _object]':\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc:694:58:   required from here\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc:634:20: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\n     FormatTypeError(arg, \"int, long\");\r\n                    ^\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc: In instantiation of 'bool google::protobuf::python::CheckAndGetInteger(PyObject*, T*) [with T = long int; PyObject = _object]':\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc:695:58:   required from here\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc:634:20: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc: In instantiation of 'bool google::protobuf::python::CheckAndGetInteger(PyObject*, T*) [with T = unsigned int; PyObject = _object]':\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc:696:60:   required from here\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc:634:20: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc: In instantiation of 'bool google::protobuf::python::CheckAndGetInteger(PyObject*, T*) [with T = long unsigned int; PyObject = _object]':\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc:697:60:   required from here\r\nexternal/protobuf/python/google/protobuf/pyext/message.cc:634:20: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nINFO: From Compiling external/protobuf/python/google/protobuf/pyext/descriptor_pool.cc:\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor_pool.cc: In function 'PyObject* google::protobuf::python::cdescriptor_pool::New(PyTypeObject*, PyObject*, PyObject*)':\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor_pool.cc:138:46: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\n   static char* kwlist[] = {\"descriptor_db\", 0};\r\n                                              ^\r\nINFO: From Compiling external/protobuf/python/google/protobuf/pyext/message_factory.cc:\r\nexternal/protobuf/python/google/protobuf/pyext/message_factory.cc: In function 'PyObject* google::protobuf::python::message_factory::New(PyTypeObject*, PyObject*, PyObject*)':\r\nexternal/protobuf/python/google/protobuf/pyext/message_factory.cc:78:37: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\n   static char* kwlist[] = {\"pool\", 0};\r\n                                     ^\r\nexternal/protobuf/python/google/protobuf/pyext/message_factory.cc: At global scope:\r\nexternal/protobuf/python/google/protobuf/pyext/message_factory.cc:224:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\n };\r\n ^\r\nexternal/protobuf/python/google/protobuf/pyext/message_factory.cc:224:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nINFO: From Compiling external/protobuf/python/google/protobuf/pyext/descriptor.cc:\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\n };\r\n ^\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:627:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\n };\r\n ^\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:924:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1077:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\n };\r\n ^\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1077:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1077:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1077:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1077:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1077:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1077:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1077:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1077:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1077:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1077:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1077:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1077:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1077:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1077:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1077:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1077:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1077:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1186:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\n };\r\n ^\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1186:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1186:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1186:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1186:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1186:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1186:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1186:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1186:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1186:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1186:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1186:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1354:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\n };\r\n ^\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1354:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1354:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1354:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1354:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1354:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1354:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1354:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1354:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1354:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1354:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1354:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1354:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1354:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1354:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1354:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1354:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1354:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1354:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1354:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1354:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1354:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1354:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1354:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1354:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1507:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\n };\r\n ^\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1507:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1507:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1507:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1507:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1507:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1507:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1507:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1507:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1507:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1507:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1507:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1507:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1507:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1618:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\n };\r\n ^\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1618:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1618:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1618:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1618:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1618:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1618:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1618:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1618:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1618:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1722:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\n };\r\n ^\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1722:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1722:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1722:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1722:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1722:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1722:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1722:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1722:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1722:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1722:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nexternal/protobuf/python/google/protobuf/pyext/descriptor.cc:1722:1: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\nINFO: From Compiling tensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc: In instantiation of 'tensorflow::{anonymous}::ProductIterator<InternalType>::ProductIterator(const std::vector<std::unique_ptr<tensorflow::{anonymous}::ColumnInterface<InternalType> > >&, tensorflow::int64) [with InternalType = long long int; tensorflow::int64 = long long int]':\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:368:66:   required from 'tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::Compute(tensorflow::OpKernelContext*)::<lambda(tensorflow::int64, tensorflow::int64)> [with bool HASHED_OUTPUT = true; InternalType = long long int; bool VERSION_2 = true; tensorflow::int64 = long long int]'\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:366:46:   required from 'struct tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::Compute(tensorflow::OpKernelContext*) [with bool HASHED_OUTPUT = true; InternalType = long long int; bool VERSION_2 = true]::<lambda(tensorflow::int64, tensorflow::int64)>'\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:376:5:   required from 'void tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::Compute(tensorflow::OpKernelContext*) [with bool HASHED_OUTPUT = true; InternalType = long long int; bool VERSION_2 = true]'\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:640:1:   required from here\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:263:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     for (int i = 0; i < columns_.size(); i++) {\r\n                       ^\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc: In instantiation of 'tensorflow::{anonymous}::ProductIterator<InternalType>::ProductIterator(const std::vector<std::unique_ptr<tensorflow::{anonymous}::ColumnInterface<InternalType> > >&, tensorflow::int64) [with InternalType = std::__cxx11::basic_string<char>; tensorflow::int64 = long long int]':\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:368:66:   required from 'tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::Compute(tensorflow::OpKernelContext*)::<lambda(tensorflow::int64, tensorflow::int64)> [with bool HASHED_OUTPUT = false; InternalType = std::__cxx11::basic_string<char>; bool VERSION_2 = true; tensorflow::int64 = long long int]'\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:366:46:   required from 'struct tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::Compute(tensorflow::OpKernelContext*) [with bool HASHED_OUTPUT = false; InternalType = std::__cxx11::basic_string<char>; bool VERSION_2 = true]::<lambda(tensorflow::int64, tensorflow::int64)>'\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:376:5:   required from 'void tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::Compute(tensorflow::OpKernelContext*) [with bool HASHED_OUTPUT = false; InternalType = std::__cxx11::basic_string<char>; bool VERSION_2 = true]'\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:640:1:   required from here\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:263:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc: In instantiation of 'std::__cxx11::string tensorflow::{anonymous}::StringCrosser<InternalType>::Generate(tensorflow::int64, const std::vector<int>&) const [with InternalType = std::__cxx11::basic_string<char>; std::__cxx11::string = std::__cxx11::basic_string<char>; tensorflow::int64 = long long int]':\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:372:11:   required from 'tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::Compute(tensorflow::OpKernelContext*)::<lambda(tensorflow::int64, tensorflow::int64)> [with bool HASHED_OUTPUT = false; InternalType = std::__cxx11::basic_string<char>; bool VERSION_2 = true; tensorflow::int64 = long long int]'\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:366:46:   required from 'struct tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::Compute(tensorflow::OpKernelContext*) [with bool HASHED_OUTPUT = false; InternalType = std::__cxx11::basic_string<char>; bool VERSION_2 = true]::<lambda(tensorflow::int64, tensorflow::int64)>'\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:376:5:   required from 'void tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::Compute(tensorflow::OpKernelContext*) [with bool HASHED_OUTPUT = false; InternalType = std::__cxx11::basic_string<char>; bool VERSION_2 = true]'\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:640:1:   required from here\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:173:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     for (int i = 0; i < permutation.size(); i++) {\r\n                       ^\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc: In instantiation of 'tensorflow::{anonymous}::ProductIterator<InternalType>::ProductIterator(const std::vector<std::unique_ptr<tensorflow::{anonymous}::ColumnInterface<InternalType> > >&, tensorflow::int64) [with InternalType = tensorflow::StringPiece; tensorflow::int64 = long long int]':\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:368:66:   required from 'tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::Compute(tensorflow::OpKernelContext*)::<lambda(tensorflow::int64, tensorflow::int64)> [with bool HASHED_OUTPUT = false; InternalType = tensorflow::StringPiece; bool VERSION_2 = true; tensorflow::int64 = long long int]'\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:366:46:   required from 'struct tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::Compute(tensorflow::OpKernelContext*) [with bool HASHED_OUTPUT = false; InternalType = tensorflow::StringPiece; bool VERSION_2 = true]::<lambda(tensorflow::int64, tensorflow::int64)>'\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:376:5:   required from 'void tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::Compute(tensorflow::OpKernelContext*) [with bool HASHED_OUTPUT = false; InternalType = tensorflow::StringPiece; bool VERSION_2 = true]'\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:640:1:   required from here\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:263:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     for (int i = 0; i < columns_.size(); i++) {\r\n                       ^\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc: In instantiation of 'std::__cxx11::string tensorflow::{anonymous}::StringCrosser<InternalType>::Generate(tensorflow::int64, const std::vector<int>&) const [with InternalType = tensorflow::StringPiece; std::__cxx11::string = std::__cxx11::basic_string<char>; tensorflow::int64 = long long int]':\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:372:11:   required from 'tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::Compute(tensorflow::OpKernelContext*)::<lambda(tensorflow::int64, tensorflow::int64)> [with bool HASHED_OUTPUT = false; InternalType = tensorflow::StringPiece; bool VERSION_2 = true; tensorflow::int64 = long long int]'\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:366:46:   required from 'struct tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::Compute(tensorflow::OpKernelContext*) [with bool HASHED_OUTPUT = false; InternalType = tensorflow::StringPiece; bool VERSION_2 = true]::<lambda(tensorflow::int64, tensorflow::int64)>'\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:376:5:   required from 'void tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::Compute(tensorflow::OpKernelContext*) [with bool HASHED_OUTPUT = false; InternalType = tensorflow::StringPiece; bool VERSION_2 = true]'\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:640:1:   required from here\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:173:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     for (int i = 0; i < permutation.size(); i++) {\r\n                       ^\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc: In instantiation of 'tensorflow::int64 tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::CrossCountByBatchIndex(const std::vector<std::unique_ptr<tensorflow::{anonymous}::ColumnInterface<InternalType> > >&, int) [with bool HASHED_OUTPUT = true; InternalType = long long int; bool VERSION_2 = true; tensorflow::int64 = long long int]':\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:549:54:   required from 'void tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::CreateOutputTensors(const std::vector<std::unique_ptr<tensorflow::{anonymous}::ColumnInterface<InternalType> > >&, tensorflow::int64, tensorflow::OpKernelContext*, tensorflow::Tensor**, tensorflow::Tensor**, tensorflow::Tensor**, std::vector<long long int, std::allocator<long long int> >*) [with bool HASHED_OUTPUT = true; InternalType = long long int; bool VERSION_2 = true; tensorflow::int64 = long long int]'\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:361:24:   required from 'void tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::Compute(tensorflow::OpKernelContext*) [with bool HASHED_OUTPUT = true; InternalType = long long int; bool VERSION_2 = true]'\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:640:1:   required from here\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:576:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     for (int i = 0; i < columns.size(); i++) {\r\n                       ^\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc: In instantiation of 'tensorflow::int64 tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::CrossCountByBatchIndex(const std::vector<std::unique_ptr<tensorflow::{anonymous}::ColumnInterface<InternalType> > >&, int) [with bool HASHED_OUTPUT = false; InternalType = std::__cxx11::basic_string<char>; bool VERSION_2 = true; tensorflow::int64 = long long int]':\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:549:54:   required from 'void tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::CreateOutputTensors(const std::vector<std::unique_ptr<tensorflow::{anonymous}::ColumnInterface<InternalType> > >&, tensorflow::int64, tensorflow::OpKernelContext*, tensorflow::Tensor**, tensorflow::Tensor**, tensorflow::Tensor**, std::vector<long long int, std::allocator<long long int> >*) [with bool HASHED_OUTPUT = false; InternalType = std::__cxx11::basic_string<char>; bool VERSION_2 = true; tensorflow::int64 = long long int]'\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:361:24:   required from 'void tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::Compute(tensorflow::OpKernelContext*) [with bool HASHED_OUTPUT = false; InternalType = std::__cxx11::basic_string<char>; bool VERSION_2 = true]'\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:640:1:   required from here\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:576:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc: In instantiation of 'tensorflow::int64 tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::CrossCountByBatchIndex(const std::vector<std::unique_ptr<tensorflow::{anonymous}::ColumnInterface<InternalType> > >&, int) [with bool HASHED_OUTPUT = false; InternalType = tensorflow::StringPiece; bool VERSION_2 = true; tensorflow::int64 = long long int]':\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:549:54:   required from 'void tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::CreateOutputTensors(const std::vector<std::unique_ptr<tensorflow::{anonymous}::ColumnInterface<InternalType> > >&, tensorflow::int64, tensorflow::OpKernelContext*, tensorflow::Tensor**, tensorflow::Tensor**, tensorflow::Tensor**, std::vector<long long int, std::allocator<long long int> >*) [with bool HASHED_OUTPUT = false; InternalType = tensorflow::StringPiece; bool VERSION_2 = true; tensorflow::int64 = long long int]'\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:361:24:   required from 'void tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::Compute(tensorflow::OpKernelContext*) [with bool HASHED_OUTPUT = false; InternalType = tensorflow::StringPiece; bool VERSION_2 = true]'\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:640:1:   required from here\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:576:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc: In instantiation of 'tensorflow::int64 tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::CrossCountByBatchIndex(const std::vector<std::unique_ptr<tensorflow::{anonymous}::ColumnInterface<InternalType> > >&, int) [with bool HASHED_OUTPUT = true; InternalType = long long int; bool VERSION_2 = false; tensorflow::int64 = long long int]':\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:549:54:   required from 'void tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::CreateOutputTensors(const std::vector<std::unique_ptr<tensorflow::{anonymous}::ColumnInterface<InternalType> > >&, tensorflow::int64, tensorflow::OpKernelContext*, tensorflow::Tensor**, tensorflow::Tensor**, tensorflow::Tensor**, std::vector<long long int, std::allocator<long long int> >*) [with bool HASHED_OUTPUT = true; InternalType = long long int; bool VERSION_2 = false; tensorflow::int64 = long long int]'\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:361:24:   required from 'void tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::Compute(tensorflow::OpKernelContext*) [with bool HASHED_OUTPUT = true; InternalType = long long int; bool VERSION_2 = false]'\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:640:1:   required from here\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:576:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc: In instantiation of 'tensorflow::int64 tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::CrossCountByBatchIndex(const std::vector<std::unique_ptr<tensorflow::{anonymous}::ColumnInterface<InternalType> > >&, int) [with bool HASHED_OUTPUT = false; InternalType = std::__cxx11::basic_string<char>; bool VERSION_2 = false; tensorflow::int64 = long long int]':\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:549:54:   required from 'void tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::CreateOutputTensors(const std::vector<std::unique_ptr<tensorflow::{anonymous}::ColumnInterface<InternalType> > >&, tensorflow::int64, tensorflow::OpKernelContext*, tensorflow::Tensor**, tensorflow::Tensor**, tensorflow::Tensor**, std::vector<long long int, std::allocator<long long int> >*) [with bool HASHED_OUTPUT = false; InternalType = std::__cxx11::basic_string<char>; bool VERSION_2 = false; tensorflow::int64 = long long int]'\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:361:24:   required from 'void tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::Compute(tensorflow::OpKernelContext*) [with bool HASHED_OUTPUT = false; InternalType = std::__cxx11::basic_string<char>; bool VERSION_2 = false]'\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:640:1:   required from here\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:576:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc: In instantiation of 'tensorflow::int64 tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::CrossCountByBatchIndex(const std::vector<std::unique_ptr<tensorflow::{anonymous}::ColumnInterface<InternalType> > >&, int) [with bool HASHED_OUTPUT = false; InternalType = tensorflow::StringPiece; bool VERSION_2 = false; tensorflow::int64 = long long int]':\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:549:54:   required from 'void tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::CreateOutputTensors(const std::vector<std::unique_ptr<tensorflow::{anonymous}::ColumnInterface<InternalType> > >&, tensorflow::int64, tensorflow::OpKernelContext*, tensorflow::Tensor**, tensorflow::Tensor**, tensorflow::Tensor**, std::vector<long long int, std::allocator<long long int> >*) [with bool HASHED_OUTPUT = false; InternalType = tensorflow::StringPiece; bool VERSION_2 = false; tensorflow::int64 = long long int]'\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:361:24:   required from 'void tensorflow::SparseFeatureCrossOp<HASHED_OUTPUT, InternalType, VERSION_2>::Compute(tensorflow::OpKernelContext*) [with bool HASHED_OUTPUT = false; InternalType = tensorflow::StringPiece; bool VERSION_2 = false]'\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:640:1:   required from here\r\ntensorflow/contrib/layers/kernels/sparse_feature_cross_kernel.cc:576:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\nINFO: From Compiling tensorflow/contrib/factorization/kernels/wals_solver_ops.cc:\r\nIn file included from ./tensorflow/core/platform/default/logging.h:24:0,\r\n                 from ./tensorflow/core/platform/logging.h:25,\r\n                 from ./tensorflow/core/lib/core/status.h:24,\r\n                 from ./tensorflow/core/framework/op_def_builder.h:25,\r\n                 from ./tensorflow/core/framework/op.h:24,\r\n                 from tensorflow/contrib/factorization/kernels/wals_solver_ops.cc:26:\r\n./tensorflow/core/platform/default/logging.h: In instantiation of 'std::__cxx11::string* tensorflow::internal::Check_LEImpl(const T1&, const T2&, const char*) [with T1 = long unsigned int; T2 = long long int; std::__cxx11::string = std::__cxx11::basic_string<char>]':\r\ntensorflow/contrib/factorization/kernels/wals_solver_ops.cc:170:5:   required from here\r\n./tensorflow/core/platform/default/logging.h:230:35: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n TF_DEFINE_CHECK_OP_IMPL(Check_LE, <= )\r\n                                   ^\r\n./tensorflow/core/platform/macros.h:76:29: note: in definition of macro 'TF_PREDICT_TRUE'\r\n #define TF_PREDICT_TRUE(x) (x)\r\n                             ^\r\n./tensorflow/core/platform/default/logging.h:230:1: note: in expansion of macro 'TF_DEFINE_CHECK_OP_IMPL'\r\n TF_DEFINE_CHECK_OP_IMPL(Check_LE, <= )\r\n ^\r\n./tensorflow/core/platform/default/logging.h: In instantiation of 'std::__cxx11::string* tensorflow::internal::Check_LEImpl(const T1&, const T2&, const char*) [with T1 = long long int; T2 = long unsigned int; std::__cxx11::string = std::__cxx11::basic_string<char>]':\r\ntensorflow/contrib/factorization/kernels/wals_solver_ops.cc:213:7:   required from here\r\n./tensorflow/core/platform/default/logging.h:230:35: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n TF_DEFINE_CHECK_OP_IMPL(Check_LE, <= )\r\n                                   ^\r\n./tensorflow/core/platform/macros.h:76:29: note: in definition of macro 'TF_PREDICT_TRUE'\r\n #define TF_PREDICT_TRUE(x) (x)\r\n                             ^\r\n./tensorflow/core/platform/default/logging.h:230:1: note: in expansion of macro 'TF_DEFINE_CHECK_OP_IMPL'\r\n TF_DEFINE_CHECK_OP_IMPL(Check_LE, <= )\r\n ^\r\nINFO: From Compiling tensorflow/contrib/image/kernels/image_ops.cc:\r\nIn file included from tensorflow/contrib/image/kernels/image_ops.cc:22:0:\r\n./tensorflow/contrib/image/kernels/image_ops.h: In member function 'T tensorflow::generator::ProjectiveGenerator<Device, T>::operator()(const Eigen::array<long int, 4ul>&) const [with Device = Eigen::ThreadPoolDevice; T = unsigned char]':\r\n./tensorflow/contrib/image/kernels/image_ops.h:79:3: warning: control reaches end of non-void function [-Wreturn-type]\r\n   }\r\n   ^\r\n./tensorflow/contrib/image/kernels/image_ops.h: In member function 'T tensorflow::generator::ProjectiveGenerator<Device, T>::operator()(const Eigen::array<long int, 4ul>&) const [with Device = Eigen::ThreadPoolDevice; T = int]':\r\n./tensorflow/contrib/image/kernels/image_ops.h:79:3: warning: control reaches end of non-void function [-Wreturn-type]\r\n   }\r\n   ^\r\n./tensorflow/contrib/image/kernels/image_ops.h: In member function 'T tensorflow::generator::ProjectiveGenerator<Device, T>::operator()(const Eigen::array<long int, 4ul>&) const [with Device = Eigen::ThreadPoolDevice; T = long long int]':\r\n./tensorflow/contrib/image/kernels/image_ops.h:79:3: warning: control reaches end of non-void function [-Wreturn-type]\r\n   }\r\n   ^\r\n./tensorflow/contrib/image/kernels/image_ops.h: In member function 'T tensorflow::generator::ProjectiveGenerator<Device, T>::operator()(const Eigen::array<long int, 4ul>&) const [with Device = Eigen::ThreadPoolDevice; T = float]':\r\n./tensorflow/contrib/image/kernels/image_ops.h:79:3: warning: control reaches end of non-void function [-Wreturn-type]\r\n   }\r\n   ^\r\n./tensorflow/contrib/image/kernels/image_ops.h: In member function 'T tensorflow::generator::ProjectiveGenerator<Device, T>::operator()(const Eigen::array<long int, 4ul>&) const [with Device = Eigen::ThreadPoolDevice; T = double]':\r\n./tensorflow/contrib/image/kernels/image_ops.h:79:3: warning: control reaches end of non-void function [-Wreturn-type]\r\n   }\r\n   ^\r\nINFO: From Compiling tensorflow/core/platform/default/tracing.cc:\r\ntensorflow/core/platform/default/tracing.cc:32:19: warning: 'tensorflow::port::dummy' defined but not used [-Wunused-variable]\r\n static const bool dummy = DoInit();\r\n                   ^\r\nINFO: From Compiling tensorflow/tools/tfprof/internal/tfprof_timeline.cc:\r\ntensorflow/tools/tfprof/internal/tfprof_timeline.cc: In member function 'std::vector<tensorflow::tfprof::TimeNode*> tensorflow::tfprof::Timeline::AddGraphNode(const tensorflow::tfprof::GraphNode*)':\r\ntensorflow/tools/tfprof/internal/tfprof_timeline.cc:187:25: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n       for (int i = 0; i < shown_cinputs.size(); i++) {\r\n                         ^\r\ntensorflow/tools/tfprof/internal/tfprof_timeline.cc: In member function 'void tensorflow::tfprof::Timeline::AllocateLanes()':\r\ntensorflow/tools/tfprof/internal/tfprof_timeline.cc:204:25: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n       for (int i = 0; i < p->lanes.size(); ++i) {\r\n                         ^\r\nINFO: From Compiling tensorflow/core/kernels/range_sampler.cc:\r\nIn file included from ./tensorflow/core/platform/default/logging.h:24:0,\r\n                 from ./tensorflow/core/platform/logging.h:25,\r\n                 from ./tensorflow/core/lib/core/status.h:24,\r\n                 from ./tensorflow/core/kernels/range_sampler.h:21,\r\n                 from tensorflow/core/kernels/range_sampler.cc:16:\r\n./tensorflow/core/platform/default/logging.h: In instantiation of 'std::__cxx11::string* tensorflow::internal::Check_LEImpl(const T1&, const T2&, const char*) [with T1 = long unsigned int; T2 = long long int; std::__cxx11::string = std::__cxx11::basic_string<char>]':\r\ntensorflow/core/kernels/range_sampler.cc:86:5:   required from here\r\n./tensorflow/core/platform/default/logging.h:230:35: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n TF_DEFINE_CHECK_OP_IMPL(Check_LE, <= )\r\n                                   ^\r\n./tensorflow/core/platform/macros.h:76:29: note: in definition of macro 'TF_PREDICT_TRUE'\r\n #define TF_PREDICT_TRUE(x) (x)\r\n                             ^\r\n./tensorflow/core/platform/default/logging.h:230:1: note: in expansion of macro 'TF_DEFINE_CHECK_OP_IMPL'\r\n TF_DEFINE_CHECK_OP_IMPL(Check_LE, <= )\r\n ^\r\n./tensorflow/core/platform/default/logging.h: In instantiation of 'std::__cxx11::string* tensorflow::internal::Check_EQImpl(const T1&, const T2&, const char*) [with T1 = long long int; T2 = long unsigned int; std::__cxx11::string = std::__cxx11::basic_string<char>]':\r\ntensorflow/core/kernels/range_sampler.cc:244:3:   required from here\r\n./tensorflow/core/platform/default/logging.h:228:25: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n                         == )  // Compilation error with CHECK_EQ(NULL, x)?\r\n                         ^\r\n./tensorflow/core/platform/macros.h:76:29: note: in definition of macro 'TF_PREDICT_TRUE'\r\n #define TF_PREDICT_TRUE(x) (x)\r\n                             ^\r\n./tensorflow/core/platform/default/logging.h:227:1: note: in expansion of macro 'TF_DEFINE_CHECK_OP_IMPL'\r\n TF_DEFINE_CHECK_OP_IMPL(Check_EQ,\r\n ^\r\nINFO: From Compiling tensorflow/core/protobuf/config.pb_text.cc:\r\nbazel-out/local-py3-opt/genfiles/tensorflow/core/protobuf/config.pb_text.cc: In function 'bool tensorflow::internal::ProtoParseFromScanner(tensorflow::strings::Scanner*, bool, bool, tensorflow::ConfigProto*)':\r\nbazel-out/local-py3-opt/genfiles/tensorflow/core/protobuf/config.pb_text.cc:619:34: warning: 'map_value' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n       (*map)[map_key] = map_value;\r\n                                  ^\r\nbazel-out/local-py3-opt/genfiles/tensorflow/core/protobuf/config.pb_text.cc:610:9: note: 'map_value' was declared here\r\n   int32 map_value;\r\n         ^\r\nINFO: From Compiling tensorflow/core/framework/allocator.cc:\r\ntensorflow/core/framework/allocator.cc:113:12: warning: 'tensorflow::Allocator* tensorflow::{anonymous}::MakeCpuAllocator()' defined but not used [-Wunused-function]\r\n Allocator* MakeCpuAllocator() {\r\n            ^\r\nINFO: From Compiling tensorflow/tools/tfprof/internal/tfprof_tensor.cc:\r\nIn file included from tensorflow/tools/tfprof/internal/tfprof_tensor.cc:16:0:\r\n./tensorflow/tools/tfprof/internal/tfprof_tensor.h: In instantiation of 'tensorflow::int64 tensorflow::tfprof::TFProfTensor::BuildOutput(tensorflow::int64, int, const std::vector<_RealType>&, tensorflow::tfprof::TFProfTensorProto*) [with T = double; tensorflow::int64 = long long int]':\r\ntensorflow/tools/tfprof/internal/tfprof_tensor.cc:48:63:   required from here\r\n./tensorflow/tools/tfprof/internal/tfprof_tensor.h:78:44: warning: format '%lld' expects argument of type 'long long int', but argument 2 has type 'google::protobuf::int64 {aka long int}' [-Wformat=]\r\n           formatted_str_ += strings::Printf(\r\n                                            ^\r\n./tensorflow/tools/tfprof/internal/tfprof_tensor.h: In instantiation of 'tensorflow::int64 tensorflow::tfprof::TFProfTensor::BuildOutput(tensorflow::int64, int, const std::vector<_RealType>&, tensorflow::tfprof::TFProfTensorProto*) [with T = long long int; tensorflow::int64 = long long int]':\r\ntensorflow/tools/tfprof/internal/tfprof_tensor.cc:60:62:   required from here\r\n./tensorflow/tools/tfprof/internal/tfprof_tensor.h:78:44: warning: format '%lld' expects argument of type 'long long int', but argument 2 has type 'google::protobuf::int64 {aka long int}' [-Wformat=]\r\n./tensorflow/tools/tfprof/internal/tfprof_tensor.h: In instantiation of 'tensorflow::int64 tensorflow::tfprof::TFProfTensor::BuildOutput(tensorflow::int64, int, const std::vector<_RealType>&, tensorflow::tfprof::TFProfTensorProto*) [with T = std::__cxx11::basic_string<char>; tensorflow::int64 = long long int]':\r\ntensorflow/tools/tfprof/internal/tfprof_tensor.cc:67:63:   required from here\r\n./tensorflow/tools/tfprof/internal/tfprof_tensor.h:78:44: warning: format '%lld' expects argument of type 'long long int', but argument 2 has type 'google::protobuf::int64 {aka long int}' [-Wformat=]\r\nINFO: From Compiling tensorflow/core/ops/data_flow_ops.cc:\r\ntensorflow/core/ops/data_flow_ops.cc:233:8: warning: 'tensorflow::Status tensorflow::{anonymous}::ScalarAndTwoElementVectorInputsAndScalarOutputs(tensorflow::shape_inference::InferenceContext*)' defined but not used [-Wunused-function]\r\n Status ScalarAndTwoElementVectorInputsAndScalarOutputs(InferenceContext* c) {\r\n        ^\r\ntensorflow/core/ops/data_flow_ops.cc:252:8: warning: 'tensorflow::Status tensorflow::{anonymous}::ScalarOutput(tensorflow::shape_inference::InferenceContext*)' defined but not used [-Wunused-function]\r\n Status ScalarOutput(InferenceContext* c) {\r\n        ^\r\nINFO: From Compiling tensorflow/core/kernels/batch_dataset_op.cc:\r\nIn file included from ./tensorflow/core/framework/tensor.h:28:0,\r\n                 from ./tensorflow/core/framework/attr_value_util.h:23,\r\n                 from ./tensorflow/core/framework/node_def_util.h:23,\r\n                 from ./tensorflow/core/framework/shape_inference.h:21,\r\n                 from ./tensorflow/core/framework/common_shape_fns.h:20,\r\n                 from ./tensorflow/core/framework/resource_mgr.h:24,\r\n                 from ./tensorflow/core/kernels/dataset.h:20,\r\n                 from tensorflow/core/kernels/batch_dataset_op.cc:15:\r\n./tensorflow/core/lib/core/refcount.h: In member function 'virtual void tensorflow::{anonymous}::BatchDatasetOp::Compute(tensorflow::OpKernelContext*)':\r\n./tensorflow/core/lib/core/refcount.h:93:12: warning: 'input' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     delete this;\r\n            ^\r\ntensorflow/core/kernels/batch_dataset_op.cc:34:18: note: 'input' was declared here\r\n     DatasetBase* input;\r\n                  ^\r\nINFO: From Compiling tensorflow/core/kernels/padded_batch_dataset_op.cc:\r\nIn file included from ./tensorflow/core/framework/tensor.h:28:0,\r\n                 from ./tensorflow/core/framework/attr_value_util.h:23,\r\n                 from ./tensorflow/core/framework/node_def_util.h:23,\r\n                 from ./tensorflow/core/framework/shape_inference.h:21,\r\n                 from ./tensorflow/core/framework/common_shape_fns.h:20,\r\n                 from ./tensorflow/core/framework/resource_mgr.h:24,\r\n                 from ./tensorflow/core/kernels/dataset.h:20,\r\n                 from tensorflow/core/kernels/padded_batch_dataset_op.cc:15:\r\n./tensorflow/core/lib/core/refcount.h: In member function 'virtual void tensorflow::{anonymous}::PaddedBatchDatasetOp::Compute(tensorflow::OpKernelContext*)':\r\n./tensorflow/core/lib/core/refcount.h:93:12: warning: 'input' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     delete this;\r\n            ^\r\ntensorflow/core/kernels/padded_batch_dataset_op.cc:132:18: note: 'input' was declared here\r\n     DatasetBase* input;\r\n                  ^\r\nINFO: From Compiling tensorflow/core/kernels/take_dataset_op.cc:\r\nIn file included from ./tensorflow/core/framework/tensor.h:28:0,\r\n                 from ./tensorflow/core/framework/attr_value_util.h:23,\r\n                 from ./tensorflow/core/framework/node_def_util.h:23,\r\n                 from ./tensorflow/core/framework/shape_inference.h:21,\r\n                 from ./tensorflow/core/framework/common_shape_fns.h:20,\r\n                 from ./tensorflow/core/framework/resource_mgr.h:24,\r\n                 from ./tensorflow/core/kernels/dataset.h:20,\r\n                 from tensorflow/core/kernels/take_dataset_op.cc:15:\r\n./tensorflow/core/lib/core/refcount.h: In member function 'virtual void tensorflow::{anonymous}::TakeDatasetOp::Compute(tensorflow::OpKernelContext*)':\r\n./tensorflow/core/lib/core/refcount.h:93:12: warning: 'input' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     delete this;\r\n            ^\r\ntensorflow/core/kernels/take_dataset_op.cc:34:18: note: 'input' was declared here\r\n     DatasetBase* input;\r\n                  ^\r\nINFO: From Compiling tensorflow/core/kernels/shuffle_dataset_op.cc:\r\nIn file included from ./tensorflow/core/framework/tensor.h:28:0,\r\n                 from ./tensorflow/core/framework/attr_value_util.h:23,\r\n                 from ./tensorflow/core/framework/node_def_util.h:23,\r\n                 from ./tensorflow/core/framework/shape_inference.h:21,\r\n                 from ./tensorflow/core/framework/common_shape_fns.h:20,\r\n                 from ./tensorflow/core/framework/resource_mgr.h:24,\r\n                 from ./tensorflow/core/kernels/dataset.h:20,\r\n                 from tensorflow/core/kernels/shuffle_dataset_op.cc:15:\r\n./tensorflow/core/lib/core/refcount.h: In member function 'virtual void tensorflow::{anonymous}::ShuffleDatasetOp::Compute(tensorflow::OpKernelContext*)':\r\n./tensorflow/core/lib/core/refcount.h:93:12: warning: 'input' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     delete this;\r\n            ^\r\ntensorflow/core/kernels/shuffle_dataset_op.cc:37:18: note: 'input' was declared here\r\n     DatasetBase* input;\r\n                  ^\r\nINFO: From Compiling tensorflow/core/kernels/dense_to_sparse_batch_dataset_op.cc:\r\ntensorflow/core/kernels/dense_to_sparse_batch_dataset_op.cc: In member function 'virtual void tensorflow::{anonymous}::DenseToSparseBatchDatasetOp::Compute(tensorflow::OpKernelContext*)':\r\ntensorflow/core/kernels/dense_to_sparse_batch_dataset_op.cc:40:35: warning: 'input' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n         ctx, input->output_dtypes().size() == 1,\r\n                                   ^\r\nINFO: From Compiling tensorflow/core/kernels/repeat_dataset_op.cc:\r\nIn file included from ./tensorflow/core/framework/tensor.h:28:0,\r\n                 from ./tensorflow/core/framework/attr_value_util.h:23,\r\n                 from ./tensorflow/core/framework/node_def_util.h:23,\r\n                 from ./tensorflow/core/framework/shape_inference.h:21,\r\n                 from ./tensorflow/core/framework/common_shape_fns.h:20,\r\n                 from ./tensorflow/core/framework/resource_mgr.h:24,\r\n                 from ./tensorflow/core/kernels/dataset.h:20,\r\n                 from tensorflow/core/kernels/repeat_dataset_op.cc:15:\r\n./tensorflow/core/lib/core/refcount.h: In member function 'virtual void tensorflow::{anonymous}::RepeatDatasetOp::Compute(tensorflow::OpKernelContext*)':\r\n./tensorflow/core/lib/core/refcount.h:93:12: warning: 'input' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     delete this;\r\n            ^\r\ntensorflow/core/kernels/repeat_dataset_op.cc:34:18: note: 'input' was declared here\r\n     DatasetBase* input;\r\n                  ^\r\nINFO: From Compiling tensorflow/core/kernels/skip_dataset_op.cc:\r\nIn file included from ./tensorflow/core/framework/tensor.h:28:0,\r\n                 from ./tensorflow/core/framework/attr_value_util.h:23,\r\n                 from ./tensorflow/core/framework/node_def_util.h:23,\r\n                 from ./tensorflow/core/framework/shape_inference.h:21,\r\n                 from ./tensorflow/core/framework/common_shape_fns.h:20,\r\n                 from ./tensorflow/core/framework/resource_mgr.h:24,\r\n                 from ./tensorflow/core/kernels/dataset.h:20,\r\n                 from tensorflow/core/kernels/skip_dataset_op.cc:15:\r\n./tensorflow/core/lib/core/refcount.h: In member function 'virtual void tensorflow::{anonymous}::SkipDatasetOp::Compute(tensorflow::OpKernelContext*)':\r\n./tensorflow/core/lib/core/refcount.h:93:12: warning: 'input' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     delete this;\r\n            ^\r\ntensorflow/core/kernels/skip_dataset_op.cc:34:18: note: 'input' was declared here\r\n     DatasetBase* input;\r\n                  ^\r\nINFO: From Compiling tensorflow/core/ops/array_grad.cc:\r\nIn file included from tensorflow/core/ops/array_grad.cc:17:0:\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_0' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:490:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/core/ops/array_grad.cc:24:1: note: in expansion of macro 'REGISTER_OP_NO_GRADIENT'\r\n REGISTER_OP_NO_GRADIENT(\"Shape\");\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_1' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:490:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/core/ops/array_grad.cc:25:1: note: in expansion of macro 'REGISTER_OP_NO_GRADIENT'\r\n REGISTER_OP_NO_GRADIENT(\"Rank\");\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_2' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:490:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/core/ops/array_grad.cc:26:1: note: in expansion of macro 'REGISTER_OP_NO_GRADIENT'\r\n REGISTER_OP_NO_GRADIENT(\"Size\");\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_3' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:490:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/core/ops/array_grad.cc:27:1: note: in expansion of macro 'REGISTER_OP_NO_GRADIENT'\r\n REGISTER_OP_NO_GRADIENT(\"ZerosLike\");\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_4' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:490:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/core/ops/array_grad.cc:28:1: note: in expansion of macro 'REGISTER_OP_NO_GRADIENT'\r\n REGISTER_OP_NO_GRADIENT(\"OnesLike\");\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_5' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:490:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/core/ops/array_grad.cc:29:1: note: in expansion of macro 'REGISTER_OP_NO_GRADIENT'\r\n REGISTER_OP_NO_GRADIENT(\"Const\");\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_6' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:490:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/core/ops/array_grad.cc:30:1: note: in expansion of macro 'REGISTER_OP_NO_GRADIENT'\r\n REGISTER_OP_NO_GRADIENT(\"EditDistance\");\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_7' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:490:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/core/ops/array_grad.cc:31:1: note: in expansion of macro 'REGISTER_OP_NO_GRADIENT'\r\n REGISTER_OP_NO_GRADIENT(\"StopGradient\");\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_8' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/array_grad.cc:51:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Reshape\", ReshapeGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_9' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/array_grad.cc:52:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"ExpandDims\", ReshapeGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_10' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/array_grad.cc:71:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Squeeze\", SqueezeGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_11' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/array_grad.cc:90:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Identity\", IdentityGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_12' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/array_grad.cc:116:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Pack\", PackGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_13' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/array_grad.cc:140:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Unpack\", UnpackGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_14' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/array_grad.cc:224:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Concat\", ConcatGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_15' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/array_grad.cc:225:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"ConcatV2\", ConcatGradV2);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_16' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/array_grad.cc:245:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Split\", SplitGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_17' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/array_grad.cc:271:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"_ArrayToList\", ArrayToListGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_18' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/array_grad.cc:291:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"_ListToArray\", ListToArrayGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_19' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/array_grad.cc:314:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Fill\", FillGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_20' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/array_grad.cc:333:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Transpose\", TransposeGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_21' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/array_grad.cc:351:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Reverse\", ReverseGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_22' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/array_grad.cc:375:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"ReverseV2\", ReverseV2Grad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_23' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/array_grad.cc:410:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Slice\", SliceGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_24' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/array_grad.cc:447:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"StridedSlice\", StridedSliceGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_25' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/array_grad.cc:489:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"StridedSliceGrad\", StridedSliceGradGrad);\r\n ^\r\nINFO: From Compiling tensorflow/core/ops/random_grad.cc:\r\nIn file included from tensorflow/core/ops/random_grad.cc:16:0:\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_0' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:490:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/core/ops/random_grad.cc:19:1: note: in expansion of macro 'REGISTER_OP_NO_GRADIENT'\r\n REGISTER_OP_NO_GRADIENT(\"RandomUniform\");\r\n ^\r\nINFO: From Compiling tensorflow/core/ops/math_grad.cc:\r\nIn file included from tensorflow/core/ops/math_grad.cc:17:0:\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_0' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:51:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Abs\", AbsGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_1' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:60:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Neg\", NegGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_2' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:72:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Inv\", InvGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_3' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:73:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Reciprocal\", InvGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_4' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:85:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Square\", SquareGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_5' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:99:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Sqrt\", SqrtGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_6' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:114:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Rsqrt\", RsqrtGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_7' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:124:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Exp\", ExpGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_8' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:134:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Expm1\", Expm1Grad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_9' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:144:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Log\", LogGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_10' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:156:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Log1p\", Log1pGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_11' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:170:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Tanh\", TanhGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_12' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:184:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Sigmoid\", SigmoidGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_13' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:196:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Sign\", SignGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_14' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:206:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Sin\", SinGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_15' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:217:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Cos\", CosGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_16' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:233:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Acos\", AcosGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_17' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:248:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Asin\", AsinGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_18' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:262:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Atan\", AtanGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_19' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:274:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Tan\", TanGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_20' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:284:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Real\", RealGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_21' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:294:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Imag\", ImagGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_22' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:303:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Conj\", ConjGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_23' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:352:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Add\", AddGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_24' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:362:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Sub\", SubGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_25' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:384:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Mul\", MulGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_26' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:397:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Div\", DivGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_27' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:410:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"RealDiv\", RealDivGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_28' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:451:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Pow\", PowGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_29' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:468:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Maximum\", MaximumGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_30' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:473:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Minimum\", MinimumGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_31' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:483:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Complex\", ComplexGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_32' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:501:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Select\", SelectGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_33' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:565:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Sum\", SumGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_34' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:580:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Mean\", MeanGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_35' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:623:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Max\", MaxGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_36' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:628:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Min\", MinGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_37' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:690:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"MatMul\", MatMulGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_38' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:695:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"BatchMatMul\", BatchMatMulGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_39' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:490:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:700:1: note: in expansion of macro 'REGISTER_OP_NO_GRADIENT'\r\n REGISTER_OP_NO_GRADIENT(\"Less\");\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_40' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:490:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:701:1: note: in expansion of macro 'REGISTER_OP_NO_GRADIENT'\r\n REGISTER_OP_NO_GRADIENT(\"LessEqual\");\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_41' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:490:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:702:1: note: in expansion of macro 'REGISTER_OP_NO_GRADIENT'\r\n REGISTER_OP_NO_GRADIENT(\"Greater\");\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_42' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:490:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:703:1: note: in expansion of macro 'REGISTER_OP_NO_GRADIENT'\r\n REGISTER_OP_NO_GRADIENT(\"GreaterEqual\");\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_43' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:490:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:704:1: note: in expansion of macro 'REGISTER_OP_NO_GRADIENT'\r\n REGISTER_OP_NO_GRADIENT(\"Equal\");\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_44' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:490:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:705:1: note: in expansion of macro 'REGISTER_OP_NO_GRADIENT'\r\n REGISTER_OP_NO_GRADIENT(\"NotEqual\");\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_45' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:490:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:708:1: note: in expansion of macro 'REGISTER_OP_NO_GRADIENT'\r\n REGISTER_OP_NO_GRADIENT(\"LogicalAnd\");\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_46' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:490:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:709:1: note: in expansion of macro 'REGISTER_OP_NO_GRADIENT'\r\n REGISTER_OP_NO_GRADIENT(\"LogicalOr\");\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_47' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:490:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:710:1: note: in expansion of macro 'REGISTER_OP_NO_GRADIENT'\r\n REGISTER_OP_NO_GRADIENT(\"LogicalNot\");\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_48' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:490:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:713:1: note: in expansion of macro 'REGISTER_OP_NO_GRADIENT'\r\n REGISTER_OP_NO_GRADIENT(\"Range\");\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_49' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:490:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:714:1: note: in expansion of macro 'REGISTER_OP_NO_GRADIENT'\r\n REGISTER_OP_NO_GRADIENT(\"LinSpace\");\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_50' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:490:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:716:1: note: in expansion of macro 'REGISTER_OP_NO_GRADIENT'\r\n REGISTER_OP_NO_GRADIENT(\"Floor\");\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_51' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:490:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:717:1: note: in expansion of macro 'REGISTER_OP_NO_GRADIENT'\r\n REGISTER_OP_NO_GRADIENT(\"FloorDiv\");\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_52' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:490:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/core/ops/math_grad.cc:718:1: note: in expansion of macro 'REGISTER_OP_NO_GRADIENT'\r\n REGISTER_OP_NO_GRADIENT(\"TruncateDiv\");\r\n ^\r\nINFO: From Compiling tensorflow/core/ops/functional_grad.cc:\r\nIn file included from tensorflow/core/ops/functional_grad.cc:16:0:\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_0' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/functional_grad.cc:56:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"MapAccumulate\", MapAccumulateGrad);\r\n ^\r\nINFO: From Compiling tensorflow/core/ops/nn_grad.cc:\r\nIn file included from tensorflow/core/ops/nn_grad.cc:16:0:\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_0' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/nn_grad.cc:50:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Softmax\", SoftmaxGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_1' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/nn_grad.cc:68:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Relu\", ReluGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_2' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/nn_grad.cc:86:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Relu6\", Relu6Grad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_3' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/nn_grad.cc:115:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"CrossEntropy\", CrossEntropyGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_4' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/nn_grad.cc:151:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"Conv2D\", Conv2DGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_5' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/nn_grad.cc:182:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"MaxPool\", MaxPoolGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_6' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/nn_grad.cc:208:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"AvgPool\", AvgPoolGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_7' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/nn_grad.cc:239:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"MaxPoolGrad\", MaxPoolGradGrad);\r\n ^\r\n./tensorflow/core/framework/function.h:496:15: warning: 'tensorflow::unused_grad_8' defined but not used [-Wunused-variable]\r\n   static bool unused_grad_##ctr = SHOULD_REGISTER_OP_GRADIENT && \\\r\n               ^\r\n./tensorflow/core/framework/function.h:493:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ'\r\n   REGISTER_OP_GRADIENT_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/core/framework/function.h:487:3: note: in expansion of macro 'REGISTER_OP_GRADIENT_UNIQ_HELPER'\r\n   REGISTER_OP_GRADIENT_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/core/ops/nn_grad.cc:260:1: note: in expansion of macro 'REGISTER_OP_GRADIENT'\r\n REGISTER_OP_GRADIENT(\"BiasAdd\", BiasAddGrad);\r\n ^\r\nINFO: From Compiling tensorflow/core/grappler/optimizers/constant_folding.cc:\r\ntensorflow/core/grappler/optimizers/constant_folding.cc: In member function 'tensorflow::Status tensorflow::grappler::ConstantFolding::EvaluateOneFoldable(const tensorflow::NodeDef&, std::vector<tensorflow::NodeDef>*)':\r\ntensorflow/core/grappler/optimizers/constant_folding.cc:233:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < output_tensors.size(); i++) {\r\n                     ^\r\ntensorflow/core/grappler/optimizers/constant_folding.cc: In member function 'tensorflow::Status tensorflow::grappler::ConstantFolding::EvaluateNode(const tensorflow::NodeDef&, const TensorVector&, tensorflow::grappler::TensorVector*)':\r\ntensorflow/core/grappler/optimizers/constant_folding.cc:198:7: warning: 'num_outputs' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n   int num_outputs;\r\n       ^\r\nINFO: From Compiling tensorflow/stream_executor/stream_executor_pimpl.cc:\r\nIn file included from ./tensorflow/core/platform/default/logging.h:24:0,\r\n                 from ./tensorflow/core/platform/logging.h:25,\r\n                 from ./tensorflow/core/lib/core/status.h:24,\r\n                 from ./tensorflow/stream_executor/lib/status.h:21,\r\n                 from ./tensorflow/stream_executor/stream_executor_pimpl.h:25,\r\n                 from tensorflow/stream_executor/stream_executor_pimpl.cc:20:\r\n./tensorflow/core/platform/default/logging.h: In instantiation of 'std::__cxx11::string* tensorflow::internal::Check_EQImpl(const T1&, const T2&, const char*) [with T1 = int; T2 = long long unsigned int; std::__cxx11::string = std::__cxx11::basic_string<char>]':\r\ntensorflow/stream_executor/stream_executor_pimpl.cc:616:3:   required from here\r\n./tensorflow/core/platform/default/logging.h:228:25: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n                         == )  // Compilation error with CHECK_EQ(NULL, x)?\r\n                         ^\r\n./tensorflow/core/platform/macros.h:76:29: note: in definition of macro 'TF_PREDICT_TRUE'\r\n #define TF_PREDICT_TRUE(x) (x)\r\n                             ^\r\n./tensorflow/core/platform/default/logging.h:227:1: note: in expansion of macro 'TF_DEFINE_CHECK_OP_IMPL'\r\n TF_DEFINE_CHECK_OP_IMPL(Check_EQ,\r\n ^\r\nINFO: From Compiling tensorflow/core/debug/debug_io_utils.cc:\r\ntensorflow/core/debug/debug_io_utils.cc:396:15: warning: 'tensorflow::Status tensorflow::CloseDebugURL(const string&)' defined but not used [-Wunused-function]\r\n static Status CloseDebugURL(const string& debug_url) { return Status::OK(); }\r\n               ^\r\nINFO: From Compiling tensorflow/core/kernels/captured_function.cc:\r\nIn file included from ./tensorflow/core/common_runtime/device.h:43:0,\r\n                 from ./tensorflow/core/common_runtime/function.h:22,\r\n                 from ./tensorflow/core/kernels/captured_function.h:21,\r\n                 from tensorflow/core/kernels/captured_function.cc:15:\r\n./tensorflow/core/framework/resource_mgr.h: In static member function 'static tensorflow::Status tensorflow::CapturedFunction::Create(tensorflow::OpKernelContext*, const tensorflow::NameAttrList*, int, std::vector<tensorflow::Tensor>, std::unique_ptr<tensorflow::CapturedFunction>*)':\r\n./tensorflow/core/framework/resource_mgr.h:352:64: warning: 'resource' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n   return DoCreate(container, MakeTypeIndex<T>(), name, resource);\r\n                                                                ^\r\ntensorflow/core/kernels/captured_function.cc:52:19: note: 'resource' was declared here\r\n     ResourceType* resource;                                                    \\\r\n                   ^\r\ntensorflow/core/kernels/captured_function.cc:76:7: note: in expansion of macro 'HANDLE_RESOURCE_TYPE'\r\n       HANDLE_RESOURCE_TYPE(Var);\r\n       ^\r\nIn file included from ./tensorflow/core/common_runtime/device.h:43:0,\r\n                 from ./tensorflow/core/common_runtime/function.h:22,\r\n                 from ./tensorflow/core/kernels/captured_function.h:21,\r\n                 from tensorflow/core/kernels/captured_function.cc:15:\r\n./tensorflow/core/framework/resource_mgr.h:352:64: warning: 'resource' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n   return DoCreate(container, MakeTypeIndex<T>(), name, resource);\r\n                                                                ^\r\ntensorflow/core/kernels/captured_function.cc:52:19: note: 'resource' was declared here\r\n     ResourceType* resource;                                                    \\\r\n                   ^\r\ntensorflow/core/kernels/captured_function.cc:75:7: note: in expansion of macro 'HANDLE_RESOURCE_TYPE'\r\n       HANDLE_RESOURCE_TYPE(QueueInterface);\r\n       ^\r\nIn file included from ./tensorflow/core/common_runtime/device.h:43:0,\r\n                 from ./tensorflow/core/common_runtime/function.h:22,\r\n                 from ./tensorflow/core/kernels/captured_function.h:21,\r\n                 from tensorflow/core/kernels/captured_function.cc:15:\r\n./tensorflow/core/framework/resource_mgr.h:352:64: warning: 'resource' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n   return DoCreate(container, MakeTypeIndex<T>(), name, resource);\r\n                                                                ^\r\ntensorflow/core/kernels/captured_function.cc:52:19: note: 'resource' was declared here\r\n     ResourceType* resource;                                                    \\\r\n                   ^\r\ntensorflow/core/kernels/captured_function.cc:74:7: note: in expansion of macro 'HANDLE_RESOURCE_TYPE'\r\n       HANDLE_RESOURCE_TYPE(lookup::LookupInterface);\r\n       ^\r\nINFO: From Compiling tensorflow/core/kernels/map_dataset_op.cc:\r\nIn file included from ./tensorflow/core/framework/tensor.h:28:0,\r\n                 from ./tensorflow/core/framework/attr_value_util.h:23,\r\n                 from ./tensorflow/core/framework/node_def_util.h:23,\r\n                 from ./tensorflow/core/framework/shape_inference.h:21,\r\n                 from ./tensorflow/core/framework/common_shape_fns.h:20,\r\n                 from ./tensorflow/core/framework/resource_mgr.h:24,\r\n                 from ./tensorflow/core/kernels/dataset.h:20,\r\n                 from tensorflow/core/kernels/map_dataset_op.cc:15:\r\n./tensorflow/core/lib/core/refcount.h: In member function 'virtual void tensorflow::{anonymous}::MapDatasetOp::Compute(tensorflow::OpKernelContext*)':\r\n./tensorflow/core/lib/core/refcount.h:93:12: warning: 'input' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     delete this;\r\n            ^\r\ntensorflow/core/kernels/map_dataset_op.cc:41:18: note: 'input' was declared here\r\n     DatasetBase* input;\r\n                  ^\r\nINFO: From Compiling tensorflow/core/kernels/filter_dataset_op.cc:\r\nIn file included from ./tensorflow/core/framework/tensor.h:28:0,\r\n                 from ./tensorflow/core/framework/attr_value_util.h:23,\r\n                 from ./tensorflow/core/framework/node_def_util.h:23,\r\n                 from ./tensorflow/core/framework/shape_inference.h:21,\r\n                 from ./tensorflow/core/framework/common_shape_fns.h:20,\r\n                 from ./tensorflow/core/framework/resource_mgr.h:24,\r\n                 from ./tensorflow/core/kernels/dataset.h:20,\r\n                 from tensorflow/core/kernels/filter_dataset_op.cc:15:\r\n./tensorflow/core/lib/core/refcount.h: In member function 'virtual void tensorflow::{anonymous}::FilterDatasetOp::Compute(tensorflow::OpKernelContext*)':\r\n./tensorflow/core/lib/core/refcount.h:93:12: warning: 'input' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     delete this;\r\n            ^\r\ntensorflow/core/kernels/filter_dataset_op.cc:39:18: note: 'input' was declared here\r\n     DatasetBase* input;\r\n                  ^\r\nINFO: From Compiling tensorflow/core/kernels/parallel_map_dataset_op.cc:\r\nIn file included from ./tensorflow/core/framework/tensor.h:28:0,\r\n                 from ./tensorflow/core/framework/attr_value_util.h:23,\r\n                 from ./tensorflow/core/framework/node_def_util.h:23,\r\n                 from ./tensorflow/core/framework/shape_inference.h:21,\r\n                 from ./tensorflow/core/framework/common_shape_fns.h:20,\r\n                 from ./tensorflow/core/framework/resource_mgr.h:24,\r\n                 from ./tensorflow/core/kernels/dataset.h:20,\r\n                 from tensorflow/core/kernels/parallel_map_dataset_op.cc:17:\r\n./tensorflow/core/lib/core/refcount.h: In member function 'virtual void tensorflow::{anonymous}::ParallelMapDatasetOp::Compute(tensorflow::OpKernelContext*)':\r\n./tensorflow/core/lib/core/refcount.h:93:12: warning: 'input' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     delete this;\r\n            ^\r\ntensorflow/core/kernels/parallel_map_dataset_op.cc:42:18: note: 'input' was declared here\r\n     DatasetBase* input;\r\n                  ^\r\nINFO: From Compiling tensorflow/core/kernels/remote_fused_graph_execute_op.cc:\r\nIn file included from ./tensorflow/core/platform/default/logging.h:24:0,\r\n                 from ./tensorflow/core/platform/logging.h:25,\r\n                 from ./tensorflow/core/framework/allocator.h:26,\r\n                 from ./tensorflow/core/framework/op_kernel.h:23,\r\n                 from tensorflow/core/kernels/remote_fused_graph_execute_op.cc:18:\r\ntensorflow/core/kernels/remote_fused_graph_execute_op.cc: In member function 'virtual void tensorflow::RemoteFusedGraphExecuteOp::Compute(tensorflow::OpKernelContext*)':\r\ntensorflow/core/kernels/remote_fused_graph_execute_op.cc:77:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n           input_count == input_types_.size())\r\n                       ^\r\n./tensorflow/core/platform/macros.h:75:30: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (x)\r\n                              ^\r\ntensorflow/core/kernels/remote_fused_graph_execute_op.cc:76:5: note: in expansion of macro 'CHECK'\r\n     CHECK(input_count == graph_input_count &&\r\n     ^\r\ntensorflow/core/kernels/remote_fused_graph_execute_op.cc:100:24: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n           output_count == output_types_.size());\r\n                        ^\r\n./tensorflow/core/platform/macros.h:75:30: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (x)\r\n                              ^\r\ntensorflow/core/kernels/remote_fused_graph_execute_op.cc:99:5: note: in expansion of macro 'CHECK'\r\n     CHECK(output_count == execute_info_.graph_output_node_name_size() &&\r\n     ^\r\nINFO: From Compiling tensorflow/core/kernels/remote_fused_graph_execute_utils.cc:\r\nIn file included from ./tensorflow/core/platform/default/logging.h:24:0,\r\n                 from ./tensorflow/core/platform/logging.h:25,\r\n                 from ./tensorflow/core/lib/core/status.h:24,\r\n                 from ./tensorflow/core/lib/core/errors.h:19,\r\n                 from ./tensorflow/core/framework/tensor_shape.h:24,\r\n                 from ./tensorflow/core/framework/partial_tensor_shape.h:22,\r\n                 from ./tensorflow/core/framework/attr_value_util.h:22,\r\n                 from ./tensorflow/core/framework/function.h:20,\r\n                 from ./tensorflow/core/graph/graph.h:43,\r\n                 from ./tensorflow/core/kernels/remote_fused_graph_execute_utils.h:24,\r\n                 from tensorflow/core/kernels/remote_fused_graph_execute_utils.cc:16:\r\ntensorflow/core/kernels/remote_fused_graph_execute_utils.cc: In static member function 'static bool tensorflow::RemoteFusedGraphExecuteUtils::GetOutputTensorShapeType(const tensorflow::GraphDef&, const string&, tensorflow::DataType*, tensorflow::TensorShape*)':\r\ntensorflow/core/kernels/remote_fused_graph_execute_utils.cc:369:27: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(data_types.size() > port);\r\n                           ^\r\n./tensorflow/core/platform/macros.h:75:30: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (x)\r\n                              ^\r\ntensorflow/core/kernels/remote_fused_graph_execute_utils.cc:369:3: note: in expansion of macro 'CHECK'\r\n   CHECK(data_types.size() > port);\r\n   ^\r\ntensorflow/core/kernels/remote_fused_graph_execute_utils.cc: In static member function 'static tensorflow::Status tensorflow::RemoteFusedGraphExecuteUtils::ClusterizeNodes(const std::unordered_set<std::__cxx11::basic_string<char> >&, const tensorflow::GraphDef&, std::vector<std::tuple<std::unordered_set<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > >*)':\r\ntensorflow/core/kernels/remote_fused_graph_execute_utils.cc:728:45: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n       CHECK(input_count == 0 || input_count == node->in_edges().size());\r\n                                             ^\r\n./tensorflow/core/platform/macros.h:75:30: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (x)\r\n                              ^\r\ntensorflow/core/kernels/remote_fused_graph_execute_utils.cc:728:7: note: in expansion of macro 'CHECK'\r\n       CHECK(input_count == 0 || input_count == node->in_edges().size());\r\n       ^\r\ntensorflow/core/kernels/remote_fused_graph_execute_utils.cc: In static member function 'static tensorflow::Status tensorflow::RemoteFusedGraphExecuteUtils::FuseCluster(const tensorflow::GraphDef&, const std::vector<std::__cxx11::basic_string<char> >&, const std::vector<std::__cxx11::basic_string<char> >&, const string&, const ClusterInfo&, const string&, bool, tensorflow::GraphDef*)':\r\ntensorflow/core/kernels/remote_fused_graph_execute_utils.cc:894:25: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n       for (int j = 0; j < border_outputs.size(); ++j) {\r\n                         ^\r\ntensorflow/core/kernels/remote_fused_graph_execute_utils.cc:918:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     for (int i = 0; i < border_outputs.size(); ++i) {\r\n                       ^\r\ntensorflow/core/kernels/remote_fused_graph_execute_utils.cc: In static member function 'static tensorflow::Status tensorflow::RemoteFusedGraphExecuteUtils::FuseRemoteGraphByNodeNames(const tensorflow::GraphDef&, const std::vector<std::__cxx11::basic_string<char> >&, const std::vector<std::__cxx11::basic_string<char> >&, const string&, const std::unordered_set<std::__cxx11::basic_string<char> >&, const string&, bool, tensorflow::GraphDef*)':\r\ntensorflow/core/kernels/remote_fused_graph_execute_utils.cc:965:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < ci_vec.size(); ++i) {\r\n                     ^\r\nINFO: From Compiling tensorflow/cc/gradients/math_grad.cc:\r\nIn file included from bazel-out/local-py3-opt/genfiles/tensorflow/cc/ops/array_ops.h:8:0,\r\n                 from ./tensorflow/cc/ops/standard_ops.h:19,\r\n                 from tensorflow/cc/gradients/math_grad.cc:16:\r\n./tensorflow/cc/framework/ops.h: In instantiation of 'tensorflow::Input::Initializer::Initializer(const std::initializer_list<_Tp>&, const tensorflow::TensorShape&) [with T = int; <template-parameter-1-2> = void]':\r\nbazel-out/local-py3-opt/genfiles/tensorflow/cc/ops/logging_ops.h:217:70:   required from here\r\n./tensorflow/cc/framework/ops.h:153:27: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n       if (t.NumElements() != v.size()) {\r\n                           ^\r\nIn file included from tensorflow/cc/gradients/math_grad.cc:18:0:\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_0' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/math_grad.cc:44:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Abs\", AbsGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_1' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/math_grad.cc:53:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Neg\", NegGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_2' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/math_grad.cc:65:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Inv\", InvGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_3' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/math_grad.cc:66:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Reciprocal\", InvGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_4' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/math_grad.cc:79:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Square\", SquareGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_5' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/math_grad.cc:94:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Sqrt\", SqrtGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_6' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/math_grad.cc:111:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Rsqrt\", RsqrtGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_7' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/math_grad.cc:123:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Exp\", ExpGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_8' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/math_grad.cc:136:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Expm1\", Expm1Grad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_9' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/math_grad.cc:149:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Log\", LogGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_10' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/math_grad.cc:163:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Log1p\", Log1pGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_11' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/math_grad.cc:178:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Tanh\", TanhGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_12' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/math_grad.cc:194:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Sigmoid\", SigmoidGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_13' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/math_grad.cc:205:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Sign\", SignGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_14' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/math_grad.cc:218:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Sin\", SinGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_15' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/math_grad.cc:231:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Cos\", CosGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_16' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/math_grad.cc:246:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Asin\", AsinGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_17' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/math_grad.cc:261:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Acos\", AcosGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_18' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/math_grad.cc:274:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Tan\", TanGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_19' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/math_grad.cc:288:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Atan\", AtanGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_20' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/math_grad.cc:298:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Real\", RealGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_21' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/math_grad.cc:308:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Imag\", ImagGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_22' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/math_grad.cc:316:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Conj\", ConjGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_23' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/math_grad.cc:388:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"MatMul\", MatMulGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_24' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/math_grad.cc:396:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"BatchMatMul\", BatchMatMulGrad);\r\n ^\r\nINFO: From Compiling tensorflow/cc/gradients/nn_grad.cc:\r\nIn file included from bazel-out/local-py3-opt/genfiles/tensorflow/cc/ops/nn_ops.h:8:0,\r\n                 from tensorflow/cc/gradients/nn_grad.cc:16:\r\n./tensorflow/cc/framework/ops.h: In instantiation of 'tensorflow::Input::Initializer::Initializer(const std::initializer_list<_Tp>&, const tensorflow::TensorShape&) [with T = int; <template-parameter-1-2> = void]':\r\nbazel-out/local-py3-opt/genfiles/tensorflow/cc/ops/logging_ops.h:217:70:   required from here\r\n./tensorflow/cc/framework/ops.h:153:27: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n       if (t.NumElements() != v.size()) {\r\n                           ^\r\nIn file included from tensorflow/cc/gradients/nn_grad.cc:20:0:\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_0' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/nn_grad.cc:47:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Softmax\", SoftmaxGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_1' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/nn_grad.cc:56:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Relu\", ReluGradHelper);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_2' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/nn_grad.cc:65:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Relu6\", Relu6GradHelper);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_3' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/nn_grad.cc:74:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Elu\", EluGradHelper);\r\n ^\r\nINFO: From Compiling tensorflow/cc/framework/gradients.cc:\r\ntensorflow/cc/framework/gradients.cc: In member function 'tensorflow::Status tensorflow::{anonymous}::SymbolicGradientBuilder::Initialize()':\r\ntensorflow/cc/framework/gradients.cc:155:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < outputs_.size(); ++i) {\r\n                     ^\r\ntensorflow/cc/framework/gradients.cc:160:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < inputs_.size(); ++i) {\r\n                     ^\r\ntensorflow/cc/framework/gradients.cc: In member function 'tensorflow::Status tensorflow::{anonymous}::SymbolicGradientBuilder::AddGradients()':\r\ntensorflow/cc/framework/gradients.cc:312:66: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     if (IsPrimitiveOpWithNoGrad(n->type_string()) || num_no_grad == num_y) {\r\n                                                                  ^\r\ntensorflow/cc/framework/gradients.cc:323:40: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     if (num_no_grad > 0 && num_no_grad < num_y) {\r\n                                        ^\r\ntensorflow/cc/framework/gradients.cc:347:20: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n       if (dx_index == dx.size()) {\r\n                    ^\r\nIn file included from ./tensorflow/cc/framework/gradients.h:19:0,\r\n                 from tensorflow/cc/framework/gradients.cc:19:\r\n./tensorflow/cc/framework/ops.h: In instantiation of 'tensorflow::Input::Initializer::Initializer(const std::initializer_list<_Tp>&, const tensorflow::TensorShape&) [with T = int; <template-parameter-1-2> = void]':\r\nbazel-out/local-py3-opt/genfiles/tensorflow/cc/ops/logging_ops.h:217:70:   required from here\r\n./tensorflow/cc/framework/ops.h:153:27: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n       if (t.NumElements() != v.size()) {\r\n                           ^\r\nINFO: From Compiling tensorflow/cc/gradients/array_grad.cc:\r\nIn file included from bazel-out/local-py3-opt/genfiles/tensorflow/cc/ops/array_ops_internal.h:8:0,\r\n                 from tensorflow/cc/gradients/array_grad.cc:18:\r\n./tensorflow/cc/framework/ops.h: In instantiation of 'tensorflow::Input::Initializer::Initializer(const std::initializer_list<_Tp>&, const tensorflow::TensorShape&) [with T = int; <template-parameter-1-2> = void]':\r\nbazel-out/local-py3-opt/genfiles/tensorflow/cc/ops/logging_ops.h:217:70:   required from here\r\n./tensorflow/cc/framework/ops.h:153:27: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n       if (t.NumElements() != v.size()) {\r\n                           ^\r\nIn file included from tensorflow/cc/gradients/array_grad.cc:22:0:\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_0' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:64:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:29:1: note: in expansion of macro 'REGISTER_NO_GRADIENT_OP'\r\n REGISTER_NO_GRADIENT_OP(\"Const\");\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_1' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:64:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:30:1: note: in expansion of macro 'REGISTER_NO_GRADIENT_OP'\r\n REGISTER_NO_GRADIENT_OP(\"StopGradient\");\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_2' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:64:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:31:1: note: in expansion of macro 'REGISTER_NO_GRADIENT_OP'\r\n REGISTER_NO_GRADIENT_OP(\"ConcatOffset\");\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_3' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:64:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:32:1: note: in expansion of macro 'REGISTER_NO_GRADIENT_OP'\r\n REGISTER_NO_GRADIENT_OP(\"EditDistance\");\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_4' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:64:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:33:1: note: in expansion of macro 'REGISTER_NO_GRADIENT_OP'\r\n REGISTER_NO_GRADIENT_OP(\"ZerosLike\");\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_5' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:64:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:34:1: note: in expansion of macro 'REGISTER_NO_GRADIENT_OP'\r\n REGISTER_NO_GRADIENT_OP(\"InvertPermutation\");\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_6' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:64:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:35:1: note: in expansion of macro 'REGISTER_NO_GRADIENT_OP'\r\n REGISTER_NO_GRADIENT_OP(\"Shape\");\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_7' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:64:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:36:1: note: in expansion of macro 'REGISTER_NO_GRADIENT_OP'\r\n REGISTER_NO_GRADIENT_OP(\"ShapeN\");\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_8' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:64:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:37:1: note: in expansion of macro 'REGISTER_NO_GRADIENT_OP'\r\n REGISTER_NO_GRADIENT_OP(\"Rank\");\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_9' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:64:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:38:1: note: in expansion of macro 'REGISTER_NO_GRADIENT_OP'\r\n REGISTER_NO_GRADIENT_OP(\"Size\");\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_10' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:64:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:39:1: note: in expansion of macro 'REGISTER_NO_GRADIENT_OP'\r\n REGISTER_NO_GRADIENT_OP(\"BroadcastGradientArgs\");\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_11' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:64:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, nullptr)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:40:1: note: in expansion of macro 'REGISTER_NO_GRADIENT_OP'\r\n REGISTER_NO_GRADIENT_OP(\"OneHot\");\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_12' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:57:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Pack\", PackGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_13' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:67:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Unpack\", UnpackGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_14' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:75:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Identity\", IdentityGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_15' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:83:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"RefIdentity\", RefIdentityGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_16' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:91:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"QuantizeAndDequantize\", QuantizeAndDequantizeGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_17' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:101:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"QuantizeAndDequantizeV2\", QuantizeAndDequantizeV2Grad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_18' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:110:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Split\", SplitGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_19' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:118:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Diag\", DiagGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_20' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:126:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"DiagPart\", DiagPartGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_21' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:134:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"MatrixDiag\", MatrixDiagGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_22' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:147:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"MatrixBandPart\", MatrixBandPartGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_23' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:159:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"GatherNd\", GatherNdGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_24' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:172:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"CheckNumerics\", CheckNumericsGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_25' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:182:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Reshape\", ReshapeGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_26' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:192:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"ExpandDims\", ExpandDimsGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_27' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:201:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Squeeze\", SqueezeGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_28' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:211:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Transpose\", TransposeGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_29' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:227:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"ReverseSequence\", ReverseSequenceGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_30' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:237:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"ReverseV2\", ReverseGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_31' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:248:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"ScatterNd\", ScatterNdGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_32' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:264:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"Pad\", PadGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_33' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:276:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"SpaceToBatch\", SpaceToBatchGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_34' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:287:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"SpaceToBatchND\", SpaceToBatchNDGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_35' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:299:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"BatchToSpace\", BatchToSpaceGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_36' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:310:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"BatchToSpaceND\", BatchToSpaceNDGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_37' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:320:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"SpaceToDepth\", SpaceToDepthGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_38' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:330:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"DepthToSpace\", DepthToSpaceGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_39' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:342:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"MirrorPad\", MirrorPadGrad);\r\n ^\r\n./tensorflow/cc/framework/grad_op_registry.h:70:15: warning: 'tensorflow::ops::{anonymous}::unused_ret_val_40' defined but not used [-Wunused-variable]\r\n   static bool unused_ret_val_##ctr =             \\\r\n               ^\r\n./tensorflow/cc/framework/grad_op_registry.h:67:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ'\r\n   REGISTER_GRADIENT_OP_UNIQ(ctr, name, fn)\r\n   ^\r\n./tensorflow/cc/framework/grad_op_registry.h:61:3: note: in expansion of macro 'REGISTER_GRADIENT_OP_UNIQ_HELPER'\r\n   REGISTER_GRADIENT_OP_UNIQ_HELPER(__COUNTER__, name, fn)\r\n   ^\r\ntensorflow/cc/gradients/array_grad.cc:354:1: note: in expansion of macro 'REGISTER_GRADIENT_OP'\r\n REGISTER_GRADIENT_OP(\"MirrorPadGrad\", MirrorPadGradGrad);\r\n ^\r\nINFO: From Compiling tensorflow/python/lib/core/py_func.cc:\r\nIn file included from ./tensorflow/core/platform/default/logging.h:24:0,\r\n                 from ./tensorflow/core/platform/logging.h:25,\r\n                 from ./tensorflow/core/framework/allocator.h:26,\r\n                 from ./tensorflow/core/framework/tensor.h:21,\r\n                 from ./tensorflow/python/lib/core/py_func.h:22,\r\n                 from tensorflow/python/lib/core/py_func.cc:16:\r\n./tensorflow/core/platform/default/logging.h: In instantiation of 'std::__cxx11::string* tensorflow::internal::Check_EQImpl(const T1&, const T2&, const char*) [with T1 = long unsigned int; T2 = long long int; std::__cxx11::string = std::__cxx11::basic_string<char>]':\r\ntensorflow/python/lib/core/py_func.cc:265:5:   required from here\r\n./tensorflow/core/platform/default/logging.h:228:25: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n                         == )  // Compilation error with CHECK_EQ(NULL, x)?\r\n                         ^\r\n./tensorflow/core/platform/macros.h:76:29: note: in definition of macro 'TF_PREDICT_TRUE'\r\n #define TF_PREDICT_TRUE(x) (x)\r\n                             ^\r\n./tensorflow/core/platform/default/logging.h:227:1: note: in expansion of macro 'TF_DEFINE_CHECK_OP_IMPL'\r\n TF_DEFINE_CHECK_OP_IMPL(Check_EQ,\r\n ^\r\nINFO: From Compiling tensorflow/tools/graph_transforms/quantize_nodes.cc:\r\ntensorflow/tools/graph_transforms/quantize_nodes.cc:172:6: warning: 'bool tensorflow::graph_transforms::{anonymous}::AreAttrsEqual(const tensorflow::NodeDef*, const tensorflow::NodeDef*)' defined but not used [-Wunused-function]\r\n bool AreAttrsEqual(const NodeDef* current_node, const NodeDef* other_node) {\r\n      ^\r\nINFO: From Compiling tensorflow/python/pywrap_tensorflow_internal.cc:\r\nbazel-out/local-py3-opt/bin/tensorflow/python/pywrap_tensorflow_internal.cc: In function 'PyObject* _wrap_PyRecordReader_New(PyObject*, PyObject*)':\r\nbazel-out/local-py3-opt/bin/tensorflow/python/pywrap_tensorflow_internal.cc:4938:138: warning: 'arg2' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     result = (tensorflow::io::PyRecordReader *)tensorflow::io::PyRecordReader::New((string const &)*arg1,arg2,(string const &)*arg3,arg4);\r\n                                                                                                                                          ^\r\nAt global scope:\r\ncc1plus: warning: unrecognized command line option '-Wno-self-assign'\r\nINFO: From Compiling tensorflow/contrib/batching/kernels/batch_kernels.cc:\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc: In member function 'tensorflow::Status tensorflow::BatchKernel::ValidateAllowedBatchSizes() const':\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:550:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     for (int i = 0; i < allowed_batch_sizes_.size(); ++i) {\r\n                       ^\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:556:13: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n       if (i == allowed_batch_sizes_.size() - 1 && size != max_batch_size_) {\r\n             ^\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc: In lambda function:\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:677:27: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n         for (int i = 0; i < batch_keys.size(); ++i) {\r\n                           ^\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc: In instantiation of 'tensorflow::Status tensorflow::Concat(tensorflow::OpKernelContext*, const tensorflow::gtl::ArraySlice<tensorflow::Tensor>&, int) [with T = long long int]':\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:395:9:   required from here\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:54:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < inputs.size(); ++i) {\r\n                     ^\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc: In instantiation of 'tensorflow::Status tensorflow::Concat(tensorflow::OpKernelContext*, const tensorflow::gtl::ArraySlice<tensorflow::Tensor>&, int) [with T = int]':\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:395:9:   required from here\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:54:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc: In instantiation of 'tensorflow::Status tensorflow::Concat(tensorflow::OpKernelContext*, const tensorflow::gtl::ArraySlice<tensorflow::Tensor>&, int) [with T = short unsigned int]':\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:395:9:   required from here\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:54:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc: In instantiation of 'tensorflow::Status tensorflow::Concat(tensorflow::OpKernelContext*, const tensorflow::gtl::ArraySlice<tensorflow::Tensor>&, int) [with T = short int]':\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:395:9:   required from here\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:54:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc: In instantiation of 'tensorflow::Status tensorflow::Concat(tensorflow::OpKernelContext*, const tensorflow::gtl::ArraySlice<tensorflow::Tensor>&, int) [with T = unsigned char]':\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:395:9:   required from here\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:54:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc: In instantiation of 'tensorflow::Status tensorflow::Concat(tensorflow::OpKernelContext*, const tensorflow::gtl::ArraySlice<tensorflow::Tensor>&, int) [with T = signed char]':\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:395:9:   required from here\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:54:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc: In instantiation of 'tensorflow::Status tensorflow::Concat(tensorflow::OpKernelContext*, const tensorflow::gtl::ArraySlice<tensorflow::Tensor>&, int) [with T = Eigen::half]':\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:395:9:   required from here\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:54:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc: In instantiation of 'tensorflow::Status tensorflow::Concat(tensorflow::OpKernelContext*, const tensorflow::gtl::ArraySlice<tensorflow::Tensor>&, int) [with T = float]':\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:395:9:   required from here\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:54:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc: In instantiation of 'tensorflow::Status tensorflow::Concat(tensorflow::OpKernelContext*, const tensorflow::gtl::ArraySlice<tensorflow::Tensor>&, int) [with T = double]':\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:395:9:   required from here\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:54:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc: In instantiation of 'tensorflow::Status tensorflow::Concat(tensorflow::OpKernelContext*, const tensorflow::gtl::ArraySlice<tensorflow::Tensor>&, int) [with T = std::complex<float>]':\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:395:9:   required from here\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:54:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc: In instantiation of 'tensorflow::Status tensorflow::Concat(tensorflow::OpKernelContext*, const tensorflow::gtl::ArraySlice<tensorflow::Tensor>&, int) [with T = std::complex<double>]':\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:395:9:   required from here\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:54:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc: In instantiation of 'tensorflow::Status tensorflow::Concat(tensorflow::OpKernelContext*, const tensorflow::gtl::ArraySlice<tensorflow::Tensor>&, int) [with T = bool]':\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:395:9:   required from here\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:54:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc: In instantiation of 'tensorflow::Status tensorflow::Concat(tensorflow::OpKernelContext*, const tensorflow::gtl::ArraySlice<tensorflow::Tensor>&, int) [with T = std::__cxx11::basic_string<char>]':\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:395:9:   required from here\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:54:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc: In instantiation of 'tensorflow::Status tensorflow::Concat(tensorflow::OpKernelContext*, const tensorflow::gtl::ArraySlice<tensorflow::Tensor>&, int) [with T = tensorflow::ResourceHandle]':\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:395:9:   required from here\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:54:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\nIn file included from tensorflow/contrib/batching/kernels/batch_kernels.cc:16:0:\r\n./tensorflow/contrib/batching/shared_batch_scheduler.h: In instantiation of 'bool tensorflow::serving::internal::Queue<TaskType>::IsOpenBatchSchedulable() const [with TaskType = tensorflow::BatchResource::BatchTask]':\r\n./tensorflow/contrib/batching/shared_batch_scheduler.h:594:55:   required from 'std::unique_ptr<tensorflow::serving::Batch<TaskType> > tensorflow::serving::internal::Queue<TaskType>::ScheduleBatch() [with TaskType = tensorflow::BatchResource::BatchTask]'\r\n./tensorflow/contrib/batching/shared_batch_scheduler.h:465:24:   required from 'void tensorflow::serving::SharedBatchScheduler<TaskType>::ThreadLogic() [with TaskType = tensorflow::BatchResource::BatchTask]'\r\n./tensorflow/contrib/batching/shared_batch_scheduler.h:437:18:   required from 'tensorflow::serving::SharedBatchScheduler<TaskType>::SharedBatchScheduler(const tensorflow::serving::SharedBatchScheduler<TaskType>::Options&)::<lambda()> [with TaskType = tensorflow::BatchResource::BatchTask]'\r\n./tensorflow/contrib/batching/shared_batch_scheduler.h:437:10:   required from 'struct tensorflow::serving::SharedBatchScheduler<TaskType>::SharedBatchScheduler(const tensorflow::serving::SharedBatchScheduler<TaskType>::Options&) [with TaskType = tensorflow::BatchResource::BatchTask]::<lambda()>'\r\n./tensorflow/contrib/batching/shared_batch_scheduler.h:438:72:   required from 'tensorflow::serving::SharedBatchScheduler<TaskType>::SharedBatchScheduler(const tensorflow::serving::SharedBatchScheduler<TaskType>::Options&) [with TaskType = tensorflow::BatchResource::BatchTask]'\r\n./tensorflow/contrib/batching/shared_batch_scheduler.h:362:3:   required from 'static tensorflow::Status tensorflow::serving::SharedBatchScheduler<TaskType>::Create(const tensorflow::serving::SharedBatchScheduler<TaskType>::Options&, std::shared_ptr<tensorflow::serving::SharedBatchScheduler<TaskType> >*) [with TaskType = tensorflow::BatchResource::BatchTask]'\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:217:5:   required from here\r\n./tensorflow/contrib/batching/shared_batch_scheduler.h:664:40: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   return closed_ || open_batch->size() >= options_.max_batch_size ||\r\n                                        ^\r\n./tensorflow/contrib/batching/shared_batch_scheduler.h: In instantiation of 'tensorflow::Status tensorflow::serving::internal::Queue<TaskType>::Schedule(std::unique_ptr<TaskType>*) [with TaskType = tensorflow::BatchResource::BatchTask]':\r\n./tensorflow/contrib/batching/shared_batch_scheduler.h:682:31:   required from 'tensorflow::Status tensorflow::serving::internal::QueueHandle<TaskType>::Schedule(std::unique_ptr<TaskType>*) [with TaskType = tensorflow::BatchResource::BatchTask]'\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:996:1:   required from here\r\n./tensorflow/contrib/batching/shared_batch_scheduler.h:523:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if ((*task)->size() > options_.max_batch_size) {\r\n                       ^\r\n./tensorflow/contrib/batching/shared_batch_scheduler.h:535:51: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     if (batches_.back()->size() + (*task)->size() > options_.max_batch_size) {\r\n                                                   ^\r\n./tensorflow/contrib/batching/shared_batch_scheduler.h:536:27: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n       if (batches_.size() >= options_.max_enqueued_batches) {\r\n                           ^\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc: In member function 'virtual void tensorflow::BatchKernel::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback)':\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:261:53: warning: 'batcher_queue' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     return batcher_queue->Schedule(&batch_components);\r\n                                                     ^\r\ntensorflow/contrib/batching/kernels/batch_kernels.cc:258:19: note: 'batcher_queue' was declared here\r\n     BatcherQueue* batcher_queue;\r\n                   ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\nINFO: Elapsed time: 2527.226s, Critical Path: 72.65s\r\nlx@ubuntu:~/Downloads/tensorflow$ pip install /tmp/tensorflow_pkg/tensorflow-1.2.1-cp36-cp36m-linux_x86_64.whl \r\nProcessing /tmp/tensorflow_pkg/tensorflow-1.2.1-cp36-cp36m-linux_x86_64.whl\r\nRequirement already satisfied: protobuf>=3.2.0 in /home/lx/anaconda3/lib/python3.6/site-packages (from tensorflow==1.2.1)\r\nRequirement already satisfied: six>=1.10.0 in /home/lx/anaconda3/lib/python3.6/site-packages (from tensorflow==1.2.1)\r\nRequirement already satisfied: backports.weakref==1.0rc1 in /home/lx/anaconda3/lib/python3.6/site-packages (from tensorflow==1.2.1)\r\nRequirement already satisfied: markdown>=2.6.8 in /home/lx/anaconda3/lib/python3.6/site-packages (from tensorflow==1.2.1)\r\nRequirement already satisfied: bleach==1.5.0 in /home/lx/anaconda3/lib/python3.6/site-packages (from tensorflow==1.2.1)\r\nRequirement already satisfied: werkzeug>=0.11.10 in /home/lx/anaconda3/lib/python3.6/site-packages (from tensorflow==1.2.1)\r\nRequirement already satisfied: numpy>=1.11.0 in /home/lx/anaconda3/lib/python3.6/site-packages (from tensorflow==1.2.1)\r\nRequirement already satisfied: html5lib==0.9999999 in /home/lx/anaconda3/lib/python3.6/site-packages (from tensorflow==1.2.1)\r\nRequirement already satisfied: wheel>=0.26 in /home/lx/anaconda3/lib/python3.6/site-packages (from tensorflow==1.2.1)\r\nRequirement already satisfied: setuptools in /home/lx/anaconda3/lib/python3.6/site-packages/setuptools-27.2.0-py3.6.egg (from protobuf>=3.2.0->tensorflow==1.2.1)\r\nInstalling collected packages: tensorflow\r\nSuccessfully installed tensorflow-1.2.1\r\nlx@ubuntu:~/Downloads/tensorflow$ python\r\nPython 3.6.1 |Anaconda 4.4.0 (64-bit)| (default, May 11 2017, 13:09:58) \r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"/home/lx/Downloads/tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\nModuleNotFoundError: No module named 'tensorflow.python.pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/lx/Downloads/tensorflow/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/lx/Downloads/tensorflow/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/lx/Downloads/tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/lx/Downloads/tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\nModuleNotFoundError: No module named 'tensorflow.python.pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>> \r\n`\r\ni.e. I even cannot get it installed sucessfully\r\nP.S. I set all the options as default in ./configure", "Could you please fill in the template? I.e. what compiler are you using? What version of Linux? It seems like you are using an old distribution or old compiler if you are getting so many warning. You could start with some conservative options rather than all possibilities i.e. only -msse2 instead of all the options. My guess is you are using RHEL6 which we don't officially support. You can google \"RHEL6 compiling tensorflow\" in github to find out how to solve that.", "> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n> File \"\", line 1, in\r\n> File \"/home/lx/Downloads/tensorflow/tensorflow/init.py\", line 24, in\r\n> from tensorflow.python import *\r\n> File \"/home/lx/Downloads/tensorflow/tensorflow/python/init.py\", line 49, in\r\n> from tensorflow.python import pywrap_tensorflow\r\n> File \"/home/lx/Downloads/tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 52, in\r\n> raise ImportError(msg)\r\n> ImportError: Traceback (most recent call last):\r\n> File \"/home/lx/Downloads/tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 41, in\r\n> from tensorflow.python.pywrap_tensorflow_internal import *\r\n> ModuleNotFoundError: No module named 'tensorflow.python.pywrap_tensorflow_internal'\r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n\r\n@LuminousXLB It seems that you are in the tensorflow repository when importing tensorflow in python. You need to first change your directory to something other than the TF repo so that python does not try to import from the repo itself.", "I add `-march=native` when run `./configure`, I also add `--copt=-march=native` when build using bazel. It works for me.", "Looks like this was resolved?\r\nPlease reopen if you are still seeing a problem."]}, {"number": 11199, "title": "AttributeError: 'module' object has no attribute 'prepare_attention'", "body": "**Tensorflow Version**:\r\n```\r\n('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')\r\n```\r\n**Issue**:\r\n```\r\n$ python\r\nPython 2.7.10 (default, Feb  7 2017, 00:08:15) \r\n[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.34)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.contrib.seq2seq.prepare_attention()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: 'module' object has no attribute 'prepare_attention'\r\n```\r\nI'm not sure what why the tf.contrib.seq2seq cannot find [`prepare_attention`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/seq2seq/prepare_attention), any clue to why that is would be appreciated.", "comments": ["Hi,\r\n\r\n1. It won't compile as you have `tensor-flow API 1.1.0` which doesn't have `tf.contrib.seq2seq.prepare_attention()` and the link which you are referring is for `API 1.0`.\r\n\r\n2. For your existing version `1.1.0` the ` tf.contrib.seq2seq.DynamicAttentionWrapper` will work. Refer link [https://www.tensorflow.org/versions/r1.1/api_docs/python/tf/contrib/seq2seq/DynamicAttentionWrapper](https://www.tensorflow.org/versions/r1.1/api_docs/python/tf/contrib/seq2seq/DynamicAttentionWrapper)\r\n\r\n3. If you want to use your existing code than you have to upgrade your version of tensor flow by executing i.e. `pip3 install --upgrade tensorflow`\r\n \r\n4. Please refer suggestions and guidelines listed here [link](https://github.com/tensorflow/tensorflow/blob/master/ISSUE_TEMPLATE.md) before raising an issue.", "Regarding your item (3), I suppose you mean downgrade instead of upgrade? In the API documentation for `1.2.x` it doesn't appear `prepare_attention` went beyond `1.0` ... \r\n\r\nI was used to seeing `deprecated` labels next to the old functions and was completely oblivious to the fact I was looking at `1.0` API documentation. Apparently the tensorflow technical writers aren't bothering with that anymore perhaps ...\r\n\r\nAnyway, thank you!", "Hi,\r\n     Google hasn't hired me unfortunately.Yes, you are right that for item(3) you have to use the latest API , not your existing one.", "@printdhruv thanks for your help on this one!"]}, {"number": 11198, "title": "Enable bitcode compilation", "body": "In order to enable bitcode compilation it is necessary more than adding `-fembed-bitcode`. The `MACOSX_DEPLOYMENT_TARGET` variable must be defined and exported as well.\r\n\r\nIn this commit the variable is defined grabbing the version of the underlying running macOS.", "comments": ["Can one of the admins verify this patch?", "In hindsight `sw_vers -productVersion` seems obvious. Thanks for the suggestion @drpngx . I have updated the PR with the new code.", "Jenkins, test this please.", "I updated the commit with a small modification. The script first checks whether `MACOSX_DEPLOYMENT_TARGET` has been defined and exported, if not, then defines and exports it.\r\n\r\nThis way someone would have the option to populate it with a custom value prior to invoking the script.", "Jenkins, test this please."]}, {"number": 11197, "title": "Support placeholder for parameter k in tf.nn.in_top_k", "body": "This fix tries to address the issue raised in #9717 where it was not possible to have tensor for k in nn.in_top_k.\r\n\r\nThis fix adds the implementation of InTopKV2Op, adds addition test cases, and following similiar workflow in #10840:\r\n1. Register new kennel InTopKV2Op\r\n2. Hide InTopK and InTopKV2 in python (tensorflow/python/ops/hidden_ops.txt)\r\n3. Add a wrapper in_top_k (in tensorflow/python/ops/nn_ops.py) pointing to gen_nn_ops._in_top_k\r\n\r\nAnother PR will be created after 3 weeks once this PR is merged:\r\n1. Change the implementation of the wrapper in_top_k (in tensorflow/python/ops/nn_ops.py)\r\npointing to gen_nn_ops._in_top_kv2\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@drpngx @girving Thanks for the review. The PR has been updated. Please take a look.", "Jenkins, test this please."]}, {"number": 11196, "title": "[Feature Request] Distributed Tensorflow with Data Parallelism and DMA", "body": "Hi, I have a machine with multiple GPUs and deployed a distributed Tensorflow exploiting data parallelism. I use ClusterConfig to configure the cluster's topology and pass this to Experiment to run the distributed training. I wanted to make one of the GPUs as the parameter server and use the rest for the workers, each of which uses one GPU. I did this through launching multiple processes for parameter server and workers while setting up the env variable (CUDA_VISIBLE_DEVICES) to one of the GPUs. \r\n\r\nIn this case, I noticed that the GPUs don't communicate to each other via DMA and I guess this is because the DMA is only available between visible devices to Tensorflow.  \r\n\r\nI also tried \"device_count={\"GPU\": 1}\" to make Tensorflow see all the GPUs while using only one GPU, but this still seems to occupy the whole resource of all visible GPUs, preventing another Tensorflow process to be launched over the idle GPU. \r\n\r\nIt would be great if there is a way to use only one GPU for a Tensorflow process but still enable DMA with the other GPUs sitting in the system. Am I missing such feature even though it exists? \r\n\r\nThank you!", "comments": ["Currently distributed TensorFlow is not as performant as single-process multi-device setup. If you are not running your job over multiple machines, you could consider using a intra process parameter server (which typically only uses CPU and host RAM as there is not much work to do for PS).\r\n\r\nNevertheless, we are preparing a patch on interprocess device DMA for distributed runtime. But currently our design only consider the use case that cross the physical machine boundary through an RDMA-capable network interface card (rNIC). Could you explain a little bit more about your use case? Why do you want to run distributed TF on a single node? I am assuming it's not for testing as you mentioned DMA and thus only a performance-wise matter.\r\n\r\nIf your use case is valid, we might consider adding a shared-memory based transport. This isn't as efficient as GPU p2p (I don't know how to do that across process boundary), but certainly faster than current gRPC runtime as it doesn't go through the socket and loopback network stack. Alternatively, if you do have RDMA capable network, you could try the `grpc+verbs` or MPI based runtime which enable ordinary interprocess RDMA.", "Hi Bairen, first, thanks for your response! \r\n\r\nThe reason why I wanted to deploy a distributed TF on a single node is that my application is not a very good fit for exploiting model parallelism. However, I have multi GPUs available in a machine so I would like to exploit data parallelism. \r\n\r\nI agree with you that having an intra process parameter server could be the solution for my case. However, the problem here is the fact that I would like to have a distributed version of the pre-existing implementation that uses Tensorflow's [Experiment](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Experiment). As far as I understand, the only way to parallelize the Experiment-based application for data parallelism is to use ClusterConfig, which requires to launch multiple processes. \r\n\r\nRe-developing the application from scratch to enable the intra process parameter server imposes a lot of engineering effort on me and I believe other developers could have similar use-cases like this if they don't develop TF application from scratch. \r\n\r\nThen your question could be why I need DMA. My application has a large size of model parameters that need to be communicated and the CPU parameter server requires the communication to go through PCIe, which is not very performant. Therefore, I would like to deploy my parameter server on one of the GPUs and launch the workers on the other GPUs and have them to communicate via DMA for performant communication. (I would be even happier if Tensorflow allows me to place a parameter server with a worker on a GPU since then I could minimize the waste of compute resource. I am not quite sure if this is possible in the current implementation.)\r\n\r\nAlthough I have found people who already use a single machine for distributed TF, I think my case will be more common as the number of GPUs per machine increases because it would be unlikely for people to always assign all GPUs available in a machine to a single model replica. Moreover, as the learning model gets larger (e.g., RNNs), the PCIe communication of CPU parameter server will eventually be impractical. \r\n\r\nI think the best way of handling this issue is that TF launches a single process for parameter servers and workers sitting in the same machine and only communicate through gRPC when there is a need for machine-to-machine communication. The current way of having distributed TF through multi processes with CUDA_VISIBLE_DEVICES effectively disables DMA, which limits the use case like mine.\r\n\r\nPlease feel free to correct me if any of my understanding is wrong. Thank you!", "@martinwicke Any comments?", "Seems the [`cudaIpc*` functions](http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g02bb3632b5d223db6acae5f8744e2c91) could serve as a loopback transport for interprocess p2p. I have not used these functions so I am not sure.", "The cudaIPc* functions indeed seem to be the hope for interprocess p2p communication. I am not sure how the CUDA_VISIBLE_DEVICES play in this scenario though since the processes see different sets of GPUs (e.g., PS process sees GPU#0 while Worker process sees GPU#1). \r\n\r\nI wondered if you would consider incorporating this feature to the next patch? In any case, I really appreciate your effort in receiving feature requests from end users and trying to listen the needs! ", "@byronyi @martinwicke Hi, I wondered how this issue goes along? Thank you!", "@poxvoculi can you shed any light on this discussion? ", "The analysis of the situation by @jongsae and @byronyi seems largely correct.  Inter-process communication is less performant than intra-process, so the later is to be preferred when using multiple GPUs on a single machine.   Separate processes cannot access each other's address space, so TF will always try to use an RPC when communicating between processes, even if they're on the same machine.  Although DMA will be used in moving data on/off GPUs, there will be lots of extra useless work to execute the RPC whose data packets don't actually exit the local machine.\r\n\r\nMy first thought would be to restructure the program construction so that the elements which are now executed as independent processes could be packaged either as subgraphs assigned to a single worker, or subgraphs assigned to different workers, depending on whether real (cross-process) distribution is actually required.  I understand this may be difficult when trying to reuse software not designed for that purpose.  Perhaps @saeta  has some useful observations on the general problem of deploying data parallel programs with one versus multiple processes.\r\n", "@poxvoculi Thanks for your answer! Following your suggestion, I will consider re-implementing the existing TF application such that the parameter server and workers sit in the same process and they use the intra-process communication. @saeta, I am looking forward to hearing regarding your experience in deploying data parallel programs in a single process. \r\n\r\nIn a short-term perspective, I agree with this approach. However, in a long run,  as I argued earlier in my second comment in this issue, I believe it would be useful for many use-cases that TF incorporates the inter-process DtoD communication to support multi-process data parallel TF programs. An alternative approach would be to modify TF wrappers (e.g., Estimator or Experiment) so that they support single-process distributed TF in case where the configured parameter servers and workers (e.g., via tf.train.ClusterSpec or tf.estimators.run_config) are placed in the same local machine, which I think, is a better (or cleaner) approach. \r\n\r\nI wondered what TF folks @byronyi @martinwicke @cy89 @poxvoculi think about it?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "We are implementing better support for multi-GPU systems (available in `tf.contrib.estimator`), which will not rely on multiple processes. \r\n\r\nI will close this issue, it is too diffuse as a feature request. Please reopen a new issue with concrete suggestions.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied."]}, {"number": 11195, "title": "No error reported when a wrong argument name is inputted to tf.app.flags", "body": "The default behavior of `tf.app.flags` is: when the user inputs a wrong/undefined argument name, the program just keeps running without throwing out any error about it. For example, \r\n```\r\nflags.DEFINE_string(\"weight_path\", None, \"the path of saved model to restore from\")\r\n```\r\nBut when the user inputs \r\n```\r\npython your_prog.py --weights_path ${MODEL_PATH}\r\n```\r\nThe program just keeps running with `weight_path=None` and does not report the wrong argument of `weights_path` which has a **s** appended.\r\nShould we change this kind of default behavior? In this case, users may think they input the correct arguments to finetune the model but unfortunately it just trains from scratch. ", "comments": ["What should the behavior be here? I see a couple of options:\r\n\r\n1. Raise an error if there's an unknown argument. This would be as simple as switching from `parse_known_args` to `parse_args`\r\n2. Display a warning if there's an unknown argument. I imagine `parse_known_args` was chosen for a reason, so this might be the option more in-line with the intention behind that decision", "I prefer the first one, with lots of logs printing out, the warning info can also be easily missed.", "@wenwei202 @kheuton \r\nThe first choice maybe not so good, which is not compatible for previous versions. I think a warning is good enough. The arguments has been set default values, terminating the app maybe not good choice. \r\n\r\nMy solution:\r\n\r\n- add **required**  argument to the [DEFINE_\\*](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/flags.py#L65) APIs. which can solve the problem of lacking some required args or typo.\r\n\r\n- add warning information for unparsed arguments.\r\n\r\nI opened a PR, Feel free to share your thoughts.\r\n\r\n", "Reposting what I said on another PR, so it's centralized in one place.  This is a good feature request for a flags library.\r\n\r\nSome background is probably warranted though: TF's flags library is an API port of an internal flags library at Google that I believe predated argparse. The flags library's existence is solely to make it easier to opensource internal code, since flags.py's API is used throughout Google internally. But argparse is more than sufficient for most opensource use cases, so I would like to avoid changing the existing flags API, even to enhance it.\r\n\r\nFurthermore, I believe there are efforts to opensource the internal flags library, which already has this feature of marking flags as required via an explicit call to mark flags as required, and rather than introduce a new API that is incompatible with the internal one, I think it would be a good idea to wait for that library to be released. If it's the case that several months go by and that library is not released, let's revisit this PR and perhaps plan to make it API compatible with that library.\r\n\r\nI'm following up with the authors internally to see whether we should go ahead and add the API to the tf version in the short term.", "Okay, it sounds like it will take some time to opensource the internal flags library, so let's endeavor to make it API compatible if we're going to do this.\r\n\r\nIf the implementation can be something like:\r\n\r\n```\r\ndef mark_flag_as_required(flag_name):\r\n  # impl here\r\n```\r\n\r\nand its use looks like:\r\n\r\n```\r\nif __name__ == '__main__':\r\n      flags.mark_flag_as_required('your_flag_name')\r\n      tf.app.run()\r\n```\r\n\r\nthen I think we could accept a PR.\r\n\r\nThis means that the declaration of it being 'required' cannot be in the FLAGS definition, and so this will require some extra validation code to be run rather than using the 'requires' field of argparse.  \r\n\r\nMy suggestion would be to add code to the _FlagValues class that keeps track of required flags registered via \"mark_flag_as_required\", and prior to calling parse_known_args(), the code would check to see that 'args' contains all of the required flags, and raises an exception if not true.\r\n\r\nThe reason for doing it this way is that the this flags library is for better or worse (I think worse) built in such a way that it allows flags to be defined far from where they are parsed, and that means a flag may be defined in a file that you don't own, but you want to make required.  So decoupling the 'requirement' from the flag definition allows the binary to own the set of required flags.", "Ok, this way is reasonable to me. I'll work on it.", "The internal flags library will be released in three months, it's very like https://github.com/google/python-gflags, with some changes and many improvements.\r\n\r\nThe `mark_flag_as_required` API should be the same as https://github.com/google/python-gflags/blob/9f00edd8bb0c8c0019f98541655263d5923cec0a/gflags/__init__.py#L255\r\n\r\nTo be fully compatible, it should raise [`flags.ValidationError`](https://github.com/google/python-gflags/blob/9f00edd8bb0c8c0019f98541655263d5923cec0a/gflags/__init__.py#L88) if *flag value is not `None` after flags are parsed*.\r\n\r\nNOTE: in case the flag is defined with a non-None default value, e.g.\r\n\r\n```\r\nflags.DEFINE_string(\"weight_path\", '/tmp/module.out', \"the path of saved model to restore from\")\r\nflags.mark_flag_as_required(\"weight_path\")\r\n```\r\n\r\nbut the flag is not specified in the command line, no exception should be raised. (It's ok to display a warning though.)", "Thanks @yilei.  Though I don't think we can raise the same exception type, I think it's fine in the short term to raise a ValueError for now that will then change to ValidationError when we switch.   Please document that the exception raised will change in the future so it's documented as not being part of the official API.  Thanks!", "@vrv I have finished the code. I added the check at _parse_flags and __setattr__, which reference python-gflags. Feel free to share thoughts, Thanks.", "I'm confused.\r\n\r\n@nolanliou wrote:\r\n> My solution:\r\n> \r\n> - add required argument to the DEFINE_* APIs. which can solve the problem of lacking some required args or typo.\r\n>\r\n> - add warning information for unparsed arguments.\r\n\r\nAs far as I see only the first point has been implemented, which is to allow for required flags. I do not see where their second point has been implemented, which in my view is what the original feature request asks for. Am I missing something?", "Regardless of the current feature set, I think we're just going to move to the now opensourced https://github.com/abseil/abseil-py/tree/master/absl/flags at some point, so I'd rather avoid any further development of our homegrown version that would make it functionally different than that one.", "(I'd encourage those to avoid using tf.app.flags if you are bothered by issues with it, and use the abseil library or just argparse)."]}, {"number": 11194, "title": "Android TF Classify Multi-label output example?", "body": "What would you have to change to the existing tensorflow android TFClassify app demo to output multiple labels?  \r\n\r\nFor instance I have a multi output model trained in keras with multiple softmax output nodes. The current TFClassify example has one output node so would the solution be to merely add additional output nodes in the java files?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 11193, "title": "Document building tensorflow-gpu from source?", "body": "I have built Tensorflow 1.2 from source with GPU support. However the Python package name is \"tensorflow\" and not \"tensorflow-gpu\" as is common. We also have machines that don't have GPUs in them so I'd like to have them both. However I could not find any documentation on how to do this when building from source.", "comments": ["Hi,\r\n     It's already here -> [https://www.tensorflow.org/install/install_sources#ConfigureInstallation](https://www.tensorflow.org/install/install_sources#ConfigureInstallation). You need to have `bazel, GPU prerequisites` in order to execute `./configure`. where you will be asked following questions. \r\n```\r\nDo you wish to build TensorFlow with CUDA support? [y/N] Y     \r\nCUDA support will be enabled for TensorFlow\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:\r\nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nPlease specify the cuDNN version you want to use. [Leave empty to use system default]: 5\r\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\n```\r\n\r\nThis third party link might help too, [https://alliseesolutions.wordpress.com/2016/09/08/install-gpu-tensorflow-from-sources-w-ubuntu-16-04-and-cuda-8-0/](https://alliseesolutions.wordpress.com/2016/09/08/install-gpu-tensorflow-from-sources-w-ubuntu-16-04-and-cuda-8-0/)", "I've went through that already and confirmed I have a working tensorflow + GPU package. The issue is that the python module is named `tensorflow` and not `tensorflow-gpu`. How do you make it install as `tensorflow-gpu` instead of `tensorflow`?", "I misunderstood your question by reading heading only. I think if you try to install a common tensor-flow on non GPU machine . It won't have `CUDA , cuDNN`. So, it is obvious that will use CPU resources.\r\n`./configure` ask you CUDA support question which is sufficient for non GPU machines for simplicity.\r\nI believe for making development and distribution compatible and easy with each other they have singular source and for other use cases I guess. \r\n\r\nHere are some points in order to modify source : -\r\n\r\n1. Try to experiment by removing `third_party/gpus` folder.\r\n\r\n2. The easiest way to achieve this by comparing tensor flow-cpu vs tensor flow distribution source side by side and make your own version of tensoflow cpu by making changes in source. It's experimental and not sure will work or not.\r\n", "@jart any ideas on this one?", "At the moment, this is not possible. You will need to conditionally install CPU or GPU packages on each machine based on existence of GPU on them.\r\n\r\nCC @martinwicke ", "@morrisonlevi I think you just need to  invoke `bazel build -- //tensorflow/tools/pip_package:build_pip_package  --gpu` to have the `--project_name` of the `setup.py` of the package set to tensorflow_gpu:\r\nSee:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/build_pip_package.sh#L57", "Even if the pip packages are built with a different name, when installed they still occupy the same directory with the files with same names under your python library directories. Therefore, If you try to install both tensorflow and tensorflow-gpu you will overwrite the files the first installs with the second installation.\r\nIn the end, your installation may even not work.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "For anyone following @Mistobaan's advice (https://github.com/tensorflow/tensorflow/issues/11193#issuecomment-326819113), you should invoke `bazel build` normally, and only then use the `--gpu` arg:\r\n```\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package ${TF_ROOT}/pip/tensorflow_pkg --gpu\r\n```"]}, {"number": 11192, "title": "why use \"x_is_dict\" to check y?", "body": "In tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py\r\nOn L325:\r\n`self._y = None if y is None else \\\r\n      dict([(k, check_array(v, v.dtype)) for k, v in list(y.items())]) if x_is_dict else check_array(y, y.dtype)\r\n`\r\n   \r\nI happened to have a implementation where x is a dict but y is a numpy array, so I got an error. I wonder why we do not use y_is_dict here? ", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Ok thanks. I do think it is a bug though. A wrong variable is used here.", "Sorry, I missed that. I agree. ", "#10403 looks like the same problem, with a little more context about the x/y confusion."]}, {"number": 11191, "title": "fix incorrect usage of num_gpus & num_workers (#7312)", "body": "Removed unnecessarily restrictive check as discussed in in #7312 and validated the change using 2 single GPU VMs.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I have signed CLA", "Jenkins, test this please.", "Somehow the CLA signing did not trigger. Could you check again?", "Jenkins, test this please.", "@skaarthik you have to exactly type `I signed it!` to retrigger cla bot.", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Thanks @gunan for letting me know. I have added a new comment that says `I signed it!`.\r\nThansk @caisq for approving the PR"]}, {"number": 11190, "title": "Branch 160665742", "body": "", "comments": ["Thanks!"]}, {"number": 11189, "title": "On Network with Shared Weights, Optimizer.minimize has no effect", "body": "I have a simple siamese network--a network with two branches that share weights--and a script to train it on some very simple data. The loss and gradients are both non-zero, but executing an optimizer.minimize(loss) operation has no effect on the weights. Executing it in two steps with compute_gradients and apply_gradients also has no effect.\r\n\r\nI have tried multiple optimizers and settings, and explored StackOverflow without finding relevant information.\r\n\r\nI have included both scripts (they are small) with the relevant debug statements so that you can easily inspect the weights and gradients as well. You will see that the accuracy/weights/gradients do not change (loss changes because new batches are being computed each iteration).\r\n\r\n------------------------\r\n\r\n### System information\r\n- Windows 10, up to date\r\n- Tensorflow 1.2.0, CPU-only mode\r\n- Python 3.5\r\n- Executing from Windows terminal\r\n\r\nNetwork constructor:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nclass SameDiffNet:\r\n\t# a siamese FC network for classifying vectors as same/different\r\n\t\r\n\tdef __init__(self,inputLen):\r\n\t\t# settings\r\n\t\tself.NUM_BRANCHES = 2\r\n\t\tself.LAYER_SIZES = [20,20]\r\n\t\tself.DATA_TYPE = tf.float32\r\n\t\tself.NORM_CONSTANT = 0.0001\r\n\t\t\r\n\t\t# input\r\n\t\tself.inputs = []\r\n\t\tfor branch in range(self.NUM_BRANCHES):\r\n\t\t\tself.inputs.append(tf.placeholder(self.DATA_TYPE,[None,inputLen]))\r\n\r\n\t\t# network branches\r\n\t\tself.branches = []\r\n\t\tself.branchWeights = self.branch_weights(inputLen)\r\n\t\tfor branch in range(self.NUM_BRANCHES):\r\n\t\t\tself.branches.append(self.network_branch(branch))\r\n\t\t\t\t\r\n\t\t# combination layer and loss\r\n\t\tself.out = self.distance_layer_euclidean()\r\n\t\tself.target = tf.placeholder(self.DATA_TYPE,[None, 1])\r\n\t\tself.loss = self.contrastive_loss()\r\n\t\tself.accuracy = self.my_accuracy()\r\n\t\t\r\n\tdef branch_weights(self,inputLen):\r\n\t\t# weights are shared, so they are computed once and re-used to make multiple graphs\r\n\t\t# They are stored as a dictionary of arrays for flexible layer shapes and sizes\r\n\t\tnetWeights = {\"weights\": [], \"bias\": []}\r\n\t\tnetWeights[\"weights\"].append(tf.Variable(tf.random_normal([inputLen,self.LAYER_SIZES[0]]), name=\"weights0\"))\r\n\t\tnetWeights[\"bias\"].append(tf.Variable(tf.zeros([self.LAYER_SIZES[0]]),name=\"bias0\"))\r\n\t\t\r\n\t\tfor layer in range(1,len(self.LAYER_SIZES)):\r\n\t\t\tnetWeights[\"weights\"].append(tf.Variable(tf.random_normal([self.LAYER_SIZES[layer-1],self.LAYER_SIZES[layer]]), name=\"weights\" + str(layer)))\r\n\t\t\tnetWeights[\"bias\"].append(tf.Variable(tf.zeros([self.LAYER_SIZES[layer]]), name=\"bias\" + str(layer)))\r\n\t\t\r\n\t\treturn netWeights\r\n\t\t\r\n\tdef network_branch(self,branch):\r\n\t\tfc = self.inputs[branch]\r\n\t\tfor layer in range(len(self.LAYER_SIZES)):\r\n\t\t\tfc = tf.nn.relu(tf.nn.bias_add(tf.matmul(fc,self.branchWeights[\"weights\"][layer]), self.branchWeights[\"bias\"][layer]))\r\n\t\treturn fc\r\n\t\t\t\r\n\tdef distance_layer_euclidean(self):\r\n\t\tassert self.NUM_BRANCHES == 2\r\n\t\tdist = tf.subtract(1.0,tf.sigmoid(tf.sqrt(tf.reduce_sum(tf.pow(tf.subtract(self.branches[0],self.branches[1]),2),1)+self.NORM_CONSTANT)))\r\n\t\treturn dist\r\n\t\t\r\n\tdef cross_entropy_loss(self):\r\n\t\tloss = tf.reduce_sum(tf.multiply(-1.0,tf.add(tf.multiply(self.target,tf.log(self.out+self.NORM_CONSTANT)),tf.multiply(1-self.target,tf.log(1-self.out+self.NORM_CONSTANT)))))\r\n\t\treturn loss\r\n\t\t\r\n\tdef contrastive_loss(self):\r\n\t\tloss = tf.reduce_sum(tf.pow(tf.subtract(self.out,self.target),2))\r\n\t\treturn loss \r\n\t\t\r\n         #Does not work either, so I wrote a simple replacement\r\n\tdef tf_accuracy(self):\r\n\t\treturn tf.metrics.accuracy(tf.round(self.out),self.target)\r\n\t\t\r\n\tdef my_accuracy(self):\r\n\t\treturn tf.reduce_mean(tf.cast(tf.equal(tf.round(self.out), self.target),tf.float32))\r\n```\r\n\r\n\r\nRun script:\r\n\r\n```\r\n''' A simple test where we train our siamese network on toy examples\r\nOur training data consists of a pair of 0's and 1's, and our truth output will\r\nsimply be the XOR of these two values'''\r\n\r\nimport time\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom sameDiffNet import SameDiffNet\r\n\r\nnumTraining = 1000\r\nnumTest = 500\r\nnumIter = 10000\r\n\r\nsess = tf.InteractiveSession()\r\nnetwork = SameDiffNet(2)\r\noptimizer = tf.train.GradientDescentOptimizer(0.1)\r\ntrain = optimizer.minimize(network.loss)\r\ngradients = optimizer.compute_gradients(network.loss) #,tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))\r\napplyGrad = optimizer.apply_gradients(gradients)\r\n\r\ndata = np.random.randint(0,2,(numTraining+numTest,2))\r\ntruth = data[:,0] == data[:,1]\r\ntruth = [float(not truth[b]) for b in range(numTraining+numTest)]\r\ndata = data.astype(float)\r\n\r\ntrainData = data[:numTraining,:]\r\ntestData = data[numTraining:,:]\r\ntrainTruth = truth[:numTraining]\r\ntestTruth = truth[numTraining:]\r\n\r\n#Create test data once\r\ntestPermutationL = np.random.permutation(numTest)\r\ntestPermutationR = np.random.permutation(numTest)\r\ntestTarget = [[float(testTruth[testPermutationL[i]] == testTruth[testPermutationR[i]])] for i in range(numTest)]\r\n\r\ntf.global_variables_initializer().run()\r\n\r\n#debugging\r\nprint(\"Trainable Variables:\")\r\nprint(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))\r\n\r\nfor iter in range(numIter):\r\n\tpermutationL = np.random.permutation(numTraining)\r\n\tpermutationR = np.random.permutation(numTraining)\r\n\ttarget = [[float(trainTruth[permutationL[i]] == trainTruth[permutationR[i]])] for i in range(numTraining)]\r\n\t\r\n\t# all large debug output is included in triple-quotes\r\n\t\r\n\t'''\r\n\tprint(data[permutationL[1:5],:])\r\n\tprint(data[permutationR[1:5],:])\r\n\tprint(target[1:5])\r\n\t'''\r\n\t\r\n\t# you can run the optimization in two steps or one\r\n\t'''\r\n\tgrad = sess.run([gradients], feed_dict={\r\n\t\t\t\t\tnetwork.inputs[0]:trainData[permutationL,:],\r\n\t\t\t\t\tnetwork.inputs[1]:trainData[permutationR,:],\r\n\t\t\t\t\tnetwork.target: target})\r\n\t\r\n\tsess.run([applyGrad])\r\n\t'''\r\n\t\r\n\t_, loss = sess.run([train,network.loss], feed_dict={\r\n\t\t\t\t\tnetwork.inputs[0]:trainData[permutationL,:],\r\n\t\t\t\t\tnetwork.inputs[1]:trainData[permutationR,:],\r\n\t\t\t\t\tnetwork.target: target})\r\n\t\t\r\n\ttotalLoss = np.sum(loss)\r\n\tif np.isnan(totalLoss):\r\n\t\tprint('Model diverged with loss = NaN')\r\n\t\tquit()\r\n\r\n\tif iter % 10 == 0:\r\n\t\tprint ('step %d: loss %.3f' % (iter, totalLoss/numTraining))\r\n\t\tacc = sess.run([network.accuracy],feed_dict={\r\n\t\t\t\t\tnetwork.inputs[0]:testData[testPermutationL,:],\r\n\t\t\t\t\tnetwork.inputs[1]:testData[testPermutationR,:],\r\n\t\t\t\t\tnetwork.target: testTarget});\r\n\t\tprint ('step %d: accuracy %.3f' % (iter, np.sum(acc)))\r\n\r\n\t#debugging\r\n\t'''\r\n\tprint(\"Gradients:\")\r\n\tprint(grad)\r\n\t'''\r\n\t'''\r\n\tprint(\"First-Layer Weights:\")\r\n\tprint(sess.run(network.branchWeights[\"weights\"][0]))\r\n\t'''\r\n\r\n```", "comments": []}, {"number": 11188, "title": "[Java] Add base classes and utilities for operation wrappers.", "body": "This pull requests includes a set of basic classes, interfaces and utilities useful for the automatic generation of operations by the upcoming C++ module. They are meant to be used more as internal tools than being part of the public API. ", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Jenkins, test this please.", "(Confirming that it's okay to merge the one commit authored by me in addition to the rest of the commits by the PR author)", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please.", "IMHO, test failures are not related to what was committed by this PR", "@karllessard There are some conflicts with `master` right now -- can you rebase?", "Thanks @frankchn , rebasing completed.", "Jenkins, test this please."]}, {"number": 11187, "title": "TF Slim - allow soft placement for devices with train_image_classifier", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.12.5\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**: git tag 1.2\r\n- **Python version**: 2.7 system install\r\n- **Bazel version (if compiling from source)**: home-brew 0.4.5\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**:  NA\r\n- **Exact command to reproduce**: \r\n\r\ntrain Mobilenet_v1 on CPU like so:\r\n\r\n python train_image_classifier.py     --train_dir=${TRAIN_DIR}     --dataset_dir=${DATASET_DIR}     --dataset_name=Framing     --dataset_split_name=train     --model_name=mobilenet_v1     --checkpoint_path=${CHECKPOINT_PATH}     --checkpoint_exclude_scopes=MobilenetV1/Logits/Conv2d_1c_1x1/biases,MobilenetV1/Logits/Conv2d_1c_1x1/weights \r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nTensorflow 1.2 has no GPU support for macOS. Thus training/retraining can only happen on CPU.\r\nTF Slim doesnt appear to have an out of the box way to specify soft placement of nodes - therefore I can't appear to train a mobile net checkpoint?\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n\r\n`INFO:tensorflow:Summary name /clone_loss is illegal; using clone_loss instead.\r\nINFO:tensorflow:Fine-tuning from /Volumes/MediaArchive/datasets/SynopsisCinemaNet/model/mobilenet_v1_1.0_224_2017_06_14/mobilenet_v1_1.0_224.ckpt.index\r\nINFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, Cannot assign a device for operation 'MobilenetV1/Logits/Conv2d_1c_1x1/biases/RMSProp_1': Operation was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/cpu:0 ]. Make sure the device specification refers to a valid device.\r\n\t [[Node: MobilenetV1/Logits/Conv2d_1c_1x1/biases/RMSProp_1 = VariableV2[_class=[\"loc:@MobilenetV1/Logits/Conv2d_1c_1x1/biases\"], container=\"\", dtype=DT_FLOAT, shape=[5], shared_name=\"\", _device=\"/device:GPU:0\"]()]]\r\n`", "comments": ["So, spending some time getting more familiar with Slim, it appears that commending out some specific code in model_deploy where clone_on_cpu is checked calls into setting the GPU device has made it work. \r\n\r\nIt would be nice for this script to auto-detect available devices and not assume GPU support since a shipping, supported platform does not provide CPU support.\r\n\r\nThank you in advance for consideration.", "@sguada can you comment on this one?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "???", "You can configure using FLAGS.clone_on_cpu=True\r\nThis issue should be under https://github.com/tensorflow/models/issues"]}, {"number": 11186, "title": "tf.copy() as alternative to tf.identity()", "body": "`tf.identity(tensor)` does or does not create a copy of the tensor based on whether it's on the same device. This can lead to bugs that are hard to find. The current way of ensuring a copy is to perform an arithmetic/logic operation that doesn't change the value, such as `tensor + 0`, `1 * tensor`, or `tf.equal(tensor, True)`. Needless to say, this makes code hard to read. Moreover, different treatment is needed for different tensor types. Can we have a `tf.copy(tensor)` that does this for us?", "comments": ["Just curious, could you elaborate what kind of bugs it could lead to?", "> The current way of ensuring a copy is to perform an arithmetic/logic operation that doesn't change the value\r\n\r\nCouldn't we use `tf.assign` for that?", "@ppwwyyxx For example, I was trying to feed the transition tuple of state, action, reward, and successor state to a reinforcement learning agent. Here, `env.state` is a variable holding the current state of the environment that gets updated when calling `env.step(action)`:\r\n\r\n```python\r\n# NOTE: old_state actually refers to the new state after stepping the env. Need env.state * 1 instead.\r\nold_state = tf.identity(env.state)\r\nwith tf.control_dependency([old_state]):\r\n    action = agent.act(env.state)\r\n    reward = env.step(action)\r\nwith tf.control_dependency([reward]):\r\n    experience_op = agent.experience(old_state, action, reward, env.state)\r\n```\r\n\r\n@jyegerlehner That would require creating a second variable to hold the old state of the variable. That's wasteful if the value is only needed inside each `sess.run()` call separately. Instead, the value should be hold by a tensor.", "I believe your issue can be described in this small repro:\r\n```python\r\nones = np.ones((10,10))\r\nv = tf.get_variable('asdf', shape=[10,10], dtype=tf.float32)\r\nold_state = v.read_value()   # equivalent to tf.identity\r\nwith tf.control_dependencies([old_state]):\r\n    reward = tf.assign_add(v, ones)\r\nwith tf.Session() as sess:\r\n    tf.global_variables_initializer().run()\r\n    a, b = sess.run([reward, old_state])\r\n    print(a.sum(), b.sum())\r\n```\r\nThe above code, though appears to be deterministic (due to the control dependency), actually is not. It sometimes prints two equal results, sometimes not.\r\n\r\nUsing `get_variable(..., use_resource=True)` (will become default in the future) seems to fix the problem. Could you try this? @danijar ", "I also wonder if we could presumably apply tf.copy to tf.Variable, tf.placeholder and so on. \r\nI am referring to the situation described here: https://stackoverflow.com/questions/45947973/how-to-create-copies-of-variables-constants-placeholders-in-tensorflow", "@ppwwyyxx  I have the same problem. I post some snippets in #4663 .", "@thjashin And my same solution seems applicable.", "@ppwwyyxx Yeah using `ResourceVariable` actually works but just wonder why this happens. It's pretty strange that `control_dependencies` doesn't work with plain `Variable`.", "@danijar \r\nSo if I understand this issue correctly, does this mean that:\r\nfor identity:\r\n`var_identity = tf.identity(var)`\r\n\r\nif var and var_identity are in the same device, var_identity would be similar to a C++ reference to var (basically they are the same thing in the same memory, var_identity is just an alias for var), any change in var would be represented equally in var_identity. In this sense, I think implementation-wise, it is similar to that in get_variable() function set var_identity a reusable variable of var.\r\n\r\nif var_identity is transferred to another device, which I think it is pretty typical when training multiple workers and they are shared by one param server, the var_indentity is essentially a copy of var. This makes sense, because, between different devices, nothing can be really shared but a copy would be the best solution.", "> @danijar\r\n```\r\nimport tensorflow as tf\r\n# Create a variable.\r\nw = tf.Variable(3.)\r\ncopyw = tf.identity(w)\r\nw = w+1\r\n# Use the variable in the graph like any Tensor.\r\ny = tf.add(w, w)\r\n# The overloaded operators are available too.\r\nz = tf.sigmoid(w + y)\r\ninit_op = tf.global_variables_initializer()\r\n# Launch the graph in a session.\r\nwith tf.Session() as sess:\r\n    # Run the Op that initializes global variables.\r\n    sess.run(init_op)\r\n    w_, cw_ = sess.run([ w, copyw])\r\n    print(w_, cw_)  # output 4.0 3.0\r\n```\r\nBased on above snippet, I think tf.identity() works as deep copy.  as least in tensorflow 1.11.0.", "As demonstrated by @chuchienshu above, `tf.identity()` now works as deep copy. Closing this issue.", "@dynamicwebpaige This issue was not about whether or not `tf.identity` was working, but rather about creating a `tf.copy` alias for this behavior that would clearly reflect the intent of a deep copy. So closing it seems to be premature.", "`tf.identity` raises errors when GPU is involved. Using `tf.device('CPU')` will prevent the error:\r\n```\r\nwith tf.device('CPU'):\r\n    tensor2 = tf.identity(tensor1)         # .take(-1)\r\n```\r\nRemember tensor2 is a Variant. You may use take(-1) in case tensor1 is a Dataset and you want tensor2 to be one of the Dataset family only.\r\nActually the error is generated for the Datasets in first place."]}, {"number": 11185, "title": "Merging 1.2.1 back into master.", "body": "", "comments": []}, {"number": 11184, "title": "TF Slim scripts iterate invisible files, don't appear to handle paths with spaces?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.12\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.2 release (tagged from git)\r\n- **Python version**: system 2.7 from Mac OS X 10.12\r\n- **Bazel version (if compiling from source)**: home-brew 0.4.5\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:  TF Slim scripts such as train_image_classifier and their installed components, and download_and_convert.py \r\n\r\n### Describe the problem\r\nHello. Apologies, this seems rather trivial and potentially an oversight on my part, but I think its worth mentioning. \r\n\r\ni'm attempting to follow the TF Slim models readme to get Mobilenet_v1 trained on a custom data set. In looking through the download_and_convert.py scripts and TFRecord creation scripts, I've noticed that:\r\n\r\n\u2022 conversion functions will iterate over a directory and attempt to load invisible files on macOS such as .DS_Store files as jpegs and cause exceptions in image decode functions\r\n\u2022 train_image_classifier does not appear to handle spaces in directory path names, and also attempts to iterate over invisible items such as .DocumentRevisions-V100 (which of course is only an issue if Volume paths with spaces are used).\r\n\r\nIm likely missing some basic python understanding, or perhaps some of my issue is an interplay with the nuances of OS X, the OS X installed Python and testing not happening on OS X - but its definitely an issue for folks trying to do work on OS X setting up inference and tuning / retraining end portions of graphs.\r\n\r\n### Source code / logs\r\n\r\nI don't think I need to include any source, but will try to oblige should anything be requested. Hopefully this isn't an embarrassingly obvious oversight on my part! \r\n\r\nThank you in advance for taking the time to look this over.", "comments": ["Oh, to be clear, I have a volume named \"Media Archives\" which contains a folder with custom set of TFRecords and a TFDataset.\r\n\r\nScripts appear to work if I rename the folder and volume to not have spaces. ", "Could I ask you to open a separate bug about the \"paths with spaces\" issue? That doesn't necessarily seem like a Mac-specific problem, and might be something that we should be handling at a different level. \r\n\r\nThen we could focus the issue down to the \"invisible files\" problem on OSX. \r\nThe thread at https://stackoverflow.com/questions/284115/cross-platform-hidden-file-detection suggests that just detecting the leading \".\" might suffice for ignoring the usual hidden files on Mac and UNIX. That would seem to cover the .DS_Store case. ", "I am looking into this.\r\n\r\n@cy89 OSX has a bunch of ways to hide files, but for console sessions the only one that really matters is files that start with \".\".", "The script https://github.com/tensorflow/models/blob/master/slim/datasets/download_and_convert_flowers.py does indeed attempt to read all files in the input directory, including hidden files. If the user has the target directory open in a Finder window, the Finder could create a .DS_Store file, causing conversion to fail. I'll add some filtering.\r\n\r\nThe other scripts for importing the CIFAR10 and MNIST data sets do not suffer from this problem, as they already apply filename filters that would exclude names starting with \".\".", "Opened PR that adds filtering to `download_and_convert_flowers.py`", "Closing as this is resolved"]}, {"number": 11183, "title": "ConnectionError: HTTPSConnectionPool(host='storage.googleapis.com', port=443): Max retries exceeded with url: /tensorflow/linux/gpu/tensorflow_gpu-1.2.1-cp27-none-linux_x86_64.whl (Caused by <class 'socket.error'>: [Errno 101] Network is unreachable)", "body": "I have downloaded and removed Tensorflow a few times in order to try and fix a problems with my numpy package. I now get the error above when I try and download tensorflow. Is there a max amount of times I can download it, is that why I am getting this error? \r\n\r\nHere is the traceback: \r\nException:\r\nTraceback (most recent call last):\r\n  File \"/home/slkapur/tensorflow/local/lib/python2.7/site-packages/pip/basecommand.py\", line 122, in main\r\n    status = self.run(options, args)\r\n  File \"/home/slkapur/tensorflow/local/lib/python2.7/site-packages/pip/commands/install.py\", line 278, in run\r\n    requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle)\r\n  File \"/home/slkapur/tensorflow/local/lib/python2.7/site-packages/pip/req.py\", line 1197, in prepare_files\r\n    do_download,\r\n  File \"/home/slkapur/tensorflow/local/lib/python2.7/site-packages/pip/req.py\", line 1375, in unpack_url\r\n    self.session,\r\n  File \"/home/slkapur/tensorflow/local/lib/python2.7/site-packages/pip/download.py\", line 546, in unpack_http_url\r\n    resp = session.get(target_url, stream=True)\r\n  File \"/home/slkapur/tensorflow/local/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py\", line 395, in get\r\n    return self.request('GET', url, **kwargs)\r\n  File \"/home/slkapur/tensorflow/local/lib/python2.7/site-packages/pip/download.py\", line 237, in request\r\n    return super(PipSession, self).request(method, url, *args, **kwargs)\r\n  File \"/home/slkapur/tensorflow/local/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py\", line 383, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/home/slkapur/tensorflow/local/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py\", line 486, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/home/slkapur/tensorflow/local/lib/python2.7/site-packages/pip/_vendor/requests/adapters.py\", line 378, in send\r\n    raise ConnectionError(e)\r\nConnectionError: HTTPSConnectionPool(host='storage.googleapis.com', port=443): Max retries exceeded with url: /tensorflow/linux/cpu/tensorflow-1.2.1-cp27-none-linux_x86_64.whl (Caused by <class 'socket.error'>: [Errno 101] Network is unreachable)\r\n \r\nStoring debug log for failure in /tmp/tmp2du06v\r\n", "comments": ["There shouldn't be a retry limit. \r\n\r\nIn the meantime, can you tell us exactly what you're doing, with the usual template: \r\n\r\nPlease provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 11182, "title": "Quantize weights causes accuracy to plunge when run in mobile but not in computers?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.2.1\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n```\r\n/home/kwotsin/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=./frozen_inception_resnet_v2_for_mobile.pb \\\r\n--out_graph=./quantized_inception_resnet_v2_for_mobile_NEW.pb \\\r\n--inputs='Placeholder_only' \\\r\n--outputs='InceptionResnetV2/Logits/Predictions' \\\r\n--transforms='\r\n  add_default_attributes\r\n  strip_unused_nodes(type=float, shape=\"1,299,299,3\")\r\n  remove_nodes(op=Identity, op=CheckNumerics)\r\n  fold_constants(ignore_errors=true)\r\n  fold_batch_norms\r\n  fold_old_batch_norms\r\n  quantize_weights\r\n  strip_unused_nodes\r\n  sort_by_execution_order'\r\n```\r\n\r\n### Describe the problem\r\nUsing the above quantization method, the quantization tool works so well that there is hardly a noticeable difference in accuracy drop (less than 0.5%) for a model like inception v3, with a 1/4 size reduction and slightly faster speed. However, when using the exact same files to be run on mobile, the performance gets so poor that there's more than 70-80% accuracy decrease. I'm unsure whether the issue lies with the quantization not getting optimized on mobile architectures (ARM instead of the usual desktop amd architecture), or whether there is a problem in the operations for the tensorflow mobile library.\r\n\r\nNote that `quantize_nodes` is totally unusable. When used to quantize the model, the model size increases a little and then causes the app to crash instantly. The error log produced when using quantize_nodes is this:\r\n\r\n```\r\n07-01 00:07:01.760 28272-28357/com.mindorks.tensorflowexample E/art: No implementation found for long org.tensorflow.contrib.android.RunStats.allocate() (tried Java_org_tensorflow_contrib_android_RunStats_allocate and Java_org_tensorflow_contrib_android_RunStats_allocate__)\r\n07-01 00:07:03.508 28272-28357/com.mindorks.tensorflowexample A/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0x0 in tid 28357 (pool-1-thread-1)\r\n```\r\n\r\nAlso, for almost every model I ran, the following error appeared:\r\n`No implementation found for long org.tensorflow.contrib.android.RunStats.allocate()`\r\nWhat does this mean and how could I resolve it?\r\n\r\nFYI: Not sure if it makes a difference, but when I built my lib_tensorflow_inference.so file and the JAR file for using the TF library on mobile, the tensorflow version was cloned from the master branch and not git checked out. Would this make a difference?\r\n\r\nFurther weird phenomenon:\r\n\r\nAlthough I built my TF from source and bazel built the graph transform tool, the following warnings still appear:\r\n\r\n```\r\n2017-07-01 00:05:19.612228: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-01 00:05:19.612262: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-01 00:05:19.612278: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-01 00:05:19.612282: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-01 00:05:19.612290: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n```\r\n\r\nThank you.", "comments": ["Could you please provide us with all available information about the mobile architecture/platform you're targeting?", "Oh yes indeed I'm working on an android LG G3 phone that's running ARM architecture I believe.", "I might have just solved this problem by looking at what #8897 did. Basically, I had to include the argument `--copt=\"-DTENSORFLOW_DISABLE_META\" ` when building the `libtensorflow_inference.so` file. This reduced the library size for android and made the performance almost exactly as expected when run on a desktop.\r\n\r\nHere is the full command to build libtensorflow_inference.so:\r\n```\r\nbazel build -c opt --copt=\"-DTENSORFLOW_DISABLE_META\"  //tensorflow/contrib/android:libtensorflow_inference.so    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a\r\n```\r\n\r\nand then the JAR file subsequently:\r\n\r\n```\r\nbazel build //tensorflow/contrib/android:android_tensorflow_inference_java\r\n```\r\n\r\nFurther, I think there are two problematic transformations: Remove Nodes and Quantize Nodes. Using either one causes the accuracy to be reduced or inference timing to increase by around 5x.\r\nThe remaining 8 transformations are what I found as optimal:\r\n\r\n```\r\n/home/kwotsin/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=./frozen_inception_resnet_v2_for_mobile.pb \\\r\n--out_graph=./quantized_inception_resnet_v2_for_mobile.pb \\\r\n--inputs='Placeholder_only' \\\r\n--outputs='InceptionResnetV2/Logits/Predictions' \\\r\n--transforms='\r\n  add_default_attributes\r\n  strip_unused_nodes(type=float, shape=\"1,299,299,3\")\r\n  fold_constants(ignore_errors=true)\r\n  fold_batch_norms\r\n  fold_old_batch_norms\r\n  quantize_weights\r\n  strip_unused_nodes\r\n  sort_by_execution_order'\r\n```", "@kwotsin \r\nIt seems that the remove transformation also drops output nodes #8181. \r\nAnd sometimes removing them damages certain layers (e.g. batch norms) which happen to use some Identity ops which cannot be removed unconditionally.", "Thanks for the explanation! Hopefully there's a fix coming soon", "Trying to build the inference library like @kwotsin fails for me:\r\n\r\n```\r\nERROR: /home/androbin/.cache/bazel/_bazel_androbin/0614ddb178f91f25438b47fee5f001b7/external/protobuf/BUILD:73:1: C++ compilation of rule '@protobuf//:protobuf_lite' failed: false failed: error executing command \r\n  (cd /home/androbin/.cache/bazel/_bazel_androbin/0614ddb178f91f25438b47fee5f001b7/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda \\\r\n    CUDNN_INSTALL_PATH=/usr/local/cuda \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3.5 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\\r\n    TF_CUDA_CLANG=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=5.0 \\\r\n    TF_CUDA_VERSION=8.0 \\\r\n    TF_CUDNN_VERSION=6 \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_OPENCL=0 \\\r\n  /bin/false -DTENSORFLOW_DISABLE_META -MD -MF bazel-out/stub_armeabi-v7a-py3-opt/bin/external/protobuf/_objs/protobuf_lite/external/protobuf/src/google/protobuf/io/zero_copy_stream.pic.d '-frandom-seed=bazel-out/stub_armeabi-v7a-py3-opt/bin/external/protobuf/_objs/protobuf_lite/external/protobuf/src/google/protobuf/io/zero_copy_stream.pic.o' -fPIC -iquote external/protobuf -iquote bazel-out/stub_armeabi-v7a-py3-opt/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/stub_armeabi-v7a-py3-opt/genfiles/external/bazel_tools -isystem external/protobuf/src -isystem bazel-out/stub_armeabi-v7a-py3-opt/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -DHAVE_PTHREAD -Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare -Wno-unused-function -c external/protobuf/src/google/protobuf/io/zero_copy_stream.cc -o bazel-out/stub_armeabi-v7a-py3-opt/bin/external/protobuf/_objs/protobuf_lite/external/protobuf/src/google/protobuf/io/zero_copy_stream.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nTarget //tensorflow/contrib/android:libtensorflow_inference.so failed to build\r\n```\r\n\r\nsee #10579 for details", "@kwotsin is the bug resolved for you? The other threads that @androbin references (#8181 and #10579) suggest that there are still some issues being worked out.", "@androbin not sure if it matters, but I am using gcc and not clang when I configured my TF version, + python 2.7; also, I git checked out the master branch instead of r1.2 for the build. \r\n\r\n@cy89 yes I think the bug is resolved for me so far. I am seeing rather good results coming from the quantization when I remove those two problematic transformation - `remove_nodes` and `quantize_nodes`", "@kwotsin \r\nI am currently using gcc, but thanks for the hint!\r\nThe version of python (3.5.2 in my case) should not matter?\r\nLast pulled git commit was 454f704b094dee26a31d073c02f488cc8dce5944 from master, will update.", "@Androbin please let us know how this goes: I will close the issue if it's solved for you.", "Rebuilt everything from latest master, last error section changed slightly:\r\nI found `zero_copy_stream` to be replaced by `coded_stream` but nothing else changed.", "Pete, who is best to look at graph_transform issues, other than you?", "In this case, I believe the problem is related to metagemm, since @kwotsin references `--copt=\"-DTENSORFLOW_DISABLE_META\"` as a fix for a similar problem, and the symptoms make sense. Passing to Maciek to investigate that side.", "@Androbin It is quite strange the commands didn't work for you. Perhaps it is a system issue? Please let me know if you require information from my system build; I'll be glad to help out.", "@petewarden \r\nThat sounds plausible, could you explain, what `metagemm` should do and what it is that is not working as intended? And are there any shortcomings of not having it running?\r\n\r\n@kwotsin \r\nThat is improbable, I have a pretty clean setup, not really any other major complaints. TensorFlow and its other tools worked just well till now. But quantization, selective registration and a few other tools have been flawky (for me and others) for quite some time now.", "Hi there!\r\nMetagemm is a library of high performance Arm32/64 kernels for quantized matrix multiplication, convolutions and related primitives. If turned off reference implementations will be used for some ops so you could expect lower performance.\r\n\r\nI believe I might have let an old bug slip past. Metagemm right now has an aggressive code path for matrix multiplication and convolution with 'dot' dimension under 2048. And there is no test to actually check if k <= 2048.  If k > 2048, then depending on actual data, that you have, some aggregators might overflow.\r\n\r\nI should have a patch tomorrow, so you can see if that is actually the case.", "Ok the patch is in the Google repository and it should be on github soon.", "https://github.com/tensorflow/tensorflow/commit/bdb2967a298236e24011405907cd19737386934e", "Is this resolved?", "@aselle No, it is not resolved. Working with the mobilenet quantization, still need to add the argument --copt=\"-DTENSORFLOW_DISABLE_META\" when building the \"libtensorflow_inference.so\" file. But this made the performance slower. When using the nightly-android builds, the result is different between mobile and PC.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Anyone having the low accuracy problem on iOS? I use `transform_graph` on a retrain model (based on Inception v3) and `label_image` works fine on the quantized model `transform_dog_retrained.pb` (same accuracy as with the pre-transformed model `dog_retrained.pb`), but the iOS app gives bad recognition results. The iOS app gives the same results as `label_image` when using the retrained (but before transformed) model `dog_retrained.pb`.\r\n\r\nCommands to retrain, transform, and call label_image are as follows:\r\n\r\n```\r\npython tensorflow/examples/image_retraining/retrain.py   \\\r\n--model_dir=/tf_files/inception-v3   \\\r\n--output_graph=/tf_files/retrained_models/dog_retrained.pb   \\\r\n--output_labels=/tf_files/retrained_models/dog_retrained_labels.txt   \\\r\n--image_dir ~/Downloads/dog_images   \\\r\n--bottleneck_dir=/tf_files/dogs_bottleneck\r\n```\r\n\r\n```\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph  \\\r\n --in_graph=/tf_files/retrained_models/dog_retrained.pb   \\\r\n--out_graph=/tf_files/retrained_models/transform_dog_retrained.pb   \\\r\n--inputs='Mul'   --outputs='final_result'   \\\r\n--transforms='\r\n  add_default_attributes\r\n  strip_unused_nodes(type=float, shape=\"1,299,299,3\")\r\n  fold_constants(ignore_errors=true)\r\n  fold_batch_norms\r\n  fold_old_batch_norms\r\n  quantize_weights\r\n  strip_unused_nodes\r\n  sort_by_execution_order'\r\n```\r\n```\r\nbazel-bin/tensorflow/examples/label_image/label_image \\\r\n--graph=/tf_files/retrained_models/transform_dog_retrained.pb \\\r\n--image=/tf_files/lab.jpg --input_layer=Mul --output_layer=final_result \\\r\n--labels=/tf_files/retrained_models/dog_retrained_labels.txt\r\n```\r\n\r\nAnyone has any idea how to fix the iOS issue? Btw, if I use the older `bazel build tensorflow/python/tools:strip_unused` then `python tensorflow/tools/quantization/quantize_graph.py`, the quantized model generated this way has good results (same accuracy as `label_image`) on iOS. But I hope to use the new `transform_graph` method instead of the old way.\r\n\r\nWhat's the right way to use `transform_graph` so the generated model works on iOS with about the same accuracy as using`label_image`?\r\n\r\nThanks!", "@petewarden, do we have someone who can take a look at this since maciek is no longer working on it.\r\n", "Can @petewarden or someone please take a look? Also, a related problem: I used one quantized MobileNet model to do the retraining:\r\n```\r\npython tensorflow/examples/image_retraining/retrain.py \\\r\n  --output_graph=/tf_files/dog_retrained_mobilenet10_224.pb \\\r\n  --output_labels=/tf_files/dog_retrained_labels_mobilenet.txt \\\r\n  --image_dir ~/Downloads/Images \\\r\n  --bottleneck_dir=/tf_files/dogs_bottleneck_mobilenet \\\r\n  --architecture mobilenet_1.0_224_quantized\r\n```\r\nand then using the label_image script to verify the model's accuracy, which looks pretty good:\r\n```\r\nbazel-bin/tensorflow/examples/label_image/label_image \\\r\n--graph=/tf_files/dog_retrained_mobilenet10_224.pb \\\r\n--image=/tmp/lab1.jpg \\\r\n--input_layer=input \\\r\n--output_layer=final_result \\\r\n--labels=/tf_files/dog_retrained_labels_mobilenet.txt \\\r\n--input_height=224 \\\r\n--input_width=224 \\\r\n--input_mean=128 \\\r\n--input_std=128\r\n```\r\nBut when I use the retrained model in both iOS and Android, I have to set the threshold to 0.01 in order to see top recognition results, which look ok except the very low confidence values - for example: \r\n```\r\nPredictions: 41 0.0261  n02099712 labrador retriever\r\n60 0.024  n02106166 border collie\r\n34 0.0227  n02099849 chesapeake bay retriever\r\n31 0.0188  n02088238 basset\r\n42 0.0174  n02091032 italian greyhound\r\n``` \r\n\r\nThe non-quantized version of a retrained MobileNet model (using `--architecture mobilenet_1.0_224` on retraining) shows good top results with high confidence values for the results in iOS and Android - for example:\r\n```\r\n\r\nPredictions: 5 0.328  n02091134 whippet\r\n41 0.307  n02099712 labrador retriever\r\n20 0.0988  n02088364 beagle\r\n105 0.0815  n02087394 rhodesian ridgeback\r\n32 0.0209  n02090379 redbone\r\n```\r\n\r\nI'm using TensorFlow 1.4 and the iOS `simple` example and the Android `TF Classify` example.\r\n\r\nAny ideas on how to fix this and make retrained quantized MobileNet models work in iOS and Android? Thanks!\r\n\r\n\r\n", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "We're now moving over to TF Lite as the recommended way to run quantized models, partly because it is less complex to integrate eight-bit with than classic TensorFlow. We have a demo of running the MobileNet image classification network on Android, and there should soon be full documentation on training it. I'm going to close this bug as not likely to be fixed, since our efforts will be focused on the TF Lite path.", "Yeah, well TF Lite is nowhere near where we need it to be to run our graphs. So what's the recommended solution until we can convert graphs to TF Lite (trying to convert gives errors for unsupported ops)? `-DTENSORFLOW_DISABLE_META`?"]}, {"number": 11181, "title": "The 2015 Inception checkpoint gives incorrect results", "body": "### Problems\r\n- The 2015 Inception checkpoint file called `classify_image_graph_def.pb`, which can be found in [`inception-2015-12-05.tgz`](http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz), gives weird results.\r\n- The [tutorial on quantization](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/quantization.md#how-can-you-quantize-your-models) (together with [this one](https://www.tensorflow.org/performance/quantization)) needs corrections.\r\n\r\n### Explanation\r\nI originally wanted to try [this tutorial on quantization](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/quantization.md#how-can-you-quantize-your-models) to quantize the model and then check the result with `label_image`.\r\n\r\nUsing the same name convention as in the tutorial, the quantize step will quantize the original graph, called `classify_image_graph_def.pb`, into a quantized one, called `quantized_graph.pb`. So before checking the latter, I wanted to first `label_image` using this `classify_image_graph_def.pb` and [this sample image](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/data/grace_hopper.jpg).\r\n\r\nThe result should be the same as in [this `label_image`'s README](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/), as shown below. This example, however, uses a newer pre-trained graph, `inception_v3_2016_08_28_frozen.pb`, from [this compressed file](https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz). In addition to the pb file, the compressed tarball also contains `imagenet_slim_labels.txt`, containing 1,001 lines of label names, e.g. \"military uniform\".\r\n\r\n```bash\r\n# expected result\r\nmilitary uniform (653): 0.834306\r\nmortarboard (668): 0.0218692\r\nacademic gown (401): 0.0103579\r\npickelhaube (716): 0.00800814\r\nbulletproof vest (466): 0.00535088\r\n```\r\n\r\nSo, to check `classify_image_graph_def.pb`, I looked at [the tutorial](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/quantization.md#how-can-you-quantize-your-models) and changed the `--graph` argument from `/tmp/quantized_graph.pb` to `/tmp/classify_image_graph_def.pb`. However, the `--labels` argument also needs to be fixed, because it suggests using `imagenet_synset_to_human_label_map.txt` (from [`inception-2015-12-05.tgz`](http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz)), which actually has 21,842 lines of (synset id, label name). So, **after downloading `imagenet_slim_labels.txt` by following [this `label_image`'s README](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/)**, fix the `--labels` argument by either leaving it out (because it's the default if following those steps) or explicitly calling the labels file.\r\n\r\nThe code to run `label_image` for `classify_image_graph_def.pb` becomes:\r\n\r\n```bash\r\nbazel build tensorflow/examples/label_image:label_image\r\nbazel-bin/tensorflow/examples/label_image/label_image \\\r\n--image=tensorflow/examples/label_image/data/grace_hopper.jpg \\\r\n--graph=/tmp/classify_image_class_def.pb \\\r\n--input_width=299 \\\r\n--input_height=299 \\\r\n--input_mean=128 \\\r\n--input_std=128 \\\r\n--input_layer=\"Mul:0\" \\\r\n--output_layer=\"softmax:0\"\r\n```\r\n\r\nAnd here is the result.\r\n\r\n```bash\r\n# classify_image_graph_def.pb\r\ntoyshop (866): 0.684115\r\nshower cap (794): 0.0394605\r\nwarplane (896): 0.0219743\r\ntape player (849): 0.0138034\r\nzucchini (940): 0.013588\r\n```\r\n\r\n### Differences between checkpoint files\r\n\r\n_There is also another Inception v3 checkpoint file, `inception-v3-2016-03-01.tar.gz`, according to [this tutorial on how to fine-tune Inception](https://github.com/tensorflow/models/tree/master/inception#how-to-fine-tune-a-pre-trained-model-on-a-new-task)!_\r\n\r\nSo I used [`import_pb_to_tensorboard.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/import_pb_to_tensorboard.py) to check why the two checkpoints are different.\r\n\r\nBesides the difference in implementation (e.g. how each layers are named), the most important difference I noticed is in the last FC layer. While `inception_v3_2016_08_28_frozen.pb` maps from 2048 to 1001 (the 0th class is \"dummy\"), **`classify_image_graph_def.pb` maps from 2048 to 1008!** I'm not sure what 1008 means, but maybe it was trained with a different set of labels in a different order?\r\n\r\nThe naming difference also makes things a little more complicated. When calling `label_image` with `classify_image_graph_def.pb`, I need to specify `--input_layer=\"Mul:0\" --output_layer=\"softmax:0\"` because these are the names of the input and output layers. However, with the new checkpoint that works, we don't have to specify these two arguments because `input_layer = \"input\"` and `output_layer = \"InceptionV3/Predictions/Reshape_1\"` are already hard-coded as the default values in the implementations ([C++](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc) and [python](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/label_image.py)).\r\n\r\n### Related issues\r\nThere are question related to the differences between these two checkpoints and why they give different performances on certain tasks. I've found these issues in the [tensorflow/models](https://github.com/tensorflow/models) repo: [#1314](https://github.com/tensorflow/models/issues/1314), [#1316](https://github.com/tensorflow/models/issues/1316).\r\n\r\n### Quantization\r\nNow, back to quantization ... Following the current code in the [tutorial](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/quantization.md) to quantize `classify_image_graph_def.pb` into `quantized_graph.pb` will give an error. This is because the sample code is missing an `--inputs` argument, compared to [this tutorial on `graph_transforms`](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms#eight-bit-calculations). **The `transform_graph` example needs `--inputs=\"Mul\"`.** Then it will work, i.e. predicting \"toyshop\" as the top choice.\r\n\r\nActually, this quantization process works with the new `inception_v3_2016_08_28_frozen.pb` -- predicting that the picture is \"military uniform\" by both original and quantized graphs.\r\n\r\n### Questions\r\n- What are the actual differences between the three checkpoints? Which one should we use? Any clarification would be really helpful. When different tutorials refer to different checkpoint files, at first I thought all of them can be used interchangeably. As it turns out, they are actually not the same and this has caused a lot of confusion.\r\n- Why is the last layer in `classify_image_graph_def.pb` from 2015 have 1008 nodes, not 1001?\r\n- Is there a way to make `classify_image_graph_def.pb` work, following the tutorial? Did I miss any arguments or other settings?\r\n- _Somewhat unrelated question_: The [tutorial](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/quantization.md) on quantization mentioned above seems to be the same as [this one on tensorflow.org](https://www.tensorflow.org/performance/quantization), except for the last few commits. Therefore, the tutorial on the website is not up-to-date. Are they supposed to be the same?\r\n- How should the tutorials on quantization be updated? Personally, I think it can be fixed to use the new checkpoint, then adjust the example codes accordingly.\r\n\r\nThank you!", "comments": ["@tfboyd can you comment or redirect? Thanks.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Apologies for taking months, that guide is not mine and I do not know much about it.  I am going to close this as it has been a long time and tflite has other suggestions for quantization I believe.  It is very possible to guide is still difficult to follow.  @aselle and @petewarden incase they want to reopen or have an idea.  "]}, {"number": 11180, "title": "Cannot build TensorFlow on macOS 10.12", "body": "When I trying to build TensorFlow with avx, avx2, fma, sse4,1 sse4,2, it shown that\r\n```\r\nclang: error: no such file or directory: 'msse4.1'\r\nclang: error: no such file or directory: 'msse4.2'\r\n```\r\nclang doesn't support sse4.1 and sse4.2?\r\nAnd I can not change the bazel compiler to gcc anyway", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "Building on OS X with optimizations totally works (sans GPU of course). Something like:\r\n\r\nbazel build  -c opt --copt=-mavx --copt=-mfma --copt=-mavx2 //tensorflow:libtensorflow_cc.so", "@vade when I trying to execute\r\n`bzael build -c opt --copt=-marx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2 //tensorflow:libtensorflow_cc.so`\r\nI always got this error.", "@ali01 \r\nSorry, this is the first time I post an issue.\r\n```\r\nhardware: mid-2015 Macbook Pro with R370X\r\nos: macOS Sierra 10.12\r\ntf version: 1.2\r\n```\r\n```\r\n$ ./configure\r\nWARNING: ignoring http_proxy in environment.\r\n...............\r\nYou have bazel 0.5.2-homebrew installed.\r\nPlease specify the location of python. [Default is /XXX/anaconda/bin/python]:\r\nFound possible Python library paths:\r\n  /XXX/anaconda/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/XXX/anaconda/lib/python3.6/site-packages]\r\n\r\nUsing python library path: /XXX/anaconda/lib/python3.6/site-packages\r\nDo you wish to build TensorFlow with MKL support? [y/N] n\r\nNo MKL support will be enabled for TensorFlow\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: -march=-mavx2,-mfma,-mavx,-msse4.1,-msse4.2\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] n\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] n\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] n\r\nNo XLA JIT support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with VERBS support? [y/N] n\r\nNo VERBS support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] n\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] n\r\nNo CUDA support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with MPI support? [y/N] n\r\nMPI support will not be enabled for TensorFlow\r\nConfiguration finished\r\n\r\n$ bazel build --config=opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=msse4.1 --copt=msse4.2 //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: ignoring http_proxy in environment.\r\nWARNING: /XXX/Downloads/tensorflow-master/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': Use SavedModel Builder instead.\r\nWARNING: /XXX/Downloads/tensorflow-master/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': Use SavedModel instead.\r\nINFO: Found 1 target...\r\nERROR: /private/var/tmp/_bazel_XXX/18bc397ce425ce2714a2948ab0a1ec62/external/farmhash_archive/BUILD.bazel:19:1: C++ compilation of rule '@farmhash_archive//:farmhash' failed: wrapped_clang failed: error executing command external/local_config_cc/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 38 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nclang: error: no such file or directory: 'msse4.1'\r\nclang: error: no such file or directory: 'msse4.2'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 3.786s, Critical Path: 0.27s\r\n```", "if you are going to use tf on the same machine, why not just use `-march=native`, or our convenience option, `--config=opt`?", "I suspect thats the issue, and the comma's etc are not parsed by the config script correctly. ", "@wwh2259253, would you please try @gunan's suggestion, and let us know whether it helps?", "I was experiencing numerous issues including this one on macOS 10.12.6 using ```bazel 0.5.2-homebrew``` and finally solved them all by installing the latest Xcode v8.3.3 then running:\r\n\r\n```bazel clean --expunge```\r\n```./configure``` (accepting the default ```-march=native``` option)\r\n```bazel build --config=opt --xcode_version=8.3.3 --macos_sdk_version=10.12 //tensorflow/tools/pip_package:build_pip_package```", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 11179, "title": "Remove type constraint on shape_invariants", "body": "This PR fixes the unnecessarily strict type checking addressed in issue #11115. Turning type checking off for the shape_invariants parameter in tf.while_loop will allow the parameter to be more easily used when, for example, trying to provide an invariant for an LSTMStateTuple variable.", "comments": ["Can one of the admins verify this patch?", "@TheButlah ping regarding @drpngx's request for tests? ", "I'll try to write some tests for it eventually; this is my first open source contribution and I need to read up on how to do that exactly.", "Any updates @TheButlah ?  Thanks for the contribution!", "@TheButlah ping?", "Closing this out due to being stalled for a while. Feel free to reopen or send a new pull request if you'd like to submit this change with the test."]}]