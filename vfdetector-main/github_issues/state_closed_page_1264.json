[{"number": 15220, "title": "ResidualWrapper and HighwayWrapper require rnn inputs' last dimension to be equal to num_units", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: -\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**:  v1.4.0-rc1-11-g130a514 1.4.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: -\r\n- **GCC/Compiler version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: - \r\n- **GPU model and memory**: - \r\n- **Exact command to reproduce**:\r\n~~~python\r\nfrom tensorflow.contrib import rnn as rnn_cell\r\nfrom tensorflow.python.ops import rnn\r\nimport tensorflow as tf\r\n\r\nrnn_inputs = tf.random_uniform([10, 10, 10])\r\ncell = rnn_cell.LSTMCell(128)\r\ncell = rnn_cell.HighwayWrapper(cell) # rnn_cell.ResidualWrapper(cell)\r\n_, _ = rnn.dynamic_rnn(cell, rnn_inputs, dtype=tf.float32)\r\n~~~\r\n\r\n### Problem\r\n`rnn_cell.ResidualWrapper` and `rnn_cell.HighwayWrapper` throw an exception when rnn inputs' last dimension is not equal to `num_units`. Without wrappers, the input tensor can be different from `num_units`.\r\nWhat's the reason for the different behaviour? Is it intended?\r\n\r\n### Full Traceback\r\nhttps://pastebin.com/YvTb3WM3", "comments": ["The ResidualWrapper adds the inputs to the outputs of the RNNCell it wraps.  How do you add if the input and output depths are different?", "Yeah, I guess there is not much that can be done without adding more parameters. I usually just add a linear transformation that converts depth into num_units. Sorry for bothering."]}, {"number": 15219, "title": "tf.while_loop and tf.foldl do not support second order gradients", "body": "The example\r\n```python\r\nimport tensorflow as tf\r\nx = tf.Variable(1.)\r\nA = tf.Variable(tf.ones((3,3))) \r\ncost = tf.trace(tf.foldl(tf.matmul,tf.stack([x*A for _ in range(3)])))\r\ntf.gradients(tf.gradients(cost, A), x)  \r\n# TypeError: Second-order gradient for while loops not supported.\r\n```\r\nillustrates that despite applying `tf.foldl` to a static list, the internal implementation via while loops leads to a type error. The problem disappears if the fold operation is carried out manually using a for loop. While implementing `foldl` using the while loop clearly makes the operation  more widely applicable, it seems problematic if syntactic sugar can lead to code that has qualitative differences from a naive implementation using a static loop. I cannot help but wonder whether `foldl` could be more efficient in the static case as well, although that is more of a conjecture.\r\n\r\nI think it would be nice if `foldl` (and other while loop derivatives) had a keyword that enabled or disabled the \"dynamic mode\" using while, or if, at the very least, the TypeError would occur at the `foldl`operation so that the error is easier to trace.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nCustom code.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04 LTS\r\n- **TensorFlow installed from (source or binary)**:\r\npip install.\r\n- **TensorFlow version (use command below)**:\r\nv1.4.0-rc1-11-g130a514 1.4.0\r\n- **Python version**:\r\n3.5.4 \r\n- **Bazel version (if compiling from source)**:\r\nNot applicable.\r\n- **GCC/Compiler version (if compiling from source)**:\r\nNot applicable.\r\n- **CUDA/cuDNN version**:\r\nDid not use CUDA.\r\n- **GPU model and memory**:\r\nDid not use GPU.\r\n- **Exact command to reproduce**:\r\n```python\r\nimport tensorflow as tf\r\nx = tf.Variable(1.)\r\nA = tf.Variable(tf.ones((3,3))) \r\ncost = tf.trace(tf.foldl(tf.matmul,tf.stack([x*A for _ in range(3)])))\r\ntf.gradients(tf.gradients(cost, A), x)  \r\n# TypeError: Second-order gradient for while loops not supported.\r\n```", "comments": ["/CC @skye @mrry @ebrevdo ", "updated title.  i think we have an existing issue that this is a duplicate of?", "We don't yet have a good story for 2nd order gradients of tf.while_loop; though @alextp has given this a little bit of thought in the past.", "Having a \"static\" while loop option (i.e. unrolled) that we could propagate to derivative ops like foldl is an interesting idea. This would obviously not always be possible, e.g. in the case of foldl, the shape of the input tensor must be known (or at least the first dimension). So I'm worried that such an option could potentially be confusing since it wouldn't always work and it isn't obvious when to use one or the other.\r\n\r\nMaybe someone could add static versions of these ops to contrib for now, instead of adding it as an option to the existing ops. Then it would still be easy to switch between the two, and leaves us more room to improve the existing version moving forward. I'm gonna mark this contributions welcome for now, but if anyone wants to take this on we should discuss more before creating a PR in case people have other ideas. I'm also not sure how many people would use these new ops.\r\n\r\nre: catching the error in foldl, unfortunately this will be hard to implement, as we don't \"know\" it's a foldl by the time we hit the error. We could maybe mention foldl etc in the error message though.", "@skye: would there be a definite performance boost from unrolling? I seem to recall that `tf.while_loop` incurs a significant overhead cost.\r\n\r\nI think the main point of this issue is that the problem is hard to solve if you come across it and you don't happen to know that `foldl` and the other higher order functions use `tf.while_loop` internally (which doesn't even seem to be mentioned in the docs). \r\n\r\n", "Using while_loop should have similar performance characteristics as an unrolled loop (and makes your graph smaller). I think there have been issues where people make very small body functions, and then there may be some noticeable overhead.\r\n\r\nI agree the foldl situation is less than ideal. Thanks for filing this issue, it's very clear and at the very least will help others who run into this problem. I just don't think there's a quick fix to improve the error message unfortunately. Please feel free to file a PR clarifying the docs!", "Hi @Bonnevie ! It has been resolved in latest version [ (TF 2.8 )](https://colab.sandbox.google.com/gist/mohantym/21730ed71196ad40b2d16571729d9aca/untitled274.ipynb#scrollTo=uSm_YjKoJvI_) now . Can we move this issue to closed status now? ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 15218, "title": "stupid question but please help me to answer this", "body": "I have a problem with parallel computing. I know that CUDA or pyCUDA can do such thing like 3D convolution in parallel. \r\nI think about using tensorflow GPU by calling tf.nn.conv3d(input, filter), does it uses build in functions in CUDA to operate 3D convolution with parallel computing or there are somethings else? \r\nI am not familiar with C++ or even with pyCuda I am not sure how to implement 3D convolution with it.\r\n\r\nSame question with all type of other implement with tensorflow GPU.\r\n\r\nBests,   ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hi @32nguyen, we appreciate you posting your question but Github is only for bugs and feature requests. This question is better suited for StackOverflow which the TensorFlow team also monitors under the TensorFlow tag."]}, {"number": 15217, "title": "pip.sh: unify the way virtualenv is invoked", "body": "between python3.6 and the other versions\r\n\r\n\"python -m\" also seems to be a more robust way of calling virtualenv\r\nthan relying on the virtualenv command on path.", "comments": ["@tensorflow-jenkins test this please"]}, {"number": 15216, "title": "When data become large,parition variables can not initialized successfully", "body": "i use tensorflow to distributed trainning models,  i use the partition valriables to store an array data, when the data is not so bigger, everything looks ok,but when the array data become larger, when the session initialize, the partition variables can not  initialized and the session will wait util time out.\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request. the feat_info can initialize successfully, but the adj_info cannot initialized. the adj_info is larger than feat_info\r\n### Source code / logs\r\ni use ps_num = 4, worker_num =4 and i also try some other distributed config, like ps_num=1, worker_num=4, the result is the same\r\nsource code:\r\n    with tf.device(tf.train.replica_device_setter(\r\n        worker_device=\"/job:worker/task:%d\" % task_id,\r\n        cluster=cluster_spec)):\r\n      \r\n      feat_info = tf.get_variable(\"feature_info\", (len(id_map),FLAGS.features_column), tf.float32, trainable=False, partitioner=tf.fixed_size_partitioner(num_workers))\r\n      adj_info = tf.get_variable(\"adj_info\", (len(id_map),FLAGS.max_degree), tf.int64, trainable=False, partitioner=tf.fixed_size_partitioner(num_workers))\r\n     \r\n      with tf.device('/job:worker/task:%d' %task_id):\r\n          adj_local = tf.Variable(tf.constant(minibatch.adj, dtype=tf.int64), trainable=False, name=\"adj_local\", collections=[tf.GraphKeys.LOCAL_VARIABLES])\r\n          feat_local = tf.Variable(tf.constant(features, dtype=tf.float32), trainable=False, name=\"feat_local\", collections=[tf.GraphKeys.LOCAL_VARIABLES])\r\n     \r\n      length, begin, end = split_node_by_task(len(id_map), task_id, num_workers)\r\n      adj = tf.nn.embedding_lookup(adj_info, [x for x in range(begin, end)])\r\n      adj = adj_local\r\n      \r\n      feat = tf.nn.embedding_lookup(feat_info, [x for x in range(begin, end)])\r\n      feat = feat_local\r\n\r\nlog:\r\n2017-12-08 23:54:17.377290: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session c2b3ba9b700261ba with config: \r\nINFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  None, ready: Variables not initialized: adj_info/part_0, adj_info/part_1, adj_info/part_2, adj_info/part_3, adj_info/part_4, adj_info/part_5, adj_info/part_6, adj_info/part_7, adj_info/part_8, adj_info/part_9, adj_info/part_10, adj_info/part_11, adj_info/part_12, adj_info/part_13, adj_info/part_14, adj_info/part_15\r\n2017-12-09 00:00:35.637019: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session f35fcf332e3908ec with config: \r\nINFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  None, ready: Variables not initialized: adj_info/part_0, adj_info/part_1, adj_info/part_2, adj_info/part_3, adj_info/part_4, adj_info/part_5, adj_info/part_6, adj_info/part_7, adj_info/part_8, adj_info/part_9, adj_info/part_10, adj_info/part_11, adj_info/part_12, adj_info/part_13, adj_info/part_14, adj_info/part_15\r\nand it will alway waiting adj_info to initialize", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler Have I written custom code\r\nyes\r\nOS Platform and Distribution\r\nlinux platform \r\nTensorFlow installed from\r\nN/A\r\nTensorFlow version\r\nr1.4\r\nBazel version\r\nCUDA/cuDNN version\r\nno gpu\r\nGPU model and memory\r\nno gpu\r\nExact command to reproduce\r\nN/A", "Did you call initialize_variables? Where is the session run call? Do you have enough memory on your machine for the larger variable sizes?", "@andydavis1  i use monitored_session or supervisor to do the initialize_variables, when data is not so large, it will initialize successfully, my machine have more than 300GB memory is enough for the larger variable", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "still an issue,but i do not know how to change the label", "@tensorflowbutler  nobody help me to solve the issue and it still need to solve, but i do not know how to change the label status, please help me", "@andydavis1 @aselle @ry @jmhodges  help me how can i remove the label?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "This is a duplicate of #15835."]}, {"number": 15215, "title": "Tensorflow - Unable to import frozen graph with batchnorm : uninitialized value batch_normalization/moving_mean", "body": "I am trying to freeze in a pbtxt file a checkpoint containing batchnorm layers (ubuntu, python 2.7, tf 1.1.0).\r\n\r\ncontext : \r\n**Have I written custom code**\r\nYes, see below\r\n\r\n**OS Platform and Distribution**\r\nDocker with Ubuntu 14.04\r\n\r\n**TensorFlow installed from**\r\npip installer\r\n\r\n**TensorFlow version**\r\ntensorflow and tensorflow-gpu 1.1.0\r\n\r\n**Bazel version**\r\nN/A\r\n\r\n**CUDA/cuDNN version**\r\nCuda 8, CUDNN 5.1\r\n\r\n**GPU model and memory**\r\nNvidia titan-x * 2, 12Go Ram each\r\n\r\n**Exact command to reproduce**\r\n\r\nFor this, following these posts and issues :\r\n\r\nhttps://github.com/davidsandberg/facenet/issues/161\r\n\r\nhttps://github.com/davidsandberg/facenet/pull/172/commits/0f3ece502550714c91056f3a8630ce8c037f613f\r\n\r\nI use this function:\r\n\r\n    freeze_and_prune_graph(model_path_and_name, output_file=None):\r\n\t\t\"\"\"\r\n\t\tfreezes a model trained and saved by the trainer by :\r\n\t\t    - extracting the trainable variables between input_node and output_node\r\n\t\t    - turning them to constants\r\n\t\t    - changing the 1rst dim of input_node to None\r\n\t\t    -saving the resulting graph as a single .pb file\r\n\r\n\t\t:param model_path_and_name: must finish by .ckpt, and the checkpoint must be composed of\r\n\t\t3+ files : .ckpt.index, .ckpt.meta, and .ckpt.data-0000X-of-0000Y\r\n\r\n\t\t:param model_path_and_name: path to the trained model\r\n\t\t:param output_file: file to save to. If None, model_path_and_name.[-ckpt][+pb]\r\n\t\t:return: None\r\n\t\t\"\"\"\r\n\t\tconfig_proto = tf.ConfigProto(allow_soft_placement=True)\r\n\r\n\t\twith tf.Session(config=config_proto) as sess:\r\n\t\t    new_saver = tf.train.import_meta_graph(model_path_and_name + '.meta', clear_devices=True)\r\n\t\t    tf.get_default_session().run(tf.global_variables_initializer())\r\n\t\t    tf.get_default_session().run(tf.local_variables_initializer())\r\n\t\t    new_saver.restore(sess, model_path_and_name)\r\n\r\n\t\t    # get graph definition\r\n\t\t    gd = sess.graph.as_graph_def()\r\n\t\t    # fix batch norm nodes\r\n\t\t    for node in gd.node:\r\n\t\t        if node.op == 'RefSwitch':\r\n\t\t            node.op = 'Switch'\r\n\t\t            for index in xrange(len(node.input)):\r\n\t\t                if 'moving_' in node.input[index]:\r\n\t\t                    node.input[index] = node.input[index] + '/read'\r\n\t\t        elif node.op == 'AssignSub':\r\n\t\t            node.op = 'Sub'\r\n\t\t            if 'use_locking' in node.attr: del node.attr['use_locking']\r\n\t\t        elif node.op == 'AssignAdd':\r\n\t\t            node.op = 'Add'\r\n\t\t            if 'use_locking' in node.attr: del node.attr['use_locking']\r\n\r\n\t\t    # tf.get_collection() returns a list. In this example we only want the\r\n\t\t    input_node = sess.graph.get_tensor_by_name('input_node:0')\r\n\t\t    new_shape = [None] + input_node.get_shape().as_list()[1:]\r\n\r\n\t\t    trainables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\r\n\t\t    new_graph_def = tf.graph_util.convert_variables_to_constants(sess, gd, [\"output_node\"],\r\n\t\t                                                                 variable_names_whitelist=[t.name[:-2] for t in trainables] + ['output_node'])\r\n\r\n\t\t    for node in new_graph_def.node:\r\n\t\t        if node.name == 'input_node':\r\n\t\t            node.attr['shape'].CopyFrom(attr_value_pb2.AttrValue(shape=tf.TensorShape(new_shape).as_proto()))\r\n\t\t            break\r\n\r\n\t\t    with tf.gfile.GFile(output_file, \"wb\") as f:\r\n\t\t        f.write(new_graph_def.SerializeToString())\r\n\t\t    print(\"{0} / {1} ops in the final graph.\".format(len(new_graph_def.node), len(sess.graph.as_graph_def().node)))\r\n\r\nThis goes well and creates the pbtxt file with the following output :\r\n\r\n> Converted 201 variables to const ops.\r\n5287 / 41028 ops in the final graph.\r\n\r\nI then try to load the pbtxt model using this function :\r\n\r\n    def load_frozen_graph(frozen_graph_file):\r\n\t    \"\"\"\r\n\t    loads a graph frozen via freeze_and_prune_graph and returns the graph, its input placeholder and output tensor\r\n\r\n\t    :param frozen_graph_file: .pb file to load\r\n\t    :return: tf.graph, tf.placeholder, tf.tensor\r\n\t    \"\"\"\r\n\t    # We load the protobuf file from the disk and parse it to retrieve the\r\n\t    # unserialized graph_def\r\n\t    with tf.gfile.GFile(frozen_graph_file, \"rb\") as f:\r\n\t        graph_def = tf.GraphDef()\r\n\t        graph_def.ParseFromString(f.read())\r\n\r\n\t    # Then, we can use again a convenient built-in function to import a graph_def into the\r\n\t    # current default Graph\r\n\t    with tf.Graph().as_default() as graph:\r\n\t        tf.import_graph_def(\r\n\t            graph_def,\r\n\t            input_map=None,\r\n\t            return_elements=None,\r\n\t            name=\"prefix\",\r\n\t            op_dict=None,\r\n\t            producer_op_list=None\r\n\t        )\r\n\r\n\t    input_images_placeholder = graph.get_tensor_by_name('prefix/input_node:0')\r\n\t    input_phase_placeholder = None\r\n\t    try:\r\n\t        input_phase_placeholder = graph.get_tensor_by_name('prefix/phase:0')\r\n\t    except KeyError:\r\n\t        pass\r\n\t    output = graph.get_tensor_by_name('prefix/output_node:0')\r\n\r\n\t    return graph, input_images_placeholder, input_phase_placeholder, output\r\n\r\nusing the following snippet:\r\n\r\n    graph, input_images_placeholder, is_training_placeholder, output = load_frozen_graph(model_pbtxt)\r\n\tsess = tf.Session(config=tf_config, graph=graph)\r\n\tfeed_dict = {input_images_placeholder: prepared_input}\r\n\tif is_training_placeholder is not None:\r\n\t    feed_dict[is_training_placeholder] = False\r\n\tret = sess.run([output], feed_dict=feed_dict)\r\n\r\nThis, however, leads to the following error:\r\n\r\n> FailedPreconditionError (see above for traceback): Attempting to use uninitialized value prefix/conv0/BatchNorm/batch_normalization/moving_mean\r\n> [[Node: prefix/conv0/BatchNorm/batch_normalization/moving_mean/read = Identity[T=DT_FLOAT, _class=[\"loc:@prefix/conv0/BatchNorm/batch_normalization/moving_mean\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](prefix/conv0/BatchNorm/batch_normalization/moving_mean)]]\r\n\t [[Node: prefix/output_node/_381 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_2447_prefix/output_node\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\nFollowing the question :\r\nhttps://stackoverflow.com/questions/36007883/tensorflow-attempting-to-use-uninitialized-value-in-variable-initialization\r\nI tried initializing variables:\r\n\r\n    graph, input_images_placeholder, is_training_placeholder, output = load_frozen_graph(model_pbtxt)\r\n\tsess = tf.Session(config=tf_config, graph=graph)\r\n\tinit = [tf.global_variables_initializer(), tf.local_variables_initializer()]\r\n\tsess.run(init)\r\n\r\n\tfeed_dict = {input_images_placeholder: prepared_input}\r\n\tif is_training_placeholder is not None:\r\n\t    feed_dict[is_training_placeholder] = False\r\n\tret = sess.run([self.output], feed_dict=feed_dict)\r\n\r\nThis, however, changes the error to:\r\n\r\n> ValueError: Fetch argument <tf.Operation 'init' type=NoOp> cannot be interpreted as a Tensor. \r\n(Operation name: \"init\" op: \"NoOp\" is not an element of this graph.)\r\n\r\nwhich seems to show that there is no variable that needs to be initialized.\r\n\r\nWhat am I missing ? How to I freeze and reload the relevant values of a batch_normalization layer ?\r\n\r\nPS: I do realize that this might better be on stackoverflow, but I posted there first and got no answer in 2 weeks:\r\nhttps://stackoverflow.com/questions/47434139/tensorflow-unable-to-import-frozen-graph-with-batchnorm-uninitialized-value", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Post is updated", "Thanks for your post. Unfortunately, the issues tracker is for tracking bugs and feature requests -- not for escalating support requests. "]}, {"number": 15212, "title": "lookup.index_table_from_tensor() emits an error in eager mode when invoked more than once.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: +\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS\r\n- **TensorFlow installed from (source or binary)**: tf-nightly\r\n- **TensorFlow version (use command below)**: 1.5.0-dev20171206\r\n- **Python version**: 3\r\n- **Bazel version (if compiling from source)**: -\r\n- **GCC/Compiler version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: -\r\n- **GPU model and memory**: -\r\n- **Exact command to reproduce**: \r\n~~~python\r\nfrom tensorflow.python.ops import lookup_ops\r\nimport tensorflow.contrib.eager as tfe\r\ntfe.enable_eager_execution()\r\n\r\ninpt = ['1611', '1612', '1613', '1615', '1616', '1617', '1618', '1619', '1621']\r\n\r\na = lookup_ops.index_table_from_tensor(inpt, name='a')\r\nb = lookup_ops.index_table_from_tensor(inpt, name='b')\r\n~~~\r\n\r\n### Problem\r\nWhen the eager execution is enabled, the following error occurs:\r\n~~~console\r\nFailedPreconditionError: Table already initialized. [Op:InitializeTableV2] name: string_to_index/hash_table/string_to_index/hash_table//string_to_index/hash_table/table_init//\r\n~~~\r\n\r\n### Source code / logs\r\nFull trace:\r\nhttps://pastebin.com/GsKBjyV6\r\n\r\n### Question\r\nIs there a way to specify more than one lookup table (a `shared_name` or a special scope)?", "comments": ["Actually, exposing a `shared_name` var, [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/lookup_ops.py#L1095), and assigning explicitly a unique string solves the problem for the eager execution mode.\r\nSolution:\r\n~~~python\r\nfrom tensorflow.python.ops import lookup_ops\r\nimport tensorflow.contrib.eager as tfe\r\ntfe.enable_eager_execution()\r\n\r\ninpt = ['1611', '1612', '1613', '1615', '1616', '1617', '1618', '1619', '1621']\r\n\r\na = lookup_ops.index_table_from_tensor(inpt, shared_name='a')\r\nb = lookup_ops.index_table_from_tensor(inpt, shared_name='b')\r\n~~~\r\nI do not know if a `shared_name` was intentionally abandoned, but I can submit a PR, assuming that adding a `shared_name` argument to `lookup_ops` is an acceptable workaround.", "@josh11b any idea what that could be?", "As it is now these functions are not really eager-compatible. When you set a shared_name they work by accident.\r\n\r\nWe should add a class-based version of them like we have been doing for other packages (like feature_column).\r\n", "@alextp I couldn't find the `feature_column` with eager support code you referred to. Can you send a link? \r\nIs a `shared_name` hack +/- stable?\r\nIt's just such a pure joy to debug big NLP models with eager, so I'd like to make at least a  somewhat proper support. \r\nbtw, are there any available public roadmap for eager compatibility?", "The shared name hack will continue to work.\n\nOn Dec 11, 2017 01:16, \"MtDersvan\" <notifications@github.com> wrote:\n\n> @alextp <https://github.com/alextp> I couldn't find the feature_column\n> with eager support code you referred to. Can you send a link?\n> Is a shared_name hack +/- stable?\n> It's just such a pure joy to debug big NLP models with eager, so I'd like\n> to make at least a somewhat proper support.\n> btw, are there any available public roadmap for eager compatibility?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15212#issuecomment-350599205>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxZwPhfbZ5O54ixEPP6JiLrkVXarRks5s_IJYgaJpZM4Q7FAz>\n> .\n>\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Closing since @alextp answered the question."]}, {"number": 15211, "title": "Deep MNIST - exit code 139 (interrupted by signal 11: SIGSEGV)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I have copied all the code lines from the tutorial Deep MNIST for Experts manually into a python file.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux, kernel: 4.13.12-1-ARCH\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})\r\n\r\n### Describe the problem\r\nI am trying to run the example code from the tutorial Deep MNIST for Experts by copying every line from the tutorial into a python file and running it. The first part of the script with the model with a single linear layer runs fine, but the second part with a multilayer convolutional network fails to run. When the last line of the script executes: `print('test accuracy %g' % accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))` I get the following error: `Process finished with exit code 139 (interrupted by signal 11: SIGSEGV)`. \r\n\r\nI tried to evaluate the accuracy with a batch of size 50 from the testing data instead of supplying mnist.test.images and mnist.test.labels and that worked fine.\r\nI was directed here from Stackoverflow because this was believed to be a bug [(Stackoverflow question).](https://stackoverflow.com/questions/47640511/tensorflow-deep-mnist-exit-code-139)\r\n\r\n### Source code / logs\r\nI am unfamiliar with gdb so if I have missed to provide any relevant debugging output I'll try to add it if you can tell me how to get it.\r\n\r\n[Source code](https://hastebin.com/ebidihiwey.py)\r\n[Stackframe](https://hastebin.com/jukuzejira.cpp)\r\n[Backtrace](https://hastebin.com/baqecavora.vbs)\r\n", "comments": ["Code 139 is attempt to access a virtual address which is not in your address space, which would support a memory allocation error, which could happen easily if you are on 32-bit system \r\n\r\nSIGSEGV is a hard crash of the process do to an invalid memory access in native code, it has no chance to signal that to the multiprocessing host.\r\nIt should throw an exception on its own, is that not happening?", "I am running it on a 64-bit system and the exit code message is the only error output I get from running the program. The exit code message is from running the python code through my IDE (Intellij). When I run it from a terminal with and without gdb I only get Segmentation fault (core dumped)", "To clarify, are you using a GPU version of TensorFlow? You listed a GPU, but put N/A for CUDA/cuDNN version. I cannot reproduce with either.\r\n\r\n/CC @benoitsteiner @rmlarsen, any ideas what the issue could be?", "No, I'm using a standard build of TensorFlow running on my CPU. Yes, I realise it might be confusing so I changed the GPU to N/A aswell. ", "+1  I'm getting a similar error on the below platform: Process finished with exit code 139 (interrupted by signal 11: SIGSEGV)\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.3\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version (use command below): 1.6\r\nPython version: 3.6.1\r\nBazel version (if compiling from source): N/A\r\nGCC/Compiler version (if compiling from source): N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: (my own script, using standard tensorflow ops: while, rnn, scatter_nd, etc)", "Hi ,are you solve this problem. i getting a similar error .\r\nbut i runing on GPU,using this code https://github.com/sampepose/flownet2-tf     \r\nwaiting for you write back Thanks @jfjallid ", "@jfjallid sorry for the gap in responding. Is this still an issue for you?", "@cy89 I do not now. I could not get around it so I stopped using tensorflow while awaiting response to this ticket and now I do not have it installed anymore.", "Please check with the latest version of TensorFlow. Feel free to reopen if the issues still persists.\r\nThanks!"]}, {"number": 15210, "title": "Incorrect Result from Add Function", "body": "### System information\r\n\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Linux Ubuntu 16.04.3 LTS\r\n- Bazel version: Not applicable \r\n- TensorFlow installed from binary\r\n- TensorFlow version: 1.4.0\r\n- Python version: 3.5.2\r\n- CUDA/cuDNN version: 8.0 / 6.0 for CUDA 8.0\r\n- GPU model and memory: GM107M [GeForce GTX 960M] 4GB\r\n- Exact command to reproduce:\r\n\r\nHere is a simple program to add: \r\n\r\nsession = tf.Session()\r\n\r\na = tf.placeholder(tf.float32)\r\n#print(\"first\")\r\nb = tf.placeholder(tf.float32)\r\n#print(\"second\")\r\nresult_node = tf.add(a,b)\r\n#print(\"starting\")\r\nx = session.run(result_node, {a:2.0, b: 3.5})\r\nprint(x)\r\n\r\nOutput should be 5.5, while I am getting 2.0 on 2 machines\r\n\r\n![screenshot from 2017-12-08 17-59-49](https://user-images.githubusercontent.com/19254286/33766026-af9682da-dc41-11e7-8f81-1ff054903deb.png)\r\n![screenshot from 2017-12-08 18-00-21](https://user-images.githubusercontent.com/19254286/33766027-afd8aba6-dc41-11e7-9a85-02f2764a9d67.png)\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nBazel version\nCUDA/cuDNN version", "Updated, thank you!", "Are you still stuck on this problem?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Hi, I've reinstalled the sdk on the same machine (provided config)- it still gives incorrect result on Add function. I've separately installed it on 2 different machines (1 Mac and 1 Linux), and I do not see the issue on the new machines. \r\n", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "OK, maybe it was a transient bug. Closing."]}, {"number": 15209, "title": "CMake: If/else statement in CMAKE_CACHE_ARGS breaks CMake build on Ubuntu 17.10", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 17.10.\r\n- **TensorFlow installed from (source or binary)**: Source (CMake)\r\n- **TensorFlow version (use command below)**: 2cfb088cf72b52c74a742d780cc5c4f93a74640e (tip of master at time of writing)\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**: 7.2.0\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: cmake ../../tensorflow/contrib/cmake && make\r\n\r\n### Source code / logs\r\n    [  1%] Performing build step for 'zlib'\r\n    [ 40%] Built target zlibstatic\r\n    [ 42%] Linking C shared library libz.so\r\n    /usr/bin/ld: CMakeFiles/zlib.dir/deflate.o: relocation R_X86_64_PC32 against symbol `deflate' can not be used when making a shared object; recompile with -fPIC\r\n    /usr/bin/ld: final link failed: Bad value\r\n    collect2: error: ld returned 1 exit status\r\n\r\n### Cause\r\nIn v1.4.0, you will find the argument `-DCMAKE_POSITION_INDEPENDENT_CODE:BOOL=ON` given to  CMAKE_CACHE_ARGS for several external projects (png, zlip, sqlite etc). In master, this has been changed to \r\n\r\n\tCMAKE_CACHE_ARGS\r\n\t\tif(tensorflow_ENABLE_POSITION_INDEPENDENT_CODE)\r\n\t\t\t-DCMAKE_POSITION_INDEPENDENT_CODE:BOOL=ON\r\n\t\telse()\r\n\t\t\t-DCMAKE_POSITION_INDEPENDENT_CODE:BOOL=OFF\r\n\t\tendif()\r\n\r\nwhich yields the following init cache entries on my machine: \r\n\r\n\tset(CMAKE_POSITION_INDEPENDENT_CODE \"ON;if;(;tensorflow_ENABLE_POSITION_INDEPENDENT_CODE;);else;(;)\" CACHE BOOL \"Initial cache\" FORCE)\r\n\tset(CMAKE_POSITION_INDEPENDENT_CODE \"OFF;endif;(;)\" CACHE BOOL \"Initial cache\" FORCE)\r\n\r\nand the build fails because `CMAKE_POSITION_INDEPENDENT_CODE` ends up not being set to `ON`. I could imagine this breaks a lot of builds. Perhaps CMake behaves differently on Windows and therefore this has not been caught? Is there a reason for this change I am not aware of? \r\n\r\n### Solution\r\nA possible solution is to not inline the if/else statement and instead use the argument\r\n\r\n    -DCMAKE_POSITION_INDEPENDENT_CODE:BOOL=${tensorflow_ENABLE_POSITION_INDEPENDENT_CODE}\r\n\r\nAre you interested in a PR?\r\n", "comments": ["d0a5d885d61b837018cb931a4d577289acc826fc introduced the change @martinwicke ", "I am interested in a PR. Thank you!", "Blocked by #15002. Awaiting solution.", "This was solved in #16007 now?", "I hope so. Please reopen if that's not true."]}, {"number": 15208, "title": "Eager bugs", "body": "After I use eager to debug code, the default version of tf changed when I reusing ipython, and tf can not placement mode to GPU, but placed on CPU.\r\n\r\n```\r\nIn [2]: tf.__version__\r\nOut[2]: '1.5.0-dev20171201\r\n```\r\n\r\nIn another environment, my code is work well and version is correct:\r\n```\r\nIn [2]: tf.__version__\r\nOut[2]: '1.4.0'\r\n```\r\nCode\uff1a\r\nrerun the blew code more times.\r\n```\r\n# coding: utf-8\r\nimport tensorflow as tf\r\nfrom  tensorflow.contrib.eager.python import tfe\r\ntfe.enable_eager_execution()\r\nfrom tensor2tensor.utils import input_fn_builder\r\nfrom tensor2tensor.utils import trainer_utils\r\nh = trainer_utils.create_hparams('blstm_bahdanau_attention_librispeech', 'data')\r\ntrainer_utils.add_problem_hparams(h, 'audio_librispeech_tokens30_en')\r\ne = input_fn_builder.features_for_problem(h.problem_instances[0], h.problems[0], h, 'data', 1, tf.estimator.ModeKeys.TRAIN, 10)\r\n```\r\n\r\nOS Platform and Distribution\r\nCentOS Linux 7 (Core)   4.4.77-1.el7.elrepo.x86_64\r\n\r\nTensorFlow installed from\r\nInstlled from Docker Hub\r\n\r\nTensorFlow version\r\n1.4.0\r\n\r\nBazel version\r\nN/A\r\n\r\nCUDA/cuDNN version\r\n8.0\r\n\r\nGPU model and memory\r\nTitan XP\r\n\r\nExact command to reproduce\r\nN/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler  Done.", "@zh794390558 : I'm sorry, I'm not quite sure I understand the problem here.\r\n\r\nNote that eager execution is not part of the 1.4 release, so to use it you currently have to update to the latest nightly release, i.e., you have to install a version later than 1.4\r\n\r\nAnd with eager execution, currently, device placement has to be explicit - so you'll have to place code in a `with tf.device('/gpu:0'):` block to use the GPU.\r\n\r\nI'm not sure if those comments address your questions. If not, could you please add some more detail as to what the problem is? Thanks.", "@asimshankar So without explicit placement the device, all code default to placement on CPU ?", "@zh794390558 : Yes, at this time, eager execution will use the CPU always (even if a GPU is present) unless explicitly told to use the GPU using `with tf.device('/gpu:0'):` blocks or by explicitly copying tensors to GPU using `.gpu()`", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Closing this due to lack of activity and because I believe the question has been addressed. Please let me know if I'm mistaken. Thanks!"]}, {"number": 15207, "title": "Feature request: automatically pick all defaults in ./configure", "body": "Everytime a new configuration option is added my CI halts on the ./configure step as it's prompted for input. Instead of setting each flag could we have some `USE_DEFAULTS=1 ./configure` flag instead?\r\n\r\nI assume the intention is to slowly move away from the shell script to something better (hence configure.py, I wager) but for now, when testing projects with the master release it would be very useful to be able to just pick all default options for the WORKSPACE without knowing about them.", "comments": ["If you use mac, try \u02cbyes \"\" | ./configure \u02cb", "Haha, that's a sweet trick. Thanks!", "@yifeif although the `yes \"\" | ./configure` trick works, do you think its worth adding a USE_DEFAULTS flag/env var?", "I think it's easier to find out about the `yes \"\" | ./configure` trick than a new flag :). Closing this issue for now."]}, {"number": 15206, "title": "Unusual memory allocation while running \"tf.assign\"", "body": "### System information\r\n- **Have I written custom code**: Yes\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from**: binary\r\n- **TensorFlow version**: 1.4.0\r\n- **Python version**: 2.7\r\n- **Bazel version**: 0.8.1\r\n- **CUDA/cuDNN version**: 8.0.61 / 6.0.21\r\n- **GPU model and memory**: Tesla K80\r\n- **Exact command to reproduce**: run the script attached below\r\n\r\n### Describe the problem\r\nIn the minimal example below, I am observing an increase in RAM usage when calling `assign_new_values`. Strangely, the extend depends on the dataset size. \r\nI see no obvious connection between the dataset and the variables that get assigned. Commenting out either the iterator or the `sess.run(assign_op, ...)` prevents additional memory allocation.\r\n\r\nHow is this possible?\r\n\r\n### Source code / logs\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nDATASET_SIZE = 1000000\r\n\r\n\r\ndef create_dataset():\r\n    image_paths = tf.constant(DATASET_SIZE * ['image.jpg'])\r\n    labels = tf.constant(DATASET_SIZE * [0])\r\n    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\r\n    return dataset\r\n\r\n\r\ndef create_variables():\r\n    for i in range(20):\r\n       tf.get_variable('var_%d' % i, shape=[3, 3, 3, 32])\r\n\r\n\r\ndef create_assign_ops():\r\n    for var in tf.global_variables():\r\n        name = var.op.name\r\n        placeholder = tf.placeholder(tf.float32, shape=var.shape, name=name + '/placeholder')\r\n        tf.assign(var, placeholder, name=name + '/assign_op')\r\n\r\n\r\ndef assign_new_values(sess):\r\n    for var in tf.global_variables():\r\n        name = var.op.name\r\n        assign_op = tf.get_default_graph().get_tensor_by_name(name + '/assign_op:0')\r\n        sess.run(assign_op, feed_dict={name + '/placeholder:0': np.random.random(var.shape)})\r\n\r\n\r\nif __name__ == '__main__':\r\n    with tf.Graph().as_default():\r\n        with tf.device('/cpu:0'):\r\n            dataset = create_dataset()\r\n            iterator = dataset.make_one_shot_iterator()\r\n\r\n        create_variables()\r\n        create_assign_ops()\r\n\r\n        with tf.Session() as sess:\r\n            sess.run(tf.global_variables_initializer())\r\n            # Check RAM usage before\r\n            assign_new_values(sess)\r\n            # Check RAM usage after\r\n\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nExact command to reproduce", "I updated the missing fields.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "@mrry could you take a look?\r\n\r\nTo be sure, @stecklin could you try `tf.Graph.finalize()` to make sure that the graph is not growing?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @mrry: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @mrry: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 15205, "title": "Delete Dockerfile.devel-gpu-cuda9-cudnn7", "body": "our default dockerfiles now use cuda9-cudnn7.\r\nNo need for this file anymore.\r\n\r\nCC @flx42 ", "comments": ["Great!\r\nQuick question: to build a new CUDA 9.0+cuDNN v7 container image with the new Dockerfiles, what magic value of `TF_DOCKER_BUILD_CENTRAL_PIP` do I need to pass to `parameterized_docker_build.sh`?\r\n", "Right now, there is not a correct value for that.\r\nHowever, by tomorrow morning we should have our nightlies built, and ready to use.\r\nWe should be able to pass our nightly pip package URLs to the script.\r\n\r\nAs a note, I have a bug open, and the first chance I get I will rewrite all our dockerfiles and get rid of parameterized_docker_build script.", "thanks @gunan, I'm not seeing the new pip packages at https://pypi.python.org/pypi/tf-nightly-gpu yet.\r\n\r\nAlso, looks like there hasn't been any nightly GPU docker image for a while:\r\nhttps://hub.docker.com/r/tensorflow/tensorflow/tags/\r\n\r\nIs your CI/CD system currently **on fire**? :) Let me know if I can help.", "We are also working on the 1.4.1 release, so the CI was backed up last night. I will see what I can do to push these out today."]}, {"number": 15204, "title": "compile error with config sycl", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:1.4,the latest version I git from tensorflow\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**:0.8.1\r\n- **GCC/Compiler version (if compiling from source)**:5.4.0\r\n- **CUDA/cuDNN version**:no\r\n- **GPU model and memory**:no\r\n- **Exact command to reproduce**:bazel build -c opt --config=sycl //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\n##something else\r\nOpenCl version:1.2\r\ncomputecpp path:    /usr/local/computecpp  version 0.5.0\r\n\r\n\r\ncomputecpp_info:\r\nDevice Info:\r\nDiscovered 1 devices matching:\r\n  platform    : <any>\r\n  device type : <any>\r\n--------------------------------------------------------------------------------\r\nDevice 0:\r\n  Device is supported                     : UNTESTED - Vendor not tested on this OS\r\n  CL_DEVICE_NAME                          : Hainan\r\n  CL_DEVICE_VENDOR                        : Advanced Micro Devices, Inc.\r\n  CL_DRIVER_VERSION                       : 2482.3\r\n  CL_DEVICE_TYPE                          : CL_DEVICE_TYPE_GPU \r\n\r\n\r\n\r\nclinfo:\r\nNumber of platforms                               1\r\n  Platform Name                                   AMD Accelerated Parallel Processing\r\n  Platform Vendor                                 Advanced Micro Devices, Inc.\r\n  Platform Version                                OpenCL 2.0 AMD-APP (2482.3)\r\n  Platform Profile                                FULL_PROFILE\r\n  Platform Extensions                             cl_khr_icd cl_amd_event_callback cl_amd_offline_devices \r\n  Platform Extensions function suffix             AMD\r\n\r\n  Platform Name                                   AMD Accelerated Parallel Processing\r\nNumber of devices                                 1\r\n  Device Name                                     Hainan\r\n  Device Vendor                                   Advanced Micro Devices, Inc.\r\n  Device Vendor ID                                0x1002\r\n  Device Version                                  OpenCL 1.2 AMD-APP (2482.3)\r\n  Driver Version                                  2482.3\r\n  Device OpenCL C Version                         OpenCL C 1.2 \r\n  Device Type                                     GPU\r\n  Device Profile                                  FULL_PROFILE\r\n  Device Board Name (AMD)                         AMD Radeon HD 8500M\r\n  Device Topology (AMD)                           PCI-E, 04:00.0\r\n  Max compute units                               4\r\n  SIMD per compute unit (AMD)                     4\r\n  SIMD width (AMD)                                16\r\n  SIMD instruction width (AMD)                    1\r\n  Max clock frequency                             850MHz\r\n  Graphics IP (AMD)                               6.0\r\n  Device Partition                                (core)\r\n    Max number of sub-devices                     4\r\n    Supported partition types                     none specified\r\n  Max work item dimensions                        3\r\n  Max work item sizes                             256x256x256\r\n  Max work group size                             256\r\n  Preferred work group size multiple              64\r\n  Wavefront width (AMD)                           64\r\n  Preferred / native vector sizes                 \r\n    char                                                 4 / 4       \r\n    short                                                2 / 2       \r\n    int                                                  1 / 1       \r\n    long                                                 1 / 1       \r\n    half                                                 1 / 1        (n/a)\r\n    float                                                1 / 1       \r\n    double                                               1 / 1        (cl_khr_fp64)\r\n  Half-precision Floating-point support           (n/a)\r\n  Single-precision Floating-point support         (core)\r\n    Denormals                                     No\r\n    Infinity and NANs                             Yes\r\n    Round to nearest                              Yes\r\n    Round to zero                                 Yes\r\n    Round to infinity                             Yes\r\n    IEEE754-2008 fused multiply-add               Yes\r\n    Support is emulated in software               No\r\n    Correctly-rounded divide and sqrt operations  Yes\r\n  Double-precision Floating-point support         (cl_khr_fp64)\r\n    Denormals                                     Yes\r\n    Infinity and NANs                             Yes\r\n    Round to nearest                              Yes\r\n    Round to zero                                 Yes\r\n    Round to infinity                             Yes\r\n    IEEE754-2008 fused multiply-add               Yes\r\n    Support is emulated in software               No\r\n    Correctly-rounded divide and sqrt operations  No\r\n  Address bits                                    32, Little-Endian\r\n  Global memory size                              2140311552 (1.993GiB)\r\n  Global free memory (AMD)                        <printDeviceInfo:68: get number of CL_DEVICE_GLOBAL_FREE_MEMORY_AMD : error -33>\r\n  Global memory channels (AMD)                    2\r\n  Global memory banks per channel (AMD)           8\r\n  Global memory bank width (AMD)                  256 bytes\r\n  Error Correction support                        No\r\n  Max memory allocation                           1591773593 (1.482GiB)\r\n  Unified memory for Host and Device              No\r\n  Minimum alignment for any data type             128 bytes\r\n  Alignment of base address                       2048 bits (256 bytes)\r\n  Global Memory cache type                        Read/Write\r\n  Global Memory cache size                        16384\r\n  Global Memory cache line                        64 bytes\r\n  Image support                                   Yes\r\n    Max number of samplers per kernel             16\r\n    Max size for 1D images from buffer            134217728 pixels\r\n    Max 1D or 2D image array size                 2048 images\r\n    Base address alignment for 2D image buffers   256 bytes\r\n    Pitch alignment for 2D image buffers          256 bytes\r\n    Max 2D image size                             16384x16384 pixels\r\n    Max 3D image size                             2048x2048x2048 pixels\r\n    Max number of read image args                 128\r\n    Max number of write image args                8\r\n  Local memory type                               Local\r\n  Local memory size                               32768 (32KiB)\r\n  Local memory syze per CU (AMD)                  65536 (64KiB)\r\n  Local memory banks (AMD)                        32\r\n  Max constant buffer size                        65536 (64KiB)\r\n  Max number of constant args                     8\r\n  Max size of kernel argument                     1024\r\n  Queue properties                                \r\n    Out-of-order execution                        No\r\n    Profiling                                     Yes\r\n  Prefer user sync for interop                    Yes\r\n  Profiling timer resolution                      1ns\r\n  Profiling timer offset since Epoch (AMD)        1512650783761772748ns (Thu Dec  7 20:46:23 2017)\r\n  Execution capabilities                          \r\n    Run OpenCL kernels                            Yes\r\n    Run native kernels                            No\r\n    Thread trace supported (AMD)                  No\r\n    SPIR versions                                 1.2\r\n  printf() buffer size                            1048576 (1024KiB)\r\n  Built-in kernels                                \r\n  Device Available                                Yes\r\n  Compiler Available                              Yes\r\n  Linker Available                                Yes\r\n  Device Extensions                               cl_khr_fp64 cl_amd_fp64 cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_int64_base_atomics cl_khr_int64_extended_atomics cl_khr_3d_image_writes cl_khr_byte_addressable_store cl_khr_gl_sharing cl_amd_device_attribute_query cl_amd_vec3 cl_amd_printf cl_amd_media_ops cl_amd_media_ops2 cl_amd_popcnt cl_khr_image2d_from_buffer cl_khr_spir cl_khr_gl_event \r\n\r\nNULL platform behavior\r\n  clGetPlatformInfo(NULL, CL_PLATFORM_NAME, ...)  AMD Accelerated Parallel Processing\r\n  clGetDeviceIDs(NULL, CL_DEVICE_TYPE_ALL, ...)   Success [AMD]\r\n  clCreateContext(NULL, ...) [default]            Success [AMD]\r\n  clCreateContextFromType(NULL, CL_DEVICE_TYPE_CPU)  No devices found in platform\r\n  clCreateContextFromType(NULL, CL_DEVICE_TYPE_GPU)  Success (1)\r\n    Platform Name                                 AMD Accelerated Parallel Processing\r\n    Device Name                                   Hainan\r\n  clCreateContextFromType(NULL, CL_DEVICE_TYPE_ACCELERATOR)  No devices found in platform\r\n  clCreateContextFromType(NULL, CL_DEVICE_TYPE_CUSTOM)  No devices found in platform\r\n  clCreateContextFromType(NULL, CL_DEVICE_TYPE_ALL)  Success (1)\r\n    Platform Name                                 AMD Accelerated Parallel Processing\r\n    Device Name                                   Hainan\r\n\r\nICD loader properties\r\n  ICD loader Name                                 OpenCL ICD Loader\r\n  ICD loader Vendor                               OCL Icd free software\r\n  ICD loader Version                              2.2.8\r\n  ICD loader Profile                              OpenCL 1.2\r\n\tNOTE:\tyour OpenCL library declares to support OpenCL 1.2,\r\n\t\tbut it seems to support up to OpenCL 2.1 too.\r\n\r\n### Describe the problem\r\nI want Compile With sycl config  with the command \r\nbazel build -c opt --config=sycl //tensorflow/tools/pip_package:build_pip_package\r\n but  unfortunately get the error \r\n\r\nIllegal ambiguous match on configurable attribute \"deps\" in @local_config_sycl//sycl:sycl:\r\n@local_config_sycl//sycl:using_sycl_ccpp\r\n@local_config_sycl//sycl:using_sycl_trisycl\r\nMultiple matches are not allowed unless one is unambiguously more specialized.\r\n\r\nI did not know what casue this compile error. Please Help\r\n\r\n", "comments": ["Sorry, opencl is not fully supported.", "> Illegal ambiguous match on configurable attribute \"deps\"\r\n\r\nThat doesn't [seem](https://github.com/tensorflow/tensorflow/issues/11807) opencl \\*specific\\*", "@mirh ,yeah, I am not familiar with bazel  tools ,But I guess that some Configuration may not be appropriate because I find that  in /third_party/sycl/sycl/BUILD.tpl  some configuration confuse me.\r\nIn that file\r\nconfig_setting(\r\n    name = \"using_sycl_ccpp\",\r\n    define_values = {\r\n        \"using_sycl\": \"true\",\r\n        \"using_trisycl\": \"false\",\r\n    },\r\n)\r\n\r\nconfig_setting(\r\n    name = \"using_sycl_trisycl\",\r\n    define_values = {\r\n        \"using_sycl\": \"true\",\r\n        \"using_trisycl\": \"false\",\r\n    },\r\n)\r\nthat the setting named \"using_sycl_ccpp\"  whosed define_values are same as \"using_sycl_trisycl\",,there must be some conflict in cuda's configruation cause the same error\u3002but It's difficault for me to handle the problem . Just wait for someone is good at bazel to handle the problem. \r\n", "The thing build fine for me with [this tree](https://github.com/lukeiwanski/tensorflow/tree/dev/amd_gpu)\r\nCan you check ?", "Thanks for your guide ,I will try this tree and reply the result.\r\n\r\n\u53d1\u81ea\u8d3a\u654f\u742a\r\n\r\n\u5728 2017\u5e7412\u670819\u65e5\uff0c\u4e0a\u534812:06\uff0cmirh <notifications@github.com<mailto:notifications@github.com>> \u5199\u9053\uff1a\r\n\r\n\r\nThe thing build fine for me with this tree<https://github.com/lukeiwanski/tensorflow/tree/dev/amd_gpu>\r\nCan you check ?\r\n\r\n\u2014\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/15204#issuecomment-352470374>, or mute the thread<https://github.com/notifications/unsubscribe-auth/ANFVQuU6sM7kLREjX1wFCP4Wq6BPiBBnks5tBo0vgaJpZM4Q6qSR>.\r\n", "I ran into a similar issue but I could get a successfull SYCL-based build once switching back to ComputeCpp 0.4.0 (building against TensorFlow ~r1.4). So you might want to give that a try as well, if that is something possible for you.", "Thanks, I try the branch amd_gpu It can build. But I Have got another error about \r\n\r\nthis rule is missing dependency declarations for the following files included by 'external/protobuf_archive/python/google/protobuf/internal/api_implementation.cc'\r\n\r\nthe command is \r\nbazel build --config=opt --config=sycl --verbose_failures //tensorflow/tools/pip_package:build_pip_package 2>&1 | tee ~/tfbuild.log\r\n\r\ncan you help me with  @this@mirh ", "For as much as I know, it might have been due to one of the plenty of updates they made in this last couple of days. \r\nMaybe.. Try [this](https://github.com/lukeiwanski/tensorflow/tree/6631d9c054c9b0531e8f9fd89031bb7f0c4612e1) revision for starters?", "I am getting this issue when compiling with Intel GPU. Is there a solution to this anywhere?", "[This is the branch](https://github.com/lukeiwanski/tensorflow/tree/dev/amd_gpu). ", "[Found the actual cause](https://github.com/tensorflow/tensorflow/blob/d0a5d885d61b837018cb931a4d577289acc826fc/third_party/sycl/sycl/build_defs.bzl.tpl#L13)\r\nSomebody screwed kinda hard. \r\n\r\n@huoyushi can you reopen?", "@mirh  were you able to fix this issue?\r\n", "This specific one, yes. \r\nYou just need to remove the trisycl line. \r\nOTOH normal tf is just rotten to the core for sycl. \r\n\r\nAs I said 2 comments above, use Luke's branch. "]}, {"number": 15203, "title": "DataLossError : Checksum does not match", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:windows 7 professional,64bit\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.3.0\r\n- **Python version**: 3.6.2\r\n- **Bazel version (if compiling from source)**:N/A\r\n- **GCC/Compiler version (if compiling from source)**:N/A\r\n- **CUDA/cuDNN version**:N/A\r\n- **GPU model and memory**:N/A\r\n- **Exact command to reproduce**:N/A\r\n\r\n### Describe the problem\r\nI trained on Ubuntu16.04 to get the model, and then restore the model on  Windows7 Professional, but it occured such a mistake:\r\n\r\n`DataLossError (see above for traceback): Checksum does not match: stored 1713499277 vs. calculated on the restored bytes 1894941567`\r\n\r\n`[Node: save/RestoreV2_282 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]`\r\n\r\n**Both machines have the same version of python and TensorFlow, and the model tested in another machine succeed, but this machine failed**, how to solve it? Thank you!\r\n\r\n### Source code / logs\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"D:/tensorboxPy3/evaluate.py\", line 138, in <module>\r\n    main()\r\n  File \"D:/tensorboxPy3/evaluate.py\", line 117, in main\r\n    get_results(args, H, os.path.dirname(args.datadir))\r\n  File \"D:/tensorboxPy3/evaluate.py\", line 59, in get_results\r\n    saver.restore(sess, args.weights)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1560, in restore\r\n    {self.saver_def.filename_tensor_name: save_path})\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.DataLossError: Checksum does not match: stored 1713499277 vs. calculated on the restored bytes 1894941567\r\n\t [[Node: save/RestoreV2_282 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_282/tensor_names, save/RestoreV2_282/shape_and_slices)]]\r\n\r\nCaused by op 'save/RestoreV2_282', defined at:\r\n  File \"D:/tensorboxPy3/evaluate.py\", line 138, in <module>\r\n    main()\r\n  File \"D:/tensorboxPy3/evaluate.py\", line 117, in main\r\n    get_results(args, H, os.path.dirname(args.datadir))\r\n  File \"D:/tensorboxPy3/evaluate.py\", line 55, in get_results\r\n    saver = tf.train.Saver()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1140, in __init__\r\n    self.build()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1172, in build\r\n    filename=self._filename)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 688, in build\r\n    restore_sequentially, reshape)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 407, in _AddRestoreOps\r\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 247, in restore_op\r\n    [spec.tensor.dtype])[0])\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 663, in restore_v2\r\n    dtypes=dtypes, name=name)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2630, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nDataLossError (see above for traceback): Checksum does not match: stored 1713499277 vs. calculated on the restored bytes 1894941567\r\n\t [[Node: save/RestoreV2_282 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_282/tensor_names, save/RestoreV2_282/shape_and_slices)]]\r\n```\r\n\r\n\r\n`Process finished with exit code 1`\r\n\r\n", "comments": ["@martinwicke any ideas what the problem could be?", "@reedwm Any updates? Thanks!", "@PumayHui have you verified that the file is complete? Does the file have the same length on this machine? The checksum is there to guard against things like interrupted downloads, copies, and file corruption. Maybe it found something useful.\r\n\r\nYou can check the file size as a first step, and then compute a crc32 or md5 on a working an a non-working machine to make sure the files actually match.", "I also meet this,when i use tensorflow object detection API; i training the model more steps,the mistake missing,i guess the reason is  that tf required the trained models at least  a value,we need to reach it", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Ok, I don't know what to do here, without a repro we can work from. Please reopen this is someone has more information.", "i meet the same problem", "same probleam on Windows10 / tensorflow-gpu-1.2 / python3.6.6", "I also meet the same problem with tensorflow-gpu-1.12 / python 3.6.6", "The same problem Ubuntu 18.04, python2.7, tensorflow-1.31,1", "The same problem Ubuntu 18.04, python3.7, tensorflow-1.31.1"]}, {"number": 15202, "title": "Create test.txt", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Can one of the admins verify this patch?"]}, {"number": 15201, "title": "Remove extra param of concatenation in nnapi/NeuralNetworksShim.h", "body": "The param specifying activation is not documented in [this](https://developer.android.com/ndk/reference/group___neural_networks.html#ggaabbe492c60331b13038e39d4207940e0a44cbea825c4b224dd3ea757e9b1f65ed). It is actually needed in 8.1 Preview MR1, but not needed anymore in MR2.", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Can one of the admins verify this patch?", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@caisq Hi, is there any progress on this PR?", "@aselle gentle ping.", "Give the thumbs up to tensorflow maintainers! You are so great!", "@aselle So great. No one responds to this issue, but I receive a spam email from @tensorflowbutler every 14 days.", "I decide to reopen this PR, just to express my anger. I need an explanation and apologies for your annoying reviewer and robot. @caisq @aselle ", "TensorFlow stuff, you are so good. Now I will be a robot too, and ping you guys every day.", "2019/04/19 @caisq @aselle Please give me an explanation and apologies for your annoying reviewer and robot.", "2019/04/22 @caisq @aselle Please give me an explanation and apologies for your annoying reviewer and robot.", "2019/04/23 @caisq @aselle Please give me an explanation and apologies for your annoying reviewer and robot.", "@caisq @aselle The every-15-day spam messages from your robot has been deleted? However, I still have the screenshot: \r\n\r\n![image](https://user-images.githubusercontent.com/11607199/56546799-f410cc80-65ad-11e9-9f51-321756c9556e.png)", "2019/04/24 @caisq @aselle Please give me an explanation and apologies for your annoying reviewer and robot.", "2019/04/25 @smit-hinsu @rthadur @caisq @aselle Please give me an explanation and apologies for your annoying reviewer and robot.", "@smit-hinsu @rthadur @caisq @aselle Is there a tensorflow maintainer that can give me an explanation and apologies?", "@daquexian sorry, looks like this one got lost in the system. We have a large number of PRs and issues and sometimes this happens, we understand it can be very frustrating, so thanks for sticking with us. We're paying attention now!", "@gunan @ewilderj it's not only frustrating but also annoying. Your reviewers never respond to this PR, but I, the author, have to receive spam emails from your robot every 14 days. How ridiculous.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 44 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 15200, "title": "1.4.1 Release Updates", "body": "", "comments": []}, {"number": 15199, "title": "missing instruction for BahdanauAttention", "body": "This is just my opinion: \r\nwhen call BahdanauAttention instance , it create a variable scope with None name_or_scope variable.\r\nWhile name_or_scope is None, and get_variable with the same name inside the variable scope repeatedly, it will automatically add '_N' to the name of the scope. And I think it is not compatible with some functions like stati_rnn, because there are explicit 'for loop' inside the function and every loop of 'for loop' will create different variables inside the variable scope but not creating once and sharing", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, but only with a few lines.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: conda pip install with the instruction on the tensorflow website(binary)\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: gtx 1080 ti\r\n- **Exact command to reproduce**:\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops import rnn_cell_impl\r\n\r\ncell = rnn_cell_impl.GRUCell(5)\r\nmemory = tf.get_variable('memory', [10, 8, 5])\r\nattention = tf.contrib.seq2seq.BahdanauAttention(5, memory)\r\nwrapped_cell = tf.contrib.seq2seq.AttentionWrapper(cell, attention)\r\n\r\nrnn_inputs = [tf.get_variable('1', [10, 5]), tf.get_variable('2', [10, 5]), tf.get_variable('3', [10, 5]), tf.get_variable('4', [10, 5])]\r\nrnn_state = wrapped_cell.zero_state(10, tf.float32)\r\n\r\noutputs, state=  tf.nn.static_rnn(wrapped_cell, rnn_inputs, dtype = tf.float32)\r\n\r\nsess = tf.InteractiveSession()\r\nsess.run(outputs)\r\n\r\nI think two modules are not compatible because 1. there is a **for loop** inside ststic_rnn  2. there is a variable_scope inside bahdanau attention(or luong attention) whose first argument(name_or_scope) is set None   ->> while name_or_scope is None, each time a variable within the variable scope is created, the name of variable scope will be added '_N'  ", "I updated the Exact command to reproduce field", "I'm having a hard time understanding what the issue here is. Can you rephrase your question with an explicit bug report or feature request?", "I uploaded an exact code to reproduce the error up there.\r\nIn a nutshell, an rnn cell with attention wrapper can only be used with decoder function but not rnn function", "@lukaszkaiser, can you take a look at this?", "I believe you're right in everything you say. The only issue is that `static_rnn` is deprecated. It'd still be good to adjust BahdanuAttention to be better for sharing (e.g., take a name parameter like all layers). Feel free to send a PR if you have some code, of course! In other case, I don't think this is a high-priority problem as it seems to only affect deprecated code.", "Actually I simply changed the `with variable_scope.variable_scope(None, \"bahdanau_attention\", [query]):` to `with variable_scope.variable_scope(\"bahdanau_attention\"):` and it works fine for static_rnn. And after all, I changed to the combination of attention + dynamic_rnn and it works fine.\r\n Thanks for all your help"]}, {"number": 15198, "title": "Missing documentation for using the Dataset API in combination with image summaries", "body": "The `Dataset` API is now the recommended input pipeline, however I am missing some guidance on how to include summaries of my images.\r\n\r\n```python\r\ndef get_data():\r\n  dataset = FixedLengthRecordDataset(...)\r\n  dataset = dataset.map(parse_dataset, ...)\r\n  if is_training:\r\n    dataset = dataset.map(preprocess_for_train, ...)\r\n  # Do shuffling, batching...\r\n  return dataset\r\n\r\ndef preprocess_for_train(image, label):\r\n  # Do preprocessing...\r\n  image = tf.image.random_flip_left_right(image)\r\n  # Add summary\r\n  tf.summary.image('preprocessed_image', tf.expand_dims(image, 0))\r\n  return image, label\r\n```\r\n\r\nThis is what I would do intuitively, but since `map()` uses a different thread and therefore a different `tf.Graph` instance (?), the summaries are lost.\r\n\r\nWhat is the recommended way of adding image summaries when using the `Dataset` API? I would like to request a comment / example on that in the official docs. ", "comments": ["Hi,\r\n\r\nafaik you can run the summary, but only after you created the iterator and fetched the next element:\r\n```python\r\ndataset = get_data(is_training=True)\r\nt_example = dataset.make_one_shot_iterator().get_next()\r\ntf.summary.image('preprocessed_image', t_example['image'])\r\n```", "@marcoadurno That's true, but it comes with a drawback. Assume the training loop looks like this:\r\n\r\n```python\r\nimage_batch, label_batch = sess.run(t_example)\r\n_, summary = sess.run([train_op, summary_op], \r\n                      feed_dict={image: image_batch, label: label_batch})\r\n```\r\n\r\nThis will invoke the iterator's `get_next` twice, once you run `t_example` and once you request the summary since you introduced a dependency. ", "@stecklin I'm not sure I fully understand the reason why image_batch and label_batch have be be fetched using sess.run to then fed back in using placeholders. If doing so, I think the `summary_op` should use the `image` placeholder, same as `train_op` does but I struggle to see why mixing the Data api with placeholders.\r\n\r\nIf things can be shuffled around:\r\n```python\r\ndataset = get_data(is_training=True)\r\nt_example = dataset.make_one_shot_iterator().get_next()\r\nt_logits = infer_fn(t_example['image'])\r\nloss_op = loss_fn(t_example['labels'], t_logits)\r\ntrain_op = train_fn(loss_op)\r\nsummary_op = tf.summary.image('preprocessed_image', t_example['image'])\r\nfor i in xrange(n_steps):\r\n  fetches = [loss_op, train_op]\r\n  if i%100:\r\n    fetches += [summary_op]\r\n  sess.run(fetches)\r\n```\r\nOr alternatively you can use a monitored session and specify summary hooks. But the basic idea is that if you have a canonical training loop there is no reason why summaries can't be written using the Data api.", "@MarkDaoust maybe we need more docs.\r\n\r\n/CC @mrry ", "@marcoadurno Thank you so much for pointing that out. You made me realize that I am not using the `Dataset API` as it is meant to be used, that there is no need for placeholders. \r\nSummaries can be added as you suggested, works like a charm. ", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Thanks for sorting that out @marcoadurno. This looks like the right solution to me.\r\n\r\nYes, AFAIK, you can't recover summaries from inside a `map` (hopefully @mrry will correct me if I'm wrong). I'll add a note to the `Dataset` doc explaining this.\r\n\r\nNote, in the future, stackoverflow might be a better place for this sort of question."]}, {"number": 15197, "title": "Fix tag in source_remote_test: no_mac --> nomac", "body": "", "comments": ["@tensorflow-jenkins test this please"]}, {"number": 15196, "title": "[XLA] OSX tfcompile compile failure in ../llvm_ir/kernel_support_library.cc ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX 10.13.2 \r\n- **TensorFlow installed from (source or binary)**: Source \r\n- **TensorFlow version (use command below)**: Top of Master (34bcd09c5fd4f6435517a499987b7e5044c8f2c0) \r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:   0.7.0\r\n- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.0.0 (clang-900.0.39.2)\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: \r\n\r\nbazel build tensorflow/compiler/aot/tfcompile\r\n\r\n### Describe the problem\r\nCompiler error when compiling tfcompile on OSX. This was introduced by @sanjoy  Commit:\r\nhttps://github.com/tensorflow/tensorflow/commit/c572bc4fd7c73f4b8014ae43cdf9da5b99592f59\r\n\r\nYou will need this PR to be able to fix other compile issues on OSX. https://github.com/tensorflow/tensorflow/pull/14893\r\n\r\n\r\nERROR: /Users/tfninja/github/tensorflow/tensorflow/compiler/xla/service/llvm_ir/BUILD:171:1: C++ compilation of rule '//tensorflow/compiler/xla/service/llvm_ir:kernel_support_library' failed (Exit 1).\r\ntensorflow/compiler/xla/service/llvm_ir/kernel_support_library.cc:99:5: error: no matching function for call to 'transform'\r\n    std::transform(function->arg_begin(), function->arg_end(),\r\n    ^~~~~~~~~~~~~~\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/algorithm:1922:1: note: candidate template ignored: couldn't infer template argument '_UnaryOperation'\r\ntransform(_InputIterator __first, _InputIterator __last, _OutputIterator __result, _UnaryOperation __op)\r\n^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/algorithm:1932:1: note: candidate function template not viable: requires 5 arguments, but 4 were provided\r\ntransform(_InputIterator1 __first1, _InputIterator1 __last1, _InputIterator2 __first2,\r\n^\r\n1 error generated.\r\nTarget //tensorflow/compiler/aot:tfcompile failed to build\r\n\r\n### Source code / logs\r\n\r\n\r\nThe issue seems to be with this line of code in which works on Linux but fails on OSX/Clang tensorflow/compiler/xla/service/llvm_ir/kernel_support_library.cc\r\n\r\n```\r\n+    std::transform(function->arg_begin(), function->arg_end(),\r\n+                   std::back_inserter(arg_values), std::addressof<llvm::Value>);\r\n```\r\n", "comments": ["Same issue here, when trying to compile the pip_build_package as a target\r\n\r\nTrying to build from source on OS X Sierra 10.12.6, following the exact instructions on the Tensorflow website. Not trying to compile for GPU, only for CPU", "Do you mind verifying that replacing the `std::transform` with a for loop works on OSX and send a PR?  Alternatively, let me know that it worked, and I'll fix it internally and the change will get upstreamed in the next sync.", "In the end it worked by just building with the normal:\r\n\r\n`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nbut during the `./configure`, I had to select **No** for the following (_Yes_ for the ones not mentioned):\r\n\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)?\r\nDo you wish to build TensorFlow with VERBS support?\r\nDo you wish to build TensorFlow with OpenCL support?\r\nDo you wish to build TensorFlow with CUDA support?\r\nDo you wish to build TensorFlow with MPI support?", "@drpngx is there a fix committed ? @darsipu above just disabled XLA so it would compile", "@sanjoy as requested I have a PR up at https://github.com/tensorflow/tensorflow/pull/15229. I can't reopen this bug so will move discussion to the PR."]}, {"number": 15195, "title": "[MSVC] Add tensorflow::ops prefix for {Read,Write}File", "body": "`ReadFile` and `WriteFile` collide with the functions in `windows.h`. Tell MSVC we want Tensorflow's ones.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 15194, "title": "Fix a typo in estimators.md", "body": "A small typo `similarily` -> `similarly`\r\n", "comments": []}, {"number": 15193, "title": "Training test fix", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 15192, "title": "Branch 178260923", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "Can you change `no_mac` to `nomac`, @ankurtaly ? Yes, it is a little inconsistent with other tags.", "Actually, I'll merge this now since it's just a single test failing. I'll send a PR to fix the tag."]}, {"number": 15191, "title": "Fix ios_makefile.inc for TFLite iOS demo app", "body": "It seems broken when merging change in b2db981a6731e978453862a73dab892bc674db68.\r\nThe file was formatted with an invalid way. \r\n\r\nI'm simply reverting the change\r\n\r\nI don't know why it happened. Is it due to the `.inc` file extension name?", "comments": ["Can one of the admins verify this patch?"]}, {"number": 15190, "title": "S3 reads eventually fail with tensorflow's dataset API", "body": "\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.3 LTS\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514\r\n- **Python version**: Python 2.7.12\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CPU\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: https://gist.github.com/thunterdb/d5f86c79457eea0f1021117ea4bce0ba\r\n\r\n### Describe the problem\r\nThe given script reads the MNIST data from S3 repeatedly from a public bucket, on an EC2 machine in the same region as the S3 bucket.\r\n\r\nRunning this script eventually leads to python crashing after a few hours, and the following error:\r\n\r\n```\r\nFinished step 31890, time: 05:45:28 12/07/17, result: 32\r\nFinished step 31920, time: 05:46:01 12/07/17, result: 32\r\nterminate called after throwing an instance of 'std::system_error'\r\n  what():  Resource temporarily unavailable\r\nAborted\r\n```\r\n\r\n### Source code / logs\r\nhttps://gist.github.com/thunterdb/d5f86c79457eea0f1021117ea4bce0ba\r\n\r\nIt is hard to say what is happening without debugging symbols. From the very generic error, I suspect this is an issue with the S3 SDK. As a workaround, it would be nice for tensorflow's S3 plugin to retry or be more resilient to networking issues.\r\n\r\nIt is currently an annoying issue when running large distributed training jobs, because they crash after a few hours of reading data. Is anyone having a similar experience with S3?", "comments": ["cc @smurching", "I suspect the problem is this line:\r\n\r\n```python\r\n                  res = sess.run(iterator.get_next())\r\n```\r\n\r\n`iterator.get_next()` creates a new op each time it's called, and this op happens to be an especially heavyweight one that owns a thread, so calling it in a loop will eventually cause a resource leak. The fix is  simple, and we'll shortly start printing a warning message to advise users to do it when we detect this situation:\r\n\r\n```python\r\n      # ...\r\n      next_element = iterator.get_next()\r\n      with tf.Session() as sess:\r\n            sess.run(iterator.initializer)\r\n            for i in xrange(num_steps):\r\n                  res = sess.run(next_element)\r\n                  # ...\r\n```", "@mrry thanks for the response, I am going to give it a try and get back to you.", "By the way, one suggestion I would have for the documentation of `get_next` is emphasize that calling the resulting op repeatedly is going to return sequential batches of data. From the documentation at [1] and from most iterator interfaces (java, c++), the convention assumes that you would have to call `get_next` to fetch a new batch.\r\n\r\n[1] https://www.tensorflow.org/versions/master/api_docs/python/tf/data/Iterator#get_next", "@mrry I updated the gist following your suggestion. It failed after ~ 10M reads on S3 (a few hours) with the following stack trace.\r\n\r\nHas anyone else encountered this issue before? Our current workaround is to copy the data locally to the machines' drives before starting the training, but this adds some complexity.\r\n\r\n```\r\n  File \"/databricks/python2/local/lib/python2.7/site-packages/IPython/terminal/interactiveshell.py\", line 443, in mainloop\r\n    self.interact(display_banner=display_banner)\r\n  File \"/databricks/python2/local/lib/python2.7/site-packages/IPython/terminal/interactiveshell.py\", line 567, in interact\r\n    self.run_cell(source_raw, store_history=True)\r\n  File \"/databricks/python2/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2741, in run_cell\r\n    interactivity=interactivity, compiler=compiler)\r\n  File \"/databricks/python2/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2833, in run_ast_nodes\r\n    if self.run_code(code):\r\n  File \"/databricks/python2/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2883, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-8-201cc217ea8a>\", line 1, in <module>\r\n    test_s3_read_singlemachine(worker_index=0, num_workers=2, data_dir=data_dir, num_readers=2, shuffle_buffer_size=32, batch_size=32, num_prefetch_batches=1, num_steps=int(1e7))\r\n  File \"<ipython-input-7-58d40f4ce8d4>\", line 15, in test_s3_read_singlemachine\r\n    it_op = iterator.get_next()\r\n  File \"/databricks/python2/local/lib/python2.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 259, in get_next\r\n    name=name))\r\n  File \"/databricks/python2/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 706, in iterator_get_next\r\n    output_shapes=output_shapes, name=name)\r\n  File \"/databricks/python2/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/databricks/python2/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"/databricks/python2/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nDataLossError (see above for traceback): truncated record at 785862\r\n         [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?]], output_types=[DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\r\n```", "I believe the S3 filesystem was contributed by @yongtang... hopefully he will have some suggestions!", "I tried the script and it is still running\r\n```\r\nFinished step 1764750, time: 15:23:37 12/11/17, result: 32\r\nFinished step 1764780, time: 15:23:38 12/11/17, result: 32\r\nFinished step 1764810, time: 15:23:39 12/11/17, result: 32\r\nFinished step 1764840, time: 15:23:40 12/11/17, result: 32\r\nFinished step 1764870, time: 15:23:42 12/11/17, result: 32\r\nFinished step 1764900, time: 15:23:43 12/11/17, result: 32\r\nFinished step 1764930, time: 15:23:44 12/11/17, result: 32\r\nFinished step 1764960, time: 15:23:46 12/11/17, result: 32\r\nFinished step 1764990, time: 15:23:46 12/11/17, result: 32\r\n```\r\n\r\nProbably it is not long enough. Will see if I could reproduce it.", "Note that S3 can fail every so often. I am not sure if we have retry logic.", "My test is still running and I haven't been able to reproduce yet:\r\n```\r\nFinished step 3888180, time: 15:52:12 12/12/17, result: 32\r\nFinished step 3888210, time: 15:52:14 12/12/17, result: 32\r\nFinished step 3888240, time: 15:52:14 12/12/17, result: 32\r\n```", "I think adding retry logic to S3 support would be great.\r\n\r\nI can't commit to have it done in the short term but will give it a try at some point once I have some spare time.\r\n\r\nIn the meantime, if anyone is interested in working on retry logic for S3 that would be excellent as well.", "Something is slightly fishy here - the AWS SDK has retry logic baked in to it which `Aws::S3::S3Client` should pick up.\r\n\r\n`Aws::Client::ClientConfiguration` has [a default value for `retryStrategy`](https://sdk.amazonaws.com/cpp/api/LATEST/struct_aws_1_1_client_1_1_client_configuration.html#a48715e0c06bcf958694d1223657b51c7) which provides [a max retry count of 10](https://sdk.amazonaws.com/cpp/api/LATEST/class_aws_1_1_client_1_1_default_retry_strategy.html) and we don't appear to doing anything that would change that default value in [`s3_filesystem.cc`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/s3/s3_file_system.cc#L40)\r\n", "By the way my test on the script has been running for more than a day and it is still OK:\r\n```\r\nFinished step 4701000, time: 01:16:31 12/13/17, result: 32\r\nFinished step 4701030, time: 01:16:33 12/13/17, result: 32\r\nFinished step 4701060, time: 01:16:33 12/13/17, result: 32\r\n```", "Closing this out is it looks as though we have not been able to reproduce it on our end.", "Interesting. I will try to reproduce it with more information, and reopen the ticket if something occurs again.", "thunterdb@, sorry for being a blast from the past. Did you not see this issue later? I believe I'm seeing something similar on my end. I am curious if there was anything you had to do to get around it. ", "@thunterdb @yongtang We're running into the same issue. Is there a workaround or are we missing a config param somewhere?"]}, {"number": 15189, "title": " fix #15188 replaced isnan with std::isnan to avoid build error ", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "See comment https://github.com/tensorflow/tensorflow/commit/d87a76d7d6b053219dbd49a87b3c4b379a1c6566#r26118028\r\n\r\nReplacing `isnan` with `std::isnan` may have issues with nvcc, maybe you could try adding a\r\n```\r\nusing namespace std;\r\n```\r\n and see if it solved the issue?\r\n\r\n/cc @hawkinsp @lamerman ", "Using std::isnan causes a runtime crash on a CUDA build with nvcc \u2014 I've tried that before. I can't reproduce your failure (maybe it is compiler version dependent).\r\n\r\nCan you try using Eigen::numext::half for me? Does that compile for you?", "@hawkinsp, to reproduce the problem you will need to try to build with cuda9:\r\nhttps://github.com/tensorflow/tensorflow/pull/14773\r\n\r\nSince we know that `std:isnan` wont work, I will push the recommended update to this PR.", "Jenkins, test this please.", "Jenkins, test this please.", "Thank you all <3", "Sorry for the breakage and thanks for the PR!"]}]