[{"number": 46329, "title": "model with \"tf.keras.layers.Embedding\" and \"trainable=False\" will be loaded as \"trainable=True\" after saving", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: No\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**: source\r\n-   **TensorFlow version (use command below)**: 2.5.0\r\n-   **Python version**: 3.6\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nAfter saving a model with tf.keras.layers.Embedding as a layer and set trainable=False and loading the model, the layer has \"trainable=True\" in the get_config().\r\n\r\n### Source code / logs\r\n```\r\n# Code is from https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\r\nmodel = tf.keras.Sequential()\r\nmodel.add(tf.keras.layers.Embedding(1000, 64, input_length=10, trainable=False))\r\n\r\ninput_array = np.random.randint(1000, size=(32, 10))\r\nmodel.compile('rmsprop', 'mse')\r\n\r\nmodel.summary()\r\nmodel.save('some_path')\r\nnew_model = tf.keras.models.load_model('some_path')\r\nnew_model.summary()\r\n```\r\n\r\nmodel has 0 trainable parameters but new_model has 6400 trainable parameters.\r\nmodel summary:\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nembedding_1 (Embedding)      (None, 10, 64)            64000     \r\n=================================================================\r\nTotal params: 64,000\r\nTrainable params: 0\r\nNon-trainable params: 64,000\r\n_________________________________________________________________\r\n```\r\nnew_model summary:\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nembedding (Embedding)        (None, 10, 64)            64000     \r\n=================================================================\r\nTotal params: 64,000\r\nTrainable params: 64,000\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```\r\n\r\n", "comments": ["I have tried in colab with TF version 2.4, nightly version (`2.5.0-dev20210111`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/8d793a53aeddd21ebcd250565363ec39/untitled608.ipynb). Thanks!", "Duplicate of #40994", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46329\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46329\">No</a>\n"]}, {"number": 46328, "title": "micro: copy operator ELU kernel from lite", "body": "This is a copy with minimal modification of the kernel and test for\r\noperator ELU from tensorflow/lite/kernels.\r\nAdaptations to micro and addition to the micro build to follow.\r\n\r\nPR step 3 for issue #46323", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", ":frowning_face: Sorry, but only Googlers may change the label `cla: yes`."]}, {"number": 46327, "title": "Extract reference for operator ELU to standalone header", "body": "Move the reference implementation to its own header so that micro\r\ncan use it without the unrelated depedencies of reference_ops.h.\r\n\r\nPR step 2 for issue #46323", "comments": ["@ddavis-2015  Can you please resolve conflicts? Thanks!", "@ddavis-2015  Can you please resolve conflicts? Thanks!"]}, {"number": 46326, "title": "Extract a function for parsing operator ELU", "body": "Extract the parsing out of a switch statement case to create a\r\nstandalone function which can be called by the micro op resolver.\r\n\r\nPR step 1 for issue #46323", "comments": ["@ddavis-2015  Can you please resolve conflicts? Thanks!", "@ddavis-2015  Can you please resolve conflicts? Thanks!"]}, {"number": 46325, "title": "Can't create notrainable variables in the __ini__ function in tf.keras.layers.Layer.", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): from conda\r\n- TensorFlow version (use command below):  2.3\r\n- Python version:3.7\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): None\r\n- CUDA/cuDNN version: 10.1.0/7.6.5\r\n- GPU model and memory: TITAN RTX/24576Mib\r\n\r\n\r\n**Describe the current behavior**\r\nI manually implement the batchnormalized layer and need to create two nontrainable variables to store the mean and variance. But when I embed the custom layer into tf.keras.Model, the two nontrainable variables are not cereated.\r\n**Describe the expected behavior**\r\nprint(len(Model.variables))  should print 4 but 2.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nclass batchNormalization(tf.keras.layers.Layer):\r\n    def __init__(self, shape, Trainable, **kwargs):\r\n        super(batchNormalization, self).__init__(**kwargs)\r\n        self.shape = shape\r\n        self.Trainable = Trainable\r\n        self.beta = tf.Variable(initial_value=tf.zeros(shape), trainable=Trainable)\r\n        self.gamma = tf.Variable(initial_value=tf.ones(shape), trainable=Trainable)\r\n        self.moving_mean = tf.Variable(initial_value=tf.zeros(self.shape), trainable=False)\r\n        self.moving_var = tf.Variable(initial_value=tf.ones(self.shape), trainable=False)\r\n\r\n    def update_var(self,inputs):\r\n        wu, sigma = tf.nn.moments(inputs, axes=[0, 1, 2], shift=None, keepdims=False, name=None)\r\n        var = tf.math.sqrt(sigma)\r\n        self.moving_mean = self.moving_mean * 0.09 + wu * 0.01\r\n        self.moving_var = self.moving_var * 0.09 + var * 0.01\r\n        return wu,var\r\n\r\n    def call(self, inputs):\r\n        wu, var = self.update_var(inputs)\r\n        return tf.nn.batch_normalization(inputs, wu, var, self.beta,\r\n                                         self.gamma, variance_epsilon=0.001)\r\n\r\n\r\n@tf.function\r\ndef train_step(model, inputs, label,optimizer):\r\n    with tf.GradientTape(persistent=False) as tape:\r\n        predictions = model(inputs, training=1)\r\n        loss = tf.keras.losses.mean_squared_error(predictions,label)\r\n    grads = tape.gradient(loss, model.trainable_variables)\r\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n\r\n\r\nif __name__=='__main__':\r\n    f=tf.ones([2,256,256,8])\r\n    label=tf.ones([2,256,256,8])\r\n    inputs = tf.keras.Input(shape=(256,256,8))\r\n    outputs=batchNormalization([8],True)(inputs)\r\n    Model = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n    Layer = batchNormalization([8],True)\r\n    print(len(Model.variables))\r\n    print(len(Model.trainable_variables))\r\n    print(len(Layer.variables))\r\n    print(len(Layer.trainable_variables))\r\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\r\n    for i in range(0,100):\r\n        train_step(Layer, f, label,optimizer)\r\n        # train_step(Model,f,label,optimizer)\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nWhen I trained the model, another error was raised,\r\nTypeError: An op outside of the function building code is being passed a \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a tf.init_scope in your function building code.\r\n\r\nWhen I comment the decorator '@tf.function' before the 'train_step' function, no error is raised. zbut I didn't know wheather it works like I want.", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/4dcfb2028a9a52919d8b86f79b707b1b/46325-tf-nightly.ipynb). Thanks!", "Hi @wangwei-cmd, you can [see in the docs here](https://www.tensorflow.org/guide/keras/custom_layers_and_models#layers_can_have_non-trainable_weights) that creating non trainable variables in `__init__` is permitted. \r\n\r\nI ran your code and if I commented out the following lines, you do see  `len(Model.variables)` is 4\r\n```\r\n#self.moving_mean = self.moving_mean * 0.09 + wu * 0.01\r\n#self.moving_var = self.moving_var * 0.09 + var * 0.01\r\n```\r\nI think the issue you're having is that you're not assigning the new value to the variable. Try the following:\r\n```\r\nself.moving_mean.assign(self.moving_mean * 0.09 + wu * 0.01)\r\nself.moving_var.assign(self.moving_var * 0.09 + var * 0.01)\r\n```\r\n", "Oh, Yes, thank you so much.  I can train the network after using assign.", "Great! Closing this issue now since a solution was found.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46325\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46325\">No</a>\n"]}, {"number": 46324, "title": "tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.", "body": "System information\uff1a\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):no\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu 18.04\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version (use command below):2.5\r\nPython version:3.6\r\nCUDA/cuDNN version: 11.1\r\nGPU model and memory:8.0.4\r\nDescribe the current behavior\uff1a\r\nI used tensorflow2.5 to train my code\uff0cafter training 33 epochs raise\uff1a\r\nep 35 i 0 psemce 0.0 bbvert 0.38795894 l2 0.09308199 ce 0.7902087 siou -0.49533176 bbscore 0.0025389553 pmask 1.5875475\r\nep 35 i 0 test psem 0.0 bbvert -0.036883593 l2 0.121321626 ce 0.37901375 siou -0.537219 bbscore 0.0023807494 pmask 0.4580555\r\ntest pred bborder [[0 1 2]]\r\nep 35 i 20 psemce 0.0 bbvert 0.14242041 l2 0.07736751 ce 0.61521786 siou -0.55016494 bbscore 0.0038914941 pmask 1.0464933\r\nep 35 i 20 test psem 0.0 bbvert 0.4905021 l2 0.08581522 ce 0.8831118 siou -0.47842494 bbscore 0.04460894 pmask 1.9587145\r\ntest pred bborder [[2 1 0]]\r\nep 35 i 40 psemce 0.0 bbvert -0.26186523 l2 0.050587684 ce 0.34921426 siou -0.66166717 bbscore 0.00023075327 pmask 0.40985933\r\nep 35 i 40 test psem 0.0 bbvert -0.29428068 l2 0.10698747 ce 0.16675441 siou -0.56802255 bbscore 0.0030464008 pmask 0.43296114\r\ntest pred bborder [[0 1 2]]\r\nep 35 i 60 psemce 0.0 bbvert 1.3711776 l2 0.066582106 ce 1.7335279 siou -0.42893246 bbscore 0.0043712487 pmask 1.8511868\r\nep 35 i 60 test psem 0.0 bbvert 0.23468393 l2 0.06658577 ce 0.6473511 siou -0.47925293 bbscore 0.0020650337 pmask 0.5931383\r\ntest pred bborder [[1 0 2]]\r\nep 35 i 80 psemce 0.0 bbvert -0.18187413 l2 0.08177448 ce 0.32304624 siou -0.58669484 bbscore 0.0040581333 pmask 0.3738186\r\nep 35 i 80 test psem 0.0 bbvert 0.0770213 l2 0.0655105 ce 0.5536749 siou -0.5421641 bbscore 0.002022297 pmask 1.5775248\r\ntest pred bborder [[2 1 0]]\r\nmodel saved in :  ./log/train_mod/model035.cptk\r\nepoch  35 end time is : 2021-01-11 10:38:50.641399\r\ntrain files shuffled!\r\nis training ep :  36\r\ntotal train batch num: 100\r\nep 36 i 0 psemce 0.0 bbvert 1.249488 l2 0.091530986 ce 1.5373621 siou -0.3794051 bbscore 0.0018183877 pmask 2.2903516\r\nep 36 i 0 test psem 0.0 bbvert -0.21928397 l2 0.051158678 ce 0.38333455 siou -0.6537772 bbscore 0.0016100239 pmask 1.375641\r\ntest pred bborder [[1 2 0]]\r\n2021-01-11 10:38:53.433607: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: matrix contains invalid numeric entries\r\nTraceback (most recent call last):\r\n\r\n  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 624, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"/home/liu/disk1/3DBoNetPoint818a/helper_net.py\", line 122, in assign_mappings_valid_only\r\n    row_ind, col_ind = linear_sum_assignment(valid_cost)\r\n\r\n  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/scipy/optimize/_lsap.py\", line 93, in linear_sum_assignment\r\n    raise ValueError(\"matrix contains invalid numeric entries\")\r\n\r\nValueError: matrix contains invalid numeric entries\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1375, in _do_call\r\n    return fn(*args)\r\n  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1360, in _run_fn\r\n    target_list, run_metadata)\r\n  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1453, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument: ValueError: matrix contains invalid numeric entries\r\nTraceback (most recent call last):\r\n\r\n  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 624, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"/home/liu/disk1/3DBoNetPoint818a/helper_net.py\", line 122, in assign_mappings_valid_only\r\n    row_ind, col_ind = linear_sum_assignment(valid_cost)\r\n\r\n  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/scipy/optimize/_lsap.py\", line 93, in linear_sum_assignment\r\n    raise ValueError(\"matrix contains invalid numeric entries\")\r\n\r\nValueError: matrix contains invalid numeric entries\r\n\r\n\r\n\t [[{{node bbox/PyFunc}}]]\r\n  (1) Invalid argument: ValueError: matrix contains invalid numeric entries\r\nTraceback (most recent call last):\r\n\r\n  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 624, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"/home/liu/disk1/3DBoNetPoint818a/helper_net.py\", line 122, in assign_mappings_valid_only\r\n    row_ind, col_ind = linear_sum_assignment(valid_cost)\r\n\r\n  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/scipy/optimize/_lsap.py\", line 93, in linear_sum_assignment\r\n    raise ValueError(\"matrix contains invalid numeric entries\")\r\n\r\nValueError: matrix contains invalid numeric entries\r\n\r\n\r\n\t [[{{node bbox/PyFunc}}]]\r\n\t [[gradients/backbone/fa_layer1/ThreeInterpolate_grad/ThreeInterpolateGrad/_407]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"main_train.py\", line 78, in <module>\r\n    train(net, data,configs=configs)\r\n  File \"main_train.py\", line 33, in train\r\n    feed_dict={net.X_pc:bat_pc[:, :, 0:6], net.Y_bbvert:bat_bbvert, net.Y_pmask:bat_pmask[:,:,:], net.Y_psem:bat_psem_onehot[:,:,:], net.lr:l_rate, net.is_train:True})\r\n  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 968, in run\r\n    run_metadata_ptr)\r\n  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1191, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1369, in _do_run\r\n    run_metadata)\r\n  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1394, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument: ValueError: matrix contains invalid numeric entries\r\nTraceback (most recent call last):\r\n\r\n  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 624, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"/home/liu/disk1/3DBoNetPoint818a/helper_net.py\", line 122, in assign_mappings_valid_only\r\n    row_ind, col_ind = linear_sum_assignment(valid_cost)\r\n\r\n  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/scipy/optimize/_lsap.py\", line 93, in linear_sum_assignment\r\n    raise ValueError(\"matrix contains invalid numeric entries\")\r\n\r\nValueError: matrix contains invalid numeric entries\r\n\r\n\r\n\t [[node bbox/PyFunc (defined at /home/liu/disk1/3DBoNetPoint818a/helper_net.py:134) ]]\r\n  (1) Invalid argument: ValueError: matrix contains invalid numeric entries\r\nTraceback (most recent call last):\r\n\r\n  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 624, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"/home/liu/disk1/3DBoNetPoint818a/helper_net.py\", line 122, in assign_mappings_valid_only\r\n    row_ind, col_ind = linear_sum_assignment(valid_cost)\r\n\r\n  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/scipy/optimize/_lsap.py\", line 93, in linear_sum_assignment\r\n    raise ValueError(\"matrix contains invalid numeric entries\")\r\n\r\nValueError: matrix contains invalid numeric entries\r\n\r\n\r\n\t [[node bbox/PyFunc (defined at /home/liu/disk1/3DBoNetPoint818a/helper_net.py:134) ]]\r\n\t [[gradients/backbone/fa_layer1/ThreeInterpolate_grad/ThreeInterpolateGrad/_407]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node bbox/PyFunc:\r\n Y_bbvert (defined at /home/liu/disk1/3DBoNetPoint818a/main_3D_BoNet.py:226)\t\r\n bbox/add_11 (defined at /home/liu/disk1/3DBoNetPoint818a/helper_net.py:190)\r\n\r\nInput Source operations connected to node bbox/PyFunc:\r\n Y_bbvert (defined at /home/liu/disk1/3DBoNetPoint818a/main_3D_BoNet.py:226)\t\r\n bbox/add_11 (defined at /home/liu/disk1/3DBoNetPoint818a/helper_net.py:190)\r\n\r\nOriginal stack trace for 'bbox/PyFunc':\r\n  File \"main_train.py\", line 72, in <module>\r\n    net.build_graph()\r\n  File \"/home/liu/disk1/3DBoNetPoint818a/main_3D_BoNet.py\", line 246, in build_graph\r\n    self.y_bbvert_pred, self.pred_bborder = Ops.bbvert_association(self.X_pc,  self.y_bbvert_pred_raw, self.Y_bbvert, label=bbox_criteria)\r\n  File \"/home/liu/disk1/3DBoNetPoint818a/helper_net.py\", line 208, in bbvert_association\r\n    pred_bborder, association_score_min = Ops.hungarian(associate_maxtrix, bb_gt=Y_bbvert)\r\n  File \"/home/liu/disk1/3DBoNetPoint818a/helper_net.py\", line 134, in hungarian\r\n    ordering, loss_total = tf.compat.v1.py_func(assign_mappings_valid_only, [loss_matrix, bb_gt], [tf.int32, tf.float32])\r\n  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 337, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py\", line 206, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 636, in py_func\r\n    return py_func_common(func, inp, Tout, stateful, name=name)\r\n  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 617, in py_func_common\r\n    func=func, inp=inp, Tout=Tout, stateful=stateful, eager=False, name=name)\r\n  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 352, in _internal_py_func\r\n    input=inp, token=token, Tout=Tout, name=name)\r\n  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/ops/gen_script_ops.py\", line 150, in py_func\r\n    \"PyFunc\", input=input, token=token, Tout=Tout, name=name)\r\n  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 750, in _apply_op_helper\r\n    attrs=attr_protos, op_def=op_def)\r\n  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3543, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2023, in __init__\r\n    self._traceback = tf_stack.extract_stack_for_node(self._c_op)\r\n\r\nmain_train.py is as below\uff1a\r\n# _*_ coding:utf-8 _*_\r\nimport os\r\n# from helper_data_plot import Plot as Plot\r\n\r\nimport glob\r\nimport datetime\r\nfrom tensorflow.compat.v1 import ConfigProto\r\nfrom tensorflow.compat.v1 import InteractiveSession\r\nconfig = ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = InteractiveSession(config=config)\r\ndef train(net, data,configs):\r\n\tprint(\"train start time is : \",datetime.datetime.now())\r\n\tfor ep in range(configs.epoch):\r\n\t\tl_rate = max(0.0005/(2**(ep//20)), 0.00001) #\u5b66\u4e60\u7387\r\n\r\n\t\tdata.shuffle_train_files(ep) #\u6253\u4e71\u987a\u5e8f\r\n\r\n\t\ttotal_train_batch_num = data.total_train_batch_num \r\n\t\tprint(\"is training ep : \",ep)\r\n\t\tprint('total train batch num:', total_train_batch_num)\r\n\t\t\r\n\t\tfor i in range(total_train_batch_num):\r\n\t\t\t###### training\r\n\t\t\tbat_pc,bat_psem_onehot, bat_bbvert, bat_pmask = data.load_train_one_batch(i)\r\n\t\t\t# print(\"point num is : \",bat_pc.shape[1])\r\n\t\t\t# print(\"bat_pc : \",bat_pc.shape)#(4,4096,12) (1,11322,9)\r\n\t\t\t# print(\"bat_psem_onehot : \",bat_psem_onehot.shape)#(4,4096,13) (1,11322,2)\r\n\t\t\t# print(\"bat_bbvert : \",bat_bbvert.shape)#(4,24,2,3) (1,24,2,3)\r\n\t\t\t# print(\"bat_pmask : \",bat_pmask.shape)#(4,24,4096) (1,24,11322)\r\n\r\n\t\t\t_, ls_psemce, ls_bbvert_all, ls_bbvert_l2, ls_bbvert_ce, ls_bbvert_iou, ls_bbscore, ls_pmask = net.sess.run([\r\n\t\t\tnet.optim, net.psemce_loss, net.bbvert_loss, net.bbvert_loss_l2, net.bbvert_loss_ce, net.bbvert_loss_iou,net.bbscore_loss, net.pmask_loss],\r\n\t\t\tfeed_dict={net.X_pc:bat_pc[:, :, 0:6], net.Y_bbvert:bat_bbvert, net.Y_pmask:bat_pmask[:,:,:], net.Y_psem:bat_psem_onehot[:,:,:], net.lr:l_rate, net.is_train:True})\r\n\r\n\t\t\tif i%20==0:#\u6d4b\u8bd5val\u6570\u636e\r\n\t\t\t\tsum_train = net.sess.run(net.sum_merged,\r\n\t\t\t\tfeed_dict={net.X_pc: bat_pc[:, :, 0:6], net.Y_bbvert: bat_bbvert, net.Y_pmask: bat_pmask, net.Y_psem: bat_psem_onehot, net.lr: l_rate, net.is_train: False})\r\n\t\t\t\r\n\t\t\t\tprint ('ep', ep, 'i', i, 'psemce', ls_psemce, 'bbvert', ls_bbvert_all, 'l2', ls_bbvert_l2, 'ce', ls_bbvert_ce, 'siou', ls_bbvert_iou, 'bbscore', ls_bbscore, 'pmask', ls_pmask)\r\n\r\n\t\t\t###### random testing\r\n\t\t\tif i%20==0:\r\n\r\n\t\t\t\t#\u76f8\u6bd4 \u8bad\u7ec3\u7684\u65f6\u5019 \u5c11\u4e86 net.optim \u591a\u4e86 net.sum_merged, net.pred_bborder\r\n\t\t\t\tbat_pc, bat_psem_onehot, bat_bbvert, bat_pmask = data.load_test_one_batch()\r\n\r\n\t\t\t\tls_psemce, ls_bbvert_all, ls_bbvert_l2, ls_bbvert_ce, ls_bbvert_iou, ls_bbscore, ls_pmask, sum_test, pred_bborder = net.sess.run([\r\n\t\t\t\tnet.psemce_loss, net.bbvert_loss, net.bbvert_loss_l2, net.bbvert_loss_ce, net.bbvert_loss_iou, net.bbscore_loss, net.pmask_loss, net.sum_merged, net.pred_bborder],\r\n\t\t\t\tfeed_dict={net.X_pc:bat_pc[:, :, 0:6], net.Y_bbvert:bat_bbvert, net.Y_pmask:bat_pmask, net.Y_psem:bat_psem_onehot, net.is_train:False})\r\n\t\t\t\t\r\n\t\t\t\tprint('ep',ep,'i',i,'test psem', ls_psemce, 'bbvert', ls_bbvert_all, 'l2', ls_bbvert_l2, 'ce', ls_bbvert_ce, 'siou', ls_bbvert_iou, 'bbscore', ls_bbscore, 'pmask', ls_pmask)\r\n\t\t\t\tprint('test pred bborder', pred_bborder)\r\n\r\n\t\t\t###### saving model\r\n\t\t\tif ep % 1 == 0 and i == total_train_batch_num - 1:\r\n\t\t\t\tsave_path=net.train_mod_dir + 'model' + str(ep).zfill(3) + '.cptk'\r\n\t\t\t\tnet.saver.save(net.sess, save_path=save_path)\r\n\t\t\t\tprint(\"model saved in : \",save_path)\r\n\t\t\t\tprint(\"epoch \",ep,\"end time is :\",datetime.datetime.now())\r\n\r\n############\r\nif __name__=='__main__':\r\n\tos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\r\n\tos.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'  ## specify the GPU to use\r\n\r\n\tfrom main_3D_BoNet import BoNet\r\n\tfrom helper_data_s3dis import Data_Configs as Data_Configs\r\n\r\n\tconfigs = Data_Configs()\r\n\tnet = BoNet(configs = configs)\r\n\tnet.creat_folders(name='log', re_train=True)\r\n\tnet.build_graph()\r\n\r\n\t####\r\n\tfrom helper_data_s3dis import Dataset_PointCloud as Data\r\n\tdata=Data(configs = configs)\r\n\tdata.check_mat_file_exists()# check the data file for  input \r\n\ttrain(net, data,configs=configs)\r\nmain_3D_Bonet.py is as below\uff1a\r\n# _*_ coding:utf-8 _*_\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\nimport os\r\nimport shutil\r\nfrom helper_net import Ops as Ops\r\n\r\nclass BoNet:\r\n\tdef __init__(self, configs):\r\n\t\tself.points_cc = configs.points_cc#6\r\n\t\tself.sem_num = configs.sem_num#2\r\n\t\tself.bb_num = configs.ins_max_num#24\r\n\r\n\tdef creat_folders(self, name='log', re_train=False):\r\n\t\tself.train_mod_dir = './'+name+'/train_mod/'\r\n\t\tself.train_sum_dir = './'+name+'/train_sum/'\r\n\t\tself.test_sum_dir = './'+name+'/test_sum/'\r\n\t\tprint (\"re_train:\", re_train)\r\n\t\tdef tp(path):\r\n\t\t\tif os.path.exists(path):\r\n\t\t\t\tif re_train:\r\n\t\t\t\t\tprint (path, \": files kept!\")\r\n\t\t\t\telse:\r\n\t\t\t\t\tshutil.rmtree(path)\r\n\t\t\t\t\tos.makedirs(path)\r\n\t\t\t\t\tprint (path, ': deleted and then created!')\r\n\t\t\telse:\r\n\t\t\t\tos.makedirs(path)\r\n\t\t\t\tprint (path, ': created!')\r\n\t\ttp(self.test_sum_dir)\r\n\t\ttp(self.train_sum_dir)\r\n\t\ttp(self.train_mod_dir)\r\n\r\n\t######  1. backbone + sem\r\n\tdef backbone_pointnet(self, X_pc, is_train):\r\n\t\t[_, _, points_cc] = X_pc.get_shape()\r\n\t\tpoints_num = tf.shape(X_pc)[1]\r\n\t\tX_pc = tf.reshape(X_pc, [-1, points_num, int(points_cc), 1])\r\n\r\n\t\tl1 = Ops.xxlu(Ops.conv2d(X_pc, k=(1, points_cc), out_c=64, str=1, pad='VALID', name='l1'), label='lrelu')\r\n\t\tl2 = Ops.xxlu(Ops.conv2d(l1, k=(1, 1), out_c=64, str=1, pad='VALID', name='l2'), label='lrelu')\r\n\t\tl3 = Ops.xxlu(Ops.conv2d(l2, k=(1, 1), out_c=64, str=1, pad='VALID', name='l3'), label='lrelu')\r\n\t\tl4 = Ops.xxlu(Ops.conv2d(l3, k=(1, 1), out_c=128, str=1, pad='VALID', name='l4'), label='lrelu')\r\n\t\tl5 = Ops.xxlu(Ops.conv2d(l4, k=(1, 1), out_c=1024, str=1, pad='VALID', name='l5'), label='lrelu')\r\n\t\tglobal_features = tf.reduce_max(l5, axis=1, name='maxpool')\r\n\t\tglobal_features = tf.reshape(global_features, [-1, int(l5.shape[-1])])\r\n\t\tpoint_features = tf.reshape(l5, [-1, points_num, int(l5.shape[-1])])\r\n\r\n\t\t####  sem\r\n\t\tg1 = Ops.xxlu(Ops.fc(global_features, out_d=256, name='semg1'), label='lrelu')\r\n\t\tg2 = Ops.xxlu(Ops.fc(g1, out_d=128, name='semg2'), label='lrelu')\r\n\t\tsem1 = tf.tile(g2[:,None,None,:], [1, points_num, 1, 1])\r\n\t\tsem1 = tf.concat([l5, sem1], axis=-1)\r\n\t\tsem1 = Ops.xxlu(Ops.conv2d(sem1, k=(1,1), out_c=512, str=1, pad='VALID', name='sem1'), label='lrelu')\r\n\t\tsem2 = Ops.xxlu(Ops.conv2d(sem1, k=(1, 1), out_c=256, str=1, pad='VALID', name='sem2'), label='lrelu')\r\n\t\tsem3 = Ops.xxlu(Ops.conv2d(sem2, k=(1, 1), out_c=128, str=1, pad='VALID', name='sem3'), label='lrelu')\r\n\t\tsem3 = Ops.dropout(sem3, keep_prob=0.5, is_train=is_train, name='sem3_dropout')\r\n\t\tsem4 = Ops.conv2d(sem3, k=(1, 1), out_c=self.sem_num, str=1, pad='VALID', name='sem4')\r\n\t\tsem4 = tf.reshape(sem4, [-1, points_num, self.sem_num])\r\n\t\tself.y_psem_logits = sem4\r\n\t\ty_sem_pred = tf.nn.softmax(self.y_psem_logits, name='y_sem_pred')\r\n\r\n\t\treturn point_features, global_features, y_sem_pred\r\n\r\n\t# def backbone_pointnet2(self, X_pc, is_train=None):\r\n\t# \timport helper_pointnet2 as pnet2\r\n\t# \tpoints_num = tf.shape(X_pc)[1] #\u6bcf\u4e2abatch\u7684\u70b9\u7684\u6570\u91cf \u4e0d\u5b9a\r\n\t# \tl0_xyz = X_pc[:,:,0:3] # xyz\r\n\t# \t#\u53ef\u8c03\u53c2\u6570\uff1a\r\n\t# \tl1_xyz, l1_points, l1_indices = pnet2.pointnet_sa_module(l0_xyz, None, npoint=16384, radius=0.025, nsample=8,\r\n\t# \t\tmlp=[8, 8, 16], mlp2=None, group_all=False, is_training=None, bn_decay=None, scope='layer1')\r\n\t# \tl2_xyz, l2_points, l2_indices = pnet2.pointnet_sa_module(l1_xyz, l1_points, npoint=4096, radius=0.05, nsample=16,\r\n\t# \t\tmlp=[16, 16,32], mlp2=None, group_all=False, is_training=None, bn_decay=None, scope='layer2')\r\n\t# \tl3_xyz, l3_points, l3_indices = pnet2.pointnet_sa_module(l2_xyz, l2_points, npoint=1024, radius=0.1, nsample=32,\r\n\t# \t    mlp=[32, 32, 64], mlp2=None, group_all=False, is_training=None, bn_decay=None, scope='layer3')\r\n\t# \tl4_xyz, l4_points, l4_indices = pnet2.pointnet_sa_module(l3_xyz, l3_points, npoint=256, radius=0.2, nsample=64,\r\n\t# \t    mlp=[64, 64, 128], mlp2=None, group_all=False, is_training=None, bn_decay=None, scope='layer4')\r\n\t# \tl5_xyz, l5_points, l5_indices = pnet2.pointnet_sa_module(l4_xyz, l4_points, npoint=64, radius=0.4, nsample=128,\r\n\t# \t    mlp=[128, 128, 256], mlp2=None, group_all=False, is_training=None, bn_decay=None, scope='layer5') \r\n\r\n\t# \tl6_xyz, l6_points, l6_indices = pnet2.pointnet_sa_module(l5_xyz, l5_points, npoint=None, radius=None, nsample=None,\r\n\t# \t\tmlp=[256, 256, 512], mlp2=None, group_all=True, is_training=None, bn_decay=None, scope='layer6')\r\n\r\n\t# \t# Feature Propagation layers   \r\n\t# \tl5_points = pnet2.pointnet_fp_module(l5_xyz, l6_xyz, l5_points, l6_points, [256, 256], is_training=None, bn_decay=None, scope='fa_layer1')\r\n\t# \tl4_points = pnet2.pointnet_fp_module(l4_xyz, l5_xyz, l4_points, l5_points, [256, 256], is_training=None, bn_decay=None,scope='fa_layer2')\r\n\t# \tl3_points = pnet2.pointnet_fp_module(l3_xyz, l4_xyz, l3_points, l4_points, [256, 256], is_training=None, bn_decay=None,scope='fa_layer3')\r\n\t# \tl2_points = pnet2.pointnet_fp_module(l2_xyz, l3_xyz, l2_points, l3_points, [256, 256], is_training=None, bn_decay=None,scope='fa_layer4')\r\n\t# \tl1_points = pnet2.pointnet_fp_module(l1_xyz, l2_xyz, l1_points, l2_points, [256, 128], is_training=None, bn_decay=None,scope='fa_layer5')\r\n\t# \tl0_points = pnet2.pointnet_fp_module(l0_xyz, l1_xyz, l0_xyz, l1_points,[128, 128, 128, 128], is_training=None, bn_decay=None, scope='fa_layer6')\r\n\t# \tglobal_features = tf.reshape(l6_points, [-1, 512])\r\n\t# \tpoint_features = l0_points\r\n\r\n\t# \t# sem\r\n\t# \tl0_points = l0_points[:,:,None,:]\r\n\t# \tsem1 = Ops.xxlu(Ops.conv2d(l0_points, k=(1, 1), out_c=128, str=1, pad='VALID', name='sem1'), label='lrelu')\r\n\t# \tsem2 = Ops.xxlu(Ops.conv2d(sem1, k=(1, 1), out_c=64, str=1, pad='VALID', name='sem2'), label='lrelu')\r\n\t# \tsem2 = Ops.dropout(sem2, keep_prob=0.5, is_train=is_train, name='sem2_dropout')\r\n\t# \tsem3 = Ops.conv2d(sem2, k=(1, 1), out_c=self.sem_num, str=1, pad='VALID', name='sem3')\r\n\t# \tsem3 = tf.reshape(sem3, [-1, points_num, self.sem_num])\r\n\t# \tself.y_psem_logits = sem3\r\n\t# \ty_sem_pred = tf.nn.softmax(self.y_psem_logits, name='y_sem_pred')\r\n\r\n\t# \treturn point_features, global_features, y_sem_pred\r\n\r\n\tdef backbone_pointnet2(self, X_pc, is_train=None):\r\n\t\timport helper_pointnet2 as pnet2\r\n\t\tpoints_num = tf.shape(X_pc)[1]\r\n\t\tl0_xyz = X_pc[:,:,0:3]\r\n\r\n\t\tl1_xyz, l1_points, l1_indices = pnet2.pointnet_sa_module(l0_xyz, None, npoint=1024, radius=0.1, nsample=32,\r\n\t\t\tmlp=[32, 32, 64], mlp2=None, group_all=False, is_training=None, bn_decay=None, scope='layer1')\r\n\t\tl2_xyz, l2_points, l2_indices = pnet2.pointnet_sa_module(l1_xyz, l1_points, npoint=256, radius=0.2, nsample=64,\r\n\t\t\tmlp=[64, 64, 128], mlp2=None, group_all=False, is_training=None, bn_decay=None, scope='layer2')\r\n\t\tl3_xyz, l3_points, l3_indices = pnet2.pointnet_sa_module(l2_xyz, l2_points, npoint=64, radius=0.4, nsample=128,\r\n\t\t    mlp=[128, 128, 256], mlp2=None, group_all=False, is_training=None, bn_decay=None, scope='layer3')\r\n\t\tl4_xyz, l4_points, l4_indices = pnet2.pointnet_sa_module(l3_xyz, l3_points, npoint=None, radius=None, nsample=None,\r\n\t\t\tmlp=[256, 256, 512], mlp2=None, group_all=True, is_training=None, bn_decay=None, scope='layer4')\r\n\r\n\t\t# Feature Propagation layers\r\n\t\tl3_points = pnet2.pointnet_fp_module(l3_xyz, l4_xyz, l3_points, l4_points, [256, 256], is_training=None, bn_decay=None, scope='fa_layer1')\r\n\t\tl2_points = pnet2.pointnet_fp_module(l2_xyz, l3_xyz, l2_points, l3_points, [256, 256], is_training=None, bn_decay=None,scope='fa_layer2')\r\n\t\tl1_points = pnet2.pointnet_fp_module(l1_xyz, l2_xyz, l1_points, l2_points, [256, 128], is_training=None, bn_decay=None,scope='fa_layer3')\r\n\t\tl0_points = pnet2.pointnet_fp_module(l0_xyz, l1_xyz, l0_xyz, l1_points,[128, 128, 128, 128], is_training=None, bn_decay=None, scope='fa_layer6')\r\n\t\tglobal_features = tf.reshape(l4_points, [-1, 512])\r\n\t\tpoint_features = l0_points\r\n\r\n\t\t# sem\r\n\t\tl0_points = l0_points[:,:,None,:]\r\n\t\tsem1 = Ops.xxlu(Ops.conv2d(l0_points, k=(1, 1), out_c=128, str=1, pad='VALID', name='sem1'), label='lrelu')\r\n\t\tsem2 = Ops.xxlu(Ops.conv2d(sem1, k=(1, 1), out_c=64, str=1, pad='VALID', name='sem2'), label='lrelu')\r\n\t\tsem2 = Ops.dropout(sem2, keep_prob=0.5, is_train=is_train, name='sem2_dropout')\r\n\t\tsem3 = Ops.conv2d(sem2, k=(1, 1), out_c=self.sem_num, str=1, pad='VALID', name='sem3')\r\n\t\tsem3 = tf.reshape(sem3, [-1, points_num, self.sem_num])\r\n\t\tself.y_psem_logits = sem3\r\n\t\ty_sem_pred = tf.nn.softmax(self.y_psem_logits, name='y_sem_pred')\r\n\r\n\t\treturn point_features, global_features, y_sem_pred\r\n\r\n\tdef backbone_pointconv(self, X_pc, is_training=None,sigma=0.05,bn_decay=None,weight_decay=None):\r\n\t\timport helper_pointnet2 as pnet2\r\n\t\tpoints_num = tf.shape(X_pc)[1] #\u6bcf\u4e2abatch\u7684\u70b9\u7684\u6570\u91cf \u4e0d\u5b9a\r\n\t\tl0_xyz = X_pc[:,:,0:3] # xyz\r\n\t\tl0_points=l0_xyz\r\n\t\t# k=16\r\n\t\t#\u53ef\u8c03\u53c2\u6570\uff1a\r\n\t\tl1_xyz, l1_points = pnet2.feature_encoding_layer(l0_xyz, l0_points, npoint=1024, radius = 0.1, sigma = sigma, K=32, mlp=[32,32,64], is_training=is_training, bn_decay=bn_decay, weight_decay = weight_decay, scope='layer1')\r\n\t\tl2_xyz, l2_points = pnet2.feature_encoding_layer(l1_xyz, l1_points, npoint=256, radius = 0.2, sigma = 2 * sigma, K=32, mlp=[64,64,128], is_training=is_training, bn_decay=bn_decay, weight_decay = weight_decay, scope='layer2')\r\n\t\tl3_xyz, l3_points = pnet2.feature_encoding_layer(l2_xyz, l2_points, npoint=64, radius = 0.4, sigma = 4 * sigma, K=32, mlp=[128,128,256], is_training=is_training, bn_decay=bn_decay, weight_decay = weight_decay, scope='layer3')\r\n\t\tl4_xyz, l4_points = pnet2.feature_encoding_layer(l3_xyz, l3_points, npoint=36, radius = 0.8, sigma = 8 * sigma, K=32, mlp=[256,256,512], is_training=is_training, bn_decay=bn_decay, weight_decay = weight_decay, scope='layer4')\r\n\t\tl5_xyz, l5_points, l5_indices = pnet2.pointnet_sa_module(l4_xyz, l4_points, npoint=None, radius=None, nsample=None,mlp=[256,  512], mlp2=None, group_all=True, is_training=None, bn_decay=None, scope='layer5')\r\n\t\t# Feature decoding layers\r\n\t\tl3_points = pnet2.feature_decoding_layer(l3_xyz, l4_xyz, l3_points, l4_points, 0.8, 8 * sigma, 32, [256,256], is_training, bn_decay, weight_decay, scope='fa_layer1')\r\n\t\tl2_points = pnet2.feature_decoding_layer(l2_xyz, l3_xyz, l2_points, l3_points, 0.4, 4 * sigma, 32, [256,256], is_training, bn_decay, weight_decay, scope='fa_layer2')\r\n\t\tl1_points = pnet2.feature_decoding_layer(l1_xyz, l2_xyz, l1_points, l2_points, 0.2, 2 * sigma, 32, [256,128], is_training, bn_decay, weight_decay, scope='fa_layer3')\r\n\t\tl0_points = pnet2.feature_decoding_layer(l0_xyz, l1_xyz, l0_points, l1_points, 0.1, sigma, 32, [128,128,128], is_training, bn_decay, weight_decay, scope='fa_layer4')\r\n\r\n\t\tglobal_features = tf.reshape((l5_points), [-1, 512])\r\n\t\tpoint_features = l0_points\r\n\t\t# sem\r\n\t\tl0_points = l0_points[:,:,None,:]\r\n\t\tsem1 = Ops.xxlu(Ops.conv2d(l0_points, k=(1, 1), out_c=128, str=1, pad='VALID', name='sem1'), label='lrelu')\r\n\t\tsem2 = Ops.xxlu(Ops.conv2d(sem1, k=(1, 1), out_c=64, str=1, pad='VALID', name='sem2'), label='lrelu')\r\n\t\tsem2 = Ops.dropout(sem2, keep_prob=0.5, is_train=is_training, name='sem2_dropout')\r\n\t\tsem3 = Ops.conv2d(sem2, k=(1, 1), out_c=self.sem_num, str=1, pad='VALID', name='sem3')\r\n\t\tsem3 = tf.reshape(sem3, [-1, points_num, self.sem_num])\r\n\t\tself.y_psem_logits = sem3\r\n\t\ty_sem_pred = tf.nn.softmax(self.y_psem_logits, name='y_sem_pred')\r\n\r\n\t\treturn point_features, global_features, y_sem_pred\r\n\t######  2. bbox\r\n\tdef bbox_net(self, global_features):\r\n\t\t'''\u7531\u5168\u5c40\u7279\u5f81\u76f4\u63a5\u8f93\u51fa\u56fa\u5b9a\u6570\u91cf\u7684bb \u548c\u76f8\u5e94\u7684\u5206\u6570 \r\n\t\t'''\r\n\t\tb1 = Ops.xxlu(Ops.fc(global_features, out_d= 512, name='b1'), label='lrelu')\r\n\t\tb2 = Ops.xxlu(Ops.fc(b1, out_d= 256, name='b2'), label='lrelu')\r\n\r\n\t\t#### sub branch 1\r\n\t\tb3 = Ops.xxlu(Ops.fc(b2, out_d=256, name='b3'), label='lrelu')\r\n\t\tbbvert = Ops.fc(b3, out_d=self.bb_num * 2 * 3, name='bbvert')\r\n\t\tbbvert = tf.reshape(bbvert, [-1, self.bb_num, 2, 3])\r\n\t\tpoints_min = tf.reduce_min(bbvert, axis=-2)[:, :, None, :]\r\n\t\tpoints_max = tf.reduce_max(bbvert, axis=-2)[:, :, None, :]\r\n\t\ty_bbvert_pred = tf.concat([points_min, points_max], axis=-2, name='y_bbvert_pred')\r\n\r\n\t\t#### sub branch 2\r\n\t\tb4 = Ops.xxlu(Ops.fc(b2, out_d=256, name='b4'), label='lrelu')\r\n\t\ty_bbscore_pred = tf.sigmoid(Ops.fc(b4, out_d=self.bb_num * 1, name='y_bbscore_pred'))\r\n\r\n\t\treturn y_bbvert_pred, y_bbscore_pred\r\n\r\n\t######  3. pmask\r\n\tdef pmask_net(self, point_features, global_features, bbox, bboxscore):\r\n\t\tp_f_num = int(point_features.shape[-1])\r\n\t\tp_num = tf.shape(point_features)[1]\r\n\t\tbb_num = int(bbox.shape[1])\r\n\r\n\t\tglobal_features = tf.tile(Ops.xxlu(Ops.fc(global_features, out_d=256, name='down_g1'), label='lrelu')[:,None,None,:], [1, p_num, 1, 1])\r\n\t\tpoint_features = Ops.xxlu(Ops.conv2d(point_features[:,:,:,None],k=(1, p_f_num), out_c=256, str=1,name='down_p1',pad='VALID'), label='lrelu')\r\n\t\tpoint_features = tf.concat([point_features, global_features], axis=-1)\r\n\t\tpoint_features = Ops.xxlu(Ops.conv2d(point_features, k=(1,int(point_features.shape[-2])), out_c=128, str=1, pad='VALID', name='down_p2'), label='lrelu')\r\n\t\tpoint_features = Ops.xxlu(Ops.conv2d(point_features, k=(1, int(point_features.shape[-2])), out_c=128, str=1, pad='VALID',name='down_p3'), label='lrelu')\r\n\t\tpoint_features = tf.squeeze(point_features, axis=-2)\r\n\r\n\t\tbbox_info = tf.tile(tf.concat([tf.reshape(bbox, [-1, bb_num, 6]), bboxscore[:,:,None]],axis=-1)[:,:,None,:], [1,1,p_num,1])\r\n\t\tpmask0 = tf.tile(point_features[:,None,:,:], [1, bb_num, 1, 1])\r\n\t\tpmask0 = tf.concat([pmask0, bbox_info], axis=-1)\r\n\t\tpmask0 = tf.reshape(pmask0, [-1, p_num, int(pmask0.shape[-1]), 1])\r\n\r\n\t\tpmask1 = Ops.xxlu(Ops.conv2d(pmask0, k=(1,int(pmask0.shape[-2])), out_c=64, str=1, pad='VALID', name='pmask1'), label='lrelu')\r\n\t\tpmask2 = Ops.xxlu(Ops.conv2d(pmask1, k=(1, 1), out_c=32, str=1, pad='VALID', name='pmask2'),label='lrelu')\r\n\t\tpmask3 = Ops.conv2d(pmask2, k=(1,1), out_c=1, str=1, pad='VALID', name='pmask3')\r\n\t\tpmask3 = tf.reshape(pmask3, [-1, bb_num, p_num])\r\n\r\n\t\ty_pmask_logits = pmask3\r\n\t\ty_pmask_pred = tf.nn.sigmoid(y_pmask_logits, name='y_pmask_pred')\r\n\r\n\t\treturn y_pmask_pred\r\n\r\n\tdef build_graph(self, GPU='0'):\r\n\t\t'''\r\n\t\t\t\u6574\u4e2a\u7684\u7f51\u7edc\u7ed3\u6784\u90fd\u5728\u8fd9\u91cc\r\n\t\t'''\r\n\t\t#######   1. define inputs\r\n\t\tself.X_pc = tf.placeholder(shape=[None, None, self.points_cc], dtype=tf.float32, name='X_pc')\r\n\t\tself.Y_bbvert = tf.placeholder(shape=[None, self.bb_num, 2, 3], dtype=tf.float32, name='Y_bbvert')\r\n\t\tself.Y_pmask = tf.placeholder(shape=[None, self.bb_num, None], dtype=tf.float32, name='Y_pmask')\r\n\t\tself.Y_psem = tf.placeholder(shape=[None, None, self.sem_num], dtype=tf.float32, name='Y_psem')\r\n\t\tself.is_train = tf.placeholder(dtype=tf.bool, name='is_train')\r\n\t\tself.lr = tf.placeholder(dtype=tf.float32, name='lr')\r\n\r\n\t\t#######  2. define networks, losses\r\n\t\twith tf.variable_scope('backbone'):\r\n\t\t\t#self.point_features, self.global_features, self.y_psem_pred = self.backbone_pointnet(self.X_pc, self.is_train)\r\n\t\t\tself.point_features, self.global_features, self.y_psem_pred = self.backbone_pointnet2(self.X_pc, self.is_train)\r\n\t\t\t# self.point_features, self.global_features, self.y_psem_pred =self.backbone_pointconv(self.X_pc, is_training=self.is_train)\r\n\r\n\t\t\t### loss\r\n\t\t\tself.psemce_loss = Ops.get_loss_psem_ce(self.y_psem_logits, self.Y_psem)\r\n\t\t\tself.sum_psemce_loss = tf.summary.scalar('psemce_loss', self.psemce_loss)\r\n\r\n\t\twith tf.variable_scope('bbox'):\r\n\t\t\tself.y_bbvert_pred_raw, self.y_bbscore_pred_raw = self.bbox_net(self.global_features)\r\n\t\t\t#### association, only used for training\r\n\t\t\tbbox_criteria = 'use_all_ce_l2_iou'\r\n\t\t\tself.y_bbvert_pred, self.pred_bborder = Ops.bbvert_association(self.X_pc,  self.y_bbvert_pred_raw, self.Y_bbvert, label=bbox_criteria)\r\n\t\t\tself.y_bbscore_pred = Ops.bbscore_association(self.y_bbscore_pred_raw, self.pred_bborder)\r\n\r\n\t\t\t### loss\r\n\t\t\tself.bbvert_loss, self.bbvert_loss_l2, self.bbvert_loss_ce, self.bbvert_loss_iou = \\\r\n\t\t\t\tOps.get_loss_bbvert(self.X_pc, self.y_bbvert_pred, self.Y_bbvert, label=bbox_criteria)\r\n\t\t\tself.bbscore_loss = Ops.get_loss_bbscore(self.y_bbscore_pred, self.Y_bbvert)\r\n\t\t\t\r\n\t\t\tself.sum_bbox_vert_loss = tf.summary.scalar('bbvert_loss', self.bbvert_loss)\r\n\t\t\tself.sum_bbox_vert_loss_l2 = tf.summary.scalar('bbvert_loss_l2', self.bbvert_loss_l2)\r\n\t\t\tself.sum_bbox_vert_loss_ce = tf.summary.scalar('bbvert_loss_ce', self.bbvert_loss_ce)\r\n\t\t\tself.sum_bbox_vert_loss_iou = tf.summary.scalar('bbvert_loss_iou', self.bbvert_loss_iou)\r\n\t\t\tself.sum_bbox_score_loss = tf.summary.scalar('bbscore_loss', self.bbscore_loss)\r\n\r\n\t\twith tf.variable_scope('pmask'):\r\n\t\t\tself.y_pmask_pred = self.pmask_net(self.point_features, self.global_features, self.y_bbvert_pred, self.y_bbscore_pred)\r\n\r\n\t\t\t### loss\r\n\t\t\tself.pmask_loss = Ops.get_loss_pmask(self.X_pc, self.y_pmask_pred, self.Y_pmask)\r\n\t\t\tself.sum_pmask_loss = tf.summary.scalar('pmask_loss', self.pmask_loss)\r\n\r\n\t\twith tf.variable_scope('pmask', reuse=True):\r\n\t\t\t#### during testing, no need to associate, use unordered predictions\r\n\t\t\tself.y_pmask_pred_raw = self.pmask_net(self.point_features, self.global_features, self.y_bbvert_pred_raw, self.y_bbscore_pred_raw)\r\n\r\n\t\t######   3. define optimizers\r\n\t\tvar_backbone = [var for var in tf.trainable_variables() if var.name.startswith('backbone') and not var.name.startswith('backbone/sem')]\r\n\t\tvar_sem = [var for var in tf.trainable_variables() if var.name.startswith('backbone/sem')]\r\n\t\tvar_bbox = [var for var in tf.trainable_variables() if var.name.startswith('bbox')]\r\n\t\tvar_pmask = [var for var in tf.trainable_variables() if var.name.startswith('pmask')]\r\n\r\n\t\tend_2_end_loss = self.bbvert_loss + self.bbscore_loss  + self.pmask_loss + self.psemce_loss\r\n\t\tself.optim = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(end_2_end_loss, var_list = var_bbox+var_pmask +var_backbone+ var_sem)\r\n\r\n\t\t######   4. others\r\n\t\tprint(Ops.variable_count())\r\n\t\tself.saver = tf.train.Saver(max_to_keep=0)\r\n\t\tconfig = tf.ConfigProto(allow_soft_placement=True)\r\n\t\tconfig.gpu_options.visible_device_list = GPU\r\n\t\tself.sess = tf.Session(config=config)\r\n\t\tself.sum_writer_train = tf.summary.FileWriter(self.train_sum_dir, self.sess.graph)\r\n\t\tself.sum_write_test = tf.summary.FileWriter(self.test_sum_dir)\r\n\t\tself.sum_merged = tf.summary.merge_all()\r\n\r\n\t\tpath = self.train_mod_dir\r\n\t\tmodelsaved='model025.cptk.data-00000-of-00001'\r\n\t\tif os.path.isfile(path + modelsaved):\r\n\t\t\tprint (\"restoring saved model\"+modelsaved)\r\n\t\t\tself.saver.restore(self.sess, path + 'model025.cptk')\r\n\t\telse:\r\n\t\t\tprint (\"model not found, all weights are initilized\")\r\n\t\t\tself.sess.run(tf.global_variables_initializer())\r\n\r\n\t\treturn 0\r\nhelper_net is as below\uff1a\r\n# _*_ coding:utf-8 _*_\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom scipy.optimize import linear_sum_assignment\r\nimport tensorflow as tf\r\n#tf.compat.v1.disable_v2_behavior()\r\n#import tensorflow.compat.v1 as tf\r\n#tf.disable_v2_behavior()\r\nclass Ops:\r\n\r\n    @staticmethod\r\n    def lrelu(x, leak=0.2):\r\n        f1 = 0.5 * (1 + leak)\r\n        f2 = 0.5 * (1 - leak)\r\n        return f1 * x + f2 * abs(x)\r\n\r\n    @staticmethod\r\n    def relu(x):\r\n        return tf.nn.relu(x)\r\n\r\n    @staticmethod\r\n    def xxlu(x,label,name=None):\r\n        if label =='relu':\r\n            return  Ops.relu(x)\r\n        if label =='lrelu':\r\n            return  Ops.lrelu(x,leak=0.2)\r\n\r\n    @staticmethod\r\n    def variable_sum(var, name):\r\n        with tf.compat.v1.name_scope(name):\r\n            mean = tf.reduce_mean(input_tensor=var)\r\n            tf.compat.v1.summary.scalar('mean', mean)\r\n            stddev = tf.sqrt(tf.reduce_mean(input_tensor=tf.square(var - mean)))\r\n            tf.compat.v1.summary.scalar('stddev', stddev)\r\n            tf.compat.v1.summary.scalar('max', tf.reduce_max(input_tensor=var))\r\n            tf.compat.v1.summary.scalar('min', tf.reduce_min(input_tensor=var))\r\n            tf.compat.v1.summary.histogram('histogram', var)\r\n\r\n    @staticmethod\r\n    def variable_count():\r\n        total_para = 0\r\n        for variable in tf.compat.v1.trainable_variables():\r\n            shape = variable.get_shape()\r\n            variable_para = 1\r\n            for dim in shape:\r\n                variable_para *= dim.value\r\n            total_para += variable_para\r\n        return total_para\r\n\r\n    @staticmethod\r\n    def fc(x, out_d, name):\r\n        #xavier_init = tf.contrib.layers.xavier_initializer()\r\n        #xavier_init = tf.truncated_normal_initializer()\r\n        xavier_init = tf.initializers.GlorotUniform()\r\n        zero_init = tf.compat.v1.zeros_initializer()\r\n        in_d = x.get_shape()[1]\r\n        with tf.device('/cpu:0'):  # to create Variables stored on CPU memory\r\n            w = tf.compat.v1.get_variable(name + '_w', [in_d, out_d], initializer=xavier_init)\r\n            b = tf.compat.v1.get_variable(name + '_b', [out_d], initializer=zero_init)\r\n        y = tf.nn.bias_add(tf.matmul(x, w), b)\r\n        Ops.variable_sum(w, name)\r\n        return y\r\n\t\r\n    @staticmethod\r\n    def conv2d(x, k=(1,1), out_c=1, str=1, name='',pad='SAME'):\r\n        #xavier_init = tf.contrib.layers.xavier_initializer()\r\n        #xavier_init = tf.truncated_normal_initializer()\r\n        xavier_init = tf.initializers.GlorotUniform()\r\n        zero_init = tf.compat.v1.zeros_initializer()\r\n        in_c = x.get_shape()[3]\r\n        with tf.device('/cpu:0'):  # to create Variables stored on CPU memory\r\n            w = tf.compat.v1.get_variable(name + '_w', [k[0], k[1], in_c, out_c], initializer=xavier_init)\r\n            b = tf.compat.v1.get_variable(name + '_b', [out_c], initializer=zero_init)\r\n\r\n        stride = [1, str, str, 1]\r\n        y = tf.nn.bias_add(tf.nn.conv2d(input=x, filters=w, strides=stride, padding=pad), b)\r\n        Ops.variable_sum(w, name)\r\n        return y\r\n\r\n    @staticmethod\r\n    def dropout(x, is_train, keep_prob, name):\r\n        y = tf.cond(pred=is_train, true_fn=lambda: tf.nn.dropout(x, rate=1 - (keep_prob), name=name), false_fn=lambda: x)\r\n        return y\r\n\r\n    ####################################\r\n    @staticmethod\r\n    def gather_tensor_along_2nd_axis(bat_bb_pred, bat_bb_indices):\r\n        bat_size = tf.shape(input=bat_bb_pred)[0]\r\n        [_, ins_max_num, d1, d2] = bat_bb_pred.get_shape()\r\n        bat_size_range = tf.range(bat_size)\r\n        bat_size_range_flat = tf.reshape(bat_size_range, [-1,1])\r\n        bat_size_range_flat_repeat = tf.tile(bat_size_range_flat, [1, int(ins_max_num)])\r\n        bat_size_range_flat_repeat = tf.reshape(bat_size_range_flat_repeat, [-1])\r\n        \r\n        indices_2d_flat = tf.reshape(bat_bb_indices, [-1])\r\n        indices_2d_flat_repeat = bat_size_range_flat_repeat*int(ins_max_num) + indices_2d_flat\r\n\r\n        bat_bb_pred = tf.reshape(bat_bb_pred, [-1, int(d1), int(d2)])\r\n        bat_bb_pred_new = tf.gather(bat_bb_pred, indices_2d_flat_repeat)\r\n        bat_bb_pred_new = tf.reshape(bat_bb_pred_new, [bat_size, int(ins_max_num), int(d1), int(d2)])\r\n   \r\n        return bat_bb_pred_new\r\n\r\n    @staticmethod\r\n    def hungarian(loss_matrix, bb_gt):\r\n        box_mask = np.array([[0, 0, 0], [0, 0, 0]])\r\n\r\n        def assign_mappings_valid_only(cost, gt_boxes):\r\n            # return ordering : batch_size x num_instances\r\n            loss_total = 0.\r\n            batch_size, num_instances = cost.shape[:2]\r\n            ordering = np.zeros(shape=[batch_size, num_instances]).astype(np.int32)\r\n            for idx in range(batch_size):\r\n                ins_gt_boxes = gt_boxes[idx]\r\n                ins_count = 0\r\n                for box in ins_gt_boxes:\r\n                    if np.array_equal(box, box_mask):\r\n                        break\r\n                    else:\r\n                        ins_count += 1\r\n                valid_cost = cost[idx][:ins_count]\r\n                row_ind, col_ind = linear_sum_assignment(valid_cost)\r\n                unmapped = num_instances - ins_count\r\n                if unmapped > 0:\r\n                    rest = np.array(range(ins_count, num_instances))\r\n                    row_ind = np.concatenate([row_ind, rest])\r\n                    unmapped_ind = np.array(list(set(range(num_instances)) - set(col_ind)))\r\n                    col_ind = np.concatenate([col_ind, unmapped_ind])\r\n\r\n                loss_total += cost[idx][row_ind, col_ind].sum()\r\n                ordering[idx] = np.reshape(col_ind, [1, -1])\r\n            return ordering, (loss_total / float(batch_size * num_instances)).astype(np.float32)\r\n        ######\r\n        ordering, loss_total = tf.compat.v1.py_func(assign_mappings_valid_only, [loss_matrix, bb_gt], [tf.int32, tf.float32])\r\n\r\n        return ordering, loss_total\r\n\r\n    @staticmethod\r\n    def bbvert_association(X_pc, y_bbvert_pred, Y_bbvert, label=''):\r\n        points_num = tf.shape(input=X_pc)[1]\r\n        bbnum = int(y_bbvert_pred.shape[1])\r\n        points_xyz = X_pc[:, :, 0:3]\r\n        points_xyz = tf.tile(points_xyz[:, None, :, :], [1, bbnum, 1, 1])\r\n\r\n        ##### get points hard mask in each gt bbox\r\n        gt_bbox_min_xyz = Y_bbvert[:, :, 0, :]\r\n        gt_bbox_max_xyz = Y_bbvert[:, :, 1, :]\r\n        gt_bbox_min_xyz = tf.tile(gt_bbox_min_xyz[:, :, None, :], [1, 1, points_num, 1])\r\n        gt_bbox_max_xyz = tf.tile(gt_bbox_max_xyz[:, :, None, :], [1, 1, points_num, 1])\r\n        tp1_gt = gt_bbox_min_xyz - points_xyz\r\n        tp2_gt = points_xyz - gt_bbox_max_xyz\r\n        tp_gt = tp1_gt * tp2_gt\r\n        points_in_gt_bbox_prob = tf.cast(tf.equal(tf.reduce_mean(input_tensor=tf.cast(tf.greater_equal(tp_gt, 0.), tf.float32), axis=-1), 1.0), tf.float32)\r\n\r\n        ##### get points soft mask in each pred bbox ---> Algorithm 1\r\n        pred_bbox_min_xyz = y_bbvert_pred[:, :, 0, :]\r\n        pred_bbox_max_xyz = y_bbvert_pred[:, :, 1, :]\r\n        pred_bbox_min_xyz = tf.tile(pred_bbox_min_xyz[:, :, None, :], [1, 1, points_num, 1])\r\n        pred_bbox_max_xyz = tf.tile(pred_bbox_max_xyz[:, :, None, :], [1, 1, points_num, 1])\r\n        tp1_pred = pred_bbox_min_xyz - points_xyz\r\n        tp2_pred = points_xyz - pred_bbox_max_xyz\r\n        tp_pred = 100 * tp1_pred * tp2_pred\r\n        tp_pred = tf.maximum(tf.minimum(tp_pred, 20.0), -20.0)\r\n        points_in_pred_bbox_prob = 1.0/(1.0 + tf.exp(-1.0 * tp_pred))\r\n        points_in_pred_bbox_prob = tf.reduce_min(input_tensor=points_in_pred_bbox_prob, axis=-1)\r\n\r\n        ##### get bbox cross entropy scores\r\n        prob_gt = tf.tile(points_in_gt_bbox_prob[:, :, None, :], [1, 1, bbnum, 1])\r\n        prob_pred = tf.tile(points_in_pred_bbox_prob[:, None, :, :], [1, bbnum, 1, 1])\r\n        ce_scores_matrix = - prob_gt * tf.math.log(prob_pred + 1e-8) - (1 - prob_gt) * tf.math.log(1 - prob_pred + 1e-8)\r\n        ce_scores_matrix = tf.reduce_mean(input_tensor=ce_scores_matrix, axis=-1)\r\n\r\n        ##### get bbox soft IOU\r\n        TP = tf.reduce_sum(input_tensor=prob_gt * prob_pred, axis=-1)\r\n        FP = tf.reduce_sum(input_tensor=prob_pred, axis=-1) - TP\r\n        FN = tf.reduce_sum(input_tensor=prob_gt, axis=-1) - TP\r\n        iou_scores_matrix = TP/ (TP + FP + FN + 1e-6)\r\n        # iou_scores_matrix = 1.0/iou_scores_matrix  # bad, don't use\r\n        iou_scores_matrix = -1.0 * iou_scores_matrix  # to minimize\r\n\r\n        ##### get bbox l2 scores\r\n        l2_gt = tf.tile(Y_bbvert[:, :, None, :, :], [1, 1, bbnum, 1, 1])\r\n        l2_pred = tf.tile(y_bbvert_pred[:, None, :, :, :], [1, bbnum, 1, 1, 1])\r\n        l2_gt = tf.reshape(l2_gt, [-1, bbnum, bbnum, 2 * 3])\r\n        l2_pred = tf.reshape(l2_pred, [-1, bbnum, bbnum, 2 * 3])\r\n        l2_scores_matrix = tf.reduce_mean(input_tensor=(l2_gt - l2_pred) ** 2, axis=[-1])\r\n\r\n        ##### bbox association\r\n        if label == 'use_all_ce_l2_iou':\r\n            associate_maxtrix = ce_scores_matrix + l2_scores_matrix + iou_scores_matrix\r\n        elif label == 'use_both_ce_l2':\r\n            associate_maxtrix = ce_scores_matrix + l2_scores_matrix\r\n        elif label == 'use_both_ce_iou':\r\n            associate_maxtrix = ce_scores_matrix + iou_scores_matrix\r\n        elif label == 'use_both_l2_iou':\r\n            associate_maxtrix = l2_scores_matrix + iou_scores_matrix\r\n        elif label == 'use_only_ce':\r\n            associate_maxtrix = ce_scores_matrix\r\n        elif label == 'use_only_l2':\r\n            associate_maxtrix = l2_scores_matrix\r\n        elif label == 'use_only_iou':\r\n            associate_maxtrix = iou_scores_matrix\r\n        else:\r\n            associate_maxtrix=None\r\n            print('association label error!'); exit()\r\n\r\n        ######\r\n        pred_bborder, association_score_min = Ops.hungarian(associate_maxtrix, bb_gt=Y_bbvert)\r\n        pred_bborder = tf.cast(pred_bborder, dtype=tf.int32)\r\n        y_bbvert_pred_new = Ops.gather_tensor_along_2nd_axis(y_bbvert_pred, pred_bborder)\r\n\r\n        return y_bbvert_pred_new, pred_bborder\r\n\r\n    @staticmethod\r\n    def bbscore_association(y_bbscore_pred_raw, pred_bborder):\r\n        y_bbscore_pred_raw = y_bbscore_pred_raw[:,:,None,None]\r\n        y_bbscore_pred_new = Ops.gather_tensor_along_2nd_axis(y_bbscore_pred_raw, pred_bborder)\r\n\r\n        y_bbscore_pred_new = tf.reshape(y_bbscore_pred_new, [-1, int(y_bbscore_pred_new.shape[1])])\r\n        return y_bbscore_pred_new\r\n\r\n    ####################################  sem loss\r\n    @staticmethod\r\n    def get_loss_psem_ce(y_psem_logits, Y_psem):\r\n        psemce_loss = tf.nn.softmax_cross_entropy_with_logits(logits=y_psem_logits, labels=tf.stop_gradient(Y_psem))\r\n        psemce_loss = tf.reduce_mean(input_tensor=psemce_loss)\r\n        return psemce_loss\r\n\r\n    ####################################  bbox loss\r\n    @staticmethod\r\n    def get_loss_bbvert(X_pc, y_bbvert_pred, Y_bbvert, label=''):\r\n        points_num = tf.shape(input=X_pc)[1]\r\n        bb_num = int(Y_bbvert.shape[1])\r\n        points_xyz = X_pc[:, :, 0:3]\r\n        points_xyz = tf.tile(points_xyz[:, None, :, :], [1, bb_num, 1, 1])\r\n\r\n        ##### get points hard mask in each gt bbox\r\n        gt_bbox_min_xyz = Y_bbvert[:, :, 0, :]\r\n        gt_bbox_max_xyz = Y_bbvert[:, :, 1, :]\r\n        gt_bbox_min_xyz = tf.tile(gt_bbox_min_xyz[:, :, None, :], [1, 1, points_num, 1])\r\n        gt_bbox_max_xyz = tf.tile(gt_bbox_max_xyz[:, :, None, :], [1, 1, points_num, 1])\r\n        tp1_gt = gt_bbox_min_xyz - points_xyz\r\n        tp2_gt = points_xyz - gt_bbox_max_xyz\r\n        tp_gt = tp1_gt * tp2_gt\r\n        points_in_gt_bbox_prob = tf.cast(tf.equal(tf.reduce_mean(input_tensor=tf.cast(tf.greater_equal(tp_gt, 0.), tf.float32), axis=-1), 1.0), tf.float32)\r\n\r\n        ##### get points soft mask in each pred bbox\r\n        pred_bbox_min_xyz = y_bbvert_pred[:, :, 0, :]\r\n        pred_bbox_max_xyz = y_bbvert_pred[:, :, 1, :]\r\n        pred_bbox_min_xyz = tf.tile(pred_bbox_min_xyz[:, :, None, :], [1, 1, points_num, 1])\r\n        pred_bbox_max_xyz = tf.tile(pred_bbox_max_xyz[:, :, None, :], [1, 1, points_num, 1])\r\n        tp1_pred = pred_bbox_min_xyz - points_xyz\r\n        tp2_pred = points_xyz - pred_bbox_max_xyz\r\n        tp_pred = 100*tp1_pred*tp2_pred\r\n        tp_pred = tf.maximum(tf.minimum(tp_pred, 20.0), -20.0)\r\n        points_in_pred_bbox_prob = 1.0/(1.0 + tf.exp(-1.0 * tp_pred))\r\n        points_in_pred_bbox_prob = tf.reduce_min(input_tensor=points_in_pred_bbox_prob, axis=-1)\r\n\r\n        ##### helper -> the valid bbox (the gt boxes are zero-padded during data processing, pickup valid ones here)\r\n        Y_bbox_helper = tf.reduce_sum(input_tensor=tf.reshape(Y_bbvert, [-1, bb_num, 6]), axis=-1)\r\n        Y_bbox_helper = tf.cast(tf.greater(Y_bbox_helper, 0.), tf.float32)\r\n\r\n        ##### 1. get ce loss of valid/positive bboxes, don't count the ce_loss of invalid/negative bboxes\r\n        Y_bbox_helper_tp1 = tf.tile(Y_bbox_helper[:, :, None], [1, 1, points_num])\r\n        bbox_loss_ce_all = -points_in_gt_bbox_prob * tf.math.log(points_in_pred_bbox_prob + 1e-8) \\\r\n                       -(1.-points_in_gt_bbox_prob)*tf.math.log(1.-points_in_pred_bbox_prob + 1e-8)\r\n        bbox_loss_ce_pos = tf.reduce_sum(input_tensor=bbox_loss_ce_all*Y_bbox_helper_tp1)/tf.reduce_sum(input_tensor=Y_bbox_helper_tp1)\r\n        bbox_loss_ce = bbox_loss_ce_pos\r\n\r\n        ##### 2. get iou loss of valid/positive bboxes\r\n        TP = tf.reduce_sum(input_tensor=points_in_pred_bbox_prob * points_in_gt_bbox_prob, axis=-1)\r\n        FP = tf.reduce_sum(input_tensor=points_in_pred_bbox_prob, axis=-1) - TP\r\n        FN = tf.reduce_sum(input_tensor=points_in_gt_bbox_prob, axis=-1) - TP\r\n        bbox_loss_iou_all = TP/(TP + FP + FN + 1e-6)\r\n        bbox_loss_iou_all = -1.0*bbox_loss_iou_all\r\n        bbox_loss_iou_pos = tf.reduce_sum(input_tensor=bbox_loss_iou_all*Y_bbox_helper)/tf.reduce_sum(input_tensor=Y_bbox_helper)\r\n        bbox_loss_iou = bbox_loss_iou_pos\r\n\r\n        ##### 3. get l2 loss of both valid/positive bboxes\r\n        bbox_loss_l2_all = (Y_bbvert - y_bbvert_pred)**2\r\n        bbox_loss_l2_all = tf.reduce_mean(input_tensor=tf.reshape(bbox_loss_l2_all, [-1, bb_num, 6]), axis=-1)\r\n        bbox_loss_l2_pos = tf.reduce_sum(input_tensor=bbox_loss_l2_all*Y_bbox_helper)/tf.reduce_sum(input_tensor=Y_bbox_helper)\r\n\r\n        ## to minimize the 3D volumn of invalid/negative bboxes, it serves as a regularizer to penalize false pred bboxes\r\n        ## it turns out to be quite helpful, but not discussed in the paper\r\n        bbox_pred_neg = tf.tile((1.- Y_bbox_helper)[:,:,None,None], [1,1,2,3])*y_bbvert_pred\r\n        bbox_loss_l2_neg = (bbox_pred_neg[:,:,0,:]-bbox_pred_neg[:,:,1,:])**2\r\n        bbox_loss_l2_neg = tf.reduce_sum(input_tensor=bbox_loss_l2_neg)/(tf.reduce_sum(input_tensor=1.-Y_bbox_helper)+1e-8)\r\n\r\n        bbox_loss_l2 = bbox_loss_l2_pos + bbox_loss_l2_neg\r\n\r\n        #####\r\n        if label == 'use_all_ce_l2_iou':\r\n            bbox_loss = bbox_loss_ce + bbox_loss_l2 + bbox_loss_iou\r\n        elif label == 'use_both_ce_l2':\r\n            bbox_loss = bbox_loss_ce + bbox_loss_l2\r\n        elif label == 'use_both_ce_iou':\r\n            bbox_loss = bbox_loss_ce + bbox_loss_iou\r\n        elif label == 'use_both_l2_iou':\r\n            bbox_loss = bbox_loss_l2 + bbox_loss_iou\r\n        elif label == 'use_only_ce':\r\n            bbox_loss = bbox_loss_ce\r\n        elif label == 'use_only_l2':\r\n            bbox_loss = bbox_loss_l2\r\n        elif label == 'use_only_iou':\r\n            bbox_loss = bbox_loss_iou\r\n        else:\r\n            bbox_loss = None\r\n            print('bbox loss label error!'); exit()\r\n\r\n        return bbox_loss, bbox_loss_l2, bbox_loss_ce, bbox_loss_iou\r\n\r\n    @staticmethod\r\n    def get_loss_bbscore(y_bbscore_pred, Y_bbvert):\r\n        bb_num = int(Y_bbvert.shape[1])\r\n\r\n        ##### helper -> the valid bbox\r\n        Y_bbox_helper = tf.reduce_sum(input_tensor=tf.reshape(Y_bbvert, [-1, bb_num, 6]), axis=-1)\r\n        Y_bbox_helper = tf.cast(tf.greater(Y_bbox_helper, 0.), tf.float32)\r\n\r\n        ##### bbox score loss\r\n        bbox_loss_score = tf.reduce_mean(input_tensor=-Y_bbox_helper * tf.math.log(y_bbscore_pred + 1e-8)\r\n                                         -(1. - Y_bbox_helper) * tf.math.log(1. - y_bbscore_pred + 1e-8))\r\n        return bbox_loss_score\r\n\r\n    ####################################  pmask loss\r\n    @staticmethod\r\n    def get_loss_pmask(X_pc, y_pmask_pred, Y_pmask):\r\n        points_num = tf.shape(input=X_pc)[1]\r\n        ##### valid ins\r\n        Y_pmask_helper = tf.reduce_sum(input_tensor=Y_pmask, axis=-1)\r\n        Y_pmask_helper = tf.cast(tf.greater(Y_pmask_helper, 0.), tf.float32)\r\n        Y_pmask_helper = tf.tile(Y_pmask_helper[:, :, None], [1, 1, points_num])\r\n\r\n        Y_pmask = Y_pmask * Y_pmask_helper\r\n        y_pmask_pred = y_pmask_pred * Y_pmask_helper\r\n\r\n        ##### focal loss\r\n        alpha = 0.75\r\n        gamma = 2\r\n        pmask_loss_focal_all = -Y_pmask*alpha*((1.-y_pmask_pred)**gamma)*tf.math.log(y_pmask_pred+1e-8)\\\r\n                               -(1.-Y_pmask)*(1.-alpha)*(y_pmask_pred**gamma)*tf.math.log(1.-y_pmask_pred+1e-8)\r\n        pmask_loss_focal = tf.reduce_sum(input_tensor=pmask_loss_focal_all*Y_pmask_helper)/tf.reduce_sum(input_tensor=Y_pmask_helper)\r\n\r\n        ## the above \"alpha\" makes the loss to be small\r\n        ## then use a constant, so it's numerically comparable with other losses (e.g., semantic loss, bbox loss)\r\n        pmask_loss = 30*pmask_loss_focal\r\n\r\n        return pmask_loss\r\n", "comments": ["@lifeiwen \r\nThe code shared is not indented, please share indented code or a colab gist with reported issue.\r\nIf this is a duplicate of #46280, please close one of the issues as it will be traced.", "@Saduf2019 source code \uff1a\r\nhttps://github.com/Yang7879/3D-BoNet", "@lifeiwen \r\nThe code provided is fairly complex hence it would be difficult for us to pinpoint the issue. Could you please get the example down to the simplest possible repro? That will allow us to determine the source of the issue easily. Thanks!", "I only see that our mistakes are the same, but I still don't know how you solve them.\r\nIf you are free, please reply to me. I will pay attention to the dynamic of GitHub these days. Thank you.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46324\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46324\">No</a>\n"]}, {"number": 46323, "title": "micro: port op ELU from lite", "body": "@tensorflow/micro\r\n\r\nThis issue tracks my work porting operator ELU from lite to micro.\r\n\r\nThe port will be submitted in a number of PRs. Here's a rough flight plan per @advaitjain and @petewarden:\r\n\r\nPR 1: Extract the code for parsing the op from a flatbuffer out of ParseOpDataTfLite in tensorflow/lite/core/api/flatbuffer_conversions.cc into a standalone function that can be called from micro's op resolver\r\nPR 2: Extract the reference implementation out of tensorflow/lite/kernels/internal/reference/reference_ops.h into its own header which can be included without dragging in reference_ops.h's dependences\r\nPR 3: Copy operator from lite to micro making minimal changes and not including in the build\r\nPR 4: Delete extra code from the micro copy of the operator\r\nPR 5: Port micro copy of operator as necessary and add a corresponding test\r\nPR 6: Extract common activation code into activations.cc and activation_utils.h files.  Extract common test code into activation_test_utils.h file.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46323\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46323\">No</a>\n"]}, {"number": 46322, "title": "[TFL] Convert Size to Prod(Shape)", "body": "This PR converts `TF_SizeOp` to `TF_ProdOp(TF_ShapeOp)`. It makes TFL users can run models with `tf.size` without `SELECT_TF_OPS`. Fixes #46285.", "comments": ["Done. Moved the pattern to tf-lower-tf.", "Hi @smit-hinsu, TF->TF lowering pass is also being used in `tensorflow/compiler/mlir/xla/transforms/legalize_tf.cc`, so this PR will break `size_unranked` in `tensorflow/compiler/mlir/xla/tests/legalize-tf.mlir`. Should I remove it or correct it to the right lowering? Thank you!\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/009675faa9422ef8e2c8a30216b31adad62d25b6/tensorflow/compiler/mlir/xla/tests/legalize-tf.mlir#L3882-L3887\r\n\r\nAfter lowering\r\n\r\n```\r\nfunc @size_unranked(%arg0: tensor<*xf32>) -> tensor<i32> {\r\n  %0 = mhlo.constant dense<0> : tensor<i32>\r\n  %1 = tensor.cast %0 : tensor<i32> to tensor<i32>\r\n  %2 = shape.shape_of %arg0 : tensor<*xf32> -> tensor<?xindex>\r\n  %3 = shape.to_extent_tensor %2 : tensor<?xindex> -> tensor<?xindex>\r\n  %4 = index_cast %3 : tensor<?xindex> to tensor<?xi32>\r\n  %5 = \"mhlo.convert\"(%4) : (tensor<?xi32>) -> tensor<?xi32>\r\n  %6 = mhlo.constant dense<1> : tensor<i32>\r\n  %7 = \"mhlo.reduce\"(%5, %6) ( {\r\n  ^bb0(%arg1: tensor<i32>, %arg2: tensor<i32>):  // no predecessors\r\n    %9 = mhlo.multiply %arg1, %arg2 : tensor<i32>\r\n    \"mhlo.return\"(%9) : (tensor<i32>) -> ()\r\n  }) {dimensions = dense<0> : tensor<1xi64>} : (tensor<?xi32>, tensor<i32>) -> tensor<i32>\r\n  %8 = \"mhlo.convert\"(%7) : (tensor<i32>) -> tensor<i32>\r\n  return %8 : tensor<i32>\r\n}\r\n```", "Sure, I can constrain the pattern to ranked input only.", "> Why not simply remove that pattern? Your new pattern should have the same behavior in case of static shapes.\r\n\r\nOhoh, I misunderstand it :-(. Will do. Thank you!", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46322) for more info**.\n\n<!-- need_author_consent -->", "Umm, sorry that I mess up the branch. Let me reopen a new one #46351 ."]}, {"number": 46321, "title": "Fix InvalidArgumentError error when mixed precision policy is used in Attention/AdditiveAttention layer", "body": "This PR tries to address the issue raised in #46064 where\r\nInvalidArgumentError error is thrown when mixed precision policy is used\r\nin keras Attention/AdditiveAttention layer.\r\n\r\nThis PR fixes #46064.\r\n\r\nThis PR also fixes  #43261.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@gbaned Any update on the review?", "Thanks @reedwm. The PR has been updated to replace with `policy.policy_scope`.\r\n\r\nFor adding tests to `layer_correctness_test.py`, when I add the following:\r\n```\r\n      ('AdditiveAttentionLayerCausal',\r\n       lambda: dense_attention.AdditiveAttention(causal=True),\r\n       [(2, 3, 4), (2, 3, 4), (2, 3, 4)]),\r\n```\r\n\r\nThe model's output are all `nan`s. Not sure if this is a known issue?", "> The model's output are all `nan`s. Not sure if this is a known issue?\r\n\r\nThe issue is this line overflows:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/102e1f98552c4fe32a9abbb82ed22275f1be161b/tensorflow/python/keras/layers/dense_attention.py#L129\r\n\r\nIn float16, the `1.e9` overflows a float16 value, than when multiplied by zero, becomes NaN (Inf multiplied by 0 is NaN). To fix, if `padding_mask` is float16, use 1.e4 instead, with a comment stating it's to avoid overflow, then add the `layer_correctness_test.py` test.\r\n\r\nNote `scores` may underflow but this is OK: softmax treats -Inf values as effectively masked out, which is exactly what the code is trying to do.", "Actually, instead of replacing 1.e9 with 1.e4, replace 1.e9 with 65504, which is the maximum float16 value.", "Thanks @reedwm @fchollet for the help! The PR has been updated. Please take a look.", "@yongtang  Can you please check @reedwm's comments and keep us posted ? Thanks!", "Thanks @reedwm for the help. The PR has been updated with all comments addressed. Please take a look."]}, {"number": 46320, "title": "[tf.data] Support eager mode benchmarking", "body": "This PR adds support for eager execution based benchmarks based on the benchmarking context. This is achieved by checking for the `eager_execution()` value within the benchmarking context and iterating over the dataset eagerly in the `DatasetBenchmarkBase` class.\r\n\r\nAdditionally, the `ListFilesBenchmark` in `python/data/benchmarks/list_files_benchmark.py` has been refactored to use the `DatasetBenchmarkBase` class.\r\n\r\nA sample comparison w.r.t `RangeBenchmark` is as follows:\r\n\r\nGRAPH MODE:\r\n```python\r\nentry {\r\n  name: \"RangeBenchmark.modeling_on.graph\"\r\n  iters: 5\r\n  wall_time: 1.7887322902679443e-07\r\n  extras {\r\n    key: \"num_elements\"\r\n    value {\r\n      double_value: 10000000.0\r\n    }\r\n  }\r\n}\r\n\r\nentry {\r\n  name: \"RangeBenchmark.modeling_off.graph\"\r\n  iters: 5\r\n  wall_time: 1.541191530227661e-07\r\n  extras {\r\n    key: \"num_elements\"\r\n    value {\r\n      double_value: 50000000.0\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nEAGER MODE:\r\n```python\r\nentry {\r\n  name: \"RangeBenchmark.modeling_on.eager\"\r\n  iters: 5\r\n  wall_time: 2.0390429496765137e-07\r\n  extras {\r\n    key: \"num_elements\"\r\n    value {\r\n      double_value: 10000000.0\r\n    }\r\n  }\r\n}\r\n\r\nentry {\r\n  name: \"RangeBenchmark.modeling_off.eager\"\r\n  iters: 5\r\n  wall_time: 1.8014323711395263e-07\r\n  extras {\r\n    key: \"num_elements\"\r\n    value {\r\n      double_value: 50000000.0\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nw.r.t  `ListFilesBenchmark`\r\n```python\r\n\r\nGRAPH MODE\r\nentry {\r\n  name: \"ListFilesBenchmark.nested_directory(1024*16).graph\"\r\n  iters: 3\r\n  wall_time: 8.092029020190239e-07\r\n  extras {\r\n    key: \"num_elements\"\r\n    value {\r\n      double_value: 2048.0\r\n    }\r\n  }\r\n}\r\n\r\nEAGER MODE\r\nentry {\r\n  name: \"ListFilesBenchmark.nested_directory(1024*16).eager\"\r\n  iters: 3\r\n  wall_time: 7.745111361145973e-07\r\n  extras {\r\n    key: \"num_elements\"\r\n    value {\r\n      double_value: 2048.0\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n\r\nNOTE: Description has been updated as per review comments.", "comments": ["cc: @aaudiber for review.", "I don't think eager benchmarks are worth maintaining, since the backend dataset implementation works the same in both eager and graph mode. WDYT @jsimsa?", "I think there is value in being able to run benchmarks in eager mode but this support should certainly not result in duplicating benchmark code. Instead, the benchmark base should be modified to make it possible to run a benchmark both in graph mode or eagerly. Which mode to use could be a based on which parameter is passed to `python/data/benchmarks/benchmark_base.py` `run_benchmark` method, defaulting to the mode of the context in which the benchmark is executed.", "I see that not all the benchmarks located in `python/data/benchmarks/` and `python/data/experimental/benchmarks/` subclass the `DatasetBenchmarkBase` class. Thus, making changes to the `DatasetBenchmarkBase` class will cover a few of them but not all. This is due to the custom `_benchmark` logic that is needed instead of the default one available in `DatasetBenchmarkBase`.\r\n\r\nAs per @jsimsa 's suggestion, I can modify the `run_benchmark` method of `DatasetBenchmarkBase` class to take a parameter named `eager` which will either be `True` or `False`. It will default to `False` so that it doesn't disturb the current benchmark scenarios. I can then add conditional logic to handle `eager=True` case within that method. The benchmarks which inherit the `DatasetBenchmarkBase` can now call the `run_and_report_benchmark` method with the option of benchmarking eagerly.\r\n\r\nI can do the same for benchmarks which do not inherit the `DatasetBenchmarkBase` class and modify the `_benchmark` method to take the `eager` parameter and conditionalize the logic.\r\n\r\nWhat do you think? @jsimsa @aaudiber?\r\n\r\n\r\nUPDATE: The benchmarks do not take the `eager` parameter but execute in either `eager` or `graph` mode based on context.", "Thanks @kvignesh1420. You plan for modifying `DatasetBenchmarkBase` sounds good to me.  It would also be good to understand why do some benchmarks need custom `_benchmark` logic and whether it would be possible to refactor things so that all benchmarks inherit from `DatasetBenchmarkBase`.\r\n\r\n", "@jsimsa thanks for the go-ahead. I will update the PR with the required changes and try to refactor things.", "I have modified the `DatasetBenchmarkBase` class to support eager iteration over the datasets depending on the context. Additionally, the `ListFilesBenchmark` in `python/data/benchmarks/list_files_benchmark.py` has been refactored to use the `DatasetBenchmarkBase` class. The refactor changes the current output of the benchmark in order to be in sync with the style of other benchmarks.\r\n\r\ncc: @jsimsa @aaudiber let me know how it looks. I can take up the benchmarks in `data/experimental/benchmarks` after this.", "@jsimsa I have modified the `run_and_report_benchmark` method in `DatasetBenchmarkBase` to append the mode in which the benchmarks are running to the benchmark `name`. Also, a sample benchmark result has been added to the description. If you want additional results from eager mode benchmarks and its graph counter-part, I can attach the files with the log outputs. Let me know."]}, {"number": 46319, "title": "Optimization using Tensorflow issue", "body": "We had given below test case, we written all steps as per instructions. nit sure when we went wrong. Please guide us -\r\n\r\nWe will look at three scenarios here -\r\n\r\n- Tuning different optimisation algorithms, \r\n- Tuning learning rate and momentum of SGD optimizer\r\n- Tuning beta values of Adam optimizer\r\n\r\n```\r\nimport numpy as np\r\nfrom sklearn.model_selection import GridSearchCV\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense\r\nfrom keras.wrappers.scikit_learn import KerasClassifier\r\nfrom sklearn.datasets import load_iris\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.preprocessing import LabelEncoder,StandardScaler\r\nfrom sklearn.utils import shuffle\r\nfrom keras.utils.np_utils import to_categorical\r\nfrom keras.optimizers import SGD,Adam\r\nfrom matplotlib import pyplot\r\nimport seaborn as sns\r\nimport pandas as pd\r\nfrom keras.models import model_from_json\r\n\r\n```\r\n\r\n\r\n- Load the Dataset\r\n- Load the iris dataset using load_iris() function.\r\n- Store the data of the iris dataset in the variable X.\r\n- Store the target of the iris dataset in the variable y.\r\n- Convert the variable y into categorial variable using function to_categorial and save it in variable y.\r\n- Set seed value as 7 in the variable seed and use random.seed function in numpy to set seed value.\r\n- Now shuffle the data X and y using shuffle function and save it in variables X ,Y.\r\n\r\n```\r\niris = load_iris()\r\nX=  iris.data\r\ny = iris.target\r\ny = to_categorical(iris.target,3)\r\nseed = 7\r\nnp.random.seed(seed)\r\nX, Y =  shuffle(X, y)\r\n```\r\n\r\nIn variable optimizer pass the following optimizers as a list -\r\nSGD,RMSprop,Adam,Nadam\r\nIn param_grid pass parameter optimizer as optimizer using dict\r\n\r\n```\r\noptimizer = ['SGD', 'RMSprop',  'Adam', 'Nadam']\r\nparam_grid = dict(optimizer=optimizer)\r\n```\r\n\r\nCreate a sequential model\r\nThe model expects rows of data with 4 variables (the input_dim=4 argument)\r\nThe first hidden layer has 64 nodes and uses the relu activation function.\r\nThe second hidden layer has 32 nodes and uses the relu activation function.\r\nThe third hidden layer has 16 nodes and uses the relu activation function.\r\nThe output layer has 3 nodes and uses the softmax activation function.\r\nWhile comipling the model pass the following parameters -\r\n     -optimizer as optimizer\r\n     -loss as categorical cross entropy \r\n     -metrics as accuracy.\r\nReturn the compiled model\r\n\r\n```\r\ndef create_model(optimizer='adam'):\r\n    model = Sequential()\r\n    model.add(Dense(4, input_dim=4))\r\n    model.add(Dense(64, activation='relu'))\r\n    model.add(Dense(32, activation='relu'))\r\n    model.add(Dense(16, activation='relu'))\r\n    model.add(Dense(3, activation='softmax'))\r\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) \r\n    return model\r\n```\r\n\r\nUse the KerasClassifier function to call the model function with following parameters -\r\nbuild_fn as create_model\r\nbatch_size as 10\r\nverbose as 0\r\nepochs as 10\r\nSave the above in the variable model\r\n\r\n`model = KerasClassifier(build_fn=create_model, batch_size=10, verbose=0, epochs=10)`\r\n\r\nIn grid use the GridSearchCV function and pass the following parameters -\r\nestimator as model\r\nparam_grid as param_grid\r\nn_jobs as 1\r\nNow fit the model with X and Y using grid and save it in grid_result\r\n\r\n```\r\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\r\ngrid_result = grid.fit(X, Y)\r\n```\r\n\r\n\r\n```\r\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\r\nmeans = grid_result.cv_results_['mean_test_score']\r\nstds = grid_result.cv_results_['std_test_score']\r\nparams = grid_result.cv_results_['params']\r\nfor mean, stdev, param in zip(means, stds, params):\r\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\r\n```\r\n\r\nBest: 0.946667 using {'optimizer': 'RMSprop'}\r\n0.753333 (0.082192) with: {'optimizer': 'SGD'}\r\n0.946667 (0.047140) with: {'optimizer': 'RMSprop'}\r\n0.913333 (0.083799) with: {'optimizer': 'Adam'}\r\n0.866667 (0.106249) with: {'optimizer': 'Nadam'}\r\n\r\n\r\n```\r\npyplot.figure(figsize=(10,8))\r\n#pyplot.xticks(grid_result1.cv_results_['mean_test_score'])\r\npyplot.title(\"Performance metrics of each optimiser\")\r\nplot=sns.barplot(grid_result.cv_results_['mean_test_score'],optimizer)\r\npyplot.show() \r\n```\r\n\r\nIn variable learn_rate pass the following learn rates as a list -\r\n0.001, 0.01,0.3\r\nIn variable momentum pass the following momentums as a list -\r\n0.0, 0.4, 0.9\r\n\r\n```\r\nlearn_rate = [0.001, 0.01, 0.3]\r\nmomentum = [0.0, 0.4, 0.9]\r\n```\r\n\r\nUse the same model parameters as above to construct the model in the function create_model1\r\nIn the variable optimizer pass the following parameters using the optimizer SGD\r\n -lr as learn_rate\r\n -momentum as momentum\r\nWhile comipling the model pass the following parameters -\r\n -optimizer as optimizer\r\n -loss as categorical cross entropy \r\n -metrics as accuracy.\r\nReturn the compiled model\r\n\r\n```\r\ndef create_model1(learn_rate=0.01, momentum=0):\r\n    model1 = Sequential()\r\n    model1.add(Dense(4, input_dim=4))\r\n    model1.add(Dense(64, activation='relu'))\r\n    model1.add(Dense(32, activation='relu'))\r\n    model1.add(Dense(16, activation='relu'))\r\n    model1.add(Dense(3, activation='softmax'))\r\n    optimizer = SGD(lr=learn_rate, momentum=momentum)\r\n    model1.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])    \r\n    return model1\r\n```\r\n\r\n\r\n`model1 = KerasClassifier(build_fn=create_model1, batch_size=10, verbose=0, epochs=10)`\r\n\r\n```\r\nparam_grid1 = dict(learn_rate=learn_rate,momentum=momentum )\r\ngrid1 = GridSearchCV(estimator=model1, param_grid=param_grid1, n_jobs=1)\r\n```\r\n\r\nUse random.seed function in numpy to set seed value.\r\nNow fit the model with X and Y using grid1 and save it in grid_result1\r\n\r\n```\r\nnp.random.seed(seed)\r\ngrid_result1 = grid1.fit(X, Y)\r\n```\r\n\r\n```\r\nprint(\"Best: %f using %s\" % (grid_result1.best_score_, grid_result1.best_params_))\r\nmeans1 = grid_result1.cv_results_['mean_test_score']\r\nstds1 = grid_result1.cv_results_['std_test_score']\r\nparams1 = grid_result1.cv_results_['params']\r\nfor mean, stdev, param in zip(means1, stds1, params1):\r\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\r\n```\r\n\r\nBest: 0.893333 using {'learn_rate': 0.01, 'momentum': 0.9}\r\n0.360000 (0.032660) with: {'learn_rate': 0.001, 'momentum': 0.0}\r\n0.666667 (0.037712) with: {'learn_rate': 0.001, 'momentum': 0.4}\r\n0.780000 (0.081650) with: {'learn_rate': 0.001, 'momentum': 0.9}\r\n0.760000 (0.016330) with: {'learn_rate': 0.01, 'momentum': 0.0}\r\n0.866667 (0.160278) with: {'learn_rate': 0.01, 'momentum': 0.4}\r\n0.893333 (0.067987) with: {'learn_rate': 0.01, 'momentum': 0.9}\r\n0.413333 (0.188562) with: {'learn_rate': 0.3, 'momentum': 0.0}\r\n0.333333 (0.018856) with: {'learn_rate': 0.3, 'momentum': 0.4}\r\n0.333333 (0.018856) with: {'learn_rate': 0.3, 'momentum': 0.9}\r\n\r\n```\r\nparams1=pd.DataFrame(params1)\r\npyplot.figure(figsize=(10,8))\r\n#pyplot.xticks(grid_result1.cv_results_['mean_test_score'])\r\npyplot.title(\"Performance metrics of SGD optimiser with different learning rates and momentum\")\r\nplot1=sns.barplot(params1[\"learn_rate\"],grid_result1.cv_results_['mean_test_score'],hue=params1[\"momentum\"])\r\nplot1.set(ylabel='Score')\r\npyplot.show() \r\n```\r\n\r\n```\r\nbeta_1 = [0.001, 0.01, 0.3]\r\nbeta_2 = [0.0, 0.4, 0.9] \r\n```\r\n\r\n```\r\ndef create_model2(beta_1=0.01, beta_2=0):\r\n    model2 = Sequential()\r\n    model2.add(Dense(4, input_dim=4))\r\n    model2.add(Dense(64, activation='relu'))\r\n    model2.add(Dense(32, activation='relu'))\r\n    model2.add(Dense(16, activation='relu'))\r\n    model2.add(Dense(3, activation='softmax'))\r\n    optimizer = Adam(beta_1=beta_1,beta_2=beta_2)\r\n    model2.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\r\n    return model2\r\n```\r\n\r\n`model2 =  KerasClassifier(build_fn=create_model2,batch_size=10, verbose=0,  epochs=10)`\r\n\r\n```\r\nparam_grid2 = dict(beta_1=beta_1, beta_2=beta_2)\r\ngrid2 = GridSearchCV(estimator=model2, param_grid=param_grid2, n_jobs=1)\r\n```\r\n\r\nUse random.seed function in numpy to set seed value.\r\nNow fit the model with X and Y using grid2 and save it in grid_result2\r\n\r\n```\r\nnp.random.seed(seed)\r\ngrid_result2 = grid2.fit(X, Y)\r\n```\r\n\r\n```\r\nprint(\"Best: %f using %s\" % (grid_result2.best_score_, grid_result2.best_params_))\r\nmeans2 = grid_result2.cv_results_['mean_test_score']\r\nstds2 = grid_result2.cv_results_['std_test_score']\r\nparams2 = grid_result2.cv_results_['params']\r\nfor mean, stdev, param in zip(means2, stds2, params2):\r\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\r\n```\r\n\r\nBest: 0.966667 using {'beta_2': 0.4, 'beta_1': 0.001}\r\n0.526667 (0.264491) with: {'beta_2': 0.0, 'beta_1': 0.001}\r\n0.966667 (0.009428) with: {'beta_2': 0.4, 'beta_1': 0.001}\r\n0.840000 (0.133666) with: {'beta_2': 0.9, 'beta_1': 0.001}\r\n0.320000 (0.032660) with: {'beta_2': 0.0, 'beta_1': 0.01}\r\n0.813333 (0.111156) with: {'beta_2': 0.4, 'beta_1': 0.01}\r\n0.766667 (0.151731) with: {'beta_2': 0.9, 'beta_1': 0.01}\r\n0.360000 (0.032660) with: {'beta_2': 0.0, 'beta_1': 0.3}\r\n0.926667 (0.041096) with: {'beta_2': 0.4, 'beta_1': 0.3}\r\n0.766667 (0.151731) with: {'beta_2': 0.9, 'beta_1': 0.3}\r\n\r\n\r\n```\r\nparams2=pd.DataFrame(params2)\r\npyplot.figure(figsize=(10,8))\r\n#pyplot.xticks(grid_result1.cv_results_['mean_test_score'])\r\npyplot.title(\"Performance metrics of SGD optimiser with different beta values\")\r\nplot2=sns.barplot(params2[\"beta_1\"],grid_result2.cv_results_['mean_test_score'],hue=params2[\"beta_2\"])\r\nplot2.set(ylabel='Score')\r\npyplot.show() \r\n```\r\n\r\n```\r\nwith open(\"score.txt\",\"w\") as f:\r\n    f.write(str(round(grid_result.best_score_,2)))\r\nwith open(\"params.txt\",\"w\") as f:\r\n    f.write(str(grid_result.best_params_))\r\n\r\nwith open(\"score1.txt\",\"w\") as f:\r\n    f.write(str(round(grid_result1.best_score_,2)))\r\nwith open(\"params1.txt\",\"w\") as f:\r\n    f.write(str(grid_result1.best_params_))\r\n    \r\nwith open(\"score2.txt\",\"w\") as f:\r\n    f.write(str(round(grid_result2.best_score_,2)))\r\nwith open(\"params2.txt\",\"w\") as f:\r\n    f.write(str(grid_result2.best_params_))\r\n\r\n```        \r\n\r\n```\r\ndef save_model(model):\r\n    # saving model\r\n    json_model = model.to_json()\r\n    open('model.json', 'w').write(json_model)\r\n    # saving weights\r\n    model.save_weights('model.h5', overwrite=True)\r\nclassifier=create_model()\r\nsave_model(classifier)\r\n```\r\n\r\n ", "comments": ["@ashishsme14,\r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a TensorFlow bug or feature request. There is also a larger community that reads questions there. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "please help us\r\n", "This question is better asked on StackOverflow as it asks for help in solving a particular problem. As such, it should not be here."]}, {"number": 46318, "title": "code's", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@akeem377 \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]", "Looks like spam issue", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46318\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46318\">No</a>\n"]}, {"number": 46317, "title": "In different tf2 versions, the weight naming rules of creating keras model are different", "body": "**System information**\r\n- OS Platform and Distribution : Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): pip install tensorflow-cpu\r\n- TensorFlow version (use command below): 2.1, 2.2, 2.3, 2.4\r\n- Python version: 3.6 / 3.8\r\n\r\n**Describe the current behavior**\r\nI create keras model using same code, but got different results(different weight names) in different tf version.\r\nThis will prevent me from loading network weights (based on variable name) in different tf version.\r\n\r\n**Describe the expected behavior**\r\nI can get same results in different tf version.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nI create kears model:\r\n```python\r\nfrom tensorflow.keras import layers, Model, Sequential\r\n\r\n\r\nclass ConvBNReLU(layers.Layer):\r\n    def __init__(self, out_channel, kernel_size=3, stride=1, **kwargs):\r\n        super(ConvBNReLU, self).__init__(**kwargs)\r\n        layers_list = [layers.Conv2D(filters=out_channel, kernel_size=kernel_size,\r\n                                     strides=stride, padding='SAME', use_bias=False, name='Conv2d'),\r\n                       layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name='BatchNorm'),\r\n                       layers.ReLU(max_value=6.0)]\r\n\r\n        self.combine_layer = Sequential(layers_list, name=\"combine\")\r\n\r\n    def call(self, inputs, training=False, **kwargs):\r\n        x = self.combine_layer(inputs, training=training)\r\n        return x\r\n\r\n\r\ndef main():\r\n    input_image = layers.Input(shape=(224, 224, 3), dtype='float32')\r\n    # conv1\r\n    x = ConvBNReLU(32, stride=2)(input_image)\r\n    output = ConvBNReLU(64, stride=2)(x)\r\n    model = Model(inputs=input_image, outputs=output)\r\n\r\n    for i in model.weights:\r\n        print(i.name)\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n\r\n```\r\n\r\nIn tf2.0, 2.1 and 2.2, the printed weight name information is as follows:\r\n```\r\nconv_bn_re_lu/combine/Conv2d/kernel:0\r\nconv_bn_re_lu/combine/BatchNorm/gamma:0\r\nconv_bn_re_lu/combine/BatchNorm/beta:0\r\nconv_bn_re_lu/combine/BatchNorm/moving_mean:0\r\nconv_bn_re_lu/combine/BatchNorm/moving_variance:0\r\nconv_bn_re_lu_1/combine/Conv2d/kernel:0\r\nconv_bn_re_lu_1/combine/BatchNorm/gamma:0\r\nconv_bn_re_lu_1/combine/BatchNorm/beta:0\r\nconv_bn_re_lu_1/combine/BatchNorm/moving_mean:0\r\nconv_bn_re_lu_1/combine/BatchNorm/moving_variance:0\r\n```\r\n\r\nBut in tf2.3 and 2.4, I got different results:\r\n```\r\nConv2d/kernel:0\r\nBatchNorm/gamma:0\r\nBatchNorm/beta:0\r\nBatchNorm/moving_mean:0\r\nBatchNorm/moving_variance:0\r\nConv2d/kernel:0\r\nBatchNorm/gamma:0\r\nBatchNorm/beta:0\r\nBatchNorm/moving_mean:0\r\nBatchNorm/moving_variance:0\r\n```\r\n", "comments": ["I have tried in colab with TF 2.0,2.1,2.2,2.3 and i am seeing the below results.\r\n```\r\nconv_bn_re_lu/combine/Conv2d/kernel:0\r\nconv_bn_re_lu/combine/BatchNorm/gamma:0\r\nconv_bn_re_lu/combine/BatchNorm/beta:0\r\nconv_bn_re_lu/combine/BatchNorm/moving_mean:0\r\nconv_bn_re_lu/combine/BatchNorm/moving_variance:0\r\nconv_bn_re_lu_1/combine/Conv2d/kernel:0\r\nconv_bn_re_lu_1/combine/BatchNorm/gamma:0\r\nconv_bn_re_lu_1/combine/BatchNorm/beta:0\r\nconv_bn_re_lu_1/combine/BatchNorm/moving_mean:0\r\nconv_bn_re_lu_1/combine/BatchNorm/moving_variance:0\r\n```\r\nWith TF 2.4 and nightly version(`2.5.0-dev20210110`). I am seeing different results.\r\n```\r\nConv2d/kernel:0\r\nBatchNorm/gamma:0\r\nBatchNorm/beta:0\r\nBatchNorm/moving_mean:0\r\nBatchNorm/moving_variance:0\r\nConv2d/kernel:0\r\nBatchNorm/gamma:0\r\nBatchNorm/beta:0\r\nBatchNorm/moving_mean:0\r\nBatchNorm/moving_variance:0\r\n```\r\nPLease, find the gist [here](https://colab.research.google.com/gist/ravikyram/d678117eb1fb07e12090b4dd8a1171dc/untitled607.ipynb).Thanks!", "@WZMIAOMIAO Code change in internal modules changed the path for model weights. However, how is it affecting you. Can you please share more details (with a standalone code) on your use-case?  Thanks!", "@jvishnuvardhan This will prevent me from loading network weights **(based on variable name)** in different tf version.\r\n\r\n### When I was learning tensorflow2, I recorded my own learning process. \r\nRecorded in: [tensorflow_classification/Test5_resnet](https://github.com/WZMIAOMIAO/deep-learning-for-image-processing/tree/master/tensorflow_classification/Test5_resnet)\r\n\r\n#### 1) using [model.py](https://github.com/WZMIAOMIAO/deep-learning-for-image-processing/blob/master/tensorflow_classification/Test5_resnet/model.py) file to build resnet model.\r\n\r\n#### 2) I get resnet50 weights based on variable name through transformation.\r\n- download TF official resnet weights. http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz\r\n- using [read_ckpt.py](https://github.com/WZMIAOMIAO/deep-learning-for-image-processing/blob/master/tensorflow_classification/Test5_resnet/read_ckpt.py) file to transform weights name.\r\n\r\n#### 3) using [train.py](https://github.com/WZMIAOMIAO/deep-learning-for-image-processing/blob/master/tensorflow_classification/Test5_resnet/train.py) file to load pretrain weights and train network.\r\n\r\nWith TF 2.0, 2.1, 2.2, I can execute successfully. \r\nBut with TF2.4, when executing to ```feature.load_weights(pre_weights_path)``` I got error:\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/home/wz/miniconda3/envs/tf24/lib/python3.8/site-packages/tensorflow/python/training/py_checkpoint_reader.py\", line 69, in get_tensor\r\n    return CheckpointReader.CheckpointReader_GetTensor(\r\nRuntimeError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/wz/miniconda3/envs/tf24/lib/python3.8/site-packages/tensorflow/python/training/tracking/util.py\", line 1299, in restore\r\n    object_graph_string = reader.get_tensor(base.OBJECT_GRAPH_PROTO_KEY)\r\n  File \"/home/wz/miniconda3/envs/tf24/lib/python3.8/site-packages/tensorflow/python/training/py_checkpoint_reader.py\", line 74, in get_tensor\r\n    error_translator(e)\r\n  File \"/home/wz/miniconda3/envs/tf24/lib/python3.8/site-packages/tensorflow/python/training/py_checkpoint_reader.py\", line 35, in error_translator\r\n    raise errors_impl.NotFoundError(None, None, error_message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/wz/miniconda3/envs/tf24/lib/python3.8/site-packages/tensorflow/python/training/saving/saveable_object_util.py\", line 131, in restore\r\n    return resource_variable_ops.shape_safe_assign_variable_handle(\r\n  File \"/home/wz/miniconda3/envs/tf24/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 307, in shape_safe_assign_variable_handle\r\n    shape.assert_is_compatible_with(value_tensor.shape)\r\n  File \"/home/wz/miniconda3/envs/tf24/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py\", line 1134, in assert_is_compatible_with\r\n    raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\r\nValueError: Shapes (1, 1, 64, 256) and (7, 7, 3, 64) are incompatible\r\n```\r\n\r\n#### 4) Because of the above problems, I try to use ```feature.set_weights()``` to load weights. But I find that with TF2.4 each weights name using ```feature.weights``` to get is not complete. I think this problem was posed by ```Sequential``` function with TF2.4.\r\n", "> This will prevent me from loading network weights (based on variable name) in different tf version.\r\n\r\nVariable names have no stability guarantee. For this reason, weight loading should not be based on naming.\r\n\r\n> Because of the above problems, I try to use feature.set_weights() to load weights\r\n\r\nThis is the correct take. Use `get_weights` / `load_weights` across equivalent models.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46317\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46317\">No</a>\n", "@fchollet Thank you for your reply. \r\nAs I mentioned above, because I only have weight information and no order information, I need to know the full name of each variable to build an ordered variable weights list. Then I can load the weights by ```set_weights```. But with TF2.4 use of ```Sequential``` results in incomplete variable names, which is correct with TF 2.0, 2.1, 2.2.\r\n\r\nThere's another problem that Why can't we load weights in the form of dictionaries like Pytorch. It's very convenient.\r\n\r\n"]}, {"number": 46316, "title": "Keras model conflicting with multiprocessing", "body": "This is not necessarily a bug with tensorflow / keras but it may be classified as incompatibility issue. I'm getting a weird error when I'm trying to run 2 class methods concurrently in a third method. After eliminating large chunks of code, one at a time, I was surprised to find out that having keras model as a class attribute in the example, leads to the error.\r\n\r\n**Things to note:** \r\n\r\n - I must have a model as a class attribute, I cannot change that.\r\n - I need both tasks to run concurrently and I cannot get these 2 tasks out of the class because they interact with other class members\r\n - I get the same error using `multiprocessing.Process()`, so that also will not fix the problem.\r\n___\r\n\r\n    from concurrent.futures import ProcessPoolExecutor, as_completed\r\n    \r\n    from tensorflow.keras.models import Model\r\n    \r\n    \r\n    class Example:\r\n        def __init__(self):\r\n            self.model = Model()\r\n            # comment out the line above and uncomment the line below, the error is gone\r\n            # self.model = None\r\n    \r\n        def task1(self):\r\n            pass\r\n    \r\n        def task2(self):\r\n            pass\r\n    \r\n        def process(\r\n            self,\r\n        ):\r\n            with ProcessPoolExecutor(2) as executor:\r\n                future_items = [\r\n                    executor.submit(self.task1),\r\n                    executor.submit(self.task2),\r\n                ]\r\n                results = [\r\n                    future_item.result() for future_item in as_completed(future_items)\r\n                ]\r\n                print(results)\r\n    \r\n    \r\n    if __name__ == '__main__':\r\n        ex = Example()\r\n        ex.process()\r\n\r\n**Results in:**\r\n\r\n    2021-01-10 08:10:04.315386: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n    2021-01-10 08:10:04.315897: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\n    To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n    concurrent.futures.process._RemoteTraceback: \r\n    \"\"\"\r\n    Traceback (most recent call last):\r\n      File \"/usr/local/Cellar/python@3.8/3.8.7/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/queues.py\", line 239, in _feed\r\n        obj = _ForkingPickler.dumps(obj)\r\n      File \"/usr/local/Cellar/python@3.8/3.8.7/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/reduction.py\", line 51, in dumps\r\n        cls(buf, protocol).dump(obj)\r\n    TypeError: cannot pickle 'weakref' object\r\n    \"\"\"\r\n    \r\n    The above exception was the direct cause of the following exception:\r\n    \r\n    Traceback (most recent call last):\r\n      File \"/Users/emadboctor/Desktop/code/drl-algos/scratch.py\", line 34, in <module>\r\n        ex.process()\r\n      File \"/Users/emadboctor/Desktop/code/drl-algos/scratch.py\", line 26, in process\r\n        results = [\r\n      File \"/Users/emadboctor/Desktop/code/drl-algos/scratch.py\", line 27, in <listcomp>\r\n        future_item.result() for future_item in as_completed(future_items)\r\n      File \"/usr/local/Cellar/python@3.8/3.8.7/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py\", line 432, in result\r\n        return self.__get_result()\r\n      File \"/usr/local/Cellar/python@3.8/3.8.7/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py\", line 388, in __get_result\r\n        raise self._exception\r\n      File \"/usr/local/Cellar/python@3.8/3.8.7/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/queues.py\", line 239, in _feed\r\n        obj = _ForkingPickler.dumps(obj)\r\n      File \"/usr/local/Cellar/python@3.8/3.8.7/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/reduction.py\", line 51, in dumps\r\n        cls(buf, protocol).dump(obj)\r\n    TypeError: cannot pickle 'weakref' object\r\n\r\n\r\n", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/1a7cb5107672b21361f9cdb6024bdb49/46316.ipynb). Thanks!", "Keras models are not multiprocessing-safe. I recommend simply creating multiple independent copies of your model for use in each process."]}, {"number": 46315, "title": "TF ConvertedModel: Invoke fails with \"Node number X (CONCATENATION) failed to prepare\" error", "body": "**System information**\r\n- OS: Windows 10:\r\n- TensorFlow: 2.4.0:\r\n\r\n**Code used to infer** : \r\n`\r\n   # Load the TFLite model and allocate tensors.\r\n    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\r\n    interpreter.allocate_tensors()\r\n\r\n    # Get input and output tensors.\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n\r\n    # Test the model on random input data.\r\n    input_shape = input_details[0]['shape']\r\n    input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\n    interpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\n    interpreter.invoke()\r\n\r\n    # The function `get_tensor()` returns a copy of the tensor data.\r\n    # Use `tensor()` in order to get a pointer to the tensor.\r\n    output_data = interpreter.get_tensor(output_details[0]['index'])\r\n    print(output_data)\r\n`\r\n\r\n\r\n**Output**:\r\n`\r\nINFO: TfLiteFlexDelegate delegate: 15 nodes delegated out of 188 nodes with 2 partitions.\r\nINFO: TfLiteFlexDelegate delegate: 5 nodes delegated out of 12 nodes with 1 partitions.\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 0 nodes with 0 partitions.\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.\r\nINFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 35 nodes with 2 partitions.\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 17 nodes with 0 partitions.\r\n\r\nTraceback (most recent call last):\r\n  File \"src\\models\\net_converters\\test_tflite_model.py\", line 127, in <module>\r\n    test(args.model, args.test)\r\n  File \"src\\models\\net_converters\\test_tflite_model.py\", line 31, in test\r\n    interpreter.invoke()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py\", line 540, in invoke\r\n    self._interpreter.Invoke()\r\nRuntimeError: tensorflow/lite/kernels/concatenation.cc:76 t->dims->data[d] != t0->dims->data[d] (400 != 1)Node number 33 (CONCATENATION) failed to prepare.\r\nNode number 3 (WHILE) failed to invoke.\r\nNode number 187 (WHILE) failed to invoke.\r\n`\r\n\r\n**TF Model summary** \r\n`\r\nLayer (type)                    Output Shape         Param #     Connected to\r\n==================================================================================================\r\ninput_1 (InputLayer)            [(None, 300, 300, 3) 0\r\n__________________________________________________________________________________________________\r\nidentity_layer (Lambda)         (None, 300, 300, 3)  0           input_1[0][0]\r\n__________________________________________________________________________________________________\r\ninput_mean_normalization (Lambd (None, 300, 300, 3)  0           identity_layer[0][0]\r\n__________________________________________________________________________________________________\r\ninput_channel_swap (Lambda)     (None, 300, 300, 3)  0           input_mean_normalization[0][0]\r\n__________________________________________________________________________________________________\r\nconv1_1 (Conv2D)                (None, 300, 300, 64) 1792        input_channel_swap[0][0]\r\n__________________________________________________________________________________________________\r\nconv1_2 (Conv2D)                (None, 300, 300, 64) 36928       conv1_1[0][0]\r\n__________________________________________________________________________________________________\r\npool1 (MaxPooling2D)            (None, 150, 150, 64) 0           conv1_2[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_1 (Conv2D)                (None, 150, 150, 128 73856       pool1[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_2 (Conv2D)                (None, 150, 150, 128 147584      conv2_1[0][0]\r\n__________________________________________________________________________________________________\r\npool2 (MaxPooling2D)            (None, 75, 75, 128)  0           conv2_2[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_1 (Conv2D)                (None, 75, 75, 256)  295168      pool2[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_2 (Conv2D)                (None, 75, 75, 256)  590080      conv3_1[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_3 (Conv2D)                (None, 75, 75, 256)  590080      conv3_2[0][0]\r\n__________________________________________________________________________________________________\r\npool3 (MaxPooling2D)            (None, 38, 38, 256)  0           conv3_3[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_1 (Conv2D)                (None, 38, 38, 512)  1180160     pool3[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_2 (Conv2D)                (None, 38, 38, 512)  2359808     conv4_1[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_3 (Conv2D)                (None, 38, 38, 512)  2359808     conv4_2[0][0]\r\n__________________________________________________________________________________________________\r\npool4 (MaxPooling2D)            (None, 19, 19, 512)  0           conv4_3[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_1 (Conv2D)                (None, 19, 19, 512)  2359808     pool4[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_2 (Conv2D)                (None, 19, 19, 512)  2359808     conv5_1[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_3 (Conv2D)                (None, 19, 19, 512)  2359808     conv5_2[0][0]\r\n__________________________________________________________________________________________________\r\npool5 (MaxPooling2D)            (None, 19, 19, 512)  0           conv5_3[0][0]\r\n__________________________________________________________________________________________________\r\nfc6 (Conv2D)                    (None, 19, 19, 1024) 4719616     pool5[0][0]\r\n__________________________________________________________________________________________________\r\nfc7 (Conv2D)                    (None, 19, 19, 1024) 1049600     fc6[0][0]\r\n__________________________________________________________________________________________________\r\nconv6_1 (Conv2D)                (None, 19, 19, 256)  262400      fc7[0][0]\r\n__________________________________________________________________________________________________\r\nconv6_padding (ZeroPadding2D)   (None, 21, 21, 256)  0           conv6_1[0][0]\r\n__________________________________________________________________________________________________\r\nconv6_2 (Conv2D)                (None, 10, 10, 512)  1180160     conv6_padding[0][0]\r\n__________________________________________________________________________________________________\r\nconv7_1 (Conv2D)                (None, 10, 10, 128)  65664       conv6_2[0][0]\r\n__________________________________________________________________________________________________\r\nconv7_padding (ZeroPadding2D)   (None, 12, 12, 128)  0           conv7_1[0][0]\r\n__________________________________________________________________________________________________\r\nconv7_2 (Conv2D)                (None, 5, 5, 256)    295168      conv7_padding[0][0]\r\n__________________________________________________________________________________________________\r\nconv8_1 (Conv2D)                (None, 5, 5, 128)    32896       conv7_2[0][0]\r\n__________________________________________________________________________________________________\r\nconv8_2 (Conv2D)                (None, 3, 3, 256)    295168      conv8_1[0][0]\r\n__________________________________________________________________________________________________\r\nconv9_1 (Conv2D)                (None, 3, 3, 128)    32896       conv8_2[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_3_norm (L2Normalization)  (None, 38, 38, 512)  512         conv4_3[0][0]\r\n__________________________________________________________________________________________________\r\nconv9_2 (Conv2D)                (None, 1, 1, 256)    295168      conv9_1[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_3_norm_mbox_conf (Conv2D) (None, 38, 38, 4220) 19449980    conv4_3_norm[0][0]\r\n__________________________________________________________________________________________________\r\nfc7_mbox_conf (Conv2D)          (None, 19, 19, 5275) 48619675    fc7[0][0]\r\n__________________________________________________________________________________________________\r\nconv6_2_mbox_conf (Conv2D)      (None, 10, 10, 5275) 24312475    conv6_2[0][0]\r\n__________________________________________________________________________________________________\r\nconv7_2_mbox_conf (Conv2D)      (None, 5, 5, 5275)   12158875    conv7_2[0][0]\r\n__________________________________________________________________________________________________\r\nconv8_2_mbox_conf (Conv2D)      (None, 3, 3, 4220)   9727100     conv8_2[0][0]\r\n__________________________________________________________________________________________________\r\nconv9_2_mbox_conf (Conv2D)      (None, 1, 1, 4220)   9727100     conv9_2[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_3_norm_mbox_loc (Conv2D)  (None, 38, 38, 16)   73744       conv4_3_norm[0][0]\r\n__________________________________________________________________________________________________\r\nfc7_mbox_loc (Conv2D)           (None, 19, 19, 20)   184340      fc7[0][0]\r\n__________________________________________________________________________________________________\r\nconv6_2_mbox_loc (Conv2D)       (None, 10, 10, 20)   92180       conv6_2[0][0]\r\n__________________________________________________________________________________________________\r\nconv7_2_mbox_loc (Conv2D)       (None, 5, 5, 20)     46100       conv7_2[0][0]\r\n__________________________________________________________________________________________________\r\nconv8_2_mbox_loc (Conv2D)       (None, 3, 3, 16)     36880       conv8_2[0][0]\r\n__________________________________________________________________________________________________\r\nconv9_2_mbox_loc (Conv2D)       (None, 1, 1, 16)     36880       conv9_2[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_3_norm_mbox_conf_reshape  (None, 5776, 1055)   0           conv4_3_norm_mbox_conf[0][0]\r\n__________________________________________________________________________________________________\r\nfc7_mbox_conf_reshape (Reshape) (None, 1805, 1055)   0           fc7_mbox_conf[0][0]\r\n__________________________________________________________________________________________________\r\nconv6_2_mbox_conf_reshape (Resh (None, 500, 1055)    0           conv6_2_mbox_conf[0][0]\r\n__________________________________________________________________________________________________\r\nconv7_2_mbox_conf_reshape (Resh (None, 125, 1055)    0           conv7_2_mbox_conf[0][0]\r\n__________________________________________________________________________________________________\r\nconv8_2_mbox_conf_reshape (Resh (None, 36, 1055)     0           conv8_2_mbox_conf[0][0]\r\n__________________________________________________________________________________________________\r\nconv9_2_mbox_conf_reshape (Resh (None, 4, 1055)      0           conv9_2_mbox_conf[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_3_norm_mbox_priorbox (Anc (None, 38, 38, 4, 8) 0           conv4_3_norm_mbox_loc[0][0]\r\n__________________________________________________________________________________________________\r\nfc7_mbox_priorbox (AnchorBoxes) (None, 19, 19, 5, 8) 0           fc7_mbox_loc[0][0]\r\n__________________________________________________________________________________________________\r\nconv6_2_mbox_priorbox (AnchorBo (None, 10, 10, 5, 8) 0           conv6_2_mbox_loc[0][0]\r\n__________________________________________________________________________________________________\r\nconv7_2_mbox_priorbox (AnchorBo (None, 5, 5, 5, 8)   0           conv7_2_mbox_loc[0][0]\r\n__________________________________________________________________________________________________\r\nconv8_2_mbox_priorbox (AnchorBo (None, 3, 3, 4, 8)   0           conv8_2_mbox_loc[0][0]\r\n__________________________________________________________________________________________________\r\nconv9_2_mbox_priorbox (AnchorBo (None, 1, 1, 4, 8)   0           conv9_2_mbox_loc[0][0]\r\n__________________________________________________________________________________________________\r\nmbox_conf (Concatenate)         (None, 8246, 1055)   0           conv4_3_norm_mbox_conf_reshape[0]\r\n                                                                 fc7_mbox_conf_reshape[0][0]\r\n                                                                 conv6_2_mbox_conf_reshape[0][0]\r\n                                                                 conv7_2_mbox_conf_reshape[0][0]\r\n                                                                 conv8_2_mbox_conf_reshape[0][0]\r\n                                                                 conv9_2_mbox_conf_reshape[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_3_norm_mbox_loc_reshape ( (None, 5776, 4)      0           conv4_3_norm_mbox_loc[0][0]\r\n__________________________________________________________________________________________________\r\nfc7_mbox_loc_reshape (Reshape)  (None, 1805, 4)      0           fc7_mbox_loc[0][0]\r\n__________________________________________________________________________________________________\r\nconv6_2_mbox_loc_reshape (Resha (None, 500, 4)       0           conv6_2_mbox_loc[0][0]\r\n__________________________________________________________________________________________________\r\nconv7_2_mbox_loc_reshape (Resha (None, 125, 4)       0           conv7_2_mbox_loc[0][0]\r\n__________________________________________________________________________________________________\r\nconv8_2_mbox_loc_reshape (Resha (None, 36, 4)        0           conv8_2_mbox_loc[0][0]\r\n__________________________________________________________________________________________________\r\nconv9_2_mbox_loc_reshape (Resha (None, 4, 4)         0           conv9_2_mbox_loc[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_3_norm_mbox_priorbox_resh (None, 5776, 8)      0           conv4_3_norm_mbox_priorbox[0][0]\r\n__________________________________________________________________________________________________\r\nfc7_mbox_priorbox_reshape (Resh (None, 1805, 8)      0           fc7_mbox_priorbox[0][0]\r\n__________________________________________________________________________________________________\r\nconv6_2_mbox_priorbox_reshape ( (None, 500, 8)       0           conv6_2_mbox_priorbox[0][0]\r\n__________________________________________________________________________________________________\r\nconv7_2_mbox_priorbox_reshape ( (None, 125, 8)       0           conv7_2_mbox_priorbox[0][0]\r\n__________________________________________________________________________________________________\r\nconv8_2_mbox_priorbox_reshape ( (None, 36, 8)        0           conv8_2_mbox_priorbox[0][0]\r\n__________________________________________________________________________________________________\r\nconv9_2_mbox_priorbox_reshape ( (None, 4, 8)         0           conv9_2_mbox_priorbox[0][0]\r\n__________________________________________________________________________________________________\r\nmbox_conf_softmax (Activation)  (None, 8246, 1055)   0           mbox_conf[0][0]\r\n__________________________________________________________________________________________________\r\nmbox_loc (Concatenate)          (None, 8246, 4)      0           conv4_3_norm_mbox_loc_reshape[0][\r\n                                                                 fc7_mbox_loc_reshape[0][0]\r\n                                                                 conv6_2_mbox_loc_reshape[0][0]\r\n                                                                 conv7_2_mbox_loc_reshape[0][0]\r\n                                                                 conv8_2_mbox_loc_reshape[0][0]\r\n                                                                 conv9_2_mbox_loc_reshape[0][0]\r\n__________________________________________________________________________________________________\r\nmbox_priorbox (Concatenate)     (None, 8246, 8)      0           conv4_3_norm_mbox_priorbox_reshap\r\n                                                                 fc7_mbox_priorbox_reshape[0][0]\r\n                                                                 conv6_2_mbox_priorbox_reshape[0][\r\n                                                                 conv7_2_mbox_priorbox_reshape[0][\r\n                                                                 conv8_2_mbox_priorbox_reshape[0][\r\n                                                                 conv9_2_mbox_priorbox_reshape[0][\r\n__________________________________________________________________________________________________\r\npredictions (Concatenate)       (None, 8246, 1067)   0           mbox_conf_softmax[0][0]\r\n                                                                 mbox_loc[0][0]\r\n                                                                 mbox_priorbox[0][0]\r\n__________________________________________________________________________________________________\r\ndecoded_predictions (DecodeDete (None, 200, 6)       0           predictions[0][0]\r\n==================================================================================================\r\nTotal params: 147,409,265\r\nTrainable params: 147,409,265\r\nNon-trainable params: 0\r\n`\r\n\r\n\r\nDo you need any additional data or maybe it's kind of known issue ?\r\nThank you & best regards\r\n", "comments": ["Could you set a value for batch size instead of None and convert it again?\r\n", "@thaink  thank you for the proposal, but the question is how can I define that ?\r\nBecause the net was trained with some batch size, and I failed to find any TF API allow to define batch size during model compilation... ", "@thaink  I succeeded to get to the following TF net state:\r\n\r\n`\r\nLayer (type)                    Output Shape         Param #     Connected to\r\n==================================================================================================\r\ninput_1 (InputLayer)            [(1, 300, 300, 3)]   0\r\n__________________________________________________________________________________________________\r\nidentity_layer (Lambda)         (1, 300, 300, 3)     0           input_1[0][0]\r\n__________________________________________________________________________________________________\r\ninput_mean_normalization (Lambd (1, 300, 300, 3)     0           identity_layer[0][0]\r\n__________________________________________________________________________________________________\r\ninput_channel_swap (Lambda)     (1, 300, 300, 3)     0           input_mean_normalization[0][0]\r\n__________________________________________________________________________________________________\r\nconv1_1 (Conv2D)                (1, 300, 300, 64)    1792        input_channel_swap[0][0]\r\n__________________________________________________________________________________________________\r\nconv1_2 (Conv2D)                (1, 300, 300, 64)    36928       conv1_1[0][0]\r\n__________________________________________________________________________________________________\r\npool1 (MaxPooling2D)            (1, 150, 150, 64)    0           conv1_2[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_1 (Conv2D)                (1, 150, 150, 128)   73856       pool1[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_2 (Conv2D)                (1, 150, 150, 128)   147584      conv2_1[0][0]\r\n__________________________________________________________________________________________________\r\npool2 (MaxPooling2D)            (1, 75, 75, 128)     0           conv2_2[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_1 (Conv2D)                (1, 75, 75, 256)     295168      pool2[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_2 (Conv2D)                (1, 75, 75, 256)     590080      conv3_1[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_3 (Conv2D)                (1, 75, 75, 256)     590080      conv3_2[0][0]\r\n__________________________________________________________________________________________________\r\npool3 (MaxPooling2D)            (1, 38, 38, 256)     0           conv3_3[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_1 (Conv2D)                (1, 38, 38, 512)     1180160     pool3[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_2 (Conv2D)                (1, 38, 38, 512)     2359808     conv4_1[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_3 (Conv2D)                (1, 38, 38, 512)     2359808     conv4_2[0][0]\r\n__________________________________________________________________________________________________\r\npool4 (MaxPooling2D)            (1, 19, 19, 512)     0           conv4_3[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_1 (Conv2D)                (1, 19, 19, 512)     2359808     pool4[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_2 (Conv2D)                (1, 19, 19, 512)     2359808     conv5_1[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_3 (Conv2D)                (1, 19, 19, 512)     2359808     conv5_2[0][0]\r\n__________________________________________________________________________________________________\r\npool5 (MaxPooling2D)            (1, 19, 19, 512)     0           conv5_3[0][0]\r\n__________________________________________________________________________________________________\r\nfc6 (Conv2D)                    (1, 19, 19, 1024)    4719616     pool5[0][0]\r\n__________________________________________________________________________________________________\r\nfc7 (Conv2D)                    (1, 19, 19, 1024)    1049600     fc6[0][0]\r\n__________________________________________________________________________________________________\r\nconv6_1 (Conv2D)                (1, 19, 19, 256)     262400      fc7[0][0]\r\n__________________________________________________________________________________________________\r\nconv6_padding (ZeroPadding2D)   (1, 21, 21, 256)     0           conv6_1[0][0]\r\n__________________________________________________________________________________________________\r\nconv6_2 (Conv2D)                (1, 10, 10, 512)     1180160     conv6_padding[0][0]\r\n__________________________________________________________________________________________________\r\nconv7_1 (Conv2D)                (1, 10, 10, 128)     65664       conv6_2[0][0]\r\n__________________________________________________________________________________________________\r\nconv7_padding (ZeroPadding2D)   (1, 12, 12, 128)     0           conv7_1[0][0]\r\n__________________________________________________________________________________________________\r\nconv7_2 (Conv2D)                (1, 5, 5, 256)       295168      conv7_padding[0][0]\r\n__________________________________________________________________________________________________\r\nconv8_1 (Conv2D)                (1, 5, 5, 128)       32896       conv7_2[0][0]\r\n__________________________________________________________________________________________________\r\nconv8_2 (Conv2D)                (1, 3, 3, 256)       295168      conv8_1[0][0]\r\n__________________________________________________________________________________________________\r\nconv9_1 (Conv2D)                (1, 3, 3, 128)       32896       conv8_2[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_3_norm (L2Normalization)  (1, 38, 38, 512)     512         conv4_3[0][0]\r\n__________________________________________________________________________________________________\r\nconv9_2 (Conv2D)                (1, 1, 1, 256)       295168      conv9_1[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_3_norm_mbox_conf (Conv2D) (1, 38, 38, 4220)    19449980    conv4_3_norm[0][0]\r\n__________________________________________________________________________________________________\r\nfc7_mbox_conf (Conv2D)          (1, 19, 19, 5275)    48619675    fc7[0][0]\r\n__________________________________________________________________________________________________\r\nconv6_2_mbox_conf (Conv2D)      (1, 10, 10, 5275)    24312475    conv6_2[0][0]\r\n__________________________________________________________________________________________________\r\nconv7_2_mbox_conf (Conv2D)      (1, 5, 5, 5275)      12158875    conv7_2[0][0]\r\n__________________________________________________________________________________________________\r\nconv8_2_mbox_conf (Conv2D)      (1, 3, 3, 4220)      9727100     conv8_2[0][0]\r\n__________________________________________________________________________________________________\r\nconv9_2_mbox_conf (Conv2D)      (1, 1, 1, 4220)      9727100     conv9_2[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_3_norm_mbox_loc (Conv2D)  (1, 38, 38, 16)      73744       conv4_3_norm[0][0]\r\n__________________________________________________________________________________________________\r\nfc7_mbox_loc (Conv2D)           (1, 19, 19, 20)      184340      fc7[0][0]\r\n__________________________________________________________________________________________________\r\nconv6_2_mbox_loc (Conv2D)       (1, 10, 10, 20)      92180       conv6_2[0][0]\r\n__________________________________________________________________________________________________\r\nconv7_2_mbox_loc (Conv2D)       (1, 5, 5, 20)        46100       conv7_2[0][0]\r\n__________________________________________________________________________________________________\r\nconv8_2_mbox_loc (Conv2D)       (1, 3, 3, 16)        36880       conv8_2[0][0]\r\n__________________________________________________________________________________________________\r\nconv9_2_mbox_loc (Conv2D)       (1, 1, 1, 16)        36880       conv9_2[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_3_norm_mbox_conf_reshape  (1, 5776, 1055)      0           conv4_3_norm_mbox_conf[0][0]\r\n__________________________________________________________________________________________________\r\nfc7_mbox_conf_reshape (Reshape) (1, 1805, 1055)      0           fc7_mbox_conf[0][0]\r\n__________________________________________________________________________________________________\r\nconv6_2_mbox_conf_reshape (Resh (1, 500, 1055)       0           conv6_2_mbox_conf[0][0]\r\n__________________________________________________________________________________________________\r\nconv7_2_mbox_conf_reshape (Resh (1, 125, 1055)       0           conv7_2_mbox_conf[0][0]\r\n__________________________________________________________________________________________________\r\nconv8_2_mbox_conf_reshape (Resh (1, 36, 1055)        0           conv8_2_mbox_conf[0][0]\r\n__________________________________________________________________________________________________\r\nconv9_2_mbox_conf_reshape (Resh (1, 4, 1055)         0           conv9_2_mbox_conf[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_3_norm_mbox_priorbox (Anc (1, 38, 38, 4, 8)    0           conv4_3_norm_mbox_loc[0][0]\r\n__________________________________________________________________________________________________\r\nfc7_mbox_priorbox (AnchorBoxes) (1, 19, 19, 5, 8)    0           fc7_mbox_loc[0][0]\r\n__________________________________________________________________________________________________\r\nconv6_2_mbox_priorbox (AnchorBo (1, 10, 10, 5, 8)    0           conv6_2_mbox_loc[0][0]\r\n__________________________________________________________________________________________________\r\nconv7_2_mbox_priorbox (AnchorBo (1, 5, 5, 5, 8)      0           conv7_2_mbox_loc[0][0]\r\n__________________________________________________________________________________________________\r\nconv8_2_mbox_priorbox (AnchorBo (1, 3, 3, 4, 8)      0           conv8_2_mbox_loc[0][0]\r\n__________________________________________________________________________________________________\r\nconv9_2_mbox_priorbox (AnchorBo (1, 1, 1, 4, 8)      0           conv9_2_mbox_loc[0][0]\r\n__________________________________________________________________________________________________\r\nmbox_conf (Concatenate)         (1, 8246, 1055)      0           conv4_3_norm_mbox_conf_reshape[0]\r\n                                                                 fc7_mbox_conf_reshape[0][0]\r\n                                                                 conv6_2_mbox_conf_reshape[0][0]\r\n                                                                 conv7_2_mbox_conf_reshape[0][0]\r\n                                                                 conv8_2_mbox_conf_reshape[0][0]\r\n                                                                 conv9_2_mbox_conf_reshape[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_3_norm_mbox_loc_reshape ( (1, 5776, 4)         0           conv4_3_norm_mbox_loc[0][0]\r\n__________________________________________________________________________________________________\r\nfc7_mbox_loc_reshape (Reshape)  (1, 1805, 4)         0           fc7_mbox_loc[0][0]\r\n__________________________________________________________________________________________________\r\nconv6_2_mbox_loc_reshape (Resha (1, 500, 4)          0           conv6_2_mbox_loc[0][0]\r\n__________________________________________________________________________________________________\r\nconv7_2_mbox_loc_reshape (Resha (1, 125, 4)          0           conv7_2_mbox_loc[0][0]\r\n__________________________________________________________________________________________________\r\nconv8_2_mbox_loc_reshape (Resha (1, 36, 4)           0           conv8_2_mbox_loc[0][0]\r\n__________________________________________________________________________________________________\r\nconv9_2_mbox_loc_reshape (Resha (1, 4, 4)            0           conv9_2_mbox_loc[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_3_norm_mbox_priorbox_resh (1, 5776, 8)         0           conv4_3_norm_mbox_priorbox[0][0]\r\n__________________________________________________________________________________________________\r\nfc7_mbox_priorbox_reshape (Resh (1, 1805, 8)         0           fc7_mbox_priorbox[0][0]\r\n__________________________________________________________________________________________________\r\nconv6_2_mbox_priorbox_reshape ( (1, 500, 8)          0           conv6_2_mbox_priorbox[0][0]\r\n__________________________________________________________________________________________________\r\nconv7_2_mbox_priorbox_reshape ( (1, 125, 8)          0           conv7_2_mbox_priorbox[0][0]\r\n__________________________________________________________________________________________________\r\nconv8_2_mbox_priorbox_reshape ( (1, 36, 8)           0           conv8_2_mbox_priorbox[0][0]\r\n__________________________________________________________________________________________________\r\nconv9_2_mbox_priorbox_reshape ( (1, 4, 8)            0           conv9_2_mbox_priorbox[0][0]\r\n__________________________________________________________________________________________________\r\nmbox_conf_softmax (Activation)  (1, 8246, 1055)      0           mbox_conf[0][0]\r\n__________________________________________________________________________________________________\r\nmbox_loc (Concatenate)          (1, 8246, 4)         0           conv4_3_norm_mbox_loc_reshape[0][\r\n                                                                 fc7_mbox_loc_reshape[0][0]\r\n                                                                 conv6_2_mbox_loc_reshape[0][0]\r\n                                                                 conv7_2_mbox_loc_reshape[0][0]\r\n                                                                 conv8_2_mbox_loc_reshape[0][0]\r\n                                                                 conv9_2_mbox_loc_reshape[0][0]\r\n__________________________________________________________________________________________________\r\nmbox_priorbox (Concatenate)     (1, 8246, 8)         0           conv4_3_norm_mbox_priorbox_reshap\r\n                                                                 fc7_mbox_priorbox_reshape[0][0]\r\n                                                                 conv6_2_mbox_priorbox_reshape[0][\r\n                                                                 conv7_2_mbox_priorbox_reshape[0][\r\n                                                                 conv8_2_mbox_priorbox_reshape[0][\r\n                                                                 conv9_2_mbox_priorbox_reshape[0][\r\n__________________________________________________________________________________________________\r\npredictions (Concatenate)       (1, 8246, 1067)      0           mbox_conf_softmax[0][0]\r\n                                                                 mbox_loc[0][0]\r\n                                                                 mbox_priorbox[0][0]\r\n__________________________________________________________________________________________________\r\ndecoded_predictions (DecodeDete (1, 200, 6)          0           predictions[0][0]\r\n==================================================================================================\r\nTotal params: 147,409,265\r\nTrainable params: 147,409,265\r\nNon-trainable params: 0\r\n`\r\n\r\nBut Converted TF lite model on invoke throw the same error:\r\n**t->dims->data[d] != t0->dims->data[d] (400 != 1)Node number 33 (CONCATENATION) failed to prepare.\r\nNode number 3 (WHILE) failed to invoke.\r\nNode number 109 (WHILE) failed to invoke.**\r\n\r\n\r\n", "@MaxxTr could you share the minimal reproducible steps by using as a form of notebook or the code snippet? It is hard to predict what is going on from the only logged information.", "@MaxxTr could you do the same conversion with the TF nightly version, which can have a fix for that?", "@abattery  unfortunately, the nightly version gives the same error.\r\nWhat exactly do you need ? Net definitions or net itself ?", "It would be very helpful if you can share your Python model builder code and conversion code.", "Here is it.\r\nUpdate me if there is something else required.\r\n\r\nTo convert model to tf lite:\r\nnet_converters\\tf_to_tflite.py --net_dir <wight_dir_path>\r\n\r\nTo test:\r\nnet_converters\\test_tflite_model.py --model <path to converted tf lite model> --test <path to the img file for test>\r\n\r\n\r\n[code.zip](https://github.com/tensorflow/tensorflow/files/5801889/code.zip)\r\n", "@MaxxTr could you share your h5 file for the TFLite converter input as well, which is used for the h5_to_tflite method? Thanks", "@abattery unfortunately, it's pretty big... But you can use this one: \r\nhttps://github.com/Sujay-k/model_files/blob/master/VGG_ILSVRC_16_layers_fc_reduced.h5", "ValueError: No model found in config file.\r\n\r\nThe shared model is not loadable through keras API. Could you share the other existing model? If the model is too large, you can share google drive link or something else.\r\n\r\nIf available, could you also share your converted tflite model file as well?", "@abattery \r\nHere is it:\r\n\r\nhttps://drive.google.com/file/d/1DLeef8evu70DyTMJ1mbpommcjXQBetb1/view?usp=sharing", "@amahendrakar  @abattery Any updates ? ETA for solving this problem ?\r\nThanks", "@amahendrakar  @abattery  can you please update with the status and ETA ?", "It is hard to reproduce your problem in my side. Is it possible to create a reproducible notebook and share it to us?", "@abattery can you please explain how to create reproducible notebook ?", "@MaxxTr here is the sample colab link: https://colab.research.google.com/gist/Saduf2019/df1b5da9008a5c9dbd3a3033bf869644/untitled507.ipynb\r\n\r\n@Saduf2019 could you help @MaxxTr creating a colab to reproduce the problem?\r\n\r\n", "@Saduf2019  can you please explain what exactly expected to be done ?\r\n@abattery I failed to understand what exactly hard to reproduce... because I tried the same on other PC and got the same results, so it's independent on OS or env... ", "@Saduf2019  Could you help @MaxxTr to bring the conversion code to gist or python notebook?", "@MaxxTr I am still stuck on reproducing your error. Sorry, I need any h5 file that can reproduce this problem in order to debug... It would be nice for me to get reproducible steps to generate the problematic h5 file in a python notebook or get the problematic h5 file.", "@abattery  I provided you the relevant h5 file 2 weeks ago...\r\nPlease use this one:\r\nhttps://drive.google.com/file/d/1DLeef8evu70DyTMJ1mbpommcjXQBetb1/view?usp=sharing\r\n\r\nCan you access it ?\r\n", "@abattery  sharing is approved. Thank you", "HI @MaxxTr \r\n\r\nI tried to load the shared h5 file in my side like the following code:\r\n\r\n```\r\nmodel = tf.keras.models.load_model(\"/tmp/keras_model.h5\")\r\n```\r\n\r\nI got the following error. Could you help me load the h5 file on my side?\r\n\r\n```\r\nValueError: Unknown layer: L2Normalization. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.\r\n````", "In addition to that, we recommend using tf.keras components instead of keras library especially on the TFLite conversion.", "Can you share your tflite model, converted with tf-nightly version? Are you using the fixed input shapes only?", "> HI @MaxxTr\r\n> \r\n> I tried to load the shared h5 file in my side like the following code:\r\n> \r\n> ```\r\n> model = tf.keras.models.load_model(\"/tmp/keras_model.h5\")\r\n> ```\r\n> \r\n> I got the following error. Could you help me load the h5 file on my side?\r\n> \r\n> ```\r\n> ValueError: Unknown layer: L2Normalization. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.\r\n> ```\r\n\r\nI provided you the full code which actually contains the L2Normalization... Please pay attention.", "> In addition to that, we recommend using tf.keras components instead of keras library especially on the TFLite conversion.\r\n\r\nI will try...", "Sorry, it is hard to reproduce your stuffs even with the above zip. It would be great to make your code runnable in a gist, which is a recommended way in this forum..", "@abattery gist is good for  sharing few files, but not a project... \r\nCan you please explain (and not only to say \"hard\") what exactly hard to reproduce ?\r\nWhat is the problem ? what have you did to reproduce ?\r\nAgain, please pay attention that I'm able easily to reproduce it once I'm using the zip I provided and follow the command lines.\r\n\r\nSorry, but I'm trying to move this issue forward as it stops me and it's already takes around 3 weeks !", "@abattery  @Saduf2019  @mahendrakar  \r\nHere is a  short example:\r\nhttps://drive.google.com/drive/folders/1GFusL_IYXslx3q1RPYQAljkYXrYCGJwO\r\nPlease ask permissions to access this file.\r\njust run 'python example.py ' and the issue will be reproduced \r\n\r\n", "Thanks @MaxxTr Now, I can reproduce your issue on my side. Will take a look at them.", "@abattery great news ! Thank you", "I verified this root cause of the problem is lowering static tensor list pass in MLIR. In the original graph, the DecodeDetections keras layer's tf.map_fn is related.\r\n\r\nSimilar issue is https://github.com/tensorflow/tensorflow/issues/46809", "@haozha111 could you take a look at this, along with #46809?", "@abattery  thank you for fast response. Do you have any proposal for workaround ? Can you please estimate fix ETA ?", "Maybe using the DecodeDetectionsFast layer might be a workaround if it is working for you. For the estimation, I would defer to @haozha111 to answer.", "@abattery  unfortunately this function is used in the DecodeDetectionsFast  as well\r\n@haozha111  can you please update with the estimations ?\r\n\r\nThank you", "@abattery ,can you paste the error message to here? Is it related with dynamic shapes with tensorlist?\r\n\r\nAlso @MaxxTr could you please share the python file?", "@haozha111 I recommend looking at #46809 first, which is easy to reproduce the similar problem.\r\n\r\nThe error message is here.\r\n\r\n```\r\nt->dims->data[d] != t0->dims->data[d] (400 != 1)Node number 33 (CONCATENATION) failed to prepare.\r\nNode number 3 (WHILE) failed to invoke.\r\nNode number 109 (WHILE) failed to invoke.\r\n```\r\n\r\nThe problematic concatenation op node is for SetListItem, which is introduced by tf.map_fn.", "@haozha111 done\r\nPlease, pay attention that issue is reproducible even in case I fix the batch number to be constantly 1", "Hi, @MaxxTr \r\n\r\nJust to confirm: is this your keras model code?\r\n  model = tf.keras.models.Sequential()\r\n  model.add(tf.keras.layers.ThresholdedReLU(theta=0.3514439122821289))\r\n  model.add(tf.keras.layers.LeakyReLU(alpha=0.4855740853866919))\r\n  model.add(tf.keras.layers.AveragePooling2D(pool_size=(1, 2), padding='same'))\r\n\r\n  model.add(tf.keras.layers.Flatten())\r\n  model.add(tf.keras.layers.Dense(10, activation='softsign'))\r\n\r\nIt's copied from your colab, I want to get a minimal model to repro the issue.", "Hi @haozha111 \r\nI doesn't have colab for this problem and code is not from my shared model", "@abattery Hi Jaesung, could you point me to the code where the `tf.map_fn` is?", "The DecodeDetections keras layer can be found in the archive file mentioned in https://github.com/tensorflow/tensorflow/issues/46315#issuecomment-774644725. Keras file is located in somewhere in the one of subdirectories. @haozha111 ", "@abattery @haozha111  hello, any news ?", "Hey, **is there some updates for MaxxTr?** Its've been a month since the first  question. Is there other options of getting support / help? please advise", "@abattery  @haozha111  @mihaimaruseac any news ?", "we expect for adequate support form such a high - level professionals.  so sad that we don't get it here... @abattery @haozha111 @mihaimaruseac  could you please advise the following steps, where this kind a support can be obtained, if not here? What else can be tried? ", "Hey @MaxxTr @mariaHit  sorry for the late reply. I will post my findings here by EOD.", "From your model code (in  keras_layer_DecodeDetections.py):\r\n\r\n# Create a function that filters the predictions for the given batch item. Specifically, it performs:\r\n        # - confidence thresholding\r\n        # - non-maximum suppression (NMS)\r\n        # - top-k filtering\r\n        def filter_predictions(batch_item):\r\n\r\n            # Create a function that filters the predictions for one single class.\r\n            def filter_single_class(index):\r\n\r\n                # From a tensor of shape (n_boxes, n_classes + 4 coordinates) extract\r\n                # a tensor of shape (n_boxes, 1 + 4 coordinates) that contains the\r\n                # confidnece values for just one class, determined by `index`.\r\n                confidences = tf.expand_dims(batch_item[..., index], axis=-1)\r\n                class_id = tf.fill(dims=tf.shape(confidences), value=tf.cast(index, dtype=tf.float32))\r\n                box_coordinates = batch_item[...,-4:]\r\n\r\n                single_class = tf.concat([class_id, confidences, box_coordinates], axis=-1)\r\n\r\n                # Apply confidence thresholding with respect to the class defined by `index`.\r\n                threshold_met = single_class[:,1] > self.tf_confidence_thresh\r\n                single_class = tf.boolean_mask(tensor=single_class,\r\n                                               mask=threshold_met)\r\n\r\n                # If any boxes made the threshold, perform NMS.\r\n                def perform_nms():\r\n                    scores = single_class[...,1]\r\n\r\n                    # `tf.image.non_max_suppression()` needs the box coordinates in the format `(ymin, xmin, ymax, xmax)`.\r\n                    xmin = tf.expand_dims(single_class[...,-4], axis=-1)\r\n                    ymin = tf.expand_dims(single_class[...,-3], axis=-1)\r\n                    xmax = tf.expand_dims(single_class[...,-2], axis=-1)\r\n                    ymax = tf.expand_dims(single_class[...,-1], axis=-1)\r\n                    boxes = tf.concat(values=[ymin, xmin, ymax, xmax], axis=-1)\r\n\r\n                    maxima_indices = tf.image.non_max_suppression(boxes=boxes,\r\n                                                                  scores=scores,\r\n                                                                  max_output_size=self.tf_nms_max_output_size,\r\n                                                                  iou_threshold=self.iou_threshold,\r\n                                                                  name='non_maximum_suppresion')\r\n                    maxima = tf.gather(params=single_class,\r\n                                       indices=maxima_indices,\r\n                                       axis=0)\r\n                    return maxima\r\n\r\n                def no_confident_predictions():\r\n                    return tf.constant(value=0.0, shape=(1,6))\r\n                    \r\n                single_class_nms = tf.cond(tf.equal(tf.size(single_class), 0), no_confident_predictions, perform_nms)\r\n\r\n                # Make sure `single_class` is exactly `self.nms_max_output_size` elements long.\r\n                padded_single_class = tf.pad(tensor=single_class_nms,\r\n                                             paddings=[[0, self.tf_nms_max_output_size - tf.shape(single_class_nms)[0]], [0, 0]],\r\n                                             mode='CONSTANT',\r\n                                             constant_values=0.0)\r\n\r\n                return padded_single_class\r\n\r\n            # Iterate `filter_single_class()` over all class indices.\r\n            filtered_single_classes = tf.map_fn(fn=lambda i: filter_single_class(i),\r\n                                                elems=tf.range(1,n_classes),\r\n                                                dtype=tf.float32,\r\n                                                parallel_iterations=128,\r\n                                                back_prop=False,\r\n                                                swap_memory=False,\r\n                                                infer_shape=True,\r\n                                                name='loop_over_classes',\r\n                                                fn_output_signature=tf.TensorSpec((batch_size, 6), dtype=tf.float32))\r\n\r\nThere seems to have a couple of issues:\r\n1) The `batch_size` here is `None` when I print it out.\r\n2) The `fn_output_signature` doesn't seem to match the output shape of function `filter_single_class`, since that function applies a padding at the end. Here you specify it as [batch_size, 6], which doesn't seem correct.\r\n\r\nWhen I changed `nms_max_output_size` to 1 in your code, it works (but it's just make the shape checking correct, I think we still need to examine the 2) above.", "HI @haozha111 good to here your attention, but...\r\n1) please pay attention that batch_size = None means that it can be various, this is legal value\r\n2) nms_max_output_size controls max number of detections that can be found on one input... So limiting it to 1 it's not a solution \r\n\r\nActually I'm not asking for hacking this bug, but for the real solution. I don't know what is the complexity of this issue, but I think that the problem should be solved as it actually points on some critical issue in TF library\r\n", "Sorry for the confusion... The root cause for this problem is that, in TF tensorlists (which is used inside tf.map_fn) can support dynamic element shapes and can have the shape materialized when invoking the `TensorListSetItem` kernel. But due to the restriction of TF Lite, we don't support TensorLists with dynamic element shape (although we do some compile-time analysis to acquire the shape as much as possible) so you will need to specifically instruct the TF code to pass a shape to `tf.map_fn`.\r\n\r\n1.please pay attention that batch_size = None means that it can be various, this is legal value > Yes, this is true in TF. But in TF Lite to make conversion work as expected, you will need to pass a concrete shape argument for `fn_output_signature`. Also it doesn't seem [batch_size, 6] is the correct output shape, since you applied a padding of 400 to the first dimension (and this is why `concatenation` is complaining about dimension mismatch).\r\n2. nms_max_output_size controls max number of detections that can be found on one input... So limiting it to 1 it's not a solution >  Yes I agree with you, sorry for the confusion in the original comment and I wasn't suggesting to change this to 1. The main idea here is to supply  `fn_output_signature` with a static shape so that we can walk-around the Tensorlist issues as I mentioned above.\r\n\r\nWe noticed the design restriction of TF Lite tensorlist and we are also improving it. I think very soon we can support running those tensorlist ops in flex mode and that will remove much of the pain here. ", "Hi @haozha111 , thank you for such an elaborated and professional answer - now I can understand what the root-cause.\r\nI'll check the batch size padding - looks weird but I have no problem set batch size to be 1\r\nRegarding nm_max_output_size, I ll take a look on this (fn_output_signature ) function\r\n\r\nThank you & best regards\r\n\r\nBTW what is ETA for those tensorelist ops ? (just to understand the timelines)", "Dear @haozha111, thank you very much for your professional help!! it is highly appreciated. ", "No problem, @abattery is currently adding support for flex tensorlist ops. Jaesung, could you give a rough ETA?", "@abattery  @haozha111  any news ? Was it added ?", "Sorry for the late notice. The feature has been landed at HEAD.", "@abattery  sorry, I'm not familiar with the \"slang\" what does it mean \"The feature has been landed at HEAD\" ? \r\nDo you mean it can be taken with the nightly ?", "Yes, the feature is available at the tf nightly and tf 2.5 release candidates.", "@abattery  thank you. Do I need to do something special to enable it ?", "Please install the tf-nightly version, convert just the TF model with the it and execute the converted model with it.", "@abattery  unfortunately I see the same error:\r\n\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py\", line 833, in invoke\r\n    self._interpreter.Invoke()\r\nRuntimeError: tensorflow/lite/kernels/concatenation.cc:74 t->dims->data[d] != t0->dims->data[d] (400 != 1)Node number 34 (CONCATENATION) failed to prepare.\r\nNode number 3 (WHILE) failed to invoke.\r\nNode number 186 (WHILE) failed to invoke.\r\n\r\nThe TF Version is 2.6.0-dev20210329\r\nWhat can be a reason for this problem to appear ?\r\n", "Did you follow the haoliang's suggestion at https://github.com/tensorflow/tensorflow/issues/46315#issuecomment-782914795 ?", "Hi,\r\n\r\nSince we have landed the support for flex tensorlist ops. Could you try the following:\r\n\r\n1. Enable TF-select ops in converter:\r\n converter.target_spec.supported_ops = [\r\n      tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\r\n  ]\r\n\r\n2. Remove the `fn_output_signature` parameter in `tf.map_fn`. This should allow the converted model to use the flex TensorList ops instead.", "@haozha111 thank you for detailed explanation. I will do\r\n", "Hi @haozha111  & @abattery  thank you for helping and solving the bug - currently that doesn't causes any error, but the results of running the original net and the converted one are really different (let's say truth, the converted net doesn't work and returns garbage) Can you please advice on the way I can understand what can be the root-cause ?\r\n\r\nThank you & best regards", "hi @haozha111 @abattery  can you please assist ?", "This might be caused by various reasons (converted graph is invalid, input is not set correctly). One way to debug this is to dump the internal node's results and see where the problem may happen. You can insert some print statement here:\r\nhttps://github.com/tensorflow/tensorflow/blob/43c1ea6eed787254c814f2b2eb00bebf8d5b19df/tensorflow/lite/core/subgraph.cc#L1095\r\n\r\nThis will allow you to see the results of each node's result.", "Or you can try the following option for your case.\r\n`converter._experimental_lower_tensor_list_ops = False`\r\n\r\nPlease upgrade your TF version to the recent tf-nightly.", "Thank you very much for proposals and answers. I will debug it :-)", "@MaxxTr,\r\n\r\nWe are checking to see if this is still an issue, Can you let us know if the issue still persists in latest stable version i.e `2.7.0`? Also you can try using `converter._experimental_lower_tensor_list_ops = False`.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46315\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46315\">No</a>\n"]}, {"number": 46314, "title": "tensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(\"batch_normalization/moving_mean\"): is not immutable, try running tf-saved-model-optimize-global-tensors to prove tensors are immutable ", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 pro\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (or github SHA if from source):2.4.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\npython convert_tflite.py --weights ./checkpoints/yolov4-416 --output ./checkpoints/yolov4-416.tflite\r\n```\r\n\r\n> convert_tflite.py is as below\r\n\r\n> import tensorflow as tf\r\n> from absl import app, flags, logging\r\n> from absl.flags import FLAGS\r\n> import numpy as np\r\n> import cv2\r\n> from core.yolov4 import YOLOv4, YOLOv3, YOLOv3_tiny, decode\r\n> import core.utils as utils\r\n> import os\r\n> from core.config import cfg\r\n> \r\n> flags.DEFINE_string('weights', './checkpoints/yolov4-416', 'path to weights file')\r\n> flags.DEFINE_string('output', './checkpoints/yolov4-416-fp32.tflite', 'path to output')\r\n> flags.DEFINE_integer('input_size', 416, 'path to output')\r\n> flags.DEFINE_string('quantize_mode', 'float32', 'quantize mode (int8, float16, float32)')\r\n> flags.DEFINE_string('dataset', \"/Volumes/Elements/data/coco_dataset/coco/5k.txt\", 'path to dataset')\r\n> \r\n> def representative_data_gen():\r\n>   fimage = open(FLAGS.dataset).read().split()\r\n>   for input_value in range(10):\r\n>     if os.path.exists(fimage[input_value]):\r\n>       original_image=cv2.imread(fimage[input_value])\r\n>       original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\r\n>       image_data = utils.image_preprocess(np.copy(original_image), [FLAGS.input_size, FLAGS.input_size])\r\n>       img_in = image_data[np.newaxis, ...].astype(np.float32)\r\n>       print(\"calibration image {}\".format(fimage[input_value]))\r\n>       yield [img_in]\r\n>     else:\r\n>       continue\r\n> \r\n> def save_tflite():\r\n>   converter = tf.lite.TFLiteConverter.from_saved_model(FLAGS.weights)\r\n> \r\n>   if FLAGS.quantize_mode == 'float16':\r\n>     converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n>     converter.target_spec.supported_types = [tf.compat.v1.lite.constants.FLOAT16]\r\n>     converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n>     converter.allow_custom_ops = True\r\n>   elif FLAGS.quantize_mode == 'int8':\r\n>     converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n>     converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n>     converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n>     converter.allow_custom_ops = True\r\n>     converter.representative_dataset = representative_data_gen\r\n> \r\n>   tflite_model = converter.convert()\r\n>   open(FLAGS.output, 'wb').write(tflite_model)\r\n> \r\n>   logging.info(\"model saved to: {}\".format(FLAGS.output))\r\n> \r\n> def demo():\r\n>   interpreter = tf.lite.Interpreter(model_path=FLAGS.output)\r\n>   interpreter.allocate_tensors()\r\n>   logging.info('tflite model loaded')\r\n> \r\n>   input_details = interpreter.get_input_details()\r\n>   print(input_details)\r\n>   output_details = interpreter.get_output_details()\r\n>   print(output_details)\r\n> \r\n>   input_shape = input_details[0]['shape']\r\n> \r\n>   input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\n> \r\n>   interpreter.set_tensor(input_details[0]['index'], input_data)\r\n>   interpreter.invoke()\r\n>   output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\r\n> \r\n>   print(output_data)\r\n> \r\n> def main(_argv):\r\n>   save_tflite()\r\n>   demo()\r\n> \r\n> if __name__ == '__main__':\r\n>     try:\r\n>         app.run(main)\r\n>     except SystemExit:\r\n>         pass\r\n>\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nloc(\"batch_normalization/moving_mean\"): error: is not immutable, try running tf-saved-model-optimize-global-tensors to prove tensors are immutable\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\theum\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 213, in toco_convert_protos\r\n    enable_mlir_converter)\r\n  File \"C:\\Users\\theum\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\lite\\python\\wrap_toco.py\", line 38, in wrapped_toco_convert\r\n    enable_mlir_converter)\r\nException: <unknown>:0: error: loc(\"batch_normalization/moving_mean\"): is not immutable, try running tf-saved-model-optimize-global-tensors to prove tensors are immutable\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n> Traceback (most recent call last):\r\n>   File \"convert_tflite.py\", line 76, in <module>\r\n>     app.run(main)\r\n>   File \"C:\\Users\\theum\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\absl\\app.py\", line 300, in run\r\n>     _run_main(main, args)\r\n>   File \"C:\\Users\\theum\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\absl\\app.py\", line 251, in _run_main\r\n>     sys.exit(main(argv))\r\n>   File \"convert_tflite.py\", line 71, in main\r\n>     save_tflite()\r\n>   File \"convert_tflite.py\", line 45, in save_tflite\r\n>     tflite_model = converter.convert()\r\n>   File \"C:\\Users\\theum\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 739, in convert\r\n>     result = _convert_saved_model(**converter_kwargs)\r\n>   File \"C:\\Users\\theum\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 637, in convert_saved_model\r\n>     enable_mlir_converter=True)\r\n>   File \"C:\\Users\\theum\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 216, in toco_convert_protos\r\n>     raise ConverterError(str(e))\r\n> tensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(\"batch_normalization/moving_mean\"): is not immutable, try running tf-saved-model-optimize-global-tensors to prove tensors are immutable\r\n\r\n```\r\n\r\n\r\n[Link to saved model](https://drive.google.com/drive/folders/1wsEs6pHfbNi-CjGRTSeQ8poSreAEo3Bh?usp=sharing)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["Sorry for encountering this issue. The mutable variable cases are currently not supported by TFLite. We are working on this. I will update this thread once it is ready to use.", "> Sorry for encountering this issue. The mutable variable cases are currently not supported by TFLite. We are working on this. I will update this thread once it is ready to use.\r\n\r\nI attempt to use tf-nighty(`2.5.0.dev2021012301`) to convert model to TFLite \uff0cbut it still throw this error:`tensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(\"batch_normalization/moving_mean\")`\r\n\r\nCould you provide some alternative ways or suggestions for convert Saved Model (converted from YOLOv4) to TFLite ?", "According to https://github.com/tensorflow/tensorflow/issues/44790, you might need to use TF 2.3.0.", "FYI, mutable variable is not supported with TF 2.3 as well. We do not guarantee that the converted model including mutable variables with TF 2.3 version will work correctly.", "However, there is possibility that if mutable variables can be pruned during conversion, the model can work.", "> However, there is possibility that if mutable variables can be pruned during conversion, the model can work.\r\n\r\nMay You explain better how to make work the model. I'm trying to convert a Yolov4 model to tflite since 6 months without any success", "> > However, there is possibility that if mutable variables can be pruned during conversion, the model can work.\r\n> \r\n> May You explain better how to make work the model. I'm trying to convert a Yolov4 model to tflite since 6 months without any success\r\n\r\ntry tensorflow version 2.3.0", "@karimnosseir could you take a look?", "Hi, I resolved this problem in this form:\r\n`pip install tensorflow===2.3.0\r\npython save_model.py --weights yolov4_best.weights --output ./checkpoints/yolov4_v1_darknet-416 --input_size 416 --model yolov4 --framework tflite\r\npython convert_tflite.py --weights ./checkpoints/yolov4_v1_darknet-416 --quantize_mode fp16 --output ./checkpoints/yolov4_v1_darknet-416_fp16.tflite`\r\n\r\nI hope this solve your problem\r\n", "Hi Everyone,\r\n\r\nMutable variable support is now available in the nightly when converting using from_saved_model.\r\nYou need to set this flag to True for now.\r\n\r\n```\r\nconverter.experimental_enable_resource_variables = True\r\n```\r\n\r\n", "For me, the tf-nightly build does not fix this issue. I don't think this should be closed. Anyone else? ", "@rossGardiner please upload a new post with reproducible steps.", "> For me, the tf-nightly build does not fix this issue. I don't think this should be closed. Anyone else?\r\n\r\ntry tf 2.3.0"]}, {"number": 46313, "title": "Add tf.qint32 support for tf.zeros", "body": "This PR is part of #26069 where `tf.zeros` does not support\r\nbasic type of `tf.qint32` while all other qtypes have been supported\r\n(tf.{qint8|qint16|quint8|quint16} supported).\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 46312, "title": "micro: prepare to port operator FLOOR_MOD kernel from lite with test", "body": "Implement skeleton (non-working) code for operator and test.\r\nHeader files changed.\r\nNamespaces changed.\r\nSome original code deleted.\r\nSome original code modified.\r\n\r\nThis represents PR step 4 of the work to port operator FLOOR_MOD as tracked in Issue #45749", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 46311, "title": "micro: copy operator FLOOR_MOD kernel from lite", "body": "This is a copy with minimal modification of the kernel and test for\r\noperator FLOOR_MOD from tensorflow/lite/kernels.\r\nAdaptations to micro and addition to the micro build to follow.\r\n\r\nPR step 3 for issue #45749", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 46310, "title": "Dynamic library  libcudnn.so.8 and libcusolver.so.10 dont load", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 20.04):\r\n- TensorFlow installed from: pip in anaconda env\r\n- TensorFlow version: 2.4.0\r\n- Python version: 3.8.5\r\n- CUDA/cuDNN version:  CUDA:11.0 and cuDNN 8.05 \r\n- GPU model and memory: Dual 1080 ti 12 gb\r\n\r\n\r\n\r\n**Describe the problem**\r\nFresh build with newly installed Ubuntu. Had to downgrade the existing nvidia driver  and cuda   from 4.60 > 4.50 and cuda from 11.2 to 11.1. since cuDNN didnt list CUDA 11.2  as compatible and i ran into more issues. Running a simple model (make blobs classification) returns these errors when compiling the model :+1: \r\n\r\nlibcudnn.so.8 and libcusolver.so.10 dont load (code below run in jupyter)\r\n\r\n```\r\nKernel started: 074c56de-e52a-48bd-b7bb-20ab27829401, name: python3\r\n2021-01-09 12:25:03.574107: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-01-09 12:25:08.195326: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-09 12:25:08.195728: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-01-09 12:25:08.220794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-01-09 12:25:08.221463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:23:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.6325GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-01-09 12:25:08.221502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-01-09 12:25:08.222043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \r\npciBusID: 0000:2d:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.6325GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-01-09 12:25:08.222053: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-01-09 12:25:08.222983: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-01-09 12:25:08.223005: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-01-09 12:25:08.223355: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-01-09 12:25:08.223461: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-01-09 12:25:08.223514: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] _Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory_\r\n2021-01-09 12:25:08.223746: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-01-09 12:25:08.223775: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] _Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory_\r\n2021-01-09 12:25:08.223780: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2021-01-09 12:25:08.224169: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-09 12:25:08.224183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-01-09 12:25:08.224188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]    \r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI changed the existing drier from 4.60 to 4.50 by the software page. prior to this  cuda 11.2 was installed but was not in /usr/local/\r\nI installed cuda 11.0 from .deb after purging and autoremoving cuda. \r\n\r\nI've tried adding the the path following the nvidia documenation replacing the 11.2 with 11.1\r\n```export PATH=/usr/local/cuda-11.2/bin${PATH:+:${PATH}}```\r\nbut it hasnt helped", "comments": ["@Corneliuscob,\r\nCould you please go through the comments from issues [#43947](https://github.com/tensorflow/tensorflow/issues/43947#issuecomment-715295153), [#45848](https://github.com/tensorflow/tensorflow/issues/45848#issuecomment-748130779) with a similar error log and check if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I managed to get it working with the solution from #43947. Thank you! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46310\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46310\">No</a>\n", "for anyone reading this, the error was because the person did not install the cuda and cudaNN libraries on their linux box(server)\r\nplease keep in mind that the cuda libraries and cudaNN libraries are 2 totally different things\r\n\r\njust go to the NVIDIA url below and download the libraries needed (should be a totally of 3)\r\n\r\ncuDNN Library for Linux (x86_64)\r\n\r\ncuDNN Runtime Library for Ubuntu20.04 x86_64 (Deb)\r\n\r\ncuDNN Developer Library for Ubuntu20.04 x86_64 (Deb)\r\n\r\n\r\n\r\nhttps://developer.nvidia.com/rdp/cudnn-download\r\n\r\nthat should solve the libcudnn.so.8 issue\r\n\r\nsearch for my other answer on the github forums to see how to get the latest \"cuda\" download\r\n\r\n"]}, {"number": 46309, "title": "Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED", "body": "\r\n**System information**\r\n- Windows 10 Pro 2004\r\n- TensorFlow  installed from (pip):\r\n- TensorFlow version (2.4.0):\r\n- Python version 3.8.1:\r\n- Installed in anaconda venv\r\n- CUDA version 11.0, cuDNN version 8.0.4\r\n- GPU model gtx 1660ti, 6Gb vram:\r\n\r\n\r\n**The code**\r\n`import os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\r\nimport tensorflow_addons as tfa\r\nimport math\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow import keras\r\n\r\nprint(tf.__version__)\r\nif tf.test.gpu_device_name():\r\n    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\r\nelse:\r\n    print(\"Please install GPU version of TF\")\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\nprint(physical_devices)\r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n\r\n(ds_train, ds_test), ds_info = tfds.load('mnist',\r\n                                         split=['train', 'test'],\r\n                                         shuffle_files=False,\r\n                                         as_supervised=True,\r\n                                         with_info=True,)\r\n\r\n@tf.function\r\ndef normalize_img(image, label):\r\n    return tf.cast(image, tf.float32) / 255.0, label\r\n\r\n@tf.function\r\ndef rotate(img, max_degrees=25):\r\n    degrees = tf.random.uniform([], -max_degrees, max_degrees, dtype=tf.float32)\r\n    img = tfa.image.rotate(img, degrees*math.pi / 180, interpolation='BILINEAR')\r\n    return img\r\n\r\n#\r\n@tf.function\r\ndef augment(image, label):\r\n    image = tf.image.resize(image, size=[28, 28])\r\n    image = rotate(image)\r\n\r\n    # coloring of image\r\n    image = tf.image.random_brightness(image, max_delta=0.2)\r\n    image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\r\n    return image, label\r\n\r\nAUTOTUNE = tf.data.experimental.AUTOTUNE\r\nBATCH_SIZE = 32\r\n\r\nds_train = ds_train.cache()\r\nds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\r\nds_train = ds_train.map(normalize_img, num_parallel_calls=AUTOTUNE)\r\nds_train = ds_train.map(augment, num_parallel_calls=AUTOTUNE)\r\nds_train = ds_train.batch(BATCH_SIZE)\r\nds_train = ds_train.prefetch(AUTOTUNE)\r\n\r\nds_test = ds_test.map(normalize_img, num_parallel_calls=AUTOTUNE)\r\nds_test = ds_test.batch(BATCH_SIZE)\r\nds_test = ds_test.prefetch(AUTOTUNE)\r\n\r\ndef my_model():\r\n    inputs = keras.Input(shape=(28, 28, 1))\r\n    x = layers.Conv2D(32, 3)(inputs)\r\n    x = layers.BatchNormalization()(x)\r\n    x = keras.activations.relu(x)\r\n    x = layers.MaxPooling2D()(x)\r\n    x = layers.Conv2D(64, 3)(x)\r\n    x = layers.BatchNormalization()(x)\r\n    x = keras.activations.relu(x)\r\n    x = layers.MaxPooling2D()(x)\r\n    x = layers.Conv2D(128, 3)(x)\r\n    x = layers.BatchNormalization()(x)\r\n    x = keras.activations.relu(x)\r\n    x = layers.Flatten()(x)\r\n    x = layers.Dense(64, activation='relu')(x)\r\n    outputs = layers.Dense(10, activation='softmax')(x)\r\n    return keras.Model(inputs=inputs, outputs=outputs)\r\n\r\nmodel = my_model()\r\n# compile model\r\nmodel.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\r\n              optimizer=keras.optimizers.Adam(lr=1e-4),\r\n              metrics=['accuracy'])\r\n# model.fit\r\nmodel.fit(ds_train, epochs=30, verbose=2)\r\n# model.evaluate\r\nmodel.evaluate(ds_test)\r\nmodel.save('model')\r\n\r\n`\r\n**The error it produced**\r\n> 2.4.0\r\nDefault GPU Device:/device:GPU:0\r\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\nEpoch 1/30\r\n2021-01-09 11:59:09.630855: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2021-01-09 11:59:09.631034: E tensorflow/stream_executor/cuda/cuda_dnn.cc:340] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\r\n2021-01-09 11:59:09.632741: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2021-01-09 11:59:09.632910: E tensorflow/stream_executor/cuda/cuda_dnn.cc:340] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\r\nTraceback (most recent call last):\r\n  File \"D:/Projects/pythonProject/main.py\", line 85, in <module>\r\n    model.fit(ds_train, epochs=30, verbose=2)\r\n  File \"C:\\Users\\.\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1100, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n  File \"C:\\Users\\.\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\Users\\.\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 888, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"C:\\Users\\.\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2942, in __call__\r\n    return graph_function._call_flat(\r\n  File \"C:\\Users\\.\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1918, in _call_flat\r\n    return self._build_call_outputs(self._inference_function.call(\r\n  File \"C:\\Users\\.\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\", line 555, in call\r\n    outputs = execute.execute(\r\n  File \"C:\\Users\\.\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node model/conv2d/Conv2D (defined at D:/Projects/pythonProject/main.py:85) ]] [Op:__inference_train_function_1489]\r\nFunction call stack:\r\ntrain_function\r\n\r\n\r\nVerified versions of drivers, cuda toolkit and cudnn several times, and reinstalled them also several times.\r\nAny advice or suggestion will be appriciated.\r\nThx\r\n", "comments": ["@Heizenber \r\nPlease refer to this resolved issue and let us know:#45779, #42298, #34214", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Solved the issue using this link https://yann-leguilly.gitlab.io/post/2019-10-08-tensorflow-and-cuda/, changed only the versions, thank you!!!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46309\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46309\">No</a>\n"]}, {"number": 46308, "title": "Error at importing! ", "body": "### My 'tensorflow-GPU' was working fine yesterday but when I import it now it says-\r\n `Traceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Sahil Singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\Sahil Singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"C:\\Users\\Sahil Singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 32, in <module>\r\n    from tensorflow.core.framework import function_pb2\r\n  File \"C:\\Users\\Sahil Singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\core\\framework\\function_pb2.py\", line 7, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"C:\\Users\\Sahil Singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\google\\protobuf\\__init__.py\", line 37, in <module>\r\n    __import__('pkg_resources').declare_namespace(__name__)\r\n  File \"C:\\Users\\Sahil Singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pkg_resources.py\", line 1479, in <module>\r\n    register_loader_type(importlib_bootstrap.SourceFileLoader, DefaultProvider)'`\r\n`AttributeError: module 'importlib._bootstrap' has no attribute 'SourceFileLoader`", "comments": ["sorry my bad just installed `pip install tensorflow-gpu==2.3.0` it worked.\r\nThanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46308\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46308\">No</a>\n"]}, {"number": 46307, "title": "I can't find the tensorflow-lite model at the specified location(new->other->tensorflow-lite model) What should i do?", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@DhilipKumarDK,\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. Thanks!", "I was using android studio 4.0.1 ver which didnt have the tfmodel and Thank\nyou for your concern\n\nOn Mon, 11 Jan 2021, 2:14 pm Abhilash Mahendrakar, <notifications@github.com>\nwrote:\n\n> @DhilipKumarDK <https://github.com/DhilipKumarDK>,\n> In order to reproduce the issue reported here, could you please provide\n> the TensorFlow version, the complete code and the dataset you are using.\n> Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/46307#issuecomment-757732240>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ASB5TIB6KTGTCFZA3QHUI2LSZK25XANCNFSM4V3L3TLQ>\n> .\n>\n", "TFLite ML Model Binding plugin for Android Studio is only available from Android Studio 4.1.\r\n\r\nPlease see [here](https://www.tensorflow.org/lite/guide/android#use_android_studio_ml_model_binding) for more details.\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46307\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46307\">No</a>\n"]}, {"number": 46306, "title": "ModuleNotFoundError: No module named 'tensorflow.models'", "body": "hi,dear all friends\r\nwhen I run the [cifar rp](https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/models/image/cifar10/cifar10_train.py),\r\nI find the problem down\r\ncould you please help me ?\r\n", "comments": ["some env\r\ntf-1.15\r\ncuda 10.1\r\nnvidia 418.87\r\nOS ubuntu 16.08", "try reinstalling tensorflow.\r\ntry : pip install tensorflow --ignore-installed --user\r\n", "@ucasiggcas \r\nIs there any particular reason for using old version of tf, we have 2.x later versions can you please upgrade and let us know.", "some function not in tf2.x?\r\nwhat should I do ?\r\n", "such as tf.placeholder,tf.Session\r\n", "```\r\n>>> import tensorflow as tf\r\n2021-01-11 15:22:58.507388: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n>>> tf.models\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute 'models'\r\n>>> tf.__version__\r\n'2.3.0'\r\n\r\n```", "@ucasiggcas \r\nCan you please try the below code instead and let us know, please refer to [this gist](https://colab.research.google.com/gist/Saduf2019/c45816d2a93c909fa7efe71fd486ea4f/untitled493.ipynb).\r\n```import tensorflow as tf```\r\n```tf.keras.models.Model()```", "hi,I have the same situation as your codes\r\n```\r\n>>> from tensorflow.models.image.cifar10 import cifar10\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensorflow.models'\r\n```\r\n```\r\n>>> tf.keras.models.Model()\r\n<tensorflow.python.keras.engine.training.Model object at 0x7fb383052b90>\r\n```\r\n", "`tensorflow.models` is not available for version 1.15 (actually it's no longer available from Tensorflow 1).\r\nThe link that you shared ([cifar](https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/models/image/cifar10/cifar10_train.py)) seems to be in version r0.7 (according to the branch name).\r\n\r\nThat's not the case of `tensorflow.keras.models`; its's still available for the latest version of Tensorflow.\r\nIf you want to use  `tensorflow.models`, downgrade Tensorflow to version r0.7.\r\nAlso note that you'll probably have to install it from source, I don't think it's still available in pip or conda.\r\n\r\nEdit: Let me know.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "thx\r\nI know the answer\r\nwill try later\r\n"]}, {"number": 46305, "title": "Merge pull request #1 from tensorflow/master", "body": "Merge from master", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46305) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 46304, "title": "Speedup DepthToSpace", "body": "This PR speedups DepthToSpace with Eigen Tensor Generator. Below is my benchmark\r\n\r\nOn ***Intel(R) Core(TM) i5-9600K CPU @ 3.70GHz***\r\n\r\n| input_shape (`N_H_W_C`) | block_size | Old (M items/s) | New (M items/s) |\r\n| ---------------------------- | ----------- | ---- | ----- |\r\n| 1_64_64_64                    |  2             |  1123 | 1523 |\r\n| 16_64_64_64                 | 2              | 1299 | 2137 |\r\n| 32_64_64_64                   |  2             |  1123 | 1912|\r\n| 1_128_128_64                  | 2              | 1306| 2644 |\r\n| 16_128_128_64                     |  2             |   627| 1796 |\r\n| 32_64_64_64                  | 2              | 618| 1827 |\r\n| 1_64_64_64                     |  4             |  694| 1283|\r\n| 16_64_64_64                  | 4             | 767 | 1953 |\r\n| 32_64_64_64                     |  4             |  483 | 1515 |\r\n| 1_128_128_64                  | 4              | 806| 1892 |\r\n| 16_128_128_64                     |  4             |   478| 1525 |\r\n| 32_64_64_64                  | 4              | 469| 1514 |", "comments": ["@WindQAQ  Can you please check @cantonios's comments and keep us posted ? Thanks!", "@WindQAQ Any update on this PR? Please. Thanks!", "@WindQAQ Any update on this PR? Please. Thanks!", "It has been 19 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 46302, "title": "Include C env API into part of the libtensorflow_framework.so", "body": "`env` only uses some transitive dependencies of `c_api` ( It doesn't even call any function in `c_api.h` ) and `//tensorflow/core:lib`. So I replace these `deps` by its actual `deps`.\r\n\r\nIt is required by `gcs_filesystem` plugin.\r\n\r\nPart of https://github.com/tensorflow/io/issues/1183\r\n\r\n/cc: @yongtang, @mihaimaruseac ", "comments": []}, {"number": 46300, "title": "Tf32 fixes", "body": "Currently, enable_tensor_float_32_execution(False) does not fully disable TensorFloat32 evaluation in RNNs. This PR fixes this and also disabled TF32 execution for a few additional tests. \r\n\r\nAttn: @reedwm ", "comments": ["Added release note. @reedwm, PTAL.", "@nluehr can you please resolve sanity build failures ?", "@rthadur, Should be good to go now.", "@nluehr  Can you please resolve conflicts? Thanks!", "Seems auto-merge is not happening but the changes are merged into master now, so we can close this. Thank you for the PR."]}, {"number": 46299, "title": "TF-TRT Test ConvertTopK in dynamic shape mode", "body": "This PR adds explicit batch and dynamic shape unit tests for TopK op converter.\r\n\r\nAdditionally the `RunValidationAndConversion` test routine is extended to handle multiple output tensors. \r\n\r\nTagging @bixia1 for review and @DEKHTIARJonathan for visibility.\r\n\r\nTracker: #45481", "comments": ["I have updated the PR to resolve conflicts."]}, {"number": 46298, "title": "Allow more of the internal checks to have open-source counterparts.", "body": " * gtest includes will be flagged as errors\r\n * use of the error reporter without the wrapper macros will be an error (https://github.com/tensorflow/tensorflow/pull/45457#discussion_r547434839)\r\n * assert can not be used (static_assert is ok).\r\n\r\nManually tested by adding the disallowed strings to the code and confirmed that an error is raised.\r\n\r\nFixes #46297\r\nFixes http://b/175657165\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "> Looks good. I've tested it, and it does indeed flag the errors with `ReportError()` and `#include <{gmock,gtest}.h>` that only internal CI flagged in [88f5e7a](https://github.com/tensorflow/tensorflow/commit/88f5e7a3e0f075fbf45ba2e94a4251beb8af9efb) during PR #45457.\r\n> \r\n> It's slightly unfortunate that these checks will trigger on the words _assert_, _gtest_, and _gmock_ in comments. Perhaps in a future enhancement, `check_contents` or these patterns could ignore comments.\r\n\r\nYes, the check is certainly more broad than it needs to be. It does mirror what happens internally (which is also a regex search and flagging). In an ideal world these would be build/test errors instead of what we currently have.", "> > It's slightly unfortunate that these checks will trigger on the words _assert_, _gtest_, and _gmock_ in comments. Perhaps in a future enhancement, `check_contents` or these patterns could ignore comments.\r\n> \r\n> Yes, the check is certainly more broad than it needs to be. It does mirror what happens internally (which is also a regex search and flagging). In an ideal world these would be build/test errors instead of what we currently have.\r\n\r\nAah, good point. It's certainly desirable for these public checks to mirror the internal checks as closely as possible, including any over-eager matching within comments. Any enhancement would have to be done internally first."]}]