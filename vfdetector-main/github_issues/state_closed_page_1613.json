[{"number": 4521, "title": "Floating Point Exception with Conv3d ", "body": "I am getting a floating point exception with the following example:\n\n``` python\nimport numpy as np\nimport tensorflow as tf\n\ninputShape = (16, 3, 64, 256, 6)\nweightShape = (2, 16, 32, 6, 1028)\n\nnp_in = np.random.rand(*inputShape).astype(np.float32)\nnp_w = np.random.rand(*weightShape).astype(np.float32)\n\ntf_in = tf.Variable(np_in)\ntf_w = tf.Variable(np_w)\n\ntf_out = tf.nn.conv3d(tf_in, tf_w, [1, 1, 1, 1, 1], padding=\"SAME\")\n\ninit = tf.initialize_all_variables()\nsess = tf.Session()\nsess.run(init)\n\nnp_out = sess.run(tf_out)\n```\n\nI'm currently using Cuda 8.0.27 and CUDNN 5.1.5, running on Ubuntu 16.04.1 LTS\nTensorFlow built from source, commit a0d929df36ca7d6f72184371f6c3d6d877dded3e\nBazel version 0.3.1\n\nThe code does not error out if you change the kernel height (currently 16) to other numbers (I've tried 14, 15, 17, and 18).\n\nThanks in advanced.\n", "comments": ["what was the error message?\n", "I get the following error message:\nFloating point exception (core dumped)\n", "It seems to work on CPU only for me, does it work on CPU for you?\n", "This does seem to be a GPU specific bug, as the code works fine on the CPU.\n", "Passing this to Michal. \n", "Hi all,\r\nI have a very similar problem. This MWE fails with a floating point exception only when `w_size = 16`:\r\n```python\r\nimport tensorflow as tf\r\n\r\nw_size = 16\r\n\r\nat = tf.random_normal((1, 128, 128, 5, 1))\r\nbt = tf.random_normal((w_size, w_size, 5, 1, 1))\r\n\r\nwith tf.device('/gpu:0'):\r\n    ct = tf.nn.convolution(at, bt, \"SAME\", strides=[w_size, w_size, 5])\r\n    \r\nwith tf.Session() as sess:\r\n    sess.run(ct)\r\n```\r\nTested with TensorFlow 0.12.0, CUDA 8.0.44, cuDNN 5.1 on the following systems:\r\n* Ubuntu 16.04.1 LTS, Tesla K40c, driver version 375.20\r\n* Ubuntu 14.04.5 LTS, GeForce GTX 980, driver version 367.57", "I can independently confirm this bug. Tried this with Tensorflow 1.0, [CUDA 8.0.44, 8.0.61], cuDNN 5.1, Ubuntu 16.04 LTS, [Tesla K80, GTX 1080]. This even crashes at w_size = 8 in some cases. Are there any updates?", "I'm having the same problem here with tf.nn.conv3d_transpose on Ubuntu 16.04, TF1.0, Cuda 8.0.61, cuDNN 5.1 with a Titan X (Maxwell)\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nstride = 8\r\nw_size = stride * 2 \r\n\r\nat = tf.random_normal((1, 2, 1, 1, 1))\r\nbt = tf.random_normal((w_size, 1, 1, 1, 1))\r\n\r\nout_shape = [1, 2*stride, 1, 1, 1]\r\n            \r\n\r\nwith tf.device('/gpu:0'):\r\n    deconv = tf.nn.conv3d_transpose(at, bt, output_shape=out_shape,  strides=[1, stride, 1, 1, 1], padding='SAME')\r\n    \r\nwith tf.Session() as sess:\r\n    print(sess.run(deconv).shape)\r\n```\r\n\r\nIt works with stride < 8 and stride > 8 (tested 9, 10).", "@mrajchl @slundqui Could you test it on tensorflow 1.1 and see if this bug still exists?\r\nAlso, @mjanusz , any update on this?", "Seems to be working fine with TensorFlow 1.2.0!", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Verified with TensorFlow 1.4.1 and it has been fixed."]}, {"number": 4520, "title": "Develop device framework", "body": "Beginning of solution to [Issue #4359](https://github.com/tensorflow/tensorflow/issues/4359) \n", "comments": ["Can one of the admins verify this patch?\n", "I'm going to close this for now since we're making progress on the https://github.com/tensorflow/tensorflow/issues/4359.  Once we have everything in the framework to add a device, we can add the docs.  Thanks!\n"]}, {"number": 4519, "title": "raw_rnn", "body": "For @ebrevdo on his new [raw_rnn implementation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L1018).\n\nIn your [example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L1064)\n\nWhen you define `next_input` as the following:\n\n``` python\nelements_finished = (time >= sequence_length)\n...\nnext_input = tf.cond(\n    finished,\n    lambda: tf.zeros([batch_size, input_depth], dtype=tf.float32),\n    lambda: inputs_ta.read(time))\n```\n\nWhy make it return an zero input? Will this not cause the last state to be computed incorrectly (with zeros as input)?\n\nAlternatively the zero input can be removed, by rewriting it the following way\n\n``` python\nelements_finished = (time >= (sequence_length-1))\n...\nnext_input = inputs_ta.read(time)\n```\n\nThanks\n", "comments": ["Or wait ... the zero state won't actually be used because `(next_output, cell_state) = cell(current_input, state)` happens before ...\n", "Yeah it's an unfortunate consequence of every calculation needing some\nvalue to be provided. Nevertheless there's no slowdown from this extra cond.\n\nOn Sep 21, 2016 3:22 PM, \"Alexander Rosenberg Johansen\" <\nnotifications@github.com> wrote:\n\n> Closed #4519 https://github.com/tensorflow/tensorflow/issues/4519.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4519#event-798099374,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim3_yNT9OUfw5FUyuc246jbcD9ZOUks5qsa4TgaJpZM4KDL62\n> .\n"]}, {"number": 4518, "title": "GPU kernel for MatrixTriangularSolve ", "body": "A feature request. This should be possible without too much disruption because it relies on a CUBLAS call and therefore would not require changes to stream_executor, see section 3.4.6 of the linked document. \n\nhttps://developer.nvidia.com/sites/default/files/akamai/cuda/files/Misc/mygpu.pdf\n\nThe requisite blas function is called _cublasStrsm_ and seems to be already in stream executor in this file\n\nhttps://github.com/tensorflow/tensorflow/blob/6d04d601e9e8758ec4642fa9d548b7321d804d63/tensorflow/stream_executor/cuda/cuda_blas.cc\n\nI know that @rmlarsen has been interested in this sort of area in the past. \n\nI don't think it is possible to do this as a user op added at run time because stream executor is not part of the tf includes in the binary, but I'd be happy to be wrong. \n", "comments": ["I forwarded this feature request to the team.  In the mean time, marking this as contributions welcome in case someone wants to take a stab.\n", "I've got this in my [fork](https://github.com/c0g/tensorflow/blob/master/tensorflow/core/kernels/matrix_triangular_solve_op.cc), happy for @alexggmatthews / someone to PR it or I'll try get to it later in the month.\n", "This is a PR in #5010 . Hopefully it'll make sense!\n", "It looks like the PR was merged. Thanks! Closing.\n"]}, {"number": 4517, "title": "Documentation: add warning to GPU setup instructions that Intel not supported", "body": "This is obvious in retrospect and isn't TF specific, but it might save some people some time to say something like \"Tensorflow only supports CUDA, which is proprietary to NVIDIA, and not all Macs have an NVIDIA GPU\".\n", "comments": ["We expect people to know what cuda/openCL is (no need to be familiar) if they want to use GPU for computing. Other wise there will be a lot to explain in the docs.\n"]}, {"number": 4516, "title": "Interface improvement suggestions for tf.tranpose and tf.slice", "body": "I would like to suggest the following improvements for the interfaces of `tf.transpose` and `tf.slice`.\n1. `tf.transpose` currently accepts a boolean mask that selects the dimensions to be reversed. AFAIK this is the only op in Tensorflow that uses a mask for this purpose, whereas other, e.g. `tf.reduce_***`, take an axis number or a list of axis numbers. I think `tf.tranpose` could also be changed accordingly.\n2. `tf.slice` signature is `tf.slice(input_, begin, size, name=None)`, and it is very unusual for Python that the slices are defined by their size, not by their end. Also, there is a `slice` object in Python, which would be a very natural argument for `tf.slice`. \n", "comments": ["BTW, there's tf.strided_slice\nhttps://www.tensorflow.org/versions/r0.10/api_docs/python/array_ops.html#strided_slice\nwhich is closer to the API you want.\nThe issue with changing tf.slice is that a lot of models will get broken by\nthis change\n\nPS, here's the function that is used by TensorFlow to turn Python slice\nobjects into slice specifications\nhttps://github.com/tensorflow/tensorflow/blob/640353d5d1db50d6601f9410b9d06462a8a71ce4/tensorflow/python/ops/array_ops.py#L262\n\nOn Wed, Sep 21, 2016 at 11:40 AM, Dzmitry Bahdanau <notifications@github.com\n\n> wrote:\n> \n> I would like to suggest the following improvements for the interfaces of\n> tf.transpose and tf.slice.\n> \n>    1.\n> \n>    tf.transpose currently accepts a boolean mask that selects the\n>    dimensions to be reversed. AFAIK this is the only op in Tensorflow that\n>    uses a mask for this purpose, whereas other, e.g. tf.reduce_***, take\n>    an axis number or a list of axis numbers. I think tf.tranpose could\n>    also be changed accordingly.\n>    2.\n> \n>    tf.slice signature is tf.slice(input_, begin, size, name=None), and it\n>    is very unusual for Python that the slices are defined by their size, not\n>    by their end. Also, there is a slice object in Python, which would be\n>    a very natural argument for tf.slice.\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4516, or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHEWcg9bQCVBWfBwSqsfAnCYqoGt-ks5qsXoNgaJpZM4KDI29\n> .\n", "> The issue with changing tf.slice is that a lot of models will get broken by\n> this change\n\nShould it be deprecated then, if it's only purpose is backward compatibility?\n", "@aselle Is deprecating `tf.slice()` on your list?\n", "I have no immediate plans to deprecate tf.slice(). I think we should update the documentation, but I'm not sure we should deprecate it even in 1.0. \n", "But what's the point of having both tf.slice and tf.strided_slice? Is there\nanything that the latter can not do and the former can?\n\nOn Wed, 28 Sep 2016 at 13:37 Andrew Selle notifications@github.com wrote:\n\n> I have no immediate plans to deprecate tf.slice(). I think we should\n> update the documentation, but I'm not sure we should deprecate it even in\n> 1.0.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4516#issuecomment-250239807,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAn8Ys6CVqjVTg9v2TauWWpFvGIcIxFVks5quqXhgaJpZM4KDI29\n> .\n", "@rizar removing tf.slice would break existing models which use it\n", "I understand that. But I thought that this is exactly what deprecation is\nused for. By deprecating tf.slice officially you could encourage people to\nnot use it in new projects and replace it with tf.strided_slice in the old\nones. This way it could be eventually removed.\n\nOn Mon, 3 Oct 2016 at 11:38 Yaroslav Bulatov notifications@github.com\nwrote:\n\n> @rizar https://github.com/rizar removing tf.slice would break existing\n> models which use it\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> \n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4516#issuecomment-251140229,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAn8Yi7eDmNQbN-UlVgfp9qNWwQ_JmCHks5qwSFygaJpZM4KDI29\n> .\n", "I of course would like to get rid of redundancy, however there is always a tradeoff. While it is true that deprecation helps make changes more graceful, it does not make them zero cost for users. It just defers that cost to a potentially more convenient time in the future instead of forcing it immediately. We are thinking about deprecating it, but I don't know when that will happen.\n", "In any case, back to the original issue. \n1. tf.slice: I will add a pointer to strided_slice in the slice documentation for now as it works like you are suggesting.\n2. tf.transpose: I don't know what you mean by mask. You specify which axis indices should be swapped i.e. for a rank-2 matrix  A, tf.transpose(A, [1,0]) is normal transposing and tf.transpose(A,[0,1]) is identity. This is consistent with how NumPy works.\n\nAre you ok with me closing the issue?\n", "1. Adding a pointer would alleviate the issue, thanks.\n2. This is totally my bad: I confused `tf.transpose` and `tf.reverse`. It's the interface of the latter that I think could be improved. See the documentation below:\n\n```\nReverses specific dimensions of a tensor.\n\nGiven a `tensor`, and a `bool` tensor `dims` representing the dimensions\nof `tensor`, this operation reverses each dimension i of `tensor` where\n`dims[i]` is `True`.\n```\n\nThe interface of `tf.reverse` stands out from interfaces of other tf functions. such as e.g. `tf.transpose`, by accepting a boolean mask of dimensions instead of a list of dimensions.\n", "The `tf.reverse` part of the issue has been resolved as of TensorFlow 1.0, and I just resolved the documentation issue (pointing users from tf.slice to tf.getitem)."]}, {"number": 4515, "title": "Fix docs for tf.zeros and tf.ones", "body": "Qualify types.\n\nFixes #4514\n", "comments": []}, {"number": 4514, "title": "a small bug in docstring of tensorflow.zeros:", "body": "a small bug in docstring of tensorflow.zeros:\n`print tf.zeros.__doc__`\nit give with example:\n\n```\n  tf.zeros([3, 4], int32) ==> [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n```\n\nbut it should be:\n\n```\n  tf.zeros([3, 4], 'int32') ==> [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n```\n\n![small bug](https://cloud.githubusercontent.com/assets/3587832/18719636/bee6f7f4-805a-11e6-8dca-bbe7953b9825.png)\n", "comments": []}, {"number": 4513, "title": "text summarization ", "body": "hello everone,\nany one tried to use tensorflow for text summarization ? (javascript)\nany do/not do ..etc ?\n", "comments": ["This is a question better suited for StackOverflow. Please ask it there and tag it with the `tensorflow` tag.\n"]}, {"number": 4512, "title": "Indexing the last N-1 elements of  a 1D tensor raises a \"stop condition\" error.", "body": "### Environment info\n\nOperating System:\nOSX 10.11.6\n1. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\n0.10.0rc0\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nI'm trying to select the last N-1 element of a 1D tensor with N elements, for the array [0,1,2,3,4] I'm trying to extract [1,2,3,4]\n\n```\nx = tf.convert_to_tensor([0,1,2,3,4])\nb = x[:tf.shape(x)[0]-1]\n```\n### Logs or other output that would be helpful\n\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 313, in _SliceHelper\n    if s.stop is not None and s.stop < 0:\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 529, in __nonzero__\n    raise TypeError(\"Using a `tf.Tensor` as a Python `bool` is not allowed. \"\nTypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use the logical TensorFlow ops to test the value of a tensor.\n(If logs are large, please upload as attachment or provide link).\n", "comments": ["Sorry for the noise, before posting this issue I had searched for hours, however just now I found the solution to my problem -and I'm not sure the issue I've raised is an actual bug or not-\n\n```\nb = tf.slice(x, [1], [-1])\n```\n"]}, {"number": 4511, "title": "adding flake8 config", "body": "If you are going to use 2 spaces instead of four, at least reconfigure the\nlinter to not complain.\n\n:)\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "We're not using flake8 yet, and I don't think we want to merge this until we get a handle on the configuration we want to use for it. A related issue: https://github.com/tensorflow/tensorflow/issues/217\n\nIt'll otherwise give too many warnings besides the indentation that would make it pretty useless.\n\nNote: we do run pylint with this config:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/pylintrc\n", "ok, i'll see if I can make a user specific config work for me.\n"]}, {"number": 4510, "title": "No module named 'contrib.ctc'", "body": "I am trying to run an example provided by _keras_ documentation **'imdb.py**' and this error occurs \n*\\* import tensorflow.contrib.ctc as ctc\nImportError: No module named contrib.ctc **\n\nIs this a bug or Am I missing any file?\n", "comments": ["Please post this in stackoverflow. Thanks.\n"]}, {"number": 4509, "title": "fail to run py_func in parameter server, which implements as a gradient function", "body": "W tensorflow/core/framework/op_kernel.cc:968] Internal: Failed to run py callback pyfunc_2: see error log.\n", "comments": []}, {"number": 4508, "title": "Adding support for Linux s390x", "body": "We have added support for Linux s390x platform( big endian ) in TensorFlow.\nCould someone please review?\n", "comments": ["Can one of the admins verify this patch?\n", "@namrata-ibm, thanks for your PR! By analyzing the annotation information on this pull request, we identified @keveman, @petewarden and @tensorflower-gardener to be potential reviewers\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "We are working for IBM and covered as part of Google Corporate CLA.\nBelow are the IDs added:\n1. bhavenamrata@gmail.com\n2. nayana.thorat@gmail.com\n\nPlease let us know if any additional info is required.\n", "I think the corporate CLA would trigger if you used your ibm email addresses (in the commits in particular). \n\n@willnorris would that be the correct course of action?\n", "> I think the corporate CLA would trigger if you used your ibm email addresses (in the commits in particular).\n\nIt looks like these commits **do** have ibm email addresses, but @namrata-ibm's comment above suggests two gmail addresses were added to the corporate CLA?  Whichever address is used, the address in the commits needs to be covered by the corporate CLA.  So either change the commits to use one of those gmail addresses, or add the us.ibm.com address used here to the IBM contributor group.\n", "Our IBM ids are also covered as part of Google Corporate CLA.\nBelow are the IDs added:\nnbhave@us.ibm.com\nnthorat@us.ibm.com\n\nCould you please check.\nPlease let us know if any additional info is required.\n", "(moved CLA discussion to email to try and resolve it)\n", "@willnorris  Could you also send an email to nthorat@us.ibm.com id ?\n(moved CLA discussion to email)\n", "@willnorris \nSorry for the confusion.\nOur Google id's are added as part of IBM LTC Google CLA.\n1. bhavenamrata@gmail.com\n2. nayana.thorat@gmail.com\n\nWe have set the Google id as Primary email id on GitHub.\n", "@aselle, thank you for your review comments.\nwe have incorporated your suggestion about cpu_info.h. Also we are working on decode_raw_op test as per above comments.\nWe will update PR once we verify the changes.\n\nCould you please provide your feedback about the remaining code changes as well?\n", "@Nayana-ibm your github commits show your ibm.com email address.  You need it to match what you have signed the CLA with (and what is in your corporate account).  \n\nI'll take a look after you've done this and then after you address @aselle's comments.\n", "ping?\n", "@vrv We are creating a new PR with our IDs having CLA signed by tomorrow with the changes suggested by @aselle .\n", "Okay, i will close this for now, thanks!\n"]}, {"number": 4507, "title": "Installing tensorflow on Anaconda", "body": "I have a linux machine to which i installed Anaconda. I am following:\n\nhttps://www.tensorflow.org/versions/r0.10/get_started/os_setup.html\n\npip instaltion part.\n\nTo be more specific:\n\nwhich python\ngives\n\n/home/user/anaconda2/bin/python\n\nAfter which i entered:\n\nexport TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl\nAnd after:\n\nsudo pip install --upgrade $TF_BINARY_URL\nHowever, while trying:\n\npython -c \"import tensorflow\"\nI get an import error:\n\nImportError: No module named tensorflow\n", "comments": ["The 'sudo' makes pip install tensorflow outside the env. Try:\n\npip install --upgrade $TF_BINARY_URL\n\nJust tested this on Ubuntu 14.04 w/ conda env, was able to reproduce (with sudo) and resolve issue (without sudo).\n\nYou can also check the \"Using Conda\" instructions section vs just pip install of tensorflow.org. They list the below:\n\n//Python 2\n(tensorflow)$ pip install --ignore-installed --upgrade $TF_BINARY_URL\n\n//Python 3\n(tensorflow)$ pip3 install --ignore-installed --upgrade $TF_BINARY_URL\n"]}, {"number": 4506, "title": "Compute the \"local gradients\"?", "body": "Hi all, \nI wonder if the tensorflow can do the backprop given the gradient from the loss?\neg.\nbased on the chain rule, for y=f(x)\ndloss/dx = dloss/dy \\* dy/dx\nCan I do the backprop given the dloss/dy?\n", "comments": ["I believe `tf.gradients(y, x, grad_ys=<dloss/dy>)` does what you want.\n", "thx! solved\n"]}, {"number": 4505, "title": "Pack error for SparseTensor", "body": "Hello, \nI'm using the git version of tensorflow (a6c5f8e4e013e54fed8dfcf49fb6de365f018022) without CUDA\nI'm on Ubuntu 16.04\n\nDoes SparseTensor are considered as Tensor, because when i try to pack some of them, there is an error of conversion.\n\n```\nFile \"/home/siniac/lplaton/Project/RNApred/src/python/BMKSOM.py\", line 31, in __init__\n    self.mksom_N = tf.pack([self.filter_op() for _ in range(self.b)])\n  File \"/home/siniac/lplaton/.virtualenvs/tensorflow_py3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 655, in pack\n    value_shape = ops.convert_to_tensor(values[0], name=name).get_shape()\n  File \"/home/siniac/lplaton/.virtualenvs/tensorflow_py3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 657, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/home/siniac/lplaton/.virtualenvs/tensorflow_py3/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\", line 180, in _constant_tensor_conversion_function\n    return constant(v, dtype=dtype, name=name)\n  File \"/home/siniac/lplaton/.virtualenvs/tensorflow_py3/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\", line 163, in constant\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape))\n  File \"/home/siniac/lplaton/.virtualenvs/tensorflow_py3/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py\", line 422, in make_tensor_proto\n    tensor_proto.string_val.extend([compat.as_bytes(x) for x in proto_values])\n  File \"/home/siniac/lplaton/.virtualenvs/tensorflow_py3/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py\", line 422, in <listcomp>\n    tensor_proto.string_val.extend([compat.as_bytes(x) for x in proto_values])\n  File \"/home/siniac/lplaton/.virtualenvs/tensorflow_py3/lib/python3.5/site-packages/tensorflow/python/util/compat.py\", line 45, in as_bytes\n    (bytes_or_text,))\nTypeError: Expected binary or unicode string, got <tensorflow.python.framework.ops.SparseTensor object at 0x7f6de596a390>\n```\n\nNB: the filter_op() method product SparseTensor \n", "comments": ["I think pack() only works with dense tensor at this point.\n"]}, {"number": 4504, "title": "Fix symbol duplication for iOS on Xcode/clang", "body": "I tried to use Tensorflow on iPhone.\nI successfully created library files for iOS (libtensorflow-core-*.a)\nusing `tensorflow/contrib/makefile/build_all_ios.sh`.\n\nBut when I tried to use the library, following linker error happened on Xcode:\n\n```\nduplicate symbol __Z14tf_git_versionv in:\n    /Users/admin/tmp/tensorflow/tensorflow/contrib/ios_examples/simple/../../makefile/gen/lib/libtensorflow-core.a(version_info.o)\nduplicate symbol __Z19tf_compiler_versionv in:\n    /Users/admin/tmp/tensorflow/tensorflow/contrib/ios_examples/simple/../../makefile/gen/lib/libtensorflow-core.a(version_info.o)\nld: 2 duplicate symbols for architecture x86_64\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\n```\n\nThese functions are defined in `tensorflow/core/util/version_info.cc`.\nIt seems the object file is added twice in Makefile:\n(`$(wildcard tensorflow/core/util/*.cc)` and `tensorflow/core/util/version_info.cc`)\n\nThe source file `version_info.cc` is generated from a script\nand not included in the repository,\nso adding the source file to `CORE_CC_ALL_SRCS` manually on line 431 is necessary.\nBut once it is generated, it is listed on `$(wildcard tensorflow/core/util/*.cc)` too, and added twice.\n\nUsing `$(sort ...)` function removes this duplication and solves the problem.\n", "comments": ["Can one of the admins verify this patch?\n", "@tyfkda, thanks for your PR! By analyzing the annotation information on this pull request, we identified @martinwicke, @tensorflower-gardener and @vrv to be potential reviewers\n", "Great. I had a similar fix ready for PR. Yours is better to me.\n\nNo problem with missing ops after applying your fix? I give up on my PR, but I also had [to add missing ops](https://github.com/ic/tensorflow/commit/a6a1bfa25860ed9f5f01aaabb72136ce58c6e2be#diff-d251197c17294875e26b96cfff5bb1d8) to get a linkable build on iOS. Please note that the linked diff is valid for `tensorflow/contrib/makefile/tf_op_files.txt` only (my change on the `Makefile` does not cover one case and would err).\n", "@ic In my environment,\nI have succeeded to link and execute iOS examples (simple and camera)\nwith the library built with this change.\nI haven't encountered missing ops, so I'm not sure it is needed.\n", "@aselle Thank you for your review!\nCould you tap Jenkins?\n", "@tensorflow-jenkins test this please\n", "@tyfkda I hope the Xcode version you are using is Xcode 8 ? I was able to get everything working with Xcode 7.3 without the changes you made in this commit. However, I required to add the change you made in order to get tensor flow properly working and build the app without error in Xcode 8. The app builds fine for an iOS 10 iPhone but crashes.\n\nAre you using using an iOS 10 device ? \nI cannot make it work for iOS 10 with Xcode 8.\n\nCould you please share your versions and all ? That would be really helpful. I can, then, maybe raise a proper question at Stack Overflow.\n\nThanks !\n", "@arundasan91 Yes, I have updated Xcode to 8.\nHere is my environment:\n- Xcode: Version 8.0 (8A218a)\n- iOS: 10.0.1 (14A403)\n\nI can run an application with this change on iOS simulator, and iPhone 6.\n\nAs you said,\nI built and ran successfully without this change on Xcode 7.3 in the past (2 months or so).\nRecently I updated my device's OS from 9 to 10,\nand it requires Xcode 8,\nthen `duplicate symbol` error happens.\n\nI'm not sure this is caused by some source code change in the repository,\nor by Xcode 8.\n", "Thank You ! That clears a lot. \n\nDid you change or add any build options or similar settings though ? I came across some errors and found that adding `Privacy - Photo Library Usage Description`, `Privacy - Camera Usage Description` to `Info.plist` might help remove the errors. It did build properly. But the app, when opening up in the device, breaks with this error \n\n```\n2016-09-23 23:17:17.086861 CameraExample[2190:405056] 0x16ea0dc0 Copy matching assets reply: XPC_TYPE_DICTIONARY  <dictionary: 0x16ea0dc0> { count = 2, transaction: 0, voucher = 0x0, contents =\n    \"Result\" => <int64: 0x16e9d490>: 0\n    \"Assets\" => <data: 0x16ea0e30>: { length = 1229 bytes, contents = 0x62706c6973743030d401020304050866675424746f705824... }\n}\n2016-09-23 23:17:17.092728 CameraExample[2190:405056] 0x16d73990 Copy assets attributes reply: XPC_TYPE_DICTIONARY  <dictionary: 0x16d73990> { count = 1, transaction: 0, voucher = 0x0, contents =\n    \"Result\" => <int64: 0x16d73a00>: 1\n}\n2016-09-23 23:17:17.093023 CameraExample[2190:405056] [MobileAssetError:1] Unable to copy asset attributes\n2016-09-23 23:17:17.093369 CameraExample[2190:405056] Could not get attribute 'LocalURL': Error Domain=MobileAssetError Code=1 \"Unable to copy asset attributes\" UserInfo={NSDescription=Unable to copy asset attributes}\n2016-09-23 23:17:17.100138 CameraExample[2190:405056] 0x16d702f0 Copy matching assets reply: XPC_TYPE_DICTIONARY  <dictionary: 0x16d702f0> { count = 2, transaction: 0, voucher = 0x0, contents =\n    \"Result\" => <int64: 0x16d64090>: 0\n    \"Assets\" => <data: 0x16d81380>: { length = 1237 bytes, contents = 0x62706c6973743030d401020304050865665424746f705824... }\n}\n2016-09-23 23:17:17.104951 CameraExample[2190:405056] 0x16ea3640 Copy assets attributes reply: XPC_TYPE_DICTIONARY  <dictionary: 0x16ea3640> { count = 1, transaction: 0, voucher = 0x0, contents =\n    \"Result\" => <int64: 0x16ea3620>: 1\n}\n2016-09-23 23:17:17.105176 CameraExample[2190:405056] [MobileAssetError:1] Unable to copy asset attributes\n2016-09-23 23:17:17.105510 CameraExample[2190:405056] Could not get attribute 'LocalURL': Error Domain=MobileAssetError Code=1 \"Unable to copy asset attributes\" UserInfo={NSDescription=Unable to copy asset attributes}\n2016-09-23 23:17:18.997743 CameraExample[2190:404997] [MC] System group container for systemgroup.com.apple.configurationprofiles path is /private/var/containers/Shared/SystemGroup/systemgroup.com.apple.configurationprofiles\n2016-09-23 23:17:19.002128 CameraExample[2190:404997] [MC] Reading from public effective user settings.\nF /Users/arundas/tensorflow/tensorflow/contrib/ios_examples/camera/../../../../tensorflow/core/lib/core/refcount.h:79] Check failed: ref_.load() == 0 (1 vs. 0)\n```\n\nwith `Thread 4: signal SIGABRT`\n\nI am sorry if I am not supposed to add more comments here.\n\nI will add a new question in StackOverflow tomorrow.\n\nThanks a lot for the info about the versions!\n", "@arundasan91 \nI'm sorry for my wrong information,\nI didn't test the examples well.\n\nNow I test ios_examples/camera,\nI can build it without any modification for bulid settings or `Info.plist` (but set `Team` to run on real device).\nBut when I try to run the app, it crashes as you said.\n\nI can run ios_example/simple though,\nI think the error is not related to Tensorflow library,\nand need modification for application code.\n\nIt is better to ask in other place about the crash (StackOverflow),\nand send a pull request to fix the issue :)\n", "Thanks a lot !\n", "I encountered same issue (duplicate symbol) using Xcode 8.0 and iOS 9.3.5 (iPhone 6 Plus).\nAnd I used @tyfkda patch and fixed the issue.\nNow CameraExample is working fine.\nJFYI.\n"]}, {"number": 4503, "title": "Branch 133795652", "body": "", "comments": ["@martinwicke, thanks for your PR! By analyzing the annotation information on this pull request, we identified @keveman, @charlesnicholson and @andrewharp to be potential reviewers\n"]}, {"number": 4502, "title": "segmentarion fault(core dumped)", "body": "when i try to \"import tensorflow\", it often occurs that \"segmentarion fault(core dumped)\" and quits. It does not happen all the time. Maybe it can function well sometimes. I dont know why.\n", "comments": ["Please post this with more detailed information on stackoverflow. Thanks.\n"]}, {"number": 4501, "title": "Updating CallWithRetries to do exponential backoff", "body": "- setting maximum backoff seconds to be 32 seconds.\n", "comments": ["Can one of the admins verify this patch?\n", "@moontails, thanks for your PR! By analyzing the annotation information on this pull request, we identified @rinugun and @tensorflower-gardener to be potential reviewers\n", "@rinugun Thank you for reviewing my PR.\n\nParameterizing the intial_delay_seconds seems like a good approach, I have addressed the feedback and pushed the changes. Please take a look.\n", "assigning @rinugun to reflect reality. Thanks for reviewing!\n", "@rinugun Thank you for reviewing my code quickly each time. Learned a few things along the way while helping enhance this :)\n", "@rinugun the pleasure was mine! Your feedback helped polish the code I had submitted. Really appreciate your quick feedback on each commit :) Thank you!\n", "Actually I'm not authorized to merge pull requests, so hopefully something good will happen with this PR automatically now when I approved. If not, please ping this thread again, I'll try to investigate.\n", "I will keep that in mind. Thank you once again @rinugun :)\n", "Jenkins, test this please (@rinugun, you can trigger tests as well, and you for bonus points you can add the \"awaiting tests (then merge)\" label)\n", "I apologize for some minor typos that should have been checked for. Can someone please trigger the tests again?\n", "Jenkins, test this please\n", "Error details -\n\n```\ntensorflow/core/platform/cloud/retrying_file_system.cc: In member function 'virtual tensorflow::Status tensorflow::RetryingFileSystem::IsDirectory(const string&)':\ntensorflow/core/platform/cloud/retrying_file_system.cc:200:76: error: too few arguments to function 'tensorflow::Status tensorflow::{anonymous}::CallWithRetries(const std::function<tensorflow::Status()>&, int)'\n       std::bind(&FileSystem::IsDirectory, base_file_system_.get(), dirname));\n                                                                            ^\ntensorflow/core/platform/cloud/retrying_file_system.cc:42:8: note: declared here\n Status CallWithRetries(const std::function<Status()>& f,\n        ^\ntensorflow/core/platform/cloud/retrying_file_system.cc:201:1: warning: control reaches end of non-void function [-Wreturn-type]\n }\n ^\n```\n\nI checked and see that the `IsDirectory` method is not overridden. But I fail to see why this causes an error as the base class implementation does not wrap any call with `CallWithRetries`. \n", "Not quite yet. :)\n", "@moontails, I think your local version of the repository is outdated, the function is overridden in the latest version on GitHub https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/cloud/retrying_file_system.cc#L170\n", "@rinugun I will sync with master and make the changes. \n", "Jenkins, test this please\n", "@rinugun @martinwicke the tests werent trigerred\n", "Jenkins, test this please?\n", "looks like the failed jobs lost connection?\n", "Jenkins, test this please\n", "Hmm, that failure looks unrelated. Can you try rebasing on top of the latest master and we'll retest?\n", "Sure will do. Do you want me to rebase or merge with latest master?\n\nBest Regards,\nBalachander Ramachandran\n\nOn Sep 27, 2016 1:13 PM, \"Jonathan Hseu\" notifications@github.com wrote:\n\n> Hmm, that failure looks unrelated. Can you try rebasing on top of the\n> latest master and we'll retest?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4501#issuecomment-249983540,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AEL334CZH4uxTzvd4rNapE2-TualfS6sks5quXjggaJpZM4KCXcW\n> .\n", "Merge is fine too (and probably easier). We have some changes in master that are needed for tests to work.\n", "@moontails, I just realized that sleep() might not be the canonical way of making a delay, e.g it might not be supported on Windows. I was wondering if you could replace it with \nEnv::Default()->SleepForMilliseconds, e.g see this example:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/cloud/gcs_file_system.cc#L379\n\nThis also prepares the code for the subsequent implementation of random millisecond part, that we discussed in another thread with @davidj-github.\n\nAlso I had another comment related to this, even though this contradicts to what I suggested before. Instead of making 'initial_delay' a ctor parameter, you could have 'Env\\* env' a ctor parameter instead, which would default to Env::Default(). This way you could provide a fake implementation of SleepForMilliseconds from the test code. The fake implementation would return immediately, but would record how it was called.\nThe benefit of the latter is that the test code can still work fast, but also check the sleep intervals that were passed to SleepForMilliseconds from the main code and make sure they all look as expected.\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@jhseu I have merged the latest changes from master. Could you please trigger the tests?\n", "Sure, but mind making the requested changes before we commit?\nJenkins, test this please\n", "@jhseu @rinugun committed the changes. Please review and trigger tests!\n", "Sorry for not sending all comments at once -- hit a wrong button on Github.\nPlease see my other comments as well.\n\nOn Tue, Sep 27, 2016 at 9:32 PM, Balachander Ramachandran <\nnotifications@github.com> wrote:\n\n> ## _@moontails_ commented on this pull request.\n> \n> In tensorflow/core/platform/cloud/retrying_file_system.cc\n> https://github.com/tensorflow/tensorflow/pull/4501:\n> \n> >  #include \"tensorflow/core/lib/core/errors.h\"\n> >  #include \"tensorflow/core/platform/file_system.h\"\n> \n>  namespace tensorflow {\n> \n>  namespace {\n> \n> -// In case of failure, every call will be retried kMaxAttempts-1 times.\n> -constexpr int kMaxAttempts = 4;\n> +// In case of failure, every call will be retried kMaxRetries times.\n> +constexpr int kMaxRetries = 3;\n> +// Maximum backoff time in seconds.\n> \n> Done\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4501, or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABnZsn9f8SkYsVLLGybXBuw5Iywy8QYeks5que3ngaJpZM4KCXcW\n> .\n", "@rinugun I have addressed all your comments\n", "@jhseu @martinwicke could one of you please trigger the tests\n", "Jenkins, test this please\n", "Jenkins, test this please\n", " @jhseu @rinugun thank you for all the feedback during this code review and responding to my questions so quickly each time :)\n", "Thanks you for your contribution!\n"]}, {"number": 4500, "title": "Attempting to use uninitialized value lstm/LSTMCell/W_0 on Distributed TensorFlow", "body": "On a cluster of Tensorflow, which has one ps server, and one worker:\n\n``` python\ndef inference(self, inds, early_stops):\n\n            ...\n\n            with tf.variable_scope('lstm_encoder') as scope:\n                initializer = tf.random_uniform_initializer(-0.08, 0.08)\n                lstm_cell = tf.nn.rnn_cell.LSTMCell(self.hidden_units, initializer = initializer, state_is_tuple = True)\n                if self.is_training and self.keep_prob < 1:\n                    lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob = self.keep_prob)\n\n       ...\n```\n\n``` python\n with tf.device(tf.train.replica_device_setter(worker_device =\n                                                          '/job:worker/task:%d' % int(os.environ['task_index']), cluster = cluster)):\n                train_op, loss = inference()\n```\n\nit turns out that:\n\n```\ntensorflow.python.framework.errors.FailedPreconditionError: Attempting to use uninitialized value sentiment/lstm_encoder/RNN/LSTMCell/W_0\n     [[Node: sentiment/lstm_encoder/RNN/LSTMCell/W_0_S8 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/gpu:0\", send_device=\"/job:ps/replica:0/task:0/gpu:0\", send_device_incarnation=-5768356229269645157, tensor_name=\"edge_226_sentiment/lstm_encoder/RNN/LSTMCell/W_0\", _device=\"/job:ps/replica:0/task:0/gpu:0\"](sentiment/lstm_encoder/RNN/LSTMCell/W_0)]]\n     [[Node: sentiment/lstm_encoder/RNN/LSTMCell/B/Assign_S4 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/gpu:0\", send_device=\"/job:ps/replica:0/task:0/gpu:0\", send_device_incarnation=-5768356229269645157, tensor_name=\"edge_214_sentiment/lstm_encoder/RNN/LSTMCell/B/Assign\", tensor_type=DT_FLOAT, _device=\"/job:worker/replica:0/task:0/gpu:0\"]()]]\n```\n\nI try to place variables on cpu or gpu, both of them did not work.\n", "comments": [" Can you share the code that you use to initialize the variables?\n", "I push it to github, you can reference [this](https://github.com/qingzew/tfcluster/blob/master/examples/sentiment/trainer.py#L57)\n", "Thanks for sending over that code. There's nothing obvious in that file that would lead to variables being uninitialized, but I haven't read all of the files.\n\nCould you try adding an explicit call to `sess.run(init_op)` in your session, to see if the initialization is being missed for some reason?\n\nIf that doesn't fix things, the next step would be to package up your code as a single file with no external code or data dependencies, and we can take a look at why the initialization isn't working as expected.\n", "@mrry Thank you for your help. I have tried `sess.run(init_op)`, it doesn't work. Before that, I wrote it on singal machine, putting all variables  and ops on gpu, it was fine. Then I modify it on distributed version, it does not work any more.\n\nIn fact, I write a cluster base on mesos,  I run this example on it. I am not sure if the cluster has bugs, although a mnist example can run on it\n\nso I will update this issue\n", "@qingzew Do you have any updates on this issue?\n", "not yet, I rewritted my distributed code, and want to use sync_trainer instead async_trainer, but the gpu is busy,\nso you will close this issue?\n", "I'll close it for now, but please feel free to re-open if you can reproduce it with a smaller example!\n"]}, {"number": 4499, "title": "tf distributed mnist between graph sync example hang", "body": "when i use distributed mnist between graph sync example hang\n\nexample code:\n[distributed_between_sync.py.txt](https://github.com/tensorflow/tensorflow/files/484050/distributed_between_sync.py.txt)\n\nwho can tell me ,,why it hang???\n\nuse tensorflow 0.10 release version\n\n1.start PS:\n   nohup python distributed_between_sync.py --job_name=\"ps\" --task_index=0 > ps_0.log &\n  nohup python distributed_between_sync.py --job_name=\"ps\" --task_index=1 > ps_1.log & \n\n2.start worker0:\n  nohup python distributed_between_sync.py --job_name=\"worker\" --task_index=0 --max_steps=10000 > worker_0.log &\n\n3.start worker1:\n  nohup python distributed_between_sync.py --job_name=\"worker\" --task_index=1 --max_steps=10000 > worker_1.log &\n\nworker log \n[worker_0.log.txt](https://github.com/tensorflow/tensorflow/files/484048/worker_0.log.txt)\n[worker_1.log.txt](https://github.com/tensorflow/tensorflow/files/484049/worker_1.log.txt)\n", "comments": ["who can tell me ???why hang??\n", "The links are all empty.\n", "the links can download ...not empty @jmchen-g \n", "could you try to whittle this down to a smaller example?\n", "Automatically closing due to lack of recent activity, we will reopen when further information becomes available.\n", "See #5394\n"]}, {"number": 4498, "title": "TF Distributed not using the full network bandwith", "body": "Hi,\nI have a one ps, 4 worker setup (2 workers are in the same machine as ps), I found no matter what batch size I choose, the network usage is always around 1.3 Gbit/s, which leads to very slow performance with small batch size. However, I do have 40 Gbit network, and when I run iperf -s/c between these two machines, I can usually get over 30 Gbit/s speed. \n\nI am not sure if the issue is in TF implementation or my network setup. but since iperf proved, I doubt it's my infrastructure problem.  \n\nBelow is the configuration \nps machine: Ubuntu 14.04, TF v0.10.0\nworker machine: CentOS 7, TF v0.10.0 \nPython 2.7\nGPU is Tesla M40 \n", "comments": ["Are you bottlenecked by CPU?\n\nOn Tue, Sep 20, 2016 at 1:34 PM, Jianbang Zhang notifications@github.com\nwrote:\n\n> Hi,\n> I have a one ps, 4 worker setup (2 workers are in the same machine as ps),\n> I found no matter what batch size I choose, the network usage is always\n> around 1.3 Gbit/s, which leads to very slow performance with small batch\n> size. However, I do have 40 Gbit network, and when I run iperf -s/c between\n> these two machines, I can usually get over 30 Gbit/s speed.\n> \n> I am not sure if the issue is in TF implementation or my network setup.\n> but since iperf proved, I doubt it's my infrastructure problem.\n> \n> Below is the configuration\n> ps machine: Ubuntu 14.04, TF v0.10.0\n> worker machine: CentOS 7, TF v0.10.0\n> Python 2.7\n> GPU is Tesla M40\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4498, or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHFzFkchi5igJlrx18-rAS-FSK3dNks5qsENngaJpZM4KCHMD\n> .\n", "We are also seeing slowness an order of magnitude worse what's expected, and I tried to isolate the problem in a simple benchmark: https://gist.github.com/yaroslavvb/1124bb02a9fd4abce3d86caf2f950cb2\n\nIt launches 2 local tensorflow processes and tries to add 10MB vectors of in one process to variable stored in another one as fast as possible.\n\nTo run:\n\n```\n> python local_distributed_benchmark.py\n....\nLocal rate:       6932.62 MB per second\nDistributed rate: 161.63 MB per second\n```\n\nThe rate of data tops out at 1.3 Gbps on my laptop, I wonder if it's a coincidence. Running on bigger machine doesn't give a speed increase, and CPU utilization stays under 20% . Same with latest nightly (GIT_VERSION='v0.10.0-1452-gdcba217-dirty')\n\n@mrry \n", "Hi Yaroslav, I think your program is bottlenecked by this line:\n\n``` python\nsess.run(add_op)\n```\n\nThis ensures that the entire result is copied back from the worker to the master to the client, deserialized from a protobuf into Python and discarded. A simple change to the following makes the numbers more promising:\n\n``` python\nsess.run(add_op.op)\n```\n- Before (with `sess.run(add_op)`):\n  \n  ```\n  Adding data in 10 MB chunks\n  Local rate:       3693.02 MB per second\n  Distributed rate: 115.94 MB per second\n  ```\n- After (with `sess.run(add_op.op)`):\n  \n  ```\n  Adding data in 10 MB chunks\n  Local rate:       10565.81 MB per second\n  Distributed rate: 6379.86 MB per second\n  ```\n\nIndeed, I'm almost certain that the placer will put the constant in your program  on the same device as the `assign_add()` (because it has only one consumer), so this is purely measuring dispatch overhead. Changing `update` to be a `tf.Variable`, so that it is pinned to the different device gives a more realistic number that falls in between the two cases above:\n- With tensors pinned to different devices (and `sess.run(add_op.op)`):\n  \n  ```\n  Adding data in 10 MB chunks\n  Local rate:       10259.25 MB per second\n  Distributed rate: 832.56 MB per second\n  ```\n\nThis throughput obviously still isn't as good as it could be, but I haven't tried very hard to saturate the \"network\" (e.g. by issuing parallel transfers). @JianbangZ, there are many different reasons why your code might not be saturating your 40GbE network. Being CPU-bound is the most likely reason, if your network is doing anything non-trivial. It's also possible that there's contention on the parameter server, or in some part of the TensorFlow runtime. Without more details about the code you are running, it's impossible to pinpoint the root cause, so if you can share more details, that'd be great!\n", "@mrry Thanks for the quick response! Haven't realized that \"tf.device\" was ignored here.  I'm now getting 900 MB/s which is pretty good. So it looks like deserializing gRPC into Python is slow (<1 Gbps regardless of number of cores) and should be avoided. @JianbangZ are you by chance fetching large tensors in your Python client?\n", "@mrry I have tried different things to ease the CPU work such as only allowing one worker in ps and get rid of the image preprocessing on ps CPU, but the results is the same. My code is a mimic of the inception distributed example, the code to control training op is the same as this https://github.com/tensorflow/models/blob/master/inception/inception/inception_distributed_train.py\nHas the team seen data transfer slowness with this implementation?\n@yaroslavvb I don't think I am fetching large tensors. As I mentioned to mrry, my code is mostly a mimic of https://github.com/tensorflow/models/blob/master/inception/inception/inception_distributed_train.py\n", "@JianbangZ What's the duration of each step when running on a single process, and what's the size of the model that's being retrieved on each step? What batch size are you using? For Inception in the default configuration, I wouldn't expect it to saturate 40GbE, because the computation will be the bottleneck.\n\nAnd just to confirm, are you running 0.10 or 0.10rc0 (because there were several performance fixes after the release candidate)?\n", "@mrry Thank you for the quick response. I am not training on GoogleNet, I am just using the method it uses to do distributed on my own network.\nMy net only has 3 conv + 2 fc + 1 softmax readout layers to handle the input image size of 99*99. The total parameter count is about 9 M, which means the parameter size is 36 MB, so 40 GbE is way enough.\n\nMy training speed is around 600 images/sec on single GPU/single process. Usually I use batch_size=128. with single machine 2 GPUs, so I can get 1200 images/sec. \nWith distributed, no matter what batch size I try, it shows my network usage is around 1~1.3 Gbit/s, thus smaller batch size shows decreased performance. \n\nI am using the 0.10.0 pip binary.\n\nTo make this easy, I uploaded my code to github, although it can't run b/c it's tied my with my own dataset and couple of environment variables, you can still see all my code.\nThe script to kick off the task is https://github.com/JianbangZ/eva/blob/master/eva/applications/facial_analysis/distributed_train.sh\nand the code to control most of the training logic is https://github.com/JianbangZ/eva/blob/master/eva/platform/base_distributed_train.py\nI am sorry this file has a lot of commented out multi-GPU code.\nthe code to define my network architect is here (I am using my own CustomNet although I implemented other nets in the same file) https://github.com/JianbangZ/eva/blob/master/eva/applications/facial_analysis/cnn_model.py#L625\n", "The github code appears to have disappeared? Is this issue still current?\n", "Automatically closing due to lack of recent activity, we will reopen if further information becomes available. Thanks!\n", "Related to #6116 "]}, {"number": 4497, "title": "bidirectional_dynamic_rnn: Negative indices are currently unsupported", "body": "I am getting the 'negaive indexing' error since I switched from using `bidirectional_rnn` to `bidirectional_dynamic_rnn`:\n\n```\n            (output_fw, output_bw), output_states  = tf.nn.bidirectional_dynamic_rnn(\n                cell_fw=lstm_fw_cell,\n                cell_bw=lstm_bw_cell,\n                inputs=x,\n                dtype=tf.float32,\n                sequence_length=seq_len\n                )\n            output_fw = tf.transpose(output_fw, perm=[1,0,2])\n            output_bw = tf.transpose(output_bw, perm=[1,0,2])\n            rnn_outputs = tf.concat(2, [output_fw, output_bw])\n\n        with tf.variable_scope('output', reuse=reuse):\n            with tf.variable_scope('softmax'):\n                W = tf.get_variable('W', [self.num_hidden*2, self.num_classes],\n                                    initializer=tf.truncated_normal_initializer(stddev=0.1))\n                b = tf.get_variable('b', [self.num_classes], initializer=tf.constant_initializer(0.1))\n            logits = tf.matmul(rnn_outputs[-1], W) + b\n```\n\n```\nTraceback (most recent call last):\n  File \"train_lstm.py\", line 62, in <module>\n    logits = model.inference()\n  File \"/home/ccrmad/Code/TDLSTM/models/rnn_classifier.py\", line 70, in inference\n    logits = tf.matmul(rnn_outputs[-1], W) + b\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 334, in _SliceHelper\n    raise NotImplementedError(\"Negative indices are currently unsupported\")\nNotImplementedError: Negative indices are currently unsupported\n```\n\nHow come I could negative-index the output from `bidirectional_rnn` but not `bidirectional_dynamic_rnn` this way? Thanks.\n", "comments": ["bidirectional_rnn works and returns lists of tensors while bidirectional_dynamic_rnn works and returns tensors (each tensor packed along time-dimension).  Could this be the issue?\n", "Thanks I have just noticed that. I will close this issue now. \n"]}, {"number": 4496, "title": "Chit", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\n\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n### Environment info\n\nOperating System:\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n### What other attempted solutions have you tried?\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n", "comments": ["Good\n"]}, {"number": 4495, "title": "Documentation on how reusable variables sync between CPU and GPU", "body": "I haven't been able to find anything online that documents how the sync of reusable variables happens between CPU and GPU. Specifically, I'm interested in if a CPU pinned variable is used multiple times in a GPU computation without being updated, does it sync only once (i.e. use a dirty flag), or will it try sync again each time?\n", "comments": ["@zheng-xq Could you take a look? Thanks.\n", "In general, TensorFlow transfers a tensor between ops across different devices. \n\nIf a variable is pinned on CPU, reading it on GPU will cause it to transfer. If your graph reads it several times, it will be transferred several times. The way to avoid that is to read from a tf.identity that is pinned on GPU. In that case, the tensor flowing from the CPU variable to its GPU tf.identity will cause it to be transferred once. The remaining calculation reads from that GPU copy. This gives the model builder the control how the data transfer should happen. \n", "Thanks @zheng-xq for your response, so just to clarify things:\n\nFor an RNN Cell, whose weights are pinned to CPU but ops executed on GPU, the cell will be called multiple times under `dynamic_rnn` with `tf.python.ops.control_flow_ops.while_loop`. In this case, it would try copy the weights on every iteration. The correct approach to this, is to modify the `RNNCell` class, and modify the weight variables to use an `tf.identity` on the get_variable, is that correct?\n", "get_variable already supports that. You can use \"caching_device\" to specify which device to cache this variable read for. \n", "Yes but if there are multiple GPU cards and you want to duplicate the same weights for the model on all of them, but allow each to have a local copy of the variable cached. So in this case the beat approach would be to use identity to force a local copy right? Or is there another approach. \n", "Closing due to lack of recent activity, please reopen if you have further. questions. Also since this involves usage best practices, Stack Overflow is more appropriate for this question."]}, {"number": 4494, "title": "Tag classifier_test as manual for now to unblock nightly builds", "body": "", "comments": ["@caisq, thanks for your PR! By analyzing the annotation information on this pull request, we identified @ilblackdragon, @tensorflower-gardener and @itsmeolivia to be potential reviewers\n"]}, {"number": 4493, "title": "Tensorflow Bazel build error", "body": "Hello,\n\nI was pointed here from StackOverflow, hopefully I am in the right place.\n\nI have been trying to setup tensorflow on my Raspberry Pi 3B using the guide [here](https://github.com/samjabrahams/tensorflow-on-raspberry-pi/blob/master/GUIDE.md). However I run into an error when executing the command:\n\n`bazel build -c opt --copt=\"-mfpu=neon\" --local_resources 1024,1.0,1.0 --verbose_failures tensorflow/tools/pip_package:build_pip_package`\n\nBelow is the error:\n\n```\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nERROR: /home/pi/makevoicedemo/tf/tensorflow/tensorflow/tensorflow.bzl:571:26: Traceback (most recent call last):\n        File \"/home/pi/makevoicedemo/tf/tensorflow/tensorflow/tensorflow.bzl\", line 565\n                rule(attrs = {\"srcs\": attr.label_list...\"), <3 more arguments>)}, <2 more arguments>)\n        File \"/home/pi/makevoicedemo/tf/tensorflow/tensorflow/tensorflow.bzl\", line 571, in rule\n                attr.label_list(cfg = \"data\", allow_files = True)\nexpected ConfigurationTransition or NoneType for 'cfg' while calling label_list but got string instead: data.\nERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Extension file 'tensorflow/tensorflow.bzl' has errors.\nINFO: Elapsed time: 0.337s\n```\n\nI was told that this is possibly due to a bug.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nError report #4319 however the thread seemed to stray from the original topic and I wasn't able to pick out if a fix had been found for the initial issue.  \n### Environment info\n\nOperating System:\nRaspbian Jessie\n\nInstalled version of CUDA and cuDNN: \nN/A\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n`ls: cannot access /path/to/cuda/lib/libcud*: No such file or directory`\n\nCommit hash (`git rev-parse HEAD`):\n7df9c6860e00b91eda0e550b11d9be52d9341d85\n\nOutput of `bazel version`:\n\n```\nBuild label: 0.2.1-2016-09-15 (@e7a95e5)\nBuild target: bazel-out/local_linux-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Sep 16 00:46:39 2016 (1473986799)\nBuild timestamp: 1473986799\nBuild timestamp as int: 1473986799\n\n```\n### What other attempted solutions have you tried?\n\nAttempted to install from pip, both attempts (python 2.7, and 3.3+) failed.\n", "comments": ["Just to cover the basics. Did you run \"./configure\" before running \"bazel build...\"?\n", "Yes, I had to cancel the first time because I entered the wrong file path, I did run `./configure` using the default file paths and with \"Google Cloud Platform\" and GPU support disabled.\n", "Try upgrading bazel to 0.3.1 ?\n\nI wish I knew why https://github.com/tensorflow/tensorflow/blob/master/WORKSPACE#L23 is no longer working...\n", "I used `git checkout 0.2.1` to get bazel, should I upgrade using `git checkout 0.3.1` or is there a different method you'd recommend. \n\nUsing `git checkout 0.3.1` gives me:\n\n```\n error: Your local changes to the following files would be overwritten by checkout:\n        scripts/bootstrap/compile.sh\nPlease, commit your changes or stash them before you can switch branches.\nAborting\n\n```\n", "I would check out bazel.io or github.com/bazelbuild/bazel for upgrading\n", "Related to #4531\n", "I have just tried with both Bazel @0.3.1 and the present master. Both lead to the same errors reported by @KjerG, trying to build Tensorflow on the present master, too.\n\nInterestingly, building 95ddc4e (just before the possibly breaking 7bcdcbbf) seems to work (I just need to build `//tensorflow/cc:cc_ops`, though).\n", "I had the same error after trying to `./configure` the latest master. After upgrading Bazel it works again.\n", "Sorry for taking so long to respond, I haven't had access to my project for a bit so I had no way of testing.\n\nAnyway, upgrading to 0.3.1 (`git checkout 0.3.1`) and running `./compile.sh` leaves me with: \n`gRPC Java plugin not found in`\nfollowed by nothing (I know there's supposed to be an error message here usually).\n\nAs said, I haven't touched the system since the last post so Java should still be where I left it. I'll try to fix this and see if upgrading to 0.3.1 helped.\n", "You are not missing Java, but the gRPC plugin. It may also be that your are trying to build on a broken commit.\n", "@KjerG, Closing due to lack of recent activity. Please reopen when additional information becomes available. \n", "I am attempting a clean install using the guide [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile#raspberry-pi) in the hopes of circumventing this issue. \n\nI have kept my old Tensorflow build as an (inactive) backup. If I make any more headway on this issue I will post it here.\n"]}, {"number": 4492, "title": "Missing C++ headers for Session API in PIP packages.", "body": "C++ headers for the session APIs, such as tensorflow/core/public/<session.h session_options.h tensor_c_api.h> are not included in the official PIP packages. Could we have them included in the list of headers to export in the framework_headers target in tensorflow/core/BUILD or is there a reason to exclude them?\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nhttps://github.com/tensorflow/tensorflow/issues/3536\nhttps://github.com/tensorflow/tensorflow/issues/720\n### Environment info\n\nOperating System: Ubuntu 16.04 (Xenial)\n\nInstalled version of CUDA and cuDNN: n/a\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\nhttp://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0-cp27-none-linux_x86_64.whl\n1. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   0.9.0\n", "comments": []}]