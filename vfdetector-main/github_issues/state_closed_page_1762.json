[{"number": 25, "title": "Cuda 3.0?", "body": "Are there plans to support Cuda compute capability 3.0? \n", "comments": ["Officially, Cuda compute capability 3.5 and 5.2 are supported. You can try to enable other compute capability by modifying the build script: \n\nhttps://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc#L236\n", "Thanks! Will try it and report here.\n", "This is not officially supported yet. But if you want to enable Cuda 3.0 locally, here are the additional places to change: \n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.cc#L610\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.cc#L629\nWhere the smaller GPU device is ignored. \n\nThe official support will eventually come in a different form, where we make sure the fix works on all different computational environment.\n", "I made the changes to the lines above, and was able to compile and run the basic example on the Getting Started page: http://tensorflow.org/get_started/os_setup.md#try_your_first_tensorflow_program - it did not complain about gpu, but it didn't report using the gpu either.\n\nHow can I help with next steps?\n", "infojunkie@, could you post your step and upload the log?\n\nIf you were following this example: \n\nbazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\nbazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu\n\nIf you see the following line, the GPU logic device is being created: \n\nCreating TensorFlow device (/gpu:0) -> (device: ..., name: ..., pci bus id: ...)\n\nIf you want to be absolutely sure GPU was used, set CUDA_PROFILE=1 and enable Cuda profiler. If the Cuda profiler logs were generated, it was a sure sign GPU was used. \n\nhttp://docs.nvidia.com/cuda/profiler-users-guide/#command-line-profiler-control\n", "I got the following log: \n\n```\nI tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 8\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:888] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:88] Found device 0 with properties: \nname: GeForce GT 750M\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.967\npciBusID 0000:02:00.0\nTotal memory: 2.00GiB\nFree memory: 896.49MiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:112] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:122] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:643] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:02:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:643] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:02:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:643] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:02:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:643] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:02:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:643] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:02:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:643] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:02:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:47] Setting region size to 730324992\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:643] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:02:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:643] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:02:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:643] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:02:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:643] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:02:00.0)\nI tensorflow/core/common_runtime/local_session.cc:45] Local session inter op parallelism threads: 8\n```\n\nI guess it means the GPU was found and used. I can try the CUDA profiler if you think it's useful.\n", "Please prioritize this issue. It is blocking gpu usage on both OSX and AWS's K520 and for many people this is the only environments available.\nThanks!\n", "Not the nicest fix, but just comment out the the cuda compute version check at _gpu_device.c_ line [610](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.cc#L610) to [616](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.cc#L616), recompile, and amazon g2 GPU acceleration seems to works fine:\n\n![example](http://im.ezgif.com/tmp/ezgif-12713768.gif)\n", "For reference, here's my very primitive patch to work with Cuda 3.0: https://gist.github.com/infojunkie/cb6d1a4e8bf674c6e38e\n", "@infojunkie I applied your fix, but I got lots of nan's in the computation output:\n\n```\n$ bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu\n000006/000003 lambda =     -nan x = [0.000000 0.000000] y = [0.000000 0.000000]\n000004/000003 lambda = 2.000027 x = [79795.101562 -39896.468750] y = [159592.375000 -79795.101562]\n000005/000006 lambda = 2.000054 x = [39896.468750 -19947.152344] y = [79795.101562 -39896.468750]\n000001/000007 lambda =     -nan x = [0.000000 0.000000] y = [0.000000 0.000000]\n000002/000003 lambda =     -nan x = [0.000000 0.000000] y = [0.000000 0.000000]\n000009/000008 lambda =     -nan x = [0.000000 0.000000] y = [0.000000 0.000000]\n000004/000004 lambda =     -nan x = [0.000000 0.000000] y = [0.000000 0.000000]\n000001/000005 lambda =     -nan x = [0.000000 0.000000] y = [0.000000 0.000000]\n000006/000007 lambda =     -nan x = [0.000000 0.000000] y = [0.000000 0.000000]\n000003/000006 lambda =     -nan x = [0.000000 0.000000] y = [0.000000 0.000000]\n000006/000006 lambda =     -nan x = [0.000000 0.000000] y = [0.000000 0.000000]\n```\n", "@markusdr, this is very strange. Could you post the completely steps you build the binary? \n\nCould what GPU and OS are you running with? Are you using Cuda 7.0 and Cudnn 6.5 V2? \n", "Just +1 to fix this problem on AWS as soon as possible. We don't have any other GPU cards for our research.\n", "Hi, not sure if this is a separate issue but I'm trying to build with a CUDA 3.0 GPU (Geforce 660 Ti) and am getting many errors with --config=cuda. See the attached file below. It seems unrelated to the recommended changes above. I've noticed that it tries to compile a temporary compute_52.cpp1.ii file which would be the wrong version for my GPU.\n\nI'm on Ubuntu 15.10. I modified the host_config.h in the Cuda includes to remove the version check on gcc. I'm using Cuda 7.0 and cuDNN 6.5 v2 as recommended, although I have newer versions installed as well.\n\n[cuda_build_fail.txt](https://github.com/tensorflow/tensorflow/files/31955/cuda_build_fail.txt)\n", "Yes, I was using Cuda 7.0 and Cudnn 6.5 on an EC2 g2.2xlarge instance with this AIM:\ncuda_7 - ami-12fd8178\nubuntu 14.04, gcc 4.8, cuda 7.0, atlas, and opencv.\nTo build, I followed the instructions on tensorflow.org.\n", "It looks like we are seeing an API incompatibility between Compute Capability v3 and Compute Capability v3.5; post **infojunkie's** patch fix, I stumped onto this issue\n\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:643] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro K2100M, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/local_session.cc:45] Local session inter op parallelism threads: 8\nF tensorflow/stream_executor/cuda/cuda_blas.cc:229] Check failed: f != nullptr _could not find **cublasCreate_v2** in cuBLAS DSO_; dlerror: bazel-bin/tensorflow/cc/tutorials_example_trainer: undefined symbol: cublasCreate_v2\n\nI run on Ubuntu 15.04, gcc 4.9.2, CUDA Toolkit 7.5, cuDNN 6.5;\n\n+1 for having Compute Capability v3 Support \n", "is cublas installed? and where does it link to \nls -lah /usr/local/cuda/lib64/libcublas.so ?\n", "@allanzelener, what OS and GCC versions do you have? Your errors seem to come from incompatible C++ compilers.\n\nIt is recommended to use Ubuntu 14.04 and GCC 4.8 with TensorFlow. \n", "@vsrikarunyan, it is better to use CUDA Toolkit 7.0, as recommended. You can install an older CUDA Toolkit along with your newer toolkit. Just point TensorFlow \"configure\" and maybe LD_LIBRARY_PATH to the CUDA 7.0 when you run TensorFlow. \n", "@avostryakov, @infojunkie's early patch should work on AWS. \n\nhttps://gist.github.com/infojunkie/cb6d1a4e8bf674c6e38e\n\nAn official patch is working its way through the pipeline. It would expose a configuration option to let you choose your compute target. But underneath, it does similar changes. I've tried it on AWS g2, and find out once things would work, after I completely uninstall NVIDIA driver, and reinstall the latest GPU driver from NVIDIA. \n\nOnce again, the recommended setting on AWS at this point is the following. \nUbuntu 14.04, GCC 4.8, CUDA Toolkit 7.0 and CUDNN 6.5. For the last two, it is okay to install them without affecting your existing installation of other versions. Also the official recommended versions for the last two might change soon as well. \n", "I applied the same patch on a g2.2xlarge instance and got the same result as @markusdr... a bunch of nan's. \n", "@zheng-xq Yes, I'm on Ubuntu 15.10 and I was using GCC 5.2.1. The issue was the compiler. I couldn't figure out how to change the compiler with bazel but simply installing gcc-4.8 and using update-alternatives to change the symlinks in usr/bin seems to have worked. (More info: http://askubuntu.com/questions/26498/choose-gcc-and-g-version). Thanks for the help, I'll report back if I experience any further issues.\n", "I did get this to work on a g2.2xlarge instance and got the training example to run, and verified that the gpu was active using the nvidia-smi tool , but when running mnist's convolutional.py , it ran out of memory.  I suspect this just has to do with the batch size and the fact that the aws gpus don't have a lot of memory, but just wanted to throw that out there to make sure it sounds correct.  To clarify, I ran the following, and it ran for like 15 minutes , and then ran out of memory.  \n\npython tensorflow/models/image/mnist/convolutional.py\n", "@nbenhaim, just what did you have to do to get it to work?\n", "@markusdr, @jbencook, the NAN is quite troubling. I ran the same thing myself, and didn't have any problem. \n\nIf you use the recommended software setting: Ubuntu 14.04, GCC 4.8, Cuda 7.0 and Cudnn 6.5, then my next guess is the Cuda driver. Could you uninstall and reinstall the latest Cuda driver. \n\nThis is the sequence I tried on AWS, your mileage may vary:\n\nsudo apt-get remove --purge \"nvidia*\"\nwget http://us.download.nvidia.com/XFree86/Linux-x86_64/352.55/NVIDIA-Linux-x86_64-352.55.run\nsudo ./NVIDIA-Linux-x86_64-352.55.run --accept-license --no-x-check --no-recursion\n", "Thanks for following up @zheng-xq - I'll give that a shot today.\n", "Another +1 for supporting pre-3.5 GPUs, as someone else whose only realistic option for training on real data is AWS GPU instances.\n\nEven for local testing, turns out my (recent, developer) laptop's GPU doesn't support 3.5 :-(\n", "@anjishnu I just followed @infojunkie 's patch https://gist.github.com/infojunkie/cb6d1a4e8bf674c6e38e after doing a clean install and build by following the directions.\n\nA few comments - The AMI I was using had the NVIDIA cuda toolkit 6.5 installed, so when I followed the link in the tensorflow getting started guide, I downloaded the 7.0 .run file for ubuntu 14.04 , upgraded the driver, and installed cuda 7.0 into /usr/local/cuda-7.0 without creating a symlink to /usr/local/cuda since I already had 6.5 installed and didn't wanna kill it\n\nThen, when building I just specified the right location of cuda 7.0.  One confusing thing is that when builting the python library, the tutorial doesn't remind you to specify --config=cuda , but you have to do that if you want the python lib to utilize gpu\n", "@markusdr, @jbencook, I got an NaN and all kinds of messed up values as well when I applied the patch initially, but what fixed it was doing a \"bazel clean\" and rebuilding from scratch after making the proposed changes outlined in @infojunkie 's patch.  Did you try this?\n", "Interesing... no I haven't had a chance yet. Did you try running the CNN from the Getting Started guide?\n\n```\npython tensorflow/models/image/mnist/convolutional.py\n```\n\nCurious to hear if that worked correctly.\n", "@jbencook  as I mentioned , convolutional.py seems to run correctly, but after like 15 minutes it crashes due to out of memory, but the output looks correct and I used nvidia-smi's tool to verify that it's actually running on the GPU and it is.  I suspect that this is because the batch size ... i know that the gpus on ec2 don't have that much memory, but I'm really unsure at this moment why it ran out of memory\n", "The convolutional.py example ran out of GPU memory for me too, on a GeForce GTX 780 Ti.\n", "I was able to install it on AWS after lots of pain. See https://gist.github.com/erikbern/78ba519b97b440e10640 \u2013\u00a0I also built an AMI: ami-cf5028a5 (in Virginia region)\n\nIt works on g2.2xlarge and g2.8xlarge and it detects the devices correctly (1 and 4 respectively). However I'm not seeing any speedup from the 4 GPU cards on the g2.8xlarge. Both machines process about 330 examples/sec running the CIFAR 10 example with multiple GPUs. Also very similar performance on the MNIST convolutional example. It also crashes after about 15 minutes with \"Out of GPU memory, see memory state dump above\" as some other people mentioned above\n\nI've run the CIFAR example for about an hour and it seems to chug along quite well so far\n", "As for building for Cuda 3.0 device, if you sync the latest TensorFlow code, you can do the following. The official documentation will update soon. But this is what it looks like: \n\n$ TF_UNOFFICIAL_SETTING=1 ./configure\n\n... Same as the official settings above\n\nWARNING: You are configuring unofficial settings in TensorFlow. Because some\nexternal libraries are not backward compatible, these settings are largely\nuntested and unsupported.\n\nPlease specify a list of comma-separated Cuda compute capabilities you want to \nbuild with. You can find the compute capability of your device at: \nhttps://developer.nvidia.com/cuda-gpus. \nPlease note that each additional compute capability significantly increases \nyour build time and binary size. [Default is: \"3.5,5.2\"]: 3.0\n\nSetting up Cuda include\nSetting up Cuda lib64\nSetting up Cuda bin\nSetting up Cuda nvvm\nConfiguration finished\n", "@nbenhaim @markusdr \n\nOut of memory issue may be due to fact that `convolutional.py` runs evaluation on the whole test dataset (10000) examples. It happens after training is finished, as the last step:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/mnist/convolutional.py#L266\n\nCan you try slicing `train_data` and `test_labels` to make is smaller?\n", "I can confirm that with @erikbern's install script and the latest TensorFlow master branch the `cifar10_multi_gpu_train.py` works as expected on the GPU:\n\n```\nstep 100, loss = 4.49 (330.8 examples/sec; 0.387 sec/batch)\n```\n\nAlthough [this line](https://gist.github.com/erikbern/78ba519b97b440e10640#file-install-tensorflow-sh-L64) now breaks because of the code changes.\n\nAlso if I take 1000 test samples the `convolutional.py` example works too.\n\nEDIT: The `bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu` example also works without giving me a bunch of nan's.\n", "I confirm that the latest build supports specifying the compute capability via \n`$ TF_UNOFFICIAL_SETTING=1 ./configure` \nwithout need for a patch. Thanks!\n\nI think this issue can be closed, unless someone encounters an actual function that fails for Cuda < 3.5.\n", "Actually, let me take that back :-) The `./configure` script modifies the source code by changing the relevant lines with the hand-specified Cuda versions. Then git reports uncommitted changes and it becomes very difficult to work with this codebase without reverting the change, `git pull`ing, and configuring again, not to mention submitting contributions.\n\nA better approach would be to read those version settings from a config file.\n", "ErikBern above and his AMI is working for cifar for me - ami-cf5028a5 \n\nGetting ~320 samples per sec versus my i7 windows box on docker which gets ~105 samples per second for  cifar10_train.py\n", "@infojunkie: yes, this isn't ideal (@zheng-xq and I discussed this a bit during the review!).\n\nWe'll try to think of a better way to handle this, though we would like to keep the ability for the runtime device filtering to be in sync with the way the binary was built (hence needing to edit the source code for both compile and runtime).  Otherwise users get hard-to-debug errors.\n\nWe'll continue to work on making this easier, but hopefully this allows some forward progress for you.\n", "@vrv: yes, I can definitely continue my work with these fixes. Thanks for the support!\n", "Just curious, as c4.4xlarge with 16 vCpus is about .88 per hour versus the gpu instance which is .65 per hour, wouldn't that be better to use multiple cpu than gpu?\n", "@timshephard I doubt it, but feel free to run some benchmarks\u00a0\u2013\u00a0you can install my AMI (ami-cf5028a5) on a c4.4xlarge and run cifar10_train.py\n", "Actually, the g2.2xlarge has 8 cpus alongside the GPU.  Going to try that.\n", "multi threaded CPU is supported , but if you want to do any real training,\nGPU 4 Life, until they release the distributed implementation\n\nOn Thu, Nov 12, 2015 at 4:53 PM, Erik Bernhardsson <notifications@github.com\n\n> wrote:\n> \n> @timshephard https://github.com/timshephard I doubt it, but feel free\n> to run some benchmarks \u2013 you can install my AMI (ami-cf5028a5) on a\n> c4.4xlarge and run cifar10_train.py\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/25#issuecomment-156274039\n> .\n", "I was only getting a 3x speed up for amazon GPU over my windows CPU on docker.  Nice, but that was only 1 of my cores.  All 4 cores on my windows box could probably beat an amazon GPU.\n", "that's interesting, because with caffe , I didn't do any actual benchmarks,\nbut training in CPU mode is horrible, like order of magnitude or more\ndifference.  Maybe TF is optimized better in CPU mode - wouldnt surprise\nme.\n\nOn Thu, Nov 12, 2015 at 5:01 PM, timshephard notifications@github.com\nwrote:\n\n> I was only getting a 3x speed up for amazon GPU over my windows CPU on\n> docker. Nice, but that was only 1 of my cores. All for 4 cores on my\n> windows box could probably beat an amazon GPU.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/25#issuecomment-156275410\n> .\n", "Please bear in mind that the cifar10 tutorial as it is is not meant to be a benchmark. It is meant to show-case a few different features, such as saver and summary. In its current form, it will be CPU-limited, even with GPU. To benchmark, one will have to be more careful and only use essential features.\n", "Could be just amazon GPUs are slow for some reason https://www.reddit.com/r/MachineLearning/comments/305me5/slow_gpu_performance_on_amazon_g22xlarge/\nInteresting report: \"A g2.2xlarge is a downclocked GK104 (797 MHz), that would make it 1/4 the speed of the recently released TitanX and 2.7x slower than a GTX 980.\"\n", "fwiw, getting 2015-11-13 00:38:05.472034: step 20, loss = 4.64 (362.5 examples/sec; 0.353 sec/batch)\nnow with 7 cpus and cifar10_multi_gpu_train.py.  I changed the all of the device references from gpu to cpu, if that makes sense.  \n\nok, weird. 2015-11-13 00:43:56.914273: step 10, loss = 4.65 (347.4 examples/sec; 0.368 sec/batch) and using 2 cpus, so clearly something failed here.   Must be using the GPU still.  Interesting that it processes a bit faster than single gpu version of the script.  \n", "even with erikbern's instructions I am still getting \n\nAssertionError: Model diverged with loss = NaN when I try cifar_train.py and this when running mnist/convolutional.py\n\nEpoch 1.63\nMinibatch loss: nan, learning rate: nan\nMinibatch error: 90.6%\nValidation error: 90.4%\nEpoch 1.75\nMinibatch loss: nan, learning rate: 0.000000\nMinibatch error: 92.2%\nValidation error: 90.4%\nEpoch 1.86\nMinibatch loss: nan, learning rate: 0.000000\n", "I got it to run on GPU on AWS, but like the others I am getting unimpressive speeds.\n", "I was able to get the convolutional.py example running without running out of memory after using the correct fix suggested by @zheng-xq of setting the option when running configure\n", "The install script provided by @erikbern no longer works as of commit 9c3043ff3bf31a6a81810b4ce9e87ef936f1f529\n\nThe most recent commit introduced this bug, @keveman already made a note on the commit here:\nhttps://github.com/tensorflow/tensorflow/commit/9c3043ff3bf31a6a81810b4ce9e87ef936f1f529#diff-1a60d717df0f558f55ec004e6af5c7deL25\n", "Hi! I have a problem with compilation of tensorflow with GTX 670. I run \n\n```\nTF_UNOFFICIAL_SETTING=1 ./configure\nbazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\n```\n\nI got error:\n\n```\nINFO: Found 1 target...\nINFO: From Compiling tensorflow/core/kernels/bias_op_gpu.cu.cc:\ntensorflow/core/kernels/bias_op_gpu.cu.cc(40): error: identifier \"__ldg\" is undefined\n          detected during:\n            instantiation of \"void tensorflow::functor::BiasOpCustomKernel(int, const T *, const T *, int, int, T *) [with T=float]\" \n(57): here\n            instantiation of \"void tensorflow::functor::Bias<tensorflow::GPUDevice, T, Dims>::operator()(const tensorflow::functor::Bias<tensorflow::GPUDevice, T, Dims>::Device &, tensorflow::TTypes<T, Dims, Eigen::DenseIndex>::ConstTensor, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<T, Dims, Eigen::DenseIndex>::Tensor) [with T=float, Dims=2]\" \n(69): here\n\ntensorflow/core/kernels/bias_op_gpu.cu.cc(40): error: identifier \"__ldg\" is undefined\n          detected during:\n            instantiation of \"void tensorflow::functor::BiasOpCustomKernel(int, const T *, const T *, int, int, T *) [with T=double]\" \n(57): here\n            instantiation of \"void tensorflow::functor::Bias<tensorflow::GPUDevice, T, Dims>::operator()(const tensorflow::functor::Bias<tensorflow::GPUDevice, T, Dims>::Device &, tensorflow::TTypes<T, Dims, Eigen::DenseIndex>::ConstTensor, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<T, Dims, Eigen::DenseIndex>::Tensor) [with T=double, Dims=2]\" \n(69): here\n\n2 errors detected in the compilation of \"/tmp/tmpxft_000067dd_00000000-7_bias_op_gpu.cu.cpp1.ii\".\nERROR: /home/piotr/tensorflow/tensorflow/tensorflow/core/BUILD:248:1: output 'tensorflow/core/_objs/gpu_kernels/tensorflow/core/kernels/bias_op_gpu.cu.o' was not created.\nERROR: /home/piotr/tensorflow/tensorflow/tensorflow/core/BUILD:248:1: not all outputs were created.\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\n```\n\nInformation about my card from NVIDIA samples deviceQuery:\n\n```\nDevice 0: \"GeForce GTX 670\"\n  CUDA Driver Version / Runtime Version          7.5 / 7.0\n  CUDA Capability Major/Minor version number:    3.0\n  Total amount of global memory:                 2046 MBytes (2145235968 bytes)\n  ( 7) Multiprocessors, (192) CUDA Cores/MP:     1344 CUDA Cores\n  GPU Max Clock rate:                            980 MHz (0.98 GHz)\n  Memory Clock rate:                             3004 Mhz\n  Memory Bus Width:                              256-bit\n  L2 Cache Size:                                 524288 bytes\n  Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)\n  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers\n  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers\n  Total amount of constant memory:               65536 bytes\n  Total amount of shared memory per block:       49152 bytes\n  Total number of registers available per block: 65536\n  Warp size:                                     32\n  Maximum number of threads per multiprocessor:  2048\n  Maximum number of threads per block:           1024\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n  Maximum memory pitch:                          2147483647 bytes\n  Texture alignment:                             512 bytes\n  Concurrent copy and kernel execution:          Yes with 1 copy engine(s)\n  Run time limit on kernels:                     Yes\n  Integrated GPU sharing Host Memory:            No\n  Support host page-locked memory mapping:       Yes\n  Alignment requirement for Surfaces:            Yes\n  Device has ECC support:                        Disabled\n  Device supports Unified Addressing (UVA):      Yes\n  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0\n  Compute Mode:\n     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 7.5, CUDA Runtime Version = 7.0, NumDevs = 1, Device0 = GeForce GTX 670\n```\n\nAny ideas why it is not working?\nThanks!\n", "the __ldg primitive only exists for 3.5+ I think.  We have an internal fix to support both that we'll try to push out soon.\n\nSee https://github.com/tensorflow/tensorflow/issues/320 for more details\n", "Thanks! Adding fix from #320 helped me, I can compile (with a lot of warnings) and execute \n\n```\nbazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu\n```\n\nWhen I run examples:\n\n```\ntensorflow/models/image/mnist$ python convolutional.py \n```\n\nI get warning that:\n\n```\nIgnoring gpu device (device: 0, name: GeForce GTX 670, pci bus id: 0000:01:00.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.\n```\n\nHow to enable GPU in examples from tensorflow/models/images?\n", "@erikbern\ndid you figure out multiple GPU issue on Amazon? I am also running CIFAR multiple GPU instance but see no speedup.\n\nHere is the GPU utilization status, it seems like all GPUs are in use but they do not do anything.\n\n+------------------------------------------------------+\n| NVIDIA-SMI 346.46     Driver Version: 346.46         |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GRID K520           Off  | 0000:00:03.0     Off |                  N/A |\n| N/A   54C    P0    55W / 125W |   3832MiB /  4095MiB |     37%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  GRID K520           Off  | 0000:00:04.0     Off |                  N/A |\n| N/A   42C    P0    42W / 125W |   3796MiB /  4095MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  GRID K520           Off  | 0000:00:05.0     Off |                  N/A |\n| N/A   46C    P0    43W / 125W |   3796MiB /  4095MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   3  GRID K520           Off  | 0000:00:06.0     Off |                  N/A |\n| N/A   43C    P0    41W / 125W |   3796MiB /  4095MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0     60160    C   python                                        3819MiB |\n|    1     60160    C   python                                        3783MiB |\n|    2     60160    C   python                                        3783MiB |\n|    3     60160    C   python                                        3783MiB |\n+-----------------------------------------------------------------------------+\n", "@mhejrati according to a comment on https://news.ycombinator.com/item?id=10555692 it seems like you can't do it in AWS:\n\n> Xen virtualization disables P2P copies ergo GPUs have what we call a \"failure to communicate and some GPUs you just can't reach (without going through the CPU that is).\"\n\nNot sure how trustworthy HN comments are, but that's all I know so far\n", "@erikbern @mhejrati I'm not so sure that specific property of Xen is a problem. P2P copies don't seem to be necessary as the cpu can still assign work to each GPU without GPUs needing to communicate to each other. It's still strange that all GPUs on the instance seem to be in this semi-utilized state but work proceeds without error.\n", "I'll close this bug. Please open a new one with a more specific title if some issues in here remain unresolved.\n", "Does it means that the last version of tensorflow works on Amazon g2 instances without any hacks? And Does it mean that it works more than one GPU there?\n", "I'm not sure whether we should call TF_UNOFFICIAL_\\* \"not a hack\", but yes, it _should_ work. If it doesn't, it's likely unrelated to Cuda 3.0 per se, and we should have a more specific bug.\n", "And is it possible to execute code on two or more GPUs on an amazon instance? For example, data parallelism for training a model like in CIFAR example. Several guys just 5 comments above this comment wrote that it was not possible.\n", "I don't know. But if that's still an issue with 0.6.0, it should be a bug, just a more specific one about multiple GPUs.\n", "I am using 0.6.0 on ubuntu, not able to use more than one GPUs. The GPU utilization on one GPU is always 0.\n", "Just for point of reference, renting a K40 or K80 is not actually prohibitively expensive.  Amazon doesn't have them, but several of the options on http://www.nvidia.com/object/gpu-cloud-computing-services.html do.  (Some for as low as like 3$/hr)\n", "Theano and Torch have no problem with compute 3.0 whatsoever. Can we expect TensorFlow to support compute 3.0 anytime soon?\n\nOr at least add the ability to override the restriction without having to recompile.\n", "@Dringite, you can enable Cuda 3.0 using the following: \n\nTF_UNOFFICIAL_SETTING=1 ./configure\n\nIt should be functional. And if it doesn't, feel free to file another issue to track it. \n", "The tensorflow install guide now includes a fix for cuda 3.0 as well\n\nOn Wed, Feb 10, 2016 at 2:37 PM, zheng-xq notifications@github.com wrote:\n\n> @Dringite https://github.com/Dringite, you can enable Cuda 3.0 using\n> the following:\n> \n> TF_UNOFFICIAL_SETTING=1 ./configure\n> \n> It should be functional. And if it doesn't, feel free to file another\n> issue to track it.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/25#issuecomment-182610763\n> .\n", "I think current guide does not work for gpu's - the test returns nan's as reported before.\nIn particular you still need to do this:\nTF_UNOFFICIAL_SETTING=1 ./configure\n", "I can't find the install guide including a fix for cuda 3.0, could someone point out for me? THX!\n", "printf \"\\ny\\n7.5\\n\\n\\n\\n3.0\\n\" | ./configure\n\n7.5 is the cuda version, 3.0 is the compute.\n", "Still no performance improvement for multiple GPUs at Amazon (CUDA=7.5, cudnn =4.0 ,compute = 3.0) comparing with single GPU.\n", "anyone succeed on Cuda compute capability 2.0?\n", "Verified that 'TF_UNOFFICIAL_SETTING=1 ./configure' works on a macbook pro with at GeForce GT 750M.  Thanks!\n", "Is there an ETA for the official fix? It's really a pain to maintain (e.g. build images with our own dockerfile) in production.\n", "My laptop gives me this log when I try to run mnist sample :\n\"Ignoring gpu device (device:0,name:GeForce GT 635M, pci bus id) with Cuda compute capability 2.1. The minimum required Cuda capability is 3.0 . \"\nSo does this mean that I can't use GPU version because the minimum Cuda for tensorflow is 3.0 ?\nThanks\n", "If you use the prebuilt binaries, yes. If you build from source you can\nbuild with Cuda 2.1 support but I don't know if that actually works. It's\nlikely that the effective minimum is cuda 3.0.\nOn Sat, Sep 10, 2016 at 11:51 Mojtaba Tabatabaie notifications@github.com\nwrote:\n\n> My laptop gives me this log when I try to run mnist sample :\n> \"Ignoring gpu device (device:0,name:GeForce GT 635M, pci bus id) with Cuda\n> compute capability 2.1. The minimum required Cuda capability is 3.0 . \"\n> So does this mean that I can't use GPU version because the minimum Cuda\n> for tensorflow is 3.0 ?\n> Thanks\n> \n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/25#issuecomment-246128896,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAjO_RvNrRMQEmsueXWoaU5FX4tWHZq3ks5qovwegaJpZM4Ge0kc\n> .\n", "@smtabatabaie Have you tried to build cuDNN from source as suggested by @martinwicke, I am facing exactly same issues as yours and it would help me a lot if you share your exprience?\n", "Some help please. I'm getting the same error message with \"Ignoring visible gpu device (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.\"\r\n\r\nI've read through the posts from others, the only issue is that this is a direct windows installation and not on AWS as I'm assuming most of the people here have. In the tensorflow website, it's stated that a minimum of 3.0 is required, why am I unable to use this? and how can I get around it? \r\n\r\nSuggestions on how to do this welcome please.", "@gunan @mrry are the windows packages not built with cuda 3.0? Is that why\nthey are so small?\n", "@martinwicke The nightlies are and rc1 should be too.", "nightlies yes.\r\nrc0 I think was 3.5.\r\nDid we cherrypick the change to use 3.0 to r0.12?", "We did cherrypick the change.\r\n@cydal you may use the nightly builds at here:\r\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-win/14/DEVICE=gpu,OS=windows/artifact/cmake_build/tf_python/dist/tensorflow_gpu-0.12.0rc0-cp35-cp35m-win_amd64.whl\r\n\r\nOr you can wait for 0.12.0rc1, which should be landing in a few days.", "Thanks guys for the quick response, I wasn't expecting one for a while at least. Sorry if this sounds like a bi of a dumb question, how do I install this? do I simply pip install it? (if so, do I removed the previous tensorflow gpu? or does it do so automatically?) or does it require downloading it and manually installing it in some way? consider me a bit of a newbie.", "The link points to a \"PIP package\".\r\nIf you used the `pip install` command, you should be able to use the same command with `--upgrade` flag.\r\nOr you can run `pip uninstall tensorflow` and then install the package listed above.\r\nOnce you give pip command the URL, it will automatically download and install.\r\n\r\nThis is all I can give with limited knowledge on your system, your python distribution, etc.\r\nConsider doing a google search for more details on how pip package installation works with your python distribution.", "Hi, I simply uninstalled the previous one and reinstalled and it works! Thank you so much, you saved me from buying a new laptop.", "Hi @gunan with the latest change for 3.5 compatibility, I get following log: \r\n```\r\n>>>> sess = tf.Session()\r\nI c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\r\n\\common_runtime\\gpu\\gpu_device.cc:885] Found device 0 with properties:\r\nname: Quadro K4100M\r\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.7055\r\npciBusID 0000:01:00.0\r\nTotal memory: 4.00GiB\r\nFree memory: 3.69GiB\r\nI c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\r\n\\common_runtime\\gpu\\gpu_device.cc:906] DMA: 0\r\nI c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\r\n\\common_runtime\\gpu\\gpu_device.cc:916] 0:   Y\r\nI c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\r\n\\common_runtime\\gpu\\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (d\r\nevice: 0, name: Quadro K4100M, pci bus id: 0000:01:00.0)\r\nE c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\r\n\\common_runtime\\gpu\\gpu_device.cc:586] Could not identify NUMA node of /job:loca\r\nlhost/replica:0/task:0/gpu:0, defaulting to 0.  Your kernel may not have been bu\r\nilt with NUMA support.\r\n```\r\nHow can I get around it? Suggestions on how to do this most welcome.", "@kay10 It looks like it worked. That error message on the last line is innocuous, and going to be removed in the release.", "As i see in this thread, everyone has a compatibility level 3. For those who has a compability of 2, is there any solution without compiling source code? \r\nI tried nightly build shared by @gunan and got the error:\r\n`tensorflow_gpu-0.12.0rc0-cp35-cp35m-win_amd64.whl is not a supported wheel on this platform.`\r\nit is not a linux wheel and i realised it a bit soon.\r\n\r\nCurrent situation on a 16.04 Ubuntu.\r\n`I tensorflow/core/common_runtime/gpu/gpu_device.cc:948] Ignoring visible gpu device (device: 0, name: GeForce GTX 590, pci bus id: 0000:03:00.0) with Cuda compute capability 2.0. The minimum required Cuda capability is 3.0.\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:948] Ignoring visible gpu device (device: 1, name: GeForce GTX 590, pci bus id: 0000:04:00.0) with Cuda compute capability 2.0. The minimum required Cuda capability is 3.0.\r\n`", "@batuhandayioglugil too many of our GPU kernels rely on functionality that is only available in in 3.0 and above, so unfortunately you will need a newer GPU.  You might also consider trying one of the cloud services.", "@vrv i came to this point after spending quite time on these issues and buying a new PSU so it costed me a lot. To avoid further waste of time, i want to ask a question: there are at least 15 deep learning library that i heard. Cuda and cuDNN was necessary for tensorflow. Is this situation (compute capability) special for cuda library? May i have any other chances? if not, i will give up right know and go on to work with CPU (Forgive my ignorence)", "I think it will be more trouble than it's worth trying to get your 2.0 card working -- it's possible your existing CPU might be as fast or faster than your specific GPU, and a lot less trouble to get started.  I do not know what other libraries require, unfortunately.", "is it already support GPU compute 3.0?    ", "yes.", "@martinwicke thank you for fast response. do I still have to build it from source, or just directly pip install it? Im on Arch linux and struggle to build it from source giving error with c compiler.", "I think it should work from binary.", "I have the same problem \uff1a\"Ignoring gpu device (device:0,name:GeForce GT 635M, pci bus id) with Cuda compute capability 2.1. The minimum required Cuda capability is 3.0 .\" .  @smtabatabaie @martinwicke @alphajatin. help !!!!", "Compute capability 2.1 is too low to run TensorFlow. You'll need a newer (or more powerful) graphics card to run TensorFlow on a GPU.", "The url of answer to the question is invalid. Can you update it?", "For nightly pip packages, recommended way to install is to use `pip install tf-nightly` command.\r\nci.tensorflow.org is deprecated."]}, {"number": 24, "title": "Problems running the image example (Python 2.7.10, PyEnv, Xubuntu 14.04 64bit)", "body": "JFYI, I'm getting the following error when trying to execute the models/image/mnist/convolutional.py example (Python 2.7.10 in PyEnv, on Xubuntu 14.04, 64bit):\n\n``` bash\n[samuel site-packages]$ pwd\n/home/samuel/.pyenv/versions/2.7.10/lib/python2.7/site-packages\n[samuel site-packages]$ python tensorflow/models/image/mnist/convolutional.py\nTraceback (most recent call last):\n  File \"tensorflow/models/image/mnist/convolutional.py\", line 12, in <module>\n    import tensorflow.python.platform\n  File \"/home/samuel/.pyenv/versions/2.7.10/lib/python2.7/site-packages/tensorflow/__init__.py\", line 4, in <module>\n    from tensorflow.python import *\n  File \"/home/samuel/.pyenv/versions/2.7.10/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 22, in <module>\n    from tensorflow.python.client.client_lib import *\n  File \"/home/samuel/.pyenv/versions/2.7.10/lib/python2.7/site-packages/tensorflow/python/client/client_lib.py\", line 35, in <module>\n    from tensorflow.python.client.session import InteractiveSession\n  File \"/home/samuel/.pyenv/versions/2.7.10/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 11, in <module>\n    from tensorflow.python import pywrap_tensorflow as tf_session\n  File \"/home/samuel/.pyenv/versions/2.7.10/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"/home/samuel/.pyenv/versions/2.7.10/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\nImportError: /home/samuel/.pyenv/versions/2.7.10/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: PyUnicodeUCS4_FromStringAndSize\n```\n\nWill see if I can figure out something, but thought to report in the meanwhile.\n", "comments": ["Ah, great description of the problem [here](http://effbot.org/pyfaq/when-importing-module-x-why-do-i-get-undefined-symbol-pyunicodeucs2.htm).\n", "For now, you can build your own pip package via instructions [here](http://tensorflow.org/get_started/os_setup.md); see \"Create the pip package and install\".  Let us know if that doesn't work.\n", "Please re-open if there's something left to diagnose here!\n", "I'm seeing what might be a related error:\n\n`_pywrap_tensorflow.so: undefined symbol: cudnnCreate`\n\nFull traceback\n\n``` bash\njvanderdoes@ubuntu:~/Code/tensorflow/tensorflow/models/image/imagenet$ python ./classify_image.py \nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcublas.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:93] Couldn't open CUDA library libcudnn.so.6.5. LD_LIBRARY_PATH: /usr/local/cuda-7.0/lib64\nI tensorflow/stream_executor/cuda/cuda_dnn.cc:1382] Unable to load cuDNN DSO\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcufft.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcurand.so.7.0 locally\nI tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 2\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:103] Found device 0 with properties: \nname: GeForce GTX 960\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.342\npciBusID 0000:03:00.0\nTotal memory: 4.00GiB\nFree memory: 3.91GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:127] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:137] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:702] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960, pci bus id: 0000:03:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Allocating 3.62GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:52] GPU 0 memory begins at 0x402780000 extends to 0x4e9f61000\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 16.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 32.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 64.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 128.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 256.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 512.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 16.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 32.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 64.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 128.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 256.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 512.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.00GiB\nI tensorflow/core/common_runtime/direct_session.cc:58] Direct session inter op parallelism threads: 2\nF tensorflow/stream_executor/cuda/cuda_dnn.cc:207] could not find cudnnCreate in cudnn DSO; dlerror: /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: cudnnCreate\n```\n", "My apologies, I was simply missing cudnn. :/\n", "@FreakTheMighty\r\nI have the same problem,but I don't understand what's your mean'My apologies, I was simply missing cudnn. :/' ,could express more clearly?"]}, {"number": 23, "title": "Distributed Version", "body": "Is there any distributed version of TensorFlow that could work on multiple machines?\n\n-Minjie\n", "comments": ["I think that's the point - Google hasn't open sourced \"scalable\" version :) \n", "Thanks for the question!  To reiterate what I said [here](https://github.com/tensorflow/tensorflow/issues/12#issuecomment-155150681), we are working on making a distributed implementation available, it's currently not in the initial release.  Please stay tuned, and take a look at the cifar multi-gpu tutorial for a flavor of how we handle multiple 'devices': http://tensorflow.org/tutorials/deep_cnn/index.md\n", "would appreciate any insight on the availability of the distributed version. Is the distributed code that is being worked on in github? That is one place where some of us who are interested can contribute\n", ":+1: \n", "Hello,\n\nAfter reading these plans and ideas, I'm somewhat surprised. According to http://static.googleusercontent.com/media/research.google.com/en//people/jeff/BayLearn2015.pdf, both data and model parallel are needed to train large and powerful models quickly. BTW, GPUs transferring data takes time as described in http://tensorflow.org/tutorials/deep_cnn/index.md. Then, how it's possible to efficiently support both model parallelism and heterogeneous multi-devices (of a single node) on distributed cluster? Could you please roughly explain how different it from DistBelief?\n\nThanks!\n", "P.S., GPU acceleration also could be limited by model partition strategies. \n", "Our current internal distributed extensions are somewhat entangled with Google internal infrastructure, which is why we released the single-machine version first. The code is not yet in GitHub, because it has dependencies on other parts of the Google code base at the moment, most of which have been trimmed, but there are some remaining ones.\n\nWe realize that distributed support is really important, and it's one of the top features we're prioritizing at the moment.\n", "Awesome. \n\nAfter reading the whitepaper, I just realized that large neural network model can be partitioned into sub-graphs by layer (horizontal partitioning) and executed in a serial way. \n\nOne thing not clear is the performance for fully connected network on multi node equipped with GPUs cluster ..\n", "In theory, something like [Dask](http://dask.pydata.org/en/latest/) could be layered on top for handling this - at least for the Python front-end.\n", "Any update on timeline? \n", "Dask looks interesting project but the drawback of the blocking algorithm is that it's not memory optimal. Since a large amount of memory is required for fully-connected layers, I was thought that Pregel-like model parallelism on CPUs w/ vertical partitioning is more attractive for fully connected layers (blocking mat-mult on GPU also appears to me slow and memory demanding). Of course, I maybe wrong but that's why I launched Apache Horn project recently. Since layers can be pipelined, I hope we can collaborate each projects in complementary way.\n", "I don't know if you can give us some anticipation on the framework choice but..\n\nhttps://github.com/cloudera/spark-dataflow\n\nhttps://spark-summit.org/east-2016/events/distributed-tensor-flow-on-spark-scaling-googles-deep-learning-library/\n", "@bhack I wonder what their Java/Scala interface looks like... \n", "We've started work on this using gRPC. We hopefully have something to show soon.\n", "Really in desperate need for the distribute version.. Dream to see it to be released early and contribute my efforts in this great project..\n", "@saudet /cc @ctn\n", "Any update on timeline? Can't wait any longer... @martinwicke \n", "Bump. I'm really glad this is your top priority. Thanks for all your amazing work so far.\n", "+1\n", "+1\n", "Sorry I completely missed the mention; have been heads down on various matters. This looked just like the other hundreds of commit notifications in my mailbox.\n\nYes we'll be talking about a distributed implementation of TensorFlow on Spark, pyspark in particular. Some pretty interesting results on scaling and GPU vs CPU etc. I'll see you there if you'll be in NYC for SparkSummit, else on live streaming.\n\nThe primary motivation is from the Spark perspective---e.g., easily adding another useful workload to an existing Spark deployment.\n\nFor distributed TensorFlow in the abstract, Google will release a distributed implementation \"soon\".\n\n## HTH.\n\nUpdate on the above (Distributed Tensorflow on Spark):\n- Slide deck: http://www.slideshare.net/arimoinc/distributed-tensorflow-scaling-googles-deep-learning-library-on-spark-58527889\n- Repo: https://github.com/adatao/tensorspark\n", "+1\n", "+1\n", "TensorFlow-serving released today, it seems networking in gRPC has been proved to be a mature solution, great news!\n", "From what i understand, tensorflow-serving takes only \"trained model\". So, for the question \"how to train model on multiple machine\", the answer isn't tensorflow-serving yet.\n", "Yeah of course tensorflow serving itself is not the answer. My point is that the core of distributed training is how to pass tensors (activations/weights/gradients) efficiently between machines, which is half done with a proved decent network framework as gRPC.\n", "Is there a (rough) estimate of how soon this'll happen? I know estimates like these are difficult to do, but it'd be really helpful to know whether \"soon\" is closer to a few weeks or a few months. :smile: \n", "+1\n", "In the meantime, if anyone wants to experiment there is a initial PR for Spark by @amplab at https://github.com/amplab/SparkNet/pull/91\n", "I just pushed an initial version of the distributed runtime, based on gRPC. Currently, `tensorflow/core/distributed_runtime` contains most of the C++ implementation needed to get started, and we're still working on the higher-level libraries that will use multiple processes. However, you can get started today by taking a look at the [readme](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/README.md).\n", "Awesome! I can't wait to try it!\n", "Cong! Great & impressive work!\n\n---\n\nwangyu5@jd.commailto:wangyu5@jd.com\n\n\u5728 2016\u5e742\u670826\u65e5\uff0c22:02\uff0cDerek Murray <notifications@github.com<mailto:notifications@github.com>> \u5199\u9053\uff1a\n\nI just pushed an initial version of the distributed runtime, based on gRPC. Currently, tensorflow/core/distributed_runtime contains most of the C++ implementation needed to get started, and we're still working on the higher-level libraries that will use multiple processes. However, you can get started today by taking a look at the readmehttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/README.md.\n\n\u2015\nReply to this email directly or view it on GitHubhttps://github.com/tensorflow/tensorflow/issues/23#issuecomment-189285409.\n", "Well done @mrry !\n", "Heh, looks like merging another code base can inadvertently close issues :). Reopening this, because there's still work to do.\n", "Thanks for re-opening it, @mrry .\n\nSince the initial effort already merged, would it be easier to track open tasks as individual issues?\n", "It appears there were two parallel approaches here:  (a) SparkNet  and (b) gRpc using C++. Is that correct?  And both are also currently active?\n", "In my opinion, spark+tensorflow is mainly for data parallel, and are useful for samll scale networks. gRPC and tensorflow's distributed support would be more helpful for large networks.\n", "@JinXinDeep    Would you please clarify? Spark allows access to large clusters - potentially for each machine having GPU or many CPU's. So why do you say for small scale networks?\n", "@javadba sorry for the confusion, with \"small networks\" I mean the deep neural networks can not have too many model parameters, otherwise the communication time between machines will exceed the computation time, which means speedup is not high.\n", "@JinXinDeep  Thx for the clarification. Are model parameter sizes really that large - as in many MB?\n", "E.g. in this paper arxiv.org/abs/1602.02410 we trained LSTMs that have 200M+ parameters and 3B+ total parameters in some cases.\n", "/cc @robertnishihara that probably could give you a better overview of SparkNet target.\n\n/cc @maxpumperla if interested in the thread\n", "Thanks @bhack, I was indeed missing out on the discussion.\n", "Thanks @bhack! Indeed SparkNet is designed for data parallel training or data parallel use cases like featurizing data with an already trained net.\n", "Pardon my ignorance but did anyone succeeded running Tensorflow on multiple machines?\n\nUnable to find how to run tensorflow on multiple machines.\n\nI was successful in starting a local server.\n\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:199] Initialize HostPortsGrpcChannelCache for job worker -> {localhost:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:203] Started server with target: grpc://localhost:2222\n\nI tried to give Ip for another machine instead of localhost but it still starts localhost only.\n\nPlease help.\n", "@kanwar2preet: Can you open another issue with details of (i) the TensorFlow program that you ran, and (ii) the configuration you used for the servers? Thanks!\n", "The docs for distribution seem out of date when I tried to do this. `tf.make_cluster_def` is now private (i.e. `_make_cluster_def`)? I put together a simple example for testing purposes. I had a few segfaults along the way when I mismatched things, eg: if you only have one worker specified and set `task_index=1` will SEGFAULT. \n\nHere is what I did:\nWorker Nodes setup:\n\n``` python\nimport tensorflow as tf\n\ncluster_spec = tf.ClusterSpec({\"worker\": [\"tensorflow-worker0:2222\",\n                                           \"tensorflow-worker1:2222\"],\n                                \"ps\": [\"tensorflow-master0:2222\"]})\nserver_def = tf.ServerDef(cluster=cluster_spec.as_cluster_def(),\n                          job_name=\"worker\",\n                          task_index=1,  # switch this to 0 for worker1\n                          protocol=\"grpc\")\nserver = tf.GrpcServer(server_def)\nserver.join()\n```\n\nps Node Setup:\n\n``` python\nimport tensorflow as tf\n\ncluster_spec = tf.ClusterSpec({\"worker\": [\"tensorflow-worker0:2222\",\n                                           \"tensorflow-worker1:2222\"],\n                                \"ps\": [\"tensorflow-master0:2222\"]})\nserver_def = tf.ServerDef(cluster=cluster_spec.as_cluster_def(),\n                          job_name=\"ps\",\n                          task_index=0,\n                          protocol=\"grpc\")\nserver = tf.GrpcServer(server_def)\nserver.join()\n```\n\nHow to deploy a job:\n\n``` python\nimport tensorflow as tf\n\nwith tf.device(\"/job:ps/task:0\"):\n    weights0 = tf.Variable(tf.random_normal(shape=[1024, 512]))\n    bias0 = tf.Variable(tf.zeros(shape=[512]))\n\nwith tf.device(\"/job:worker/task:1\"):\n    inputs = tf.random_normal(shape=[10, 1024])\n    l0 = tf.nn.relu(tf.matmul(inputs, weights0) + bias0)\n    l1 = tf.nn.relu(tf.matmul(l0, tf.transpose(weights0)))\n    loss = tf.nn.l2_loss(l1-inputs)\n    train_op = tf.train.AdamOptimizer().minimize(loss)\n\nwith tf.Session(\"grpc://tensorflow-master0:2222\") as sess:\n   for _ in range(1000):\n       sess.run(tf.initialize_all_variables())\n       _, l = sess.run([train_op, loss])\n       print l\n```\n\nRegarding deployment systems: it would be nice to have a simple ansible deployment for this.\n", "@jramapuram: Thanks for pointing this out. We have a fix to the docs coming later today. (Also, these interfaces are subject to a little bit of churn before they freeze in the next release, so please bear with us!)\n\nThe segfault is rather embarrassing, so we should fix that. Can you please create an issue with the exact code that reproduces it?\n\nRegarding Ansible, I don't have any experience with that platform, but we would be glad to accept contributions... feel free to open another issue to suggest that. (https://github.com/tensorflow/tensorflow/issues/1686 suggests adding support for Slurm, for example.)\n", "+1\n", "I just wanted to draw everyone's attention to https://github.com/tensorflow/tensorflow/commit/6d838744c43ccaaa821bfcdfb8e2b1d39fa21f45, which modifies the interface to some of the distributed runtime methods. In particular, `tf.GrpcServer` becomes `tf.train.Server`, and its constructor is more ergonomic, so you no longer need to construct a (now-renamed-to) `tf.train.ServerDef` proto to instantiate a server. The [docs in the repository](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/distributed/index.md) are now updated, but haven't yet made it onto the website.\n\nLet me know if there are any questions!\n", "Since 0.8 is now released, I think it's time to close this issue. Please create new issues for anything that arises with the distributed version, and thanks for all of your input!\n", "Great! Amazing work\n\nOn Fri, Apr 15, 2016 at 10:00 PM Derek Murray notifications@github.com\nwrote:\n\n> Since 0.8 is now released, I think it's time to close this issue. Please\n> create new issues for anything that arises with the distributed version,\n> and thanks for all of your input!\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/23#issuecomment-210596890\n", "@mrry  Thanks for the distributed version of TensorFlow since v0.8. In my opinion, TensorFlow is very flexible, it can do model parallel, data parallel, or mixed parallel, although the examples are for data parallel. \n\nFor example, for model parallel, typically there is a cluster consists of a number of distributed nodes (e.g. machines) for model training. If we use the **In-graph** mode, a main program can be used to define the tasks for all the nodes. To reduce communication overhead and to do model parallel efficiently, each node includes a parameter server (ps) task contains the sub-model parameters and a worker task that corresponding to the computations of the sub-model; each node contains a different sub-model, which is assigned by the main program, and these sub-models collectively form the whole model. \nEach ps task from each node will receive the computation results from at least one node\u2019s worker in the cluster. In model parallel, for example, for each node, its worker task typically do the compution corresponding to its ps task\u2019s sub-model; the ps task in each node will update its sub-model according to the training results it received.\n\nIs that right? Thanks!\n", "I am wondering how can I run Distributed Tensor Flow on Top of Spark \n"]}, {"number": 22, "title": "OpenCL support", "body": "I understand TensorFlow only supports CUDA. What would need to be done to add in OpenCL support?\n", "comments": ["It's strange that Google ditched open OpenCL for proprietary CUDA.\n![im-just-saying](https://cloud.githubusercontent.com/assets/1548848/11042379/c2cf01c6-873c-11e5-8216-a00474c8e717.jpg)\n", "At the very least, the [Eigen](http://eigen.tuxfamily.org) library would have to support OpenCL.\n", ":+1:\n", ":+1: \n", ":+1:\n", "thumbs up and all that.\n", "I will be interested in expanding Tensor Flow with OpenCL. As we have already released OpenCL caffe. https://github.com/amd/OpenCL-caffe.  Hopefully it can get integrated in light way? Is anyone interested in working together on this?\n", "@gujunli Nice to see AMD here. /cc @naibaf7 @lunochod\n", "would be great.\n", ":+1:\n", "/cc @lukeiwanski for Eigen/OpenCL/SYCL\n", "@gujunli Certainly would be interested in contributing. Please let me know when you plan to start. \n", "Hi all, \n\nHere at Codeplay we are looking into Eigen's tensor running on GPU using SYCL (a modern C++ layer on top of OpenCL). From what we have gathered so far, GPU tensor design is very closely coupled with CUDA and it will require interface changes for another programming model and particularly a SYCL and OpenCL 1.2 version. \n\nIf anyone is interested in digging deeper / helping out, we are most certainly interested in contributing.\n\nThanks,\nLuke\n", "@lukeiwanski Thank you for the feedback. I think that @benoitsteiner worked at the tensor extension part of eigen.\n", ":+1: I can help code some OpenCL/SYCL if someone makes a plan, divides work into tasks etc. I recommend using Boost.Compute as a wrapper for OpenCL (it makes running kernels, testing, templating easier).\n", "+1\n", ":+1: \n", "Hi all,\n\nJust to keep you posted, we are still investigating how we can change the Eigen interface to better fit the SYCL/OpenCL 1.2 programming model. \nOnce we come up with a reasonable approach that targets heterogeneous programming models ( not only OpenCL / SYCL )  we will create a proposal. \n\nThanks,\nLuke\n", "Pls keep me update. I developed opencl-caffe for AMD. I am also looking at\ntensor flow.\n\nThanks.\nJunlu\nOn Dec 8, 2015 10:19 AM, \"Luke Iwanski\" notifications@github.com wrote:\n\n> Hi all,\n> \n> Just to keep you posted, we are still investigating how we can change the\n> Eigen interface to better fit the SYCL/OpenCL 1.2 programming model.\n> Once we come up with a reasonable approach we will create a proposal.\n> \n> Thanks,\n> Luke\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/22#issuecomment-162967662\n> .\n", "/cc @ptillet @gongzg Is there any interest in this by Intel? I really hope that we don't fragment OPENCL here like in Caffe where we have an AMD fork, Intel unmerged PRs, another semi-unofficial AMD PR, and a long staging user PR (plus two old abandoned Opencl efforts). If somebody is interested in the history can take a look at https://github.com/BVLC/caffe/pull/2610 comments.\n", "@bhack We do have interest in this. Thanks for letting me know. If there is a proposal for Eigen's OpenCL/SYCL implementation, we will see what we can do from Intel side.\n", ":+1: \n", "An interesting initiative at https://github.com/ptillet/isaac also if here we rely on Eigen tensor extension.\n", "I also would like to contribute. @benoitsteiner can you organize it?\n", "This was included in the Roadmap but also tagged as contribution so a direction/bootstrap could be really useful.\n", "I can contribute to organize it. who is responsible for OpenCL support in\nTensor flow now?\n\nThanks a lot.\nJunli\n\nOn Tue, Jan 19, 2016 at 7:50 AM, bhack notifications@github.com wrote:\n\n> This was included in the Roadmap but also tagged as contribution so a\n> direction/bootstrap could be really useful.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/22#issuecomment-172894538\n> .\n\n## \n\n---\n\nJunli Gu--\u8c37\u4fca\u4e3d\nCoordinated Science Lab\nUniversity of Illinois at Urbana-Champaign\n\n---\n", "I just assumed Benoit because he self assigned the feature, but I think you've got it Junli! Maybe start with an email or forum thread of interested parties?\n", "@benoitsteiner knows more about interested parties that may not have shown\nup in this thread (or this issue). I'd wait for him to coordinate to make\nsure we avoid duplicating work.\n\nOn Tue, Jan 19, 2016 at 11:42 AM Dan McLaughlin notifications@github.com\nwrote:\n\n> I just assumed Benoit because he self assigned the feature, but I think\n> you've got it Junli! Maybe start with an email or forum thread of\n> interested parties?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/22#issuecomment-172963537\n> .\n", "I'm interested. Is there any roadmap?\n\n> On Jan 19, 2016, at 11:46 AM, Martin Wicke notifications@github.com wrote:\n> \n> @benoitsteiner knows more about interested parties that may not have shown\n> up in this thread (or this issue). I'd wait for him to coordinate to make\n> sure we avoid duplicating work.\n> \n> On Tue, Jan 19, 2016 at 11:42 AM Dan McLaughlin notifications@github.com\n> wrote:\n> \n> > I just assumed Benoit because he self assigned the feature, but I think\n> > you've got it Junli! Maybe start with an email or forum thread of\n> > interested parties?\n> > \n> > \u2014\n> > Reply to this email directly or view it on GitHub\n> > https://github.com/tensorflow/tensorflow/issues/22#issuecomment-172963537\n> > .\n> > \n> > \u2014\n> > Reply to this email directly or view it on GitHub.\n", "Is there a list of CUDA dependency libraries that Tensorflow relying on?\n\nThis would help to see if we could have immediate OpenCL alternatives.\n", "@hsaputra \nThere is clFFT, clBLAS (alternatively ViennaCL). Random number generator is a bit more tricky (no curand), either use a CPU generator and transfer to GPU or use another existing kernel for RNG.\n\nThe biggest pitfall will again be efficient convolution implementations (something like cuDNN).\n\nThere is experience about such issues here:\nhttps://github.com/BVLC/caffe/pull/2610\nhttps://github.com/BVLC/caffe/pull/2195\nhttps://github.com/amd/OpenCL-caffe\n", "Tensorflow use tensor extension upstreamed to Eigen. So I think that an Opencl/Sycl support to Eigen is needed. See [this thread](http://listengine.tuxfamily.org/lists.tuxfamily.org/eigen/2015/09/msg00005.html)\n", "Thanks @naibaf7. Yeah, I don't think there is a viable alternative for cuDNN for OpenCL right now. \n", "The website http://opencl.org is created to support open source porting projects just like these! We're currently installing all necessary tools at the website and have space for repositories at https://github.com/OpenCL/ - later on we're adding build-servers to test for several types of hardware and can provide our expertise in how to write code that runs at full speed on numerous hardware.\n\nWe're launching a porting initiative for GEGL next week, but we're happy to also support you.\n", "@bhack from that thread and here it seems like @lukeiwanski is looking into it. I think we have enough willing people to work on it, we just need @benoitsteiner, @lukeiwanski or @gujunli to coordinate. Benoit has been quiet, maybe he's on holiday.\n", "I would love to help contribute with this initiative.\n", "hi all,\n\nwe will coordinate the effort of porting Eigen\u2019s tensor module to SYCL for OpenCL as we already have something mostly working, but it\u2019s not ready for review yet.\n\nWe are in favour of this approach as it will introduce less invasion to the code base. SYCL supports the single-source C++ templated model that eigen already uses.\n\nRoad map design is in progress so it shouldn\u2019t be too long now.\n\nThanks,\nLuke\n", "@lukeiwanski Are you working or in contact with upstream? Do you think will be accepted upstream in Eigen?\n", "+1\n", "Great news @lukeiwanski, let us know of any help you need. \n\nI'll guess you are using your own implementation of SYCL - will that be available for developers/researchers? On what platforms?\n", "@lukeiwanski SYCL seems like the right way to go given the amount of template metaprogramming involved with Eigen. I'm an experienced c++ developer with OpenCL experience gained from developing my own [neural nets and linear algebra library](https://github.com/ville-k/vinn). I'd love to help with this effort and get started developing with SYCL.\n", "@bhack We are in contact with @benoitsteiner, but we will discuss our proposal with the upstream maintainers before we invest too much effort.\n\n@DanMcLaughlin , @ville-k We are developing our implementation of SYCL, ComputeCpp (https://www.codeplay.com/products/computecpp). For more information, can you please contact me off-list via the email address on my profile?\n", "@lukeiwanski is there any update/estimate regarding plans?\n", "+1.\nI've an AMD GPU and an Intel GPU in the laptop. I think both have OpenCL drivers and AMD's support seems to be much better. I'd have higher performance, because I've 2 OpenCL devices. I hope you make it scale with OpenCL devices.\n", "Hi all,\n\nThanks for the interest!\nAt this point we are getting our testing infrastructure set up to make sure that nothing that we do introduces regression.\nWe are in touch with @benoitsteiner to make sure we are in sync with what he's done so far. \n\nWe are still in compiling a road map for the integration process - it should be done in couple weeks time, as there is a couple of business details to clarify.\n\nOur goal is to bring the OpenCL to TensorFlow via Eigen by end of this year.\n\nThanks, \n", "interested. would love to contribute.\n", "Ok so actually seems that it is an effort of Codeplay with some kind of sync to Google internal. What are the role of AMD and Intel subscribers here?\n", "/cc @keryell if you have any interest on this from SYCL/FPGA universe\n", "My apologies for not contributing more to this discussion recently, my plate has been more than full these past 2 weeks.\n\nI'll be coordinating the OpenCL effort on the TensorFlow side. Our current thinking is:\n- TensorFlow relies on c++11 and has taken a \"single source\" approach, so SYCL seems like a great fit.\n- We don't have a lot of OpenCL experience in house, so we're collaborating closely with Codeplay to bridge this gap. In particular, Codeplay is currently leading the effort to add support for SYCL to the Eigen tensor library.\n- TensorFlow relies on the cuDNN library to compute convolutions on NVidia GPUs. If somebody is interested in contributing an OpenCL equivalent, we'd be happy to help.\n\nIn order to help structure the effort, I created a mailing list: tensorflow-opencl@googlegroups.com.\n", "@bhack sure I have some interest for high-end C++ on FPGA :-)\nTensorFlow sounds like a good validation use-case for triSYCL too.\nBy the way, if some people here are looking for some internships on this subject, I have some positions. It looks like Codeplay is looking for some people too, if I trust their web site.\n", "I'm really interested in @karlrupp and @hughperkins opinions. I hope they want to join in the discussion on the new google  group. \n", "@benoitsteiner Thank you for the update. It would be wonderful if all involved partners in @KhronosGroup (Google, Nvidia, Amd, Intel, Codeplay, Xilinx etc.) will promote a cudnn like API in a standardized way. A sort of Khronos [openvx](http://www.gipsa-lab.grenoble-inp.fr/thematic-school/gpu2015/presentations/GIPSA-Lab-GPU2015-T-Lepley.pdf) computer vision standardization effort but for deep learning.\n", "@bhack Which new Google group?\n\nOther than that, OpenCL and CUDA are too different programming approaches. CUDA works the way it is because one company has full control over everything, so it can embed binary blobs and who knows what in the final executable. This cannot be done with OpenCL, unless one goes down the SyCL path (I have my concerns...) and the SyCL compiler vendor has full control over all possible target architectures (unlikely or impossible in practice). Overall, my opinion is that a good OpenCL-enabled library needs more than just a few tweaks here and there. Probably not what you wanted to hear, but you asked for my opinion :-)\n", "@karlrupp  See https://github.com/tensorflow/tensorflow/issues/22#issuecomment-176406416 at the end for the google group.\nI asked your opinion cause you have a great experience with ViennaCL interfacing an algebra library with multiple backends (CPU, GPU, MIC). Tensorflow rely on Eigein library and its new tensor extension contributed by Google upstream (but only with CUDA backend). I think that they don't experienced much all the pitfall you have already encountered with ViennaCL in this years of development.\n", "@bhack We are currently at the face-to-face meeting in Seattle this week but of course I cannot say whether we are talking about DNN libraries or not... :-)\n", "@keryell Try to push the cause in Seattle ;)\n", "@karlrupp You are right, OpenCL and CUDA are too different programming approaches. The single-source aspect found for example in CUDA and OpenMP 4.5 is extremely powerful from a software engineering perspective. This is why there is this SYCL standard for the real C++ programmers. SYCL can be seen as CUDA on steroids without any language extension and with some OpenMP aspects (the tasks). A typical SYCL device compiler is expected to generate SPIR-V kernels.\n\nYour concerns about portability are less an issue with the SPIR-V standard (kind of portable equivalent of nVidia PTX/AMDIL/... in the Vulkan & OpenCL world) which is mandatory to accept in OpenCL 2.1 and Vulkan. So the beauty is that if you have a front-end that generates SPIR-V, you do not need special knowledge of the very details of the hardware to run on. There is a Khronos open-source bidirectional translator between LLVM IR and SPIR-V, so it opens quite new territories.\n", "@keryell I agree that SPIR-V is a step forward. However, it does not address all issues of exhaustive jitting.\n\n> you do not need special knowledge of the very details of the hardware to run on\n\nIs this a copy&paste from OpenCL 1.0 marketing, which claimed exactly the same? You will _always_ need to go down to the details of the underlying hardware if you aim for maximum performance. This is especially the case in the context of fast tensor contractions.\n", "...as  @scott-gray demonstrated with [neon](https://github.com/soumith/convnet-benchmarks/blob/master/README.md)\n", "@karlrupp\n\n> Is this a copy&paste from OpenCL 1.0 marketing, which claimed exactly the same?\n\nHaha. :-)\n\n> You will always need to go down to the details of the underlying hardware if you aim for maximum performance. This is especially the case in the context of fast tensor contractions.\n\nOf course, but before playing with the second-order optimization, it is useful to have the huge part of the whole templated C++ code running in some accelerated way.\n\nFor the optimization, either you stitch your optimized binary kernels \u00e0 la NervanaSys or, since SYCL is pure C++, you can use asm(\"...\") in it with a lot of #ifdef to test the target architecture. :-) That said, SPIR-V is itself extensible and I cannot see why we could not put inline VHDL or Verilog in it at some point. :-)\n\nBut more concretely, the recent introduction of sub-group operations should help to achieve good performance in a portable way and using simple built-in ad-hoc functions may help.\n\nC++ adds interesting metaprogramming features that allows to replace most of the code generators used such as in clBLAS or other frameworks to generate code more adapted to X or Y hardware.\n", "Also N4355 in c++17 could enter in the game soon or later\n", "@karlrupp, @bhack The tensorflow approach is to rely on a hardware abstraction (the tensor module) for the majority of the operations needed in by a typical neural network, while relying on specialized libraries (such as cudnn) for the few operations that are really critical performance wise. The hardware abstraction enables us to implement most TensorFlow operations once and have them run on an accelerator with more than good enough performance.\n", "@bhack Yes I love multidimensional arrays. Also in our domain of interest, there is the SG14 in the C++ committee that tries to have all the people interested in these issues to converge into **the** standard.\nhttps://groups.google.com/a/isocpp.org/forum/#!forum/sg14\nOf course SYCL is in the discussions. :-)\n", "@benoitsteiner Mainly on cudnn for pooling and convolution. I think that if every vendor will produce an API with its own hardware for this operations with its own binary assembly will not be a so scalable approach. That is why I think some performance crucial API calls would be better to be standardized in some way.\n", "@keryell There are really interesting topics for Matrix/Tensor  in the new SG14 c++ specially in vector/SIMD calls agenda. But seems that nobody talked  of convolution, pooling, and others useful \"stabilized\" deep learning interfaces. Also seems to me that in this specific standardization subgroups there are people from Nvidia, Intel, Amd, CodePlay etc.. but not from Google also if it is in others groups.\n", ":+1:\n", "@bhack Yes there is no machine-learning style proposal in SG14 yet. But participation is open, so you can send some proposals. :-) But perhaps SG6 (numerics topics) is more relevant. I do not think they have their own mailing-list/forum yet.\n", "@gujunli Does OpenCL Caffe run on Android? Sorry for asking this here but I didn't find anywhere else to ask it :) Would be great with a deep learning library that ran on Android devices _and_ could use the GPU but it seems like there are no at the moment. (Correct me if I'm wrong!)\n", "@krikru \nThe official (but experimental) OpenCL Caffe branch can be made to run on Android GPUs, however the performance at the moment is far from optimal. See https://github.com/sh1r0/caffe-android-lib/issues/23 and https://github.com/BVLC/caffe/tree/opencl.\n", "A real alternative to cudnn could be the extension of [OpenVx standard objects](https://www.khronos.org/registry/vx/specs/1.0/html/d0/da0/group__group__basic__objects.html) with support to Tensor, NdConvolution, NdPooling operators and (probably) some other operator that could be considered standardizable.\nAlso cudnn team need to make some choice on what new API and operators they will introduce in every release. Of course a standard can not move as fast as cudnn releases but I think some operations and objects has enough \"citations history\" to be standardized.\n", "@hughperkins At the moment, I haven't tried any deep learning library; I'm just doing some scouting to see which library I could potentially use. Have you tried cltorch and DeepCL on Android? I just assumed cltorch did work on Android, since there is an implementation of Torch that is dedicated specifically for Android. And why would you have such an implementation if there already was one that both worked on Android _and_ used OpenCL, right? But maybe I should have known better.\n", "@hughperkins For some reason I imagined that [torch-android](https://github.com/soumith/torch-android) was an official Torch implementation for Android, meaning that no other Torch implementation (at least not official) was likely to run smoothly on Android, including cltorch. I don't know why I thought that, it of course doesn't make any sense.\n", "Well... Soumith kind of coordinates torch development.  He works at Facebook AI Research.  So, since torch-android repo belongs to Soumith, I would say it's fairly close to official.  But it maybe is not part of core for some reason.  I guess you can ask the question as an issue in that repo, or in https://groups.google.com/forum/#!forum/torch7  Actually, since Soumith is kind of the main person that handles the requests in https://groups.google.com/forum/#!forum/torch7 , I reckon you probably want to post your question there.\n", "> meaning that no other Torch implementation (at least not official) was likely to run smoothly on Android, including cltorch\n\nNote that cltorch is not an implementatino of torch.  It's a plugin, thta provides OpenCL.  You need both.\n", "> Note that cltorch is not an implementatino of torch. It's a plugin, thta provides OpenCL. You need both.\n\nAh, thanks for the clarification.\n", "@naibaf7 Do the [OpenCL Caffe](https://github.com/BVLC/caffe/tree/opencl) branch and the [OpenCL Caffe](https://github.com/amd/OpenCL-caffe) implementation by AMD have anything more in common besides the name? Have you compared the two or do you know if there is any difference in performance? You write that the OpenCL branch is far from optimal performance. What does that mean and what would be necessary in order to improve it? It would be interesting to try it on Android.\n", "We are going off topic\n", "@bhack Yeah, sorry for hijacking this thread. I just didn't know where to ask the question.\n", "@krikru \nplease raise an issue about it on the Caffe branch, flag it with Android and OpenCL. Then we can discuss this further. Thanks.\n", "@keryell Seems that the next f2f SG14 meeting in [March will be hosted by Google](https://groups.google.com/a/isocpp.org/forum/m/#!topic/sg14/qnbWDK9t0gY). Will be any tensorflow internal there?\n", "/cc @jfbastien\n", "Perhaps @benoitsteiner could drop by, since he is local.\nBut before this event there is the full C++ F2F at the end of the month in Jacksonville, Florida.\nhttps://isocpp.org/files/papers/N4568.pdf\nUnfortunately I will not be able to attend any of them.\n", "I don't know if CppCon 2015 talk [C++ Multi-dimensional Arrays for Computational Physics and Applied Mathematics](https://youtu.be/CPPX4kwqh80) generated some paper follow-up.\n", "+1\n", "@bhack Thank you for pointing the talk on multi-dimensional arrays. It is interesting and address the real issues but looks too ad-hoc to be ratified in C++ as is. Personally I use Boost.MultiArray and I am more confident in a polished version of Boost.MultiArray.\n", "There are also some [papers at WG21](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/). As you can see @jfbastien at Google has  some activity at WG21 and also helped to host the SG14 f2f meeting at Google in March.\n", "@bhack @keryell I think it would be worth taking this discussion to [the SG14 mailing list](https://groups.google.com/a/isocpp.org/forum/#!forum/sg14) as the details aren't related to OpenCL / tensorflow.\n", "Yes probably it is no more so strictly confined here with all the details. Other than Eigen/sycl support Is there a plan for the cudnn calls? \n", "+1 very interesting topic. Hope it coming soon.\n", "This thread is very interesting. I've been trying to get caffe to work on android. The results seem to be surprising: caffe running with Mali gpu seems to be 2-3 slower than cpu, but about 4-5x more energy efficient. The test was run on Galaxy S6 (Mali T760, Peak Performance 200 GFlops). \n\nSince GEMM is the core of convolution in caffe, I decided to profile its performance on Android. It seems that ViennaCL is not as efficient as some simple kernels. Now I am able to get GPU run as fast as CPU for large matrices (2k x 2k). This is still counter-intuitive, since normally we expect GPUs to be much faster.\n\nSee: \n[https://github.com/strin/mocha-profile](https://github.com/strin/mocha-profile)\n\nThe kernel implementations can be found here:\n\nOpenCL kernels for GEMM: [https://github.com/strin/gemm-android](https://github.com/strin/gemm-android)\n\nAny thoughts?\n", "@strin Have you already followed this thread https://community.arm.com/thread/4935?\n", "@bhack thanks for sharing. this thread looks very interesting. i tried to turn of the DVFS as suggested, but no significant performance was seen for sgemm in ViennaCL. \n", "+1\n", "@strin Have you tried the last sgemm version in the MALI SDK?\n", "Tensorflow is latee ! ahah\nhttps://gist.github.com/jarutis/ff28bca8cfb9ce0c8b1a\n", "This will have an impact on the strategy: http://lists.llvm.org/pipermail/llvm-dev/2016-March/096576.html?\nEDIT: \n\"StreamExecutor is currently used as the runtime for the vast majority of Google's internal GPGPU applications, and a snapshot of it is included in the open-source TensorFlow_ project, where it serves as the GPGPU runtime.\"\n", "+1\n\nHope people working on it manage to overcome the CUDNN alternative problem by the time tensorflow gets close to 1.0\n", "@martinwicke why is this issues closed ?\n\nI don't think your commit fixes this.\n", "You can't always use the same commit comments in different repository ;) https://github.com/tensorflow/skflow/issues/22\n", "Oh GitHub\n", "@vrv Now that you have hyper notified us can you give some feedback on stream executor strategy? ;)\n", "I'm just going to blame GitHub for everything, including lack of OpenCL support. ;)\n\n@benoitsteiner might be able to comment more though.  I don't really know what you mean by 'stream executor' strategy.  We currently use a version of stream executor and CuDNN and Eigen and they all play well together, so I'm not sure how any plans have changed for the OpenCL side of things\n", "I mean:\n\"What is StreamExecutor?\n ======================== \n**StreamExecutor** is a unified wrapper around the **CUDA** and **OpenCL** host-side programming models (runtimes). It lets host code target either CUDA or OpenCL devices with identically-functioning data-parallel kernels.\"\n", "Canned operations\n ================== \nStreamExecutor provides several predefined kernels for common data-parallel operations. \nThe supported classes of operations are:\n- BLAS: basic linear algebra subprograms, \n- DNN: deep neural networks, \n- FFT: fast Fourier transforms, and \n- RNG: random number generation.\n", "@keryell Hi, I also have interest in implementing TensorFlow on FPGA, using high level programming languages like Xilinx C++ or OpenCL. I am with pleasure to contribute if you have some plan.\n", "@henline Can you explain what will be the role of StreamExecutor on Opencl and of relevant Canned\n operations for Tensorflow. I still cannot see how this will integrate with SyCL plans on Eigen and cudnn (replacement?)\n", ":+1: I would like to contribute to this, too.\n", "@bhack StreamExecutor provides functionality equivalent to that of the CUDA runtime and some of the CUDA libraries (such as cublas or cudnn). However you still need to write your GPU kernels, which is what we use Eigen for.\n", "@benoitsteiner So is it still needed to write two kernels, one for CUDA and one for Opencl?\n", "@benoitsteiner So don't you still have a tensorflow/tensorflow/stream_executor/opencl/ counterpart internally? What about \"Canned operators\"?\n", "@bhack  Eigen enables you to write an expression that describes the computation you want to perform once, and automatically generate a kernel (which we call the evaluator) to evaluate that expression on CPU and another kernel to evaluate the expression on a CUDA device. Once we have support for OpenCL in Eigen (we're getting close), it will be possible to also generate the OpenCL kernel automatically.\nFor a few TensorFlow operations that are performance critical (such as convolution), we use hand optimized kernels and/or third party libraries. In these cases, we'll need a good OpenCL implementation of these operations.\n", ":+1: \n", "Is there a plan to push more code in https://bitbucket.org/benoitsteiner/eigen-opencl? What about sycl compiler? Seems that there are no opensource GPU target implementatons released.\n", "@bhack @benoitsteiner \nI'm soon releasing a cuDNN replacement (only the convolution part, as it is the most performance and memory critical to have this done) for OpenCL on Caffe. Maybe it will also be to some use for the Tensorflow port.\n", "@bhack: Codeplay has made a lot of progress on the opencl front. Stay tuned for a big push to https://bitbucket.org/benoitsteiner/eigen-opencl in the next few weeks.\n", "@naibaf7: A fast implementation of the convolution operation would be extremely helpful in TensorFlow. Looking forward to it. \n", "@benoitsteiner [Nice catch, they got the chair](https://groups.google.com/a/isocpp.org/forum/m/#!topic/sg14/jVeMTC70uTI)\n", "@benoitsteiner  How Can I simply remove cuda implementation? because '#ifdef GOOGLE_CUDA' is so complicated. It sometimes means CUDA, sometimes means GPU. \n", "Since this issue has made its way to the [roadmap](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/resources/roadmap.md) (see _Platforms_): do we roughly have an idea of when OpenCL support will hit TensorFlow? Like version 0.9 / 1.0? Q3/4 2016? Or is 2017 more realistic?\n", "@benoitsteiner  Is the eigen-opencl https://bitbucket.org/benoitsteiner/eigen-opencl ready enough to support an opencl tensor flow development ?\n\nDoes tensorflow depend only on Eigen tensors or are there any other dependencies of Eigen ? \n", "@NEELMCW Codeplay has just released partial support for OpenCL to Eigen Tensors. The code is available in this [bitbucket repository](https://bitbucket.org/benoitsteiner/opencl). For the most part, TensorFlow depends on Eigen tensors. There are additional dependencies on Eigen for the linear algebra operations, but we don't have to provide an OpenCL compatible implementation of these operations (at least not initially). Therefore we're in a very good position to start supporting OpenCL in TensorFlow.\n\nIf you are interested in contributing, I have started to track what needs to be done in [this spreadsheet](https://docs.google.com/spreadsheets/d/1YbHn7dAFPPG_PgTtgCJlWhMGorUPYsF681TsZ4Y4LP0/edit?usp=sharing)\n", "@benoitsteiner I am the author of a C++11 OpenCL BLAS library (https://github.com/CNugteren/CLBlast) and am currently implementing half-precision support there. I am happy to contribute on the BLAS/GEMM part of this project and/or modify CLBlast to fit your needs better.\n", "@CNugteren \nCLBlast is now also available in OpenCL-Caffe, have you seen that? :)\nDid you also have a chance to look at the libDNN convolutions?\n", "@naibaf7 I saw it, yes! :) I haven't looked at libDNN at all so far, but I am not sure what you mean exactly. I assume convolution is implemented using GEMM?\n", "@CNugteren \nYes, I just thought it would be nice if you could look over it and maybe give some improvement or tuning tips on libdnn.\n(https://github.com/naibaf7/caffe/blob/master/src/caffe/greentea/libdnn.cpp).\nIt is using GEMM, but implicit (not through a BLAS, only small GEMM's on a workgroup level) so that a higher level of parallelism is possible, as well as no intermediate buffer are necessary (to unroll the data into a GEMM scheme).\n", "Hey all,\n\n@benoitsteiner thanks for mentioning our push! Hope it will be useful! \n\nTo compile this code, you need a SYCL compiler. Currently, the only supported compiler is Codeplay's ComputeCpp, which is available via a Codeplay's evaluation program. ComputeCpp will be made available for free as a public open beta, later in 2016 and then released with a free version (the ComputeCpp Community Edition) in 2017. This will let anyone compile and develop TensorFlow on OpenCL devices, such as AMD or Intel GPUs and CPUs. \n\nbtw. shouldn't this issue have OpenCL label? :)\n\nThanks,\nLuke\n", "I really hope that could be compiled also with an opensource tool. @keryell how it is going with your new [Opencl branch](https://github.com/keryell/triSYCL/commits/opencl)\n", "@bhack It would be nice to see if it can work with triSYCL in CPU OpenMP host device mode first. But I do not have the bandwidth to enter the TensorFlow/Eigen build system rnow. :-( If someone wants to try, feel free to do so. :-)\n\nhttps://github.com/keryell/triSYCL/commits/opencl should allow to run OpenCL kernels soon in OpenCL interoperability mode, but not in the SYCL single source mode we all dream about because we do not have the Clang/LLVM outliner yet to extract the kernels from SYCL. But Khronos open-sourced recently the components from AMD & Intel to support OpenCL C++ 2.2 & SPIR-V that would be the basis of it. So it is \"just\" a question of time...\n", "Could someone provide estimates for when Tensorflow might be able to run with OpenCL (AMD GPUs)? And what the curve of performance/usability looks like over time? It's difficult to parse all the past information into actionable hardware buying information. :)\n\nThanks in advance!\n", "@djan92 \nI'd say give it a year until it's usable, unfortunately. It looks like it's going to be built on quite cutting-edge libraries and technologies, most of them not quite ready yet.\nI'm also only going to jump onboard as soon as the complete tool stack is available as OpenSource and not before.\n", "@naibaf7 \n\n> I'd say give it a year until it's usable, unfortunately. It looks like it's going to be built on quite cutting-edge libraries and technologies, most of them not quite ready yet.\n> I'm also only going to jump onboard as soon as the complete tool stack is available as OpenSource and not before.\n\nWhy not implement a CL version first while waiting for the SYCL port to be ready? I assume there are quite a few folks here willing to help out. One year just sounds too long.\n", "@djan92 \nYes, you are right, #22 is almost 8 months old and has over 100 posts! The information can get swamped!\n\n> Could someone provide estimates for when Tensorflow might be able to run with OpenCL (AMD GPUs)?\n\nTensorFlow uses the Eigen library for tensor computation (in the Tensor module). We have committed a partial implementation for OpenCL 1.2 using SYCL (https://bitbucket.org/benoitsteiner/opencl branch Codeplay). The reason we used SYCL for this work is that this section of TensorFlow uses C++ expression trees, which is possible with SYCL for OpenCL, but not possible with OpenCL C directly. Other components of TensorFlow, such as convolutions or BLAS, could use OpenCL C directly.\n\nCurrently, I am working on integrating ComputeCpp (Codeplay's SYCL compiler) into the bazel build system. This should be ready soon ( follow this repo: https://github.com/benoitsteiner/tensorflow-opencl/ ). After that is done, TensorFlow should be accelerated on systems that support OpenCL SPIR (such as AMD or Intel) with ComputeCpp. Further work will continue on accelerating more of TensorFlow, as well as supporting more OpenCL implementations and the triSYCL open-source SYCL. SYCL and OpenCL are multi-vendor, royalty-free open standards, so there are lots of platforms and devices that can be supported using this approach (not just AMD GPUs).\n\nThe ComputeCpp Community Edition compiler will be available for free later in 2016 (in beta form: full conformance will be released for free early 2017).\n\nThe work on accelerating the non-C++ parts of TensorFlow (e.g. BLAS and convolutions) could be done without SYCL and implemented separately. Different hardware vendors may have their own optimized libraries for these features which could aid acceleration. Or, we could use Eigen with C++ for these features. \n\n> And what the curve of performance/usability looks like over time?\n\nWe believe the performance will improve steadily. To accelerate on a wide variety of devices, we need to manage the data more efficiently, which is why there is a \"managed tensor\" item of work, so that data movement can be more efficiently managed between host and multiple devices. It is hard to predict how the performance will vary over a wide range of devices, right now. Currently, very little is accelerated, but we are putting the infrastructure is in place to allow open-standard acceleration in TensorFlow.\n", "@naibaf7\n\n> I'd say give it a year until it's usable, unfortunately.\n\nThe basic operations should be here very soon. We are putting the basic infrastructure in place within the code to support open standards-based acceleration. We believe that with community support, an accelerated and usable version will be ready in much less than a year.\n\n> I'm also only going to jump onboard as soon as the complete tool stack is available as OpenSource and not before.\n\nComputeCpp will be available publicly, for free, in 2016. The open-source triSYCL support should follow behind. Open-source OpenCL is already supported with pocl, Shamrock, Clover, Beignet. \n", "@robertwgh \nThe C++ tensor code in Eigen would not easily be portable to OpenCL C without SYCL, but there are other features that would work well on OpenCL C. Have a look at this spreadsheet: https://docs.google.com/spreadsheets/d/1YbHn7dAFPPG_PgTtgCJlWhMGorUPYsF681TsZ4Y4LP0/edit#gid=0 and fill free put your name down on the features that should use normal OpenCL C (such as BLAS and convolutions).\n\nWe are giving evaluation version for ComputeCpp before the public release. If you would like one, please drop me an email :)\n", "@lukeiwanski Great, thanks for the update. I hope you are right about getting it done fully-featured in less than a year.\n", "[Another step](http://lists.llvm.org/pipermail/llvm-dev/2016-June/101028.html) of Streamexecutor in LLVM\n", "any chance of getting acceleration on the rx 480?\n", "@benoitsteiner \nLibDNN standalone would be available for integration:\nhttps://github.com/naibaf7/libdnn\n", "Great to read this is being worked on. It would help if Beignet 2.0 were polished. Lots of potential with Skylake and Iris right now.\n", "A recent pull request was added at https://github.com/benoitsteiner/tensorflow-opencl/pull/1 if somebody want to take a look.\n", "The Imagination(GPU)'s OpenCL SDK needs NDA to get accessible, we only have the shared library.  Is it possible to run tensorflow based on this libs?\n", "@alephman\nYou don't need vendor specific header files to build any OpenCL program. Try cl.hpp from https://www.khronos.org/registry/cl/api/1.2/cl.hpp and opencl.h/cl.h from any other SDK. For example - I have at least 3 OpenCL platform and all it works with one shared /usr/include/CL/cl.h\n", "We don't yet support TensorFlow running on OpenCL. It is a work in progress. Currently we are working on AMD GPUs. PowerVR support should follow. If you want to contribute to the development you should contact us (Codeplay) directly. If you want to run TensorFlow on PowerVR you should wait for a little more progress.\n", "@inferrna   thanks,  it looks  similar the OpenGL that hides the implementation of vendor specific.\n\n@andrewrichards  I love to contribute the development, how to contact with you? \n", "The easiest is if you click on \"register your interest\" on our page here: https://www.codeplay.com/products/computecpp\nThat will get you into our developer program and we can work together on this @alephman\n", "If you want you can cotribute also to let to compile with an opensource alternative. See https://github.com/tensorflow/tensorflow/issues/22#issuecomment-221841173\n", "Hi everyone!\nVery happy to hear Tensorflow support is being extended outside Nvidia Cuda. I wonder if you also consider making it work on APUs like this: http://www.amd.com/en-us/products/processors/laptop-processors#sectionOne ?\n", "@kgocheva \nAPUs do support OpenCL for both the CPU and GPU part.\nThis should work pretty much out of the box when the OpenCL support is ready.\nMeanwhile, if you already have an APU and want to try out another ML framework, BVLC OpenCL Caffe already works.\n", "@naibaf7 Thanks for the clarification.  I'm looking at cost effective hardware/software combinations to locally run Tensorflow and will definitely follow the OpenCL development progress.\n", "@hughperkins \nYes can be an issue, but I think parts such as im2col/col2im and other convolution implementations could also be plugged in as external APIs if it's really an issue with the GCLA. This may also be better for the original authors of such work.\n", "@hughperkins We are working on bringing the OpenCL to the TensorFlow via the SYCL for OpenCL 1.2. \nPlease have a look at https://docs.google.com/spreadsheets/d/1YbHn7dAFPPG_PgTtgCJlWhMGorUPYsF681TsZ4Y4LP0/edit#gid=1625897530 for \"todos\" and progress.\nRecently we released a compiler for SYCL https://www.codeplay.com/products/computesuite/computecpp called ComputeCpp Comunity Edition. People can try it out!\nAs well, we are focusing on the Eigen library https://bitbucket.org/benoitsteiner/opencl/branch/ComputeCpp - getting it to the stage required by TensorFlow's MNIST - there are a couple things remaining.\nAs for constraints, the current ComputeCpp CE release has been tested for Intel (CPU, GPU) and AMD (CPU, GPU) as for the platforms we support Ubuntu 14.04 64bit and CentOS 64bit.\nComptueCpp is downloadable for free and can be used in commercial and open source projects.\nBecause we <3 open communities :)\n", "@lukeiwanski Sorry for discussing/asking this here in the thread, but I think it may be of interest to others as well: I understand that Codeplay is highly interested in the SYCL for OpenCL implementation and I already heard others being interested in this work of you, too. I read some post by a Movidius official for example. However, I would like to ask what Google's contribution to this really is? Since Movidius, besides AMD and others, are listed as Codeplay's partners I can understand that they encourage or even support SYCL for OpenCL, but as far as I am aware of it, Google is not your partner and has not contributed so far?!\n\nDo not get me wrong, I really like your work, but wouldn't it be a good idea to consolidate your efforts, pool the resources and try to work together with Google? To me it looks like many different parties are interested in OpenCL for TensorFlow, but a huge potential is not used, because these parties do not develop together?!\n\nI may be wrong and please apologize myself if this has been discussed sufficiently, but I am still unaware of any major attempts by Google (or other parties) to work together on this and, as a result, I am still unaware of how the community could help or support (like single individuals), either via direct contributions, testing or other things.\n", "@ascenator We at Google have been working closely with Luke and his Codeplay colleagues on this project for almost 12 months now. Codeplay's contribution to this effort has been tremendous, so we felt that we should let them take the lead when it comes down to communicating updates related to OpenCL. This is why you haven't heard much from us on the topic :)\n\nNow that the ComputeCpp compiler is broadly available, we are planning to merge the work that has been done so far. But first we want to put together a comprehensive test infrastructure to make sure that we don't destabilize the exiting codebase.\n\nWe welcome all contributions to this effort, so feel free to contact me if you want to help. We're especially interested in high performance OpenCL kernels for matrix multiplication and convolutions. Several candidates have been suggested, but we haven't started looking into the pros and cons of each one or how to integrate them.\n", "@benoitsteiner thank you very much for the clarification & sorry for my misinformation! This sounds very good & promising! I will definitely have a look at ComputeCpp then. I am really looking forward to OpenCL support for TensorFlow, because this offers a lot of new possiblities for robotics (which is the field where I am researching and using TensorFlow for deep learning applications). I will at least have a look at early releases and try to test / debug. We have some Intel Chips plus a number of ARM CPUs that are waiting for tests ;) \n", "@hughperkins... sorry but isn't this completely off topic here? I don't see how this is relevant in OpenCL TF?\n", "I'm more interested here to know if will be taken a tuning approach to matrix multiplication and convolution kernels and if will be a valid [open source alternative](https://github.com/amd/triSYCL/issues/16#issuecomment-243779140) to CompiteCpp that will produce SPIR-V.\n", "http://llvm.org/docs/CompileCudaWithLLVM.html\n", "Two new Kronos Group standards are released at https://www.khronos.org/news/press/khronos-launches-dual-neural-network-standard-initiatives\n", "If it helps, a better version of isaac is out: https://github.com/ptillet/isaac, and provides significant speed-ups over clBLAS and cuBLAS on Maxwell, Pascal and Fiji. Also provides faster (input-aware) kernels than Tensorflow for 1D and 2D reductions.\n", "@hughperkins seems you have more chances to write CUDA compiler for any OpenCL device, rather than CUDA-OpenCL translator.\n", "@hughperkins Maybe OpenCL 2.0's SVM feature could solve the pointer issue? Since everyone besides Nvidia(AMD, Intel, ARM, Qualcomm) is starting to support OpenCL 2.0. Maybe it's a good solution?\n", "@hughperkins it's a blas implementation itself. It implements some of the symbols in clblas and cublas headers so no recompilation and code modification. is necessary. I could also implement some of the symbols for clblast.h, since it uses a different header. Some advantages of Isaac are:\n- Entirely dynamic, so that it can use either/both CUDA or OpenCL without recompilation.\n- Input-aware , it doesn't tune kernels for large square matrices. It should perform well on all shapes you can think of without retuning.\n- C++ API similar to numpy/arrayfire. Some fusion for combining elementwise operation with reductions\n", "@marty1885 \nNot really. AMD went back to 1.2 support on the AMDGPU-PRO drivers. Might be a while until full 2.0 support is widespread. Definitely not a short-term solution there.\n", "- Yes\n- I could hack compatibility for a bunch of operations if needed (e.g., forward **MV to GEMV). Complex support will be tricky. Double support is already here but no architecture is tuned for it.\n", "@hughperkins\n\n> Seems like my code doesnt violate any obvious OpenCL rules\n\nYes, plainly passing any __global structure (like array or struct) containing pointers is incorrect just because those pointers can point to memory of another device (OpenCL supports multi-device paradigm where one device can't access memory of another). But it seems to be possible to overcome on IR level, w/o intermediate translation to OpenCL code - that's what I assumed :)\n", "@benoitsteiner, @henline , from the https://github.com/henline/streamexecutordoc, it suggests the streamexecutor has supported the CL version canned operation(like DNN, BLAS) out-of-box. Does it suggest google has already has the clDNN, clBLAS implementation ready for Tensorflow, but just not open source it yet?\n", "Otherwise OpenCL 2.0+ and SYCL 2.2 support SVM, if you want to keep the same software architecture.\nOpenCL 2.0+ is supported by AMD and Intel GPU for example. In the embedded world, it is often supported by side effect even with OpenCL 1.x, since the host and device memories are often the same for cost reasons.\n", "@keryell \nBut the most notable platforms, Linux + the new AMD GPUs (RX 480, upcoming Vega) do only support OpenCL 1.2 for now... and who knows when that's gonna change (my bet is on in a year). Beignet (opensource Linux Intel) for OpenCL 2.0 is also still a buggy mess; the stable version has 1.2.\nAlso considering all the smaller companies that make OpenCL compatible chips are barely pulling 1.2 support. So I guess anything relying on OpenCL 2.0 will see very bad adaption rates in practice.\n", "I think.. any hardware vedor has the urgency of consuming SPIR-V? I think that Graphic/Shaders pressure on Vulkan could help Opencl side..\n", "@naibaf7 to go back to the discussion on OpenCL 2 or not, at some point real things have to be delivered... Otherwise there is already nVidia GPU and CUDA with TensorFlow running... :-)\nBut of course, a version of TensorFlow without SVM has some interest.\n", "@keryell How much of the Vulkan SPIR-V work on drivers (that has already a good devices coverage) do you think will push modern Opencl versions?\n", "@naibaf7 Khronos meeting is next week in Seoul with both OpenCL and Vulkan people, but discussions are not public. But that sounds like a good idea to have each world to improve the other, and at some point benefits to TensorFlow. :-)\n", "@keryell \nYes, I hope they discuss some DeepLearning beneficial stuff :)\n", "Congrats! Be sure to check the HIP project, as they tried to solve the same problem. They chose to create a new language called HIP, which defines what manually needs to be converted (like checking double precision support by checking compute level). While the project advances, the amound of manual translations would go down. See: https://github.com/GPUOpen-ProfessionalCompute-Tools/HIP\n\nMy suggestion for you is to use HIP and fix some bugs that are blocking for advancing Tensorflow or your own goals, as you now have the understanding of LLVM to do it. This way you don't have to solve the problems they already fixed.\n", "@hughperkins \ncan't build python module with your fork following this https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#create-the-pip-package-and-install\n\n```\nINFO: From Compiling tensorflow/core/kernels/gather_functor_gpu.cu.cc:\ngpus/crosstool: -x cuda\ngpus/crosstool: using cocl\ngpus/crosstool: PATH=/usr/bin:/usr/local/bin /usr/local/bin/cocl -D_FORCE_INLINES -gencode=arch=compute_30,\\\"code=sm_30,compute_30\\\"   -U_FORTIFY_SOURCE -D_FORTIFY_SOURCE=1 -DNDEBUG -DEIGEN_MPL2_ONLY -std=c++11  -I. -Ibazel-out/local_linux-py3-opt/genfiles -Iexternal/bazel_tools -Ibazel-out/local_linux-py3-opt/genfiles/external/bazel_tools -Iexternal/eigen_archive -Ibazel-out/local_linux-py3-opt/genfiles/external/eigen_archive  --compiler-bindir=/usr/bin/gcc -I . -fPIC  -x cu  -O2 -c  -o bazel-out/local_linux-py3-opt/bin/tensorflow/core/kernels/_objs/gather_functor_gpu/tensorflow/core/kernels/gather_functor_gpu.cu.pic.o tensorflow/core/kernels/gather_functor_gpu.cu.cc\ndirname: invalid option -- 'O'\nTry 'dirname --help' for more information.\n```\n\nI'm on ubuntu 16.04, dirname is from coreutils-8.25-2ubuntu2\n", "@hughperkins I think that tweaking the TF dockerfile on your repository with this istructions could easy the setup for others.\n", "Yes, when there will be something more functional. Basically it is quite a copy and past of this istructions you have posted.\n", "I'm experimenting building this on MacOS 10.10.5 on a MacBook late 2015 with ATI 6770M (OpenCL 1.2).\n\nI've installed Xcode 8, Anaconda (Python 3.5), and MacPorts equivalents of clang+llvm:\n#instead of apt-get lines, do:\nsudo port install clang-3.8 llvm-3.8\n#Instead of using /proc/cpuinfo, do:\nNUM_PROCS=$(system_profiler SPHardwareDataType  | grep \"Total Number of Cores\" | cut -d \":\" -f 2)\n#Then modify Makefile to use macports and run make\nperl -pi.bak -e 's|(CLANG)=.+|$1=/opt/local/libexec/llvm-3.8/bin/clag++|' Makefile\nperl -pi -e 's|(LLVM_CONFIG)=.+|$1=/opt/local/bin/llvm-config-mp-3.8|' Makefile\nperl -pi -e 's|(LLVM_INCLUDE)=.+|$1=/opt/local/libexec/llvm-3.8/include|' Makefile\n#update to Macos OpenCL dirs; future: use /System/Library/Frameworks/OpenCL.framework/Versions/Current/Headers/cl.h '#ifdef **APPLE**' conditional\ngrep -Rl 'include \"CL/' \\* | xargs perl -pi.bak -e 's|include \"CL/|include \"OpenCL/|g'\nmake -j ${NUM_PROCS}\n\nThis is as far as I get:\n\n$ make -j ${NUM_PROCS}\nmkdir -p build\nmkdir -p build\nmkdir -p build\n/opt/local/libexec/llvm-3.8/bin/clang++ -c -o build/hostside_opencl_funcs.o -std=c++11 -fPIC -g -O2 -I`pwd`/include -I`pwd`/src/EasyCL src/hostside_opencl_funcs.cpp\n/opt/local/libexec/llvm-3.8/bin/clang++ -I/usr/lib/llvm-3.8/include -fPIC -fvisibility-inlines-hidden -ffunction-sections -fdata-sections -g -D_GNU_SOURCE -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -D__STDC_LIMIT_MACROS -std=c++11 -fcxx-exceptions -c -o build/mutations.o -g -I/opt/local/libexec/llvm-3.8/include src/mutations.cpp\n/opt/local/libexec/llvm-3.8/bin/clang++ -I/usr/lib/llvm-3.8/include -fPIC -fvisibility-inlines-hidden -ffunction-sections -fdata-sections -g -D_GNU_SOURCE -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -D__STDC_LIMIT_MACROS -std=c++11 -fcxx-exceptions -c -o build/struct_clone.o -g -I/opt/local/libexec/llvm-3.8/include src/struct_clone.cpp\n/opt/local/libexec/llvm-3.8/bin/clang++ -I/usr/lib/llvm-3.8/include -fPIC -fvisibility-inlines-hidden -ffunction-sections -fdata-sections -g -D_GNU_SOURCE -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -D__STDC_LIMIT_MACROS -std=c++11 -fcxx-exceptions -c -o build/readIR.o -g -I/opt/local/libexec/llvm-3.8/include src/readIR.cpp\nIn file included from src/hostside_opencl_funcs.cpp:17:\n/Users/erybski/git/tensorflow-cl/third_party/cuda-on-cl/include/cocl/cocl.h:91:16: warning: 'host' attribute ignored [-Wignored-attributes]\n**attribute**((host)) inline unsigned long long atomicExch(volatile unsigned long long _p, unsigned long long val) {\n               ^\nsrc/hostside_opencl_funcs.cpp:194:33: error: call to member function 'in' is ambiguous\n    launchConfiguration.kernel->in(offset);\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~\n/Users/erybski/git/tensorflow-cl/third_party/cuda-on-cl/src/EasyCL/CLKernel.h:101:15: note: candidate function\n    CLKernel *in(float value);\n              ^\n/Users/erybski/git/tensorflow-cl/third_party/cuda-on-cl/src/EasyCL/CLKernel.h:104:15: note: candidate function\n    CLKernel *in(int32_t value);\n              ^\n/Users/erybski/git/tensorflow-cl/third_party/cuda-on-cl/src/EasyCL/CLKernel.h:106:15: note: candidate function\n    CLKernel *in(int64_t value);\n              ^\n/Users/erybski/git/tensorflow-cl/third_party/cuda-on-cl/src/EasyCL/CLKernel.h:108:15: note: candidate function\n    CLKernel *in(uint64_t value);\n              ^\n/Users/erybski/git/tensorflow-cl/third_party/cuda-on-cl/src/EasyCL/CLKernel.h:110:15: note: candidate function\n    CLKernel *in(uint32_t value);\n              ^\n/Users/erybski/git/tensorflow-cl/third_party/cuda-on-cl/src/EasyCL/CLKernel.h:73:15: note: candidate function not viable: no known conversion from 'size_t' (aka 'unsigned long') to 'easycl::CLArray *'\n      for 1st argument\n    CLKernel *in(CLArray *clarray1d) { return input(clarray1d); }\n              ^\n/Users/erybski/git/tensorflow-cl/third_party/cuda-on-cl/src/EasyCL/CLKernel.h:83:15: note: candidate function not viable: no known conversion from 'size_t' (aka 'unsigned long') to\n      'easycl::CLWrapper *' for 1st argument\n    CLKernel *in(CLWrapper *wrapper) { return input(wrapper); }\n              ^\n/Users/erybski/git/tensorflow-cl/third_party/cuda-on-cl/src/EasyCL/CLKernel.h:91:36: note: candidate function template not viable: requires 2 arguments, but 1 was provided\n    template<typename T> CLKernel *in(int N, const T *data);\n                                   ^\n1 warning and 1 error generated.\nmake: *_\\* [build/hostside_opencl_funcs.o] Error 1\nmake: **\\* Waiting for unfinished jobs....\nsrc/struct_clone.cpp:245:12: warning: 11 enumeration values not handled in switch: 'HalfTyID', 'X86_FP80TyID', 'FP128TyID'... [-Wswitch]\n    switch(typeID) {\n           ^\n1 warning generated.\n", "> launchConfiguration.kernel->in((int64_t)offset);\n\nThis patch worked.  Thank you.\n\nAfter applying this, continuing the build resulted in size_t namespace errors:\n\n$ make -j ${NUM_PROCS}\nmkdir -p build\nmkdir -p build\n/opt/local/libexec/llvm-3.8/bin/clang++ -c -o build/hostside_opencl_funcs.o -std=c++11 -fPIC -g -O2 -I`pwd`/include -I`pwd`/src/EasyCL src/hostside_opencl_funcs.cpp\n/opt/local/libexec/llvm-3.8/bin/clang++ -I/usr/lib/llvm-3.8/include -fPIC -fvisibility-inlines-hidden -ffunction-sections -fdata-sections -g -D_GNU_SOURCE -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -D__STDC_LIMIT_MACROS -std=c++11 -fcxx-exceptions -o build/ir-to-opencl -g -I/opt/local/libexec/llvm-3.8/include src/ir-to-opencl.cpp build/struct_clone.o build/readIR.o src/ir-to-opencl-common.cpp build/mutations.o `/opt/local/bin/llvm-config-mp-3.8 --ldflags --system-libs --libs all`\n/opt/local/libexec/llvm-3.8/bin/clang++ -c -o build/cocl_events.o -std=c++11  -fPIC -g -O2 -I`pwd`/src/CLBlast/include -I`pwd`/include -I`pwd`/src/EasyCL src/cocl_events.cpp\n/opt/local/libexec/llvm-3.8/bin/clang++ -I/usr/lib/llvm-3.8/include -fPIC -fvisibility-inlines-hidden -ffunction-sections -fdata-sections -g -D_GNU_SOURCE -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -D__STDC_LIMIT_MACROS -std=c++11 -fcxx-exceptions -o build/patch-hostside -g -I/opt/local/libexec/llvm-3.8/include src/patch-hostside.cpp build/readIR.o build/mutations.o build/struct_clone.o src/ir-to-opencl-common.cpp `/opt/local/bin/llvm-config-mp-3.8 --ldflags --system-libs --libs all`\nIn file included from src/hostside_opencl_funcs.cpp:17:\n/Users/erybski/git/tensorflow-cl/third_party/cuda-on-cl/include/cocl/cocl.h:91:16: warning: 'host' attribute ignored [-Wignored-attributes]\n**attribute**((host)) inline unsigned long long atomicExch(volatile unsigned long long _p, unsigned long long val) {\n               ^\n/opt/local/libexec/llvm-3.8/bin/clang++ -c -o build/cocl_blas.o -std=c++11  -fPIC -g -O2 -I`pwd`/src/CLBlast/include -I`pwd`/include -I`pwd`/src/EasyCL src/cocl_blas.cpp\n1 warning generated.\n/opt/local/libexec/llvm-3.8/bin/clang++ -c -o build/cocl_error.o -std=c++11  -fPIC -g -O2 -I`pwd`/src/CLBlast/include -I`pwd`/include -I`pwd`/src/EasyCL src/cocl_error.cpp\nIn file included from src/cocl_blas.cpp:15:\n/Users/erybski/git/tensorflow-cl/third_party/cuda-on-cl/include/cocl/cocl_blas.h:8:9: error: no type named 'size_t' in namespace 'std'; did you mean simply 'size_t'?\ntypedef std::size_t cublasStatus_t;\n        ^~~~~~~~~~~\n        size_t\n/opt/local/libexec/llvm-3.8/bin/../lib/clang/3.8.1/include/stddef.h:62:23: note: 'size_t' declared here\ntypedef **SIZE_TYPE** size_t;\n                      ^\nIn file included from src/cocl_blas.cpp:15:\n/Users/erybski/git/tensorflow-cl/third_party/cuda-on-cl/include/cocl/cocl_blas.h:17:5: error: no type named 'size_t' in namespace 'std'; did you mean simply 'size_t'?\n    std::size_t cublasCreate(cublasHandle_t *phandle);\n    ^~~~~~~~~~~\n    size_t\n/opt/local/libexec/llvm-3.8/bin/../lib/clang/3.8.1/include/stddef.h:62:23: note: 'size_t' declared here\ntypedef **SIZE_TYPE** size_t;\n                      ^\nIn file included from src/cocl_blas.cpp:15:\n/Users/erybski/git/tensorflow-cl/third_party/cuda-on-cl/include/cocl/cocl_blas.h:18:5: error: no type named 'size_t' in namespace 'std'; did you mean simply 'size_t'?\n    std::size_t cublasDestroy(cublasHandle_t handle);\n    ^~~~~~~~~~~\n    size_t\n/opt/local/libexec/llvm-3.8/bin/../lib/clang/3.8.1/include/stddef.h:62:23: note: 'size_t' declared here\ntypedef **SIZE_TYPE** size_t;\n                      ^\nIn file included from src/cocl_blas.cpp:15:\n/Users/erybski/git/tensorflow-cl/third_party/cuda-on-cl/include/cocl/cocl_blas.h:19:5: error: no type named 'size_t' in namespace 'std'; did you mean simply 'size_t'?\n    std::size_t cublasSgemm(cublasHandle_t blas, int transA, int transB, int M, int N, int K,\n    ^~~~~~~~~~~\n    size_t\n/opt/local/libexec/llvm-3.8/bin/../lib/clang/3.8.1/include/stddef.h:62:23: note: 'size_t' declared here\ntypedef **SIZE_TYPE** size_t;\n                      ^\nIn file included from src/cocl_blas.cpp:15:\n/Users/erybski/git/tensorflow-cl/third_party/cuda-on-cl/include/cocl/cocl_blas.h:21:5: error: no type named 'size_t' in namespace 'std'; did you mean simply 'size_t'?\n    std::size_t cublasSetPointerMode(cublasHandle_t handle, cublasPointerMode_t mode);\n    ^~~~~~~~~~~\n    size_t\n/opt/local/libexec/llvm-3.8/bin/../lib/clang/3.8.1/include/stddef.h:62:23: note: 'size_t' declared here\ntypedef **SIZE_TYPE** size_t;\n                      ^\nIn file included from src/cocl_blas.cpp:15:\n/Users/erybski/git/tensorflow-cl/third_party/cuda-on-cl/include/cocl/cocl_blas.h:22:5: error: no type named 'size_t' in namespace 'std'; did you mean simply 'size_t'?\n    std::size_t cublasGetPointerMode(cublasHandle_t handle, cublasPointerMode_t *mode);\n    ^~~~~~~~~~~\n    size_t\n/opt/local/libexec/llvm-3.8/bin/../lib/clang/3.8.1/include/stddef.h:62:23: note: 'size_t' declared here\ntypedef **SIZE_TYPE** size_t;\n                      ^\nIn file included from src/cocl_blas.cpp:15:\n/Users/erybski/git/tensorflow-cl/third_party/cuda-on-cl/include/cocl/cocl_blas.h:23:5: error: no type named 'size_t' in namespace 'std'; did you mean simply 'size_t'?\n    std::size_t cublasSetStream(cublasHandle_t handle, cudaStream_t streamId);\n    ^~~~~~~~~~~\n    size_t\n/opt/local/libexec/llvm-3.8/bin/../lib/clang/3.8.1/include/stddef.h:62:23: note: 'size_t' declared here\ntypedef **SIZE_TYPE** size_t;\n                      ^\n/opt/local/libexec/llvm-3.8/bin/clang++ -c -o build/cocl_memory.o -std=c++11  -fPIC -g -O2 -I`pwd`/src/CLBlast/include -I`pwd`/include -I`pwd`/src/EasyCL src/cocl_memory.cpp\n/opt/local/libexec/llvm-3.8/bin/clang++ -c -o build/cocl_device.o -std=c++11  -fPIC -g -O2 -I`pwd`/src/CLBlast/include -I`pwd`/include -I`pwd`/src/EasyCL src/cocl_device.cpp\n7 errors generated.\nmake: *_\\* [build/cocl_blas.o] Error 1\nmake: **\\* Waiting for unfinished jobs....\n", "Can we push long log on gist to let the thread to be still readable?\n", "> question: how are you guys solving the issue of address spaces? \n\n@hughperkins The SYCL specs describe in section 5.8 (\"Address-space deduction\")\nhow an implementation needs to deal with different memory types. This\nis similar to previous work done for PlayStation 3 and described in\nthis paper: [Offload \u2013 Automating Code Migration to Heterogeneous\nMulticore Systems](http://www.par.univie.ac.at/project/peppher/publications/Published/HiPEAC10.pdf) or [C++ on Accelerators: Supporting Single-Source SYCL and HSA Programming Models Using Clang](http://llvm.org/devmtg/2016-03/#presentation13)\n\nhope that helps.\n", "@hughperkins   Can I compile your tensorflow-opencl repo code  to apply my ARM board?  My ARM board has Imagination GPU which support opencl 1.2 .\n", "I stumbled on this thread while searching for tf/intel support.\n\nI have an intel MacBook Pro, how can I help? I don't know c/c++, but I can follow build/compile/test instructions and pass back (pastebin) results...\n\nderek$ system_profiler SPDisplaysDataType\nGraphics/Displays:\n\n```\nIntel Iris:\n\n  Chipset Model: Intel Iris\n  Type: GPU\n  Bus: Built-In\n  VRAM (Dynamic, Max): 1536 MB\n  Vendor: Intel (0x8086)\n  Device ID: 0x0a2e\n  Revision ID: 0x0009\n  Metal: Supported\n  Displays:\n    Color LCD:\n      Display Type: Retina LCD\n      Resolution: 2560 x 1600 Retina\n      Retina: Yes\n      Pixel Depth: 32-Bit Color (ARGB8888)\n      Main Display: Yes\n      Mirror: Off\n      Online: Yes\n      Automatically Adjust Brightness: Yes\n      Built-In: Yes\n    PL2202W:\n      Resolution: 1680 x 1050 @ 60 Hz\n      Pixel Depth: 32-Bit Color (ARGB8888)\n      Display Serial Number: 05884C7A57014\n      Mirror: Off\n      Online: Yes\n      Rotation: Supported\n      Adapter Type: Apple Mini DisplayPort To VGA Adapter\n      Automatically Adjust Brightness: No\n      Adapter Firmware Version: 1.03\n```\n", "@hughperkins  Thanks for your instructions! \n I try to compile your cuda-on-cl on arm platform. Following your cuda-on-cl's  guide:\nMy ARM board info:\narm64,  gcc 4.9 , clang and llvm 3.5, openCL 1.2\n\n*\\* Do I have to use clang++-3.8 version?**\ngit clone --recursive https://github.com/hughperkins/cuda-on-cl\n make\nerror:\nclang++-3.8: Command not found \nI edit the Makefile like this:  CLANG=clang++ LLVM_CONFIG=llvm-config LLVM_INCLUDE=/usr/include/llvm\nthen make again:\nerror:\nsrc/mutations.h:3:10: fatal error: 'llvm/IR/Module.h' file not found\n\ntry run make run-test-cocl-cuda_sample:\nmake: cocl: Command not found\n", "@hughperkins let me give it a try. \n", "Got error while testing keras with tensorflow\n\n```\nkeras$ KERAS_BACKEND=tensorflow pytest3\n```\n\nOutput errors:\n\n```\nInvalid kernel name, code -46, kernel _ZN5Eigen8internal15EigenMetaKernelINS_15TensorEvaluatorIKNS_14TensorAssignOpINS_9TensorMapINS_6TensorIfLi1ELi1EiEELi16ENS_11MakePointerEEEKNS_18TensorCwiseUnaryOpINS0_12scalar_rightIffNS0_17scalar_product_opIffEEEEKNS4_INS5_IKfLi1ELi1EiEELi16ES7_EEEEEENS_9GpuDeviceEEEiEEvT_T0_\n__internal__ build log: \n\"/tmp/OCL11307T1.cl\", line 3: error: variable with automatic storage duration\n          cannot be stored in the named address space\n      local float mem[1024];\n```\n\nCode:\n\n``` C\ninline float __shfl_down_3(float v0, int v1, int v2) {\n    local float mem[1024];\n    int tid = get_local_id(0);\n    int warpid = tid % 32;\n    int warpstart = tid - warpid;\n    mem[tid] = v0;\n    //barrier(CLK_LOCAL_MEM_FENCE);\n    int warpsrc = warpid + v1;\n    warpsrc = warpsrc >= 32 ? warpid : warpsrc;\n    return mem[warpstart + warpsrc];\n}\n```\n", "hi everyone, my name is ricardo , i am a C++ programmer with many years in C++ experience, and little on Cuda, i will be glade in contribute to this effort. How can i contribute to this job? \n", "Ok, i have an Odroid Xu3 with a Mali-T628 MP6(OpenGL ES 3.0/2.0/1.1 and OpenCL 1.1 Full profile)\nrunning on OS: LUbuntu 1404 64 bits\nI will make a complete installation and post the result on this platform.\nAbout bugs, there is a list of bugs (something like Bugzilla?) or an spreadsheet with a list of bugs?\nCheers!\n", "What about using HIP ?\nhttps://github.com/GPUOpen-ProfessionalCompute-Tools/HIP/blob/master/docs/markdown/hip_faq.md#how-does-hip-compare-with-opencl\nhttps://github.com/RadeonOpenCompute/hcc\nhttps://github.com/GPUOpen-ProfessionalCompute-Tools/HIP/issues/45\n\"Your wish is being granted, Eigen is being ported over AMD GPU via HIP. The second part of your request is can we bring standardized tool supporting FLOAT16 that ships with all our GFX8 GPU's, wish granted.\"\nOur development branch of AMDGPU compiler now support's both Float16 and Int16 native instruction, instead of emulating FP16/Int16 with up convert & down convert instructions to convert from FP16/Int16 to Float and back.\n\nThis is f16 tests on Fiji hardware successfully executing a matrix multiplication with half types with conversion and with Native instructions.\" \n\nAlso, not related but you should use syCL/openCL 2.0 instead of 1.2, because nvidia is already supported via CUDA. And openCL 2.0 is supported on both AMD and Intel Windows drivers. Also AMD has said that they will soon opensource un openCL 2.0 driver for Linux (which could be used by Intel, opensource magic) (and Intel already has a Linux openCL 2.0 implementation which Just need maturation.) if you ask Intel and AMD,  maybe they could speed up the work, because tensorflow is important for their economic interests. And they already have said in this comment section that they wanted to help. Also all the major ARM makers support openCL 2.0. This could open a lot of opportunitys for Android (which is in the economic interest of Google) , raspberry like, smart TVs, etc \n\nAnd in mid term we could eventually develop an opencl 1.2 fallback layer for non supported hardware. \nAnd the implementation should use also openVX (which is now supoorted by all major hardware makers, and AMD has an opensource implementation) and with https://www.khronos.org/news/press/khronos-launches-dual-neural-network-standard-initiatives \nAnd the all with Spir-V (which can be use simultaneously by Vulkan and openGL). \nYou could say that I'm making a duplicate of what was already said, but synthetizing is important. \nAnd finally, could tensorflow use HSA ? \n\nhttp://www.hsafoundation.com\nHSA would be awesome on Android. \n", "I don't know if HIP would be useful or not. It is only supported on some AMD cards so that we need an OpenCL implementation anyway if we want to support all devices. It might still be worth it if the HIP implementation is notably faster. This might be the case but I haven't seen many benchmarks (HIP vs. OpenCL) yet. Another reason might be MLOpen (which is written in HC) as an replacement for cudnn but again I have no idea how fast that is or which features it supports.\n\nTensorFlow would not use HSA directly because it is quite low-level. But HC (and HIP) is implemented on top of it and you can also implement OpenCL on top of if (pocl does that).\n", "Would the relooper algorithm be helpful here? http://mozakai.blogspot.ca/2012/05/reloop-all-blocks.html \n", "@hughperkins Nice to see you have some progress with your compiler, but I think it becomes off-topic for TensorFlow. You should start many smaller discussion threads on the GitHub page of your compiler project instead. It would be more focused and productive I guess.\n", "See https://github.com/chihchun/opencl-docker\n", "There's also https://github.com/kripken/emscripten-fastcomp/blob/master/lib/Target/JSBackend/Relooper.cpp\n\nHere's the associated paper: https://github.com/kripken/emscripten/blob/master/docs/paper.pdf?raw=true\n", "Initial OpenCL/SyCL support was merged in master with https://github.com/tensorflow/tensorflow/pull/5267\n", "Congratulations!\n\n@keryell Btw, what happened to the triSYCL repository? It seems to be gone and I can only find a reference to Khronos' Gitlab which is not publicly accessible.\n\nEDIT: I found your private clone, only the one from amd is gone.\n", "@bhack,  does the opencl-docker support in mac platform? \n", "@alephman I don't have an OSX platform but I think that adapting a little bit the launching command could works.\n", "@bhack @alephman: see my comment about mac above, if you point me to the build instructions I'll have a go\n", "@olesalscheider: yes, triSYCL moved from AMD to Xilinx https://github.com/Xilinx/triSYCL but you are right, the version on my GitHub workspace works too at https://github.com/keryell/triSYCL\n\nWe have not tried triSYCL on TensorFlow yet. There is already a big build config work to do just to try...\n", "@keryell What is the triSYCL status?\n", "Intel beignet opencl 2.0 support is almost done !\nhttp://phoronix.com/scan.php?page=news_item&px=Beignet-Birthday-CL2\n", "@bhack triSYCL is mainly developed at Xilinx now. Still adding more and more features. The Clang/LLVM-based outlining compiler is still in development to have a full single-source experience on a device. But the OpenCL compatibility mode, already implemented, has some value too, by simplifying the communications between host and kernels with the SYCL runtime doing the lazy transfers according to the dependencies expressed by the accessors.\n", "My mac is OpenCL compatible, so how can I run my tensorflow with openCL? I just found that opencl had been supported in tensorflow, when I configure the new codes.\n", "@hughperkins there is no `clinfo` instruction in my mac, what can I do for it? But I can compile the test code [here](http://stackoverflow.com/questions/7892955/how-can-i-test-for-opencl-compability) for opencl with clang and result the following info:\n`clang -framework OpenCL dumpcl.c -o dumpcl && ./dumpcl\nDevice Intel(R) Core(TM) i5-5257U CPU @ 2.70GHz supports OpenCL 1.2 \nDevice Intel(R) Iris(TM) Graphics 6100 supports OpenCL 1.2`\n", "Thank you @hughperkins, but I think I had tried the computecpp yesterday, and it's seem that the macbook system is still not supported with computecpp. So, maybe keep waiting for new updates is the only thing I can do (T.T).  BTW, my Iris 6100 is eight generation, which is good for OpenCL 1.2.  \n", "@hughperkins yes SYCL 1.2 is a priori for OpenCL 1.2 and SYCL 2.2 is a priori for OpenCL 2.2.\nI said \"a priori\" since, if you do not use anything requiring the OpenCL-compatibility mode of SYCL, SYCL does not really require OpenCL at all. Actually SYCL is a very generic model for heterogeneous computing and can run on top of anything. But of course a real implementation may require OpenCL too.\n", "Hello,\n\nI am learning/working with TensorFlow and Keras for the time being and I would be interested to get the OpenCL support working under macOS ... Is there some news on the work done around macOS ? \n\nI succeeded to compile TensorFlow but if I try to configure for OpenCL it ask me for the computeCpp 1.2 location, and there is no ComputeCpp for macOS it seems to me.\n", "Hello. By no means an expert in ML / Tensorflow / or even OpenCL, but I'm an experienced Mac graphics dev who desperately wants faster performance of Tensorflow on systems with integrated and AMD GPU's using built in libraries and simple dependencies :) \n\nHow can I help? \n", "Looking at the last compile failure on OS X in the travis log @hughperkins - it looks like running 'xcode-select --install' might a solve? It should re-link the /usr/include directory. I had this issue myself when updating Xcode beta to release and had issues compiling some C++ code.\n", "It seems like the XLA compiler (https://www.tensorflow.org/versions/master/resources/xla_prerelease.html) will provide LLVM code generation from dataflow graphs. This means very easy access to spir-v and therefore Vulkan's compute API. With code generation sorted out, I can't imagine Google not providing Vulkan compatibility given the high number of unused integrated GPUs running on Android.\n", "@hughperkins \n\nQuickly: Right now I am running Inception v3 on a custom C++ / Object-C codebase and passing in decoded video frames to the network. I don't know enough about TF to know low level needs, but high level: load models, run session, expect stuff to work. I think that means 100% compatibility to be really honest. I know thats of no help in prioritizing. Basically the C++ Image Recognition using TF /InceptionV3 was my starting point.\n\ncuda-on-cl running on Mac: I've checked out the repo and can help debug and run builds on my systems and verify results on a variety of hardware:I have access to AMD Mac Pros with Dual D700s, Nvidia Mac Laptops and Desktop systems.\n\nThanks for your detailed feedback. I'll monitor the repo, try to follow along, and try to help best I can.\n", "Hugh, you might want to look at http://chrec.cs.vt.edu/cu2cl/ to learn how some functions are mapped.\n", "At my company StreamComputing we have various GPUs for build-testing and benchmarking, which we use for our customer-projects. I could hook your Github into our Jenkins to do a weekly run.\n", "Thank you for the answer, I will go back on the subject at work this week, with specific scripts.\n\nMy use cases are around text/syntaxic matching analysis, using Gensim and Keras/tensorflow in my experiments.\n", "I am wiling to help you for testing\r\n\r\nI hava a Windows PC with an AMD card\r\nA MBP with an AMD card\r\nAn MB with an Intel integrated GPU", "Hey @hughperkins - I am going through the test set above, this evening, on an AMD R9 390 8GB. So far I've already got one different result; `logistic_regression.py` trains and doesn't return `nan`. So, good! It segfaults at the end, so I'll investigate whether the script or the cl code is at fault.\r\n\r\nWhere should I push my results, where they can be most useful to you?\r\nPerhaps we could get a standard \"test script\" that generates a standard set of results that volunteers can push to you (or set up on local CIs or whatever)?", "`py.test` is as good a solution as any; it's just a `pip` away and that's part of the process for installing `tensorflow` anyway.\r\n\r\nI've discovered a few interesting things since starting my tests, and they may not be debuggable using Python output alone, however:\r\n\r\n* Different calls to the same script may crash early, or may \"hang\" (no output, no progress, no response to `Ctrl-C`, process needs to be `pkill -9`'d), or may crash late either at the validation part or after the script completes successfully. Crashes (segfaults) may take down Xorg.\r\n* The results vary for seemingly no reason: I may call a script and have it segfault, then call it again and it will work.\r\n* Hangs can occur in portions of code that were working literally moments ago, I've had one hang occur within or after a training batch, after several hundred batches just happened successfully.\r\n\r\nSo, it might be that there's unresolved stuff on the GPU side, and that a good segfault is needed to clear it out? I don't know much about the GPU model or OpenCL yet, so I can't contribute much here. But, it might be that GPU debugging output is needed to properly explore what's happening.", "Also, I thought you were with AMD from your github, but it seems you're a \"rogue agent\" doing this whole CUDA-on-CL thing on your own time. Thanks sincerely for spearheading this! Is there some way that I and others can contribute for your efforts, perhaps by crowdfunding you a GPU? Or you could set up a Patreon, I'm happy to sign up for a monthly contribution to the project?", "Concerning AMD GPUs, we're a partner of AMD. See my message of 8 days ago, which you might have missed:\r\n> At my company StreamComputing we have various GPUs for build-testing and benchmarking, which we use for our customer-projects. I could hook your Github into our Jenkins to do a weekly run.", "> I wonder if you might have the possibility of setting up a CI server, that runs on each commit?\r\n\r\nNo problem. I probably need write-access to the project, so Jenkins can write the log-file into a build-log directory. I just spammend you, so we can discuss.", "Hi all, \r\n\r\nAs you probably see already, a bunch of SYCL stuff has been pushed to TensorFlow. We are not complete yet,  and there is plenty to do. But we are progressing to get there.\r\n\r\nIf you are interested in contributing or just curious on the current state, check the breakdown below.\r\n\r\n**Infrastructure**\r\nGoogle kindly donated two machines that are set up to test @benoitsteiner's fork of TensorFlow (https://github.com/benoitsteiner/tensorflow-opencl) periodically\r\n\r\nBoth have AMD GPUs:\r\n\r\nCL_DEVICE_NAME                          : Hawaii\r\nCL_DRIVER_VERSION                    : 1912.5 (VM)\r\n\r\nand \r\n\r\nCL_DEVICE_NAME                          : Fiji\r\nCL_DRIVER_VERSION                    : 1912.5 (VM)\r\n\r\nWe at Codeplay are looking to dedicate machine(s) next year too. To improve the OpenCL device diversity coverage.\r\n\r\nWe are looking for contributors on that front if anyone is interested in providing a test build server for relevant platforms that we support.\r\nCurrently, the requirements are:\r\n    - Ubuntu 14.04\r\n    - OpenCL drivers that support SPIR ( Intel CPU / GPU or AMD GPU )\r\n\r\n@VincentSC perhaps you could help out with that?\r\n\r\n**Tests**\r\nOn the Fiji machine ( https://ci.tensorflow.org/job/tensorflow-opencl/127/consoleFull ) we are facing 164 fails.\r\n\r\nOn the Hawaii machine ( https://ci.tensorflow.org/job/tensorflow-opencl/129/consoleFull ) we are down to 56 fails.\r\n\r\nWe are looking into fixing the failing gradient tests and investigating the origins of the additional fails on the Fiji machine.\r\n\r\n**Eigen**\r\nFor the past few months we have been actively implementing features needed by TensorFlow including: Reshaping, Slicing, Basic Reduction etc. Currently we are implementing Contraction. A detailed breakdown can be found in the Eigen Tensor tab of  https://docs.google.com/spreadsheets/d/1YbHn7dAFPPG_PgTtgCJlWhMGorUPYsF681TsZ4Y4LP0/edit#gid=0.\r\n\r\n**TensorFlow**\r\nA lot of Coefficient-wise operations have been implemented including Abs, Floor, IsFinite, Log, Pow, Mul, etc., as well as Tensor Manipulations like Reshape, Shape, Identity, Fill etc. \r\nA detailed breakdown can be found in the TensorFlow Kernels tab of https://docs.google.com/spreadsheets/d/1YbHn7dAFPPG_PgTtgCJlWhMGorUPYsF681TsZ4Y4LP0/edit#gid=1719702219\r\n\r\n**Organisation**\r\nThe above spreadsheed has several tabs that categorise the efforts of the project like: Overall Plan, Eigen Tensor, TensorFlow Kernels, Models.\r\n\r\nIf you would like to get involved, please put your name next to the item you are working on or add anything important that is missing.\r\nThanks,\r\nLuke", "Is this roadmap active?", "@lukeiwanski Yes, no problem. Contact us via info@streamcomputing.eu", "After reading through all this I'm guessing there's no solid solution yet for using OpenCL on macOS/OS X? I tried to compile Tensorflow C++ with OpenCL support ( which I assume requires\u00a0ComputeCpp for SYCL 1.2 as someone pointed out ). \r\n\r\nI looked around and couldn't seem to locate where to download, compile or build the SYCL library. Is it here https://www.codeplay.com/ ? I'm unsure really how to proceed, thanks...", "@dylib As far as I know that there is still not a ComputeCpp for macOS. So that means OpenCL for macOS is not ready. ", "Still can't make it working on Ubuntu 16.04 with AMD card and catalyst driver https://github.com/tensorflow/tensorflow/issues/6497. Is there any howto? ", "I had to look at /usr/local/computecpp/bin/computecpp_info output before trying to use TF compiled with OpenCL support. In my case it showing\r\n```\r\n  Device is supported                     : NO - Unsupported vendor\r\n  CL_DEVICE_NAME                          : Pitcairn\r\n  CL_DEVICE_VENDOR                        : Advanced Micro Devices, Inc.\r\n```\r\nnow there is 2 choices for running TF on GPU:\r\ngood working on limited (by vendor) number of devices, but proprietary CUDA\r\nbad working on limited (by computecpp developers) number of devices and also proprietary computecpp\r\nStill no OpenCL support.", "@inferrna There in an [OpenCL specific section](https://github.com/benoitsteiner/tensorflow-opencl/blob/master/tensorflow/g3doc/get_started/os_setup.md#optional-install-opencl-experimental-linux-only) in the overall TensorFlow documentation. This will be published on the tensorflow.org site soon.", "@benoitsteiner What is the current state on opencl convolutions support? Are you planning on leveraging the exiting kernels directly? What about matrix multiplications?\r\n\r\nAny ETA?\r\n", "How about using HIP to port CUDA code to platform agnostic one? https://github.com/GPUOpen-ProfessionalCompute-Tools/HIP/blob/master/docs/markdown/hip_porting_guide.md", "It seems AMD is working on that: https://github.com/GPUOpen-ProfessionalCompute-Tools/HIP/issues/45#issuecomment-269827686", "Can XLA backends LLVM IR be converted to SPIR-V with https://github.com/KhronosGroup/SPIRV-LLVM?", "How about this? I think this package can work on Radeon GPU. \r\n\r\nhttps://github.com/RadeonOpenCompute/ROCm", "@bhack From https://github.com/tensorflow/tensorflow/issues/6449#issuecomment-269245727 \r\n\r\n> @lukeiwanski Will XLA impact also your effort?\r\n\r\nXLA and SYCL solutions are complementary for different situations: SYCL is designed to provide full programmability and customizability. XLA is for optimizing well defined patterns in graphs.\r\n\r\nMy understanding of XLA is that it optimizes some existing TensorFlow graphs at runtime using the LLVM compiler. It requires optimization passes to be implemented in the compiler for each algorithm used in the graph.\r\nThe SYCL approach is the only approach that will deliver a CUDA level of programming - which is what developers need.\r\n\r\nWith SYCL we are aiming to provide support for all the TensorFlow Ops and ease development of new operations.\r\n\r\nThis means SYCL lets you write new high performance operations very easily, while XLA can optimize whole graphs if it supports all the ops in the graph.\r\n\r\n> Can XLA backends LLVM IR be converted to SPIR-V with https://github.com/KhronosGroup/SPIRV-LLVM?\r\n\r\nI don't see any reason why that wouldn't be possible.\r\n", "@lukeiwanski Thanks, I was looking specifically to https://www.tensorflow.org/versions/master/experimental/xla/developing_new_backend", "@k-hashimoto: we are discussing here about porting TensorFlow to OpenCL, a standard from Khronos Group, and actually more OpenCL SYCL, the post-modern C++ single source standard from Khronos Group.\r\nROCm looks like yet-another-non-standard-solution-from-some-vendor.\r\nIf you are interested in proprietary solutions, there is already a CUDA version of TensorFlow which looks working well. :-)", "Agreed: keep conversation / efforts on OpenCL, and let vendors implement their whatevers atop that open standard.\n\nOn 17 January 2017 10:01:32 GMT+00:00, Ronan Keryell <notifications@github.com> wrote:\n>@k-hashimoto: we are discussing here about porting TensorFlow to\n>OpenCL, a standard from Khronos Group, and actually more OpenCL SYCL,\n>the post-modern C++ single source standard from Khronos Group.\n>ROCm looks like yet-another-non-standard-solution-from-some-vendor.\n>If you are interested in proprietary solutions, there is already a CUDA\n>version of TensorFlow which looks working well. :-)\n>\n>-- \n>You are receiving this because you commented.\n>Reply to this email directly or view it on GitHub:\n>https://github.com/tensorflow/tensorflow/issues/22#issuecomment-273076892\n\n-- \nSent from my Android device with K-9 Mail. Please excuse my brevity.", ":+1:", "\ud83d\udc4d ", ":+1: ", "This message was created automatically by mail delivery software.\n\nA message that you sent could not be delivered to one or more of its\nrecipients. This is a temporary error. The following address(es) deferred:\n\n  acmeideal@gmail.com\n    Domain biomassiv.es has exceeded the max emails per hour (111/100 (111%)) allowed.  Message will be reattempted later\n\n------- This is a copy of the message, including all the headers. ------\nReceived: from github-smtp2-ext6.iad.github.net ([192.30.252.197]:48606 helo=github-smtp2b-ext-cp1-prd.iad.github.net)\n\tby chi-server32.websitehostserver.net with esmtps (TLSv1.2:ECDHE-RSA-AES256-GCM-SHA384:256)\n\t(Exim 4.87)\n\t(envelope-from <noreply@github.com>)\n\tid 1cWmiQ-0032as-W9\n\tfor greg@biomassiv.es; Thu, 26 Jan 2017 10:16:03 -0600\nDate: Wed, 25 Jan 2017 04:09:21 -0800\nDKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=github.com;\n\ts=pf2014; t=1485346161;\n\tbh=N1Pjga2Q9PtEE8ncEMXBtSJzd3kd6HAkJRnj6H2dDEg=;\n\th=From:Reply-To:To:Cc:In-Reply-To:References:Subject:List-ID:\n\t List-Archive:List-Post:List-Unsubscribe:From;\n\tb=e5r+VKm/UtpLYj0OCnfEPSYlL6a7xCOd9bN+jS3gify2mRv/g4kofW7ZrEeDyeJT+\n\t GvddVV/w5htZFUbHy9+92pYUHGEYEn2XrmFqc6ZFVoPqBsPW5Cxk31O3Kvi1cwuSPI\n\t g8J4X/qvl1DT+yKrh1es7CeXkr23c8mFNgWkG5qk=\nFrom: Miguel \u00c1ngel <notifications@github.com>\nReply-To: tensorflow/tensorflow <reply@reply.github.com>\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Subscribed <subscribed@noreply.github.com>\nMessage-ID: <tensorflow/tensorflow/issues/22/275092277@github.com>\nIn-Reply-To: <tensorflow/tensorflow/issues/22@github.com>\nReferences: <tensorflow/tensorflow/issues/22@github.com>\nSubject: Re: [tensorflow/tensorflow] OpenCL support (#22)\nMime-Version: 1.0\nContent-Type: multipart/alternative;\n boundary=\"--==_mimepart_5888957158d12_78b73ff902fe113c148134\";\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\nPrecedence: list\nX-GitHub-Sender: migpradel\nX-GitHub-Recipient: biomassives\nX-GitHub-Reason: subscribed\nList-ID: tensorflow/tensorflow <tensorflow.tensorflow.github.com>\nList-Archive: https://github.com/tensorflow/tensorflow\nList-Post: <mailto:reply@reply.github.com>\nList-Unsubscribe: <mailto:unsub+0042d4e284a2a3e3b749c573db2a9ebf5162497e5da6bc8592cf0000000114a0577192a169ce06e8ec21@reply.github.com>,\n <https://github.com/notifications/unsubscribe/AELU4lfFKxIqjh4jaQkUHuRKD7zj_eKCks5rVztxgaJpZM4Gex3i>\nX-Auto-Response-Suppress: All\nX-GitHub-Recipient-Address: greg@biomassiv.es\n\n\n----==_mimepart_5888957158d12_78b73ff902fe113c148134\nContent-Type: text/plain;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\n![image](https://cloud.githubusercontent.com/assets/11524882/22290102/6488ece2-e2ff-11e6-8299-f672978a50e8.png)\n\n\n-- \nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub:\nhttps://github.com/tensorflow/tensorflow/issues/22#issuecomment-275092277\n----==_mimepart_5888957158d12_78b73ff902fe113c148134\nContent-Type: text/html;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\n<p><a href=\"https://cloud.githubusercontent.com/assets/11524882/22290102/6488ece2-e2ff-11e6-8299-f672978a50e8.png\" target=\"_blank\"><img src=\"https://cloud.githubusercontent.com/assets/11524882/22290102/6488ece2-e2ff-11e6-8299-f672978a50e8.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n\n<p style=\"font-size:small;-webkit-text-size-adjust:none;color:#666;\">&mdash;<br />You are receiving this because you are subscribed to this thread.<br />Reply to this email directly, <a href=\"https://github.com/tensorflow/tensorflow/issues/22#issuecomment-275092277\">view it on GitHub</a>, or <a href=\"https://github.com/notifications/unsubscribe-auth/AELU4seV7pIMHk5Ji8m57G5_F-am02s-ks5rVztxgaJpZM4Gex3i\">mute the thread</a>.<img alt=\"\" height=\"1\" src=\"https://github.com/notifications/beacon/AELU4nvZeDhEaHBieXaVuH6PZoZmC-B_ks5rVztxgaJpZM4Gex3i.gif\" width=\"1\" /></p>\n<div itemscope itemtype=\"http://schema.org/EmailMessage\">\n<div itemprop=\"action\" itemscope itemtype=\"http://schema.org/ViewAction\">\n  <link itemprop=\"url\" href=\"https://github.com/tensorflow/tensorflow/issues/22#issuecomment-275092277\"></link>\n  <meta itemprop=\"name\" content=\"View Issue\"></meta>\n</div>\n<meta itemprop=\"description\" content=\"View this Issue on GitHub\"></meta>\n</div>\n\n<script type=\"application/json\" data-scope=\"inboxmarkup\">{\"api_version\":\"1.0\",\"publisher\":{\"api_key\":\"05dde50f1d1a384dd78767c55493e4bb\",\"name\":\"GitHub\"},\"entity\":{\"external_key\":\"github/tensorflow/tensorflow\",\"title\":\"tensorflow/tensorflow\",\"subtitle\":\"GitHub repository\",\"main_image_url\":\"https://cloud.githubusercontent.com/assets/143418/17495839/a5054eac-5d88-11e6-95fc-7290892c7bb5.png\",\"avatar_image_url\":\"https://cloud.githubusercontent.com/assets/143418/15842166/7c72db34-2c0b-11e6-9aed-b52498112777.png\",\"action\":{\"name\":\"Open in GitHub\",\"url\":\"https://github.com/tensorflow/tensorflow\"}},\"updates\":{\"snippets\":[{\"icon\":\"PERSON\",\"message\":\"@migpradel in #22: ![image](https://cloud.githubusercontent.com/assets/11524882/22290102/6488ece2-e2ff-11e6-8299-f672978a50e8.png)\\r\\n\"}],\"action\":{\"name\":\"View Issue\",\"url\":\"https://github.com/tensorflow/tensorflow/issues/22#issuecomment-275092277\"}}}</script>\n----==_mimepart_5888957158d12_78b73ff902fe113c148134--\n", "New here. Wanted to ask if there will be OpenCL support in tensorflow in the future, would that mean there will be support to run tensorflow on FPGA?\r\nThank you", "@atinzad: yes if the OpenCL or SYCL version & source code is supported by the FPGA environment. But since TensorFlow is perhaps the most ported framework with various means, it might already have some part running on an FPGA already somewhere...", "OMG. https://www.youtube.com/watch?v=LqLyrl-agOw", "What are the differences between the sycl development effort and XLA targetting SPIR-V other than PTX in the middle term vision?", "What a great question. Probably - the number of people involved? Would be very interesting to know!\n\n> On Feb 16, 2017, at 1:35 PM, bhack <notifications@github.com> wrote:\n> \n> What are the difference between the sycl development effort and XLA targetting SPIR-V other than PTX in the middle term vision?\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n", ">What are the difference between the sycl development effort and XLA targetting SPIR-V other than PTX in the middle term vision?\r\n\r\n@bhack That would be a great discussion to be had at yesterday's TensorFlow Dev Summit\r\n\r\nDo you ask about the resources available / type of programmers needed to contribute?\r\n\r\nIf so, in OpenCL/SYCL approach C++ programmers / OpenCL C programmers can quickly be brought up to speed and be able to contribute. XLA approach requires a compiler / llvm experience.\r\n\r\nXLA is Google's internal project by extension they have more resouces associated with it. But, on the other hand their task is way bigger too.. Writing a compiler is not an easy task.\r\n\r\nOtherwise, if you are asking about the model:\r\n \r\nAs I mentioned earlier in https://github.com/tensorflow/tensorflow/issues/22#issuecomment-272908870 we are seeing both efforts as complementary approches and both having different use cases. I still stand with that statement.\r\n\r\nFor instance @tatatodd in his presentation mentioned that some of the Ops will never have XLA as a target. I believe that we are possible to fill that gap.\r\n\r\nOther things to consider are new platforms. I will use mobile and embedded enviroment for this argument's sake as a new chips tend to pop out more frequently than GPUs ( the principle is the same ).\r\n\r\nIf the semiconductor support SYCL / OpenCL you get TF support out of the box ( some performance tweaks might be required ). \r\n\r\nIf the architecture is exotic and there is no LLVM backend for it yet XLA needs to add it ( that might not happen too often but still ). What happens more often is the architecture changes a bit and then new optimisation passes need to be added or existing one must be modified to gain the benefit. Tweaking kernel code is easier.\r\n\r\nI haven't looked very deep to XLA but I assume that XLA has to call into the CUDA API somehow to run the PTX kernel code, so would have to be ported to OpenCL or Vulkan to run SPIR-V kernels instead - that I assume would go through StreamExecutor - another framework to get familliar with - probably quite a big effort.\r\n\r\nIn short, we are providing an unified / stable platform in very fragmented / diverted ecosystem that both semiconductor companies and developers can target. Where as XLA would have to commit to support.\r\n\r\n@benoitsteiner or @drpngx  might be able to give more inside knowledge of XLA as I am working with a lot of assumptions / conclusions based on conversations.\r\n\r\nOh, as well I have created slack channel to eas up communication https://tensorflowopencl.slack.com/shared_invite/MTQzNDQ0NzgzNzAyLTE0ODcyOTE1NjctMDZhM2RkODRlYg\r\n\r\nEidt:\r\nSlack link is no longer valid. Please ping me if you'd like to join.", "I think that is correct and partially will depend in which direction the semiconductors producers will be oriented. \r\n\"These backends emit the LLVM IR necessary to represent the XLA HLO computation in an efficient manner, and then invoke LLVM to emit native code from this LLVM IR.\" So [LLVM IR could be converted to SPIR-V](https://github.com/KhronosGroup/SPIRV-LLVM). But Opencl SPIRV dialect it is [different from Vulkan](https://github.com/KhronosGroup/SPIRV-LLVM/issues/202#issuecomment-278367134). Streamexecutor is being pushed in LLVM [parallel-lib](https://reviews.llvm.org/diffusion/L/browse/parallel-libs/trunk/) and in the original @henline [description](https://github.com/henline/streamexecutordoc) the original plan seems to cover opencl.", "/cc @dneto0", "http://phoronix.com/scan.php?page=news_item&px=OpenCL-2.0-NVIDIA-Preps\r\nNvidia should soon support opencl 2.0 on both Linux and Windows, this is YUGE ! ", "Performance wise it's likely to be slower than CUDA though.", "Remember also that Noveau guys are working independently on [Opencl with SPIR-V](https://phabricator.pmoreau.org/w/mesa/opencl_through_spirv_current_status/). The status is a little bit outdated but there are fresh commits.", "Opencl isn't inherently slower than Cuda, it's Just nvidia virtually locking the market by crippling his opencl driver. \r\nBut nvidia leading is finally coming to an end and even their ammoral anticompetitives practices will not save them. With the impressive Cuda autotranslator HIP (  https://github.com/GPUOpen-ProfessionalCompute-Tools/HIP) \r\nThe upcoming vega apus and dgpus and ARM coming to Windows, Nvidia has no future, this is why the industry NEED to support opencl/syCL/HIP/HSA very soon and massively. ", "What about slide 40 of https://autodiff-workshop.github.io/slides/JeffDean.pdf?", "Hello, that I planning that tensorflow will support the new AMD Radeon Instinct? (http://instinct.radeon.com/en-us/)", "Hi, is there any progress in the TF-OpenCL support for FPGAs?", "@alexivia https://github.com/iteong/tensorflow/blob/master/tensorflow/stream_executor/platform.h#L30 was removed some months ago and Streamexecutor roadmap it is not clear. ", "@bhack thanks for the quick response\r\nSo, does this mean that there is no support, or that correct operation is not guaranteed?\r\nAlso, from what I read on this thread I see that the testings are mainly on AMD GPUs... is anyone training nets on Nvidia GPUs with this OpenCL port?", "Streamexecutor was renamed in LLVM parallel-libs and now is [acxxel](https://reviews.llvm.org/diffusion/L/browse/parallel-libs/trunk/acxxel/)", "Can any Google member explain the difference and roadmaps between streamexecutor and https://reviews.llvm.org/rL285111?", "CC @zheng-xq ", "@henline and @jlebar are the experts to answer the difference between streamexecutor and https://reviews.llvm.org/rL285111?", "Axcell and StreamExecutor are separate projects.  There are no current plans to merge them.  I leave it up to the TensorFlow folks to say whether or not they plan to switch.", "So also StreamExecutor and StreamExecutor llvm was not the same projects?", "Correct, they are not the same project.\n\nOn Thu, Mar 16, 2017 at 11:06 AM, bhack <notifications@github.com> wrote:\n\n> So also StreamExecutor and StreamExecutor llvm was not the same projects?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22#issuecomment-287143104>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAJMh_4ODoCVglGRbFBs8UmtSEm6D47_ks5rmXoUgaJpZM4Gex3i>\n> .\n>\n", "@jlebar Next time a creative unit for naming ;) but probably was not a lack of creativity motivation but just an upstreaming effort of an internal tool that diverged to the one maintained in TF..", "@bhack, we *did* change the name, precisely when we realized that we did\nnot think it made sense to move StreamExecutor into LLVM wholesale.  It's\nnow called \"Acxxel\".\n\nI'm sorry for the confusion and I appreciate the feedback..  It was a\nlearning process for sure.\n\nOn Thu, Mar 16, 2017 at 11:24 AM, bhack <notifications@github.com> wrote:\n\n> @jlebar <https://github.com/jlebar> Next time a creative unit for naming\n> ;) but probably was not a lack of creativity motivation but just an\n> upstreaming effort of an internal tool that diverged to the one maintained\n> in TF..\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22#issuecomment-287148247>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAJMh0MMZvdTJ-bUoa71FBrEqHqFpDjvks5rmX5IgaJpZM4Gex3i>\n> .\n>\n", "Yes I have still a bit of confusion between StreamExecutor, SyCL in eigen, XLA (that actually has only a CUDA backend, other than CPU and opencl in some slides)", "Bump", "Has anyone in Google spoken to Apple or AMD to ease this? I guess [AMD people](https://community.amd.com/thread/212460) is so lost they don't even know the problem is there and they are still wondering why Nvidia has such a huge market share. I guess too Apple AI team would be more than happy to help in here... [if OpenCL wasn't an abandonware from their side since 2013](https://forums.developer.apple.com/message/217981#217981) and, even worse, their bosses wouldn't mad at Google.", "What's the latest on this?", "According to [TF 1.1 Release Notes](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md#deprecations) Mac GPU (Nvidia only) support has been deprecated. Let's hope this will help to improve OpenCL approach (not very confident on this).", "You can follow also the status of the PR https://github.com/tensorflow/tensorflow/pull/9117", "Thanks! I'm following this issue during last months. I'm not confident about the Apple OpenCL commitment, given they're stuck onto OpenCL 1.2 since 2013 (Apple is not providing SPIR 1.2 support yet).", "If TensorFlow on OpenCL would help you in your work let me know, to the extent I can help advance research and practice of deep learning I'd like to help. My company has built an OpenCL back end for TensorFlow tuned for a variety of GPUs as part of our work in on-device inference. We have tested on the major mobile & desktop GPU families including common configurations on Windows & Mac. If there's enough interest we may do some kind of public distribution. We also have Metal (Apple GPU) and LLVM (CPU), along with a way to do zero-dependency deployment. The idea here being to give every device great support for deep learning.", "@choongng - all of that sounds incredibly useful and helpful. My personal project https://github.com/Synopsis/ would greatly benefit from OpenCL on OS X, as well as Metal for iOS and Desktop deployment. If its possible for this to be introduced to Tensorflow proper I think it would be a tremendous boon for tons of developers. \r\n\r\nThank you.", "@choongng\r\n\r\nIf your company publish an OpenCL version, or more interesting a Metal version of TensorFlow, I think that this is going to be a great news for a lot of people, I am in the process to build an eGPU with an NVidia card to get TensorFlow / Keras running on my MBP for my job... \r\n\r\nFor people interested ... go to eGPU.io community", "@choongng\n\nI would be *very* interested in seeing this, so I'd be very grateful of you could pursue it! Especially if it doesn't require the sketchy closed-source compilers TF have chosen for CL support..\n\nOn 26 April 2017 03:33:51 GMT+01:00, Choong Ng <notifications@github.com> wrote:\n>If TensorFlow on OpenCL would help you in your work let me know, to the\n>extent I can help advance research and practice of deep learning I'd\n>like to help. My company has built an OpenCL back end for TensorFlow\n>tuned for a variety of GPUs as part of our work in on-device inference.\n>We have tested on the major mobile & desktop GPU families including\n>common configurations on Windows & Mac. If there's enough interest we\n>may do some kind of public distribution. We also have Metal (Apple GPU)\n>and LLVM (CPU), along with a way to do zero-dependency deployment. The\n>idea here being to give every device great support for deep learning.\n>\n>-- \n>You are receiving this because you commented.\n>Reply to this email directly or view it on GitHub:\n>https://github.com/tensorflow/tensorflow/issues/22#issuecomment-297220160\n\n-- \nSent from my Android device with K-9 Mail. Please excuse my brevity.", "I think that it would be revolutionary ;)\r\n\r\n", "@choongng Maybe it would help if you join forces with these guys\r\nhttps://github.com/benoitsteiner/tensorflow-opencl", " @cathalgarvey what is the open source compiler you propose to use then? It is difficult to find an open source OpenCL-compliant solution to address a lot of devices in the wild...\r\nWe need to bootstrap a solution at some point somehow...", "I didn't say it was an easy fix. But, OpenCL isn't the problem, there. After all, CUDA is entirely proprietary, much worse than even the OpenCL option TensorFlow chose.\n\nThat all said, there are options for a CL-or-cuda system if you're starting from scratch, including portable middleware runtimes or arrayfire, etc. Tensorflow is too tied to CUDA though.\n\nI find it frustrating that people are willing to write kernels in CUDA but balk at doing it for CL, even though it would reach more users and seriously grow the market ecosystem. There are direct and indirect benefits to an open platform for everyone, possibly leading to big cost savings for everyone in the long run.\n\nIf SYSCL is how that eventually happens, great: so why aren't some big names putting money on an open SYSCL distribution instead of buying into fringey proprietary options, which kind of defeat the purpose of an open standard?\n\nOn 28 April 2017 09:13:06 GMT+01:00, Ronan Keryell <notifications@github.com> wrote:\n>@cathalgarvey what is the open source compiler you propose to use then?\n>It is difficult to find an open source OpenCL-compliant solution to\n>address a lot of devices in the wild...\n>We need to bootstrap a solution at some point somehow...\n>\n>-- \n>You are receiving this because you were mentioned.\n>Reply to this email directly or view it on GitHub:\n>https://github.com/tensorflow/tensorflow/issues/22#issuecomment-297936468\n\n-- \nSent from my Android device with K-9 Mail. Please excuse my brevity.", "What I want to ask in this context is this:\r\n\r\nSo some deep learning frameworks like Tensorflow are somewhat tepidly exploring the use of opencl as an alternative to CUDA. Of course CUDA is just the \"language\" that cuDNN was developped on, and that's (if my understanding is correct) is what most deep learning languages are actually using. In this context, I am not sure what the opencl version of cuDNN is.\r\n\r\nAlso AMD has been talking about open-source alternatives to CUDA which they are continuously developing and are calling rocM. They are also talking about miOpen to be the cuDNN equivalent (handcrafted assembler libraries for common deep learning functions), which however has not been released yet. The AMD approach is somewhat more holistic: We are not just exporting heavy-compute to the GPU.\r\n\r\nIn this context, I am genuinely confused. How do opencl efforts like the ones listed above fit together? For NVIDIA GPUs, it's easy....there is CUDA, and there is cuDNN written in CUDA. For non-NVIDIA/or in this case AMD, it seems so much less clear. When is HIP preferred? When is using the HCC preferred? When is using opencl preferred? Any insights would truly be appreciated! ", "@cathalgarvey there is a lot of politics behind all these huge software/hardware infrastructures... :-(\r\nEven if we can dream of a clean-slate solution based on pure scientific criteria, I think we have to be pragmatic.\r\nGoogle does not want to change too much of the TensorFlow architecture. This is why the OpenCL-based architecture has to be very similar, requiring single-source C++ like \"CUDA runtime\" instead  of the lower-level non-single-source OpenCL C solution. In the Khronos realm, the single-source C++ version of OpenCL is called SYCL.\r\nLet's discuss this when you drop by Dublin, for example, since you look to be based in Ireland too. :-)\r\nIn the meantime, feel free to contribute to https://github.com/triSYCL/triSYCL and the TensorFlow & Eigen branches dealing with SYCL...", "@keryell Do you know if also XLA:GPU:OpenCL is planned on SyCL?", "Hi @benoitsteiner, regarding:\r\n> There in an OpenCL specific section in the overall TensorFlow documentation. This will be published on the tensorflow.org site soon.\r\n\r\nI did a [search on tensorflow.org for OpenCL](https://www.tensorflow.org/s/results/?q=opencl&p=%2F) and didn't seem to be able to find anything significant, it to all seems to point right back here.  By \"soon\", do you mean before ______? ( _insert funny sarcasm here_ ).\r\n\r\nI was able to compile your repo (yay!), although I'm guessing that it needs something else to create a working `Tensorflow OpenCL` for Mac; I tried building the `triSYCL`compiler mentioned but sadly failed.\r\n", "@bhack SInce I do not work for Google, I have no idea about XLA details...", "@dylib unfortunately all this is a work-in-progress...", "@keryell Yes I know.. I was just curious if was discussed in some meetings.", "OpenCL is radically different from CUDA. I would however definitively see this ported to HIP instead.\r\nSo +1 For all of you that suggested it.\r\nhttps://github.com/GPUOpen-ProfessionalCompute-Tools/HIP\r\n\r\n> HIP allows developers to convert CUDA code to portable C++. The same source code can be compiled to run on NVIDIA or AMD GPUs\r\n\r\nNot many people know about HIP. \r\nYou can find more information about tensorflow and HIP here :\r\nhttps://github.com/GPUOpen-ProfessionalCompute-Tools/HIP/issues/37\r\nand\r\nhttps://github.com/GPUOpen-ProfessionalCompute-Tools/HIP/issues/45\r\n\r\n**Side note:**\r\nI don't think we should fight / brag about Nvidia vs AMD. Those are respectable companies that make amazing hardware and software period. We should instead focus on delivering tensorflow to a larger user base.\r\nTargeting many languages through bindings is already a good starting point, but we also need to target as many hardware as possible. (Even if cloud solutions are amazing, they aren't always the answer)", "We have experience with HIP, here at Stream. Let me take a look.\r\n\r\nAgree on the \"my company is better\" arguing. I would like to know which GPUs TensorFlow should be targeting. It has to be pragmatic and useful. For instance Intel GPUs or embedded GPUs (Qualcomm, ARM, Imagination), RaspBerry Pi - yes or no? ", "[AMD Radeon Vega Frontier Edition](http://pro.radeon.com/en-us/vega-frontier-edition/)\r\n> We continue to aggressively improve our ROCm open software platform and machine learning libraries.  We\u2019re also supporting open machine intelligence frameworks like Caffe (released in April). Later this quarter we plan to offer support for Torch, and **Tensor Flow** is in the works.", "They've already released Caffe, would be very interested to hear others on this thread sharing their experiences with building/testing:\n\nhttps://github.com/ROCmSoftwarePlatform/hipCaffe\n\nI've started installing but hit a roadblock where anything requiring CL just freezes, even `clinfo`. Not sure if this is because of some software issue, or if my card (R9 390) simply isn't supported by ROCm.\n\nOn 17 May 2017 15:18:32 GMT+01:00, Bryan Li <notifications@github.com> wrote:\n>[AMD Radeon Vega Frontier\n>Edition](http://pro.radeon.com/en-us/vega-frontier-edition/)\n>> We continue to aggressively improve our ROCm open software platform\n>and machine learning libraries.  We\u2019re also supporting open machine\n>intelligence frameworks like Caffe (released in April). Later this\n>quarter we plan to offer support for Torch, and **Tensor Flow** is in\n>the works.\n>\n>-- \n>You are receiving this because you were mentioned.\n>Reply to this email directly or view it on GitHub:\n>https://github.com/tensorflow/tensorflow/issues/22#issuecomment-302103815\n\n-- \nSent from my Android device with K-9 Mail. Please excuse my brevity.", "@cathalgarvey I have been using Caffe OpenCL branch on AMD's GPUs and it works just fine. `make run test` passed all tests except one", "Good to hear; may I ask about your HW/SW setup? E.g., what card you're \nusing, what distro/version of Linux, etc?\n\nI previously had AMDGPU-pro, but uninstalled it when installing ROCm. \nIt's possible there's some legacy thing interfering with me.\n\n--\n@onetruecathal / @cathal@quitter.is\n\n\nOn Wed, May 17, 2017 at 3:50 PM, Bryan Li <notifications@github.com> \nwrote:\n> @cathalgarvey I have been using Caffe OpenCL branch on AMD's GPUs and \n> it works just fine. make run test passed all tests except one\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n\n", "@cathalgarvey \r\n- Caffe OpenCL branch (tested commit [c61d48746b2df1d237c64abc7416342ce98f3251](https://github.com/BVLC/caffe/tree/c61d48746b2df1d237c64abc7416342ce98f3251))\r\n- OS: Ubuntu 16.04.2 LTS\r\n- Tested on Polaris (RX460), Fiji (Fury X) and Tonga (W7100)\r\n- Driver: AMDGPU-Pro driver for Linux 16.40 or above\r\n- [ViennaCL](http://viennacl.sourceforge.net/)\r\n- General dependencies: `libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler libatlas-base-dev libblas-dev libgflags-dev libgoogle-glog-dev liblmdb-dev libboost-all-dev cmake python-numpy`\r\n- cmake: `cmake -DViennaCL_INCLUDE_DIR=<wherever you downloaded ViennaCL>/ViennaCL-<version> -DOPENCL_INCLUDE_DIRS=<wherever you downloaded ViennaCL>/ViennaCL-<version>/CL/ -DOPENCL_LIBRARIES=/opt/amdgpu-pro/lib/x86_64-linux-gnu/libOpenCL.so.1 ..`\r\n", "Yes in addition to the OpenCL branch above, naibaf7 will be publishing a book (very soon) on using it for real-time inference on commodity hardware using amd and hd graphics.", "Ah; I was talking about hipCaffe, not the OpenCL branch:\n\nhttps://github.com/ROCmSoftwarePlatform/hipCaffe\n\nInstalling ROCm to build/test hipCaffe required me to uninstall \nAMDGPU-pro, perhaps I'll try the vanilla branch again. It's poorly \ndocumented, unfortunately.. I suppose I'll try a blind \"make\" and see.\n\nSo, I'm still interested to hear others' experiences with the AMD \nROCm/HIP stack; if they're working on a Tensorflow fork it'd be great, \nprovided it actually works on more than 3/4 models of AMD card in the \nwild.\n\n--\n@onetruecathal / @cathal@quitter.is\n\n\nOn Wed, May 17, 2017 at 4:09 PM, Bryan Li <notifications@github.com> \nwrote:\n> @cathalgarvey\n> \n> Caffe OpenCL branch (tested commit \n> c61d48746b2df1d237c64abc7416342ce98f3251)\n> OS: Ubuntu 16.04.2 LTS\n> Tested on Polaris (RX460), Fiji (Fury X) and Tonga (W7100)\n> Driver: AMDGPU-Pro driver for Linux 16.40 or above\n> ViennaCL\n> General dependencies: libprotobuf-dev libleveldb-dev libsnappy-dev \n> libopencv-dev libhdf5-serial-dev protobuf-compiler libatlas-base-dev \n> libblas-dev libgflags-dev libgoogle-glog-dev liblmdb-dev \n> libboost-all-dev cmake git python-numpy cmake\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n\n", "@cathalgarvey I do hope they are working on a hip backend, not a full fork. That would be sad and only split the working effort.\r\nThere is enough tooling already :/", "@YvanDaSilva AMD's efforts are a bit poorly coordinated at the moment (yes, all forks). Also it doesn't seem to work that well on a large variety of devices yet, unlike the OpenCL branch of Caffe, for example...", "@naibaf7 I absolutely agree. \r\nTo be honest, they really seem to lack human resources, they are working on all the fronts.\r\nBy the way: Didn't know ETH had a Neuroinformatics ;) nice !", "@cathalgarvey Can you elaborate on the ROCm/HIP stack for a layman like myself. I've been playing AMGPU-pro and AMDGPU with my Sea Islands cards so I'm sure I could post some useful results.", "@YvanDaSilva \r\nThey sponsored my original Caffe OpenCL project, and unfortunately didn't coordinate well, so AMD research and an independent guy at AMD also worked on OpenCL ports in parallel - the former AMD research team is now disfunct and most of them actually work for Tesla (self driving car project) now... so an unfortunate chain of events.\r\nI'm still in collaboration & contact with them though. Vega is going to be interesting :)", "@naibaf7 Nice, lucky you! Wish there was such studies when I was at the Heig-vd would have continued to a MSc certainly.\r\n\r\nYeah... That's what I figured. So much work, so little human resources available in these fields.", "All that stuff sounds great, but let's refocus the discussion on having TensorFlow working with OpenCL SYCL and not only vendor-specific solutions... :-)\r\nI hope RoC and other HiP have their own GitHub to discuss their own issues...\r\n@naibaf7: at least i am still in the OpenCL realm. Join the club again! :-)", "@keryell I think the discussion on HIP is valid, if there's a HIP port for Tensorflow in the works. After all, the official Tensorflow-on-CL solution is to use a proprietary SYCL framework with sharply limited platform and kernel support, so it's not really any better than the \"vendor-specific\" HIP solutions that offer a new way out of CUDA.\r\n\r\nHIP may be mostly AMD's doing right now, but AFAIK it's an open standard? Perhaps I'm mistaken. If it is though, and if AMD can deliver a tensorflow-on-HIP port, it would immediately be more open than the official tensorflow-on-SYCL one.", "HIP is a subset of CUDA, so it's as open as CUDA.", "OK, fine; HIP-the-API is a subset of CUDA-the-API, but unless NVidia is craven enough to start  channeling Oracle I doubt that will be an issue. I was referring to the runtime/compilers for HIP, of which I think AMDs are ~open.\r\n\r\n*edit*: Sorry if the above came out sounding rude; just trying to clarify my position above!", "Will Vulkan and [Opencl be fused](https://www.khronos.org/news/press/khronos-releases-opencl-2.2-with-spir-v-1.2)?", "[according to Raja's Reddit AMA, Vega will support Tensorflow, Cafe2, Cafe, Torch7 and MxNet via MIOpen.](https://www.reddit.com/r/Amd/comments/6bklro/we_are_radeon_technologies_group_at_amd_and_were/dhqoalj/)", "@cathalgarvey the discussion is clearly valid, but not here...\r\nYou are on GitHub here, on the track discussing the port of TensorFlow & Eigen using Khronos Group standards.\r\nThis is not Twitter or your Facebook wall... :-)\r\nSo please contribute with some commits on these projects ! :-)", "There's a new version of the setup guide for compiling TensorFlow with ComputeCpp, Codeplay's implementation of SYCL, so that OpenCL devices can be used. We would appreciate any feedback you can give us on it. https://www.codeplay.com/products/computesuite/computecpp/guides/how-to-setup-tensorflow-with-computecpp", "do you have any idea what the success rate is for getting this working on untested AMD gpus?  I'm specifically interested if it has been tested for AMD Radeon Pro 460  @rodburns.  I would be happy to spend a few hours  getting ubuntu running on my Macbook laptop if there is any hope with an untested GPU", "@samhains we have not tested this but you could give it a try. You will need to use some older AMD drivers with Ubuntu that support the SPIR extension. I haven't yet been able to figure out what drivers those are yet.", "@samhains If the codeplay route fails to deliver, don't miss out on [tf-coriander](https://github.com/hughperkins/tf-coriander), which is finally at a state of practical usage on Ubuntu/Mac.\r\n\r\nI'm currently testing it on convnets, bidirectional rnns, etcetera and everything seems to be working great. It runs on \"vanilla\" OpenCL 1.2 so that should enable Tensorflow on a huge range of relatively old hardware.\r\n\r\nThe rub is, for now, that it's based on Tensorflow 0.11.", "@rodburns. I tried following the steps listed on the link https://www.codeplay.com/products/computesuite/computecpp/guides/how-to-setup-tensorflow-with-computecpp\r\nI get the following error:\r\nERROR: /home/sayantan/.cache/bazel/_bazel_sayantan/6f05f78a1e215999d72e42c1e87a8c1d/external/protobuf/BUILD:609:1: undeclared inclusion(s) in rule '@protobuf//:python/google/protobuf/internal/_api_implementation.so':\r\nActually I am getting the same error if I try to compile tensorflow from source. I have compiled it earlier, not sure what has changed though.\r\n", "@rahasayantan what is that gets included? As well do you get it when compiling without --config=sycl ?", "@lukeiwanski : The issue as I understand is Bazel is trying to compile Protobuf and its not finding or downloading the sub directories. I did a pull with recursive-submodule still it has the same issues. And it has the same issue without --config = sycl. In fact I am facing the same issue when I do a git pull from tensorflow main project. I dont think this is linked with openCL, its some issues with the way I am doing the pull. When I manually download the project zip from your repo without git and compile, it compiles properly,  but then I get a segmentation fault. I have already raised this issue on your GIT project and we are talking there, I will give the updates related to the segmentation fault on that thread (no point duplicating things). Thanks for your response.", "Opensource triSYCL is arriving. See https://github.com/triSYCL/triSYCL/pull/45", "I'm new here. Very interested in seeing TF support OpenCL. How do I get updates from this thread?", "emmm...Interesting, but why? I mean why Tensorflow choose cuda but opencl at the beginning? Some commercial reason I guess? ", "Hi @tensorflower-gardener,\r\n\r\n@hughperkins has created [Coriander](https://github.com/hughperkins/coriander-dnn) that could run NVIDIA\u00ae CUDA\u2122 code on OpenCL 1.2 devices. You might want to take a look if that suits your need to connect TF to OpenCL 1.2 devices. Kindly attribute his name and his contribution in case if you plan to use his work.", "It seems hopes of ever seeing `OpenCL` support for Mac have gone from little to `tf.zero`. I just read that TensorFlow Mac will no longer have **ANY** GPU support apparently (1.2+):\r\n```\r\nNote: As of version 1.2, TensorFlow no longer provides GPU support on Mac OS X.\r\n```\r\n![wtf](https://user-images.githubusercontent.com/17288131/27427545-3adb5a4c-56f4-11e7-8bb2-4a58f64e8b70.png)\r\n\r\nhttps://www.tensorflow.org/install/install_mac", "TF-Coriander is tested on Mac, so if/when it reaches version parity you should be able to use that.\n\nOn 22 June 2017 11:46:51 CEST, dylib <notifications@github.com> wrote:\n>It seems hopes of ever seeing `OpenCL` support for Mac have gone from\n>little to `tf.zero`. I just read that Macs there will no longer have\n>**ANY** GPU support apparently (1.2+):\n>```\n>Note: As of version 1.2, TensorFlow no longer provides GPU support on\n>Mac OS X.\n>```\n>![wtf](https://user-images.githubusercontent.com/17288131/27427545-3adb5a4c-56f4-11e7-8bb2-4a58f64e8b70.png)\n>\n>https://www.tensorflow.org/install/install_mac\n>\n>-- \n>You are receiving this because you were mentioned.\n>Reply to this email directly or view it on GitHub:\n>https://github.com/tensorflow/tensorflow/issues/22#issuecomment-310331281\n\n-- \nSent from my Android device with K-9 Mail. Please excuse my brevity.", "Sad because now with an eGPU and n Nvidia 980 Ti inside we get driver working and Cuda working \r\n\r\nI didn't have the time for now to try Tensor Flow in my configuration yet.\r\n\r\nwebdriver and Cuda toolkit installed on my computer and the Cuda samples work well\r\n\r\nhttps://youtu.be/JN9fDmqf010\r\n", "@cathalgarvey you said that you test convnets on tf-coriander, but it doesn't seem like convnets are working yet. Can you please clarify if you managed to get convnets to run on the GPU using tf-coriander?", "FYI,\r\nhttps://github.com/tensorflow/tensorflow/issues/10703\r\n", "Why does tensorflow no longer support GPUs on OS X? I was planning on using Tensorflow with an eGPU setup I have on order.", "@justinrmiller they claim they can't test it anymore on mac os, and therefore decided to stop the support. however, I'm having a hard time believing that. Going forward with the advert of egpus on high sierra and with new nvidia drivers, this will no longer be the case.", "@tscholak yeah exactly. I was going to use my new egpu enclosure to ditch my windows box for good.", "Keeping in mind that although Nvidia cards work in eGPU enclosures, Apple will only officially support the RX580 in their dev kit, so the need for OpenCL will not go away.", "OpenCL on Mac is 1.2 which means that there seems to be no active driver\ndevelopment. I think adding Metal support to TF is a painstaking process\n(enabling Eigen and stream executor) but doable.\n\n\nOn Sun, Jul 16, 2017 at 3:17 PM Ferdia McKeogh <notifications@github.com>\nwrote:\n\n> Keeping in mind that although Nvidia cards work in eGPU enclosures, Apple\n> will only officially support the RX580 in their dev kit, so the need for\n> OpenCL will not go away.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22#issuecomment-315634166>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ACFkv3bmDr_KFSydC-QW_xbuR008pvLXks5sOm_kgaJpZM4Gex3i>\n> .\n>\n", "I'm very sad about the abandon of GPU support for macOS. \r\n\r\nStill looking for OpenCL support for GPU on macOS because Apple will, obviously, not change to Nvidia GPUs any time soon.\r\n\r\nTensorflow is my my engine of choice. Using GPU acceleration locally on my MacBook Pro or future iMac Pro would be awesome. \r\n", "For Microsoft it would make sense to sabotage Apple, but since Google has no desktop OS they are only hurting themselves.", "Honestly someone smarter than I should look into integrating Mac OS 10.13's MPS - Metal Performance Shaders which have support for a large set of neural network primitives out of the box. This would allow up to date, high performance GPU for mobile and Desktop iOS and macOS Tensorflow inference deployment. \r\n\r\nYou can't train with Apples primitives as I understand it (they don't provide anything), but with Tensorflow support maybe you could? I imagine it for folks on the Apple platform that would be a boon. \r\n\r\nI don't think Google would provide this internally, and I don't have nearly the requisite skills to attempt it myself. Posting this idea so folks more talented than I might take it on.\r\n\r\n:)", "Apple is solely aimed to sell Apple devices. Google is aimed to hire Google massive services. \r\n\r\nIf you're willing to do AI (learning) with one single device, like one Apple Laptop, you'll do \"Superficial Learning\" instead of \"Deep Learning\", so you'd better give up doing anything but tutorials. Inference results in a trained model for one single user, in a single device (even in a not-too-many-multicore phone), could be neat to be done through GPUs, but is perfectly doable only with CPUs.\r\n\r\nOn the other side, GPUs are absolutely needed if you're gonna feed extremely large datasets for learning or you're gonna serve trained inference to extremely large concurrent customer groups.\r\n\r\nEven though, doing it in such scale, is not that easy due to the network issues. Just have a look to the TPU-Pods physical architecture. It's in the antipode of a laptop (several GPUs per memory-overloaded multi-core server, with dedicated optical-fiber for inter-server communications).\r\n\r\nI have a MacBook Pro. It's a nice terminal to get to the cloud :-D", "I see TF on Metal can extend to iOS too. If anyone interested in picking it up, I recommend adding Metal support to Eigen first (can use OpenCL as reference).", "@rogerpasky For school I had to use Tensorflow for training models, not just for evaluating a model. And I will have to repeat this again in near future. For students like me, having training on GPU is a must, saving a lot of time. It not just a matter to serve multiple concurrent user. \r\n", "@rogerpasky it's about the ability to develop models and solutions locally on a mac", "@rogerpasky respectfully disagree. While cloud based multi-GPU solutions work great for internet services, I'm targeting professional video production pipelines where inference is being run on hours and hours of pro-res and uncompressed HD, 2K, 4K footage, which a) no production house is going to upload to a cloud, b) they don't want google or whomever to have their data, c) they have rooms full of multi GPU capable systems (Mac and Windows) locally which they would like to leverage, and d) while inference on a single image is fine on CPU, running entire movies for inference through multiple graphs 100% sees an increase in perf using something like MPS vs CPU. Because the community has declined to support / embrace standards and instead uses Nvidia only code, real world use cases get pigeon holed and its really a shame.\r\n\r\nThis isn't an idle request from someone who is a hobbyist running tutorials - GPU inference is important as is supporting diverse GPU / CPU families for diverse workloads on real world hardware. I really hope Google takes this seriously, because it would be great to be able to stick with a single library like TF, which is awesome.\r\n\r\nThank you for hearing me, I'm not trying to rant, but to provide an alternate point of view to the community. ", "@pldelisle , @tscholak , @vade please don't get me wrong. I'd love to have it, and if you search in this thread I joined as a supporter, but as far as I've been following it I reached to the conclusion I wrote, not just because I think it so (I guess a MacBook would melt down if trained with thousands of videos :-D), but with the actual industry facts. Don't wait to have it solved in a short timeframe (IMHO, it won't be solved ever because Apple and Google hate each other since iPhone/Android issue).", "@rogerpasky There already was support for nvidia GPUs on Mac OS. It was just removed in 1.2. \r\n\r\n```Note: As of version 1.2, TensorFlow no longer provides GPU support on Mac OS X.```\r\n\r\nI've cancelled my order for an eGPU (Sonnet's) and will just dual boot Linux on my gaming rig, but this it's kind of bad to just stop supporting something that people were using. Was really hoping to do this on my mac with an eGPU (model training), but I guess that won't happen now: https://github.com/lengstrom/fast-style-transfer", "@rogerpasky Er, you know CoreML supports importing tensor flow models by way of Keras? Apple doesnt 'hate' Google, business is business, One of Apples suppliers is Samsung. Read into that for a moment. Google, Apple, Samsung are businesses and will do what makes money. As a side note. My MacBook Pro hasn't melted from running inference on thousands of movies by the way..  I suspect CUDA was super convenient to adopt and continued support from Nvidia and missed opportunities from AMD got us to where we are. I don't think its nefarious, just cost of making a change vs performance deltas vs cost of staying the course.\r\n\r\nI suspect some genius will come along to help solve the issue.", "I've created a Google Group for a collaborative discussion about bringing deep learning to new places like OpenCL, Mac, iOS, CoreML, Vulkan, etc. If you'd like to help make these happen please join and post a note with your use case or what part of the problem you're working on. There already are people working really hard on efforts to bring TF to more platforms including MIOpen, Codeplay's work, TF-Coriander, and an internal project at my company (Vertex.AI). It would be great to get developers & users all in one place as these efforts are all closely related.\r\n\r\nhttps://groups.google.com/forum/#!forum/deep-learning-everywhere\r\n\r\n@benoitsteiner @hughperkins @cathalgarvey \r\n@rogerpasky @vade @tscholak @pldelisle @adityaatluri @chocol4te @justinrmiller", "@justinrmiller I have a eGPU on Sierra (Titan Xp in a Sonnet enclosure) running Tensorflow 1.2.1 (CUDA 8, cuDNN 6) which wasnt too much trouble if you dont mind building from scratch. If you have any trouble let me know.\r\n\r\n```\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN Xp, pci bus id: 0000:4f:00.0)\r\n\r\nIn [5]: tf.__version__\r\nOut[5]: '1.2.1'\r\n```", "@danbarnes333 That's awesome! Thanks for the info!", "@danbarnes333 how did you get tf 1.2 to build with cuDNN 6? Did you use LLVM? GCC? I only managed to get it to build with cuDNN 5...", "FYI, https://machinelearning.apple.com/", "@tscholak I wont post it here to keep this on OpenCL but ill summarise the steps [here](https://gist.github.com/danbarnes333/486fc02f98046048f7d6bfd5908d561a#file-osx-sierra-titan-xp-egpu-tensorflow-1-2-1-cuda-8-cudnn-6-md).", "@choongng I joined the Google Group but it seems to be quiet. So I'll rant here ;-)\r\n\r\n1. Machine learning / high performance / GPU computing is a fiercely competitive market. NVidia, like it or not, dominates the market *and* keeps their cards and software close to the vest. If you've got a budget and a deadline, you're more or less stuck with NVidia for now.\r\n\r\n2. I've got an old-ish AMD card (\"Bonaire\") and *zero* budget - hobbyist. I've got `caffe` running with the proprietary AMD OpenCL 2 implementation on Arch Linux as of yesterday, and I just got AMD's open source `MIOpen` running the same way this morning. That'll let me train some models; the Bonaire peaks around 1800 GFLOPS single-precision. So if TensorFlow won't run with OpenCL on the Bonaire I won't be using TensorFlow.\r\n\r\n3. If a budget should magically appear I would buy an Intel CPU and an NVidia card and run vendor-supported proprietary software. I'm done doing unpaid QA for vendors like Google, Red Hat, Canonical and AMD. \r\n\r\n    It's taken me three months (and three distros - Fedora 25, Ubuntu 16.04 LTS and Arch) to get something out of a GPU I've had for three years. There are unfixed bugs in Fedora's bug tracker with my name on them. Same goes for Ubuntu and Freedesktop.org. Most of the people who'd be fixing them aren't getting paid either, or they're getting paid to do something else.\r\n\r\n    Yes, AMD's new CPUs are impressive, and yes, most of their software is open source, but budgets and deadlines change things. Support is key. Support is everything!", "@znmeb I did not even knew you could use pre-GCN hardware for TF.\r\nWith my Tahiti I only have support threw one distro (ubuntu 14.01.x) as AMD proprietary drivers only work with older linux kernels for GCN1. (I get TF + openCL via SYCL (untested on 7970) )\r\n\r\nWhere I work the entire R&D department runs green team. They all have PHDs and all but none wrote a single line of cuda (nor OCL). But the tooling is here to accelerate their `Keras` workload. I'm kind of an oddball with my recycled mining GPUs trying to squeeze a second life out of them.\r\n\r\ntl;dr other than green team support will only show up if the AMD GPU market share shows up. \r\nIt's a chicken and egg problem. I have hopes for vega \u2026 but yeah \u2026 aint no 1080Ti killer.", "@acoye FWIW here's the GitHub post that got me going this weekend after thrashing and Googling since April: <https://github.com/BVLC/caffe/issues/5804#issuecomment-318789942>. See also <https://github.com/cdeterman/gpuR/issues/77#issuecomment-318814154>. That was my original issue - trying to use my Bonaire to accelerate linear algebra on R.", "@acoye \r\nYou can move on to the latest Linux distros & use a recent custom compiled kernel like 4.11/4.12 with AMDGPU drivers enabled, RADEON disabled and with `CONFIG_DRM_AMDGPU_SI=Y` and/or `CONFIG_DRM_AMDGPU_CIK=Y` set in the kernel configuration, plus AMD firmware for 7970 (Tahiti) in the initramfs => newest AMDGPU-PRO OpenCL will work on any GCN cards. Forget about FGLRX (on older Linux distros) and Clover via RADEON drivers, both are sub-par.\r\nForget about pre-GCN cards also. I tested them using OpenCL on Windows for Caffe, the performance is not worth making an effort for such old cards. As all AMD cards post 2012 should be GCN anyways.", "@naibaf7 I spent a few hours yesterday trying to get AMD's open source stack working. I got `MIOpen` and its dependencies but `hcc` is still missing some bits. I may need to do a custom kernel build to get everything. I don't much care about porting CUDA code or running compiled C++ on the GPU - I want to do number crunching on it. ;-)\r\n\r\nI also saw something on their website about programming it in assembler - *that* I might be interested in, because it's easy to go from assembler to FORTH. ;-)", "@znmeb Yeah I am also trying to get some MIOpen and TensorFlow stuff working on my RX 480, but I don't want to destroy my main development rig, so instead I use IOMMU virtualization and use an Ubuntu 16.04 virtual machine that can use the RX 480. The AMD drivers are very friendly to virtualization (unlike nVidia drivers made for the gaming cards - only the Quadro drivers do).", "@znmeb All you gotta do is `sudo apt-get install rocm miopen-hip`", "@adityaatluri It's in the Arch User Repository but it doesn't install - it doesn't install from GitHub source either. It looks like something simple - it can't find a few dependencies.", "@znmeb Can you create an issue here (https://github.com/RadeonOpenCompute/ROCm/issues) so that we can discuss there? Thanks!", "@adityaatluri Sure - I'm heading to dinner but I'll file it when I get back", "@ebrevdo Any way to use tensorflow GPU in Mac with AMD processor ?\r\n", "@gautam1858 https://news.ycombinator.com/item?id=14894653", "My company has been working on OpenCL deep learning for a while and we have some early results to show. We are focusing on Keras in the near term however we also have built (very) experimental TensorFlow support and will revisit that after our initial release. More details here including initial throughput numbers on AMD: http://vertex.ai/blog/bringing-deep-learning-to-opencl", "Cool!\n\nTiny nitpick: AFAIK, MIOpen is not AMD-specific, as it can link to OpenCL as well as to ROCm. The latter is probably faster, but still; MIOpen is a huge step forward for the \"Open Source Neural Networks On GPU\" shtick, and AMD deserve huge cred for it if it works well on OpenCL.\n\nAugust 14, 2017 5:19 PM, \"Choong Ng\"  wrote:\n\tMy company has been working on OpenCL deep learning for a while and we have some early results to show. We are focusing on Keras in the near term however we also have built (very) experimental TensorFlow support and will revisit that after our initial release. More details here including initial throughput numbers on AMD: http://vertex.ai/blog/bringing-deep-learning-to-opencl (http://vertex.ai/blog/bringing-deep-learning-to-opencl) \n\n\t\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub (https://github.com/tensorflow/tensorflow/issues/22#issuecomment-322235416), or mute the thread (https://github.com/notifications/unsubscribe-auth/ABHR3VYHXFDEX0gPHTGLSbFeHjPfEfsXks5sYHOGgaJpZM4Gex3i).\n", "@cathalgarvey Thanks for the correction, I based my comment off of the system requirements in the MIOpen documentation (https://rocmsoftwareplatform.github.io/MIOpen/doc/html/install.html#prerequisites) but happy to update if there's a better link.", "Wait, I'v been reading this thread/issue for 10 mins now. I got halfway through and I skipped through the rest. Are AMD GPUs supported yet?", "Using a finicky closed source thing that only works on one very old combination of Kernel/OS (codeplay): yes\n\nUsing an old version of tensorflow and without support for some nonlinearities yet (tf-coriander): yes.\n\nReally: not officially. Though AMD are porting to HIP, so I'd expect progress within 3 months or so. Other frameworks already work well due to their efforts.\n\nOn 18 August 2017 02:09:55 GMT+01:00, abrad1212 <notifications@github.com> wrote:\n>Wait, I'v been reading this thread/issue for 10 mins now. I got halfway\n>through and I skipped through the rest. Are AMD GPUs supported yet?\n>\n>-- \n>You are receiving this because you were mentioned.\n>Reply to this email directly or view it on GitHub:\n>https://github.com/tensorflow/tensorflow/issues/22#issuecomment-323233294\n\n-- \nSent from my Android device with K-9 Mail. Please excuse my brevity.", "FWIW I believe the recent versions of PyGpu can use either CUDA or OpenCL. I have all of the software installed on my Arch box but I haven't tested it yet. ", "@abrad1212 yes, this issue has been around on for a while now. The effort is massive and a lot of people are trying to \"get it working\" as @cathalgarvey mentioned.\r\n\r\nA bit of an update from our side. You should be able to use ComputeCpp 0.3.0 on the AMDGPU-pro driver stack for Ubuntu 16.04 the instructions can be found here: http://deep-beta.co.uk/tensorflow-1-3-on-ubuntu-16-04-lts/\r\n\r\nAs well, we are focusing now on performance improvements for different models - there is a lot to do but we are getting there. ", "@lukeiwanski What's your approach to benchmarking? We time the models included with Keras and normalize against TF+cuDNN+K80 because that is a common and well optimized configuration. Our methodology is similar to Max Woolf (http://minimaxir.com/2017/06/keras-cntk/), it's not much code but we'd be happy to share it. We have some throughput numbers on our web site (http://vertex.ai), our code is very slightly faster than TF 1.2 on Xception inference and it would be interesting to compare more approaches side-by-side.", "Are there any Windows solutions? I would install Ubuntu on my PC but I currently don't have enough space to do it.", "ubuntu 14.04\r\ntensorflow master branch\r\nbuild opencl support, and only opencl intel cpu runtime installed.\r\npython2.7\r\nfollow https://developer.codeplay.com/computecppce/latest/getting-started-with-tensflow guide\r\nexecute python classify_image.py\r\nit seems didn't call opencl driver. (I added my opencl icd wrapper, didn't see anything)\r\nIs there any config need add in python code?\r\nLike sess.graph.device('/cpu0')\r\n\r\nBut if I use Eigen skcl guide can run on cpu with OpenCL support. (Also this guide code is a little out of date, need some modify)\r\nhttps://developer.codeplay.com/computecppce/latest/getting-started-with-eigen\r\n\r\nAny people can help to check how tensorflow python interface can also run with OpenCL support.\r\n\r\nAnd build tensorflow with this opt set will not really generate tensorflow binary.  --config=sycl\r\nJust build tensorflow in this command:\r\nbazel build -c opt /tensorflow/tools/pip_package:build_pip_package\r\n\r\nMaybe I build forget --config=sycl\r\nI will try build command and verify whether it can call OpenCL library. After get result, I will post here.\r\nbazel build -c opt tensorflow/tools/pip_package:build_pip_package", "@joe8086 If you modify the tf.Session creation with the below it will show a log in the terminal, is this mentioning SYCL anywhere? \r\ntf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n\r\nFor the Eigen guide do you have any specific feedback where it is out of date?", "@rodburns Thanks.\r\nMy err is build tensorflow miss config option --config=sycl\r\nAfter add this option with this branch code https://github.com/lukeiwanski/tensorflow.git\r\nI can see tensorflow run with OpenCL backend.\r\n\r\nFor Eigen guide, the main error is at:\r\n1, not give correct include file.\r\n2, for array, Tensor, TensorMap not give correct templet parameter.\r\n3, for static_cast not give data type.\r\n\r\nadd more information which maybe give this discuss topic some help.\r\n1, Main tensorflow can't build tensorflow with --config=sycl correct.\r\n2, With CPU OpenCL, the speed is about 4x~8x times spend than normal CPU implement in my environment. \r\n\r\n\r\ntime python classify_image.py \r\n2017-09-07 16:56:29.076054: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\r\n2017-09-07 16:56:29.077967: W ./tensorflow/core/common_runtime/sycl/sycl_device.h:49] No OpenCL GPU found that is supported by ComputeCpp, trying OpenCL CPU\r\n2017-09-07 16:56:29.159775: I ./tensorflow/core/common_runtime/sycl/sycl_device.h:66] Found following OpenCL devices:\r\n2017-09-07 16:56:29.159825: I ./tensorflow/core/common_runtime/sycl/sycl_device.h:68] id: 0, type: CPU, name: Intel(R) Core(TM) i7-6700HQ CPU @ 2.60GHz, vendor: Intel(R) Corporation, profile: FULL_PROFILE\r\n2017-09-07 16:56:30.213375: W ./tensorflow/core/framework/op_def_util.cc:333] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().\r\ngiant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca (score = 0.89107)\r\nindri, indris, Indri indri, Indri brevicaudatus (score = 0.00779)\r\nlesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens (score = 0.00296)\r\ncustard apple (score = 0.00147)\r\nearthstar (score = 0.00117)\r\n\r\nreal\t1m44.473s\r\nuser\t2m8.980s\r\nsys\t1m20.024s\r\n", "Guys, I'm not going to read this entire thread, but if someone could answer my question that'd be great! Can I use Tensorflow with an AMD GPU yet. If so in what operating system, and can I do it with RX Vega? Thanks! ", "@M3L0NM4N Hmmm ... I haven't been following the thread but it looks like there's possibly testable OpenCL code now, at least on *CPU* OpenCL. I have an older AMD GPU (\"Bonaire\") and I have OpenCL running on both the GPU and CPU, so I can test this. I might take a shot at it over the weekend; I really want OpenCL TensorFlow on my GPU.", "tf-coriander works  https://github.com/hughperkins/tf-coriander", "Any tensorflow 1.3 gpu/opencl support out there on macos?", "Latest news: I have successfully built TensorFlow 1.3.1 with OpenCL from the GitHub source. There are quite a few missing pieces in the documentation, and I haven't tried to run anything in the GPU yet, but it is at least working for non-OpenCL CPU. BTW, I do *not* have CPU OpenCL installed, just GPU OpenCL.\r\n\r\nDoes anyone have any test cases for TensorFlow with an OpenCL GPU? I'll have to build one for myself eventually but I was hoping for a quick check.", "@znmeb Yeah, there is test app in issue that I reported. https://github.com/hughperkins/tf-coriander/issues/64\r\n\r\nCould you please let me know if it works in your case ?", "@unoexperto  Yeah - it works (doesn't crash) but there's no indication whether or not it found OpenCL.\r\n\r\n```\r\n\u00a0python ./hello-tensorflow.py \r\nb'Hello, TensorFlow!'\r\n```\r\n\r\nI think the best course of action here is to file a separate issue to request documentation, since it's clear (when you run `./configure` building from source) that there is *code* for OpenCL. That's how I found it, anyhow.", "@znmeb I'm doubtful it found GPU device in your case because in mine it printed debug info in the beginning about selecting GPU device. Perhaps you can recompile with added `printf` to console somewhere in `tensorflow/core/common_runtime/gpu/gpu_device.cc`.", "@unoexperto I joined the discussion Google Group and posted a request for documentation. I'm going to wait to see if anyone responds before I put more effort into this.", "@znmeb What instructions are you following? Have you run clinfo? Have you run computecpp_info? Does that indicate that your OpenCL drivers are installed as expected? The instructions for Ubuntu 14.04 are here https://developer.codeplay.com/computecppce/latest/getting-started-with-tensflow and if you are using 16.04 there are some experimental instructions here http://deep-beta.co.uk/tensorflow-1-3-on-ubuntu-16-04-lts/ ", "@rodburns clinfo and clpeak both run. I haven't done this recently, but when I build caffe from source and run the tests it definitely hits the GPU. So I'm pretty sure the OpenCL / GPU drivers / libraries are working.\r\n\r\nI'm on Arch Linux - kernel is their LTS - linux-lts 4.9.52-1. If it matters, the \"Bonaire\" peaks about 1.7 TFLOPS in 32-bit mode and is in the \"Sea Island\" family of AMD GPUs.\r\n\r\n```\r\nbin/computecpp_info \r\n********************************************************************************\r\n\r\nComputeCpp Info (CE 0.3.2)\r\n\r\n********************************************************************************\r\n\r\nToolchain information:\r\n\r\nGLIBC version: 2.26\r\nGLIBCXX: 20160609\r\nThis version of libstdc++ is supported.\r\n\r\n********************************************************************************\r\n\r\n\r\nDevice Info:\r\n\r\nDiscovered 1 devices matching:\r\n  platform    : <any>\r\n  device type : <any>\r\n\r\n--------------------------------------------------------------------------------\r\nDevice 0:\r\n\r\n  Device is supported                     : UNTESTED - Untested OS\r\n  CL_DEVICE_NAME                          : Bonaire\r\n  CL_DEVICE_VENDOR                        : Advanced Micro Devices, Inc.\r\n  CL_DRIVER_VERSION                       : 2442.7\r\n  CL_DEVICE_TYPE                          : CL_DEVICE_TYPE_GPU \r\n\r\nIf you encounter problems when using any of these OpenCL devices, please consult\r\nthis website for known issues:\r\nhttps://computecpp.codeplay.com/releases/v0.3.2/platform-support-notes\r\n```\r\n\r\nIs somebody collect test logs? It says my device is untested so I'll be testing it. ;-)", "\r\nImpossible for me to build TensorFlow for Sycl/OpenCL !\r\n\r\n**Config :**\r\nUbuntu 16.04\r\nTensorflow r1.3\r\nOpenCL 2.0\r\nComputeCpp CE 0.3.2 (computecpp_info  OK)\r\nIntel HD Graphics 620\r\nBazel 0.5.4\r\n\r\n**Install instruction (OpenCL Intel / ComputeCpp build) :**\r\nhttps://software.intel.com/en-us/articles/opencl-drivers#philinux\r\nhttps://www.codeplay.com/portal/03-30-17-setting-up-tensorflow-with-opencl-using-sycl\r\n\r\n**Error :** \r\n```\r\nERROR: /home/erwang/workspace/ia/tf_original/tensorflow/tensorflow/core/kernels/BUILD:1695:1: C++ compilation of rule '//tensorflow/core/kernels:adjust_contrast_op' failed (Exit 1)\r\nIn file included from tensorflow/core/kernels/adjust_contrast_op.cc:19:\r\nIn file included from ./tensorflow/core/kernels/adjust_contrast_op.h:18:\r\nIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14:\r\nIn file included from external/eigen_archive/Eigen/Core:299:\r\nIn file included from external/local_config_sycl/crosstool/../sycl/include/SYCL/sycl.hpp:20:\r\nIn file included from external/local_config_sycl/crosstool/../sycl/include/SYCL/sycl_interface.h:54:\r\nexternal/local_config_sycl/crosstool/../sycl/include/SYCL/multi_pointer.h:342:3: error: multiple overloads of 'global_ptr' instantiate to the same signature 'void (pointer_t)' (aka 'void (__attribute__((address_space(1))) float *)')\r\n```\r\n\r\nTraining models on my CPU takes ages, I really need OpenCL/GPU acceleration ...", "@ErwanGalline We are in process of upsreaming changes to Eigen ( https://bitbucket.org/benoitsteiner/opencl/pull-requests/16/changes-required-for-new-computecpp-ce/diff#comment-None ) that will fix issue you are seeing. \r\n\r\nAs well we are preparing to upstream performance improvements to Eigen - this is bit tricky and needs coordination with @benoitsteiner to avoid stream of merge conflicts - but we are getting there.\r\n\r\nFor AMD users I would suggest trying out my fork: https://github.com/lukeiwanski/tensorflow/tree/dev/amd_gpu\r\nSet Up instructions for Ubuntu 16.04 can be found here: http://deep-beta.co.uk/tensorflow-1-3-on-ubuntu-16-04-lts/\r\nAll the changes will be upstream to tensorflow after mentioned earlier Eigen changes are in place.\r\n\r\nHope that helps.", "@lukeiwanski Does your fork only support AMD R9 Nano / AMD FirePro GPU? ", "@lukeiwanski Is there a test case I can use to verify that I'm using the GPU? I can monitor it with `radeontop` but I'd like something that uses TensorFlow itself.", "@ZixuanLiang no, not only.. \r\nWe currently test on AMD ( R9 380, R9 Nano, FirePro ). We know Intel GPU exposes some driver bugs, but there are fixes coming. And we have announced Renesas R-Car and expect more to follow.\r\n  \r\nI believe that Xilinx is upstreaming support for triSYCL https://github.com/tensorflow/tensorflow/pull/12882 - so FPG's (?) - @keryell should know more about that \r\n\r\n@znmeb  `bazel test -c opt --config=sycl --test_output=all //tensorflow/python/kernel_tests:basic_gpu_test` should be a fair verification .. output should looks something like this:\r\n```INFO: From Testing //tensorflow/python/kernel_tests:basic_gpu_test:\r\n==================== Test output for //tensorflow/python/kernel_tests:basic_gpu_test:\r\n2017-10-05 10:53:52.727745: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2017-10-05 10:53:53.059908: I ./tensorflow/core/common_runtime/sycl/sycl_device.h:66] Found following OpenCL devices:\r\n2017-10-05 10:53:53.059926: I ./tensorflow/core/common_runtime/sycl/sycl_device.h:68] id: 0, type: GPU, name: Tonga, vendor: Advanced Micro Devices, Inc., profile: FULL_PROFILE\r\n.....\r\n```\r\n", "@lukeiwanski Thank you I will try it on AMD GPU", "@lukeiwanski The build and test seem to be working on my Bonaire. I am using Python 3.6, though, and the instructions use Python 2.7. Do I need to use 2.7 or will 3.6 work? ", "@znmeb Following https://github.com/tensorflow/tensorflow/issues/6533#issuecomment-273852647 it seems Python 3.6 should be working - I haven not tried it though", "@lukeiwanski Is that a ComputeCpp version that can build TF at the moment ?\r\nI tried various versions between 0.3.2 and 0.1.4 and none worked. They all ended up with the \"multiple overloads of 'global_ptr' instantiate to the same signature\" error.\r\nBtw, I cannot find the TensorDeviceSycl.h file in TF sources, is that a renamed one ? Is it possible to apply the patch to current sources ?\r\n\r\nThanks in advance.", "@eLvErDe ComputeCpp 0.3.2 can build: https://github.com/lukeiwanski/tensorflow/tree/dev/amd_gpu\r\n\r\nUpstream is missing a patch to Eigen that fixes it.. see https://github.com/tensorflow/tensorflow/issues/22#issuecomment-334154564", "Any idea how to inject this Eigen patch during bazel build ? Maybe we should bump somewhere Eigen tgz version to get the fixed one ?\r\n\r\nThanks, Adam.", "https://github.com/lukeiwanski/tensorflow/commit/8468d65e87e083337f18616f75ac56d3296d6ab1\r\n\r\nThis commit would be enough to get it built ?", "yes, you should be able to cherry-pick that", "Well sadly, that's clearly not sufficient, here're some of the next build failures:\r\n\r\n```\r\nexternal/eigen_archive/Eigen/src/Core/util/BlasUtil.h:63:63: error: no type named 'ReturnType' in 'Eigen::ScalarBinaryOpTraits<cl::sycl::vec<float, 4>, std::complex<float>, Eigen::internal::scalar_product_op<cl::sycl::vec<float, 4>, std::complex<float> > >'\r\n  typedef typename ScalarBinaryOpTraits<LhsScalar,RhsScalar>::ReturnType Scalar;\r\n          ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~\r\n```\r\n\r\n```\r\nexternal/eigen_archive/Eigen/src/Core/util/BlasUtil.h:69:34: error: invalid operands to binary expression ('const cl::sycl::vec<float, 4>' and 'const std::complex<float>')\r\n  { return conj_if<ConjLhs>()(x) *  conj_if<ConjRhs>()(y); }\r\n           ~~~~~~~~~~~~~~~~~~~~~ ^  ~~~~~~~~~~~~~~~~~~~~~\r\n```", "@eLvErDe there are few commits that you have to apply to get it compiling.\r\nI would suggest using tip of dev/amd_gpu or if you don't want to change your current branch.. you can merge dev/amd_gpu to it.", "Actually I m working on my unofficial Debian/Ubuntu package so I'm trying to keep close to official 1.3.1 release. I can live without OpenCL support but I'd to be ready to enable it as soon as it's correctly supported. Maybe I'll update packages against your branch for testing purposes, but that's enough for today ;)", "I have around ten different varieties of AMD GPUs in my mining rigs. (from 7970 to RX 480 running ubuntu 16.04 and amdgpu-pro). Let me know if I can contribute by testing anything.", ">> Let me know if I can contribute by testing anything.\nHow about https://github.com/ROCmSoftwarePlatform/hipCaffe\nhttps://github.com/ROCmSoftwarePlatform/hipeigen\n\n\nOn Tue, Oct 17, 2017 at 10:54 AM, slundell <notifications@github.com> wrote:\n\n> I have around ten different varieties of AMD GPUs in my mining rigs. (from\n> 7970 to RX 480 running ubuntu 16.04 and amdgpu-pro). Let me know if I can\n> contribute by testing anything.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22#issuecomment-337309059>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AA6MNxXJ-G3nCQUA9RucrJ8y4vs5NPtLks5stOnbgaJpZM4Gex3i>\n> .\n>\n", "@lukeiwanski Will your fork support AMD GPUs on macOS as well?", "Hi, \r\n  I was building tensorflow APIs on Ubuntu16.04 x64 for my Android device with GPU(Mali-T720) enabled, \r\n\r\nMy OS info:\r\nUbuntu 16.04 x64\r\nComputer GPU: NVIDIA 1080Ti\r\nCUDA 8.0\r\nCUDNN 5.1 ( though I do not use cuda or cudnn for building )\r\nbazel 0.5.2\r\nComputeCpp CE 0.3.2\r\n\r\nmy build.sh is:\r\n'\r\n bazel build -c opt --config=sycl //tensorflow/contrib/android:libtensorflow_cc.so --cxxopt=\"-\r\n  std=c++11\" --cxxopt=\"-DTENSORFLOW_DISABLE_META\" --verbose_failures --\r\n  crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --\r\n  cpu=armeabi-v7a \r\n'\r\n  before build. I  export LD_LIBRARY_PATH=my_sycl_lib_path=$LD_LIBRARY_PATH, build without ' --config=sycl ' is fine and I got the correct libtensorflow_cc.so, but with ' --config=sycl ', the final result turned out missing -lComputeCpp without any other compile errors\r\n\r\n  Full log like this:\r\n\r\nERROR: /home/e0024/workspace/tensorflow/tensorflow/contrib/android/BUILD:102:1: Linking of rule '//tensorflow/contrib/android:libtensorflow.so' failed: link_dynamic_library.sh failed: error executing command \r\n  (cd /home/e0024/.cache/bazel/_bazel_e0024/783dad02ec856015f56356584726dd10/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    COMPUTECPP_TOOLKIT_PATH=/home/e0024/workspace/source/computeCppForSYCL1.2 \\\r\n    HOST_CXX_COMPILER=/usr/bin/g++ \\\r\n    HOST_C_COMPILER=/usr/bin/gcc \\\r\n    LD_LIBRARY_PATH=/home/e0024/workspace/source/computeCppForSYCL1.2/lib:/home/e0024/workspace/caffe/build/lib:/home/e0024/workspace/cudnn/lib64: \\\r\n    PATH=/home/e0024/bin:/home/e0024/.local/bin:/home/e0024/workspace/Anaconda2/bin:/opt/cuda:/home/e0024/workspace/source/protoc-3.3.0-linux-x86_64/bin:/home/e0024/workspace/bazel/output:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/home/e0024/workspace/Anaconda2/bin/python \\\r\n    PYTHON_LIB_PATH=/home/e0024/workspace/Anaconda2/lib/python2.7/site-packages \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL=1 \\\r\n  external/bazel_tools/tools/cpp/link_dynamic_library.sh no ignored ignored ignored external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc -shared -o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/contrib/android/libtensorflow.so '-Wl,-rpath,$ORIGIN/../../../_solib_armeabi-v7a/_U@local_Uconfig_Usycl_S_Ssycl_Csyclrt___Uexternal_Slocal_Uconfig_Usycl_Ssycl_Slib' -Lbazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/_solib_armeabi-v7a/_U@local_Uconfig_Usycl_S_Ssycl_Csyclrt___Uexternal_Slocal_Uconfig_Usycl_Ssycl_Slib -Wl,-whole-archive bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/c/libc_api.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/libandroid_tensorflow_lib.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/libandroid_tensorflow_kernels.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/libandroid_tensorflow_lib_lite.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/libprotos_all_cc.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/external/protobuf/libprotobuf.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/external/protobuf/libprotobuf_lite.a -Wl,-no-whole-archive -lComputeCpp external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/libgnustl_static.a external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/libsupc++.a -landroid -llog -lm -z defs -s -Wl,--gc-sections '-Wl,-soname=libtensorflow.so' -Wl,--version-script tensorflow/c/version_script.lds -lz -static-libgcc -no-canonical-prefixes '-march=armv7-a' -Wl,--fix-cortex-a8 '--sysroot=external/androidndk/ndk/platforms/android-14/arch-arm'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nexternal/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/../lib/gcc/arm-linux-androideabi/4.9/../../../../arm-linux-androideabi/bin/ld: warning: skipping incompatible bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/_solib_armeabi-v7a/_U@local_Uconfig_Usycl_S_Ssycl_Csyclrt___Uexternal_Slocal_Uconfig_Usycl_Ssycl_Slib/libComputeCpp.so while searching for ComputeCpp\r\nexternal/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/../lib/gcc/arm-linux-androideabi/4.9/../../../../arm-linux-androideabi/bin/ld: error: cannot find -lComputeCpp\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/contrib/android:libtensorflow.so failed to build\r\nINFO: Elapsed time: 617.736s, Critical Path: 54.66s\r\n\r\n  uhm.... I wanna build tensorflow APIs in the arch of arm with GPU(Mali-T720) enabled, \r\nAppreciated if someone could leave some experiences or suggestions here. Thx a looooot.\r\n\r\n\r\n\r\n\r\n", "Come to my talk next week at Arm TechCon, @laMia482 ! http://schedule.armtechcon.com/session/running-tensorflow-machine-learning-on-arm-embedded-hardware/850230\r\n\r\nYou will need Mali drivers with SPIR-V support, which is probably not easily available, yet. And you will need a ComputeCpp runtime for Android with Arm CPU support and SPIR-V support, which is also not available (yet). So, you will have to be just a _little_ bit patient.", "We (Vertex.AI) have just open sourced PlaidML, our deep learning stack with support for running Keras on OpenCL. TensorFlow support is coming, help there would be welcome. And yes, Mac support is on the way (also Windows). http://vertex.ai/blog/announcing-plaidml @ggaabe ", "@choongng I wanted to give it a try but failed. \r\n``pip search plaidml`` returns\r\n```\r\nplaidml (0.1.0rc3)        - PlaidML machine learning accelerator\r\n```\r\nBut ``pip install plaidml`` or ``pip install plaidml==0.1.0rc3``\r\nreturns\r\n```\r\nCould not find a version that satisfies the requirement plaidml (from versions: )\r\nNo matching distribution found for plaidml\r\n```", "@hy9be  I think it would be more appropriate to make an issue at [plaidml repository](https://github.com/plaidml/plaidml) instead of here, since this issue is about supporting OpenCL in tensorflow. Additionally by looking at the installation instructions there your pip install command may be incorrect.", "  Thank you @andrewrichards  fo your attention and your session speech.\r\n\r\n  But for now for me(a graduate student), to build an app using Tensorflow on Android device and want GPU(Mali-T720) activated, what are required to abtain Mali driver with SPIP-V support and ComputeCpp runtime for Android with Arm CPU support and SPIR-V support.\r\n\r\n  Since I've downloaded ComputeCpp(Ubuntu16.04 x64 with bin/ doc/ include/ lib/) on CodePlay homepage, yesterday I run:\r\n`\r\nbazel build -c opt --config=sycl //tensorflow/contrib/android:libtensorflow_cc.so --cxxopt=\"-std=c++11\" --cxxopt=\"-DTENSORFLOW_DISABLE_META\" --verbose_failures --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a\r\n`\r\n  errors said ` libComputeCpp.so incompatible `, so I consider maybe I need ComputeCpp for Android with Arm CPU support and SPIR-V support, but I could not find any source code to build an Android ComputeCpp, there are only samples at github. \r\n\r\n  And you've said ComputeCpp for Android is now not available, so is there any plan to support Android device or how can I get it if supported.\r\n\r\n", "For AMD gpu and linux users, AMD recently released HIP port of tensorflow [here](https://github.com/ROCmSoftwarePlatform/hiptensorflow). You might be interested.\r\n\r\nI haven't tested it, though.", "I can test it - stay tuned. Looks like it's failing CI though.", "Indeed it's failing. Still at an early stage, I guess.", "I tested it, got segfault in MNIST example immediately.\r\nDon't know what I am doing wrong here.\r\n\r\n```\r\n$ python ./convolutional.py \r\nI tensorflow/stream_executor/dso_loader.cc:130] Couldn't open CUDA library libhipblas.so. LD_LIBRARY_PATH: :/home/masa/project/rendering/RadeonProRender-Baikal/Bin/Release/x64:/usr/local/lib64:/opt/CodeXL_2.5-25:/usr/lib/x86_64-linux-gnu/:/opt/CodeXL_2.5-25/RuntimeLibs/QT/\r\nI tensorflow/stream_executor/cuda/cuda_blas.cc:2305] Unable to load HIPBLAS DSO.\r\nI tensorflow/stream_executor/dso_loader.cc:130] Couldn't open CUDA library libhipfft.so. LD_LIBRARY_PATH: :/home/masa/project/rendering/RadeonProRender-Baikal/Bin/Release/x64:/usr/local/lib64:/opt/CodeXL_2.5-25:/usr/lib/x86_64-linux-gnu/:/opt/CodeXL_2.5-25/RuntimeLibs/QT/\r\nI tensorflow/stream_executor/cuda/cuda_fft.cc:344] Unable to load cuFFT DSO.\r\nI tensorflow/stream_executor/dso_loader.cc:139] successfully opened CUDA library libhip_hcc.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:130] Couldn't open CUDA library libhiprng.so. LD_LIBRARY_PATH: :/home/masa/project/rendering/RadeonProRender-Baikal/Bin/Release/x64:/usr/local/lib64:/opt/CodeXL_2.5-25:/usr/lib/x86_64-linux-gnu/:/opt/CodeXL_2.5-25/RuntimeLibs/QT/\r\nI tensorflow/stream_executor/cuda/cuda_rng.cc:338] Unable to load cuRAND DSO.\r\nI tensorflow/stream_executor/dso_loader.cc:139] successfully opened CUDA library libMIOpen.so locally\r\nExtracting data/train-images-idx3-ubyte.gz\r\nExtracting data/train-labels-idx1-ubyte.gz\r\nExtracting data/t10k-images-idx3-ubyte.gz\r\nExtracting data/t10k-labels-idx1-ubyte.gz\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:633] creating context when one is currently active; existing: 0x7f94fa357e90\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:892] Found device 0 with properties: \r\nname: Fiji [Radeon R9 FURY / NANO Series]\r\nmajor: 2 minor: 0 memoryClockRate (GHz) 1\r\npciBusID 1\ufffd\ufffd\ufffd\ufffd\r\nTotal memory: 4.00GiB\r\nFree memory: 3.75GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:913] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:923] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:972] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Fiji [Radeon R9 FURY / NANO Series], pci bus id: 1\ufffd\ufffd\ufffd\ufffd)\r\nInitialized!\r\nI tensorflow/core/kernels/conv_ops.cc:604] running auto-tune for Convolve\r\nInvoking clang-ocl on \"/tmp/miopen-MIOpenUtilKernels.cl-c377-1df5-8b6a-884c/MIOpenUtilKernels.cl\"\r\n/opt/rocm/bin/clang-ocl -DNUM_CH_PER_WG=1 -DNUM_IM_BLKS_X=1 -DNUM_IM_BLKS=4 -DLOCAL_MEM_SIZE=432 -DSTRIDE_GT_1=0 -DTILE_SZ_X=32 -DTILE_SZ_Y=8 -DUSE_IM_OFF_GUARD=1 -mcpu=gfx803 -Wno-everything MIOpenUtilKernels.cl -o /tmp/miopen-MIOpenUtilKernels.cl-c377-1df5-8b6a-884c/MIOpenUtilKernels.cl.o\r\nwriting gemm kernel to \"/tmp/miopen-tinygemm.cl-836e-c4d4-abd3-b292/tinygemm.cl\"\r\nInvoking clang-ocl on \"/tmp/miopen-tinygemm.cl-836e-c4d4-abd3-b292/tinygemm.cl\"\r\n/opt/rocm/bin/clang-ocl -mcpu=gfx803 -Wno-everything tinygemm.cl -o /tmp/miopen-tinygemm.cl-836e-c4d4-abd3-b292/tinygemm.cl.o\r\nGCN assember path: /opt/rocm/opencl/bin/x86_64/clang\r\nArugment: --version \r\nInvoking clang-ocl on \"/tmp/miopen-MIOpenConvDirUniC.cl-f5fc-85f4-7079-a024/MIOpenConvDirUniC.cl\"\r\n/opt/rocm/bin/clang-ocl -DMLO_HW_WAVE_SZ=64 -DMLO_DIR_FORWARD=1 -DMLO_FILTER_SIZE0=5 -DMLO_FILTER_SIZE1=5 -DMLO_FILTER_PAD0=2 -DMLO_FILTER_PAD1=2 -DMLO_N_OUTPUTS=32 -DMLO_N_INPUTS=1 -DMLO_BATCH_SZ=64 -DMLO_OUT_WIDTH=28 -DMLO_OUT_HEIGHT=28 -DMLO_OUT_BATCH_STRIDE=25088 -DMLO_OUT_CHANNEL_STRIDE=784 -DMLO_OUT_STRIDE=28 -DMLO_IN_WIDTH=28 -DMLO_IN_HEIGHT=28 -DMLO_IN_BATCH_STRIDE=784 -DMLO_IN_CHANNEL_STRIDE=784 -DMLO_IN_STRIDE=28 -DMLO_IN_TILE0=28 -DMLO_IN_TILE1=8 -DMLO_OUT_TILE0=28 -DMLO_OUT_TILE1=8 -DMLO_GRP_TILE0=16 -DMLO_GRP_TILE1=8 -DMLO_ACTIVE_ALUS=112 -DMLO_N_ALUTILES_PERSTACK=2 -DMLO_OUT_PIX_TILE0=2 -DMLO_OUT_PIX_TILE1=2 -DMLO_N_STACKS=1 -DMLO_N_OUT_TILES=8 -DMLO_N_OUT_TILES_PERSTACK=16 -DMLO_N_IN_TILES_PERSTACK=1 -DMLO_N_READ_PROCS=128 -DMLO_CONV_BIAS=0 -DMLO_ALU_VTILE0=14 -DMLO_ALU_VTILE1=4 -mcpu=gfx803 -Wno-everything MIOpenConvDirUniC.cl -o /tmp/miopen-MIOpenConvDirUniC.cl-f5fc-85f4-7079-a024/MIOpenConvDirUniC.cl.o\r\nInvoking clang-ocl on \"/tmp/miopen-MIOpenConvFFT.cl-2fbf-2ba2-0088-ebfc/MIOpenConvFFT.cl\"\r\n/opt/rocm/bin/clang-ocl -DCFF_TRANSP_WT_MOD16=1 -DCFF_CGEMM_CHOICE_0=1 -DCFF_IMG_SZ_28_28 -DCFF_IMG_H=28 -DCFF_IMG_W=28 -DCFF_BATCH=64 -DCFF_NFILTER=32 -DCFF_CHANNELS=1 -DCFF_HALFW=1148928 -mcpu=gfx803 -Wno-everything MIOpenConvFFT.cl -o /tmp/miopen-MIOpenConvFFT.cl-2fbf-2ba2-0088-ebfc/MIOpenConvFFT.cl.o\r\nSegmentation fault (core dumped)\r\n```", "@masahi  - make sure you have rocm 1.6.4 base installed.\r\n", "@bensander Thanks, I'll upgrade.", "@bensander Anything else I need from the AMD stack? All I have now is the AMD proprietary opencl library that uses the open source \"amdgpu\" driver.", "@masahi - if you install the rocm and rocm-libs (i.e. \"apt-get install rocm rocm-libs\") that should be all your need.  The rocm_docs at the repot has full instructions including expected results.", "@bensander how do I know if I am running rocm 1.6.4 correctly (and not 1.6.3) ?", "@masahi just a guess : you should ask the question on a more related place for your issue, such as AMD or RoCM project rather than here...", "@keryell right, I'm getting off topic. I stop here.\r\nAnyway, I couldn't get hiptensorflow working on my system. I will try later with clean Ubuntu install.", "@masahi - just open an issue over there and we'll get you set up. ", "Hi, I just want to mention that I was able to get hiptensorflow working, thanks to @bensander and other folks at AMD. I can run all examples in their quickstart guide.\r\n\r\nThanks", "For those who want to try TensorFlow on AMD hardware using ROCm, I wrote a blog describing how to run Fast.ai notebooks using AMD Fury Nano. \r\nhttp://briansp2020.github.io/2017/11/05/fast_ai_ROCm/", "\ud83d\udc4d can't wait for this!", "ROCm 1.7 is on the way, with what sounds like proper Tensorflow support!\r\n\r\nhttps://www.phoronix.com/scan.php?page=news_item&px=AMD-ROCm-1.7-Released", "Tensorflow port to AMD GPU:\r\nhttps://github.com/ROCmSoftwarePlatform/hiptensorflow/blob/hip/README.ROCm.md\r\n\r\nIt works great for me.  My hardware setting:\r\nGPU: AMD Radeon RX 480\r\nCPU: Intel Xeon 2603 v3\r\nMB: supermicro x10srl-f\r\n\r\nThe key is mother board and CPU have to support PCIe v3\r\n\r\nIts performance is similar to Nvidia 980Ti\r\n", "I can't even get the \"supported\" AMD drivers to work on my \"supported\" Ubuntu 16.04 LTS install. Planned obsolescence?", "znmeb, what is your AMD GPU ?  If you have dual GPUs, disable the unsupported one from BIOS.", "Couldn't read the whole thread... what's the present status for tensorflow on OpenCL on MacOS (sierra +)? Specifically, I have an Intell Iris GPU and was hoping if I could build from source Tf+Open CL support for it.\r\nAlso, tf corrainder seems to run fine, at version 1.2.", "@varun19299 FWIW there's an Intel SDK for OpenCL - I've got it on my ancient Sandy Bridge laptop but I'm sure it'll work on your machine. https://software.intel.com/en-us/intel-opencl", "Is this currently in a usable state on non-ubuntu linux systems?  The roadmap page simply links here.", "@pfc Is what currently usable on non-Ubuntu Linux? TensorFlow using OpenCL in general? Or TensorFlow using OpenCL on an AMD GPU? I'll assume the latter, since it's the only reason you'd want to run TensorFlow using OpenCL. For an NVidia GPU you'd use the NVidia drivers / libraries and for CPU-only there's nothing to gain from OpenCL.\r\n\r\nI had this working a few weeks ago on Arch Linux, using the proprietary ComputeCpp SYCL library and an AMD \"Bonaire\" (Sea Islands architecture) GPU. There's a new ComputeCpp release that I need to test but I'm guessing it will work. \r\n\r\nIt turns out that the AMDGPU Pro proprietary libraries you need to make this work don't run on Ubuntu 16.04.3. The upgrade from 16.04.2 brought in a newer Linux kernel and X Server, and AMD has yet to ship something that works on it. See <http://support.amd.com/en-us/kb-articles/Pages/AMDGPU-PRO-Driver-Compatibility-Advisory-with-Ubuntu-16.04.2-and-16.04.3.aspx> for the details. I have been unable to make AMD OpenCL work on Ubuntu.\r\n\r\nThere's an experimental AMD version of TensorFlow that uses a compiler to translate CUDA code to OpenCL code but I haven't tested that either. In the absence of a supported driver it's useless.", "https://github.com/ROCmSoftwarePlatform/hiptensorflow/tree/hip/rocm_docs is the officially supported way to run tensor flow on AMD hardware.", "@bensander Does the ROCm runtime work on Ubuntu 16.04.3? I haven't been able to get it working.\r\n\r\nP.S.: Do you have any insight if / when the AMDGPU-Pro setup will work on Ubuntu 16.04.3? I need that for another project.", "Hmm, I don\u2019t (and wouldn\u2019t) fun Ubuntu anywhere but I do have a CentOS 7 w/ repos and a GTX1080TI in it, running kernel 4.14.x and the latest Nvidia beta driver, so I could help test it out on there at some point today if it helps?\n\n--\nSam McLeod\n\n> On 7 Dec 2017, at 07:28, M. Edward (Ed) Borasky <notifications@github.com> wrote:\n> \n> @bensander Does the ROCm runtime work on Ubuntu 16.04.3? I haven't been able to get it working.\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n", "@sammcj Why would you run an NVidia GPU with OpenCL when there are perfectly good CUDA libraries for it?", "Just to help test it for you!\n\nNo worries if you don\u2019t need a hand testing, just thought I\u2019d offer. I haven\u2019t even tried that machine with cuda TBH, I\u2019ve only tried it on MacOS where I can\u2019t use OpenCL through Docker at the moment.\n\n--\nSam McLeod\n\n> On 7 Dec 2017, at 08:16, M. Edward (Ed) Borasky <notifications@github.com> wrote:\n> \n> @sammcj Why would you run an NVidia GPU with OpenCL when there are perfectly good CUDA libraries for it?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n", "@znmeb I was going to try ComputeCpp SYCL however they only provide the ubuntu installer(I am also on arch) and the aur install script is broken.  It is good to hear that it can work.  If I get desperate enough I may try it out.\r\n@bensander That looks like exactly what I need to get ADM support however I am worried by the fact that this code has not been back-ported to TF and that its code was last updated over 2 months ago given that my code targets TF 1.4.0\r\nIt seems like at the moment tensorflow basically ties you to Nvidia, at least for us \"mortal\" programmers.  Lack of documentation/updated roadmap doesn't help.  I wouldn't mind helping out in any way I could however I've had little success getting things working so far.", "@pfc I got the ComputeCpp SYCL working on Arch - there was a binary tarball on their website when I did it.", "In this news about the release of SYCL 1.2.1\r\nhttps://www.roboticstomorrow.com/news/2017/12/06/the-khronos-group-releases-finalized-sycl-121-/11107/\r\nit says : \r\n **_The new specification incorporates significant experience gained from three separate implementations and feedback from developers of machine learning frameworks such as TensorFlow, which now supports SYCL alongside the original CUDA accelerator back-end._**\r\n\r\nDoes that mean it is now possible to \"easily\" run TensorFlow on AMD GPU that support OpenCL 1.2 on which SYCL is built ?\r\n", "\"Easily\" in the sense that some low-level software / drivers / libraries for the AMD hardware are where most of the broken stuff is, not in the hardware or TensorFlow or the OpenCL standards or SYCL. ;-) If you've got working AMD GPU drivers and working OpenCL libraries you've got TensorFlow on AMD GPUs.\r\n\r\nMy working setup for an AMD Bonaire (Sea Islands architecture):\r\n\r\nArch Linux with the `amdgpu` kernel module loaded and the `radeon` kernel module blacklisted\r\nThe Arch User Repository package `opencl-amd`\r\nThe ComputeCpp library\r\nTensorFlow built from source on my workstation using @lukeiwanski's fork: \r\n\r\n<https://github.com/tensorflow/tensorflow/issues/22#issuecomment-334154564>\r\n", "I am a bit surprise by what you said \" If you've got working AMD GPU drivers and working OpenCL libraries you've got TensorFlow on AMD GPUs\". I had understood that TensorFlow \"Official\" version was not running on OpenCL (CUDA only). Seems I got confused.\r\nI was quite happy to find the PlaidML project that at least allow for some Keras code to run on my iMac with AMD Redeon HD 6970. (https://groups.google.com/forum/#!topic/plaidml-dev/ksFMgxjgKrM)  AFAIK you have also tried that Framework.\r\nI will give a go running TensorFlow on the Ubuntu VirtualBox were Tensorflow is already running (CPU only).", "@PALYGAP I don't think VirtualBox exports OpenCL from a Mac host into a Linux guest, and Ubuntu 16.04.3 doesn't work right now. I don't have a Mac so I don't have any way of testing things.", "Did anyone successfully try out working TensorFlow on AMD via OpenCL  and succeed ?. ", "@mohnkhan I have the @lukeiwanski fork working (Arch Linux) - see <https://github.com/tensorflow/tensorflow/issues/22#issuecomment-349877056>. I'm waiting on some more AMDGPU-Pro work before I publish a blog post - see <https://github.com/corngood/archlinux-amdgpu/pull/54>.", "@znmeb  Thank you for the inputs", "@mohnkhan BTW, AMD are building an alternative path that's fully open source - translating the CUDA code to OpenCL code with a compiler toolchain. I'm not sure what the status of that is for the older cards like mine though.", "If you are going to write an article, I guess it wouldn't hurt to also explain (took 3 hours to get the whole picture): \r\n\r\n* TF has in fact a SYCL 1.2 backend. No \\*actual\\* opencl. \r\n* in turn, you have two [implementations](https://github.com/tensorflow/tensorflow/tree/master/third_party/sycl/crosstool) of the standard (trisycl [looks](https://www.youtube.com/watch?v=LUzZxdElY0Y) cool, but it's [limited](https://github.com/tensorflow/tensorflow/tree/master/third_party/sycl/crosstool) atm)\r\n* In the end, ComputeCpp 'hooks' [SPIR](https://en.wikipedia.org/wiki/Standard_Portable_Intermediate_Representation)/SPIR-V (in addition to PTX, but this is really [another story](https://www.codeplay.com/portal/12-06-17-adding-experimental-ptx-support-to-computecpp-for-nvidia))\r\n\r\nAnd this is what eventually gets you straight to your bloody yearned OpenCL 1.2 (w/  cl_khr_spir ext)\r\n\r\nHIP instead is yet another backend, sits [opposite](https://www.phoronix.com/forums/forum/linux-graphics-x-org-drivers/open-source-amd-linux/911118-radeon-open-compute-1-3-platform-brings-polaris-other-features/page3#post911265) to SYCL, and targets only and exclusively ROCm (or well, lol, even in turn cuda if you have an nvidia gpu.. but this is again another story)\r\n\r\n> AMD are building an alternative path that's fully open source - translating the CUDA code to OpenCL code with a compiler toolchain. \r\n\r\nNope. You are talking about HIP, and.. that's actually it, what you eventually convert your code *to*. Which is [**not**](https://github.com/ROCm-Developer-Tools/HIP/blob/master/docs/markdown/hip_faq.md#how-does-hip-compare-with-opencl) OpenCL. \r\nHIP then runs on ROCm as I was saying... \r\nROCm which is *also* what runs OpenCL for you [(on supported cards](https://www.phoronix.com/forums/forum/linux-graphics-x-org-drivers/open-source-amd-linux/911118-radeon-open-compute-1-3-platform-brings-polaris-other-features#post911129)), but please I'd stress everybody to notice how the relations is only forward from ROCm, never \"intra-sub-layers\"\r\n\r\nWhat you are perhaps thinking about could be [coriander](https://github.com/hughperkins/coriander). \r\n\r\n> I'm not sure what the status of that is for the older cards like mine though.\r\n\r\nSummed up [here](https://www.phoronix.com/forums/forum/linux-graphics-x-org-drivers/open-source-amd-linux/911118-radeon-open-compute-1-3-platform-brings-polaris-other-features/page4#post911344): fully fledged AMDGPU-PRO, amdgpu-pro-opencl-only driver as you are doing now ... Or continuing to wait until the end of the decade for somebody to finally make clover usable. \r\n\r\nAlso, fglrx... But if that's hard to recommend for pre-gcn cards, I guess it's just better to draw a veil over. ", "@mirh \r\n1. I'm not concerned with pre-GCN cards. Mine's a Sea Islands and I'm not planning on acquiring anything older. Then again, I'm not planning on acquiring another AMD GPU either. ;-)\r\n2. I don't know whether ROCm will run on my workstation - there's no open source hardware tester that can give me a yes-or-no answer. I've opened an issue for that and received no response.\r\n3. SPIR-V is a compiler target - I took a look at it and threw my hands up, not having a budget to hire a compiler writer.\r\n\r\nSo that leaves SYCL ... or throwing up my other two hands and doing everything with Keras, which has TensorFlow, Theano (which is getting frozen), CNTK or PlaidML back ends. From a purely engineering economics point of view, Keras / PlaidML is a big winner provided I get TensorBoard somehow.", "@mirh thanks for the good summary with all the links. I think you have not wasted your 3 hours... :-)", "> I don't know whether ROCm will run on my workstation - there's no open source hardware tester that can give me a yes-or-no answer. I've opened an issue for that and received no response.\r\n\r\nAs I told you quite [some times](https://github.com/grmat/opencl-amd/issues/2#ref-pullrequest-271195948), no it won't work. \r\nPre GCN 3rd gen gpus simply lack the hardware for ROCm to either perform or even work at all. \r\n\r\nSPIR(-V).. I'm not sure what you are talking about. It's not your job to care about that. Computecpp makes it from SYCL \"commands\", and then it's all (opencl) driver business. \r\n\r\nYou have what I'm tentatively calling amdgpu-pro-opencl-only, and I'm not sure what's the problem then. \r\nEDIT: would be also cool to have some sort of ETA for Luke's code to land", "@znmeb and everyone\r\n\r\nI have (L)Ubuntu 17.10 incl. kernel 4.14.x and the OpenCL library parts from the AMDGPU Pro 17.40 driver running and can run OpenCL applications like clinfo or Boinc (e.g. Engima@Home, Milkyway@Home) without issue on my AMD A12-9800E APU.\r\n\r\nI can also successfull compile and use the tensorflow (currently version 1.4.1) CPU version. But i fail to successfully compile the OpenCL version of tensorflow. I use computecpp 0.5 (the current one i can download without need to registering) together with vanilla tensorflow 1.4.1 and with the \"dev/amd_gpu\" branch from @lukeiwanski's fork.\r\n\r\nSo could please someone who successfully compiled the OpenCL version of tensorflow provide some information which version of the computecpp library and which branch of which tensorflow git he/she is using?\r\n\r\nThank you", "@AlphasCodes I don't have anything running on Ubuntu - all my working things are on Arch. I do have the machine dual-booted with Ubuntu 16.04.3 but the AMD proprietary libraries don't work there yet. As far as I know they're not supported on 17.10, but if you've got the OpenCL piece working on 17.10 I might add a third boot - I have plenty of disk space. ;-)\r\n\r\nWhat kind of errors are you getting? If they're build errors, you might have a Bazel incompatibility. Bazel is constatnly moving forward like TensorFlow is and sometimes one will get ahead of the other. ", "What you mean, \"not supported\" ?", "[This](http://support.amd.com/en-us/kb-articles/Pages/AMDGPU-PRO-Driver-for-Linux-Release-Notes.aspx). \r\nAs for ubuntu, only 16.04.3 is said to be supported (at least officially then, considering even arch can get it to work after some script magic)\r\nEDIT: 'complete' AMDGPU-PRO driver requires kernel 4.9, that was likely the problem", "If anyone cares, the port of AMDGPU-Pro Driver 17.40 to Arch is ongoing and is very active on GitHub at <https://github.com/corngood/archlinux-amdgpu/pull/54>,\r\n\r\nWe really should close this issue, since, as @mirh pointed out, TensorFlow uses SYCL, not OpenCL. Maybe we should open another one, \"TensorFlow on AMD cards\"??", "No, it's totally legit. \r\nYou want tensorflow to run *eventually* on opencl devices, that's the aim. Legit and end. \r\nSaying it was actually using SYCL was only a technical nitpick I made, because all these acronyms of magically random technologies were making me mad. \r\nEDIT: I'd also like to thank all codeplay guys for their egregious work\r\n\r\nIf you want something all-that-specifically-crafted for amd, I'd recommend to check their [hiptensorflow](https://github.com/ROCmSoftwarePlatform/hiptensorflow). ROCm-only though. And please, let's leave behind this argument. ", "OK i don't know if i have enough time to do the build again and provide the compile errors until the weekend. But i added my existing documentation to my new github repo.\r\n\r\nSee https://github.com/AlphasCodes/DeepLearning for details (my hardware/software setup + AMD OpenCL setup + Tensorflow setup).", "@mirh to clarify the \"acronyms of magically random technologies [...] making [you] mad\":\r\n\r\nIn the Khronos Group realm, OpenCL is the low-level non-single source API and SYCL is the high-level *single-source* C++ domain-specific embedded language (DSeL). SYCL is expected to be built on top of OpenCL, so by transitivity when you use SYCL, often you use OpenCL.\r\n\r\nSince TensorFlow uses Eigen which uses a *single-source* C++ approach with *single-source* CUDA, when it was later ported to OpenCL, SYCL was chosen because it is the Khronos Group standard way to have *single-source* C++.\r\n\r\nBut if you think about CUDA, it is even more subtle.\r\n\r\nAlmost everybody uses the high-level  *single-source* version of CUDA which is actually named \"CUDA **Runtime** API\". This is somehow similar to SYCL.\r\nBut there is actually a less known low-level *non single-source* version of CUDA which is called \"CUDA **Driver** API\", similar to OpenCL, and used for example by the \"CUDA **Runtime** API\" implementation itself.\r\n\r\nSince it is a kind of FAQ, I clarified a little bit https://en.wikipedia.org/wiki/SYCL and https://en.wikipedia.org/wiki/CUDA", "ComputeCpp which is the SYCL implementation you are using with TensorFlow does not yet support Ubuntu 17.10. You would need to stick to Ubuntu 16.04 which is the current LTS. Instructions and pre-requisites are here https://developer.codeplay.com/computecppce/latest/getting-started-with-tensflow\r\n\r\nAs an aside, OpenCL support for TensorFlow does not mean just AMD device support. The SYCL integration is also enabling other OpenCL devices. As part of the work we are doing with TensorFlow, support for ARM and Intel GPUs will be available when the latest drivers from these companies are available. We are also working to enable support for Renesas accelerator processors too for the R-Car platform.", "@rodburns Thanks! I have this working on Arch Linux (4.14.4 kernel) with the opencl-amd library from the Arch User Repository. The card is a Bonaire (GCN 2.0). I'll run the tests on that page to verify that it's doing what it should.", "GCN 2nd gen (aka 1.1) if any, 2.0 doesn't exist. \r\n(should stoop to be so pedantic)", "SUCCESS!\r\n\r\nThe latest \"dev/amd_gpu\" branch commits in @lukeiwanski fork fixed my Tensorflow OpenCL compile issue. I assume it was the SysCL 1.2.1 related commits.\r\n\r\nI successfully compiled a Tensorflow OpenCL version and can use it. See my [Tensorflow Setup documents](https://github.com/AlphasCodes/DeepLearning) for details.\r\n\r\nI also added a benchmarks page where you can find some benchmarks of my setup under different Tensorflow setups (non CPU optimized, CPU optimized, OpenCL) in the future.\r\n\r\nThe AMDGPU Pro driver version 17.50 is also working for me. I updated the related [AMD OpenCL Setup](https://github.com/AlphasCodes/DeepLearning/blob/master/AMD_OpenCL_Setup.md) document.\r\n\r\nThank you to all contributors.", "I did some benchmarks and it seems the iGPU is slower than the 4 available CPU threads except of the [matmul_bench.py](https://github.com/AlphasCodes/DeepLearning/blob/master/matmul_bench.py) benchmark.\r\n\r\nThe initialization of a OpenCL Tensorflow run is also much slower than a CPU only OpenCL Tensorflow run. Something like 5 seconds for CPU vs 1-2 minutes for OpenCL.\r\n\r\nCan anybody confirm such results?", "OK i did some more troubleshooting.\r\n\r\n- i used the Tensorflow MNIST example, see the [Validate a Tensorflow Setup](https://github.com/AlphasCodes/DeepLearning/blob/master/Tensorflow_Validate_Setup.md) document\r\n- i used \"sudo cat /sys/kernel/debug/dri/0/amdgpu_pm_info\" to check/watch the iGPU clock/load and \"top\" to check the CPU load\r\n- the intialization phase until Step 0 took about 6 minutes, the iGPU load was about 0%, the iGPU clock at 300 MHz (the minimal available clock) and the python process CPU usage was about 200% (= 2 threads)\r\n- starting with Step 0 the iGPU load was about 90%, the iGPU clock switched always from 654 MHz - 720 MHz - 800 MHz - 900 MHz (max available clock) and back, the python process CPU usage was about 100% (= 1 CPU thread)", "> I am still trying to get things to compile on Arch.\r\n\r\n[What I used yesterday](https://gist.github.com/mirh/f0042f84e57e610c6b065f17b66212e6). \r\nAfter 14 hours (yes, my potato is very slow) I got [this binary](http://www34.zippyshare.com/v/Nkg9V93b/file.html), if you want to try. ", "I have tried figuring out whats happening but unfortunately i wasn't able to. I'd appreciate if someone who knows about the following can help me come up to speed!\r\n\r\nMost of the above discussion pertained to getting Tensorflow running with OpenCL acceleration on AMD chips. Am I correct in saying this? If I want to get gpu accelerated tensorflow using my integrated graphics card (intel HD 5000) which supports opencl, what should be my approach?\r\n\r\nThanks in advance!", "@znmeb Hi Ed, thanks for replying. I have gotten OpenCL downloaded and running on my system. But my question was - how can I compile tensorflow to actually use the OpenCL libraries?", "https://developer.codeplay.com/computecppce/latest/getting-started-with-tensflow\r\nAnd most importantly for intel https://github.com/codeplaysoftware/computecpp-sdk/issues/78#issuecomment-352411192", "@AlphaCodes Thanks for publishing your results. With regard to the initialisation time, the way OpenCL works is that the code is compiled before execution, so the startup time is the compilation process.\r\n\r\n@brainwave For Intel devices, there is a thread with @mirh [here]( https://github.com/codeplaysoftware/computecpp-sdk/issues/78) that explains how to remove the restrictions around running devices. We have seen issues with Intel drivers which is why these device types are restricted, but we are hoping that updated drivers will be available soon for Intel devices that improve the support. In the meantime you can re-compile TensorFlow with the change to test your own Intel hardware. We are looking at removing the device restrictions in the codebase.", "@AlphasCodes Guys, I apologize for perhaps naive question but why is this build AMD GPU only ? Isn't OpenCL supposed to be standard ? Do I understand it correctly that it won't work on my Intel Carbon X1 with installed OpenCL 2.0 drivers ?", "If you read the issue that was linked twice, you'd see there's nothing about amd gpu. \r\nIntel's is currently excluded, but it has nothing to do with wanting to force users, and there is a temporary workaround - discuss there if really any. ", "When i use the amd_gpu branch with a jupyter notebook, there seems to be a left over thread. python still uses 100% of one cpu, even after the computation has finished. Restarting the kernel finishes the stray thread. Does anybody else experience this?", "@brainwave @unoexperto \r\nSorry I cannot help with Intel OpenCL because i have only AMD OpenCL hardware.\r\n\r\n@desperadoduck \r\nI don't use jupyter yet. I use a plain bash shell and a virtual Python 3 environment ([see my Python 3 +  Tensorflow setup](https://github.com/AlphasCodes/DeepLearning)). But i cannot reproduce the issue. There is no CPU usage by a python process after the compute has been completed.\r\n\r\n@rodburns \r\nThank you for the information. Is it possible to speed up the initial compile time? e.g. using all available CPU threads instead of only 50%.\r\n\r\n", "@brainwave @rodburns \r\nFor Intel GPU (Gen9) under Linux we have seen significantly better DNN performance with Intel's open source Beignet implementation vs the closed source one when benchmarking with common vision nets on PlaidML. Beignet is also easier to install which is nice.", "Does it support intel graphics hd615 (7th gen cpu) on ubuntu17.10?\r\n\r\nThe opencl dirver SRB5.0 for linux64 is running well on ubuntu17.10.\r\n\r\nAnd It has not been updated for a long time:\r\nhttps://bitbucket.org/mehdi_goli/opencl/branch/IntelGPU", "For the love of god, can't you read just 2 (two!) posts above?\r\nDiscuss the lack of intel gpu (or amd cpu) support here https://github.com/codeplaysoftware/computecpp-sdk/issues/78", "@znmeb it is a goal to make full use of various computing resources(e.g. cpu,gpu,DSP, any other coprocessor).\r\nIn fact, it depends on the support of hardware vendors: **dirver**  and OS. \r\nAs i know, you may can't enable both intel GPU and nvida GPU for video during the same time, due to the limitation from vedio driver. (You might be able to switch between them).\r\nHowever, opencl can use them at the same time. They are both \"devices\" in it.", "@choongng That's interesting to know, we did some work to help enable Beignet but the activity on this project seems to have gone a bit quiet.\r\n\r\n@znmeb Yes any GPU will probably not perform much better on a small problem, glad you are making some progress though!\r\n\r\n@unoexperto ComputeCpp with TensorFlow is able to be used by any hardware that supports SPIR OpenCL intermediate instructions which includes Intel GPUs however as in the thread [here](https://github.com/codeplaysoftware/computecpp-sdk/issues/78#issuecomment-352411192), we had intentionally prevented it running because we didn't think the current drivers were working at the moment. You can remove that restriction since it sounds like some users have got it working with different Intel drivers. We are also working on enabling this for [ARM](https://developer.codeplay.com/computecppce/latest/tensorflow-arm-setup-guide) and [Renesas](https://www.codeplay.com/portal/12-09-17-open-standard-software-frameworks-facilitate-development-using-renesas-rcar-socs) processors that have OpenCL drivers.", "@sxpc722 That should work then. By the way, the new machine is Windows 10 and I am not planning to dual-boot it with Linux until I absolutely have to do so! I'm sick of chasing down driver and library bugs for vendors (looking at you, AMD). In fact, I may put a Windows partition on my workstation for the same AMD reason. ;-)", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "The Tensorflow AMD OpenCL performance is very slow according to my tests. So i did some basic tests with an other Deep Learning framework. You will find my setup and benchmarks on my GitHub page [here](https://github.com/AlphasCodes/DeepLearning).\r\n\r\nLong story short. The other Deep Learning framework is about 10 times faster than Tensorflow AMD OpenCL currently.", "@AlphasCodes @znmeb I know the TF team prefer to keep the thread TF-only, we're happy to host the PlaidML-specific conversation over on the PlaidML project. That said, we do hope to eventually support TensorFlow itself as well as non-OpenCL platforms (e.g. Apple's Metal for iOS which currently exists in prototype form).\r\n\r\nhttps://github.com/plaidml/plaidml", "@choongng Thanks for the information i edited my message accordingly.\r\n\r\n@znmeb The AMD A12-9800E iGPU should be GCN v3.\r\n\r\nThe main and only reason for me to do the benchmarks/tests is to find an answer on my question \"Stay with AMD or switch to Nvidia for my Deep Learning adventure\".\r\n\r\nAnd the answer is. I really like the opensource approach of AMD but i will likely switch to Nvidia due to 2 factors. First the Deep Learning software stack (e.g. Tensorflow) is much more mature for Nvidia. Second the graphic card offers for my very specific needs (must fit into a Dan A4 SFX Case and must be very very silent / almost noiseless under full load for hours) is very limited or even non existing on AMD side.", "Is Intel GPUs supported? I think my Iris Pro can speed up the age-long training for a bit.", "> Discuss the lack of intel gpu (or amd cpu) support here codeplaysoftware/computecpp-sdk#78\r\n\r\nhttps://github.com/codeplaysoftware/computecpp-sdk/issues/82", "Just trying to get a sense of the state of this issue. Am I right to say that this repo:\r\n\r\nhttps://github.com/lukeiwanski/tensorflow\r\n\r\n...built with ComputeCpp, is the current best option for building Tensorflow with general AMD GPU support? And if so, is there any benchmark evidence that this build provides a speedup over CPU?", "Depends on what you mean by \"general AMD GPU support\". If you mean really old dGPU or APUs, I don't know. But if you have newer (2nd Gen GCN or newer), hipTensorFlow (v1.0.1) running on ROCm was working pretty well.", "@briansp2020 Ah yes I have seen AMD's work on ROCm. Unfortunately they only support Linux though, and it doesn't even seem like support for any other OS is on their roadmap. I'm hoping for something that supports Windows.", "@mjmax Is there any GPU accelerated tensorflow package available for Windows? I thought, if you want GPU accelerated deeplearning, Linux was only choice. If TensorFlow was ported to OpenCL, would that make it easier to port to Windows? I'm not sure why TensorFlow is not available on windows with GPU acceleration when CUDA is supported there. \r\n\r\nI guess this is now off topic, but if anyone know of TensorFlow and/or PyTorch for windows that is GPU accelerated, I'd like to know about it as well... ", "@briansp2020 As far as I know, Tensorflow already supports Nvidia GPU acceleration on Windows. ", "CL tensofrflow is already a mess on linux, don't expect anything any soon. \r\nIf you want to accelerate stuff there, there's only plaidML. \r\n(and please, we are already at 500 comments.. let's try to only post if really, really necessary)", "@mirh OpenCL Caffe does work on Windows. Sure it's not TensorFlow in terms of features, but pretty solid for Software that has to be deployed everywhere.", "What about replacing the openCL port with the HIP port backed by AMD ? \r\n\r\n https://github.com/ROCmSoftwarePlatform/hiptensorflow", "Haha! @LifeIsStrange Life is very strange actually... Are you working for the HiP marketing team of AMD ? :-)\r\nPlease look at the subject of this issue : \"OpenCL support\".\r\n\r\nThis means it is about the Khronos standard https://en.wikipedia.org/wiki/OpenCL (and the other SYCL standard from the OpenCL Khronos working group appears at the end of the \"Overview\" section).\r\n\r\nOf course there is a world outside of this issue, but it is... *outside*! :-)\r\n\r\nPlease try not to increase inconsiderately the entropy of the universe by posting some random posts on this already too lengthy discussion... :-)\r\nThis comment applies to some other posters here, not only you, by the way.\r\nThis is a GitHub issue to solve a *technical* problem: having TensorFlow running on devices supporting the OpenCL standard, not a FaceBook page about how people like or dislike tool A or B. :-)\r\nBut please feel free to send some git commits related to this issue we can look at...", "There is a fork of TensorFlow supporting OpenCL https://github.com/hughperkins/tf-coriander \r\n\r\nAnd of course @benoitsteiner 's work https://github.com/benoitsteiner/tensorflow-opencl\r\n\r\nIMHO, it is ridiculous that mainstream TF still didn't merged their work.", "Is the focus here on getting-it-to-run-as-lomg-as-it-is-OpenCL, or making it actually run faster? I'd prefer there not a holy war, but focusing on getting it to run fast on several GPUs. LifeIsStrange's focus is on getting it to work on AMD GPUs and then HIP makes good sense. For others the focus is to make it work on Intel GPUs or Android, and then OpenCL makes much more sense. GPU-languages are a mess, so please keep practical, \r\n\r\nIf I read some of the comments here, performance is an issue with the OpenCL ports. But unfortunately I cannot see many benchmarks around. Are there more benchmarks than this one? https://github.com/AlphasCodes/DeepLearning/blob/master/Tensorflow_Benchmarks.md", "As I understand it, benchmarking is hard if you compare CUDA to OpenCL, because you have to use different hardware. Allegedly, nVidia deliberately made/allowed their OpenCL implementation to be somewhat broken, so benchmarking on the same hardware will always result in CUDA looking great.\n\nOn 12 February 2018 14:26:11 GMT+00:00, VincentSC <notifications@github.com> wrote:\n>Is the focus here on getting-it-to-run-as-lomg-as-it-is-OpenCL, or\n>making it actually run faster? I'd prefer there not a holy war, but\n>focusing on getting it to run fast on several GPUs. LifeIsStrange's\n>focus is on getting it to work on AMD GPUs and then HIP makes good\n>sense. For others the focus is to make it work on Intel GPUs or\n>Android, and then OpenCL makes much more sense. GPU-languages are a\n>mess, so please keep practical, \n>\n>If I read some of the comments here, performance is an issue with the\n>OpenCL ports. But unfortunately I cannot see many benchmarks around.\n>Are there more benchmarks than this one?\n>https://github.com/AlphasCodes/DeepLearning/blob/master/Tensorflow_Benchmarks.md\n>\n>-- \n>You are receiving this because you were mentioned.\n>Reply to this email directly or view it on GitHub:\n>https://github.com/tensorflow/tensorflow/issues/22#issuecomment-364936498\n\n-- \nSent from my Android device with K-9 Mail. Please excuse my brevity.", "Comparing only 2 numbers is no information - who cares if OpenCL on NVidia runs at half speed if it runs at 4x speed on other GPUs?\r\n\r\nI think we'd need these benchmarks:\r\n1. CUDA on NV GPUs (reference benchmarks)\r\n1. https://github.com/hughperkins/tf-coriander on AMD, Nvidia and Intel GPUs\r\n1. https://github.com/benoitsteiner/tensorflow-opencl on AMD, Nvidia and Intel GPUs\r\n1. https://github.com/lukeiwanski/tensorflow on AMD, Nvidia and Intel GPUs\r\n\r\nThe reference benchmarks are easy to be found. We have some high end GPUs here, so only need a place to put the numbers in (with links to building-docs).", "OpenCL support It must become true. \r\n\r\ncuda too limited,and nvidia dont want to share it.\r\ncuda only work for Nv gpus.\r\nthat is dead end for TensorFlow,\r\nif another \"TensorFlow\" come out but more support than TensorFlow.\r\nif TensorFlow still only support cuda in windows.\r\nyou have to realize TensorFlow not the only choose.\r\n", "Why is OpenCL better than HIP? I think OpenCL has failed to gain traction and supporting OpenCL at this point in time probably is counter productive and waist of resources for the whole comunity/industry. I'd rather see TensorFlow support HIP directly and let the compiler/tool/library to take care of the portability.\r\n\r\nIsn't it better for software to support 1 language/programming model?", "Software has to support what it has to support to cover every use case. \r\nHIP is all bells and whistles (at least on the paper) if you have supported hardware. But there aren't just \"newer amd and nvidia cards\" to this world. \r\n\r\nNow please, for the love of god, complain [here](https://github.com/ROCmSoftwarePlatform/hiptensorflow/issues) for any problem with that. \r\nAnd [here](https://github.com/lukeiwanski/tensorflow/issues) for everybody else interested to the continuation of this issue. ", "I thought, that SPIR-V would directly replace CUDA as a cross-hardware alternative:\r\nhttp://alphanew.net/index.php?section=alphanew&site=overview&lang=eng&newsID=111\r\n\r\nWhy does Google still rely on CUDA?", "Can these help?\r\n\r\nOpenCL random number generation(Thomas Wang's):\r\n\r\n```\r\nuint wang_hash(uint seed)\r\n{\r\n               seed = (seed ^ 61) ^ (seed >> 16);\r\n               seed *= 9;\r\n               seed = seed ^ (seed >> 4);\r\n               seed *= 0x27d4eb2d;\r\n               seed = seed ^ (seed >> 15);\r\n               return seed;\r\n}\r\n            \r\nvoid wang_rnd_0(__global unsigned int * intSeeds,int id)                \r\n{\r\n               uint maxint=0;\r\n               maxint--;\r\n               uint rndint=wang_hash(id);\r\n               intSeeds[id]=rndint;\r\n}\r\n\r\nfloat wang_rnd(__global unsigned int * intSeeds,int id)                \r\n{\r\n               uint maxint=0;\r\n               maxint--;\r\n               uint rndint=wang_hash(intSeeds[id]);\r\n               intSeeds[id]=rndint;\r\n               return ((float)rndint)/(float)maxint;\r\n}\r\n\r\n\r\n// initialize each thread's own random number seed\r\n__kernel void rnd_0(__global unsigned int * intSeeds)\r\n{\r\n               int id=get_global_id(0);\r\n               wang_rnd_0(intSeeds,id);     \r\n}\r\n\r\n// get a new random value by each thread\r\n__kernel void rnd_1(__global unsigned int * intSeeds)\r\n{\r\n               int id=get_global_id(0);\r\n               float randomFloat=wang_rnd(intSeeds,id);\r\n}\r\n\r\n```\r\n\r\nOpenCL SHA3hashing(forgot who wrote this)\r\n\r\nhttps://gist.github.com/tugrul512bit/c8170f74846e36e350607664f12c525c", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "It is in Google's interest to support OpenCL, \r\nby having a specific (company/brand/vendor)'s specific hardware as a dependency for your software, you enforce yourself to pay more for hardware, market competition lowers costs.\r\nGoogle has always been about commodity hardware since the very beginning which was and still crucial for Google's success (market dominance), having lower data center operating costs, enabled the revolutionary generous essentially free services offerings like Gmail (storage space) and Google Photos (storage space and auto-tagging).\r\n", "@wesamco No, it isn't necessarily in Google's interest. They make their own hardware - something called a \"TensorBoard\", IIRC. They can bypass OpenCL and CUDA / CUDnn and make the board run raw TensorFlow code.", "> raw TensorFlow code.\r\n\r\nThere is no such thing - it's not like unprocessed food. TPUs need their own DNN-library that handles the different types of calls.\r\n\r\nIt seems it's time for compressing the above discussion into one list again:\r\n- CodePlay is working on a SYCL backend\r\n- Hugh Perkins is working on tf-coriander\r\n- AMD is working on a HIP backend\r\n- PlaidML only supports CPUs at the moment.\r\n- Status of support for Intel GPUs is unclear.\r\n\r\nSo choose a project you like and start supporting them. Maybe each of the groups can give a status-update on their project?\r\n\r\nDo understand that OpenCL has been transformed from a full language to a language-definition/hardware-specification that is represented in SPIRV (kernels), which then can be run on top of a platform like OpenCL-drivers and later also on Vulkan-drivers (platforms). So by supporting SYCL, you also support OpenCL.", "Perfect sum-up, but plaidml does run on gpus too. \r\nIt's just that at the moment they are a backend for keras, not tensorflow. So it's kinda OT there. ", "Hi all, \r\n@VincentSC thanks for great sum up of the different efforts! \r\n\r\n> So choose a project you like and start supporting them. Maybe each of the groups can give a status-update on their project?\r\n\r\nThe SYCL approach supports a variety of platforms  / devices now. The ones I can mention are:\r\n\r\n- AMD GPUs (FirePro W8100, R9 Nano and R9 380 Series ) Instructions available [here](http://deep-beta.co.uk/tensorflow-1-x-on-ubuntu-16-04-lts) or [here](https://developer.codeplay.com/computecppce/latest/getting-started-with-tensorflow) \r\n- ARM Mali ( HiKey 960 ) Instructions available [here](https://developer.codeplay.com/computecppce/latest/tensorflow-arm-setup-guide) \r\n- Intel GPU ( SkyLake series ) with Intel NEO OpenCL driver\r\n\r\nWhen it comes to AMD, at the moment the GPUs mentioned above are using the AMDGPU-Pro drivers 17.40-xxx with legacy OpenCL enabled. \r\nI don\u2019t see any obvious reason why other series would not work ( with the assumption that SPIR / SPIR-V is supported that is) \r\n\r\nThe main platform we are focusing on is Linux - however, we have ongoing efforts to enable Windows in the future. We have no plans to support OSX in near future. I know sad face.\r\n\r\nOur focus is on improving performance for CNNs. Current performance is unoptimized and nowhere near where we see it ending up. That said, we are already beating CPU performance for most models on different targets.\r\n\r\nIn order to speed up the development cycle and reduce overall compilation time of TensorFlow (as well as improve portability) we are working on Eigen, BLAS and DNN libraries.\r\nThese libraries aim to solve the performance issue as well as building up an ecosystem of portable libraries that can be easily integrated with complex projects like TensorFlow.\r\n\r\nBelow, see graphs for performance that we can share at present. They are taken from my fork https://github.com/lukeiwanski/tensorflow/tree/dev/amd_gpu at 271093b21cc5ca38e8699e154b5cada96bd7ac0d.\r\nThe benchmark used is https://github.com/tensorflow/benchmarks\r\n\r\n![cpuvssycl](https://user-images.githubusercontent.com/8373795/38691700-893f77dc-3e79-11e8-894f-d79fccd55194.png)\r\nGraph is normalised to Intel i7-4790K results.\r\n\r\nWe are slowly upstreaming changes to Eigen once that happens we will follow with TensorFlow.\r\n\r\nHope that helps,\r\nLuke\r\n", "For deep learning inference on mobile devices with GPU/OpenCL support, you can checkout [MACE](https://github.com/xiaomi/mace), which is optimized for Adreno, Mali and PowerVR GPUs. Here are some [benchmark results](https://github.com/XiaoMi/mace/issues/1).", "@keryell @benoitsteiner , which version of tensorflow and trisycl are required for integration. I am having trouble building tensorflow (1.9) with latest trisycl release.", "Unfortunately the latest TensorFlow is using more advanced features than the current triSYCL can cope with, so you have to use ComputeCpp, currently the only fully compliant SYCL implementation...", "Tensorflow is supported by Google Brain, and Google has partnership with nVidia, I guess that we shall not expect from Tensorflow to support OpenCL\r\nBig OpenCL community effort is needed", "OpenCL support please!", "OpenCL is more suitable for us too.", "@Makhaon Me too. I can't afford to buy a machine with NVIDIA graphic card.", "Besides the above 2 posts, I'd like to add that now AMD's Vega GPUs (including the ones inside Raven Ridge APUs) can do FP16 at twice the FLOPS, so if TF could support them (through OpenCL) it would really help people with less budget. Also a lot of these people would be students, and if we get them to use TF as the starting point of their DNN journey, they would probably stick with TF down the road, and even tell others about TF; it's a great way to help expand this project.", "I think this thread is mostly meaningless for developers (too much noise - and I'll add some more ;-) but I think many comments are missing the point:\r\n**If you want to run Tensorflow with AMD cards OpenCL IS NOT what you are looking for** - please head over to https://github.com/ROCmSoftwarePlatform/ and install the ROCm stack. AFAIK **AMD's current strategy is based on ROCm instead of OpenCL for Tensorflow/pytorch**.\r\n\r\nGeneric OpenCL was too much maintenance/did not give enough performance benefits to be worthwhile for AMD. Therefore **this ticket is only interesting if you are running (e.g.) an ARM platform** which uses OpenCL only.\r\n\r\n(Disclaimer: just an outsider, no real inside into Tensorflow development so maybe the information above completely wrong and misleading. Feel free to bash me if you know better.)", "Just a thought, what about llvm with the new GPU offload?  That would put a great level of abstraction between tensorflow and cuda specific code.  ", "What about all of you reading just 10 posts above and noticing there already is a fork by lukeiwanski/codeplaysoftware you can try ?\r\n(also my hats off to xiaomi for, once, contributing some serious kind of open source effort)", "@FelixSchwarz  Just so you are aware ROCm uses OpenCL, it is AMD's userspace OpenCL driver on Linux (that is why is why it doesn't support windows), so if you are not aware of how AMD's driver ecosystem on linux works, they have their kernel side drivers AMDGPU and AMDKFD(which is now getting merged into AMDGPU) then there is the userspace drivers RadeonSI(for OpenGL) RadV/AMDVLK(for Vulkan) and ROCm(for OpenCL). ", "Judging by the dynamics of this bug and other forks Google has zero interest in this and will **never** implement this in the official repository. I would vote for closing this issue (or locking it) at all to not give any false hopes for everyone.", "The issue should be here to at least point here all the folks who will\ninevitably open it again.\n\nOn Sat, Sep 15, 2018, 09:45 Anton Kochkov <notifications@github.com> wrote:\n\n> Judging by the dynamics of this bug and other forks Google has zero\n> interest in this and will *never* implement this in the official\n> repository. I would vote for closing this issue (or locking it) at all to\n> not give any false hopes for everyone.\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22#issuecomment-421535747>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AB1qNyDrfbiQ4h3kQyqObEfpK3O0FqRGks5ubKIBgaJpZM4Gex3i>\n> .\n>\n", "There is a TensorRT that supports Movidius Pi Hat. And that Movidius Pi Hat is Google\u2019s $45 \u201cAIY Vision Kit\u201d. Google links to Target to buy it.\r\n\r\nThis doesn't have any ties to CUDA or Nvidia? Says it uses an Intel chip. At its heart, maybe the chip is a FPGA? Anyone know anything more about it? ", "I know quite a bit about the big Movidius unit - it's inference only and it runs either TensorFlow or Caffe pre-compiled models. IIRC they're all in 16 bit mode.\r\n\r\nThe Movidius chip itself is much more powerful but you have to be a qualified partner to get the SDK.", "Have some link for other that try to have tensor opencl:\r\n\r\nhttps://github.com/hughperkins/tf-coriander\r\nhttps://github.com/ChiahungTai/tensorflow-cl\r\nhttps://github.com/guoyejun/tensorflow-cl\r\nhttps://github.com/honggui/tensorflow-cl\r\nhttps://github.com/benoitsteiner/tensorflow-opencl\r\nhttps://github.com/lukeiwanski/tensorflow (repository is out of date)\r\nhttps://github.com/codeplaysoftware/tensorflow\r\nMaybe worth to check also:\r\n\r\nhttps://documen.tician.de/pyopencl/\r\nhttps://pypi.org/project/DeepCL/\r\nhttps://www.khronos.org/sycl/\r\n\r\nFeel free add working projects. ", "Is there any update? This issue is over 3 years old.", "YES THERE IS JUST LOOK AT THE LAST HANDFUL OF POSTS. ", "@filips123 no, there are no updates and will never be in any foreseeable future - probability of that is lower than of alien invasion and finding a way to travel back in time.", "This intel initiative PlaidML works reasonably well, worth checking it out. \r\nhttps://github.com/plaidml/plaidml\r\nIt runs on opencl OR metal on mac. It works with Macbook Pro AMD gpus, which is what I was looking for.\r\nMeanwhile, could you guys help vote for Pytorch support in PlaidML? https://github.com/plaidml/plaidml/issues/63", "PlaidML is certainly all nice and dandy (I, for one, somehow could get more performance on an nvidia gpu on opencl than with tf's cuda itself)..\r\nBut it's a backend for keras? In complete replacement to tensorflow, which you know, it's the repo we are discussing this in?\r\n(for as much as I seem to understand latest tf versions can export models directly to keras? so there's that..)\r\n\r\nAnyway, for the fourth damn time, if you want a recent solution **on opencl** and something still being actively developed (*and also* the thing with the actual chances to be merged here for real one day), there's just codeplay stack. \r\nAgain: \r\nhttps://developer.codeplay.com/computecppce/latest/tensorflow-overview\r\nhttps://github.com/Rbiessy/tensorflow/tree/dev/amd_gpu", "> PlaidML is certainly all nice and dandy (I, for one, somehow could get more performance on an nvidia gpu on opencl than with tf's cuda itself)..\r\n> But it's a backend for keras? In complete replacement to tensorflow, which you know, it's the repo we are discussing this in?\r\n> (for as much as I seem to understand latest tf versions can export models directly to keras? so there's that..)\r\n> \r\n> Anyway, for the fourth damn time, if you want a recent solution **on opencl** and something still being actively developed (_and also_ the thing with the actual chances to be merged here for real one day), there's just codeplay stack.\r\n> Again:\r\n> https://developer.codeplay.com/computecppce/latest/tensorflow-overview\r\n> https://github.com/Rbiessy/tensorflow/tree/dev/amd_gpu\r\n\r\nMy apologies, I had not realised there was no tensorflow support. My assuming brain thought that keras gpu support == tensorflow support. ", "plaidML is super cool. Works on keras.\r\nOf course I had to transfer some tf code to pure keras in order to work on plaidML backend (for example tf.image.ssim)\r\nBut result - my code works on NVIDIA and AMD cards.\r\n\r\nAlso plaidML is heaven for researchers. It automatically generates gradient for any function you will write on \"Tile\" language and it will work on your GPU with 80% speed of tensorflow.\r\n\r\nSo I cannot understand why ML researchers still using PyTorch ? Let's boost ML science with Intel's plaidML ?", "@iperov Care to know why practically no one uses PlaidML ? \r\n\r\n1. It runs pitifully slow on AMD's OpenCL implementations compared to Tensorflow's CUDA backend so there goes at least half the reason to use it. Performance so bad that using Tensorflow with CPUs is competitive or even outright beats their hardware using PlaidML ? \r\n\r\n2. Nobody is interested in maintaining their specialized Tile programming language in which only someone like a pure maths professor would concoct so PlaidML's code quality just goes down the drain and no serious programmers in their right mind would want to deal with overly clever code ...\r\n\r\n3. This pretty much ties into #2 but ever since Intel bought out Vertex.AI, they don't care about PlaidML anymore. Intel's solution for GPU compute accelerated machine learning is introducing a new compiler specifically for deep learning now known as [nGraph](https://github.com/NervanaSystems/ngraph) to target Tensorflow, PyTorch or other deep learning frameworks as a backend for them. No reason for them to keep developing PlaidML anymore as their intermediary when they have nGraph ... \r\n\r\nPeople use PyTorch for other reasons such as maintainability or other features so to sum it up PlaidML is Intel's tool and they probably don't intend for it to play in any role of the final parts of their plans. nGraph's current Intel GPU backend is based off of OpenCL 2.1 of which only Intel has a conformant implementation so Intel only exists to look out for themselves rather than purely for the betterment of machine learning. When Intel goes on to further developing nGraph, I can't see them continue basing off their GPU backend on OpenCL 2.1 alone since many deep learning frameworks have templated kernels which are not compatible with OpenCL, Metal or Vulkan's separate source programming models so it's probably only for experimentation purposes. Intel's final GPU backend is probably going to either be based off of SYCL 2.2 or something else entirely different like OpenMP and maybe they'll even bring a vendor specific solution ... \r\n\r\nAs for AMD, who cares ? OpenCL is irrelevant to them and they're finally showing some results with their work on HIP ... ", "> @iperov Care to know why practically no one uses PlaidML ?\r\n> \r\n> 1. It runs pitifully slow on AMD's OpenCL implementations compared to Tensorflow's CUDA backend so there goes at least half the reason to use it. Performance so bad that using Tensorflow with CPUs is competitive or even outright beats their hardware using PlaidML ?\r\n> 2. Nobody is interested in maintaining their specialized Tile programming language in which only someone like a pure maths professor would concoct so PlaidML's code quality just goes down the drain and no serious programmers in their right mind would want to deal with overly clever code ...\r\n> 3. This pretty much ties into #2 but ever since Intel bought out Vertex.AI, they don't care about PlaidML anymore. Intel's solution for GPU compute accelerated machine learning is introducing a new compiler specifically for deep learning now known as [nGraph](https://github.com/NervanaSystems/ngraph) to target Tensorflow, PyTorch or other deep learning frameworks as a backend for them. No reason for them to keep developing PlaidML anymore as their intermediary when they have nGraph ...\r\n> \r\n> People use PyTorch for other reasons such as maintainability or other features so to sum it up PlaidML is Intel's tool and they probably don't intend for it to play in any role of the final parts of their plans. nGraph's current Intel GPU backend is based off of OpenCL 2.1 of which only Intel has a conformant implementation so Intel only exists to look out for themselves rather than purely for the betterment of machine learning. When Intel goes on to further developing nGraph, I can't see them continue basing off their GPU backend on OpenCL 2.1 alone since many deep learning frameworks have templated kernels which are not compatible with OpenCL, Metal or Vulkan's separate source programming models so it's probably only for experimentation purposes. Intel's final GPU backend is probably going to either be based off of SYCL 2.2 or something else entirely different like OpenMP and maybe they'll even bring a vendor specific solution ...\r\n> \r\n> As for AMD, who cares ? OpenCL is irrelevant to them and they're finally showing some results with their work on HIP ...\r\n\r\nWhat about all GPU inside arm machine like mobile phones and raspberry pi odroid and etc?   \r\nThey don't support opencl?  \r\nGoogle should care about insert tensorflow on gpu on android.   \r\nThe biggest libraries of neural network training run only on Nvidia gpu, it just make Nvidia gpu more and more expensive (because it people and companies only buy it for professional neural network training), then google will lose more money that way. \r\n", "@Degerz from which planet you are came from?\r\nHow you can compare tf-CPU and AMD GPU ?\r\nAMD GPU on plaidML x30 faster than tf-CPU \r\n\r\n> 1. It runs pitifully slow on AMD's OpenCL implementations compared to Tensorflow's CUDA backend so there goes at least half the reason to use it\r\n\r\nin my deepfakes tests OpenCL slower only by 20%, but in some mini networks OpenCL is 20% FASTER.\r\n\r\nMy project DeepFaceLab has many users that have been waiting for the support of AMD. How many people were delighted when deepfakes can finally be trained on AMD cards.\r\nAlso plaidML is the only backend for keras that supports AMD/IntelHD out of the box.\r\nIf a new AMD backend for keras appears, of course my project will switch to it.\r\nPyTorch has no future. \r\n\r\nWhat to maintain in plaidML ? Ops are auto differentiable, there is nothing to maintain.\r\n\r\n> Tile programming language in which only someone like a pure maths professor would concoct\r\n\r\nMachine learning is invented by professors of mathematics, isn't it?\r\n", "@talregev What about ARM or Broadcom ? The former probably has subpar OpenCL implementation and the latter doesn't even officially provide OpenCL drivers! It's not Google's responsibility to create and maintain a competent compute stack for hardware vendors ... \r\n\r\n@iperov You realize that training neural nets with embedding layers on PlaidML is painful, right ? PlaidML also has a bunch of other limitations as well such as not being all that well suited for DenseNets or the fact that it's computation graphs are static and does PlaidML even work well with RNNs ?\r\n\r\nAs for your project, don't worry about it. You'll move on to something better like Tensorflow since AMD will soon offer a native GPU backend for it once MIOpen gets upstreamed which is their GPU accelerated library of primitives for deep neural networks similar to their competitor's cuDNN library both of which will leave PlaidML in the dust in terms of performance. Who cares about Intel iGPUs anyway ? If Intel is truly committed to delivering high performance deep learning on their future discrete graphics hardware then they'll offer a single source option just like the others (AMD/HIP and Nvidia/CUDA) did before them ... \r\n\r\n> PyTorch has no future.\r\n\r\nEnvy much ? PyTorch is ~10x more popular than PlaidML, newest techniques in DL are implemented easily on PyTorch, tons of different contributors and is actively developed by Facebook all the while Intel hasn't contributed to PlaidML in nearly a month ?\r\n\r\n> What to maintain in plaidML ? Ops are auto differentiable, there is nothing to maintain.\r\n\r\nSo I take it from you that PlaidML shouldn't receive any new fixes or new features in the future going forward ? If you don't see the value in improving code then there's no point in convincing you to acknowledge PlaidML's glaring flaws ...\r\n\r\n> Machine learning is invented by professors of mathematics, isn't it?\r\n\r\nDoesn't mean we have to take up whatever programming language they make up especially in the case of Tile where elegance is clearly favoured over readability. It's no wonder why so many potential contributors are scared away from contributing ...", "Jesus, I wish you guys STFU and get back to work instead. I'll have to unsubscribe from the ticket because it's unbearable to get emails with flame wars. Too bad maintainers do not mute the thread.\r\n\r\n@gunan @caisq @sanjoy Could you please do something about it ?", "Closing the issue , please refer here https://blog.tensorflow.org/2020/08/faster-mobile-gpu-inference-with-opencl.html"]}, {"number": 21, "title": "GPU_Base dockerfile image not found", "body": "I'm attempting to use the GPU dockerfile to build an image with gpu support, however it points at b.gcr.io/tensorflow-testing/tensorflow-full which doesn't appear to be accessible. The image at b.gcr.io/tensorflow/tensorflow-full doesn't have CUDA in it, is there a different image I need to point the script at?\n", "comments": ["hey @sibleyd -- yes, that Dockerfile was still undergoing a little churn. An updated version will come along shortly, and I'll update this issue as soon as it's ready.\n\nThe image is already up: b.gcr.io/tensorflow/tensorflow-full-gpu ... you'll need to follow [these instructions](http://tensorflow.org/get_started/os_setup.md#install_cuda) to pick up the CUDA libraries. The Dockerfile will assume those files are available locally (which is the missing `cuda/` path you spotted).\n", "hey @sibleyd -- this should be cleaned up now:\n- Back to a [single Dockerfile](../blob/master/tensorflow/tools/docker/Dockerfile.gpu_base)\n- Complete instructions (well, with pointers) in [the README](../blob/master/tensorflow/tools/docker/README.md)\n", "Closing for now, but reopen if you hit issues.\n"]}, {"number": 20, "title": "CUDA 7.5 fails with pip install and docker (Ubuntu 14.04)", "body": "Installing via:\n\n``` bash\n# For GPU-enabled version (only install this version if you have the CUDA sdk installed)\n$ pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\n```\n\nTried to run the alexnet_benchmark.py and it's looking for CUDA 7.0 specifically.\n\nI have CUDA 7.5 on my machine.\n\nFull stack:\n\n``` bash\nTraceback (most recent call last):\n  File \"alexnet_benchmark.py\", line 21, in <module>\n    import tensorflow.python.platform\n  File \"/home/awesomebox/anaconda/lib/python2.7/site-packages/tensorflow/__init__.py\", line 4, in <module>\n    from tensorflow.python import *\n  File \"/home/awesomebox/anaconda/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 22, in <module>\n    from tensorflow.python.client.client_lib import *\n  File \"/home/awesomebox/anaconda/lib/python2.7/site-packages/tensorflow/python/client/client_lib.py\", line 35, in <module>\n    from tensorflow.python.client.session import InteractiveSession\n  File \"/home/awesomebox/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 11, in <module>\n    from tensorflow.python import pywrap_tensorflow as tf_session\n  File \"/home/awesomebox/anaconda/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"/home/awesomebox/anaconda/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\nImportError: libcudart.so.7.0: cannot open shared object file: No such file or directory\n```\n\nTried the docker install, but the docker image is configured for a particular NVIDIA driver version, and doesn't work with others. (this is a known issue: docker driver version and system driver version must exactly match)\n", "comments": ["@nivwusquorum haha that's a terrible workaround, as it'll start issues with other libraries. Thanks a lot though. I'm installing CUDA 7.0\n", "@nivwusquorum no.\n\nI'm running into the same issue. Looks like I'll be downgrading then.\n", "Out of curiosity, have you set your LD_LIBRARY_PATH to your cuda installation's lib64 directory?\n", "@ebrevdo yes\n\n```\nprintenv LD_LIBRARY_PATH\n/usr/local/cuda-7.5/lib64\n```\n", "On the subject of CUDA library versions ...  CUDA 7.0 works for me (as expected), but it really insists on cuDNN 6.5 (which Nvidia now has as 'legacy').  \n\nExact same library locations, etc, but downgrading from cuDNN 7.0 to 6.5 worked.\n", "yes its within the tensorflow code, so just some simple python hacking wont solve it :)\n(_pywrap_tensorflow.so when you pip install the binary):\n\n```\n_mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\nImportError: libcudart.so.7.0: cannot open shared object file: No such file or directory\n```\n", "i assume i have same problem: \nI have the 7.5 installed  with tensorflow and when I try (like in the tutorial about gpu)\nwith tf.device('/gpu:0'):\n  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n  c = tf.matmul(a, b)\n  print(c)\n  sess.run(c)\n\nit breaks !\n(in torch7, I have no pbs with gpus)\n", "Can we assume tensorflow to be forward compatible with cuda 7.5?\n", "I've got the same problem. \n\nSo are you saying that downgrading from cuda 7.5 to 7 should do the trick?\n", "If it helps, I've installed cuda toolkit 7.0 and changed the bash profile reference to the new one and it works.\n", "yes you do not need to reinstall the cuda 7.0 driver, just provide the path to libcudart.so.7.0 at the end of the LD_LIBRARY_PATH variable. After discussing of it with someone of the team, they told me a strange story. It appears thar the old CUDA drivers are very much in demand by the people using AWS of amazon !\ncan someone confirm ?\n", "Long story short: tensorflow currently requires cuda 7.0.  If you install version 7.0 in a separate directory from 7.5, and point tensorflow at it via the configure script (or LD_LIBRARY_PATH), it will work.  Leaving this open to track future upgrades to the 7.5 SDK.\n", "I'm curious why it is hard to upgrade to CUDA 7.5 and CuDNN v3? Anyone helps me understand?\n", "@emergix so you got it to work just by providing libcudart.so.7.0 without reinstalling / downgrading to old cuda?\n", "Yes. It doesn't require you to uninstall the previous one. You can just install them separately and reference v7.0 in your bashrc file \n", "Thanks! Workaround confirmed here.\n", "symbolic link libcuda_.7.5 to libcuda_.7.0\nIt works.\n", "This issue and co. are open for more than two months now. Is there any progress to support 7.5 cuda and 7.0 cudnn other than the workarounds? This could be a turn off for some people who already have been working with 7.5 cuda for a while. \n", "I completely agree. Ideally (assuming reasonable API/ABI stability on NVIDIA's side), TensorFlow should not be dependent on specific older versions of CUDA and cuDNN. (I'd rather understand it if the _latest_ version was required to make use of certain features.)\nThis is the one issue that puts me off using TensorFlow with GPU support.\n", "+1 Please support CUDA 7.5\n", "If you want to try out CUDA 7.5 under Linux,  you could try building my pull request branch:\nhttps://github.com/tensorflow/tensorflow/pull/664\nIt adds support for CUDA on OSX and uses CUDA 7.5 when building under OSX. I haven't tried 7.5 under Linux,  but it seems to work ok with OSX. \nThe only change you'd need to make is to edit the configure file and set CUDA_VERSION='7.5' when Linux is detected.  Lines 48-49 would look like:\n\n```\nif [ \"$OSNAME\" == \"Linux\" ]; then\n  CUDA_VERSION='7.5'\n```\n", "@ebrevdo @andorremus Could you share the method of installing different cuda(7.0 and 7.5) separately? \n\n@emergix @fivejjs Does symbolic linking libcuda.7.5 to libcuda.7.0 or just providing libcuda.7.0 have side effects? \n", "@wangg12 A simple google search \"install cuda 7.0/7.5 for 'OS here'\" should surface all of the information you need\n", "@andorremus I can find a lot of instructions for installing a single version cuda, but I can't find any information about installing 7.0 and 7.5 simultaneously. How do you make it?\n", "Well you install them separately as they are both standalone and the you point to the one you want in the bashrc file afterwards \n", "@andorremus Thanks.\n", "@andorremus Sorry to bother you. One more question, do I need to install driver when I install the second cuda version or I just need to install the libraries?\n", "I think if you've already got the drivers it should be only the libraries\nbut I'm not really sure on that.\n\nOn Tuesday, 19 January 2016, Wang Gu notifications@github.com wrote:\n\n> @andorremus https://github.com/andorremus Sorry to bother you. One more\n> question, do I need to install driver when I install the second cuda\n> version or I just need to install the libraries?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/20#issuecomment-172777044\n> .\n", "Just need the toolkit and cudnn. You only need one driver (the latest for\nyour system, that is)\nOn Jan 19, 2016 3:45 AM, \"Wang Gu\" notifications@github.com wrote:\n\n> @andorremus https://github.com/andorremus Sorry to bother you. One more\n> question, do I need to install driver when I install the second cuda\n> version or I just need to install the libraries?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/20#issuecomment-172777044\n> .\n", "Thank you @andorremus @esube. I've installed cuda 7.5 and then install 7.0's libraries. It seems to work for me.\n", "/cc @ducha-aiki\n", "@wangg12\ncould you please clarify, at the end, CUDA 7.0 is needed for only installing, or operate as well? Same for cudnn. My concerns are having two drivers (your and other experiences say it is OK) and performance. CuDNN 2 and 3/4 differs really a lot. Sorry for stupid question :)\n\n@bhack thanks for tagging!\n", "Linking libcudart.so.7.0 to 7.5 solved the problem.\nAnd maybe it should also link libcufft and some other files.\nPlease support cuda 7.5 and cudnn v3\n", "Installing the download for Ubuntu 14.10 works for me. After installing 7.5 as prescribed on NVIDIA's website. I downloaded the Local Package Installer and used the Synaptic Package Manager to install. My installation looks like this:\n\nlrwxrwxrwx  1 root root    8 Feb  2 15:27 cuda -> cuda-7.0\ndrwxr-xr-x 13 root root 4096 Feb  2 15:27 cuda-7.0\ndrwxr-xr-x 13 root root 4096 Feb  2 15:09 cuda-7.5\n\nMake sure the PATH and LD_LIBRARY_PATH variables reflect the right location of the cuda toolkit you use. Using the path the cuda link works very well.\n", "@zheng-xq recently added a feature so that:\n\n```\nTF_UNOFFICIAL_SETTING=1 ./configure\n```\n\nallows you to configure which version of cuda/cudnn/etc to use without requiring manual symlinks. \n", "Hello,\n\nI replaced cuda 7.0 -> 7.5 and cudnn 6.5 -> 7.0 directly in the code.\nSeems to work on ubuntu 14.04.\nhttps://github.com/aboulch/tensorflow\n\nI did :\n\nperl -pi -e 's/7.0/7.5/g' configure\nperl -pi -e 's/6.5/7.0/g' configure\nperl -pi -e 's/7.0/7.5/g' tensorflow/stream_executor/dso_loader.cc\nperl -pi -e 's/6.5/7.0/g' tensorflow/stream_executor/dso_loader.cc\nperl -pi -e 's/7.0/7.5/g' tensorflow/core/platform/default/build_config/BUILD\nperl -pi -e 's/6.5/7.0/g' tensorflow/core/platform/default/build_config/BUILD\nperl -pi -e 's/7.0/7.5/g' third_party/gpus/cuda/BUILD\nperl -pi -e 's/6.5/7.0/g' third_party/gpus/cuda/BUILD\nperl -pi -e 's/7.0/7.5/g' third_party/gpus/cuda/cuda_config.sh\nperl -pi -e 's/6.5/7.0/g' third_party/gpus/cuda/cuda_config.sh\n", "I pip installed tf 0.7 with CUDA 7.5 and cuDNN4 on Ubuntu 14.04, and tf still sought CUDA 7.0 (libcudart.so.7.0). I had to symlink it to the 7.5 version to get it to work.\n", "@esafak how dod you do that?\n", "@esafak do you mind explaining what you did? Perhaps in a gist?\n", "The pip packages for 0.7.0 should be unversioned -- they should look for libcudart.so, do you have logs to suggest this isn't happening?\n", "```\n(tensorflow)tsainbur@txori:~$ pip freeze | grep tensorflow\ntensorflow==0.7.0\n```\n\nInstalled using the virtualenv method, where I had to rename the whl to:\n`pip install --upgrade tensorflow-0.7.0-cp27-none-linux_x86_64.whl` \nto avoid\n `tensorflow-0.7.0-py2-none-linux_x86_64.whl is not a supported wheel on this platform.`\n\nNow I get\n `ImportError: libcudart.so.7.0: cannot open shared object file: No such file or directory`\n", "Weird, we were supposed to have built the wheels unversioned.  https://github.com/tensorflow/tensorflow/search?utf8=%E2%9C%93&q=libcudart shows no missing hardcoding of 7.0 anywhere...\n\n@martinwicke, @jendap, any ideas?\n", "@timsainb I changed to the appropriate directory and executed `sudo ln -s libcudart.so.7.5 libcudart.so.7.0`. But you might not have to do this if the next paragraph applies.\n\n@EderSantana @vrv I found the problem: the new installation of CUDA did not overwrite or otherwise remove CUDA 7.0, but installed it in /usr/local/cuda7.5 leaving /usr/local/cuda, which got picked up by $LD_LIBRARY_PATH. I removed the old files using\n\n```\nsudo apt remove cuda-command-line-tools-7-0 cuda-core-7-0 cuda-cublas-7-0 cuda-cublas-dev-7-0 cuda-cudart-7-0 cuda-cudart-dev-7-0 cuda-cufft-7-0 cuda-cufft-dev-7-0 cuda-curand-7-0 cuda-curand-dev-7-0 cuda-cusolver-7-0 cuda-cusolver-dev-7-0 cuda-cusparse-7-0 cuda-cusparse-dev-7-0 cuda-documentation-7-0 cuda-driver-dev-7-0 cuda-license-7-0 cuda-misc-headers-7-0 cuda-npp-7-0 cuda-npp-dev-7-0 cuda-nvrtc-7-0 cuda-nvrtc-dev-7-0 cuda-repo-ubuntu1404-7-0-local cuda-samples-7-0 cuda-toolkit-7-0 cuda-visual-tools-7-0\n```\n\nNow it seems to work.\n", "See https://github.com/tensorflow/tensorflow/pull/1166\n", "I have the same problem using a docker container and the nvidia-docker tool with an image based on the `cuda:7.5-cudnn4-devel` image. I tried to use @esafak suggestion and I endup with:\n\n```\nroot@9ad6f042d266:/usr/local/cuda/lib64# readlink -f libcudart.so.7.0\n/usr/local/cuda-7.5/targets/x86_64-linux/lib/libcudart.so.7.5.18\n\nroot@9ad6f042d266:/usr/local/cuda/lib64# readlink -f libcudart.so\n/usr/local/cuda-7.5/targets/x86_64-linux/lib/libcudart.so.7.5.18\n```\n\nBut i still have:\n\n``` python\n...\n  File \"/opt/conda/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\nImportError: libcudart.so.7.0: cannot open shared object file: No such file or directory\n```\n\nIf I run python in `/usr/local/cuda/lib64` I have errors related to #808:\n\n``` python\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:99] Couldn't open CUDA library libcuda.so. LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:121] hostname: 9ad6f042d266\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:146] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:257] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  352.63  Sat Nov  7 21:25:42 PST 2015\nGCC version:  gcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04)\n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel reported version is: 352.63\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1051] LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1052] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so; dlerror: libcuda.so: cannot open shared object file: No such file or directory\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n```\n\nDid you manage to find the problem?\n\nTx!\n", "@tboquet , your `unable to find libcuda.so` might be related to the bottom of this rabbit hole: https://github.com/tensorflow/tensorflow/issues/970#issuecomment-185862391\n", "@ruffsl, tx, I will try to build the devel version and post my results.\n", "Release 0.7.1 wheels now are built assuming cuda-7.5 and cudnn v4.  If you want earlier versions supported, you can build from sources and set the appropriate library paths / versions during ./configure.\n", "For this error\nImportError: libcudart.so.7.5: cannot open shared object file: No such file or directory\n\nI have executed this command to solve it\nsudo ldconfig /usr/local/cuda/lib64\n", "If you build from the latest TF, it would ask you for Cuda SDK and Cudnn versions. You can point it to a different version. \n", "@fivejjs  how did you do so? can you please write the detailed command?\n", "why does ldconfig /usr/local/cuda/lib64 solve it @zdx3578 \n\nI currently cannot sudo in the cluster Im using so that didnt work, any alternatives?\n", "I have just installed CUDA v8 and the issue still persists. I got the following error:\n\n```\nImportError: libcudart.so.7.5: cannot open shared object file: No such file or directory\n```\n\nIn case anyone else encounters the same issue, the solution is to create symlink. \n1. Go to the cuda directory: `cd /usr/local/cuda/lib64`\n2. Create the symlink: `sudo ln -s libcudart.so libcudart.so.7.5`\nThen, you'll be able to import tensorflow. To check if it is using the GPU, try running:\n\n```\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n```\n", "Thank you @philoniare I too have CUDA 8 and it works now !\n"]}, {"number": 19, "title": "Swift API", "body": "Swift is a very popular and expressive language with a large community of pro developers.\n", "comments": ["Providing APIs for every 'very popular' lang would probably bloat the main project quite quickly.\n", "I understand the sentiment, but Swift, C# and Java are not \"every\" language, and this is not \"every project\" either. This is open source and Github so there might be enough hands to man the maintanance of ports to the most popular languages for such an important piece of software. Python is quite popular in academia but a little less popular with the bulk of private sector developers.\n", "Given that TensorFlow is designed to also run on mobile and that Apple is recommending [Swift](https://developer.apple.com/swift/) as a primary language for iOS development, I think having a Swift API for TensorFlow would be a good idea.\n\nI don't think adding support for the most widely-used languages such as Java, C#, and Swift would be a bad idea. The [gRPC](http://www.grpc.io/) project already supports 10 languages, including Java, C#, and JavaScript (via Node.js).\n\n@jamesliu96 Feel free to open an issue for adding a Go API.\n", "A Swift frontend, especially for simply running graphs for inference on mobile would be great. I don't know of current plans to provide one, so feel free to dive in. Since our exact plans on accepting external contributions are still in flux, it would be a good idea to check in with the discuss mailing list with a draft of the code ahead of time to figure out where it should live exactly.\n", "I couldn't agree more with janerivi and davidzchen. Swift has many advanced features that make it one, if not the best choice for full API support for TensorFlow.\n\nThis is especially true since Swift is now open source. This language is very pleasant to work with and has all of the great features of top expressive languages like Haskell; Generics, Closures, etc.\n\nI would not agree that C# or Java are good candidates in terms of speed. Glyn Williams is a trusted and respected contributor and writer in many areas and had this to say at:\nhttps://www.quora.com/In-terms-of-performance-speed-is-Swift-faster-than-Java\n\nGlyn Williams:\nSwift is significantly faster than Java.\nIt compiles to native code. And a number of the language features enable an optimizing compiler to produce very fast code indeed.\n\nVivian Keating:\nSwift will typically run faster than Java, since it compiles into native code via the LLVM compiler. Most recent metrics find it comparable to C++ in production.\n\nThis is the tip of the iceberg in terms of comparisons. Apple took great care in designing this modern language and how it compiles to native code, and now it's Open Source.\n", "I'll take on this.\n", "I have this swift docker file with parsley mcparsefsce / tensorflow + separate swift 3 dev container. - I will plugin swift grpc once google resolve an issue with their just released  grpc library\n\nhttps://github.com/johndpope/DockerParseyMcParsefaceAPI \n\nI could use some help to get it all working. \n\nSome @nubbel stubbed out proto files in swift here\nhttps://github.com/nubbel/swift-tensorflow/tree/master/types\nShould be able to leverage this environment to get above working.\n", "I have been working on a painful private project for a while using Swift. I'll be migrating the project API library to Swift 3 soon and [opening it up](https://gitlab.com/Omnijar/Ocular) under the same licensing as TensorFlow (Apache 2.0). It was reasonably complete before Swift 3 and I plan to have the Swift 3 version complete before the end of 2016, sans mistakes from original, and including lessons learned. \n", "@siilime fyi - Google released an official swift3 grpc library last week.\nhttps://github.com/grpc/grpc-swift\nThere's still some heavy lifting to do to get it all working(eg generate swift 3 services classes). You may want  to revisit your implementation and yield to official library. Surely less painful in the long run. I'm aware of some other  grpc libraries and wrapping the c classes into swift module.(just search on GitHub (swift + grpc) there's one by Huawei. My interest is to get it working server side. If anyone wants to help / check the issues grpc-swift for docker issues ticket. \n", "@johndpope Good tip. Thanks. My focus was on the server side originally, and it's where the majority of my work has been so I'll be taking a look at that.\n", "Looks like @rxwei has made some progress here around providing some basic test cases  around a swift wrapper for the  tensorflow c api.\n\nhttps://github.com/rxwei/tensorflow/tree/master/tensorflow/swift\nhttps://github.com/rxwei/CTensorFlow\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.h\n", "I've done a lot of review work over my original code base and the official Swift gRPC code base in the past few days, and although there's advantages to using the gRPC in some cases, there's still a lot missing, (specifically an easy option to train TensorFlow directly from Swift), and those gaps are the areas I'll be focusing on, whilst aiming to develop a complete solution in Swift.\n\nGood start with @rxwei contribution so will aim to assist in that, whilst migrating existing code base.\n", "Currently the C API is under-documented and offers only a few core components, Graph, Tensor, and Session. I start to wonder if wrapping all the C API is worthwhile.\n", "@rxwei What feasible alternative approaches are there? \n", "for reference - found this - https://github.com/somaticio/tensorflow.rb\na ruby wrapper around tensorflow. (there maybe more)\nIt looks like they wrapped the c api in another c wrapper. Not sure if this helps.\nhttps://github.com/somaticio/tensorflow.rb/tree/master/ext/sciruby/tensorflow_c/files\n\n<img width=\"536\" alt=\"screen shot 2016-11-02 at 17 13 09\" src=\"https://cloud.githubusercontent.com/assets/289994/19947655/838c8a3a-a11f-11e6-9e0e-84f2a1ea2026.png\">\n\nAnother shot is to use grpc to interact with tensor. \nbut the services layer needs fleshing out which will be simpler once google can spit out service class definitions (think afnetworking for protobuf) for grpc \n<img width=\"276\" alt=\"screen shot 2016-11-02 at 17 02 28\" src=\"https://cloud.githubusercontent.com/assets/289994/19947679/a5a38524-a11f-11e6-8c93-5d1a96eb1956.png\">\n\nthe python API has hundreds of granular parameters. \nhttps://www.tensorflow.org/versions/r0.10/api_docs/python/index.html\n\ncompared to c++ API / c api.\nhttps://www.tensorflow.org/versions/r0.10/api_docs/cc/index.html\nEnv\n\ntensorflow::Env\ntensorflow::RandomAccessFile\ntensorflow::WritableFile\ntensorflow::EnvWrapper\nSession\n\ntensorflow::Session\ntensorflow::SessionOptions\nStatus\n\ntensorflow::Status\ntensorflow::Status::State\nTensor\n\ntensorflow::Tensor\ntensorflow::TensorShape\ntensorflow::TensorShapeDim\ntensorflow::TensorShapeUtils\ntensorflow::PartialTensorShape\ntensorflow::PartialTensorShapeUtils\nThread\n\ntensorflow::Thread\ntensorflow::ThreadOptions\n\nIf you're looking to machine learning in swift today (nov-2016) it'd pay to  shop around for other libraries. https://github.com/search?utf8=%E2%9C%93&q=swift+ml&type=Repositories&ref=searchresults\n\n One day - another  (somewhat unfeasible) approach could be to rewrite the python library in swift.\n", "@johndpope Honestly **none** of those Swift ML libraries are nearly ready,  cross-platform, nor written in Swift 3.0+.\n", "@johndpope service approach makes sense for now, as a wrapper.\n", "related https://github.com/grpc/grpc-swift/issues/2\nalso there is this swift library for those that want to get their feet? / toes wet. https://github.com/qoncept/TensorSwift\n", "@johndpope That one is dependent on Darwin platforms and Apple's Accelerate framework. Not suitable for general use.\n", "@rxwei Are there not significant limitations by using the service approach?\n", "@siilime many significant limitations. I'm working on other approaches. Stay tuned ;)\n", "fyi - I cherry picked @nubbel proto definitions from https://github.com/nubbel/swift-tensorflow\nand have two docker containers \n- tensorflow / parsey mcparseface api\n- swift3 dev container\n\nthese should fire up by simply typing \n**make start** (warning this could take > 90 minutes to compile for this image)\nhttps://github.com/johndpope/DockerParseyMcParsefaceAPI/\n\n(there is a node client that will hit the grpc service of parsey / but that's not the tensor flow grpc)\n\nIt might be worthwhile jumping on to this slack tensorflow hangout\nhttps://tensor-flow-talk-invite.herokuapp.com/invite\n\nfound this from this link\nhttps://github.com/node-tensorflow/node-tensorflow\n\nI'm currently blocked on  progressing until I/we can resolve this\nhttps://github.com/grpc/grpc-swift/issues/6\n\nLooking through the protobufs here - \nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/protobuf\nservices route may not be so restrictive.\n![screen shot 2016-11-03 at 12 24 19](https://cloud.githubusercontent.com/assets/289994/19974920/86c5cc10-a1c0-11e6-9c8a-81049803e8c8.png)\n", "Has anyone had any success with wrapping the objc ios camera example in a swift project? \n", "@MattSich presumably you saw https://github.com/acerbetti/TensorFlowPod? maybe open a ticket on that repo if it's broken? I didn't try it.\n\nI cobbled this script together to spit out every swift files for every proto file in tensorflow  for grpc communication - but without intermediary services layer - this isn't going anywhere.   https://gist.github.com/johndpope/5d176f4eebeb7ec983fa77d945c18fb1\n\nWe do have  an objective-c  services layer. but it might as well be in c++\n<img width=\"255\" alt=\"screen shot 2016-11-05 at 00 09 57\" src=\"https://cloud.githubusercontent.com/assets/289994/20027507/314a538c-a2ec-11e6-820a-8e7297b464d9.png\">\n\n<img width=\"993\" alt=\"screen shot 2016-11-05 at 00 19 14\" src=\"https://cloud.githubusercontent.com/assets/289994/20027544/68266962-a2ed-11e6-8ab5-b8ecec2f0ad2.png\">\n\nPlease upvote / help out this issue which would provide a native swift services layer.\nhttps://github.com/grpc/grpc-swift/issues/2\n\nFrankly I'm a bit in the dark with next steps.\nWill need to pain stakingly  map each respective swift file  to tensorflow api.\n", "@MattSich I just rewrote the camera part to Swift but the runCNNOnFrame method is still in Objective-C++.", "@chengsam actually already did it but have been waiting for the client for my current project to approve the contribution back to this repo", "@chengsam @MattSich how were you able to rewrite the camera in Swift while keeping the runCNNOnFrame method in Obj-C++ ? Looking for some pointers; trying to do the same thing.", "@ssgutierrez42 You have to use a bridging header to call Obj-C++ code from Swift.", "Would it be useful to include a swift project in the repo that's setup to call runCNNOnFrame via Obj-C++? If so I can probably contribute later on.\r\n\r\n@chengsam yup. My problem was with linking libraries but I got it. \r\n\r\n", "so - I have some base grpc  SERVICE classes in swift 3 using google grpc.\r\nFeel free to grab my script to do mass conversion-> \r\nhttps://github.com/nubbel/swift-tensorflow\r\n\r\nhttps://github.com/nubbel/swift-tensorflow/tree/master/Generated\r\nlooking for help if anyone has extra bandwidth\r\n", "<img width=\"1424\" alt=\"screen shot 2017-05-25 at 1 41 52 am\" src=\"https://cloud.githubusercontent.com/assets/289994/26437456/600b99cc-40eb-11e7-9e02-574b8bfc54f8.png\">\r\n\r\n\r\n\r\n<img width=\"259\" alt=\"screen shot 2017-05-28 at 10 28 57 am\" src=\"https://cloud.githubusercontent.com/assets/289994/26529594/842042b6-4390-11e7-9e43-b9689b3bea18.png\">\r\n\r\nshould align to golang library.\r\n<img width=\"598\" alt=\"screen shot 2017-05-25 at 8 47 58 pm\" src=\"https://cloud.githubusercontent.com/assets/289994/26476256/793b2dfe-418b-11e7-8bd6-245b741d14ed.png\">\r\n\r\n\r\n\r\nTo use swift via GRPC - you can use this https://github.com/johndpope/swift-grpc-tensorflow\r\n depends on grpc / (has a minor compile issue out of the box with missing boring ssl - see issues log for instructions.) I tagged a simplified version 0.0.1 which just has the protobuffer files / no grpc. \r\n\r\n\r\nhit a bit of a tech spike with Operations. > 700kb\r\nhttps://github.com/johndpope/tensorflow/blob/swift/tensorflow/swift/Sources/GoOpWrapper.swift\r\n\r\n", "so proto buffers are neat. turns out you can rip apart trained models with a few lines of swift code.\r\nhere's a gist using inception  .pb   trained model\r\nhttps://gist.github.com/johndpope/5b6bb864d335398bf9b0886c4a09217d\r\n\r\n\r\nlatest code here\r\nhttps://github.com/johndpope/tensorflow/tree/master/tensorflow/swift\r\n(I'm yet to cut in tensors.)\r\n\r\n\r\n\r\n\r\n", "so good news & bad news\r\n\r\nTo anyone - coming to pick up a 'swift' port of tensorflow - as of this writing - \r\nthere's a limit to how far the c api will take this codebase.\r\nhttps://www.tensorflow.org/versions/r0.12/how_tos/language_bindings/\r\n\r\n<img width=\"746\" alt=\"screen shot 2017-06-05 at 9 40 22 am\" src=\"https://cloud.githubusercontent.com/assets/289994/26786018/115356c6-49d3-11e7-9a3f-cc937b04c109.png\">\r\n\r\nit seems there's **no plans** to have any language other than python do the actual training.\r\n<img width=\"740\" alt=\"screen shot 2017-06-05 at 9 41 22 am\" src=\"https://cloud.githubusercontent.com/assets/289994/26786044/2cca1f48-49d3-11e7-9f7c-43f070e34b57.png\">\r\n\r\nthere appears to be a train wreck of github repos attempting to port tensorflow over.\r\nOne that was quite a stand out - \r\nhttps://github.com/kmalakoff/tensorflow-node\r\n\r\n\r\nGood News -> Easily craft fast Neural Networks on iOS! Use TensorFlow models. \r\nMetal under the hood.\r\nhttps://github.com/xmartlabs/Bender\r\n\r\n\r\n", "I think the major standout TensorFlow porting project right now is the C#/F# port called [TensorFlowSharp by Miguel de Icaza.](https://github.com/migueldeicaza/TensorFlowSharp)", "fyi - thanks to @RockfordWei - we have https://github.com/PerfectlySoft/Perfect-TensorFlow \ud83c\udf89\r\n\r\n", "@johndpope You are welcome! I am doing the documentation now and community examples based on Perfect-TensorFlow are absolutely \u26a0\ufe0f*WELCOME*\u26a0\ufe0f!!! Because of the Power of Perfect, a Server Side Swift, now you can integrate any brilliant ideas of TensorFlow into a Server written in Swift and deploy to cloud in hours!\r\n\r\nPlease join us Slack [http://perfect.ly](http://perfect.ly) to get instant feedback online (I am always there),  we support both English and Chinese, as the Perfect-TensorFlow and all Perfect frameworks.\r\nWe are planning to publish all documents to our website [perfect.org](https://perfect.org),\r\nAnd building more examples on Perfect Team's example repo [github/PerfectExamples](https://github.com/PerfectExamples). \r\n\r\nAlso, Perfect-TensorFlow will support the incoming Swift 4.0 soon (honestly, only one or two source code issues for the new features listed in 4.0, majorly because of the protocol buffer made by Apple/Swift).\r\n", "[https://github.com/PerfectlySoft/Perfect-TensorFlow](https://github.com/PerfectlySoft/Perfect-TensorFlow.git) has upgraded to 1.3.0!!!\r\n\r\nNew Features:\r\n1. Fixing QInt16 size issue\r\n2. Gradient has new namespaces (`graph.addGradients()` has been activated since 1.2.1)\r\n3. Adding Devices to Session for distributive computation.\r\n4. Adding Matrix Tensor with auto shape/auto flat functionality.\r\n5. Fixing memory leak between sessions.\r\n6. Fully supports Swift 4.0\r\n\r\n\r\n*NOTE* using `install.sh` is a good start.", "Today published first version for high-level swift API [TensorFlowKit](https://github.com/Octadero/TensorFlow). Have plans to push to google repository in future.", "@VolodymyrPavliukevych please feel free to absorb mature components of the first successful TensorFlow Swift language binding acknowledged by google : Perfect TensorFlow, which works well in OS X and Ubuntu and simple easy installation and as well as the runtime downcast compatibility from TensorFlow 1.2 to 1.4. Welcome aboard! The Perfect TensorFlow has already a full testing script with 34 different tests to completely cover the TensorFlow C API. You can take as many as possible, it\u2019s Apache 2.0 open source ", "Anyway it also has Chinese documents and Perfect Swift server native compatibility ", "@RockfordWei, how to write model and statistic to file system for tensorboar using Perfect?", "@VolodymyrPavliukevych\r\n\r\nPerfect TensorFlow provides a super high level & convenient API to deal with the graph:\r\n\r\nFor example, there is a graph protocol buffer file \"graph.pb\", you can do the following steps to load / save it:\r\n\r\n``` swift\r\nimport PerfectTensorFlow\r\nimport PerfectLib\r\nimport Foundation\r\ntypealias TF = TensorFlow\r\n\r\n// load file into Data structure first\r\nlet fModel = File(\"graph.pb\")\r\ntry fModel.open(.read)\r\nlet bytes = try fModel.readSomeBytes(count: fModel.size)\r\nlet data = Data(bytes: bytes)\r\n\r\n// turn the data into graph definition\r\nlet def = try TF.GraphDef(serializedData: data)\r\n\r\n// import it into the running graph\r\nlet graph = try TF.Graph()\r\ntry graph.import(definition: def)\r\n\r\n// run a session\r\nlet session. = try graph.runner()\r\n   .feed(\"DecodeJpeg/contents\", tensor: someTensor)\r\n   .fetch(\"final_result\")\r\n   .run()\r\n\r\n// get the current buffer after session\r\nif let dat = graph.buffer?.data {\r\n// now you can save the data bytes into another file\r\n}\r\n\r\n```\r\n", "@RockfordWei but how to save and open graphs and statistic of changes for TensorBoard?", "@VolodymyrPavliukevych `summary` is a python object with those statistics as a writer:\r\n[tensorflow/python/summary/write](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/summary/writer/writer.py)\r\n\r\nLike I said, it would grab the graph bytes and then add an event with other tags as \"meta assets\" - so you can load/save the model with these statistics.\r\n\r\nYou can surely implement it in Swift with the same method.\r\n", "@RockfordWei, So there isn\u2019t that write feature in perfect? am I right?", "No one asked me to do that. So no, there is no `summary` object in Perfect TensorFlow now, but you can request one then I can put it on the list, or you can write one to pull a request. Currently, I am working for the incoming \"Function\" of 1.4.0, so it can be a part of the next subversion.", "@RockfordWei, don\u2019t worry, you can use TensorFlowKit library for that. ", "@VolodymyrPavliukevych \r\nNo, let me clarify a bit: Perfect does have all required protobufs, such as summary and events:\r\nhttps://github.com/PerfectlySoft/Perfect-TensorFlow/blob/f638c65e03ffc6d6bf488576f582246eee12540a/Sources/PerfectTensorFlow/pb.summary.swift\r\nhttps://github.com/PerfectlySoft/Perfect-TensorFlow/blob/f638c65e03ffc6d6bf488576f582246eee12540a/Sources/PerfectTensorFlow/pb.event.swift\r\n\r\nIt is up to the end user and I can surely add it with TensorFlow 1.4.0 if need.", "Thanks, @VolodymyrPavliukevych , Now Perfect-TensorFlow has the same test script from you for how to use **TensorBoard** - thank you for reminding me to double check that Perfect-TensorFlow's powerful features - it had been there since 1.1.5, the very first release, actually, my fault, I didn't realize the end user would like such a demo.", "@RockfordWei any plan to create a pod? So that we can use it in ios apps too?", "@tirrorex Thanks for asking. However, PerfectlySoft currently doesn't have such a plan because Apple has already published CoreML framework which supports mobile application well enough. Perfect-TensorFlow is a Server Side Swift to maximize the power of the cloud computation and makes zero conflicts with Apple's strategy.\r\n\r\n\r\nHowever, you can still have a good try. Tips are right here: [tensorflow ios examples](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/ios)\r\n\r\nThis is a very old pod release for 1.1.x, which can help you build an iOS version of tensorflow framework, which is theoretically compatible with Perfect-TensorFlow by setting the customized framework into a dylib to call. Check Perfect-TensorFlow APIloader.swift for detail - it is using `dlopen()` and `dlsym()` to load TensorFlow binaries on demand.\r\n\r\nHowever, even it works, you may need to accept the fact that the old iOS framework example would like to cost your app an extra 300MB or even larger, so please make sure that you know exactly what you want and read some valuable CoreML articles before doing it.\r\n\r\nThank you.\r\n\r\n", "@RockfordWei what my company want is to keep supporting people not running ios 11, either because they don't want to or because they can't. \r\nSince we use tensorflow already on android and with coreML not being available for all our customers we have no choice but to use tensorflow.\r\n\r\nps : looking more closely at the examples i can now see that the code is in fact objectice-c ++\r\n\r\nSo there is no way to use it in swift without bridging everything, at least none that i am aware off.\r\nShouldn't it be possible to copy some of PerfectlySoft's work to get this bridge?\r\nIt could even be included into tensorflow itself and fix this issue for everyone using swift ?\r\n\r\n@ssgutierrez42 yes please ", "Just released new version of [swift API for TensorFlow](https://github.com/Octadero/TensorFlow):\r\n* Added SaveModel class, now you can store and restore your state;\r\n* Added Summary class to visualize your graph and training progress;  \r\nYou are welcome to try!\r\n ", "@VolodymyrPavliukevych , sorry for dumb question, but how to include your library into Cocoa Pods project? I'm new to Swift Package Manager, and don't know how to make it friends with Cocoa Pods...", "Hello! @alexnivanov \r\nIt is not common way, but you could try:\r\n1) Install libtensorflow (from [sources](https://www.octadero.com/2017/08/27/tensorflow-c-environment/), but for iOS platform).\r\n2) clone [TensorFlowKit repository](https://github.com/Octadero/TensorFlow)\r\n3) create Xcode project file:\r\n```\r\nswift package generate-xcodeproj\r\n```\r\n4) Build project in the Xcode.\r\n5) At the end you will have list of Frameworks, you could use at your project just add them to project. \r\nBut I think you will have fake (empty) CTensorFlow Framework, just create empty framework to avoid link errors. \r\nIf I will have enough time, I will do example. \r\n", "@alexnivanov Actually, please note that TensorFlow itself has a brand new lib for mobile app: TensorFlow Lite\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite\r\n\r\nIt looks pretty cool", "@VolodymyrPavliukevych thanks, it's pretty complex though :)\r\nI managed to integrate objc with my swift project, so I'll live with it for now...\r\n\r\n@RockfordWei yeah it's cool, but not all models can be converted to TF Lite. I'm waiting for TF 1.5 release to check it again...", "Perfect-TensorFlow - [Swift Lanugage Binding for TensorFlow on Server Side](https://github.com/tensorflow/tensorflow/blob/28c3c5dd38e3b397c2cf0acdaa6388dcbf0349f7/tensorflow/docs_src/community/welcome.md) just synchronized to\r\n[1.5.0](https://github.com/PerfectlySoft/Perfect-TensorFlow/releases/tag/1.5.0):\r\n\r\nAdding `Results` for Graph: `results.missingUnusedInputMappings()`\r\nPreparing next GM for:\r\n\r\n- TF_NewApiDefMap\r\n- TF_DeleteApiDefMap\r\n- TF_ApiDefMapPut\r\n- TF_ApiDefMapGet\r\n- TF_SetAttrFuncName\r\n- TF_GraphNumFunctions\r\n- TF_GraphGetFunctions", "1.5 years ago, I said: https://github.com/tensorflow/tensorflow/issues/19#issuecomment-252031224\r\n\r\nAnd now I\u2019m here to close this issue :) \r\nWelcome to [Swift for TensorFlow](http://tensorflow.org/community/swift). \r\n\r\n[Swift for TensorFlow video | TensorFlow Dev Summit 2018](https://youtu.be/Yze693W4MaU)", "Hi @rxwei \r\nI am not able to use the TensorFlow module inside Swift. Is it available in the swift package manager.\r\n\r\n`Welcome to Apple Swift version 4.1 (swiftlang-902.0.48 clang-902.0.39.1). Type :help for assistance.\r\n  1> import TensorFlow\r\nerror: repl.swift:1:8: error: no such module 'TensorFlow'\r\nimport TensorFlow`\r\n", "@droidresearch We will be open-sourcing Swift for TensorFlow in April.", "Thanks for the clarification."]}, {"number": 18, "title": "C# api", "body": "C# is very popular and expressive language with a large community of pro developers.\n", "comments": ["Providing APIs for every 'very popular' lang would probably bloat the main project quite quickly.\n", "I'd really like to see a C#/.net wrapper. Shouldn't be too hard to p/invoke into the C++ API.\n", "Perhaps [CppSharp](https://github.com/mono/CppSharp) could be of use? As opposed to SWIG?\n", ":+1: \n", "This is something the core TensorFlow team is unlikely to tackle in the near future, so if you want to contribute it, please go ahead! I would recommend circulating a proposed implementation on the discuss mailing list early on, so that a consensus about where such API might live (in repo / off repo / in 'contrib' directory) can be reached ahead of time.\n", "FYI, @zaphar has just contributed initial [C# rules to Bazel](https://github.com/bazelbuild/bazel/tree/master/tools/build_defs/dotnet). These rules are currently in an early state and currently only support Mono, but the plan is to add Microsoft .NET support once [Bazel supports Windows](https://github.com/bazelbuild/bazel/issues/276). If anyone is interested in helping with improving Bazel's C# rules, contributions are definitely welcome. :)\n", "Just a general +1 from me! \n@davidzchen - I am not familiar with Bazel but assuming the that the idea behind you mentioning it being extended to run on Windows, along with the new C# rules, would be to allow us to build this repo on Windows, including the code for the potential .NET (mono) interop library?\n\nCould an alternative approach be to just build and distribute the native tensorflow libs (dll's) on NuGet - and then in a completely seperate repo, create this.NET interop. This seperate repo can be built using MSBUILD or something so would eliminate Bazel as a blocker?\n", "@dazinator Correct. I am not sure what the TensorFlow team has decided about where support for additional language should live (as @vincentvanhoucke mentioned, in the `tensorflow` repo, in separate repos, or in a `contrib` directory). In the meantime, I'm sure it would be fine for somebody to pick this up, circulate a proposal, and implement C# support built using msbuild. We can add BUILD files later for users who would want to use Bazel instead of msbuild.\n\nI just wanted to put that out there in case somebody is interested in helping out with Bazel's C# rules. :)\n", "The problem is not to invoke C++ code, but to translate the Python part of TensorFlow to either C# or C++. It has about 60K lines of code. That's where TensorFlow \"magic\" resides on...\n\nI'm currently working on porting TensorFlow to Node.js, and we got stuck on this. Maintaining a synced version of some thousand lines of code will get out of control as soon as something changes in the Master. The way I see it working, is to translate the Python part of TensorFlow to C++.\n\nIf you guys are willing to help, please join us at slack: https://github.com/node-tensorflow/node-tensorflow\n", "@ivanseidel - I think one solution for the python code from a .NET perspective, would be to use IronPython - which allows you to:\n- call your Python .py from within .NET\n- run it\n- get a result back into your .NET app from the Python code.\n\nSo i'd assume that for a C# implementation, we'd try and keep all the original python code as is (as much as possible) to save translating 60k lines (and keeping them in sync)\n\nSaying that, this is a completely pie in the sky idea and I really haven't looked at any specifics yet! :)\n", "@dazinator That is a way out, but then, you cannot \"writte\" code from within C# context, you will be limited to writting TensorFlow code in `.py` and calling it from other languages...\n\nThe Idea is to have all the tools available native in the language. Of course this is a working \"work-around\", but if you need a dynamic TF code, you will get somewhere limited by it =/\n\nBut, translating all those lines of codes from Python to C++, wouldn't need to be \"in sync\" anymore. Just the callers, but not the implementation code... It takes an effort, but a single effort, once, to make things the right way =)\n", "@ivanseidel - I understand.\n\n .NET also has good native interop, so porting this python code to C++ version could be good for both .NET and NodeJS\n\nMy comment around the \"sync\" was really about - if google decide to continue to evolve their python code, we'd have to continue to keep the c++ code base in line with that.. It would essentially be having to maintain there implementation as a fork - but I do understand the benefits :)\n", "Yep, you are right...\n\nAs I see, keeping a fork for it is required, be it a Node.js fork for the Python code, or C#, or even Java. But if there is a fork for C++, all can use it in favor.. C++ is universal in this case, while no other language can support all others as much as C++...\n\nIn any case, if you feel engaged to it and would like to help, join us at slack  (link: https://tensor-flow-talk-invite.herokuapp.com/). And good luck with it :)\n", "As I think we have mentioned elsewhere, there is a lot of code in python that we do hope to move into the core C implementation (shape inference, gradients, etc), so that the language bindings only have to be thin, idiomatic wrappers around the core functionality.  But as you know, it is quite a bit of work to do this, and we already have some 100+ open issues to prioritize against too, so please be patient with us.\n", "@vrv , I'm not asking for you guys to priorityze this, but willing to help on this matter. It will save people time when It's ready, and make TF awesome in all other languages... I'm planning on using my vacations on this, but It's not a one-person job. Should I open an issue calling for help on that? I guess Java/C#/Node guys would be willing to help\n", "Great to hear that you are willing to help!  We're not at a state where the work can easily be parallelized across multiple developers, but once we are, we'll definitely try to conscript the community to help.\n", "@dazinator @ivanseidel - I don't understand why IronPython isn't a good idea. Even if all we can do is in / out the data from the c# / .net app - that is a huge step forward. What am i missing?\n", "I don't think you are missing anything, my understanding is that on the one\nhand we'd between writing our own surface layer / api with iron python that\nsits on top of the tensor flow api's. Only yhat surface that we create will\nbe calllable from c#.\n\nWith the convert to c++ approach, we'd end up being able to use the\nexisting tensor flow api's in c# directly (via native interop)\n\nOn Fri, 4 Dec 2015 2:31 am Joe Booth notifications@github.com wrote:\n\n> @dazinator https://github.com/dazinator @ivanseidel\n> https://github.com/ivanseidel - I don't understand why IronPython isn't\n> a good idea. Even if all we can do is in / out the data from the c# / .net\n> app - that is a huge step forward. What am i missing?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/18#issuecomment-161853432\n> .\n", "Has anyone actually tried to interpret the TensorFlow python using IronPython?I appreciate there's now a way to use numpy and scipy with IronPython - though I don't know how long ago this was last updated - but IronPython doesn't play nicely with CPython C extensions.\n", "Anyone tried IronPython approach?\n", "No. IronPython will struggle with any C extensions. Though I think there's a work around for numpy and scipy. Does IronPython work on Mono or CoreCLR? My understanding was that TensorFlow won't run on Windows.\u00a0\n\nOn Fri, Jan 8, 2016 at 5:03 AM -0800, \"Eugene\" notifications@github.com wrote:\n\nAnyone tried IronPython approach?\n\n\u2014\nReply to this email directly or view it on GitHub.\n", "what about efficiency in a distributed platform if you use Ironpython ? \nsomewhere in the doc it is mentioned that :\nTo do efficient numerical computing in Python, we typically use libraries like\nNumPy that do expensive operations such as matrix multiplication outside Python,\nusing highly efficient code implemented in another language.\nUnfortunately, **there can still be a lot of overhead from switching back to\nPython every operation.** This overhead is especially bad if you want to run\ncomputations on GPUs or in a distributed manner, where there can be a high cost\nto transferring data.\n", "Here is a port of the c api to csharp I just made, using SWIG, to keep the maintenance to a minimum.\nhttps://github.com/unrealwill/tensorflow-csharp-c-api\nStill a work in progress to make it more easy to install and use. \nBut example graph runs fine.\n", "FYI: We recently added some documentation in the form of a  [how-to](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/language_bindings/index.md) around how TensorFlow intends to support other language bindings.\n", "Some general porting advice / FYIs regarding C# -- (I've seen some clumsily ported libraries in the past, and they were super awkward to work with, I would hate to see something like that happen to a cool library like TensorFlow!)\n\nC# and .NET have built-in support for a lot of the simple stuff (high speed math, SIMD instructions using the built-in vectors and matrices structs, etc.), so there's not as much of a need to shell out to another process to do regular math.\n\nAlso, it has built-in operator overloading (so no need for a \"mul(x, y)\"-like function, etc.  because you can just overload the multiplication operator).  [It also has actual properties, and you can overload the indexer operator, so no need to ever write \"get\" or \"set\" functions directly; and strong typed enums, so no need for random strings or constants to ever be passed around.]\n\nAdditionally there's already some patterns in C# for building a \"graph\" of operations and deferring their execution to be done later, or even on other hardware (i.e. LINQ can work this way with IEnumerable, but especially with IQueryable where an expression tree can be built and translated or passed on to another machine for execution, etc. -- also the CSOM pattern -- even though it's meant to solve a different problem -- might translate well here -- because the pattern let's you build up multiple queries and then execute them all at once).\n\nIf session has any resources that need to be cleaned up when it's done or if it needs to be \"stopped\" or something, consider making it IDisposable (then we can just wrap it in a using block, etc.) -- IDisposable can also be used to introduce dynamically definable scope (i.e. because it works well with using blocks).\n\nAlso worth mentioning (as I'm looking through the porting document right now, and this isn't a super well known feature): code generation is built into the .NET platform using T4 templates (.tt files).  You can run any kind of C# code there (access a database, read a file, do P/Invoke, etc.) to output a .cs file at compile time.\n\nIt would be super cool to be able to build a TensorFlow statement in C# using LINQ and have it execute on the GPU.\n\n_[Nothing here is meant to be patronizing, (hopefully none of it was!), this is just meant as some tips from a C# power-user that I hope will be of use to someone.]_\n", "Another thing I just stumbled on to -- if IronPython won't work, you could try Python for .NET (pythonnet):\n- [https://github.com/pythonnet/pythonnet/](https://github.com/pythonnet/pythonnet/).\n- [http://pythonnet.github.io/readme.html](http://pythonnet.github.io/readme.html)\n- [http://stackoverflow.com/a/7368156/398630](http://stackoverflow.com/a/7368156/398630)\n\n> Note that this package does not implement Python as a first-class CLR language - it does not produce managed code (IL) from Python code. Rather, it is an integration of the CPython engine with the .NET or Mono runtime. This approach allows you to use use CLR services and continue to use existing Python code and C-API extensions while maintaining native execution speeds for Python code.\n\nAnd\n\n> Python for .NET uses the PYTHONPATH (sys.path) to look for assemblies to load, in addition to the usual application base and the GAC. To ensure that you can implicitly import an assembly, put the directory containing the assembly in sys.path.\n\nIt looks like you still have to have Python installed for it to work, but maybe it could be a help where IronPython couldn't?\n", "@asimshankar could you please update this HowTo [[C]](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/language_bindings/index.md) with the[ [C++ API] ](https://www.tensorflow.org/versions/master/api_docs/cc/index.html)\n\nLooking through the Java language binding using javacpp[ [1]](https://github.com/tensorflow/tensorflow/issues/5#issuecomment-246237396)[[All APIs implemented in java]](https://github.com/bytedeco/javacpp-presets/blob/master/tensorflow/src/main/java/org/bytedeco/javacpp/helper/tensorflow.java), I think [cppSharp ](https://github.com/mono/CppSharp)would also be an good alternative than the SWIG approach.\n", "@JimSEOW : What sort of update are you suggesting? Our intention is to have the Java API implemented on top of the C API as mentioned in the doc using JNI. \n\nI haven't looked into the details of javacpp/cppsharp, perhaps @jhseu has and might have some thoughts on the pros and cons of the two approaches.\n", "Yeah, we suggest that you implement it on top of the C API because we'll have versioning stability there after the 1.0 release. It's also what the other language bindings will be using, including Java, which won't be built using javacpp.\n", "@asimshankar @jhseu I understand that you plan to provide Java API using JNI. Do you have a path for .NET API? e.g. SWIG path using C API as stated by **@unrealwill** \n\nWhy abandon  [C++ API?](https://www.tensorflow.org/versions/master/api_docs/cc/index.html) Users of Javacpp has already using the C++ API. It is a matter of time for cppSharp to catch up. \n\nThere are reasons why cppSharp is prefered over SWIG when come to .NET for cross platorms, as successful company like XAMARIN uses it.\n\nIt will help the .NET user if we know there is a commitment, instead of leaving to the .NET users to come out with an ad hoc heterogeneous solutions. We need some clear leadership comments.\n", "@JimSEOW \n- There is no intention to abandon the C++ API. The C++ API is fully supported and in fact is what [TensorFlow Serving](https://tensorflow.github.io/serving/) is built on. As a general rule, if it is documented on the TensorFlow website, it is supported :).\n- That said, the C API was designed with the intent of it being used to build other language bindings (see [howto](https://www.tensorflow.org/versions/r0.11/how_tos/language_bindings/index.html)) and that is what other language bindings such as [Go](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go), [Rust](https://github.com/tensorflow/rust), [Ruby](https://github.com/somaticio/tensorflow.rb) and others are built on (most of which are built and maintained by the community, not the TensorFlow team). Specifically, the C API should be appropriate for use with [FFI](https://en.wikipedia.org/wiki/Foreign_function_interface) support that languages typically provide (e.g, [JNI](https://en.wikipedia.org/wiki/Java_Native_Interface), [cgo](https://golang.org/cmd/cgo/), [LuaJITs FFI](http://luajit.org/ext_ffi.html) etc.).\n\nI am perhaps not fully grasping your concern here. Are you looking for a recommendation as to whether (1) using something like [P/Invoke](https://msdn.microsoft.com/en-us/library/aa446536.aspx) to wrap over the C API, or (2) cppsharp over the C API, or (3) cppsharp over the C++ API or (4) SWIG over the C API is preferable?\n\nI don't believe we have enough expertise in the .NET framework within the TensorFlow maintainers to make an informed recommendation, and we sincerely welcome any community contributions towards this. My gut instinct would say that it would be preferable to wrap over the small API surface of the C API using whatever [FFI](https://en.wikipedia.org/wiki/Foreign_function_interface) support is canonical in .NET and use that as a basis to provide an idiomatic C# library. As opposed to depending on other frameworks or \"transliterating\" everything in C++ API which may result in non-idiomatic APIs in the language. Though again, I say this without enough expertise in .NET or C# specifically.\n", "(oops, didn't mean to close the issue)\n", "Feedback: please take a look http://bytedeco.org/   (tensorflow is listed)\n\nWhy cppSharp? Many who have tried e.g. .NET wrapper for OpenCV for years (P/Invoke) are now trying to explore how to use cppSharp to generate .NET wrapper for OpenCV. \n\nI believe both cppSharp and JavaCpp are similar in the ways of creating wrapper codes automatically. They compile C++ codes and generate the c# or java binding codes respectively. \n\nThere is no more need to MANUALLY (in the case of SWIG and P/Invoke) to do regular catch-up with the fast development of the master c++ codes.  \n\nThis is my view, I may be wrong, I am open to feedback and to learn from others.\n", "Hey guys, dev from CppSharp here, just been made aware of this discussion. Just some clarifications.\n\nIf you want to bind a library with a generator like CppSharp, it's usually best to start with a C++ API from the start since that will lead to a more idiomatic generated API.\n\nWe can also generate bindings for the C API, but you will usually need to develop an higher-level C# API on top of the generated bindings to have a more idiomatic C# object-oriented API.\n\nThe only exception to this is when the original C++ public interface is complex / not very \"clean\" (boost, lots of templates) and then the auto-generated API might not be very clean and starting from the C API might be preferable.\n\nIf you do end up going with CppSharp we'll do our best to support the project and fix any CppSharp bugs found along the way.\n", "fyi - I built this script to spit out c# files (en masse) from tensor flow proto buffers for use with grpc https://gist.github.com/johndpope/5d176f4eebeb7ec983fa77d945c18fb1\n\nIf you want to co-ordinate efforts further -> jump on #grpc channel\nhttps://tensor-flow-talk-invite.herokuapp.com/invite\n", "Having the C# wrapper use the C API through DllImport is the right approach. I have written many such things (having had to interface with Matlab-exported C a number of times). You can see something of my similar work here: https://github.com/BrannonKing/NLoptNet . I'm willing to work on it. Now I just need a Windows DLL.\n", "JUST DO IT\r\n", "You can now use tensorflow with pythonnet to call from .NET languages:\r\n\r\nhttps://github.com/pythonnet/pythonnet/issues/299", "@denfromufa have you tried it (running tensorflow from .net via phytonnet)? \r\nif you did - can you give some pointer on how to do it?", "Is first class support for c# even planned?\r\nDevelopment with wrappers and such is almost always a sub-par experience; and using tf in serious projects when there's no actual support from the devs is not something I'd do.\r\n\r\nBut I think its worth it. So many things are written using .NET nowadays...", "The TensorFlow maintainers do not currently have plans for creating a C# API. We do however maintain the [C API](https://www.tensorflow.org/how_tos/language_bindings/) intended for building language bindings and are open to changes to that to assist the community in building C# APIs.", "> @denfromufa have you tried it (running tensorflow from .net via phytonnet)?\r\n> if you did - can you give some pointer on how to do it?\r\n\r\n@tiagosomda you can ask from @snarb about his experience with tensorflow and pythonnet. I just tried importing it successfully, but have not played with it yet.", " @denfromufa it is ok. works, but I have used it mainly for sort of unit-testing of my C# implementation :)", "We are interested in doing this work.\r\n\r\nI believe someone in my team had started work on this, if not, we will be happy to contribute it.", "@migueldeicaza That'd be great! Please look at https://www.tensorflow.org/versions/r0.11/how_tos/language_bindings/ as a starter guide.\r\n\r\nAlso, if you'd like code review from the TensorFlow team, then please loop in me and @asimshankar after it's committed somewhere. Thanks!", "Pythonnet in .net works, but is not particularly practical. SWIG (using Linux/Mono) generated wrapper/manually exposing the C API in a wrapper is working well for me", "@luketg8 with SWIG, someone(s) [most likely from the C API guys who also must know .NET - unfortunately not in this case here] to **\"manually exposing\" to keep/catch up** with the _latest code change._ \r\n\r\nWith CppSharp, as discussed by @tritao, you need an experienced specialist committed to setup the **workflow** to create the .NET bindings, after that, users like us could just simply use the created workflow to catch up with the _latest code change_ \r\n\r\nWith CppSharp, not only there \"should not be\" issue making the .NET wrapper available for e.g. Linux/Mono, but if we lucky, we also get cross platform support. \r\n\r\nImagine having .NET bindings for tensorflow [through CppSharp]  while coding for Mac, iOS, Android and UWP. If anyone need concrete example, follow what @migueldeicaza's team does with creating .NET binding for c++ [Urho3D ](https://github.com/urho3d/Urho3D)to cross platform [UrhoSharp](https://github.com/xamarin/urho)\r\n\r\nIn my experience, there still need much work to make embedding python in .NET through Pythonnet reliable and hence practical.", "@JimSEOW I agree that it is a lot of effort to maintain the wrapper for the C API when there are new updates, but it is just what is working for me at the minute with the limited amount of C# I need to do. For now, I have just had to write a lot of my code in C++ and prototype in Python.\r\n\r\nHowever, I do agree (having experimented briefly) with the use of CppSharp for this, but I think I'd need to look into it more to write a proper conclusion.", "@jhseu Thanks for the pointer, it has proved to be very useful.\r\n\r\nI am making good progress on the API, I am doing a line-by-line port of the C API test to make sure that I got the proper API coverage, and also using this as an opportunity to make sure that the API is idiomatic C#.\r\n\r\nIt is not ready to be reviewed, but for reference, the code is here:\r\n\r\n     https://github.com/migueldeicaza/TensorFlowSharp\r\n\r\nI'll post on this thread when it is ready to be reviewed.", "Nice! Feel free to ping me by e-mail if you run into any issues or have any questions.", "Just emailed you on your profile email address :-)", "@jhseu [Not sure if you are the right person to ask] \r\nIs there a plan to build native library [[libtensorflow.dll](https://github.com/migueldeicaza/TensorFlowSharp/issues/21)] on Windows?\r\n\r\nSo we could persuade @migueldeicaza to get tensorflow/tensorsharp working on Windows?\r\n", "We don't have any immediate plans, but making it would likely be trivial once Bazel works on Windows. I believe that's coming soon, but I'm not sure.", "FYI: Here is some[ recent effort ](https://github.com/migueldeicaza/TensorFlowSharp/issues/22)to use Bazel Windows to build the native library [libtensorflow.dll]", "Along with these solutions, Emgu CV have also added a tensorflow importer to their DNN module https://github.com/emgucv/emgucv", "@jhseu It seems that Bazel NOW works on Windows\r\nhttps://github.com/tensorflow/tensorflow/issues/8919", "TensorFlowSharp now works on Window using **libtensorflow.dll** extracted from tensorflow-1.1.0rc1-cp35-cp35m-win_amd64.whl\r\nhttps://github.com/migueldeicaza/TensorFlowSharp/issues/49\r\n\r\nImage Recognition using Tensorflow now works through TensorflowSharp in Windows (c#)", "@JimSEOW great news, thank you everybody for your efforts!", "great news to me as well, thanks to tensorflow team! Genius!", "\r\nImage Recognition based on this [example](https://github.com/migueldeicaza/TensorFlowSharp/tree/master/Examples/ExampleInceptionInference)\r\n![exampleinceptioninference_2017-04-06_12-10-19](https://cloud.githubusercontent.com/assets/8244633/24749119/2b519c08-1ac2-11e7-8f8e-fa57f796216e.png)\r\n", "JimSEOW, thanks for sharing, you are great!", "NO! @migueldeicaza makes this possible!!", "@DomLi and @luketg8 Could both you share your feedbacks, issues at [TensorflowSharp ](https://github.com/migueldeicaza/TensorFlowSharp/issues)\r\n\r\nIf you look through these open issues, there are still open technical challenges need to be addressed to make TensorflowSharp or .NET same FIRST class citizen as python for Tensorflow.", "will keep trying, but too busy with some other projects at moment, luke and me in the same office:):)", "Starting Release 1.2, Windows DLL will be  made available  and [tensorSharp binding work](https://github.com/tensorflow/tensorflow/issues/7258#issuecomment-301001697)s!", "Which is the status of .Net binding as of today? Are C# and F# really usable languages to work with Tensor Flow or the situation is still as described in [this previous comment](https://github.com/tensorflow/tensorflow/issues/18#issuecomment-270672765)? Thanks for the quick recap", "@giuliohome : The TensorFlow maintainers do not maintain or support the C#/F# bindings and have no plans of doing so. However, we happily point you to [TensorFlowSharp](https://github.com/migueldeicaza/TensorFlowSharp), which is built on top of the supported C API. Any questions/comments/concerns you have there are better raised in the TensorFlowSharp repository.", "@giuliohome please visit Tensorflowsharp. We are using the latest window dll contributed by this group C API. We need more people to port python tensorflow examples/tutorials codes to .NET. We have tested Xamarin Workbook (Jupiter notebook alternative) to support the porting and sharing of .NET example tensorflow codes", "can anyone plz tell me how to write a fresh tensor flow C# project in Visual Studio MacOS? What project should I choose? \r\n<img width=\"1109\" alt=\"screen shot 2017-09-23 at 1 42 03 am\" src=\"https://user-images.githubusercontent.com/15967311/30762971-d0ea0e42-a001-11e7-95d9-ccedaaa404cf.png\">\r\n\r\nAlso I have installed TensorFlowSharp using Nuget from inside of Visual Studio MacOS but when I built the existing example project from within of Visual Studio nothing happens\r\n\r\n![tensorflowsharp](https://user-images.githubusercontent.com/15967311/30763374-884a4010-a003-11e7-9b3c-92bd20a5288c.gif)\r\n\r\nIt says built successful but nothing happens. What's the output for `Example Common` generates? Anything that it generates(outfile, image, graph etc)?\r\n\r\nIn case of Tensor Flow & Python....it's shows output in Jupyter(automatically fires in browser) but don't know here :( \r\n\r\nSomeone plz help me out :( \r\n\r\n@migueldeicaza tried to help me on Twitter & on this https://github.com/migueldeicaza/TensorFlowSharp/issues/149#issuecomment-331361048\r\n\r\nissue but no luck so far", "Your best bet is the instruction right here:\r\nhttps://github.com/migueldeicaza/TensorFlowSharp/issues/149#issuecomment-331329430\r\n\r\nThere is still no official C# support in TF itself, so we wont be able to help much.", "@gunan but how has he created this projects? ", "ExampleCommon is a library project, not an executable project.   That is why it does not have a [>] icon, but a [|>] icon.   Open an actual example.", "@migueldeicaza  amazing effort! Thanks a ton!", "so is this thread for C# TensorFlow. I mean will you be able to train models in C# after that? Just like we do in Python?", "Yes, you can train models in C#.   It seems like people like to use it to reuse existing code that imports their existing data into TensorFlow.   \r\n\r\nToday it is better to use Python and Keras to define your models/graphs and then load the result from C# and run that.", "Probably this solution will be helpful for C# developers: [TensorFlowServingCSharpClient](https://github.com/Wertugo/TensorFlowServingCSharpClient)\r\n\r\nThere are TF MNIST Deep model learning and save model for TF Serving. \r\n.NET console application and SPA .NET Core 2.0 ReactJS examples. It use TensorFlow Serving API with gRPC service calls.", "@migueldeicaza Thanks again for TensorFlowSharp! As the feature has been already implemented, I am closing this issue", "My company is developing a full C# binding to Python API: https://losttech.software/gradient.html", "Another solution is [TensorFlow.NET](https://github.com/SciSharp/TensorFlow.NET).", "Latest [tensorflow.dll ](https://github.com/tensorflow/tensorflow/issues/35405#issuecomment-653621878)for c# api "]}, {"number": 17, "title": "Windows Support and Documentation", "body": "I was excited to see tensorflow, but as many other users, we are on Windows, would be nice to see this support happen. Will you accept Windows port contributions?\n\nIn the meantime, Microsoft recently released their Deep Learning toolkit which scales on multiple machines with GPUs for both Linux and Windows. https://github.com/Microsoft/CNTK\n", "comments": ["I think its a great suggestion!\n", "Same, I was kind of disappointed to see no mention of Windows in the download and install page.\n", "Anyone have an idea of what the major incompatibilities/accommodations are? Is it it mostly issues with file paths, etc? \n\nIt's built with Bazel, which only supports linux/mac, but the good news is that windows support for bazel seems will be out by the [end of this year.](https://github.com/bazelbuild/bazel/issues/276)\n", ":thumbsup:\n", "You can already use TensorFlow on a Windows machine by using Docker. \nDetails are in this thread.\nhttps://github.com/tensorflow/tensorflow/issues/42\n", "I would like to use TensorFlow on Windows without Docker, in order to use GPU compute. Using Docker in this case is not using a Windows container, but a Linux virtual machine on Hyper-V or VirtualBox, and so the GPU will not be passed through.\n", ":+1: \n", "google, give us  windows support , please!\n", "`+1/0.0`\n", "+1\n", "Yeah, windows support would be super nice.\n", "+1\n+1\n+1\n\nyes please\n", "Would love to use TensorFlow on Windows (native, not in a VM).\n", "I'll take a look into whether Continuum can provide a conda package for tensorflow.\n", "Judging by the use of bazel across the documentation, I assume its a matter of waiting for bazel to support windows. Is there anything specific to Tensorflow that would need to be addressed for windows to be supported, or is just bazel?\n", "One question I have is when Bazel support is actually arriving on Windows.  Looking at the Bazel repository it says they are planning to support Android in Windows, but I didn't see any reference to building (what I assume are) native packages.\n", "Here is the bug we are using to track Bazel support for Windows: https://github.com/bazelbuild/bazel/issues/276. A month ago, @dslomov was able to [get Bazel to bootstrap itself on Windows](https://twitter.com/bazelbuild/status/657583995849457665). The plan is to finish Windows support by end of this year.\n", "@davidzchen I was actually working with that repo earlier this afternoon to try and give it a go without much luck, I might play around with it some more to see if I can get it to successfully bootstrap itself.\n", "+1 native support on windows should be available\n", "+1\n", "or maybe we could port it to a more reasonable build system. perhaps cmake.\n", "@ahmadia do you have any tips on how to install tensorflow using conda?\n", "Right now we're in the same boat as everybody else because there is no port of TensorFlow to Windows.  If somebody can put together a Windows port I'm happy to help with the binary build/deployment.  Somebody has already put together a recipe for OS X/Linux available with `conda install -c memex tensorflow`.\n", "+1 for this\n", "+1\n", "+1\nalso cmake support would be great\n", "I put together an article with instructions and screenshots for getting [TensorFlow to work on a Windows machine using Docker here](http://www.netinstructions.com/how-to-install-and-run-tensorflow-on-a-windows-pc/) using the tips from [issue 42](https://github.com/tensorflow/tensorflow/issues/42) if it's helpful for anyone. \n\nAlthough I'm still not sure how I can take advantage of my GPU though if TensorFlow is running in a VM on my Windows machine.\n", "+1 native windows support would be extremely helpful\n", "+1\n", "I want have a version of TensorFlow for Windows 7,if you have,Please send my email.\nMy email address:   410524814@qq.com.\nThanks!\n", "+1\n", "Windows support is a bit of a dramatic effort. Maybe supporting Windows only for a truly standard compiler (gcc / tdm-gcc / mingwpy) and on a modern Python (3.4?) would be a much more easy target ?\n", "Perhaps using Clang frontend with VC++ CodeGen would be a fastest pathway to get working Windows builds -> [Clang with Microsoft CodeGen in VS 2015 Update 1](http://blogs.msdn.com/b/vcblog/archive/2015/12/04/introducing-clang-with-microsoft-codegen-in-vs-2015-update-1.aspx). Project was designed to provide consistently working toolchain for cross platform   builds of code primarily targeting windows, nonetheless scenario in which *nix code is compiled on windows could be equally interesting. MSFT C++ team states that code they write is contributed back to LLVM project so it should allow for fast and joint work on getting good cross platform support. \n", "+1\n", "+1\n", "+1\n", "+1\n", "+1\n", "+1\n", "+1\n", "+1 . This is the first Python package I recall not working on Windows.  It seems odd to me, as my understanding is that NVIDIA's drivers have always been better on Windows than Linux, and I have a GTX 970 that's feeling bored these days.\n", "I have ported most of the tensorflow c++ to a windows build use vc 2013. so far most code compile and linked use my own sources file. the biggest challenge for me to work around is that: it use static variable to register op and kernel. I build them as static library and app link to it will skip those static variable. I have to use a stub.cpp in app folder to include used ops and kernels.\n", "@yuanhua8 any chance you push your changes to a github repo? BTW, I solved the static registration thing in caffe by using dumpin to generate a header that forces symbol liking. See https://github.com/BVLC/caffe/pull/2816 and  https://github.com/willyd/caffe/tree/msvc for details.\n", "Hope native Tensorflow for Windows will be ready soon, might have to wait until bazel for Windows is stable, thrilled to try [Deep Learning course](https://www.udacity.com/course/deep-learning--ud730) by Google and Udacity!\n", "FYI Windows support for Bazel is [currently one of our top priorities](https://groups.google.com/d/msg/bazel-discuss/G-GL9F_HgD0/0CAwKEFhGAAJ). Stay tuned.\n", "Same here, i begun the deep learning course offered by Google and here i am stuck because i am running Windows.\nWhen should we expect support for Windows ?\n", "I noticed lot of people facing a problem running this under Windows so did a quick write up on how to set it under Windows using Vagrant and Docker:\n\nhttps://medium.com/@Rapchik/running-google-s-deep-learning-course-material-under-windows-82d468b6d5be\n", "@umarniz Thanks for the guide. However, I think, what most people here anticipate to see is TensorFlow running natively on Windows **because of GPU support**. That unfortunately doesn't work with Docker as far as I know nor any other solution using virtualization... \n", "@SeveQ I completely agree and I personally prefer using Linux for all my ML experiments due to the ease of setting up multiple unique programming environments. This guide was meant to run the course material from Google as the training sets and examples they have are for beginners with smaller data sets which should be able to run in a CPU only environment too.\n", "@umarniz Alright, didn't mean to denigrate your effort. Definitely not! You did a great job writing that up! Thanks again!\n\nBy the way, only to mention this... the Deep MNIST example, which is admittedly already not that simple anymore, takes several hours on my CPU whereas my GPU (GTX 980Ti) rushes through it in at most a few seconds. On a native Ubuntu that is. I've one set up on a USB stick. It works, however dual boot can be quite cumbersome and a huge demotivating factor. Even more so since a hibernated Windows on a UEFI system can be pretty picky when it comes to accessing data on NTFS partitions from Linux...\n\nSure, I totally agree that Linux (Ubuntu) is the best OS for ML tasks because it's the one most people use, and not without reason. But it still has its disadvantages for enthusiast people like me who not only do ML for a living but also as hobbyists, and who have other hobbies in parallel that rely on Windows... like gaming for example. \n\nI guess that's some first world problems that I'd absolutely love to see solved nevertheless. \n", "@SeveQ I didn't take it that way either :)\n\nI agree with dual boot being a pain, I had to completely move to Ubuntu for a few months otherwise it becomes quite cumbersome to switch but I love the power that bash shell gives for working with large data sets.\n\nI think i'll see what I can do to help add Windows support for TensorFlow myself.\n", "+1\n", "@umarniz Sounds good. I don't have the time right now to also dedicate myself to this, unfortunately. Other priorities like graduating... Windows support for TensorFlow would be a great help though in this matter. I'm graduating as MSc., systems engineer, specializing in, who would have thought, machine learning... \n", "any update on this\n", "I make a port of the c++ code into our internal branch. Sorry can not publish back since it take dependency of our code branch. Find some gap btw c++ and Python. But the core engine work for me in our windows/Vc environment.\nH y \n\nSent from my iPhone\n\n> On Jan 30, 2016, at 2:46 AM, datashinobi notifications@github.com wrote:\n> \n> any update on this\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n", "For all those who want to use Tensorflow and can surrender GPU support [here is an Blogpost](http://data-shaker.com/docker-tensorflow-with-jupyter-notebook-on-windows/) about installing Tensorflow with jupyter notebook support for Windows with Docker.\n", "An update on Windows support for Bazel: the initial set of patches to get Bazel working on Windows has been merged (see bazelbuild/bazel#276), and @dslomov has been able to get Bazel to [bootstrap itself on Windows](https://twitter.com/mulambda/status/700735662354333696).\n\nWe are planning to have experimental support for Windows in [Bazel 0.3](http://bazel.io/roadmap.html). If you would like to follow our progress, see the [issues tagged \"Windows\"](https://github.com/bazelbuild/bazel/labels/Windows) on the Bazel issue tracker.\n", "C:\\Users\\Desktop>docker run -it b.gcr.io/tensorflow/tensorflow\n\nUnable to find image 'b.gcr.io/tensorflow/tensorflow:latest' locally\n\ndocker: Error response from daemon: unable to ping registry endpoint https://b.g\ncr.io/v0/\nv2 ping attempt failed with error: Get https://b.gcr.io/v2/: dial tcp 64.233.188\n.82:443: i/o timeout\n", "Any news regarding being able to use TensorFlow in Windows, without Docker?\nHow hard would it be to offer a CMake build process that works both for Linux and Windows (creating a MVS project)?\n", "I wrote [instructions](https://gist.github.com/yoavram/36676bde2847f225842694f22aa4a99c) for installing tensorflow on a CentOS virtual machine. If you're having trouble working with Docker, this might be your solution.\n", "Guys if you install bash on Windows using the new developer preview tensorflow does install and kinda work.  So you can use that to run it on Windows without using docker. (And it would probably improve the bash for Windows because more people are using it.)\n", "That's fantastic. \nUsing bash on Windows was my biggest hope to use  tensorflow on Windows. \n\nThe biggest question is whether you  can use the GPU drivers? \n\nThat's the main reason people asking for a native tensorflow installation in windows. Otherwise cpu support is  just fine through docker or Linux virtual machine \n", "I have no knowledge of that but we should ask the Windows team about that.\nPeople have gotten GUI programs to work through windows but it uses CPU not\nGPU so I'm thinking it does not currently have access\nOn Apr 19, 2016 5:37 PM, \"hayder78\" notifications@github.com wrote:\n\n> That's fantastic.\n> Using bash on Windows was my biggest hope to use tensorflow on Windows.\n> \n> The biggest question is whether you can use the GPU drivers?\n> \n> That's the main reason people asking for a native tensorflow installation\n> in windows. Otherwise cpu support is just fine through docker or Linux\n> virtual machine\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/17#issuecomment-212156142\n", "Yeah. I read that GUI programs run through bash in windows will use a generic GPU driver. Which means does not use the nvidia drivers.\n\nLet's pray that Google is working on a tensorflow on Windows under the hood.  \n\nI think bash for Windows  needs time to be mature enough and bug free. In the meantime probably VM and docker will be a better choice. \n", "I am now making a choice between tensorflow and CNTK. \n", "+1 for Windows native support, including GPU.\n", "The link to follow Bazel's Windows issues provided by @davidzchen isn't working, the correct one is https://github.com/bazelbuild/bazel/labels/category%3A%20windows\n\n(I know everyone is hoping for Windows native support + GPU, I am waiting too, but we have to wait until Bazel for Windows gets stable, helping in testing and contributing to Bazel will probably speed up the process)\n", "another +1 for windows with GPU support. Although TF seemed a better option, I had to switch to Theano as a result for lack of GPU support on windows. It will be nice if there is a mention on TF's roadmap as to whether support is going to be available in future releases, as it will help with loads of researchers decision making. \n", "+1\n", "+1\n", "One interesting piece of news: @shanselman wrote a [blog post](http://www.hanselman.com/blog/PlayingWithTensorFlowOnWindows.aspx) about running TensorFlow on  Bash for Windows.\n\nWe still intend to provide first-class Windows support, but adventurous users might find this a good way to get started in the mean time.\n", "@mrry Hi Derek Murray, \nI am glad to hear  from you -  as a Google software engineer -  that you intend to provide a native windows support for tensorflow.  Are you part of the tensorflow developer team at Google? \n\nAt least I have a hope now. \n\nAny rough estimate when will be a beta release? \n", "Another +1, very interested to see this happen. If and when Bazel actually runs on Windows, will TF actually compile on the platform? Or are there other portability issues as well e.g. network, file system calls?\n", "+1\n", "Folks, could you please use [GitHub reactions](https://github.com/blog/2119-add-reactions-to-pull-requests-issues-and-comments) rather than writing \"+1\" in a new message? Right now it produces a lot of spam for people who subscribed to updates in this thread. Thanks!\n", "+1 for GitHub reactions!\n", "Well bazel seems to work most of the time on windows now, aside from a few name errors (illegal characters or very long commands). There definitely are issues to getting a working windows tensorflow other than just having a working bazel. All the errors I have got so far are of the type:\n`ERROR: C:/tensorflow-orig/tensorflow/contrib/metrics/BUILD:16:1: in linkshared attribute of cc_binary rule //tensorflow/contrib/metrics:python/ops/_set_ops.so: 'linkshared' used in non-shared library. Since this rule was created by the macro 'tf_custom_op_library', the error might have been caused by the macro implementation in C:/tensorflow-orig/tensorflow/tensorflow.bzl:599:31.`\nSo I am guessing build rules need to be updated for windows first.\n", "The official roadmap of tensorflow :\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/resources/roadmap.md\n\nWindows support is part of the near future roadmap that is targeted for the next few months!\n", "@Sabrewarrior I tried to build the tensorflow on windows using bazel,  but the configure file just give some errors. I wondering if you changed or added some parts on that file to be supported on windows\n", "@Fhrozen https://github.com/Sabrewarrior/tensorflow/blob/test/tf_win_env.txt \nThese are my installed packages when I ran configure without any changes. I am running this on msys2 with python 2.7 on a Windows 10 machine.\n", "@Sabrewarrior thanks for the answer. I checked it, was the spaces in the folder name my problem.\nNow i got this error on the cuda version. \n\n>  `Please specify which gcc nvcc should use as the host compiler. [Default is /mingw64/bin/gcc]:\n> Please specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 7.5\n> Please specify the location where CUDA 7.5 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: C:/CUDA/v7.5\n> Please specify the Cudnn version you want to use. [Leave empty to use system default]: 4.0.7\n> Please specify the location where cuDNN 4.0.7 library is installed. Refer to README.md for more details. [Default is C:/CUDA/v7.5]: cuda/\n> Please specify a list of comma-separated Cuda compute capabilities you want to build with.\n> You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\n> Please note that each additional compute capability significantly increases your build time and binary size.\n\nSetting up Cuda include\nSetting up Cuda\nln: fallo al crear el enlace simb\u00f3lico '../../../third_party/gpus/cuda/extras/visual_studio_integration/MSBuildExtensions/CUDA': No such file or directory\nln: fallo al crear el enlace simb\u00f3lico '../../../third_party/gpus/cuda/extras/visual_studio_integration/MSBuildExtensions/7.5.props': No such file or directory\nln: fallo al crear el enlace simb\u00f3lico '../../../third_party/gpus/cuda/extras/visual_studio_integration/MSBuildExtensions/CUDA': No such file or directory\nln: fallo al crear el enlace simb\u00f3lico '../../../third_party/gpus/cuda/extras/visual_studio_integration/MSBuildExtensions/7.5.targets': No such file or directory\nln: fallo al crear el enlace simb\u00f3lico '../../../third_party/gpus/cuda/extras/visual_studio_integration/MSBuildExtensions/CUDA': No such file or directory\nln: fallo al crear el enlace simb\u00f3lico '../../../third_party/gpus/cuda/extras/visual_studio_integration/MSBuildExtensions/7.5.xml': No such file or directory\nxargs: bash: finish with status 255; aborting\n`\n", "@Fhrozen AFAIK you can't use gcc as a compiler for CUDA on Windows. Only Visual C++ Compiler is supported by CUDA on Windows ([see here](http://docs.nvidia.com/cuda/cuda-getting-started-guide-for-microsoft-windows/)). \n", "+1\n", "# AGAIN! PLEASE DON'T MAKE USELESS _+1_ COMMENTS! USE THE GITHUB REACTIONS INSTEAD!\n", "+1\n", "# stop +1!!!\n\nI repeat, stop +1 for $%{^\u00a5 sake! Anyone who comments +1 after this is an idiot! :-P Use Github reactions, dammit! \n\nCould someone give me some kind of moderator status? I'd like to clean this issue from useless +1 comments. \n", "Hi team. I'm trying to understand what work needs to be done to be able to use TensorFlow on Windows (10, I guess) with GPU support via CUDA. From what I know so far, that rules out Docker because Docker cannot access the host's GPU, and means that we must use Visual Studio's cl.exe (which is what Bazel for Windows does use).\n\nFrom that, is it just Bazel for Windows that needs to be progressed? Is that the only unsatisfied dependency here? Happy to be corrected on any point.\n", "If I want to turn on the Bash on Windows, I need to check the **Windows Subsystem for Linux (Beta)** right? But I can't find this menu in the Windows Features dialog. I'm following the guide [here](http://blog.mosthege.net/2016/05/11/running-tensorflow-with-native-linux-binaries-in-the-windows-subsystem-for-linux/) and [here](http://www.hanselman.com/blog/DevelopersCanRunBashShellAndUsermodeUbuntuLinuxBinariesOnWindows10.aspx).\nDo I need to tick Hyper-V check box in the Windows Features dialog?\nI've also turned on the Developer mode in the settings already.\n\nIt seems like I need to desert these Windows beta bash features and unwillingly use Docker?\n", "@off99555 You're Windows 10 should have the latest preview release (Fast Ring). Check your build, if it is lower than ~14000, then you don't have this feature yet. (This update will be GA August 2nd)\n", "Hello,\n\nFor Tensorflow Windows support, are we going to be able to build out Windows Binaries that are not msys2 binaries? This makes sense since it would be a possible to do actual deployment on most user's machines.\n\nIf this is not possible, can we have a way to just deploy a stripped down version of the prediction pass/forward pass of the library for this purpose? I can train on Linux just fine but for the actual prediction process, I will like to be able to deploy on a regular Windows box without msys2. Is this on the roadmap?\n\nIf you look at what CNTK is doing:\nhttps://github.com/Microsoft/CNTK/wiki/Native-Evaluation-Interface\n\nThey have a much easier way to do evaluation/prediction using a stripped down DLL. Can we have something like this in Tensorflow as well for Windows ( and other platforms I can imagine will find it very useful).\n\nThanks!\n", "I don't understand why you are added support for Mac OS and not Windows. You cannot use normal CUDA GPU on most Macs. It is useless platform for deep learning training\n", "Can the team tell us  please about estimates for native windows support?\n", "@mrry is actively working on this and can provide more updates on this if you have specific questions.\n", "@aselle  Thanks for the useful update, much appreciated!\n\n@mrry Hello, how are the specifics of the Windows implementation going to be rolled out?\n\n1) Will we have (ideally) a VS2015 solution that we can build with our apps/tools? A cmake method that generates the VS solution works too.\n\n2) Is this going to be a statically linked or dynamically linked library?\n\n3) Will there be a lightweight forward pass/evaluation module/library that we can use for deployment with our tools/apps?\n\nThe current resource I could find regarding deploying tensorflow with a C++ standalone app is here:\nhttps://medium.com/jim-fleming/loading-a-tensorflow-graph-with-the-c-api-4caaff88463f#.7ejb8h7zk\n\nMy main concern is what is stated here:\n\"The build is huge, coming in at 103MB, even for this simple example. Much of this is for TensorFlow, CUDA support and numerous dependencies we never use. This is especially true since the C++ API doesn\u2019t support much functionality right now, as a large portion of the TensorFlow API is Python-only. There is probably a better way of linking to TensorFlow (e.g. shared library) but I haven\u2019t gotten it working yet.\"\n\nIf there is a better way to get around linking a large static library that will be great, especially for the purposes of a forward pass/evaluation which does not really need GPU support.\n\nThanks!\n", "@mrry How can others get involved in this? How can we help to get TensorFlow running on Windows as soon as possible? At least we can help with testing and bug reporting.\n", "+1\n", "Any updates on windows? whats needed to get it run natively having GPU available as well?\n", "We need windows support!!!!\nCan the team tell us please about estimates for native windows support?\n", "No windows? why? at least CPU only versions?\n", "Here is a summary of what I think is happening in this Windows support issue:\n\nTensorflow requires [Bazel](https://github.com/bazelbuild/bazel/) to build from source, If I am not mistaken, Bazel is a build system like GNU Make, but not a compiler.\n\nBased on what I saw as I briefly scanned through the code, source code of Tensorflow itself uses mostly standard C++ library for things like threading so it should be no problem to compile on Windows, essential third-party libraries it uses all have Windows support.\n\nTherefore, I think the main problem lies on lack of build method for Windows rather than source code itself. There are works on using CMake to build instead of Bazel but not complete yet. If someone can translate Bazel build rules to CMake's, I think we will be able to build it on Windows.\n- https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile\n- https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake\n\nStable Bazel support for Windows still have [a long way to go](https://github.com/bazelbuild/bazel/issues?q=is%3Aopen+is%3Aissue+label%3A%22category%3A+multi-platform+%3E+windows%22).\n\nPlease correct me if I made a mistake here.\n", "@eiva If you want CPU-only support you can run Tensorflow in a virtual machine. I've run it on Ubuntu 14 on a Windows 10 host without issue.\n", "@rongjiecomputer I honestly wonder why they used Bazel at all...\n\n @eiva There's also a ready built Docker container that you can use on a Windows host. Works out of the box. It's just not that much fun to work with tensorflow without GPU support. For the cool stuff at least a single GPU is a requirement. You can of course run basic stuff like linear or logistic regression without a GPU. But that's just not tensorflow's long suit. It's made for deep learning tasks which basically require a GPU. \n", "@marklit i use it this way already, but most of my app running on windows, and most of development is going on windows... Its not easy to switch os each time i doing something with different parts of system...\n", "@Loo Rong\nYour speculation is right!\nhttps://www.quora.com/Why-did-Google-decide-to-use-Bazel-with-TensorFlow/answer/Derek-Murray-3\n\nOn Sep 12, 2016 12:17 PM, \"Eugene Ivanchenko\" notifications@github.com\nwrote:\n\n> @marklit https://github.com/marklit i use it this way already, but most\n> of my app running on windows, and most of development is going on\n> windows... Its not easy to switch os each time i doing something with\n> different parts of system...\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/17#issuecomment-246239719,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AEBIBhhA-t1MrFnddm5k-wER0KAm_p30ks5qpNJTgaJpZM4GexU4\n> .\n", "Thanks for all of the interest in TensorFlow on Windows! We're making progress on two main fronts:\n1. Adapting TensorFlow's Bazel `BUILD` files to work on Windows. You may have seen some recent pull requests from @meteorcloudy in that direction (like #4449), and our eventual plan is to have full support for building TensorFlow, using Bazel, on Windows.\n2. Modifying the TensorFlow runtime to build with the Visual C++ 2015 compiler. I've been working on this effort, and the diff between HEAD and my working branch is getting smaller every day. For now, I'm using CMake to build TensorFlow using Visual Studio/MSBuild, but I plan to switch to Bazel when the `BUILD` files work cross-platform.\n\nWe expect to make an announcement soon, with a binary PIP package and instructions for building TensorFlow on Windows. In the meantime, we will update this issue when we have news to share.\n\n---\n\nResponding to the [specific questions](https://github.com/tensorflow/tensorflow/issues/17#issuecomment-239970521) from @kestrelm:\n1. I am currently using CMake to generate a VS solution with multiple projects, and we'll merge this as part of item (2) above.\n2. At the very least, we can generate a Python extension DLL containing the runtime and all kernels (the equivalent of `tensorflow/python/_pywrap_tensorflow.so`), and provide instructions for building a statically linked C++ binary (the equivalent of `tensorflow/cc/tutorials/example_trainer.cc`).\n3. We can easily make a version of the runtime with those features (something like the Android subset of inference-related kernels in a DLL behind the C API), although it probably won't be part of the initial release. It will however be possible to modify the build files to make a target that has these properties, and I'll be happy to help anyone who's trying to do that.\n", "Awesome work, @mrry!\n\n> We expect to make an announcement soon, with a binary PIP package and instructions for building TensorFlow on Windows\n\nCan you give any rough estimates on when that announcement will be? I can't wait!\n", "@mrry : Derek, solid update. Do you have a binary GPU release (BETA) before the rest of the makefile/compiler framework solidifies? At least folks could try out against the Python API ...\n", "Dear Gurus:\n\nI tried to build tensorflow from source code on Windows 10. It reports bazel is already supported on Windows although might not be stable. Nevertheless I got a working one.\nHowever, when I tried to build tensorflow, there're two problems, first it indicates \"bazel clean --expunge_async\" failed with permission problem. I bet it's not that severe. The next problem is a little vital since it blocks the build. it reports:\n\n...\nINFO: Found 1 target...\nINFO: Writing explanation of rebuilds to 'logfile'\nERROR: missing input file '@local_config_cuda//cuda:lib64/libcublas.so'.\nERROR: C:/tensorflow/tensorflow/tools/pip_package/BUILD:23:1: //tensorflow/tools/pip_package:build_pip_package: missing input file '@local_config_cuda//cuda:lib64/libcublas.so'.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nERROR: C:/tensorflow/tensorflow/tools/pip_package/BUILD:23:1 1 input file(s) do not exist.\nINFO: Elapsed time: 34.644s, Critical Path: 1.52s\n\nActually during the configuration process, I deliberately disable GPU support, so why it tries to load dynamic library related to CUDA?\n\nSince I disable GPU support, the stub libraries are correctly created with zero size and with the DLL file extension. However, as in the error message, I don't know where the requirement of loading CUDA related libraries originated. I checked several files but still have no idea. Meantime, I also think the file is poorly written, since it should look for DLL files instead SO files. However I still have no clue.\n\nAnyone has any idea to this problem? Thanks a lot.\n", "As you may have seen with PR #4778 merged, we now have preliminary support for building TensorFlow on Windows using CMake. It supports build a CPU-only version of TensorFlow for use in the C++ example trainer program, and a PIP package for Python use. For more details and instructions on how to build TensorFlow from source on Windows, see the [CMake readme](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md).\n\nThis is alpha-quality code, and we expect there to be bugs in this initial version. If you find one, please raise a new GitHub issue for the specific problem.\n\nOur focus now shifts to getting GPU support for TensorFlow on Windows, and transitioning to a common Bazel-based build for Windows and Linux. If you have other feature requests, please raise a new GitHub issue.\n", "I was using this prebuilt project https://github.com/shishaochen/TensorFlow-0.8-Win, but it does not support GPU.\n", "@mrry Hi, Derek, appreciate your contribution to TensorFlow.\n@cesardelgadof Hi, Cesar, thanks for your information.\n\nHowever, I think the previously obstacle hindering build TensorFlow on Windows platform is the unavailability of bazel on Windows. Since now it's available, though may not be stable, so in my opinion we'd better try to solve the problem ahead when building TensorFlow using bazel.\n\nI personally think guys at Google, if they do have the plan to support native build on Windows, they probably will choose bazel. And once the official solution available, I guess people will switch from cmake to bazel. So if there's such a day, why we work together to have the day come early?\n\nNevertheless I will try the cmake version, and hopefully someone else will also try the bazel build system. Probably we can figure out what's the nasty error comes from, and locate the poorly written configuration file.\n\nThanks for you all. \n", "@mrry Thanks for the major contribution! I've tried to follow your instructions. All seem to work flawlessly (for about 45min) until it failed with:\n\nThe target \"BeforeGenerateProjectPriFile\" listed in a BeforeTargets attribute at \"C:\\Program Files (x86)\\MSBuild\\Microsoft\\NuGet\\Microsoft.NuGet.targets (186,61)\" does not exist in the project, and will be ignored.\nDone Building Project \"H:\\PycharmProjects\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_build_pip_package.vcxproj\" (default targets) -- FAILED.\n\n\"H:\\PycharmProjects\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_build_pip_package.vcxproj\" (default target) (1) ->\n\"H:\\PycharmProjects\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow.vcxproj\" (default target) (3) ->\n\"H:\\PycharmProjects\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_cpu.vcxproj\" (default target) (4) ->\n\"H:\\PycharmProjects\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_framework.vcxproj\" (default target) (5) ->\n(CustomBuild target) ->\n  C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(171,5): error MSB6006: \"cmd.exe\" exited with code 1. [H:\\PycharmProjects\\t\nensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_framework.vcxproj]\n\n```\n30 Warning(s)\n1 Error(s)\n```\n", "@laudney Thanks for giving it a try, and I'm sorry to see that you're getting an error! Since this issue has a lot of subscribers, I've created issue #4798 to track the fix for this problem, so let's continue the discussion there.\n", "@mingyr We hope to have TensorFlow building on Windows with Bazel as soon as possible. @meteorcloudy has been doing sterling work to get this to happen, and he just sent PR #4796, which enables Bazel to build `//tensorflow/cc:tutorials_example_trainer` on Windows. We'll aim to get that merged soon.\n", "@mrry  Thanks for #4798 , it now builds ok, also tested python whl install and no problems found so far. \n", "Hope to see tensorflow in conda soon.\n", "Use Tensorflow on virtualized environments  as Docker/VirtualBox is a very bad solution. Tensorflow should run on Windows natively. (And since the most is coded in Python should not be a problem create a distribution for Windows.)\n", "Hi everyone! As mentioned by @mrry, I am working on TensorFlow Windows build with Bazel. With PR #4796 and PR #4874 merged, the C++ example trainer now builds on Windows with [Bazel 0.3.2](https://www.bazel.io/blog/2016/09/07/bazel-windows.html)!\n\nHere is a brief instruction about how to build TF with Bazel on Windows:\n- Install Bazel on Windows([installation instructions](https://www.bazel.io/blog/2016/09/07/bazel-windows.html))\n- Setup the environment Bazel needs for [building C++ with MSVC](https://www.bazel.io/versions/master/docs/windows.html#build-c)\n- Clone TF repository, run `./configure` in MSYS\n- Run `bazel build -c opt --cpu=x64_windows_msvc --host_cpu=x64_windows_msvc //tensorflow/cc:tutorials_example_trainer --verbose_failures`\n\nFor convenience, you can put the build options in `~/.bazelrc`, mine is like:\n\n```\nbuild -c opt\nbuild --cpu=x64_windows_msvc --host_cpu=x64_windows_msvc\nbuild --copt=\"/w\"                  # Suppress some warning messages\nbuild --experimental_ui            # Enable a nice UI \n```\n\nA known issue is that Bazel's MSVC [wrapper script](https://github.com/bazelbuild/bazel/tree/master/tools/cpp/wrapper/bin/pydir) doesn't support python3 in Bazel 0.3.2, but it's already been fixed at https://github.com/bazelbuild/bazel/commit/ce5c33dd7e96aff0cf1eb993edd41a5fe8c423f2. You can solve it by setting `BAZEL_PYTHON` to a python 2.7 binary or building Bazel from HEAD.\n\nBuilding the C++ example trainer with Bazel takes about 20 minutes on my machine(Windows 7, CPU 2.9GHz, RAM 64G), an incremental build after changing some source file (i.e. [`array_ops.cc`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/array_ops.cc)) is around 1 minute.\n\nI am also making some progress on building the python PIP package on Windows with Bazel, I will send a PR as soon as possible.\n\nPlease try to build TensorFlow with Bazel on Windows, and tell us what we could improve!\n", "@meteorcloudy, Hi, Yun:\nThe C++ example trainer must built by the prebuilt bazel binary installed on Windows, or could I sync to the latest git version of bazel, and use the custom built bazel?\n\nActually I choose the second way, use the customer build bazel, however, it complains as follows:\n\nERROR: C:/tensorflow/tensorflow/core/BUILD:108:1: output 'tensorflow/core/exampl                                                                                                       e/example.pb.h' was not created.\nERROR: C:/tensorflow/tensorflow/core/BUILD:108:1: output 'tensorflow/core/exampl                                                                                                       e/example_parser_configuration.pb.h' was not created.\nERROR: C:/tensorflow/tensorflow/core/BUILD:108:1: output 'tensorflow/core/exampl                                                                                                       e/example.pb.h' was not created.\nERROR: C:/tensorflow/tensorflow/core/BUILD:108:1: output 'tensorflow/core/exampl                                                                                                       e/feature.pb.h' was not created.\n......\n\nProcedures for build I took:\n$ cd c:/tensorflow\n$ export JAVA_HOME=\"$(ls -d C:/Program\\ Files/Java/jdk\\* | sort | tail -n 1)\"\n$ export BAZEL_SH=c:/tools/msys64/usr/bin/bash.exe\n$ export BAZEL_VS=\"C:/Program Files (x86)/Microsoft Visual Studio 14.0\"\n$ export BAZEL_PYTHON=C:/Python27/python.exe\n$ export PATH=$PATH:/c/Python27:/c/Python27/Scripts:/c/tools/swigwin-3.0.10:/c/bazel/output\n\n./configure\n\nbazel build -c opt --cpu=x64_windows_msvc --host_cpu=x64_windows_msvc //tensorflow/cc:tutorials_example_trainer --verbose_failures\n\nAny suggestion?\n\nThanks.\n", "@meteorcloudy Hi Yun,\n\nI have followed similar procedure but I get the following error:\ncd C:/tools/msys64/var/tmp/Bazel/V1uFCi$H/execroot/tensorflow\nbazel-out/host/bin/external/protobuf/protoc.exe --cpp_out=bazel-out/vc_14_0_x64-opt/genfiles/ --plugin=protoc-gen-grpc=bazel-out/host/bin/external/grpc/grpc_cpp_plugin.exe --grpc_out=bazel-out/vc_14_0_x64-opt/genfiles/ -I. -Iexternal/protobuf/src -Ibazel-out/vc_14_0_x64-opt/genfiles/external/protobuf/src tensorflow/core/debug/debug_service.proto: com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nazel-out/vc_14_0_x64-opt/genfiles/external/protobuf/src: warning: directory does not exist.\n\nI checked the folders and there is no src directory in protobuf. Also there is no grpc folder in bazel_out/host/bin/external/ (so no grpc_cpp_plugin.exe file). \n\nthis error is followed by a bunch of errors that files in tensorflow/core are not created\n\nAny idea how to fix it?\n\nThanks\n", "@mingyr @karthiek Thank you for trying to build TF with Bazel! \nUnfortunately, according to http://ci.bazel.io/job/TensorFlow/ , TF build is even failing on Linux with Bazel at HEAD with similar error given by you. We are fixing it, meanwhile, can you try again with [Bazel 0.3.2](https://github.com/bazelbuild/bazel/releases)?\n", "@meteorcloudy Hi Yun:\n\nWhat do you mean by \"can you try again with Bazel 0.3.2\"?\n\nYou mean the official prebuilt Windows binary, or we can sync to the latest version of bazel, and use my build?\n\nActually I already sync to the latest version of bazel and use my build. If you indicate I should try the official prebuilt binary to see whether the problem is still pending, please kindly let me know.\n\nB. R.\n", "@mingyr Yes, I mean the \"official prebuilt binary\". I am having the same error after sync to HEAD.\nPlease see https://github.com/bazelbuild/bazel/issues/1929\n", "@meteorcloudy Hi, Yun\n\nThanks for the suggestion. I tried using 0.3.2 pre-compiled binary and was able to compile and generate tutorilas_example_trainer.exe file. \n", "@karthiek So glad you succeed! How long did it take? Is it faster than the CMake build?\n", "@meteorcloudy  It took around 15 minutes (Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz). Its just yesterday I had the necessity to build tensor flow on windows. So, I directly tried the last method you suggested. I didn't try building it using CMake. \n", "Hi, I tried to follow @meteorcloudy  approach but ended up with errors as below. Could anyone help?\n\n bazel build -c opt --cpu=x64_windows_msvc --host_cpu=x64_windows_msvc //tensorflow/cc:tutorials_example_trainer --verbose_failures\nINFO: Analysed target //tensorflow/cc:tutorials_example_trainer.\nINFO: Found 1 target...\nERROR: C:/Programming/tensorflow/tensorflow/core/BUILD:1064:1: Executing genrule //tensorflow/core:version_info_gen failed: bash.exe failed: error executing command\n  cd C:/tools/msys64/var/tmp/Bazel/s5nfxnhX/execroot/tensorflow\n  SET PATH=C:\\tools\\msys64\\usr\\local\\bin;C:\\tools\\msys64\\usr\\bin;C:\\tools\\msys64\\usr\\bin;C:\\tools\\msys64\\opt\\bin;C:\\Windows\\System32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\tools\\msys64\\usr\\bin\\site_perl;C:\\tools\\msys64\\usr\\bin\\vendor_perl;C:\\tools\\msys64\\usr\\bin\\core_perl;C:\\Program Files\\Google;C:\\WinPython-64bit-2.7.10.2\\python-2.7.10.amd64;C:\\WinPython-64bit-2.7.10.2\\python-2.7.10.amd64\\Scripts;C:\\tools\\swigwin-3.0.10;C:\\Program Files\\java\\jdk1.8.0_102\\bin\n  C:/tools/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; tensorflow/tools/git/gen_git_source.py --generate tensorflow/tools/git/gen/spec.json tensorflow/tools/git/gen/head tensorflow/tools/git/gen/branch_ref \"bazel-out/vc_14_0_x64-opt/genfiles/tensorflow/core/util/version_info.cc\": com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1\nTraceback (most recent call last):\n  File \"tensorflow/tools/git/gen_git_source.py\", line 258, in <module>\n    generate(args.generate)\n  File \"tensorflow/tools/git/gen_git_source.py\", line 210, in generate\n    git_version = get_git_version(data[\"path\"])\n  File \"tensorflow/tools/git/gen_git_source.py\", line 151, in get_git_version\n    \"--long\", \"--dirty\", \"--tags\"]).strip()\n  File \"C:\\WinPython-64bit-2.7.10.2\\python-2.7.10.amd64\\lib\\subprocess.py\", line 566, in check_output\n    process = Popen(stdout=PIPE, _popenargs, *_kwargs)\n  File \"C:\\WinPython-64bit-2.7.10.2\\python-2.7.10.amd64\\lib\\subprocess.py\", line 710, in **init**\n    errread, errwrite)\n  File \"C:\\WinPython-64bit-2.7.10.2\\python-2.7.10.amd64\\lib\\subprocess.py\", line 958, in _execute_child\n    startupinfo)\nWindowsError: [Error 2] The system cannot find the file specified\nERROR: C:/tools/msys64/var/tmp/Bazel/s5nfxnhX/external/jpeg_archive/BUILD:74:1: declared output 'external/jpeg_archive/jconfig.h' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)\nERROR: C:/tools/msys64/var/tmp/Bazel/s5nfxnhX/external/gif_archive/BUILD:35:1: declared output 'external/gif_archive/windows/unistd.h' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nINFO: Elapsed time: 4.967s, Critical Path: 0.96s\nFAILED: Build did NOT complete successfully\n", "@bssrdf your symptoms seem similar to https://github.com/bazelbuild/bazel/issues/1463.\nIt is a known issue of bazel on Windows that we break if your workspace is not on `C:` drive.\n\nIf that's the case, the workaround is to create a junction - if your tensorflow checkout is in `d:\\src\\tensorflow` the following should work\n\n```\nC:\\> mklink /j d_src d:\\src\nC:\\> cd c:\\d_src\\tensorflow\nC:\\D_SRC\\TENSORFLOW> bazel build ...\n```\n\nSee https://github.com/bazelbuild/bazel/issues/1463#issuecomment-251967323\nSorry for the inconvenience, we are fixing this asap.\n", "@bssrdf hmm on a second look maybe it is not it - could it be that you do not have `git` in PATH?\n", "@meteorcloudy Hi, Yun:\n\nI also confirm that there's no problem compiling the C++ example trainer using pre-compiled bazel binary version 0.3.2.\n\nMy notebook is a old one and meantime it runs other tasks, so the compiling time is nonsense from bench marking perspective.\n\nThanks for your work and look farword to compiling the python PIP package on Windows\n\nB. R.\n", "@mingyr That's great! \nBut just a notification for everyone, the example trainer is not buildable at TF HEAD, because my fix was reverted at be3bc472a52571a83f048479d6a4fa528b5a495e for some reason. \nThe good news is, building python PIP package with Bazel is close! Please see PR #4942\n", "@dslomov thanks for the reply. I did have to solve the bazel non-c: drive workspace problem as you suggested. I installed git for windows and git can be seen in the PATH by MSYS2. \n\nI just relaunched the bazel build command and it's working now, like a miracle. I don't know what happened yesterday. Anyway, this is just another confirmation of success on Windows 10 home, VS 2015 community edition, prebuilt bazel 0.3.2 and Winpython-2.7.10.2\n\nThanks a lot for Yun's work. \n", "@meteorcloudy, is it possible to build the GPU version following the procedure (with changes in bazel build of course)?\n", "@mrry Hi Derek, thanks for your contribution, but I have this problem with tensorboard, it does not working:\n\n```\nC:\\Users\\Cesar\\Documents\\Notebooks\\DeepLearning>tensorboard --logdir='./my_graph/'\nTraceback (most recent call last):\n  File \"c:\\program files\\anaconda3\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\program files\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Program Files\\Anaconda3\\Scripts\\tensorboard.exe\\__main__.py\", line 5, in <module>\nImportError: No module named 'tensorflow.tensorboard'\n```\n", "@bssrdf Probably not, because the code doesn't support GPU on Windows yet. @mrry did all the work of making TF code compile with MSVC, and he has been focusing on fixing GPU support.\n", "Do we have some sort of educated guess on ETA for production use?\n", "Since it seems relevant to people here using Bazel on Windows; i created a chocolatey package to make installation easier. https://chocolatey.org/packages/bazel. Please try it out and if there are any problems (it _is_ new), tweet or raise an issue over at bazelbuild/bazel. \n", "GPU support is out thanks to @mrry and @guschmue.\nInstructions will be updated on [readme](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md).\n", "Has anyone been able to compile on Windows and create a PIP package? Thank you. :)\n", "master (66f979714c9a8582059f383ec1505d13c9e8c523) should build ok for windows cpu and gpu. Instructions are in tensorflow/contrib/cmake/README.md (use cmake 3.6 for now). \n", "Tensorboard is not included?, because it not works:\n\n```\nC:\\Users\\Cesar\\Documents\\Notebooks\\DeepLearning>tensorboard --logdir='./my_graph/'\nTraceback (most recent call last):\n  File \"c:\\program files\\anaconda3\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\program files\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Program Files\\Anaconda3\\Scripts\\tensorboard.exe\\__main__.py\", line 5, in <module>\nImportError: No module named 'tensorflow.tensorboard'\n```\n", "Some python modules are missing from the pip: examples, some of contrib and tensorboard, doesn't get copied into the wheel when building with cmake.\nLet me check what it is just a matter of adding it with an option to tf_python.cmake or if there are complications.\n", "I compiled tensorflow for python as per the instructions, but I get a crash originating from `cudnn` when using `tf.nn.conv2d` on my GPU.\n\nSample code that causes the crash is\n\n``` python\nimport tensorflow as tf\nimport numpy as np\nsess = tf.InteractiveSession()\nimage = tf.placeholder(tf.float32, shape=[None, 10, 10, 1], name='image')\nkernel = tf.truncated_normal([5, 5, 1, 1], stddev=0.1)\nconv = tf.nn.conv2d(image, kernel, strides=[1, 1, 1, 1], padding='SAME')\nsess.run(tf.initialize_all_variables())\nsess.run(conv, feed_dict={image: [np.zeros((10, 10, 1))]})\n```\n\nThis causes the python process to crash and gives this output in the jupyter console\n\n```\nWARNING:root:kernel 800a0f42-d62b-4128-a61a-ba64ee725ca8 restarted\nI d:\\tensorflow\\tensorflow\\tensorflow\\stream_executor\\dso_loader.cc:125] successfully opened CUDA library cublas64_80.dll locally\nI d:\\tensorflow\\tensorflow\\tensorflow\\stream_executor\\dso_loader.cc:125] successfully opened CUDA library cudnn64_5.dll locally\nI d:\\tensorflow\\tensorflow\\tensorflow\\stream_executor\\dso_loader.cc:125] successfully opened CUDA library cufft64_80.dll locally\nI d:\\tensorflow\\tensorflow\\tensorflow\\stream_executor\\dso_loader.cc:125] successfully opened CUDA library nvcuda.dll locally\nI d:\\tensorflow\\tensorflow\\tensorflow\\stream_executor\\dso_loader.cc:125] successfully opened CUDA library curand64_80.dll locally\nI d:\\tensorflow\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:944] Found device 0 with properties:\nname: GeForce GTX 970\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.3165\npciBusID 0000:01:00.0\nTotal memory: 4.00GiB\nFree memory: 3.31GiB\nI d:\\tensorflow\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:965] DMA: 0\nI d:\\tensorflow\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:975] 0:   Y\nI d:\\tensorflow\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1034] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0)\nE d:\\tensorflow\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:662] Could not identify NUMA node of /job:localhost/replica:0/task:0/gpu:0, defaulting to 0.  Your kernel may not have been built with NUMA support.\nE d:\\tensorflow\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\nE d:\\tensorflow\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:392] error retrieving driver version: Permission denied: could not open driver version path for reading: /proc/driver/nvidia/version\nE d:\\tensorflow\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\nF d:\\tensorflow\\tensorflow\\tensorflow\\core\\kernels\\conv_ops.cc:532] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\n```\n\nWhile messing around with the trying different versions of `cudnn64_5.dll` I was able to get the code to work once and confirmed that the GPU was being used, but even after trying all the versions of `cudnn`  that I downloaded I have had no luck making it work a second time.\n", "tried your script and it works for me. We tested only with cudnn5.1, everything older I don't know.\nI'd get a clean install of cudnn5.1, build with that and make sure the cudnn64_5.dll in PATH is 5.1.\nAnother thing to check: your free memory is a little lower than I'd expect - is there maybe another python running that has the device open?\n", "I tried a clean rebuild with a fresh cudnn downloaded and on the PATH (`cudnn-8.0-windows10-x64-v5.1`) but no cigar. `cudnn64_5.dll` from this download is 'File version': `6.14.11.8000` and has 'File description': `NVIDIA CUDA 80.0.29 CUDNN Library` .\n\n`cublas64_80.dll` from the CUDA 8.0 path has the same 'File version' but 'File description': `NVIDIA CUDA 80.0.45 BLAS Library`\n\n`cufft64_80.dll` and `curand64_80.dll` also have matching 'File versions' but are `8.0.44` in 'File description'\n\nThe GPU drivers I have installed are version `369.30` from latest CUDA drivers and `nvcuda.dll` in System32 has 'File version': `6.14.13.6930` and has 'File description': `NVIDIA CUDA 80.0.44 driver` .\n\n## \n\nI believe the low device memory is just because I have a 970 which only has 3.5GB of useful memory and CUDA doesn't like using the last 0.5GB.\n", "I have the same cudnn version and its works fine for me. There are a bunch of conv python tests in the tf tree and they are all passing. I mostly use a 970 for testing. Your log was specific that it had issues initializing cudnn. Let me take a look at the init code.\n", "OK the issue appears to be a driver one. I can successfully use cudnn and convolutions once each time I restart the video driver (using device manager disable then enable). Once I have used it in one process however, any future uses fail to initialise.\n\nFurthermore I think other processes are having driver-related crashes once cudnn has been initialised.\n\n---\n\nUpdate: I've uninstalled and reinstalled my graphics driver and now I can have one process (jupyter kernel) with cudnn work correctly, but until I close that kernel every attempt to use cudnn in other kernels will encounter the same crash. If I restart the working kernel I can run the code in a different kernel without having to reload the driver.\n", "It compiled for me, but I had to add shell=True in the line 150 of the file gen_git_source.py. Otherwise, I would get the same error as showed in:\nhttp://stackoverflow.com/questions/24306205/file-not-found-error-when-launching-a-subprocess\n", "@synap5e if you run this in a normal tf.Session() a second instance will not work because tf manages all memory on the gpu. You can change the config for Session() like this:\nconfig = tf.ConfigProto()\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.4\nwith tf.Session(config=config) as sess:\n   ...\nwith that I can run 2 scripts in a loop that hit cudnn. This is the same on linux.\nBut it crashes is different on windows - on linux it gets CUDA_ERROR_OUT_OF_MEMORY, on windows it get CUDNN_STATUS_NOT_INITIALIZED. Going to take a look at this.\n", "Ah so the problem was that my system was using too much video memory even before initialising tensorflow. I guess the driver stuff was just bringing the used device memory down so that tensorflow could initialise properly. Thanks!\n", "Guys, is it possible to build on Windows without MSVS? Windows 10 SDK is supposed to contain the C++ compiler, wouldn't that suffice for the task?\n", "@zandaqo You only need [standalone MSVC compiler with Windows 10 SDK](https://landinghub.visualstudio.com/visual-cpp-build-tools) and CMake, so there is no need to install the whole Visual Studio. Windows 10 SDK contains dll, static libraries, header files and source codes for the libraries and some useful tools, but it does not contain compiler.\n", "Hello!\n\nI'm new to both Python and Tensorflow, but I have done some MNIST stuff in the past. I followed the instructions on tensorflow/contrib/cmake/README.md, and it worked fine, GPU seems to be working fine as well. Tensorflow on Windows seems to be breaking in some places, as expected, and I just want to check whether my understanding of the breakage is right.\n\nI tried to follow https://www.tensorflow.org/versions/r0.11/tutorials/mnist/beginners/index.html and it doesn't work, since this import path (tensorflow.examples.tutorials.mnist) doesnt exist. This is ultimately due to `tf.contrib` not being included in the pip package, right?\n\nJust out of curiosity, I did something apparently pretty awful: I copied the contents of the tensorflow repo over the lib location at site-packages\\tensorflow, which I expected to cause a lot of stuff to break. This is how it breaks:\n\n```\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\framework\\python\\ops\\variables.py\", line 23, in <module>\n    from tensorflow.contrib.framework.python.ops import gen_variable_ops\nImportError: cannot import name 'gen_variable_ops'\n```\n\nI couldn't find the definition of gen_variable_ops anywhere, but the documentation says that `tf.load_op_library()` is not currently implemented. So, `gen_variable_ops` is implemented somewhere else, which needs to be loaded using `tf.load_op_library()`. So, the lack of tf.load_op_library is what is causing the problem, right?\n", "I've managed to build successfully the pip package following the cmake path, and installed it with `pip install tensorflow-0.11.0rc1_cmake_experimental-py3-none-any.whl` which reported a successful installation. However, when I try to import tensorflow, I get the following errors:\n\n```\n>>> import tensorflow as tf\nTraceback (most recent call last):\n  File \"C:\\Users\\Tester\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\n    return importlib.import_module(mname)\n  File \"C:\\Users\\Tester\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\nImportError: DLL load failed: The specified module could not be found.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\Tester\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"C:\\Users\\Tester\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 21, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"C:\\Users\\Tester\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\n    return importlib.import_module('_pywrap_tensorflow')\n  File \"C:\\Users\\Tester\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\nImportError: No module named '_pywrap_tensorflow'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"C:\\Users\\Tester\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"C:\\Users\\Tester\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 60, in <module>\n    raise ImportError(msg)\nImportError: Traceback (most recent call last):\n  File \"C:\\Users\\Tester\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\n    return importlib.import_module(mname)\n  File \"C:\\Users\\Tester\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\nImportError: DLL load failed: The specified module could not be found.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\Tester\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"C:\\Users\\Tester\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 21, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"C:\\Users\\Tester\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\n    return importlib.import_module('_pywrap_tensorflow')\n  File \"C:\\Users\\Tester\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\nImportError: No module named '_pywrap_tensorflow'\n\n\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there.\n```\n\nDid I miss some dependency here or something was wrong with the build/installation?\n", "@zandaqo, I got this problem when I first run tensorflow on Windows. You just need to compile the examples, and add the folder with the compiled examples to your path. When you compile the sample codes, it will also compile some DLLs needed for Tensorflow.\n", "@ErivaldoJunior By compiling the examples do you mean this: `MSBuild /p:Configuration=Release tf_tutorials_example_trainer.vcxproj`? I did that before compiling the pip package. It compiles to `.\\Release`, is that the folder to add to path? I have just tried adding it, but no luck so far.\n\nI do see files named `pywrap_tensor` mentioned in the errors in that `.\\Release` folder, maybe manually copying them somewhere might help Python to see them. Not sure where though.\n", "@zandaqo, that is right. I just add that Release folder to my Windows path and it worked. However, I tested by launching python my_tensorflow_script.py from a CMD window. I don't know if it work if you use some kind of IDE or iPython.\n", "@ErivaldoJunior Thanks, you were right about missing DLLs, though in my case it turned out to be `zlib.dll` that was missing from both the package and `.\\Release` folder. I've manually copied `zlib.dll` into the folder, as it was suggested [here](https://github.com/tensorflow/tensorflow/issues/5275#issuecomment-258006589) and the import now works.\n", "I successed to build the tensorflow! But `cmake 3.3.1` causes build error, `3.6.3` works fine. \n", "Good to know! We'd welcome a PR with an appropriate minimum version\nstatement in the cmakelist file.\n\nOn Saturday, November 5, 2016, Takahiro Kubo notifications@github.com\nwrote:\n\n> I successed to build the tensorflow! But cmake 3.3.1 causes build error,\n> 3.6.3 works fine.\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/17#issuecomment-258649999,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAjO_Ys4MI03pcc3tCR_BzsqIXFrQTTcks5q7RVRgaJpZM4GexU4\n> .\n", "@martinwicke as in [here](https://github.com/tensorflow/tensorflow/pull/5071#issuecomment-255490169) it seems the minimum known working Cmake version is 3.5. Should I change [minimum requirement](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/CMakeLists.txt#L2) to 3.5 and update the readme for minimum 3.5 up to 3.6?\n", "Nice to see that TensorFlow can now work on Windows with GPU now. Are there plans for a version that need not be installed from sources? And if so, any ETA on that? It would be great if the setup process were as straightforward as on Linux, I'm using TensorFlow on Linux without problems and would like to do so on Windows, but the requisites and setup process documented in the readme currently looks a bit cumbersome for those of us that don't have much time...\n", "Hey guys! With the release of [Bazel 0.4.0](https://github.com/bazelbuild/bazel/releases), TensorFlow PIP package can now build with Bazel on Windows.\r\nThanks to @petemounce's contribution, you can use `choco install bazel --version 0.4.0` to install Bazel easily.\r\n\r\n@gunan has already set up a ci job for it: http://ci.tensorflow.org/job/tensorflow-pr-win-bazel/\r\nBasically, we are using [this script](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/windows/cpu/pip/build_tf_windows.sh). So with proper modification to the environment variables (if your installations of Visual Studio, Python 3 or msys is not at the standard places), you'll be able to build the PIP package with Bazel as well!\r\n\r\nUpdate: I would recommend to run the commands in the script manually if you have trouble running it directly.\r\n", "@meteorcloudy Thanks for update!\nBuild with bazel did not work for me. I get ERROR: CreateFile(C:\\tmp\\Bazel\\44EoGNou\\install)\nAny way to debug that? I have localized windows (russian), maybe that's the cause\n\nFull log. Running in PowerShell as admin:\n\n```\nC:\\libs\\tensorflow>c:\\tools\\msys64\\usr\\bin\\bash -l C:\\libs\\tensorflow/tensorflow/tools/ci_build/windows/cpu/pip/build_tf_windows.sh\n+ set -e\n++ dirname 'C:\\libs\\tensorflow/tensorflow/tools/ci_build/windows/cpu/pip/build_tf_windows.sh'\n+ script_dir='C:\\libs\\tensorflow/tensorflow/tools/ci_build/windows/cpu/pip'\n+ cd 'C:\\libs\\tensorflow/'\n+ export TMPDIR=C:/tmp\n+ TMPDIR=C:/tmp\n+ export BAZEL_SH=C:/tools/msys64/usr/bin/bash\n+ BAZEL_SH=C:/tools/msys64/usr/bin/bash\n+ export 'PYTHON_BIN_PATH=C:\\tools\\Anaconda3/python'\n+ PYTHON_BIN_PATH='C:\\tools\\Anaconda3/python'\n+ export 'BAZEL_PYTHON=C:\\tools\\Anaconda3/python'\n+ BAZEL_PYTHON='C:\\tools\\Anaconda3/python'\n+ export 'BAZEL_VS=C:/Program Files (x86)/Microsoft Visual Studio 14.0'\n+ BAZEL_VS='C:/Program Files (x86)/Microsoft Visual Studio 14.0'\n+ export 'PATH=/c/tools/bazel:/c/Program Files/Anaconda3:/usr/local/bin:/usr/bin:/bin:/opt/bin:/c/Windows/System32:/c/Windows:/c/Windows/System32/Wbem:/c/Windows/System32/WindowsPowerShell/v1.0/'\n+ PATH='/c/tools/bazel:/c/Program Files/Anaconda3:/usr/local/bin:/usr/bin:/bin:/opt/bin:/c/Windows/System32:/c/Windows:/c/Windows/System32/Wbem:/c/Windows/System32/WindowsPowerShell/v1.0/'\n+ bazel clean\nExtracting Bazel installation...\nERROR: CreateFile(C:\\tmp\\Bazel\\44EoGNou\\install): \u2592\u2592 \u2592\u2592\u2592\u2592\u2592\u2592\u2592 \u2592\u2592\u2592\u2592\u2592 \u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592 \u2592\u2592\u2592\u2592.\n (2)\n.....................................................................................................................................................................................................................................................................................................................................\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\n++ bazel info output_base\n+ output_base=C:/tmp/Bazel/44EoGNou\n+ bazel shutdown\n+ rm -rf C:/tmp/Bazel/44EoGNou\nrm: \u043d\u0435\u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e \u0443\u0434\u0430\u043b\u0438\u0442\u044c 'C:/tmp/Bazel/44EoGNou/server/jvm.out': Device or resource busy\n```\n", "@KhabarlakKonstantin That's (`ERROR: CreateFile(C:\\tmp\\Bazel\\44EoGNou\\install)`) bazelbuild/bazel#1744 I think. It only happens the first time after bazel is installed. There don't seem to be any repercussions.\n", "@KhabarlakKonstantin As @petemounce pointed out, you can safely ignore that error. And you can also skip `rm -rf C:/tmp/Bazel/44EoGNou` if the `Device or resource busy` error is blocking you. I guess it's because it tried to remove `jvm.out` before jvm fully shutdown after invoking `bazel shutdown`.\n", "@petemounce @meteorcloudy Thanks for help\nThe problem is that it doesn't start building. I've commented out `bazel shutdown` and it goes now a little bit furhter, but still crashes\n\n```\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\n++ bazel info output_base\n+ output_base=C:/tmp/Bazel/44EoGNou\n+ echo ''\n+ ./configure\n/c/libs/tensorflow /c/libs/tensorflow\nFound possible Python library paths:\n  C:\\Program Files\\Anaconda3\\lib\\site-packages\n  C:\\Program Files\\Anaconda3\nPlease input the desired Python library path to use.  Default is [C:\\Program Files\\Anaconda3\\lib\\site-packages]\nUsing python library path: C:\\Program Files\\Anaconda3\\lib\\site-packages\nJunction created for util\\python\\python_include <<===>> C:\\Program Files\\Anaconda3\\include\nJunction created for util\\python\\python_lib <<===>> C:\\Program Files\\Anaconda3\\lib\\site-packages\nJunction created for third_party\\py\\numpy\\numpy_include <<===>> C:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\core\\include\n```\n\nNo more output here\n", "@KhabarlakKonstantin It seems OpenCL configuration is just added into `./configure` but not turned off by default on Windows, so please manually run that script or add `export TF_NEED_OPENCL=0` before `echo \"\" | ./configure`\n\n```\npcloudy@PCLOUDY1-W MSYS ~/workspace/tensorflow\n$ ./configure\n~/workspace/tensorflow ~/workspace/tensorflow\nPlease specify the location of python. [Default is /c/Program Files/Anaconda3/python]:\nFound possible Python library paths:\n  C:\\Program Files\\Anaconda3\\lib\\site-packages\n  C:\\Program Files\\Anaconda3\nPlease input the desired Python library path to use.  Default is [C:\\Program Files\\Anaconda3\\lib\\site-packages]\n\nUsing python library path: C:\\Program Files\\Anaconda3\\lib\\site-packages\nJunction created for util\\python\\python_include <<===>> C:\\Program Files\\Anaconda3\\include\nJunction created for util\\python\\python_lib <<===>> C:\\Program Files\\Anaconda3\\lib\\site-packages\nJunction created for third_party\\py\\numpy\\numpy_include <<===>> C:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\core\\include\nDo you wish to build TensorFlow with OpenCL support? [y/N] n\nNo OpenCL support will be enabled for TensorFlow\nConfiguration finished\n```\n", "Is anybody else having [these errors](https://github.com/Carmezim/tensorflow/blob/master/tensorflow/contrib/cmake/errors) with Pragma (ignore pywrap one)?\n\nBuilding with GPU and all specs in accord to readme.\n", "@Carmezim, it worked fine for me. I did not get any of these errors.\n", "I have a pr to fix this here: https://github.com/tensorflow/tensorflow/pull/5421\n", "@guschmue Nice! Thanks for the quick response\n", "After fixing _Pragma, this is what I get in the same file:\n\n```\nC:/tensorflow/tensorflow/core/kernels/parameterized_truncated_normal_op_gpu.cu.cc(168): error : more than\none operator \"/\" matches these operands: [C:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.v\ncxproj]\n```\n", "#5421 will fix this as well.\n", "Can I build with GPU support using bazel?\n", "@edwin100394 no GPU support on windows using bazel yet.\nWe are still working on it.\n\nOn Mon, Nov 7, 2016 at 9:33 PM, edwin100394 notifications@github.com\nwrote:\n\n> Can I build with GPU support using bazel?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/17#issuecomment-259049695,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AHlCOXNRNoMFmnSPfnkTMLsRHcIcl9TXks5q8AmigaJpZM4GexU4\n> .\n", "@meteorcloudy Adding `export TF_NEED_OPENCL=0` helped me to go a little further\n\nNow I'm getting another error:\n\n```\nERROR: C:/libs/tensorflow/tensorflow/core/BUILD:1115:1: Executing genrule //tensorflow/core:version_info_gen failed: bash failed: error executing command\n  cd C:/tmp/Bazel/44EoGNou/execroot/tensorflow\n  SET PATH=C:\\tools;C:\\tools\\Anaconda;C:\\tools\\msys64\\usr\\local\\bin;C:\\tools\\msys64\\usr\\bin;C:\\tools\\msys64\\usr\\bin;C:\\tools\\msys64\\opt\\bin;C:\\Windows\\System32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\tools\\msys64\\usr\\bin\\site_perl;C:\\tools\\msys64\\usr\\bin\\vendor_perl;C:\\tools\\msys64\\usr\\bin\\core_perl\n  C:/tools/msys64/usr/bin/bash -c source external/bazel_tools/tools/genrule/genrule-setup.sh; tensorflow/tools/git/gen_git_source.py --generate tensorflow/tools/git/gen/spec.json tensorflow/tools/git/gen/head tensorflow/tools/git/gen/branch_ref \"bazel-out/vc_14_0_x64-py3-opt/genfiles/tensorflow/core/util/version_info.cc\": com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1\nTraceback (most recent call last):\n  File \"tensorflow/tools/git/gen_git_source.py\", line 260, in <module>\n    generate(args.generate)\n  File \"tensorflow/tools/git/gen_git_source.py\", line 212, in generate\n    git_version = get_git_version(data[\"path\"])\n  File \"tensorflow/tools/git/gen_git_source.py\", line 152, in get_git_version\n    str(\"--work-tree=\" + git_base_path), \"describe\", \"--long\", \"--dirty\", \"--tags\"\n  File \"C:\\tools\\Anaconda\\lib\\subprocess.py\", line 626, in check_output\n    **kwargs).stdout\n  File \"C:\\tools\\Anaconda\\lib\\subprocess.py\", line 693, in run\n    with Popen(*popenargs, **kwargs) as process:\n  File \"C:\\tools\\Anaconda\\lib\\subprocess.py\", line 947, in __init__\n    restore_signals, start_new_session)\n  File \"C:\\tools\\Anaconda\\lib\\subprocess.py\", line 1224, in _execute_child\n    startupinfo)\nFileNotFoundError: [WinError 2] The system cannot find the file specified\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nINFO: Elapsed time: 10.997s, Critical Path: 3.44s\nFAILED: Build did NOT complete successfully\n```\n", "I've started from scratch and I've build bazel from sources. Now it building almost till the end\n\nIt fails at link time in the end:\n\n```\nERROR: C:/libs/tensorflow/tensorflow/python/BUILD:1907:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow.so' failed: msvc_link.bat failed: error executing command\n  cd C:/tmp/Bazel/44EoGNou/execroot/tensorflow\n  SET PATH=external/local_config_cc/wrapper/bin\n  external/local_config_cc/wrapper/bin/msvc_link.bat /DLL /WHOLEARCHIVE -pthread -m64 -Xcompilation-mode=opt -Wl,@bazel-out/vc_14_0_x64-py3-opt/bin/tensorflow/python/_pywrap_tensorflow.so-2.params: com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1169\nWarning: Unmatched arguments: -ldl -lm -ldl -lm\nWarning: Unmatched arguments: -pthread\nlibversion_lib.a(version_info.o) : error LNK2005: \"char const * __cdecl tf_compiler_version(void)\" (?tf_compiler_version@@YAPEBDXZ) already defined in libframework_internal.lo(version_info.o)\nlibversion_lib.a(version_info.o) : error LNK2005: \"char const * __cdecl tf_git_version(void)\" (?tf_git_version@@YAPEBDXZ) already defined in libframework_internal.lo(version_info.o)\n   Creating library bazel-out/vc_14_0_x64-py3-opt/bin/tensorflow/python/_pywrap_tensorflow.lib and object bazel-out/vc_14_0_x64-py3-opt/bin/tensorflow/python/_pywrap_tensorflow.exp\nbazel-out/vc_14_0_x64-py3-opt/bin/tensorflow/python/_pywrap_tensorflow.so : fatal error LNK1169: one or more multiply defined symbols found\nERROR: C:/libs/tensorflow/tensorflow/python/BUILD:639:1: output 'tensorflow/python/gen_control_flow_ops_py_wrappers_cc.exe' was not created\nERROR: C:/libs/tensorflow/tensorflow/python/BUILD:698:1: output 'tensorflow/python/gen_resource_variable_ops_py_wrappers_cc.exe' was not created\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\n\n```\n", "Hi folks, now that rudimentary Windows support is in place, I am going to lock this issue for new comments, but keep it open until we have a stable release. Going forward:\n- If you have an issue, please open a new issue against TensorFlow to describe your problem, and fill out the standard issue template. This will help us to track and assign the remaining work that we have to do to get TensorFlow-on-Windows released.\n- We will continue to use this issue for (rare) announcements about Windows support, so you can continue to subscribe to the issue for a low-traffic feed about improvements to TensorFlow-on-Windows.\n\nOnce again, thanks for all your interest in TensorFlow-on-Windows, and now we're going to get back to work!\n", "Today we [announced native Windows support in TensorFlow 0.12](https://developers.googleblog.com/2016/11/tensorflow-0-12-adds-support-for-windows.html), with packages for Python 3.5. If you have installed the 64-bit version of Python 3.5 (either from Python.org or Anaconda), you can install TensorFlow with a single command:\r\n\r\n```\r\nC:\\> pip install tensorflow\r\n```\r\n\r\nFor GPU support, if you have CUDA 8.0 installed, you can install the following package instead:\r\n\r\n```\r\nC:\\> pip install tensorflow-gpu\r\n```\r\n\r\nNow that Windows is a supported configuration, I'm going to close this issue; please open a new issue if you have problems with these packages. There are still some discrepancies between the Windows package and the other platforms, which are outlined in [the release notes](https://github.com/tensorflow/tensorflow/blob/r0.12/RELEASE.md#release-0120). We'll be working over the next few weeks to close these gaps.\r\n\r\nThanks for all of your interest in TensorFlow on Windows!"]}, {"number": 16, "title": "iOS Support and Example", "body": "Currently there is [an example on Android](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android) but no corresponding IOS example even though the white paper calls out both Android and IOS.\n", "comments": ["Sorry about that Alex, we'll update the docs. We are actively working on iOS support, though I can't give a timeline I'm afraid.\n", "@petewarden will you also be open sourcing the development of the iOS integration? i.e. will developers be able to contribute to the development?\n", "Yes, we do plan on making it available as open source, and we'll welcome contributions!\n", "awesome look forward to the day its open sourced!\n", "@woffle Meantime there is an initial [Tensorflow javacpp-preset](https://github.com/bytedeco/javacpp-presets/blob/master/tensorflow/README.md). Some of this presets generally can run on IOS through RoboVM. Follow also https://github.com/bytedeco/javacpp-presets/issues/111\n", "Reopening as a tracking bug.\n", "Just a comment on the progress. We were blocked on a build issue for quite a while, but with the latest release of Bazel (0.2.2) we have linkopts which we hope should let us get https://github.com/tensorflow/tensorflow/pull/1631 integrated soon.\n", "@petewarden Any further update on this? \n", "I'm working with the protobuf team to get https://github.com/google/protobuf/pull/1500 in, so we can update protobuf in TensorFlow and get iOS support checked in.\n", "May I know, When will the tensor flow for iOS will be out? Any reference example that will help get started. Thanks\n", "The documentation is still in progress, and we're still testing it, but you can build an iOS version of the library by using the new makefile system:\n\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile\n\nAfter that's built, the tensorflow/contrib/ios_examples folder has a couple of sample applications.\n", "@petewarden I may sound dumb, can you give me an example use case of having the machine learning on an ios client, please. How does the neuralnet learn when it's on an iOS client? Will the training data be only users' data i.e their photos, playlists etc. Does the neural net sit in the background and train itself with user's data like a cron? e.g it can understand user's preferences. Am I right in saying that this app won't have any global knowledge like on taking a picture of a vegetable it will classify it as brocolli without connecting to a pre-trained machine learning api hosted remotely?\n", "The example demo for iOS is very very slow in comparison with the Android version. Is there anything I can change within the demo code to improve the speed? \n\nAlso the match percentages vary wildly. On Android it with report accuracy levels about 70% sometimes but I can't get it above about 15% on iOS. I'm using the same files for both.\n", "@karthiksekarnz I understand that the architecture for a photo recognition app on mobile client is that a pre-trained model is bundled with the app and downloaded by the user from an app store. The app then runs an inference on the neural network (i.e. graph execution) only. There is no learning capability as yet in the mobile app.\n\nI suspect that this approach is taken because a mobile phone doesn't have enough computing power to do learning efficiently, given that lots of models are built with multi-GPUs. But that remains to be seen because the TensorFlow public C++ API doesn't yet support the wide range of network training capabilities that the Python API does.\n\nIt's also probably true that to transfer images from the camera's video stream to a server in real-time would be a bandwidth hog. If the user was on a cellular network, it may also be expensive, depending on their bundle.\n\nIt's an interesting architecture problem though. Lots of interesting data that could be modelled will be captured by phones, but the learning has to be server based at the moment.\n", "@StephenOman that pretty much answers my question\nI came across this PR after exploring about Google's Vision API and machine learning API.\nI am looking for a client based approach to do object classification, precisely food items. I was thinking about the bandwidth issue as well, i don't want to go over the wire to just find out the scanned picture is broccoli.\n\nI definitely don't want to be that app, that eats all the device memory to just learn data, didn't know I can bundle the neural network with data model.\n\nNow I need to find out about how to train the neural net to classify food items and how to bundle the pre-trained data model with the app, thank you so much :)\n", "@petewarden Hey, I tried to run the sample iOS examples, in tensflow directory, but I am getting \n    #include \"eigen-eigen-d02e6a705c30/unsupported/Eigen/CXX11/Tensor\"\nand\nalso libprotobuf-lite.a and libprotobuf.a files are missing.\n\nAny help would be greatly apprciated, Thanks \n", "@sahil912 That sounds like you don't have things built correctly. Can you try running the tensorflow/contrib/makefile/build_all_ios.sh script?\n", "@karthiksekarnz You can check out the TensorFlow for Poets codelab that should help you train your model:\n\nhttps://codelabs.developers.google.com/codelabs/tensorflow-for-poets/index.html?index=..%2F..%2Findex#0\n", "Hi, does this support GPU computation on iOS? \n", "I was running the simple example on iphone6, and it took about 4 second to classify the example image, is this expected?\n", "It's not yet documented, but you can pass in \"-Os\" to the build_all_ios.sh script to compile an optimized version that should run in under a second.\n", "Since we now have build support in tensorflow/contrib/makefile, and examples in tensorflow/contrib/ios_examples, I'm closing this bug.\n", "Hi there, Thanks for the information on building an optimised version. Just curious but why isn't this the default for the sample iOS projects? When I've shown people the iOS demo they've all remarked on the slowness vs Android. Finally to build the optimised version do I just run:\n\n`> ./build_all_ios.sh -Os`\n\nThis made no difference in my testing,\nThanks in advance.\n", "@randomiser The build_all_ios.sh script currently ignores the command line parameters. So instead, you need to run the compile_ios_tensorflow.sh script:\n\n```\ntensorflow/contrib/makefile/compile_ios_tensorflow.sh \"-Os\"\n```\n\nNote that this gcc flag optimises the library for size, making the app footprint smaller (see #2716). For speed optimisation, gcc accepts an -O3 flag. So this command _should_ build a faster execution library, at the cost of a bigger memory footprint:\n\n```\ntensorflow/contrib/makefile/compile_ios_tensorflow.sh \"-O3\"\n```\n\nAlthough I haven't actually tested that assertion.\n", "Thanks Stephen for those extra details. Will try it now and see how I get on.\n", "Based on this discussion I actually updated the build_all_ios.sh script to use \"-Os\" by default, and to build in parallel to lower overall compile times:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/build_all_ios.sh#L32\n", "Hi \r\nIs there any update on tensorflow/core/framework/types.pb.h not found on iOS compile?", "https://webappcodes.com/category/ios\r\n\r\nHere, above link you find all ios example codes"]}, {"number": 15, "title": "Quantized ops?", "body": "In the data types section, there are `quantized ops`\nhttp://tensorflow.org/resources/dims_types.md#data_types\n\nFound a note here:\nhttps://github.com/tensorflow/tensorflow/blob/d6357a5849db980df51d00d8a9ff874cda2faeb3/tensorflow/cc/ops/const_op.h#L65\n\nCan you explain what these are?\n", "comments": ["We are working on support for operators and kernels that take quantized (fixed-point) data -- we're still working on the right APIs to expose them, and with that we'll have more documentation on why and how they are used.  Stay tuned!\n", "Awesome, this is great work :open_mouth:  \nPlease give my sincere thanks to :santa:\n", "Closing for now. When the quantized ops come online, they'll come with documentation.\n", "@vrv Any news on fixed-point / quantized data type support?\nI am designing an FPGA-based accelerator for CNNs as my master thesis in the next few months. It would be awesome to be able to train the models with fixed-point weights and activations.\nDo you have any roadmap for fixed-point support?\n", "@dgschwend have you find the answer?", "@vrv could you please reopen, point to the correct doc, or confirm that is still should be considered internal (though it is exposed in many place by now)"]}, {"number": 14, "title": "g3doc format", "body": "Has the g3doc specification been publicly released?  Is an effort going to be made so that outside contributors know exactly how comments should be written in order to be correctly parsed?\n", "comments": ["The g3doc used in Tensorflow is, I believe, identical to [Markdown](https://daringfireball.net/projects/markdown/syntax).   If you find anything that doesn't meet the markdown specification, file an issue.\n", "I mean the comments in the C++ that are used to generate the markdown files.\n\nExample:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/public/env.h#L47\n\n```\n  /// \\brief Returns a default environment suitable for the current operating\n  /// system.\n  ///\n  /// Sophisticated users may wish to provide their own Env\n  /// implementation instead of relying on this default environment.\n  ///\n  /// The result of Default() belongs to this library and must never be deleted.\n  static Env* Default();\n```\n\nis used to generate\n\n```\n* [`static Env* tensorflow::Env::Default()`](#static_Env_tensorflow_Env_Default)\n  * Returns a default environment suitable for the current operating system.\n```\n\nand\n\n```\n#### `static Env* tensorflow::Env::Default()` {#static_Env_tensorflow_Env_Default}\n\nReturns a default environment suitable for the current operating system.\n\nSophisticated users may wish to provide their own Env implementation instead of relying on this default environment.\n\nThe result of Default() belongs to this library and must never be deleted.\n```\n\nMy point is that the docs were pre-generated. I can't find another project where Google has open sourced the g3doc utility so that others may rerun it and update the documentation.\n\nIf the answer is \"the syntax is just like doxygen\", then that's an acceptable answer, although it would still be nice to be given the same tool to generate the markdown.\n", "The syntax is, in fact, doxygen: part of our docs generation pipeline calls doxygen. We are working on making more of the doc generator publicly available, but it requires a decent amount of untangling.\n", "Is there a blog post or guide out for this yet?\r\n", "Check out the documentation authoring guide: https://www.tensorflow.org/community/documentation", "Is the engine now open source though? (If I didn't want to build it specifically as a part of tensorflow docs, rather use it for my own markdown rendering )?\r\n\r\nThis [video](https://www.usenix.org/sites/default/files/conference/protected-files/srecon16europe_slides_macnamara.pdf) (dated 2016) is really fascinating in concept. It would  be great if the whole system were fully elaborated upon.", "Everything from TensorFlow source code to markdown is open-source. It should be usable for other projects, although the code may have aspects specific to TensorFlow which may not trivial to extract. \r\n\r\nTensorFlow docs are not served using g3docs. I cannot speak to the specifics of that presentation. "]}, {"number": 13, "title": "[doc] typo", "body": "Its JavaScript, google\n", "comments": []}, {"number": 12, "title": "Remote worker configuration", "body": "Does this release support remote workers out of the box?  I couldn't find details in the tutorials, but the whitepaper and `tensorflow/core/common_runtime/device.h` talked about adding remote devices through RPC.  Is a parameter server or alternative already implemented in here somewhere?  If remote workers are supported out of the box, what is the execution model?  Is there a daemon or do you have to configure a session in a certain way to act as one?\n", "comments": ["This release does not support distributed computation.\n", "Hi kmatzen: As mentioned in our FAQ, this release does not have an RPC and distributed implementation available yet: http://tensorflow.org/resources/faq.md#running_a_tensorflow_computation, but we are working on it and hope to make it available when it's ready.\n\nAs a short-term proxy, you can see the Cifar tutorial for an example of how training and distributing among multiple GPUs works -- http://tensorflow.org/tutorials/deep_cnn/index.md\n", "(De-duping with https://github.com/tensorflow/tensorflow/issues/23)\n"]}, {"number": 11, "title": "0.5.0 wheel install on Mac OS X using Homebrew python broken", "body": "This is what happens:\n\n``` bash\n~: pip install https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl\nCollecting tensorflow==0.5.0 from https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl\n  Downloading https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl (9.8MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.8MB 51kB/s\nCollecting six>=1.10.0 (from tensorflow==0.5.0)\n  Downloading six-1.10.0-py2.py3-none-any.whl\nRequirement already satisfied (use --upgrade to upgrade): numpy>=1.9.2 in /usr/local/lib/python2.7/site-packages (from tensorflow==0.5.0)\nInstalling collected packages: six, tensorflow\n  Found existing installation: six 1.9.0\n    Uninstalling six-1.9.0:\n      Successfully uninstalled six-1.9.0\nSuccessfully installed six-1.10.0 tensorflow-0.5.0\n~: python\nPython 2.7.10 (default, Jul 13 2015, 12:05:58)\n[GCC 4.2.1 Compatible Apple LLVM 6.1.0 (clang-602.0.53)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import tensorflow as tf\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/__init__.py\", line 4, in <module>\n    from tensorflow.python import *\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 13, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py\", line 16, in <module>\n    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/attr_value_pb2.py\", line 16, in <module>\n    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/tensor_pb2.py\", line 16, in <module>\n    from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/tensor_shape_pb2.py\", line 22, in <module>\n    serialized_pb=_b('\\n,tensorflow/core/framework/tensor_shape.proto\\x12\\ntensorflow\\\"d\\n\\x10TensorShapeProto\\x12-\\n\\x03\\x64im\\x18\\x02 \\x03(\\x0b\\x32 .tensorflow.TensorShapeProto.Dim\\x1a!\\n\\x03\\x44im\\x12\\x0c\\n\\x04size\\x18\\x01 \\x01(\\x03\\x12\\x0c\\n\\x04name\\x18\\x02 \\x01(\\tb\\x06proto3')\nTypeError: __init__() got an unexpected keyword argument 'syntax'\n```\n", "comments": ["Okay, I was able to reproduce this in virtualenv by installing protobuf==2.6.1\n\nThe short answer is that we depend on protobuf 3.0.0, and having the protobuf pip library installed seems to interfere with ours.\n\nTwo solutions:\n\n1) pip install protobuf=3.0.0a1 or higher, then pip install tensorflow\n2) pip uninstall protobuf first, and then install tensorflow again -- it should bring in the dependency\n\nI was able to do both in virtualenv and they worked -- let me know if either suffices for you.  We might add protobuf >= 3 to our whl dependencies if so.\n", "Tensorflow does not bring dependency on protobuf:\n\n``` sh\ngp@MacBook-Pro-Gregory ~> pip list\nmercurial (3.5.2)\npip (7.1.2)\nsetuptools (18.5)\nvboxapi (1.0)\nwheel (0.26.0)\n\ngp@MacBook-Pro-Gregory ~> pip install --no-cache-dir https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl\nCollecting tensorflow==0.5.0 from https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl\n  Downloading https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl (9.8MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.8MB 13.7MB/s\nCollecting six>=1.10.0 (from tensorflow==0.5.0)\n  Downloading six-1.10.0-py2.py3-none-any.whl\nCollecting numpy>=1.9.2 (from tensorflow==0.5.0)\n  Downloading numpy-1.10.1-cp27-none-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (3.7MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.7MB 9.5MB/s\nInstalling collected packages: six, numpy, tensorflow\nSuccessfully installed numpy-1.10.1 six-1.10.0 tensorflow-0.5.0\n\ngp@MacBook-Pro-Gregory ~> pip list\nmercurial (3.5.2)\nnumpy (1.10.1)\npip (7.1.2)\nsetuptools (18.5)\nsix (1.10.0)\ntensorflow (0.5.0)\nvboxapi (1.0)\nwheel (0.26.0)\n```\n\nInstalling protobuf3 before does not help, either:\n\n``` sh\ngp@MacBook-Pro-Gregory ~> pip list\nmercurial (3.5.2)\npip (7.1.2)\nsetuptools (18.5)\nvboxapi (1.0)\nwheel (0.26.0)\n\ngp@MacBook-Pro-Gregory ~> pip install protobuf==3.0.0a1\nCollecting protobuf==3.0.0a1\nRequirement already satisfied (use --upgrade to upgrade): setuptools in /usr/local/lib/python2.7/site-packages (from protobuf==3.0.0a1)\nInstalling collected packages: protobuf\nSuccessfully installed protobuf-3.0.0a1\n\ngp@MacBook-Pro-Gregory ~> pip install --no-cache-dir https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl\nCollecting tensorflow==0.5.0 from https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl\n  Downloading https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl (9.8MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.8MB 65.8MB/s\nCollecting six>=1.10.0 (from tensorflow==0.5.0)\n  Downloading six-1.10.0-py2.py3-none-any.whl\nCollecting numpy>=1.9.2 (from tensorflow==0.5.0)\n  Downloading numpy-1.10.1-cp27-none-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (3.7MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.7MB 3.4MB/s\nInstalling collected packages: six, numpy, tensorflow\nSuccessfully installed numpy-1.10.1 six-1.10.0 tensorflow-0.5.0\n\ngp@MacBook-Pro-Gregory ~> python\nPython 2.7.10 (default, Nov  9 2015, 20:28:23)\n[GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.1.76)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import tensorflow as tf\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/__init__.py\", line 4, in <module>\n    from tensorflow.python import *\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 13, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py\", line 16, in <module>\n    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/attr_value_pb2.py\", line 16, in <module>\n    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/tensor_pb2.py\", line 16, in <module>\n    from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/tensor_shape_pb2.py\", line 22, in <module>\n    serialized_pb=_b('\\n,tensorflow/core/framework/tensor_shape.proto\\x12\\ntensorflow\\\"d\\n\\x10TensorShapeProto\\x12-\\n\\x03\\x64im\\x18\\x02 \\x03(\\x0b\\x32 .tensorflow.TensorShapeProto.Dim\\x1a!\\n\\x03\\x44im\\x12\\x0c\\n\\x04size\\x18\\x01 \\x01(\\x03\\x12\\x0c\\n\\x04name\\x18\\x02 \\x01(\\tb\\x06proto3')\nTypeError: __init__() got an unexpected keyword argument 'syntax'\n```\n\nUnless I am missing something, neither of the solutions work for me. Latest OS X, python2 + python3 installed from Homebrew.\n", "This fixed the problem for me:\n\n```\npip uninstall protobuf\npip uninstall tensorflow\nbrew uninstall protobuf\npip install https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl\n```\n", "flyingmutant: to appease my curiosity, can you let us know what you get when you do the following in python:\n\n``` python\nimport google.protobuf\n>>> print google.protobuf.__version__\n3.0.0a4\n```\n\n(it's possible google.protobuf.__version__ doesn't exist either in one of the environments I've tested).\n\nBasically the symptom of the problem is that python is finding the older version of the protobuf library that doesn't support the generated python proto3 we require.\n", "Found the same issue on my end.\n\n```\npip install https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl\nCollecting tensorflow==0.5.0 from https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl\n  Using cached https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl\nCollecting six>=1.10.0 (from tensorflow==0.5.0)\n  Using cached six-1.10.0-py2.py3-none-any.whl\nCollecting numpy>=1.9.2 (from tensorflow==0.5.0)\n  Using cached numpy-1.10.1-cp27-none-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\nInstalling collected packages: six, numpy, tensorflow\n  Found existing installation: six 1.4.1\n    DEPRECATION: Uninstalling a distutils installed project (six) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\n    Uninstalling six-1.4.1:\nException:\nTraceback (most recent call last):\n  File \"/Library/Python/2.7/site-packages/pip-7.1.2-py2.7.egg/pip/basecommand.py\", line 211, in main\n    status = self.run(options, args)\n  File \"/Library/Python/2.7/site-packages/pip-7.1.2-py2.7.egg/pip/commands/install.py\", line 311, in run\n    root=options.root_path,\n  File \"/Library/Python/2.7/site-packages/pip-7.1.2-py2.7.egg/pip/req/req_set.py\", line 640, in install\n    requirement.uninstall(auto_confirm=True)\n  File \"/Library/Python/2.7/site-packages/pip-7.1.2-py2.7.egg/pip/req/req_install.py\", line 716, in uninstall\n    paths_to_remove.remove(auto_confirm)\n  File \"/Library/Python/2.7/site-packages/pip-7.1.2-py2.7.egg/pip/req/req_uninstall.py\", line 125, in remove\n    renames(path, new_path)\n  File \"/Library/Python/2.7/site-packages/pip-7.1.2-py2.7.egg/pip/utils/__init__.py\", line 315, in renames\n    shutil.move(old, new)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py\", line 302, in move\n    copy2(src, real_dst)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py\", line 131, in copy2\n    copystat(src, dst)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py\", line 103, in copystat\n    os.chflags(dst, st.st_flags)\nOSError: [Errno 1] Operation not permitted: '/var/folders/_f/95xpdnrx4yncxz91dt53mvfh0000gn/T/pip-ZXxpkX-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/six-1.4.1-py2.7.egg-info'\n```\n\nThis issue (https://github.com/pypa/pip/issues/3165) has more insight into the same. And according to this its the issue with `six` library being preinstalled in El Captain systems.\n\nMoreover there seems to be no solution as of now except installing python locally using brew rather than using pre-installed package by Apple.\n", "@vrv I've got `AttributeError: 'module' object has no attribute '__version__'` for both 3.0.0a1 and 3.0.0a3 (which is the latest you can install with pip).\n\nI've found what fixes the issue for me:\n\n``` sh\n$ brew uninstall protobuf\n$ pip uninstall protobuf\n$ pip install 'protobuf>=3.0.0a3'\n$ pip install https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl\n```\n\nIt appears that Homebrew's protobuf package was causing the issues. Removing it, then installing protobuf3 with pip does the trick.\n", "For the protobuf issue above on OS X, not only should you `pip uninstall protobuf` to get rid of protobuf 2.x, but make sure you `brew uninstall protobuf` if you are using homebrew.\n\n@VikramTiwari : that is a different issue, already addressed in the \"common problems\" section of the 'Get Started' page\n", "@dvj Yup! Got it just now. Instructions by @flyingmutant did the trick.\n", "Thanks for the help debugging everyone!  We'll try to update our 'common problems' section soon to include these suggestions.\n", "By the way, if you would like to use homebrew, you can install protobuf 3 via:\n\nbrew install --devel protobuf\n\nFor more details please refer to https://github.com/grpc/grpc-common/issues/162\n", "I was able to solve this by doing `brew reinstall --devel protobuf`, which installs `3.0.0a1`. Feel free to close the issue whenever you update the docs.\n", "I have the same problem, but my protobuf is apparently up-to-date?\n\nHere is the output to `pip show numpy protobuf`\n\n```\nName: numpy\nVersion: 1.10.1\nLocation: /mnt/4tb_internal/python/tensorflow/lib/python2.7/site-packages\nRequires: \n\nName: protobuf\nVersion: 3.0.0a3\nLocation: /mnt/4tb_internal/python/tensorflow/lib/python2.7/site-packages\nRequires: setuptools\n```\n\nHere is the output to `uname -a`. My machine runs Ubuntu 14.04 LTS.\n\n```\nLinux yi-2014 3.13.0-64-generic #104-Ubuntu SMP Wed Sep 9 12:36:12 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\n```\n\nHere are the details of the output:\n\n```\n>>> import tensorflow\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/mnt/4tb_internal/python/tensorflow/local/lib/python2.7/site-packages/tensorflow/__init__.py\", line 4, in <module>\n    from tensorflow.python import *\n  File \"/mnt/4tb_internal/python/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 13, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\n  File \"/mnt/4tb_internal/python/tensorflow/local/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py\", line 16, in <module>\n    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\n  File \"/mnt/4tb_internal/python/tensorflow/local/lib/python2.7/site-packages/tensorflow/core/framework/attr_value_pb2.py\", line 16, in <module>\n    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\n  File \"/mnt/4tb_internal/python/tensorflow/local/lib/python2.7/site-packages/tensorflow/core/framework/tensor_pb2.py\", line 16, in <module>\n    from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n  File \"/mnt/4tb_internal/python/tensorflow/local/lib/python2.7/site-packages/tensorflow/core/framework/tensor_shape_pb2.py\", line 22, in <module>\n    serialized_pb=_b('\\n,tensorflow/core/framework/tensor_shape.proto\\x12\\ntensorflow\\\"d\\n\\x10TensorShapeProto\\x12-\\n\\x03\\x64im\\x18\\x02 \\x03(\\x0b\\x32 .tensorflow.TensorShapeProto.Dim\\x1a!\\n\\x03\\x44im\\x12\\x0c\\n\\x04size\\x18\\x01 \\x01(\\x03\\x12\\x0c\\n\\x04name\\x18\\x02 \\x01(\\tb\\x06proto3')\nTypeError: __init__() got an unexpected keyword argument 'syntax'\n>>> \n```\n", "My thread got closed due to being similar to this, but I am not on Mac!  I would very much appreciate it if we can leave this one open until this gets resolved!\n", "Woops, sorry liuyipei@!  Will re-open the original one.  It's the same issue, just on a different platform.\n", "We've updated our docs to what we believe can help this -- closing this bug for now but we can re-open if there are some other related issues unaddressed\n", "I have  a similar issue on my mac running the eval lua command (th eval.lua -model .. /model_id1-501-1448236541.t7 -image_folder .. /img -num_images 1) on a small folder of ten images. \n\nI am in the neurotalk2 folder but nonetheless I get:\n\n> > > /Users/localadmin/torch/install/bin/luajit: ...rs/localadmin/torch/install/share/lua/5.1/trepl/init.lua:383: module 'misc.utils' not found:No LuaRocks module found for misc.utils\n> > >     no field package.preload['misc.utils']\n> > >     no file '/Users/localadmin/.luarocks/share/lua/5.1/misc/utils.lua'\n> > >     no file '/Users/localadmin/.luarocks/share/lua/5.1/misc/utils/init.lua'\n> > >     no file '/Users/localadmin/torch/install/share/lua/5.1/misc/utils.lua'\n> > >     no file '/Users/localadmin/torch/install/share/lua/5.1/misc/utils/init.lua'\n> > >     no file './misc/utils.lua'\n> > >     no file '/Users/localadmin/torch/install/share/luajit-2.1.0-beta1/misc/utils.lua'\n> > >     no file '/usr/local/share/lua/5.1/misc/utils.lua'\n> > >     no file '/usr/local/share/lua/5.1/misc/utils/init.lua'\n> > >     no file '/Users/localadmin/.luarocks/lib/lua/5.1/misc/utils.so'\n> > >     no file '/Users/localadmin/torch/install/lib/lua/5.1/misc/utils.so'\n> > >     no file './misc/utils.so'\n> > >     no file '/usr/local/lib/lua/5.1/misc/utils.so'\n> > >     no file '/usr/local/lib/lua/5.1/loadall.so'\n> > >     no file '/Users/localadmin/.luarocks/lib/lua/5.1/misc.so'\n> > >     no file '/Users/localadmin/torch/install/lib/lua/5.1/misc.so'\n> > >     no file './misc.so'\n> > >     no file '/usr/local/lib/lua/5.1/misc.so'\n> > >     no file '/usr/local/lib/lua/5.1/loadall.so'\n> > > stack traceback:\n> > >     [C]: in function 'error'\n> > >     ...rs/localadmin/torch/install/share/lua/5.1/trepl/init.lua:383: in function 'require'\n> > >     eval.lua:7: in main chunk\n> > >     [C]: in function 'dofile'\n> > >     ...dmin/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:145: in main chunk\n> > >     [C]: at 0x010f243bc0\n\nAny suggestions what I m doing wrong? \n\nI traced back all the steps (except pointing the library path to the cudnn - I didn not get that). \n", "I got the same `syntax` keyword error with both 0.5 and 0.6 versions of tensorflow.\n\nI'm using a `virtualenv` setup. Oddly, I got the error within `ipython` sessions, but didn't get the error when running `python`. My solution was to remove ipython from the system install and install it in the virtualenv environment, instead. Hope this helps folks sole their install issues.\n\nRelevant IPython warning that pointed me to this solution:\n\n```\n$ ipython\nWARNING: Attempting to work in a virtualenv. If you encounter problems, please install IPython inside the virtualenv.\n...\n```\n", "## I got the same syntax error on the gpu 0.5 version installed using python 2. I'm using a  virtualenv setup. The other  i am not understanding is that when i wrote the 'pip show numpy protobuf ' command the result was as follows\n\nName: numpy\nVersion: 1.8.2\nLocation: /usr/lib/python2.7/dist-packages\n\n## Requires:\n\nName: protobuf\nVersion: 2.5.0\nLocation: /usr/lib/python2.7/dist-packages\nRequires:\n where as while importing the google.protobuf and printing the google.protobuf version the result was as follows\n(tensorflow)root@dmlserver:/home/rzibello/Documents# python\nPython 2.7.6 (default, Jun 22 2015, 17:58:13)\n[GCC 4.8.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n> > > import google.protobuf\n> > > print google.protobuf.**version**\n> > > 3.0.0-alpha-1\n\nThe OS i am using Linux Ubuntu 14.04. can sombody please let me know their difference and what i could do get rid off the syntax error as well? \n", "This is how I installed tensorflow on OSX 10.11.3\n- su\n- python --version ( check su is using python 2.7 )\n- python -m pip install https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none.any.whl\n\nbash-3.2$ su\nPassword:\nsh-3.2# whoami\nroot\nsh-3.2# python --version\nPython 2.7.11 :: Anaconda 2.4.1 (x86_64)\nsh-3.2# python -m pip install https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl\nCollecting tensorflow==0.5.0 from https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl\n  Downloading https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl (9.8MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.8MB 35kB/s \nRequirement already satisfied (use --upgrade to upgrade): six>=1.10.0 in ./anaconda2/lib/python2.7/site-packages (from tensorflow==0.5.0)\nRequirement already satisfied (use --upgrade to upgrade): numpy>=1.9.2 in ./anaconda2/lib/python2.7/site-packages (from tensorflow==0.5.0)\nInstalling collected packages: tensorflow\nSuccessfully installed tensorflow-0.5.0\n", "I have exact the same problem.\nIt works for me to remove the protobuf 2.6 in the brew. \nActually there are two versions of protobuf in my system, 2.6 below brew and 3.0 blew pip. \nJust one command to uninstall the protobuf 2.6 \n\n> brew uninstall protobuf\n\neverything is ok now. \n", "I have updated to reinstalled with pip to  protobuf (3.2.0) and uninstalled with brew just in case (it wasn't there) but I still get the same problem from importing tensor flow\r\n\r\n> >>> import tensorflow\r\n> Traceback (most recent call last):\r\n>   File \"/Users/kaja/tensorFlow/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 61, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n>   File \"/Users/kaja/tensorFlow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n>     _pywrap_tensorflow = swig_import_helper()\r\n>   File \"/Users/kaja/tensorFlow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\n>   File \"/Users/kaja/tensorFlow/lib/python3.5/imp.py\", line 242, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"/Users/kaja/tensorFlow/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: dlopen(/Users/kaja/tensorFlow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Library not loaded: @rpath/libcudart.8.0.dylib\r\n>   Referenced from: /Users/kaja/tensorFlow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n>   Reason: image not found\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"<stdin>\", line 1, in <module>\r\n>   File \"/Users/kaja/tensorFlow/lib/python3.5/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n>     from tensorflow.python import *\r\n>   File \"/Users/kaja/tensorFlow/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 72, in <module>\r\n>     raise ImportError(msg)\r\n> ImportError: Traceback (most recent call last):\r\n>   File \"/Users/kaja/tensorFlow/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 61, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n>   File \"/Users/kaja/tensorFlow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n>     _pywrap_tensorflow = swig_import_helper()\r\n>   File \"/Users/kaja/tensorFlow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\n>   File \"/Users/kaja/tensorFlow/lib/python3.5/imp.py\", line 242, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"/Users/kaja/tensorFlow/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: dlopen(/Users/kaja/tensorFlow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Library not loaded: @rpath/libcudart.8.0.dylib\r\n>   Referenced from: /Users/kaja/tensorFlow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n>   Reason: image not found"]}, {"number": 10, "title": "Go API", "body": "Hi!,\nI'm really interested on contribute in my spare time on this project, it  would be great for me port the Python libraries to Go, or help with this task, the problem is that after read the \"Contributing guidelines\" I still don't know how to do it.\nI tried to sing the Individual CLA, but I get an error that says: \"You must be an owner of the contributors group in order to submit this CLA.\".\n\nBtw, are you working, or do you plan to implement soon the Go libs?\n\nSome related projects that I implemented in Go: https://github.com/alonsovidales/pit - This is a distributed recommender system based in the \"Adaptive Bootstrapping of Recommender Systems Using Decision Trees\" paper by Yahoo. The site of the project: http://pitia.info/\nAnd the Go ML: Some machine learning algorithms in Go: https://github.com/alonsovidales/go_ml I also implemented Neural Networks with CUDA: https://github.com/alonsovidales/go_ml/tree/cuda_implementation\n\nThanks,\n", "comments": [":+1: \n", "I'd be interested in helping @alonsovidales !\n", "Idk if I'm qualified but I'd like to do whatever to help @alonsovidales !\n", "I can help too because I need this library in Go\n", "+1/0\n", "I still have no answer, but we could start working on the libs, and if Google wants, they can use the code, if not we can release the libs as a separate project :) I think that this shouldn't be a problem since the project is licensed using an Apache 2.0 license.\nMy main concern is that we could be doing the same that they are doing the same in parallel, so, by the moment we can keep reading and trying to understand how the currently available libs works, and if this weekend we don't have any news, we can prepare, plan and distribute the different tasks to do the libraries.\n\nWhat do you think?\nI'm going to send an e-mail to the golang-nuts mail list.\n", "agree! wait till weekend then we can decide.\n", ":+1:\n", "Great move!\n", "The current state of Go is that we have a few people who've expressed interest internally in contributing language bindings, but nothing concrete at the moment. I would suggest that if you got started on this, which would be awesome, you could whip up a proposal (preferably in the form of prototype code) and run the design by the discuss mailing list.\n", "Thanks @vincentvanhoucke , then, let's try to prepare a prototype and let's see how it works :) I had been digging a bit in the Python libs, and it shouldn't be too difficult to prepare the Go libraries.\n\nBtw, which is the discuss mailing list?, I can't find it.\n\nI love the project and it would be awesome for me be able to contribute.\n", "Found the discuss mailing list, sorry :)\n", "Should we keep this issue open to track this? I think all the other Go-related issues were de-duped against this one.\n", "Sure, better keep this issue open so other people can find it easily.\n\nI think that it would be better to move the conversation about the prototype, etc to another place, we can:\n- Use the official \"discuss mailing list\"\n- Create a new mailing list just for this\n- Use the issue tracker of a GitHub repo, I created this repo just to start playing around: https://github.com/alonsovidales/tensorflow\n\nRegarding to the prototype, my proposal is to use one of the examples:\nhttp://tensorflow.org/tutorials/mnist/beginners/index.md\nAnd studying how the Python libs works, port just all the involved code to execute the example, since an example like the one for beginners covers most of the functionality, and can give us a great overview of how to port all the libraries.\nTo reproduce the behaviour of the Python libs, the best option would be to use TDD, we can go method by method and based on the input/output of the python code prepare the test on Go.\n\nWe can distribute the tasks to prepare the prototype or work in parallel and put all our ideas together at the end. Since for me the goal here is try to get a general idea about how to implement the final version, tasks that we have to do, etc, I think that work in parallel can be a good approach.\n\nWhat do you think?\n", ":+1: \n", "Just a little update of what I have been researching this weekend. I couldn't find too much time for this, but I made some small progresses.\n\nI have been researching how the Python libs works internally, and basically, the logic to generate the graph and plan how to execute it, etc is managed from Python that sends all the graph to be executed to the C++ libraries, this libs take the graph and executes all the nodes of the graph. They are working on a pure C++ version of this logic, but it is still not ready.\nIn order to execute the C++ code from python they use SWIG: www.swig.org and to build the system Bazel: http://bazel.io/ .\nThe good news is that SWIG also supports Go ( http://www.swig.org/Doc2.0/Go.html ) and Bazel introduces support for Go ( http://bazel.io/docs/be/go.html )  after the 0.1.1 version.\nThe bad news are that I couldn't build the system using Bazel 0.1.1 by many different problems with the linker, etc that I'm trying to resolve. And the other problem is that we have to create the .i files for SWIG in order to prepare the build for Go, I almost have this part done, but I'm also having a lot of problems.\nThe main issue is that I have no experience with Bazel and SWIG, so I'm spending most of the time reading documentation and trying to fix stupid problems caused because of I misunderstand something from the docu.\n\nIn case of I finally can't prepare a wrapper using SWIG, I think that I'm going to try to prepare the prototype using cgo directly, I already have a small proof of concept that generates the session in Go, but it would be much better if the libs works with SWIG and Bazel as the Python libs does.\n\nI'm not having too much spare time lately, but it is being super interesting for me to research how all this system is done, the code is pretty well documented, and easy to understand.\n", "It seems like Bazel doesn't yet have C/C++ interop for Go so cgo may be the only option unless Bazel adds support for it soon.\nhttp://bazel.io/docs/be/go.html\n", "I think that it would be possible to do a small workaround using a sh_binary to compile run swig like they are doing with the Python libs:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/BUILD#L21\nBut by the moment for the PoC I'm using an ugly MakeFile:\nhttps://github.com/alonsovidales/tensorflow/blob/master/tensorflow/go/makefile\nAnd setting the flags for the linker here:\nhttps://github.com/alonsovidales/tensorflow/blob/master/tensorflow/go/cgoflags.go\nAnd also a wrapper to access the C++ libraries from C:\nhttps://github.com/alonsovidales/tensorflow/tree/master/tensorflow/go/wrapper\nBut this is just for the prototype, I plan to move all to Bazel + SWIG :)\n", "I'm still not 100% clear on the long-term delineation between the external library's responsibilities and the underlying TF C++ API.\n\nFrom http://www.tensorflow.org/api_docs/cc/index.html, this sentence implies that maybe at some point in the future the C++ libraries will add the support to build the graphs: \"TensorFlow's public C++ API includes only the API for executing graphs, as of version 0.5.\"\n\nIt seems like the supported way outside of python is to still use python to build the graph, then use tf.train.write_graph() to write the graph to a file, and then load it into a session in your language of choice and then run it using the C++ API.\n\nAre you planning to port the graph building mechanism into Go? It seems like it might be a good idea to get the consensus of the TF dev team before embarking on something as ambitious as that if they're planning on supporting this in the public API at some point in the future.\n", "Well, my idea is to build the graph in the Go libs as it is being builded by the Python libs. They are working on the C++ code, but by the moment it is a WIP and it is, by the moment, unstable.\n\nI'm not sure about their future plans, but from my point of view have this process in Python is not a bad approach since the tensors takes a lot of advantage of the dynamic typing and operators overloading that Python offers, this makes the code more simple to write and to understand, this is being one of the biggest problem I'm having porting the code to Go. I also think that keep the graph building process in Python is a good idea since this process requires almost no time compared with the operations that are implemented in C++, so scarify simplicity in favour of performance don't makes too much sense. I think that they make the good choice here.\n\nI'm planning to port the process to Go by two reasons:\n- You could create a pure Go application to train your model, that could be interesting for some situations.\n- I'm working on just a PoC, and I prefer to explore all the code in order to get a global idea of how all the different pieces works together.\n\nAnd well, port the process is going to be funny for me :)\n", "@alonsovidales any movement on your PoC ?\n", "No :'( I had a personal problem and I couldn't spend a single min on personal projects during last month, I'll try to resume it on next week.\n", "I have a start of a swig-based binding started here: [tmc/tensorflow/go_bindings](https://github.com/tmc/tensorflow/tree/go_bindings) -- mostly cargo-culted and incomplete.\n\n``` sh\n\u269b ~/go/src/github.com/tensorflow/tensorflow(go_bindings)$ cat tensorflow_test.go\npackage tensorflow_test\n\nimport (\n    \"fmt\"\n    \"testing\"\n\n    \"github.com/davecgh/go-spew/spew\"\n    \"github.com/golang/protobuf/proto\"\n    \"github.com/tensorflow/tensorflow\"\n    tf \"github.com/tensorflow/tensorflow/tensorflow/go\"\n)\n\nvar exampleGraph = `node {\n  name: \"output1\"\n  op: \"Const\"\n  attr {\n    key: \"dtype\"\n    value {\n      type: DT_STRING\n    }\n  }\n  attr {\n    key: \"value\"\n    value {\n      tensor {\n        dtype: DT_STRING\n        tensor_shape {\n        }\n        string_val: \"Hello, TensorFlow!\"\n      }\n    }\n  }\n}\nnode {\n  name: \"Const_1\"\n  op: \"Const\"\n  attr {\n    key: \"dtype\"\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: \"value\"\n    value {\n      tensor {\n        dtype: DT_INT32\n        tensor_shape {\n        }\n        int_val: 10\n      }\n    }\n  }\n}\nnode {\n  name: \"Const_2\"\n  op: \"Const\"\n  attr {\n    key: \"dtype\"\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: \"value\"\n    value {\n      tensor {\n        dtype: DT_INT32\n        tensor_shape {\n        }\n        int_val: 32\n      }\n    }\n  }\n}\nnode {\n  name: \"output2\"\n  op: \"Add\"\n  input: \"Const_1\"\n  input: \"Const_2\"\n  attr {\n    key: \"T\"\n    value {\n      type: DT_INT32\n    }\n  }\n}\nversion: 5`\n\nfunc TestNewSession(t *testing.T) {\n    graph := &tf.GraphDef{}\n    if err := proto.UnmarshalText(exampleGraph, graph); err != nil {\n        t.Fatal(err)\n    }\n    s, err := tensorflow.NewSession()\n    if err := s.ExtendGraph(graph); err != nil {\n        t.Fatal(err)\n    }\n    output, err := s.Run(nil, []string{\"output1\", \"output2\"}, nil)\n    if err != nil {\n        t.Fatal(err)\n    }\n    fmt.Println(output)\n    for _, out := range output {\n        spew.Dump(out, out.Data())\n    }\n}\n\u269b ~/go/src/github.com/tensorflow/tensorflow(go_bindings)$ go test\n[DT_STRING: dims:0 size:27 DT_INT32: dims:0 size:4]\n(*tensorflow.Tensor)(0xc820011050)(DT_STRING: dims:0 size:27)\n([]uint8) (len=27 cap=27) {\n 00000000  00 00 00 00 00 00 00 00  12 48 65 6c 6c 6f 2c 20  |.........Hello, |\n 00000010  54 65 6e 73 6f 72 46 6c  6f 77 21                 |TensorFlow!|\n}\n(*tensorflow.Tensor)(0xc820011080)(DT_INT32: dims:0 size:4)\n([]uint8) (len=4 cap=4) {\n 00000000  2a 00 00 00                                       |*...|\n}\nPASS\nok      github.com/tensorflow/tensorflow    0.082s\n```\n", ":+1:\n", ":+1:\n", "tmc sent a pull request - https://github.com/tensorflow/tensorflow/pull/1237 - which we should be able to get in with a few iterations.\n", "Hi,\nBased on the @tmc work I'm working on some helpers to create tensors from Go, to access to their data and so on. I'm also trying to cover all the code with tests.\n\nThis is the branch where I'm working:\n  https://github.com/alonsovidales/tensorflow/tree/go_bindings_tensors/tensorflow/go\n\nThere is a practical example in the README.md that shows how to create and export the Graph from Python, import it on Go, create some tensors from Go in an easy way and read the execution outputs. More or less all the necessary process to run our graphs in Go :)\nYou can also check the *_tests.go to see more practical examples.\n\nThis is a work in progress, there are some data types that are still not supported and I'm performing some important changes.\nI've made a PR to the origin branch, pls check the code and let me know your thoughts, how to improve it, possible bugs, etc:\n  https://github.com/tmc/tensorflow/pull/1/files\n\nI'll try to finish, test and stabilise the code as much as possible and prepare some of the tutos to work with the Go library.  I plan to start with this one:\n   https://www.tensorflow.org/versions/r0.7/tutorials/image_recognition/index.html\n\nI also plan to add also methods to save and load tensors and for Graph generation (just for fun and this perhaps could be useful in some situations, but I think that is a better approach to generate and test the Graph on Python), thanks to protobuf this shouldn't be difficult to achieve.\n", "I finally have the C++ example of the Image Recognition tutorial ported to Go. Implement all the Graph construction, constants and other stuff was relatively easy since there is available the list of operations perfectly defined here:\n- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/ops.pbtxt\n\nHere you have the Go port of the tuto example:\n- https://github.com/alonsovidales/tensorflow/blob/go_bindings_tensors/tensorflow/examples/label_image_go/main.go\n\nthat corresponds with:\n- https://github.com/alonsovidales/tensorflow/blob/go_bindings_tensors/tensorflow/examples/label_image/main.cc\n\nThis is how looks like running a picture of Ceres my Labrador:\n\n```\navidales@bigbrain:~/go-src/src/github.com/tensorflow/tensorflow/tensorflow/examples/label_image_go$ go run main.go data/ceres_paris.jpg\nLabrador retriever : 0.9383322\nAmerican Staffordshire terrier : 0.009385344\nRhodesian ridgeback : 0.007571198\nChesapeake Bay retriever : 0.0027833031\ngolden retriever : 0.0026763931\n```\n\nI'm going to clean the code and include a section for Go libs on the tutorial.\n", "@dave-andersen What's the status of this? \n", "Closed by #1771.\n", "#1771 appears to be in the 'go' branch but not master -- what's the plan to merge that in?\n", "Realistically, TBD.  I've maybe found someone @ google who might be interested in taking ownership of the bigger picture of TF + go.  I think we're of a mind that the current state isn't what we need yet -- it's not integrated with the normal tests, nor is it go-buildable in the normal way.  Those are the major blockers to getting it into the main branch, but I think there's a further concern that because of the amount of magic that still happens only in Python with shapes & gradients, we're not at a point where we can have the Go bindings we'd like as a first-class supported language.  There's some ongoing work looking at improving our ability to support more languages, and I think some of the momentum on Go is stalled waiting on that.\n", "Fair enough. I personally just wanted a way to execute graphs vs have python-level support.\n", "Let's leave this bug open then, to go along with the bunch of other open language bugs that are much less far along.\n", "@alonsovidales are you still working on this ?\nI tried the go branch after a rebase and it works on my mac. Now I would like to propose some changes with a PR (https://github.com/Mistobaan/tensorflow/tree/go). But clearly the current go branch is behind respect to master and the PR is huge.\nShould we rebase the main go branch or merge it in master and keep continuing the development there? \nWhat is the best feasible strategy to keep patches coming into the go interface? \n", "Hi @Mistobaan , I've been a bit out the last months because I'm moving to Dublin. @dave-andersen mentions on a previous comment that someone at Google could be interested on taking ownership of the Go development, the code was lent in the Go branch waiting for the integration with Jenkins and for other people to play with it and provide some feedback, I'm not sure about what is the current status.\nI've took a look at your commit and it seems really interesting, I think that you could make a PR to the current branch and we  can care to rebase it in a future :)\nI'm using the Go libs on some personal projects and I got feedback from other people that are using them, It would be awesome for us to count with improvements.\n", "@alonsovidales see #3258 \n", "FYI #3542 resolves a build error I experienced on the Go branch.\n", "I've just published a PR to bump the go branch to the latest master and do some minor fixups in #4223 \n", "I'm not sure what's left to do in the go branch to get it merged to master. I might be able to help!\n", "Updated #4223.\n\nI think there was a goal of getting this to build with `go get` and `go install`. Currently, the generated protobuf doesn't have the right import paths to build correctly. (It's a version of the problem in https://github.com/golang/protobuf/issues/230)\n\nWe're going to have to probably do some sed work to fix it.\n", "Before you do more work on that branch, keep in mind that we have in-progress Go bindings that the TensorFlow team is making :)\n", "That's cool! Didn't know! Maybe missed a comment. Anything I can do to help?\n", "@jhseu , do you have any plan about when we can get the official release?\n", "We've got a handle on them :) For obvious reasons, we're particular about the structure of the Go API and some details (like autosubmitting generated protobuf code).\n\nWe need some additions to the C API (work-in-progress) to complete them. Should be in about a week or two.\n", "Note that it's here:\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/go\n\nWe're making a more reasonable import path for people to use.\n", "Cool! What's the plan for the protobuf generated packages? I'm currently copying the proto files into my own build to pop out the Go code I want and they just barely don't fit there trivially (because protoc-gen-proto has some rules about where its generated code goes and what import paths they refer to each other with)\n", "Oh, man, after weeks of hunting, I just found https://github.com/golang/protobuf/issues/139 so now I understand how it'll work\n", "@jmhodges : So far whatever functionality the [Go API provides](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go)  doesn't require protobuf. But of course, the functionality is limited at this point.\n\nIf y'all have any feedback on the API as it is today and what you'd find most useful in terms of additional functionality or changes, we'd greatly appreciate that.\n", "Hello folks, the Go API is in a reasonably usable shape at this point. Graphs can be imported, executed, constructed (using primitives or functions generated from the TensorFlow op definitions). See [the example on godoc.org](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go#ex-package) that demonstrates all this.\r\n\r\nThe API is by no means _complete_, but at this point I'm tempted to close out this generic \"Go API\" issue with the expectation that as the Go API sees real usage, new issues will be filed with specific feature requests and bug reports.\r\n", "I created this package to enable tensorboard for tensorflow golang api. Commenting here in case anyone interested https://github.com/helinwang/tfsum", "@helinwang, very cool! If you need any help with this I would be happy to contribute ", "save and load tensorflow tensors using tensorflow golang api: https://gist.github.com/helinwang/7782c6b2815c334c77653fc0e52b6069", "Hi Folks, I am just curious why wrapper util functions in \"op\" folder don't seem to take node name as input argument. This prevents me from calling functions twice in a scope. For instance, I can't define two constants in a graph. There are no errors when I explicitly define constants or placeholders using tf.OpSpec by providing values for the Name field.", "@sdeoras : The alternative for now is to use [`Subscope`](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go/op#Scope.SubScope), which is [used in the label image example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/go/example_inception_inference_test.go#L211) as well (resulting in constants named `make_batch/Const`, `size/Const` etc. in the graph).\r\n\r\nThe answer to your question though is just that it felt less cumbersome to require a `name` argument on every op addition when it can often be elided. We'd want it to be an \"optional\" argument. One option is to do what we do in C++ graph construction API and add a [`WithOpName`](https://github.com/tensorflow/tensorflow/blob/r0.12/tensorflow/cc/framework/scope.h#L104) to the `Scope` type.\r\n\r\nIf you have strong opinions or suggestions, please do share (though perhaps in a separate issue focused on this aspect). In the mean time, hopefully the alternative suggested above lets you make progress.", "@asimshankar : Thanks for your quick reply. I started working with scope as opposed to working directly with graphs and finding the workflow better. Yes, subscope solves the namespace issue, I also don't have to name every operation and error management is so much easier, i.e., basically catch error at scope finalize step.", "Hi Folks, I am not sure if this thread is the best place to ask TF GO API related questions. If not, please suggest a link to the right forum.\r\n\r\nSo I can't seem to get following code to work. It complains on second use of op.Reshape at runtime but allows two calls to op.Const... what am I doing wrong?\r\n\t```\r\nscope := op.NewScope()\r\n\tx := op.Placeholder(scope, tf.Double)\r\n\ts := op.Const(scope, []int32{3, 3})\r\n\ty := op.Reshape(scope, x, s)\r\n\tyInv := op.MatrixInverse(scope, y)\r\n\ts2 := op.Const(scope.SubScope(\"scope.a\"), []int32{9})\r\n\tyInvReshaped := op.Reshape(scope.SubScope(\"scope.b\"), yInv, s2)\r\n\tgraph, err := scope.Finalize()\r\n\tif err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n```\r\n", "Please use stackoverflow for general questions, or file new github issues for bugs/feature requests. In this case I've filed #6799 - I think I know what the problem is, will update there.", "Hi Folks, I am running into another issue with shape argument in op.VarHandleOp. Pl. advise if this should be filed as a bug.\r\n\r\n```\r\n\r\npackage bugs\r\n\r\nimport (\r\n\ttf \"github.com/tensorflow/tensorflow/tensorflow/go\"\r\n\t\"github.com/tensorflow/tensorflow/tensorflow/go/op\"\r\n\t\"testing\"\r\n)\r\n\r\nfunc TestVarHandleOpShape(t *testing.T) {\r\n\tscope := op.NewScope()\r\n\t_ = op.VarHandleOp(scope, tf.Int32, []int64{2, 3})\r\n\t_, err := scope.Finalize()\r\n\tif err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n}\r\n\r\n/*\r\n=== RUN   TestVarHandleOpShape\r\n--- FAIL: TestVarHandleOpShape (0.03s)\r\n        bugs_test.go:14: failed to add operation \"VarHandleOp\": AttrValue had value with type 'list(int)' when 'shape' expected\r\n                         for attr 'shape'\r\n                        ; NodeDef: VarHandleOp = VarHandleOp[_class=[], container=\"\", dtype=DT_INT32, shape=[2, 3], shared_name=\"\"](); Op<name=VarHandleOp; signature= -> resource:resource; attr=container:string,default=\"\"; attr=shared_name:string,default=\"\"; attr=dtype:type; attr=shape:shape; is_stateful=true> (Stacktrace: goroutine 5 [running]:\r\n                runtime/debug.Stack(0x0, 0x0, 0x0)\r\n                        /usr/lib/golang/src/runtime/debug/stack.go:24 +0x79\r\n                github.com/tensorflow/tensorflow/tensorflow/go/op.(*Scope).UpdateErr(0xc42000c420, 0x53e4eb, 0xb, 0x7cce00, 0xc420054040)\r\n                        /home/sdeoras/go/src/github.com/tensorflow/tensorflow/tensorflow/go/op/scope.go:113 +0x72\r\n                github.com/tensorflow/tensorflow/tensorflow/go/op.(*Scope).AddOperation(0xc42000c420, 0x53e4eb, 0xb, 0x53e4eb, 0xb, 0x0, 0x0, 0x0, 0xc42000c450, 0xc42000c3f0)\r\n                        /home/sdeoras/go/src/github.com/tensorflow/tensorflow/tensorflow/go/op/scope.go:78 +0xf9\r\n                github.com/tensorflow/tensorflow/tensorflow/go/op.VarHandleOp(0xc42000c420, 0xc400000003, 0xc4200106f0, 0x2, 0x2, 0x0, 0x0, 0x0, 0x171e4d44, 0xc420051f78)\r\n                        /home/sdeoras/go/src/github.com/tensorflow/tensorflow/tensorflow/go/op/wrappers.go:14045 +0x27d\r\n                bitbucket.hgst.com/x/tensorflow.git/tfutil.TestVarHandleOpShape(0xc4200b8180)\r\n                        /home/sdeoras/go/src/bitbucket.hgst.com/x/tensorflow.git/tfutil/bugs_test.go:11 +0x96\r\n                testing.tRunner(0xc4200b8180, 0x54f7f0)\r\n                        /usr/lib/golang/src/testing/testing.go:610 +0x81\r\n                created by testing.(*T).Run\r\n                        /usr/lib/golang/src/testing/testing.go:646 +0x2ec\r\n                )\r\nFAIL\r\nexit status 1\r\nFAIL    bitbucket.hgst.com/x/tensorflow.git/tfutil      0.130s\r\n */\r\n\r\n```", "@sdeoras: Yeah, shape attributes handling [wasn't finished](https://github.com/tensorflow/tensorflow/blob/8f6cb22/tensorflow/go/graph.go#L263), but should be easy. Feel free to file a new issue for this (and don't hesitate to file new issues for other things instead of adding to this closed one).", "@helinwang Great work with enabling TensorBoard! Could you please include a license file for us to be able to use it for commercial purposes?", "@sdeoras Glad to know that the library helps! Added 3-clause BSD.", "Thank you @helinwang ", "Apologies in advance for not following this issue thoroughly, but what's the state of the Go binding? If it's finished can someone point me to tutorial implementations, for example linear regression over the MNIST dataset? Thanks", "@4d55397500 I believe the go binding of tensorflow is still under heavy developing. See: https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go\r\n\r\nIt is not stable enough and lots of features haven't been implemented.\r\n\r\n~~(I've just compiled the latest version and sadly speaking that it can not pass the builtin test on mac os x. A signal `kill` occurred when running tests.)~~\r\n\r\nApologies. After upgrade go from 1.8 to 1.8.1, tests passed.", "@lipixun @4d55397500 : The Go APIs available today are intended to be usable and tests for both Linux and OS X are included in the continuous builds on Jenkins. \r\n\r\n@4d55397500 : The APIs are a little low level though. They should be sufficient to build things like networks for MNIST and linear regression models on top of, but at this time there are no plans for writing convenience higher level APIs."]}, {"number": 9, "title": "Typo in getting started guide", "body": "http://tensorflow.org/get_started/basic_usage.md#variables\n\nIn the code example, 1 is being added to \"counter\", which is named \"var\" initially, but referred to as \"state\" later. My guess is that these are meant to refer to the same thing, and a typo in one of the variable names made it into the example code.\n", "comments": ["Apparently the fix has already been put into their pipeline.\n", "I just pushed the fix in https://github.com/tensorflow/tensorflow/commit/db0b5da485e1d1f23003ee08ed2e191451ee0319\n\nThanks!\n"]}, {"number": 8, "title": "Setting lower gcc version for cuda", "body": "gcc 4.10 and up are not supported for the required cuda toolkit.  How can I specify a different version of gcc to bazel?  I tried setting a new version with --compiler, but it reported that no toolchain was found.  Do I need to provide a CROSSTOOL file or is there an easier way?\n", "comments": ["Could you try the docker-based installation for now? It should have the correct gcc version. \n\nhttp://tensorflow.org/get_started/os_setup.md#docker-based_installation\n\nIt is possible to modify the lower-level scripts to use a different gcc. But you will need to modify a few places in the source code to make that happen. \n", "In case you have any trouble with the docker flow, this is the place you can modify to use a different compiler with Cuda, although we don't currently have a more user-friendly way of doing that.\n\nhttps://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc\n\nCPU_COMPILER = ('/usr/bin/gcc')\nNVCC_PATH = CURRENT_DIR + '/../../../cuda/bin/nvcc'\nGCC_HOST_COMPILER_PATH = ('/usr/bin/gcc')\nLLVM_HOST_COMPILER_PATH = ('/usr/bin/gcc')\n", "I changed the required gcc version in CUDA before installing so I could use my system version.\nSeems to have worked. There are rumors about possible problems but I've seen none so far.\n\nhttps://www.udacity.com/wiki/cs344/troubleshoot-gcc47\n", "Here are some other options for you:\nhttps://stackoverflow.com/questions/6622454/cuda-incompatible-with-my-gcc-version\n"]}, {"number": 7, "title": "API docs does not list RNNs", "body": "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/nn.md\n\n(It has convolutional layers listed, for instance, but does not show the RNNs. Actually, I don't quite see anything corresponding to a fully-connected layer either)\n", "comments": ["You should look at the RNN tutorial: http://tensorflow.org/tutorials/recurrent/index.md .\n", "(Thanks, I have seen the tutorial but it is not a substitute for an API; I filed the issue after consulting with a friend at Google Brain)\n", "Hi Avanti -- internally we've been working on iterating the API for RNNs, and we were happy enough with the current API to use it in the tutorial, but we're making sure it's solid before promoting it to the public API, since we'd then have to support it indefinitely.  (Anything not in the public API is a work-in-progress :)\n\nWe'll keep this bug open in the meantime, and for now you can look at the source code documentation if you're interested in playing around: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/rnn.py#L9\n", "Ah, got it, thanks for the explanation.\n", "The white paper of TensorFlow mentions looping control within the graph. Is it already available? If so, are there examples to show how it can be done?\n\nThe [RNN example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/rnn.py#L86) has a Python loop. Will TensorFlow treat that as a symbolic loop and compile it?\n\nAlso, the explanation of `sequence_length` [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/rnn.py#L30) isn't clear to me. What does it mean by **dynamic** calculations? When `t` is past `max_sequence_length`, can it just break from the loop instead of continuing with `zeros` state? Returning `zeros` state is different from returning the state at `max_sequence_length`, isn't it?\n", "On your first question, see https://github.com/tensorflow/tensorflow/issues/208.\n\nOn your second question: the core TF engine currently only sees the GraphDef produced by python, so the RNN example is an unrolled one today.\n\nI'm not super familiar with that RNN example -- @lukaszkaiser or @ludimagister might know better.\n", "I zer0n,\n\nthe current RNN is statically unrolled, there is no (not yet) dynamic unrolling based on the length of the sequence. Th dynamic calculation means the graph is unrolled up to the max_sequence_length, but if a sequence_length is provided the calculations on the unrolled graph are cut short once the sequence_length is reached, using a conditional op. Depending on the application this may result in shorter processing time.\n", "Yes, to add to what @ludimagister says: the conditional op will plug in zeros to the output & state past max(sequence_length), thus reducing the total amount of computation (if not memory).\n\nI may actually modify this so that instead of plugging in zeros to the state, it just copies the state.  This way the final state will represent the \"final\" state at max(sequence_length).  However, I'm undecided on this.  If you want the final state at time sequence_length, you can concat the state vectors and use transpose() followed by gather() with sequence_length in order to pull out the states you care about.  That's probably what you would want to do, in fact, because if you have batch_size = 2 and sequence_length = [1, 2], then for the first minibatch entry, the state at max(sequence_length) will not equal the state at sequence_length[0].\n\nAn alternative solution is to right-align your inputs so that they always \"end\" on the final time step.  This breaks down the dynamic calculation performed when you pass sequence_length (because it assumes left-aligned inputs).  I may extend this by adding a bool flag like \"right_aligned\" to the rnn call, which assumes that calculation starts at len(inputs) - max(sequence_length), and copies the initial state through appropriately.  But that doesn't exist now.\n", "Thanks @vrv, @ludimagister, and @ebrevdo for the answers. However, some details still confuse me.\n1. @ludimagister, the code doesn't seem to statically unroll. It has [a loop which depends on the length of the inputs](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/rnn.py#L86). Plus, `max_sequence_length` is not a const; instead it's just the scalar of the `sequence_length` parameter, which can be and is `None` by default. So, by default, the unrolling is not truncated. Correct me if I misread the code.\n2. @ebrevdo I understand the computational saving motivation. However, returning zeros is logically very different from returning the state at `sequence_length` (if provided). The former is just wrong. Again, please correct if I misread the code.\n\nThanks.\n", "@zer0n It depends on your task.\n\nReturning zeros is fine if you only care about outputs (i.e., you're not hooking up to a decoder); and your loss function knows to ignore outputs past the sequence_length.\n\nReturning the state from the end of the last time step might also be considered \"wrong\", but will generally always happen if you have inputs of different lengths (and aren't performing dynamic computation).  This is a typical approach to performing RNN with minibatches.  For this reason when performing encoding/decoding, people usually right-align with left-side padding instead, so the last input of any example always corresponds to the very last state.  This seems like the cleanest solution for now.\n\nAnyway, this part of the API may change; not sure yet the best approach.\n", "(also, specifically returning the state at sequence_length for every entry is taxing both in terms of computation and in terms of memory, both in short supply with RNNs )\n", "OK, I did miss the line `outputs.append(output)`. I originally thought that it returned the final state, not a sequence of states.\n\nAnyway, this implementation still looks weird (I'm aware it's changing so I'm only discussing the current state). Usually, for truncated BPTT implementation, people pad `eos` for short sentences and truncate the sentences if the lengths are larger than `max_length`. This enables static unrolling and efficient mini-batching.\n\nThe RNN example seems doing the reverse. What I see is that it's doing _dynamic_ unrolling (i.e. with dynamic output size), but padding zeros to the outputs past `max_length`.\n", "(This discussion is probably better off had on the discussion mailing list, rather than this bug about documentation)\n", "@vrv:\n\n> but we're making sure it's solid before promoting it to the public API, since we'd then have to support it indefinitely.\n\nMay we assume that TF is going to use semantic versioning for [releases](/tensorflow/tensorflow/releases) (ie. major.minor.patch)?\n\nMajor releases can have backward-incompatible API changes, and minor releases can certainly add a new API (or extend an existing one in a backward-compatible way) especialy if it marks the old API as deprecated (and to be removed in the next major release).\n\nSince TF has not yet had a major version release, (the current release is only [0.5.0](/tensorflow/tensorflow/releases/tag/0.5.0)), you have a lot of wiggle room between now and an eventual 1.0.0 release that then really would commit you to maintaining backward compatibility for quite a while.\n", "@webmaven: We are going to be using semver, and we will publish exactly what we mean by that too. We'll try reasonably hard to maintain API stability even before 1.0.0, especially in the parts of the API that are official. For now, we have decided that something becomes \"official\" when it shows up on the API docs. You'll notice that many other functions are documented but not included in the docs, that means their interface is still in flux.\n\nI'm closing this bug for now.\n", "@martinwicke:\n\n> We'll try reasonably hard to maintain API stability even before 1.0.0, especially in the parts of the API that are official. For now, we have decided that something becomes \"official\" when it shows up on the API docs. \n\nHmm. OK, but wouldn't putting APIs in the docs and marking them as 'draft' or 'unofficial' get you valuable external feedback while you're still iterating on an API? Or is external feedback only wanted/needed _after_ an API becomes 'official'?\n", "Yeah, we're definitely considering adding something like this since it helps to get community feedback on some experimental APIs before they are fully ready.\n", "@vrv and @martinwicke, so getting back to the original topic, is the RNN API currently a good candidate for such 'draft' treatment, and does that mean this issue on documentation should be re-opened?\n", "We are reconsidering the api, deciding if we should make the current\nrnncell implementations stateful, or whether to store some of the non\nvariable state in graph collections.  After that decision is made, we will\nlikely add documentation.\nOn Dec 20, 2015 6:45 AM, \"Michael R. Bernstein\" notifications@github.com\nwrote:\n\n> @vrv https://github.com/vrv and @martinwicke\n> https://github.com/martinwicke, so getting back to the original topic,\n> is the RNN API currently a good candidate for such 'draft' treatment, and\n> does that mean this issue on documentation should be re-opened?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/7#issuecomment-166134490\n> .\n", "@ebrevdo what are the considerations surrounding this decision between stateful rnncell implementations vs. storing the state in the graph collections?\n", "The main consideration is in how calculations common across time are\ncached.  Simpler caching is traded off against having RNNCell be a pure\nlogic object (non-stateful).\n\nFor example, if you access two Variables at each time step and then concat\nthem, using the result in your **call** calculation, then this is something\nthat should be cached beforehand because it creates redundant computation.\nThe two approaches to caching are:\n1. RNNCell is stateful: create and cache this Tensor inside the RNNCell\n   object\n2. RNNCell is non-stateful: **call** looks for the cached Tensor inside a\n   graph collection; if it doesn't exist, it creates it (similar to using\n   get_variable).\n\nWith a stateful RNNCell, Variables are created when the RNNCell is created;\nand so that variable scope is used.  With a non-stateful RNNCell, Variables\nare created / accessed during **call** and the variable scope used is\nwhatever it was when you ran rnn() or bidirectional_rnn() or whatever.\n\nBecause of this, moving from non-stateful RNNCell to a stateful one (and\nmodifying the associated implementations of LSTM, GRU, etc cells) would be\na breaking change.\n\nI personally prefer stateful objects, because it's easier to understand and\ndebug them.  But there are arguments in both directions that have to be\nconsidered.\n\nOn Sat, Dec 26, 2015 at 2:50 PM, Michael R. Bernstein <\nnotifications@github.com> wrote:\n\n> @ebrevdo https://github.com/ebrevdo what are the considerations\n> surrounding this decision between stateful rnncell implementations vs.\n> storing the state in the graph collections?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/7#issuecomment-167369971\n> .\n", "@ebrevdo:\n\n> [snip explanation]\n> But there are arguments in both directions that have to be considered.\n\nWhat are those arguments?\n", "For example:\n\nMoving to stateful objects now would break a bunch of external dependencies.  Of course, this is an undocumented API and therefore folks should expect it to break in their projects.  However, I'm afraid of breaking external projects in subtle ways that don't emit errors.  This indeed may happen with this change.  Especially for those who depth-stack RNNs on top of each other using the same instance.\n\nIn addition, there are those who argue that the RNNCell should continue to be a purely logical object with no state, so you can reuse the same instance of RNNCell across multiple RNNs without fear of reusing the same variable in multiple places (though get_variable's checks for over-sharing may ameliorate this somewhat).\n\nEDIT: scratch that last sentence.  the get_variable would then be called only once in the RNNCell's initialization, and all those get_variable protections would go out the door :(.\n", "So, you _can't_ actually reuse the same RNNCell instance across multiple RNNs in either case?\n", "Currently you can.  You can also use it with a shared name scope to tie the\nparameters across multiple rnns.\nOn Dec 29, 2015 3:31 PM, \"Michael R. Bernstein\" notifications@github.com\nwrote:\n\n> So, you _can't_ actually reuse the same RNNCell across multiple RNNs in\n> either case?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/7#issuecomment-167899548\n> .\n", "OK, so you _can_ currently reuse an instance of RNNCell across multiple RNNs without worrying about reusing the same variable in multiple places.\n\nDoes using a shared name scope with a single RNNCell instance across multiple RNNs gain you anything? Or is that really for reusing parameters across multiple RNNCell instances in multiple RNNs?\n", "Right. It gains you the ability to tie parameters not only within one lstm,\nbut also across multiple lstms.\nOn Jan 5, 2016 3:52 PM, \"Michael R. Bernstein\" notifications@github.com\nwrote:\n\n> OK, so you _can_ currently reuse an instance of RNNCell across multiple\n> RNNs without worrying about reusing the same variable in multiple places.\n> \n> Does using a shared name scope with a single RNNCell instance across\n> multiple RNNs gain you anything? Or is that really for reusing parameters\n> across multiple RNNCell instances in multiple RNNs?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/7#issuecomment-169172049\n> .\n"]}, {"number": 6, "title": "Pre-trained models", "body": "Is there any plan to include caffe-like pre-trained models to use for predictions?\n", "comments": ["+1\n", "You can find one pre-trained network as part of the Android example, at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/assets/tensorflow_inception_graph.pb\nThis implements a version of the Inception architecture for Imagenet classification.\n", ":thumbsup:\n", "Reopening to de-dupe future requests for this\n", "I converted the VGG-16 caffemodel to TensorFlow https://github.com/ry/tensorflow-vgg16 which might be helpful for a generalized script\n", "The link to pretrained model of inception for imagenet seems broken\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/assets/tensorflow_inception_graph.pb\n", "This is working as expected. We've moved the model out of the source repo, but it's available for separate download. See the android example README for the latest instructions:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/README.md\nCurrently the link is https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip but the README will hold the most up-to-date location going forward.\n", "The inception model checkpoint is downloadable (and we have examples using it for classification and deepdream), and more models are being added to https://github.com/tensorflow/models. Closing this.\n", "I found that the file size of https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip is very small, ~50MB. The Inception v3 model in tensorflow/tensorflow/examples/label_image/data/inception_dec_2015.zip is rather larger ~100MB but still portable on mobile devices. However when I tried to convert VGG-16 models into .pb file, it cost ~500MB. Is that the architecture of Inception is more superior than VGG in term of portability?\n", "@shlens, can you comment on this?\n", "@martinwicke I think my guess is right. I tried to freeze the inception-v1 (GoogleNet) and found it just 27MB, a half of inception-v3. This is probably due to the less number of fully connected layers used (infact just one layer in the case of inception-v1) compared to VGG-x architectures. This turns out that  inception arch is really friendly for embedding apps.\n", "Yes, Inception is known to be far smaller in terms of parameter count then VGG. The first version of Inception contained roughly 5M model parameters; VGG was >75M model parameters (I don't know the exact the number for VGG). As @vodp mentions, the primary reason for this difference in parameter count is due to the fact that VGG uses several large fully connected layers at the top where as Inception minimizes to just 1 layer for classification.\n", "for this inceptiion5h model, anyway to see the code that it was trained with?\n", "As part of our free course on [Creative Applications of Deep Learning](https://www.kadenze.com/courses/creative-applications-of-deep-learning-with-tensorflow/info), I've included code for loading pre-trained networks for inception v3,v5, vgg16, vgg face, i2v, and i2v tag: https://github.com/pkmital/CADL/tree/master/session-4/libs \n", "Is there any way to load a graph partially in tensorflow? \r\n\r\nI would like to be able to use the weights trained for classification, but replace the fully connected layers and retrain for regression. \r\n\r\nThis can be done in Keras using the \"by_name\" option when loading a model, but I couldn't find a way to do it in tensorflow.", "You might take a look at the TensorFlow for Poets codelab and sample code to see a similar approach to removing the final fully-connected layer:\r\nhttps://codelabs.developers.google.com/codelabs/tensorflow-for-poets/index.html#0\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/image_retraining/retrain.py", "Thanks @petewarden!\r\nThe code in retrain.py is very helpful!\r\nHowever, since inception has 3 output layers, don't I need to retrain all the 3 outputs?\r\nIn the retrain.py code a new layer is added on top of the bottleneck 'pool_3', but won't my regression be affected by not replacing all the outputs? ", "The inception5h model doesn't look like it can be used for classification or transfer learning, because it is missing the required layers: https://stackoverflow.com/questions/42003846/retraining-inception5h-model-from-tensorflow-android-camera-demo\r\n\r\nIs there an un-stripped version of the model? ", "pls check answers and comments on stackoverflow link @ProGamerGov ", "@GeorgianaPetria Hi ! I also want to add regression to the current inception v3 model.\r\nif you already did it. Do you have any useful clue for it ?\r\n\r\nThank you b4.", "Hi guys, is there any pre trained model file for person identification in tf using inception? Re-training isn't working for me and I need to use model file in my Android project.", "How to see content of `http://download.tensorflow.org/models/` ? i.e. I want to see full list of models that are available."]}, {"number": 5, "title": "Java interface", "body": "Issue to trace effort of swig interface for java. Started implementation - will update with progress. If anyone has any comments/tips - please feel welcome to join the discussion!\n", "comments": ["Nice!\n", "Moving this comment here from https://github.com/tensorflow/tensorflow/issues/3:\n\n---\n\nThere's a testsuite with fairly good converge, but currently it's mostly Python with a few C++ tests. There's also a lot of functionality for building graphs that's currently Python only, in particular the automatic differentiation functionality, though that doesn't matter for evaluation of graphs in Java. There are plans to move this functionality into the underlying C++ in future, at which point Java SWIG bindings would be more useful for creating graphs.\n\nIf someone takes up the Java SWIG challenge, we'd be happy to accept it upstream pending review, etc., at which point it would be part of our continuous testing. The details of accepting contributions is in flux at the moment, but that will stabilize.\n", "Hello guys\nWe are also interested in adapting TensorFlow for Java. @ravwojdyla  Have you, by any chance, started working on the Swig Interface for Java? If you have, we could join our efforts and collaborate on that\n", "Hello,\nI am working on a SWIG wrap of the main C++ API. You can see my progress thus far on my fork, but what's up there is not finished; I'm currently experiencing a problem in which `#include \"tensorflow/core/lib/core/error_codes.pb.h\"` is unable to be resolved and I cannot find the intended file anywhere within the project files. Input of any kind would be greatly appreciated.\n", "There are javacpp preset available for libiraries like Caffe  and OpenCV. See also  https://github.com/bytedeco/javacpp-presets/issues/111. Java-cpp enable also IOS with RoboVM \n", "@girving Initial commit at https://github.com/bytedeco/javacpp-presets/commit/374e1d5ea3cb4db36144aeec2cac33d32e1a7489\n", "/cc @saudet\n", "@pslam - I was able to work just a little bit on this - could definitely use some help!\n", "Hi guys, I believe I have pretty functional bindings for JavaCPP: https://github.com/bytedeco/javacpp-presets/tree/master/tensorflow. Let me know if you see anything that could be done with SWIG, but not JavaCPP. I could definitely use the feedback. (Thanks for the cc @bhack!)\n", "Very nicely done @saudet! I have almost finished a SWIG wrap, but it seems that your implementation works just as well. I do not see anything that my SWIG wrap can do that yours cannot do. JavaCPP seems very cool, I'll have to look into using it for future projects.\n", "Hi @kylevedder, have you resolved the issue related to `error_codes.pb.h` ?\n[Edited]\nAll .pb.h files are compiled from .proto\n", "@tngan Yes, that is what I discovered as well. Additionally, the `.proto` files in this project require ProtoBuff3 to be used. I'm using Ubuntu 14.04 and ProtoBuff3 was not available in my package manager, so I compiled it from source, which I got from the [3.0.0 beta release](https://github.com/google/protobuf/releases/tag/v3.0.0-beta-1). \n\nThe current roadblock which I am trying to solve is how to get ProtoBuff to recurse over the entire file tree and compile the `.proto` files into `.h` and `.cc` files; doing each folder piecemeal results in failures due to unsatisfied dependencies upon other yet to be compiled `.proto` files.\n", "@kylevedder Are your SWIG wrappers in a separate repository or are you working in the tensorflow repository? `protoc` works similar to other compilers. If you are working in the tensorflow repository or are using Bazel, then you would need to set up the protobuf build targets and the dependencies among them.\n\nIf you are working in a separate repository and using a different build system, then you would need to use the protobuf plugin for that build system.\n\nI'd be happy to help you set up the build if you would like.\n", "@davidzchen Thank you for the offer, any and all help is greatly appreciated. \n\n**What I have thus far:**\n\nI have already setup Bazel and gotten it to compile into a `.whl` file, which I then handed over to `pip` and confirmed that I can run the [First TensorFlow program](https://github.com/tensorflow/tensorflow#try-your-first-tensorflow-program). \n\nI have generated SWIG wrapper files in my forked repository. They are in a folder under `core/javaWrapper`. [[link](https://github.com/kylevedder/tensorflow/tree/master/tensorflow/core/javaWrapper)]\n\n**What I am trying to do:**\n\nUltimately, my goal is to generate a `.so` file which than can be called as a native library in Java. Currently, I'm attempting to use g++ to compile the entire system into a `.so` file; however, the `.proto` files need to first be expanded into `.h`s and `.cc`s prior to this compilation, and that is what I am trying to do with `protoc`.\n\nYou can see my attempt at a wrap script [here](https://github.com/kylevedder/tensorflow/blob/master/tensorflow/core/javaWrapper/createWrapper.sh) to potentially get a better idea of what it is I am getting at, although thus far all of my attempts at using `protoc` has been directory by directory and, consequently, not in the script.\n\nFinally, any feedback on areas of improvement would be greatly appreciated. Thanks!\n", "@kylevedder I already have an `.so` build as part of the JavaCPP Presets: https://github.com/bytedeco/javacpp-presets/tree/master/tensorflow. Thanks to Bazel, it's really simple. Just apply a patch like this:\n\n``` diff\ndiff -ruN tensorflow/tensorflow/cc/BUILD tensorflow-patch/tensorflow/cc/BUILD\n--- tensorflow/tensorflow/cc/BUILD  2015-11-22 00:00:02.441829192 +0900\n+++ tensorflow-patch/tensorflow/cc/BUILD    2015-11-14 11:15:12.689330351 +0900\n@@ -75,6 +75,17 @@\n     ],\n )\n\n+cc_binary(\n+    name = \"libtensorflow.so\",\n+    copts = tf_copts(),\n+    linkshared = 1,\n+    deps = [\n+        \":cc_ops\",\n+        \"//tensorflow/core:kernels\",\n+        \"//tensorflow/core:tensorflow\",\n+    ],\n+)\n+\n filegroup(\n     name = \"all_files\",\n     srcs = glob(\n```\n\nAnd run Bazel like this, for example:\n\n```\nbazel build -c opt //tensorflow/cc:libtensorflow.so\n```\n\nAFAIK, this should gobble up pretty much anything of interest for the C++ API.\n", "@saudet Is there a reason why you are using a `cc_binary` rule to build the shared library rather than `cc_library`? You can just have a `cc_library` rule with the name `tensorflow` and the build target will build a shared library called `libtensorflow.so`.\n\n@kylevedder If your goal is to generate an `.so` file, then something similar to what @saudet suggested would work.\n\nIf you need to use the TensorFlow protos in Java code, then you would need to add dependencies from your `java_*` Bazel build targets to the `proto_library` targets that generate the Java classes from the `.proto` files.\n\nWe still have a bit of work to do before we open-source the native `proto_library` rules (see bazelbuild/bazel#52), but in the meantime, TensorFlow uses the [`cc_proto_library` and `py_proto_library` rules provided by protobuf](https://github.com/google/protobuf/blob/master/protobuf.bzl), and for Java, you should be able to use the [Java `genproto` rule that is included with Bazel](https://github.com/bazelbuild/bazel/blob/master/tools/build_rules/genproto.bzl). I will check with the team to find out what the timeline for `proto_library` is and whether it would be worthwhile to unify the rules provided by Protobuf with `genproto`.\n\nA few other bits of feedback:\n- I think it would be better to keep the directory names consistent and use `java_wrapper` rather than `javaWrapper`\n- Perhaps a better place for the Java wrapper would be `//tensorflow/java/wrapper` rather than `//tensorflow/core/java_wrapper`?\n- Internally, we have some build rules that take `.swig` files and generate the sources. This is more ideal because we would avoid checking in the generated files. I can take a look to see how difficult it would be for us to add some SWIG build rules for Bazel to make stuff like this easier.\n", "@davidzchen No reason in particular. I'm new to Bazel and just using `linkshared=1` as I've seen mentioned on the mailing list worked. So thanks for the tip! I'll be updating that.\n", "@saudet Thanks! I was just checking to make sure that it wasn't an issue with Bazel. :) Feel free to let me know or open a bug if you run into any issues.\n", "@saudet Thanks for the info on using Bazel. I too am new to it and did not realize it was capable of generating a `.so` in that manner.\n\n@davidzchen Thanks for the addendum about using a `cc_library`, I modified the example from @saudet accordingly when I implemented [my Bazil wrapper build](https://github.com/kylevedder/tensorflow/blob/master/tensorflow/core/java/wrapper/BUILD). Also, thank you for the input regarding the directory structure; I have updated my folder structure to align with your suggestions.\n\nAdditionally, I was not very clear in my previous comment about generating `.so` files; while my objective is to generate a `.so` file from the original source, I also want to include [the `.cxx` file that SWIG generates](https://github.com/kylevedder/tensorflow/blob/master/tensorflow/core/java/wrapper/tensor_c_api_wrap.cxx)  inside of the `.so` in order to facilitate the JNI calls. Currently, I'm running into an issue in which I cannot get the SWIG generated `.cxx` file to compile; it's trying to reference `JNI.h`, a header located in `$JAVA_HOME/include/`, but I cannot seem to get Bazel to understand the external include path.\n", "@davidzchen Hum, nope, `cc_library` doesn't work. I don't see any other way to make Bazel pass the `-shared` option to the compiler: http://bazel.io/docs/be/c-cpp.html.\n", "@saudet I don't think you need to pass `-shared` yourself. `cc_library` should be building a `.so` by default. Does that work for you?\n", "@kylevedder You won't be able to add the JNI headers that way since they're outside the workspace. However, Bazel includes the local JDK as a [local repository](http://bazel.io/docs/be/workspace.html#local_repository) and provides a number of built-in targets (see [`jdk.WORKSPACE`](https://github.com/bazelbuild/bazel/blob/master/src/main/java/com/google/devtools/build/lib/bazel/rules/java/jdk.WORKSPACE) and corresponding [`jdk.BUILD`](https://github.com/bazelbuild/bazel/blob/master/src/main/tools/jdk.BUILD)) you can use to depend on local JDK. These are included in each Bazel workspace by default.\n\nBazel itself uses JNI and interfaces with the local JDK this way (see [`src/main/native/BUILD`](https://github.com/bazelbuild/bazel/blob/master/src/main/native/BUILD)). In this BUILD file, there are two `genrule`s to copy the JNI headers and a `cc_library` target for the library it is building that uses JNI that depends on the headers, and a `includes = [\".\"]` so that the C++ code can include the JNI header with `#include <jni.h>`. This is currently not documented because we are working on a number of improvements to the external repository mechanism, and the `@local-jdk` name might change, but we can use it for TensorFlow and any other Bazel project that uses JNI in the meantime.\n\nHere is a patch for your BUILD file that adds the `genrule` targets for copying the JNI headers you need and some changes to the `cc_library` target to set up the right dependencies, namely:\n1. Add `jni.h` and `jni_md.h`, which are copied to the current package by the `genrule`s to `srcs`\n2. Add a dependency on `//tensorflow/core` so that you can include the headers under `tensorflow/core/public`. Note that, headers or any source file in a separate directory are in a separate package from Bazel's point of view, and you will need to add a dependency on the build target that contains those files.\n\n``` diff\ndiff --git a/tensorflow/core/java/wrapper/BUILD b/tensorflow/core/java/wrapper/BUILD\nindex 72b4076..04a3394 100644\n--- a/tensorflow/core/java/wrapper/BUILD\n+++ b/tensorflow/core/java/wrapper/BUILD\n@@ -7,10 +7,30 @@ exports_files([\"LICENSE\"])\n load(\"/tensorflow/tensorflow\", \"tf_copts\")\n load(\"/tensorflow/tensorflow\", \"tf_gen_op_wrappers_cc\")\n\n+genrule(\n+    name = \"copy_link_jni_md_header\",\n+    srcs = [\"//external:jni_md_header-linux\"],\n+    outs = [\"jni_md.h\"],\n+    cmd = \"cp -f $< $@\",\n+)\n+\n+genrule(\n+    name = \"copy_link_jni_header\",\n+    srcs = [\"//external:jni_header\"],\n+    outs = [\"jni.h\"],\n+    cmd = \"cp -f $< $@\",\n+)\n+\n cc_library(\n     name = \"java_wrapper\",\n-    srcs = glob([\"*.cc\",\"*.cxx\",\"*.h\"]),\n-    copts = [\"-I$$JAVA_HOME/include/\", \"-I$$JAVA_HOME/include/linux/\"],\n+    srcs = glob([\"*.cc\", \"*.cxx\", \"*.h\"]) + [\n+        \":jni.h\",\n+        \":jni_md.h\",\n+    ],\n+    includes = [\".\"],\n+    deps = [\n+        \"//tensorflow/core\",\n+    ],\n     visibility = [\"//visibility:public\"],\n )\n```\n\nNote that in general, compile actions in Bazel are run from the root of the source tree, and you would need to change the includes in your SWIG file as follows and then re-generate the C++ files so that they will have the correct includes as well:\n\n``` diff\ndiff --git a/tensorflow/core/java/wrapper/tensor_c_api.i b/tensorflow/core/java/wrapper/tensor_c_api.i\nindex d08b571..9ab1fa1 100644\n--- a/tensorflow/core/java/wrapper/tensor_c_api.i\n+++ b/tensorflow/core/java/wrapper/tensor_c_api.i\n@@ -1,8 +1,8 @@\n %module tensor_c_api_module\n %{\n-#include \"../../public/tensor_c_api.h\"\n+#include \"tensorflow/core/public/tensor_c_api.h\"\n %}\n-%include \"../../public/tensor_c_api.h\"\n+%include \"tensorflow/core/public/tensor_c_api.h\"\n %include \"stddef.h\"\n```\n\nOnce this works, you would have the JNI build set up for Linux since the `copy_link_jni_md_header` `genrule` only copies the Linux-specific header. To have it copy the correct platform-specific JNI header, we would need to do the following:\n1. Set up `cpu` `config_setting`s for other platforms. Currently, tensorflow has a `config_setting` for `--cpu=darwin` in [`tensorflow/python/BUILD`](https://github.com/tensorflow/tensorflow/blob/04f1932f053dd7865b191719b33860270461943a/tensorflow/python/BUILD#L17). We should probably move that a more appropriate package such as `//tensorflow/core`. Basically, we would want the same set of `config_setting`s as Bazel (see [`src/BUILD`](https://github.com/bazelbuild/bazel/blob/master/src/BUILD#L135)).\n2. Have `copy_link_jni_md_header` copy the right JNI header based on which config setting is set using `select()`, similar to [the one in Bazel](https://github.com/bazelbuild/bazel/blob/master/src/main/native/BUILD#L1). Our `genrule` would look something like the following:\n\n``` python\ngenrule(\n    name = \"copy_link_jni_md_header\",\n    srcs = select({\n        \"//tensorflow/core:darwin\": [\"//external:jni_md_header-darwin\"],\n        \"//tensorflow/core:darwin_x86_64\": [\"//external:jni_md_header-darwin\"],\n        \"//tensorflow/core:freebsd\": [\"//external:jni_md_header-freebsd\"],\n        \"//conditions:default\": [\"//external:jni_md_header-linux\"],\n    }),\n    outs = [\"jni_md.h\"],\n    cmd = \"cp -f $< $@\",\n)\n```\n\nI'd be happy to help you with this if you run into any issues. Let me know if this works for you.\n", "@davidzchen cc_library generates a bunch of .a files, but no .so file. I'm using 0.1.0 as was previously recommended for TensorFlow... Maybe it's fixed in 0.1.1? I'll have to try again.\n", "@davidzchen Thank you very much for your help. I have followed your instructions and updated both the Java wrapper [`BUILD` file](https://github.com/kylevedder/tensorflow/blob/master/tensorflow/core/java/wrapper/BUILD) as well as the [SWIG `.i` file](https://github.com/kylevedder/tensorflow/blob/master/tensorflow/core/java/wrapper/tensor_c_api.i) as you suggested. Additionally, I moved the wrap script from `core/java/wrapper` to the root directory and updated the links accordingly.\n\nFor now, I have skipped the generalization the `genrule` for the `jni_md.h` file, instead focusing on trying to get `libtensorflow.so` built. Unfortunately, it appears to me as though `libtensorflow.so` is not being generated; I ended up searching my entire file system for anything named some variant of \"libtensorflow\" and nothing relevant appeared. It may be named differently or this may be a simple case of user error. Additionally, there is a possibility that it may be related to the issue that @saudet is experiencing with the `cc_library` rule for `.so` generation.\n\nOnce again, thank you for all of your help, I really appreciate it.\n", "Sorry, it turns out I was wrong. In order to build a `.so` that includes the transitive dependencies, what @saudet did using `cc_binary` with `linkshared = 1` and `name = \"libtensorflow.so\"` was correct. From [the `cc_binary.linkshared` documentation](http://bazel.io/docs/be/c-cpp.html#cc_binary.linkshared):\n\n> Create a shared library. To enable this attribute, include linkshared=1 in your rule. By default this option is off. If you enable it, you must name your binary libfoo.so (or whatever is the naming convention of libraries on the target platform) for some sensible value of foo.\n\nThe main difference between the `.so`'s built by `cc_library` targets and the `.so` built with `cc_binary` using the method described above is that the `cc_library` artifacts only contain the code in `srcs`. This is why building `cc_library` targets with no `srcs` and only `deps`, such as `//tensorflow/core`, do not produce any artifacts. On the other hand, `cc_binary` targets will link in all the transitive dependencies.\n\nI apologize for the confusion. Perhaps we should improve our documentation and add an example on building `.so`s.\n", "I guess you should follow those steps to build Tensorflow and all it's dependencies. We are working on porting TensorFlow to node.js, and I've implemented a shell script to compile and getter only essential sources from the whole repo:\nhttps://github.com/node-tensorflow/node-tensorflow/blob/1.0.0/tools/install.sh#L233-L282\n", "@davidzchen Thank you for the information regarding the creation of a `.so`. I have updated my setup accordingly and I have created a [`tensorflow/core/java/wrapper/example`](https://github.com/kylevedder/tensorflow/tree/master/tensorflow/core/java/wrapper/example) with an _extremely_ basic tester to prove that JNI function calls to the `.so` work. Note that [`createWrapper.sh`](https://github.com/kylevedder/tensorflow/blob/master/createWrapper.sh) must be run prior to running [`compileAndRun.sh`](https://github.com/kylevedder/tensorflow/blob/master/tensorflow/core/java/wrapper/example/compileAndRun.sh).\n\nI will try to improve the SWIG wrapper and make a better example, the one I have now is simply a bare minimum proof of working bindings.\n\nFinally, I want to thank @davidzchen and @saudet for all of their help; I would not have been able to do this without them.\n", "Nice! Thanks for working on this, @kylevedder!\n\nIf you're interested, I can try integrating your `createWrapper.sh` and `compileAndRun.sh` scripts into the Bazel build by 1) creating Skylark SWIG rule and 2) using [Bazel's Java rules](http://bazel.io/docs/be/java.html) to build the Java code.\n", "@davidzchen That would be great! I will work on improving the SWIG wrapper and the base example.\n", "I've finalized the presets for JavaCPP and ported the `example_trainer.cc` sample:\nhttps://github.com/bytedeco/javacpp-presets/tree/master/tensorflow\nLooking forward to compare this with an equivalent wrapper using SWIG!\n", "Looks like the API link is broken: http://bytedeco.org/javacpp-presets/tensorflow/apidocs/\n", "@verdiyanto Sorry, I don't have CI yet, but uploading the API docs is easy enough, so I've at least done that. Enjoy!\n", "@saudet Nice work on the JavaCPP presets!\n\nAn update on my work: I have done some more work on the SWIG wrapper, and you can see the work I've done [here](https://github.com/kylevedder/tensorflow/tree/master/tensorflow/core/java/wrapper). However, I am at a bit of a cross roads and I am not sure of the best way to proceed.\n\nI'm rather new to SWIG, given this is my first major project using it, so I read the SWIG documentation on [SWIG Basics](http://www.swig.org/Doc3.0/SWIG.html) and on [SWIG and Java](http://www.swig.org/Doc3.0/Java.html) which run through how SWIG works and how to wrap C/C++ with SWIG Java wrappers.\n\nThe documentation explains how SWIG converts pointers in C/C++ into opaque Java objects, which is why you get classes like `SWIGTYPE_p_void` generated by SWIG. The issue is there is not an easy way to convert POJOs into these SWIG classes. \n\nSo, for example, in [`tensor_c_api.h`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/public/tensor_c_api.h), the C method `TF_CreateTensor()` takes a `void*` which points to the input data and a `size` parameter to specify the size of the input data in bytes. This is a perfectly reasonable design pattern for C/C++, but completely nonsensical in Java. The SWIG generated Java method `TF_CreateTensor()` takes a `SWIGTYPE_p_void` object as its data, along with `size`, but there is no way to convert a POJO such as a `String` into a `SWIGTYPE_p_void` without handwriting a lot of code.\n\nAnd this is the crossroads at which I currently lie: I either write a ton of C/C++ conversion methods which take any type defined in `TF_DataType` and convert to a `void*`, or write a bunch of SWIG typemaps to do the same thing. The SWIG documentation does not seem to favor either solution, as they do both seemingly interchangeably. \n\nSo, the question is, C/C++ conversion functions or SWIG typemaps?\n", "@kylevedder I see you're starting to understand why I created JavaCPP in the first place. :)\n", "I've been using @saudet's JavaCPP presets, extremely useful, thanks! I'm using it to build a Clojure interface to tensorflow. \n\nSome comments:\n\na) There is opportunity for simplification / a higher-level layer\n\nA lot of the JavaCPP api replicates protobuf functionality that can be directly achieved on the JVM, without the bridge. Took me a bit to realize this, but one is simply building up a protobuf object using the JavaCPP bindings, producing this platform-independent representation using interop, and then stuffing it into the Session.\n\nI've ended up just using jvm-based protobufs to build the graph directly, bypassing the JavaCPP constructor functions. This has several advantages -- a simpler api to program against, and also nice .toString format that shows the human-readable protobuf. \n\nParticularly for Clojure, its much easier to describe the tensorflow graph in terms of data structures and then convert directly them to protobuf, than it is to look up and invoke a constructor function for each node in my data structure.\n\nb) Building and package improvements\n\nI'm not expert in building native code, or in the build tools used in these projects. It would be great to have maven-ized artifacts; in particular if they also included the generated java protobuf classes. It took an embarrassing amount of time for me to figure out how to do this. \n\nc) It would be useful to have a small number of graph test cases to target. \n\nRight now my methodology is somewhat cumbersome: Use the JavaCPP constructor functions to generate a graph, mashall it into my JVM protobufs and see the human-readable form, and figure out how to build my own constructors to make the same form. \n\nIt would be useful to have a small collection of very simple graphs that exercise the core functionalities of TensorFlow, so people like me have a reasonable set of test cases to target for interop to different languages. \n\nAnyway thanks for everyones efforts and keep up the good work!\n", "@kovasb Thanks for the feedback! Obviously, there is much to be done to make the interface more natural to Java, Scala, Clojure, etc.\n\nIf you have helper classes to integrate the C++ API with the Java protobuf API, feel free to put all that in the following package, including the generated Java protobuf classes themselves, and send a PR:\nhttps://github.com/bytedeco/javacpp-presets/tree/master/tensorflow/src/main/java/org/bytedeco/javacpp/helper\nThat's what it's meant for, and it will automatically get packaged in the Maven artifact, something that Bazel doesn't appear to support. In any case, thanks for looking into this!\n", "@kovasb A clojure interface sounds really interesting. Got any code to share yet?\n\nThanks!\n", "So people in this thread are aware also, in https://github.com/tensorflow/tensorflow/issues/3 has been raised: automatic differentiation doesn't currently work unless you use TF from the python api. This seems like a showstopper pending that functionality being ported to C++.\n\nI don't quite understand the data flow but perhaps it is possible to launch the python helper stuff together with the C++ lib?\n\nAnother solution I'm looking at is just using Jpy or one of the other bridges (anyone have recommendations?) JyNi also looks quite interesting but pretty far from primetime (though it would be great to see more momentum/community behind it)\n\nIf JyNi gets sorted out, it + jython would give the JVM a really awesome story re python ecosystem interop. One can dream. \n", "+1 for a Java interface!\n", "if we could use javaCPP, is SWIG still necessary? shall we collaborate to implement the SWIG interface?\n", "@maxiwu I like to think that JavaCPP does a better job than SWIG, but I'm all for comparing them to actually proove it :)\n", "@kovasb I'd be very much interested in helping/contributing to the Clojure interface. \n", "@sorenmacbeth email me at my first-name dot last-name at gmail, happy to walk you through what I have... \n", "Seems that we have here a quite complete Javacpp preset. Is it an acceptable solution for \"the team\"?\n", "@saudet I'm trying to build a copy of the JavaCPP wrappers, but it seems that due to the rapid rate of change of the tensorflow source they are not compatible with either the 0.6.0 release or today's master branch. Would it be possible to update them with a pointer to the exact tensorflow commit/version they were tested with?\n", "@nikitakit I've just made an update for the master branch here: https://github.com/bytedeco/javacpp-presets/commit/43bdcdf03beaaddb4bd5badf5d4f79669e9e78dd\n\nUnlike Caffe though, TensorFlow actually seems to get a release every month or so, so I guess I'll start stabilizing the bindings at those points, starting with the next release (0.7.0?)\n", "@martinwicke What do you think?\n", "When there's a stable Java binding, I'm happy to work on the Scala API.\n", "/cc @databricks\n", "@kovasb I think I missed this first time through. Are you saying that all the nice auto-differentiation magic that we get from using TensorFlow through python is implemented within python, not within the c++ libs? So in practice, a Java API would either need to re-implement all this, or would be just another numerics library? I'm not familiar enough with the internals of TensorFlow or the python glue to understand exactly what heavy lifting is done where.\n", "@drdozer that's my understanding, based on comments by @girving and then looking the source a bit myself. Reimplementing stuff in Java seems like a nonstarter. I suggest looking at the comments in #3 \n\nIf someone is really interested I would just recommend trying to do some training examples using the Java api (so far I've just seen/done the forward path). \n", "I wonder how far we'd get running the Python code with Jython...\n", "I believe the Python API layer has lots of logic that the C++ layer API does not expose.\nI was trying to follow JavaCpp path but at the end there will be lots of duplication code and will be hard to maintain consistencies when something change in the Python implementation.\n\nProbably easier path is to use Jython as @saudet had mentioned before ...\n", "It was assigned in https://github.com/tensorflow/tensorflow/issues/476 to  @josh11b. If he is working on this doesn't make sense to use Jython. \n", "If we use jython will the c++ code still work?  I am looking to use this for a server that's in Java but I'm stuck between trying a Java route directly or just sending the data over a socket to a Python process\n", "I'd like to mention that although the Java API doesn't include a lot of features like auto-differentiation, I haven't found this to be a barrier for my own work. I've had great success generating a model in python, serializing it to a .proto file, and then opening it through the Java wrapper for training. The same can be done for test-time, since I believe the Saver functionality is available through the C++ and Java APIs.\n", "+1\n", "@saudet\nThanks for creating javacpp and the presets for tensorflow.  I was able to successfully recreate a Python graph in Java, but I am stuck trying to restore from a saved model file.  This line doesn't work:\n\nTensor fn = new Tensor(tensorflow.DT_STRING, new TensorShape(1));\n        CharBuffer buffer = fn.createBuffer();\n                buffer.put(\"modelfile.tf\");\n                session.Run(...);\n\nbut the CharBuffer turns out to be NULL.  If I change DT_STRING to DT_FLOAT, I get a FloatBuffer, but DT_STRING doesn't seem to work.\n\n @nikitakit  you said you got this to work.  could you share your code?\n", "@lakshmanok \n\nEDIT: sorry, misread what you said here. I can't provide any help for using external savers from Java\n\nFor reference, the part of my code that imports tensorflow graphs is here: https://gist.github.com/nikitakit/d3ec270aee9d930267cec3efa844d5aa\n\nIt's in Scala, but porting to Java / other JVM language should be straightforward.\n\nMy code for actually running nodes in the graph is unfortunately heavily tied up with a Scala framework I'm using, so you'll have to rely on the tensorflow API docs for this part.\n", "Has anyone got anywhere with embedding the python tensorflow environment in the jvm? Say with jython + JyNI? Or is this all a bit too experimental to get to work reliably?\n", "I am currently working on expanding the C API to add support for graph definition.  Not sure when it will be done, but it is one of our goals before 1.0.\n", "I am working on using tensor flow from java. I am approaching the problem by using jython and modifying the tensor flow cpython library to accomodate another python interpreter.  the cpython should keep working flawlessly and my code is detecting if the interpreter is Jython and modifying the imports / modules to allow it to work. Underneath it uses the javacpp bindings for libtensorflow_cc.so. Is this something the google team would be open to have in the official repo ? @vrv \n", "That seems like a nice proof of concept but I think an official binding would probably want to bind more natively than going through Python :(\n", "no, instead of calling the c-python wrapper, we call the javaccp wrapper. So it would be the same thing as cpython tensor flow but evaluated from the JVM using Jython. Reimplementing the whole python logic in another language seems too much, you end up with another API.  javacpp bindings allow you to run Inference without problem but the model has to be built/trained from a cpython script at the moment.  \n", "Has anyone looked at making tensorflow work with Kotlin?  It seems a more natural fit and it's still 100% java at the end of the day.  I find the Kotlin language to be a very nice middle ground between python and pure Java.\n", "Update: I was able to successfully get things going with javacpp (thanks to @saudet ) and have Java programs read/execute TensorFlow models.\n\nhttps://medium.com/google-cloud/how-to-invoke-a-trained-tensorflow-model-from-java-programs-27ed5f4f502d#.tx8nyds5v\n", "Thanks @lakshmanok and @saudet . The `javacpp` project seems to implement most TensorFlow APIs. We're trying to run the [tensorflow/serving](https://github.com/tensorflow/serving) in Java.\n\nThe API is simple and defined by `protobuf`. Now we have implemented the server and want the implement the client in Java. It just need to construct the `TensorProto` in Java and invoke the `gRPC` call. TensorFlow has provides helper functions to convert multiple dimention arrays for Python and C++, but not Java.\n\nCan you tell how to use `javacpp` or implement by ourselves for this?\n", "What you are looking for is probably already in https://github.com/bytedeco/javacpp-presets/blob/master/tensorflow/src/main/java/org/bytedeco/javacpp/helper/tensorflow.java but let me know if something is missing there. Thanks!\n", "is this still being worked on? is there an official github repo for this porting project? I see a couple of random repos, but can't tell. \n", "Yep, but probably sometime in October/November. We're using the C API instead of SWIGing to the C++ API. In the meantime, you can use the bindings that saudet mentioned.\n", "how did you come to the conclusion to use the C API? we are working on a\nruby interface using swig:\nhttp://github.com/somaticio/tensorflow.rb\n\nOn Tue, Sep 13, 2016 at 6:22 PM, Jonathan Hseu notifications@github.com\nwrote:\n\n> Yep, but probably sometime in October/November. We're using the C API\n> instead of SWIGing to the C++ API. In the meantime, you can use the\n> bindings that saudet mentioned.\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5#issuecomment-246844192,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAA5v3g86Z6D1rz-aTGdMyMWnQZhrZUYks5qpyIJgaJpZM4Getd8\n> .\n", "Going forward, we'd prefer that all language bindings use the C API. A doc is forthcoming.\n\nYou can see example usage here:\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/go\n\nThere's no urgency, though, and building on top of SWIG is fine for now.\n", "@jhseu Does that mean that the C API will be expanded to cover all that the Python bindings currently have access to?\n", "Wow, big change.  Wish this was decided earlier on.  Anyway to see the docs\nsooner?\n\nOn Wed, Sep 14, 2016 at 5:56 PM, Samuel Audet notifications@github.com\nwrote:\n\n> @jhseu https://github.com/jhseu Does that mean that the C API will be\n> expanded to cover all that the Python bindings currently have access to?\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5#issuecomment-247167887,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAA5vwfBJoZC2s33_7E9Xy6-NYNUjHjnks5qqG2FgaJpZM4Getd8\n> .\n", "@saudet Most functionality, except in the short term it'll be missing some things (like gradients, optimizers).\n@jtoy There's no urgency for you to migrate. SWIG will continue to work for a while.\n\nThe docs just describe how to do it and naming conventions. You can start migrating to the C API without them, though:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.h\n", "Thanks @saudet . I have found this in [stackoverflow](http://stackoverflow.com/questions/39443019/how-can-i-create-tensorproto-for-tensorflow-in-java) about generating `TensorProto` with pure protobuf API. And here is the [example code](https://github.com/tobegit3hub/deep_recommend_system/blob/master/grpc_service/java/src/main/java/com/tobe/GenericInferenceClient.java) of TensorFlow serving gRPC Java client.\n", "@tobegit3hub Nice, if you can make this work with the C++ API, please add it to the helper package of the JavaCPP Presets and send a pull request! This guy would be interested in something like that: https://github.com/bytedeco/javacpp-presets/issues/240\n", "@girving Does javacpp solve the problem already?\nI want to contribute to tensorflow java api , I prefer implement it like python.\n", "Hi guys, did somebody already start working on the Java/Scala language bindings using the C API ?\n(instead of building on top of SWIG)\n", "I have a working Java/Scala interface to tensorflow using only the C API via [JNR](https://github.com/jnr/jnr-ffi). Unfortunately, I don't yet have permission to open source it. I will post here if and when I release it. It is still a work in progress, but it's very functional.\n", "@jdolson Does the API you expose accept TensorFlow's protocol buffer objects? One of the biggest issues I've had using the javacpp presets from @saudet is that when you are manipulating tensor objects in Java client code you're dealing with a org.tensorflow.framework.TensorProto which is generated by the protocol buffer compiler when configured to output java. But in the TensorFlow API wrapper you are dealing with a org.bytedeco.javacpp.tensorflow.TensorProto.TensorProto which is generated by javacpp when pointed at the c code generated by the protocol buffer compiler when configured to produce C. Since the types aren't the same you can't directly use your java code's tensors when calling the wrapped TensorFlow API.\n", "@Intropy Yes, I compile all the tensorflow `*.proto` sources to Java source code with `protoc` and use those classes in the API.\n", "@jhseu Is the C API interface still on track to be released sometime within November? If not, what's the current status?\n", "@eaplatanios : The C API is mostly stable (and will be officially so by 1.0) and usable though not complete (still missing the ability to automatically gradient computations to the graph). A doc describing how the C API can be used to build language bindings is at https://www.tensorflow.org/how_tos/language_bindings/index.html\r\n\r\nThe [Go API](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go) was implemented using the C API as a first example of following the above document.\r\n\r\nWe hope to have the Java bindings be built on top of this as well (using JNI) and have started exploring that a bit. Any comments/learnings folks have based on using @saudet 's wonderful work with getting JavaCPP working would be nice to know about.", "I do have a few suggestions based on using JavaCPP bindings.\r\n\r\nFirst, since protocol buffers compile directly to java, the java versions should be used. Preferably I think that the protocol buffers that take part in the API should be separately available as a maven module and should come with the proto definitions so that people on a Java stack have an easy way to get the definitions as binary as well as an easy way to get the proto definitions for inclusion within other proto definitions.\r\n\r\nSecond, it would be helpful to find the minimum version of libc that TensorFlow needs and build against that.\r\n\r\nThird, it is much easier to use a thoughtfully designed API than an automatically generated one. I know that that's obvious and kind of sounds like a shot at JavaCPP. I don't mean it to be. I'm really glad the automatically generated interface exists. It _is_ usable. But it requires odd circumlocutions, it has a lot of warts, and it's pretty hard to read the code to figure out how to do what you're trying to do. I wish this suggestion was more helpful than \"you should make it good\", but I guess the point is that look how different the C++ API and the python API are. Both are straightforward because they fit their environment in a way that automatically converted code is unlikely to match.", "It would have been maybe nicer to support C backend of Swig and generate TF C API via Swig as well: https://github.com/swig/swig/issues/800 so that other languages like Go, Ruby, R can use the C api to write their own bindings.", "We have an existing C API for adding support for any language with a C FFI:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.h\r\n\r\n(And that is what is used to build the Go, Java, Rust etc. bindings for TensorFlow)", "Could the C API be accessed using [JNA](https://github.com/java-native-access/jna)?", "@jhseu I meant, it could have been maybe generated from C++ API earlier, before manually implementing the C API. ", "@Quantum64, [here](https://github.com/mskimm/tensorflow-scala) is a Scala binding of tensorflow that uses JNA.", "Since this issue still open, how does\r\n  `https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java`\r\nbeing implemented and what was the PR for the commit?", "@hsaputra : Could you elaborate on what you're looking for? There are multiple commits that contribute to the code in [`tensorflow/java`](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java), most of which are referenced in this issue (such as 2b1cd28, d73a266 and many others in between)", "HI @asimshankar , thanks for the reply.\r\n\r\nI am just wondering what was the path that `tensorflow/java` take to implement Java API since this ticket is not closed.\r\nThere were discussions about using JavaCPP vs SWIG vs call via Jython.\r\n\r\nSeemed like the `tensorflow/java` is implemented with direct JNI to call C APIs instead?\r\n", "Correct.", "Hey,\r\n\r\nI just got these Swig bindings working yesterday. I have a request for an API change. Currently in order to generate Tensors reflection is required and the format of the arrays are a bit unweildy, as they require the use of n-dimensional native Java arrays. Can we keep this interface, but also add some methods for creating tensors that require 1-dimensional arrays and specifying the shape using another array of long? I imagine it could look something like this:\r\n\r\n```\r\ndouble[] matrix = {1.414, 2.718, 3.1415, 3.4, 56.7, 89.0};\r\nlong[] shape = {2, 3};\r\n\r\n// add a method for each primitive type\r\norg.tensorflow.Tensor tensor = org.tensorflow.Tensor.createDouble(matrix, shape);\r\n```\r\n\r\nThis would also lead to the possibility of create int8, int16, uint8, uint16, uint32 tensors as well, which will help with compatibility.\r\n\r\nShould I make this an issue? Or is it ok here?\r\n\r\nAlso, more than happy to take a stab at building these methods out.", "@hollinwilkins : I'm hoping PR #6577 addresses this, with just a slight tweak to your proposed factory method:\r\n\r\n```java\r\nTensor tensor = Tensor.create(shape, DoubleBuffer.wrap(matrix));\r\n```\r\n", "@asimshankar This is great! Thanks for the quick reply. It looks like it's pretty close to being merged too :+1:", "I'm trying to use the new java API, and I've come across some things that make it harder to use than I think it ought to be:\r\n\r\n1. The java API should accept a GraphDef object. Currently it only accepts a byte array representing the serialized binary of the GraphDef protocol buffer. It's odd to require a serialization/deserialization step at the library boundary.\r\n2. Session.Runner.feed should be able to accept org.tensorflow.framework.TensorProto or there should be a  good way to create org.tensorflow.Tensor from org.tensorflow.framework.TensorProto.\r\n3. Session.Runner.run returns a list of Tensor objects. Similar to above there should be an easy way to get TensorProto output either directly or by giving org.tensorflow.Tensor a good way to convert to TensorProto.\r\n4. Session.Runner.run swallows Status. There should be a way to get that out information about failures, perhaps through throwing an exception.\r\n\r\nAlso, it's possible I missed the way to handle this, but it looks to me like I can't get all supported tensor types in the output from run. For example if my output tensor is of dtype INT16, then there's no way to extract the value from it. There's no Tensor.shortValue or the like, and Tensor.intValue seems to rquire an exact match. I'm basing this on reading DEFINE_GET_SCALAR_METHOD in tensor_jni.cc.", "@Intropy : Thanks for your comments and they definitely make sense. For now I can share some quick thoughts with you:\r\n\r\nRE: protobufs: At this point we're trying to keep the core API independent of protobufs for a number of reasons (including use on resource restricted systems where something like [nanproto](https://github.com/google/protobuf/tree/master/javanano#nano-version) may be more appropriate). So, that's the reason why we have been hesitant, but it's something we're thinking about and suggestions are appreciated. One possibility is to have all the protobuf related functionality in a separate package so that there is a clear separation.\r\n\r\nSo, going back to your points:\r\n\r\n1. See above. Though, I'd wager that there are many cases where the `byte[]` makes more sense (such as reading the graph from a file or network channel)\r\n\r\n2. Point taken\r\n\r\n3. See above.\r\n\r\n4. `Session.runner.run` should *not* be swallowing status. If there is an error, an exception will be thrown ([`session_jni.cc:166`](https://github.com/tensorflow/tensorflow/blob/f074dc8/tensorflow/java/src/main/native/session_jni.cc#L166)). If that is not happening, please do file a bug.\r\n\r\nYou are right, not all types are supported yet, but should be easy enough to add. If you have a pressing need for the missing types, please feel free to file an issue and/or send in a PR. Contributions are welcome :)\r\n", "@asimshankar Thanks for your thoughts.\r\n\r\nRegarding the first point, it's not really a big deal. As you say there are times where a byte[] makes the most sense. In my own use case I have an InputStream, which is trivial to convert to byte[]. The protocol buffer API makes conversion straightforward. I just consider the byte[] a wart on the API because you're going to have to deserialize anyway (in TF_GraphImportGraphDef) and this way you lose some type safety. There's also proto3's json serialization to consider.\r\n\r\nOn swallowing status, you're right. I missed the unchecked exception.\r\n\r\nThe most obvious way to handle 2 and 3 is to give org.tensorflow.Tensor a factory that converts from a TensorProto and some toTensorProto(). If resource limited use cases is the issue with protocol buffers, then people in those circumstances could simply not use those functions. The problem is that people who do use those functions would be paying the cost of a conversion that could likely be avoided by having the Tensor store its data directly in a protobuff. I've never worked with jni before, so I'm having trouble following how the data are stored, but it looks like it's essentially treating nativeHandle like a pointer to a TF_Tensor which has a TensorBuffer that is treated essentially like a sized void*.", "Could we break up this issue and file separate issues for each feature in the Java interface? It'll make easier to track/parse, then we can close this issue.", "@drpngx : My intention is to get a couple more changes in (reading tensors from buffers) before closing this out like we did for Go and having features/bugs be filed individually. So hopefully soon.", "Sounds good, thanks!", "Alright, seems like we have enough of a base to build on (e.g., enough to build the [LabelImage example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java/src/main/java/org/tensorflow/examples) and folks are filing more specific bugs/feature requests.\r\n\r\nI'm going to close this issue out. There is still much to do in the Java API, but let's discuss/track those in separate issues. Thanks!", "@asimshankar we are in the process of selecting deep learning framework (mxnet/tf) and our etl/api are based on spark/akka flow...Is there a plan to add distributed_runtime support to Java API to run model parallel training using ps nodes ? ps node is critical to us for many use-cases...javacpp presets may be easier to export for the first cut since the C API itself does not seem to have distributed_runtime in it...", "@debasish83 : Including the distributed runtime by itself is trivial, but there are a bunch of higher level constructs in the Python API like the `Estimator` class that take care of a bunch of things (checkpointing, summary saving etc. that make visualization through TensorBoard trivial) that may make it more suitable to run the training jobs in Python.\r\n\r\nAll this can be built using the existing primitives in the Java API, but the right approach will depend on your precise needs.\r\n\r\nPerhaps we should sync up off thread?", "@asimshankar Is there a way already from the tensorflow Java binding to retrieve information from a graphDef (built from the .pb file of the graph on disk) like the list of nodes, input and output format or is it an incoming feature? Thanks!", "@asimshankar I'm not sure to understand what is missing to do training with TF Java. Is it a problem of the numeric library (missing numpy)? I mean if you are not interesting in TensorBoard dataviz, but training only, using a native Java numeric library, why to use python only for training (as you suggest about the `Estimator` class)?\r\n\r\nThanks.", "What is the state of training models in Java? I have been thinking of writing an ImageJ (popular and free image analysis suite) plugin to apply approaches such as https://arxiv.org/pdf/1505.04597.pdf (recently wildly popular in image segmentation for cell tracking and biomedical applications). I think it would be useful to provide a range of pre-trained models and enable users to refine these for their specific use case. I have been looking into DL4J for this purpose. Are there any concrete plans to allow fitting in the TF Java bindings?", "@bergwerf : Training in Java is certainly possible, if not particularly convenient.\r\nYou can find a sample at https://github.com/tensorflow/models/tree/master/samples/languages/java/training \r\n\r\n(Also, I'm sure you're aware, but see also https://imagej.net/TensorFlow)", "Oh, awesome! My information must be outdated then ;-). I thought I had read\nsomewhere the Java API was only intended for predicting with pre-trained\nmodels. I will look into the example.\n\nOn Wed, Mar 28, 2018, 22:01 Asim Shankar <notifications@github.com> wrote:\n\n> @bergwerf <https://github.com/bergwerf> : Training in Java is certainly\n> possibly, if not particularly convenient.\n> You can find a sample at\n> https://github.com/tensorflow/models/tree/master/samples/languages/java/training\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5#issuecomment-377015867>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEQJ1UD9-xACQAII5996ees_UFJ_NzL-ks5ti-wSgaJpZM4Getd8>\n> .\n>\n", "@asimshankar that's awesome \ud83d\udc4d \ud83d\udcaf \ud83e\udd47 , I'm going to add to my repo https://github.com/loretoparisi/tensorflow-java"]}, {"number": 4, "title": "Installation over pip fails to import with protobuf 2.6.1", "body": "Pip freeze reports the following protobuf version\n\n```\nprotobuf==2.6.1\n```\n\nHowever, upon importing I get the following error\n\n```\nTraceback (most recent call last):\n  File \"/home/panmari/PycharmProjects/tensor_stuff/stuff.py\", line 150, in <module>\n    import tensorflow as tf\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 4, in <module>\n    from tensorflow.python import *\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 13, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py\", line 16, in <module>\n    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/attr_value_pb2.py\", line 16, in <module>\n    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/tensor_pb2.py\", line 16, in <module>\n    from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/tensor_shape_pb2.py\", line 22, in <module>\n    serialized_pb=_b('\\n,tensorflow/core/framework/tensor_shape.proto\\x12\\ntensorflow\\\"d\\n\\x10TensorShapeProto\\x12-\\n\\x03\\x64im\\x18\\x02 \\x03(\\x0b\\x32 .tensorflow.TensorShapeProto.Dim\\x1a!\\n\\x03\\x44im\\x12\\x0c\\n\\x04size\\x18\\x01 \\x01(\\x03\\x12\\x0c\\n\\x04name\\x18\\x02 \\x01(\\tb\\x06proto3')\nTypeError: __init__() got an unexpected keyword argument 'syntax'\n```\n\nDo I need a newer version of the protobuf package?\n", "comments": ["We definitely depend on protobuf 3.0 (it's what we use as a git submodule). \n\nTo help us debug, can you please try following the instructions to install within a virtualenv here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#virtualenv-based-installation- ?\n", "Got it running by uninstalling the distribution-owned numpy package (?) and reinstalling it from pip. Guess they were interfering in some weird way. Thanks for your help!\n", "got it running by explicitly installing\nsudo pip install -Iv protobuf==3.0a3 \n"]}, {"number": 3, "title": "JVM, .NET Language Support", "body": "Your website says \"... contribute SWIG interfaces to your favorite language ...\". It will be good if yo have pure implementations in other languages like JVM and .NET without having to use SWIG.\n", "comments": [" Since raw performance is one of the goals of this library I think that pure implementations don't make too much sense.\nA prettier API on top of the generated interfaces would be nice though.\n", "Although we may not pursue this ourselves, the internal formats used for communication (`GraphDef`, etc.) are all platform independent protobuf.  Thus, it is possible to implement all or part of the tensorflow API in Java while preserving communication compatibility with the existing code.  Since a Java version would likely be slower, one useful bit would be a pure inference layer that evaluates graphs but isn't necessarily able to build them; this would allow graphs built in Python and trained in Python / C++ on GPUs to be run from Java servers.\n", "Is there a canonical / repeatable test suite, so language bindings can have a target and level of confidence? Is there a build server? Could there be a build server? \n", "There's a testsuite with fairly good converge, but currently it's mostly Python with a few C++ tests.  There's also a lot of functionality for building graphs that's currently Python only, in particular the automatic differentiation functionality, though that doesn't matter for evaluation of graphs in Java.  There are plans to move this functionality into the underlying C++ in future, at which point Java SWIG bindings would be more useful for creating graphs.\n\nIf someone takes up the Java SWIG challenge, we'd be happy to accept it upstream pending review, etc., at which point it would be part of our continuous testing.  The details of accepting contributions is in flux at the moment, but that will stabilize.\n", "Also perhaps https://github.com/bytedeco/javacpp can be used to generate bindings.\n", "@girving What aspect of automatic differentiation is not available from other languages? Is that just for defining new ops? \n", "If you search for `RegisterGradient` in the python directory, you'll see all the places we define the gradients of various ops.  The gradients are defined by Python code, so they aren't available from pure C++ as yet (we hope to change this at some point).\n", "I don't fully grasp the architecture/order of operations, but could one populate the gradients using python, while still calling into the C++ code via a bridge?\n\nAlternatively: Run the python code once to generate the gradients for every op, serialize all those gradients to a file, and then other runtimes can read them off and register them?\n", "Both of those might work, but I don't know how much of a savings they are compared to doing it the right way and porting the code to C++.\n", "I fully agree, just sounding out a temporary solution so I'm not blocked (I imagine it might be a while before all that code is ported)\n", "If all the functionality is not available in the C++ core then porting will be very difficult and some cases not possible (with respect to a fully functional version). Also I opened this: https://github.com/tensorflow/tensorflow/issues/476\n", "+1 for Java\n", "+1 for java\n", "+1 for Java. \n", "+1 for Java\n", "+1 for Java/Scala \n", "+1\n", "@girving I want to contribute to tensorflow java api,\nI used to implement the PHP frontend of Apache Spark alone.\nWhere to start?\n", "Closing this out as a duplicate of #5 and #18. "]}, {"number": 2, "title": "OSX Yosemite \"can't determine number of CPU cores: assuming 4\"", "body": "``` python\n>>> import tensorflow as tf\n>>> hello = tf.constant('hello, tensorflow')\n>>> sess = tf.Session()\ncan't determine number of CPU cores: assuming 4\nI tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 4\ncan't determine number of CPU cores: assuming 4\nI tensorflow/core/common_runtime/local_session.cc:45] Local session inter op parallelism threads: 4\n```\n\ngot this warning but still works though\n", "comments": ["This should be fixed by https://github.com/tensorflow/tensorflow/commit/430a054d6134f00e5188906bc4080fb7c5035ad5 .\n", "I still got this message.\n", "We haven't updated the binary package until our next binary release -- it is fixed at HEAD if you build from source though.\n", "Okay.\n"]}, {"number": 1, "title": "Add support for Python 3.x", "body": "Currently we only support Python 2.7, but we should support Python 3.\n", "comments": ["Main things this involves: `print -> print()`, handle `__floordiv__` / `__truediv__` / `__div__` correctly.\n", ":+1:  to this issue\n", ":+1:\n", ":+1:\n", ":+1: \n", "We're working on it.\n", "Python 3 is a must have. :+1: \n", "How do we contribute towards python3 support?  Are there any specific tickets open?  Is there a python3 development branch?\n", "@mgcdanny seems they require contributors to sign an agreement first.\nhttps://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md\n", ":+1:\n", "I'm running futurize on the code at the moment; once that's done it'll be easier to parallelize the remaining work.  So far `futurize --stage1` is checked in and I'm working through `futurize --stage2` now (checking each of our divisions very carefully :)).\n\nUnfortunately our contribution process needs a bit of improvement (we're working on streamlining it), but I'll see if there are natural chunks of work to break off and ask for help in fixing once the initial futurize push is done.  Before that it's tricky to parallelize.\n\nThe only non-obvious questions (so far) are what to do with unicode and where.  Cc'ing @mrry since he was taking a look at that.  Most of the \"names\" for things, such as names for ops and names for tensors in a graph, are already restricted to be roughly alphanumeric, so hopefully we should be able to leave them as C++ `string` while still accepting unicode input from Python.\n", "Also, question for people that have done such conversions before / recently: is `six` still the recommended way to make code transparently support both?  In particular, we need stuff like `xrange` and `iteritems` as symbols that exist in both 2.7 and 3.x.\n", "@girving I would follow the guide written by python3 core dev Brett Cannon: https://docs.python.org/3.5/howto/pyporting.html.  He's been heavily involved with the push to move existing libs to python3.\n", "@madisonmay: Thanks, should have asked you earlier.  Modernize probably would have been a better choice, since after reading that page I am essentially taking the output of futurize and then rewriting it into what modernize might have already spit out.  Mostly done with that phase, though, so I'll probably just finish it up using futurize.\n\nFor the record: `futurize --stage` was mostly `print_function` and a few other safe trivialities.  I'll write a detailed comment on what `futurize --stage2` involved for us once I'm done with it (still in the middle of reading diffs).\n", "+1\n", "I think it is pretty clear that there are many people interested in this issue. However, I would suggest users to use the <kbd>Subscribe</kbd> button instead of adding a new :+1:.\n\n![screenshot from 2015-11-10 10-36-53](https://cloud.githubusercontent.com/assets/526577/11058980/2644a5da-8797-11e5-8761-da311f8b3774.png)\n\nThis way developers (and all subscribed users) can get notifications on _useful_ future updates. :wink:\n\n_PS: If you already posted a :+1: you might as well consider removing it to keep the discussion cleaner._\n", "Thank you @Peque!  It would be great if further comments were limited to technical discussions about Python 3 support.\n", "From the README:\n\n> The TensorFlow Python API currently requires Python 2.7:\n> we are working on adding support for Python 3.0.\n\nIf you require 2.7 for the 2.X line, then why restrict yourselves to 3.0 (which was [released](https://www.python.org/downloads/) about the time of 2.6)? If you target 3.3+, then you'll save yourselves a lot of headache, especially if you're using `u'\u3042'` unicode strings. (also see [`unicode_literals`](https://docs.python.org/3.3/howto/pyporting.html#from-future-import-unicode-literals)).\n\nAnd :+1: for [six](https://pypi.python.org/pypi/six) for more complicated things.\n", "Sorry, I meant Python 3, not 3.0 -- was in a rush to fix :P.  I'll fix this today\n", "@goodmami: Yes, we will likely only support 3.3+, possibly 3.4+ (I'm not sure what the differences are, but will look that up).  I also changed the issue title.\n", "I'd really like you support from 3.3 up to the latest release of Python (currently 3.5; then 4.0 in the future). This piece of software is going to be the heart of many things in human life in the future.\n", "@jli05: https://docs.python.org/3/whatsnew/3.4.html makes it look like supporting 3.3 won't be any harder than 3.4, so we should be good to go.  It is possible our internal tests will be run only in 3.4 (and 2.7), but if anything ever breaks we'll be happy to accept patches.\n", "@girving  now this version support python3.4 ??\n", "i look forward to that progress!\n", "We don't support 3.x yet, but we're getting closer.  As of 1d765834110, all our Python files import `absolute_import`, `division`, and `print_function` from `__future__`, and `Tensor` objects have the correct suite of division operators for compatibility with 2.7 and 3.x.  This involved running `futurize` and them inspecting every call to division to see whether it should be `/` or `//`.  I've also scrubbed all of our uses of `xrange` / `range` / `zip` / `map` / similar.\n\nNext steps: set up testing with 3.x, deal with unicode correctly, fix imports for libraries that have moved around, etc.\n", "I want to add TensorFlow to [Kaggle Scripts](kaggle.com/scripts), where we only support Python 3. Do you have an estimated timeframe of getting to Python 3 support (a couple days? a week? a month?).\n", "We're most of the way there: all the hard bits are hopefully done and only build issues and occasional incompatibilities remain.  I don't have a timeline though.\n", "how close is this? if you point me the right direction, I may have the bandwidth to help over Thanksgiving break as well\n", "@benhamner: As of today, we're extremely close!  All of the tests pass, but there are a few build cleanups before it'll be usable outside of my hacked up git repo.  Unfortunately it proved impractical to divide the work up, since it was largely a mechanical process of going through and sticking b's in front of strings and other test-driven development.\n", "@girving: Thanks so much for your efforts to get this done quickly. Super excited to get my hands on TensorFlow!\n", "Any progress ? Do you know, now, if we are weeks or months away from Python 3 support ? \n", "@stonebig: We're very close!  I was waiting on some build cleanups, but those are checked in now and I'm going to try them out tomorrow.  When I left off, all of tensorflow's source files were Python 3 compatible; only build and configure stuff remained. \n", "I know you can do it\uff01\n", "Hi, thank you for your amazing work.\nI guess it will be possible to install TensorFlow with pip3. Will the models.embedding module be available from pip install, or will it be necessary to build it from source?\n", "@Netizen29: We should have a better answer soon about tutorials, models, etc. in pip (see #247).\n\nStatus update: Build fixes for tensorflow proper are in, so we just need to upstream a small BUILD change to protobuf.\n", "As of cdf0dbff784c55b2e599e956148b5c48828435e7, tensorflow should be Python 3 compatible.  Anyone want to try it out and report problems?  I'll leave this bug open for a day or two for that purpose.\n\nNote: To try it out, grab an up-to-date git tree, run `configure`, and enter `/usr/bin/python3` or the equivalent when it asks for the path to Python.\n\nAssuming testing is positive, we should have a 0.6.0 release incorporating Python 3 support and various other features soon.\n", "@girving - successfully built without gpu support on OSX 10.10.5 with Anaconda python3. I tested the MNIST example without problems.\n\nI had no issues installing minus forgetting to change the path on the python package install, \n\n```\nsudo pip install /tmp/tensorflow_pkg/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\n```\n\nto\n\n```\nsudo pip install /tmp/tensorflow_pkg/tensorflow-0.5.0-py3-none-any.whl\n```\n\nThanks a ton!\n", "Thanks @xysmas!  We'll hopefully have a 0.6.0 release out with this and other stuff fairly soon. \n", "Building the wheel fails with Python 3.5 on Linux unless wheel 0.26 is installed. virtualenv is currently installing 0.24 by default it seems.\n\nRelevant issues:\n- wheel 0.24:  [issue 146](https://bitbucket.org/pypa/wheel/issues/146/wheel-building-fails-on-cpython-350b3)\n- wheel 0.25: [issue 148](https://bitbucket.org/pypa/wheel/issues/148/unorderable-types-error-for-python-3)\n\n`pip install --upgrade wheel` before running the `build_pip_package` script works. After that, all is well (tested MNIST example no GPU).\n", "@ixjlyons: That's good to know; cc'ing @ebrevdo and @vrv in case there is something we can do about that when we push out 0.6.0.\n", "We must change [this line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/setup.py#L31) to 'wheel >= 0.26', i believe.\n", "Are there automated tests in place to ensure that Python 3 support isn't broken in the future?\n\nThanks for all your work!\n", "@joshburkart: Unfortunately not just yet, though they are planned.  That is: all of the unit tests do work with Python 3 at the moment, but we are not yet running them automatically.  This will hopefully change soon.\n", "Closing since it seems to work.  Python 3 support will ship with version 0.6.0, which is coming very soon.  Future Python 3 bugs should be filed as separate issues!\n", "Thanks for all your work - looking forward to get to finally test it along with the rest of my environment! :)\n", "milestone?\n", "@thelostscientist: Do you mean when?  This will be part of 0.6.0. \n", "@girving sorry for not being clear, yes!\n"]}]