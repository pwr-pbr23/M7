[{"number": 52297, "title": "TypeError: unhashable type: 'DictWrapper'   \u3010self.model.add_metric\u3011", "body": "I add  my custom metrics  \r\nthe code are as follows:\r\n        def get_accu1(args):\r\n            y_true, y_pred, mask = args\r\n            y_true = K.cast(K.round(y_true), 'int32')\r\n            y_pred = K.cast(K.round(y_pred), 'int32')\r\n            accu = K.cast(K.equal(y_true, y_pred), 'float32')  # \u9884\u6d4b\u4f4d\u7f6e\u6b63\u786e\r\n            return K.sum(accu * mask) / K.sum(mask)\r\n        accu1 = Lambda(get_accu1)([y_true, seq, loss_mask])\r\n        self.model.add_metric(value=accu1,name='accu1')\r\n\r\nbut it is wrong under TF2.3.1, but has no problem under TF1.15.1. BUT I have to use the TF2.3.1\r\n![image](https://user-images.githubusercontent.com/74778093/136537299-a04acf00-7377-4a76-9d73-1903ee06301a.png)\r\ncould you like to give me some  suggestions?", "comments": ["Hi @tolerancecky!\r\nCould you please the template too as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52297\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52297\">No</a>\n"]}, {"number": 52296, "title": "loss function bombs", "body": "def my_loss(y_true, y_pred):\r\n    epsilon=1e-7\r\n    tloss=0.0\r\n    for i in range(len(pos_weights)):\r\n# for each class, add average weighted loss for that class \r\n            tloss +=-(pos_weights[i]*y_true[i]*K.log(y_pred[i]+epsilon)+neg_weights[i]*(1-y_true[i])*K.log(1-(y_pred[i])+epsilon))\r\n    return K.mean(tloss)\r\nopt = keras.optimizers.Adam(learning_rate=0.0005)\r\nmodel.compile(optimizer=opt,\r\n              loss=my_loss,\r\n              metrics=[tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.CategoricalAccuracy(),\r\n                       tf.keras.metrics.FalseNegatives(),\r\n                      tf.keras.metrics.FalsePositives()])\r\nmodel.summary() ( model summary outtput received)\r\nhistory = model.fit(train_generator, steps_per_epoch=10,\r\n\tvalidation_data=valid_generator, validation_steps=10, epochs=20, verbose=2)\r\nEpoch 1/20\r\nWARNING:tensorflow:AutoGraph could not transform <function my_loss at 0x000001D9006A63A0> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Index'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING: AutoGraph could not transform <function my_loss at 0x000001D9006A63A0> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Index'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-14-5bd50e19a974> in <module>\r\n----> 1 history = model.fit(train_generator, steps_per_epoch=10,\r\n      2 \tvalidation_data=valid_generator, validation_steps=10, epochs=20, verbose=2)\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in _method_wrapper(self, *args, **kwargs)\r\n    106   def _method_wrapper(self, *args, **kwargs):\r\n    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n--> 108       return method(self, *args, **kwargs)\r\n    109 \r\n    110     # Running inside `run_distribute_coordinator` already.\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1096                 batch_size=batch_size):\r\n   1097               callbacks.on_train_batch_begin(step)\r\n-> 1098               tmp_logs = train_function(iterator)\r\n   1099               if data_handler.should_sync:\r\n   1100                 context.async_wait()\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    778       else:\r\n    779         compiler = \"nonXla\"\r\n--> 780         result = self._call(*args, **kwds)\r\n    781 \r\n    782       new_tracing_count = self._get_tracing_count()\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    821       # This is the first call of __call__, so we have to initialize.\r\n    822       initializers = []\r\n--> 823       self._initialize(args, kwds, add_initializers_to=initializers)\r\n    824     finally:\r\n    825       # At this point we know that the initialization is complete (or less\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    694     self._graph_deleter = FunctionDeleter(self._lifted_initializer_graph)\r\n    695     self._concrete_stateful_fn = (\r\n--> 696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n    697             *args, **kwds))\r\n    698 \r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   2853       args, kwargs = None, None\r\n   2854     with self._lock:\r\n-> 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   2856     return graph_function\r\n   2857 \r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in _maybe_define_function(self, args, kwargs)\r\n   3211 \r\n   3212       self._function_cache.missed.add(call_context_key)\r\n-> 3213       graph_function = self._create_graph_function(args, kwargs)\r\n   3214       self._function_cache.primary[cache_key] = graph_function\r\n   3215       return graph_function, args, kwargs\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   3063     arg_names = base_arg_names + missing_arg_names\r\n   3064     graph_function = ConcreteFunction(\r\n-> 3065         func_graph_module.func_graph_from_py_func(\r\n   3066             self._name,\r\n   3067             self._python_function,\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    984         _, original_func = tf_decorator.unwrap(python_func)\r\n    985 \r\n--> 986       func_outputs = python_func(*func_args, **func_kwargs)\r\n    987 \r\n    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in wrapped_fn(*args, **kwds)\r\n    598         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    599         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 600         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    601     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    602 \r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py in wrapper(*args, **kwargs)\r\n    971           except Exception as e:  # pylint:disable=broad-except\r\n    972             if hasattr(e, \"ag_error_metadata\"):\r\n--> 973               raise e.ag_error_metadata.to_exception(e)\r\n    974             else:\r\n    975               raise\r\n\r\nTypeError: in user code:\r\n\r\n    C:\\Users\\owner\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\r\n        return step_function(self, iterator)\r\n    C:\\Users\\owner\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    C:\\Users\\owner\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    C:\\Users\\owner\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    C:\\Users\\owner\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    C:\\Users\\owner\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\r\n        outputs = model.train_step(data)\r\n    C:\\Users\\owner\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:748 train_step\r\n        loss = self.compiled_loss(\r\n    C:\\Users\\owner\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:204 __call__\r\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\r\n    C:\\Users\\owner\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:149 __call__\r\n        losses = ag_call(y_true, y_pred)\r\n    C:\\Users\\owner\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:253 call  **\r\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\r\n    <ipython-input-11-595b5a4de413>:6 my_loss  **\r\n        tloss +=-(pos_weights[i]*y_true[i]*K.log(y_pred[i]+epsilon)+neg_weights[i]*(1-y_true[i])*K.log(1-(y_pred[i])+epsilon))\r\n    C:\\Users\\owner\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1141 binary_op_wrapper\r\n        raise e\r\n    C:\\Users\\owner\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1125 binary_op_wrapper\r\n        return func(x, y, name=name)\r\n    C:\\Users\\owner\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1457 _mul_dispatch\r\n        return multiply(x, y, name=name)\r\n    C:\\Users\\owner\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\r\n        return target(*args, **kwargs)\r\n    C:\\Users\\owner\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:509 multiply\r\n        return gen_math_ops.mul(x, y, name)\r\n    C:\\Users\\owner\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:6174 mul\r\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\r\n    C:\\Users\\owner\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:503 _apply_op_helper\r\n        raise TypeError(\r\n\r\n    TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int64 of argument 'x'.", "comments": ["@vssankar181181  In order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),Thanks!", "i reported the bug i the issue template only. The message itself asked me\nto report it. I have reproduced the essageas it was output. Ihave also give\nthe relevant jupyter notebook cell contents.\n\nOn Fri, Oct 8, 2021 at 6:26 PM sushreebarsa ***@***.***>\nwrote:\n\n> @vssankar181181 <https://github.com/vssankar181181> In order to expedite\n> the trouble-shooting process here,Could you please fill the issue template\n> <https://github.com/tensorflow/tensorflow/issues/new/choose>,Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/52296#issuecomment-938621052>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ANXZEBQR5MZJL4LENRIZAC3UF3S77ANCNFSM5FTD7DNQ>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n>\n\n\n-- \nV S Sankar\n", "@vssankar181181  In order to reproduce the issue reported here, could you please provide the complete code and the dataset , tensorflow version you are using? Please have a look at the [issue](https://github.com/tensorflow/tensorflow/issues/43650) for reference. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52296\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52296\">No</a>\n"]}, {"number": 52295, "title": "[TF:TRT]Support dict input for TF-TRT convert", "body": "This PR is to support dict input for TF-TRT converter api: build(input_fn) and convert(cal_input_fn).", "comments": ["Looks good to me. Thanks for working on this! I have a few suggestion:\r\n* In the PR description, please briefly describe what this PR does and why we need such an extension.\r\n* Please modify the comment for the public methods you change, to update the description about the input function arguments.\r\n* Please add test cases for the functionality you add.", "I can see some code duplication, but we will refactor this after your PR.\r\nCan you address my request [here](https://github.com/tensorflow/tensorflow/pull/52295#issuecomment-941054633) for a PR description and the reason we need the extension here? Also can you please squash your two commits into one, which is usually done before the final approval. I will approve your PR right this.\r\nthanks for working on this!\r\n", "@bixia1 I've added some PR description and comments of method according to your suggestion. \r\n\r\nHowever, after a quick look at the test code in \"tensorflow/python/compiler/tensorrt/test/\", I found I have to break https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py#L993 in tf_trt_integration_test_base which is not I supposed to do. \r\n\r\nSince I don't have too much time to dive into the test case and I didn't find any developer documentation for it, I think maybe tf team can design how to add test for it later if you think this feature is useful.\r\n\r\nThanks!", "The commits are squashed.", "@gbaned What should I do to merge this PR?"]}, {"number": 52294, "title": "Using the same seed kwarg returns different values between GlorotUniform and GlorotUniformV2", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.5 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Pip (binary)\r\n- TensorFlow version (use command below): v2.6.0-0-g919f693420e 2.6.0\r\n- Python version: 3.7.12\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nSetting the `operation` seed returns different tensors when using GlorotUniform when imported from `tf.compat.v1` and `tensorflow.keras.initializers`\r\n\r\n**Describe the expected behavior**\r\nBoth tensors must be equal\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing): \r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n[Colab link](https://colab.research.google.com/drive/13V8v7a7fCQQMroaLz1PGvgrQ5Wf81IRp?usp=sharing)\r\n\r\nIn the notebook, I share reproducible code about the current behavior and the expected behavior, as well as *why* this might be happening. \r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n", "comments": ["@fernandobperezm ,\r\nPlease post this issue on [keras-team/keras](https://github.com/keras-team/keras/issues) repo.\r\nTo know more refer to:\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\n", "Thanks, @tilakrayal for your reply. I've just created the issue in the Keras repo :-) \r\n\r\nIf needed, you can close the issue.", "@fernandobperezm ,\r\nPlease feel free to close this issue here, so that the issue can be tracked in Keras repo. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52294\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52294\">No</a>\n"]}, {"number": 52293, "title": "Cannot create interpreter:  Op builtin_code out of range: 142. Are you using old TFLite binary with newer model?", "body": "Hi TensorFlow, I am trying to create a tflite that is trainable on device using your article\u00a0\r\nhttps://www.tensorflow.org/lite/examples/on_device_training/overview\r\n\r\n have been facing this issue\u00a0\u00a0\r\n\r\n**Cannot create interpreter:  Op builtin_code out of range: 142. Are you using old TFLite binary with newer model?**\r\n\r\nWhile trying to\u00a0create\u00a0an interpreter for\u00a0my tflite in Java\r\n\u00a0Interpreter interpreter = new Interpreter(loadModelFile(MainActivity.this))\r\n\r\nto hopefully run the function\u00a0interpreter.runSignature(inputs,\u00a0outputs,\u00a0\"train\");\r\n\r\nExactly Similar to the issue found\u00a0in\u00a0https://github.com/tensorflow/tensorflow/issues/51849\u00a0\r\n\r\n... However I am using java and the remedy of removing jcenter() doesn't seem to be fixing my problem.\r\nHere are the dependencies I am using for android\r\n\r\nimplementation\u00a0'org.tensorflow:tensorflow-lite:0.0.0-nightly-SNAPSHOT'\r\n\r\nmavenCentral()\r\nmaven { // Only for snapshot artifacts\r\n name 'ossrh-snapshot'\r\n url 'https://oss.sonatype.org/content/repositories/snapshots'\r\n}\r\n\r\nIf possible can you please send me the\u00a0working code for android implementation of on Device Training, as discussed in this article\u00a0\r\nhttps://www.tensorflow.org/lite/examples/on_device_training/overview\u00a0\r\nor please help me resolve this issue.", "comments": ["Hi @raggyie! Could you please fill the template from [here ](https://github.com/tensorflow/tensorflow/issues/new/choose)and  update the device details too as It helps expedite the issue.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@haozha111 when the android app is released, do post a link to the it here as the user is waiting on the app.", "Using the dependencies ...\r\nimplementation 'org.tensorflow:tensorflow-lite:+'\r\nimplementation 'org.tensorflow:tensorflow-lite-select-tf-ops:+'\r\n\r\nRather than nightly builds have fixed this issue ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52293\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52293\">No</a>\n", "Sure!"]}, {"number": 52292, "title": "Increase the keras{-nightly} version for the 2.7 release.", "body": "PiperOrigin-RevId: 401288349\r\nChange-Id: I386f02aa49813c7a69a1cce3839779e39c8c4232", "comments": []}, {"number": 52291, "title": "[TF:TRT] Support ConcatV2 op when inputs are constants", "body": "Inputs to concatV2 can be constants. Previously, we do not convert concatV2 with constant inputs. This PR extends the converter for explicit batch mode to convert concatV2 with constant inputs.\r\n\r\nModify the unit tests to test the conversion for concatV2 with constant inputs.\r\n\r\nSigned-off-by: Meenakshi Venkataraman <meenakshiv@nvidia.com>\r\n\r\nTagging @bixia1 for review\r\nCC: @tfeher @DEKHTIARJonathan", "comments": ["I modify the PR description, please check."]}, {"number": 52290, "title": "Issue with conversion of dilated Convolutions  #29509 reappears in versions 2.5.0, 2.5.1 and 2.6.0", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution Ubuntu 20.04 \r\n- TensorFlow installation (pip package or built from source): pip + official docker images\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.5.0, 2.5.1, 2.6.0\r\n\r\n### 2. Code \r\n\r\nMinimal model to reproduce issue:\r\n\r\n```python\r\nimport pathlib\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\ninput = keras.Input([128, 128, 3])\r\nx = keras.layers.Conv2D(8, 5, dilation_rate=2, padding=\"same\", use_bias=False)(input)\r\nx = keras.layers.BatchNormalization()(x)\r\noutput = keras.layers.ReLU()(x)\r\n\r\nm = keras.Model(inputs=input, outputs=output)\r\n\r\n\r\n\r\ndef representative_data_gen():\r\n  for input_value in range(10):\r\n    # Model has only one input so each data point has one element.\r\n    yield [tf.random.uniform((1,128,128,3), dtype=tf.float32)]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(m)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_data_gen\r\n\r\ntflite_model = converter.convert()\r\noutput_file = \"./test.tflite\"\r\nwith open(output_file, 'wb') as f:\r\n  f.write(tflite_model)\r\n\r\nprint(f\"Converted model was written to {output_file}\")\r\n```\r\n\r\n\r\n\r\n### 3. Failure after conversion\r\n\r\n- Model produces correct results, but it is slower than expected.\r\n\r\nMapping to EdgeTPU fails. \r\nThis is the same as #29509. Issue was solved but appears again on newer releases.\r\n\r\n\r\n```bash\r\nModel successfully compiled but not all operations are supported by the Edge TPU. A percentage of the model will instead run on the CPU, which is slower. If possible, consider updating your model to use only operations supported by the Edge TPU. For details, visit g.co/coral/model-reqs.\r\nNumber of operations that will run on Edge TPU: 2\r\nNumber of operations that will run on CPU: 5\r\n\r\nOperator                       Count      Status\r\n\r\nDEQUANTIZE                     1          Operation is working on an unsupported data type\r\nSPACE_TO_BATCH_ND              1          Tensor has unsupported rank (up to 3 innermost dimensions mapped)\r\nMUL                            1          Mapped to Edge TPU\r\nQUANTIZE                       1          Operation is otherwise supported, but not mapped due to some unspecified limitation\r\nCONV_2D                        1          Tensor has unsupported rank (up to 3 innermost dimensions mapped)\r\nBATCH_TO_SPACE_ND              1          Tensor has unsupported rank (up to 3 innermost dimensions mapped)\r\nADD                            1          Mapped to Edge TPU\r\nCompilation child process completed within timeout period.\r\nCompilation succeeded! \r\n(base) waterview@waterview-w\r\n\r\n```\r\n\r\nWith TF 2.4.3 used to be:\r\n\r\n```bash\r\nModel successfully compiled but not all operations are supported by the Edge TPU. A percentage of the model will instead run on the CPU, which is slower. If possible, consider updating your model to use only operations supported by the Edge TPU. For details, visit g.co/coral/model-reqs.\r\nNumber of operations that will run on Edge TPU: 1\r\nNumber of operations that will run on CPU: 2\r\n\r\nOperator                       Count      Status\r\n\r\nDEQUANTIZE                     1          Operation is working on an unsupported data type\r\nCONV_2D                        1          Mapped to Edge TPU\r\nQUANTIZE                       1          Operation is otherwise supported, but not mapped due to some unspecified limitation\r\nCompilation child process completed within timeout period.\r\nCompilation succeeded! \r\n\r\n```\r\n\r\n\r\n", "comments": ["@desmoteo Did you try `tf-nightly`? Also, the current conversion code is for [\" full integer quantization with float fallback option\"](https://www.tensorflow.org/lite/performance/post_training_integer_quant#convert_using_float_fallback_quantization). \r\n\r\n> That's (Convert using float fallback quantization) usually good for compatibility, but it won't be compatible with devices that perform only integer-based operations, such as the Edge TPU.\r\n\r\nI have updated your code to use [\"full integer quantization\"](https://www.tensorflow.org/lite/performance/post_training_integer_quant#convert_using_integer-only_quantization) to convert the model. Can you please try this model and see whether Edge TPU throws any error? [Here](https://colab.research.google.com/gist/jvishnuvardhan/b445045b5bab612b9b8910d0c9ece42b/untitled1091.ipynb) is a gist for our reference. Thanks!", "I am having exactly same issue, where the tf lite converter is breaking the dilated convolution down to the spacetodepth, conv2d and depth to space and then does not apply quantization to conv2d. I checked the QAT model and looks fine and there are no spaceToDepth and depthToSpace layers in the model summary. I did not encounter this problem in TF 1.15. The issue that this creates is that the model that is quantized and converted in TF 1.15 runs faster than one quantized and converted in TF 2.6, due to the float conv layers.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52290\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52290\">No</a>\n", "@bayesian-mind Please open a new issue with a simple standalone code to reproduce the issue. Thanks!"]}, {"number": 52289, "title": "Latest TensorFlow fails to install due to yesterday's Keras requirement change ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: latest as of https://github.com/tensorflow/tensorflow/commit/ca0fa154128d793df5e6809cb86336ac1fed3466 \r\n- Python version: python3.8\r\n- Installed using virtualenv? pip? conda?: pip \r\n- Bazel version (if compiling from source):  3.7.2\r\n- GCC/Compiler version (if compiling from source): GCC9.3\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Build was successful, but installation of the wheel failed with the following Error message\r\n09:34:20  ERROR: No matching distribution found for keras<2.8,>=2.7 (from tensorflow==2.8.0)**\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\npip install /tmp/pip3/tensorflow-2.8.0-cp38-cp38-linux_x86_64.whl\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nYou can take a detailed look here: https://tensorflow-ci.intel.com/job/tensorflow-mkl-build-whl-nightly/941/console\r\n", "comments": ["I hit the same issue when installing against the TF master. And it doesn't help when I upgraded the keras to keras==2.7.0rc0, since there is no 2.7 distribution yet.", "Broken for Linaro CI as well.\r\nhttps://ci.linaro.org/job/ldcg-python-manylinux-tensorflow-nightly/123/console\r\nThe subtype: label for Ubuntu seems wrong, this is not related to the OS in use.", "Could please provide the link or document of wheel file which you are trying.Thanks.", "@sachinprasadhs , this is a link that shows we were able to build TensorFlow successfully. But failed at installation steps. \r\nhttps://tensorflow-ci.intel.com/job/tensorflow-mkl-build-whl-nightly/942/console\r\n\r\n02:46:49  \u001b[0m\u001b[91mINFO: Elapsed time: 2622.058s, Critical Path: 429.44s\r\n02:46:49  INFO: 11295 processes: 1302 internal, 9993 local.\r\n02:46:49  \u001b[0m\u001b[91mINFO: Build completed successfully, 11295 total actions\r\n02:46:49  \u001b[0m\u001b[91mINFO: Build completed successfully, 11295 total actions\r\n02:46:49  \u001b[0mFri Oct 8 09:46:36 UTC 2021 : === Preparing sources in dir: /tmp/tmp.2qdIZr4JRR\r\n02:46:49  /tensorflow /tensorflow\r\n02:47:01  /tensorflow\r\n02:47:01  /tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow /tensorflow\r\n02:47:01  /tensorflow\r\n02:47:01  /tmp/tmp.2qdIZr4JRR/tensorflow/include /tensorflow\r\n02:47:01  /tensorflow\r\n02:47:01  Fri Oct 8 09:47:00 UTC 2021 : === Building wheel\r\n02:47:01  \u001b[91mwarning: no files found matching 'README'\r\n02:47:01  \u001b[0m\u001b[91mwarning: no files found matching '*.pyd' under directory '*'\r\n02:47:02  \u001b[0m\u001b[91mwarning: no files found matching '*.pyi' under directory '*'\r\n02:47:02  \u001b[0m\u001b[91mwarning: no files found matching '*.pd' under directory '*'\r\n02:47:03  \u001b[0m\u001b[91mwarning: no files found matching '*.dylib' under directory '*'\r\n02:47:04  \u001b[0m\u001b[91mwarning: no files found matching '*.dll' under directory '*'\r\n02:47:04  \u001b[0m\u001b[91mwarning: no files found matching '*.lib' under directory '*'\r\n02:47:04  \u001b[0m\u001b[91mwarning: no files found matching '*.csv' under directory '*'\r\n02:47:04  warning: no files found matching '*.h' under directory 'tensorflow/include/tensorflow'\r\n02:47:04  \u001b[0m\u001b[91mwarning: no files found matching '*.proto' under directory 'tensorflow/include/tensorflow'\r\n02:47:04  \u001b[0m\u001b[91mwarning: no files found matching '*' under directory 'tensorflow/include/third_party'\r\n02:47:51  \u001b[0mFri Oct 8 09:47:45 UTC 2021 : === Output wheel file is in: /tmp/pip3\r\n02:47:51  Processing /tmp/pip3/tensorflow-2.8.0-cp38-cp38-linux_x86_64.whl\r\n02:47:51  Requirement already satisfied, skipping upgrade: wheel<1.0,>=0.32.0 in /usr/lib/python3/dist-packages (from tensorflow==2.8.0) (0.34.2)\r\n02:47:51  Collecting google-pasta>=0.1.1\r\n02:47:51    Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\r\n02:47:51  Collecting tensorflow-io-gcs-filesystem>=0.21.0\r\n02:47:51    Downloading tensorflow_io_gcs_filesystem-0.21.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\r\n02:47:51  Requirement already satisfied, skipping upgrade: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.0) (3.4.0)\r\n02:47:51  Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.0) (1.1.2)\r\n02:47:51  Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow==2.8.0) (1.14.0)\r\n02:47:51  Collecting wrapt>=1.11.0\r\n02:47:51    Downloading wrapt-1.13.1-cp38-cp38-manylinux2010_x86_64.whl (84 kB)\r\n02:47:51  \u001b[91mERROR: Could not find a version that satisfies the requirement keras<2.8,>=2.7 (from tensorflow==2.8.0) (from versions: 0.2.0, 0.3.0, 0.3.1, 0.3.2, 0.3.3, 1.0.0, 1.0.1, 1.0.2, 1.0.3, 1.0.4, 1.0.5, 1.0.6, 1.0.7, 1.0.8, 1.1.0, 1.1.1, 1.1.2, 1.2.0, 1.2.1, 1.2.2, 2.0.0, 2.0.1, 2.0.2, 2.0.3, 2.0.4, 2.0.5, 2.0.6, 2.0.7, 2.0.8, 2.0.9, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.1.4, 2.1.5, 2.1.6, 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.2.4, 2.2.5, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.4.2, 2.4.3, 2.5.0rc0, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0rc3, 2.6.0, 2.7.0rc0)\r\n02:47:51  ERROR: No matching distribution found for keras<2.8,>=2.7 (from tensorflow==2.8.0)\r\n\r\nIn general, the installation is checking keras<2.8, and >=2.7. Looks like pypi keras is not released with v2.7? \r\n\r\n\r\n", "Just rebuild the TF against the latest master branch. It seems the issue has been fixed when we update the keras to 2.7.0rc0.", "Thank you, https://github.com/tensorflow/tensorflow/commit/540219f9c97149e95e50d4c618d0b9fda9c0e851 fixed it. Hence closing. Have a nice weekend~", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52289\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52289\">No</a>\n"]}, {"number": 52288, "title": "save_model() still fails with custom layer and SavedModel format", "body": "Basically same issue that was closed before:\r\nhttps://github.com/tensorflow/tensorflow/issues/40912\r\n\r\nWith TF 2.6 I'm experiencing exactly the same problem.\r\nAlso with tf-nightly the issue is still there.", "comments": ["Hi @jvcrnc! Could you please fill the template too as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced]. \r\n\r\nRegarding the #40912 , I tried to replicate and resolve issue by adding extension of model file  . Attaching [Gist ](https://colab.research.google.com/gist/mohantym/782508b9c9cdafd08d85a7dee0a02c5c/github_52288.ipynb)for Reference .Thanks! ", "Hi @mohantym, here the filled in template:\r\nHave I written custom code: Yes\r\nOS Platform and Distribution: Windows 10\r\nTensorFlow version (use command below): 2.6 and 2.8.0-dev20211003\r\nPython version: 3.9.6\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nCurrent behavior\r\nI have defined a custom Keras layer which acts as a learnable lookup table. The layer is functional and learns throughout training. I am able to save the Keras model using both model.save('/path/to/dir') and model.save('/path/to/dir/model.h5').\r\n\r\nHowever, tf.keras.models.load_model('/path/to/dir') fails whereas the H5 model seems to load properly with tf.keras.models.load_model('/path/to/dir/model.h5', custom_objects = {\"Lookup\": Lookup}). Passing custom_objects to the SavedModel call (without .h5) makes no difference.\r\n\r\nFor code example to reproduce, see related ticket:\r\nhttps://github.com/tensorflow/tensorflow/issues/40912\r\n", "Hi @jvcrnc! \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) too.\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) .  Thanks!", "Hi @mohantym, ok I posted on keras repo too:\r\nhttps://github.com/keras-team/keras/issues/15483", "Ok! @jvcrnc! Could you please close this issue then ?", "Moved to keras repo.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52288\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52288\">No</a>\n"]}, {"number": 52286, "title": "Not able to merge results", "body": "I am working on a project in which I am using different models of machine learning.\r\n![1](https://user-images.githubusercontent.com/26819449/136387688-78e5b8b2-39c9-4acb-bd0e-b643a88a24d3.JPG)\r\nI am able to generate results for a particular model. But not able to generate loss for all models in the same graph so I could compare well.\r\nI am doing a project in google collab and different models are running in different files of colab.\r\nPlease can you tell me a way how could I solve this or some alternative?\r\nThanks! \r\n", "comments": ["@starboyvarun ,\r\nPlease take a look at this links [1](https://stackoverflow.com/questions/16748577/matplotlib-combine-different-figures-and-put-them-in-a-single-subplot-sharing-a) and [2](https://www.geeksforgeeks.org/plot-multiple-plots-in-matplotlib/) with the similar error.It helps.Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 52285, "title": "Duplicate PID in multiple GPU", "body": "Tensorflow creates a duplicate PID on all the GPUs. I am running on a system with 2 GPUs and everytime I execute a training, it allocates memory from both the GPU's with the same PID, although one GPU is used to the computations\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/50949120/136385342-31389d3d-ec38-42ad-816f-1705dd140ab5.png)\r\n", "comments": ["Setting up the `CUDA_VISIBLE_DEVICES` to a specific GPU index works, but is there any other way to avoid spawning a process in other GPU with the same PID?\r\n\r\n`os.system('nvidia-smi -q -d Memory |grep -A4 GPU|grep Free >tmp')`\r\n`memory_available = [int(x.split()[2]) for x in open('tmp', 'r').readlines()]`\r\n`free_gpu_id = np.argmax(memory_available)`\r\n`os.environ[\"CUDA_VISIBLE_DEVICES\"]='{}'.format(str(free_gpu_id))`", "Hi @Kumudaya! Could you please the template too as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 52284, "title": "Segmentation fault (core dumped) while getting the node attributes from the TF Graphdef in C++", "body": "Dear Tensorflow Team,\r\n\r\nI am trying to load the Tensorflow model(frozen graph) and get the attributes of each Node in C++.  Having said that, I am able to get the deserialized object from the model correctly using the below code:\r\n\r\n```\r\n    FILE* fd = fopen(graphFile, \"rb\");\r\n    google::protobuf::io::FileInputStream inStream(fileno(fd));\r\n    google::protobuf::io::CodedInputStream coded_stream(&inStream);\r\n    coded_stream.SetTotalBytesLimit(1024LL << 20, 512LL << 20);\r\n    bool success = graphDef.ParseFromCodedStream(&coded_stream);\r\n\r\n```\r\nAnd I am able to print the Graphdef content and all seems to be right. I am able to get the correct content from the graphDef using the below code.\r\n\r\n```\r\n    for (int i = 0; i < graphDef.node_size(); i++)\r\n    {\r\n            graphDef.node(i).PrintDebugString();\r\n\r\n    }\r\n\r\n```\r\nbut while I am trying to get the node attributes from the graphDef, I am getting the Segmentation fault (core dumped).\r\n\r\n```\r\n\tconst tensorflow::NodeDef node = graphDef.node(2);\r\n        auto attr = node.attr(); //getting the error here\r\n\r\n```\r\n\r\nI am attaching trace below:\r\n\r\n```\r\n0x00007ffff5f372ea in google::protobuf::hash<char const*>::operator() (this=0x7fffffffcea7, str=0x5de58948f7894855 <error: Cannot access memory at address 0x5de58948f7894855>)\r\n    at /home/darshan/project/sw/external/protobuf/src/google/protobuf/stubs/hash.h:66\r\n66\t    for (; *str != '\\0'; str++) {\r\n(gdb) bt\r\n#0  0x00007ffff5f372ea in google::protobuf::hash<char const*>::operator() (this=0x7fffffffcea7, str=0x5de58948f7894855 <error: Cannot access memory at address 0x5de58948f7894855>)\r\n    at /home/darshan/project/sw/external/protobuf/src/google/protobuf/src/google/protobuf/stubs/hash.h:66\r\n#1  0x00007ffff5f3735b in google::protobuf::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >::operator() (this=0x5555557c5450, \r\n    key=<error: Cannot access memory at address 0x5de58948f7894855>) at /home/darshan/project/sw/external/protobuf/src/google/protobuf/stubs/hash.h:83\r\n#2  0x00007ffff5f44e49 in google::protobuf::Map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::AttrValue>::InnerMap::BucketNumber (this=0x5555557c5450, \r\n    k=<error: Cannot access memory at address 0x5de58948f7894855>) at /home/darshan/project/sw/external/protobuf/src/google/protobuf/map.h:888\r\n#3  0x00007ffff5f4321e in google::protobuf::Map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::AttrValue>::InnerMap::FindHelper (this=0x5555557c5450, \r\n    k=<error: Cannot access memory at address 0x5de58948f7894855>, it=0x0) at /home/darshan/project/sw/external/protobuf/src/google/protobuf/map.h:647\r\n#4  0x00007ffff5f40376 in google::protobuf::Map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::AttrValue>::InnerMap::FindHelper (this=0x5555557c5450, \r\n    k=<error: Cannot access memory at address 0x5de58948f7894855>) at/home/darshan/project/sw/external/protobuf/src/google/protobuf/map.h:643\r\n#5  0x00007ffff5f3c782 in google::protobuf::Map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::AttrValue>::InnerMap::find (this=0x5555557c5450, \r\n    k=<error: Cannot access memory at address 0x5de58948f7894855>) at /home/darshan/project/sw/external/protobuf/src/google/protobuf/map.h:558\r\n#6  0x00007ffff5f3c9d0 in google::protobuf::Map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::AttrValue>::find (this=0x7fffffffd210, \r\n    key=<error: Cannot access memory at address 0x5de58948f7894855>) at /home/darshan/project/sw/external/protobuf/src/google/protobuf/map.h:1078\r\n#7  0x00007ffff5f3a623 in google::protobuf::Map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::AttrValue>::insert<google::protobuf::Map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::AttrValue>::const_iterator> (this=0x7fffffffd210, first=..., last=...)\r\n    at /home/darshan/project/sw/external/protobuf/src/google/protobuf/src/google/protobuf/map.h:1112\r\n#8  0x00007ffff5f38a5a in google::protobuf::Map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::AttrValue>::Map (this=0x7fffffffd210, other=...)\r\n    at /home/darshan/project/sw/external/protobuf/src/google/protobuf/src/google/protobuf/map.h:149\r\n#9  0x00007ffff5e1c7c9 in parse ()\r\n    at tensorflow/TFParser.cpp:824\r\n#10 0x0000555555563632 in parseTFNetwork() ()\r\n#11 0x0000555555563e34 in parseAndCompile() ()\r\n#12 0x000055555555e2cb in launchTest() ()\r\n#13 0x000055555555f44d in main ()\r\n\r\n```\r\nI am not sure what exactly is the issue here. Whether it is a deserialization issue, or the way of accessing is wrong, or any other?\r\n\r\nCan you please help me to resolve the issue? It would be a great help from your side.\r\n\r\n\r\n**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from: Source\r\n- TensorFlow version : 2.3\r\n- Python version: 3.6\r\n- Bazel version : 3.1.0\r\n- Protobuf version: 3.9.2\r\n\r\n\r\nBest Regards,\r\nDarshan C G", "comments": ["@Darshvino Could you please try on the stable version of `TF 2.6.0` and have a look at the [link](https://stackoverflow.com/questions/46785852/while-traversing-graph-in-bfs-order-gives-segmentation-fault-core-dumped-in-c), [link1](https://www.tensorflow.org/api_docs/python/tf/Graph) ,[link2 ](https://github.com/tensorflow/tensorflow/issues/19347)for reference ?Please let us know if it helps?Thank you!", "Hi @sushreebarsa,\r\n\r\nFirst of all thank you very much for your quick reply.\r\n\r\nI have referred to all the links you have shared, but no luck. I am facing the same issue for 2 weeks :(\r\n\r\nAs you suggested I will definitely try with the 2.6 version and update you.\r\n\r\nThanks again for your kind response", "@Darshvino Could you please let us know if you have tried with the TF v2.6.0 and let us know the outcome  ? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52284\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52284\">No</a>\n"]}, {"number": 52283, "title": "Build r2.2.0 fails when build ppc64le machine", "body": "when i Compiling tensorflow-gpu from source,I do not know how to solve the following error\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version:v2.2.0\r\n- Python version:3.6\r\n- Installed using virtualenv? pip? conda?:conda\r\n- Bazel version (if compiling from source):2.0.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:CUDA 10.1/cuDNN 7\r\n- GPU model and memory:k80\r\n\r\n./configure\r\nYou have bazel 2.0.0- (@non-git) installed.\r\nPlease specify the location of python. [Default is /home/owner/anaconda3/envs/tensorflow/bin/python]: /home/owner/anaconda3/envs/tensorflow/bin/python3.6\r\n\r\n\r\nFound possible Python library paths:\r\n  /home/owner/anaconda3/envs/tensorflow/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/owner/anaconda3/envs/tensorflow/lib/python3.6/site-packages]\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: n\r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.1 in:\r\n    /usr/local/cuda/lib64\r\n    /usr/local/cuda/include\r\nFound cuDNN 7 in:\r\n    /usr/local/cuda/lib64\r\n    /usr/local/cuda/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: \r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: n\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -mcpu=native]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=ngraph      \t# Build with Intel nGraph support.\r\n\t--config=numa        \t# Build with NUMA support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n\t--config=v2          \t# Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nohdfs      \t# Disable HDFS support.\r\n\t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n\r\n\r\nwhen build blow error is happen and abend build...\r\n\r\nERROR: An error occurred during the fetch of repository 'local_config_cuda':\r\n   Traceback (most recent call last):\r\n\tFile \"/home/owner/Downloads/tensorflow-2.2.0/third_party/gpus/cuda_configure.bzl\", line 1210\r\n\t\t_create_local_cuda_repository(<1 more arguments>)\r\n\tFile \"/home/owner/Downloads/tensorflow-2.2.0/third_party/gpus/cuda_configure.bzl\", line 934, in _create_local_cuda_repository\r\n\t\t_find_libs(repository_ctx, <2 more arguments>)\r\n\tFile \"/home/owner/Downloads/tensorflow-2.2.0/third_party/gpus/cuda_configure.bzl\", line 577, in _find_libs\r\n\t\t_check_cuda_libs(repository_ctx, <2 more arguments>)\r\n\tFile \"/home/owner/Downloads/tensorflow-2.2.0/third_party/gpus/cuda_configure.bzl\", line 479, in _check_cuda_libs\r\n\t\texecute(repository_ctx, <1 more arguments>)\r\n\tFile \"/home/owner/Downloads/tensorflow-2.2.0/third_party/remote_config/common.bzl\", line 208, in execute\r\n\t\tfail(<1 more arguments>)\r\nRepository command failed\r\nTraceback (most recent call last):\r\n  File \"script.py\", line 88, in <module>\r\n    main()\r\n  File \"script.py\", line 77, in main\r\n    check_cuda_lib(path, check_soname=args[i + 1] == \"True\")\r\n  File \"script.py\", line 62, in check_cuda_lib\r\n    output = subprocess.check_output([objdump, \"-p\", path]).decode(\"ascii\")\r\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xef in position 40: ordinal not in range(128)\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/home/owner/Downloads/tensorflow-2.2.0/third_party/gpus/cuda_configure.bzl\", line 1210\r\n\t\t_create_local_cuda_repository(<1 more arguments>)\r\n\tFile \"/home/owner/Downloads/tensorflow-2.2.0/third_party/gpus/cuda_configure.bzl\", line 934, in _create_local_cuda_repository\r\n\t\t_find_libs(repository_ctx, <2 more arguments>)\r\n\tFile \"/home/owner/Downloads/tensorflow-2.2.0/third_party/gpus/cuda_configure.bzl\", line 577, in _find_libs\r\n\t\t_check_cuda_libs(repository_ctx, <2 more arguments>)\r\n\tFile \"/home/owner/Downloads/tensorflow-2.2.0/third_party/gpus/cuda_configure.bzl\", line 479, in _check_cuda_libs\r\n\t\texecute(repository_ctx, <1 more arguments>)\r\n\tFile \"/home/owner/Downloads/tensorflow-2.2.0/third_party/remote_config/common.bzl\", line 208, in execute\r\n\t\tfail(<1 more arguments>)\r\nRepository command failed\r\nTraceback (most recent call last):\r\n  File \"script.py\", line 88, in <module>\r\n    main()\r\n  File \"script.py\", line 77, in main\r\n    check_cuda_lib(path, check_soname=args[i + 1] == \"True\")\r\n  File \"script.py\", line 62, in check_cuda_lib\r\n    output = subprocess.check_output([objdump, \"-p\", path]).decode(\"ascii\")\r\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xef in position 40: ordinal not in range(128)\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/home/owner/Downloads/tensorflow-2.2.0/third_party/gpus/cuda_configure.bzl\", line 1210\r\n\t\t_create_local_cuda_repository(<1 more arguments>)\r\n\tFile \"/home/owner/Downloads/tensorflow-2.2.0/third_party/gpus/cuda_configure.bzl\", line 934, in _create_local_cuda_repository\r\n\t\t_find_libs(repository_ctx, <2 more arguments>)\r\n\tFile \"/home/owner/Downloads/tensorflow-2.2.0/third_party/gpus/cuda_configure.bzl\", line 577, in _find_libs\r\n\t\t_check_cuda_libs(repository_ctx, <2 more arguments>)\r\n\tFile \"/home/owner/Downloads/tensorflow-2.2.0/third_party/gpus/cuda_configure.bzl\", line 479, in _check_cuda_libs\r\n\t\texecute(repository_ctx, <1 more arguments>)\r\n\tFile \"/home/owner/Downloads/tensorflow-2.2.0/third_party/remote_config/common.bzl\", line 208, in execute\r\n\t\tfail(<1 more arguments>)\r\nRepository command failed\r\nTraceback (most recent call last):\r\n  File \"script.py\", line 88, in <module>\r\n    main()\r\n  File \"script.py\", line 77, in main\r\n    check_cuda_lib(path, check_soname=args[i + 1] == \"True\")\r\n  File \"script.py\", line 62, in check_cuda_lib\r\n    output = subprocess.check_output([objdump, \"-p\", path]).decode(\"ascii\")\r\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xef in position 40: ordinal not in range(128)\r\nINFO: Elapsed time: 0.404s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package\r\n", "comments": ["a new error is :Fetching ...docker; Cloning 251f6a68b439744094faff800cd029798edf9faa of ht\\\r\ntps://github.com/bazelbuild/rules_docker.git.it can`t clone reles_docker.git\r\n", "@aoki897 ,\r\nPlease take a look at this [issue](https://github.com/tensorflow/tensorflow/issues/38660) with similar error.Also can you please try installing TensorFlow v2.2 with CUDA 10.1 and cuDNN 7.6 and refer the link for tested build [configurations](https://www.tensorflow.org/install/source#gpu).It helps.Thanks!\r\n ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52283\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52283\">No</a>\n"]}, {"number": 52282, "title": "SyntaxError: invalid syntax && ImportError: cannot import name pywrap_tensorflow", "body": "**System information**\r\n- Linux Ubuntu 16.04\r\n- TensorFlow version: tensorflow-gpu 1.5\r\n- Python version: 2.7\r\n- Installed using virtualenv: pip\r\n- CUDA/cuDNN version: CUDA9.0 & CUDNN7.0\r\n\r\n**Describe the problem**\r\n    Using anaconda to create a virtual environment named tf.\r\n```\r\n(tf) \u279c  ~ python                         \r\nPython 2.7.18 |Anaconda, Inc.| (default, Jun  4 2021, 14:47:46) \r\n[GCC 7.3.0] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/xander/anaconda3/envs/tf/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/xander/anaconda3/envs/tf/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 52, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"/home/xander/anaconda3/envs/tf/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"/home/xander/anaconda3/envs/tf/lib/python2.7/site-packages/google/protobuf/descriptor.py\", line 113\r\n    class DescriptorBase(metaclass=DescriptorMetaclass):\r\n                                  ^\r\nSyntaxError: invalid syntax\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/xander/anaconda3/envs/tf/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/xander/anaconda3/envs/tf/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\nImportError: cannot import name pywrap_tensorflow\r\n```", "comments": ["Hi @zdj-creator! Could you try again with another environment name like \"Tf_test\" or something else? I think creating an environment with name \"tf\" is creating this issue. Please try again with Python 3.6-3.8 if that does not work.\r\n\r\nIt  also looks like  you are using older versions(1.x versions) of Tensorflow. We recommend that you upgrade  your code base to 2.x  versions as many features and bug fixes has been done in newer versions and let us know if the issue still persists in newer versions.   [Reference](https://www.tensorflow.org/install/pip). Thanks!", "Thanks for your  reply! @mohantym \r\nI have tried many times, and the env name is not the problem.And older versions are often preferred in production environments.\r\nWhatever,Thanks.", "Hi @zdj-creator , Did you try with python 3.6  yet? \r\n[Reference](https://stackoverflow.com/questions/60705802/importerror-cannot-import-name-pywrap-tensorflow)", "Sorry,the existing code is based on python 2.7.Let me try it later.", "Ok @zdj-creator ,Feel free to close this issue if it gets resolved .", "Sorry,I was doing my homework all day ,I'll try it now.", "Yes,it does work on python 3.6,but I must run my code on python 2.7.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52282\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52282\">No</a>\n"]}, {"number": 52281, "title": "RuntimeError: Encountered unresolved custom op: ReorderAxes. See instructions: https://www.tensorflow.org/lite/guide/ops_customNode number 284 (ReorderAxes) failed to prepare.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- TensorFlow installed from (source or binary): Colab \r\n- TensorFlow version (or github SHA if from source): convert model in TF1.x / interpreter in  latest tf-nightly.\r\n\r\nI convert a model to tflite using following codes:\r\n\r\n    co = tf.compat.v1.lite.TFLiteConverter.from_saved_model(modelName)\r\n    co.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n    co.allow_custom_ops=True\r\n\r\nThere are no error in converting.But when I try to interpreter the tfliter model, I encoder error:\r\n--> 916     self._interpreter.Invoke()\r\n    917 \r\n    918   def reset_all_variables(self):\r\n\r\n**RuntimeError: Encountered unresolved custom op: ReorderAxes.**\r\nSee instructions: https://www.tensorflow.org/lite/guide/ops_customNode number 284 (ReorderAxes) failed to prepare.\r\nCould you please tell me how to solve this?\r\n", "comments": ["When I just setting select_tf_ops and convert model successful and then interpreter :\r\n\r\n      co.target_spec.supported_ops = [tf.lite.OpsSet.SELECT_TF_OPS]\r\n\r\n**I get another error message:**\r\nRuntimeError: Encountered unresolved custom op: FusedBiasAct.\r\nSee instructions: https://www.tensorflow.org/lite/guide/ops_customNode number 317 (FusedBiasAct) failed to prepare.\r\n\r\nHere is  **FuseBiasAct ops** link: https://github.com/NVlabs/stylegan2/blob/master/dnnlib/tflib/ops/fused_bias_act.py\r\nBTW:  This tflite model **can  interpreter without gpu only cpu in Colab**, I don't know why different between GPU and CPU.", "@ymzlygw Could you please let us know which TF version you are using and In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "@sushreebarsa \r\nHere are the code that using generated tflite file in Google Colab ,.\r\nTo reproduce the error, you can just set the run_time_type to gpu in Colab and run following code(Put the tflite in right path) : \r\n\r\n        import numpy as np\r\n        import tensorflow as tf\r\n        \r\n        import matplotlib.pyplot as plt\r\n        \r\n        # stylegan2_justtinpinkney\r\n        interpreter = tf.lite.Interpreter(model_path=\"XXX.tflite\")\r\n        \r\n        interpreter.allocate_tensors()\r\n        input_details = interpreter.get_input_details()\r\n        output_details = interpreter.get_output_details()\r\n        print(input_details)\r\n        print(output_details)\r\n        input_shape = input_details[0]['shape']\r\n        \r\n        random_input = np.random.randn(1, 512).astype('float32')\r\n        interpreter.set_tensor(input_details[0]['index'], random_input)\r\n        interpreter.invoke()\r\n        output_data = interpreter.get_tensor(output_details[0]['index'])\r\n        \r\n        \r\n\r\nHowever, when you set the run time type to None. The above code can run correctly. \r\nTflite file download code in colab: \r\n\r\n    !gdown \"https://drive.google.com/uc?id=16zZls_wiv-t4DWxh7EL7SzxV9W9zoEGU\"", "@ymzlygw  Thank you for the update! Was able to reproduce the issue on colab using `TF 2.6 `and `tf-nightly` on `gpu runtime`, Please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/4774bc7fd89fb3430868361dd4761c98/untitled448.ipynb). This issue is not replicating on [`TF 2.6-cpu` ](https://colab.research.google.com/gist/sushreebarsa/b9ef0cd01875e6d6bba53e510396af3e/untitled446.ipynb)runtime.Could you please find the  attached gists and let us know if it is the right way to reproduce the issue? Thanks!", "> @ymzlygw Thank you for the update! Was able to reproduce the issue on colab using `TF 2.6 `and `tf-nightly` on `gpu runtime`, Please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/4774bc7fd89fb3430868361dd4761c98/untitled448.ipynb). This issue is not replicating on [`TF 2.6-cpu` ](https://colab.research.google.com/gist/sushreebarsa/b9ef0cd01875e6d6bba53e510396af3e/untitled446.ipynb)runtime.Could you please find the attached gists and let us know if it is the right way to reproduce the issue? Thanks!\r\n\r\n@sushreebarsa Thanks for you reply.\r\nAnd sorry I should make a more detailed error occurs situation as follows:\r\n\r\n**Without change any_code in** \r\nhttps://github.com/NVlabs/stylegan2/blob/81ae61d7753c6a7b5a38c29aa81aed15adb19c2f/dnnlib/tflib/ops/fused_bias_act.py#L64-L66. \r\nhttps://github.com/NVlabs/stylegan2/blob/81ae61d7753c6a7b5a38c29aa81aed15adb19c2f/dnnlib/tflib/ops/upfirdn_2d.py#L58-L60\r\nI mean: if change **[ 'cuda': _fused_bias_act_cuda] --> ['cuda': _fused_bias_act_ref]**  and  **['cuda': _upfirdn_2d_cuda] --> ['cuda': _upfirdn_2d_ref]** ,  then it can be converted to a tflite_model that can interpreter in cpu run_time as above.\r\n\r\n1):**Tflite Model_1 download:** \r\n\r\n    !gdown \"https://drive.google.com/uc?id=1fdhClAZHUIiJoIhCmTlzZi3qCPaHIu-z\"\r\n1): tflite model converted in tf1.15.2 with setting as follows:\r\n\r\n    co.target_spec.supported_ops = [tf.lite.OpsSet.SELECT_TF_OPS]\r\n1): interpreter in **tf 1.15.2** and get error:\r\n\r\n    RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you invoke the Flex delegate before inference.Node number 0 (FlexConst) failed to prepare.\r\n1): interpreter in latest tf-nightly and get error:\r\n\r\n    RuntimeError: Encountered unresolved custom op: FusedBiasAct.See instructions: https://www.tensorflow.org/lite/guide/ops_customNode number 317 (FusedBiasAct) failed to prepare.\r\n\r\n2): 1):**Tflite Model_2 download:** \r\n\r\n    !gdown \"https://drive.google.com/uc?id=1DvFOmx_VY2hmJASMeFyW9mB6mLSEnSdJ\"\r\n2): tflite model converted in tf1.15.2 with setting as follows:\r\n\r\n     co.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n2): interpreter in **tf 1.15.2** and get error:\r\n\r\n    RuntimeError: tensorflow/lite/kernels/strided_slice.cc new_axis_mask is not implemented yet.Node number 1 (STRIDED_SLICE) failed to prepare.\r\n2): interpreter in latest tf-nightly and get error:\r\n\r\n    RuntimeError: Encountered unresolved custom op: FusedBiasAct.See instructions: https://www.tensorflow.org/lite/guide/ops_customNode number 29 (FusedBiasAct) failed to prepare.\r\n", "> > @ymzlygw Thank you for the update! Was able to reproduce the issue on colab using `TF 2.6 `and `tf-nightly` on `gpu runtime`, Please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/4774bc7fd89fb3430868361dd4761c98/untitled448.ipynb). This issue is not replicating on [`TF 2.6-cpu` ](https://colab.research.google.com/gist/sushreebarsa/b9ef0cd01875e6d6bba53e510396af3e/untitled446.ipynb)runtime.Could you please find the attached gists and let us know if it is the right way to reproduce the issue? Thanks!\r\n> \r\n> @sushreebarsa Thanks for you reply and gist. And sorry I should make a more detailed error occurs situation as follows:\r\n> \r\n> **Without change any_code in** https://github.com/NVlabs/stylegan2/blob/81ae61d7753c6a7b5a38c29aa81aed15adb19c2f/dnnlib/tflib/ops/fused_bias_act.py#L64-L66. https://github.com/NVlabs/stylegan2/blob/81ae61d7753c6a7b5a38c29aa81aed15adb19c2f/dnnlib/tflib/ops/upfirdn_2d.py#L58-L60 I mean: if change **[ 'cuda': _fused_bias_act_cuda] --> ['cuda': _fused_bias_act_ref]** and **['cuda': _upfirdn_2d_cuda] --> ['cuda': _upfirdn_2d_ref]** , then it can be converted to a tflite_model that can interpreter in cpu run_time as above.\r\n> \r\n> 1):**Tflite Model_1 download:**\r\n> \r\n> ```\r\n> !gdown \"https://drive.google.com/uc?id=1fdhClAZHUIiJoIhCmTlzZi3qCPaHIu-z\"\r\n> ```\r\n> \r\n> 1): tflite model converted in tf1.15.2 with setting as follows:\r\n> \r\n> ```\r\n> co.target_spec.supported_ops = [tf.lite.OpsSet.SELECT_TF_OPS]\r\n> ```\r\n> \r\n> 1): interpreter in **tf 1.15.2** and get error:\r\n> \r\n> ```\r\n> RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you invoke the Flex delegate before inference.Node number 0 (FlexConst) failed to prepare.\r\n> ```\r\n> \r\n> 1): interpreter in latest tf-nightly and get error:\r\n> \r\n> ```\r\n> RuntimeError: Encountered unresolved custom op: FusedBiasAct.See instructions: https://www.tensorflow.org/lite/guide/ops_customNode number 317 (FusedBiasAct) failed to prepare.\r\n> ```\r\n> \r\n> 2): 1):**Tflite Model_2 download:**\r\n> \r\n> ```\r\n> !gdown \"https://drive.google.com/uc?id=1DvFOmx_VY2hmJASMeFyW9mB6mLSEnSdJ\"\r\n> ```\r\n> \r\n> 2): tflite model converted in tf1.15.2 with setting as follows:\r\n> \r\n> ```\r\n>  co.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n> ```\r\n> \r\n> 2): interpreter in **tf 1.15.2** and get error:\r\n> \r\n> ```\r\n> RuntimeError: tensorflow/lite/kernels/strided_slice.cc new_axis_mask is not implemented yet.Node number 1 (STRIDED_SLICE) failed to prepare.\r\n> ```\r\n> \r\n> 2): interpreter in latest tf-nightly and get error:\r\n> \r\n> ```\r\n> RuntimeError: Encountered unresolved custom op: FusedBiasAct.See instructions: https://www.tensorflow.org/lite/guide/ops_customNode number 29 (FusedBiasAct) failed to prepare.\r\n> ```\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52281\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52281\">No</a>\n", "Anyone helps?", "@ymzlygw You need to write your own custom version of the `FusedBiasAct` op in TFLite. The [TFLite custom ops page](https://www.tensorflow.org/lite/guide/ops_custom) is unfortunately incomplete (we don't have the bandwidth to update it) but the [TF custom op page](https://github.com/tensorflow/custom-op), though not relevant to you, is more detailed. \r\n\r\nAnother option is, if you can substitute `FusedBiasAct` with any of the operators [here](https://www.tensorflow.org/lite/guide/op_select_allowlist) then you can simply use the [TF operator in TFLite](https://www.tensorflow.org/lite/guide/ops_select).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52281\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52281\">No</a>\n"]}, {"number": 52280, "title": "Add Metadata Text Fields Before Model Trained, Enabling Clear Line of IP Ownership.", "body": "Feature Request: Add Metadata Text Fields to TF API, Just Before Training Model, Enabling Clear Line of IP Ownership.\r\n\r\nTF API could contain metadata text fields: \r\n1) Model Name\r\n2) Model Version\r\n3) Author Name(s)\r\n4) Author email Contact\r\n5) Content Source(s) Used to Train Model\r\n6) Source Content Copyright Author(s), Owner(s), Holder(s), Copyright Registration Numbers from copyright.gov\r\n7) 256 Character Free Form Text Field\r\n8) QR code\r\n\r\nBENEFITS\r\n1) Clear Line of IP ownership, authorship. Independent inventor(s), engineer(s) and corporations need to easily demonstrate clear line of IP ownership from original dataset copyrights to machine learning model. \r\n\r\n2) API example tf.model.metadata, tf.model.info, tf.model.qrcode. Specific metadata: Exact model name, model version, creator name, source data used, copyright registration numbers, patent registration numbers of source data and potentially a 256 character free form field. Enables consistent, legitimate line of IP ownership and registrations if model used in a larger project, larger software product or license.\r\n\r\n3) Training to create model only occurs if metadata filled out before model trained. If metadata not filled out meaningfully or correctly by developer, machine learning model users will know and can choose to use model or not. Adds to Author, content source(s) credibiliity. Also enables model users to know exactly or at least more closely what content was used to train model.\r\n\r\n4) Metadata queries with one or more: command line, API, mobile app, TF utility or operating system file properties to easily view Properties/metadata of model. Once model trained, metadata needs to be read only, embedded in model as to not interfere with model and not be altered in model.\r\n\r\n5) IP more defensible in contracts, disputes, sale/purchase of IP, partnering, gaining potential investment in software projects, valuations.\r\n\r\nDRAWBACKS\r\n1) Will add miniscule amount of text to model somewhere- at end of model? \r\n2) Formal specification needed, project owner, developer time.\r\n3) Need to ensure data cannot be erased, altered or truncated once model is trained, created.\r\n", "comments": ["Hi, Could you please move this feature request under https://github.com/keras-team/keras repo since it is related to tf.keras.models.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 52279, "title": "support TensorRT 8.2", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.7.0rc0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nTensorRT Optimization of tensorflow model,  currently TensorRT is not supported\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who use TensorRT\r\n\r\n**Any Other info.**\r\n", "comments": ["@alanpurple, Can you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose).Also can you please elaborate about your Feature and please specify the Use Cases for this feature. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 52278, "title": "W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found 2021-10-06 23:46:08.198654: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@HafsaBano \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),Please refer to the similar [issue1](https://stackoverflow.com/questions/49397373/tensorflow-gpu-importerror-could-not-find-nvcuda-dll/49397448), [issue2](https://stackoverflow.com/questions/61370105/import-tensorflow-cpu-error-nvcuda-dll-not-found) and let us know if it helps?\r\nThanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52278\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52278\">No</a>\n"]}, {"number": 52276, "title": "[TF:TRT] Fix native segment execution with int32 tensors", "body": "If the TRTEnginOp falls back to native segment execution then we have a problem with device placement of int32 tensors:\r\n- the native segment expects these to be on the host,\r\n- the TRTEnginOp is assigned to  GPU, therefore all of its inputs / outputs are on the GPU.\r\n\r\nThe existing workaround 8b5102cf53 sets IntsOnDevice attribute for the native segment in case we had int32 inputs. It does not handle int32 outputs the same way, becaues that would break native segment execution of CombinedNMS. To fix int32 output, we had an explicit copy inserted into the native segment 6f2656ce2e.\r\n\r\nThe existing workround did not cover all the possible cases that can produce int32 output. This PR reverts the existing workaround (6f2656ce2e and 8b5102cf53) and inserts explicit copy operations before and after the native segment is called.\r\n\r\nTracker: #45481\r\n\r\nTagging @bixia1 for review.\r\n\r\n", "comments": ["@bixia1 I have fixed the remaining issue."]}, {"number": 52275, "title": "RuntimeError: Encountered unresolved custom op: BoostedTreesEnsembleResourceHandleOp. Node number 0 (BoostedTreesEnsembleResourceHandleOp) failed to prepare.", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.8.0-dev20211004\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nConverter:\r\n \r\n        # Convert the model\r\n        saved_model_obj = tf.saved_model.load(self.pb_model_path)\r\n        concrete_func = saved_model_obj.signatures['predict']\r\n        converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\n\r\n        converter.experimental_new_converter = True\r\n\r\n        converter.target_spec.supported_ops = [\r\n            tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\r\n            tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.\r\n        ]\r\n        converter.allow_custom_ops = True\r\n\r\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\n        tflite_model = converter.convert()\r\n\r\nInference:\r\n\r\n        test_data = np.expand_dims(data, axis=0).astype(np.float32)\r\n\r\n        interpreter.resize_tensor_input(input_index, [test_data.shape[0], test_data.shape[1]])\r\n\r\n        interpreter.allocate_tensors()\r\n\r\n        interpreter.set_tensor(input_index, test_data)\r\n\r\n        # Run inference.\r\n        interpreter.invoke()\r\n\r\n        # Post-processing: remove batch dimension and find the digit with highest probability.\r\n        output = interpreter.get_tensor(output_index)\r\n\r\n\r\nBut, \r\n\r\nThe output from the converter invocation:\r\n\r\n> Traceback (most recent call last):\r\n> File \"/home/aiteam/daeho/PomaUpdrs/runTensorFlowModels/PomaGBT/poma_GBT_estimator_ver_tflite_inference.py\", line 122, in <module>\r\n>     print(tf_lite_inference(model_path='/home/aiteam/daeho/PomaUpdrs/TFLite_models/GBT_TFLite_models/poma_GBT_TFLite_Model/poma_gbt_tflite_20211006_160120.tflite',\r\n>   File \"/home/aiteam/daeho/PomaUpdrs/runTensorFlowModels/PomaGBT/poma_GBT_estimator_ver_tflite_inference.py\", line 69, in tf_lite_inference\r\n>     interpreter.allocate_tensors()\r\n>   File \"/home/aiteam/.conda/envs/daehoPython38/lib/python3.8/site-packages/tensorflow/lite/python/interpreter.py\", line 514, in allocate_tensors\r\n>     return self._interpreter.AllocateTensors()\r\n> RuntimeError: Encountered unresolved custom op: BoostedTreesEnsembleResourceHandleOp.\r\n> See instructions: https://www.tensorflow.org/lite/guide/ops_customNode number 0 (BoostedTreesEnsembleResourceHandleOp) failed to prepare.\r\n\r\n\r\n\r\nI'm wondering why the flex delegate couldn't prepare the BoostedTreesEnsembleResourceHandleOp function, even though I have the SELECT_TF_OPS / experimental_new_converter /  allow_custom_ops flag enabled?", "comments": ["Hi @handaeho! Could you please look at these similar issues. [link1](https://github.com/tensorflow/tensorflow/issues/41012).[link2](https://medium.com/@bsramasubramanian/running-a-tensorflow-lite-model-in-python-with-custom-ops-9b2b46efd355)", "So you mean build the function for BoostedTreesEnsembleResourceHandleOp as Custom Operators?\r\n\r\nsorry. I'm not familiar with C++  :(", "Ok @handaeho ,Could you please provide a link to model file too ?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 52274, "title": "r2.7 cherry-pick request: Fix missing-device unit test failures ", "body": "Fix oneDNN-related test failures that were because of missing device settings.\r\n\r\nOriginal PR has been merged to master: https://github.com/tensorflow/tensorflow/pull/52021/", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52274) for more info**.\n\n<!-- need_author_consent -->", "Manually setting CLA to yes because the commit is from a PR that is already merged into master (https://github.com/tensorflow/tensorflow/pull/52021).", "I think this PR https://github.com/tensorflow/tensorflow/pull/52551 needs to be cherry-picked as well to fix the MAC build issue."]}, {"number": 52273, "title": "Update README.md", "body": null, "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52273) for more info**.\n\n<!-- need_sender_cla -->", "We will not be encouraging one liner grammatical changes as this is expensive process, thank you for your interest.\r\nCC @mihaimaruseac"]}, {"number": 52271, "title": "[TF:TRT] Add structured op converter", "body": "Adds StructuredOpConverter as a Curiously recurring template pattern (CRTP) template class. This is to prepare for registering more future op converters or refactoring of existing op converters. An example for using StructuredOpConverter is provided in op_converter_registry_test.\r\n\r\nPR's #52248 and #52188 use this class for adding new QDQ op converters and refactoring the resize op converters, respectively.", "comments": ["I updated the PR description, please check.", "@christopherbate Can you please check @bixia1's comments and keep us posted ? Thanks!", "There are still a few comments that aren't addressed. @christopherbate can you please check?", "I just pushed changes and addressed everything that except unresolved comments\r\n\r\n", "Pushed small change that fixes the new `_test.cc` file missing from BUILD file"]}, {"number": 52270, "title": "Make scatter ops deterministic.", "body": "The GPU deterministic implementation copies the variable to the CPU, runs the scatter op, then copies the result back to the GPU, which is what the Scatter ND ops already do.\r\n\r\nOnly the ResourceVariable scatter ops are made deterministic, not legacy ref variables.", "comments": []}, {"number": 52269, "title": "[TF:TRT] Remove old ConvertBiasAdd specialization", "body": "Removes a branch and helper function for TF-TRT's ConvertBiasAdd converter. The referenced optimization no longer applies in supported versions of TensorRT, and the code in the specialized function `ConvertBiasAddInt8WithoutCalibration` does not handle modes other than non-implicit batch mode.", "comments": []}, {"number": 52268, "title": "[TF:TRT] Support GatherV2 op when indices is constant", "body": "Convert the 'indices' input to a GatherV2 op into a constant layer when 'indices' is a constant.\r\n\r\nModify the unit tests to generate constant indices.\r\n\r\nSigned-off-by: Meenakshi Venkataraman <meenakshiv@nvidia.com>\r\n\r\nTagging @bixia1 for review\r\nCC: @tfeher @DEKHTIARJonathan", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52268) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "In your PR description, you have:\r\nNote: Support for this op in TRT implicit batch mode when both input and indices are both constants is limited to the case when batch size is 1.\r\n\r\nwe actually require batch size 1 when (1) both input and indices are tensors, or (2) both input and indices are constants.\r\nBut why do we add the case for (2), as we apply constant folding? In TF-TRT bridge, we report a problem when we see constant operations like this.", "@bixia1 , regarding your comment:\r\n\r\n> we actually require batch size 1 when (1) both input and indices are tensors, or (2) both input and indices are constants.\r\n> But why do we add the case for (2), as we apply constant folding? In TF-TRT bridge, we report a problem when we see constant operations like this.\r\n\r\nThanks for pointing this out.\r\n1)  is already covered in the converter -- and we flag it.\r\n2) I added support for with this patch. Previously we weren't supporting  indices as constant at all, now we do.\r\n\r\nI could remove my note on the PR, as it sounds redundant.\r\n\r\n", "Looks good to me. I modified the PR description, please check.\r\nWould you please squash your commits into one before I approve it? This is because the commit history will become part of the commit message if you don't squash the commits.", "@bixia1  -- squash done.", "I am working on fixing some minor format issues and merging this PR from my end."]}, {"number": 52267, "title": "[r2.7] Bump tensorflow-io-gcs-filesystem to 0.21.0", "body": "**NOTE: This PR is a cherry-pick of #51976**\r\n\r\nThis PR bumps tensorflow-io-gcs-filesystem to 0.21.0 so that TF 2.7\r\nbranch cut can carry the latest change which includes the fix that removes the extra\r\nbuild directory in the python pip package (https://github.com/tensorflow/io/pull/1497)\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks. Will merge this as soon as we land RC0 (we want to limit number of changes that go in RC0 compared to master at branch cut). Hopefully by end of week it should be merged", "(it seems we already merged these with another cherry-pick when solving merge conflicts in that)", "@mihaimaruseac Thanks for the help :+1 !"]}, {"number": 52264, "title": "Add single/parametrized test run doc", "body": "https://github.com/keras-team/keras/issues/15338 but for Tensorflow core\r\n\r\n/cc @lamberta @MarkDaoust @mihaimaruseac ", "comments": []}, {"number": 52263, "title": "Pruned TFLite model has invalid model identifier after post training quantisation", "body": "### 1. System information\r\n\r\n- Linux Ubuntu 20.04\r\n- Pip TensorFlow package version 2.6.0\r\n\r\n### 2. Code\r\nReproducable code in this [gist](https://colab.research.google.com/gist/niciBume/2fb8d9eb36d5e13d8faaea9e1273e980/tensorflow-lite-debugger-colab.ipynb#scrollTo=RD0CEfccGN2C)\r\n\r\n### 3. Description\r\nAfter pruning a keras model according to [tf colab](https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras), the post training quantized tflite can't be invoked (the quantisation works for the un-pruned model).\r\n\r\n`interpreter` = tf.lite.Interpreter(model_path=MODEL_TF_PRUNE)`\r\n\r\nLeads to the error:\r\n```\r\nValueError                                Traceback (most recent call last)\r\n\r\n<ipython-input-56-edee2625144a> in <module>()\r\n----> 1 interpreter = tf.lite.Interpreter(model_path=MODEL_TF_PRUNE)\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/interpreter.py in __init__(self, model_path, model_content, experimental_delegates, num_threads, experimental_op_resolver_type, experimental_preserve_all_tensors)\r\n    365           _interpreter_wrapper.CreateWrapperFromFile(\r\n    366               model_path, op_resolver_id, custom_op_registerers_by_name,\r\n--> 367               custom_op_registerers_by_func, experimental_preserve_all_tensors))\r\n    368       if not self._interpreter:\r\n    369         raise ValueError('Failed to open {}'.format(model_path))\r\n\r\nValueError: Model provided has model identifier '\r\n\u001a\r\n', should be 'TFL3'\r\n```\r\n", "comments": ["@niciBume \r\nPlease refer to these solutions and let us know: [link](https://github.com/tensorflow/tensorflow/issues/32204#issuecomment-703506041),[link1](https://stackoverflow.com/questions/56857400/converting-mobilenet-segmentation-model-to-tflite)", "@Saduf2019 \r\nThanks for the reply. Regarding the [first link](https://github.com/tensorflow/tensorflow/issues/32204#issuecomment-703506041), the problem arises because the tflite file was downloaded and it somehow downloaded a html file (if I am not mistaking), so I don't see how I could solve this issue using the stated method?\r\n\r\n\r\n", "Oh wow guys... I found the error. \r\nProblem was on [this line](https://colab.research.google.com/gist/niciBume/2fb8d9eb36d5e13d8faaea9e1273e980/tensorflow-lite-debugger-colab.ipynb#scrollTo=YT8OIvQiKDhg&line=1&uniqifier=1) I was feeding the interpreter a `.h5` model and not the actual pruned TFlite file.\r\n\r\nSorry for wasting your time!\r\n\r\nTLDR: Quantizing pruned networks works if you feed the interpreter the correct model...", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52263\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52263\">No</a>\n", "@niciBume \r\nThank you for the update glad your issue is resolved."]}]