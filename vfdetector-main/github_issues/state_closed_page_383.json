[{"number": 42502, "title": "Missing momentum in documentation of ResourceApplyCenteredRMSProp", "body": "The issue is for this page: https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/resource-apply-centered-r-m-s-prop\r\n\r\n`momentum` is missing from the list of arguments.\r\n\r\nCurrent doc has this:\r\n\r\n```\r\nscope: A Scope object\r\nvar: Should be from a Variable().\r\nmg: Should be from a Variable().\r\nms: Should be from a Variable().\r\nmom: Should be from a Variable().\r\nlr: Scaling factor. Must be a scalar.\r\nrho: Decay rate. Must be a scalar.\r\nepsilon: Ridge term. Must be a scalar.\r\ngrad: The gradient.\r\n```\r\n\r\nThis should be instead like this (momentum added after rho):\r\n\r\n```\r\nscope: A Scope object\r\nvar: Should be from a Variable().\r\nmg: Should be from a Variable().\r\nms: Should be from a Variable().\r\nmom: Should be from a Variable().\r\nlr: Scaling factor. Must be a scalar.\r\nrho: Decay rate. Must be a scalar.\r\nmomentum: momentum scale. Must be a scalar.\r\nepsilon: Ridge term. Must be a scalar.\r\ngrad: The gradient.\r\n```\r\n\r\n### Clear description\r\nThis could be due to a bug in the scripts that auto-generate the api-def.\r\n\r\nThis is the op registration from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/training_ops.cc#L1056-L1069 \r\n\r\n```\r\nREGISTER_OP(\"ResourceApplyCenteredRMSProp\")\r\n    .Input(\"var: resource\")\r\n    .Input(\"mg: resource\")\r\n    .Input(\"ms: resource\")\r\n    .Input(\"mom: resource\")\r\n    .Input(\"lr: T\")\r\n    .Input(\"rho: T\")\r\n    .Input(\"momentum: T\")\r\n    .Input(\"epsilon: T\")\r\n    .Input(\"grad: T\")\r\n    .Attr(\"T: numbertype\")\r\n    .Attr(\"use_locking: bool = false\")\r\n    .SetShapeFn(\r\n        ApplyCenteredRMSPropShapeFn</*is_sparse=*/false, /*is_resource=*/true>);\r\n```\r\n", "comments": ["@lamberta I would like to take this one up regarding the change in the documentation. Is it okay?", "@ashwin-phadke Please do! Thanks", "It seems as though momentum was only missed in the C++ documentation as it appears in the python documentation which can be seen here : https://www.tensorflow.org/api_docs/python/tf/raw_ops/ResourceApplyCenteredRMSProp.\r\n\r\n", "@lamberta : I believe https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_ResourceApplyCenteredRMSProp.pbtxt is the right place for the proposed document edit. \r\n\r\nAs a initial contributor I will just need to change the file and not build the entire `tf` or will I need to build the repo to check if the doc was properly generated after editing the file? [This](https://www.tensorflow.org/community/contribute/docs_ref) mentions what needs to be done for Python documentation, however I cannot find the same for C++\r\n\r\nPlease confirm for both.", "I created a change for [ApplyCenteredRMSProp](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/apply-centered-r-m-s-prop) whereas @pooyadavoodi has mentioned that momentum is missing in [ResourceApplyCenteredRMSProp](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/resource-apply-centered-r-m-s-prop) documentation. \r\n\r\nIt slipped my sight while making the change.\r\n\r\n1) ApplyCenteredRMSProp - needs to be changed too?  - @pooyadavoodi \r\n2) I will create the change for ResourceApplyCenteredRMSProp. - created the change\r\n", "Thank you @ashwin-phadke !\r\n\r\n@MarkDaoust Is it possible to build the Markdown for the OSS C++ API docs?", "Rebuild them to publish this change? \r\n\r\nNo. This just went in, and the cc docs only show latest stable (no nightly option). This will get published to the site with 2.4.", "> 1. ApplyCenteredRMSProp - needs to be changed too?  - @pooyadavoodi\r\nI think so. I thought the pbtxt files are auto-generated but @mihaimaruseac says no. It would be good to automate this to avoid future bugs like this.\r\n", "This should be fixed by #42624"]}, {"number": 42501, "title": "Update CONTRIBUTING.md", "body": "Fixed typos", "comments": ["cc @mihaimaruseac ", "@mihaimaruseac thank you"]}, {"number": 42500, "title": "tf.dynamic_partition causes crash when using multiple GPUs via tf.distribute.MirroredStrategy", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04.2**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **2.3.0**\r\n- Python version: **3.6.9**\r\n- Bazel version (if compiling from source): **N/A**\r\n- GCC/Compiler version (if compiling from source): **N/A**\r\n- CUDA/cuDNN version: **CUDA 10.1 / cuDNN 7.6.1**\r\n- GPU model and memory: **RTX 2080 8GB**\r\n\r\n**Describe the current behavior**\r\n\r\nThe `tf.dynamic_partition` operation crashes when running on multiple GPUs using `tf.distribute.MirroredStrategy`.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe same code, also using `tf.distribute.MirroredStrategy` runs succesfully when limited to a single GPU by setting `CUDA_VISIBLE_DEVICES=0`.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n    import tensorflow as tf\r\n\r\n    N = 100\r\n    M = 4\r\n\r\n    distribute_strategy = tf.distribute.MirroredStrategy()\r\n\r\n    def op():\r\n      data = tf.random.uniform((N,))\r\n      partitions = tf.random.uniform((N,), maxval=M, dtype=tf.int32)\r\n      return tf.dynamic_partition(data, partitions, M)\r\n\r\n    distribute_strategy.run(op)\r\n\r\n**Other info / logs**\r\n\r\nFull output of the above code:\r\n\r\n```\r\n2020-08-19 12:05:36.898086: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-08-19 12:05:37.828508: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-08-19 12:05:37.900555: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:09:00.0 name: GeForce RTX 2080 computeCapability: 7.5\r\ncoreClock: 1.83GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.23GiB/s\r\n2020-08-19 12:05:37.901044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: \r\npciBusID: 0000:42:00.0 name: GeForce RTX 2080 computeCapability: 7.5\r\ncoreClock: 1.83GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.23GiB/s\r\n2020-08-19 12:05:37.901070: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-08-19 12:05:37.902404: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-08-19 12:05:37.903988: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-08-19 12:05:37.904184: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-08-19 12:05:37.905511: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-08-19 12:05:37.906204: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-08-19 12:05:37.908753: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-08-19 12:05:37.910997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1\r\n2020-08-19 12:05:37.911427: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-08-19 12:05:37.938447: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2994045000 Hz\r\n2020-08-19 12:05:37.941540: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x49ac240 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-08-19 12:05:37.941596: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-08-19 12:05:42.073849: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4a182c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-08-19 12:05:42.073925: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080, Compute Capability 7.5\r\n2020-08-19 12:05:42.073967: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce RTX 2080, Compute Capability 7.5\r\n2020-08-19 12:05:42.075578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:09:00.0 name: GeForce RTX 2080 computeCapability: 7.5\r\ncoreClock: 1.83GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.23GiB/s\r\n2020-08-19 12:05:42.076318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: \r\npciBusID: 0000:42:00.0 name: GeForce RTX 2080 computeCapability: 7.5\r\ncoreClock: 1.83GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.23GiB/s\r\n2020-08-19 12:05:42.076367: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-08-19 12:05:42.076406: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-08-19 12:05:42.076433: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-08-19 12:05:42.076457: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-08-19 12:05:42.076478: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-08-19 12:05:42.076499: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-08-19 12:05:42.076524: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-08-19 12:05:42.079838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1\r\n2020-08-19 12:05:42.079887: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-08-19 12:05:42.939356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-08-19 12:05:42.939406: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 1 \r\n2020-08-19 12:05:42.939413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N N \r\n2020-08-19 12:05:42.939417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 1:   N N \r\n2020-08-19 12:05:42.941258: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\r\n2020-08-19 12:05:42.941298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7252 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:09:00.0, compute capability: 7.5)\r\n2020-08-19 12:05:42.942216: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\r\n2020-08-19 12:05:42.942237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 2566 MB memory) -> physical GPU (device: 1, name: GeForce RTX 2080, pci bus id: 0000:42:00.0, compute capability: 7.5)\r\nWARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.\r\n2020-08-19 12:05:42.972712: F tensorflow/core/kernels/dynamic_partition_op_gpu.cu.cc:108] Non-OK-status: GpuLaunchKernel(GatherOpKernel<T, int32, true>, config.block_count, config.thread_per_block, 0, d.stream(), params, indices, out, gather_dim_size, indices_size, slice_size, out_size) status: Internal: invalid resource handle\r\nAborted (core dumped)\r\n```\r\n\r\n", "comments": ["@drebain \r\n\r\nI have tried in colab with Tensorflow -GPU version 2.3.0 and i am not seeing any issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/1179cb07cd83075aacbfa1dc7835e8f9/untitled267.ipynb).Thanks!", "The output includes `INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)` which I think indicates that it is only running with a single GPU available. The crash only happens when there are at least two devices present. I don't know if this is possible to test with colab.", "This might have something to do with the version of CUDA, or the type of GPU being used (see [similar issue](https://github.com/tensorflow/tensorflow/issues/30665). Could you try this with the latest version of TF?\r\n\r\n`!pip install tf-nightly-gpu`", "I have encountered the same \"invalid resource handle\" crash with Tensorflow 2.2.0 and 2.3.0 on machines with RTX 2080 as well as V100 GPUs. I have tried CUDA 10.0, 10.1, and 10.2.\r\n\r\nI have just now updated my personal machine to the NVIDIA 450 driver and installed CUDA 11.0 + tf-nightly-gpu:\r\n```\r\n2020-08-22 18:24:52.424497: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\r\n2020-08-22 18:24:54.262314: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-08-22 18:24:54.262349: I tensorflow/compiler/jit/xla_gpu_device.cc:69] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-08-22 18:24:54.271839: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-08-22 18:24:54.354837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:09:00.0 name: GeForce RTX 2080 computeCapability: 7.5\r\ncoreClock: 1.83GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.23GiB/s\r\n2020-08-22 18:24:54.355405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: \r\npciBusID: 0000:42:00.0 name: GeForce RTX 2080 computeCapability: 7.5\r\ncoreClock: 1.83GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.23GiB/s\r\n2020-08-22 18:24:54.355431: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\r\n2020-08-22 18:24:54.372287: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11\r\n2020-08-22 18:24:54.383247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-08-22 18:24:54.387885: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-08-22 18:24:54.405073: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-08-22 18:24:54.409284: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11\r\n2020-08-22 18:24:54.411034: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8\r\n2020-08-22 18:24:54.415315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1\r\n2020-08-22 18:24:54.416756: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-08-22 18:24:54.420115: I tensorflow/compiler/jit/xla_cpu_device.cc:54] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-08-22 18:24:54.420145: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-08-22 18:24:54.664832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:09:00.0 name: GeForce RTX 2080 computeCapability: 7.5\r\ncoreClock: 1.83GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.23GiB/s\r\n2020-08-22 18:24:54.665355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: \r\npciBusID: 0000:42:00.0 name: GeForce RTX 2080 computeCapability: 7.5\r\ncoreClock: 1.83GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.23GiB/s\r\n2020-08-22 18:24:54.665387: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\r\n2020-08-22 18:24:54.665407: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11\r\n2020-08-22 18:24:54.665418: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-08-22 18:24:54.665428: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-08-22 18:24:54.665437: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-08-22 18:24:54.665446: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11\r\n2020-08-22 18:24:54.665459: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8\r\n2020-08-22 18:24:54.667603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1\r\n2020-08-22 18:24:54.668415: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\r\n2020-08-22 18:24:56.003639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-08-22 18:24:56.003708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 1 \r\n2020-08-22 18:24:56.003716: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N N \r\n2020-08-22 18:24:56.003720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 1:   N N \r\n2020-08-22 18:24:56.008774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7253 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:09:00.0, compute capability: 7.5)\r\n2020-08-22 18:24:56.012643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 5707 MB memory) -> physical GPU (device: 1, name: GeForce RTX 2080, pci bus id: 0000:42:00.0, compute capability: 7.5)\r\nWARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\r\n2020-08-22 18:24:56.171346: F tensorflow/core/kernels/dynamic_partition_op_gpu.cu.cc:108] Non-OK-status: GpuLaunchKernel(GatherOpKernel<T, int32, true>, config.block_count, config.thread_per_block, 0, d.stream(), params, indices, out, gather_dim_size, indices_size, slice_size, out_size) status: Internal: invalid resource handle\r\nAborted (core dumped)\r\n```\r\nThis is the output of the reproduction script above running with `tf.__version__ == '2.4.0-dev20200822'`", "I think I have found the cause of the issue. The GPU implementation of the dynamic_partition op ends by registering a callback which runs after `ComputeAsync()` returns, but apparently does not ensure that the correct CUDA device is active when the callback executes. As long as the only GPU being used is the default \"GPU:0\" this works, but it can fail when attempting to run on a different device. In fact, I have been able to reproduce the crash with an even simpler script that does not require `MirroredStrategy`:\r\n\r\n    import tensorflow as tf\r\n    N = 100\r\n    M = 4\r\n    with tf.device(\"GPU:1\"):\r\n      data = tf.random.uniform((N,))\r\n      partitions = tf.random.uniform((N,), maxval=M, dtype=tf.int32)\r\n      tf.dynamic_partition(data, partitions, M)\r\n\r\nI have made a [simple patch](https://github.com/drebain/tensorflow/commit/113555fab7c1a607991b41cdf3974f5f8e0873ea) for the dynamic_stitch op that makes the above script (as well as the one in the original issue) work by copying some of the code from before `ComputeAsync()` is called, which I think sets up the CUDA device. I doubt this is a robust or correct solution, but hopefully it saves someone some time debugging.\r\n\r\nAs I mentioned before, I have consistently been able to reproduce this under a variety of conditions, but it's possible that the error is non-deterministic if the scheduling of the callback has an effect.", "Thanks @drebain for digging into the root cause! We have been able to repro the issue as well on different GPUs, and do not believe it is specific to GPU or TF version. Assigning to @sanjoy to take a look further. ", "Hi @drebain,\r\n\r\nYour fix looks correct to me; would you be willing to create a PR fixing the bug and adding the test?  You can use [`collective_nccl_test`](https://github.com/tensorflow/tensorflow/blob/09f5609f0fd282943defd4608ee90bb6883a394b/tensorflow/core/kernels/BUILD#L240-L261) as an example on how to add a multi-GPU test to TensorFlow.\r\n\r\nYou'll need to switch between ROCm and CUDA though, like we do [here](https://github.com/tensorflow/tensorflow/blob/09f5609f0fd282943defd4608ee90bb6883a394b/tensorflow/core/kernels/segment_reduction_ops_impl.h#L47-L56).", "@sanjoy Yes, I will add a test that makes sure the op can run when launched on any GPU + the ROCm switch and open a PR.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42500\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42500\">No</a>\n"]}, {"number": 42499, "title": "multi-dimensional array frequency count TF API  ", "body": "**System information** \r\n\r\n     OS Platform and Distribution : macOS Catalina 10.15.3\r\n\r\n    TensorFlow installed from : binary\r\n\r\n    TensorFlow version : 1.15.0\r\n\r\n    Python version: 3.7.3\r\n\r\n**Describe the current behavior**\r\n\r\nWe have a tensor \r\n\r\ninput = `tf.Tensor([[1296,266,504,190,44,60,13,2,337,6742,2667,14,1,119,580,338,785,739,855,200,37,1,3,4,5,6],\r\n [1296,266,504,190,44,60,13,2,337,6742,2667,14,1,119,580,338,785,739,855,200,37,1,3,4,5,6]], shape=(2, 29), dtype=int64)`\r\n\r\noutput = `tf.Tensor([[0,2,1, ... 0. 0. 0.][0, 2, 1, ... 0, 0, 0]], shape=(2, 10000), dtype=float32)`\r\n\r\nHere 10000 is the dictionary size. \r\n\r\n**Describe the expected behavior**\r\n\r\nWe want vector output such that for each index it tell the frequency of each element \r\nie in \r\n`[1296  266  504  190   44   60   13    2  337 6742 2667   14    1  119\r\n   580  338  785  739  855  200   37    1    3    4    5    6]` ie we see 0 occurs 0 times , 1 occurs 2 times and so on .\r\n\r\nCurrently what we are getting is `tf.Tensor([[0. 4. 2. ... 0. 0. 0.]], shape=(1, 10000), dtype=float32)` we want of shape (2,10000)\r\n\r\n\r\nPlease tell us the right TF API transformation to do this . We tried code in https://github.com/tensorflow/tensorflow/issues/42374 but with no luck. \r\n", "comments": ["@17patelumang,\r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a TensorFlow bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n", "@amahendrakar  thanks created question - https://stackoverflow.com/questions/63515872/multi-dimensional-array-frequency-count-in-tensorflow-api ", "This is for others who read this ticket , we used recently released https://blog.tensorflow.org/2020/08/introducing-tensorflow-coder-tool.html?linkId=98162087&m=1  and found our fix , super nice tool ! "]}, {"number": 42498, "title": "ConnectionResetByPeer using MultiWorkerMirroredStrategy with Native Keras BERT Model", "body": "**Describe the current behavior**\r\n\r\nI am trying to use the following script to run multi-worker distributed training on a Google Kubernetes Engine (GKE) cluster. The script completes and uploads the trained model when there is only one worker. However, it fails with a connection reset by peer error after some arbitrary number of steps when I use multiple nodes.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nTraining script: https://gist.github.com/Eric-Le-Ge/21fe4df4c8e361c60d20a683b08aa743\r\nOriginal example: https://github.com/tensorflow/tfx/tree/master/tfx/examples/bert/cola\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Container-Optimized OS (cos) (default)\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.6.9\r\n- Docker Image: tensorflow/tensorflow:latest-gpu\r\n- GPU model and memory: NVIDIA K80\r\n\r\n**Other info / logs** \r\nPod execution logs:\r\nhttps://gist.github.com/Eric-Le-Ge/693640219f54cb6350e2ea1cc4d97547\r\n", "comments": ["Hi @Eric-Le-Ge, looking at the logs it's not clear to me that the training started. Where are you seeing that?\r\n\r\nThis seems like it will be a bit tricky to reproduce. For starters, are you seeing this happen every time you try to kick off multi worker training on GKE? If so, can you try out a simpler model to help determine if the problem is with TF or GKE?\r\n\r\nAdditionally, MultiWorkerMirroredStrategy can be run on a single machine without extra setup. Can you confirm that you're able to train successfully on each machine individually? If so, can you provide the logs for the successful training?", "Hi @nikitamaia , thank you for following up on this issue. I've tried training another model with the same setup and it ran successfully, so I think the issue is related to this specific model. In addition, I have successfully trained this model using a single node with the same script, and here are the logs from the worker:\r\nhttps://gist.github.com/Eric-Le-Ge/48380f2c84afa635989dfc82d175d03a\r\n\r\n", "So to clarify, every time you try and run this model with two nodes you get this exact same error `Aborting RingReduce with Unavailable: Connection reset by peer`?\r\nFor the model that you were able to train successfully, was it also from tf hub?\r\nI can try to repro this on my end. Can you provide complete instructions with data and how to run the script?", "Actually one more question -- for the single case it looks like the collective communication was set to RING. And it was AUTO when you trained with two nodes. This shouldn't make a difference, but just to sanity check can you also confirm that the two nodes fail with RING set as well.", "Thanks for following up! Yes, every time I run the script I get the exact same error, this also happens with 4 nodes as well. The other model is directly imported from `tf.keras.applications.MobileNet` and this model does not have the collective communication error. \r\nAlso, when I got the original error the collective communication was set to AUTO. I tried setting it to RING and NCCL and both fail as well.\r\n\r\nI think the error should be relatively easy to reproduce, but it requires setup for running TFX pipelines. The original example is here: https://github.com/tensorflow/tfx/tree/master/tfx/examples/bert/cola. The fastest way to reproduce this is to:\r\n1. Run the pipeline locally and collect the data artifacts from the output of the Transform component. \r\n2. Pack the data artifacts with the training script into a container image.\r\n3. Start a GKE cluster with 2 nodes, and set up TF Config for the nodes to communicate with each other.\r\n4. Run the run_fn in both containers where fn_args contains the path to the transformed data.\r\n\r\nLet me know if there is anything I can provide. Thank you!", "Do you face the same problem when running the original bert model with MWMS?", "Not sure since I have only tried running the BERT model provided in the example.", "To clarify, you are using the same `bert_cola_pipeline.py` and `bert_cola_pipeline_e2e_test.py` scripts from the `tfx/tfx/examples/bert/cola/` repo, and just swapping out the `bert_cola_utils.py` file? If so, I think trying to run the original COLA model with MWMS would be worthwhile. If that trains okay then it will help narrow down where the issue is, and if it fails then we can reach out to the maintainers of that code.", "The original `bert_cola_pipeline.py` file runs on a single node so I only followed this pipeline up to the trainer part. By original COLA model do you mean the one provided in `bert_cola_utils.py`? If so the modified `bert_cola_utils.py` should be still running the same model.", "> I think the error should be relatively easy to reproduce, but it requires setup for running TFX pipelines. The original example is here: https://github.com/tensorflow/tfx/tree/master/tfx/examples/bert/cola. The fastest way to reproduce this is to:\r\n> \r\n> 1. Run the pipeline locally and collect the data artifacts from the output of the Transform component.\r\n> 2. Pack the data artifacts with the training script into a container image.\r\n> 3. Start a GKE cluster with 2 nodes, and set up TF Config for the nodes to communicate with each other.\r\n> 4. Run the run_fn in both containers where fn_args contains the path to the transformed data.\r\n> \r\n\r\nI'm still a little confused by your above instructions to reproduce the issue. You linked to the original example but it seems that is not needed to reproduce, correct? All I need is [this bert_cola.utils.py](https://gist.github.com/Eric-Le-Ge/21fe4df4c8e361c60d20a683b08aa743) script from your repo?\r\n\r\n\r\n\r\n", "Yes, the original example is not needed to reproduce this issue, but the training steps requires transformed data and running the original example up to the Transform component would produce this data. ", "I suspect this is caused by instance preemption. MultiworkerMirroredStrategy requires all workers to be up to do gradient aggregation. Can you try using [BackupAndRestore](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/experimental/BackupAndRestore) callback, and wraps `model.fit` with a loop and try/catch [UnavailableError](https://www.tensorflow.org/api_docs/python/tf/errors/UnavailableError)?\r\n\r\nAlternatively, just restart all the instances should also work.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42498\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42498\">No</a>\n"]}, {"number": 42497, "title": "Fix table for README.md", "body": "", "comments": []}, {"number": 42496, "title": "[T.F 2.0 API Docs] Adding to the documentation of tf.math.acos and tf.math.add functions", "body": "PR for issue #25802. \r\n\r\nThis PR is similar to #26017 and #25802. However, neither of them were merged.\r\n\r\nI have attempted to add improvements to the documentation for `tf.math.acos` as well as `tf.math.add`. \r\nThis is my first contribution so please do help me identify and fix any errors I might have made.\r\n\r\nThank you", "comments": ["See 8582a58 for how to expose to Python and add documentation there, as an example", "> The `base_api` documentation is available cross language bindings. Python examples should be inserted in Python documentation only, using testable docstrings to ensure that the examples stay in line with code changes.\r\n> \r\n> https://www.tensorflow.org/community/contribute/docs_ref\r\n\r\nWhoops, I saw your comment on #25802 however I didn't quite understand the significance of the issue. Thank you for clarifying once again.\r\nI will be removing the python examples from the `base_api` documentation and resubmit for review. I will then open another PR to add documentation to `python/ops/math_ops.py`.", "Pinging @mihaimaruseac for review. \r\nI'm assuming that along with adding examples to `python/ops/math_ops.py`, I will similarly have to do the same to `core/ops/math_ops.cc`. ", "Created a PR #42619 to address changes to the python documentation."]}, {"number": 42495, "title": "error while importing tensorflow", "body": "ImportError                               Traceback (most recent call last)\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     63   try:\r\n---> 64     from tensorflow.python._pywrap_tensorflow_internal import *\r\n     65   # This try catch logic is because there is no bazel equivalent for py_extension.\r\n\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-64156d691fe5> in <module>\r\n----> 1 import tensorflow as tf\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     39 import sys as _sys\r\n     40 \r\n---> 41 from tensorflow.python.tools import module_util as _module_util\r\n     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader\r\n     43 \r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     38 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\r\n     39 \r\n---> 40 from tensorflow.python.eager import context\r\n     41 \r\n     42 # pylint: enable=wildcard-import\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py in <module>\r\n     33 from tensorflow.core.protobuf import config_pb2\r\n     34 from tensorflow.core.protobuf import rewriter_config_pb2\r\n---> 35 from tensorflow.python import pywrap_tfe\r\n     36 from tensorflow.python import tf2\r\n     37 from tensorflow.python.client import pywrap_tf_session\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py in <module>\r\n     26 \r\n     27 # pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import\r\n---> 28 from tensorflow.python import pywrap_tensorflow\r\n     29 from tensorflow.python._pywrap_tfe import *\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     81 for some common reasons and solutions.  Include the entire stack trace\r\n     82 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 83   raise ImportError(msg)\r\n     84 \r\n     85 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\sooraj rawat\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.", "comments": ["@s00r16 \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Please, refer similar issues #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!", "@ravikyram \r\non anaconda is it necessary to make a different environment for tensorflow?\r\nmake/model of my cpu is Intel(R) Core(TM) i5-8265U CPU @ 1.60GHz  1.80GHz", "@s00r16 \r\n\r\nThe default environment has tensorflow preinstalled.No need to make different environment for Tensorflow.\r\n\r\nDid you install the [latest microsoft visual c++ redistributable](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).", "@ravikyram \r\nwhat about the bazel equivalent error .", "@s00r16 \r\n\r\nDid you install bazel by following the instructions mentioned in [website](https://www.tensorflow.org/install/source_windows#install_bazel).Also, refer #39903.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42495\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42495\">No</a>\n", "![Capture](https://user-images.githubusercontent.com/81622361/112976282-2057ff00-916e-11eb-9655-16e7b8e71951.PNG)\r\nchange the settings"]}, {"number": 42494, "title": "[DOC] Minor fix on AutoGraph limitation doc", "body": "There needs a minor correction for the AutoGraph limitation example.\r\n\r\n```\r\ndef MyClass(object):\r\n  def change(self):\r\n    self.y += 1\r\n\r\nc = MyClass()\r\nwhile x > 0:\r\n  c.change()  # Problem -- modification to c.y is not visible here!\r\n```\r\n\r\nand\r\n\r\n```\r\ndef MyClass(object):\r\n  def change(self):\r\n    self.y += 1\r\n    return self\r\n\r\nc = MyClass()\r\nwhile x > 0:\r\n  c = c.change()  # Okay -- c is now a loop var.\r\n```\r\n\r\nIn these examples, `def MyClass` should be `class MyClass`", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42494) for more info**.\n\n<!-- need_sender_cla -->", "@kooyunmo  Can you please sign CLA. Thanks!", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42494) for more info**.\n\n<!-- ok -->"]}, {"number": 42493, "title": "TypeError: unsupported operand type(s) for *: 'NoneType' and 'NoneType'", "body": "batch_size = 32\r\npatch_size = 48\r\nsess = tf.Session(config=config)\r\ninput_low = tf.placeholder(tf.float32, [None, None, None, 3], name='input_low')\r\ninput_high = tf.placeholder(tf.float32, [None, None, None, 3], name='input_high')\r\nout_image = network(input_low)\r\ndef network(input):\r\n......\r\nconv_transition_01 = slim.conv2d(concat01,32,[3,3], rate=1, activation_fn=lrelu)#b,h,w,32\r\nconv_transition_01 = NonLocalBlock(conv_transition_01, subsample=True)\r\n.........\r\ndef NonLocalBlock(input, subsample=True):\r\n\"\"\"\r\n@Non-local Neural Networks\r\nNon-local Block\r\n\"\"\"\r\n_, height, width, channel = input.get_shape().as_list()     # (B, H, W, C)\r\n\r\ntheta = tf.layers.conv2d(input, channel // 2, 1)    # (B, H, W, C // 2)\r\ntheta = tf.reshape(theta, [-1, height*width, channel // 2])  # (B, H*W, C // 2)\r\n..................\r\nBut there is such a problem\r\nFile \"train.py\", line 25, in \r\nout_image = network(input_low)\r\nFile \"D:\\DeepLearningCode\\hjc\\memnet.py\", line 45, in network\r\nconv_transition_01 = NonLocalBlock(conv_transition_01, subsample=True)\r\nFile \"D:\\DeepLearningCode\\hjc\\NonLocalblock.py\", line 115, in NonLocalBlock\r\ntheta = tf.reshape(theta, [-1, heightwidth, channel // 2]) # (B, HW, C // 2)\r\nTypeError: unsupported operand type(s) for *: 'NoneType' and 'NoneType'\r\nI don't know where is the problem.", "comments": ["@hjc1009jin,\r\nIn order to expedite the trouble-shooting process, could you please provide the TensorFlow version, the complete code to reproduce the issue and the dataset you are using. Thanks!"]}, {"number": 42492, "title": "Missing headers when using the C++ library", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint 19\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r2.3\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): clang 6.0.0\r\n\r\n**Describe the problem**\r\n\r\nI'm having a hard time using the C++ library of TensorFlow. First, is there an official documentation about how to compile it? I couldn't find anything on the official TensorFlow website. I used to use the `contrib/cmake` folder but it was removed. I only manage to compile the C++ library by gathering information here and there and following issues, but it's really hard to tell what is the correct procedure.\r\n\r\nAnyway, I did manage to compile it using bazel (see below), but then when I try to compile my C++ program, I run into a missing file problem:\r\n\r\n```sh\r\nIn file included from /home/simon/programs/third_party/tensorflow/tensorflow/cc/client/client_session.h:24:\r\nIn file included from /home/simon/programs/third_party/tensorflow/tensorflow/cc/framework/ops.h:21:\r\nIn file included from /home/simon/programs/third_party/tensorflow/tensorflow/core/framework/tensor.h:23:\r\n/home/simon/programs/third_party/tensorflow/tensorflow/core/framework/allocator.h:24:10: fatal error: 'absl/strings/string_view.h' file not found\r\n#include \"absl/strings/string_view.h\"\r\n```\r\n\r\nI found some issues on the tracker, but all of them seem to concern TensorFlow 1.x. It's not clear to me why this file is not included (and if it should be?). Anyway, I downloaded _absl_ by hand and included the files on my project. Now I run on this other missing file error:\r\n\r\n```sh\r\nIn file included from /home/simon/programs/third_party/tensorflow/tensorflow/cc/client/client_session.h:24:\r\nIn file included from /home/simon/programs/third_party/tensorflow/tensorflow/cc/framework/ops.h:21:\r\nIn file included from /home/simon/programs/third_party/tensorflow/tensorflow/core/framework/tensor.h:24:\r\n/home/simon/programs/third_party/tensorflow/tensorflow/core/framework/tensor_shape.h:22:10: fatal error: 'tensorflow/core/framework/types.pb.h' file not found\r\n#include \"tensorflow/core/framework/types.pb.h\"\r\n```\r\n\r\nFrom what I found in the issues, this is a file that is supposed to be generated by bazel.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI used `./configure` with all default options, and I don't have Cuda/GPU support.\r\n\r\nThen I tried:\r\n\r\n```sh\r\nbazel build -j 4 -c opt --verbose_failures //tensorflow:libtensorflow_cc.so\r\n```\r\n\r\nand also:\r\n\r\n```sh\r\nbazel build -j 4 -c opt --verbose_failures  //tensorflow:tensorflow_cc\r\n```\r\n\r\nbut both give me the same error when using the library after compilation.\r\n\r\n", "comments": ["I'm sorry to see that this issue has not been addressed in 4 months. Just to be clear, this is not a criticism, I do understand that the C++ library is not a priority and that TF developers are already quite busy with the Python version. In the mean time, should I consider the TensorFlow C++ library as officially broken and move on?", "Dear sgiraudot,\r\nmaybe this is of help for you.\r\nI also needed a C++ library for tensorflow to apply already trained models. The compilation of the required libraries was, however, frustrating. As you mentioned, the built time on a normal machine is very long, which hampers a systematic approach to test different options. Luckily, I have a HPC available, where I can run the compilation within seconds, allowing a more systematic approach.\r\n\r\nI tried to compile the tensorflow libraries (libtensorflow.so, libtensorflow_cc.so and libtensorflow_framework.so)  starting from r2.4.0 to r1.15, followed by linking the libraries to my C++ project. The compilation of the libraries was usually not an issue at all, and all versions compiled successfully, if the protocol from the official side is followed accurately. For the interested reader, this means: Install Go, Bazelisk, the suggested python version and the gcc compiler. However, for all libraries >r2.0.0, I ran into linking issues in the final stage. Specifically, the parameter list of the function Tensor::TensorShape was different with regard to the header file. I noticed also discrepancies between other functions, which I could avoid using in my C++ project. For Tensor::TensorShape, I unsuccessfully tried various alternatives.\r\n\r\nThe final built, which was successful, used the options as follows:\r\n**System Information**\r\n- Centos 7\r\n- Tensorflow version: r1.15\r\n- Python version: 3.6\r\n- GCC Compiler: 6.3.0\r\n- Bazel version: 0.26.1 (Bazelisk/go did not work for this library for me, so I compiled this to from source) \r\n- Latest absl version at the time writing this post\r\n- cmake >3.1, fftw > 3.3.3, gsl > 2. (for compilation of the C++ project)\r\n- command for compilation: bazel build --jobs 192 --config=opt [...]\r\n\r\nReading your post, it looks like that the references to the header files might be an issue for you.\r\nThis also took me some time, however, including the paths in the cmake as follows worked for me:\r\n\r\n\r\ninclude_directories(\r\n/cluster/work/igc/boeseltl/tensorflow_2.4.0/tensorflow/bazel-bin/  \r\n/cluster/work/igc/boeseltl/tensorflow_2.4.0/tensorflow/bazel-bin/tensorflow/\r\n/cluster/work/igc/boeseltl/tensorflow_2.4.0/\r\n/cluster/work/igc/boeseltl/tensorflow_2.4.0/tensorflow/bazel-tensorflow \r\n/cluster/work/igc/boeseltl/tensorflow_2.4.0/tensorflow/bazel-tensorflow/external/eigen_archive/\r\n/cluster/work/igc/boeseltl/tensorflow_2.4.0/abseil-cpp/\r\n/cluster/work/igc/boeseltl/tensorflow_2.4.0/protobuf/src)\r\n\r\nDo not get confused by the directory name, I just copy pasted it from the cmake file. The version installed is r1.15.\r\nLinking in the final stage is obviously done as\r\n\r\nlink_directories(/cluster/work/igc/boeseltl/tensorflow_2.4.0/tensorflow/bazel-bin/tensorflow)\r\nand\r\ntarget_link_libraries(${LOOP_VAR} ${EXTERNAL_LIBRARIES} tensorflow_cc tensorflow_framework)\r\n\r\n\r\nBest, and good luck with your issue\r\nLennard\r\n\r\n", "@sgiraudot,\r\n\r\nCan you take a look at this above comment from @Lennard94 and let us know if helps in resolving your issue? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42492\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42492\">No</a>\n"]}, {"number": 42490, "title": "Minimum requirement for Tensorflow Lite", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):  source\r\n- TensorFlow version: N/A\r\n- Python version: N/A\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI tried to make an inference program with C/C++ (TF Lite Microcontroller) in ARMv7(32-bit) environment at the very beginning, but the model that I converted includes complex bidirectional LSTM and DNN, and it was hard to use with the C/C++ environment. \r\nI got errors when my codes load the converted model in C/C++ even though there was no error while conversion.\r\n\r\nBefore converting the model, I checked that  simple exmaple binary such as hello_world (TF Lite example) is running well on c/c++ with armv7 environment, and the converted model is also running well on python with x86 environment.\r\n\r\nI changed my plan into using Python, but my embedded board has no tools like pip, wget, python, and other basic binaries to use. (Thus, now I'm trying to build python for my board.) Additionally, there is only 300 MB that I can use in the board.\r\nI want to know the minimum requirement of Tensorflow Lite for Python and Microcontroller (C/C++).\r\n\r\nThe board has nxp4330(ARM32) chipset and it has only minimum binaries to run, so I have no idea what I have to do first for using TF Lite with this board. \r\nThank you in advance.\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["You can start with TFLite C library.\r\nhttps://www.tensorflow.org/lite/guide/build_rpi#c_library\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/c\r\n\r\nIf you need more tiny inference platform, you can consider TFLite micro.\r\nhttps://www.tensorflow.org/lite/microcontrollers", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42490\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42490\">No</a>\n"]}, {"number": 42489, "title": "Enable Conv + Biasadd + LeakyRelu Fusion with Eigen implementation in CPU", "body": "This PR enables Conv + Biasadd + LeakyRelu fusion in CPU. \r\nThe fusion pattern is very common in YOLOv3 model.", "comments": ["It was automatically rolled back because of internal test failure. Will take a look tomorrow.\r\n\r\n```\r\n\"strided_convolution_stack_layer_test.py\", line 119, in test_strided_conv2d_stack_leaky_relu\r\n    self.assertTrue((output >= 0).all())\r\nAssertionError: False is not true\r\n```", "LeakyRelu alpha is `0.0` in test, but `0.2` in `_FusedConv2D` node.", "`CopyConv2DAttributes` is missing an `activation` pointer, and it does not copy the `alpha` value. Can you please also add a test to  catch this bug.", "I resubmit the PR https://github.com/tensorflow/tensorflow/pull/42551 to fix the errors. Thanks."]}, {"number": 42488, "title": "Questions about ctc_decode in tensorflow&keras", "body": "https://github.com/tensorflow/tensorflow/blob/b36436b087bd8e8701ef51718179037cccdfc26e/tensorflow/python/ops/ctc_ops.py#L288-L335\r\n\r\nAbout this function: tf.nn.ctc_greedy_decoder\r\n[https://www.tensorflow.org/api_docs/python/tf/nn/ctc_greedy_decoder]\r\n\r\nI want to know how this function specifies the Class Index to be used for the BLANK LABEL\r\nBeacuse:  \r\n  # gen_ctc_ops.ctc_loss_v2 differs from gen_ctc_ops.ctc_loss. v2 assumes the blank index to be 0, but v1 views it as the last index.\r\nIf it doesn't know which is the BLANK LABEL, how to combine and del BLANK LABEL for the final result?\r\nIf it takes the last index as a blank label by default, there is a contradiction with the new version of the API\r\n\r\n", "comments": ["https://github.com/tensorflow/tensorflow/blob/a195c757e3e968f6dcbb5b1ad76c95c61dc7fb67/tensorflow/core/kernels/ctc_decoder_ops.cc#L223-L224\r\n\r\nI have found its implementation in the c language source code, and found that it does takes num_classes - 1 as the blank label by default, so this is a bug.\r\n", "@Haskely \r\nPlease fill in the issue template for us to help you.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42486, "title": "AttributeError: module 'tensorflow' has no attribute 'python'", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version: 2.3.0\r\n- Python version: 3.6.10\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: Nvidia 920MX\r\n\r\n\r\n\r\n**AttributeError: module 'tensorflow' has no attribute 'python'**\r\n\r\n**I am working on a siamese model with tensorflow keras backend. When i tried to import keras backend, t shows above error.**\r\n`import tensorflow.python.keras.backend as K`\r\n`AttributeError: module 'tensorflow' has no attribute 'python'`\r\n\r\n**Any other info / logs**\r\nFollowings are some of other packages installed in the conda environment.\r\n\r\n- keras                    2.3.1\r\n- pip                       20.2.2\r\n- python                 3.6.10\r\n- tensorflow            2.3.0\r\n\r\n", "comments": ["`import tensorflow.keras.backend as K` worked for me.\r\n\r\nhttps://stackoverflow.com/questions/63481401/attributeerror-module-tensorflow-has-no-attribute-python-in-keras-tensorflo?noredirect=1#comment112254441_63481401", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42486\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42486\">No</a>\n"]}, {"number": 42485, "title": "how to get a tensorflow tf by index?", "body": "```\r\n    object_for_each_prior = tf.constant([1 for i in range(8732)])\r\n    -><tf.Tensor: shape=(8732,), dtype=int32, numpy=array([1, 1, 1, ..., 1, 1, 1], dtype=int32)>\r\n```\r\n\r\nThen if I want to get the position 1148,1149<br/>\r\n\r\n```\r\n    prior_for_each_object = tf.constant([1148,1149])\r\n    object_for_each_prior[prior_for_each_object]\r\n```\r\n\r\nThen I got the following error <br/>\r\n\r\n`    TypeError: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got <tf.Tensor: shape=(2,), dtype=int32, numpy=array([1148, 1149], dtype=int32)>\r\n`\r\nIf I want to get the tensor's number by index how should I approach it?", "comments": ["and if I want to set the different number \r\n\r\n`            object_for_each_prior[prior_for_each_object] = tf.constant(list(range(n_objects)),dtype=tf.int64)\r\n`\r\nHow should I do it? ", "@SlowMonk \r\nPlease fill the issue template for us to assist you with the issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42485\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42485\">No</a>\n"]}, {"number": 42484, "title": "non-zero `bias` of `Conv2D` in tflite file", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 1.15.0\r\n\r\nAll information is in [Colab](https://colab.research.google.com/drive/1yWwmAXHyBjNIivJKwmMMJT5V-37TEgph?usp=sharing)\r\n\r\nQuestion:\r\n\r\nWhy the `Conv2D` op has non-zero `bias` input while they were setup `use_bias=False`.\r\n", "comments": ["@KangGrandesty \r\n\r\nPlease, grant me access for the colab link.Thanks!", "@ravikyram ,you can try again. [Colab](https://colab.research.google.com/drive/1yWwmAXHyBjNIivJKwmMMJT5V-37TEgph?usp=sharing)", "My ssd lite model has bias like above, too.\r\n\r\nAnd then the ssd model throws:\r\n\r\n```\r\nInternal error: Failed to apply delegate: tensorflow/lite/kernels/kernel_util.cc:129 std::abs(input_product_scale - bias_scale) <= 1e-6 * std::min(input_product_scale, bias_scale) was not true.\r\n    Node number 35 (CONV_2D) failed to prepare.\r\n```\r\n\r\nNode number 35 is first `conv` of the 3rd squeeze-and-excite module in mobilenet v3 small. All of bias'value in this `conv` are -2147483648\r\n", "@KangGrandesty Can you try your example in TF2 and expand the `bias` values in the Netron app and inspect?\r\nYou should be seeing a bunch of zero values.\r\nI made a toy example to work with TF 2.X way since all new updates are available for TF 2.\r\nPlease see the [gist](https://colab.research.google.com/gist/ymodak/178876aac1e5d019244b436479cf3b99/post_training_integer_quant.ipynb)\r\n\r\nIn the netron viewer on expanding the biases you will see all `zeros` when `use_bias=False`.\r\nPlease see attached screenshot.\r\n<img width=\"687\" alt=\"Screen Shot 2020-08-19 at 11 44 06 AM\" src=\"https://user-images.githubusercontent.com/42785357/90676792-5a911380-e211-11ea-9fa3-d0f9c861e4cd.png\">\r\n\r\n\r\n\r\nFor the same example when `use_bias=True`\r\n<img width=\"665\" alt=\"Screen Shot 2020-08-19 at 11 46 27 AM\" src=\"https://user-images.githubusercontent.com/42785357/90677014-a17f0900-e211-11ea-9f1f-2716200083f8.png\">\r\n", "I run my example in tf2, but the bias is still non-zero. \r\n\r\n[mnist tf2 Colab](https://colab.research.google.com/drive/19avZS7BdPdwAEAEqyVmYoxB_uICijULw?usp=sharing)\r\n\r\n![20200820101651](https://user-images.githubusercontent.com/19343603/90709235-4fe88400-e2ce-11ea-850f-a57780f91ed6.png)\r\n\r\nIf I remove  the `bn` layer:\r\n\r\n[mnist no bn tf2 Colab](https://colab.research.google.com/drive/1aoOacX7BeSS3SomFpIii9VqCwpgp8JdP?usp=sharing)\r\n\r\n![20200820102504](https://user-images.githubusercontent.com/19343603/90709735-722ed180-e2cf-11ea-84d5-925ebf6f453c.png)\r\n\r\nThe key is `bn` layer. Maybe?"]}, {"number": 42483, "title": "ValueError: Tried to convert 'shape' to a tensor and failed. Error: None values not supported.", "body": "in tensorflow-gpu1.13.0\uff0c\r\ndef _phase_shift(I, r):\r\n\r\n    bsize, w, h, c = I.get_shape().as_list()\r\n\r\n    bsize = tf.shape(I)[0]\r\n\r\n    X = tf.reshape(I, (bsize, w, h, r, r))\r\n\r\nBut there is such a problem\r\n  File \"D:\\DeepLearningCode\\hjc\\model.py\", line 236, in _phase_shift\r\n    X = tf.reshape(I, (bsize, w, h, r, r))\r\n  File \"D:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 8462, in reshape\r\n    \"Reshape\", tensor=tensor, shape=shape, name=name)\r\n  File \"D:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 529, in _apply_op_helper\r\n    (input_name, err))\r\nValueError: Tried to convert 'shape' to a tensor and failed. Error: None values not supported.\r\n\r\nI don't know where is the problem", "comments": ["@hjc1009jin,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and the dataset you are using.\r\n\r\nAlso, TensorFlow v1.13 is not actively supported. Please update TensorFlow to v1.15 or v2.3 and check if you are facing the same issue. Thanks!", "\r\n\r\n\r\n\r\n> @hjc1009jin,\r\n> In order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and the dataset you are using.\r\n> Also, TensorFlow v1.13 is not actively supported. Please update TensorFlow to v1.15 or v2.3 and check if you are facing the same issue. Thanks!\r\n\r\nbatch_size = 32\r\npatch_size = 48\r\nsess = tf.Session(config=config)\r\ninput_low = tf.placeholder(tf.float32, [None, None, None, 3], name='input_low')\r\ninput_high = tf.placeholder(tf.float32, [None, None, None, 3], name='input_high')\r\nout_image = network(input_low)\r\n\r\ndef network(input):\r\n    ......\r\n    conv_transition_01 = slim.conv2d(concat01,32,[3,3], rate=1, activation_fn=lrelu)#b,h,w,32\r\n    conv_transition_01 = NonLocalBlock(conv_transition_01, subsample=True)\r\n   .........\r\n\r\ndef NonLocalBlock(input, subsample=True):\r\n    \"\"\"\r\n    @Non-local Neural Networks\r\n    Non-local Block\r\n    \"\"\"\r\n    \r\n    _, height, width, channel = input.get_shape().as_list()     # (B, H, W, C)\r\n\r\n    theta = tf.layers.conv2d(input, channel // 2, 1)    # (B, H, W, C // 2)\r\n    theta = tf.reshape(theta, [-1, height*width, channel // 2])  # (B, H*W, C // 2)\r\n..................\r\n\r\n\r\nBut there is such a problem\r\n  File \"train.py\", line 25, in <module>\r\n    out_image = network(input_low)\r\n  File \"D:\\DeepLearningCode\\hjc\\memnet.py\", line 45, in network\r\n    conv_transition_01 = NonLocalBlock(conv_transition_01, subsample=True)\r\n  File \"D:\\DeepLearningCode\\hjc\\NonLocalblock.py\", line 115, in NonLocalBlock\r\n    theta = tf.reshape(theta, [-1, height*width, channel // 2])  # (B, H*W, C // 2)\r\nTypeError: unsupported operand type(s) for *: 'NoneType' and 'NoneType'\r\n\r\nI don't know where is the problem.Can you help me?THANKS.\r\n"]}, {"number": 42482, "title": "Missing masOS Big Sur build, Could not find a version that satisfies the requirement tensorflow", "body": "**System information**\r\n- OS Platform and Distribution: masOS Big Sur\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.3.0\r\n- Python version: 3.7/3.8\r\n- Installed using virtualenv? pip? conda?: virtualenv + pip\r\n\r\n**Describe the problem**\r\nCould not find a version that satisfies the requirement tensorflow (from versions: none)\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```bash\r\n$ pip install tensorflow\r\nERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\r\nERROR: No matching distribution found for tensorflow\r\n```\r\n\r\n```bash\r\n$ pip install tensorflow==2.3.0\r\nERROR: Could not find a version that satisfies the requirement tensorflow==2.3.0 (from versions: none)\r\nERROR: No matching distribution found for tensorflow==2.3.0\r\n```\r\n\r\n```bash\r\n$ pip install /Users/brikerman/Downloads/tensorflow-2.3.0-cp38-cp38-macosx_10_11_x86_64.whl\r\nERROR: tensorflow-2.3.0-cp38-cp38-macosx_10_11_x86_64.whl is not a supported wheel on this platform.\r\n```", "comments": ["@BrikerMan \r\nPlease check if you have the latest version of pip installed. As per the [documentation](https://www.tensorflow.org/install) TensorFlow 2 packages require a pip version >19.0.\r\nYou can upgrade pip using the below command:\r\n\r\npython3 -m pip install --upgrade pip first\r\n\r\nAlso, please check the Python version installed on your machine. As per the [PyPI website](https://pypi.org/project/tensorflow/), \r\nand try python3 -m pip install tensorflow==2.3.0\r\n\r\n#37457 #41262 [Link](https://github.com/tensorflow/tensorflow/issues/38882#issuecomment-624759253) please check compatibility issue.", "Python: 3.7.8\r\nPip: 20.2.2\r\n\r\n```bash\r\n\u279c which python\r\n/Users/brikerman/Desktop/ai/rl/unity_rl/venv/bin/python\r\n\r\n\u279c which python3\r\n/Users/brikerman/Desktop/ai/rl/unity_rl/venv/bin/python3\r\n\r\n\u279c python --version\r\nPython 3.7.8\r\n\r\n\u279c pip --version\r\npip 20.2.2 from /Users/brikerman/Desktop/ai/rl/unity_rl/venv/lib/python3.7/site-packages/pip (python 3.7)\r\n\r\n\u279c pip install tensorflow==2.3.0\r\nERROR: Could not find a version that satisfies the requirement tensorflow==2.3.0 (from versions: none)\r\nERROR: No matching distribution found for tensorflow==2.3.0\r\n\r\n\u279c python3 -m pip install tensorflow==2.3.0\r\nERROR: Could not find a version that satisfies the requirement tensorflow==2.3.0 (from versions: none)\r\nERROR: No matching distribution found for tensorflow==2.3.0\r\n\r\n\u279c python3 -c \"import platform; print(platform.architecture())\"\r\n('64bit', '')\r\n```", "@BrikerMan \r\n\r\nCan you please verify is you are using 32 bit python version.\r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the latest [microsoft visual c++ redistributable from here.](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, refer #36167 and see if it helps you.", "It is a 64bit python. checkout this. And I am using MacBook Pro (13-inch, 2017, Two Thunderbolt 3 ports). I am able to run tf 2.2.0 before I installed the `Big Sur Beta`\r\n\r\n```\r\n\u279c python3 -c \"import platform; print(platform.architecture())\"\r\n('64bit', '')\r\n```", "I think I found the issue. It is caused by `pyenv`\r\nI am able to install tensorflow==2.2.0, 2.3.0 on Python3.8 (pre-installed on mac Big Sur)\r\nWhen I using python 3.7.8 installed by pyenv, then I have this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42482\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42482\">No</a>\n", "@BrikerMan I am having the same issue with Pyenv. Did you end up finding a solution, or have you just stopped using Pyenv on Big Sur?", "@BrikerMan @prouast Same issue here using Pyenv on MacOS 11.0.1, with Python 3.7.9 & 3.8.5. \r\n\r\nI also tried running Python installed using brew install python3@7, it still does not work. ", "@prouast @chris-moov I just stopped using Pyenv on Big Sur. I used miniconda and it works just fine.", "@BrikerMan Thanks for the pointer. I installed miniconda via Pyenv and that seems to solve the problem. ", "I was facing the same problem after I upgraded to macOS Big Sur (11.0.1). I was unable make it work with pyenv as well. Upgrading my local Python build to 3.8.6 solved my problem. ", "I tried installing miniconda, that failed too. I reckon there is a bug in the Python builds in pyenv and/or brew. \r\n\r\nI have downloaded Python from python.org and it worked! ", "I don't think this is a bug since Big Sur is a new major release of macOS. The current platform matching behavior for macOS wheels matches the exact major version and will not match wheels built for the platform `macosx_10_9_x86_64`, which is the platform that tensorflow wheels currently target. I do not understand how changing Python versions would help this problem, since this is the behavior dictated by `pip`.\r\n\r\nTo guarantee proper wheel support, tensorfow needs to just reupload existing wheels with `macosx_11_0_x86_64` tag and update PyPI publishing pipeline to upload wheels for both major versions.\r\n\r\nTo mitigate the problem right now, you can download the wheel for macOS 10.x and rename the tag. E.g. for Python 3.7:\r\n```shell\r\n$ wget https://files.pythonhosted.org/packages/0d/ea/f936c14b6e886221e53354e1992d0c4e0eb9566fcc70201047bb664ce777/tensorflow-2.3.1-cp37-cp37m-macosx_10_9_x86_64.whl\r\n$ mv tensorflow-2.3.1-cp37-cp37m-macosx_10_9_x86_64.whl tensorflow-2.3.1-cp37-cp37m-macosx_11_0_x86_64.whl\r\n$ pip install ./tensorflow-2.3.1-cp37-cp37m-macosx_11_0_x86_64.whl\r\n```", "@dgladkov I get: \r\n```\r\n$ pip install ./tensorflow-2.3.1-cp37-cp37m-macosx_11_0_x86_64.whl\r\n\r\nERROR: tensorflow-2.3.1-cp37-cp37m-macosx_11_0_x86_64.whl is not a supported wheel on this platform.```", "Try this: https://github.com/apple/tensorflow_macos", "Have the same problem\r\nnot only with tensorflow but also with scikit-learn\r\nfound some hacks that helps me for now (until we have to wait for pip to fix this issue)\r\nhttps://github.com/scikit-learn/scikit-learn/issues/18861\r\n\r\n1. install python from conda-forge (via miniconda):\r\n- brew cask install miniconda\r\n- conda config --add channels conda-forge\r\n- conda install libpython-static python\r\n2. create venv with that interpreter (mkvirtualenv dl4cv -p /usr/local/Caskroom/miniconda/base/bin/python)\r\n3. install tensorflow with pip\r\n4. profit", "you can also patch `site-packages/pip/_vendor/packaging/tags.py` and patch the `version_str`:\r\n```\r\n    version_str, _, cpu_arch = platform.mac_ver()  # type: ignore\r\n    version_str = \"10.15\"  # add this\r\n```\r\n", "I am having problem installing TensorFlow 2.1.0 with anaconda3 and python3.8. Trying to create an environment and `pip3 install tensorflow==2.1.0` but still get an error missing package:\r\n\r\n`ERROR: Could not find a version that satisfies the requirement tensorflow==2.1.0`\r\n`ERROR: No matching distribution found for tensorflow==2.1.0`\r\n\r\nbut when I run `pip3 install tensorflow` it runs just fine:\r\n`Collecting tensorflow`\r\n`    Downloading tensorflow-2.4.1-cp38-cp38-macosx_10_11_x86_64.whl (173.9 MB)`\r\n", "I am trying to install tensorflow on metal following these instructions:\r\nhttps://developer.apple.com/metal/tensorflow-plugin/\r\n\r\nERROR: Could not find a version that satisfies the requirement tensorflow-macos (from versions: none)\r\nERROR: No matching distribution found for tensorflow-macos\r\n\r\nI am on Big Sur OS X 11.4 the tensorflow_metal virtual environment.\r\n```\r\n% python3 -c \"import platform; print(platform.architecture())\"\r\n('64bit', '')\r\n(base) (tensorflow-metal) davidlaxer@x86_64-apple-darwin13 notebooks % python --version\r\nPython 3.8.5\r\n(base) (tensorflow-metal) davidlaxer@x86_64-apple-darwin13 notebooks % \r\n\r\n\r\n python -m pip install tensorflow-macos\r\nERROR: Could not find a version that satisfies the requirement tensorflow-macos (from versions: none)\r\nERROR: No matching distribution found for tensorflow-macos\r\n(base) (tensorflow-metal) davidlaxer@x86_64-apple-darwin13 notebooks % python -m pip install tensorflow-metal\r\n```\r\nI added:\r\nvi  /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages/pip/_vendor/packaging/tags.py \r\n version_str = \"10.15\" # added this dbl\r\n\r\n", "Having the same issue, did anyone figure this out?"]}, {"number": 42481, "title": "TensorFlow Lite converter emits incorrect mask for StridedSlice when using ellipsis", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux, Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): tf-nightly-cpu-2.4.0.dev20200818\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\ndef main():\r\n    graph = tf.Graph()\r\n\r\n    # Create a basic graph with only an overlap_and_add on some random data.\r\n    shape = (1, 1, 1024, 512)\r\n    with graph.as_default():\r\n        _input = tf.random.uniform(shape)\r\n        ola = tf.signal.overlap_and_add(_input, 256, name='output')\r\n\r\n    # Try executing that graph in regular TensorFlow.\r\n    with tf.compat.v1.Session(graph=graph) as session:\r\n        print(f\"With regular TensorFlow, result is: {session.run(ola)}\")\r\n\r\n    # Convert to TFLite (using the V1 interface for simplicity)\r\n    converter = tf.compat.v1.lite.TFLiteConverter(graph.as_graph_def(), [_input], [ola])\r\n    tflite_model = converter.convert()\r\n\r\n    # Write the model to disk...\r\n    model_path = f'./{__file__}.tflite'\r\n    with open(model_path, 'wb') as f:\r\n        f.write(tflite_model)\r\n\r\n    # ...so that we can load it into an interpreter and see the error!\r\n    interpreter = tf.lite.Interpreter(model_path=model_path)\r\n    interpreter.allocate_tensors()\r\n\r\n    # This line should throw (as of tf-nightly-cpu-2.4.0.dev20200818):\r\n    #   RuntimeError: tensorflow/lite/kernels/reshape.cc:66 \\\r\n    #   num_input_elements != num_output_elements (0 != 524800) \\\r\n    #   Node number 5 (RESHAPE) failed to prepare.\r\n    interpreter.invoke()\r\n    print(\"If we got here, the bug did not appear!\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n```\r\n\r\n**Failure details**\r\nThe provided graph works in regular TensorFlow, and the TensorFlow Lite converter executes with no errors, but the TFLite model fails at runtime with:\r\n```\r\nRuntimeError: tensorflow/lite/kernels/reshape.cc:66 num_input_elements != num_output_elements (0 != 524800)Node number 5 (RESHAPE) failed to prepare.\r\n```\r\n\r\nA visualization of the resulting graph in Netron shows that the node in question should have an input shape of `(1, 1, 2050, 256)` with no unknown dimensions:\r\n![image](https://user-images.githubusercontent.com/213293/90586031-3519f080-e1a4-11ea-85d0-0539ace7fa7a.png)\r\n\r\n", "comments": ["I've narrowed this down to [one line in `overlap_and_add` that does a `StridedSlice`](https://github.com/tensorflow/tensorflow/blob/77245d0/tensorflow/python/ops/signal/reconstruction_ops.py#L148):\r\n\r\n```python\r\n    # Truncate so that signal.shape = (15, 2)\r\n    # ab fg kl 00 00 00 cd hi mn 00 00 00 e0 j0 o0\r\n    signal = signal[..., :(frames + segments - 1) * segments, :]\r\n```\r\n\r\nThis seems to result in a `StridedSlice` op with the following properties:\r\n```\r\nbegin = [0, 0]\r\nend = [2050, 0]\r\nstrides = [1, 1]\r\nbegin_mask = 1 (i.e.: dims[0])\r\nend_mask = 1 (i.e.: dims[0])\r\n```\r\n\r\nThis is odd, as the slice includes a `:` as its final element, which should indicate that the entire dimension should be used, but the serialized graph includes both the beginning and end of that dimension as `0`. The `begin_mask` and `end_mask` are both set to `1`, indicating that the stride should use the whole of the _first_ dimension (bit 0), according to [the docs](https://www.tensorflow.org/api_docs/python/tf/strided_slice):\r\n\r\n> If the ith bit of begin_mask is set, begin[i] is ignored and the fullest possible range in that dimension is used instead. end_mask works analogously, except with the end range.\r\n\r\nGiven that the regular TensorFlow runtime seems to handle this slice just fine, this leads me to believe that the issue might be in the TFLite converter. Indeed, before TFLite conversion, the `StridedSlice` takes in an additional dimension has the following properties:\r\n\r\n```\r\nbegin = [0, 0, 0]\r\nend = [0, 2050, 0]\r\nstrides = [1, 1, 1]\r\nellipsis_mask = 1\r\nbegin_mask = 6 (i.e.: dims[1] and dims[2])\r\nend_mask = 4 (i.e.: dims[2] alone)\r\n```\r\n\r\nIt looks like the TensorFlow Lite converter may be incorrectly optimizing the `begin_mask` and `end_mask` here, due to the `ellipsis_mask`. **If I remove the ellipsis entirely, the problem does go away.**\r\n\r\nA quick fix for this would be to change `reconstruction_ops.py:148` to specify the exact end of its slice operation, to work around this bug:\r\n```python\r\nsignal = signal[..., : (frames + segments - 1) * segments, :array_ops.shape(signal)[-1]]\r\n```", "@renjie-liu Can you please have a look", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42481\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42481\">No</a>\n"]}, {"number": 42480, "title": "SetNumThreads makes inference slower!", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution: Android 9\r\n- Mobile device: Vsmart JOY 3\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version:\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nInference time when using SetNumThreads (numThreads = 4):\r\n\r\n![image](https://user-images.githubusercontent.com/16491585/90585698-a5099480-e1ff-11ea-831e-a1294f910b68.png)\r\n\r\nInference time without SetNumThreads:\r\n\r\n![image](https://user-images.githubusercontent.com/16491585/90585816-f74ab580-e1ff-11ea-9490-87cfb3304b44.png)\r\n\r\nCurrently, I'm using 2 tflite models in our project, the first one runs normally without any performance degradation,\r\n\r\n**Describe the expected behavior**\r\n\r\nSetNumThreads shouldn't make the inference slower.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```c++\r\nFaceDetector::FaceDetector(void *modelBuff, int size, int threadNum)\r\n{\r\n    if (size < 1)\r\n        return;\r\n\r\n    this->_modelBytes = (char *)malloc(sizeof(char) * size);\r\n    memcpy(this->_modelBytes, modelBuff, sizeof(char) * size);\r\n\r\n    // Build the model\r\n    this->_model = tflite::FlatBufferModel::BuildFromBuffer(this->_modelBytes, size);\r\n\r\n    // Build the interpreter\r\n    tflite::ops::builtin::BuiltinOpResolver resolver;\r\n    tflite::InterpreterBuilder builder(*this->_model, resolver);\r\n    builder(&this->_interpreter);\r\n\r\n    // Allocate tensor buffers.\r\n    this->_interpreter->AllocateTensors();\r\n\r\n    // Set threads num\r\n    this->_interpreter->SetNumThreads(threadNum);\r\n\r\n    this->_ready = true;\r\n}\r\n\r\nFaceDetector::~FaceDetector()\r\n{\r\n    free(this->_modelBytes);\r\n    this->_modelBytes = nullptr;\r\n}\r\n\r\nbool FaceDetector::isReady()\r\n{\r\n    return this->_ready;\r\n}\r\n\r\nvoid FaceDetector::detect(void *data, int width, int height, int channel)\r\n{\r\n    // Set input/output tensor ptr\r\n    float* _inputTensor = this->_interpreter->typed_input_tensor<float>(0);\r\n    float* _boxesTensor = this->_interpreter->typed_output_tensor<float>(0);\r\n    float* _scoresTensor = this->_interpreter->typed_output_tensor<float>(1);\r\n\r\n    Mat image = Mat(height, width, CV_8UC3, data);\r\n    Mat scaled, fImage;\r\n    resize(image, scaled, Size(128, 128), 0, 0, INTER_AREA); // Scale RGB image to model input size\r\n    float IMAGE_STD = 127.5f, IMAGE_MEAN = 127.5f;\r\n    scaled.convertTo(fImage, CV_32FC3, 1 / IMAGE_STD, -IMAGE_MEAN / IMAGE_STD);\r\n\r\n    memcpy(_inputTensor, fImage.data, 128*128*3*sizeof(float));\r\n\r\n    this->_interpreter->Invoke();\r\n}\r\n```\r\n", "comments": ["If your device have less than 4 cores with same capability or some of the cores are busy, this is likely to happen. ", "@freedomtan \r\n\r\nThanks for your reply! My device has 8 cores.", "It's unlikely you have a phone having 8 cores with same capabilities. Most SoCs available for Android nowadays are 4+4, 1+3+4, and 2+2+4.", "I don't think it's the problem since the other tflite model runs well with 4 threads.\r\n\r\n> Currently, I'm using 2 tflite models in our project, the first one runs normally without any performance degradation\r\n\r\n(When testing this model I disabled the first one)\r\n\r\nIs there any other factor that could affect the performance of tf multithread?", "@anhkhoa45 \r\nCould you run the performance benchmark(https://www.tensorflow.org/lite/performance/benchmarks) on both models with the --enable_op_profiling=true?\r\nIt will give much more details for analysis. Thanks.", "@thaink Sure! Here's the result:\r\n\r\nFirst model: https://pastebin.com/Z0KkJWFK\r\n\r\nSecond model: https://pastebin.com/8E0Ksq4d", "This is my mistake during argument passing. I'll close this issue. Thanks \ud83d\ude47 "]}, {"number": 42479, "title": "Build Failure of version 2.3.0 in macOS, MacPorts #42090", "body": "**Describe the current behavior**\r\n\r\nVersion 2.3.0 fails to build on macOS.\r\n\r\n**Describe the expected behavior**\r\n\r\nFirst issue, `Action failed to execute: java.io.IOException: Cannot run program\u2026 error=24, Too many open files`:\r\n```\r\nERROR: /opt/local/var/macports/build/_opt_local_ports_python_py-tensorflow/py37-tensorflow/work/tensorflow-tensorflow-b36436b/tensorflow/core/common_runtime/BUILD:328:11: C++ compilation of rule '//tensorflow/core/common_runtime:collective_executor_mgr' failed (Exit -1): wrapped_clang failed: error executing command \r\n  (cd /opt/local/var/macports/build/_opt_local_ports_python_py-tensorflow/py37-tensorflow/work/e3571a779784f9da03a7824d69817047/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    APPLE_SDK_PLATFORM=MacOSX \\\r\n    APPLE_SDK_VERSION_OVERRIDE=10.15 \\\r\n    PATH=/opt/local/bin:/opt/local/sbin:/bin:/sbin:/usr/bin:/usr/sbin \\\r\n    XCODE_VERSION_OVERRIDE=11.6.0.11E708 \\\r\n  external/local_config_cc/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG '-std=c++11' -iquote . -iquote bazel-out/host/bin -iquote external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/bin/external/local_config_sycl -iquote external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/gif -iquote bazel-out/host/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/host/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/host/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/host/bin/external/zlib -iquote external/double_conversion -iquote bazel-out/host/bin/external/double_conversion -isystem external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem external/gif -isystem bazel-out/host/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/host/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/host/bin/external/zlib -isystem external/double_conversion -isystem bazel-out/host/bin/external/double_conversion -MD -MF bazel-out/host/bin/tensorflow/core/common_runtime/_objs/collective_executor_mgr/collective_executor_mgr.d -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' '-frandom-seed=bazel-out/host/bin/tensorflow/core/common_runtime/_objs/collective_executor_mgr/collective_executor_mgr.o' -isysroot __BAZEL_XCODE_SDKROOT__ -F__BAZEL_XCODE_SDKROOT__/System/Library/Frameworks -F__BAZEL_XCODE_DEVELOPER_DIR__/Platforms/MacOSX.platform/Developer/Library/Frameworks '-mmacosx-version-min=10.15' -g0 '-march=x86-64' -g0 '-std=c++14' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DTENSORFLOW_USE_XLA=1' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/core/common_runtime/collective_executor_mgr.cc -o bazel-out/host/bin/tensorflow/core/common_runtime/_objs/collective_executor_mgr/collective_executor_mgr.o)\r\nExecution platform: @local_execution_config_platform//:platform. Note: Remote connection/protocol failed with: execution failed\r\nAction failed to execute: java.io.IOException: Cannot run program \"/opt/local/var/macports/build/_opt_local_ports_python_py-tensorflow/py37-tensorflow/work/install/1eb24b6f9fb447fbef56fd6c7521f126/process-wrapper\" (in directory \"/opt/local/var/macports/build/_opt_local_ports_python_py-tensorflow/py37-tensorflow/work/e3571a779784f9da03a7824d69817047/execroot/org_tensorflow\"): error=24, Too many open files\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n\r\nThis is with system settings:\r\n```bash\r\n$ ulimit -n\r\n65536\r\n$ launchctl limit maxfiles\r\n\tmaxfiles    65536          200000   \r\n```\r\n\r\nStarting the build again, another issue is encountered, apparently arising from https://github.com/tensorflow/tensorflow/pull/40654.\r\n\r\nSecond issue, `error: no matching function for call to object of type '(lambda at tensorflow/python/lib/core/bfloat16.cc:637:25)`:\r\n```\r\n:info:build tensorflow/python/lib/core/bfloat16.cc:678:8: error: no matching function for call to object of type '(lambda at tensorflow/python/lib/core/bfloat16.cc:637:25)'\r\n:info:build   if (!register_ufunc(\"less_equal\", CompareUFunc<Bfloat16LeFunctor>,\r\n:info:build        ^~~~~~~~~~~~~~\r\n:info:build tensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate function not viable: no overload of 'CompareUFunc' matching 'PyUFuncGenericFunction' (aka 'void (*)(char **, const long *, const long *, void *)') for 2nd argument\r\n:info:build   auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,\r\n:info:build                         ^\r\n:info:build tensorflow/python/lib/core/bfloat16.cc:682:8: error: no matching function for call to object of type '(lambda at tensorflow/python/lib/core/bfloat16.cc:637:25)'\r\n:info:build   if (!register_ufunc(\"greater_equal\", CompareUFunc<Bfloat16GeFunctor>,\r\n:info:build        ^~~~~~~~~~~~~~\r\n```\r\n\r\nmacOS 10.15.6 19G73\r\nXcode 11.6 11E708 \r\n\r\nRelated:\r\n* https://trac.macports.org/ticket/60960\r\n* https://github.com/macports/macports-ports/pull/7575\r\n* https://github.com/tensorflow/tensorflow/pull/40654\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nMacPorts Portfile:\r\n[Portfile_py-tensorflow-2.3.0.txt](https://github.com/tensorflow/tensorflow/files/5093501/Portfile_py-tensorflow-2.3.0.txt)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nlogs using `port -vst build py37-tensorflow-2.3.0`:\r\n\r\n[py37-tensorflow-2.3.0_stdout0.log](https://github.com/tensorflow/tensorflow/files/5093661/py37-tensorflow-2.3.0_stdout0.log)\r\n[py37-tensorflow-2.3.0_stdout1.log](https://github.com/tensorflow/tensorflow/files/5093678/py37-tensorflow-2.3.0_stdout1.log)\r\n\r\nThe full log files are 40MB and too large to attach to GitHub.", "comments": ["Please take a look at this [article](https://medium.com/@thomaschou_9652/customized-tensorflow-for-macos-1fe31110d92c) and let me know if it helps. Thanks!", "That doesn't help. This is an existing MacPorts build. It already uses Bazel and source code. It was working until version 2.2.0. The most recent version breaks the build. I've linked to a likely related issue based on the error message.", "@essandess \r\nuse Anaconda3-2020.07-MacOSX to compile", "> @essandess\r\n> use Anaconda3-2020.07-MacOSX to compile\r\n\r\nNo\u2014this is MacPorts. It has to be compiled from source using bazel for package management.\r\n\r\nVersion 2.3.0 changes have broken this capability.", "I thought that errors is python's issue, I got that errors when compiling with brew python(3.8.5).", "The `no matching function for call to object of type` error is because MacPorts upgraded to NumPy 1.19.x; I've opened a PR to address this in MacPorts which backports 75ea0b3147: macports/macports-ports#8203", "This is now fixed. See https://github.com/macports/macports-ports/pull/8648", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42479\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42479\">No</a>\n"]}, {"number": 42478, "title": "Extremely Large Slow Down in Performance Between 21th July & 22nd July Nightly Release", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from (source or binary): Tf-Nightly-GPU\r\n- TensorFlow version (use command below): 2.4.0.dev\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 10.1 / 7.6\r\n- GPU model and memory: nVidia 2070 Max-Q\r\n\r\nIt appears some change was made between the nightly release of the 21st and the 22nd July 2020 that has caused a dramatic (over x10) slow down in the time to complete an epoch of training apart of an optimization I am running. Furthermore, I have checked right up to the current date and this slow down still exists. As well as up to TF2.4 (21st July), TF 2.3 release also runs fine.\r\n\r\nUnfortunately, for both reasons of IP and that the code is rather involved I can't share it publicly on here. And thus I appreciate trying to resolve this issue is challenging. However, is there anything in particular that was changed over these two dates that stands that out that could potential cause such a large change in time to optimize and  / or anything I could do further to investigate and thus assist with what might have resulted in this issue? \r\n\r\nBetween the two release, when running the code, there is no obvious difference in the start-up debug output that is displayed in terms of adding gpu device, successfully opening the cuda libraries, etc.", "comments": ["@oracle3001 \r\n\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42478\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42478\">No</a>\n"]}, {"number": 42477, "title": "TFLite: reduced redundant calculation in uint8/float conv.h", "body": "This PR contains almost identical changes to TF Lite convolution kernel as in #41947 but different versions (uint8 and float.) The testing platform is identical as well, note that this platform has no FPU available so floating point operations are soft floating point operations.\r\n\r\nThe following is the summary of improvements on TFLM examples in the repo:\r\n* [uint8] Sped up inference by 12.50% on person_detection (uint8 model)\r\n* [float] Sped up inference by 1.93% on magic_wand (float model)", "comments": ["the diff is fine (so long as tests pass).\r\nThis reference code was written as explicitly reference-only code where speed was not a concern at all. It was not intended to be ever run in any application. If I understand correctly the above comment, TFLite-for-Microcontrollers breaks this assumption by actually using this as the user code path?\r\n\r\nIn that case, you have much more low-hanging fruit to pick here. The Offset() calculations in the inner loop are a good place to start: it would be more efficient to expand these Offset() calls as local code, then distribute the resulting code as much as possible out of the inner loop, typically performing only simple pointer increments in the inner loops.\r\n\r\nHowever, doing so would start to break down the design of this code as easy-to-read reference code.\r\n\r\nBottom line: there is a contradiction between, on the one hand, TFLM using reference code, and on the other hand, TFLM users caring about 10% speed differences. It seems reasonable to care about 10% speed differences, so I suppose that the natural course of action here would be for TFLM to start having its own kernels rather than use reference code. Going in that direction, however, I would suggest checking with @Maratyszcza  if XNNPACK might readily provide better optimized code suitable for the portability requirements of TFLM (likely meaning - no NEON code).", "For TFLM as well, the goal is for reference code to be readable to serve as a way to implement and debug optimized kernels. As such, reference kernels are not recommended for applications where speed is important.\r\n\r\n@danielyou0230 can you please give more context on why a speed up of 10% with the reference kernels is important for you?\r\n\r\nI'll defer to @bjacob on the readability impact of this particular PR, but would also recommend against going down the path of optimizing the reference kernels.\r\n", "Thanks @bjacob and @advaitjain for letting me know what the reference kernels are for, I really appreciate your time and effort reviewing this PR.\r\n\r\n@advaitjain I'm currently a SWE intern at Google and my project is to make Tensorflow faster on microcontrollers, e.g. Arduino or soft-CPU on FPGAs like VexRISCV (no Vector extension and SIMD, which is what I mainly test on,) where most of them have no hardware-optimized instructions available, e.g. NEON, SSE, AVX, RISC-V Vector extension. Having that in mind, I turned to the kernels used by TFLM and saw there's something that can be changed to make the inference faster on those microcontrollers. @bjacob as what you understand, TFLM is using those reference kernels from TF Lite right now, that's why I'm optimizing them.\r\n\r\nI understand that TF Lite needs to have those references kernels (intended not optimized for speed) for people who want to write their optimized code for their particular platform. At the same time, TFLM needs to have their own kernels in the future, they will probably begin with the reference kernel that TF Lite have right now. But the changes I made is hardware independent, so any platform with no vector instruction support can still benefit from the change. Hopefully, when TFLM releases their own kernels, people can have these changes as a reference of how to improve the performance on microcontrollers without additional hardware instructions.", "I shouldn't be a reviewer here - I'm not actively working on TFLite.\r\n\r\nRegarding this particular diff, I don't think it significantly changes readability, so as said above I'm OK with it. That said, the code idiom here is shared with several other reference code kernels, so changing it here will create a discrepancy.\r\n\r\nGenerally I agree with Advait that we shouldn't do anything to speed up reference code. But OK to make an exception if some production code (TFLM?) unwittingly started relying on reference code and now needs a quick change.\r\n\r\nI would like to propose though to prioritize the migration of TFLM away from reference code. That should be as simple as forking the current reference code into a TFLM code directory, stripping any 'reference' in the identifier or namespace names. Then, optimizations like this could be done easily without such discussions, and as I said, there is much more still that could be done to speed up this code while still remaining self-contained portable standard C++ code, starting with dropping the Offset() calls.\r\n", "Sounds good. I have approved this PR. We'll figure out the path for any future optimizations in a separate conversation."]}, {"number": 42476, "title": "Incorrect output for unit test example in tensorflow/tensorflow/lite/micro/examples/hello_world/hello_world_test.cc ", "body": "@tensorflow/micro\r\n\r\n**System information**\r\nLaptop Model: Inspiron-7370 \r\nOS: Ubuntu 18.04.5 LTS x86_64 \r\nHost: Inspiron 7370 \r\nKernel: 5.4.0-42-generic \r\nUptime: 4 hours, 48 mins \r\nPackages: 2256 \r\nShell: bash 4.4.20 \r\nResolution: 1920x1080, 1920x1080 \r\nDE: GNOME 3.28.4 \r\nWM: GNOME Shell \r\nWM Theme: Adwaita \r\nTheme: Ambiance [GTK2/3] \r\nIcons: Ubuntu-mono-dark [GTK2/3] \r\nTerminal: gnome-terminal \r\nCPU: Intel i7-8550U (8) @ 4.000GHz \r\nRAM: 8GB\r\nGPU: Intel UHD Graphics 620 \r\nMemory: 4790MiB / 7573MiB \r\n\r\n- TensorFlow installed from (source or binary): git clone https://github.com/tensorflow/tensorflow.git\r\n- Tensorflow version (commit SHA if source): \r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Mbed OS board (but this is irrelevant since this test runs locally)\r\n\r\n**Describe the problem**\r\nWhen I make a change in the hello_world_test.cc file such as for example changing a value to something that I know should return an error such as checking:\r\n```C++\r\ninput->data.f[0] = 4.f;\r\n  invoke_status = interpreter.Invoke();\r\n  TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, invoke_status);\r\n\r\n  value = output->data.f[0];\r\n  TF_LITE_MICRO_EXPECT_NEAR(-0.959f, value, 0.05f);\r\n```\r\nwhen I run this test \r\n```\r\nrun make -f tensorflow/lite/micro/tools/make/Makefile test_hello_world_test \r\n```\r\nI should obtain:\r\n```\r\nTesting LoadModelAndPerformInference\r\n[whatever this value is] near value failed at tensorflow/lite/micro/examples/hello_world/hello_world_test.cc:94\r\n0/1 tests passed\r\n~~~SOME TESTS FAILED~~~\r\n```\r\nInstead, I obtain the following:\r\n```\r\n~~~ALL TESTS PASSED~~~'\r\ntensorflow/lite/micro/examples/hello_world/Makefile.inc:33: recipe for target 'test_hello_world_test' failed\r\nmake: *** [test_hello_world_test] Error 1\r\n```\r\n>In other words, the \"tensorflow/lite/micro/testing/micro_test.h\" framework incorrectly asserts if tests have passed and prints \"ALL TESTS PASSED\" when some tests have in fact failed. Also, it doesn't specify which tests failed (or the line in hello_world_test.cc). As seen in the error message this is probably a problem with the Makefile.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\n```\r\nme$ git clone https://github.com/tensorflow/tensorflow.git\r\nme$ ls \r\n/tensorflow\r\nme$ cd tensorflow/tensorflow/lite/micro/examples/hello_world/\r\nme$ nano hello_world_test.cc \r\n```\r\n\r\n**Changed the test comparison values at the bottom to make them fail on purpose i.e**\r\n```C++\r\ninput->data.f[0] = 4.f; // this was \"input->data.f[0] = 5.f;\" so setting it to 4 should cause an error as sin(4) != sin(5)\r\n  invoke_status = interpreter.Invoke();\r\n  TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, invoke_status);\r\n\r\n  value = output->data.f[0];\r\n  TF_LITE_MICRO_EXPECT_NEAR(-0.959f, value, 0.05f);\r\n```\r\n\r\n```\r\nme$ cd ~\r\nme$ cd tensorflow\r\nme$ run make -f tensorflow/lite/micro/tools/make/Makefile test_hello_world_test \r\n~~~ALL TESTS PASSED~~~'\r\ntensorflow/lite/micro/examples/hello_world/Makefile.inc:33: recipe for target 'test_hello_world_test' failed\r\nmake: *** [test_hello_world_test] Error 1", "comments": ["Confirming that this issue is reproducible.\r\n\r\nThe root cause is the ```-e``` flag to bash at [this line](https://github.com/tensorflow/tensorflow/blob/59c06b9016700dbf1ab0cefc062d247345cdd0f0/tensorflow/lite/micro/testing/test_linux_binary.sh#L1).\r\n\r\nModifying hello_world_test.cc to cause an error and then running the following command:\r\n```\r\ntensorflow/lite/micro/testing/test_linux_binary.sh tensorflow/lite/micro/tools/make/gen/linux_x86_64/bin/hello_world_test '~~~ALL TESTS PASSED~~~'\r\n```\r\n\r\nresults in no output log on the terminal (while the error log is in the /tmp directory):\r\n```\r\n$ cat /tmp/test_linux_binary/tensorflow/lite/micro/tools/make/gen/linux_x86_64/bin/hello_world_test/logs.txt \r\nTesting LoadModelAndPerformInference\r\n-0.959f (-1.0*2^-1) near value (-1.0*2^-1) failed at tensorflow/lite/micro/examples/hello_world/hello_world_test.cc:119\r\n0/1 tests passed\r\n~~~SOME TESTS FAILED~~~\r\n```\r\n\r\nThe issue is that the ```-e``` flag causes the test_linux_binary.sh script to exit before it has a chance to check whether the test passed or failed.\r\n\r\nRemoving the ```-e``` flag fixes the issue.\r\n```\r\n$ tensorflow/lite/micro/testing/test_linux_binary.sh tensorflow/lite/micro/tools/make/gen/linux_x86_64/bin/hello_world_test '~~~ALL TESTS PASSED~~~'\r\ntensorflow/lite/micro/tools/make/gen/linux_x86_64/bin/hello_world_test: FAIL - '~~~ALL TESTS PASSED~~~' not found in logs.\r\nTesting LoadModelAndPerformInference\r\n-0.959f (-1.0*2^-1) near value (-1.0*2^-1) failed at tensorflow/lite/micro/examples/hello_world/hello_world_test.cc:119\r\n0/1 tests passed\r\n~~~SOME TESTS FAILED~~~\r\n```\r\n\r\nAnd running the test via the Makefile also works as expected:\r\n```\r\n$ make -f tensorflow/lite/micro/tools/make/Makefile -j8 test_hello_world_test\r\ntensorflow/lite/micro/tools/make/Makefile:303: warning: overriding recipe for target 'tensorflow/lite/micro/tools/make/downloads/ruy'\r\ntensorflow/lite/micro/tools/make/Makefile:303: warning: ignoring old recipe for target 'tensorflow/lite/micro/tools/make/downloads/ruy'\r\ntensorflow/lite/micro/tools/make/Makefile:303: warning: overriding recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_grayscale'\r\ntensorflow/lite/micro/tools/make/Makefile:303: warning: ignoring old recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_grayscale'\r\ntensorflow/lite/micro/tools/make/Makefile:303: warning: overriding recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'\r\ntensorflow/lite/micro/tools/make/Makefile:303: warning: ignoring old recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'\r\ntensorflow/lite/micro/testing/test_linux_binary.sh tensorflow/lite/micro/tools/make/gen/linux_x86_64/bin/hello_world_test '~~~ALL TESTS PASSED~~~'\r\ntensorflow/lite/micro/tools/make/gen/linux_x86_64/bin/hello_world_test: FAIL - '~~~ALL TESTS PASSED~~~' not found in logs.\r\nTesting LoadModelAndPerformInference\r\n-0.959f (-1.0*2^-1) near value (-1.0*2^-1) failed at tensorflow/lite/micro/examples/hello_world/hello_world_test.cc:119\r\n0/1 tests passed\r\n~~~SOME TESTS FAILED~~~\r\n\r\nmake: *** [tensorflow/lite/micro/examples/hello_world/Makefile.inc:34: test_hello_world_test] Error 1\r\n```\r\n\r\n\r\nI have an internal change under review that will fix this issue."]}, {"number": 42475, "title": "Tensorflow 2.3.0 is much slower than PyTorch 1.4.0 in backward propagation. (20 times slower)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n   Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n   Win10\r\n- CUDA/cuDNN version: \r\n   CUDA 10.1/cuDNN 7\r\n- TensorFlow version (use command below): \r\n   2.3.0\r\n- Python version: \r\n   3.7.4\r\n- GPU model and memory: \r\n   Geforce GTX 1660Ti (6GB)\r\n\r\n**Describe the current behavior**\r\n\r\n- I tried to use both TF2 and PyTorch to train SkipGram model with negative sampling. But notice a huge training speed difference. \r\n- TF2 takes ~0.5s to finish one step while PyTorch takes ~0.01s. One step refers to use 1 batch data to do forward and backward and update the weights. \r\n- The main difference is in backward propagation. \r\n- **More specifically, 'apply_gradients()' in TF2 is much slower than counterpart 'step()' in PyTorch. Does anyone know why?**\r\n\r\n\r\n- Notes:\r\n1. Both versions use GPU.\r\n2. tf.function is used.\r\n3. Training batch size and data is exactly the same.\r\n4. Model size (vocab_size, emb_size) is the same.\r\n\r\n\r\n- TF2:\r\n\r\n```\r\n@tf.function\r\ndef train_step_tf(pos_w, pos_v, neg_v):\r\n    print('-'*50)\r\n    with tf.GradientTape() as tape:  \r\n        loss = skip_gram_model_tf(pos_w, pos_v, neg_v)\r\n    \r\n    start_time = time.time()\r\n    variables = skip_gram_model_tf.trainable_variables\r\n    print('getting variables time = {}'.format(time.time() - start_time))\r\n\r\n    start_time = time.time()\r\n    gradients = tape.gradient(loss, variables)\r\n    print('init gradient time = {}'.format(time.time() - start_time))\r\n\r\n    start_time = time.time()\r\n    optimizer.apply_gradients(zip(gradients, variables))  # !!!!! this is where the major speed difference happens!!!!!\r\n    print('apply gradient time = {}'.format(time.time() - start_time))\r\n    \r\n    print('-'*50)\r\n\r\n    return loss\r\n```\r\n\r\n- PyTorch:\r\n\r\n```\r\ndef train_step_torch(pos_w, pos_v, neg_v):  \r\n    print('-'*50)\r\n    start_time = time.time()\r\n    optimizer_torch.zero_grad()\r\n    loss = skip_gram_model_torch.forward(pos_w, pos_v, neg_v)\r\n    print('getting variables time = {}'.format(time.time() - start_time))\r\n\r\n    start_time = time.time()\r\n    loss.backward()\r\n    print('init gradient time = {}'.format(time.time() - start_time))\r\n    \r\n    start_time = time.time()\r\n    optimizer_torch.step() # !!!!this is fast!!!!\r\n    print('apply gradient time = {}'.format(time.time() - start_time))\r\n    print('-'*50)\r\n\r\n    return loss\r\n```\r\n\r\n**Describe the expected behavior**\r\nI expect the performance shouldn't be that different. Is there anything I miss to use in TF2?\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nColab link of full code:\r\nhttps://colab.research.google.com/drive/17QsTkV271LPvo6aJf1QOGuumdU8OXmVj?usp=sharing\r\n", "comments": ["Hi @TAIYISONG , I make some changes and see the speed is the same. Please ignore the first step because tensorflow allocates optimizer's variable slots in this step.\r\n\r\n1. You use adam for tf and sgd for pytorch: adam is much more time-consuming than sgd.\r\n2. remove `tf.function` decorator: I'm not sure why removing this is faster.\r\n\r\nhttps://colab.research.google.com/drive/1a__vqNBplMV8SvQpjfTlACFJsjwqsFn3?usp=sharing", "> Hi @TAIYISONG , I make some changes and see the speed is the same. Please ignore the first step because tensorflow allocates optimizer's variable slots in this step.\r\n> \r\n> 1. You use adam for tf and sgd for pytorch: adam is much more time-consuming than sgd.\r\n> 2. remove `tf.function` decorator: I'm not sure why removing this is faster.\r\n> \r\n> https://colab.research.google.com/drive/1a__vqNBplMV8SvQpjfTlACFJsjwqsFn3?usp=sharing\r\n\r\nHi @WindQAQ , thanks for helping me out! (What is the 'first step' you are referring to?)\r\n\r\nYour suggestion works. **But** I test out some scenarios on my own machine (1660Ti) following your idea: \r\n  \r\n  1. **TF-SGD-without@tf.function** takes 0.009s.\r\n  2. **Torch-SGD** takes 0.009s.\r\n  3. **TF-SGD-with@tf.function** takes 0.09s\r\n   (above points are the same as in @WindQAQ notebook and findings)\r\n  4. **TF-Adam-without@tf.function** takes 0.9s\r\n  5. **TF-Adam-with@tf.function** takes 0.5s\r\n  6. **Torch-Adam** takes 0.03s\r\n\r\nand **still find something worth digging into**:\r\n- Compare (1 vs 2);\r\n  Yes TF with this setting could be as fast as PyTorch as you point out.\r\n\r\n- Compare (1 vs 3) and (4 vs 5); \r\n  **Why @tf.function works to accelerate Adam, but not SGD** (even 10 times slower)?\r\n\r\n- Compare (5 vs 6);\r\n  **In terms of using Adam, why TF is ~20 times slower than PyTorch ?** (original question)\r\n\r\n- Compare (1 vs 4):\r\n  **Different optimizer and tf.function usage can make 100 times training speed difference?!**\r\n\r\n- Compare (2 vs 6):\r\n  **PyTorch is more stable in terms of switching optimizer.** (yes Adam is much more time-consuming than SGD.)    \r\n\r\n\r\nWhat is the black magic behind the scene? \r\nIt's kind of awkward if TF is much less stable than PyTorch in terms of switching and testing different optimizer.", "@TAIYISONG @WindQAQ maybe this issue related with (https://github.com/tensorflow/tensorflow/issues/38287).", "> @TAIYISONG @WindQAQ maybe this issue related with (#38287).\r\n\r\n@dathudeptrai \r\nYes it seems similar, and this issue is more on optimizer side #42475. Both are about unacceptable slow training speed.", "@TAIYISONG this is an interesting comparison. There are a few things which are probably not obvious, and I think this issue highlights some of them. Let me detail the differences first, and then I'll look into making the necessary modifications so that we can measure the actual performance of tf.function.\r\n\r\nFirst, I should note that `@tf.function` is more similar to TorchScript than it it to PyTorch. When comparing TensorFlow directly with PyTorch, it's more of an apples-to-apples comparison to run the TF code eagerly. So in this case, you should remove the `@tf.function`.\r\n\r\nOf course, the same code (or something similar) should work with `tf.function` as well, but in this case it's running into problem of excessive retracing. I'll add a separate comment about that.\r\n\r\nOnce I removed `@tf.function` on the original code, TF was actually faster in the limit:\r\n\r\nTF Eager:\r\n```\r\n1 step takes 0.4307682514190674 s\r\n1 step takes 0.013715505599975586 s\r\n1 step takes 0.013556718826293945 s\r\n```\r\n\r\nPyTorch:\r\n```\r\n1 step takes 0.21025729179382324 s\r\n1 step takes 0.010649442672729492 s\r\n1 step takes 0.015627145767211914 s\r\n```\r\n\r\nI'll modify the code in your colab a bit to demonstrate how to effectively run it with `@tf.function` as well, for comparison. Will post an update in a few hours with that.", "I had a chance to have a closer look at the `tf.function` instance. I see the latest version of the colab does avoid excessive retracing, which is good. It's still significantly slower than PyTorch, and we need to have a closer look why.\r\n\r\nI had a suspicion that it's because the GPU placement is inefficient, so I re-ran the tests on CPU:\r\n\r\n5. TF-Adam-without@tf.function takes ~ 0.71s\r\n6. TF-Adam-with@tf.function takes ~ 0.44s\r\n7. Torch-Adam takes ~ 0.69s\r\n6. TF-Adam-with@tf.function(experimental_compile=True) takes ~ 0.06s\r\n\r\nSo, the TF tests are slower on GPU than on CPU, which is a strong indication that some part of the computation is placed on the CPU, causing massive slowdown due to the relatively slow CPU->GPU transfers.\r\n\r\nAnother observation - if I enable XLA with `tf.function(experimental_compile=True)`, it's way faster than all three. But, that errors out on GPU, yielding a hint to the problem: `Trying to access resource skip_gram_model_tf_18/w_emb/embeddings_34250 located in device /job:localhost/replica:0/task:0/device:CPU:0 from device /job:localhost/replica:0/task:0/device:GPU:0`.\r\n\r\nThis is a sure indication that the embedding is incorrectly placed on the CPU, slowing everything down.\r\n\r\n@tomerk @fchollet @omalleyt12  for more thoughts on why the embedding might be misplaced", "**Update**: I re-ran all tests on GPU, in **tf-nightly**. Seems the slowdown is still there:\r\n\r\n**GPU**:\r\n\r\nTF-Adam-without@tf.function takes ~ 0.72s\r\nTF-Adam-with@tf.function takes ~ 0.47s\r\nTorch-Adam takes ~ 0.035s\r\nTF-Adam-with@tf.function(experimental_compile=True) takes ~ raises an error different from 2.3\r\n\r\nTF-SGD-without@tf.function takes ~ 0.7s\r\nTF-SGD-with@tf.function takes ~ 0.082s\r\nTorch-SGD takes ~ 0.0095s\r\nTF-SGD-with@tf.function(experimental_compile=True) takes ~ 0.0005s (@cheshire FYI)\r\n\r\n**CPU**:\r\n\r\nTF-Adam-without@tf.function takes ~ 0.72s\r\nTF-Adam-with@tf.function takes ~ 0.43s\r\nTorch-Adam takes ~ 0.69s\r\nTF-Adam-with@tf.function(experimental_compile=True) takes ~ raises error\r\n\r\nNote: \r\n* results fluctuate a bit\r\n* for a fair comparison should also include a TorchScript version.\r\n\r\n**Edits**: fixed wrong numbers and conclusion.", "> @TAIYISONG this is an interesting comparison. There are a few things which are probably not obvious, and I think this issue highlights some of them. Let me detail the differences first, and then I'll look into making the necessary modifications so that we can measure the actual performance of tf.function.\r\n> \r\n> First, I should note that `@tf.function` is more similar to TorchScript than it it to PyTorch. When comparing TensorFlow directly with PyTorch, it's more of an apples-to-apples comparison to run the TF code eagerly. So in this case, you should remove the `@tf.function`.\r\n> \r\n> Of course, the same code (or something similar) should work with `tf.function` as well, but in this case it's running into problem of excessive retracing. I'll add a separate comment about that.\r\n> \r\n> Once I removed `@tf.function` on the original code, TF was actually faster in the limit:\r\n> \r\n> TF Eager:\r\n> \r\n> ```\r\n> 1 step takes 0.4307682514190674 s\r\n> 1 step takes 0.013715505599975586 s\r\n> 1 step takes 0.013556718826293945 s\r\n> ```\r\n> \r\n> PyTorch:\r\n> \r\n> ```\r\n> 1 step takes 0.21025729179382324 s\r\n> 1 step takes 0.010649442672729492 s\r\n> 1 step takes 0.015627145767211914 s\r\n> ```\r\n> \r\n> I'll modify the code in your colab a bit to demonstrate how to effectively run it with `@tf.function` as well, for comparison. Will post an update in a few hours with that.\r\n\r\n@mdanatg Hi, thanks for the reply. \r\n\r\nIf I'm correct I notice you change the optimizer from Adam() to Adagrad() to get the above result. \r\nI rerun the colab with **Adam()** on **GPU**, ignoring **tf.function**, but still see my original question3: \"In terms of using Adam, why TF is ~20 times slower than PyTorch ? (original question)\".", "> **Update**: I re-ran all tests on GPU, in **tf-nightly**. Things seem to be much improved, although the non-XLA version is still significantly slower:\r\n> \r\n> [TF-Adam-without@tf.function](mailto:TF-Adam-without@tf.function) takes ~ 0.7s\r\n> [TF-Adam-with@tf.function](mailto:TF-Adam-with@tf.function) takes ~ 0.082s\r\n> Torch-Adam takes ~ 0.0095s\r\n> [TF-Adam-with@tf.function](mailto:TF-Adam-with@tf.function)(experimental_compile=True) takes ~ 0.0005s (@cheshire FYI)\r\n> \r\n> Note that for a comparison should also include a TorchScript version.\r\n> \r\n> **Edit**: fixed wrong numbers and conclusion.\r\n\r\n\r\n\r\n> **Update**: I re-ran all tests on GPU, in **tf-nightly**. Things seem to be much improved, although the non-XLA version is still significantly slower:\r\n> \r\n> [TF-Adam-without@tf.function](mailto:TF-Adam-without@tf.function) takes ~ 0.7s\r\n> [TF-Adam-with@tf.function](mailto:TF-Adam-with@tf.function) takes ~ 0.082s\r\n> Torch-Adam takes ~ 0.0095s\r\n> [TF-Adam-with@tf.function](mailto:TF-Adam-with@tf.function)(experimental_compile=True) takes ~ 0.0005s (@cheshire FYI)\r\n> \r\n> Note that for a comparison should also include a TorchScript version.\r\n> \r\n> **Edit**: fixed wrong numbers and conclusion.\r\n\r\nIt seems in **1st** case TF still doesn't perform well on Adam() without **tf.function**, compared to **3rd** torch case.\r\n\r\nAnd I tried to use _Lookahead(RAdam(0.02))_ optimizer in TF, it's worse than Adam() case in both with/without tf.function() cases.\r\nI will try upgrading to **tf-nightly** to see if this kind of more advanced optimizer is supported well in TF.\r\n\r\nThanks again for your testing. I will keep an eye on this for any updates. \r\n", "Thanks. I don't think I have access to the original colab so I made some copies. But indeed I forgot to change `SGD` back to `Adam` in my colab. I fixed the results. Here are the colabs as well:\r\n\r\n* [CPU](https://colab.research.google.com/drive/1v3lOdA0wIS_q7xWJcRdSRH7roEp8Wh29)\r\n* [GPU](https://colab.research.google.com/drive/1SMAzg6F05iNkTFbw0tqy8NwcG02xGnbB#scrollTo=IMl1XRfotLmV)\r\n\r\nSo it seems that even in nightly, TF is still >20 x slower than PyTorch, and judging from the CPU-only results, it seems that TF doesn't properly use the GPU in this case.", "> Thanks. I don't think I have access to the original colab so I made some copies. But indeed I forgot to change `SGD` back to `Adam` in my colab. I fixed the results. Here are the colabs as well:\r\n> \r\n> * [CPU](https://colab.research.google.com/drive/1v3lOdA0wIS_q7xWJcRdSRH7roEp8Wh29)\r\n> * [GPU](https://colab.research.google.com/drive/1SMAzg6F05iNkTFbw0tqy8NwcG02xGnbB#scrollTo=IMl1XRfotLmV)\r\n> \r\n> So it seems that even in nightly, TF is still >20 x slower than PyTorch, and judging from the CPU-only results, it seems that TF doesn't properly use the GPU in this case.\r\n\r\nI set the original colab as \"Anyone on the internet with this link can edit\". I don't have access to above 2 new colabs either, but not a big deal lol.\r\n\r\n**Is there any plan under way to 'fix' this issue in TF?** \r\nNot only about Adam optimizer inefficiency but also other original questions about tf.function are still unclear:\r\n(quoted from the comparison I did with @WindQAQ in the beggining)\r\n\r\n> Compare (1 vs 2);\r\n> Yes TF with this setting could be as fast as PyTorch as you point out.\r\n\r\n> Compare (1 vs 3) and (4 vs 5);\r\n> Why @tf.function works to accelerate Adam, but not SGD (even 10 times slower)?\r\n\r\n> Compare (5 vs 6);\r\n> In terms of using Adam, why TF is ~20 times slower than PyTorch ? (original question)\r\n\r\n> Compare (1 vs 4):\r\n> Different optimizer and tf.function usage can make 100 times training speed difference?!\r\n\r\n> Compare (2 vs 6):\r\n> PyTorch is more stable in terms of switching optimizer. (yes Adam is much more time-consuming than SGD.)\r\n\r\n\r\nStill I would like to use the most advanced optimizer in my work. RANGER optimizer, which refers to the combo of newly introduced optimizers LOOKAHEAD and RADAM, is even worse than Adam in our tests above. \r\n\r\nJust out of interest sake, why difference optimizers could make such big difference in performance in TF? \r\nIs it because Adam is not supported well in GPU?", "Yes, this is a significant bug and we definitely plan to fix it, though I couldn't give an exact timeline. We're currently discussing a few solutions internally.\r\n\r\nThe choice of optimizer influences performance so much is due to the nature of the bug: essentially, we have some computations that are executed on the CPU (likely the embedding lookups). That causes data transfers between CPU and GPU, which are very slow. Different optimizers result in different numbers of such transfers, and that would explain why results vary so wildly. \r\n\r\nP.S. I just realized I edited your original colab - that was not intentional, sorry about it!", "For more context, this is explicit CPU placement in the embeddings layer: https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/python/keras/layers/embeddings.py;l=141?q=layers%2Fembeddings.py%20file:tensorflow", "Is this bug specific only to tensorflow==2.3.0 or does it apply also to older versions?", "The explicit placement was done two years ago, so it probably applies to all versions of `kers.layers.Embedding`. That said, it might go unnoticed in practice if for example the embedding table is too large to fit the GPU memory, or if the model is large enough to dominate the compute time.", "Thanks @mdanatg, good to know more about TF schema behind the scene and you guys are planning to update it. Looking forward to seeing the issue go away. :)", "@TAIYISONG Looks like this was resolved in recent TF versions. I ran your code with `TF2.7` and the computation times are similar. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/c8ddfaadda2ae1bfb75e8b3cd8bf01e2/training_speed_dif_tf2_vs_torch.ipynb). Thanks!", "@jvishnuvardhan Thanks for the update. So glad to see the issue is resolved in 2.7.0! \r\n@mdanatg Thanks for taking this issue previously, also many thanks to the engineers who fixed this!!! :)"]}, {"number": 42474, "title": "tensorflow.python.framework.errors_impl.NotFoundError:  No algorithm worked!", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux -  Linux 5.8.1\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.8\r\n- CUDA/cuDNN version: 11.0.2 / 8.0.2.39\r\n- GPU model and memory: RTX 2060 6 GB\r\n\r\n**Describe the current behavior**\r\nCrashing on conv_ops.cc when running on basic MNIST dataset\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://gist.github.com/Ashiix/9b00c28ee11411de4f052c0733fcbffd\r\n\r\n**Other info / logs**\r\nhttps://gist.github.com/Ashiix/053749aed670c3c2f3de02c3b193e5d3", "comments": ["@Ashiix \r\n\r\nI have tried in colab with TF version 2.3. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/3ca66219954eafae5dcb46e33f9959a1/untitled264.ipynb).You are also seeing the same behavior?\r\n\r\nThanks!", "Downgrading to version 2.2 solves the problem.\r\nCommand:\r\n$ downgrade python-tensorflow-cuda\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@Ashiix \r\n\r\nAny update on this issue please. Thanks!", "**System Information** \r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 \r\nTensorFlow version (use command below): 2.3.0\r\nPython version: 3.8\r\nCUDA/cuDNN version: 11.0.2 / 8.0.2.39\r\nGPU model and memory: 1080Ti 10GB\r\n\r\n**Describe the current behavior**\r\n\r\n2020-09-01 20:57:59.779251: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at conv_ops_3d.cc:491 : Not found: No algorithm worked!\r\n\r\ntensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.NotFoundError:  No algorithm worked!\r\n         [[node functional_1/conv3d/Conv3D (defined at train.py:348) ]] [Op:__inference_train_function_1168]", "Try again with TensorFlow version 2.2.\r\nWorks for me.", "@yenigma Thanks for the suggestion. Unfortunately, I have also tried Tensorflow 2.2 and also changed CUDA 10.1, cuDNN 7.6.5 and the error doesn't change. ", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42474\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42474\">No</a>\n"]}, {"number": 42473, "title": "Check libraries linked into mlir-hlo-opt", "body": "Adds a call to mlir_check_all_link_libraries() to check all libraries\r\nlinked into mlir-hlo-opt.", "comments": []}, {"number": 42472, "title": "Hash first in _make_input_signature_hashable", "body": "Benchmarks at end", "comments": ["lgtm pending benchmark numbers", "Benchmarks over 3 batches of 10 trials each\r\n\r\nMinimum\r\ndefun_matmul_2_by_2_CPU: 164 -> 148 us\r\ndefun_matmul_2_by_2_CPU_async: 83.6 -> 80.5 us\r\ndefun_matmul_2_by_2_with_signature_CPU: 219 -> 195 us\r\n\r\nMean\r\ndefun_matmul_2_by_2_CPU: 175 -> 156 us\r\ndefun_matmul_2_by_2_CPU_async: 86 -> 82.5 us\r\ndefun_matmul_2_by_2_with_signature_CPU: 236 -> 213 us\r\n\r\nMedian\r\ndefun_matmul_2_by_2_CPU: 172-> 156 us\r\ndefun_matmul_2_by_2_CPU_async: 86 -> 82.5 us\r\ndefun_matmul_2_by_2_with_signature_CPU: 227 -> 204 us"]}, {"number": 42471, "title": "TFLite Shared Library build error on Raspberry Pi 4 (native build with Bazel). Trying to build with XNNPACK enabled", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Raspbian Buster\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Raspberry Pi 4\r\n- TensorFlow installed from (source or binary): source (attempt to compile tensorflow lite libraries)\r\n- TensorFlow version: 2.3\r\n- Python version: 3.7.3\r\n- Installed using virtualenv? pip? conda?: --\r\n- Bazel version (if compiling from source): Bazel 3.4.1\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 8.3.0\r\n- GPU model and memory: not relevant\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nTrying to compile the shared library for Raspberry Pi 4 using Bazel to enable XNNPACK doesn't work (**doing a native build on the Raspberry Pi 4**). I have used the instructions from https://blog.tensorflow.org/2020/07/accelerating-tensorflow-lite-xnnpack-integration.html to enable XNNPACK and https://www.tensorflow.org/lite/guide/build_rpi as instructed.\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n`bazel build --define tflite_with_xnnpack=true -c opt //tensorflow/lite/c:libtensorflowlite_c.so`\r\n\r\nI have removed the cross compile option mentioned in https://www.tensorflow.org/lite/guide/build_rpi  which is `https://www.tensorflow.org/lite/guide/build_rpi` and added `--define tflite_with_xnnpack=true` to enable xnnpack.\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nERROR: /home/lahiru/workfolder/bazel_patch_test/tensorflow/tensorflow/lite/c/BUILD:22:24: Linking of rule '//tensorflow/lite/c:libtensorflowlite_c.so' failed (Exit 1)\r\nbazel-out/arm-opt/bin/tensorflow/lite/kernels/internal/_objs/neon_tensor_utils/neon_tensor_utils.pic.o:neon_tensor_utils.cc:function void ruy::RunKernel<(ruy::Path)4, signed char, signed char, int, ruy::MulParams<int, int> >(ruy::Tuning,\r\nruy::SidePair<ruy::PEMat> const&, void*, ruy::SidePair<int> const&, ruy::SidePair<int> const&, ruy::EMat*): error: undefined reference to 'ruy::Kernel8bitNeonOutOfOrder(ruy::KernelParams8bit<4, 2> const&)'\r\nbazel-out/arm-opt/bin/tensorflow/lite/kernels/internal/_objs/neon_tensor_utils/neon_tensor_utils.pic.o:neon_tensor_utils.cc:function void ruy::RunKernel<(ruy::Path)4, signed char, signed char, int, ruy::MulParams<int, int> >(ruy::Tuning,\r\nruy::SidePair<ruy::PEMat> const&, void*, ruy::SidePair<int> const&, ruy::SidePair<int> const&, ruy::EMat*): error: undefined reference to 'ruy::Kernel8bitNeonOutOfOrder1Col(ruy::KernelParams8bit<4, 2> const&)'\r\nbazel-out/arm-opt/bin/tensorflow/lite/kernels/internal/_objs/neon_tensor_utils/neon_tensor_utils.pic.o:neon_tensor_utils.cc:function void ruy::RunPack<(ruy::Path)4, ruy::FixedKernelLayout<(ruy::Order)0, 16, 2>, signed char, signed char>(r\r\nuy::Tuning, ruy::EMat const&, ruy::PEMat*, int, int): error: undefined reference to 'ruy::Pack8bitNeonOutOfOrder2Cols(ruy::PackParams8bit const&)'\r\nbazel-out/arm-opt/bin/tensorflow/lite/kernels/internal/_objs/neon_tensor_utils/neon_tensor_utils.pic.o:neon_tensor_utils.cc:function void ruy::RunPack<(ruy::Path)4, ruy::FixedKernelLayout<(ruy::Order)0, 16, 2>, signed char, signed char>(r\r\nuy::Tuning, ruy::EMat const&, ruy::PEMat*, int, int): error: undefined reference to 'ruy::Pack8bitNeonOutOfOrder2Cols(ruy::PackParams8bit const&)'\r\nbazel-out/arm-opt/bin/tensorflow/lite/kernels/internal/_objs/neon_tensor_utils/neon_tensor_utils.pic.o:neon_tensor_utils.cc:function void ruy::RunPack<(ruy::Path)4, ruy::FixedKernelLayout<(ruy::Order)0, 16, 2>, signed char, signed char>(r\r\nuy::Tuning, ruy::EMat const&, ruy::PEMat*, int, int): error: undefined reference to 'ruy::Pack8bitNeonOutOfOrder2Cols(ruy::PackParams8bit const&)'\r\nbazel-out/arm-opt/bin/tensorflow/lite/kernels/internal/_objs/neon_tensor_utils/neon_tensor_utils.pic.o:neon_tensor_utils.cc:function void ruy::RunPack<(ruy::Path)4, ruy::FixedKernelLayout<(ruy::Order)0, 16, 2>, signed char, signed char>(r\r\nuy::Tuning, ruy::EMat const&, ruy::PEMat*, int, int): error: undefined reference to 'ruy::Pack8bitNeonOutOfOrder2Cols(ruy::PackParams8bit const&)'\r\nbazel-out/arm-opt/bin/tensorflow/lite/kernels/internal/_objs/neon_tensor_utils/neon_tensor_utils.pic.o:neon_tensor_utils.cc:function void ruy::RunPack<(ruy::Path)4, ruy::FixedKernelLayout<(ruy::Order)0, 16, 4>, signed char, signed char>(r\r\nuy::Tuning, ruy::EMat const&, ruy::PEMat*, int, int): error: undefined reference to 'ruy::Pack8bitNeonOutOfOrder4Cols(ruy::PackParams8bit const&)'\r\nbazel-out/arm-opt/bin/tensorflow/lite/kernels/internal/_objs/neon_tensor_utils/neon_tensor_utils.pic.o:neon_tensor_utils.cc:function void ruy::RunPack<(ruy::Path)4, ruy::FixedKernelLayout<(ruy::Order)0, 16, 4>, signed char, signed char>(r\r\nuy::Tuning, ruy::EMat const&, ruy::PEMat*, int, int): error: undefined reference to 'ruy::Pack8bitNeonOutOfOrder4Cols(ruy::PackParams8bit const&)'\r\nbazel-out/arm-opt/bin/tensorflow/lite/kernels/internal/_objs/neon_tensor_utils/neon_tensor_utils.pic.o:neon_tensor_utils.cc:function void ruy::RunPack<(ruy::Path)4, ruy::FixedKernelLayout<(ruy::Order)0, 16, 4>, signed char, signed char>(r\r\nuy::Tuning, ruy::EMat const&, ruy::PEMat*, int, int): error: undefined reference to 'ruy::Pack8bitNeonOutOfOrder4Cols(ruy::PackParams8bit const&)'\r\nbazel-out/arm-opt/bin/tensorflow/lite/kernels/internal/_objs/neon_tensor_utils/neon_tensor_utils.pic.o:neon_tensor_utils.cc:function void ruy::RunPack<(ruy::Path)4, ruy::FixedKernelLayout<(ruy::Order)0, 16, 4>, signed char, signed char>(r\r\nuy::Tuning, ruy::EMat const&, ruy::PEMat*, int, int): error: undefined reference to 'ruy::Pack8bitNeonOutOfOrder4Cols(ruy::PackParams8bit const&)'\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/lite/c:libtensorflowlite_c.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1574.126s, Critical Path: 260.79s\r\nINFO: 1088 processes: 1088 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n\r\n\r\n", "comments": ["Current build system assumes that Bazel is running under x86_64. So you'd better build it on x86_64 machine with \"--config=elinux_armhf\"", "Okay, but shouldn't this still be equivalent. Or am I missing something here ?  (Sorry I just got started with looking at bazel and bulding tflite with XNNPACK enabled). \r\n\r\n**Some background to this** I am trying to enable `XNNPACK` for the following devices `Raspberry Pi 4`, `Nvidia Jetson TX1`, `Nvidia Jetson TX2`, `Nvidia Jetson AGX Xavier`  (which has been introduced in tflite 2.3 as mentioned in --> https://blog.tensorflow.org/2020/07/accelerating-tensorflow-lite-xnnpack-integration.html) which is why I am trying to build with bazel on the the device itself. Even if bazel is not the route to go that's fine. \r\n\r\nSince the weekend is near, hope you have a good one! :-) ", "We don't use bazel for ARM native build. For the native build, we can use Makefile but it doesn't build XNNPACK.\r\nIf you want to build ARM binary with XNNPACK enabled, cross compiling with bazel is the way to go.\r\n\r\nActually you can also use CMake on ARM devices.\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/cmake\r\nBut it wasn't tested with ARM devices.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42471\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42471\">No</a>\n"]}]