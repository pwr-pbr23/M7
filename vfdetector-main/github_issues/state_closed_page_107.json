[{"number": 51873, "title": "[tf.data] extend multi-threaded MapDataset based tests to eager mode", "body": "This PR attempts to extend the multi-threaded map dataset tests to eager mode.\r\n\r\nThe `cached_session` context is left untouched as it yields a dummy eager session that simulates execution in eager mode.\r\nTEST LOG\r\n```\r\nINFO: Build completed successfully, 20 total actions\r\n//tensorflow/python/data/kernel_tests:map_test                           PASSED in 33.1s\r\n  Stats over 19 runs: max = 33.1s, min = 15.3s, avg = 20.5s, dev = 4.2s\r\n\r\nINFO: Build completed successfully, 20 total actions\r\n```\r\n\r\ncc: @aaudiber I tried removing the session context and executing these tests with eager only combinations and it worked fine. However, the tests fail when the `cached_session` is not used during thread `start` and `join` steps in graph mode. Thus, I kept the `cached_session` intact and let it handle the eager and graph scenarios. Let me know what you think of this approach.", "comments": ["@jsimsa I tried to run the test cases without explicitly using the session and by using the `self.evaluate` method in both eager and graph modes. However, the `graph` mode execution fails with:\r\n\r\n```console\r\nAssertionError: Error in checkedThread: Unsupported type <class 'tensorflow.python.framework.ops.Tensor'>.\r\n\r\nAssertionError: A checked thread was not joined.\r\n```\r\n\r\nAlso, the eager mode execution was a bit flaky when it comes to multi-threaded iterations.\r\ncc: @aaudiber ", "@jsimsa  Can you please assist on above comments from @kvignesh1420. Thanks!", "I do not have bandwidth to assist @kvignesh1420 to figure out how to add support for the missing test combinations. If he is blocked resolving the issues he has encountered, my suggestion would be to close this PR.", "@jsimsa I understand. Let me just put this into the draft state. I Will take a look when I find some time. cc: @gbaned ", "@kvignesh1420  Any update on this PR? Please. Thanks!", "@gbaned I think it's better to close this PR as I currently don't have access to the computing systems that I previously had at IBM. "]}, {"number": 51872, "title": "Hexagon delegate", "body": "I am trying to build a simple \"Hexagon delegate\" based android app by using https://www.tensorflow.org/lite/performance/hexagon_delegate link. \r\nI have pixel 2xl phone for test this. I have rooted my phone for access all facility but i was fail to make this app. \r\nI am facing 1 error which is  \"java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol \"dlopen\" referenced by \"/data/app/~~2R44BqWkRYHRwpMUOwTYfg==/com.example.myapplication-2UccuMnXIaXolmx6-6ftSQ==/lib/arm64/libtensorflowlite_hexagon_jni.so\"...\"\r\nNow, requesting help from anybody. Thank you\r\n", "comments": ["@waqarahmad84  In order to expedite the trouble-shooting process here, Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose), and explain more about your issue ?Thanks!", "@waqarahmad84 Could you please refer to the similar [issue1](https://github.com/tensorflow/tensorflow/issues/39539), [issue2 ](https://github.com/tensorflow/tensorflow/issues/36804) and please let us know if it helps? Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 51869, "title": "Don't constant-fold DT_RESOURCE constants.", "body": "PiperOrigin-RevId: 391803952\r\nChange-Id: I0ea3ec31d3e7dfda0f03b4027a237f08d00a3091", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51869) for more info**.\n\n<!-- need_sender_cla -->", "@pranve  Can you please sign CLA. Thanks!", "@googlebot I signed it!"]}, {"number": 51868, "title": "[INTEL MKL] Not rewrite conv_grad ops to MKL with explicit padding", "body": "This change is for fixing a unit test tensorflow/python/kernel_tests/depthwise_conv_op_test.py. \r\nThe test specifically tests backward ops with explicit padding and oneDNN kernels yet do not support such a case. So we are disabling re-write for such cases.", "comments": []}, {"number": 51867, "title": "[oneDNN] Fixing tensorflow/python/framework/node_file_writer_test", "body": "This PR fixes failure in the test //tensorflow/python/framework/node_file_writer_test when oneDNN is enabled.\r\nThis test tries to write all executed nodes in eager mode to a file with skipping duplicates under certain conditions.\r\nSince we rewrite some ops in eager mode, expected op name can be different. Also number of written nodes can be different since we only rewrite ops that have supported datatypes and are assigned to CPU device.", "comments": []}, {"number": 51866, "title": "[XLA, GPU] Make kExp not expensive", "body": "This trigger better fusion for EfficientNet and allow to increase the mini-batch size used.\r\nThis remove one temporary from the forward to the backward.\r\n\r\nAt the bottom, there is a python script with a snippet of EfficientNet that show the difference in behavior before and after this PR.\r\nUsing replay_computation on the hlo from this script on a V100, I have before this PR:\r\n```\r\n2021-09-07 18:59:47.897674: I tensorflow/compiler/xla/tools/replay_computation.cc:336] Done executing in 0.005307s: a_inference_train_315__XlaMustCompile_true_config_proto___n_007_n_0...02_001_000__executor_type____.188\r\n```\r\nand after this PR\r\n```\r\n2021-09-07 18:59:24.847251: I tensorflow/compiler/xla/tools/replay_computation.cc:336] Done executing in 0.004999s: a_inference_train_315__XlaMustCompile_true_config_proto___n_007_n_0...02_001_000__executor_type____.188\r\n```\r\nSo this also give a speed up on V100. \r\n\r\n```\r\n#!/usr/bin/env python\r\n\r\n# Usage: XLA_FLAGS=\"--xla_dump_to=xla-swish --xla_dump_hlo_as_text --xla_dump_hlo_as_dot --xla_dump_hlo_as_html\" python swish.v3.py\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ntf.keras.backend.clear_session()\r\ntf.config.optimizer.set_jit(True)\r\ntf.keras.backend.set_image_data_format('channels_last')\r\npolicy = tf.keras.mixed_precision.experimental.Policy('mixed_float16', loss_scale='dynamic')\r\ntf.keras.mixed_precision.experimental.set_policy(policy)\r\n\r\n# Can't use model = tf.keras.models.Sequential([]) due to BN training\r\nclass Foo(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Foo, self).__init__()\r\n        self.conv1 = tf.keras.layers.DepthwiseConv2D(kernel_size=3, padding='same', data_format='channels_last', use_bias=False)\r\n        self.bn1 = tf.keras.layers.BatchNormalization(axis=-1)\r\n        self.act1 = tf.keras.layers.Activation('swish')\r\n        self.conv2 = tf.keras.layers.DepthwiseConv2D(kernel_size=3, padding='same', data_format='channels_last', use_bias=False)\r\n        self.fc = tf.keras.layers.Dense(10, use_bias=False)\r\n\r\n    def call(self, x):\r\n        x = self.act1(self.bn1(self.conv1(x), training=True))\r\n        return self.fc(self.conv2(x))\r\n\r\nmodel = Foo()\r\n\r\ninp = tf.Variable(np.random.normal(size=(512, 14, 14, 672)), dtype=tf.float16)\r\noup = tf.random.uniform((512, 14, 14, 10), dtype=tf.float16)\r\n\r\n@tf.function(experimental_compile=True)\r\ndef train(inp, oup):\r\n    with tf.GradientTape() as tape:\r\n        preds = model(inp)\r\n        loss = tf.keras.losses.mse(preds, oup)\r\n        grads = tape.gradient(loss, model.trainable_variables)\r\n    return loss, grads\r\n\r\nprint(\"Before train\")\r\ntrain(inp, oup)\r\nprint(\"Script end\")\r\n```\r\n\r\n@sanjoy @cheshire ", "comments": ["The CI give this error:\r\n```\r\n@copybara-service\r\nimport/copybara \u2014 An error happened while migrating the change\r\n```\r\n\r\nAny idea what is going on?"]}, {"number": 51865, "title": "Replace distutils.sysconfig.get_python_lib() with sysconfig.get_path('purelib') - python 3.10 support", "body": "This is part of the effort for #51776 to get python 3.10 support.\r\nThis PR replace distutils.sysconfig.get_python_lib() with sysconfig.get_path('purelib')\r\nas distutils has been deprecated by python 3.10.\r\n\r\nNote sysconfig has been available since python 3.2.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang Can you please resolve conflicts? Thanks!", "@yongtang  Can you please resolve conflicts? Thanks!", "@yongtang Can you please resolve conflicts? Thanks!", "@yongtang  Any update on this PR? Please. Thanks!", "@yongtang Any update on this PR? Please. Thanks!"]}, {"number": 51864, "title": "Fixed a small typo", "body": null, "comments": ["We will not be encouraging one liner grammatical changes as this is expensive process, thank you for your interest.\r\nCC @mihaimaruseac "]}, {"number": 51862, "title": "Custom metrics doc doesn't mention `assign` method", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/guide/keras/train_and_evaluate#custom_metrics\r\n\r\n## Description of issue (what needs changing):\r\n\r\n**First small issue**:  `reset_states` is deprecated, il should be `reset_state`, shouldn't it? Furthermore, this method is not required when creating a custom metrics.\r\n\r\n**Second issue**:\r\nIt is not necessary to subclass Metric (cf https://github.com/tensorflow/tensorflow/issues/28601#issuecomment-505098700). This should be written in the documentation. I am sure many users would be interested by the \"easy\" way, i.e. passing a function `def my_metric(y_true, y_pred)` as a metric\r\n\r\n**Third issue**:\r\nThe doc shows an example with `assign_add` when updating the metrics. However, it may not fit users need.\r\nFor example, let's consider I want to compute a Peak Signal to Noise Ratio metric (a kind of logarithmic MSE). If I follow the doc, I would write something like this: \r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport math\r\n\r\nclass PSNR(keras.metrics.Metric):\r\n    \"\"\"\r\n    Peak Signal to Noise Ratio metric\r\n    \"\"\"\r\n    def __init__(self, name='PSNR', **kwargs):\r\n        super().__init__(name, **kwargs)\r\n        self.psnr = self.add_weight(name='PSNR', initializer='zeros')\r\n\r\n    def update_state(self, y_true, y_pred, sample_weight=None):\r\n        y_pred, y_true = tf.cast(y_pred, tf.float32), tf.cast(y_true, tf.float32)\r\n        mse = tf.reduce_mean(keras.metrics.mean_squared_error(y_true, y_pred))\r\n        psnr = 10.0 * tf.divide(tf.math.log(tf.divide(10000**2, mse)), math.log(10))\r\n\r\n        self.psnr.assign_add(psnr)\r\n\r\n    def result(self):\r\n        return self.psnr\r\n```\r\n\r\nAs a high level API user, I am not familiar with `tf.Variable` methods. Thus, I used `assign_add` like in the documentation instead of `assign` because I had no clue `assign` existed. And it took me some time to figure why my metric was so high and increasing so fast...\r\n\r\n\r\n### Clear description\r\n- The documentation should mention the \"easy\" way to create a metric, i.e. passing a function `def my_metric(y_true, y_pred)`. It should be quite the same as the custom losses section (https://www.tensorflow.org/guide/keras/train_and_evaluate#custom_losses)\r\n- I think an exemple with `assign` would benefit some users who would need to implement a metric that is not a sum\r\n\r\n### Correct links\r\nYes\r\n\r\n### Parameters defined\r\nYes\r\n\r\n### Returns defined\r\nYes\r\n\r\n### Raises listed and defined\r\nYes\r\n\r\n### Usage example\r\nYes\r\n\r\n### Request visuals, if applicable\r\nNo\r\n\r\n### Submit a pull request?\r\ndon't know\r\n", "comments": ["@nicolasnn ,\r\nWe see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyse the issue.Thanks!", "Edited", "@tilakrayal @sanatmpa1 i would like to work on this issue. Could you guide me regarding the same?", "Will take your issue one by one as follow.\r\n\r\n1. You are right `reset_states` has been depreciated, instead you can use `reset_state` as mentioned in the warning, I can help you with creating PR for thew same.\r\n\r\n2. Subclassing metric will resolve many of the complex scenarios =, whereas other methods will be having certain limitations. You can still use the simplest method to define custom metric as you have mentioned. You can also look into [this](https://medium.com/swlh/custom-loss-and-custom-metrics-using-keras-sequential-model-api-d5bcd3a4ff28) article where he has explained multiple ways to define custom metrics.\r\n\r\n3. `assign_add` to update the metrics was used specific to the training example shown in the document, however, as per your requirement you can use any of the state ops mentioned [here](https://www.tensorflow.org/api_docs/cc/group/state-ops).", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 51861, "title": "Replacement for #51392", "body": "This is a resubmission of https://github.com/tensorflow/tensorflow/pull/51392 but with the new kernels disabled by default as suggested in https://github.com/tensorflow/tensorflow/pull/51392#issuecomment-912882899 (new changes are all in the second commit).\r\nIt also attempts to fix the determinism test failure on Windows.\r\n\r\ncc @nluehr @reedwm ", "comments": []}, {"number": 51860, "title": "Wrong training count in eager execution mode!", "body": "Hi,\r\n\r\nI am using MNIST dataset with 54,000 training data on a CNN model. TensorFlow version 2.6.0.\r\n\r\nWhen the code runs in enabled eager execution, in the training step only 844 data are used in each epoch!\r\n I think the output shots is clear:\r\n\r\n`1- When eager execution is enabled in default`\r\n![image](https://user-images.githubusercontent.com/84374136/132291914-1b899fd0-ef9d-4975-acfe-f5d7c1aca693.png)\r\n\r\n\r\n\r\n`2- tf.compat.v1.disable_eager_execution()`\r\n![image](https://user-images.githubusercontent.com/84374136/132291580-30fbd06e-c704-4f39-824e-f02621aac804.png)\r\n\r\nWhat's the problem? Does anyone know the reason for this difference in numbers?\r\n", "comments": ["Hi @Nima-pw !We see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Not solved yet!", "Hi @Nima-pw ,Could you please share a stand alone code to reproduce this issue then?It helps expedite the issues.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51860\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51860\">No</a>\n"]}, {"number": 51858, "title": "[tflite conversion] Output order changes with multiple outputs", "body": "\r\nI maybe wrong... is so, could anyone provide how to fix this?\r\n\r\n### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installation (pip package or built from source): pip package version 2.6.0\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): N/A\r\n\r\n### 2. Code\r\n\r\n```py\r\nimport tensorflow as tf\r\n\r\nprint(tf.version.VERSION)\r\n\r\ninput = tf.keras.layers.Input(shape=(3,3,32), name=\"input\")\r\n\r\no1 = tf.keras.layers.Conv2D(2, (1,1), activation='relu', input_shape=(1,3,3,32))(input)\r\no2 = tf.keras.layers.Conv2D(16, (1,1), activation='relu', input_shape=(1,3,3,32))(input)\r\no3 = tf.keras.layers.Conv2D(32, (1,1), activation='relu', input_shape=(1,3,3,32))(input)\r\n\r\nmodel = tf.keras.Model(inputs=input, outputs=[o1,o2,o3])\r\nmodel.summary()\r\n\r\ntf.keras.models.save_model(model, \"test_saved_model\")\r\n```\r\n\r\n```py\r\nimport tensorflow as tf\r\n\r\nprint(tf.version.VERSION)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(\"test_saved_model\")\r\nconverter.allow_custom_ops = True\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\n\r\ntflite_model = converter.convert()\r\nopen(\"test_saved_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\nProvide code to help us reproduce your issues using one of the following options:\r\n\r\n\r\n\r\n### 3. Failure after conversion\r\nconversion is successful, but the generated model is wrong;\r\n\r\n- Output order has changed (order of last axis: 2, 32, 16)\r\n\r\n![image](https://user-images.githubusercontent.com/4616940/132270784-01d0bfcf-c036-4cd9-9cd7-98eea93fd075.png)\r\n\r\n- Expected something like this (order of last axis: 2, 16, 32)\r\n![image](https://user-images.githubusercontent.com/4616940/132270636-fdff902f-c468-4006-abd8-58a58f0df919.png)\r\n\r\n### 4. (optional) RNN conversion support\r\n\r\n\r\n### 5. (optional) Any other info / logs\r\n", "comments": ["Please consider using the signature instead of relying on the tensor indices.\r\nhttps://www.tensorflow.org/lite/guide/signatures", "> Please consider using the signature instead of relying on the tensor indices.\r\n\r\nThanks for point this out.\r\nI think need some time to understand how to apply to my situation.", "@abattery , if possible, could you please apply this signature to my above examples?\r\nI use `tflite` file as input to another projects but not the interpreter.", "In such cases, you can read the signature information from the TensorFlow Lite model file, stored in the flatbuffer schema:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/schema.fbs#L1193", "FYI, the signature information will be automatically carried into the TensorFlow Lite flatbuffer file since the TensorFlow 2.5 version if you are using the `from_saved_model` path.", "> In such cases, you can read the signature information from the TensorFlow Lite model file, stored in the flatbuffer schema:\r\n> FYI, the signature information will be automatically carried into the TensorFlow Lite flatbuffer file since the TensorFlow 2.5 version if you are using the from_saved_model path.\r\n\r\nThanks for the information!\r\nWe're currently based on tflite schema from TF 2.3.0, so `SignatureDef` doesn't exist.\r\nBut will upgrade to latest version soon... found a good reason for this :)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51858\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51858\">No</a>\n"]}, {"number": 51856, "title": "Error when reading .wav files using TPU", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n\r\n**Describe the current behavior**\r\n\r\nI am trying to read .wav files in my input pipeline and train on TPU. I am using tfio.audio.AudioIOTensor but it is not working with TPU.\r\n\r\nHere is a colab that shows the error:\r\nhttps://colab.research.google.com/drive/1gSP3bhmDkMFwHwxWWc3B3hdR9SNA0hM9#scrollTo=FItPzgJ1xJzZ\r\n\r\nThe error is the following:\r\n```\r\nNotFoundError: Op type not registered 'IO>AudioReadableInit' in binary running on n-6a025964-w-0. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed. [Op:BatchDatasetV2]\r\n```\r\n\r\nIs there another way to decode .wav files when using TPU?\r\n", "comments": ["@invoxiaglo \r\nPlease refer to [this link](https://github.com/tensorflow/io/issues/716#issuecomment-879999571) and let us know.\r\nYou can use tf.audio.decode_wav,it is part of tensorflow and will eliminate tfio, avoiding the error faced.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51856\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51856\">No</a>\n"]}, {"number": 51855, "title": "Add CITATION.cff for GitHub software citation support", "body": "Hi,\r\n\r\nGitHub recently has added support for citing software (See official post and documentation [here](https://github.blog/2021-08-19-enhanced-support-citations-github/) and [here](https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/creating-a-repository-on-github/about-citation-files)). For repositories that have a CITATION.cff file in their main branch, GitHub will automatically parse a citation for users.\r\n\r\nI happen to notice that the project website has a page clarifying how to cite TensorFlow. A BibTex is also carefully provided. What I did is simply to turn that BibTex into a valid CITATION.cff.\r\n\r\nIf interested you could also specify version releases in the CITATION.cff. For more information about it, see [here](https://citation-file-format.github.io/) and [here](https://github.com/citation-file-format/citation-file-format).\r\n\r\nThanks for attending to my PR.", "comments": ["I just found that the base branch is restricted after I opened the PR. In this case the CITATION.cff has to be put in the base branch to be automatically parsed by GitHub so far. If any change is desired, just let me know. Thanks."]}, {"number": 51854, "title": "Bugfix/label image fixes", "body": "This PR fixes following issues with label_image TFLite example: \r\n\r\n1. Behavior of _loop count_ (-c) and _warm_up count_ (-w) command line switch:\r\nWithout this patch the warmup_count depends on loop_counts. If the loop_count is less than 2, no warmup loop is performed. E.g. this call will make no warm up loops, however explicitly requested:\r\n`$ ./label_image -c1 -w2`\r\nThis call will make 2 Interptetter::Invoke() loops and 2 warm up calls:\r\n`$ ./label_image -c2 -w2`\r\nWith this patch, both commands will do 1 resp. 2 Intepretter::Invoke loops and 2 warm up loops, as intuitively expected. \r\n\r\n2. Help message printing for label_image:\r\nThere are two paths from where the help message is printed, with different content:\r\n`$ ./label_image -h` will print the help message defined in display_usage(). Moreover it does not even recognize the -h option and getopt print Unrecognized switch error (-h is missing in getopt optstring).\r\n`$ ./label_image --help` prints the help message provided by delegate registrar. \r\nSwitches in both help messages are valid, so none of the help messages provides a complete usage information. \r\nThis patch fix the issue and unites the messages. ", "comments": ["@multiverse-tf I have reviewed the change and it looks good. Could you review 1 of the 2 changes in this commit again? - https://github.com/tensorflow/tensorflow/pull/51854/commits/d93205254d9fedab091fddc97ec9731ebac5ab31 (I'm not sure if this is required)", "\r\n\r\n> @multiverse-tf I have reviewed the change and it looks good. Could you review 1 of the 2 changes in this commit again? - [d932052](https://github.com/tensorflow/tensorflow/commit/d93205254d9fedab091fddc97ec9731ebac5ab31) (I'm not sure if this is required)\r\n\r\n@MeghnaNatraj which change you mean? The [d932052](https://github.com/tensorflow/tensorflow/commit/d93205254d9fedab091fddc97ec9731ebac5ab31) contains multiple changes (there are changes in 5 files). Maybe I can clarify them. \r\n", "@multiverse-tf  Can you please review this PR ? Thanks!", "@multiverse-tf Can you please review this PR ? Thanks!", "Sorry for the late review, and thanks for the contribution! Overall, it looks good to me except one small comment."]}, {"number": 51852, "title": "UserangeAnalysis clean up and fixes.", "body": "Improvements to the Userange Analysis:\r\n- Removed unnecessary copies of data structures.\r\n- Comment clean up.\r\n- Added a `UseInterval` struct.\r\n- Added UsePositions, that stores an Id for each use of a `Value`.\r\n- Added new helper Methods for upcoming passes.\r\n- Minor bug fixes.", "comments": []}, {"number": 51851, "title": "[TFLite] Force the MLIR quantizer to assign the same scale for the quantized operand and result of the FILL op", "body": "Hi,\r\n\r\nThis PR forces the MLIR quantizer to assign the same scale for the quantized operand and result of the FILL operator in a way [similar](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/operator_property.cc#L200) to the old TOCO quantizer as it's [required](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/fill.cc#L95) by the operator.\r\n\r\nThibaut", "comments": []}, {"number": 51850, "title": "tf-nightly dev version installed with mismatch keras-nightly version, which cause importing error.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution : Ubuntu 18.04.5 LTS (GNU/Linux 5.4.0-1045-aws x86_64)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: tf-nightly-\r\n- Python version: 3.6\r\n- Installed using virtualenv? no pip? yes conda? no \r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nRecently I install by `pip3.6 install tf-nightly`, then run:\r\n```\r\nfrom tensorflow.keras import layers\r\n```\r\nwill get error:\r\n```\r\nAttributeError: module 'tensorflow.compat.v2.__internal__.tracking' has no attribute 'DelegatingTrackableMixin'\r\n```\r\n\r\nI check the installed version, `keras-nightly-2.7.0.dev2021090607` and `tf-nightly-2.7.0.dev20210806` was installed. The devXXX part is mismatched.\r\n\r\nWhen I reinstall keras-nightly with `keras-nightly==2.7.0.dev2021080600`, it seems address this issue.\r\n\r\nSo can we ensure when run `pip install tf-nightly`, it will always install the matched version of `keras-nightly` ?\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["seems my fault, looks like tf-nightly no longer support pip 3.6:\r\nhttps://pypi.org/project/tf-nightly/2.7.0.dev20210905/#files", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51850\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51850\">No</a>\n"]}, {"number": 51849, "title": "Cannot create interpreter error occurred when test 'On-Device Training in TensorFlow Lite'", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.2 LTS\r\n- TensorFlow installation (pip package or built from source): pip package (pip install tf-nightly)\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): \r\n![image](https://user-images.githubusercontent.com/37358784/132177843-72c65706-a28e-43f2-9ec4-b6285ca12b5f.png)\r\n\r\n### 2. Code\r\n\r\n#### - model & tflite converter\r\n```\r\nimport tensorflow as tf\r\n\r\nIMG_SIZE = 28\r\n\r\nclass Model(tf.Module):\r\n\r\n  def __init__(self):\r\n    self.model = tf.keras.Sequential([\r\n        tf.keras.layers.Flatten(input_shape=(IMG_SIZE, IMG_SIZE)),\r\n        tf.keras.layers.Dense(128, activation='relu'),\r\n        tf.keras.layers.Dense(10, activation='softmax')\r\n    ])\r\n    self.model.compile(\r\n        optimizer='sgd',\r\n        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\r\n        metrics=['accuracy'])\r\n    self._LOSS_FN = tf.keras.losses.CategoricalCrossentropy()\r\n    self._OPTIM = tf.optimizers.SGD()\r\n\r\n  @tf.function(input_signature=[\r\n      tf.TensorSpec([None, IMG_SIZE, IMG_SIZE], tf.float32),\r\n      tf.TensorSpec([None, 10], tf.float32),\r\n  ])\r\n  def train(self, x, y):\r\n    with tf.GradientTape() as tape:\r\n      prediction = self.model(x)\r\n      loss = self._LOSS_FN(prediction, y)\r\n    gradients = tape.gradient(loss, self.model.trainable_variables)\r\n    self._OPTIM.apply_gradients(\r\n        zip(gradients, self.model.trainable_variables))\r\n    result = {\"loss\": loss}\r\n    for grad in gradients:\r\n      result[grad.name] = grad\r\n    return result\r\n\r\n  @tf.function(input_signature=[tf.TensorSpec([None, IMG_SIZE, IMG_SIZE], tf.float32)])\r\n  def predict(self, x):\r\n    return {\r\n        \"output\": self.model(x)\r\n    }\r\n\r\n  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\r\n  def save(self, checkpoint_path):\r\n    tensor_names = [weight.name for weight in self.model.weights]\r\n    tensors_to_save = [weight.read_value() for weight in self.model.weights]\r\n    tf.raw_ops.Save(\r\n        filename=checkpoint_path, tensor_names=tensor_names,\r\n        data=tensors_to_save, name='save')\r\n    return {\r\n        \"checkpoint_path\": checkpoint_path\r\n    }\r\n\r\n  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\r\n  def restore(self, checkpoint_path):\r\n    restored_tensors = {}\r\n    for var in self.model.weights:\r\n      restored = tf.raw_ops.Restore(\r\n          file_pattern=checkpoint_path, tensor_name=var.name, dt=var.dtype,\r\n          name='restore')\r\n      var.assign(restored)\r\n      restored_tensors[var.name] = restored\r\n    return restored_tensors\r\n\r\nSAVED_MODEL_DIR = \"saved_model\"\r\nm= Model()\r\ntf.saved_model.save(\r\n    m,\r\n    SAVED_MODEL_DIR,\r\n    signatures={\r\n        'train':\r\n            m.train.get_concrete_function(),\r\n        'infer':\r\n            m.predict.get_concrete_function(),\r\n        'save':\r\n            m.save.get_concrete_function(),\r\n        'restore':\r\n            m.restore.get_concrete_function(),\r\n    })\r\n\r\n# Convert the model\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_DIR)\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\r\n    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.\r\n]\r\nconverter.experimental_enable_resource_variables = True\r\ntflite_model = converter.convert()\r\nwith open('model.tflite','wb') as f:\r\n    f.write(tflite_model)\r\n# Press the green button in the gutter to run the script.\r\nif __name__ == '__main__':\r\n    print(\"main\")\r\n```\r\n#### - android root gradle\r\n\r\n```\r\n// Top-level build file where you can add configuration options common to all sub-projects/modules.\r\nbuildscript {\r\n    ext.kotlin_version = \"1.4.32\"\r\n    repositories {\r\n        mavenCentral()\r\n        maven {  // Only for snapshot artifacts\r\n            name 'ossrh-snapshot'\r\n            url 'https://oss.sonatype.org/content/repositories/snapshots'\r\n        }\r\n        google()\r\n        jcenter()\r\n    }\r\n    dependencies {\r\n        classpath \"com.android.tools.build:gradle:7.0.2\"\r\n        classpath \"org.jetbrains.kotlin:kotlin-gradle-plugin:$kotlin_version\"\r\n\r\n        // NOTE: Do not place your application dependencies here; they belong\r\n        // in the individual module build.gradle files\r\n    }\r\n}\r\n\r\nallprojects {\r\n    repositories {\r\n        google()\r\n        jcenter()\r\n    }\r\n}\r\n\r\ntask clean(type: Delete) {\r\n    delete rootProject.buildDir\r\n}\r\n```\r\n#### - android app gradle\r\n\r\n```\r\nplugins {\r\n    id 'com.android.application'\r\n    id 'kotlin-android'\r\n}\r\n\r\nandroid {\r\n    compileSdkVersion 30\r\n    buildToolsVersion \"30.0.3\"\r\n\r\n    defaultConfig {\r\n        applicationId \"com.tvstorm.tflitetest\"\r\n        minSdkVersion 28\r\n        targetSdkVersion 30\r\n        versionCode 1\r\n        versionName \"1.0\"\r\n\r\n        testInstrumentationRunner \"androidx.test.runner.AndroidJUnitRunner\"\r\n    }\r\n\r\n    buildTypes {\r\n        release {\r\n            minifyEnabled false\r\n            proguardFiles getDefaultProguardFile('proguard-android-optimize.txt'), 'proguard-rules.pro'\r\n        }\r\n    }\r\n    compileOptions {\r\n        sourceCompatibility JavaVersion.VERSION_1_8\r\n        targetCompatibility JavaVersion.VERSION_1_8\r\n    }\r\n    kotlinOptions {\r\n        jvmTarget = '1.8'\r\n    }\r\n    buildFeatures {\r\n        mlModelBinding true\r\n    }\r\n}\r\n\r\ndependencies {\r\n    implementation('commons-io:commons-io:2.6')\r\n    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly-SNAPSHOT'\r\n    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly-SNAPSHOT'\r\n    implementation \"org.jetbrains.kotlin:kotlin-stdlib:$kotlin_version\"\r\n    implementation 'androidx.core:core-ktx:1.6.0'\r\n    implementation 'androidx.appcompat:appcompat:1.3.1'\r\n    implementation 'com.google.android.material:material:1.3.0'\r\n    implementation 'androidx.constraintlayout:constraintlayout:2.1.0'\r\n    testImplementation 'junit:junit:4.+'\r\n    androidTestImplementation 'androidx.test.ext:junit:1.1.2'\r\n    androidTestImplementation 'androidx.test.espresso:espresso-core:3.3.0'\r\n}\r\n```\r\n#### - android activity code\r\n\r\n```\r\nclass MainActivity : AppCompatActivity() {\r\n    override fun onCreate(savedInstanceState: Bundle?) {\r\n        super.onCreate(savedInstanceState)\r\n        setContentView(R.layout.activity_main)\r\n        runTFLite()\r\n    }\r\n\r\n    val NUM_EPOCHS = 100\r\n    val NUM_TRAININGS = 60000\r\n    val trainImages = Array(NUM_TRAININGS) { Array(28) { FloatArray(28) } }\r\n    val trainLabels = Array(NUM_TRAININGS) { FloatArray(10) }\r\n\r\n    fun runTFLite() {\r\n        val interpreter = Interpreter(convertInputStreamToFile(resources.openRawResource(R.raw.model)))\r\n        for (i in 0 until NUM_EPOCHS) {\r\n            val inputs: MutableMap<String, Any> = HashMap()\r\n            inputs[\"x\"] = trainImages\r\n            inputs[\"y\"] = trainLabels\r\n            val outputs: MutableMap<String, Any> = HashMap()\r\n            val loss: FloatBuffer = FloatBuffer.allocate(1)\r\n            outputs[\"loss\"] = loss\r\n            interpreter.runSignature(inputs, outputs, \"train\")\r\n        }\r\n    }\r\n\r\n    fun convertInputStreamToFile(inputStream: InputStream): File {\r\n        val tempFile = File.createTempFile(java.lang.String.valueOf(inputStream.hashCode()), \".tmp\")\r\n        tempFile.deleteOnExit()\r\n        copyInputStreamToFile(inputStream, tempFile) // commons-io:commons-io\r\n        return tempFile\r\n    }\r\n}\r\n```\r\n\r\n### 3. Failure after conversion\r\n\r\nWe have referenced the article in the link below.\r\nhttps://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb\r\n\r\nA few warnings occurred in the process of creating the tflite model, but they were made. When loading this model in android tflite, the following error log occurs.\r\n\r\n### 4. error logs\r\n\r\n#### - Converter side\r\n\r\n```\r\n2021-09-06 16:34:59.531193: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.1/lib64\r\n2021-09-06 16:34:59.531227: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2021-09-06 16:34:59.532327: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-09-06 16:35:01.916373: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nWARNING:absl:Importing a function (__inference_internal_grad_fn_1051) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_internal_grad_fn_1079) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\n2021-09-06 16:35:03.747781: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.\r\n2021-09-06 16:35:03.747821: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.\r\n2021-09-06 16:35:03.747826: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:372] Ignored change_concat_input_ranges.\r\n2021-09-06 16:35:03.749030: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: saved_model\r\n2021-09-06 16:35:03.752172: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\r\n2021-09-06 16:35:03.752276: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: saved_model\r\n2021-09-06 16:35:03.769526: I tensorflow/cc/saved_model/loader.cc:210] Restoring SavedModel bundle.\r\n2021-09-06 16:35:03.815918: I tensorflow/cc/saved_model/loader.cc:194] Running initialization op on SavedModel bundle at path: saved_model\r\n2021-09-06 16:35:03.826002: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 76975 microseconds.\r\n2021-09-06 16:35:03.861382: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n2021-09-06 16:35:03.957610: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1890] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\r\nFlex ops: FlexBroadcastGradientArgs, FlexReluGrad, FlexRestore, FlexSave\r\nDetails:\r\n\ttf.BroadcastGradientArgs(tensor<2xi32>, tensor<2xi32>) -> (tensor<?xi32>, tensor<?xi32>) : {device = \"\"}\r\n\ttf.ReluGrad(tensor<?x128xf32>, tensor<?x128xf32>) -> (tensor<?x128xf32>) : {device = \"\"}\r\n\ttf.Restore(tensor<!tf_type.string>, tensor<!tf_type.string>) -> (tensor<*xf32>) : {device = \"\", preferred_shard = -1 : i64}\r\n\ttf.Save(tensor<!tf_type.string>, tensor<4x!tf_type.string>, tensor<784x128xf32>, tensor<128xf32>, tensor<128x10xf32>, tensor<10xf32>) -> () : {device = \"\"}\r\nSee instructions: https://www.tensorflow.org/lite/guide/ops_select\r\nWARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\r\nmain\r\n```\r\n\r\n#### - mobile side error log\r\n\r\n```\r\n\r\n2021-09-06 16:23:34.298 18282-18282/com.tvstorm.tflitetest D/AndroidRuntime: Shutting down VM\r\n2021-09-06 16:23:34.298 18282-18282/com.tvstorm.tflitetest E/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: com.tvstorm.tflitetest, PID: 18282\r\n    java.lang.RuntimeException: Unable to start activity ComponentInfo{com.tvstorm.tflitetest/com.tvstorm.tflitetest.MainActivity}: java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Op builtin_code out of range: 142. Are you using old TFLite binary with newer model?\r\n    Registration failed.\r\n    \r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3792)\r\n        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3968)\r\n        at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:85)\r\n        at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:135)\r\n        at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:95)\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2307)\r\n        at android.os.Handler.dispatchMessage(Handler.java:106)\r\n        at android.os.Looper.loop(Looper.java:246)\r\n        at android.app.ActivityThread.main(ActivityThread.java:8512)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:596)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1130)\r\n     Caused by: java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Op builtin_code out of range: 142. Are you using old TFLite binary with newer model?\r\n    Registration failed.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:75)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:51)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:227)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:202)\r\n        at com.tvstorm.tflitetest.MainActivity.runTFLite(MainActivity.kt:34)\r\n        at com.tvstorm.tflitetest.MainActivity.onCreate(MainActivity.kt:20)\r\n        at android.app.Activity.performCreate(Activity.java:8198)\r\n        at android.app.Activity.performCreate(Activity.java:8182)\r\n        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1309)\r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3765)\r\n        \t... 11 more\r\n```\r\n", "comments": ["Could you remove `jcenter()` from the gradle file and depend on the only ossrh-snapshot only?\r\n\r\n ```\r\nbuildscript {\r\n    repositories {\r\n        google()\r\n        mavenCentral()\r\n\r\n    }\r\n    dependencies {\r\n        classpath 'com.android.tools.build:gradle:4.1.3'\r\n        classpath 'de.undercouch:gradle-download-task:4.1.1'\r\n    }\r\n}\r\n\r\nallprojects {\r\n    repositories {\r\n        google()\r\n        mavenCentral()\r\n        maven {\r\n            name 'ossrh-snapshot'\r\n            url 'http://oss.sonatype.org/content/repositories/snapshots'\r\n        }\r\n    }\r\n}\r\n```", "I could reproduce your issue. The jcenter dependency in the build.gradle in your application setting makes the android application depend on the March 31st version of the TensorFlow Lite library.\r\n\r\nYou can fix the problem by the removal of the `jcenter()` as the above build.gradle example.", "###Cannot create interpreter: Op builtin_code out of range: 142. Are you using old TFLite binary with newer model?\r\n\r\nStill facing this issue even without jcenter()\r\n\r\nPlease Help\r\n\r\n`// Top-level build file where you can add configuration options common to all sub-projects/modules.\r\nbuildscript {\r\n    repositories {\r\n        google()\r\n        mavenCentral()\r\n        maven {  // Only for snapshot artifacts\r\n            name 'ossrh-snapshot'\r\n            url 'https://oss.sonatype.org/content/repositories/snapshots'\r\n        }\r\n    }\r\n    dependencies {\r\n        classpath \"com.android.tools.build:gradle:7.0.0\"\r\n    }\r\n}\r\n\r\ntask clean(type: Delete) {\r\n    delete rootProject.buildDir\r\n}`\r\n\r\n\r\n> I could reproduce your issue. The jcenter dependency in the build.gradle in your application setting makes the android application depend on the March 31st version of the TensorFlow Lite library.\r\n> \r\n> You can fix the problem by the removal of the `jcenter()` as the above build.gradle example.\r\n\r\n"]}, {"number": 51846, "title": "backport pr 45534 to TF 2.4", "body": "backport #45534 to TF 2.4, which will solve the issue for CRF function in TensorFlow addons tensorflow/addons#2250 .\r\n\r\nThe credits belongs to @WindQAQ ", "comments": []}, {"number": 51845, "title": "Convert Functional API to Model Subclassing in TensorFlow Tutorial?", "body": "Could anyone please teach me how to convert the Functional API to Model subclassing in this [TensorFlow Official Tutorial](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers)? I suppose an elegant chuck of code should be what combines the Normalization layer with the remaining layers. Thank you very much!", "comments": ["@TianruiZhang Could you please have a look at the [link1](https://www.tensorflow.org/guide/keras/functional) , [link2](https://www.tensorflow.org/guide/keras/custom_layers_and_models) and similar [issue1](https://stackoverflow.com/questions/65851897/subclassing-of-model-class-and-model-functional-api-give-different-results-in-te).Please let us know if it helps?Thanks!", "Thank you for your reply. I went through the TensorFlow [tutorial](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers), trying to reproduced the process to fit my purpose. I have only 1 predictor (`age`) and 1 target variable (`se`), both are continues variables.\r\n\r\nHere's how I refactored the code:\r\n\r\n```\r\nfrom tensorflow.data import Dataset\r\nfrom tensorflow.keras import Input\r\nfrom tensorflow.keras.backend import clear_session\r\nfrom tensorflow.keras.layers import Normalization\r\n\r\n\r\ndef df_to_dataset(dataframe, batch_size, shuffle=True):\r\n  dataframe = dataframe.copy()\r\n  labels = dataframe.pop(\"se\")\r\n  ds = Dataset.from_tensor_slices((dict(dataframe), labels))\r\n  if shuffle:\r\n    ds = ds.shuffle(buffer_size=len(dataframe))\r\n  ds = ds.batch(batch_size)\r\n  ds = ds.prefetch(batch_size)\r\n  return ds\r\n\r\ndef get_normalization_layer(name, dataset):\r\n  normalizer = Normalization(axis=None)\r\n  feature_ds = dataset.map(lambda x, y: x[name])\r\n  normalizer.adapt(feature_ds)\r\n  return normalizer\r\n\r\n\r\ntest_ds = df_to_dataset(test, batch_size=5, shuffle=False)\r\n\r\n[(train_features, label_batch)] = test_ds.take(1)\r\nprint(f\"Every Feature: {list(train_features.keys())}\")\r\nprint(f\"A batch of ages: {train_features['age']}\")\r\nprint(f\"A batch of targets: {label_batch}\")\r\n\r\nbatch_size = 128\r\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\r\nval_ds = df_to_dataset(val, batch_size=batch_size, shuffle=False)\r\ntest_ds = df_to_dataset(test, batch_size=batch_size, shuffle=False)\r\n\r\nall_inputs = []\r\nencoded_features = []\r\n\r\n# Numeric features\r\nclear_session()\r\nfor header in [\"age\"]:\r\n  numeric_col = Input(shape=(1, ), name=header)\r\n  normalization_layer = get_normalization_layer(header, train_ds)\r\n  encoded_numeric_col = normalization_layer(numeric_col)\r\n  all_inputs.append(numeric_col)\r\n  encoded_features.append(encoded_numeric_col)\r\n```\r\n\r\nBelow the Functional API part, which worked as expected, like in the tutorial:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import math\r\nfrom tensorflow.keras import Model, Input\r\nfrom tensorflow.keras.backend import clear_session\r\nfrom tensorflow.keras.losses import Loss\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom tensorflow.keras.layers import Dense, Normalization, Concatenate, Dropout\r\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\r\n\r\n\r\nclass QuantileLoss(Loss):\r\n\r\n  def __init__(self, quantiles):\r\n    super().__init__()\r\n    self.quantiles = tf.convert_to_tensor(quantiles)\r\n  \r\n  def call(self, y_true, y_pred):\r\n    y_true = tf.convert_to_tensor(y_true)\r\n    y_pred = tf.convert_to_tensor(y_pred)\r\n    errors = math.subtract(y_true, y_pred)\r\n    loss = math.reduce_mean(\r\n        math.maximum(\r\n            math.multiply(self.quantiles, errors),\r\n            math.multiply(\r\n                math.subtract(\r\n                    self.quantiles, 1\r\n                ),\r\n                errors\r\n            )\r\n        ),\r\n        axis=-1\r\n    )    \r\n    return loss\r\n\r\nclear_session()\r\n\r\nearlystopping = EarlyStopping(patience=10)\r\nlr_schedule = ReduceLROnPlateau(\r\n    patience=5, \r\n    monitor=\"val_loss\",\r\n    verbose=1\r\n)\r\ncallbacks = [lr_schedule, earlystopping]\r\nquantiles = [0.021, 0.157, 0.5, 0.841, 0.977, 0.998]\r\nall_features = Concatenate()(encoded_features)\r\nprint(all_features.shape)\r\nx = Dense(256, activation=\"selu\")(all_features)\r\nx = Dropout(0.3)(x)\r\nx = Dense(64, activation=\"selu\")(x)\r\noutput = Dense(len(quantiles))(x)\r\nmodel = Model(all_inputs, output)\r\nquantile_loss = QuantileLoss(quantiles)\r\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss=quantile_loss)\r\nhistory = model.fit(train_ds, validation_data=val_ds, epochs=100, callbacks=callbacks)\r\n```\r\n\r\nI was trying to refactor the Functional implementation to make the code look more elegant.\r\n\r\n```\r\nclass QuantileRegressor(Model):\r\n\r\n  def __init__(self, quantiles, hidden_units):\r\n    super().__init__()\r\n    self.quantiles = quantiles\r\n    self.concatenate = Concatenate()\r\n    self.normalizer = Normalization(axis=None)\r\n    self.hidden_dense = Dense(hidden_units, activation=\"selu\")\r\n    self.dropout = Dropout(0.3)\r\n    self.output_dense = Dense(len(quantiles))\r\n  \r\n  def call(self, inputs):\r\n    self.normalizer.adapt(inputs[\"age\"])\r\n    # The line above gave me an error! \r\n    # Is it a good idea to place encoded_features, all_inputs here?\r\n    # inputs here seemed to be a dictionary. \r\n    return None\r\n\r\nearlystopping = EarlyStopping(patience=10)\r\nlr_schedule = ReduceLROnPlateau(\r\n    patience=5, \r\n    monitor=\"val_loss\",\r\n    verbose=1\r\n)\r\ncallbacks = [lr_schedule, earlystopping]\r\nquantiles = [0.021, 0.157, 0.5, 0.841, 0.977, 0.998]\r\nhidden_units = 256\r\nclear_session()\r\nmodel = QuantileRegressor(quantiles, hidden_units)\r\nquantile_loss = QuantileLoss(quantiles)\r\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss=quantile_loss)\r\nhistory = model.fit(train_ds, validation_data=val_ds, epochs=10, callbacks=callbacks)\r\n```\r\n\r\n`self.normalizer.adapt(inputs[\"age\"])` in the `call` method resulted in \r\n```\r\nRuntimeError: in user code:\r\n\r\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:853 train_function  *\r\n        return step_function(self, iterator)\r\n    <ipython-input-24-3f4f9f8eec72>:18 call  *\r\n        self.normalizer.adapt(inputs[\"age\"])\r\n    /usr/local/lib/python3.7/dist-packages/keras/engine/base_preprocessing_layer.py:230 adapt  **\r\n        _disallow_inside_tf_function('adapt')\r\n    /usr/local/lib/python3.7/dist-packages/keras/engine/base_preprocessing_layer.py:591 _disallow_inside_tf_function\r\n        raise RuntimeError(error_msg)\r\n\r\n    RuntimeError: Detected a call to `PreprocessingLayer.adapt` inside a `tf.function`. `PreprocessingLayer.adapt is a high-level endpoint that manages its own `tf.function`. Please move the call to `PreprocessingLayer.adapt` outside of all enclosing `tf.function`s. Note that you can call a `PreprocessingLayer` directly on `Tensor`s inside a `tf.function` like: `layer(x)`, or update its state like: `layer.update_state(x)`.\r\n```\r\nThat's why I am asking the question. What is the standard or preferred way using Model Subclassing, when adapting a `Normalization` layer?", "@TianruiZhang Thank you for the update! Could you please  post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThanks!", "@sushreebarsa Thanks, I\u2019ve already done so.", "@TianruiZhang Thank you for the quick update! Could you please close this ticket if you have posted this issue on Keras repo? Thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51845\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51845\">No</a>\n"]}, {"number": 51844, "title": "Add mlir-hlo unfuse-batch-norm-training pattern", "body": "Add mlir-hlo unfuse-batch-norm-training pattern.  \r\nShould I add dynamic shape support?", "comments": ["If you can add the dynamic shape support that'd be great indeed.\r\n\r\nThere is a TODO in `include/mlir-hlo/Dialect/mhlo/rewriters.h` (around line 100) which you're resolving here I believe, can you remove it? Pinging @stellaraccident since they're mentioned in the TODO.", "> If you can add the dynamic shape support that'd be great indeed.\r\n> \r\n> There is a TODO in `include/mlir-hlo/Dialect/mhlo/rewriters.h` (around line 100) which you're resolving here I believe, can you remove it? Pinging @stellaraccident since they're mentioned in the TODO.\r\n\r\n\ud83d\udc4c, and give me some more time to add more testcase and dynamic shape support."]}, {"number": 51843, "title": "tf.linalg.diag issue", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.6.0\r\n- Python version: 3.8.11\r\n\r\n**Describe the current behavior**\r\nWhen providing the tf.linalg.diag function an input > length of 32, the function returns this error:\r\n\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [32,32] vs. shape[1] = [1,1] [Op:ConcatV2] name: concat\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe function should return a tensor of shape [n,n] no mater how large the tensor is.\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n#input\r\ninput_data = np.ones(33)\r\n\r\n#model\r\ninput = tf.keras.layers.Input([],dtype=tf.float32)\r\nout = tf.linalg.diag(input)\r\nmodel = tf.keras.Model(inputs=input, outputs=out)\r\n\r\n#inference\r\npred = model.predict(input_data)\r\n```\r\n", "comments": ["Keras is expecting to receive inputs in batch\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n#input\r\ninput_data = np.ones((1, 33))\r\n\r\n#model\r\ninput = tf.keras.layers.Input([],dtype=tf.float32)\r\nout = tf.linalg.diag(input)\r\nmodel = tf.keras.Model(inputs=input, outputs=out)\r\n\r\n#inference\r\npred = model.predict(input_data)\r\n\r\nprint(pred)\r\nprint(pred.shape)\r\n```\r\nIt works\r\n\r\nThe `predict` method of model has an attribute `batch_size`, the doc says\r\n\r\n```\r\nbatch_size: Integer or `None`.\r\n    Number of samples per batch.\r\n    If unspecified, `batch_size` will default to 32.\r\n    Do not specify the `batch_size` if your data is in the\r\n    form of dataset, generators, or `keras.utils.Sequence` instances\r\n    (since they generate batches).\r\n```\r\nIn your case, the code is seperated into two batches, I think that produces the problem", "thank you, i realized this soon after! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51843\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51843\">No</a>\n"]}, {"number": 51842, "title": "The ModifyGraphWithDelegate function fails on some devices.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac OS Big sur (11.3.1)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  Galaxy s21 ultra\r\n- TensorFlow installed from (source or binary): source  \r\n- TensorFlow version: r2.6\r\n- Python version: 3.8.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 3.7.2\r\n\r\nI currently use libtensorflowlite_gpu_delegate.so which compiled from source(r2.6).\r\nModifyGraphWithDelegate function works fine on Galaxy Note 10.\r\nBut, it's not working in Galaxy s21 ultra.\r\n(returns kTfLiteApplicationError.)\r\n\r\nDoes it work according to the device? \r\n(Can't use gpu delegate for Galaxy s21 ultra?)\r\nOr do I need to set additional options?\r\n\r\n(Currently I am setting it as the default as below.)\r\n```\r\n  TfLiteGpuDelegateOptionsV2 option = TfLiteGpuDelegateOptionsV2Default();\r\n  delegate_ = TfLiteGpuDelegateV2Create(/*default options=*/&option);\r\n  TfLiteStatus result = interpreter_->ModifyGraphWithDelegate(delegate_);\r\n\r\n  if (result != kTfLiteOk) {\r\n    return kAudioNodeError;\r\n  }\r\n```\r\n\r\nI built the libraries as follows.\r\n\r\nlibtensorflowlite.so(3.7MB): bazel build -c opt --config android_x86 android_x86_64 android_arm android_arm64 --define tflite_with_xnnpack=true\r\nlibtensorflowlite_gpu_delegate.so(98.5MB): bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so \r\n\r\nThanks!!\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hi @hotstone1993 ,Could you look into  [link ](https://www.tensorflow.org/lite/api_docs/cc/class/tflite/interpreter?hl=zh-cn#modifygraphwithdelegate)of this error.", "thanks for answering.\r\nBut, I already read the page.\r\n\r\n\"Delegation failed to be applied due to the incompatibility with the TfLite runtime\" \r\nDoes this sentence mean that some devices may not work because of compatibility?\r\n\r\nIs there no way??", "Each device could have different OpenCL or OpenGL version and capability so it is possible.\r\n\r\nBut could you share the logcat dump of  Galaxy s21 Ultra? I'm curious what kind of issue the device has.", "Sorry now that I check, the model is the problem.\r\n\r\nmy log is \r\n\"Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors (tensor#42 is a dynamic-sized tensor).\"\r\n\r\nthanks for your help\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51842\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51842\">No</a>\n"]}, {"number": 51841, "title": "OS Error while installing tensorflow on virtualenv", "body": "**System information**\r\n- OS Platform and Distribution : Windows 10\r\n- TensorFlow version: 2.6.0\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: virtualenv with pip install --upgrade tensorflow\r\n- GPU model and memory: none (Using CPU)\r\n\r\n\r\n**Describe the problem** : \r\nI tried installing tensorflow on my local machine in a virtual environment I got this error  : \r\n```\r\nInstalling collected packages: tensorflow\r\nERROR: Could not install packages due to an OSError: [Errno 2] \r\nNo such file or directory: 'E:\\\\Pathik\\\\KJ\\\\internships\\\\Facial Expression recognition\\\\Github Repo\\\\Emotion-recognition\\\\venv\\\\Lib\\\\sitepackages\\\\tensorflow\\\\include\\\\external\\\\cudnn_frontend_archive\\\\_virtual_includes\\\\cudnn_frontend\\\\third_party\\\\cudnn_frontend\\\\include\\\\cudnn_frontend_EngineConfigGenerator.h'\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nvirtualenv venv\r\n.\\venv\\Scripts\\activate \r\npip install --upgrade tensorflow\r\n```\r\n\r\n**Any other info / logs**\r\nAll the dependencies were successfully installed except the tensorflow.\r\n", "comments": ["@pathikg Could you please have a look at the [link](https://www.tensorflow.org/install/source_windows) and similar [issue1](https://github.com/tensorflow/tensorflow/issues/46934),[issue2](https://stackoverflow.com/questions/59302043/error-when-installing-tensorflow-python-3-8) .Please let us know if it helps?", "Yes enabling long_path solved the error , thanks \ud83d\udc4d ", "@pathikg Could you Please close the ticket if it is resolved for you ?Thank you! ", "Yes", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51841\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51841\">No</a>\n"]}, {"number": 51840, "title": "Saving a composite model that includes a custom layer results in error - None has NoneType, but expected one of: bytes, unicode", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): A custom variant of RedHat, outside of my control\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.7.2\r\n- CUDA/cuDNN version: 11.1.0/8.1.1\r\n- GPU model and memory: Nvidia K40 12GB\r\n\r\n**Describe the current behavior**\r\nI'm trying to save a model which is a composite model of composite models.\r\n\r\nThe first model is a sequential model of two sequential models. Both of the two sub-models have custom layers that perform scaling operations like MinMax and cube root. This model saves and loads without any issues.\r\n\r\nThe first model is then loaded in a different script without compiling. There is not issue with this. Let's call this model MODEL_1.\r\n\r\nThe next step may be a bit confusing. There are then two more models added in parallel to each other but sequentially with MODEL_1. Let's call these models MODEL_2a and MODEL_2b. The output of MODEL_1 has the input of MODEL_1 concatenated to it and this serves as the input to MODEL_2a and MODEL_2b. It should be noted that that the \"MODEL_1 input\" goes through a custom scaling layer before being concatenated to the output of MODEL_1. The is scaling layer that was also implemented in MODEL_1 without any issues.\r\n\r\nFinally, there is a single, custom layer that performs a 'simple' weighted sum of the outputs of MODEL_2a and MODEL_2b to produce the model output. This weighting happens via `alpha*OUTPUT_2a + [1-alpha]*OUTPUT_2b`. 'alpha' is a trainable, scalar parameter. I haven't used this before in a model that I have saved, so I'm guessing this is the cause.\r\n\r\nThe model compiles and trains, but fails to save.\r\n\r\nThe custom weighted sum layer is this,\r\n\r\n```\r\nclass WeightedSum(krs.layers.Layer):\r\n    def __init__( self, n_models = 2, name = 'weighted_sum_0' ):\r\n        super( WeightedSum, self ).__init__( name = name)\r\n        self.n_models = n_models\r\n        self.ensemble_weights = []\r\n        self.output_init = tf.Variable(0.,validate_shape=False,trainable=False)\r\n\r\n    def build(self,input_shape):\r\n        for i in range(self.n_models):\r\n            self.ensemble_weights.append( self.add_weight(shape=(1,),\r\n                                    initializer = 'ones',\r\n                                    trainable = True) )\r\n\r\n    def call(self,inputs):\r\n        new_normalizer = tf.convert_to_tensor(0.,dtype = inputs[0].dtype)\r\n        for i in range(self.n_models):\r\n            new_normalizer = new_normalizer + self.ensemble_weights[i]\r\n        new_normalizer = tf.constant(1.,dtype=new_normalizer.dtype)/new_normalizer\r\n        output = self.output_init\r\n\r\n        for i in range(self.n_models):\r\n            output = tf.add(output,tf.multiply(self.ensemble_weights[i],inputs[i]))\r\n        output = tf.multiply( output, new_normalizer )\r\n        return output\r\n```\r\n\r\nThe save command is this, (NOTE: I use `import tensorflow.keras as krs`)\r\n```\r\nkrs.models.save_model(linked_model,\"test_failed_save.mdl\")\r\n```\r\n\r\nThe error that is produced is this.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"multi_fidelity_training_full_link.py\", line 304, in <module>\r\n    main()\r\n  File \"multi_fidelity_training_full_link.py\", line 265, in main\r\n    krs.models.save_model(linked_model,\"test_failed_save.mdl\")\r\n  File \"/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\", line 151, in save_model\r\n    signatures, options, save_traces)\r\n  File \"/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save.py\", line 90, in save\r\n    model, filepath, signatures, options)\r\n  File \"/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\", line 1104, in save_and_return_nodes\r\n    raise_metadata_warning))\r\n  File \"/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\", line 1291, in _build_meta_graph\r\n    raise_metadata_warning)\r\n  File \"/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\", line 1225, in _build_meta_graph_impl\r\n    options.namespace_whitelist)\r\n  File \"/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\", line 713, in _fill_meta_graph_def\r\n    _call_function_with_mapped_captures, resource_map=resource_map)))\r\n  File \"/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/training/tracking/graph_view.py\", line 424, in frozen_saveable_objects\r\n    call_with_mapped_captures)\r\n  File \"/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/training/tracking/graph_view.py\", line 375, in _serialize_gathered_objects\r\n    slot_variables=slot_variables)\r\n  File \"/usr/WS2/mvander/py3venv/py3venv/lib/python3.7/site-packages/tensorflow/python/training/tracking/graph_view.py\", line 355, in _fill_object_graph_proto\r\n    child_proto.local_name = child.name\r\nTypeError: None has type NoneType, but expected one of: bytes, unicode\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\nIt should save the model.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport tensorflow.keras as krs\r\nimport tensorflow as tf\r\n\r\n\r\n\r\n\r\nclass WeightedSum(krs.layers.Layer):\r\n    def __init__( self, n_models = 2, **kwargs):\r\n        super( WeightedSum, self ).__init__( **kwargs)\r\n        self.n_models = n_models\r\n        self.ensemble_weights = []\r\n        self.output_init = tf.Variable(0.,validate_shape=False,trainable=False)\r\n\r\n    def build(self,input_shape):\r\n        for i in range(self.n_models):\r\n            self.ensemble_weights.append( self.add_weight(shape=(1,),\r\n                                    initializer = 'ones',\r\n                                    trainable = True) )\r\n\r\n    def call(self,inputs):\r\n        new_normalizer = tf.convert_to_tensor(0.,dtype = inputs[0].dtype)\r\n        for i in range(self.n_models):\r\n            new_normalizer = new_normalizer + self.ensemble_weights[i]\r\n        new_normalizer = tf.constant(1.,dtype=new_normalizer.dtype)/new_normalizer\r\n        output = tf.cast(self.output_init,dtype=inputs[0].dtype)\r\n\r\n        for i in range(self.n_models):\r\n            output = tf.add(output,tf.multiply(tf.cast(self.ensemble_weights[i],dtype=inputs[i].dtype),inputs[i]))\r\n        output = tf.multiply( output, new_normalizer )\r\n        return output\r\n\r\n\r\ninput_lf = krs.Input((4,))\r\n\r\nx = input_lf\r\nx = krs.layers.Dense(10,activation = 'relu')(x)\r\nx = krs.layers.Dense(10,activation = 'relu')(x)\r\nlf_out = krs.layers.Dense(10,activation = 'relu')(x)\r\n\r\nlf_mod = krs.Model(input_lf,lf_out,name='lf')\r\n\r\ninput_hf_lin = krs.Input((14,))\r\nx = input_hf_lin\r\nx = krs.layers.Dense(10)(x)\r\nx = krs.layers.Dense(10)(x)\r\nhf_lin_out = krs.layers.Dense(10,activation = 'relu')(x)\r\n\r\nhf_lin_mod = krs.Model(input_hf_lin,hf_lin_out,name='hf_linear')\r\n\r\ninput_hf_nonlin = krs.Input((14,))\r\n\r\nx = input_hf_nonlin\r\nx = krs.layers.Dense(10,activation = 'relu')(x)\r\nx = krs.layers.Dense(10,activation = 'relu')(x)\r\nhf_nonlin_out = krs.layers.Dense(10,activation = 'relu')(x)\r\n\r\nhf_nonlin_mod = krs.Model(input_hf_lin,hf_lin_out,name='hf_nonlinear')\r\n\r\ninput_hf = krs.Input((14,))\r\n\r\nx = input_hf\r\nlin = hf_lin_mod(x)\r\nnonlin = hf_nonlin_mod(x)\r\nsummed_out = WeightedSum(n_models=2)([lin,nonlin])\r\n\r\nhf_mod = krs.Model(input_hf,summed_out,name='hf')\r\n\r\ninput_full_mod = krs.Input((4,))\r\nx = input_full_mod\r\n\r\nlow = lf_mod(x)\r\nx = krs.layers.Concatenate()([low,x])\r\nfull_out = hf_mod(x)\r\n\r\nfull_mod = krs.Model(input_full_mod,outputs = {'low_fidelity':low,'high_fidelity':full_out},name='full_model')\r\n\r\nopt = krs.optimizers.Adam()\r\nloss = krs.losses.MSE\r\nfull_mod.compile(optimizer = opt,loss = loss)\r\n\r\nx_train = np.random.uniform(0,10,(20,4))\r\ny_train_low = np.random.uniform(0,10,(20,10))\r\ny_train_high = np.random.uniform(0,10,(20,10))\r\ny = {\"low_fidelity\": y_train_low,\r\n     \"high_fidelity\": y_train_high}\r\n\r\n\r\n\r\nfull_mod.fit(x_train,y,epochs=5)\r\n\r\nkrs.models.save_model(full_mod,\"test_model.mdl\")\r\n```\r\n\r\nEDIT: I goofed during my copy and paste from VIM. I didn't get all of the code orginally.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Before someone says that I should post this to StackOverflow, I have already done this and have received no response in the 2 weeks that it has been posted. Also, similar issues appear to generally be bugs.", "@EnderWiggin14 ,\r\nI was able to execute the given code without facing any errror.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/d9c1dcadfd3315cc4c7948c54635b5f7/un51840.ipynb).Can you please try to execute the code in new virtual environment and let us know if you are facing same issue.Thanks!", "@tilakrayal I goofed during copy-paste and missed the last bit of code. I have now edited the post which includes all of the necessary code. FYI I'm running in 2.5.0 not 2.6.0. I did install 2.6.0 to test like you suggested, but it failed. That is how I realized I failed to do a proper copy-paste. I will be reverting to 2.5.0 because I can't justify moving to 2.6.0 as I don't have control over what CUDA/cuDNN versions are available to me on servers.\r\n\r\nAlso, I did drop the part I missed into the gist that you linked, and it failed. I don't know how to save my changes to that gist provided that I'm actually allowed to do such a thing. I'm not well versed in the use of gist.", "@tilakrayal I believe I have found the issue. The issue is that I do not have the \"name\" parameter set in `self.add_weight`. \r\n\r\nI have no idea if it was intended by the developers to absolutely need a name provided, but the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#add_weight) doesn't make that clear at all. It also doesn't follow the general convention within the Tensorflow API where names are optional (or at least appear to be) and default to `None`.\r\n\r\nYou'll also notice that I don't have a `get_config` implementation in the post, but that is a minor detail and is fixed in my actual implementation.", "@EnderWiggin14 ,\r\n \r\nPlease post this issue on [keras-team/keras](https://github.com/keras-team/keras/issues) repo.\r\nTo know more refer to:\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\n", "@tilakrayal \r\n\r\nI just posted this to the Keras issues page.", "@EnderWiggin14 ,\r\nPlease feel free to move this issue to closed status as it has been tracked in Keras repo.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51840\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51840\">No</a>\n"]}, {"number": 51839, "title": "memory leak in tf.py_function", "body": "tensorflow 2.7 python 3.7\r\n\r\nminimum code\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\n\r\ndef test(xyz_batch,  k):\r\n    indices = np.zeros((1000, 1000, 20), dtype=np.int64)\r\n    dist = np.zeros((1000, 1000, 20), dtype=np.float32)\r\n    return indices, dist\r\n\r\n\r\nwhile True:\r\n    ret = tf.py_function(test, [0,0], [tf.int64, tf.int64])\r\n```", "comments": ["same memory leak issue\r\n```\r\ndef test(indices,  dist):\r\n    # indices = np.zeros((1000, 1000, 20), dtype=np.int64)\r\n    # dist = np.zeros((1000, 1000, 20), dtype=np.int64)\r\n    return indices, dist\r\n\r\n\r\nwhile True:\r\n    indices = np.zeros((1000, 1000, 20), dtype=np.int64)\r\n    dist = np.zeros((1000, 1000, 20), dtype=np.int64)\r\n    ret = tf.py_function(test, [indices,dist], [tf.int64, tf.int64])\r\n```", "with the below as the comparison of no memory leak\r\n```\r\ndef test(indices,  dist):\r\n    indices = np.zeros((1000, 1000, 20), dtype=np.int64)\r\n    dist = np.zeros((1000, 1000, 20), dtype=np.int64)\r\n    return indices, dist\r\n\r\nwhile True:\r\n    indices = np.zeros((1000, 1000, 20), dtype=np.int64)\r\n    dist = np.zeros((1000, 1000, 20), dtype=np.int64)\r\n    test(indices, dist)\r\n```\r\n\r\n```\r\ndef test(xyz_batch,  k):\r\n    indices = np.zeros((1000, 1000, 20), dtype=np.int64)\r\n    dist = np.zeros((1000, 1000, 20), dtype=np.float32)\r\n    # print(indices.type)\r\n    # print(xyz_batch.type)\r\n    return indices, dist\r\n\r\n\r\nwhile True:\r\n    indices, dist = tf.numpy_function(test, [0,0], [tf.int64, tf.int64])\r\n```\r\n", "@Saduf2019 Was able to reproduce the issue on Colab using TF 2.6 and tf-nightly,Please find the [gist](https://colab.research.google.com/gist/sushreebarsa/a739fc42e9dcd921c83591c773565f0a/untitled434.ipynb) here for reference.Thank you!", "@sushreebarsa  @Saduf2019 I would like to work on this issue. Could you guide me with the same?\r\n", "@sushreebarsa @Saduf2019 Please make some updates, even this issue is very hard, thanks!", "@sushreebarsa @Saduf2019 @ymodak Please take this issue seriously because the memory issue is intolerant in any product. I still remember in my university class, my C++ teacher gave the low mark in the final exam because of forgetting to free the memory. I would also say that if I do not treat the memory issue seriously in the company, my boss would fire me. So I hope tensorflow would and should be a successful product, ever and forever. It cannot do so without your diligence. So make it and we believe it.", "Hi,\r\n\r\nThanks for the investigation.\r\n\r\nThis looks like a known issue, https://github.com/tensorflow/tensorflow/issues/35084 which was closed due to inactivity.\r\n\r\nThe error is caused by py_function creating a new entry in tape_cache on every call, where it stores a new reference to the input arguments and the function for tape. The tape_cache entry is consumed (via popping) during gradient calculation.\r\n\r\nThis is the necessary overhead for recording the tape. The problem is we are incurring it when there is no active Tape.\r\nI haven't thought through how best to fix this -- maybe we can detect whether there is an active tape and act accordingly.\r\n\r\nMeanwhile, here are a few workarounds that I can think of:\r\n\r\n- (mentioned in the docstring of _eager_py_func) use the internal function _eager_py_func, and set use_tape_cache=False. This disables gradient support I believe (but I haven't tried).\r\n- consume the tape_cache entry by computing the gradient per loop entry. This is the typical pattern in a training loop.\r\n- (as you have shown in the previous comment) use numpy_function, which plays nicely with tapes. \r\n\r\n", "@rainwoodman Thanks for your professional reply. I've tried rewriting all the numpy code to tf code, but there's still memory leak. I do not know how to find the bugs not, currently no any method to analyze. Is there any function like graph.finalize() in tf 1.x?\r\n\r\n\r\n", "Sorry I do not know much about graph.finalize() or tf 1.x to directly answer your question.\r\n\r\nOn detecting memory leak, the test case for #35084  may serve as an example:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/kernel_tests/memory_cleanup_test.py#L151\r\n\r\nFYI: here is the workaround used by #35084, via _eager_py_func.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/ops/dataset_ops.py#L1082\r\n\r\nThe bug manifests as memory leak when not requesting gradients during eager mode execution of EagerPyFunc (via py_function) and also as memory leak during graph execution (py_function inside tf.function), so I think you will be affected it as long as you use tf.py_function and you do not use gradients.\r\n\r\nWe are working on a tentative fix, there is a possibility the fix makes into the 2.7 release.\r\n\r\nPS: In case you are boarding tensorflow recently, I strongly recommend reading the [intro_to_graphs guide](https://www.tensorflow.org/guide/intro_to_graphs), and other guides on the doc site to get the most out of tensorflow 2, if you haven't done so.", "@rainwoodman Thanks again very much for helping me find the solutions. I will try the memory cleanup test. Here I find a fix relating to the memory leak,\r\nhttps://stackoverflow.com/questions/58915522/tf2-0-memory-leak-from-applying-keras-model-to-symbolic-tensor\r\n\r\nI wonder whether this issue has being fixed? Is it a must to put the loop inside the tf.function?", "Hmm... I am not aware of a \"Keras + random in a loop\" leak or if there is a fix (but I am also new to the TF codebase).\r\n\r\nIf your use case is affected, what about filing a new issue with a reproducer? We can continue from there.", "@rainwoodman That's just the solution! The loop must be placed inside the tf.function, otherwise there will be the memory leak. Hope someone will work on this issue,after all this issue has been on post for over a year.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51839\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51839\">No</a>\n", "Please do not close this issue as it involves two bugs. The first is the tf.py_function and the second is that the loop should be inside the tf.function referenced in https://stackoverflow.com/questions/58915522/tf2-0-memory-leak-from-applying-keras-model-to-symbolic-tensor.So please reopen it.", "@sjtusmartboy:\r\n\r\nThe SO example runs in eager mode, calls tf.random, and doesn't call tf.py_function. \r\n\r\nDid you encounter the leak in a similar context? ", "@sjtusmartboy Could you please update on the above request.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51839\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51839\">No</a>\n"]}, {"number": 51838, "title": "tensorflow support multi-evaluator.", "body": "Issues:\r\n1. **OOM**: The model from distributed training is too large (memory) to be restored to the evaluator.\r\n2. **Performance**: The evaluation data is too large (TB), and the stand-alone evaluator is too slow to process the data.\r\nso:\r\nDo we have plans to support distributed evaluator?\r\n\r\n", "comments": ["@zhaozheng09 ,\r\nCan you please elaborate about your Feature. Also, please specify the Use Cases for this feature. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "OOM-Please try limiting GPU memory growth using any of the methods listed in this guide and check if it helps.\r\nhttps://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\r\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 51837, "title": "tensorflow support multi-evaluator.", "body": "Issues:\r\n1. **OOM**: The model from distributed training is too large (memory) to be restored to the evaluator.\r\n2. **Performance**: The evaluation data is too large (TB), and the stand-alone evaluator is too slow to process the data.\r\nso:\r\nDo we have plans to support distributed evaluator?\r\n\r\n", "comments": []}, {"number": 51836, "title": "Added a hyperparameter beta in swish activation function.", "body": "As described in (https://arxiv.org/abs/1710.05941) swish consists of a beta hyperparamter that is inputted in the sigmoid activation function. Implemented to paramter beta with default value of 1.0 in order to replicate the same behavior as current if no value is provided.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51836) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "@sanjoy @yongtang Hello guys. I am new in committing to open source. Can anyone guide me on how to proceed further in this pull request?", "@sanjoy Could you please have a look at the pr and update me with the required changes if there are any?", "I have updated the gradient calculation formulae.", "@rohan100jain Sir could you please review the PR. Thank you for your time.", "@rohan100jain Can you please review this PR ? Thanks!", "@rohan100jain I am getting 4 failing checks. Can u please guide me with that? ", "As this changes the API (adds a beta parameter), you need to update the API goldens\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/api/tests should have the instructions", "> As this changes the API (adds a beta parameter), you need to update the API goldens\r\n> \r\n> https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/api/tests should have the instructions\r\n\r\nI am not able to understand how to do this from the link provided. Can you share some other resources which can help me?\r\n", "```\r\nbazel run tensorflow/tools/api/tests:api_compatibility_test  -- --update_goldens True\r\n```\r\n\r\nShould update the golden files", "@rohan100jain Please check files once before approving. Thank you", "@rohan100jain why am i not able to merge? Is it waiting for @allenlavoie review?", "Still same issue :(\r\n", "I'm working with the internal failures to push this change through. Apologies for the rollback.", "> I'm working with the internal failures to push this change through. Apologies for the rollback.\r\n\r\nLet me know if I can help in any way. Thanks for the cooperation. :)", "@rohan100jain any update on the issue?\r\n", "Working on it. I am to push the change through by tomorrow at the latest.", "> Working on it. I am to push the change through by tomorrow at the latest.\r\n\r\nThank you for the update :)\r\n"]}]