[{"number": 2413, "title": "Compile Failed Numerous Times", "body": "Ubuntu 14.04\nGCC 4.8.4\nCuda 7.5\nCudnn 4.0.7\nBazel 0.2.2b\n\n---\n\n`bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer`\nINFO: Found 1 target...\nERROR: /home/peiguo/.cache/bazel/_bazel_peiguo/67521296d96959142b2d5d303d9c774c/external/re2/BUILD:9:1: undeclared inclusion(s) in rule '@re2//:re2':\nthis rule is missing dependency declarations for the following files included by 'external/re2/util/rune.cc':\n  '/usr/include/stdc-predef.h'\n  '/usr/lib/gcc/x86_64-linux-gnu/4.8/include/stdarg.h'\n  '/usr/include/string.h'\n  '/usr/include/features.h'\n  '/usr/include/x86_64-linux-gnu/sys/cdefs.h'\n  '/usr/include/x86_64-linux-gnu/bits/wordsize.h'\n  '/usr/include/x86_64-linux-gnu/gnu/stubs.h'\n  '/usr/include/x86_64-linux-gnu/gnu/stubs-64.h'\n  '/usr/lib/gcc/x86_64-linux-gnu/4.8/include/stddef.h'\n  '/usr/include/xlocale.h'\n  '/usr/include/x86_64-linux-gnu/bits/string3.h'\n  '/usr/lib/gcc/x86_64-linux-gnu/4.8/include/stdint.h'\n  '/usr/include/stdint.h'\n  '/usr/include/x86_64-linux-gnu/bits/wchar.h'.\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 2.329s, Critical Path: 2.01s\n\n---\n\nI have tried many times, the error is always undeclared inclusion(s) in rule xxxx, but the xxxx is changing constantly.\n", "comments": ["Does this problem only occur when you build that target with `--config=cuda`?\n\n@damienmg: You've helped out with a few of these errors before, so perhaps you'll have a quick answer. Is there an obvious cause for `/usr/include` not being in the builtin include paths? As far as I can tell it's explicitly defined in `CROSSTOOL` [here](https://github.com/tensorflow/tensorflow/blob/449d122b46d6e7fa2deba151356ffda200d1be7c/third_party/gpus/crosstool/CROSSTOOL#L62).\n", "@mrry Yes, it occurs without --config=cuda\n", "If on amazon use this ami number ami-a19b67c1\n", "Can you try updating Bazel to 0.2.3?\n\nCan you upload the file under bazel-tensorflow/external/local_config_cc/CROSSTOOL?\nWhat if you try with `--crosstool_top=@bazel_tools//tools/cpp:raw-toolchain`?\n", "Thanks for the help! But it still doesn't work.\n\n**Use 0.2.3 doesn't solve the problem. This time error message becomes:**\n\nERROR: /scratch/peiguo/tensor-flow/tensorflow/tensorflow/core/kernels/BUILD:1267:1: undeclared inclusion(s) in rule '//tensorflow/core/kernels:batchtospace_op_gpu':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/batchtospace_op_gpu.cu.cc':\n  '/home/.slash_usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/limits.h'\n  '/home/.slash_usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/syslimits.h'\n  '/home/.slash_usr/lib/gcc/x86_64-linux-gnu/4.8/include/stddef.h'\n  '/home/.slash_usr/lib/gcc/x86_64-linux-gnu/4.8/include/stdarg.h'\n  '/home/.slash_usr/lib/gcc/x86_64-linux-gnu/4.8/include/stdint.h'\n  '/home/.slash_usr/lib/gcc/x86_64-linux-gnu/4.8/include/x86intrin.h'\n  '/home/.slash_usr/lib/gcc/x86_64-linux-gnu/4.8/include/ia32intrin.h'\n  '/home/.slash_usr/lib/gcc/x86_64-linux-gnu/4.8/include/mmintrin.h'\n  '/home/.slash_usr/lib/gcc/x86_64-linux-gnu/4.8/include/xmmintrin.h'\n  '/home/.slash_usr/lib/gcc/x86_64-linux-gnu/4.8/include/mm_malloc.h'\n  '/home/.slash_usr/lib/gcc/x86_64-linux-gnu/4.8/include/emmintrin.h'\n  '/home/.slash_usr/lib/gcc/x86_64-linux-gnu/4.8/include/immintrin.h'\n  '/home/.slash_usr/lib/gcc/x86_64-linux-gnu/4.8/include/fxsrintrin.h'\n  '/home/.slash_usr/lib/gcc/x86_64-linux-gnu/4.8/include/adxintrin.h'\n  '/home/.slash_usr/lib/gcc/x86_64-linux-gnu/4.8/include/float.h'.\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\n\n**# CROSSTOOL:**\n\nmajor_version: \"local\"\nminor_version: \"\"\ndefault_target_cpu: \"same_as_host\"\n\ndefault_toolchain {\n  cpu: \"k8\"\n  toolchain_identifier: \"local\"\n}\n\ndefault_toolchain {\n  cpu: \"armeabi-v7a\"\n  toolchain_identifier: \"stub_armeabi-v7a\"\n}\n\n# Android tooling requires a default toolchain for the armeabi-v7a cpu.\ntoolchain {\n  abi_version: \"armeabi-v7a\"\n  abi_libc_version: \"armeabi-v7a\"\n  builtin_sysroot: \"\"\n  compiler: \"compiler\"\n  host_system_name: \"armeabi-v7a\"\n  needsPic: true\n  supports_gold_linker: false\n  supports_incremental_linker: false\n  supports_fission: false\n  supports_interface_shared_objects: false\n  supports_normalizing_ar: false\n  supports_start_end_lib: false\n  supports_thin_archives: false\n  target_libc: \"armeabi-v7a\"\n  target_cpu: \"armeabi-v7a\"\n  target_system_name: \"armeabi-v7a\"\n  toolchain_identifier: \"stub_armeabi-v7a\"\n\n  tool_path { name: \"ar\" path: \"/bin/false\" }\n  tool_path { name: \"compat-ld\" path: \"/bin/false\" }\n  tool_path { name: \"cpp\" path: \"/bin/false\" }\n  tool_path { name: \"dwp\" path: \"/bin/false\" }\n  tool_path { name: \"gcc\" path: \"/bin/false\" }\n  tool_path { name: \"gcov\" path: \"/bin/false\" }\n  tool_path { name: \"ld\" path: \"/bin/false\" }\n\n  tool_path { name: \"nm\" path: \"/bin/false\" }\n  tool_path { name: \"objcopy\" path: \"/bin/false\" }\n  tool_path { name: \"objdump\" path: \"/bin/false\" }\n  tool_path { name: \"strip\" path: \"/bin/false\" }\n  linking_mode_flags { mode: DYNAMIC }\n}\n\ntoolchain {\n  toolchain_identifier: \"local\"\n  abi_libc_version: \"local\"\n  abi_version: \"local\"\n  builtin_sysroot: \"\"\n  compiler: \"compiler\"\n  compiler_flag: \"-U_FORTIFY_SOURCE\"\n  compiler_flag: \"-D_FORTIFY_SOURCE=1\"\n  compiler_flag: \"-fstack-protector\"\n  compiler_flag: \"-Wall\"\n  compiler_flag: \"-Wl,-z,-relro,-z,now\"\n  compiler_flag: \"-B/usr/bin\"\n  compiler_flag: \"-B/usr/bin\"\n  compiler_flag: \"-Wunused-but-set-parameter\"\n  compiler_flag: \"-Wno-free-nonheap-object\"\n  compiler_flag: \"-fno-omit-frame-pointer\"\n  cxx_builtin_include_directory: \"/home/.slash_usr/lib/gcc/x86_64-linux-gnu/4.8/include\"\n  cxx_builtin_include_directory: \"/home/.slash_usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed\"\n  cxx_builtin_include_directory: \"/usr/include/c++/4.8\"\n  cxx_builtin_include_directory: \"/usr/include/x86_64-linux-gnu/c++/4.8\"\n  cxx_builtin_include_directory: \"/usr/include/c++/4.8/backward\"\n  cxx_builtin_include_directory: \"/usr/local/include\"\n  cxx_builtin_include_directory: \"/usr/include/x86_64-linux-gnu\"\n  cxx_builtin_include_directory: \"/usr/include\"\n  cxx_flag: \"-std=c++0x\"\n  host_system_name: \"local\"\n  linker_flag: \"-lstdc++\"\n  linker_flag: \"-lm\"\n  linker_flag: \"-no-canonical-prefixes\"\n  linker_flag: \"-Wl,-no-as-needed\"\n  linker_flag: \"-B/usr/bin\"\n  linker_flag: \"-B/usr/bin\"\n  linker_flag: \"-pass-exit-codes\"\n  linker_flag: \"-Wl,--build-id=md5\"\n  linker_flag: \"-Wl,--hash-style=gnu\"\n  linker_flag: \"-Wl,-rpath,/home/peiguo/.cache/bazel/_bazel_peiguo/f093626f144ca8499d84c1ae3f196d29/external/local_config_cc\"\n  linker_flag: \"-L/home/peiguo/.cache/bazel/_bazel_peiguo/f093626f144ca8499d84c1ae3f196d29/external/local_config_cc\"\n  linker_flag: \"-Wl,-rpath,/usr/local/lib\"\n  linker_flag: \"-L/usr/local/lib\"\n  linker_flag: \"-Wl,-rpath,/usr/local/cuda/lib64\"\n  linker_flag: \"-L/usr/local/cuda/lib64\"\n  linker_flag: \"-Wl,-rpath,/usr/local/cuda/lib64\"\n  linker_flag: \"-L/usr/local/cuda/lib64\"\n  linker_flag: \"-Wl,-rpath,/usr/local/cudnn-v4/lib64\"\n  linker_flag: \"-L/usr/local/cudnn-v4/lib64\"\n  needsPic: true\n  objcopy_embed_flag: \"-I\"\n  objcopy_embed_flag: \"binary\"\n  supports_fission: false\n  supports_gold_linker: false\n  supports_incremental_linker: false\n  supports_interface_shared_objects: false\n  supports_normalizing_ar: false\n  supports_start_end_lib: false\n  supports_thin_archives: false\n  target_cpu: \"k8\"\n  target_libc: \"local\"\n  target_system_name: \"local\"\n  unfiltered_cxx_flag: \"-no-canonical-prefixes\"\n  unfiltered_cxx_flag: \"-fno-canonical-system-headers\"\n  unfiltered_cxx_flag: \"-Wno-builtin-macro-redefined\"\n  unfiltered_cxx_flag: \"-D__DATE__=\\\"redacted\\\"\"\n  unfiltered_cxx_flag: \"-D__TIMESTAMP__=\\\"redacted\\\"\"\n  unfiltered_cxx_flag: \"-D__TIME__=\\\"redacted\\\"\"\n  tool_path {name: \"ar\" path: \"/usr/bin/ar\" }\n  tool_path {name: \"cpp\" path: \"/usr/bin/cpp\" }\n  tool_path {name: \"dwp\" path: \"/usr/bin/dwp\" }\n  tool_path {name: \"gcc\" path: \"/usr/bin/gcc\" }\n  tool_path {name: \"gcov\" path: \"/usr/bin/gcov\" }\n  tool_path {name: \"ld\" path: \"/usr/bin/ld\" }\n  tool_path {name: \"nm\" path: \"/usr/bin/nm\" }\n  tool_path {name: \"objcopy\" path: \"/usr/bin/objcopy\" }\n  tool_path {name: \"objdump\" path: \"/usr/bin/objdump\" }\n  tool_path {name: \"strip\" path: \"/usr/bin/strip\" }\n\n  compilation_mode_flags {\n    mode: DBG\n    compiler_flag: \"-g\"\n  }\n  compilation_mode_flags {\n    mode: OPT\n    compiler_flag: \"-g0\"\n    compiler_flag: \"-O2\"\n    compiler_flag: \"-DNDEBUG\"\n    compiler_flag: \"-ffunction-sections\"\n    compiler_flag: \"-fdata-sections\"\n    linker_flag: \"-Wl,--gc-sections\"\n  }\n  linking_mode_flags { mode: DYNAMIC }\n}\n\n**Do you mean use**: `bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer --crosstool_top=@bazel_tools//tools/cpp:raw-toolchain`\n\n**The error message is:** \n\nWARNING: Config values are not defined in any .rc file: cuda\nERROR: no such package 'tensorflow/cc': BUILD file not found on package path.\nINFO: Elapsed time: 0.069s\n", "no without --config=cuda\n\n--config=cuda is the issue on the error message you are showing\n", "**Now without --config=cuda:**\n\n`bazel build -c opt //tensorflow/cc:tutorials_example_trainer --crosstool_top=@bazel_tools//tools/cpp:raw-toolchain`\nERROR: Loading of target '@bazel_tools//tools/cpp:raw-toolchain' failed; build aborted: no such target '@bazel_tools//tools/cpp:raw-toolchain': target 'raw-toolchain' not declared in package 'tools/cpp' defined by /home/peiguo/.cache/bazel/_bazel_peiguo/f093626f144ca8499d84c1ae3f196d29/external/bazel_tools/tools/cpp/BUILD.\nERROR: Loading failed; build aborted.\nINFO: Elapsed time: 0.080s\n", "Sorry my bad, my memory served me wrong. So here the correct command:\n`bazel build -c opt //tensorflow/cc:tutorials_example_trainer --crosstool_top=@bazel_tools//tools/cpp:default-toolchain`\n\nCan you also try:\n`bazel build -c opt //tensorflow/cc:tutorials_example_trainer` (without anything else)?\n", "@DKP-90 No I am not on Amazon. \n", "Thanks @damienmg , `bazel build -c opt //tensorflow/cc:tutorials_example_trainer --crosstool_top=@bazel_tools//tools/cpp:default-toolchain` works out well!\n\nBut one more question, is there any solution for building with cuda support?\n", "And what about the second option: `bazel build -c opt //tensorflow/cc:tutorials_example_trainer`?\n\nFor CUDA, we should make the previous command work first.\n", "This is what I got from `bazel build -c opt //tensorflow/cc:tutorials_example_trainer`\n\nWARNING: /home/peiguo/.cache/bazel/_bazel_peiguo/f093626f144ca8499d84c1ae3f196d29/external/re2/WORKSPACE:1: Workspace name in /home/peiguo/.cache/bazel/_bazel_peiguo/f093626f144ca8499d84c1ae3f196d29/external/re2/WORKSPACE (@__main__) does not match the name given in the repository's definition (@re2); this will cause a build error in future versions.\nWARNING: /scratch/peiguo/tensor-flow/tensorflow/google/protobuf/BUILD:59:16: in includes attribute of cc_library rule //google/protobuf:protobuf_lite: 'src/' resolves to 'google/protobuf/src' not in 'third_party'. This will be an error in the future.\nWARNING: /scratch/peiguo/tensor-flow/tensorflow/google/protobuf/BUILD:124:16: in includes attribute of cc_library rule //google/protobuf:protobuf: 'src/' resolves to 'google/protobuf/src' not in 'third_party'. This will be an error in the future.\nWARNING: /scratch/peiguo/tensor-flow/tensorflow/google/protobuf/BUILD:266:16: in includes attribute of cc_library rule //google/protobuf:protoc_lib: 'src/' resolves to 'google/protobuf/src' not in 'third_party'. This will be an error in the future.\nINFO: Found 1 target...\nERROR: /home/peiguo/.cache/bazel/_bazel_peiguo/f093626f144ca8499d84c1ae3f196d29/external/re2/BUILD:9:1: undeclared inclusion(s) in rule '@re2//:re2':\nthis rule is missing dependency declarations for the following files included by 'external/re2/util/rune.cc':\n  '/usr/lib/gcc/x86_64-linux-gnu/4.8/include/stdarg.h'\n  '/usr/lib/gcc/x86_64-linux-gnu/4.8/include/stddef.h'\n  '/usr/lib/gcc/x86_64-linux-gnu/4.8/include/stdint.h'.\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 4.951s, Critical Path: 1.28s\n", "what does `echo | ${CC} -E -xc++ - -v` gives you?\n", "echo | ${CC} -E -xc++ - -v\n-E: command not found\n", "oops `echo | gcc -E -xc++ - -v` (sorry for the number of command)\n", "Using built-in specs.\nCOLLECT_GCC=gcc\nTarget: x86_64-linux-gnu\nConfigured with: ../src/configure -v --with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04' --with-bugurl=file:///usr/share/doc/gcc-4.8/README.Bugs --enable-languages=c,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-4.8 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --with-gxx-include-dir=/usr/include/c++/4.8 --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --enable-gnu-unique-object --disable-libmudflap --enable-plugin --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-amd64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-amd64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-amd64 --with-arch-directory=amd64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --enable-objc-gc --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu\nThread model: posix\ngcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04) \nCOLLECT_GCC_OPTIONS='-E' '-v' '-mtune=generic' '-march=x86-64'\n /home/.slash_usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/cc1plus -E -quiet -v -imultiarch x86_64-linux-gnu -iprefix /home/.slash_usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/ -D_GNU_SOURCE - -mtune=generic -march=x86-64 -fstack-protector -Wformat -Wformat-security\nignoring nonexistent directory \"/home/.slash_usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../x86_64-linux-gnu/include\"\nignoring duplicate directory \"/usr/include/x86_64-linux-gnu/c++/4.8\"\nignoring duplicate directory \"/home/.slash_usr/bin/../lib/gcc/../../lib/gcc/x86_64-linux-gnu/4.8/include\"\nignoring nonexistent directory \"/usr/local/include/x86_64-linux-gnu\"\nignoring duplicate directory \"/home/.slash_usr/bin/../lib/gcc/../../lib/gcc/x86_64-linux-gnu/4.8/include-fixed\"\nignoring nonexistent directory \"/home/.slash_usr/bin/../lib/gcc/../../lib/gcc/x86_64-linux-gnu/4.8/../../../../x86_64-linux-gnu/include\"\n#include \"...\" search starts here:\n#include <...> search starts here:\n /home/.slash_usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/include\n /home/.slash_usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/include-fixed\n /usr/include/c++/4.8\n /usr/include/x86_64-linux-gnu/c++/4.8\n /usr/include/c++/4.8/backward\n /usr/local/include\n /usr/include/x86_64-linux-gnu\n /usr/include\nEnd of search list.\n# 1 \"<stdin>\"\n# 1 \"<built-in>\"\n# 1 \"<command-line>\"\n# 1 \"/usr/include/stdc-predef.h\" 1 3 4\n# 1 \"<command-line>\" 2\n# 1 \"<stdin>\"\nCOMPILER_PATH=/home/.slash_usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/:/home/.slash_usr/bin/../lib/gcc/x86_64-linux-gnu/:/home/.slash_usr/bin/../lib/gcc/\nLIBRARY_PATH=/home/.slash_usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/:/home/.slash_usr/bin/../lib/gcc/x86_64-linux-gnu/:/home/.slash_usr/bin/../lib/gcc/:/home/.slash_usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/:/home/.slash_usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/:/lib/x86_64-linux-gnu/:/lib/../lib/:/usr/lib/x86_64-linux-gnu/:/usr/lib/../lib/:/home/.slash_usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../:/lib/:/usr/lib/\nCOLLECT_GCC_OPTIONS='-E' '-v' '-mtune=generic' '-march=x86-64'\n", "`/usr/lib/gcc/x86_64-linux-gnu` is not in the list of include directories returned by gcc which explains why bazel cannot find it. Weird that it found stuff there.\n\nAnyhow, the fix for cuda should be to add the following line:\n\n```\n  cxx_builtin_include_directory: \"/home/.slash_usr/lib/gcc/\"\n```\n\nat that line: https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/crosstool/CROSSTOOL#L62\n", "This time it works like magic! \n\nThanks a lot, this problem confuses me for at least two days... \n\nNow I have built Tensorflow from source and I can do [fine-tuning on Inception-V3 model](https://github.com/tensorflow/models/blob/master/inception/README.md#how-to-fine-tune-a-pre-trained-model-on-a-new-task)!\n", "Thanks for persevering @guopei and massive thanks to @damienmg for helping! Damien, is there something we could/should do in the TF/Bazel configuration to make this less likely to happen, or is this just a case of adding documentation?\n", "TF CUDA crosstool file should be configured depending on the used C++ compiler, just like we do for the default crosstool in Bazel.\n\nI am a bit more puzzled on how @guopei compiler includes file from /usr/lib/gcc/x86_64-linux-gnu/4.8/include while it is not reported by gcc as a system include directory. But that might just be an issue of setting correctly the CC environment variable.\n", "Thanks again. @keveman, sorry for picking on you, but would you be able to make these changes (feel free to reassign if someone else would be better)? It looks like there are some longstanding TODOs in our crosstool from the original OSS release, and we might be able to revisit them now.\n", "@damienmg Damien, are you suggesting I add `/home/.slash_usr/lib/gcc/` to the `CROSSTOOL`? Or is this something that should be done via the `configure` script by querying the `gcc` installed on the system?\n", "Yes should be done at configuration step, see my comment on #1157\n", "Hi,\n\nI am getting this error when running:\n\nbazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\n\nERROR: /home/audio-dnn/tensorflow/tensorflow/core/kernels/BUILD:855:1: undeclared inclusion(s) in rule '//tensorflow/core/kernels:crop_and_resize_op_gpu':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/crop_and_resize_op_gpu.cu.cc':\n  '/usr/local/cuda-7.5/include/cuda_runtime.h'\n  '/usr/local/cuda-7.5/include/host_config.h'\n  '/usr/local/cuda-7.5/include/builtin_types.h'\n  '/usr/local/cuda-7.5/include/device_types.h'\n  '/usr/local/cuda-7.5/include/host_defines.h'\n  '/usr/local/cuda-7.5/include/driver_types.h'\n  '/usr/local/cuda-7.5/include/surface_types.h'\n  '/usr/local/cuda-7.5/include/texture_types.h'\n  '/usr/local/cuda-7.5/include/vector_types.h'\n  '/usr/local/cuda-7.5/include/channel_descriptor.h'\n  '/usr/local/cuda-7.5/include/cuda_runtime_api.h'\n  '/usr/local/cuda-7.5/include/cuda_device_runtime_api.h'\n  '/usr/local/cuda-7.5/include/driver_functions.h'\n  '/usr/local/cuda-7.5/include/vector_functions.h'\n  '/usr/local/cuda-7.5/include/vector_functions.hpp'\n  '/usr/local/cuda-7.5/include/common_functions.h'\n  '/usr/local/cuda-7.5/include/math_functions.h'\n  '/usr/local/cuda-7.5/include/math_functions.hpp'\n  '/usr/local/cuda-7.5/include/math_functions_dbl_ptx3.h'\n  '/usr/local/cuda-7.5/include/math_functions_dbl_ptx3.hpp'\n  '/usr/local/cuda-7.5/include/cuda_surface_types.h'\n  '/usr/local/cuda-7.5/include/cuda_texture_types.h'\n  '/usr/local/cuda-7.5/include/device_functions.h'\n  '/usr/local/cuda-7.5/include/device_functions.hpp'\n  '/usr/local/cuda-7.5/include/device_atomic_functions.h'\n  '/usr/local/cuda-7.5/include/device_atomic_functions.hpp'\n  '/usr/local/cuda-7.5/include/device_double_functions.h'\n  '/usr/local/cuda-7.5/include/device_double_functions.hpp'\n  '/usr/local/cuda-7.5/include/sm_20_atomic_functions.h'\n  '/usr/local/cuda-7.5/include/sm_20_atomic_functions.hpp'\n  '/usr/local/cuda-7.5/include/sm_32_atomic_functions.h'\n  '/usr/local/cuda-7.5/include/sm_32_atomic_functions.hpp'\n  '/usr/local/cuda-7.5/include/sm_35_atomic_functions.h'\n  '/usr/local/cuda-7.5/include/sm_20_intrinsics.h'\n  '/usr/local/cuda-7.5/include/sm_20_intrinsics.hpp'\n  '/usr/local/cuda-7.5/include/sm_30_intrinsics.h'\n  '/usr/local/cuda-7.5/include/sm_30_intrinsics.hpp'\n  '/usr/local/cuda-7.5/include/sm_32_intrinsics.h'\n  '/usr/local/cuda-7.5/include/sm_32_intrinsics.hpp'\n  '/usr/local/cuda-7.5/include/sm_35_intrinsics.h'\n  '/usr/local/cuda-7.5/include/surface_functions.h'\n  '/usr/local/cuda-7.5/include/surface_functions.hpp'\n  '/usr/local/cuda-7.5/include/texture_fetch_functions.h'\n  '/usr/local/cuda-7.5/include/texture_fetch_functions.hpp'\n  '/usr/local/cuda-7.5/include/texture_indirect_functions.h'\n  '/usr/local/cuda-7.5/include/texture_indirect_functions.hpp'\n  '/usr/local/cuda-7.5/include/surface_indirect_functions.h'\n  '/usr/local/cuda-7.5/include/surface_indirect_functions.hpp'\n  '/usr/local/cuda-7.5/include/device_launch_parameters.h'\n  '/usr/local/cuda-7.5/include/cuda_fp16.h'\n  '/usr/local/cuda-7.5/include/math_constants.h'\n  '/usr/local/cuda-7.5/include/curand_kernel.h'\n  '/usr/local/cuda-7.5/include/curand.h'\n  '/usr/local/cuda-7.5/include/curand_discrete.h'\n  '/usr/local/cuda-7.5/include/curand_precalc.h'\n  '/usr/local/cuda-7.5/include/curand_mrg32k3a.h'\n  '/usr/local/cuda-7.5/include/curand_mtgp32_kernel.h'\n  '/usr/local/cuda-7.5/include/cuda.h'\n  '/usr/local/cuda-7.5/include/curand_mtgp32.h'\n  '/usr/local/cuda-7.5/include/curand_philox4x32_x.h'\n  '/usr/local/cuda-7.5/include/curand_globals.h'\n  '/usr/local/cuda-7.5/include/curand_uniform.h'\n  '/usr/local/cuda-7.5/include/curand_normal.h'\n  '/usr/local/cuda-7.5/include/curand_normal_static.h'\n  '/usr/local/cuda-7.5/include/curand_lognormal.h'\n  '/usr/local/cuda-7.5/include/curand_poisson.h'\n  '/usr/local/cuda-7.5/include/curand_discrete2.h'.\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\n\nHere is the output of  \n\necho | gcc -E -xc++ - -v \n\nUsing built-in specs.\nCOLLECT_GCC=gcc\nTarget: x86_64-linux-gnu\nConfigured with: ../src/configure -v --with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04.3' --with-bugurl=file:///usr/share/doc/gcc-4.8/README.Bugs --enable-languages=c,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-4.8 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --with-gxx-include-dir=/usr/include/c++/4.8 --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --enable-gnu-unique-object --disable-libmudflap --enable-plugin --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-amd64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-amd64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-amd64 --with-arch-directory=amd64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --enable-objc-gc --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu\nThread model: posix\ngcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04.3)\nCOLLECT_GCC_OPTIONS='-E' '-v' '-mtune=generic' '-march=x86-64'\n /usr/lib/gcc/x86_64-linux-gnu/4.8/cc1plus -E -quiet -v -imultiarch x86_64-linux-gnu -D_GNU_SOURCE - -mtune=generic -march=x86-64 -fstack-protector -Wformat -Wformat-security\nignoring duplicate directory \"/usr/include/x86_64-linux-gnu/c++/4.8\"\nignoring nonexistent directory \"/usr/local/include/x86_64-linux-gnu\"\nignoring nonexistent directory \"/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../x86_64-linux-gnu/include\"\n#include \"...\" search starts here:\n#include <...> search starts here:\n /usr/include/c++/4.8\n /usr/include/x86_64-linux-gnu/c++/4.8\n /usr/include/c++/4.8/backward\n /usr/lib/gcc/x86_64-linux-gnu/4.8/include\n /usr/local/include\n /usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed\n /usr/include/x86_64-linux-gnu\n /usr/include\nEnd of search list.\n\n# 1 \"<stdin>\"\n\n# 1 \"<built-in>\"\n\n# 1 \"<command-line>\"\n\n# 1 \"/usr/include/stdc-predef.h\" 1 3 4\n\n# 1 \"<command-line>\" 2\n\n# 1 \"<stdin>\"\n\nCOMPILER_PATH=/usr/lib/gcc/x86_64-linux-gnu/4.8/:/usr/lib/gcc/x86_64-linux-gnu/4.8/:/usr/lib/gcc/x86_64-linux-gnu/:/usr/lib/gcc/x86_64-linux-gnu/4.8/:/usr/lib/gcc/x86_64-linux-gnu/\nLIBRARY_PATH=/usr/lib/gcc/x86_64-linux-gnu/4.8/:/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/:/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/:/lib/x86_64-linux-gnu/:/lib/../lib/:/usr/lib/x86_64-linux-gnu/:/usr/lib/../lib/:/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../:/lib/:/usr/lib/\nCOLLECT_GCC_OPTIONS='-E' '-v' '-mtune=generic' '-march=x86-64'\n\nI ran the the command suggested by damienmg in above, and they built without error.\n\nWould you please help me.\n\nPS: nvidia driver version is 367.35, and I did not install the driver when I install cuda from runfile.\n", "@bostonbuddy Maybe add `cxx_builtin_include_directory` pointing to your 7.5 cuda similar to https://github.com/tensorflow/tensorflow/issues/3431\n", "Thank you so much @yaroslavvb , adding the following line to third_party/gpus/crosstool/CROSSTOOL worked:\n\ncxx_builtin_include_directory: \"/usr/local/cuda-7.5/include\"\n", "Closing as this seems to be resolved. Please reopen or create new issue if I'm wrong.\n", "bazel build -c opt //tensorflow/cc:tutorials_example_trainer --crosstool_top=@bazel_tools//tools/cpp:default-toolchain\n\nI have used the command to build tensorflow , but how to install tensorflow? \nand I want to know It can be used for gpu?\n", "With latest tensorflow, I have CROSSTOOL.tlp. Should we copy this as CROSSTOOL and make modifications to this (adding cxx_builtin_include_directory)?\n", "@yuye2133 \nYour command is to build and run the example of tensorflow. To install TensorFlow to your python, you have to run the command to build the pip package (wheel) and then install it. Following are copied from TensorFlow website:\n\n```\n$ bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n\n# To build with GPU support:\n$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n\n$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\n\n# The name of the .whl file will depend on your platform.\n$ sudo pip install /tmp/tensorflow_pkg/tensorflow-0.10.0rc0-py2-none-any.whl\n```\n\nTo test if new installed tensorflow works as you expected, you can try running the following python script, if there is no error then it should be okay.\n\n```\nimport tensorflow as tf\n# Creates a graph.\nwith tf.device('/gpu:0'):\n  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n  c = tf.matmul(a, b)\n# Creates a session with log_device_placement set to True.\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n# Runs the op.\nprint sess.run(c)\n```\n", "@srp1970 CROSSTOOL.tpl is the same as CROSSTOOL (I have just compiled tensorflow successfully today). There are already some `cxx_builtin_include_directory` in that file, you only need to update it with new paths.\n"]}, {"number": 2412, "title": "Packaged TensorFlow C++ library for bazel-independent use", "body": "Currently, building a C++ application in tensorflow requires creating a project in the tensorflow source tree and compiling with bazel. In my case I would like to use tensorflow in a (fairly large) existing application with an existing build system that would be difficult to port to bazel. The solution to me seems to be exposing tensorflow as a library that can be linked with.\n", "comments": ["The `\"//tensorflow:libtensorflow.so\"` bazel target enables you to build a standalone TensorFlow library that you can dynamically link into your other project. Would that be sufficient?\n", "Yes, that is what I need. It would be nice if this was documented somewhere too!\n", "could you give a more specific example? \n\nsuppose I have this file, and I want to compile it with tensorflow with g++ directly (no bazel, not in tensorflow's repo), how should I do this??\n\n[loader.cc.txt](https://github.com/tensorflow/tensorflow/files/425860/loader.cc.txt)\n", "@cgao3 working on this myself... check these docs:\n- https://medium.com/jim-fleming/loading-a-tensorflow-graph-with-the-c-api-4caaff88463f#.4zblcgk1z\n- https://medium.com/jim-fleming/loading-tensorflow-graphs-via-host-languages-be10fd81876f#.3dka1t307\n- https://github.com/tensorflow/tensorflow/pull/695\n- http://tldp.org/HOWTO/Program-Library-HOWTO/dl-libraries.html\n", "@vladfi1  @mrry what is the headers should I include while linking libtensorflow.so?", "@MisayaZ That depends on what you're building. The most reliable interface is the C API, which you can include with  `#include \"tensorflow/c/c_api.h\"`.", "@asimshankar I think we can call this resolved once c distributable is ready, right?", "Yes and no :)\r\nI'm hoping to have the C API distributable soon, but C++ will be a little while longer since we're still working on the right API surface.\r\n\r\nFWIW, for using the C API you need just `c_api.h` and `libtensorflow.so`, which we will package into a tarball.", "Hi, I'm trying to play around with the C API. After running `bazel build //tensorflow:libtensorflow.so`\r\nand copy the shared library into another folder, I successfully compiled and linked a small example. But when I tried to run it, it showed something like this:\r\n```bash\r\ndyld: Library not loaded: bazel-out/local-py3-opt/bin/tensorflow/libtensorflow.so\r\n  Referenced from: /Users/inflation/workspace/link_test/./a.out\r\n  Reason: image not found\r\n```\r\n\r\nI must put the library in the same folder structure as it said to run it without error. So what is wrong here?", "Maybe you should set\nDYLD_LIBRARY_PATH ?\n---------- Forwarded message ----------\nFrom: Aaron Hu <notifications@github.com>\nDate: Sun, Feb 5, 2017 at 12:30 PM\nSubject: Re: [tensorflow/tensorflow] Tensorflow as C++ library (#2412)\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: cgao3 <cgao3@ualberta.ca>, Mention <mention@noreply.github.com>\n\n\nHi, I'm trying to play around with the C API. After running bazel build\n//tensorflow:libtensorflow.so\nand copy the shared library into another folder, I successfully compiled\nand linked a small example. But when I tried to run it, it showed something\nlike this:\n\ndyld: Library not loaded:\nbazel-out/local-py3-opt/bin/tensorflow/libtensorflow.so\n  Referenced from: /Users/inflation/workspace/link_test/./a.out\n  Reason: image not found\n\nI must put the library in the same folder structure as it said to run it\nwithout error. So what is wrong here?\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\n<https://github.com/tensorflow/tensorflow/issues/2412#issuecomment-277542634>,\nor mute the thread\n<https://github.com/notifications/unsubscribe-auth/APhNiYAGXNVHAPTjpal5Ru-z1r4eg1TOks5rZiNIgaJpZM4Ig4U3>\n.\n", "Seems like you're using OS X.\r\nFor that your choices are to either:\r\n\r\n- Copy `libtensorflow.so` into `/usr/local/lib`\r\n- Or use `DYLD_LIBRARY_PATH` (e.g., `export DYLD_LIBRARY_PATH=/path/to/dir/containing/libtensorflow.so`)\r\n\r\nHope that helps", "@asimshankar Thanks a lot!", "@mrry bit off topic, what would be needed to build it as a lib for windows and is there a plan for this do you know?", "@acomarce Two things, really:\r\n\r\n1. We'd need to set up the `__declspec(dllexport)`/`__declspec(dllimport)` annotations to define the set of symbols that would be exported by/imported from such a DLL.\r\n2. We'd need some additional build rules for creating the DLL in either the CMake or Bazel builds.\r\n\r\nPart 1 will probably be easier if we focus on the C API, which has been designed as a stable API for consumption by many different projects, and has stronger versioning guarantees. The C++ API is more sprawling, and it would require invasive changes to set up the appropriate annotations. (I was just chatting with @guschmue, who observed that CMake has a newish feature for generating these definitions without code modification, but the resulting DLL ends up with more than the maximum 65536 symbols exported....)\r\n\r\nI'm not aware of anyone who is working on this right now. Contributions would be welcome!", "The cmake support for this is called CMAKE_WINDOWS_EXPORT_ALL_SYMBOLS. In theory this would be perfect but it adds everything public to the .def which the linker is unhappy about (just too much). Doing the c-api with dllexport is fine (I have a version of this) but I think even to do an user_op to be loaded with tf.load_library() you  need some of the c++ api and the doc shows a lot of classes for the c++ api: https://www.tensorflow.org/api_docs/cc/.\r\nStill looking for some better way to do this.\r\nSame ask here: https://github.com/tensorflow/tensorflow/issues/7258", "Maybe let me double check: is the c++ api one needs to support this\r\nhttps://www.tensorflow.org/api_docs/cc/\r\nor\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/g3doc/api_docs/cc\r\nThe later is manageable. Until 1.0 it was the same.\r\n", "@guschmue I think it should be the later.", "i made the changes in my local tree for https://github.com/tensorflow/tensorflow/tree/master/tensorflow/g3doc/api_docs/cc - around 130 methods/classes. With that I got the c# bindings to work. Going to try if I can get tensorflow/user_ops to work with that.", "@guschmue Do you think the project would ever consider explicitly exposing just the public API for all platforms? Would it be fair to say this is currently implicit based on the doc generation process? Looks like GCC 4 supports something similar to __declspec(dllexport) - http://gcc.gnu.org/wiki/Visibility. Although, the caveats on that page regarding exceptions sound messy, so perhaps this has already been discussed and decided against...", "Yes - for windows this would be perfect. But its a lot of classes/files to be touched and its hard for me as external contributor to decide what is long term public and what is private on the tensorflow api; I think a tensorflow team member would need to go over the entire api and make that call. \r\nIn the meantime I have fixed the tf.load_library()  #8217 ", "@guschmue Apologies, I wrongly assumed you were an internal contributor! Would be interested to see your changes to get the C# bindings to work though.\r\n\r\nPerhaps my question would have been better directed at @mrry", "For the c# binding: I actually got https://github.com/migueldeicaza/tensorflowsharp to work with my changes. The 'tensorflow.dll' is in the python wheel, only that it is called _pywrap_tensorflow.pyd.\r\nIf you copy _pywrap_tensorflow.pyd to tensorflow.dll and than point the tensorflowsharp project to that dll it should work. I think the only other thing I had to do was to tell the project that tensorflow.dll is a x64 native dll.\r\nWould be cool if we could make this a nuget package ... thinking about that.", "For the public *client* API, it should be relatively easy to make a DLL that exports just the symbols in [`c_api.h`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.h) and I think we'd welcome contributions to the CMake or Bazel/Windows builds that make this possible.\r\n\r\nThe `_pywrap_tensorflow.pyd` library exports far more symbols in order to support the implicity-defined public *framework* API which custom op kernels use. Making `tf.load_library()` work on Windows required heroic efforts from @guschmue to scrape all of the necessary symbols :) from the library (and then subset them to avoid hitting symbol table limits). Part of the issue is that some of the public framework API includes generated code (e.g. protobuf headers), and threading the appropriate definitions through to the linker is somewhat challenging....\r\n\r\nAll of which is a long-winded way of saying (in my opinion) the de facto public API for the framework (i.e. to support custom ops) is too unwieldy, and I don't think we should exhaustively trace down every symbol that needs to be exported. It would be better to spend time defining a clean, self-contained, and possibly versioned API for creating extension modules, probably exporting C bindings like the client API, and then we could have more confidence about modules working across different builds of TensorFlow. This would be a big undertaking though....", "Totally agree. _pywrap_tensorflow.pyd includes a ton more than the public c api and exporting the c api only would be easy (+ maybe tensorflow::ops?). We added the TF_EXPORT define so we could just annotate the c api with TF_EXPORT.\r\nBUT if your model requires a custom op that needs to be loaded (say a gru) I think you'd need to have my large .def file again. All a bit painful.\r\nI'm actively trying to use  a tensorflow.dll so I can get rid of the static linking and still have a few issues with it: tensorflow/tools/benchmark/ works, tensorflow/examples/label_image/ has a few unresolved symbols, mostly in the namespace tensorflow::ops ... still looking at that.", "This is still an open issue. Even when creating the libfile-file by:\r\n```\r\nbazel build //tensorflow:libtensorflow.so\r\n```\r\n\r\nI cannot build the file\r\n```cpp\r\n#include <tensorflow/core/framework/tensor.pb.h>\r\n\r\nint main(int argc, char const *argv[])\r\n{\r\n    tensorflow::TensorProto tensor;\r\n    return 0;\r\n}\r\n```\r\nbecause it misses some important parts:\r\n\r\n```\r\n/tmp/ccHrvxj1.o: In function `main':\r\nstandalone.cpp:(.text.startup+0x1f): undefined reference to `tensorflow::TensorProto::TensorProto()'\r\nstandalone.cpp:(.text.startup+0x27): undefined reference to `tensorflow::TensorProto::~TensorProto()'\r\ncollect2: error: ld returned 1 exit status\r\n```\r\n\r\n```\r\ng++ standalone.cpp -Wall -Wextra  -std=c++11 -O3 -march=native -fPIC -I /home/myusername/.local/lib/python2.7/site-packages/tensorflow/include -L/path/to/tensorflow/bazel-bin/tensorflow -ltensorflow\r\n```\r\n\r\nThis basically prevents to build a c++ project with tensorflow as dependencies without messing files around in the official repo.", "@PatWie : `libtensorflow.so` only export symbols from the C API (`tensorflow/c/c_api.h`). It does not export any protocol buffer symbols or other C++ ones. This issue is still open since there is not C++ API distribution yet.\r\n\r\n", "@mrry @asimshankar : What would be the steps to build and link to TF as a static C++ library then? Or would that not be a feasible task outside of `bazel` build environment?", "@skye is looking into a distribution for the C++ library (i.e., library + all required header files).\r\nHowever, you can build from source for now yourself with the `//tensorflow:libtensorflow_cc.so` target.\r\n\r\nIf you need a static library, we're currently blocked on https://github.com/bazelbuild/bazel/issues/1920", "@asimshankar \r\nSo, \r\n`//tensorflow:libtensorflow_cc.so` is a C++ API\r\nAnd `//tensorflow:libtensorflow.so` is a C API\r\n\r\nCorrect?", "@nikolai3d : Correct.", "Are there are any plans to provide binary distributions for C/C++ like you already do for C (`libtensorflow-cpu-linux-x86_64-1.1.0.tar.gz`)? That would be very helpful for people who want to try out C++ API.", "Btw, for the binary C++ distributions I would suggest making it dual-ABI (GCC4-compatible and new default C++11 ABI) if any of API interfaces are using `std::string`, `std::list` and some types.", "@asimshankar Once `//tensorflow:libtensorflow_cc.so` is built, what are the next steps to compiling, linked  a new program properly? I have yet to see any docs regarding this. Could you paste a very simple example, please?", "@lababidi : There are no docs about this yet since it is not something we currently support. @skye is looking into packaging, but this not something that has gotten priority at the moment given all the other pursuits. In particular, the built shared library and all the header files need to be packaged.\r\n\r\nNote though, you can still use bazel to build C++ programs as in https://www.tensorflow.org/api_guides/cc/guide\r\n\r\nI sincerely apologize, but I don't have a good answer for how to package all the required header files and for now we'd have to just encourage folks out there to figure it out and share their ideas.\r\n\r\nOne possible starting point might be to look at the `g++` commands that bazel executes and learn the include directories from there. For example, I did the following:\r\n\r\n```sh\r\n# Followed the instructions in https://www.tensorflow.org/api_guides/cc/guide to build\r\n# an example C++ program using bazel\r\nbazel build -c opt //tensorflow/cc/example:example\r\n\r\n# Delete the generated files\r\nrm -rf ./bazel-out/local-opt/tensorflow/cc/example\r\n\r\n# Recompile the example program and get the GCC command used:\r\nbazel build -s -c opt //tensorflow/cc/example:example\r\n```\r\n\r\nThat showed me something like the following:\r\n\r\n```sh\r\n(cd /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow && \\\r\n  exec env - \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/local-opt/bin/tensorflow/cc/example/_objs/example/tensorflow/cc/example/example.d '-frandom-seed=bazel-out/local-opt/bin/tensorflow/cc/example/_objs/example/tensorflow/cc/example/example.o' -DEIGEN_MPL2_ONLY -DTENSORFLOW_USE_JEMALLOC -iquote . -iquote bazel-out/local-opt/genfiles -iquote external/jemalloc -iquote bazel-out/local-opt/genfiles/external/jemalloc -iquote external/bazel_tools -iquote bazel-out/local-opt/genfiles/external/bazel_tools -iquote external/protobuf -iquote bazel-out/local-opt/genfiles/external/protobuf -iquote external/eigen_archive -iquote bazel-out/local-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/local-opt/genfiles/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/local-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/local-opt/genfiles/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/local-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/local-opt/genfiles/external/farmhash_archive -iquote external/highwayhash -iquote bazel-out/local-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/local-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/local-opt/genfiles/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/local-opt/genfiles/external/local_config_cuda -iquote external/gemmlowp -iquote bazel-out/local-opt/genfiles/external/gemmlowp -isystem external/jemalloc/include -isystem bazel-out/local-opt/genfiles/external/jemalloc/include -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/protobuf/src -isystem bazel-out/local-opt/genfiles/external/protobuf/src -isystem external/eigen_archive -isystem bazel-out/local-opt/genfiles/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/local-opt/genfiles/external/gif_archive/lib -isystem external/farmhash_archive/src -isystem bazel-out/local-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/local-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/local-opt/genfiles/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/local-opt/genfiles/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/include -isystem bazel-out/local-opt/genfiles/external/local_config_cuda/cuda/include -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/cc/example/example.cc -o bazel-out/local-opt/bin/tensorflow/cc/example/_objs/example/tensorflow/cc/example/example.o)\r\n```\r\n\r\nSo under the bazel cache root directory: `/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow` (will be different on your machine) one can find all the include files and the directories needed are the arguments to the `-iquote` and `-isystem` flags in the `gcc` command. So all of those will have to be included.\r\n\r\nAgain, I provide this as a possible starting point. I realize that use of the C++ API without bazel at this time is inconvenient to say the least. It's something we'd like get done, but haven't gotten around to at this point an contributions (in terms of say bazel rules or scripts that can package the `.so` and all headers files into a single tarball) would be welcome", "@asimshankar Thanks for your sincere comment and thanks to @HossamAmer12 at https://github.com/tensorflow/tensorflow/issues/1890 for providing a zip of his work, deriving from the Medium article example, I have the follow:\r\n\r\nFirst, after installing `protobuf` and `eigen`:\r\n```\r\n./configure\r\nbazel build  //tensorflow:libtensorflow_cc.so\r\n```\r\n\r\nThen Copy the following include headers and dynamic shared library to `/usr/local/lib` and `/usr/local/include`:\r\n```\r\nmkdir /usr/local/include/tf\r\ncp -r bazel-genfiles/ /usr/local/include/tf/\r\ncp -r tensorflow /usr/local/include/tf/\r\ncp -r third_party /usr/local/include/tf/\r\ncp -r bazel-bin/libtensorflow_cc.so /usr/local/lib/\r\n```\r\n\r\nLastly, compile using an example:\r\n```\r\ng++ -std=c++11 -o tLoader -I/usr/local/include/tf -I/usr/local/include/eigen3 -g -Wall -D_DEBUG -Wshadow -Wno-sign-compare -w  -L/usr/local/lib/libtensorflow_cc `pkg-config --cflags --libs protobuf`  -ltensorflow_cc loader.cpp\r\n```", "@lababidi I am glad that it worked for you. It also me took me a while to figure it out. I actually thought of writing a tutorial, but I decided that sharing my ZIP project is an easier solution.\r\n\r\n@asimshankar It will be great if you can add @lababidi steps and my ZIP project into your documentation. ", "Despite the advice of @lababidi,  the following command:\r\n```\r\ng++ -std=c++11 -o tf_basic \\\r\n-I /home/saubin/src/tensorflow/ \\\r\n-I /home/saubin/src/tensorflow/bazel-genfiles/ \\\r\n-I /home/saubin/src/tensorflow/third_party/ \\\r\n-g -Wall -D_DEBUG -Wshadow -Wno-sign-compare -w \\\r\n-L /home/saubin/src/tensorflow/bazel-bin/tensorflow/ \\\r\n`pkg-config --cflags --libs protobuf` -l tensorflow_cc basic.cc\r\n```\r\n\r\nTo compile:\r\n```\r\n#include <tensorflow/core/public/session.h>\r\n#include <tensorflow/core/platform/env.h>\r\n\r\n#include <iostream>\r\n#include <chrono>\r\n#include <fstream>\r\n#include <string>\r\n#include <vector>\r\n#include <sstream>\r\n\r\nusing namespace std;\r\nusing namespace chrono;\r\nusing namespace tensorflow;\r\n\r\nint main(int argc, char* argv[]) {\r\n\r\n  // Initialize a tensorflow session\r\n  cout << \"start initalize session\" << \"\\n\";\r\n  Session* session;\r\n  Status status = NewSession(SessionOptions(), &session);\r\n  if (!status.ok()) {\r\n    cout << status.ToString() << \"\\n\";\r\n    return 1;\r\n  }\r\n}\r\n```\r\n\r\nBut this gives me the error:\r\n```\r\nIn file included from /home/saubin/src/tensorflow/tensorflow/core/framework/tensor.h:19:0,\r\n                 from /home/saubin/src/tensorflow/tensorflow/core/public/session.h:23,\r\n                 from basic.cc:1:\r\n/home/saubin/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:42: fatal error: unsupported/Eigen/CXX11/Tensor: No such file or directory\r\ncompilation terminated.\r\n```\r\n\r\nI just thought I would throw in my own experience struggling with this problem to help whoever has enough C++ knowledge to make this work.", "You must install Eigen and link to it properly.\nI ran into a similar issue until I installed Eigen.\n\nOn Wed, May 24, 2017 at 3:26 AM Sean Aubin <notifications@github.com> wrote:\n\n> Despite the advice of @lababidi <https://github.com/lababidi>, the\n> following command:\n>\n> g++ -std=c++11 -o tf_basic \\\n> -I /home/saubin/src/tensorflow/ \\\n> -I /home/saubin/src/tensorflow/bazel-genfiles/ \\\n> -I /home/saubin/src/tensorflow/third_party/ \\\n> -g -Wall -D_DEBUG -Wshadow -Wno-sign-compare -w \\\n> -L /home/saubin/src/tensorflow/bazel-bin/tensorflow/ \\\n> `pkg-config --cflags --libs protobuf` -l tensorflow_cc basic.cc\n>\n> To compile:\n>\n> #include <tensorflow/core/public/session.h>\n> #include <tensorflow/core/platform/env.h>\n>\n> #include <iostream>\n> #include <chrono>\n> #include <fstream>\n> #include <string>\n> #include <vector>\n> #include <sstream>\n>\n> using namespace std;\n> using namespace chrono;\n> using namespace tensorflow;\n>\n> int main(int argc, char* argv[]) {\n>\n>   // Initialize a tensorflow session\n>   cout << \"start initalize session\" << \"\\n\";\n>   Session* session;\n>   Status status = NewSession(SessionOptions(), &session);\n>   if (!status.ok()) {\n>     cout << status.ToString() << \"\\n\";\n>     return 1;\n>   }\n> }\n>\n> But this gives me the error:\n>\n> In file included from /home/saubin/src/tensorflow/tensorflow/core/framework/tensor.h:19:0,\n>                  from /home/saubin/src/tensorflow/tensorflow/core/public/session.h:23,\n>                  from basic.cc:1:\n> /home/saubin/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:42: fatal error: unsupported/Eigen/CXX11/Tensor: No such file or directory\n> compilation terminated.\n>\n> I just thought I would throw in my own experience struggling with this\n> problem to help whoever has enough C++ knowledge to make this work.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/2412#issuecomment-303640207>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAePJ_QWZyv8oHIq7r503GQuP0eovPemks5r89udgaJpZM4Ig4U3>\n> .\n>\n", "@lababidi, any chance that you encountered the following error?\r\n\r\n```\r\n/usr/local/include/tf/tensorflow/core/public/session.h:22:48: fatal error: tensorflow/core/framework/graph.pb.h: No such file or directory\r\n #include \"tensorflow/core/framework/graph.pb.h\"\r\n```\r\nI know that this is a generated file, and I could locate it undre the bazel-genfiles folder, but I still got the error. When including the genfiles folder (with -I) I get a bunch of different errors.\r\n(and thanks for sharing the compilation steps!)", "@lirchi This comment might help. https://github.com/tensorflow/tensorflow/issues/1890#issuecomment-295745418", "@Seanny123 I followed your steps , but I get these errors:\r\nroot@ubuntu-Lenovo-Product:/home/ubuntu/tf# g++ -std=c++11 -o tf_basic \\\r\n> -I /home/ubuntu/tf/tensorflow-r1.1/ \\\r\n> -I /home/ubuntu/tf/tensorflow-r1.1/bazel-genfiles/ \\\r\n> -I /home/ubuntu/tf/tensorflow-r1.1/third_party/ \\\r\n> -I /usr/include/eigen3/ \\\r\n> -g -Wall -D_DEBUG -Wshadow -Wno-sign-compare -w \\\r\n> -L /home/ubuntu/tf/tensorflow-r1.1/bazel-bin/tensorflow/ \\\r\n> `pkg-config --cflags --libs protobuf` -ltensorflow_cc basic.cc\r\n/tmp/ccpDZzze.o: In function `main':\r\n/home/ubuntu/tf/basic.cc:20: undefined reference to `tensorflow::SessionOptions::SessionOptions()'\r\n/home/ubuntu/tf/basic.cc:20: undefined reference to `tensorflow::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**)'\r\n/home/ubuntu/tf/basic.cc:22: undefined reference to `tensorflow::Status::ToString() const'\r\n/tmp/ccpDZzze.o: In function `tensorflow::core::RefCounted::~RefCounted()':\r\n/home/ubuntu/tf/tensorflow-r1.1/tensorflow/core/lib/core/refcount.h:79: undefined reference to `tensorflow::internal::LogMessageFatal::LogMessageFatal(char const*, int)'\r\n/home/ubuntu/tf/tensorflow-r1.1/tensorflow/core/lib/core/refcount.h:79: undefined reference to `tensorflow::internal::LogMessageFatal::~LogMessageFatal()'\r\n/tmp/ccpDZzze.o: In function `tensorflow::SessionOptions::~SessionOptions()':\r\n/home/ubuntu/tf/tensorflow-r1.1/tensorflow/core/public/session_options.h:28: undefined reference to `tensorflow::ConfigProto::~ConfigProto()'\r\n/tmp/ccpDZzze.o: In function `std::string* tensorflow::internal::MakeCheckOpString<long, int>(long const&, int const&, char const*)':\r\n/home/ubuntu/tf/tensorflow-r1.1/tensorflow/core/platform/default/logging.h:184: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::CheckOpMessageBuilder(char const*)'\r\n/home/ubuntu/tf/tensorflow-r1.1/tensorflow/core/platform/default/logging.h:186: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::ForVar2()'\r\n/home/ubuntu/tf/tensorflow-r1.1/tensorflow/core/platform/default/logging.h:187: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString()'\r\n/home/ubuntu/tf/tensorflow-r1.1/tensorflow/core/platform/default/logging.h:187: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder()'\r\n/home/ubuntu/tf/tensorflow-r1.1/tensorflow/core/platform/default/logging.h:187: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder()'\r\ncollect2: error: ld returned 1 exit status\r\n\r\nmany undefined errors , anybody knows why?", "Hi everyone. I have prepared a project that automatically downloads, builds and installs the TensorFlow C++ library, including all the necessary headers and dependencies. Furthermore, it installs a CMake interface library you can easily link against to automatically configure your compiler.\r\n\r\nhttps://github.com/FloopCZ/tensorflow_cc\r\n\r\nThe project also contains an example and it is tested on a clean Ubuntu 16.04 through Docker ([Dockerfile](https://github.com/FloopCZ/tensorflow_cc/blob/master/Dockerfile) is a part of the project). I also have it working on Arch.\r\n\r\nFuture work (that would be great if some experienced tensorflower would do):\r\n- forge it into a contrib project\r\n- add support for making packages for widely used OSes (.deb package, .rpm package, [AUR](https://aur.archlinux.org/) package, ...; this requires disabling the `-march=native` flag)\r\n- and finally, forge it into an officially downloadable TensorFlow C++ library\r\n", "In case it's useful for someone, I've writtend up quite a crude approach for getting a C++ project on windows that depends on tensorflow.dll built with the contrib CMake in Release\r\nhttps://gist.github.com/Garoe/a6a82b75ea8277d12829eee81d6d2203\r\n\r\nI've tested it with the example C++ code in\r\nhttps://www.tensorflow.org/tutorials/image_recognition", "@lirchi You need to include the generated protobuf headers. If you built using the makefile, add the following to your include path:\r\n\r\n    -Itensorflow/tensorflow/contrib/makefile/gen/proto/", "@Garoe, what version of Visual Studio and what MSBuild target have you used in your initial build? I keep getting \"compiler is out of heap space\" error when I compile ALL_BUILD.vcxproj or tf_tutorials_example_trainer.vcxproj target with CUDA support.", "@rivi, I've only compiled without GPU support on windows. My machine has 16GB of RAM, Visual Studio Community 2015 Update 3 (14.0.25720), and I set the initial target to Release. I found that the library buids fine regardless on the initial target, but some intermediate files like the symbol definition are overwriten if the target is changed from Visual Studio.", "Like @smarttafirt's post above, I'm getting the error:\r\n\r\n```\r\n/opt/clion-2017.1.2/bin/cmake/bin/cmake --build /home/hal9000/tf_as_shared_lib/cmake-build-debug --target tf_as_shared_lib -- -j 8\r\nScanning dependencies of target tf_as_shared_lib\r\n[ 50%] Building CXX object CMakeFiles/tf_as_shared_lib.dir/main.cpp.o\r\n[100%] Linking CXX executable tf_as_shared_lib\r\nCMakeFiles/tf_as_shared_lib.dir/main.cpp.o: In function `tensorflow::core::RefCounted::~RefCounted()':\r\n/home/hal9000/Sources/tensorflow/tensorflow/core/lib/core/refcount.h:79: undefined reference to `tensorflow::internal::LogMessageFatal::LogMessageFatal(char const*, int)'\r\n/home/hal9000/Sources/tensorflow/tensorflow/core/lib/core/refcount.h:79: undefined reference to `tensorflow::internal::LogMessageFatal::~LogMessageFatal()'\r\nCMakeFiles/tf_as_shared_lib.dir/main.cpp.o: In function `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >* tensorflow::internal::MakeCheckOpString<long, int>(long const&, int const&, char const*)':\r\n/home/hal9000/Sources/tensorflow/tensorflow/core/platform/default/logging.h:184: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::CheckOpMessageBuilder(char const*)'\r\n/home/hal9000/Sources/tensorflow/tensorflow/core/platform/default/logging.h:186: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::ForVar2()'\r\n/home/hal9000/Sources/tensorflow/tensorflow/core/platform/default/logging.h:187: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString[abi:cxx11]()'\r\n/home/hal9000/Sources/tensorflow/tensorflow/core/platform/default/logging.h:184: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder()'\r\n/home/hal9000/Sources/tensorflow/tensorflow/core/platform/default/logging.h:184: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder()'\r\ncollect2: error: ld returned 1 exit status\r\nCMakeFiles/tf_as_shared_lib.dir/build.make:94: recipe for target 'tf_as_shared_lib' failed\r\nmake[3]: *** [tf_as_shared_lib] Error 1\r\nCMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/tf_as_shared_lib.dir/all' failed\r\nmake[2]: *** [CMakeFiles/tf_as_shared_lib.dir/all] Error 2\r\nCMakeFiles/Makefile2:79: recipe for target 'CMakeFiles/tf_as_shared_lib.dir/rule' failed\r\nmake[1]: *** [CMakeFiles/tf_as_shared_lib.dir/rule] Error 2\r\nMakefile:118: recipe for target 'tf_as_shared_lib' failed\r\nmake: *** [tf_as_shared_lib] Error 2\r\n```", "@9thDimension  @smarttafirt I also have the same issues. Have you guys resolved it? Thanks.", "Followed the steps provided by @lababidi, but I keep getting a bunch of similar errors when I try to compile, and ends with: `fatal error: too many errors emitted, stopping now [-ferror-limit=]\r\n20 errors generated.`\r\n\r\n`In file included from loader.cc:1:\r\nIn file included from /usr/local/include/tf/tensorflow/core/public/session.h:22:\r\n/usr/local/include/tf/tensorflow/core/framework/device_attributes.pb.h:17:2: error: This file was generated by an older\r\n      version of protoc which is\r\n#error This file was generated by an older version of protoc which is\r\n ^\r\n/usr/local/include/tf/tensorflow/core/framework/device_attributes.pb.h:18:2: error: incompatible with your Protocol\r\n      Buffer headers. Please\r\n#error incompatible with your Protocol Buffer headers.  Please\r\n ^\r\n/usr/local/include/tf/tensorflow/core/framework/device_attributes.pb.h:19:2: error: regenerate this file with a newer\r\n      version of protoc.\r\n#error regenerate this file with a newer version of protoc.\r\n ^`\r\n\r\nThe protobuf version I installed using brew is 3.4.0 on macOS.\r\n\r\nEDIT:\r\n\r\nInstalled protobuf 3.1.0 instead as it was mentioned in the getting started documentation for tensorflow. I do not get as many errors, but there are still some that exists.\r\n\r\n`In file included from loader.cc:1:\r\nIn file included from /usr/local/include/tf/tensorflow/core/public/session.h:22:\r\n/usr/local/include/tf/tensorflow/core/framework/device_attributes.pb.h:12:2: error: This file was generated by a newer\r\n      version of protoc which is\r\n#error This file was generated by a newer version of protoc which is\r\n ^\r\n/usr/local/include/tf/tensorflow/core/framework/device_attributes.pb.h:13:2: error: incompatible with your Protocol\r\n      Buffer headers. Please update\r\n#error incompatible with your Protocol Buffer headers.  Please update\r\n ^\r\n/usr/local/include/tf/tensorflow/core/framework/device_attributes.pb.h:14:2: error: your headers.\r\n#error your headers.\r\n ^\r\n/usr/local/include/tf/tensorflow/core/framework/device_attributes.pb.h:25:10: fatal error: \r\n      'google/protobuf/generated_message_table_driven.h' file not found\r\n#include <google/protobuf/generated_message_table_driven.h>\r\n         ^\r\n4 errors generated.`\r\n\r\nCould someone let me know the correct version of protobuf that I am supposed to be using? Thanks.\r\n\r\nEDIT2: \r\n\r\nLooks like the version for protobuf is 3.3. However, there are still other errors. Not sure if it is still wrong version of protobuf or something.\r\n\r\n`In file included from example.cc:1:\r\nIn file included from /usr/local/include/tf/tensorflow/core/platform/env.h:24:\r\nIn file included from /usr/local/include/tf/tensorflow/core/lib/core/errors.h:19:\r\nIn file included from /usr/local/include/tf/tensorflow/core/lib/core/status.h:25:\r\nIn file included from /usr/local/include/tf/tensorflow/core/platform/logging.h:25:\r\n/usr/local/include/tf/tensorflow/core/platform/default/logging.h:333:33: error: invalid operands to binary expression\r\n      ('tensorflow::internal::LogMessageFatal' and 'string' (aka 'basic_string<char, char_traits<char>, allocator<char>\r\n      >'))\r\n    LogMessageFatal(file, line) << string(exprtext);\r\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^  ~~~~~~~~~~~~~~~~\r\nIn file included from example.cc:1:\r\nIn file included from /usr/local/include/tf/tensorflow/core/platform/env.h:24:\r\nIn file included from /usr/local/include/tf/tensorflow/core/lib/core/errors.h:19:\r\n/usr/local/include/tf/tensorflow/core/lib/core/status.h:53:36: error: use of overloaded operator '==' is ambiguous (with\r\n      operand types 'const std::unique_ptr<State>' and 'long')\r\n  bool ok() const { return (state_ == NULL); }\r\n                            ~~~~~~ ^  ~~~~\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/memory:3021:1: note: \r\n      candidate function [with _T1 = tensorflow::Status::State, _D1 =\r\n      std::__1::default_delete<tensorflow::Status::State>]\r\noperator==(const unique_ptr<_T1, _D1>& __x, nullptr_t) _NOEXCEPT\r\n^\r\n/usr/local/include/tf/tensorflow/core/lib/core/status.h:53:36: note: built-in candidate operator==(int, long)\r\n  bool ok() const { return (state_ == NULL); }\r\n `\r\n\r\nEDIT3:\r\n\r\nTurns out I had to include the -std flag for c++11. One last error now.. \r\n\r\n`In file included from example.cc:1:\r\nIn file included from /usr/local/include/tf/tensorflow/core/platform/env.h:30:\r\nIn file included from /usr/local/include/tf/tensorflow/core/platform/mutex.h:31:\r\n/usr/local/include/tf/tensorflow/core/platform/default/mutex.h:25:10: fatal error: 'nsync_cv.h' file not found\r\n#include \"nsync_cv.h\"`", "I'm getting this same error trying to build a custom operator on Ubuntu 14.04 using TF 1.3 GPU installed from pip. \r\n\r\nI'm following these instructions\r\n\r\nTF_INC=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_include())')\r\ng++ -std=c++11 -shared word2vec_ops.cc word2vec_kernels.cc -o word2vec_ops.so -fPIC -I $TF_INC -O2 -D_GLIBCXX_USE_CXX11_ABI=0\r\n\r\nfound here:\r\n\r\nhttps://github.com/tensorflow/models/tree/master/tutorials/embedding\r\n\r\nWould be really grateful if anyone has solved this ", "@avanish Have you figured out the issue? I found `nsync_cv.h` in the python site packages folder.\r\nOn my Mac it is in `~/anaconda2/envs/tensorflow/lib/python2.7/site-packages/external/nsync/public`, since I'm using Anaconda for python. Just add this folder to your include path and it should fix the issue.", "I'm having the same issue with TF plugin. Will [official TF plugin build instructions](https://www.tensorflow.org/extend/adding_an_op#compile_the_op_using_your_system_compiler_tensorflow_binary_installation) be updated to account for `nsync_cv.h`?", "` bazel build //tensorflow:libtensorflow.so` for C API.\r\nand \r\n`bazel build //tensorflow:libtensorflow_cc.so` for C++. ", "I found [tutorial](http://tuatini.me/building-tensorflow-as-a-standalone-project/) to  build TensorFlow 1.3.0 as a standalone project. After we build the project to be standalone project we only need regular g++ to compile our tensorflow project. I even shared [my own tensorflow standalone project on github](https://github.com/kikirizki/tensorflow-nobazel) ", "Hi,\r\n\r\nSince I need to be able to package the C/C++ libtensorflow as debian packages for development/deployment, I worked on adding bazel rules to build:\r\n- libtensorflow deb package that contains both c and cc CPU lib\r\n- libtensorflow-dev deb package that contains necessary headers to build and link against libtensorflow in C++\r\n\r\nYou can find the changes here(fork+branch against r1.3): https://github.com/tabmo/tensorflow/tree/issue2412\r\nI've tried to 'fit in', feedback is welcome since it's my first time tinkering with Bazel.\r\n@asimshankar , I tried to followup on your comment, let me know about guidance/changes. Depending on priorities, I can work on it If this can make its way as a contribution at some point as I have a need to maintain this on my side anyway.\r\nIt seems related to issue 720 as well but it would be a much larger scope: https://github.com/tensorflow/tensorflow/issues/720\r\n\r\nIf you are using bazel 0.6.0 (released very recently), specify --incompatible_disallow_set_constructor=false, since it breaks tensorflow build currently.\r\n\r\nto get only the lib do:\r\n$bazel build //tensorflow:libtensorflow_deb \r\n\r\nif you want the headers as well:\r\n\r\n$bazel build //tensorflow:libtensorflow-dev_deb\r\nNote: libtensorflow-dev debian package depends on libtensorflow being installed on the system as well, so you need both for development (the lib only can be used for deployment).\r\n\r\n", "Not official, but I provide C++ libraries and headers for use outside bazel, and build instructions too (mainly to be used with openFrameworks, but not limited to).\r\nhttps://github.com/memo/ofxMSATensorFlow\r\nhttps://github.com/memo/ofxMSATensorFlow/wiki/", "Hey @memo ;) Out of curiosity, have you had reports of linker issues with ofxMSATensorFlow on OS X with TF 1.3 and TF 1.4? Ive been getting the following linker issue in my Synopsis project which is basically your instructions the wiki you linked almost verbatim:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/12539\r\n\r\nTrying to isolate - but I cant quite deduce where my issues lies.", "Hey @vade, I haven't built on OSX since r1.0 I'm afraid. Linux only now.\r\n\r\nDid you update the headers? This is a bit of a dirty solution, but right now (I think because the C++ API is changing), you need to change headers for each lib (not always, but sometimes). So it might be that if you're using the 1.2 headers, it's not finding the implementations in the 1.3 lib?\r\n\r\nThis script updates the headers\r\nhttps://github.com/memo/ofxMSATensorFlow/blob/master/scripts/ubuntu/copy_headers.sh\r\n\r\nOtherwise I'm not sure what it might be.\r\n\r\n", "Memo- thanks. I had done that before, but had a different compiler error (Nsync issue as noted here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/12482\r\n\r\nBut the new fixes in that thread resolved the issue. Much appreciated!", "@memo sir does the source code https://github.com/memo/ofxMSATensorFlow.git has GPU support ?\r\nthx", "I've written a small guide for C++ CMake TensorFlow on Mac: https://medium.com/@TomPJacobs/c-tensorflow-a-journey-bdecbbdd0f65", "@vade \r\nglad to hear that that problem is sorted. Interestingly I just tried building 1.4 (I'd only gone as far as 1.3), and I too get the nsync error! I'm currently looking for a solution which doesn't involved modifying the src, but will resort to that if I have to :)\r\n\r\n@kikirizki \r\nthe source code itself is agnostic to GPU or CPU or platform. For tf1.3 I provide precompiled libraries for linux only with GPU support ([here](https://github.com/memo/ofxMSATensorFlow/releases)). If you wanted without GPU support, or for another platform (i.e. OSX) you can build the library yourself (following instructions [here](https://github.com/memo/ofxMSATensorFlow/wiki/Rebuilding-library-from-scratch-(for-advanced-users)) - or @tjacobs's guide also looks great!). Or if someone else builds the lib, I can include it in my repo for download. ", "Closing the issue. There's a lot of discussion here, but we already release shared objects which are usable without bazel on many platforms. Please file new specific issues if this is not enough.", "Hi @alextp Maybe a naive question: where do we get the released shared objects you mentioned? I can get `libtensorflow_framework.so` from my pip-installed tensorflow, but lack `tensorflow_cc`. ", "@prclibo You have to build tensorflow from source for that. \r\nClone the repo, run `./configure` and then `bazel build //tensorflow:libtensorflow_cc.so `", "For those who like Docker, the [tensorflow_cc](https://github.com/FloopCZ/tensorflow_cc) project provides images with prebuilt TensorFlow C++ API.", "I have success compile the libtensorflow_cc.so, and managed to complie my release version of classification so. However,  if I tried to compile my demo to use the so file in release mode, it failed with many undefined link errrors. If I tried in the default debug mode, I succeed. The log of errors is:\r\n\r\n`\r\n/home/eli/tensorflow/tensorflow-master/tensorflow/cc/pointnet/build/libclassification.so\uff1a\u5bf9\u2018tensorflow::Tensor::CheckTypeAndIsAligned(tensorflow::DataType) const\u2019\u672a\u5b9a\u4e49\u7684\u5f15\u7528\r\n/home/eli/tensorflow/tensorflow-master/tensorflow/cc/pointnet/build/libclassification.so\uff1a\u5bf9\u2018tensorflow::TensorShapeRep::DestructorOutOfLine()\u2019\u672a\u5b9a\u4e49\u7684\u5f15\u7528\r\n/home/eli/tensorflow/tensorflow-master/tensorflow/cc/pointnet/build/libclassification.so\uff1a\u5bf9\u2018tensorflow::operator<<(std::ostream&, tensorflow::Status const&)\u2019\u672a\u5b9a\u4e49\u7684\u5f15\u7528\r\n/home/eli/tensorflow/tensorflow-master/tensorflow/cc/pointnet/build/libclassification.so\uff1a\u5bf9\u2018tensorflow::ReadBinaryProto(tensorflow::Env*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, google::protobuf::MessageLite*)\u2019\u672a\u5b9a\u4e49\u7684\u5f15\u7528\r\n/home/eli/tensorflow/tensorflow-master/tensorflow/cc/pointnet/build/libclassification.so\uff1a\u5bf9\u2018tensorflow::TensorShapeBase<tensorflow::TensorShape>::TensorShapeBase()\u2019\u672a\u5b9a\u4e49\u7684\u5f15\u7528\r\n/home/eli/tensorflow/tensorflow-master/tensorflow/cc/pointnet/build/libclassification.so\uff1a\u5bf9\u2018tensorflow::internal::CheckOpMessageBuilder::NewString[abi:cxx11]()\u2019\u672a\u5b9a\u4e49\u7684\u5f15\u7528\r\n/home/eli/tensorflow/tensorflow-master/tensorflow/cc/pointnet/build/libclassification.so\uff1a\u5bf9\u2018tensorflow::internal::LogMessage::LogMessage(char const*, int, int)\u2019\u672a\u5b9a\u4e49\u7684\u5f15\u7528\r\n/home/eli/tensorflow/tensorflow-master/tensorflow/cc/pointnet/build/libclassification.so\uff1a\u5bf9\u2018tensorflow::strings::StrCat[abi:cxx11](tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)\u2019\u672a\u5b9a\u4e49\u7684\u5f15\u7528\r\n/home/eli/tensorflow/tensorflow-master/tensorflow/cc/pointnet/build/libclassification.so\uff1a\u5bf9\u2018tensorflow::TensorShapeRep::SlowCopyFrom(tensorflow::TensorShapeRep const&)\u2019\u672a\u5b9a\u4e49\u7684\u5f15\u7528\r\n/home/eli/tensorflow/tensorflow-master/tensorflow/cc/pointnet/build/libclassification.so\uff1a\u5bf9\u2018tensorflow::ConfigProto::~ConfigProto()\u2019\u672a\u5b9a\u4e49\u7684\u5f15\u7528\r\n/home/eli/tensorflow/tensorflow-master/tensorflow/cc/pointnet/build/libclassification.so\uff1a\u5bf9\u2018tensorflow::NewSession(tensorflow::SessionOptions const&)\u2019\u672a\u5b9a\u4e49\u7684\u5f15\u7528\r\n/home/eli/tensorflow/tensorflow-master/tensorflow/cc/pointnet/build/libclassification.so\uff1a\u5bf9\u2018tensorflow::Env::Default()\u2019\u672a\u5b9a\u4e49\u7684\u5f15\u7528\r\n/home/eli/tensorflow/tensorflow-master/tensorflow/cc/pointnet/build/libclassification.so\uff1a\u5bf9\u2018tensorflow::internal::LogMessageFatal::~LogMessageFatal()\u2019\u672a\u5b9a\u4e49\u7684\u5f15\u7528\r\n/home/eli/tensorflow/tensorflow-master/tensorflow/cc/pointnet/build/libclassification.so\uff1a\u5bf9\u2018tensorflow::internal::CheckOpMessageBuilder::CheckOpMessageBuilder(char const*)\u2019\u672a\u5b9a\u4e49\u7684\u5f15\u7528\r\n/home/eli/tensorflow/tensorflow-master/tensorflow/cc/pointnet/build/libclassification.so\uff1a\u5bf9\u2018tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder()\u2019\u672a\u5b9a\u4e49\u7684\u5f15\u7528\r\n/home/eli/tensorflow/tensorflow-master/tensorflow/cc/pointnet/build/libclassification.so\uff1a\u5bf9\u2018tensorflow::Tensor::CheckIsAlignedAndSingleElement() const\u2019\u672a\u5b9a\u4e49\u7684\u5f15\u7528\r\n/home/eli/tensorflow/tensorflow-master/tensorflow/cc/pointnet/build/libclassification.so\uff1a\u5bf9\u2018tensorflow::SessionOptions::SessionOptions()\u2019\u672a\u5b9a\u4e49\u7684\u5f15\u7528\r\n/home/eli/tensorflow/tensorflow-master/tensorflow/cc/pointnet/build/libclassification.so\uff1a\u5bf9\u2018tensorflow::Tensor::~Tensor()\u2019\u672a\u5b9a\u4e49\u7684\u5f15\u7528\r\n/home/eli/tensorflow/tensorflow-master/tensorflow/cc/pointnet/build/libclassification.so\uff1a\u5bf9\u2018tensorflow::TensorShapeBase<tensorflow::TensorShape>::TensorShapeBase(std::initializer_list<long long>)\u2019\u672a\u5b9a\u4e49\u7684\u5f15\u7528\r\n/home/eli/tensorflow/tensorflow-master/tensorflow/cc/pointnet/build/libclassification.so\uff1a\u5bf9\u2018tensorflow::internal::LogMessage::~LogMessage()\u2019\u672a\u5b9a\u4e49\u7684\u5f15\u7528\r\n/home/eli/tensorflow/tensorflow-master/tensorflow/cc/pointnet/build/libclassification.so\uff1a\u5bf9\u2018tensorflow::Tensor::Tensor(tensorflow::DataType, tensorflow::TensorShape const&)\u2019\u672a\u5b9a\u4e49\u7684\u5f15\u7528\r\n/home/eli/tensorflow/tensorflow-master/tensorflow/cc/pointnet/build/libclassification.so\uff1a\u5bf9\u2018tensorflow::GraphDef::~GraphDef()\u2019\u672a\u5b9a\u4e49\u7684\u5f15\u7528\r\n/home/eli/tensorflow/tensorflow-master/tensorflow/cc/pointnet/build/libclassification.so\uff1a\u5bf9\u2018tensorflow::GraphDef::GraphDef()\u2019\u672a\u5b9a\u4e49\u7684\u5f15\u7528\r\n/home/eli/tensorflow/tensorflow-master/tensorflow/cc/pointnet/build/libclassification.so\uff1a\u5bf9\u2018tensorflow::port::InitMain(char const*, int*, char***)\u2019\u672a\u5b9a\u4e49\u7684\u5f15\u7528\r\n/home/eli/tensorflow/tensorflow-master/tensorflow/cc/pointnet/build/libclassification.so\uff1a\u5bf9\u2018tensorflow::internal::LogMessageFatal::LogMessageFatal(char const*, int)\u2019\u672a\u5b9a\u4e49\u7684\u5f15\u7528\r\n/home/eli/tensorflow/tensorflow-master/tensorflow/cc/pointnet/build/libclassification.so\uff1a\u5bf9\u2018tensorflow::internal::CheckOpMessageBuilder::ForVar2()\u2019\u672a\u5b9a\u4e49\u7684\u5f15\u7528\r\n/home/eli/tensorflow/tensorflow-master/tensorflow/cc/pointnet/build/libclassification.so\uff1a\u5bf9\u2018tensorflow::Status::Status(tensorflow::error::Code, tensorflow::StringPiece)\u2019\u672a\u5b9a\u4e49\u7684\u5f15\u7528\r\n`\r\nAny help~", "I can also recommend [FloopCZ's project](https://github.com/tensorflow/tensorflow/issues/2412#issuecomment-381959842), especially if you want to use Tensorflow inference with GPU. Those who wants a packaged version, I packaged both the C and C++ API in [my project](https://github.com/kecsap/tensorflow_cpp_packaging) and I release [Debian packages](https://github.com/kecsap/tensorflow_cpp_packaging/releases) time to time. The disadvantage of my project that it is based on contrib/makefiles therefore it is CPU-only.", "@kikirizki\r\nCan you kindly write a short tutorial to show how to use your code?", "Guys i tried to build libtensorflow.so  using bazel and tried to import it in my qt-based project but getting the following error.\r\neferenced in function \"public: __cdecl tensorflow::SessionOptions::~SessionOptions(void)\" (??1SessionOptions@tensorflow@@QEAA@XZ)\r\nmain.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::SessionOptions::SessionOptions(void)\" (??0SessionOptions@tensorflow@@QEAA@XZ) referenced in function main\r\nmain.obj : error LNK2019: unresolved external symbol \"class tensorflow::Status __cdecl tensorflow::NewSession(struct tensorflow::SessionOptions const &,class tensorflow::Session * *)\" (?NewSession@tensorflow@@YA?AVStatus@1@AEBUSessionOptions@1@PEAPEAVSession@1@@Z) referenced in function main\r\ndebug\\TensorFlowExp1.exe : fatal error LNK1120: 3 unresolved externals.\r\nBeside i also tried to run the pre-built windows binary and end up getting the same error.\r\n", "If you guys have same demand on Windows, I have asked a [question](https://stackoverflow.com/questions/53809398/how-to-program-with-c-api-library-on-windows-using-bazel) which may help you to some extent.", "@guikarist : Ok :)", "I've just published my old code and some prebuilt stuff here: https://github.com/hluu11/SimpleTF-CPP\r\nDidn't know people need it", "\r\n> Yes, that is what I need. It would be nice if this was documented somewhere too!\r\n\r\nHey all, visitor from the future here who also needed/needs exactly this build rule. Looks like this is still only documented in this issue---I'd be happy to put a note in the docs on building from source about this, but I haven't contributed to TensorFlow before*, so perhaps if somebody who's a known contributor could add that in, that would be awesome.\r\n\r\n*If this isn't likely to be an issue, I can do that", "Doc PRs are super easy for us to approve, and might take you just as long\nto do that as to write the issue in the first place.\n\nThanks for bringing this up!\n\nOn Wed, Jun 3, 2020 at 10:14 AM Zane Jakobs <notifications@github.com>\nwrote:\n\n> The \"//tensorflow:libtensorflow.so\" bazel target enables you to build a\n> standalone TensorFlow library that you can dynamically link into your other\n> project. Would that be sufficient?\n>\n> Visitor from the future here --\n>\n> Yes, that is what I need. It would be nice if this was documented\n> somewhere too!\n>\n> Hey all, visitor from the future here who also needed/needs exactly this\n> build rule. Looks like this is still only documented in this issue---I'd be\n> happy to put a note in the docs on building from source about this, but I\n> haven't contributed to TensorFlow before*, so perhaps if somebody who's a\n> known contributor could add that in, that would be awesome.\n>\n> *If this isn't likely to be an issue, I can do that\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/2412#issuecomment-638334308>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRIVNWGV6EB7XGIWGODRU2AGBANCNFSM4CEDQU3Q>\n> .\n>\n\n\n-- \n - Alex\n"]}, {"number": 2411, "title": "Android - Binary XML file line #*: Error inflating class com.android.internal.widget.ActionBarView", "body": "I got this issue when trying to test tensorflow demo on android device.\n\n```\n05-18 10:25:44.494: E/AndroidRuntime(29965): FATAL EXCEPTION: main\n05-18 10:25:44.494: E/AndroidRuntime(29965): java.lang.RuntimeException: Unable to start activity ComponentInfo{org.tensorflow.demo/org.tensorflow.demo.CameraActivity}: android.view.InflateException: Binary XML file line #39: Error inflating class com.android.internal.widget.ActionBarView\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2214)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2264)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at android.app.ActivityThread.access$600(ActivityThread.java:144)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1259)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at android.os.Handler.dispatchMessage(Handler.java:99)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at android.os.Looper.loop(Looper.java:137)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at android.app.ActivityThread.main(ActivityThread.java:5136)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at java.lang.reflect.Method.invokeNative(Native Method)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at java.lang.reflect.Method.invoke(Method.java:525)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:737)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:553)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at dalvik.system.NativeStart.main(Native Method)\n05-18 10:25:44.494: E/AndroidRuntime(29965): Caused by: android.view.InflateException: Binary XML file line #39: Error inflating class com.android.internal.widget.ActionBarView\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at android.view.LayoutInflater.createView(LayoutInflater.java:620)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at android.view.LayoutInflater.createViewFromTag(LayoutInflater.java:696)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at android.view.LayoutInflater.rInflate(LayoutInflater.java:755)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at android.view.LayoutInflater.rInflate(LayoutInflater.java:758)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at android.view.LayoutInflater.rInflate(LayoutInflater.java:758)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at android.view.LayoutInflater.inflate(LayoutInflater.java:492)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at android.view.LayoutInflater.inflate(LayoutInflater.java:397)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at android.view.LayoutInflater.inflate(LayoutInflater.java:353)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at com.android.internal.policy.impl.PhoneWindow.generateLayout(PhoneWindow.java:2825)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at com.android.internal.policy.impl.PhoneWindow.installDecor(PhoneWindow.java:2888)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at com.android.internal.policy.impl.PhoneWindow.setContentView(PhoneWindow.java:264)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at android.app.Activity.setContentView(Activity.java:1895)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at org.tensorflow.demo.CameraActivity.onCreate(CameraActivity.java:29)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at android.app.Activity.performCreate(Activity.java:5133)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1087)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2178)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    ... 11 more\n05-18 10:25:44.494: E/AndroidRuntime(29965): Caused by: java.lang.reflect.InvocationTargetException\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at java.lang.reflect.Constructor.constructNative(Native Method)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at java.lang.reflect.Constructor.newInstance(Constructor.java:417)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at android.view.LayoutInflater.createView(LayoutInflater.java:594)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    ... 26 more\n05-18 10:25:44.494: E/AndroidRuntime(29965): Caused by: android.view.InflateException: Binary XML file line #35: Error inflating class android.widget.TextView\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at android.view.LayoutInflater.createView(LayoutInflater.java:620)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at com.android.internal.policy.impl.PhoneLayoutInflater.onCreateView(PhoneLayoutInflater.java:56)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at android.view.LayoutInflater.onCreateView(LayoutInflater.java:669)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at android.view.LayoutInflater.createViewFromTag(LayoutInflater.java:694)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at android.view.LayoutInflater.rInflate(LayoutInflater.java:755)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at android.view.LayoutInflater.rInflate(LayoutInflater.java:758)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at android.view.LayoutInflater.inflate(LayoutInflater.java:492)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at android.view.LayoutInflater.inflate(LayoutInflater.java:397)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at com.android.internal.widget.ActionBarView.initTitle(ActionBarView.java:852)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at com.android.internal.widget.ActionBarView.setDisplayOptions(ActionBarView.java:670)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at com.android.internal.widget.ActionBarView.<init>(ActionBarView.java:254)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    ... 29 more\n05-18 10:25:44.494: E/AndroidRuntime(29965): Caused by: java.lang.reflect.InvocationTargetException\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at java.lang.reflect.Constructor.constructNative(Native Method)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at java.lang.reflect.Constructor.newInstance(Constructor.java:417)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at android.view.LayoutInflater.createView(LayoutInflater.java:594)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    ... 39 more\n05-18 10:25:44.494: E/AndroidRuntime(29965): Caused by: java.lang.UnsupportedOperationException: Can't convert to color: type=0x2\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at android.content.res.TypedArray.getColor(TypedArray.java:326)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at android.widget.TextView.<init>(TextView.java:673)\n05-18 10:25:44.494: E/AndroidRuntime(29965):    at android.widget.TextView.<init>(TextView.jav\n```\n\nCameraActivity.java\n\n```\npackage org.tensorflow.demo;\n\nimport android.app.Activity;\nimport android.os.Bundle;\nimport android.view.WindowManager;\n\npublic class CameraActivity extends Activity {\n  @Override\n  protected void onCreate(final Bundle savedInstanceState) {\n    super.onCreate(savedInstanceState);\n    getWindow().addFlags(WindowManager.LayoutParams.FLAG_KEEP_SCREEN_ON);\n\n    setContentView(R.layout.activity_camera);\n    if (null == savedInstanceState) {\n      getFragmentManager()\n          .beginTransaction()\n          .replace(R.id.container, CameraConnectionFragment.newInstance())\n          .commit();\n    }\n  }\n}\n```\n\nactivity_camera.xml\n\n```\n<?xml version=\"1.0\" encoding=\"utf-8\"?><!--\n<FrameLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    xmlns:tools=\"http://schemas.android.com/tools\"\n    android:id=\"@+id/container\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\"\n    android:background=\"#000\"\n    tools:context=\"org.tensorflow.demo.CameraActivity\" />\n```\n\nPlease advice. Thank you.\n", "comments": ["Hi, \nA few questions first:\n- have you made any changes to the code?\n- which SDK build tools version are you using?\n- how did you build the APK?\n- What Android API level is the device you're testing it on?\n\nMy initial guess after a little googling is that it has something to do with API levels, themes, or Android support libraries (which we don't use -- did your IDE try to add one?).\n", "1. I convert into gradle format. No major changes except gradle file.\n2. **23.0.2**\n3. Build using android studio and upload it into device (not simulator).\n4. **4.3**\n\nMy gradle setting\n\n```\napply plugin: 'com.android.application'\n\nandroid {\n    compileSdkVersion 21\n    buildToolsVersion \"23.0.2\"\n\n    defaultConfig {\n        applicationId \"org.tensorflow.demo\"\n        minSdkVersion 18\n        targetSdkVersion 18\n    }\n\n    buildTypes {\n        release {\n            minifyEnabled false\n            proguardFiles getDefaultProguardFile('proguard-android.txt'), 'proguard-rules.txt'\n        }\n    }\n}\n\ndependencies{\n    compile 'com.android.support:appcompat-v7:22.2.1'\n    compile 'com.android.support:design:22.2.0'\n}\n```\n", "Android 4.3 is API level 18. The demo requires 21 due to the camera2 api. I'm not sure why Gradle allowed you to compile it for API 18 when the AndroidManifest.xml specifies 21 (did you change that?), but your errors are almost definitely related to the appcompat dependencies shown at the bottom of the gradle settings you provided.\n\nAppcompat libraries should not be required for tensorflow, as we don't do any special theming or use any new UI elements in the demo. Try getting rid of those, and you'll also need to convert the camera code from the camera2 api to use android.hardware.Camera. You can see https://github.com/hamidb/tensorflow/tree/api20 for an example of this.\n", "I already use Android 5 but I got another error message\n\n```\n19406/org.tensorflow.demo E/AndroidRuntime: FATAL EXCEPTION: main\n                                                                     Process: org.tensorflow.demo, PID: 19406\n                                                                     java.lang.UnsatisfiedLinkError: com.android.tools.fd.runtime.IncrementalClassLoader$DelegateClassLoader[DexPathList[[dex file \"/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_9-classes.dex\", dex file \"/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_8-classes.dex\", dex file \"/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_7-classes.dex\", dex file \"/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_6-classes.dex\", dex file \"/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_5-classes.dex\", dex file \"/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_4-classes.dex\", dex file \"/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_3-classes.dex\", dex file \"/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_2-classes.dex\", dex file \"/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_1-classes.dex\", dex file \"/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_0-classes.dex\"],nativeLibraryDirectories=[/vendor/lib, /system/lib, /vendor/lib, /system/lib]]] couldn't find \"libtensorflow_demo.so\"\n                                                                         at java.lang.Runtime.loadLibrary(Runtime.java:366)\n                                                                         at java.lang.System.loadLibrary(System.java:989)\n                                                                         at org.tensorflow.demo.TensorflowClassifier.<clinit>(TensorflowClassifier.java:47)\n                                                                         at org.tensorflow.demo.TensorflowImageListener.<init>(TensorflowImageListener.java:56)\n                                                                         at org.tensorflow.demo.CameraConnectionFragment.<init>(CameraConnectionFragment.java:445)\n                                                                         at org.tensorflow.demo.CameraConnectionFragment.newInstance(CameraConnectionFragment.java:266)\n                                                                         at org.tensorflow.demo.CameraActivity.onCreate(CameraActivity.java:33)\n                                                                         at android.app.Activity.performCreate(Activity.java:6289)\n                                                                         at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1119)\n                                                                         at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2646)\n                                                                         at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2758)\n                                                                         at android.app.ActivityThread.access$900(ActivityThread.java:177)\n                                                                         at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1448)\n                                                                         at android.os.Handler.dispatchMessage(Handler.java:102)\n                                                                         at android.os.Looper.loop(Looper.java:145)\n                                                                         at android.app.ActivityThread.main(ActivityThread.java:5942)\n                                                                         at java.lang.reflect.Method.invoke(Native Method)\n                                                                         at java.lang.reflect.Method.invoke(Method.java:372)\n                                                                         at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:1389)\n                                                                         at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1184)\n```\n", "I take it you were able to get past the original error? If so, was the fix removing the dependencies or something else?\n", "Closing due to inactivity, please reopen if you're still experiencing the issue.\n"]}, {"number": 2410, "title": "Possible typo in doc of constant_initializer", "body": "In the [doc](https://www.tensorflow.org/versions/master/api_docs/python/state_ops.html#constant_initializer) of `constant_initializer`, it says that the argument `value` should be:\n\n> value: A Python scalar. All elements of the initialized variable will be set to this value.\n\nBut in fact, it can take array-like argument and initialize variables properly with it. So I guess it's a mistake in the doc? It's kind of misleading for beginners.\n\nThanks!\n", "comments": ["The documentation is intentionally vague on this point. Currently, you can pass any array-like argument that is supported by `tf.constant()`, but this is an implementation detail, and it is unreliable since `constant_initializer()` can be used to create variables of different shapes. Only a scalar argument has a well-defined meaning (fill a tensor of the given shape with that scalar value) for all possible shapes.\n", "Got it, thanks.\n\nBest Regards,\nLifu Huang\n\n2016-05-18 14:37 GMT+08:00 Derek Murray notifications@github.com:\n\n> Closed #2410 https://github.com/tensorflow/tensorflow/issues/2410.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2410#event-663975886\n"]}, {"number": 2409, "title": "Testing new Eigen version in isolation", "body": "Change: 122462177\n", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "@tensorflow-jenkins test this please.\n"]}, {"number": 2408, "title": "Branch 122541517", "body": "To sync @nsthorat\n\ntest this please\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "The most likely culprit for the failure is https://github.com/tensorflow/tensorflow/pull/2408/commits/d0fa481d59228e39ee4e76d5191633cc7e9d1c29 (since the other changes seem to be Python only, and the failing test itself is unmodified). There were 3 days' lag between the Jenkins presubmit test for that change and its submission, so that could explain the breakage. Might be worth trying to roll it back?\n", "Looks like https://github.com/tensorflow/tensorflow/pull/2408/commits/d0fa481d59228e39ee4e76d5191633cc7e9d1c29 works in isolation, so there goes that theory. Let's run the tests again, in case it was a flake.\n", "@tensorflow-jenkins test this please.\n", "@vrv, should be ready to merge. The cmake build failure is pre-existing.\n", "OK, looks like the test failure was a flake :). I'll go ahead and merge this one, thanks!\n", "Merged in https://github.com/tensorflow/tensorflow/commit/b54b59e0e65861a624ff5273308565487c8ac25e.\n"]}, {"number": 2407, "title": "TF learn has a problem with discontinuous categorical outputs", "body": "I have a `train.csv` which looks like:\n\n```\nv1,v2,v3\n5,4,10\n1,2,5\n4,5,20\n1,11,50\n1,12,20\n```\n\nand `test.csv`:\n\n```\nv1,v2\n4,5\n2,3\n2,4\n5,6\n```\n\nThe idea is: the output variable (`v3`) can take four values:  `5,10,20,50` and nothing more. It is classification task.\n\nThen code:\n\n``` python\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.contrib import learn\n\nprint (\"Reading data\")\n\ndf_train = pd.read_csv(\"train.csv\")\ndf_test = pd.read_csv(\"test.csv\")\n\nprint (\"Dividing to train and test set\")\n\ny_train = df_train ['v3']\nX_train = df_train [['v1','v2']]\n\nX_test = df_test[['v1','v2']]\n\nclassifier_1 = learn.TensorFlowDNNClassifier(\n     hidden_units=[20, 10], \n     n_classes=4, \n     batch_size=128, \n     steps=500, \n     learning_rate=0.05)\n\n# fit\nprint (\"Fitting\")\nclassifier_1.fit(X_train, y_train)\n```\n\nThen I have a problem:\n\n```\nTraceback (most recent call last):\n  File \"sample.py\", line 26, in <module>\n    classifier_1.fit(X_train, y_train)\n  File \"/Users/qdang/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/base.py\", line 254, in fit\n    feed_params_fn=self._data_feeder.get_feed_params)\n  File \"/Users/qdang/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/trainer.py\", line 49, in train\n    feed_dict = feed_dict_fn()\n  File \"/Users/qdang/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/io/data_feeder.py\", line 281, in _feed_dict_fn\n    out.itemset((i, self.y[sample]), 1.0)\nIndexError: index 5 is out of bounds for axis 1 with size 4\n```\n\nI think the issue is because the absolute value of output value (`5`) is larger than the number of classes (`4`)\n", "comments": ["@vinhqdang you need to encode your `y_train` values to `[0, 1, 2, 3]` classes.\n\n``` python\ny_train = pd.factorize(df_train['v3'])[0]\n```\n", "Hello,\n\nAfterwards, how could i convert the predicting outputs back to the normal one.\n\nFor example, after training a model, and use the model to predict a case, the output is 1.\n\nOf course, 1 is not correct, I need to convert it back to [5,10,20,50] before.\n", "@vinhqdang you'll just save your map somewhere\n", "Hi @terrytangyuan yeah, as discussed in gitter, it is a valid workaround and actually I am using it, but it seems not (very) official way.\n", "@vinhqdang `pd.factorize` return a tuple; the encoded values and the index/labels.\n\nif you need to know what 1 means you just need to save your labels as @terrytangyuan suggested\n\n``` python\ny_train, label_index = pd.factorize(df_train['v3'])\n# label_index[1] would return the original value encoded to 1\n```\n\nIn your case it would:\n\n```\nlabel_index[0] == 10\nlabel_index[1] == 5\nlabel_index[2] == 20\nlabel_index[3] == 50\n```\n", "This kind of question should be asked on StackOverflow, since it is a question about how to use TensorFlow rather than a bug in TensorFlow itself.\n"]}, {"number": 2406, "title": "TensorFlow grpc tensorflow server build on CentOS 7.2", "body": "### Environment info\n\nOperating System:\nLinux 3.10.0-327.13.1.el7.x86_64\nCentOS Linux release 7.2.1511 (Core)\n\nInstalled version of CUDA and cuDNN: \nCUDA is 7.5 and cuDNN is 4.0.7\nls -l /usr/local/cuda/lib64/libcud*\n-rw-r--r-- 1 root root   322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 May 11 22:38 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root       19 May 11 22:38 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root   383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root   720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a\nlrwxrwxrwx 1 root root       13 May 12 14:47 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.4\nlrwxrwxrwx 1 root root       17 May 12 14:47 /usr/local/cuda/lib64/libcudnn.so.4 -> libcudnn.so.4.0.7\n-rwxr-xr-x 1 root root 61453024 Feb  8 17:12 /usr/local/cuda/lib64/libcudnn.so.4.0.7\n-rw-r--r-- 1 root root 62025862 Feb  8 17:12 /usr/local/cuda/lib64/libcudnn_static.a\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n   python -c \"import tensorflow; print(tensorflow.**version**)\"\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n   0.8.0\n\nIf installed from sources, provide the commit hash:\nTrying to build from sources the distributed version of tensorflow\n### Steps to reproduce\n1. git clone --recurse-submodules https://github.com/tensorflow/tensorflow\n   The latest commit hash is 66edcda74c0f4823c9b9e3cddd17db9dfff63ad6\n2. installed bazel from binaries\n   sh bazel-0.2.2b-installer-linux-x86_64.sh\n3. building with bazel fails\n   bazel build -c opt --config=cuda --verbose_failures --spawn_strategy=standalone --genrule_strategy=standalone //tensorflow/core/distributed_runtime/rpc:grpc_tensorflow_server\n   WARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\n   INFO: Found 1 target...\n   ERROR: /root/tmp/tensorflow/google/protobuf/BUILD:29:1: undeclared inclusion(s) in rule '//google/protobuf:protobuf_lite':\n   this rule is missing dependency declarations for the following files included by 'google/protobuf/src/google/protobuf/stubs/once.cc':\n   '/include/c++/4.8.5/string'\n   '/include/c++/4.8.5/x86_64-redhat-linux/bits/c++config.h'\n   '/include/c++/4.8.5/x86_64-redhat-linux/bits/os_defines.h'\n   '/include/c++/4.8.5/x86_64-redhat-linux/bits/cpu_defines.h'\n   '/include/c++/4.8.5/bits/stringfwd.h'\n   '/include/c++/4.8.5/bits/memoryfwd.h'\n   '/include/c++/4.8.5/bits/char_traits.h'\n   '/include/c++/4.8.5/bits/stl_algobase.h'\n   '/include/c++/4.8.5/bits/functexcept.h'\n   '/include/c++/4.8.5/bits/exception_defines.h'\n   '/include/c++/4.8.5/bits/cpp_type_traits.h'\n   '/include/c++/4.8.5/ext/type_traits.h'\n   '/include/c++/4.8.5/ext/numeric_traits.h'\n   '/include/c++/4.8.5/bits/stl_pair.h'\n   '/include/c++/4.8.5/bits/move.h'\n   '/include/c++/4.8.5/bits/concept_check.h'\n   '/include/c++/4.8.5/type_traits'\n   '/include/c++/4.8.5/bits/stl_iterator_base_types.h'\n   '/include/c++/4.8.5/bits/stl_iterator_base_funcs.h'\n   '/include/c++/4.8.5/debug/debug.h'\n   '/include/c++/4.8.5/bits/stl_iterator.h'\n   '/include/c++/4.8.5/bits/postypes.h'\n   '/include/c++/4.8.5/cwchar'\n   '/lib/gcc/x86_64-redhat-linux/4.8.5/include/stdarg.h'\n   '/lib/gcc/x86_64-redhat-linux/4.8.5/include/stddef.h'\n   '/include/c++/4.8.5/cstdint'\n   '/lib/gcc/x86_64-redhat-linux/4.8.5/include/stdint.h'\n   '/include/c++/4.8.5/bits/allocator.h'\n   '/include/c++/4.8.5/x86_64-redhat-linux/bits/c++allocator.h'\n   '/include/c++/4.8.5/ext/new_allocator.h'\n   '/include/c++/4.8.5/new'\n   '/include/c++/4.8.5/exception'\n   '/include/c++/4.8.5/bits/atomic_lockfree_defines.h'\n   '/include/c++/4.8.5/bits/exception_ptr.h'\n   '/include/c++/4.8.5/bits/nested_exception.h'\n   '/include/c++/4.8.5/bits/localefwd.h'\n   '/include/c++/4.8.5/x86_64-redhat-linux/bits/c++locale.h'\n   '/include/c++/4.8.5/clocale'\n   '/include/c++/4.8.5/iosfwd'\n   '/include/c++/4.8.5/cctype'\n   '/include/c++/4.8.5/bits/ostream_insert.h'\n   '/include/c++/4.8.5/bits/cxxabi_forced.h'\n   '/include/c++/4.8.5/bits/stl_function.h'\n   '/include/c++/4.8.5/backward/binders.h'\n   '/include/c++/4.8.5/bits/range_access.h'\n   '/include/c++/4.8.5/bits/basic_string.h'\n   '/include/c++/4.8.5/ext/atomicity.h'\n   '/include/c++/4.8.5/x86_64-redhat-linux/bits/gthr.h'\n   '/include/c++/4.8.5/x86_64-redhat-linux/bits/gthr-default.h'\n   '/include/c++/4.8.5/x86_64-redhat-linux/bits/atomic_word.h'\n   '/include/c++/4.8.5/initializer_list'\n   '/include/c++/4.8.5/ext/string_conversions.h'\n   '/include/c++/4.8.5/cstdlib'\n   '/include/c++/4.8.5/cstdio'\n   '/include/c++/4.8.5/cerrno'\n   '/include/c++/4.8.5/bits/functional_hash.h'\n   '/include/c++/4.8.5/bits/hash_bytes.h'\n   '/include/c++/4.8.5/bits/basic_string.tcc'\n   '/include/c++/4.8.5/cstddef'\n   '/lib/gcc/x86_64-redhat-linux/4.8.5/include/limits.h'\n   '/lib/gcc/x86_64-redhat-linux/4.8.5/include/syslimits.h'\n   '/include/c++/4.8.5/utility'\n   '/include/c++/4.8.5/bits/stl_relops.h'.\n   Target //tensorflow/core/distributed_runtime/rpc:grpc_tensorflow_server failed to build\n   INFO: Elapsed time: 8.212s, Critical Path: 1.98s\n\nrpm -qa | grep gcc\ngcc-c++-4.8.5-4.el7.x86_64\nlibgcc-4.8.5-4.el7.x86_64\ngcc-4.8.5-4.el7.x86_64\nrpm -qa | grep glibc\nglibc-common-2.17-106.el7_2.4.x86_64\nglibc-devel-2.17-106.el7_2.4.x86_64\nglibc-2.17-106.el7_2.4.x86_64\nglibc-headers-2.17-106.el7_2.4.x86_64\n\nI am not sure what are the dependencies. I saw that glibc 2.17 should at least meet the minimum.\n### What have you tried?\n1. Nothing else yet. Trying to build bazel from source.\n\nAny thoughts?\n", "comments": ["This looks like a duplicate of #2109. Closing this so we can merge the discussion over there (but feel free to reopen if it looks like this is a different issue).\n"]}, {"number": 2405, "title": "HEAD compiles without error, but not completely? build_pip_package fails.", "body": "CentOS 6.7\nCuda 7.0\ncuDNN 4.0.7\n\nI resolve https://github.com/tensorflow/tensorflow/issues/2266 by hard coding the python2.7 path in `third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc`.\n\nAfter also resolving a similar problem with `swig`, it compiles without errors:\n\n```\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\nINFO: Elapsed time: 228.884s, Critical Path: 199.55s\n```\n\nBut it seems that `build_pip_package` was only partially (?) completed:\n\n```\nrdipiet2@jhu.edu@login-node03 tensorflow $ bazel-bin/tensorflow/tools/pip_package/build_pip_package testonly\nTue May 17 11:09:33 EDT 2016 : === Using tmpdir: /tmp/tmp.akIEI1WBAX\ncp: cannot stat `bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/tensorflow': No such file or directory\ncp: cannot stat `bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/external': No such file or directory\n```\n\nAny ideas? Could something be failing silently?\n", "comments": ["refer to `https://github.com/tensorflow/tensorflow/issues/2040`, they have solved this problem.\n\n`cp -r bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/__main__/* bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/`\n", "Thanks for replying @zszhong! Closing this as a duplicate of #2040.\n"]}, {"number": 2404, "title": "Updated installation instructions for conda.", "body": "This PR updates documentation of installation instructions for conda users.\n\nThe new version was tested so it seems that nobody has changed documentation of installation instructions.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for the update!\n"]}, {"number": 2403, "title": "\"Total error: nan\" for neural network model", "body": "Hi \n\nI have randomly generated exponential signal. I am training the network to predict y values depending on the random time constant (tau). I have added one hidden layer and used gardient descent for optimization. The code runs fine, but it doesnt print error properly. I get this - ('Total Error: ', nan)\n\nhere is my code-\n\n`import tensorflow as tf\n import numpy as np\n import random\n# input data\n\n lorange= 1\n hirange= 10\n amplitude= random.uniform(-10,10)\n t= 10\n random.seed()\n tau=random.uniform(lorange,hirange)\n\n def generate_data(randomsignal):\n       x= np.arange(t)\n       y= amplitude*np.exp(-x/tau)\n       return x, y\n #tensors for input data\n\n x_input= tf.placeholder(tf.float32, shape=(10,))# t=10\n y_input= tf.placeholder(tf.float32, shape=(10,))\n\n #use 10 neurons-- just one layer for now\n\n weights_1= tf.Variable(tf.truncated_normal([10,10], stddev= .1)) \n bias_1= tf.Variable(.1)\n\n #hidden output \n hidden_output= tf.nn.relu(tf.matmul(tf.reshape(x_input,[1,10]), weights_1) + bias_1)\n\n weights_2 = tf.Variable(tf.truncated_normal([10, 10], stddev=.1))\n bias_2= tf.Variable(.1)\n\n calculated_output = tf.nn.softmax(tf.matmul(hidden_output, weights_2) + \n\n bias_2)\n\n cross_entropy = tf.reduce_mean(y_input \\* tf.log(calculated_output))\n\n optimizer = tf.train.GradientDescentOptimizer(.5).minimize(cross_entropy)\n\n sess = tf.Session()\n #session\n sess.run(tf.initialize_all_variables())\n\n for i in range(1000):\n      x, y = generate_data(100)\n      sess.run(optimizer, feed_dict={x_input: x, y_input: y})\n\n error = tf.reduce_sum(tf.abs(calculated_output - y_input))\n\n x, y = generate_data(100)\n print(\"Total Error: \", sess.run(error, feed_dict={x_input: x, y_input: y}))`\n", "comments": ["`nan` can appear in many situations like, for example, division by zero.\nI guess in your case `calculated_output` produces zero, therefore a subsequent log produces `nan`.\n\nTo prevent this from happening you can, for example, clip the value like this:\n\n```\nclipped_output =  tf.clip_by_value(calculated_output, 1e-37, 1e+37)\n```\n", "This question comes up quite often on Stack Overflow, and @Remper has pointed out a good solution. You can also see a Stack Overflow answer about the same problem [here](http://stackoverflow.com/a/33713196/3574081).\n"]}, {"number": 2402, "title": "Problem with documentation regarding SparseTensors and TFRecord.", "body": "I can't seem to find how to store sparse tensors to TFRecord. GitHub issues is not really a place for such a question but documentation was really hard to understand. Could anybody help me with this? Maybe link to a code that uses VarLenFeature or something similiar?\n", "comments": ["I think the most realistic code for generating TFRecord files of Example protos is here: https://github.com/tensorflow/models/blob/master/inception/inception/data/build_imagenet_data.py\n\nHowever, it's not quite clear what you're trying to do. I'm closing this issue so we can move it to Stack Overflow.\n"]}, {"number": 2401, "title": "add complex64 support to tf.diag", "body": "#1783\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins, test this please!\n", "LGTM, thanks!\n"]}, {"number": 2400, "title": "modify grpc for local_repository", "body": "Due to network limited, I modify grpc for local_repository in workspace.bzl file,  as shown below:\n\n  #native.git_repository(\n  native.local_repository(\n    name = \"grpc\",\n    path = \"/home/jason/localDep/grpc/grpc\",\n    #commit = \"3d62fc6\",\n    #init_submodules = True,\n    #remote = \"https://github.com/grpc/grpc.git\",\n  )\nAnd when run  bazel build -c opt --config=cuda --verbose_failures --genrule_strategy=standalone //tensorflow/tools/pip_package:build_pip_package\n\nit will report:\n\nERROR: /root/.cache/bazel/_bazel_root/5f279d276e3315e6a8baf89d39adcaeb/external/grpc/BUILD:514:1: no such target '//external:nanopb': target 'nanopb' not declared in package 'external' defined by /home/jason/tensor_5_9/tensorflow/WORKSPACE and referenced by '@grpc//:grpc_unsecure'.\nERROR: Loading failed; build aborted.\nINFO: Elapsed time: 0.535s\n\nDoes anyone know why?\n", "comments": ["A tangle of problems.....\n", "It sounds like you have checked out the `master` branch of gRPC. Subsequent updates have added a new dependency on `nanopb`, which we haven't added to TensorFlow yet. The solution is to check out exactly the named commit `3d62fc6` in your local grpc repository.\n", "@mrry    yea, it works, thank you !\n"]}, {"number": 2399, "title": "Is it possible to add summaries to an existing event file ?", "body": "When I continue to train my model from last interrupted training, `tf.train.SummaryWriter` create a new event file.\nCould I add new summaries to my existing event file?  So I can visualize the whole learning run.\n\nP.S. What's the right way to suspend the training binary on GPU?   We share one GPU, and I always use  \"Ctrl+c\" \u2026\u2026  \n", "comments": ["The SummaryWriter API doesn't let you append to an existing file, but you can use TensorBoard to visualize multiple files in the same log directory.\n\nAssigning to @danmane in case there are any other tricks you could use.\n", "Correct, if you point TensorBoard to the directory containing event files, then creating a new event file within that directory will be fine, and tensorboard will combine the runs.\n\nN.b. you should use a clean directory for each separate training job.\n", "see \r\n\r\n> TensorBoard reads data from a full directory, and organizes it into the history of a single TensorFlow execution.\r\n\r\nat **Event Files & LogDirs: How TensorBoard loads the data** section in https://github.com/tensorflow/tensorboard/blob/master/README.md](url)", "@decentralion, all, but how can I ensure that plotting of events of a certain event file, in the scalar plots, starts at the right training step? After restarting from a checkpoint I assumed I could make Tensorboard show new values starting from the training step it left off before the training restart.\r\n\r\nShowing the horizontal axis in `relative` or `wall` mode is not a satisfactory solution to me: there is a big gap between the current run and the previous if there is a lot of time between the runs.\r\n\r\nIt seems such an obvious use case, I'm a bit surprised this hasn't been incorporated. Interested to hear what was the thinking not to enable this use case (if it's not possible).", "Whenever you add summaries, you also include the `global_step`, like this: `summary_writer.add_summary(summary, global_step)`.\r\nSo it's then just a matter of passing the right global_step values when you add your summaries. You can do this for example by keeping a non-trainable Tensorflow variable, which is going to be checkpointed and can be restored from the checkpoint together with all other savable variables. This 0D tensor is incremented at each training step if you pass it as part of the list of tensors to `sess.run`.\r\nSome helpful operations:\r\n```\r\nglobal_step_tensor = tf.Variable(0, trainable=False, name='global_step')\r\noutput = self.sess.run(global_step_tensor + tensors_to_run, feed_dict=feed_dict)\r\nglobal_step = output[0]\r\n```\r\nThe **real** problem is that Tensorboard doesn't let you pick your own colors for displaying curves corresponding to different event files, which means that if you restore a training session from a checkpoint, the different parts will be displayed with different colors. This can be very annoying, especially if you compare several models at the same time. "]}, {"number": 2398, "title": "imac install failed.", "body": "Installing collected packages: protobuf, wheel, numpy, tensorflow\n  Found existing installation: numpy 1.8.0rc1\n    DEPRECATION: Uninstalling a distutils installed project (numpy) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\n    Uninstalling numpy-1.8.0rc1:\nException:\nTraceback (most recent call last):\n  File \"/Library/Python/2.7/site-packages/pip-8.1.2-py2.7.egg/pip/basecommand.py\", line 215, in main\n    status = self.run(options, args)\n  File \"/Library/Python/2.7/site-packages/pip-8.1.2-py2.7.egg/pip/commands/install.py\", line 317, in run\n    prefix=options.prefix_path,\n  File \"/Library/Python/2.7/site-packages/pip-8.1.2-py2.7.egg/pip/req/req_set.py\", line 736, in install\n    requirement.uninstall(auto_confirm=True)\n  File \"/Library/Python/2.7/site-packages/pip-8.1.2-py2.7.egg/pip/req/req_install.py\", line 742, in uninstall\n    paths_to_remove.remove(auto_confirm)\n  File \"/Library/Python/2.7/site-packages/pip-8.1.2-py2.7.egg/pip/req/req_uninstall.py\", line 115, in remove\n    renames(path, new_path)\n  File \"/Library/Python/2.7/site-packages/pip-8.1.2-py2.7.egg/pip/utils/**init**.py\", line 267, in renames\n    shutil.move(old, new)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py\", line 302, in move\n    copy2(src, real_dst)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py\", line 131, in copy2\n    copystat(src, dst)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py\", line 103, in copystat\n    os.chflags(dst, st.st_flags)\nOSError: [Errno 1] Operation not permitted: '/tmp/pip-hgaRu9-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy-1.8.0rc1-py2.7.egg-info'\n", "comments": ["It looks like the `pip` is trying to remove an old version of a system library. To avoid version clashes, we recommend that you follow the [virtualenv installation instructions](https://www.tensorflow.org/versions/r0.8/get_started/os_setup.html#virtualenv-installation).\n", "I got the same error, and even with virtualenv, I have the same error too. Any idea how to work around? Thanks,\n", "One [suggestion from the FAQ](https://www.tensorflow.org/versions/r0.8/get_started/os_setup.html#mac-os-x-oserror-errno-1-operation-not-permitted) is to use the `--ignore-installed` flag when running `pip install`. Depending on the error message, you may need to pass `--ignore-installed six` or `--ignore-installed numpy` to ignore the appropriate package. \n", "@mrry it worked for me\n", "@mrry worked for by adding the --ignore-installed numpy, but then when I try to run the test example (https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#test-the-tensorflow-installation) and try to import the tf library I get this\r\n\r\n```\r\nRuntimeError: module compiled against API version 0xa but this version of numpy is 0x9\r\n```\r\n\r\nWhich is indeed what was done adding the --ignore option, ignoring the existing numpy version? I'm completely new to python, so you'll have to excuse any straight forward problems I can't see :).", "@coreation It sounds like your version of NumPy is older than the version that TensorFlow requires (I think it's 0.10 or 0.11). If you upgrade your local version of NumPy (which may be easier to do in a virtualenv), TensorFlow should work.", "@mrry ended up following the pip3 approach which worked very smoothly, thanks!"]}, {"number": 2397, "title": "Performance of Tensorflow distributed training is much slower than caffe multi-GPU training", "body": "I have trained inceptionv3 using tensorflow both on multi-GPU version and distributed version (two machine, four GPU each). Each GPU processes 32 images per iteration under both settings. However, the distributed training speed is twice as slow as the `caffe` multi-GPU version. I'm wondering how to improve the performance of distributed training. \n\n**Configuration:** \n\nTwo machine, both of them have totally same environment: CentOS 7, 4 GTX TITAN X GPUs, 32 Intel Xeon CPU E5-2630 v3 2.40GHz processors, and 131GB Memory. Network IO between machines is 125MB/s, `ping` delay is 0.1ms and local read speed is 1GB/s (RAID5). The distributed training code is the newest master branch [here](https://github.com/tensorflow/models/tree/master/inception). In distributed version, there are 4 `workers` on each machine, each work are assigned to 1 GPU and there is only one `ps` server started. I read data file from local disk. \n\nthe start script for each worker (8 workers in total) is\n\n```\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=2 \\\n--ps_hosts='10.10.102.28:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.28:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n```\n\n**Runtime logging:**\n\nThe training speed is \n\n```\nINFO:tensorflow:Worker 6: 2016-05-16 21:07:22.101672: step 390, loss = 8.15(24.0 examples/sec; 1.334  sec/batch)\nINFO:tensorflow:Worker 5: 2016-05-16 21:07:22.101666: step 390, loss = 8.10(24.0 examples/sec; 1.335  sec/batch)\nINFO:tensorflow:Worker 4: 2016-05-16 21:07:22.101768: step 390, loss = 8.11(24.0 examples/sec; 1.333  sec/batch)\nINFO:tensorflow:Worker 7: 2016-05-16 21:07:22.102245: step 390, loss = 8.03(24.1 examples/sec; 1.330  sec/batch)\n```\n\nThis speed is twice as slow as `caffe` multi-GPU training on one machine(ensure that experiments are under same settings, each GPU process 32 images per iteration). I used `nvidia-smi -l 1` to watch the GPU usage, and find that GPUs only busy in 25% running time. The `top` command print like this:\n\n```\n %Cpu(s):  2.4 us, 35.2 sy,  0.0 ni, 62.3 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\nKiB Mem : 13174038+total, 21064972 free, 18241616 used, 92433792 buff/cache\nKiB Swap: 16777212 total, 16724332 free,    52880 used. 88948128 avail Mem \n\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND     \n  685 peter+  20   0  0.191t 2.336g 453624 S 878.1  1.9  83:03.03 python      \n15102 peter+  20   0 13.715g 5.522g  41820 S 293.7  4.4 348:39.74 python      \n  682 peter+  20   0  0.192t 2.476g 453608 S  13.0  2.0  69:18.00 python      \n  683 peter+  20   0  0.193t 4.093g 978228 S  11.6  3.3  84:40.56 python      \n  684 peter+  20   0  0.191t 2.410g 453612 S  10.6  1.9  88:50.77 python \n```\n\n**What I have tried**\n- By modifying the `num_preprocess_threads` and the `num_readers`, I got the best performance `1.333  sec/batch`, when I set both variable to 1.\n- Modify the queue capacity is not helpful.\n- I believe the `bath_inputs()` is executed on `CPU:0`, because in `image_processing.py` line 132: `with tf.device('/cpu:0'):`\n- I refered to this [issue](https://github.com/tensorflow/models/issues/47) and wondering if the bottleneck is CPU/IO. \n\nHow to get a better performance ?\n", "comments": ["Since you have a Xeon CPU, it is less likely your CPU is the bottleneck, however, you have 8 workers but only 1 PS which is also very busy processing the input for the GPUs, that PS is likely to get congested and become the bottle neck. Two things you can try here:\n1. Run the TF multi-GPU version with 4 GPUs locally and see if and how much slower it will be compared to 1 GPU.\n2. Use more hosts from the network as the PS. At least make 2 PS (one on each machine) and see if and how much improvement it could get.\n\nLMK how it goes.\n", "I tested locally both with 1 GPU and 4 GPUs. 1 GPU version has the speed of 0.8 sec/batch, and 4 GPUs version has the speed of 1.3 sec/batch, same as distributed version.(with 2 machine, each has 4 GPU)\n", "This 4 GPUs is not the distributed setting, right? If so, that means the slow down is not related to the distributed setting then.\n\nI thought Xeon should be good enough to process images for 4 GPUs. Can you try to increase the threads for the preprocessing? What is the current number of threads?\n", "Yes, 4 GPU is not the distributed setting. OK, I know the point. I will adjust the Multi-GPU version until it is close to the one-GPU version.  _And, according to your experience, Mult-GPU version is just slightly slower than one-GPU version, right ?_\n", "It should be just slightly slower than 1 GPU version if the CPU is powerful\nenough and the connection to the 4 GPUs are also fast enough. People have\nreported that with one core i7 with 4 Titan X, the system cannot feed GPUs\nfast enough. Since you have a Xeon, maybe you can try increasing the\nthreads and see if it helps.\n\nOn Wed, May 18, 2016 at 9:59 PM, ZhuFengdaaa notifications@github.com\nwrote:\n\n> Yes, 4 GPU is not the distributed setting. OK, I know the point. I will\n> adjust the Multi-GPU version until it is close to the one-GPU version. _And,\n> according to your experience, Mult-GPU version is just slightly slower than\n> one-GPU version, right ?_\n> \n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2397#issuecomment-220227723\n", "Hello, i had the same problem while using distributed train.\n\n# **\\- Hardware**\n\n```\n4x(Tesla K80)  on GPU-1\n4x(Tesla K40m) on GPU-2\n40 threads of Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz\n96G KiB Mem : 98738064 total\nNetwork speed between two machine 100MB/s\nping gpu-1\n64 bytes from 192.168.1.100: icmp_seq=2 ttl=64 time=4.58 ms\n64 bytes from 192.168.1.100: icmp_seq=4 ttl=64 time=0.359 ms\n64 bytes from 192.168.1.100: icmp_seq=6 ttl=64 time=3.94 ms\n64 bytes from 192.168.1.100: icmp_seq=8 ttl=64 time=0.573 ms\n64 bytes from 192.168.1.100: icmp_seq=10 ttl=64 time=3.97 ms\n64 bytes from 192.168.1.100: icmp_seq=12 ttl=64 time=0.244 ms\n```\n\n# **\\- Script**\n\nds.sh\n\n```\nJOBNAME=$1\nWID=$2\nGPUID=$3\nif [ \"$JOBNAME\" == \"ps\" ]; then\n    CUDA_VISIBLE_DEVICES='' bazel-bin/inception/imagenet_distributed_train --batch_size=32 --data_dir=$HOME/imagenet-data --ps_hosts='gpu-1:2050' --worker_hosts='gpu-1:2055,gpu-1:2051,gpu-2:2052,gpu-2:2056' --job_name=''${JOBNAME}'' --task_id=${WID} --gpu_id=${GPUID} --train_dir=/tmp/imagenet_train_4core_dist2\nfi\nCUDA_VISIBLE_DEVICES=''${GPUID}'' bazel-bin/inception/imagenet_distributed_train --batch_size=32 --data_dir=$HOME/imagenet-data--ps_hosts='gpu-1:2050' --worker_hosts='gpu-1:2055,gpu-1:2051,gpu-2:2052,gpu-2:2056' --job_name=''${JOBNAME}'' --task_id=${WID} --gpu_id=${GPUID} --train_dir=/tmp/imagenet_train_4core_dist2`\n```\n\n# **\\- TEST**\n\n## 1. Single Core\n\n```\nnohup bazel-bin/inception/imagenet_train --num_gpus=1 --batch_size=32 --data_dir=$HOME/imagenet-data --train_dir=/tmp/imagenet_train_single > logs/train_single.log &\n 2016-06-30 12:23:25.858032: step 31810, loss = 8.61 (21.4 examples/sec; 1.495 sec/batch)\n 2016-06-30 12:23:40.958675: step 31820, loss = 7.83 (20.9 examples/sec; 1.528 sec/batch)\n 2016-06-30 12:23:55.986678: step 31830, loss = 8.29 (21.4 examples/sec; 1.494 sec/batch)\n 2016-06-30 12:24:11.043151: step 31840, loss = 7.73 (21.5 examples/sec; 1.487 sec/batch)\n 2016-06-30 12:24:26.349167: step 31850, loss = 7.46 (21.1 examples/sec; 1.513 sec/batch)\n 2016-06-30 12:24:41.373939: step 31860, loss = 7.86 (21.3 examples/sec; 1.500 sec/batch)\n 2016-06-30 12:24:56.400347: step 31870, loss = 8.06 (21.3 examples/sec; 1.504 sec/batch)\n 2016-06-30 12:25:11.848877: step 31880, loss = 7.72 (21.6 examples/sec; 1.484 sec/batch)\n 2016-06-30 12:25:26.909544: step 31890, loss = 7.66 (20.8 examples/sec; 1.541 sec/batch)\n 2016-06-30 12:25:42.093352: step 31900, loss = 8.14 (20.7 examples/sec; 1.546 sec/batch)\n```\n\n## 2. 2 Cores\n\n```\nnohup bazel-bin/inception/imagenet_train --num_gpus=2 --batch_size=64 --train_dir=/tmp/imagenet_train_2core --data_dir=/home/chenxiu/data1/ilsvrc2012 > logs/train_2core.log &\n\n2016-07-01 16:45:55.176167: step 360, loss = 12.96 (37.6 examples/sec; 1.700 sec/batch)\n2016-07-01 16:46:12.253974: step 370, loss = 12.97 (37.6 examples/sec; 1.704 sec/batch)\n2016-07-01 16:46:29.297706: step 380, loss = 13.07 (37.6 examples/sec; 1.701 sec/batch)\n2016-07-01 16:46:46.365005: step 390, loss = 12.96 (37.5 examples/sec; 1.705 sec/batch)\n2016-07-01 16:47:03.472910: step 400, loss = 13.05 (37.4 examples/sec; 1.711 sec/batch)\n2016-07-01 16:47:23.656588: step 410, loss = 13.04 (36.9 examples/sec; 1.734 sec/batch)\n2016-07-01 16:47:40.896221: step 420, loss = 13.00 (37.2 examples/sec; 1.719 sec/batch)\n2016-07-01 16:47:58.285889: step 430, loss = 12.96 (37.5 examples/sec; 1.707 sec/batch)\n2016-07-01 16:48:15.344814: step 440, loss = 12.97 (37.3 examples/sec; 1.716 sec/batch)\n2016-07-01 16:48:32.417732: step 450, loss = 13.04 (37.5 examples/sec; 1.708 sec/batch)\n```\n\n## 3. 4 Cores\n\n```\nnohup bazel-bin/inception/imagenet_train --num_gpus=4 --batch_size=128 --train_dir=/tmp/imagenet_train_4core --data_dir=/home/chenxiu/data1/ilsvrc2012 > logs/train_4core.log &\n2016-07-01 17:28:45.255619: step 640, loss = 12.69 (35.2 examples/sec; 3.634 sec/batch)\n2016-07-01 17:29:05.687976: step 650, loss = 12.63 (57.5 examples/sec; 2.228 sec/batch)\n2016-07-01 17:29:26.619168: step 660, loss = 12.54 (70.7 examples/sec; 1.810 sec/batch)\n2016-07-01 17:29:46.492567: step 670, loss = 12.68 (45.6 examples/sec; 2.808 sec/batch)\n2016-07-01 17:30:06.778141: step 680, loss = 12.73 (70.2 examples/sec; 1.823 sec/batch)\n2016-07-01 17:30:26.910362: step 690, loss = 12.64 (64.6 examples/sec; 1.982 sec/batch)\n2016-07-01 17:30:48.887897: step 700, loss = 12.51 (70.6 examples/sec; 1.813 sec/batch)\n2016-07-01 17:31:09.440310: step 710, loss = 12.59 (71.1 examples/sec; 1.799 sec/batch)\n2016-07-01 17:31:27.458959: step 720, loss = 12.60 (69.4 examples/sec; 1.846 sec/batch)\n2016-07-01 17:31:46.270831: step 730, loss = 12.70 (71.0 examples/sec; 1.802 sec/batch)\n```\n\n## 4.Distributed on GPU1/GPU5\n\nGPU-1\n\n```\nnohup sh ds.sh ps 0 0 > logs/ps.log & \nnohup sh ds.sh worker 0 7 > logs/worker0.log & \nnohup sh ds.sh worker 1 6 > logs/worker1.log &\n```\n\nGPU-2\n\n```\nnohup sh ds.sh worker 2 1 > logs/worker2.log &\nnohup sh ds.sh worker 3 2 > logs/worker3.log &\n```\n\n```\ntail logs/worker0.log:\n INFO:tensorflow:Worker 0: 2016-07-01 13:03:19.768646: step 360, loss = 12.82(8.3 examples/sec; 3.879  sec/batch)\n INFO:tensorflow:global_step/sec: 0.241671\n INFO:tensorflow:Worker 0: 2016-07-01 13:05:16.359716: step 390, loss = 12.91(8.0 examples/sec; 3.976  sec/batch)\n INFO:tensorflow:global_step/sec: 0.258332\n INFO:tensorflow:Running Summary operation on the chief.\n INFO:tensorflow:Finished running Summary operation.\n INFO:tensorflow:Worker 0: 2016-07-01 13:07:12.427695: step 420, loss = 12.76(8.2 examples/sec; 3.900  sec/batch)\n INFO:tensorflow:global_step/sec: 0.258331\n INFO:tensorflow:Running Summary operation on the chief.\n INFO:tensorflow:Finished running Summary operation.\n INFO:tensorflow:Worker 0: 2016-07-01 13:09:10.921383: step 450, loss = 12.85(7.9 examples/sec; 4.056  sec/batch)\n INFO:tensorflow:global_step/sec: 0.257871\n INFO:tensorflow:Worker 0: 2016-07-01 13:11:13.658113: step 480, loss = 12.83(8.3 examples/sec; 3.867  sec/batch)\n INFO:tensorflow:global_step/sec: 0.242095\n INFO:tensorflow:Running Summary operation on the chief.\n INFO:tensorflow:Finished running Summary operation.\n INFO:tensorflow:Worker 0: 2016-07-01 13:13:11.261521: step 510, loss = 12.66(8.1 examples/sec; 3.930  sec/batch)\n INFO:tensorflow:global_step/sec: 0.250013\n INFO:tensorflow:Running Summary operation on the chief.\n INFO:tensorflow:Finished running Summary operation.\n INFO:tensorflow:Worker 0: 2016-07-01 13:15:09.110845: step 540, loss = 12.78(8.4 examples/sec; 3.827  sec/batch)\n INFO:tensorflow:global_step/sec: 0.258329\n INFO:tensorflow:Worker 0: 2016-07-01 13:17:07.101986: step 570, loss = 12.66(8.1 examples/sec; 3.951  sec/batch)\n INFO:tensorflow:global_step/sec: 0.250001\n INFO:tensorflow:Running Summary operation on the chief.\n INFO:tensorflow:Finished running Summary operation.\n INFO:tensorflow:Worker 0: 2016-07-01 13:19:04.647026: step 600, loss = 12.61(8.1 examples/sec; 3.967  sec/batch)\n INFO:tensorflow:Running Summary operation on the chief.\n INFO:tensorflow:Finished running Summary operation.\n INFO:tensorflow:Worker 0: 2016-07-01 13:21:03.685468: step 630, loss = 12.60(8.1 examples/sec; 3.939  sec/batch)\n```\n\n## 5.Distributed on GPU1 with 4 Cores\n\n  In this case, the sec/batch avg=2.24697\n\nI had also tried `--num_preprocess_threads=4/8/16 --num_readers=4`, but useless.\n", "One thing you guys can try is to remove the function to modify the image in the image_preprocess code and see if it helps. If yes, then it means the CPU just couldn't feed this many GPUs concurrently. Otherwise, it could mean that the bandwidth is not enough or some other system bottleneck.\n", "Did you try to increase \"replicas_to_aggregate\" parameter? It looks it can increase the speed of training because fewer synchronizations will be done.\n", "Have you ever thought about your network bandwidth as bottleneck? It seems it's only GbE in your setting (125 MB/s) and that's 2 orders of magnitude slower than your PCI-e interconnect.", "Closing due to lack of activity.\n", "Use tensorflow 1.3 version still got the same issue. It seems not the hardware problem.", "Yes I am also facing the same issue now. I am trying to run with 3 workers and 2 ps and still ending up with more duration per step. Did you guys try to solve this?", "Get a proper network if you plan to do distributed training of large models. \r\n\r\nBottom line: don't do distributed TF with <10Gbps network...", "@byronyi Thanks for the reply. I am training [Tensorflow Object Detection models](https://github.com/tensorflow/models/tree/master/research/object_detection) with new dataset on distributed training. The dataset is < 1GB, I am using hadoop for sharing the files between the workers and parameter servers. Do I still need a network > 10Gbps to train?. Sorry, if I was redundant \r\n \r\nCurrently:\r\nI am having 1.2 to 3 secs/step implementing distributed training and on single machine training, I have ~0.3secs/step.", "@saikishor how about your trying of object detection distributed training, so with distributed training you get much slower speed than single machine?", "Exactly, that's what I am facing. @MrWanter.\r\nMay be as @byronyi said, we might need at-least 10Gbps, but I don't have such network. So, still the same issue..", "@saikishor I believe both Azure and AWS offer 10/25 Gbps accelerated network for their VMs. You could give that a shot. After all, 10 Gbps Ethernet should be pretty affordable in 2018 :)", "@byronyi Thanks for the suggestion, I will give it a try. :)", "> @saikishor I believe both Azure and AWS offer 10/25 Gbps accelerated network for their VMs. You could give that a shot. After all, 10 Gbps Ethernet should be pretty affordable in 2018 :)\r\n\r\nI am running distributed object detection with TF 1.8 and on servers with 2X10Gbps network. With 9 GPU on each machine, I got 0.2 sec/step in single machine mode and 0.6 sec /step in 2 worker mode. Tried increasing number of PS servers but seems no difference. During training, I observed around 700MB/s network traffic on both machine. Is this expected?", "Try increase your batch size.", "I don\u2019t think it\u2019s because the batch size because my image per card is the\nsame in single mode and cluster mode. Also, I cannot increase batch size\nanymore because it already reached the memory limit.\n\nOn Fri, 2 Nov 2018 at 21:43, Bairen Yi <notifications@github.com> wrote:\n\n> Try increase your batch size.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/2397#issuecomment-435383217>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ACAYJn4Dpyml-Y-BoLyzV0ClL2YOxXfnks5urEwSgaJpZM4If97l>\n> .\n>\n-- \n\nXiaoning Yue\n\nMail: ustcyue@gmail.com\n", "Looking closely, 0.2 sec/step in single machine mode and 0.6 sec /step in 2 worker mode seems to be free from scalability issues; what\u2019s your single GPU and single node performance? If it is slow, the networking alone could do little to help.", "> Looking closely, 0.2 sec/step in single machine mode and 0.6 sec /step in 2 worker mode seems to be free from scalability issues; what\u2019s your single GPU and single node performance? If it is slow, the networking alone could do little to help.\r\n\r\n0.2 sec/step is running without parameter server. I also tried running one worker and one parameter server on the same machine, speed became 0.6 sec/step, network throughput and CPU is far from limit. \r\nI am running object detection with Slim, not the one using Estimator, will this be a problem?"]}, {"number": 2396, "title": "AttributeError: module 'tensorflow.contrib.learn' has no attribute 'datasets'", "body": "I was running :  tensorflow/tensorflow/examples/skflow/text_classification_cnn.py\n\nThe line at : dbpedia = learn.datasets.load_dataset('dbpedia') fails with the attribute error . \n\nError: AttributeError: module 'tensorflow.contrib.learn' has no attribute 'datasets'\n", "comments": ["Please install latest version, .e.g. one of the recent nightly builds\n", "I install version `tensorflow-0.8.0` but I have the same problem. EDIT: command which I use is `sudo sudo pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.8.0-py3-none-any.whl`\n", "@nguyendn: See the answer to #2421. You need to install a **nightly** build to run the examples on the `master` branch, or run the examples from the branch that corresponds to the release (https://github.com/tensorflow/tensorflow/tree/r0.8/tensorflow/examples/skflow).\n", "Hey, if you dont want to re-install the nightly build, you can manually download the same dataset.  I've documented ez steps here:  http://stackoverflow.com/questions/37206459/tensorflow-examples-all-fail-due-to-attributeerror-module-object-has-no-attri\n"]}, {"number": 2395, "title": "Tensors should have an \"ndim\" property", "body": "This is a useful shortcut property on NumPy arrays, and would nice to have in TensorFlow for the sake of cross-compatibility. `tensor.ndim` would be equivalent to `tf.rank(tensor)`.\n", "comments": ["You can call `Tensor.get_shape().ndims` to get the (inferred) rank of a tensor, but it is allowed to be `None` if the rank is unknown. Or you can call `tf.rank(tensor)` and the result is a `tf.Tensor` that you have to evaluate (e.g. using `Session.run()`) in order to get an integer.\n\nNeither of these is quite the same as the NumPy property, so I don't see the advantages of having an identical interface here.\n", "This is fair enough. I didn't realize it was possible to have tensors with unknown rank.\n", "It's possible but rare :) (at least in reasonable-quality code...). For example you can create one with `tf.placeholder(tf.float32)`, but you're encouraged to use `tf.placeholder(tf.float32, shape=[...])`. Unfortunately, we have some highly dynamic custom and legacy op code that requires this behavior....\n"]}, {"number": 2394, "title": "Push changes from internal: 122460440", "body": "", "comments": ["Can one of the admins verify this patch?\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins test this please.\n", "common_runtime_gpu_gpu_debug_allocator_test failed. Testing this again to see if it is a flaky test.\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "The common_runtime_gpu_gpu_debug_allocator_test failure seems to be reproducible:\n\n==================== Test output for //tensorflow/core:common_runtime_gpu_gpu_debug_allocator_test:\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nRunning main() from test_main.cc\n[==========] Running 7 tests from 1 test case.\n[----------] Global test environment set-up.\n[----------] 7 tests from GPUDebugAllocatorTest\n[ RUN      ] GPUDebugAllocatorTest.OverwriteDetection_None\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GRID K520\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.797\npciBusID 0000:00:03.0\nTotal memory: 4.00GiB\nFree memory: 3.95GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \n[       OK ] GPUDebugAllocatorTest.OverwriteDetection_None (275 ms)\n[ RUN      ] GPUDebugAllocatorTest.OverwriteDetection_Header\n\n# [WARNING] external/gmock_archive/gmock-1.7.0/gtest/src/gtest-death-test.cc:825:: Death tests use fork(), which is unsafe particularly in a threaded context. For this test, Google Test couldn't detect the number of threads.\n", "@caisq: That warning looks ominous, but it doesn't appear to be new in gmock, nor is the death test in that target new. Is there a good way to run that test in isolation on the test machine?\n\n@keveman is trying to reproduce this locally. If he doesn't succeed, I propose we continue the push and disable the test for now.\n", "@tensorflow-jenkins test this please.\n", "@caisq: @keveman figured out the breakage (due to a subtle change in the behavior of our CUDA code across `fork()`). Can you please try running the tests again, in case @tensorflow-jenkins doesn't listen to me? :)\n", "Jenkins, test this please.\n"]}, {"number": 2393, "title": "Update FAQ", "body": "The FAQ here: https://www.tensorflow.org/versions/r0.8/resources/faq.html\nstill says that multi-machine isn't available yet. But it is available now.\n", "comments": []}, {"number": 2392, "title": "Push changes from internal: 122444497", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "There are some issues in contrib/python/learn that need to be resolved.\n", "@caisq: I just sent a PR for pushing from internal because I'm taking over the rotation this week. Do you want to drop this one?\n", "(Closing this as it was subsumed by #2394.)\n"]}, {"number": 2391, "title": "Implemented Tensorflow Op Kernel not found", "body": "I would like to implement an fft op kernel for CPU. Since eigen got an fft implementation this shouldn't be too hard. It's compiling without problems, but I am always running into an InvalidArgumentError when trying to use it from python (see below).\n\nCode of kernel for fft op on CPU (modification of [this file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/fft_ops.cc)):\n\n``` ...\n// CPU Implementation\ntemplate <bool Forward>\nclass FFTCPU1D : public OpKernel {\n public:\n  explicit FFTCPU1D(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& in = ctx->input(0);\n    const TensorShape& shape = in.shape();\n    OP_REQUIRES(ctx, shape.dims() != 1,\n      errors::InvalidArgument(\"Input must have rank 1, but got: \",\n                                shape.DebugString()));\n\n    Tensor* out;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, shape, &out));\n    if (Forward) {\n        out.vec<std::complex<Scalar>>() = kernel_variant.template fft<Eigen::BothParts, FFT_FORWARD>(fft).vec()<std::complex<Scalar>>;\n    } else {\n        out.vec<std::complex<Scalar>>() = kernel_variant.template fft<Eigen::BothParts, FFT_REVERSE>(fft).vec()<std::complex<Scalar>>;\n    }\n  }\n\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"FFT\").Device(DEVICE_CPU), FFTCPU1D<true>);\nREGISTER_KERNEL_BUILDER(Name(\"IFFT\").Device(DEVICE_CPU), FFTCPU1D<false>);\n...\n```\n\nPython-Script for testing:\n\n```\nimport numpy as np\nimport tensorflow as tf\n\nsess = tf.Session()\ndata = np.complex64(np.random.normal(size=1024))\ninput = tf.constant(data)\nfftOp = tf.fft(input)\n\nprint(sess.run(fftOp))\n```\n\nError-Message:\n\n```\n# python ~/test.py\nTraceback (most recent call last):\n  File \"/root/test.py\", line 12, in <module>\n    print(sess.run(fftOp))\n  File \"/tensorflow/_python_build/tensorflow/python/client/session.py\", line 332, in run\n    run_metadata_ptr)\n  File \"/tensorflow/_python_build/tensorflow/python/client/session.py\", line 572, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/tensorflow/_python_build/tensorflow/python/client/session.py\", line 652, in _do_run\n    target_list, options, run_metadata)\n  File \"/tensorflow/_python_build/tensorflow/python/client/session.py\", line 672, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: No OpKernel was registered to support Op 'FFT' with these attrs\n         [[Node: FFT = FFT[](Const)]]\nCaused by op u'FFT', defined at:\n  File \"/root/test.py\", line 10, in <module>\n    fftOp = tf.fft(input)\n  File \"/tensorflow/_python_build/tensorflow/python/ops/gen_math_ops.py\", line 518, in fft\n    return _op_def_lib.apply_op(\"FFT\", input=input, name=name)\n  File \"/tensorflow/_python_build/tensorflow/python/ops/op_def_library.py\", line 693, in apply_op\n    op_def=op_def)\n  File \"/tensorflow/_python_build/tensorflow/python/framework/ops.py\", line 2177, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/tensorflow/_python_build/tensorflow/python/framework/ops.py\", line 1161, in __init__\n    self._traceback = _extract_stack()\n```\n\nI've got the gcr.io/tensorflow/tensorflow:latest-devel docker image and commit 5f4524 and built tensorflow according to the [instructions](https://www.tensorflow.org/versions/master/get_started/os_setup.html#setting_up_tensorflow_for_development).\n\nI'm not sure whether it's a bug or I'm doing something wrong. I've already opened a [stackoverflow question](https://stackoverflow.com/questions/37252600/implemented-tensorflow-op-kernel-not-found).\n", "comments": ["Can you share the code for your modified file? In particular, it looks like there is a big `#ifdef GOOGLE_CUDA` in that file, so my first suspicion is that the registration might not be executing.\n", "This is great!\n\nBut I thought adding a CPU fft op would be difficult, as the Eigen version needs a backend (either fftw or mkl): http://eigen.tuxfamily.org/dox/unsupported/group__FFT__Module.html\n", "Thanks, it was the big `#ifdef GOOGLE_CUDA` was the problem. It looks like there is already ab implementation, at least the convolution utilizes FFT (https://github.com/tensorflow/tensorflow/blob/e39d8feebb9666a331345cd8d960f5ade4652bba/third_party/eigen3/unsupported/Eigen/CXX11/src/NeuralNetworks/TensorConvolutionByFFT.h#L234). Maybe I'm misunderstanding this line (the `.template` is a little bit confusing).\n", "I find bazel does not output anything, but there are compile errors in my code, only ycm helps me.\n", "Do you plan to add a CPU version for FFT/FFT2D etc in tensorflow ? ", "I've started to implement it, but I didn't came far and ran out of time. But it seems to be there is an approach in main branch (https://github.com/tensorflow/tensorflow/blob/0c58158903ea9a3af612ae428c6cb3dede4abf16/tensorflow/core/kernels/fft_ops.cc#L115)", "Yes, FFT is now done with Eigen builtin which is slower than fftw or MKL, but at least it works, which is a big improvement.", "> Yes, FFT is now done with Eigen builtin which is slower than fftw or MKL, but at least it works, which is a big improvement.\r\n\r\n@aselle, where can we find this? Are there any packaged version of the code, or do we need to built from bazel?"]}, {"number": 2390, "title": "R0.8", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Is this a misoperation?\n", "I am pretty sure this is misoperation. Closing the PR.\n"]}, {"number": 2389, "title": "Add float64 to _valid_dtypes in Optimizer", "body": "Added float64 to _valid_dtypes in class Optimizer(). Tested locally.\n\nThis solves #1061 as well as #761. For #1061, the change was tested with the clip_by_value operator as mentioned in [this comment](https://github.com/tensorflow/tensorflow/issues/1061#issuecomment-219238294).\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "some other tests failed, please take a look\n", "@vrv Fixed the errors. This time I ran all the tests.\n", "Thanks, seems reasonable -- I guess we were testing that float64 wasn't supported explicitly...\n\n@tensorflow-jenkins test this please\n"]}, {"number": 2388, "title": "pip install failing for Python3.4 but fine for Python3.5", "body": "I have been following the installation for Python3.4 using\n\nsudo pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp34-cp34m-linux_x86_64.whl\n\nThe installation says it was successful\n\nWhen I try to run the examples I get an error\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.4/inspect.py\", line 977, in getfullargspec\n    skip_bound_arg=False)\n  File \"/usr/lib/python3.4/inspect.py\", line 1965, in _signature_internal\n    skip_bound_arg=skip_bound_arg)\n  File \"/usr/lib/python3.4/inspect.py\", line 1890, in _signature_from_builtin\n    raise ValueError(\"no signature found for builtin {!r}\".format(func))\nValueError: no signature found for builtin <method 'max' of 'numpy.ndarray' objects>\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"tensor_flow_test.py\", line 1, in <module>\n    import tensorflow as tf\n  File \"/home/padraig/.local/lib/python3.4/site-packages/tensorflow/**init**.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/home/padraig/.local/lib/python3.4/site-packages/tensorflow/python/**init**.py\", line 62, in <module>\n    import tensorflow.contrib as contrib\n  File \"/home/padraig/.local/lib/python3.4/site-packages/tensorflow/contrib/**init**.py\", line 26, in <module>\n    from tensorflow.contrib import learn\n  File \"/home/padraig/.local/lib/python3.4/site-packages/tensorflow/contrib/learn/**init**.py\", line 20, in <module>\n    from tensorflow.contrib.learn.python.learn import *\n  File \"/home/padraig/.local/lib/python3.4/site-packages/tensorflow/contrib/learn/python/**init**.py\", line 20, in <module>\n    from tensorflow.contrib.learn.python.learn import *\n  File \"/home/padraig/.local/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/**init**.py\", line 22, in <module>\n    from tensorflow.contrib.learn.python.learn.io import *\n  File \"/home/padraig/.local/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/io/**init**.py\", line 20, in <module>\n    from tensorflow.contrib.learn.python.learn.io.dask_io import *\n  File \"/home/padraig/.local/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/io/dask_io.py\", line 23, in <module>\n    import dask.dataframe as dd\n  File \"/usr/local/lib/python3.4/dist-packages/dask/dataframe/**init**.py\", line 1, in <module>\n    from .core import (DataFrame, Series, Index, _Frame, map_partitions,\n  File \"/usr/local/lib/python3.4/dist-packages/dask/dataframe/core.py\", line 1234, in <module>\n    class Index(Series):\n  File \"/usr/local/lib/python3.4/dist-packages/dask/dataframe/core.py\", line 1266, in Index\n    @derived_from(pd.Index)\n  File \"/usr/local/lib/python3.4/dist-packages/dask/utils.py\", line 526, in wrapper\n    original_args = getargspec(original_method).args\n  File \"/usr/local/lib/python3.4/dist-packages/dask/compatibility.py\", line 190, in getargspec\n    return _getargspec(func)\n  File \"/usr/local/lib/python3.4/dist-packages/dask/compatibility.py\", line 35, in _getargspec\n    return inspect.getfullargspec(func)\n  File \"/usr/lib/python3.4/inspect.py\", line 983, in getfullargspec\n    raise TypeError('unsupported callable') from ex\nTypeError: unsupported callable\n\nI manually downloaded, renamed and installed the wheel for Python3.5 and it is now working.\n", "comments": ["With PR #2585, we now have Linux Python 3.5 whl files built and tested nightly. The links to the whl files and build history can be found in the main README.md: \nhttps://github.com/tensorflow/tensorflow/\n\nLinux Python 3.5 whl files will also be included in future releases.\n", "`pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp34-cp34m-linux_x86_64.whl`\r\nwork for my py3.4.  thanks "]}, {"number": 2387, "title": "Documentation on fault tolerance in distributed tensorflow", "body": "Hi,\n\nI know the white paper describes how Tensorflow handles failures in distributed execution, but the docs for open source tensorflow (specifically: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/distributed/index.md) don't really explain what to expect if a machine goes down in a cluster. It would be nice to know:\n1. What the expected behavior is if a machine fails.\n2. If it's possible to add a new machine to the cluster if one fails.\n\nThanks!\nSulaiman\n", "comments": ["This type of question is better asked on StackOverflow, since it isn't about an improvement or bug fix to TensorFlow itself.\n"]}, {"number": 2386, "title": "Distributed tensorflow hang when turning from Async to Sync", "body": "I follow [Distributed tensorflow](https://www.tensorflow.org/versions/r0.8/how_tos/distributed/index.html) to port our RNN to multi-GPU cards. I start 4 processes and each process run on a different GPU card.\nAfter solving the [issue](https://github.com/tensorflow/tensorflow/issues/2292), I could run it on my 4 GPU cards server in Asynchronized mode. \n\nThen I change my code to Synchronized mode by adding only the following code:\n    optimizer = tf.train.SyncReplicasOptimizer(\n        optimizer,\n        replicas_to_aggregate = 4, \n        replica_id = FLAGS.task_index,\n        total_num_replicas = 4\n        #variable_averages = exp_moving_averager,    # Remove this for simple.\n        #variables_to_average = variables_to_average # Remove this for simple.\n        )\n\nThen the job will hang in session.run(...)\nIs there any other thing needed to change when switch from Async mode to Sync mode?\n\nThanks a lot in advance!\n", "comments": ["You also need to start the chief queue runner which will start the synchronization thread. Please follow the comments and example in the sync_replicas_optimizer.py. Another example is the inception_distributed_train.py in tensorflow/models/inception\n", "@jmchen-g \nThanks a lot for your help! I turn to [sync_replicas_optimizer.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/sync_replicas_optimizer.py) and follow the \"Usage\". In order to simplify the code for you to see, I just use [mnist_softmax.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_softmax.py) as example and chane input from file data to random data. \n\nI start 1 ps and 2 workers in the same machine(it has 4 GPU cards). The worker 0 runs smoothly and print out many lines of \"accuracy=XXX\" as expected. But the worker 1 is crashed with the following error:\n\n`I tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40c, pci bus id: 0000:02:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K40c, pci bus id: 0000:04:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla K40c, pci bus id: 0000:84:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla K40c, pci bus id: 0000:85:00.0)\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {10.141.33.61:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {10.141.33.61:2223, localhost:2224}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2224\nTraceback (most recent call last):\n  File \"./mnist_softmax.py\", line 87, in <module>\n    tf.app.run()\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"./mnist_softmax.py\", line 84, in main\n    run_training(server, cluster)\n  File \"./mnist_softmax.py\", line 72, in run_training\n    _, cost, acc, step = sess.run([train_step, cross_entropy, accuracy, global_step], feed_dict = { x: source_data, y_ : labels_one_hot })\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 340, in run\n    run_metadata_ptr)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 564, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 637, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 659, in _do_call\n    e.code)\ntensorflow.python.framework.errors.InternalError: No worker known as /job:worker/replica:0/task:1\n     [[Node: sync_replicas/cond/switch_f_S24 = _Recv[client_terminated=false, recv_device=\"/job:ps/replica:0/task:0/cpu:0\", send_device=\"/job:worker/replica:0/task:1/gpu:1\", send_device_incarnation=-240247217847712065, tensor_name=\"edge_42_sync_replicas/cond/switch_f\", tensor_type=DT_FLOAT, _device=\"/job:ps/replica:0/task:0/cpu:0\"]()]]\n`\n\nThe command line of worker1:\n`python ./mnist_softmax.py --ps_hosts=10.141.33.61:2222 --worker_hosts=10.141.33.61:2223,10.141.33.61:2224 --job_name=worker --task_index=1 --gpu_cards=2`\n\nAll of my code:\n\n[mnist_softmax_Sync.py.txt](https://github.com/tensorflow/tensorflow/files/269622/mnist_softmax_Sync.py.txt)\n", "@jmchen-g \nI've also follow the style of [inception_distributed_train.py](https://github.com/tensorflow/models/blob/master/inception/inception/inception_distributed_train.py) (Running it needs 100+GB ILSVRC2012 data set and we are downloading it these days) , based on my above code, I added:\n`with tf.device('/job:worker/task:%d' % FLAGS.task_index) :`\nAnd I removed:\n\n```\nwith tf.device(tf.train.replica_device_setter(\n                worker_device = \"/job:worker/task:%d\" % FLAGS.task_index,\n                cluster = cluster)) :\n       with tf.device('/gpu:%d' % (FLAGS.task_index % FLAGS.gpu_cards)) :\n```\n\nThen I restart the job, still  with 1 ps and 2 workers. Then worker 0 still runs smoothly and print out results. Worker 1 hangs inner \"sv.prepare_or_wait_for_session\"(no error messages are shown).\n\nThis is my code: (Added some \"print\" to locate where to hang)\n[mnist_softmax.py.txt](https://github.com/tensorflow/tensorflow/files/269636/mnist_softmax.py.txt)\n", "Actually the inception distributed train is using slim which has its own trick related to device setter. For you case, can you try follow the instructions for the replica_device_setter:\n  # To build a cluster with two ps jobs on hosts ps0 and ps1, and 3 worker\n  # jobs on hosts worker0, worker1 and worker2.\n  cluster_spec = {\n      \"ps\": [\"ps0:2222\", \"ps1:2222\"],\n      \"worker\": [\"worker0:2222\", \"worker1:2222\", \"worker2:2222\"]}\n  with tf.device(tf.replica_device_setter(cluster=cluster_spec)):\n    # Build your graph\n    v1 = tf.Variable(...)  # assigned to /job:ps/task:0\n    v2 = tf.Variable(...)  # assigned to /job:ps/task:1\n    v3 = tf.Variable(...)  # assigned to /job:ps/task:0\n", "@jmchen-g \nThank you very much for your quick help!\nI updated my code following your guide. I run 1 ps and 2 workers like this:\n\n```\ncluster_spec = tf.train.ClusterSpec({ \"ps\":[\"10.141.33.61:2222\"], \"worker\" : [\"10.141.33.61:2223\", \"10.141.33.65:2223\"] })\nwith tf.device(tf.train.replica_device_setter(cluster = cluster_spec)) :\n```\n\nMy code file:\n[mnist_softmax.py.txt](https://github.com/tensorflow/tensorflow/files/269861/mnist_softmax.py.txt)\n\nThen worker0 runs smoothly, worker1 shows the following error:\n\n```\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {10.141.33.61:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {10.141.33.61:2223, localhost:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2223\nTraceback (most recent call last):\n  File \"./mnist_softmax.py\", line 83, in <module>\n    tf.app.run()\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"./mnist_softmax.py\", line 80, in main\n    run_training(server, cluster_spec)\n  File \"./mnist_softmax.py\", line 70, in run_training\n    _, cost, acc, step = sess.run([train_step, cross_entropy, accuracy, global_step], feed_dict = { x: source_data, y_ : labels_one_hot })\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 340, in run\n    run_metadata_ptr)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 564, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 637, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 659, in _do_call\n    e.code)\ntensorflow.python.framework.errors.InvalidArgumentError: Expected size[0] in [0, 0], but got 1\n     [[Node: get_local_step = Slice[Index=DT_INT32, T=DT_FLOAT, _device=\"/job:worker/replica:0/task:0/gpu:0\"](local_steps_S11, Reshape, get_local_step/size)]]\nCaused by op u'get_local_step', defined at:\n  File \"./mnist_softmax.py\", line 83, in <module>\n    tf.app.run()\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"./mnist_softmax.py\", line 80, in main\n    run_training(server, cluster_spec)\n  File \"./mnist_softmax.py\", line 45, in run_training\n    train_step = opt.minimize(cross_entropy, global_step = global_step)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 192, in minimize\n    name=name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/sync_replicas_optimizer.py\", line 334, in apply_gradients\n    name=\"get_local_step\")\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 217, in slice\n    return gen_array_ops._slice(input_, begin, size, name=name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1318, in _slice\n    name=name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1154, in __init__\n    self._traceback = _extract_stack()\n```\n\nWhen I change it to run the 2 workers in the same machine as follows, then both of the workers runs smoothly.\n`cluster_spec = tf.train.ClusterSpec({ \"ps\":[\"10.141.33.61:2222\"], \"worker\" : [\"10.141.33.61:2223\", \"10.141.33.61:2224\"] })`\n\nSo is there something wrong in my multi-machine code?\n", "Can you remove all the checkpoint before starting the run?\n\nIf you want to have checkpoint, you need to make sure the path is\naccessible from all PS and chief worker.\n\nOn Wed, May 18, 2016 at 12:52 AM, smartcat2010 notifications@github.com\nwrote:\n\n> @jmchen-g https://github.com/jmchen-g\n> Thank you very much for your quick help!\n> I updated my code following your guide. I run 1 ps and 2 workers like this:\n> \n> cluster_spec = tf.train.ClusterSpec({ \"ps\":[\"10.141.33.61:2222\"], \"worker\" : [\"10.141.33.61:2223\", \"10.141.33.65:2223\"] })\n> with tf.device(tf.train.replica_device_setter(cluster = cluster_spec)) :\n> \n> My code file:\n> mnist_softmax.py.txt\n> https://github.com/tensorflow/tensorflow/files/269861/mnist_softmax.py.txt\n> \n> Then worker0 runs smoothly, worker1 shows the following error:\n> \n> I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {10.141.33.61:2222}\n> I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {10.141.33.61:2223, localhost:2223}\n> I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2223\n> Traceback (most recent call last):\n>   File \"./mnist_softmax.py\", line 83, in <module>\n>     tf.app.run()\n>   File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n>     sys.exit(main(sys.argv))\n>   File \"./mnist_softmax.py\", line 80, in main\n>     run_training(server, cluster_spec)\n>   File \"./mnist_softmax.py\", line 70, in run_training\n>     _, cost, acc, step = sess.run([train_step, cross_entropy, accuracy, global_step], feed_dict = { x: source_data, y_ : labels_one_hot })\n>   File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 340, in run\n>     run_metadata_ptr)\n>   File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 564, in _run\n>     feed_dict_string, options, run_metadata)\n>   File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 637, in _do_run\n>     target_list, options, run_metadata)\n>   File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 659, in _do_call\n>     e.code)\n> tensorflow.python.framework.errors.InvalidArgumentError: Expected size[0] in [0, 0], but got 1\n>      [[Node: get_local_step = Slice[Index=DT_INT32, T=DT_FLOAT, _device=\"/job:worker/replica:0/task:0/gpu:0\"](local_steps_S11, Reshape, get_local_step/size)]]\n> Caused by op u'get_local_step', defined at:\n>   File \"./mnist_softmax.py\", line 83, in <module>\n>     tf.app.run()\n>   File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n>     sys.exit(main(sys.argv))\n>   File \"./mnist_softmax.py\", line 80, in main\n>     run_training(server, cluster_spec)\n>   File \"./mnist_softmax.py\", line 45, in run_training\n>     train_step = opt.minimize(cross_entropy, global_step = global_step)\n>   File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 192, in minimize\n>     name=name)\n>   File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/sync_replicas_optimizer.py\", line 334, in apply_gradients\n>     name=\"get_local_step\")\n>   File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 217, in slice\n>     return gen_array_ops._slice(input_, begin, size, name=name)\n>   File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1318, in _slice\n>     name=name)\n>   File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n>     op_def=op_def)\n>   File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op\n>     original_op=self._default_original_op, op_def=op_def)\n>   File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1154, in __init__\n>     self._traceback = _extract_stack()\n> \n> When I change it to run the 2 workers in the same machine as follows, then\n> both of the workers runs smoothly.\n> cluster_spec = tf.train.ClusterSpec({ \"ps\":[\"10.141.33.61:2222\"],\n> \"worker\" : [\"10.141.33.61:2223\", \"10.141.33.61:2224\"] })\n> \n> So is there something wrong in my multi-machine code?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2386#issuecomment-219951942\n", "@jmchen-g \nDo you mean the file with name \"checkpoint\"? I've just deleted all files including that under the code's folder of both machines. But the same error still shows.\n", "Looks like you only specified 1 worker? In your case, they should be 2 instead.\n\n```\n            **num_workers** = FLAGS.gpu_cards # Use only 1 worker for test.\n            opt = tf.train.SyncReplicasOptimizer(opt, replicas_to_aggregate = num_workers,\n                replica_id = FLAGS.task_index, total_num_replicas = num_workers)\n```\n", "@jmchen-g \nThank you very much! It's my stupid mistake of specifying 1 worker in previous code. Now it works on multi-nodes/multi-cards after fixing my bug. Really sorry for bring so much trouble to you.\nMy fixed code which works:\n[mnist_softmax.py.txt](https://github.com/tensorflow/tensorflow/files/271770/mnist_softmax.py.txt)\n\nMaybe it's a good idea to put the usage of Sync-mode into [tutorial](https://www.tensorflow.org/versions/r0.8/how_tos/distributed/index.html). The current tutorial only has a Async-mode code example. Although Inception example code uses Sync-mode, it also uses more complex things such as slim, queues, so it's not easy for junior user to start.\n\nThanks a lot again for your kindly help!\n", "No problem. Glad it worked~:)\n\nYa, we might add an example at some point. Thanks for the suggestion.\n\nOn Wed, May 18, 2016 at 8:43 PM, smartcat2010 notifications@github.com\nwrote:\n\n> @jmchen-g https://github.com/jmchen-g\n> Thank you very much! It's my stupid mistake of specifying 1 worker in\n> previous code. Now it works on multi-nodes/multi-cards after fixing my bug.\n> Really sorry for bring so much trouble to you.\n> My fixed code which works:\n> mnist_softmax.py.txt\n> https://github.com/tensorflow/tensorflow/files/271770/mnist_softmax.py.txt\n> \n> Maybe it's a good idea to put the usage of Sync-mode into tutorial\n> https://www.tensorflow.org/versions/r0.8/how_tos/distributed/index.html.\n> The current tutorial only has a Async-mode code example. Although Inception\n> example code uses Sync-mode, it also uses more complex things such as slim,\n> queues, so it's not easy for junior user to start.\n> \n> Thanks a lot again for your kindly help!\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2386#issuecomment-220220342\n", "I tried the newest mnist_softmax.py.txt provided above.\nIt seems hang at sv.prepare_or_wait_for_session in worker 1.\n", "I tried both [Distributed tensorflow](https://www.tensorflow.org/versions/r0.8/how_tos/distributed/index.html)   and [minist_softmax.py.txt](https://github.com/tensorflow/tensorflow/files/271770/mnist_softmax.py.txt).  They both hang at the prepare_or_wait_for_session() function with non-chief worker tasks. \n\ntask_index == 0(chief worker ) can run as expected.\nI can not find out the reasion why it behaves like this. \nCan anyone please give some advice?\n", "@s0okiym: It sounds like you might be having a different problem. Please open a new issue and describe exactly how you are invoking the various binaries.\n", "@s0okiym \nI have the same problem!\nonly the chief task works, and the ps prints a list of errors \n\"E tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 564.97M (592413952 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\"\nHave you fixed your problem? \n", "@smartcat2010 \nHi,\nI tried your mnist_softmax.py.txt example and did run it on 1 parameter server and 2 workers.\nUnfortunately, I get a two times longer runtime with using 2 workers compared to using only 1 worker.\nCan you confirm this behavior?\nA feedback would be great!\n", "@DjangoPeng @icklerly  \nHi all. I also meet the problem when i run mnist_softmax.py.txt on  1 parameter server and 2 workers. only the chief task works, and the ps prints a list of errors . \ncan you help me?\n", "Did anybody got any answers or hints on this one?", "@jmchen-g Is there any changes with v1.3?\r\nI tried to run with the MonitoredTrainingSession(), but the process seemed to hang in the middle.\r\nBasically I followed the asynchronized tutorial (works fine) and add the SyncReplicasOptimizer.\r\nI tried to play with the `get_init_tokens_op()` but no luck..\r\nAny hints are welcome.  Thanks!\r\n\r\nHere is my sample code and outputs:\r\n\r\n`import tensorflow as tf\r\nimport sys\r\n\r\ncluster_spec = {\"ps\":[\"localhost:8864\"],\r\n                 \"worker\":[\"localhost:8865\",\"localhost:8866\"]}\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    if len(sys.argv) != 3:\r\n        print(\"test.py ps|worker 0|1\")\r\n        exit(1)\r\n\r\n    role = sys.argv[1]\r\n    task_id = int(sys.argv[2])\r\n\r\n    cluster = tf.train.ClusterSpec(cluster_spec)\r\n    server = tf.train.Server(cluster, job_name=role, task_index=task_id)\r\n\r\n    if role == \"ps\":\r\n        server.join()\r\n    else:\r\n        with tf.device(\r\n                tf.train.replica_device_setter(cluster=cluster_spec,\r\n                    worker_device=\"/job:worker/task:%d\"%task_id)):\r\n            global_steps = tf.contrib.framework.get_or_create_global_step()\r\n            # a very simple model with pseudo data\r\n            inputs = tf.placeholder(tf.float32, shape=[None, 3])\r\n            labels = tf.placeholder(tf.float32, shape=[None])\r\n            w = tf.Variable([0.11,10,0.3])\r\n            y = tf.reduce_sum(tf.multiply(inputs, w), axis=1)\r\n            loss = tf.losses.mean_squared_error(labels, y)\r\n            opt = tf.train.GradientDescentOptimizer(0.00001)\r\n            opt_sync = tf.train.SyncReplicasOptimizer(opt,\r\n                                                      replicas_to_aggregate=2,\r\n                                                      # total_num_replicas=2,\r\n                                                      use_locking=True)\r\n            hooks = [opt_sync.make_session_run_hook((task_id==0))]\r\n            op = opt_sync.minimize(loss, global_step=global_steps)\r\n\r\n        with tf.train.MonitoredTrainingSession(master=server.target,\r\n                                               is_chief=(task_id==0),\r\n                                               hooks=hooks) as sess:\r\n            for i in range(10):\r\n                j = sess.run(global_steps)\r\n                t = sess.run(loss, feed_dict={inputs:[[1,2,3]],labels:[1]})\r\n                sess.run(op, feed_dict={inputs:[[1,2,3],[2,3,1]],labels:[2,1]})\r\n                print(\"%d %d %f\"%(i,j,t))\r\n\r\n        print(\"finished\")`\r\n\r\nfrom worker 1:\r\n`2017-10-03 15:48:37.079376: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session eb05b8a25a96918e with config: \r\n0 0 400.400116\r\n1 1 400.16366`\r\n\r\nfrom worker 2:\r\n`2017-10-03 15:48:3 6.882892: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session e54098121f5f8494 with config: \r\n0 0 400.400116\r\n1 1 400.400116`\r\n", "@jiayiliu Have you solve this problem?  the chief worker is hang when i use tf.train.SyncReplicasOptimizer and tf.train.MonitoredTrainingSession, but the other worker was finished and exit successfully. then the chief worker show as follow:\r\nException in thread QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany:\r\n      CancelledError: Step was cancelled by an explicit call to `Session::Close()`.", "@dingevin  I have the same problem that you describe.Have you solve this problem?"]}, {"number": 2385, "title": "Hard to understand error message ", "body": "x is a list of list of tensors, and im passing in one list at a time to the rnn but I get this hard to understand error message\n\nTraceback (most recent call last):\n  File \"autoencoder_m.py\", line 60, in <module>\n    outputs, state = rnn.rnn(cell=dropout_cell, inputs = x[i], initial_state=initial_state, sequence_length=s[i], scope = scope)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 127, in rnn\n    array_ops.pack([batch_size, cell.output_size]), inputs[0].dtype)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 241, in pack\n    return gen_array_ops._pack(values, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 916, in _pack\n    return _op_def_lib.apply_op(\"Pack\", values=values, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 379, in apply_op\n    assert dtype is not None, \"Should not fail if dtype is None\"\n**AssertionError: Should not fail if dtype is None**\n\nDoes this mean that x[i][0] is of type None? I already checked that it is `<dtype: 'float32'>`\n\nThis is the code I am trying to run, basically trying to run rnn on multiple gpu in the following manner.\n\n```\nx = [[tf.placeholder(tf.float32, shape=[batch_size, input_width]) for _ in xrange(max_sequence_length)] for _ in xrange(num_gpu)]\ny = [[tf.placeholder(tf.float32, shape=[batch_size, input_width]) for _ in xrange(max_sequence_length)] for _ in xrange(num_gpu)]\ns = [tf.placeholder(tf.int32, shape=[batch_size]) for _ in xrange(num_gpu)]\ncell = BasicLSTMCell(num_units = hidden_neurons, input_size = [batch_size, input_width])\ninitial_state = tf.zeros([batch_size, hidden_neurons * 2]) \ndropout_cell = DropoutWrapper(cell, input_keep_prob=0.9, output_keep_prob=1.0)\ninitial_state = tf.zeros([batch_size, cell.state_size])\n\ngpu_grads = []  \nlosses = []\nopt = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.99, beta2=0.97)\n\nfor i in xrange(num_gpu):\n        gpu_id = '/gpu:'+str(i)\n        with tf.device(gpu_id), tf.name_scope(gpu_id) as scope:\n                print x[i][0].dtype\n                outputs, state = rnn.rnn(cell=dropout_cell, inputs = x[i], initial_state=initial_state, sequence_length=s[i], scope = scope)\n```\n\nAfter this I get average of gradients similar to cifar10 multi gpu tutorial example.\n", "comments": ["This looks like a bad error message. Can you try printing the dtypes of all elements in `x`? My guess is that they have incompatible types....\n", "Thank you for your response. All the dtypes in x appear to be `<dtype: 'float32'>`.\n\nMaybe I'm not doing this the right way, and you can point me in the right direction. Is there something obviously wrong with how I am trying to run rnn on multiple GPU, using the param server paradigm? I'm happy to provide more code if that helps.\n", "@mrry Do you have any idea what can raise this error? It's not the input dtypes themselves\n", "The most likely culprit is an invalid value in the list - e.g. one that cannot be converted to a tensor. I'm about to submit a change that will improve the error message here, so it should help to discover the real problem.\n", "Well this is how I am creating the placeholders now.\n\n```\nx1 = []\nx2 = []\ny1 = []\ny2 = []\nfor _ in xrange(max_sequence_length):\n        with tf.device('/gpu:0'):\n                x = tf.placeholder(tf.float32, shape=[batch_size, input_width])\n                x1.append(x)\n                y = tf.placeholder(tf.float32, shape=[batch_size, input_width])                                                                                                \n                y1.append(y)                                                                                                                                                   \n        with tf.device('/gpu:1'):                                                                                                                                              \n                x = tf.placeholder(tf.float32, shape=[batch_size, input_width])                                                                                                \n                x2.append(x)                                                                                                                                                   \n                y = tf.placeholder(tf.float32, shape=[batch_size, input_width])                                                                                                \n                y2.append(y)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \nwith tf.device('/gpu:0'):                                                                                                                                                      \n                s1 = tf.placeholder(tf.int32, shape=[batch_size])                                                                                                                                                                                                                                                                                 \n                cell1 = BasicLSTMCell(num_units = hidden_neurons, input_size = [batch_size, input_width])                                                                      \n                initial_state1 = tf.zeros([batch_size, hidden_neurons * 2], name = 'initial_state1')                                                                           \n                with tf.name_scope('/gpu:0') as scope:                                                                                                                         \n                        outputs1, state1 = rnn.rnn(cell=cell1, inputs = x1, initial_state=initial_state1, sequence_length=s1, scope = scope)\netc etc\n\nwith tf.device('/gpu:1'):                                                                                                                                                      \n                s2 = tf.placeholder(tf.int32, shape=[batch_size])                                                                                                              \n                cell2 = BasicLSTMCell(num_units = hidden_neurons, input_size = [batch_size, input_width])                                                                      \n                initial_state2 = tf.zeros([batch_size, hidden_neurons * 2], name = 'initial_state2')                                                                           \n                with tf.name_scope('/gpu:1') as scope:                                                                                                                         \n                        outputs2, state2 = rnn.rnn(cell=cell2, inputs = x2, initial_state=initial_state2, sequence_length=s2, scope = scope)        \n```\n\nI use both `output1` and `output2` to create two losses with which to compute gradients twice and then I get the average, similar to `cifar10_multi_gpu.py`. However this exact same error happens at the line where i call `rnn.rnn`. I don't think it is because of an invalid tensor in the list.\n", "Looking closer at the exception backtrace, it is failing on [this line](https://github.com/tensorflow/tensorflow/blob/449d122b46d6e7fa2deba151356ffda200d1be7c/tensorflow/python/ops/rnn.py#L128) in `rnn.py`:\n\n``` python\narray_ops.pack([batch_size, cell.output_size]), inputs[0].dtype)\n```\n\nThis suggests that either `batch_size` or `cell.output_size` has the wrong type. With the forthcoming change it will be easier to tell which one of these is incorrect, but perhaps you can add some debugging code to find out?\n", "Both of these are `<type 'int'>`, and both have value 32 (I have batch size 32 and so is the number of neurons in my LSTM cell). Should they be some other type?\n"]}, {"number": 2384, "title": "ps job in the distributed inception trainer killed by OOM killer", "body": "Hi all:\n  It seems that the distributed runtime has a memory leak problem, my tf version is the 0.8.0 wheel package from http://tensorflow.org.\n  When I started the distributed inception trainer in http://github.com/tensorflow/models/ on a 64G RAM Linux server with 1 ps and 4 replicas, the footprint of the ps process increased slowly but continously, and the process was finally killed by the system oom killer after several days. \n  Anyone can reproduce this? @mrry \n", "comments": ["Thanks for the report! It might take some time to figure this out, but we'll get to the bottom of it.\n\nFirst of all, was it only the PS task that died? Do the worker replica tasks show an upward trend in memory consumption over time?\n", "I'd tried several times, only the PS task was OOM, all the worker tasks seemed ok.\n", "My chief worker task was killed in a similar setup with an upward trend in memory consumption; the ps job remained fine. Repeated the experiment dozens of times now, each time the workers keep growing in memory until they occupy 100%, then one of them (mostly the chief worker) gets killed. For my setup which uses `replica_device_setter`, the ps job always remains fine wrt memory.\nI do see multiple bfc_allocator warnings indicating that more memory could give performance gains. Could it be due to accumulating summaries?\n", "@mrry: Any suggestions for further debugging?  I don't think we have enough information to diagnose just yet.  If it's an actual memory leak valgrind could help, but that would be difficult to set up.\n\n@mmuneebs: The summaries should be written to disk soon after they are generated, so I don't think that's the issue.  The allocator warnings are likely a symptom, not a cause.\n", "I fixed this issue with the workers by commenting the `with tf.device('/cpu:0'):` spec while calling `batch_inputs` in [`image_processing.py`](https://github.com/tensorflow/models/blob/master/inception/inception/image_processing.py#L132). One reason this may have happened with my setup though not completely clear is that I use\n\n```\nwith tf.device(tf.train.replica_device_setter(ps_tasks=1,\n                                              worker_device=\"/job:worker/task:%d\" % FLAGS.task_id,\n                                              cluster=cluster_spec)):\n```\n\ninstead of\n\n```\n# Ops are assigned to worker by default.\nwith tf.device('/job:worker/task:%d' % FLAGS.task_id):\n    # Variables and its related init/assign ops are assigned to ps.\n    with slim.scopes.arg_scope(\n            [slim.variables.variable, slim.variables.global_step],\n            device=slim.variables.VariableDeviceChooser(num_parameter_servers)):\n```\n\nas the outermost training scope inside which the batch processing gets called ([`inception_distributed_train.py`](https://github.com/tensorflow/models/blob/master/inception/inception/inception_distributed_train.py#L113)).\n\nNot sure why exactly this became a problem for my modified setup, but now the memory increase trend has reduced at least ten-fold, and tested to run for a 100 epochs.\nMaybe the original code will work fine without this CPU device specification as well.\n", "Unfortunately I haven't been able to reproduce this, but we have seen a simpler apparent memory leak in #2942, which is caused by memory fragmentation. It's possible that this could be causing the problem here as well.\n\nOur preferred workaround is to use [tcmalloc](http://goog-perftools.sourceforge.net/doc/tcmalloc.html) as the malloc implementation, which more closely reproduces the way we run the distributed runtime internally. You can install tcmalloc with many package managers, and then use it with TensorFlow by setting `LD_PRELOAD` to include the tcmalloc library.\n\nAn alternative, which might help to test the root cause, but might also cause undesirable performance issues, is to set the `MALLOC_MMAP_THRESHOLD_` environment variable to a small value (like 100000). This should reduce fragmentation, at the possible downside of increasing the cost for some allocations, since it will use `mmap()` and `munmap()` to implement `malloc()` and `free()` for more allocations.\n", "@shendiaomo, were you able to resolve your issue using @mrry or @mmuneebs?\n", "Automatically closing due to lack of recent activity. Please reopen when further information becomes available.\n", "@mrry I love you"]}]