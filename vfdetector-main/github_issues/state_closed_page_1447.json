[{"number": 9549, "title": "Tensorflow Failed to create Session", "body": "Hi\r\n\r\nI tried basic program in python shell. It fails to create session. Please assist.\r\n\r\nThanks \r\n\r\n```\r\nPython 2.7.6 (default, Oct 26 2016, 20:30:19)\r\n[GCC 4.8.4] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.7.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.7.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.7.5 locally\r\n>>> hello = tf.constant('hi,tensorflow')\r\n>>> sess = tf.Session()\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node                                                                                                              read from SysFS had negative value (-1), but there must be at least one NUMA no                                                                                                             de, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with prop                                                                                                             erties:\r\nname: Tesla K40c\r\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.745\r\npciBusID 0000:02:00.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.10GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one                                                                                                              is currently active; existing: 0x2e0fe90\r\nE tensorflow/core/common_runtime/direct_session.cc:137] Internal: failed initial                                                                                                             izing StreamExecutor for CUDA device ordinal 1: Internal: failed call to cuDevic                                                                                                             ePrimaryCtxRetain: CUDA_ERROR_INVALID_DEVICE\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.                                                                                                             py\", line 1187, in __init__\r\n    super(Session, self).__init__(target, graph, config=config)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.                                                                                                             py\", line 552, in __init__\r\n    self._session = tf_session.TF_NewDeprecatedSession(opts, status)\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/error                                                                                                             s_impl.py\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InternalError: Failed to create session.\r\n```", "comments": ["maybe out of GPU memory? Try running with `export CUDA_VISIBLE_DEVICES=''`", "Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "System details : ubuntu 14.04 x86_64\r\nTensorflow version: 0.10.0\r\nCuda Version : 7.5\r\n\r\nI was able to install cpu version and execute my model but GPU version fails. I tried using latest Tensorflow Version 1.0 but again it was not able to create session. ", "Try upgrading your cuda drivers and sdk?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Restarting the system helped in my case.", "Getting unable to create tensorflow session. Error occurs even after restarting the system. Also I checked that the GPU consumption is almost 0. \r\n\r\ni am using windows operating system. Let me know if you need more information", "Updating NVIDIA drivers fixed this for me", "Can you provide the use information of your GPUS? Maybe somebody else is using the GPU, which leads to this problem. This is the case for me .", "I have met this problem ,and i didn't solve it. i use anonaconda3 commond conda install tensorflow-gpu but when i run a python file ,it told me tensorflow.python.framework.errors_impl.InternalError: Failed to create session.I don't think out of my memory.Because I have eight NVIDIA K80", "For me, I had to downgrade to tensorflow 1.9.0. It might be a cuda toolkit incompatibility issue, I'm not sure. It looked like tensorflow v10 used cuda toolkit 9.2, while I only had 9.0 installed.", "That helped me a lot when I updated my tensorflow to 1.10.0, thank you very much @baker-travis ", "In the case I just solved, it was updating the GPU driver to the latest and installing the cuda toolkit. First, the ppa was added and GPU driver installed:\r\n\r\n    sudo add-apt-repository ppa:graphics-drivers/ppa\r\n    sudo apt update\r\n    sudo apt install nvidia-390\r\n\r\nAfter adding the ppa, it showed options for driver versions, and 390 was the latest 'stable' version that was shown.\r\n\r\nThen install the cuda toolkit:\r\n\r\n    sudo apt install nvidia-cuda-toolkit\r\n\r\nThen reboot:\r\n\r\n    sudo reboot\r\n\r\nIt updated the drivers to a newer version than the 390 originally installed in the first step (it was 410; this was a p2.xlarge instance on AWS).", "for me  i key \"conda install cudnn==7.1.2\u201c \u3002\r\nmy previous cudnn edition was 7.3.1 which respondes to  cuda9.2_0  ,but my cuda edition is 9.0.175.so i reduce the edition of cudnn to 7.1.2 which respondes to cuda 9.0.0.", "Adding these lines worked for me. \r\n\r\n#Provide the GPU number to be used\r\nimport os\r\nos.environ['CUDA_VISIBLE_DEVICES'] =''", "If your GPU utility shows 0% and still tensorflow failed to create the session, there are some forcefully closed tensorflow session. These will not usally visible with nvidia-smi. But  `sudo fuser -v /dev/nvidia*` can capture these processes. Killing those processes can help without restarting the server."]}, {"number": 9548, "title": "distributed alexnet error : alexnet_v2/pool1/MaxPool : tensor_in must be 4-dimensional", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: linux Centos 7.1\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: tensorflow (0.11.0)\r\n\r\n\r\n### Describe the problem\r\nWhen I try to run distributed alexnet training in TensorFlow, the alexnet model definition is referred to [here](https://github.com/tensorflow/models/blob/master/slim/nets/alexnet.py). The Error occurred at 'alexnet_v2/pool1/MaxPool' as follows\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: tensor_in must be 4-dimensional\r\n         [[Node: alexnet_v2/pool1/MaxPool = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:worker/replica:0/task:0/cpu:0\"](alexnet_v2/conv1/Relu)]]\r\n```\r\n\r\n### Source code / logs\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport argparse\r\nfrom datetime import datetime\r\nimport math\r\nimport sys\r\nimport time\r\n\r\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\r\nimport tensorflow as tf\r\n\r\ntf.app.flags.DEFINE_string(\"ps_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\r\ntf.app.flags.DEFINE_string(\"worker_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\r\n\r\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\r\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\r\ntf.app.flags.DEFINE_integer(\"batch_size\", 100, \"Training batch size\")\r\ntf.app.flags.DEFINE_integer('num_batches', 100, \"Number of batches to run.\")\r\n\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\nslim = tf.contrib.slim\r\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0, 0, stddev)\r\n\r\ndef alexnet_v2_arg_scope(weight_decay=0.0005):\r\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\r\n                      activation_fn=tf.nn.relu,\r\n                      biases_initializer=tf.constant_initializer(0.1),\r\n                      weights_regularizer=slim.l2_regularizer(weight_decay)):\r\n    with slim.arg_scope([slim.conv2d], padding='SAME'):\r\n      with slim.arg_scope([slim.max_pool2d], padding='VALID') as arg_sc:\r\n        return arg_sc\r\n\r\n\r\ndef alexnet_v2(inputs,\r\n               num_classes=1000,\r\n               is_training=True,\r\n               dropout_keep_prob=0.5,\r\n               spatial_squeeze=True,\r\n               scope='alexnet_v2'):\r\n  with tf.variable_scope(scope, 'alexnet_v2', [inputs]) as sc:\r\n    end_points_collection = sc.name + '_end_points'\r\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\r\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\r\n                        outputs_collections=[end_points_collection]):\r\n      net = slim.conv2d(inputs, 64, [11, 11], 4, padding='VALID',\r\n                        scope='conv1')\r\n      net = slim.max_pool2d(net, [3, 3], 2, scope='pool1')\r\n      net = slim.conv2d(net, 192, [5, 5], scope='conv2')\r\n      net = slim.max_pool2d(net, [3, 3], 2, scope='pool2')\r\n      net = slim.conv2d(net, 384, [3, 3], scope='conv3')\r\n      net = slim.conv2d(net, 384, [3, 3], scope='conv4')\r\n      net = slim.conv2d(net, 256, [3, 3], scope='conv5')\r\n      net = slim.max_pool2d(net, [3, 3], 2, scope='pool5')\r\n\r\n      # Use conv2d instead of fully_connected layers.\r\n      with slim.arg_scope([slim.conv2d],\r\n                          weights_initializer=trunc_normal(0.005),\r\n                          biases_initializer=tf.constant_initializer(0.1)):\r\n        net = slim.conv2d(net, 4096, [5, 5], padding='VALID',\r\n                          scope='fc6')\r\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\r\n                           scope='dropout6')\r\n        net = slim.conv2d(net, 4096, [1, 1], scope='fc7')\r\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\r\n                           scope='dropout7')\r\n        net = slim.conv2d(net, num_classes, [1, 1],\r\n                          activation_fn=None,\r\n                          normalizer_fn=None,\r\n                          biases_initializer=tf.zeros_initializer,\r\n                          scope='fc8')\r\n\r\n      # Convert end_points_collection into a end_point dict.\r\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\r\n      if spatial_squeeze:\r\n        net = tf.squeeze(net, [1, 2], name='fc8/squeezed')\r\n        end_points[sc.name + '/fc8'] = net\r\n      return net, end_points\r\n\r\ndef main(_):\r\n\r\n  #Construct the cluster and start the server\r\n  ps_hosts = FLAGS.ps_hosts.split(\",\")\r\n  worker_hosts = FLAGS.worker_hosts.split(\",\")\r\n\r\n  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\r\n\r\n  server = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)  \r\n\r\n\r\n  if FLAGS.job_name == \"ps\":\r\n    server.join()\r\n\r\n  elif FLAGS.job_name == \"worker\":\r\n    with tf.device(tf.train.replica_device_setter(worker_device=\"/job:worker/task:%d\" % FLAGS.task_index, cluster=cluster)):\r\n\r\n      image_size = 224\r\n      images = tf.Variable(tf.random_normal([FLAGS.batch_size, image_size, image_size, 3], dtype=tf.float32, stddev=1e-1))\r\n      \r\n      with slim.arg_scope(alexnet_v2_arg_scope()):\r\n        logits, end_points = alexnet_v2(images, is_training = False)    \r\n      saver = tf.train.Saver()\r\n      summary_op = tf.merge_all_summaries()\r\n      #summary_op = tf.summary.merge_all()\r\n      init_op = tf.global_variables_initializer()\r\n\r\n    # Create a Supervisor that will checkpoint the model and computes summaries\u3002\r\n    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0), logdir=\"./alexnet_train_logs\", init_op=init_op, summary_op=summary_op, saver=saver, save_model_secs=600)\r\n\r\n    # Get a TensorFlow session managed by the supervisor.\r\n    with sv.managed_session(server.target) as sess:\r\n      num_steps_burn_in = 10\r\n      total_duration = 0.0\r\n      total_duration_squared = 0.0\r\n      for i in xrange(FLAGS.num_batches + num_steps_burn_in):\r\n        start_time = time.time()\r\n        _ = sess.run(logits)\r\n        duration = time.time() - start_time\r\n        if i >= num_steps_burn_in:\r\n          if not i % 10:\r\n            print ('%s: step %d, duration = %.3f' % (datetime.now(), i - num_steps_burn_in, duration))\r\n        total_duration += duration\r\n        total_duration_squared += duration * duration\r\n      mn = total_duration / FLAGS.num_batches\r\n      vr = total_duration_squared / FLAGS.num_batches - mn * mn\r\n      sd = math.sqrt(vr)\r\n      print ('%s: across %d steps, %.3f +/- %.3f sec / batch' % (datetime.now(), FLAGS.num_batches, mn, sd))\r\n         \r\n    # Stop TensorFlow Session\r\n    sv.stop()\r\n\r\nif __name__ == \"__main__\":\r\n  tf.app.run()\r\n```\r\n\r\n", "comments": ["Can you try two things:\r\n- Use directly the alexnet_v2 network\r\n- Use just try using images = tf.random_normal([FLAGS.batch_size, image_size, image_size, 3], dtype=tf.float32, stddev=1e-1)", "Re-open if needed."]}, {"number": 9547, "title": "Fix build issue when `/usr/bin/python` path is not available", "body": "This fix tries to fix the build error where `PYTHON_BIN_PATH` is not used in `tf_version_info_genrule()` (which will cause build error if `python` is not available in the `PATH`).\r\n\r\n(See Ln 1188 in `tensorflow/tensorflow.bzl`)\r\n\r\nThis could be an issue even if `python` is available yet a different python location is used in `./configure`.\r\n\r\nThis fix fixes the issue by passing `$(PYTHON_BIN_PATH)` explicitly so that same python version could be used:\r\n```udiff\r\n-      \"$(location //tensorflow/tools/git:gen_git_source.py) --generate $(SRCS) \\\"$@\\\"\",\r\n+      \"$(PYTHON_BIN_PATH) $(location //tensorflow/tools/git:gen_git_source.py) --generate $(SRCS) \\\"$@\\\"\",\r\n```\r\n\r\n**Below is the detailed issue description:**\r\n\r\nIn one of the build machines (Ubuntu 16.04) only python3 is available\r\n(/usr/bin/python3) without python 2:\r\n```\r\nubuntu@ubuntu:~/tensorflow$ python --version\r\npython: command not found\r\nubuntu@ubuntu:~/tensorflow$ python3 --version\r\nPython 3.5.2+\r\nubuntu@ubuntu:~/tensorflow$ which python3\r\n/usr/bin/python3\r\n```\r\n\r\nAfter configure with `/usr/bin/python3` through `./configure`:\r\n```\r\nubuntu@ubuntu:~/tensorflow$ ./configure\r\nPlease specify the location of python. [Default is /usr/bin/python3]:\r\n...\r\n```\r\n\r\nThe following is the error:\r\n```\r\nubuntu@ubuntu:~/tensorflow$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: /home/ubuntu/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': Use SavedModel Builder instead.\r\nWARNING: /home/ubuntu/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': Use SavedModel instead.\r\nINFO: Found 1 target...\r\nERROR: /home/ubuntu/tensorflow/tensorflow/core/BUILD:1363:1: Executing genrule //tensorflow/core:version_info_gen failed: bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 127.\r\n/usr/bin/env: 'python': No such file or directory\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 3.548s, Critical Path: 0.20s\r\n```\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "@yongtang I had to revert this CL, because it turns out it broke with a parallel change to unhide the bazel.rc file -- this exposes a problem where PYTHON_BIN_PATH is not being exported in one of the android CI configurations (because it uses a 'sandboxed' build strategy).\r\n\r\nI think @andrewharp may be looking at what we can do here."]}, {"number": 9546, "title": "Update get_started.md", "body": "replace deprecated function", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "initialize_all_variables() is the deprecated API, the new one is global_variables_initializer().  Thanks for the PR though!"]}, {"number": 9545, "title": "Duplicate variable shown in Tensorboard expected?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nMacOS Sierra 12.12.4\r\n- **TensorFlow installed from (source or binary)**:\r\npip\r\n- **TensorFlow version (use command below)**:\r\n1.1.0 (CPU)\r\n### Describe the problem\r\nI am trying to implement E2C (available from https://arxiv.org/pdf/1506.07365.pdf). Basically it is a neural network that is used for learning a transition model using neural networks. In the training set I have data of the form (X_t, X_t+1) where both X_t and X_t+1 needs to be transformed by an encoding network (e.g. a variational autoencoder). I use the following snippet for creating the encoding network (adapted from https://github.com/ericjang/e2c):\r\n\r\n```python\r\n    def encode(self, x, share=None):\r\n        fc = tf.contrib.layers.fully_connected\r\n        with tf.variable_scope('Encoder', reuse=share):\r\n            l1 = fc(x, 400, weights_initializer=tf.orthogonal_initializer(),\r\n                    activation_fn=tf.nn.relu)\r\n            l2 = fc(l1, 100, weights_initializer=tf.orthogonal_initializer(),\r\n                    activation_fn=tf.nn.relu)\r\n            return l2\r\n\r\n    def decode(self, z, share=None):\r\n        fc = tf.contrib.layers.fully_connected\r\n        with tf.variable_scope(\"Decoder\", reuse=share):\r\n            l1 = fc(z, 100, weights_initializer=tf.orthogonal_initializer(1.1),\r\n                    activation_fn=tf.nn.relu)\r\n            l2 = fc(l1, 400, weights_initializer=tf.orthogonal_initializer(1.1),\r\n                    activation_fn=tf.nn.relu)\r\n\r\n            return fc(l2, self.x_dim,\r\n                      weights_initializer=tf.orthogonal_initializer(1.1),\r\n                      activation_fn=tf.nn.sigmoid)\r\n```\r\nThen I would use something like\r\n\r\n```python\r\nh_enc_t = encoder(X_t)\r\nh_enc_t_next = encoder(X_{t+1}, share=True)\r\n```\r\nto create the encoded output for the model.\r\n\r\nThe problem is that when visualizing this on Tensorboard, while it is sharing the variables by setting `share=True` for the variable scope, on the graph visulisation you will have `Encoder` and `Encoder_1` instead of just a `Decoder` scope. Of course they took different input since we need to transform X_t and X_t+1, but shouldn't the network be wrapped in the same scope since underneath we are reusing the same weights? I wonder if it is a feature to have `Encoder_1` and `Encoder` separately or it is a limitation of the variable scoping. The problem is illustrated in the screenshot below, you will see duplicates for 'Encoder' 'SampleQPhi\" etc:\r\n\r\n![graph-run](https://cloud.githubusercontent.com/assets/6040760/25559064/98fecc5a-2d2b-11e7-8669-00b1227abf17.png)\r\n\r\nHowever, I would expect something like this (as appeared in the paper) to be a more reasonable visualization (h_enc) with input x_t and x_t+1 are the same network:\r\n\r\n<img width=\"546\" alt=\"screenshot 2017-04-30 15 51 01\" src=\"https://cloud.githubusercontent.com/assets/6040760/25565322/fc04ab8a-2dbc-11e7-9876-5e823a3bf47b.png\">\r\n\r\n\r\nMany thanks in advance!", "comments": ["@dandelionmane, could you please take a look?", "I need to take a look more closely, but I think calling `tf.scope` creates a new namespace in general, appending _[Some int] to the name if necessary for uniqueness.\r\n\r\nThis might deviate from your intention, but what if you move the `Encoder` scope outside of `encode` like this?\r\n\r\n```\r\nwith tf.variable_scope('Encoder'):\r\n  h_enc_t = encoder(X_t)\r\n  h_enc_t_next = encoder(X_{t+1}, share=True)\r\n```\r\n\r\nAnd then\r\n\r\n```\r\ndef encode(self, x, share=None):\r\n  fc = tf.contrib.layers.fully_connected\r\n  with tf.variable_scope('encode', reuse=share):\r\n    l1 = fc(x, 400, weights_initializer=tf.orthogonal_initializer(),\r\n            activation_fn=tf.nn.relu)\r\n    l2 = fc(l1, 100, weights_initializer=tf.orthogonal_initializer(),\r\n            activation_fn=tf.nn.relu)\r\n    return l2\r\n```\r\n\r\n?", "@chihuahua Nope this won't work. You would obtain something that's even weirder.\r\n\r\nThis is the graph:\r\n![image](https://cloud.githubusercontent.com/assets/6040760/25756242/f0754a2c-31bd-11e7-8951-2ae16760e318.png)\r\n\r\nA closer look:\r\n![image](https://cloud.githubusercontent.com/assets/6040760/25756118/81e85bee-31bd-11e7-9ad3-016893b18d14.png)\r\n\r\nThe reason I think this is not the intended behavior is because. In some settings, it's really common to take a part of the computation graph as a module and take input from different sources. It turns out that Tensorboard does not recognize that the same part of the graph is used for different inputs. Thus it fails to use a common scope for them.\r\n\r\n**Update**\r\nI tried to use [make_template](https://www.tensorflow.org/api_docs/python/tf/make_template) to see this would work in that setting. But apparently it is not....\r\n", "Reusing a lot of layers I encounter the same messy graphs in TensorBoard.\r\nIt would be nice to have the two namespaces merged if they contain the same variables!", "Have same issue", "Any updates on this in recent releases?", "Why does a `tf.VariableScope` open up a name scope?\r\nI am confused why in the end I have a node called `ns/s4_1/concat` and one called `s4/concat`.\r\nI would expect it to be `ns/s4/concat` and `ns/s4/concat_1` or something\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nwith tf.name_scope('ns'):\r\n  s4 = tf.VariableScope(tf.AUTO_REUSE, name='s4')\r\n\r\n  with tf.variable_scope(s4, tf.AUTO_REUSE):\r\n    print(tf.get_variable('f', []))\r\n\r\n  with tf.variable_scope(s4, True):\r\n    print(tf.get_variable('f', []))\r\n    print(tf.concat([tf.constant([1]), tf.constant([1])], 0))\r\n\r\nwith tf.variable_scope(s4, True):\r\n    print(tf.concat([tf.constant([1]), tf.constant([1])], 0))\r\n\r\nsession = tf.Session()\r\n\r\ntf.summary.FileWriter('./tmp/scope/', session.graph)\r\n```", "Maybe @jart has some ideas?", "A workaround to prevent variable scope from opening new namescopes is the following\r\n\r\n```python\r\nfrom tensorflow.python.ops import variable_scope as var_scope\r\n\r\ndef simple_variable_scope(name_or_scope, reuse=None):\r\n  \"\"\"Creates a variable scope without also creating a name scope.\"\"\"\r\n  return var_scope.variable_scope(name_or_scope, reuse=reuse,\r\n                                  auxiliary_name_scope=False)\r\n```", "@dsmilkov Is this a tensorboard-side issue?", "I still observe that even when a variable scope does not create an extra name scope tensorboard still treats parts of the graph that receive varying inputs as multiple/duplicate nodes.", "Does this Problem by any chance take up excess gpu memory ?", "I would hope not, but feel free to create a test case.", "I had the same issue issue: each time the variable_scope is closed and reopened, it creates a new scope name by appending some integer at the end. As a consequence, the variables are duplicated (as we can witness on tensorboard).\r\nMy (unelegant) solution to the problem: add everything that has to rest under the same scope in one go and close the scope afterward. I am aware that this might not be a solution in a lot of cases but I hope that it will help at least some!\r\n", "/cc @tanzhenyu Probably related to https://github.com/tensorflow/tensorflow/issues/16468#issuecomment-385662734", "Anybody tested if this has any memory impact?", "@ewilderj What is the default policy to handle these cases? \r\nThe assignee was more the 1 year inactive (without comments) on this and was labeled as bug. \r\nEvery time someone make a comment the bot reset the Nagging Assignee counter. \r\nHow the TF team could be aware of the status of issues like this one (it is not the only one with a similar status/history)?\r\nI think that the @tensorflowbutler needs to count only TF team no activity days. \r\n\r\n/cc @av8ramit  What do you think?", "@aselle thoughts?", "@av8ramit sorry I didn't want to bother you. Was just for the @tensorflowbutler counting behavior :smile:  I think that the nagging assignee counter doesn't need to have a reset after non TF team members comments.", "@qmeeus If you really want to reuse name scopes then you can do this:\r\n```python\r\ndef absolute_name_scope(scope):\r\n  \"\"\"Builds an absolute tf.name_scope relative to the current_scope.\r\n  This is helpful to reuse nested name scopes.\r\n\r\n  E.g. The following will happen when using regular tf.name_scope:\r\n\r\n    with tf.name_scope('outer'):\r\n      with tf.name_scope('inner'):\r\n        print(tf.constant(1)) # Will print outer/inner/Const:0\r\n    with tf.name_scope('outer'):\r\n      with tf.name_scope('inner'):\r\n        print(tf.constant(1)) # Will print outer/inner_1/Const:0\r\n\r\n  With absolute_name_scope:\r\n\r\n    with absolute_name_scope('outer'):\r\n      with absolute_name_scope('inner'):\r\n        print(tf.constant(1)) # Will print outer/inner/Const:0\r\n    with absolute_name_scope('outer'):\r\n      with absolute_name_scope('inner'):\r\n        print(tf.constant(1)) # Will print outer/inner/Const_1:0\r\n  \"\"\"\r\n  current_scope = tf.get_default_graph().get_name_scope()\r\n  if not current_scope:\r\n    if scope.endswith('/'):\r\n      name_scope = tf.name_scope(scope)\r\n    else:\r\n      name_scope = tf.name_scope('{}/'.format(scope))\r\n  else:\r\n    name_scope = tf.name_scope('{}/{}/'.format(current_scope, scope))\r\n  return name_scope\r\n```\r\n\r\nTensorflow team, I can also add this as a PR if desired. I find this rather useful to get clean scopes in Tensorboard.", "It is not a problem at all @bhack and absolutely no bother. I'm always here to help the community. I'll default to Andy on this one. ", "Im not sure if it does have an impact on memory usage but I was training a model similar to segnet with image input size as 640x1024 and, at max I could only set my batch size to 1 image which is kind of strange (not sure if it should take that much memory) i was using a titanX (12gb ) and it ended up using the whole 12 gb.\r\n(if I tried increasing batch size to anything greater than 1 I got CUDA out of memory error)\r\nIs there any way to check how much memory a particular model should be taking ?", "@rajnunes Have you tried to check memory with https://www.tensorflow.org/guide/graph_viz#runtime_statistics?", "thanks @sleighsoft, appreciate it! \r\n\r\n@bhack the memory use was the thing that tipped me off. Indeed, I think that the duplication impacts memory, as I was getting those \"More than 10% of total memory allocated\" warnings and eventually, the program crashed... Those warnings and crashed disappeared after modifying my code\r\n", "@qmeeus I don't know if it is strictly related to the `variable_scope` handling. \r\nIn cases like https://github.com/tensorflow/tensorflow/issues/16468#issuecomment-385662734:\r\n I am not using an explicit variable scope but seems that has some impact on TB grouping/rendering.\r\nThe runtime statistics on the Tensorboard tell that `out_a` and `out_b` has its own memory allocated and on the TB seems a duplicated allocation.", "@bhack if there are systemic issues with the bug nagging/updating, then let's file this as a bug itself so we can track it, assign it, and collect the multiple instances of where it's happening. Thanks for highlighting this!", "@ewilderj \n> the @tensorflowbutler counting behavior \ud83d\ude04 I think that the nagging assignee counter doesn't need to have a reset after non TF team members comments. \n\nThis could help to have a better rapresentation of MIA", "The normal TensorFlow way to handle using the same scope in two places is:\r\n\r\n```\r\n    with tf.name_scope('outer'):\r\n      with tf.name_scope('inner') as scope:\r\n        print(tf.constant(1)) # Will print outer/inner/Const:0\r\n    with tf.name_scope(scope):\r\n      print(tf.constant(1)) # Will print outer/inner/Const_1:0\r\n```", "AFAIK, this bug is not about shared scopes but having variables abnormally duplicated in tensorboard plots when said variable is used at different locations in the code.", "I am facing the same issue. Its a problem with tensor-board creating extra instances of shared layers.\r\n\r\n```\r\ndef inference():\r\n    with tf.variable_scope(\"inner\", reuse=tf.AUTO_REUSE):\r\n        x = tf.placeholder(dtype = tf.float32, shape=(None, 30,30,3))\r\n        h = tf.layers.conv2d(x, 5, [3, 3], [1, 1], padding='SAME')\r\n        print(tf.get_default_graph().get_name_scope()) \r\n        return h\r\n\r\nwith tf.variable_scope(\"outer\"):\r\n    x = inference() _# will print outer/inner_\r\n    x2 = inference() _# will print outer/inner_1_\r\n```\r\n\r\n\r\nBut output of the model summary shows that the variables are being shared.\r\n\r\n```\r\nouter/inner/conv2d/kernel:0 (float32_ref 3x3x3x5) [135, bytes: 540]\r\nouter/inner/conv2d/bias:0 (float32_ref 5) [5, bytes: 20]\r\nTotal size of variables: 140\r\nTotal bytes of variables: 560\r\n```\r\n\r\n@sleighsoft's PR is still in review. Is there any suggested workaround?", "This is a stale issue. Please check the issue with latest TensorFlow. If the issue still persists in the newer version of TF, please feel free to reopen it by providing details about the issue and a standalone code to reproduce the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=9545\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=9545\">No</a>\n", "The graph associated to this model:\r\n```python\r\nimport tensorflow as tf\r\n\r\n# Meaningless model\r\ninput_A = tf.keras.Input(shape=(1,2), name='Input_A')\r\ninput_B = tf.keras.Input(shape=(1,2), name='Input_B')\r\n\r\ninner_block = tf.keras.Sequential(\r\n    [ tf.keras.layers.Dense(1) for i in range(50) ])\r\n\r\noutput = inner_block(input_A) + inner_block(input_B)\r\n\r\nmodel = tf.keras.Model((input_A,input_B), output)\r\n\r\n# Write\r\ntf.keras.callbacks.TensorBoard('.', write_graph=True).set_model(model)\r\n```\r\nis:\r\n![Screenshot from 2020-01-05 17-13-11](https://user-images.githubusercontent.com/25563883/71782839-f2255700-2fde-11ea-9efd-3296200885fc.png)\r\nAll layers and namespaces are working as expected, however the data flow is made more complex by dependencies of ReadVariableOp inside sequential_1. In more complex models, the real inputs become the thinner connections and they are hard to follow. Also, shared layers appear with different colors or gray (probably due to the different ops inside the original and reused variables). For example, in\r\n<img src=\"https://user-images.githubusercontent.com/25563883/71783501-e5593100-2fe7-11ea-8ffa-632e212b3c99.png\" width=\"452\" height=\"300\">\r\nthe three generators are the same layer, called three times, and the thicker arrows are propagating variables.\r\n\r\nI don't want to force Tf to merge namescopes, as this would make each block accept multiple inputs, giving a wrong visual hint (multiple dependencies, not runs).\r\nA nice toggle in TensorBoard to exclude inputs of ReadVariableOp (and why not, control dependencies) would simplify graphs a lot.\r\n\r\nDetails: Tensorflow 2.1.0-rc0 built from source.\r\n", "Maybe you should open this as a separate issue at /tensorboard.", "Posted [https://github.com/tensorflow/tensorboard/issues/3118](https://github.com/tensorflow/tensorboard/issues/3118), thanks"]}, {"number": 9544, "title": "Branch 154631802", "body": "", "comments": ["Looks like this test failure is related to PR #8461. Any thoughts, @karandesai-96 ?\r\n\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-cpu/4602/consoleFull\r\n\r\n```\r\n======================================================================\r\nFAIL: testConv3DTransposeBiasRegularizer (__main__.Conv3DTransposeTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/bazel_pip/tensorflow/python/layers_convolutional_test.runfiles/org_tensorflow/bazel_pip/tensorflow/python/layers/convolutional_test.py\", line 763, in testConv3DTransposeBiasRegularizer\r\n    self.assertListEqual(layer.losses, loss_keys)\r\nAssertionError: Lists differ: [] != [<tf.Tensor 'conv3d_transpose/...\r\n\r\nSecond list contains 1 additional elements.\r\nFirst extra element 0:\r\nTensor(\"conv3d_transpose/bias/Regularizer/mul:0\", shape=(), dtype=float32)\r\n\r\n- []\r\n+ [<tf.Tensor 'conv3d_transpose/bias/Regularizer/mul:0' shape=() dtype=float32>]\r\n\r\n======================================================================\r\nFAIL: testConv3DTransposeKernelRegularizer (__main__.Conv3DTransposeTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/bazel_pip/tensorflow/python/layers_convolutional_test.runfiles/org_tensorflow/bazel_pip/tensorflow/python/layers/convolutional_test.py\", line 753, in testConv3DTransposeKernelRegularizer\r\n    self.assertListEqual(layer.losses, loss_keys)\r\nAssertionError: Lists differ: [] != [<tf.Tensor 'conv3d_transpose/...\r\n\r\nSecond list contains 1 additional elements.\r\nFirst extra element 0:\r\nTensor(\"conv3d_transpose/kernel/Regularizer/mul:0\", shape=(), dtype=float32)\r\n\r\n- []\r\n+ [<tf.Tensor 'conv3d_transpose/kernel/Regularizer/mul:0' shape=() dtype=float32>]\r\n\r\n----------------------------------------------------------------------\r\nRan 73 tests in 3.422s\r\n\r\nFAILED (failures=2)\r\n================================================================================\r\n```", "Hi @caisq, this is strange. I pulled your branch locally and tested it. I could not reproduce the error. Bazel tests passed successfully.", "@karandesai-96 Let me run it again and see if it is reproducible or flaky. This may have to do with some of the layers refactoring in this push.\r\n\r\ncc @fchollet ", "@tensorflow-jenkins test this please", "Note that after the refactoring, one should create layer variables via the `add_variable` method (see other layers for examples). This takes care of keeping track of the weights and losses.\r\n\r\nBasically the recently introduced `Conv3DTranspose` layer (added via a GitHub PR) should be edited to include this fix. It's just a 2-line change. How do we proceed?", "@fchollet @karandesai-96 Since it is just a two-line fix, can you send me the diff so that I can add a commit to this push to fix it?", "In `Conv3DTranspose`, replace\r\n\r\n```python\r\n +    self.kernel = vs.get_variable('kernel',\r\n +                                  shape=kernel_shape,\r\n +                                  initializer=self.kernel_initializer,\r\n +                                  regularizer=self.kernel_regularizer,\r\n +                                  trainable=True,\r\n +                                  dtype=self.dtype)\r\n +    if self.use_bias:\r\n +      self.bias = vs.get_variable('bias',\r\n +                                  shape=(self.filters,),\r\n +                                  initializer=self.bias_initializer,\r\n +                                  regularizer=self.bias_regularizer,\r\n +                                  trainable=True,\r\n +                                  dtype=self.dtype)\r\n```\r\n\r\nwith\r\n\r\n```python\r\n +    self.kernel = self.add_variable(name='kernel',\r\n +                                  shape=kernel_shape,\r\n +                                  initializer=self.kernel_initializer,\r\n +                                  regularizer=self.kernel_regularizer,\r\n +                                  trainable=True,\r\n +                                  dtype=self.dtype)\r\n +    if self.use_bias:\r\n +      self.bias = self.add_variable(name='bias',\r\n +                                  shape=(self.filters,),\r\n +                                  initializer=self.bias_initializer,\r\n +                                  regularizer=self.bias_regularizer,\r\n +                                  trainable=True,\r\n +                                  dtype=self.dtype)\r\n```", "@fchollet Thanks. Done.", "@tensorflow-jenkins test this please"]}, {"number": 9543, "title": "Branch 154631802", "body": "", "comments": []}, {"number": 9542, "title": "PreventGradient Signature Changed in TF 1.1.0 Breaking Forward Compatibility when Loading / Running Graph", "body": "Loading and then attempting to run a graph that was saved by TF 1.1.0 on TF 1.0.1 leads to:\r\n```\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 767, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 965, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef mentions attr 'message' not in Op<name=PreventGradient; signat\r\nure=input:T -> output:T; attr=T:type>; NodeDef: gradients/sparse_softmax_cross_entropy_loss/xentropy/xentropy_grad/PreventGradient =\r\n PreventGradient[T=DT_FLOAT, message=\"Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_l\r\nogits due to the fused implementation\\'s interaction with tf.gradients()\"](sparse_softmax_cross_entropy_loss/xentropy/xentropy:1)\r\n```\r\nIt seems as if the the NodeDef proto has changed in a manner that breaks older versions. The new field, `message` seems like it should be optional.\r\n\r\nSystem info:\r\n```\r\n== cat /etc/issue ===============================================\r\nLinux 6da178ae8f14 3.13.0-57-generic #95-Ubuntu SMP Fri Jun 19 09:28:15 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.2 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux 6da178ae8f14 3.13.0-57-generic #95-Ubuntu SMP Fri Jun 19 09:28:15 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.12.0)\r\nprotobuf (3.2.0)\r\ntensorflow (1.0.1)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are a\r\nvailable on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are\r\n available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are\r\n available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are av\r\nailable on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are a\r\nvailable on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are av\r\nailable on your machine and could speed up CPU computations.\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there\r\nmust be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:1e.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus\r\nid: 0000:00:1e.0)\r\ntf.VERSION = 1.0.1\r\ntf.GIT_VERSION = v1.0.0-65-g4763edf-dirty\r\ntf.COMPILER_VERSION = v1.0.0-65-g4763edf-dirty\r\nSanity check: array([1], dtype=int32)\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nSat Apr 29 12:36:12 2017\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           Off  | 0000:00:1E.0     Off |                    0 |\r\n| N/A   53C    P0    57W / 149W |      0MiB / 11439MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61\r\n```", "comments": ["This is working as intended, based on https://www.tensorflow.org/programmers_guide/data_versions#backward_and_partial_forward_compatibility\r\n\r\nForwards compatibility only really exists for patch releases; if you want forwards compatibility beyond that, there's more work you have to do (typically, it involves graph rewriting).  Some of the utilities in graph_def_util.h are precisely meant for this: https://github.com/tensorflow/tensorflow/blob/799e31f3840c21322e380e1ec6e5bacb95d016fa/tensorflow/core/framework/graph_def_util.h#L88, but unfortunately the user has to deal with this.\r\n\r\n(Reminder: You can't have both infinite forwards compatibility and infinite backwards compatibility without that meaning nothing ever changes :)).\r\n"]}, {"number": 9541, "title": "Several typos", "body": "This fix fixes a couple of typos:\r\n```\r\ns/unneccessary/unnecessary/\r\ns/accidently/accidentally/\r\n```", "comments": ["Can one of the admins verify this patch?"]}, {"number": 9540, "title": "Add support of Sparse Tensor Slice", "body": "This fix tries to address the issue raised in #1588 to support slice of sparse tensors.\r\n\r\nThis fix fixes #1588.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "This sounds good for API review.", "Can one of the admins verify this patch?", "@yongtang Whendo you think you can address @ebrevdo's comments?", "@ebrevdo @josh11b @vrv @rmlarsen Thanks for the review and sorry for the delay. The PR has been updated to address the comments. Please take a look and let me know if there are any issues.", "Looks good to me, @ebrevdo can you take another look and approve/request more changes?", "Jenkins, test this please.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.", "@martinwicke The Jenkins build error seems to be for real. Let me take a look and see if I could fix it.", "@martinwicke The PR has been updated with build error fixed. Please take a look.", "Jenkins, test this please.", "Jenkins, test this please.", "@martinwicke There was another Jenkins error so the PR has been updated again. Now all the tests should pass. Sorry for the back and forth updates in the last several PR updates.", "Not your fault. Thanks for updating!\r\n\r\nJenkins, test this please.", "@martinwicke The PR has been updated with API goldens updater so that `//tensorflow/tools/api/tests:api_compatibility_test` passes. Please take a look.", "@tensorflow-jenkins test this please", "It looks like @martinwicke approved previously, so good to go?", "Good to go."]}, {"number": 9539, "title": "load a checkpoint and use it to create a new graph", "body": "**system information:**\r\nI am using the latest Tensorflow code on Ubuntu 16.04\r\n\r\n**problem:**\r\nbecause the tensorflow SSD can't directly output the final bounding box that i want, so i want to use the orginal checkpoint to create my graph. But i failed, i really wish someone could help me!!! Thanks!!\r\n\r\n**error:**\r\ni get the error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"haha.py\", line 43, in <module>\r\n    select_threshold=select_threshold, img_shape=net_shape, num_classes=2, decode=True)\r\n  File \"/home/wahaha/documents/haha/SSD-Tensorflow-master/nets/np_methods.py\", line 120, in ssd_bboxes_select\r\n    select_threshold, img_shape, num_classes, decode)\r\n  File \"/home/wahaha/documents/haha/SSD-Tensorflow-master/nets/np_methods.py\", line 70, in ssd_bboxes_select_layer\r\n    localizations_layer = ssd_bboxes_decode(localizations_layer, anchors_layer)\r\n  File \"/home/wahaha/documents/haha/SSD-Tensorflow-master/nets/np_methods.py\", line 35, in ssd_bboxes_decode\r\n    (-1, l_shape[-2], l_shape[-1]))\r\n  File \"/usr/lib/python2.7/dist-packages/numpy/core/fromnumeric.py\", line 224, in reshape\r\n    return _wrapit(a, 'reshape', newshape, order=order)\r\n  File \"/usr/lib/python2.7/dist-packages/numpy/core/fromnumeric.py\", line 48, in _wrapit\r\n    result = getattr(asarray(obj), method)(*args, **kwds)\r\nValueError: total size of new array must be unchanged\r\n```\r\n\r\n**Source code**\r\n```\r\nimport os\r\nimport math\r\nimport random\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nslim = tf.contrib.slim\r\nimport matplotlib.image as mpimg\r\n\r\nimport sys\r\nsys.path.append('../')\r\n\r\nfrom nets import ssd_vgg_300, ssd_common, np_methods\r\nfrom preprocessing import ssd_vgg_preprocessing\r\n\r\ngpu_options = tf.GPUOptions(allow_growth=True)\r\nconfig = tf.ConfigProto(log_device_placement=False, gpu_options=gpu_options)\r\n\r\nwith tf.Graph().as_default() as g:\r\n    with g.name_scope('haha'):\r\n\t\tnet_shape = (300, 300)\r\n\t\tdata_format = 'NHWC'\r\n\t\tselect_threshold=0.5\r\n\t\tnms_threshold=.45\r\n\t\t# Create graph\r\n\t\timage_input=tf.placeholder(tf.float32,shape=[None,None,3],name='input')\r\n\r\n\t\theight = image_input.shape[0]\r\n\t\twidth = image_input.shape[1]\r\n\r\n\t\timage_pre, labels_pre, bboxes_pre, bbox_img = ssd_vgg_preprocessing.preprocess_for_eval(\r\n\t\t\timage_input, None, None, net_shape, data_format, resize=ssd_vgg_preprocessing.Resize.WARP_RESIZE)\r\n\t\timage_4d = tf.expand_dims(image_pre, 0)\r\n\t\t# Define the SSD model.\r\n\t\treuse = True if 'ssd_net' in locals() else None\r\n\t\tssd_net = ssd_vgg_300.SSDNet()\r\n\t\twith slim.arg_scope(ssd_net.arg_scope(data_format=data_format)):\r\n\t\t\tpredictions, localisations, _, _ = ssd_net.net(image_4d, is_training=False, reuse=reuse)\r\n\t\tssd_anchors = ssd_net.anchors(net_shape)\r\n\t\trclasses, rscores, rbboxes = np_methods.ssd_bboxes_select(\r\n\t\t\tpredictions, localisations, ssd_anchors,\r\n\t\t\tselect_threshold=select_threshold, img_shape=net_shape, num_classes=2, decode=True)\r\n\t\t\t    \r\n\t\trbboxes = np_methods.bboxes_clip(rbbox_img, rbboxes)\r\n\t\trclasses, rscores, rbboxes = np_methods.bboxes_sort(rclasses, rscores, rbboxes, top_k=400)\r\n\t\trclasses, rscores, rbboxes = np_methods.bboxes_nms(rclasses, rscores, rbboxes, nms_threshold=nms_threshold)\r\n\t\t\t    # Resize bboxes to original image shape. Note: useless for Resize.WARP!\r\n\t\trbboxes = np_methods.bboxes_resize(rbbox_img, rbboxes)\r\n\r\n\t\ttemp=tf.stack([height,width,height,width])\r\n\t\trbboxes=rbboxes*temp\r\n\t\tfacePredictions=rbboxes\r\n\t\tsaver = tf.train.Saver()\r\n\r\nimage_test=np.ones((500,500,3))\r\n\r\nwith tf.Session(config=config) as sess:\r\n\tsess.run(tf.global_variables_initializer())\r\n\tsaver.restore(sess, \"/home/wahaha/documents/haha/SSD-Tensorflow-master/log/model.ckpt-50000\")\r\n\tpredictions_val=facePredictions.eval(feed_dict={image_input:image_test})\r\n\toutput_graph_def = tf.graph_until.convert_variables_to_constants(sess, g.as_graph_def, output_node_names=['haha'])\r\n\r\n\twith tf.gfile.FastGFile(hahaFace.pb, mode = 'wb') as f:\r\n\t\tf.write(output_graph_def.SerializeToString())\r\n```", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9538, "title": "Spelling", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 9537, "title": "Cifar10 classification with Resnet", "body": "This tutorial is a basic one regarding classification of a more advanced kind than the MNIST data, that includes some basic information preprocessing, data storage (hdf5) and a simplified Residual Network model that is easily understood for entry into the field of Deep Learning.", "comments": ["Can one of the admins verify this patch?", "Thanks for writing this tutorial, the community I'm sure will find it useful!\r\n\r\nWe've been trying to avoid having tutorials checked into our code (it's another thing for our team to maintain), so we've been encouraging people to host their tutorials on their own GitHub accounts, and we have pages like https://www.tensorflow.org/community/welcome where we can link content that becomes popular.  Thanks!"]}, {"number": 9536, "title": "[bug] Distributed FIFOQueue dequeue duplicate data", "body": "\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 14.04 \r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.0.0\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n8.0 5.1\r\n- **GPU model and memory**:\r\nPascal TitanX \r\n- **Exact command to reproduce**:\r\nhttps://gist.github.com/lispc/27c9f4fe935abade5de90c18276e1742\r\n\r\n### Describe the problem\r\nIn distributed tensorflow, if I do multiple dequeue from a remote FIFOQueue, duplicate results will be popped. For example, 1 2 3 4 5 were pushed into the FIFOQueue, then the popped results may be 1 1 2 3 4 4 5. Is it a bug? Or designed feature?\r\nThe code is posted above. Logs is also in that gist.\r\n", "comments": ["This is a bug fixed in 1.1.\r\n#7038", "Please re-open if upgrading to 1.1 does not solve the problem."]}, {"number": 9535, "title": "Update address in Readme", "body": "side note, git > svn", "comments": ["@EddieOne, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv to be a potential reviewer.", "Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it! :taco: ", "CLAs look good, thanks!\n\n<!-- ok -->", "Thanks, I'll accept this, though we don't plan on releasing new binaries or website for r0.7, since it is over a year old. "]}, {"number": 9534, "title": "update doc", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 9533, "title": "Added gradients + tests for ImageProjectiveTransform", "body": "Fixes #9423 by providing gradients for the base op.\r\n\r\nGradients are computed using the inverse of the transformation matrix which can be slow (even though it's 3x3 matrices only) but allows for dynamic transformation matrices. In case the transform is static, this might be precomputed. No gradients for the transform itself yet.\r\n\r\nDisclaimer: First time TF contributor, feedback is welcome.", "comments": ["Can one of the admins verify this patch?", "Thanks for the PR!\r\n\r\n@tensorflow-jenkins test this please\r\n\r\n@ringw can you please review for correctness and overall idea?\r\n", "@tensorflow-jenkins test this please", "@vrv Thanks for fixing pending linter issues \ud83d\ude0a"]}, {"number": 9532, "title": "event_accumulator is missing in v1.1", "body": "### System information\r\n* TensorFlow version: v1.1\r\n\r\n### Describe the problem\r\nBefore v1.1, there is a class called EventAccumulater in `python/summary/event_accumulator.py` which is used to export the data from tensorboard. However, in v1.1 this file is missing and the `README.md` is still the old version, saying \"If you wish to load TensorFlow events, you should use an EventAccumulator (to load from a single events file) or an EventMultiplexer (to load from multiple events files)\".\r\nIs there any alternative API to load the data from tensorboard record files?\r\n", "comments": ["Ok. I've found the `event_accumulator.py` in `tensorflow/tensorflow/tensorboard/backend/event_processing/`.\r\nThe movement of the file was done in commit 8c1c861ccc488497ad44bb8ec7b1b49ff5ef0a2c.", "Yeah, the Event Accumulator is part of TensorBoard and was never actually exposed as part of the TensorFlow API (although we have mentioned it in the README). I wouldn't expect it to stay in the TensorFlow codebase indefinitely."]}, {"number": 9531, "title": "Avoid extra TensorReference allocation in gRPC", "body": "The original TODO comments pretty much speak for themselves.", "comments": ["Can one of the admins verify this patch?", "By the way, I measured the first change, and it improved RPC performance by 2%, so yay! That's actually pretty significant for us.", "Jenkins, test this please", "Turns out the RPC benchmark to compare `SetProtoFromGPU` versus `CopyGPUTensorToCPU` is more complex than I thought. It might be a good idea to split the two changes so the first one could be merged. \r\n\r\nI've reverted the other one.", "Jenkins, test this please", "Can we merge this now? Thanks!", "Yeah, @vrv is handling merges this week and will do that when he gets the chance. Thanks!"]}, {"number": 9530, "title": "PyCharm won't correctly import tensorflow", "body": "I get the following error when I try to run a python program:\r\n\r\n`ImportError: libcudnn.so.5: cannot open shared object file: No such file or directory`\r\n\r\nI understand this is a simple error most people would encounter when they start tensorflow. I did checked the solutions on Stackoverflow and I managed to run the same program in Terminal. What I did is I made an virtualenv and I activated it and when I try using python3 to run the program it successfully proceeds without any error. However the error always appear when I run the program in PyCharm even with python interpreter set to the location of virtualenv I created. Can someone tell me why as I had no idea why it works in terminal but not with PyCharm.\r\n", "comments": ["I read this Stackoverflow post [Pycharm environment different than command line](https://stackoverflow.com/questions/43691706/pycharm-tensorflow-importerror-but-works-fine-with-terminal) and it seems that the problem is not related to tensorflow but PyCharm itself. PyCharm won't keep the bash environment unless evoked from command line. So the solution to the problem is to type `charm` from command line and everything works fine now. Closing this issue now.", "Got the same today, but for 7.0:\r\n`ImportError: libcudnn.so.7: cannot open shared object file: No such file or directory` \r\nAbove solution is still relevant", "![screenshot from 2018-06-01 12-32-39](https://user-images.githubusercontent.com/33860245/40821210-0645be68-6598-11e8-92a1-8085d97ff9f2.png)\r\njust add an environment variable `LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH` on Pycharm, it's the bug of PyCharm", "when I use remote intepreter in my linux, I encounter the problem in my local windows Pycharm:\r\nImportError: libcublsa.so.9.0: cannot open shared object file: No such file or directory", "This helps which is from Stackoverflow @Sachin G. (https://stackoverflow.com/questions/27063361/how-to-run-pycharm-in-ubuntu-run-in-terminal-or-run)\r\n> The question is already answered, Updating answer to add the PyCharm bin directory to $PATH var, so that pycharm editor can be opened from anywhere(path) in terminal.\r\n> \r\n> Edit the bashrc file,\r\n>` nano .bashrc`\r\n> \r\n> Add following line at the end of bashrc file\r\n> `export PATH=\"<path-to-unpacked-pycharm-installation-directory>/bin:$PATH\"`\r\n> \r\n> Now you can open pycharm from anywhere in terminal\r\n> `pycharm.sh`", "> when I use remote intepreter in my linux, I encounter the problem in my local windows Pycharm:\r\n> ImportError: libcublsa.so.9.0: cannot open shared object file: No such file or directory\r\n\r\nany updates on this matter? I have the same issue, remote interpreter, connected to AWS-GPU instance. I can start python scripts without problems over ssh, but when running from PyCharm i get the same error", "Same for me @toxato . I am running Pycharm on a remote interpreter (in a remote virtual-env). It seems it does not save the paths. As @chamwen says, this can be solved. However, Pycharm will recognize /usr/local/cuda/lib64:$LD_LIBRARY_PATH as the path in your local computer, not remotely. Any ideas how can we set up that path in such a way that Pycharm will recognize this as a remote path?", "I think I fixed it @toxato. You should create a shell wrapper (remoteEnv.sh) which looks like: \r\n\r\n```\r\n!/bin/bash\r\nexport PATH=/usr/local/cuda-9.0/bin${PATH:+:${PATH}}\r\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\r\n\r\n\r\n# added by Anaconda3 installer\r\nexport PATH=\"/home/javier/anaconda3/bin:$PATH\"\r\n\r\n\r\n\r\nsource /home/javier/anaconda3/bin/activate AIenv\r\n#Now we call python which should be python from our virtualenv\r\npython \"$@\"\r\n\r\n```\r\n\r\nand then configure that as your remote interpreter. Hope it helps! ", "> when I use remote intepreter in my linux, I encounter the problem in my local windows Pycharm:\r\nImportError: libcublsa.so.9.0: cannot open shared object file: No such file or directory\r\n\r\nI have the same problem when using remote interpreter. ", "\r\n![default](https://user-images.githubusercontent.com/41919591/50206391-362ec500-03a6-11e9-858d-277ee5723d4b.png)\r\nAdd a environment variable LIBRARY_PATH=/usr/local/cuda-9.0/lib64(depends on your path), then you don't need to start pycharm form console.\r\n![default](https://user-images.githubusercontent.com/41919591/50206482-8574f580-03a6-11e9-8a5f-9cf25a6dd515.png)\r\n\r\n\r\n\r\n", "There is an another way in ubuntu: change the quick start file:`etbrains-pycharm.desktop`, which can be found in `/usr/share/applications/` or `~/.local/share/applications/`, just depends on pycharm's installation.\r\nthen modify this line:\r\n``` bash\r\nExec = /...\r\n```\r\ninto\r\n```bash\r\nExec = bash -i -c /...\r\n```\r\nthen restart your pycharm, u'll find bug fixed."]}, {"number": 9529, "title": "Unveil type check part in _VerifyGeneratedGradients that was not actu\u2026", "body": "\u2026ally used", "comments": ["Can one of the admins verify this patch?", "cc @yaroslavvb in case he has an idea of how he wanted this code to be checked in the first place", "rats! Python indendation!\r\n\r\nI added the check because @girving asked me to add a check that gradients must be complex when inputs are complex in https://github.com/tensorflow/tensorflow/pull/6868\r\n\r\nBut, I'm not sure of a way to exercise complex gradients, @girving any idea? Since nobody complained, maybe we can just drop that check completely?\r\n\r\nThe original reason for PR is that it enforced that activation value and backprop value have the same type, but we wanted to mix types (float32, float16). What if someone decides to mix real/complex types in the future?", "It's hard for me to imagine a use of replacing real with complex or vice versa during backprop.  You'd be throwing away half the information, or adding zeros for some reason.\r\n\r\nExercising complex gradients is pretty easy: just make a complex constant and square it, for instance.", "I think something extending \"synthetic-gradients\" idea could mix complex real for gradient -- they use a different function for forward and backward, so you throw away half the information, but gain some regularization/efficiency.\r\n\r\nI don't have any strong opinion for what to do here, I think the number of people doing complex gradients is pretty small", "Let's just get rid of the check -- we can add it back later, it's clear no one was depending on it.", "@YuMS could you just delete that whole block? Thanks for the find", "@yaroslavvb GitHub now allows you to make edits like these in their PR (click the pencil button in the review tab) -- if you think this is the way to go.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Yes, that's super convenient, although last time I tried it, it broke the CLA bot, lets see what happens this time", "OK, looks like CLA bot is still confused by github inline edits, same as in https://github.com/tensorflow/tensorflow/pull/7442#issuecomment-279272947 , it needs someone to manually add \"cla: yes\" label @martinwicke ", "It's possible that it doesn't like it when the person making the edit isn't an admin (it allows me to do it all the time without complaining...).  IN any case, this is fine, so I'll test and merge.\r\n\r\n@tensorflow-jenkins test this please", "It just doesn't like it period. It's fine to ignore the CLAbot if it\ncomplains about the consent thing and all the commits are from the\nsubmitter or maintainers.\n\nOn May 7, 2017 12:20 PM, \"Vijay Vasudevan\" <notifications@github.com> wrote:\n\n> It's possible that it doesn't like it when the person making the edit\n> isn't an admin (it allows me to do it all the time without complaining...).\n> IN any case, this is fine, so I'll test and merge.\n>\n> @tensorflow-jenkins <https://github.com/tensorflow-jenkins> test this\n> please\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9529#issuecomment-299728122>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAjO_T6F9sc4ytPiNtCzvbErFfXEDzpCks5r3hmGgaJpZM4NMIlt>\n> .\n>\n", "I've been making edits all week and it never complained :)"]}, {"number": 9528, "title": "TensorFlow processes and core engine documentations ", "body": "Hi all,\r\n\r\nI am interesting to understand more details about TF core engine. My focus interest on how a process create and launch when session is being created. Also, how the threads work and how python api translate these threads to C++ TF engine. Is there any documentation about that ? \r\n\r\nSincerely\r\n", "comments": ["I'm afraid that there is little documentation on the internals of TensorFlow. The best way to get started is to read the source code on the parts that you are interested in. It is also unlikely we will have time to extensively document the workings of TensorFlow, as most our documentation effort is aimed at users.\r\n", "I'd also recommend reading the published papers on TensorFlow.  E.g., http://download.tensorflow.org/paper/whitepaper2015.pdf."]}, {"number": 9527, "title": "tf.while_loop much slower than static graph?", "body": "I'm running on TF 1.1, and I've used `tf.while_loop` + `TensorArray` to implement dynamic unrolling of a type of recurrence that I previously unrolled statically through python code. The difference in speed is very dramatic, with forward inference being about 200x slower when dynamically unrolled, and backprop about 2x slower. Is this expected? Are there any tricks for optimization that I'm missing? This is on CPU. Performance gap on GPU is even larger.", "comments": ["Have you tried the official API of dynamic unrolling `tf.nn.dynamic_rnn` ? Maybe comparing the speed of your implementation and official one can provide the insights.  ", "The code I'm implementing has nothing to do with RNNs. It's a sequence of geometric transformations.", "@fxsuper, could you include some of your code that demonstrates this problem? Specifically from your general description it is only possible to guess what your two versions look I imagine if the amount of work required by each \"iteration\" is tiny, you might see the same behavior. I would also recommend asking on StackOverflow, because there are many more users of TensorFlow asking there, and this is mainly a forum for bugs and feature requests. Thanks.", "Sure, I've tried to simplify the code as much as possible below (sorry I realize it's still a bit long). This simpler version shows a 3x difference in training and ~500x difference in inference. And yes I'm aware that GitHub is not meant for help and debugging, but I thought the speed gap was large enough for this to almost be a bug or at least a feature request.\r\n\r\n**Static version**\r\n```\r\nimport numpy as np\r\nimport numpy.random as npr\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops.nn import l2_normalize\r\nnpr.seed(0)\r\nbatch_size = 32\r\nnum_steps = 1000\r\ninputs = tf.constant(npr.rand(num_steps * 3, batch_size, 3), dtype=tf.float32)\r\nvar = tf.Variable([1, 1, 1], dtype=tf.float32)\r\n\r\nid_mat = np.identity(3, dtype='float32')\r\ninit = []\r\nfor row in id_mat:\r\n    r = tf.tile(row[np.newaxis], [batch_size, 1])\r\n    init.append(r)\r\n\r\nfor d in tf.unstack(inputs):\r\n    a, b, c = init[-3:]                                                 \r\n    m = tf.transpose(tf.stack([c, tf.cross(b, c), b]), perm=[1, 2, 0])\r\n    p = l2_normalize(tf.squeeze(tf.matmul(m, tf.expand_dims(d, 2))), 1)\r\n    init.append(p)\r\n\r\nfinal = tf.stack(init[2:-1])\r\n    \r\nloss = tf.reduce_sum(tf.reduce_sum(final, axis=[0, 1]) * var)\r\ntrain = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\r\n\r\nsess = tf.Session()\r\ntf.global_variables_initializer().run(session=sess)\r\n\r\nfinal.eval(session=sess) # ~0.0007 secs\r\ntrain.run(session=sess) # ~0.12 secs\r\n```\r\n\r\n**Dynamic version**\r\n```\r\nimport collections\r\nimport numpy as np\r\nimport numpy.random as npr\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops.nn import l2_normalize\r\nnpr.seed(0)\r\nbatch_size = 32\r\nnum_steps = 1000\r\ninputs = tf.constant(npr.rand(num_steps * 3, batch_size, 3), dtype=tf.float32)\r\nvar = tf.Variable([1, 1, 1], dtype=tf.float32)\r\n\r\nTriplet = collections.namedtuple('Triplet', 'a, b, c')\r\nid_mat = np.identity(3, dtype='float32')\r\ninit = Triplet(*[tf.tile(row[np.newaxis], [batch_size, 1]) for row in id_mat])\r\n\r\ndef extend(tri, inputs):\r\n    m = tf.transpose(tf.stack([tri.c, tf.cross(tri.b, tri.c), tri.b]), perm=[1, 2, 0])\r\n    p = l2_normalize(tf.squeeze(tf.matmul(m, tf.expand_dims(inputs, 2))), 1)\r\n    return p\r\n\r\ni = tf.constant(1)\r\ns = inputs.get_shape().as_list()[0]\r\nta = tf.TensorArray(tf.float32, size=s)\r\n\r\ndef body(i, tri, ta):\r\n    p = extend(tri, inputs[i - 1])\r\n    return [i + 1, Triplet(tri.b, tri.c, p), ta.write(i, p)]\r\n\r\n_, _, final_ta = tf.while_loop(lambda i, _1, _2: i < s, body, \r\n                               [i, init, ta.write(0, init.c)],\r\n                               parallel_iterations=1, swap_memory=False) \r\n\r\nfinal = final_ta.stack()\r\n\r\nloss = tf.reduce_sum(tf.reduce_sum(final, axis=[0, 1]) * var)\r\ntrain = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\r\n\r\nsess = tf.Session()\r\ntf.global_variables_initializer().run(session=sess)\r\n\r\nfinal.eval(session=sess) # ~0.37 secs\r\ntrain.run(session=sess) # ~0.37 secs\r\n```\r\n\r\nNote that increasing the number of `parallel_iterations` only makes things worse, likely because the computation is very sequential.", "It looks like your code samples are missing some imports so I'm not able to run them; could you edit the code you posted so it runs as-is? That said, as @aselle mentioned, it looks like each script isn't doing much (according to the comments that report subsecond runtime), so the time is probably dominated by overhead rather than the actual calculation.", "I updated the code with all the necessary imports. As for the calculations, yes the individual calculations are small (a few matrix multiplies and cross products), although I'm not sure if it's really all that different from an RNN, at least on the backprop side. A training step is taking 0.37 sec which is about the amount of time taken by a decent sized LSTM.", "Like @skye says, it is probably due to the loop overhead. Not sure if there's much you can do here.\r\n\r\nCC @tfboyd @yuanbyu ", "The first call to session.run sets up a lot of internal objects and isn't appropriate for timing.\r\n\r\ncall it 1-5 times to warm up, and then call it 10-100 times and get the average of those remaining run calls.", "@ebrevdo that is indeed what I do. I just didn't want to clutter the code above, but the numbers reported are averages of many runs after it's been warmed up.", "Why only 1 parallel iteration?  Try increasing it.\n\nOn May 17, 2017 1:48 PM, \"fxsuper\" <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> that is indeed what I do. I just\n> didn't want to clutter the code above, but the numbers reported are\n> averages of many runs after it's been warmed up.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9527#issuecomment-302226984>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimyfbk63G5wZaUHOXm1pfCnmLX3GKks5r610XgaJpZM4NMF3P>\n> .\n>\n", "I did; it only made things worse.", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.", "Please reopen this. Nothing has changed in the latest release, and 500x difference in inference speed seems very much like a performance bug to me.", "Try replacing the inputs from a tf.constant to a tf.Variable.  the static\ngraph is probably being optimized away via constant folding, and the\ndynamic one is not.  if your input is not a tf.constant but a placeholder\nor Variable, then both the static and dynamic graphs will have to execute\nall operations at each session.run.\n\n\nOn Mon, Jun 19, 2017 at 1:37 PM, Skye Wanderman-Milne <\nnotifications@github.com> wrote:\n\n> Reopened #9527 <https://github.com/tensorflow/tensorflow/issues/9527>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9527#event-1129858662>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimwbYplAnuIncaofNrwmgRMeety5Wks5sFtvwgaJpZM4NMF3P>\n> .\n>\n", "Thanks for the suggestion. Everything does slow down, but the gap remains. For training, it's now more like 4x difference in speed (dynamic at 2.5 sec vs. static at 0.6 sec per iteration). For forward pass, the gap has shrunk but is still about 4x (dynamic at 0.45 sec vs. static at 0.12 sec per iteration).\r\n\r\nI should say that in my actual model, where this is only a part of a larger pipeline, the slowdown I see is more substantial. This component takes about 20% of training time when static, but slows down the entire pipeline by a factor of 3 when dynamic, so more like an 11x slowdown.", "Unsure if this is the place to write this info, but I have been working on a comparison of different unfolding methods with LSTMs, and I am getting about 2x slower training speeds when using dynamic_rnn versus manually unrolling the graph statically in python, and about 1.5x slower training speeds when using static_rnn versus unrolling the graph statically in python. Given that I am using dynamic_rnn in a traditional use case, I find this surprising considering that most of the internet claims that dynamic_rnn is the better option of the three. I am basing these results on both GPU and CPU testing, with a training set size of 200 and 2,000 epochs\r\n\r\nHaving trouble pasting the code, so here is the file. Relevant code is from lines 83 to 131.\r\nhttps://github.com/TheButlah/LSTM/blob/a9f54879640c75c32903e2331a5478044879ff34/model.py", "static_rnn with sequence_length= passed in will generally outperform manual\nunroll if the unroll size is bigger than *all* of your sequence lengths.\n this is not the case for BPTT.  when using BPTT, do not pass the\nsequence_length argument to static_rnn since you usually  want to calculate\nall time steps.  that argument is more useful when doing static unroll for\nseq2seq or other nontrivial architectures.\n\ndynamic_rnn is as performant as static unroll for large batch sizes and\ndepths; but not for smaller batch sizes and depths.  what are your batch\nsize and depth?\n\nfurthermore, if you have very large sequence lengths, dynamic_rnn can copy\nactivations to CPU from the GPU to allow you to train much bigger graphs\n(if you pass swap_memory=True).  static unrolling would just lead to GPU\nOOM in these cases.  again this is more useful for large batch size, depth,\nand sequence length values.\n\n\n\nOn Tue, Jun 27, 2017 at 7:22 AM, Ryan Butler <notifications@github.com>\nwrote:\n\n> Unsure if this is the place to write this info, but I have been working on\n> a comparison of different unfolding methods with LSTMs, and I am getting\n> about 2x slower training speeds when using dynamic_rnn versus manually\n> unrolling the graph statically in python, and about 1.5x slower training\n> speeds when using static_rnn versus unrolling the graph statically in\n> python. Given that I am using dynamic_rnn in a traditional use case, I find\n> this unexpected considering that most of the internet claims that\n> dynamic_rnn is the better option of the three.\n>\n> `with tf.variable_scope('Unrolled') as scope:\n> lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=cell_size) # This\n> defines the cell structure\n> initial_state = lstm_cell.zero_state(batch_size=batch_size,\n> dtype=tf.float32) # Initial state\n>\n>             self._sequence_lengths = tf.random_uniform(\n>                 shape=(batch_size,), minval=1, maxval=num_steps+1, dtype=tf.int32\n>             )  # , trainable=False, validate_shape=False, collections=[], name='Sequence-Lengths')\n>\n>             def traditional_bptt():\n>                 \"\"\"Calls the lstm cell with the state and output for each time until num_steps.\"\"\"\n>                 state = initial_state\n>                 # Unroll the graph num_steps back into the \"past\"\n>                 for i in range(num_steps):\n>                     if i > 0: scope.reuse_variables()  # Reuse the variables created in the 1st LSTM cell\n>                     output, state = lstm_cell(  # Step the LSTM through the sequence\n>                         self._hot[i, ...] if time_major else self._hot[:, i, ...],\n>                         state\n>                     )\n>                 return output\n>\n>             def dynamic_bptt():\n>                 \"\"\"Uses dynamic_rnn to unroll the graph.\"\"\"\n>                 outputs, states = tf.nn.dynamic_rnn(\n>                     lstm_cell, self._hot,\n>                     sequence_length=self._sequence_lengths,\n>                     initial_state=initial_state,\n>                     time_major=time_major,\n>                     scope=scope\n>                 )\n>                 return outputs[-1, ...] if time_major else outputs[:, -1, ...]\n>\n>             def static_bptt():\n>                 \"\"\"Uses static_rnn to unroll the graph\"\"\"\n>                 inputs = tf.unstack(self._hot, axis=0 if time_major else 1)\n>\n>                 outputs, states = tf.nn.static_rnn(\n>                     lstm_cell, inputs,\n>                     sequence_length=self._sequence_lengths,\n>                     initial_state=initial_state,\n>                     scope=scope\n>                 )\n>                 return outputs[-1]\n>\n>             # Emulate a switch statement\n>             final_output = {\n>                 'traditional': traditional_bptt,\n>                 'dynamic': dynamic_bptt,\n>                 'static': static_bptt\n>             }.get(bptt_method)()\n>\n> `\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9527#issuecomment-311373632>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim5MDn21ejaFTjEgzd_UUs5jQkZFUks5sIRAygaJpZM4NMF3P>\n> .\n>\n", "I need to accept variable length sequences, so I am essentially comparing dyamic_rnn vs static_rnn vs using a python loop and then a per-timestep mask on the loss function. I have tested using batch sizes of 200 on both 10 and 100 long sequences of one-hot embedded vectors (10 'words' in dictionary). What do you define as larger batch sizes and timesteps? I get the logic behind the GPU swap thing, but I don't think that really warrants this 2x-2.5x slowdown even when swapping is set to be off. I also just wrote a custom, simpler version of dynamic_rnn that doesn't use TensorArrays or tf.where and it performs ~25% faster than dynamic_rnn; still not as fast as manual unrolls in python however. Also, according to my tests, static_rnn with sequence length passed in does not outperform manual unrolling because the tf.cond calls are tremendously expensive. With a batch size of 200 and 100 timesteps, supplying static_rnn with a sequence length vector filled with the value 10, dynamic_rnn takes ~90 seconds, static_rnn takes ~70 seconds, and manual unrolling takes ~30 seconds, even though manual unrolling strictly speaking is wasting 90/100 = 90% of its timesteps.", "Can you provide examples/gists of your manual unrolling code, how you're calling static_rnn, and also your custom dynamic_rnn which doesn't use TensorArrays?\r\n\r\nIt's true that if you don't care about the final state or properly zeroing out your outputs (both require using tf.where) then you can get faster performance.  You disable this by passing sequence_length=None to dynamic_rnn.  You can also use tf.contrib.cudnn_rnn if you've got very performance critical code.  I'm trying to understand if tf.where isthe only source of slowdown compared to your streamlined versions.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "I tried a graph traversal using both while_loop with auto JIT option on and simply while under eager mode. It seems eager while runs faster. And my implementation is slower than existing method in sklearn based on cython, although I tried to use matrix manipulation as much as possible to gain efficiency from GPU.  ", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Yes it is. I am still waiting for new opinions. I guess while_loop could not be compiled or optimised in general situations. I also tried implementing DBSCAN using tf. Same algorithm with tf is just a bit better than using numpy, but much worse than the built in implementation in scikit, with cython. It is disappointing to see my friend's matlab implementation is faster than mine. ", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 89 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 9526, "title": "[Tutorial Update] Logging and Monitoring Basics", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\nTensorFlow 1.1\r\nn/a as purely documentation related\r\n\r\n### Describe the problem\r\n\r\nThe currently published tutorial for [Logging and Monitoring](https://www.tensorflow.org/get_started/monitors) is based on functionality that has been identified as _deprecated_ (see i.p. the discussion in #7669 .)\r\n\r\nI truly appreciate all the hard work \ud83d\udcaf  put into moving the functionality of TF itself forward.\r\n\r\nIt would be most appreciated, if this tutorial could be updated to reflect suggested best practices when using `sessionRunHook` et al. to ease adoptabilty of TF.\r\n\r\n\r\n### Source code / logs\r\n\r\nn/a\r\n", "comments": ["@martinwicke Seems like a reasonable request.  Assigning to you for learn triage.", "@ispirmustafa I forget the last state of the Hooks tutorial. @sandersk, were you, or did you, already sign up to do this? \r\n\r\n@dr4b @wolffg FYI for docs planning.", "Any update on this? Thx in advance.", "We're working on a docs update that does include a rewrite of this tutorial. It's not done yet.", "@jugglerix feel free to close once the new Programmer's Guide section is up.", "The current goal is to release the new Programmer's Guide in July.", "@jugglerix Is there an updated ETA for the new Programmer's Guide? Thx in advance.", "The new Programmer's Guide will go out with V1.3.  However, we may add a\nfew additional chapters after 1.3 is released.\n\nOn Fri, Jul 21, 2017 at 4:10 PM, Tom Wanzek <notifications@github.com>\nwrote:\n\n> @jugglerix <https://github.com/jugglerix> Is there an updated ETA for the\n> new Programmer's Guide? Thx in advance.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9526#issuecomment-317100555>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AVnLsKLeNW9rEZZcOUe9pXjAP-qYWJDtks5sQQXAgaJpZM4NMDdJ>\n> .\n>\n", "Is there an update on the timing for this issue's resolution?\r\nAnd a slightly more general contextual question, Is there a tracking reference to see which parts of the Programmer's Guide/Tutorials are current with Release 1.3 or still awaiting update as per @jugglerix above comment?\r\n\r\nAs always, thanks for the hard work!", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "A few notes:\r\n\r\n* The \"Logging and Monitoring Basics with tf.contrib.learn\" document was removed from tensorflow.org.\r\n* The tutorial entitled \"A Guide to TF Layers: Building a Convolutional Neural Network\" demonstrates how to use SessionRunHook to set up logging during training.\r\n"]}, {"number": 9525, "title": "Statically-linked libraries in TF binary can cause symbol collisions", "body": "TensorFlow currently statically links all dependencies. This sometimes causes hard-to-diagnose crashes (e.g. segfaults) when another version of a dependency is loaded into the process. This can even happen within TensorFlow if separate TensorFlow .so's are loaded into the same Python process.\r\n\r\nPossible solutions would be to reduce the visibility of these symbols, dynamically link common libraries, or run TF in a separate process.\r\n\r\nKnown problematic libraries:\r\n* protobuf (#8403, #8394)\r\n* OpenCL, OpenCV (#7378)\r\n\r\nOther related issues:\r\n* #7480", "comments": ["I think, this relates to https://github.com/tensorflow/tensorflow/issues/9391\r\nThis is needed if people want to integrated tensorflow binaries into their existing software.", "I'm trying to solve this in #9391 . Follow up there.", "@nkhdiscovery @drpngx Should we dedup this with #9391?\r\n\r\n@nkhdiscovery We're planning to fix this for good using more restricted exports.  In particular, protos should not appear in the public API once we're done.  Do you mind if I close that other bug? ", "@girving Thanks for closing that one, I would be happy if I could help. I already tried hiding symbols with adding version scripts to cc project but I couldn't figure a clean way to write the regex matching unnecessary symbols. I would do that if you give me a hint on what are the API functions to make them public (global) and make else hidden (local) (_TF* as  didn't work as global), or a hint on how to hide symbols coming from specific headers. I Googled a lot and found almost nothing good enough on using version scripts to solve this.", "Version scripts are not so good for C++ based projects, because it's hard to control what needs to be public and not. This is way visibility attributes were added. Everything, except symbol versions are available as attributes :( Versions  script is a good starting point (I would prefer to have symbol versions).", "We'll be marking exported symbols with a TF_EXPORT macro, but there's a bunch of upfront work to do to minimize the API surface area before we do that.", "@davidlt Are you suggesting to put ```__attribute__ ((visibility (\"default\")))``` in the code wherever there is something to hide? Because that requires lots of additions in third_party headers, e.g. protobuf itself, which will cause further expenses if one of those third_party libraries planned to be upgraded. I just didn't think of that as a solution because I felt it's just making another mess, even re-packing those libraries with new names and dynamically link against those specific versions seems much more cleaner to me.\r\n\r\n@girving First, as I understood from your comment, this TF_EXPORT macro will then help us to recognize what has to be exported and what is not to, right? I mean at least I will be able to put every single function which has that macro in my version script and temporarily solve the problem for my own usage. Am I right?\r\nSecond, I see you have to do this later to avoid the redundant work (putting the macro wherever it is needed and then removing that whole part of API in minimization will not be logical, for sure); But if this is the only problem for not doing that now, I can do that redundant work in a fork or a temporary branch so we can use the remained parts after minimization. I have to solve this for myself as soon as possible, so let's do it in a way which is helpful for further contribution. Can you give me any hint on how to do that? I saw the usage in tensorflow/core/framework/types.h in master, I think I can give a try if I know what exactly has to be exported. \r\n\r\nThanks for your replies guys.", "@nkhdiscovery Once we're done, the code will be compiled with `-fvisibility=hidden`, and only symbols marked with `TF_EXPORT` will be exported.  Unfortunately I don't understand the details of versions scripts, so I don't know enough to answer what you can do that will be useful.  Most of the work is restructuring the actual code to make it easy to restrict exports, not doing the actual export restriction.", "@girving Thanks for your answer, I just understood what you are doing as the solution. Is there any way I can contribute to accelerate this? Isn't it just enough to add this macro to all API functions? ", "Most of the complexity is refactoring the code so that protos don't need to be exposed, since we don't have control over those.  I'm not sure how to parallelize the required refactoring, and unfortunately a good chunk of the complexity is making sure said refactoring doesn't break non-opensource code.", "I'm wondering, will the approach described here also help with the problem of building a debug mode DLL for windows via cmake?  Currently the issue is that the .def file generated by create_def_file.py contains more symbols than the 65535 limit.", "@adennie Yes, we should be able to fit within that limit.  Rather embarrassing that we can't yet. :)", "@girving I just noticed, \" non-opensource\" ? Which parts are non-opensource? Did you just meant 3d parties? I thought TensorFlow is all open-source! ", "TensorFlow is all open source, but there is a lot of downstream Google code that uses it.  Some of it is tightly integrated and needs refactoring too. ", "Any updates on this issue?  My project is kind of blocked by the inability to build a debug tensorflow DLL.", "Still working on it, but no usable progress yet.", "Transferring issue ownership.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "@adennie  take a look at last commits of branch fd-devel in our fork:\r\nhttps://github.com/Faraadid/tensorflow/tree/fd-devel?files=1\r\n\r\nHope it helps. Let me know the result.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Cross-referencing https://github.com/tensorflow/tensorflow/issues/16104.\r\n\r\nWe now have better support for dynamic loading into split libraries, so it's close.", "Nagging Assignee @drpngx: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @drpngx: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @drpngx: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "TensorFlow is [no longer exporting any symbols globally](https://github.com/tensorflow/tensorflow/commit/5c7f9e316d8c7735308a217310350d416d7498cc), which sounds like the original issue? The protobuf refactoring is still useful and ongoing AFAIK, but turned out to be tangential. And we'd still like to split more of our dependencies into separate shared objects, also ongoing.\r\n\r\nI'll close this, but @skye feel free to reopen if you had something else in mind.", "@allenlavoie Great news to hear, thanks! I would like to help splitting, let me know if you have any open issues ... ", "@nkhdiscovery the next step IMO would be to split off a shared object with the implementations of our protocol buffers (libtensorflow_protobufs.so?).\r\n\r\nThe main benefit would be that users of the C++ API would no longer need to link against libtensorflow_framework.so (or build libtensorflow_cc statically) for protocol buffer symbols, and so would run into fewer symbol conflicts. So basically https://github.com/tensorflow/tensorflow/issues/14267; it's closed at the moment, but you could re-open it and work on it. We have workarounds but no great solution for C++ API users who want to use OpenCV and use custom ops (custom ops won't work with static libtensorflow_cc, OpenCV won't work with dynamic libtensorflow_cc).\r\n\r\nThere are two things to be moved: one is the static variables for protocol buffer registration (`@protobuf_archive//:protobuf`), the other is the [generated implementations of TensorFlow's protocol buffers](https://github.com/tensorflow/tensorflow/blob/6f532f38af72cb218cb6648363964e4f3e0817d2/tensorflow/core/BUILD#L1796). My thought is that these should stay together for now.\r\n\r\nSteps I think the split would include:\r\n1. Hacking around with build rules until `bazel query 'somepath(//tensorflow:libtensorflow_framework.so, @protobuf_archive//:protobuf)'` and `bazel query 'somepath(//tensorflow:libtensorflow_framework.so, //tensorflow/core:protos_all_cc_impl)'` return empty results\r\n2. Include these explicitly in a new //tensorflow:libtensorflow_protobuf.so rule (near [libtensorflow_framework.so](https://github.com/tensorflow/tensorflow/blob/6f532f38af72cb218cb6648363964e4f3e0817d2/tensorflow/BUILD#L447)), and include `//tensorflow:libtensorflow_protobuf.so` in [tf_binary_additional_srcs](https://github.com/tensorflow/tensorflow/blob/6f532f38af72cb218cb6648363964e4f3e0817d2/tensorflow/tensorflow.bzl#L288).\r\n3. Make sure all the tests pass :). Easy to have undefined symbols when messing with linking.\r\n4. The final step would be dealing with packaging issues, such as for the Java bindings (which I believe still hard-code the names of TensorFlow libraries).\r\n\r\nHappy to chat more if this sounds interesting. Sending an email to [developers@tensorflow.org](https://www.tensorflow.org/community/contributing) with a rough plan and discussing would be a good start (@gunan and others are working on a related effort, so coordinating would be important).", "Hi,\r\n\r\nI need to load a model on a \"plugin\" for a c++ program, the codes is basically a header which calculates a matrix and returns it to the main program. There I need to:\r\n1) select the gpu on which to load the model, no other gpu should be seen by tensorflow, this needs to be done on runtime, as the gpu_id is a parameter of the constructor of the class which runs the tensorflow model.\r\n2) load the model and run.\r\n\r\nthe main program uses opencv functions imread, imencode and imwrite, the latter causing segfault if including tensorflow headers and linking dynamically tensorflow_cc and tensorflow_framework.\r\n\r\nDoing a monolithic build disables the ability to hide gpu devices from tensorflow session, otherwise it runs smoothly.\r\n\r\nI compiled using master, a particular commit of 1.8 and r1.9. (not all at the same time, and I tried all of them in the same machine)\r\n\r\nConfigure:\r\ncuda 8.0,\r\ncudnn 7.1.\r\n\r\nBazel command:\r\nbazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 //tensorflow:libtensorflow_cc.so\r\n\r\nMy problems:\r\n1) using tensorflow headers and opencv imwrite, imread and imencode doens't work unless you use monolithic build.\r\n2) using monolithic build disables setting gpu visible devices, which is a very important issue for some, as it needs protocol buffer symbols, linking libprotobuf doesn't work.\r\n\r\nPossible solutions: (may break other things)\r\n1) Posible fix:\r\nsomeone in another issue suggested doing a change around line 41 in thirdparty/jpeg/jpeg.BUILD. \"-fPIC\" is needed as the result of \"-fvisibility\".\r\n\"//conditions:default\": [\r\n\"-fvisibility=hidden -fPIC\"\r\n],\r\nThis enables the use of opencv imread, imwrite and imencode, but it probably breaks other things.\r\n\r\n\r\n\r\n", "@JosephIWB \r\n\r\n> Doing a monolithic build disables the ability to hide gpu devices from tensorflow session,\r\n\r\nHow are you hiding them? CUDA_VISIBLE_DEVICES? I have no idea why this wouldn't work, but if you have a quick repro someone can take a look.\r\n\r\nThere's also the [\"add yet another shared object\" workaround for the OpenCV symbol conflict](https://github.com/tensorflow/tensorflow/issues/14267#issuecomment-381660514).", "I can't use CUDA_VISIBLE_DEVICES because the program is multi-threaded and is supposed to work on various gpus separately (you can configure the program  to process multiple video streams, then you also can decide which gpu each thread should use), so CUDA_VISIBLE_DEVICES doesn't work, as you need to decide which gpu a thread uses on runtime.\r\n\r\nI use this code to generate the session configuration options:\r\n```\r\n    tf::Session* session_ptr;\r\n    auto options = tf::SessionOptions();\r\n    const std::string gpu_vis_device = std::to_string(gpu_id);\r\n    options.config.mutable_gpu_options()->set_visible_device_list(gpu_vis_device);\r\n    options.config.mutable_gpu_options()->set_per_process_gpu_memory_fraction(0.025);\r\n    options.config.mutable_gpu_options()->set_allow_growth(true);\r\n    auto status = NewSession(options, &session_ptr);\r\n    if (!status.ok()) {\r\n        std::cout << status.ToString() << \"\\n\";\r\n        return false;\r\n    }\r\n    session.reset(session_ptr);\r\n```\r\nThe gpu_memory_fraction will be changed in the future to be passed by the function, and not hardcoded like that.\r\n\r\nWhen you do it like that, the program cries about linking issues with protobuf, and linking protobuf didn't solve the problem (-lprotobuf)\r\n\r\nDoing the trick I stated above made the program work correctly when using imread, imencode and imwrite from opencv.\r\n\r\nAnother fix I think should work should be compiling tensorflow and opencv using the same dependencies, so they share the same symbols and don't generate any conflicts between them, but that is a lot of time, and as the workaround I found in an issue here worked, I think we will not be trying that (as it a lot more work to tell the compiler to use the same dependencies, bazel and cmake seem not to like each other very much)\r\n\r\nThanks for your response.\r\n", "Oh I see, the issue is that C++ API doesn't include protobuf symbols. You need to link against libtensorflow_framework.so for those (unfortunately a [known issue](https://github.com/tensorflow/tensorflow/issues/9525#issuecomment-385475293)).\r\n\r\nDo you think the fvisibility change is submittable? May be worth running the tests (e.g. bazel test -c opt //tensorflow/core/... //tensorflow/python/...), and if they pass making a pull request out of it (I'm happy to review). If we don't need the symbols which conflict with OpenCV, we should stop exporting them.", "I don't think so, I'm not very savvy anyway.\r\n\r\nIn [issue #14627](https://github.com/tensorflow/tensorflow/issues/14267#issuecomment-389692737) @ruanjiandong proposed that workaround, and worked for me.\r\n\r\nA quote from what was said there regarding the probable use of the fvisibility change:\r\n> There are some linking warnings for undefined dynamic symbol in tensorflow/contrib/lite/toco/toco, which I don't use.\r\n\r\nCurrently I have a lot of dependency problems and I'm also very short on time to run these tests on my machine (I also have gpu driver problems, among other things), so I probably won't be able to help.\r\n\r\nI posted here to let others know that this workaround works, although other problems my arise because of that. \r\n\r\nAnyway, the compatibility problems between opencv and tensorflow may come from the image libraries, and the protobuf library, a good solution would probably involve testing if compiling against the same dependencies work, so both opencv and tensorflow work together. (this probably is a common issue among people who work on computer vision)\r\n\r\nMaybe I will do some test on the weekend, I will keep you posted.", "I tried running the tests suggested by @allenlavoie . Unfortunately, my workaround breaks the test build. k8-py3-opt/bin/_solib_local/libtensorflow_Score_Slibjpeg_Uinternal.so needs those jpeg symbols exported.\r\n\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/k8-py3-opt/bin/tensorflow/core/grappler/costs/utils_test '-Wl,-rpath,$ORIGIN/../../../../_solib_local/' '-Wl,-rpath,$ORIGIN/../../../../_solib_local/_U_S_Stensorflow_Score_Sgrappler_Scosts_Cutils_Utest___Utensorflow' '-Wl,-rpath,$ORIGIN/../../../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' '-Wl,-rpath,$ORIGIN/../../../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' '-Wl,-rpath,$ORIGIN/../../../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' -Lbazel-out/k8-py3-opt/bin/_solib_local/_U_S_Stensorflow_Score_Sgrappler_Scosts_Cutils_Utest___Utensorflow -Lbazel-out/k8-py3-opt/bin/_solib_local -Lbazel-out/k8-py3-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/k8-py3-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/k8-py3-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..,-rpath,$ORIGIN/../..,-rpath,$ORIGIN/../../..' -Wl,-z,muldefs -Wl,-z,notext -Wl,-z,notext -Wl,-z,notext -Wl,-z,notext -Wl,-z,notext -Wl,-z,notext -Wl,-z,notext -pthread -Wl,-z,notext -Wl,-z,notext -Wl,-z,notext -Wl,-z,notext -Wl,-z,notext -Wl,-z,notext -Wl,-z,notext -Wl,-rpath,../local_config_cuda/cuda/lib64 -Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64 -pthread -Wl,-no-as-needed -B/usr/bin/ -pie -Wl,-z,relro,-z,now -no-canonical-prefixes -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,--gc-sections -Wl,@bazel-out/k8-py3-opt/bin/tensorflow/core/grappler/costs/utils_test-2.params)\r\nbazel-out/k8-py3-opt/bin/_solib_local/libtensorflow_Score_Slibjpeg_Uinternal.so: undefined reference to `jpeg_abort'\r\nbazel-out/k8-py3-opt/bin/_solib_local/libtensorflow_Score_Slibjpeg_Uinternal.so: undefined reference to `jpeg_set_defaults'\r\n...", "Thanks @ruanjiandong! I guess not super surprising, but was worth a try. So the options are still (1) split out proto symbols so people don't need to link in libtensorflow_framework.so, (2) move libjpeg to the language bindings / colocated with the kernel. Possibly (2) is easier?", "@allenlavoie , I took another look at the build failure. Those tests actually need jpeg_* symbols from libjpeg.so, not libtensorflow_framework.so. Without my change, bazel will produce both static and dynamic libjpeg library for test build.\r\n\r\nI made a new change which use ld version script to selectively hide jpeg symbols when linking libtensorflow_framework.so. With the new change, all the tests passed except for 3 grpc tests (related to my test environment). The new change works only for Linux. For OS X, I don't know how to selectively hide symbols using \"-exported_symbols_list\" option.\r\n\r\nI will create a pull request for the new change.", "@allenlavoie , could you please review pull request pull #19966 ?\r\n"]}, {"number": 9524, "title": "Add notes about republishing", "body": "", "comments": ["ptal"]}, {"number": 9523, "title": "Indicate that tf.contrib.rnn functions will be moved back to core for TF 1.2, not 1.1", "body": "See https://github.com/tensorflow/tensorflow/issues/7664", "comments": []}, {"number": 9522, "title": "Changing optimizer during the training gives weird results.", "body": "I am trying to change the var_list provided to the minimize() function after some iterations. I am trying to implement a two step finetuning, where for first \"n\" iterations, I am training the last layer of the network and after that finetuning the whole network. So for first \"n\" iterations i am providing variables of last layer in var_list and after \"n\" iterations i am providing all variables in the network. \r\nIt seems like whole network is being reinitialised when I change the optimizer after \"n\" iterations. ", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request.  Presumably you are running variable initialization a second time, but there is no way to know from the information provided."]}, {"number": 9521, "title": "Documentation for tf.nn.ctc_* `label` argument is unclear", "body": "The documentation for the connectionist temporal classifiers is unclear for `label` argument.  Here is what exists currently:\r\n\r\n```\r\nlabels: An int32 SparseTensor. labels.indices[i, :] == [b, t] means labels.values[i] stores the id for (batch b, time t). labels.values[i] must take on values in [0, num_labels). See core/ops/ctc_ops.cc for more details.\r\n```\r\n\r\nThe only way I was able to figure it out is from Jerod's comment on this SO:\r\n\r\nhttp://stackoverflow.com/questions/42488070/how-to-design-the-label-for-tensorflows-ctc-loss-layer", "comments": ["Thanks for raising this issue. Do you have any suggestions on how to make it clearer? Perhaps you could provide a PR to make it clearer. If you know what the SparseTensor is, even having even this description is redundant (it seems not scalable to include detailed documentation on the semantics of sparse tensors in every routine that takes one). Should it perhaps link to the SparseTensor documentation?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I suppose it would be nice to give an example on how to prepare a sequence of labels (e.g. characters of a sentence) to be encoded using some character set, and converted into a `SparseTensor` to be passed into the ctc loss function. "]}, {"number": 9520, "title": "Minor typo", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please"]}]