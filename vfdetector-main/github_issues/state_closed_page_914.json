[{"number": 26039, "title": "Fix call to backend.placeholder()", "body": "In `base_layer.py`, there is this line:\r\n\r\n```python\r\nph = backend.placeholder(shape, self.dtype)\r\n```\r\n\r\nBut the `placeholder()` function's signature in `backend.py` is:\r\n\r\n```python\r\ndef placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None):\r\n    ...\r\n```\r\n\r\nSo it is actually sending `ndim=self.dtype`. Not sure when it will break, but it will break. So I'm replacing it with:\r\n\r\n```python\r\nph = backend.placeholder(shape=shape, dtype=self.dtype)\r\n```", "comments": []}, {"number": 26038, "title": "Warn or throw error when calling tf.train.TFRecordDataset([])", "body": "### Issue\r\n\r\nI am just here because it took me way to long to get behind this issue as somebody who tried to write `.tfrecord` files and use them in a `TFRecordDataset` for the first time.\r\n\r\nI am not sure if this is intended behavior but it turns out that `dataset = tf.data.TFRecordDataset([])` won't bother you until you run the graph in a session but will only give you \r\n\r\n```\r\nOutOfRangeError (see above for traceback): End of sequence\r\n```\r\nAs an error. I was thinking about everything else like wrong data types, not enough samples inside the files etc. until I turned out to be a typo in `file_paths` and I realized I was just passing in an empty array.\r\n\r\n### Reproduce\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\ndef main():\r\n\r\n    dataset = tf.data.TFRecordDataset([])\r\n    iterator = dataset.make_one_shot_iterator()\r\n    get_next = iterator.get_next()\r\n\r\n    with tf.Session() as session:\r\n        session.run(tf.global_variables_initializer())\r\n        batch = session.run(get_next)\r\n        print(batch)\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\n\r\n### Environment\r\n\r\n```\r\n$ pip freeze | grep tensor\r\ntensorboard==1.12.0\r\ntensorflow-gpu==1.12.0\r\n\r\n$ uname -svomp\r\nLinux #168-Ubuntu SMP Wed Jan 16 21:00:45 UTC 2019 x86_64 x86_64 GNU/Linux\r\n```", "comments": ["Although this is an unlikely path to take at the top level of an input pipeline with a literal empty list, I don't think we can make it an error, because it is a legitimate definition for an empty dataset. For example, the list might be created by a library function that shards up a list of files, and in some cases the individual list assigned to a particular worker might be empty."]}, {"number": 26037, "title": "winsock2.h", "body": "tensorflow\\core\\platform\\cloud\\gcs_dns_cache.cc \u6587\u4ef6\uff0ci think it should like below \u3002\r\n#include <winsock2.h>\r\n#include <Windows.h>", "comments": ["@kerwinxu Thank you for your post. We noticed you have not filled out the fields in the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. Are you getting the error during installation or during running some code. It would be great if you can provide a small code to reproduce the error. Thanks!"]}, {"number": 26036, "title": "event.pb.h error", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (win10):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r1.12\r\n- Python version:3.7\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 0.22\r\n- GCC/Compiler version (if compiling from source): vc 2017\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No \r\nI want to build the only cpu version first .\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\npython ./configure.py ...\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:   bazel-out/x64_windows-opt/genfiles\\tensorflow/core/util/event.\r\npb.h\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:    bazel-out/x64_windows-opt/genfiles\\tensorflow/core/framework/\r\nsummary.pb.h\r\nbazel-out/x64_windows-opt/genfiles\\tensorflow/core/util/event.pb.h(584): error C2059: \u8bed\u6cd5\u9519\u8bef:\u201c\u5e38\u6570\u201d\r\nbazel-out/x64_windows-opt/genfiles\\tensorflow/core/util/event.pb.h(585): error C2238: \u610f\u5916\u7684\u6807\u8bb0\u4f4d\u4e8e\u201c;\u201d\u4e4b\u524d\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 5943.765s, Critical Path: 303.77s\r\nINFO: 644 processes: 644 local.\r\nFAILED: Build did NOT complete successfully\r\nevent.pb.h:\r\n584: static const Level ERROR =\r\n585:   LogMessage_Level_ERROR;\r\n", "comments": []}, {"number": 26035, "title": "I having an issue that tensorflow is throwing \"Failed to load the native Tensorflow runtime\".", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: *Windows 10 Home*\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda 9.2, cuDNN 7.4.1.5\r\n- GPU model and memory: gtx 1050, 4gb DDR5\r\n-Driver version: 398.35\r\n-Laptop RAM: 8GB\r\n-Platform : Anaconda3 64-bit for python 3.6\r\n\r\n\r\nBelow is the error that it keep on getting. I also looked for the solution that asked me to install Microsoft c++ redistributable 2015. After installing that as well the error persists.\r\n*****\r\nrunfile('C:/Users/vishw/.spyder-py3/temp.py', wdir='C:/Users/vishw/.spyder-py3')\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-1-5780f59638b8>\", line 1, in <module>\r\n    runfile('C:/Users/vishw/.spyder-py3/temp.py', wdir='C:/Users/vishw/.spyder-py3')\r\n\r\n  File \"C:\\Users\\vishw\\Anaconda3\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 705, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"C:\\Users\\vishw\\Anaconda3\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 102, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"C:/Users/vishw/.spyder-py3/temp.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n\r\n  File \"C:\\Users\\vishw\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n\r\n  File \"C:\\Users\\vishw\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\n  File \"C:\\Users\\vishw\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\vishw\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\vishw\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\vishw\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\vishw\\Anaconda3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\vishw\\Anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n*******\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI need a bit of help here. If you need more information about hardware so i am using MSI GF638RC with gtx1050 nvidia graphic card.\r\n\r\nI ran a basic tensorflow code to print hello tensorflow to test. \r\n\r\nThank you !\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["In addition to installing MSVC 2015 you also have to lower your cuda version to 9.0 since TF 1.12 comes with pre-built binaries to support cuda 9.0 Further you will have to set appropriate cuda, cudnn paths. Please refer [windows setup](https://www.tensorflow.org/install/gpu#windows_setup) guide to do this.", "@ymodak I tried to install cuda 9.0 but it is showing me not compatible with your driver version. Should i go and still install it ? ", "Yes that should be fine since your driver version is 398.35, CUDA 9.0 accepts 384.81 and higher Nvidia drivers.\r\n\r\n\r\n\r\n", "Thanks the problem got resolved. Really appreciate it !!", "Awesome! I will close this issue now. Thanks!", "\r\nAre you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)\r\n"]}, {"number": 26034, "title": "Export keras.losses.{huber_loss, logloss}", "body": "See https://github.com/tensorflow/tensorflow/issues/26012#issuecomment-466645654", "comments": ["@danaugrs can you fix the build failures : https://source.cloud.google.com/results/invocations/33e50490-71a6-4de6-96e9-2fcf3683026d/log", "@rthadur I fixed it - needs to be reviewed again apparently @pavithrasv.", "Can one of the admins verify this patch?", "@pavithrasv can you please review new changes.", "Closing the PR due to inactivity. Please feel free to reopen."]}, {"number": 26032, "title": "[TF2.0] default type ", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n- Are you willing to contribute it (Yes/No):\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n", "comments": []}, {"number": 26031, "title": "Cannot create stateful metrics based on symbolic tensors using the functional API", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMac OS X 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\ntf.version.VERSION == '2.0.0-dev20190222'\r\ntf.version.GIT_VERSION == 'v1.12.0-8615-g74016a0d51'\r\n- Python version:\r\n3.6.8\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nI get an exception when I try to create a metric based on a symbolic tensor when using the functional API: `keras.metrics.Mean()(hidden1)`\r\n\r\n**Describe the expected behavior**\r\nI expect to be able to define metrics based on any layer's output, when using the functional API.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\ninputs = keras.layers.Input(shape=[2])\r\nhidden1 = keras.layers.Dense(30)(inputs)\r\nhidden1_mean = keras.metrics.Mean()(hidden1) # TypeError: see stacktrace below\r\n```\r\n\r\n**Other info / logs**\r\nI know I could use instead:\r\n\r\n```python\r\nmodel.add_metric(hidden1, name=\"hidden1_mean\", aggregation=\"mean\")\r\n```\r\n\r\nBut using stateful metrics should be possible.\r\n\r\nHere is the stacktrace:\r\n\r\n```pycon\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-1-8b9d3421cd04> in <module>\r\n      4 inputs = keras.layers.Input(shape=[2])\r\n      5 hidden1 = keras.layers.Dense(30)(inputs)\r\n----> 6 mean = keras.metrics.Mean()(hidden1)\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/metrics.py in __call__(self, *args, **kwargs)\r\n    172       The metric value tensor.\r\n    173     \"\"\"\r\n--> 174     update_op = self.update_state(*args, **kwargs)\r\n    175     with ops.control_dependencies([update_op]):\r\n    176       result_t = self.result()\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/utils/metrics_utils.py in decorated(metric_obj, *args, **kwargs)\r\n     70     \"\"\"Decorated function with `add_update()`.\"\"\"\r\n     71\r\n---> 72     update_op = update_state_fn(*args, **kwargs)\r\n     73     if update_op is not None:  # update_op will be None in eager execution.\r\n     74       metric_obj.add_update(update_op, inputs=True)\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/metrics.py in update_state(self, values, sample_weight)\r\n    324     elif self.reduction == metrics_utils.Reduction.WEIGHTED_MEAN:\r\n    325       if sample_weight is None:\r\n--> 326         num_values = math_ops.cast(array_ops.size(values), self._dtype)\r\n    327       else:\r\n    328         num_values = math_ops.reduce_sum(sample_weight)\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    178     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    179     try:\r\n--> 180       return target(*args, **kwargs)\r\n    181     except (TypeError, ValueError):\r\n    182       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in size(input, name, out_type)\r\n    397   @end_compatibility\r\n    398   \"\"\"\r\n--> 399   return size_internal(input, name, optimize=True, out_type=out_type)\r\n    400\r\n    401\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in size_internal(input, name, optimize, out_type)\r\n    418     input = ops.convert_to_tensor(input)\r\n    419     np_out_type = out_type.as_numpy_dtype\r\n--> 420     num_elements = np.prod(input._shape_tuple(), dtype=np_out_type)  # pylint: disable=protected-access\r\n    421     return ops.convert_to_tensor(num_elements, dtype=out_type)\r\n    422   with ops.name_scope(name, \"Size\", [input]) as name:\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/numpy/core/fromnumeric.py in prod(a, axis, dtype, out, keepdims, initial)\r\n   2770     \"\"\"\r\n   2771     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out, keepdims=keepdims,\r\n-> 2772                           initial=initial)\r\n   2773\r\n   2774\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/numpy/core/fromnumeric.py in _wrapreduction(obj, ufunc, method, axis, dtype, out, **kwargs)\r\n     84                 return reduction(axis=axis, out=out, **passkwargs)\r\n     85\r\n---> 86     return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\r\n     87\r\n     88\r\n\r\nTypeError: int() argument must be a string, a bytes-like object or a number, not 'NoneType'\r\n```", "comments": ["@ageron I could not reproduce the issue with the above code snippet anymore. \r\n\r\nSide note: we do not support `model.add_metric(metric_obj)` on Functional model as we cannot trace back from the metric result to model inputs for serialization. I assume you want to create metric object and use it with add_metric API?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26031\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26031\">No</a>\n"]}, {"number": 26030, "title": "added shape_invariants, doc examples and tests", "body": "`foldl` and `foldr` both use `while_loop` under-the-hood but don't expose `shape_invariants`. Exposing this makes things like `np.repeat` much more easily replicated. It also makes things that are currently only possible in eager mode also possible in graph mode (the repeat example works in eager mode without this change but cannot be done in graph mode).", "comments": ["Turns out the same functionality can be achieved using `TensorArray`s and that there's already a tensorflow `repeat` burried in `tensorflow.python.ragged.ragged_utils`. There _may_ still be edge cases where this is useful, but significantly less so knowing the above. Closing."]}, {"number": 26029, "title": "super() does not work within a tf.function", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMac OS X 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\ntf.version.VERSION == '2.0.0-dev20190222'\r\ntf.version.GIT_VERSION == 'v1.12.0-8615-g74016a0d51'\r\n- Python version:\r\n3.6.8\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nWhen I call a method decorated by `@tf.function`, I get an error if it uses `super()`: `RuntimeError: super(): __class__ cell not found`.\r\n\r\n**Describe the expected behavior**\r\nI expect no error, `@tf.function` should ensure that `super()` works normally.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass A:\r\n    def foo(self, x):\r\n        return x + 1\r\n\r\nclass B(A):\r\n    @tf.function\r\n    def bar(self, x):\r\n        return super().foo(x)\r\n\r\nb = B()\r\nb.bar(5) # raises RuntimeException\r\n```\r\n\r\n**Other info / logs**\r\nI can work around this issue in multiple ways:\r\n\r\nThe easiest is to replace `super()` with `super(B, self)`. But it's 2019, who still uses Python 2 style? ;-)\r\n\r\nOr else, I can work around the issue by using `autograph=False`. This shows that the issue is linked to autograph not recognizing `super()`, only `super(B, self)`:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass A:\r\n    def foo(self, x):\r\n        return x + 1\r\n\r\nclass B(A):\r\n    @tf.function(autograph=False)\r\n    def bar(self, x):\r\n        return super().foo(x)\r\n\r\nb = B()\r\nb.bar(5) # okay, returns 6\r\n```\r\n\r\nI can also work around this issue by calling `super()` outside of the method, e.g., in the constructor:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass A:\r\n    def foo(self, x):\r\n        return x + 1\r\n\r\nclass B(A):\r\n    def __init__(self):\r\n        self._super = super()\r\n    @tf.function\r\n    def bar(self, x):\r\n        return self._super.foo(x)\r\n\r\nb = B()\r\nb.bar(5) # okay, returns 6\r\n```\r\n\r\nI tried to work around it using a `tf.init_scope()`, but I could not get it to work, not sure why.\r\n\r\nHere is the full stacktrace for the first example code:\r\n\r\n```pycon\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-2-db8198c32b2e> in <module>\r\n      9\r\n     10 b = B()\r\n---> 11 b.bar(5)\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    424     # This is the first call of __call__, so we have to initialize.\r\n    425     initializer_map = {}\r\n--> 426     self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n    427     if self._created_variables:\r\n    428       try:\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    368     self._concrete_stateful_fn = (\r\n    369         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 370             *args, **kwds))\r\n    371\r\n    372     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   1278     if self._input_signature:\r\n   1279       args, kwargs = None, None\r\n-> 1280     graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   1281     return graph_function\r\n   1282\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   1545           or call_context_key not in self._function_cache.missed):\r\n   1546         self._function_cache.missed.add(call_context_key)\r\n-> 1547         graph_function = self._create_graph_function(args, kwargs)\r\n   1548         self._function_cache.primary[cache_key] = graph_function\r\n   1549         return graph_function, args, kwargs\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   1477             arg_names=arg_names,\r\n   1478             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 1479             capture_by_value=self._capture_by_value),\r\n   1480         self._function_attributes)\r\n   1481\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    636         tf_decorator.rewrap(python_func, original_func, converted_func)\r\n    637\r\n--> 638       func_outputs = python_func(*func_args, **func_kwargs)\r\n    639\r\n    640       # invariant: `func_outputs` contains only Tensors, IndexedSlices,\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    315         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    316         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 317         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    318     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    319\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in bound_method_wrapper(*args, **kwargs)\r\n   2060     # If __wrapped__ was replaced, then it is always an unbound function\r\n   2061     # that takes self as first argument.\r\n-> 2062     return wrapped_fn(weak_instance(), *args, **kwargs)\r\n   2063   weak_bound_method_wrapper = weakref.ref(bound_method_wrapper)\r\n   2064\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    629                   optional_features=autograph_options,\r\n    630                   force_conversion=True,\r\n--> 631               ), args, kwargs)\r\n    632\r\n    633         # Wrapping around a decorator allows checks like tf_inspect.getargspec\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, args, kwargs)\r\n    358     return f(*args, **kwargs)\r\n    359\r\n--> 360   result = converted_f(*effective_args, **kwargs)\r\n    361\r\n    362   # The converted function's closure is simply inserted into the function's\r\n\r\n/var/folders/wy/h39t6kb11pnbb0pzhksd_fqh0000gn/T/tmpd7lvn4li.py in tf__bar(self, x)\r\n      4   retval_ = None\r\n      5   do_return = True\r\n----> 6   retval_ = ag__.converted_call('foo', super(), ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (x,), {})\r\n      7   return retval_\r\n      8\r\n\r\nRuntimeError: super(): __class__ cell not found\r\n```", "comments": ["Oh, this is tricky. @mdanatg I think we can monkey-patch the locals() ag uses to define __class__ and fix this?", "Interesting. I'm confident we can handle it correctly, we just need to make sure we understand the mechanics of the new-style super.", "I've been looking at the pep ( https://www.python.org/dev/peps/pep-3135/ )\nand I think it amounts to setting the __class__ cell.\n\nOn Wed, Feb 27, 2019 at 10:06 AM Dan Moldovan <notifications@github.com>\nwrote:\n\n> Interesting. I'm confident we can handle it correctly, we just need to\n> make sure we understand the mechanics of the new-style super.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26029#issuecomment-467966267>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxYM0sOzHmCKDtnYAgaGVASVlQ146ks5vRskCgaJpZM4bN1IF>\n> .\n>\n\n\n-- \n - Alex\n", "Fixed by https://github.com/tensorflow/tensorflow/commit/612ceb6c488e228fa5246d2452799cf2691ef5f1", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26029\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26029\">No</a>\n"]}, {"number": 26028, "title": "TensorFlow library was compiled to use AVX512F instructions not importing after install", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): Source (master a2bb5db1bf7931b0dc2cd08e53b8798489568198)\r\n- TensorFlow version: \r\n- Python version: 3.6.5\r\n- Installed using virtualenv? pip? conda?: Pip from Intel Distribution for Python on Conda\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n- CUDA/cuDNN version: 10.0 / 7.4.2\r\n- GPU model and memory: 2080 Ti - 11 GB\r\n- Intel MKL Version: 2019.2.057\r\n- Intel CPU: i9 9900K\r\n\r\n**Describe the problem**\r\n\r\nAfter building and installing with `pip` and then testing via importing with python, `cpu_feature_guard.cc` reporting the TF library was compiled to use AVX512F but they aren't available on the device. I have checked this and the instruction sets are available on my Intel CPU. Can anyone help? I've been having trouble using this bazel build configuration lately but it was working fine previously before without Intel MKL. \r\n\r\n```\r\nPython 3.6.5 |Intel Corporation| (default, Aug  3 2018, 14:28:11) \r\n[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\nIntel(R) Distribution for Python is brought to you by Intel Corporation.\r\nPlease check out: https://software.intel.com/en-us/python-distribution\r\n>>> import tensorflow as tf\r\n2019-02-23 17:55:40.392830: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX512F instructions\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/2896601/tf_env.txt)\r\n, but these aren't available on your machine.\r\nAborted (core dumped)\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\n(intel_dl4cv) $ bazel build --config=opt --config=cuda --config=mkl -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mavx512f --copt=-mavx512pf --copt=-mavx512cd --copt=-mavx512er //tensorflow/tools/pip_package:build_pip_package\r\n...\r\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\nINFO: Elapsed time: 3895.082s, Critical Path: 245.69s\r\nINFO: 15722 processes: 15722 local.\r\nINFO: Build completed successfully, 16924 total actions\r\n\r\n(intel_dl4cv) $ bazel-bin/tensorflow/tools/pip_package/build_pip_package tensorflow_pkg\r\n\r\n(intel_dl4cv) $ python -V\r\nPython 3.6.5 :: Intel Corporation\r\n\r\n(intel_dl4cv) $ which pip\r\n/home/codeninja/anaconda3/envs/intel_dl4cv/bin/pip\r\n\r\n(intel_dl4cv) $ pip install tensorflow_pkg/tensorflow-1.12.0-cp36-cp36m-linux_x86_64.whl \r\nProcessing ./tensorflow_pkg/tensorflow-1.12.0-cp36-cp36m-linux_x86_64.whl\r\n...\r\nInstalling collected packages: tensorflow-estimator, tensorboard, google-pasta, tensorflow\r\nSuccessfully installed google-pasta-0.1.4 tensorboard-1.12.2 tensorflow-1.12.0 tensorflow-estimator-1.13.0\r\n\r\n(intel_dl4cv) $ python\r\nPython 3.6.5 |Intel Corporation| (default, Aug  3 2018, 14:28:11) \r\n[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\nIntel(R) Distribution for Python is brought to you by Intel Corporation.\r\nPlease check out: https://software.intel.com/en-us/python-distribution\r\n>>> import tensorflow as tf\r\n2019-02-23 17:55:40.392830: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX512F instructions, but these aren't available on your machine.\r\nAborted (core dumped)\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nChecking the instruction sets I have available on my machine.\r\n```\r\n$ gcc -march=knl -dM -E - < /dev/null | egrep \"SSE|AVX\" | sort\r\n#define __AVX__ 1\r\n#define __AVX2__ 1\r\n#define __AVX512CD__ 1\r\n#define __AVX512ER__ 1\r\n#define __AVX512F__ 1\r\n#define __AVX512PF__ 1\r\n#define __SSE__ 1\r\n#define __SSE2__ 1\r\n#define __SSE2_MATH__ 1\r\n#define __SSE3__ 1\r\n#define __SSE4_1__ 1\r\n#define __SSE4_2__ 1\r\n#define __SSE_MATH__ 1\r\n#define __SSSE3__ 1\r\n```\r\n", "comments": ["Could you please run the same instruction on \"native\" just to be sure\r\ngcc -march=native -dM -E - < /dev/null | egrep \"SSE|AVX\" | sort", "Results\r\n```\r\ngcc -march=native -dM -E - < /dev/null | egrep \"SSE|AVX\" | sort\r\n#define __AVX__ 1\r\n#define __AVX2__ 1\r\n#define __SSE__ 1\r\n#define __SSE2__ 1\r\n#define __SSE2_MATH__ 1\r\n#define __SSE3__ 1\r\n#define __SSE4_1__ 1\r\n#define __SSE4_2__ 1\r\n#define __SSE_MATH__ 1\r\n#define __SSSE3__ 1\r\n```\r\n\r\nThanks for the help. "]}, {"number": 26027, "title": "Convert tuple to list before added to a list", "body": "Propose to fix #26026 \r\n\r\nP.S.\r\n\r\n1. This is a quick & dirty fix, I can only confirm that resolve the problem demonstrated by the reproduce code in #26026 \r\n1. After this fix, the reproduce code will still fail with error `AttributeError: 'list' object has no attribute '_keras_mask'`, I believe that's because we should not use `return_state` with a `Sequential` model?", "comments": ["Ack, let me take a look for other places that need fix. ", "The fix should just be casting the tuple to list. For the follow up error after your change, it is indeed caused by using Sequential with a layer that returns more than one output (eg, in this case 2, one for output, and the second is the state). The error message I saw is:\r\n\r\n\"TypeError: All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.\"\r\n\r\nI have a pending change internally that also fix the same error for LSTM, and updated the unit test for it. Will close the issue once that is submitted.\r\n\r\nThanks for reporting the issue, and being an active contributor of Tensorflow.", "The fix has been submitted at https://github.com/tensorflow/tensorflow/commit/d51261962b3ada9f3024916d9f8cbf74aae4a8c3.\r\n\r\nThis PR can be close now. Thanks.", "Great to know this PR works and you had fixed the issue.\r\n\r\nHave a nice day!"]}, {"number": 26026, "title": "TypeError: can only concatenate list (not \"tuple\") to list", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n* Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N\r\n* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n* Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N\r\n* TensorFlow installed from (source or binary): binary\r\n* TensorFlow version (use command below): 2.0.0-dev20190217\r\n* Python version: 3.5.2\r\n* Bazel version (if compiling from source): n/a\r\n* GCC/Compiler version (if compiling from source): n/a\r\n* CUDA/cuDNN version: 10\r\n* GPU model and memory: GTX 1080 8G\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"t.py\", line 16, in <module>\r\n    model(data)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 620, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/sequential.py\", line 248, in call\r\n    outputs = layer(inputs, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/recurrent.py\", line 713, in __call__\r\n    return super(RNN, self).__call__(inputs, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 620, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/recurrent.py\", line 2132, in call\r\n    return [output] + states\r\nTypeError: can only concatenate list (not \"tuple\") to list\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nShould not throw exception\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```py\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.Masking())\r\nmodel.add(tf.keras.layers.GRU(\r\n    units=1,\r\n    # Set `return_state` to `True` will cause the TypeError:\r\n    #   \"can only concatenate list (not \"tuple\") to list\"\r\n    return_state=True,\r\n))\r\n\r\ndata = [1.]\r\ndata = tf.convert_to_tensor(data)\r\ndata = tf.reshape(data, (1, 1, 1))\r\n\r\nmodel(data)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n1. If we do not use `tf.keras.layers.Masking`, then it will not throw the exception\r\n1. If we do not use `return_state`, then it will not throw the exception\r\n\r\nThe problem is at:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/2ae06ca49145bfee2e5e98c64ae5cbe064a58a33/tensorflow/python/keras/layers/recurrent.py#L2134\r\n\r\nWhich the `state` returned from `K.rnn` is a `tuple` instead of a `list`, so that ` [output] + states` will fail and throw exception.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/2ae06ca49145bfee2e5e98c64ae5cbe064a58a33/tensorflow/python/keras/layers/recurrent.py#L2107\r\n", "comments": ["Thanks for reporting the issue, I will take a look within this week.", "Fixed."]}, {"number": 26025, "title": "unable to import tensor flow and keras", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip installed using the tensorflow guide\r\n- TensorFlow version: tried with pip install tensorflow, pip install tensorflow==1.5, pip install tensorflow==1.4\r\n- Python version: 3.6.0\r\n- Installed using virtualenv? pip? conda?: in command line using pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: intel core i5\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\npython 3.6 is installed and I have pip installed tensorflow==1.4 still receiving that error\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nC:\\Users\\kashish\\myFlask>python\r\nPython 3.6.0 (v3.6.0:41df79263a11, Dec 23 2016, 08:06:12) [MSC v.1900 64 bit (AM\r\nD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>>\r\n>>>\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\kashish\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-package\r\ns\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\kashish\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-package\r\ns\\tensorflow\\python\\__init__.py\", line 52, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"C:\\Users\\kashish\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-package\r\ns\\tensorflow\\core\\framework\\graph_pb2.py\", line 6, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"C:\\Users\\kashish\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-package\r\ns\\google\\protobuf\\descriptor.py\", line 47, in <module>\r\n    from google.protobuf.pyext import _message\r\nImportError: DLL load failed: The specified procedure could not be found.\r\n>>>\r\n", "comments": ["I also installed microsoft visual c++ 2015 redistributable, still getting errors. and few of the similar problem suggested to uninstall tensorflow and try lower versions of it because it worked for them, I have tried those as well, still stuck with this problem. Help Me!!\r\nThanks\r\n\r\nit successfully installs keras and tensorflow but then creates error while importing them", "TF 1.5 is very old version. Why don't you install the latest one? You can simple do,\r\n> pip uninstall tensorflow\r\n> pip install tensorflow\r\n\r\nNo need to specify version. It will grab the latest one.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 26024, "title": "[TF Java] Support for AddControlInput in generated factory methods for building operations ", "body": "**System information**\r\n- TensorFlow version (you are using): 1.12\r\n- Are you willing to contribute it (Yes/No): yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThere are automatically generated wrapper classes for Graph Operations that provide a neat interface and consistency in practice. We can easily create graph operations with the `create` method. \r\n\r\nHowever, there is no option to add control inputs to a newly built operation using these wrapper classes. \r\n\r\nFor example, in the generated `Add.java` file:\r\n```\r\n    public static <T> Add<T> create(Scope scope, Operand<T> x, Operand<T> y) {\r\n        OperationBuilder opBuilder = scope.graph().opBuilder(\"Add\", scope.makeOpName(\"Add\"));\r\n        opBuilder.addInput(x.asOutput());\r\n        opBuilder.addInput(y.asOutput());\r\n        return new Add(opBuilder.build());\r\n    }\r\n```\r\n\r\n**Will this change the current api? How?**\r\n\r\nOne possibility is to overload the `create` method, such that the new overloaded method would have an additional parameter that is a list of control inputs. This way, current usages of the original `create` method would not break. The proposed generated `create` method would look like:\r\n```\r\n    public static <T> Add<T> create(Scope scope, Operand<T> x, Operand<T> y, Iterable<Operand<T>> controlInputs) {\r\n        OperationBuilder opBuilder = scope.graph().opBuilder(\"Add\", scope.makeOpName(\"Add\"));\r\n        opBuilder.addInput(x.asOutput());\r\n        opBuilder.addInput(y.asOutput());\r\n       for (Operand<T> ctrl : controlInputs) { opBuilder = operBuilder.addControlInput(ctrl.asOutput().op()) }\r\n        return new Add(opBuilder.build());\r\n    }\r\n```\r\nThis would require modifying `tensorflow/tensorflow/java/src/gen/cc/op_generator.cc` to add this to the code gen. \r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone who wants to add control inputs to their graph. \r\n\r\n**Any Other info.**\r\nIt is possible to add control inputs to operations by directly using OperationBuilder like so:\r\n```\r\n  public <T> Operation addOp(String name, Output<T> x, Output<T> y, Operation z) {\r\n    return graph.opBuilder(\"Add\", name)\r\n        .addInput(x).addInput(y).addControlInput(z).build();\r\n  }\r\n```\r\nBut definitely would be best to have addControlInput as an option to use with the neat generated classes :) \r\n \r\ncc: @samdow @melissagrueter ", "comments": ["@sjamesr @karllessard \r\n\r\nAny thoughts regarding this? Thank you!", "Looks good to me, are you going to send a patch? Otherwise I can take care of it.", "@sjamesr yup, I'll open a PR soon. Thanks!", "Hi @irenedea, sorry in advance for the novel but I think it might worth it to get more into details here,\r\n\r\nI think your concern on not having control inputs available in the Ops API is justified. I would be careful though before adding a new `create` factory method inside the wrappers.\r\n\r\nThe thing is that by design, operation wrapper factories were not meant to be accessed directly by the users. The `org.tensorflow.op.*Ops` classes, which carries the scope along the graph building process, should be used instead. In other words, instead of calling `Add<T> add = Add.create(scope, x, y)`, you should call `Add<T> add = tf.add(x, y)`, where `tf` is an instance of `Ops`.\r\n\r\nThe `*Ops` classes are also generated at compile time, not by the C++ generator like the wrappers but by an annotation processor, which is triggered by the annotation `@Operator` added to the generated wrappers.  For each factory method found in the wrapper, an endpoint with the name of the operation is added to the `Ops` class. \r\n\r\nRight now, there is in general only one factory per wrapper (and in a few cases, two), so we end up having a single endpoint call `tf.add()` (or `tf.math.add()` depending on the version you are looking to). Adding a new factory into all wrappers will create a second endpoint for all the ops, which leads to double the size of the Ops API namespace. I think we should try to avoid that. \r\n\r\nWhat should we do, then? A trivial solution would be to name your new factory with a different name (e.g. `createAfter`) so it is not picked by the annotation processor to generate an endpoint in the API. But I personally don't like it because it forces us to use the factory method directly for this case only. \r\n\r\nAnother better but more complex solution is to chain the control input before creating the operation through the `Ops` API. We can already rename an operation added to a graph by invoking `tf.withName(\"addThis\").add(x.y)`, so why not following the same semantic and having something like `tf.controlledBy(assertOp).add(x,y)`?\r\n\r\nIf you also find this option interesting, we can work altogether to find out what would be the best way to implement it. What do you think?", "In case you were not aware of the existence of the `Ops` classes, here [some examples](https://github.com/karllessard/models/tree/master/samples/languages/java/mnist/src/main/java/org/tensorflow/model/sample/mnist) to show there usage. ", "Hi @karllessard, thank you for your response; the details are great! I actually had no idea of the existence of the `Ops` classes, so thank you for pointing that out or I would've missed that completely. (Is there documentation on this somewhere?)\r\n\r\nI do like the idea of the chain solution. It is definitely much neater than doubling the Ops API :) \r\n\r\nWhat do you think about this:\r\n- Add `controlInputs` as a member of `scope` that defaults to an empty list.\r\n- Add `withControlDependencies(Iterable<Operand<?>> controls)` method to scope that returns a new scope with new control inputs. \r\n- Add `controlledBy` method to `Ops`:\r\n```\r\n  public Ops controlledBy(Iterable<Operand<?>> controls) {\r\n    return new Ops(scope.withControlDependencies(controls));\r\n  }\r\n```\r\n- Modify factory method to be something like (no more overloaded method):\r\n```\r\n    public static <T> Add<T> create(Scope scope, Operand<T> x, Operand<T> y) {\r\n        OperationBuilder opBuilder = scope.graph().opBuilder(\"Add\", scope.makeOpName(\"Add\"));\r\n        opBuilder.addInput(x.asOutput());\r\n        opBuilder.addInput(y.asOutput());\r\n       for (Operand<T> ctrl : scope.getControlDependencies()) { opBuilder = operBuilder.addControlInput(ctrl.asOutput().op()) }\r\n        return new Add(opBuilder.build());\r\n    }\r\n```", "> Is there documentation on this somewhere?\r\n\r\nThere is a glitch in current Java documentation on the TF website that prevents to generated classes to appear. I pinged the documentation team about this a few days ago and they said they will take a look at it but it's not on their top-priority list.\r\n\r\nTo palliate the pain, I wanted to merge my examples (the previous link I provided) to master but there's been another issue in the build system that will prevent the latest TF release (1.13.0) to include changes those examples rely upon.\r\n\r\nSo right now, the best documentation we have is [this javadoc.](http://javadoc.io/doc/org.tensorflow/libtensorflow/1.13.0-rc2)\r\n\r\n> What do you think about this:\r\n> * Add `controlInputs` as a member of `scope` that defaults to an empty list.\r\n> * Add `withControlDependencies(Iterable<Operand<?>> controls)` method to scope that returns a new scope with new control inputs.\r\n> * Add `controlledBy` method to `Ops`:\r\n\r\nThat sounds perfect. Maybe we should keep the name of the methods consistent between the `Scope` and the `Ops` but any those should fit.\r\n\r\n> * Modify factory method to be something like (no more overloaded method):\r\n> \r\n> ```\r\n>    public static <T> Add<T> create(Scope scope, Operand<T> x, Operand<T> y) {\r\n>        OperationBuilder opBuilder = scope.graph().opBuilder(\"Add\", scope.makeOpName(\"Add\"));\r\n>        opBuilder.addInput(x.asOutput());\r\n>        opBuilder.addInput(y.asOutput());\r\n>       for (Operand<T> ctrl : scope.getControlDependencies()) { opBuilder = operBuilder.addControlInput(ctrl.asOutput().op()) }\r\n>        return new Add(opBuilder.build());\r\n>    }\r\n> ```\r\n\r\nSounds also good! Just an idea like this, maybe the `Scope` instance should be allowed to visit the operation being built so the logic of that `for` loop (or any special need we might face in the future) would be moved to this class instead (which is easier to maintain than changing the C++ op generator).\r\n\r\nSomething like this?\r\n```\r\n   public static <T> Add<T> create(Scope scope, Operand<T> x, Operand<T> y) {\r\n       OperationBuilder opBuilder = scope.graph().opBuilder(\"Add\", scope.makeOpName(\"Add\"));        \r\n       opBuilder.addInput(x.asOutput());\r\n       opBuilder.addInput(y.asOutput());\r\n       scope.apply(opBuilder);       \r\n       return new Add(opBuilder.build());\r\n   }\r\n```", "Thanks for your work on the documentation side of things as well! Hope it gets resolved soon as it would be great to have more documentation and examples with TF Java. (I haven't had much luck finding good usage examples myself.)\r\n\r\n> Maybe we should keep the name of the methods consistent between the `Scope` and the `Ops` but any those should fit\r\n\r\nGood catch!\r\n\r\n> Just an idea like this, maybe the `Scope` instance should be allowed to visit the operation being built so the logic of that `for` loop (or any special need we might face in the future) would be moved to this class instead (which is easier to maintain than changing the C++ op generator).\r\n\r\nAgreed, that sounds great. ", "closed: https://github.com/tensorflow/tensorflow/pull/26181"]}, {"number": 26023, "title": "Fix the python links for Python3.6", "body": "same as: https://github.com/tensorflow/tensorflow/pull/25989 but for release `1.13.0` branch.", "comments": ["Thanks for the PR @ashahba! We usually don't accept more changes to released branch unless we plan to do another patch release. Does master branch work for you?", "Thanks @YijinLiu `master` works and we are during `1.14` release cycle now and this one is no longer needed."]}, {"number": 26022, "title": " TPU: keras support for multiple layer nodes", "body": "Hopefully fixes #23659, but I'm having some difficulty testing it.", "comments": ["@rjpower @xiejw PTAL? Thanks!", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26022) for more info**.\n\n<!-- need_author_consent -->", "Ack! I rebased it onto the 1.13.1 release so that it would be less of a moving target for copying into colab, but there were unintended consequences! I will rebase it onto master once I'm done testing.", "So when I implement this with my model in colab, the error no longer occurs, but `tf.contrib.tpu.keras_to_tpu_model` stalls with the \"running\" symbol seemingly forever (I've been waiting for an hour now). I could try to test it with a simpler example, but because it doesn't fix my use case I think I am going to close this and wait for a more permanent solution, because my understanding is that this function is slated to be replaced anyway."]}, {"number": 26020, "title": "MirroredStrategy doesn't use GPUs", "body": "\r\n**System information**\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): pip install tensorflow-gpu==1.12\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3.5.2\r\n- CUDA/cuDNN version: CUDA 9 / cuDNN 7.3\r\n- GPU model and memory: 2 x Nvidia Titan X\r\n\r\n**Describe the current behavior**\r\n\r\n\r\nI was working on rewriting a script from the queue/threading approach to tf.data.Dataset approach of providing data. I got really nice throughput of data with over >90% util of GPUs.  Now that I have rewritten it, when starting the training with MirroredStrategy the GPUs are not used at all and I get the following output:\r\n\r\n```\r\nINFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0\r\nINFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_GPU:0\r\nINFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_CPU:0\r\nINFO:tensorflow:Configured nccl all-reduce.\r\nINFO:tensorflow:Calling model_fn.\r\n...\r\n```\r\n\r\nAt this point I am thinking there is some issue with TF1.12.\r\n\r\n**Code to reproduce the issue**\r\n\r\nHere is basically the structure of my code. I tried out different things like train directly with tf.keras.fit with multi_gpu_model() but it didn't work out. Basically I am trying to reproduce the functionality of my RandomShuffleQueue() I had before with multiple threads filling up the queue. \r\n\r\n\r\n\r\n```\r\nmodel = models.Model(inputs=input, outputs=y_output)\r\noptimizer = tf.train.AdamOptimizer(LEARNING_RATE)\r\n# model = utils.multi_gpu_model(model, gpus=NUM_GPUS, cpu_relocation=True)\r\nmodel.compile(loss=lossFunc, optimizer=optimizer)\r\n\r\ndef generator(n):\r\n    while True:\r\n        try:\r\n            imgBatch = []\r\n            \r\n            ...\r\n            \r\n            yield imgBatch\r\n                \r\n        except ValueError:\r\n            pass\r\n        \r\ndef get_generator(n):\r\n    return partial(generator, n)\r\n\r\ndef dataset(n):\r\n    return tf.data.Dataset.from_generator(get_generator(n), output_types=[(tf.float32, tf.float32)], output_shapes=[(tf.TensorShape([None,None,1]),tf.TensorShape([None,None,1]))\r\n\r\ndef input_fn():\r\n    ds = tf.data.Dataset.range(len(dataSets)).apply(tf.data.experimental.parallel_interleave(dataset, cycle_length=len(dataSets), sloppy=True))\r\n    ds = ds.map(map_func=lambda imgBatch: processImage(img,lbl), num_parallel_calls=12)\r\n    ds = ds.shuffle(SHUFFLE_SIZE)\r\n    ds = ds.batch(BATCH_SIZE)\r\n    ds = ds.prefetch(1)\r\n    return ds\r\n                                                                                                                    \r\nstrategy = tf.contrib.distribute.MirroredStrategy(num_gpus=NUM_GPUS)\r\nconfig = tf.estimator.RunConfig(train_distribute=strategy)\r\nestimator = tf.keras.estimator.model_to_estimator(model,\r\n                                                  config=config)\r\n                                                                                                                    \r\nestimator.train(lambda:input_fn())\r\n```\r\n\r\nAny help would be greatly appreciated since I'm stuck on it since a while now.\r\n\r\n", "comments": ["That output looks fine and is expected. The devices it should be using are \"/device:GPU:0\" etc which it doesn't complain about.\r\nWhy do you think GPUs are not being used? ", "I see. The info seemed strange to me. But also I I don't see a lot of util on my GPUs - to be exact it is about 5%/0%, peaking at around 27%/10% sometimes in spikes here and there. In contrast when using the old queue/threaded approach (using a RandomShuffleQueue) instead of tf.data.Dataset, I had a much higher util on the GPUs - about 50% on each GPU without any break. \r\n\r\nSo I'm thinking there must be something wrong with either my input pipeline or the initialization of the GPUs. Any ideas? Maybe there is some issue on how I use the Dataset API and there is some bottleneck regarding creating the data. I am creating multiple Generators to simulate multi-threaded providing of data for shuffling. Appreciate any help since I'm struggling with that for quite some time now. \r\n\r\nAlso any ideas how i could debug that issue?", "Is there actually a proper way to provide the Dataset Shuffle with data in a multi-threaded way? The solution that I did feels kind of hacky and not proper. Might also be the reason it doesn't work as intended.", "I believe it's an input pipeline issue. Could you check out the [performance guide](https://www.tensorflow.org/guide/performance/datasets)?", "You could monitor `$ nvidia-smi` during your training to see whether GPUs are used as well.\r\n\r\nPlease re-open if you think it's not an input pipeline issue and still suspect TF not utilizing available GPUs.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26020\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26020\">No</a>\n"]}, {"number": 26019, "title": "Revert estimator dependency version bump.", "body": "", "comments": []}, {"number": 26018, "title": "[ROCm] Adding scratch_size field to dnn::AlgorithmDesc", "body": "\r\nDescription:\r\n\r\nCurrently the scratch_size (workspace memory size), required to execute\r\na given convolution using a given algorithm, is determined via a call to\r\nthe CUDNN API. This call happens each time a convolution kernel is\r\ncalled, because the scratch_size is not stored on neither the\r\ndnn::AlgorithmDesc nor the dnn::AlgorithmConfig datastructures. Adding\r\nscratch_size to dnn::AlgorithmDesc will remove the need to call the\r\nCUDNN API to retrieve this information everytime we need to do\r\nconvolution.\r\n\r\nThere are also some ROCm platform specific reasons for making this change.\r\n\r\n1. on the ROCm platform, there is no equivalent of the CUDNN API to\r\n   determine the workspace memory size corresponding to a given\r\n   (conv_params, algo) tuple. There is a ROCm API to determine the\r\n   workspace memory size for a given conv_params, but that gives a\r\n   worst case answer since that API does not take a specific algo as an\r\n   input. As a consequence, on the ROCm fork of TF, we maintain a\r\n   scratch_size field on the dnn::AlgorithmConfig data-structure. This\r\n   change will remove the need for this discrepancy in the ROCm TF fork.\r\n\r\n2. A side-effect of this commit is that the GetConvolve*Algorithms\r\n   routines will now take the conv_params (input, filter, conv, output)\r\n   as input. This will allow TF code to leverage underlying CUDNN /\r\n   ROCm MIOpen APIs that return a list of algos that are applicable to\r\n   the given conv_params. Currently the list of algorithms list is\r\n   compiled in the TF code itself, without querying the CUDNN / MIOpen\r\n   APIs.\r\n\r\n----\r\n\r\n\r\n@tatianashp : just FYI\r\n@timshen91 : I am changing code that you recently refactored and hence including you on the cc-list\r\n@chsigg @whchung \r\n", "comments": ["I am unable to locally reproduce the failures listed in the \"GPU Python3\" details. All 15 of the failing tests listed there, pass for me locally (I am using CUDA 9.0 + CUDNN 7.3.1).\r\n\r\nSame applies to the 2 failures int he \"GPU CC\" details", "+@chsigg as well.\r\n\r\n@deven-amd and @whchung ,\r\n\r\nFor the context, internally we are discussing improving the API surface of StreamExecutor. Specifically, we don't think that it makes sense to make a consistent layer of API for both cuDNN and MIOpen. Today, they are already different in a rather non-trivial way. MIOpen may also want to reserve the right to diverge further in the future.\r\n\r\nIn the end, we should remove the unified API \"DnnSupport\", and instead use thin wrappers over MIOpen, cuDNN, and XBlas libraries, etc. Roughly speaking, the wrappers do two things:\r\n* C++-ize the API (e.g. no weakly typed void* around)\r\n* decouple the build-time dependency of cudnn/miopen using dynamic loading.\r\n\r\nAnd users should feel comfortable directly using MIOpen-specific API in their code.\r\n\r\n@chsigg is working on that, but that doesn't happen in fortnight. For now, we want to make whatever changes that solves the problem this PR solves, but also be align with the goal above.\r\n\r\nWhat I'm thinking is\r\n* add specific MIOpen APIs we need into DnnSupport, and mark them as MIOpen-only in the comments. They should be dead-thin wrappers.\r\n* In all callers, whenever necessary, branch on whether the target is MIOpen, and use the API differently. I'd prefer a runtime boolean branching, instead of \"#ifdef\".\r\n\r\nThis approach certainly puts changes on all user code e.g. tf kernels and XLA, but you already changed them anyway.\r\n\r\nWhat do you think?", "@timshen91 \r\n\r\n> In the end, we should remove the unified API \"DnnSupport\", and instead use thin wrappers over MIOpen, cuDNN, and XBlas libraries, etc\r\n\r\nIt would be nice if we can get there. Let us know if we can help in anyway.\r\n\r\nWill the above change be limited to the DNN Support layer (cuDNN/MIOpen) or will it also extend to the other parts of the Stream Executor (blas, fft, rng, etc).\r\n\r\n> What I'm thinking is\r\n> ...\r\n> What do you think?\r\n\r\nI think I follow what you mean, but a concrete example will help ensure we are on the same page.\r\n\r\n>    * add specific MIOpen APIs we need into DnnSupport, and mark them as MIOpen-only in the comments. They should be dead-thin wrappers.\r\n\r\nexample for this  would be the `dnn::DoFusedBatchNormActivationInference` routine added by PR #25436 which currently only has a MIOpen implementation. It is missing an explicit comment indicating that this routine is only functional in ROCm / MIOpen, but I think this is what you had in mind. Is my understanding correct?\r\n\r\nhttps://github.com/ROCmSoftwarePlatform/tensorflow-upstream/blob/google_upstream_add_miopen/tensorflow/stream_executor/dnn.h#L2392\r\nhttps://github.com/ROCmSoftwarePlatform/tensorflow-upstream/blob/google_upstream_add_miopen/tensorflow/stream_executor/rocm/rocm_dnn.cc#L4203\r\n\r\n>    * In all callers, whenever necessary, branch on whether the target is MIOpen, and use the API differently. I'd prefer a runtime boolean branching, instead of \"#ifdef\".\r\n\r\nWe actually go the #ifdef route in our fork route now.\r\n\r\nI am guessing we still need to #ifdef the `REGISTER_KERNEL* macros for GPU_DEVICE.  Currently they are all #ifdef CUDA and we have changed them to be #ifdef CUDA || TENSORFLOW_USE_ROCM in our code...see example below:\r\nhttps://github.com/ROCmSoftwarePlatform/tensorflow-upstream/blob/develop-upstream/tensorflow/core/kernels/conv_ops.cc#L549\r\n\r\nWe then reuse the same approach when the manner in which we use the DNN Support API diverges between CUDA and ROCM...see example below\r\nhttps://github.com/ROCmSoftwarePlatform/tensorflow-upstream/blob/develop-upstream/tensorflow/core/kernels/conv_ops.cc#L899\r\n\r\nMy assumption is that the runtime boolean suggestion only applies to the later scenario. Is the correct? If so, where should this boolean be defined, and what would be the correct way to set it?\r\n\r\nThanks\r\n\r\ndeven \r\n\r\n\r\n", "> @timshen91\r\n> \r\n> > In the end, we should remove the unified API \"DnnSupport\", and instead use thin wrappers over MIOpen, cuDNN, and XBlas libraries, etc\r\n> \r\n> It would be nice if we can get there. Let us know if we can help in anyway.\r\n> \r\n> Will the above change be limited to the DNN Support layer (cuDNN/MIOpen) or will it also extend to the other parts of the Stream Executor (blas, fft, rng, etc).\r\n\r\nYes, that's what I meant. It's not limited to DNN, but any domain-specific vendor logic.\r\n\r\n> \r\n> > What I'm thinking is\r\n> > ...\r\n> > What do you think?\r\n> \r\n> I think I follow what you mean, but a concrete example will help ensure we are on the same page.\r\n> \r\n> > * add specific MIOpen APIs we need into DnnSupport, and mark them as MIOpen-only in the comments. They should be dead-thin wrappers.\r\n> \r\n> example for this would be the `dnn::DoFusedBatchNormActivationInference` routine added by PR #25436 which currently only has a MIOpen implementation. It is missing an explicit comment indicating that this routine is only functional in ROCm / MIOpen, but I think this is what you had in mind. Is my understanding correct?\r\n\r\nYes, I think DoFusedBatchNormActivationInference is a good example to follow for now. Maybe DoMIOpenFusedBatchNormActivationInference is a better name.\r\n\r\n> \r\n> https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/blob/google_upstream_add_miopen/tensorflow/stream_executor/dnn.h#L2392\r\n> https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/blob/google_upstream_add_miopen/tensorflow/stream_executor/rocm/rocm_dnn.cc#L4203\r\n\r\nOh this is even better. You already have the separate API service `MIOpenSupport`. Maybe you can just upstream the whole class?\r\n\r\n> \r\n> > * In all callers, whenever necessary, branch on whether the target is MIOpen, and use the API differently. I'd prefer a runtime boolean branching, instead of \"#ifdef\".\r\n> \r\n> We actually go the #ifdef route in our fork route now.\r\n> \r\n> I am guessing we still need to #ifdef the `REGISTER_KERNEL* macros for GPU_DEVICE. Currently they are all #ifdef CUDA and we have changed them to be #ifdef CUDA || TENSORFLOW_USE_ROCM in our code...see example below:\r\n> https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/blob/develop-upstream/tensorflow/core/kernels/conv_ops.cc#L549\r\n\r\nCorrect, we are not touching those.\r\n\r\n> \r\n> We then reuse the same approach when the manner in which we use the DNN Support API diverges between CUDA and ROCM...see example below\r\n> https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/blob/develop-upstream/tensorflow/core/kernels/conv_ops.cc#L899\r\n> \r\n> My assumption is that the runtime boolean suggestion only applies to the later scenario. Is the correct? If so, where should this boolean be defined, and what would be the correct way to set it?\r\n\r\nCorrect. I was only suggesting all \"vendor domain-specific libraries\" callers (dnn, blas, etc) to use runtime booleans, maybe like the following:\r\n```\r\nif (stream_executor->SupportsMiOpen()) {\r\n  /* MIOpen-specific functions, with dynamic loading */\r\n}\r\n```\r\n\r\nAs for `SupportsMiOpen()`, it can be as simple as `return TENSORFLOW_USE_ROCM;`. My suggestion on runtime boolean should introduce no functional change, but makes building the tests faster.\r\n\r\nTo illustrate the motivation, imagine that we do a full build of CUDA, run all unit tests, then switch to ROCM, do a full build, and run all unit tests.\r\n\r\nIf the TF ops use `#ifdef`, they have to be rebuilt when switching to ROCM; if the ops use a runtime boolean function `SupportsMiOpen()`, and `SupportsMiOpen()` is defined in a single .cc file, switching to ROCM effectively requires to only re-compile that file.\r\n\r\nBTW, *my guess* is that, we also want to use gcc/clang to compile such op, not nvcc or hipcc; in long term, nvcc only compiles CUDA kernels, and hipcc only compiles HIP kernels. The goal is to reuse the build results across the bazel configurations.\r\n\r\nDoes \"reduce build time\" scenario make sense to you? From what I heard, people complain about TF's long-build time. Out of curiosity, do you think such problem exists, and deserves a solution? If so, we may just start with reducing `#if`s...", "rebased twice\r\n- once to remove the merge conflicts\r\n- once to add MIOpen part of the changes for this PR (now that PR #25436 has been merged)", "In today's sync this PR was mentioned. Are you still working on this, or it's my turn?", "Also, to reiterate: I suggested not to use \"#ifdef\" under the assumption that it's easy to implement. If it turns out not to be easy, describe so and we can use \"#ifdef\" for now.", "@timshen91 \r\n\r\nThank you for input during the meeting yesterday, it was very informative.\r\n\r\nRegarding this PR, I think the ball is in your court. I do not believe there are any further changes that need to be done on my end, please let me know if that is not the case.\r\n\r\nRegarding the #ifdef discussion, I think it is somewhat orhtogonal to this PR. It is more applicable to the future PRs we will be filing, and hence my interest in the outcome for it. Once the current batch of ROCm PRs is merged, we will have all the pre-requisites in place to start enabling ROCm support for TF operators. Most of that code that does GPU support for TF operators is currently within \r\n```\r\n#ifdef GOOGLE_CUDA\r\n``` \r\nand the PRs that we file will either change that to either \r\n```\r\n#if defined(GOOGLE_CUDA) || defined(TENSORFLOW_USE_ROCM)\r\n```\r\nor\r\n```\r\n#ifdef GOOGLE_CUDA\r\n...\r\n#elif TENSORFLOW_USE_ROCM\r\n...\r\n```\r\ndepending on whether or not the ROCm implementation for the TF operator needs to diverge from that of CUDA.\r\n\r\nChanging from  #ifdef to a runtime bool requires, at the very least, changing all CUDA/ROCm libs to be dynamically loaded, and thorough testing on both platforms.  I expect this to require a lot of testing effort, and what was not clear was whether this was something you were already in the process of doing, or something you were thinking of doing. Knowing that we were going to have a online meeting,  we took the opportunity to discuss this live.\r\n\r\nI guess the conclusion on it for now is that \r\n* ROCm team should continue with the #ifdef route for the PRs that we will file in the near future. \r\n* Google team will evaluate the feasibility of making the switch from #ifdef to runtime bool \r\n* If it is deemed a viable approach, \r\n  * Google team will communicate the details to the ROCm team\r\n  * ROCm team will evaluate/test whether it works on the ROCm TF\r\n\r\nLet me know if above summary is inaccurate / needs changes.\r\n\r\nThanks\r\n\r\ndeven\r\n\r\n", "No problem, let's forget about the runtime-bool for a moment.\r\n\r\nLooking at the commit, it doesn't seem to deliver what I commented before (quoting my previous comment):\r\n* add specific MIOpen APIs we need into DnnSupport, and mark them as MIOpen-only in the comments. They should be dead-thin wrappers.\r\n* In all callers, whenever necessary, branch on whether the target is MIOpen, and use the API differently. I'd prefer a runtime boolean branching, instead of \"#ifdef\".\r\n\r\nBut I still see `GetConvolveAlgorithms()` is changed to a union for both cuDNN and MIOpen; I imagine it should be `MIOpenGetConvolveAlgorithms()` as a pure addition of interface. I also didn't see any dispatch in `cudnn_conv_runner.cc`; I imagine it should call `GetConvolveAlgorithms()` or `MIOpenGetConvolveAlgorithms()`, depending on which target is built for.\r\n\r\nI know that the ROCm fork is already implemented in one way, and I'm sorry that it's probably harder for you to rebase your rest of the patches onto the suggested design, but I think we should go for the right solution.\r\n\r\nIn addition, let's talk about the runtime-bool in XLA. I knew from @jlebar that XLA doesn't contain any `#if GOOGLE_CUDA`, and you already landed some of the XLA patches. Supposedly we already have some runtime-bool for ROCm in XLA?", "@timshen91 \r\n\r\nI think we have a misunderstanding.\r\n\r\nThe signature of `GetConvolve*Algorithms()` routines is being changed, because on the *CUDA* side, I need the extra params to determine the `scratch_size` for any given algo, and I need access to the `scratch_size` to pass to the `AlgorithmDesc` constructor.\r\n\r\nWe will eventually need the same extra args on the MIOpen side as well (in a couple of months), and hence this update to `GetConvolve*Algorithms()` will be applicable to MIOpen too. \r\n\r\nI took your comment regarding \"specifc MIOpen APIs\" to mean to apply towards something in the future  where we might need to introduce some MIOpen specific funtionality.\r\n\r\nIn this PR, I guess the MIOpen specific requirement is being added to the AlgorithmDesc data structure (i.e. the addition of the `scratch_size` field).  This results in the `GetConvolve*Algorithms()` API being updated, but those updates are not cuDNN/MIOpen specifc.\r\n\r\n\r\n", "@timshen91 \r\n\r\njust wanted to confirm with you that you are not expecting any further changes on my end.\r\n\r\nthanks\r\n\r\ndeven", "> @timshen91\r\n> \r\n> I think we have a misunderstanding.\r\n> \r\n> The signature of `GetConvolve*Algorithms()` routines is being changed, because on the _CUDA_ side, I need the extra params to determine the `scratch_size` for any given algo, and I need access to the `scratch_size` to pass to the `AlgorithmDesc` constructor.\r\n\r\nAh I see, thanks for the clarification.\r\n\r\nReading back the PR description, I see that for CUDA (and maybe also ROCM), the PR intends to reduce the number of calls to `GetConvolve*Algorithms`. Basically, instead of having\r\n```\r\nGetConvolve*Algorithms();\r\nConvolve();\r\nGetConvolve*Algorithms();\r\nConvolve();\r\n```\r\nWith this PR, we now have:\r\n```\r\nGetConvolve*Algorithms();\r\nConvolve();\r\nConvolve();\r\n```\r\n\r\nIn fact, @whchung already did similar work in PR 24081 by introducing `PrepareForConvolution`. Now it's already possible to write:\r\n```\r\nPrepareForConvolution();  // returns scratch_size\r\nConvolve();\r\nConvolve();\r\n```\r\n\r\nThe callers feel free to invoke `stream_executor->AsDnn()->PrepareForConvolution()` to get the `scracth_size` as return value.\r\n\r\nI was focusing on the ROCM side because I thought there is nothing more to be done on the CUDA side.", "@timshen91 : rebased this PR to remove the merge conflicts.\r\n\r\n> Reading back the PR description, I see that for CUDA (and maybe also ROCM), the PR intends to reduce the number of calls to GetConvolve*Algorithms\r\n\r\nThis PR should not have any impact on the order/number of times the `GetConvolve*Algorithms() / Convolve()` routines are called.\r\n\r\nThis PR simply adds extra arguments to pass to the `GetConvolve*Algorithms()`, because those are needed to call the CUDA (and eventually MIOpen) APIs that determine the `scratch_size` needed for a given (conv_params, algo) tuple. \r\n* Determining the `scratch_size` needs to now be done in `GetConvolve*Algorithms()`, because it needs to be passed to the constructor of `AlgorithmDesc`, which is instantaited by the `GetConvolve*Algorithms()`\r\n\r\nNow that we will have `scratch_size` stored in `AlgorithmDesc`, we will no longer need to call the CUDA/MIOpen API to determine the `scratch_size` during each call to `Convolve()`...we can grab that value directly from `AlgorithmDesc`\r\n\r\nHoping this clears things up.\r\n\r\nLet me know if you need anything else from my side.\r\n\r\n", "> @timshen91 : rebased this PR to remove the merge conflicts.\r\n> \r\n> > Reading back the PR description, I see that for CUDA (and maybe also ROCM), the PR intends to reduce the number of calls to GetConvolve*Algorithms\r\n> \r\n> This PR should not have any impact on the order/number of times the `GetConvolve*Algorithms() / Convolve()` routines are called.\r\n> \r\n> This PR simply adds extra arguments to pass to the `GetConvolve*Algorithms()`, because those are needed to call the CUDA (and eventually MIOpen) APIs that determine the `scratch_size` needed for a given (conv_params, algo) tuple.\r\n\r\nIt increasingly sounds like you want to look at `PrepareForConvolution()`. It takes `(conv_params, AlgorithmConfig)` and returns `(AlgorithmDesc, scratch_size)`. As a special case, it takes `(conv_paras, AlgorithmConfig(specific_algo))` and returns `(specific_algo, scratch_size)`, which looks like what you described.", "`PrepareForConvolution()` might be too late for what is needed here. `AlgorightmDesc` is instantiated in the `GetConvolve*Algorithms` routines, and hence the need to determine the scratch_size within that routine, (so that we can pass it to the constuctor of `AlgorithmDesc`).  We can get around the need to specify the scratch size value at construction by having a \"set\" routine for it, but there is no compelling reason to create such a routine, because there should be no reason to change the value of the scratch_size field once initialized.\r\n\r\nAnother reason not go the `PrepareForConvolution()` route would be that it is called everytime we do convolution. The `GetConvolve*Algorithms` routine is called only once per cache miss in the AutotuneConv map. \r\n\r\n", "> `PrepareForConvolution()` might be too late for what is needed here. `AlgorightmDesc` is instantiated in the `GetConvolve*Algorithms` routines, and hence the need to determine the scratch_size within that routine, (so that we can pass it to the constuctor of `AlgorithmDesc`). We can get around the need to specify the scratch size value at construction by having a \"set\" routine for it, but there is no compelling reason to create such a routine, because there should be no reason to change the value of the scratch_size field once initialized.\r\n\r\n`GetConvolve*Algorithms` is called in the kernels (`kernels/conv_ops*`). I think you want to change the ops in order to reflect what's going on there. The result is pretty readable, since everything happens here. For example, this is roughly what we have today:\r\n```\r\n#ifdef GOOGLE_CUDA\r\nif (/* not autotuned */) {\r\n  for (auto algorithm : GetConvolve*Algorithms(...)) {\r\n    stream->ThenConvolve(...)\r\n  }\r\n  /* populate autotuning cache */\r\n}\r\nstream->ThenConvolve(...); // the actual convolution\r\n#endif\r\n```\r\nI think you should feel free to add `#ifdef TENSORFLOW_USE_ROCM` here, e.g.\r\n```\r\n#ifdef TENSORFLOW_USE_ROCM\r\nif (/* not autotued */) {\r\n  RocmSpecificAutotuning(...);\r\n  /* populate autotuning cache */\r\n}\r\nstream->ThenConvolve(...); // the actual convolution\r\n#endif\r\n```\r\n\r\n...given that the cuDNN vs MIOpen API is sufficiently different. Depending on the actual content of `RocmSpecificAutotuning()`, we may or may not be able to share the same use of `PrepareForConvolution()` between CUDA and ROCM. If they aren't able to share `PrepareForConvolution()`, that's totally fine too.\r\n\r\n> Another reason not go the `PrepareForConvolution()` route would be that it is called everytime we do convolution. The `GetConvolve*Algorithms` routine is called only once per cache miss in the AutotuneConv map.\r\n\r\nAutotuning is also in `kernels/conv_ops*`. I suggest to change those \"top-level\" implementations to reflect the underlying cuDNN and MIOpen difference.", "@deven-amd is this PR still WIP ?", "@rthadur .... it is on the back-burner for now, as we focus on the PRs for upstreaming ROCm support for various ops. \r\n\r\nI will get back to this in a week or two...will ping @timshen91 when I do. I suspect that this PR will require more discussion/changes before we reach an agreement on it :)\r\n\r\ndeven", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Closing it out this PR, since there have been changes on the TF side and there are forthcoming changes in the ROCm implementation that impact the change being discussed here. We anticipate that we will not be needing the changes in this PR.\r\n\r\ndeven"]}, {"number": 26017, "title": "modified the documentation of tf.math.add and tf.math.acos operations", "body": "A Pull Request for issues #25802   and #25846 . I have tried to improve the documentation of `tf.math.acos` and `tf.math.add` with this patch. I did this by editing the files `api_def_Acos.pbtxt` and `api_def_Add.pbtxt` and by modelling them after the `api_def_Angle.pbtxt` file within the `tensorflow/tensorflow/core/api_def/base_api` directory. The changes I made were reflected in the generated file `python/ops/gen_math_ops.py`. The `gen_math_ops.py` file was generated by following [these](https://www.tensorflow.org/community/documentation#api_documentation) instructions. \r\n\r\nThis is my first time editing a piece of documentation which is created from a generated file, and so it may not be perfect. Please feel free to point out any errors that I may have made, and I'll be happy to work on them.", "comments": ["@dynamicwebpaige, you might want to take a look.", "@Sudeepam97 - Thank you for wanting to help improve the TF 2.0 API documentation! We appreciate it, and I am happy to work with you to get the changes added. :slightly_smiling_face: \r\n\r\nThe `.pbtxt` files are generated for `tf.math.*` operations; in order to make modifications to the descriptions of these endpoints in TensorFlow's documentation, you need to modify the equivalent `.Doc`s in [`math_ops.cc`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/math_ops.cc).\r\n\r\nAn example would be description [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/math_ops.cc#L531).\r\n\r\nDoes that make sense?", "> @Sudeepam97 - Thank you for wanting to help improve the TF 2.0 API documentation! We appreciate it, and I am happy to work with you to get the changes added. slightly_smiling_face\r\n> \r\n> The `.pbtxt` files are generated for `tf.math.*` operations; in order to make modifications to the descriptions of these endpoints in TensorFlow's documentation, you need to modify the equivalent `.Doc`s in [`math_ops.cc`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/math_ops.cc).\r\n>\r\nOhh. I thought that the `.pbtxt` files were independent files since they were available in the code base beforehand, without any compilation or so. I realize that you mentioned this in the issues thread as well. I feel a little silly now. :sweat_smile: \r\n \r\n> An example would be description [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/math_ops.cc#L531).\r\n> \r\n> Does that make sense?\r\n\r\nYes it does. I'll fix these changes right away. \r\n", "similar changes are pushed by different PR ,so closing this"]}, {"number": 26016, "title": "Update package version to 1.13.0.", "body": "", "comments": []}, {"number": 26015, "title": "RaggedTile op results in XlaCompile error when calculating gradients", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.13.6\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): Both 1.13.0rc2 and tf-nightly\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: no GPU\r\n- GPU model and memory: no GPU\r\n\r\n**Describe the current behavior**\r\nGradient operations on ragged arrays requiring an implicit RaggedTile op result in\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Operation 'product/RaggedTile/Tile' has no attr named '_XlaCompile'. \r\n\r\n**Describe the expected behavior**\r\nGradients can be determined without error.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nnp_arr = np.array([[1.,2.,3.], [4.,5.,6.], [7.,8.,9.]])\r\nmyvar = tf.get_variable('myvar', initializer=np_arr)\r\nragged = tf.ragged.constant([[0,1,2],[1,2],[1]])\r\nwith tf.name_scope('gather'):\r\n    ragged_gather = tf.gather(myvar, ragged)\r\nwith tf.name_scope('product'):\r\n    product = tf.math.multiply(ragged_gather, ragged_gather, name='prod')\r\nwith tf.name_scope('output'):\r\n    output = tf.reduce_sum(product**2, name='output')\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\n\r\nprint(sess.run(ragged_gather))\r\n# <tf.RaggedTensorValue [[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]], [[4.0, 5.0, 6.0], [7.0, 8.0, 9.0]], [[4.0, 5.0, 6.0]]]>\r\nprint(sess.run(output))\r\n# 32745.0\r\n\r\ngrads = tf.gradients(output, myvar)\r\n# tensorflow.python.framework.errors_impl.InvalidArgumentError: Operation 'product/RaggedTile/Tile' has no attr named '_XlaCompile'.\r\n\r\n```\r\n[full_traceback.txt](https://github.com/tensorflow/tensorflow/files/2895038/full_traceback.txt)", "comments": ["Note: I don't seem to have this problem in eager mode. At the moment my work-around is to implement the code in eager.", "The latest tf nightly build fixes this issue. Feel free to reopen if have further questions. Thanks!"]}, {"number": 26014, "title": "TensorFlow devel containers are still built on Ubuntu:16.04 as opposed to Ubuntu:18.04", "body": "I just downloaded the most recent `devel` containers from `DockerHub` and they seem to have been updated just a few hours ago and when I check the OS versions they read: `Ubuntu:16.04`.\r\nI was under impression that those are build from `18.04` and using this [Dockerfiles](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel) however I have:\r\n```\r\n$ docker run -it tensorflow/tensorflow:devel bash\r\nUnable to find image 'tensorflow/tensorflow:devel' locally\r\ndevel: Pulling from tensorflow/tensorflow\r\n...\r\nDigest: sha256:05acb8910fcbdb7b8d7bc6c51c25d767ca5418c9cd9718fe8e6a9975af00c31e\r\nStatus: Downloaded newer image for tensorflow/tensorflow:devel\r\n...\r\nroot@9222294a20e2:/# cat /etc/*release\r\nDISTRIB_ID=Ubuntu\r\nDISTRIB_RELEASE=16.04\r\nDISTRIB_CODENAME=xenial\r\nDISTRIB_DESCRIPTION=\"Ubuntu 16.04.5 LTS\"\r\nNAME=\"Ubuntu\"\r\nVERSION=\"16.04.5 LTS (Xenial Xerus)\"\r\nID=ubuntu\r\nID_LIKE=debian\r\nPRETTY_NAME=\"Ubuntu 16.04.5 LTS\"\r\nVERSION_ID=\"16.04\"\r\nHOME_URL=\"http://www.ubuntu.com/\"\r\nSUPPORT_URL=\"http://help.ubuntu.com/\"\r\nBUG_REPORT_URL=\"http://bugs.launchpad.net/ubuntu/\"\r\nVERSION_CODENAME=xenial\r\nUBUNTU_CODENAME=xenial\r\n```\r\nand\r\n```\r\n$ docker run -it tensorflow/tensorflow:devel-py3 bash\r\n...\r\nroot@21572c4dc3fe:/# cat /etc/release\r\ncat: /etc/release: No such file or directory\r\nroot@21572c4dc3fe:/# cat /etc/*release\r\nDISTRIB_ID=Ubuntu\r\nDISTRIB_RELEASE=16.04\r\nDISTRIB_CODENAME=xenial\r\nDISTRIB_DESCRIPTION=\"Ubuntu 16.04.5 LTS\"\r\nNAME=\"Ubuntu\"\r\nVERSION=\"16.04.5 LTS (Xenial Xerus)\"\r\nID=ubuntu\r\nID_LIKE=debian\r\nPRETTY_NAME=\"Ubuntu 16.04.5 LTS\"\r\nVERSION_ID=\"16.04\"\r\nHOME_URL=\"http://www.ubuntu.com/\"\r\nSUPPORT_URL=\"http://help.ubuntu.com/\"\r\nBUG_REPORT_URL=\"http://bugs.launchpad.net/ubuntu/\"\r\nVERSION_CODENAME=xenial\r\nUBUNTU_CODENAME=xenial\r\n```\r\n", "comments": ["Our internal images are built using the Dockerfiles in the tools/dockerfiles directory, which are indeed based on 16.04. tools/docker is being removed (see https://github.com/tensorflow/tensorflow/pull/26113). Sorry for the confusion."]}, {"number": 26013, "title": "tensorflow c++ crash", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win 10 64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version:1.12.0\r\n- Python version: N/A\r\n- Installed using virtualenv? pip? conda?:N/A\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\nI build tensorflow manually , and write an simple test using c++ api , the crash dump is as follow ,\r\nplease help !!!\r\n\r\n```\r\n0:000> kn\r\n # Child-SP          RetAddr           Call Site\r\n00 00000056`4cf0f880 00007ff7`2111d70d tfdemo!_invoke_watson+0x18 [minkernel\\crts\\ucrt\\src\\appcrt\\misc\\invalid_parameter.cpp @ 224] \r\n01 00000056`4cf0f8b0 00007ff7`2111d77d tfdemo!_invalid_parameter+0xad [minkernel\\crts\\ucrt\\src\\appcrt\\misc\\invalid_parameter.cpp @ 112] \r\n02 00000056`4cf0f8f0 00007ff7`20d53c19 tfdemo!_invalid_parameter_noinfo_noreturn+0x19 [minkernel\\crts\\ucrt\\src\\appcrt\\misc\\invalid_parameter.cpp @ 126] \r\n03 00000056`4cf0f930 00007ff7`20d4cf58 tfdemo!std::_Adjust_manually_vector_aligned+0x79\r\n04 00000056`4cf0f980 00007ff7`20d58222 tfdemo!std::_Deallocate<16,0>+0x28\r\n05 00000056`4cf0f9b0 00007ff7`20d57487 tfdemo!std::allocator<char>::deallocate+0x22\r\n06 00000056`4cf0f9e0 00007ff7`20d506bc tfdemo!std::basic_string<char,std::char_traits<char>,std::allocator<char> >::_Tidy_deallocate+0x87\r\n07 00000056`4cf0fa30 00007ff7`20d5c39c tfdemo!std::basic_string<char,std::char_traits<char>,std::allocator<char> >::~basic_string<char,std::char_traits<char>,std::allocator<char> >+0x1c\r\n08 00000056`4cf0fa70 00007ff7`20d5bc8f tfdemo!tensorflow::NodeBuilder::~NodeBuilder+0x1c\r\n09 00000056`4cf0faa0 00007ff7`20d5ad37 tfdemo!tensorflow::ops::NoOp::NoOp+0x23f\r\n0a 00000056`4cf0fd30 00007ff7`20d5ad89 tfdemo!demo2+0x27\r\n0b 00000056`4cf0fd90 00007ff7`210d4d4c tfdemo!main+0x9\r\n0c (Inline Function) --------`-------- tfdemo!invoke_main+0x22 [f:\\dd\\vctools\\crt\\vcstartup\\src\\startup\\exe_common.inl @ 78] \r\n0d 00000056`4cf0fdc0 00007ffa`94543034 tfdemo!__scrt_common_main_seh+0x10c [f:\\dd\\vctools\\crt\\vcstartup\\src\\startup\\exe_common.inl @ 283] \r\n0e 00000056`4cf0fe00 00007ffa`948f3691 KERNEL32!BaseThreadInitThunk+0x14\r\n0f 00000056`4cf0fe30 00000000`00000000 ntdll!RtlUserThreadStart+0x21\r\n```\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["have found the reason", "> have found the reason\r\n\r\nAnd what is the reason?"]}, {"number": 26012, "title": "[TF 2.0] tf.keras.losses duplication?", "body": "**Windows 10\r\nTF Version: b'v1.11.0-rc2-4-gc19e29306c' 1.11.0\r\nAnaconda Python 3.6.5\r\nGPU: GeForce GTX 1070 Max-Q Design\r\nTensorflow 2.0 (gpu) preview installed via pip.**\r\n\r\nI'm building a [reinforcement learning framework](https://github.com/danaugrs/huskarl) on top of TensorFlow 2.0 using the `tf.keras` API and I've come across the following issue.\r\n\r\nThe 2.0 API docs for [`tf.keras.losses`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses) shows many objects that are not actually available in the preview package. For example the loss classes such as Huber. Hinge, etc... are not accessible.\r\n\r\n1. Why are those classes not included in the preview package?\r\n2. Why are there both classes and functions for many of the same loss types? That seems like unnecessary duplication.\r\n2a. Why is there a `Huber` class but no `huber` function?\r\n3. I'd love to contribute PRs and help fix these issues. Would that be desired?\r\n\r\nEdit: This has also been noticed here: https://github.com/tensorflow/tensorflow/issues/26007", "comments": ["@danaugrs - Adding @martinwicke and @fchollet -- and we'd love to have help resolving the issues you detailed in the API documentation. `tf.keras.losses.Hinge` exists in TF 2.0, for example; but it's listed as [`tf.losses.Hinge`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/losses/Hinge); and `hinge` is listed under `tf.keras.losses`, but doesn't exist in TF 2.0.\r\n\r\nPS: thanks for the great work on [Huskarl](https://github.com/danaugrs/huskarl)! :slightly_smiling_face: ", "It looks from the description like this is regarding 1.11 (`TF Version: b'v1.11.0-rc2-4-gc19e29306c' 1.11.0`)? \r\n\r\nThis is expected, this was fixed in later versions (it wasn't ready back in late Summer 2018 when 1.11 was cut). I'll close this.\r\n\r\n@pavithrasv FYI", "@martinwicke Thanks, I must have gotten my virtual envs confused - apologies. I updated in any case and the docs match what I see. However, with the latest version (`tf.__version__ == '2.0.0-dev20190214'`) executing `pip show tensorflow` still shows `Version: 1.11.0`. This is preventing my library from requiring TF 2.0 explicitly. Is there a reason the PyPI version hasn't been updated?\r\n\r\nAs a heads up, `tf.GIT_VERSION`, `tf.VERSION` don't exist in the latest version, so the \"Bug/Performance Issue\" template instructions don't work in this case.\r\n\r\nAlso, would someone be able to answer questions 2 and 2a ?\r\n\r\n> 2. Why are there both classes and functions for many of the same loss types? That seems like unnecessary duplication.\r\n>     2a. Why is there a `Huber` class but no `huber` function?", "That's because the preview is a different package (tf-nightly-2.0-preview).\nSo technically you haven't upgraded the TensorFlow package as far as pip is\nconcerned.\n", "=> 2. The functions compute per sample loss value. The classes call the functions in them internally and also apply weights and reduce the loss value to a scalar. \r\n=> There is a `huber` function. It is called `huber_loss`. May be the the `_loss` suffix in the name can be removed.", "@pavithrasv [`huber_loss`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/losses.py#L813) exists but as you can see it's not exported and therefore not accessible. `logloss` is also not exported but IMHO should be. I made a PR to address these issues: https://github.com/tensorflow/tensorflow/pull/26034\r\n\r\nI also strongly agree that `huber_loss` should be renamed to `huber` for consistency. I think the only loss which requires 'loss' in the name is `logloss`", "Oh yes, we can definitely export those, thank you for the PR!"]}, {"number": 26011, "title": "tf.data.experimental.bucket_by_sequence_length", "body": "I wanted to read speech data from tfrecord and use tf.data.experimental.bucket_by_sequence_length to bucketing data by seq_len, but i have same error when use it. here is my code:\r\n`\r\n\r\n\r\n    fea_conf = dict([\r\n    ('SOS_ID', 2),\r\n    ('EOS_ID', 3),\r\n    ('BATCH_SIZE', 2),\r\n    ('FRAME_SIZE', 80),\r\n    ('GRAPHEME_TARGET_SEQUENCE_LENGTH', 620),])\r\n\r\n     def read_TFRecord(file_pattern, symbol):\r\n         \"\"\"read tfrecord.\"\"\"\r\n          file_pattern = os.path.join(file_pattern, '%s.tfrecords-*'%symbol)\r\n          def ParseAndProcess(record):\r\n             \"\"\"Parses a serialized tf.Example record.\"\"\"\r\n             features = [\r\n                 ('uttid', tf.VarLenFeature(tf.string)),\r\n                 ('label', tf.VarLenFeature(tf.int64)),\r\n                 ('frames', tf.VarLenFeature(tf.float32)),\r\n              ]\r\n              example = tf.parse_single_example(record, dict(features))\r\n              fval = {k: v.values for k, v in six.iteritems(example)}\r\n              fval['frames'] = tf.reshape(\r\n                  fval['frames'], shape=[-1, fea_conf['FRAME_SIZE']])\r\n              src_paddings = tf.ones([tf.shape(fval['frames'])[0]], dtype=tf.float32)\r\n\r\n              fval['label'] = tf.reshape(fval['label'], shape=[1, -1])\r\n              tgt_labels = tf.concat(\r\n                  [tf.cast(fval['label'], tf.int32),\r\n              tf.fill([1,fea_conf['GRAPHEME_TARGET_SEQUENCE_LENGTH']-tf.shape(fval['label'])[-1]], \r\n                  fea_conf['EOS_ID'])],\r\n                  axis=1\r\n              )\r\n              tgt_ids = tf.concat(\r\n                  [tf.fill([1,1], fea_conf['SOS_ID']),\r\n              tf.slice(tgt_labels, [0,0], [1, fea_conf['GRAPHEME_TARGET_SEQUENCE_LENGTH']-1])],\r\n                  axis=1\r\n              )\r\n              tgt_paddings = tf.concat(\r\n                  [tf.zeros([1, tf.shape(fval['label'])[-1]+1], dtype=tf.float32),\r\n              tf.ones([1, fea_conf['GRAPHEME_TARGET_SEQUENCE_LENGTH']-tf.shape(fval['label'])[-1]-1], \r\n                  dtype=tf.float32)],\r\n                  axis=1\r\n              )\r\n              return (fval['uttid'], tgt_ids, tgt_labels, tgt_paddings, fval['frames'], src_paddings)\r\n          def element_length_fn(example): #(uttids, tgt_ids, tgt_labels, tgt_paddings, frames, src_paddings):\r\n              print(type(example))\r\n              batch_size = tf.shape(example)[0]\r\n              return feature_length\r\n\r\n           dataset_factory = tf.data.TFRecordDataset\r\n           dataset = (\r\n                  tf.data.Dataset.list_files(\r\n                        file_pattern.lstrip('tfrecord:'), shuffle=True).apply(\r\n                             tf.data.experimental.parallel_interleave(\r\n                                  dataset_factory,\r\n                                  cycle_length=1,\r\n                                  sloppy=True)))\r\n            dataset = dataset.apply(\r\n                  tf.data.experimental.bucket_by_sequence_length(\r\n                        element_length_func=element_length_fn,\r\n                               bucket_batch_sizes= [256, 128, 64],\r\n                               bucket_boundaries=[100, 300],\r\n                               #padded_shapes=([None], [1, None], [1, None], [1, None], [None, 80], [None]),\r\n           ))\r\n           #dataset = dataset.shuffle(500)\r\n           dataset = dataset.map(\r\n                  ParseAndProcess, num_parallel_calls=1)\r\n           #dataset = dataset.padded_batch(\r\n           #   fea_conf['BATCH_SIZE'], padded_shapes=([None], [1, None], [1, None], [1, None], [None, 80], [None]))\r\n           dataset = dataset.repeat()\r\n           iterator = dataset.make_one_shot_iterator()\r\n           input_batch = iterator.get_next()\r\n\r\n           return input_batch[0], input_batch[1], input_batch[2], input_batch[3], input_batch[4], input_batch[5]\r\n`\r\nwhen i run my code, the error is occur:   ValueError: slice index 0 of dimension 0 out of bounds. for 'strided_slice' (op: 'StridedSlice') with input shapes: [0], 1, 1, 1 and with computed input tensors: input1 = <0>, input[2] = <1>, input[3] = <1>, that i don't konw how to use tf.data.experimental.bucket_by_sequence_length with tfrecordes to buckets.  i just know that code[https://stackoverflow.com/questions/50606178/tensorflow-tf-data-dataset-and-bucketing](url)\uff1b\r\ncan anybody tell me how to use it or give me a sample to handle variable length data. \r\n", "comments": ["i have modified def element_length_fn  to:\r\n`\r\ndef element_length_fn(example):\r\n    return tf.shape(example)\r\n`\r\nbut get this error: Dimensions must be equal, but are 3 and 0 for 'LessEqual' (op: 'LessEqual') with input shapes: [3], [0].   i think this's my reason of error, but i don't know how to modify it.", "Ryan added this transformation, so I'll let him comment on the usage question.", "`element_length_func` should be a function from an element in the `Dataset` to a scalar `int32` (i.e. a `Tensor` of shape `()` and type `tf.int32`), which is the length of the element. This determines which bucket the example will be routed to (the buckets are specified by `bucket_boundaries`). Then examples in each bucket will be batched together with batch sizes specified with `bucket_batch_sizes`.", "> i have modified def element_length_fn to:\r\n> `def element_length_fn(example): return tf.shape(example)`\r\n> but get this error: Dimensions must be equal, but are 3 and 0 for 'LessEqual' (op: 'LessEqual') with input shapes: [3], [0]. i think this's my reason of error, but i don't know how to modify it.\r\n\r\nUnpack the example? ", "You may want `tf.size(example)` which returns a scalar. `tf.shape` returns a vector of the sizes of each dimension."]}, {"number": 26010, "title": "Added the commutative case for prelu", "body": "This is one of the TODO items", "comments": ["closing this PR since similar is already under review"]}, {"number": 26009, "title": "Replace special quotation marks from generated java class Ops.java to prevent encoding errors when compiling for windows.", "body": "When compiling the tensorflow java api for windows, I experienced the encoding errors shown below. This seems to be caused by the annotation processor using the system's default encoding but for the compilation UTF-8 is assumed.\r\n\r\nI found that only the special quotation marks used the javadoc generated for the Ops.java class cause this issue. They seem to be the only non-ascii characters. Therefore replacing them with standard escaped quotation marks fixes this issue.\r\n\r\nThe error message received when compiling on Windows 10 64 bit & Oracle JDK 1.8u201.\r\n```\r\nbazel --output_base=C:\\tools\\bazel\\output build --java_toolchain=@bazel_tools//tools/jdk:toolchain_hostjdk8 --config=opt //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\nd:\\development\\arconsis\\dekra\\dekra-pixelation\\tensorflow/.bazelrc\r\nWARNING: Option 'experimental_shortened_obj_file_path' is deprecated\r\nINFO: Invocation ID: 7fe5ce06-9aec-4c38-baa3-f59aa598d508\r\nWARNING: D:/development/arconsis/dekra/dekra-pixelation/tensorflow/tensorflow/java/BUILD:73:1: in genrule rule //tensorflow/java:java_op_gen_sources: target '//tensorflow/java:java_op_gen_sources' depends on deprecated target '@local_jdk//:jar': Don't depend on targets in the JDK workspace; use @bazel_tools//tools/jdk:current_java_runtime instead (see https://github.com/bazelbuild/bazel/issues/5594)\r\nINFO: Analysed 2 targets (0 packages loaded, 0 targets configured).\r\nINFO: Found 2 targets...\r\nERROR: D:/development/arconsis/dekra/dekra-pixelation/tensorflow/tensorflow/java/BUILD:20:1: Building tensorflow/java/libtensorflow.jar (27 source files, 1 source jar) and running annotation processors (OperatorProcessor) failed (Exit 1): java.exe failed: error executing command\r\n  cd C:/tools/bazel/output/execroot/org_tensorflow\r\n  SET LC_CTYPE=en_US.UTF-8\r\n    SET PATH=C:\\tools\\msys2-64\\usr\\bin;C:\\tools\\msys2-64\\bin\r\n    SET PYTHON_BIN_PATH=D:/programs/python3.5/python3.exe\r\n    SET PYTHON_LIB_PATH=D:/programs/python3.5/lib/site-packages\r\n    SET TF_DOWNLOAD_CLANG=0\r\n    SET TF_NEED_CUDA=0\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n  external/local_jdk/bin/java.exe -Xverify:none -Xbootclasspath/p:external/bazel_tools/third_party/java/jdk/langtools/javac-9+181-r4173-1.jar -jar external/bazel_tools/tools/jdk/JavaBuilder_deploy.jar @bazel-out/x64_windows-opt/bin/tensorflow/java/libtensorflow.jar-0.params\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nbazel-out\\x64_windows-opt\\bin\\tensorflow\\java\\_javac\\tensorflow\\libtensorflow_sourcegenfiles\\org\\tensorflow\\op\\Ops.java:782: error: unmappable character (0x93) for encoding UTF-8\r\n *   ops.withName(\u00b4\u2510\u00a2foo\u00b4\u2510\u00a2).constant(5); // name \u00b4\u2510\u00a2foo\u00b4\u2510\u00a2\r\n                  ^\r\nbazel-out\\x64_windows-opt\\bin\\tensorflow\\java\\_javac\\tensorflow\\libtensorflow_sourcegenfiles\\org\\tensorflow\\op\\Ops.java:782: error: unmappable character (0x94) for encoding UTF-8\r\n *   ops.withName(\u00b4\u2510\u00a2foo\u00b4\u2510\u00a2).constant(5); // name \u00b4\u2510\u00a2foo\u00b4\u2510\u00a2\r\n                      ^\r\nbazel-out\\x64_windows-opt\\bin\\tensorflow\\java\\_javac\\tensorflow\\libtensorflow_sourcegenfiles\\org\\tensorflow\\op\\Ops.java:782: error: unmappable character (0x93) for encoding UTF-8\r\n *   ops.withName(\u00b4\u2510\u00a2foo\u00b4\u2510\u00a2).constant(5); // name \u00b4\u2510\u00a2foo\u00b4\u2510\u00a2\r\n                                              ^\r\nbazel-out\\x64_windows-opt\\bin\\tensorflow\\java\\_javac\\tensorflow\\libtensorflow_sourcegenfiles\\org\\tensorflow\\op\\Ops.java:782: error: unmappable character (0x94) for encoding UTF-8\r\n *   ops.withName(\u00b4\u2510\u00a2foo\u00b4\u2510\u00a2).constant(5); // name \u00b4\u2510\u00a2foo\u00b4\u2510\u00a2\r\n                                                  ^\r\nbazel-out\\x64_windows-opt\\bin\\tensorflow\\java\\_javac\\tensorflow\\libtensorflow_sourcegenfiles\\org\\tensorflow\\op\\Ops.java:784: error: unmappable character (0x93) for encoding UTF-8\r\n *   Ops sub = ops.withSubScope(\u00b4\u2510\u00a2sub\u00b4\u2510\u00a2);\r\n                                ^\r\nbazel-out\\x64_windows-opt\\bin\\tensorflow\\java\\_javac\\tensorflow\\libtensorflow_sourcegenfiles\\org\\tensorflow\\op\\Ops.java:784: error: unmappable character (0x94) for encoding UTF-8\r\n *   Ops sub = ops.withSubScope(\u00b4\u2510\u00a2sub\u00b4\u2510\u00a2);\r\n                                    ^\r\nbazel-out\\x64_windows-opt\\bin\\tensorflow\\java\\_javac\\tensorflow\\libtensorflow_sourcegenfiles\\org\\tensorflow\\op\\Ops.java:785: error: unmappable character (0x93) for encoding UTF-8\r\n *   sub.withName(\u00b4\u2510\u00a2bar\u00b4\u2510\u00a2).constant(4); // \u00b4\u2510\u00a2sub/bar\u00b4\u2510\u00a2\r\n                  ^\r\nbazel-out\\x64_windows-opt\\bin\\tensorflow\\java\\_javac\\tensorflow\\libtensorflow_sourcegenfiles\\org\\tensorflow\\op\\Ops.java:785: error: unmappable character (0x94) for encoding UTF-8\r\n *   sub.withName(\u00b4\u2510\u00a2bar\u00b4\u2510\u00a2).constant(4); // \u00b4\u2510\u00a2sub/bar\u00b4\u2510\u00a2\r\n                      ^\r\nbazel-out\\x64_windows-opt\\bin\\tensorflow\\java\\_javac\\tensorflow\\libtensorflow_sourcegenfiles\\org\\tensorflow\\op\\Ops.java:785: error: unmappable character (0x93) for encoding UTF-8\r\n *   sub.withName(\u00b4\u2510\u00a2bar\u00b4\u2510\u00a2).constant(4); // \u00b4\u2510\u00a2sub/bar\u00b4\u2510\u00a2\r\n                                         ^\r\nbazel-out\\x64_windows-opt\\bin\\tensorflow\\java\\_javac\\tensorflow\\libtensorflow_sourcegenfiles\\org\\tensorflow\\op\\Ops.java:785: error: unmappable character (0x94) for encoding UTF-8\r\n *   sub.withName(\u00b4\u2510\u00a2bar\u00b4\u2510\u00a2).constant(4); // \u00b4\u2510\u00a2sub/bar\u00b4\u2510\u00a2\r\n                                                 ^\r\nINFO: Elapsed time: 12,228s, Critical Path: 2,49s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": []}, {"number": 26008, "title": "Lite: Softmax Op Refactored", "body": "1:> Code refactored to become more compact and reusable\r\n2:> Error message rectified", "comments": ["@nutsiepully : Please help conclude this PR, it is been long pending, TIA!", "@nutsiepully : Please help review the PR, TIA!", "@nutsiepully : please conclude this PR, TIA!", "@nutsiepully , @jianlijianli : Gentle Reminder!"]}]