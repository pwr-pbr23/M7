[{"number": 30365, "title": "Update gpu.Dockerfile", "body": "added missing /usr/local/cuda/lib64 in LD_LIBRARY_PATH\r\n\r\nfixes library not found errors in images based off tensorflow", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30365) for more info**.\n\n<!-- need_sender_cla -->", "I signed it", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30365) for more info**.\n\n<!-- ok -->", "@angersson updated partials"]}, {"number": 30364, "title": "[ROCm] Addding ROCm support for pooling ops...part two", "body": "This PR adds (the remaining) ROCm support for the pooling ops.\r\n\r\nNotes\r\n* The template for `AsDeviceMemory` is removed since it gets picked up via the inclusion of the `gpu_utils.h` header file.\r\n* The ROCm implementation needs to take a different path than the CUDA due to the need of scratch memory in the ROCm implementation.\r\n\r\nPlease review and merge.\r\n\r\n--------------------------------------------------\r\n\r\n@tatianashp @whchung @chsigg \r\n\r\n", "comments": []}, {"number": 30363, "title": "[XLA:GPU][ROCm] Introduce ROCm-Device-Libs into TensorFlow ROCm build", "body": "ROCm-Device-Libs is used by XLA on ROCm for various device intrinsics.", "comments": ["@gunan Can you take care of this PR? I'm not too familiar with this part of the codebase.", "A gentle ping. Failures on `Windows Bazel` and `Windows Bazel GPU` should have nothing to do with this PR. For `Linux GPU` and `Ubuntu Makefile` it seems the test machine isn't configured with ROCm.", "@rthadur could you help advise the next steps to get this PR in?", "will need to address conflicts introduced by the recent commit made by @gunan .", "I will do the cleanup and merge."]}, {"number": 30362, "title": "Added support for Leon processor to TFL micro (includes support for big endian systems)", "body": "I've been working on implementing ML models on the Leon 3 processor which is a space qualified radiation hardened processor. This pull request includes everything necessary for TFL micro to support this new platform.\r\n\r\nThere are new 3rd party downloads, built target and test target scripts so that projects can be built for the new 'leon' target. This includes the installation and use of TSim, an emulator of the Leon processor family.\r\n\r\nThe Leon processor family is based upon the older SparcV8 architecture which is big endian. This has required a few minor updates to the TFL micro code so that the correct values are read from the little endian flat buffers used to store TFL models.\r\n\r\nFinally the reading of a non-aligned 64 bit int which is not supported on this architectures needed to be converted to a memcpy to avoid crashing on this target.\r\n\r\nThe included micro_speech example is now building and executing and all tests are passing.\r\n\r\nI look forward to any feedback and getting this update included in the main branch.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30362) for more info**.\n\n<!-- need_author_cla -->", "Added my surrey.ac.uk email address to my google account. Don't know if this is going to update the CLA but It's worth a shot", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30362) for more info**.\n\n<!-- ok -->", "I've addressed the two comment from your review now. I also found a bug in the code which automatically downloaded the leon toolchain, this didn't show up on my dev system because the tools were already installed. I've modified the download_and_extract.sh bash script so it now supports tar.xz files and will also display an error if an unsupported archive type is downloaded, as opposed to silently failing.\r\n\r\nIf these latest updates are okay then this is good to merge as far as I can tell.", "@PeteBlackerThe3rd can you please check build failures ?", "> @PeteBlackerThe3rd can you please check build failures ?\r\n\r\nOn it. Looks like I added a dependency into tensorflow/lite/kernels/kernel_utils.h source file without updating the bazel build file. My mistake, I only tested the TF lite micro builds not the full TF build.\r\n\r\nCurrently building a (hopefully) fixed version now.", "I've added the missing dependency now and successfully built a local linux CPU version from this fork. Should be good to go this time."]}, {"number": 30361, "title": "Error when computing Jacobian in eager mode", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux version 3.10.0-957.10.1.el7.x86_64 (mockbuild@x86-040.build.eng.bos.redhat.com) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-36) (GCC) )\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary pip install\r\n- TensorFlow version (use command below): v1.13.1-0-g6612da8951 1.13.1\r\n- Python version: 3.6.4\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A (CPU)\r\n- GPU model and memory: N/A (CPU)\r\n\r\n**Describe the current behavior**\r\nError occurs when trying to compute the Jacobian in the following code\r\n\r\n**Describe the expected behavior**\r\n<tf.Tensor: id=XXX, shape=(2, 1), dtype=float32, numpy=array([[1.], [1.]], dtype=float32)>\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\n\r\nwith tf.GradientTape() as g:\r\n    x  = tf.constant([1.0])\r\n    g.watch(x)\r\n    y = tf.concat([x,x], axis=0)\r\n\r\ng.jacobian(y, x)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nAdditional info: the expected behavior occurs when creating the `GradientTape` with persistent=True and replacing the last line of code with `g.jacobian(y, x, experimental_use_pfor=False)`. However, that workaround is extremely slow with real data.\r\n\r\nLog of the code that reproduces the issue:\r\n```python\r\nAttributeError Traceback (most recent call last)\r\nin ()\r\n9 # y = tf.concat([y,y], axis=0)\r\n10\r\n---> 11 g.jacobian(y, x)\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in jacobian(self, target, sources, unconnected_gradients, parallel_iterations, experimental_use_pfor)\r\n1021 try:\r\n1022 output = pfor_ops.pfor(loop_fn, target_size,\r\n-> 1023 parallel_iterations=parallel_iterations)\r\n1024 except ValueError as err:\r\n1025 six.reraise(\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py in pfor(loop_fn, iters, parallel_iterations)\r\n149 if context.executing_eagerly():\r\n150 f = function.defun(f)\r\n--> 151 return f()\r\n152\r\n153\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/eager/function.py in call(self, *args, **kwargs)\r\n862 def call(self, *args, **kwargs):\r\n863 \"\"\"Calls a graph function specialized to the inputs.\"\"\"\r\n--> 864 graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n865 return graph_function._filtered_call(args, kwargs) # pylint: disable=protected-access\r\n866\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n1174 self._input_signature,\r\n1175 autograph=self._autograph,\r\n-> 1176 arg_names=arg_names),\r\n1177 self._function_attributes)\r\n1178 if self._input_signature:\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, add_control_dependencies, arg_names, op_return_value)\r\n446 tf_decorator.rewrap(python_func, original_func, converted_func)\r\n447\r\n--> 448 func_outputs = python_func(*func_args, **func_kwargs)\r\n449\r\n450 # invariant: func_outputs contains only Tensors, IndexedSlices,\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py in f()\r\n146 \"\"\"\r\n147 def f():\r\n--> 148 return _pfor_impl(loop_fn, iters, parallel_iterations=parallel_iterations)\r\n149 if context.executing_eagerly():\r\n150 f = function.defun(f)\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py in _pfor_impl(loop_fn, iters, parallel_iterations)\r\n157 with ops.name_scope(\"loop_body\"):\r\n158 loop_var = array_ops.placeholder(dtypes.int32, shape=[])\r\n--> 159 loop_fn_outputs = loop_fn(loop_var)\r\n160 new_ops = set(ops.get_default_graph().get_operations()) - existing_ops\r\n161 iters = ops.convert_to_tensor(iters)\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in loop_fn(i)\r\n1011 self._pop_tape()\r\n1012 return self.gradient(y, flat_sources,\r\n-> 1013 unconnected_gradients=unconnected_gradients)\r\n1014\r\n1015 try:\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)\r\n944 flat_sources,\r\n945 output_gradients=output_gradients,\r\n--> 946 unconnected_gradients=unconnected_gradients)\r\n947\r\n948 if not self._persistent:\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, unconnected_gradients)\r\n70 sources,\r\n71 output_gradients,\r\n---> 72 compat.as_str(unconnected_gradients.value))\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)\r\n129 return [None] * num_inputs\r\n130\r\n--> 131 return grad_fn(mock_op, *out_grads)\r\n132\r\n133\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py in _ConcatGradV2(op, grad)\r\n220 def _ConcatGradV2(op, grad):\r\n221 return _ConcatGradHelper(\r\n--> 222 op, grad, start_value_index=0, end_value_index=-1, dim_index=-1)\r\n223\r\n224\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py in _ConcatGradHelper(op, grad, start_value_index, end_value_index, dim_index)\r\n115 out_grads = array_ops.split(grad, sizes, non_neg_concat_dim)\r\n116 else:\r\n--> 117 if constant_op.is_constant(concat_dim):\r\n118 # If concat_dim is a constant defined in a different context,\r\n119 # then we duplicate it in the current context to avoid passing it\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in is_constant(tensor_or_op)\r\n293 def is_constant(tensor_or_op):\r\n294 if isinstance(tensor_or_op, ops.Tensor):\r\n--> 295 op = tensor_or_op.op\r\n296 else:\r\n297 op = tensor_or_op\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in op(self)\r\n925 def op(self):\r\n926 raise AttributeError(\r\n--> 927 \"Tensor.op is meaningless when eager execution is enabled.\")\r\n928\r\n929 @Property\r\n\r\nAttributeError: Tensor.op is meaningless when eager execution is enabled.\r\n```", "comments": ["@sarshalom I am able to reproduce the issue with Tensorflow version 1.13.1 on Colab. ", "Apologies for the delay in response. Looks like this is fixed in TF 1.14.0\r\nI get following output with eager execution enabled:\r\n```python\r\n<tf.Tensor: id=117, shape=(2, 1), dtype=float32, numpy=\r\narray([[1.],\r\n       [1.]], dtype=float32)>\r\n```", "Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30361\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30361\">No</a>\n"]}, {"number": 30360, "title": "Error when computing jacobian in eager mode", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux version 3.10.0-957.10.1.el7.x86_64 (mockbuild@x86-040.build.eng.bos.redhat.com) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-36) (GCC) )\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary pip install\r\n- TensorFlow version (use command below): v1.13.1-0-g6612da8951 1.13.1\r\n- Python version: 3.6.4\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A (CPU)\r\n- GPU model and memory: N/A (CPU)\r\n\r\n**Describe the current behavior**\r\nError occurs when trying to compute the Jacobian in the following code\r\n\r\n**Describe the expected behavior**\r\n<tf.Tensor: id=XXX, shape=(2, 1), dtype=float32, numpy=array([[1.], [1.]], dtype=float32)>\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\n\r\nwith tf.GradientTape() as g:\r\n    x  = tf.constant([1.0])\r\n    g.watch(x)\r\n    y = tf.concat([x,x], axis=0)\r\n\r\ng.jacobian(y, x)\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nAdditional info: the expected behavior occurs when creating the `GradientTape` with `persistent=True` and replacing the last line of code with `g.jacobian(y, x, experimental_use_pfor=False)`. However, that workaround is extremely slow with real data.\r\n\r\nLog of the code that reproduces the issue:\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-9-16331645f7bb> in <module>()\r\n      9 #     y = tf.concat([y,y], axis=0)\r\n     10 \r\n---> 11 g.jacobian(y, x)\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in jacobian(self, target, sources, unconnected_gradients, parallel_iterations, experimental_use_pfor)\r\n   1021       try:\r\n   1022         output = pfor_ops.pfor(loop_fn, target_size,\r\n-> 1023                                parallel_iterations=parallel_iterations)\r\n   1024       except ValueError as err:\r\n   1025         six.reraise(\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py in pfor(loop_fn, iters, parallel_iterations)\r\n    149   if context.executing_eagerly():\r\n    150     f = function.defun(f)\r\n--> 151   return f()\r\n    152 \r\n    153 \r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n    862   def __call__(self, *args, **kwargs):\r\n    863     \"\"\"Calls a graph function specialized to the inputs.\"\"\"\r\n--> 864     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n    865     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n    866 \r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   1174                 self._input_signature,\r\n   1175                 autograph=self._autograph,\r\n-> 1176                 arg_names=arg_names),\r\n   1177             self._function_attributes)\r\n   1178         if self._input_signature:\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, add_control_dependencies, arg_names, op_return_value)\r\n    446         tf_decorator.rewrap(python_func, original_func, converted_func)\r\n    447 \r\n--> 448       func_outputs = python_func(*func_args, **func_kwargs)\r\n    449 \r\n    450       # invariant: `func_outputs` contains only Tensors, IndexedSlices,\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py in f()\r\n    146   \"\"\"\r\n    147   def f():\r\n--> 148     return _pfor_impl(loop_fn, iters, parallel_iterations=parallel_iterations)\r\n    149   if context.executing_eagerly():\r\n    150     f = function.defun(f)\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py in _pfor_impl(loop_fn, iters, parallel_iterations)\r\n    157   with ops.name_scope(\"loop_body\"):\r\n    158     loop_var = array_ops.placeholder(dtypes.int32, shape=[])\r\n--> 159     loop_fn_outputs = loop_fn(loop_var)\r\n    160   new_ops = set(ops.get_default_graph().get_operations()) - existing_ops\r\n    161   iters = ops.convert_to_tensor(iters)\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in loop_fn(i)\r\n   1011       self._pop_tape()\r\n   1012       return self.gradient(y, flat_sources,\r\n-> 1013                            unconnected_gradients=unconnected_gradients)\r\n   1014 \r\n   1015     try:\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)\r\n    944         flat_sources,\r\n    945         output_gradients=output_gradients,\r\n--> 946         unconnected_gradients=unconnected_gradients)\r\n    947 \r\n    948     if not self._persistent:\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, unconnected_gradients)\r\n     70       sources,\r\n     71       output_gradients,\r\n---> 72       compat.as_str(unconnected_gradients.value))\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)\r\n    129     return [None] * num_inputs\r\n    130 \r\n--> 131   return grad_fn(mock_op, *out_grads)\r\n    132 \r\n    133 \r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py in _ConcatGradV2(op, grad)\r\n    220 def _ConcatGradV2(op, grad):\r\n    221   return _ConcatGradHelper(\r\n--> 222       op, grad, start_value_index=0, end_value_index=-1, dim_index=-1)\r\n    223 \r\n    224 \r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py in _ConcatGradHelper(op, grad, start_value_index, end_value_index, dim_index)\r\n    115       out_grads = array_ops.split(grad, sizes, non_neg_concat_dim)\r\n    116     else:\r\n--> 117       if constant_op.is_constant(concat_dim):\r\n    118         # If concat_dim is a constant defined in a different context,\r\n    119         # then we duplicate it in the current context to avoid passing it\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in is_constant(tensor_or_op)\r\n    293 def is_constant(tensor_or_op):\r\n    294   if isinstance(tensor_or_op, ops.Tensor):\r\n--> 295     op = tensor_or_op.op\r\n    296   else:\r\n    297     op = tensor_or_op\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in op(self)\r\n    925   def op(self):\r\n    926     raise AttributeError(\r\n--> 927         \"Tensor.op is meaningless when eager execution is enabled.\")\r\n    928 \r\n    929   @property\r\n\r\nAttributeError: Tensor.op is meaningless when eager execution is enabled.\r\n", "comments": []}, {"number": 30359, "title": "TensorFlow C binding for Raspberry Pi", "body": "**System information**\r\n- TensorFlow version (you are using): 1.13.1\r\n- Are you willing to contribute it (Yes/No): yes, at least as a tester\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI asked on [StackOverflow if there was a TensorFlow C binding for ARM/RaspberryPi](https://stackoverflow.com/questions/56837317/how-can-i-get-a-tensorflow-c-binding-for-raspberry-pi) but I got no answer after many days. \r\n\r\nI couldn't find anything on the website, so I suspect this is not yet available? \r\n\r\nIf it is just a matter that you don't distribute the binaries, I have no troubles to compile it myself if I have the instructions and if it is known to work. But I haven't found anything confirming that either. \r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nAll language bindings that use the C library and want to run on any ARM board\r\n\r\n**Any Other info.**\r\n", "comments": ["The last paragraph of the \"[TensorFlow for C](https://www.tensorflow.org/install/lang_c)\" you cited points to a [how-to](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/lib_package/README.md) file which does show you how to build TensorFlow C API from source code with `bazel`", "Thanks @freedomtan. I didn't know if something special compilation was needed for ARM. Are you aware of people/projects compiling it and using successfully?", "@marianopeck something like\r\n\r\n```\r\nCC=clang-6.0 CXX=clang++-6.0 \\\r\nbazel build --config opt --local_resources 1024.0,0.5,0.5 \\\r\n--copt=-mfpu=neon-vfpv4 \\\r\n--copt=-ftree-vectorize \\\r\n--copt=-funsafe-math-optimizations \\\r\n--copt=-ftree-loop-vectorize \\\r\n--copt=-fomit-frame-pointer \\\r\n--copt=-DRASPBERRY_PI \\\r\n--host_copt=-mfpu=neon-vfpv4 \\\r\n--host_copt=-ftree-vectorize \\\r\n--host_copt=-funsafe-math-optimizations \\\r\n--host_copt=-ftree-loop-vectorize \\\r\n--host_copt=-fomit-frame-pointer \\\r\n--host_copt=-DRASPBERRY_PI \\\r\n//tensorflow/tools/lib_package:libtensorflow\r\n```\r\nworks for me.", "@marianopeck Just to verify did you get a chance to follow instructions as suggested by freedomtan .Thanks!", "Hi @ravikyram \r\nNot yet. I will see if I can try it this week. \r\nBTW, if there is a compilation instruction and people claim it works, any reason why the binaries are not officially distributed as the rest of the  \"TensorFlow for C\"?", "Hi @freedomtan \r\nWhich version did you use? It's 2.0? Or older too? I would like to try with 1.13.1 or similar. \r\nThanks", "@marianopeck I use master branch most of the time. I think it works on TensorFlow versions before 1.10 to current master.", "Hi @freedomtan \r\nThanks, I am trying now to compile it myself but I wasn't even able to compile `bazel` on the first place :(    I am getting this issue: https://github.com/bazelbuild/bazel/issues/8882\r\nIf I am able to pass that, I will continue..", "@marianopeck I haven't tried newer bazel, general comment is to increase your swap space and maximum Java heap size (e.g., `-J-Xmx1024m` instead of `-J-Xmx512m`)", "Hi, as I found no binary available, I had to manage myself.\r\nI finally took the time to [write down everything I found during my attempt to get TensorFlow C library compiled for Raspberry Pi](https://dev.to/martinezpeck/challenge-accepted-build-tensorflow-c-binding-for-raspberry-pi-in-2019-4f89). In that tutorial, I point back to this issue in case any progress is made. \r\nThank you", "good to see another effort on Smalltalk + TensorFlow. Smalltalk is one of my favorite language. I tried [Pharo bindings for TensorFlow](https://github.com/PolyMathOrg/libtensorflow-pharo-bindings) before and was able to use both TensorFlow C API and TensorFlow Lite C API on x86 machines", "Hi @freedomtan \r\nWe are in the same boat then :)  That's great to hear. The Pharo bindings are actually a port made by @SergeStinckwich  from the [work done on Cuis Smalltalk](https://github.com/Cuis-Smalltalk/Machine-Learning). The latter was mostly done by @gerasdf and he is now doing most of the [port to VASmalltalk](https://github.com/vasmalltalk/tensorflow-vast).\r\nI am interested in hearing about your experiments. To avoid making noise in this issue, I will send you an email.\r\nThanks!\r\n", "hi @marianopeck, @freedomtan already have done some contributions to the Pharo port.\r\nYes all on the same boat, let's try to go in the same direction :-) Let's discuss about that in ESUG 2019 ?", "OK, I sent you both and the rest of the involved parts a private email to so that we can continue discussing :)", "Although I do not have C language implementation skills, I tried to generate a binary for RaspberryPi of Tensorflow v1.14.0. I have not confirmed the operation.\r\nhttps://github.com/PINTO0309/Tensorflow-bin.git\r\nhttps://github.com/PINTO0309/Tensorflow-bin/tree/master/C-library/1.14.0-armv7l", "That worked perfectly!! ", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30359\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30359\">No</a>\n"]}, {"number": 30358, "title": "[ROCm] Adding ROCm support for batch_to_space op", "body": "This PR (re) adds ROCm support for batch_to_space op\r\n\r\nThe changes in this PR are trivial, please review and merge.\r\n\r\nNote, ROCm support for this op was previously enabled and then disabled because of dependency issues. \r\n\r\n-------------------------------------------------\r\n\r\n@tatianashp @whchung @chsigg \r\n", "comments": []}, {"number": 30357, "title": "[TF 1.14 Keras] probably bug in network._map_graph_network", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Both OS-X and Ubuntu\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): 1.14.0 binary\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: 3.6.\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\nI have a CNN model that works well with tensorflow==1.13.1 (building the network, training the network and predicting from the network), but after the update to tensorflow==1.14.0 I'm getting following exception:\r\n\r\n```\r\nValueError: Graph disconnected: cannot obtain value for tensor Tensor(\"dropout_1/cond/Merge:0\", shape=(?, 160, 160, 16), dtype=float32) at layer \"concatenate\". The following previous layers were accessed without issue: ['net_input', 'initial_conv2D', 'batch_normalization', 'activation', 'conv2d', 'dropout']\r\n```\r\n\r\n**Other info / logs**\r\nAfter debugging and going through the implementation of `network._map_graph_network` it seems to me that there is either some bug (significantly changed behaviour between tensorflow==1.13.1 and tensorflow 1.14.0) in the graph disconnections checking.\r\nWhen I run the code with tensorflow==1.13.1 the tensorboard graph looks as follows:\r\n![Screenshot 2019-07-03 at 16 36 37](https://user-images.githubusercontent.com/1810839/60600754-134b8780-9db1-11e9-811f-7f36afdc4a77.png)\r\nFrom this I'm not sure why layer dropout_1 (conditional dropout) is checked in the concatenate layer? I assume that droupout should have been checked there?", "comments": ["Thank you for the report. Just a small clarifying question: can you also try against nightly version? I think this might have been fixed already, but I might be wrong.", "@mihaimaruseac I've tried it against tf-nightly and I'm getting exactly the same exception.", "I'll defer to the Keras team then, as I'm not sure where to go from here", "same problem here too\r\nI found a workaround by using tf.concat instead of tf.kears.layers.Concatenate", "@sedghi interesting, thank you, but unfortunately in my case workaround using `tf.concat`, at least as `Lambda` layer does not work for me. I'm still getting the same error message.", "very much related to https://github.com/tensorflow/tensorflow/issues/30355", "I'm further proceeding in debugging. It currently looks like in the case when layer is an input to more than one concatenation than following strange thing happen:\r\n\r\nMy testing graph (NOTE: tensorboard printed from tensorflow==1.13.2)\r\n![Screenshot 2019-10-11 at 12 16 34](https://user-images.githubusercontent.com/1810839/66657009-6306de00-ec3f-11e9-94c4-02dbde6f4b87.png)\r\n\r\nFor some reason concatenate layer in the red circle has correctly 2 input layer, but incorrectly 3 input tensors:\r\n![Screenshot 2019-10-11 at 14 26 07](https://user-images.githubusercontent.com/1810839/66657129-93e71300-ec3f-11e9-8dc8-17652e3ea542.png)\r\n\r\nI've noticed both of this in `_map_graph_network` and inside in `build_map` function. From this I assume that the bug is not in `network._map_graph_network` function, but somewhere in the construction of the graph.\r\n\r\nDoes anybody has some idea where should I look further? It seems to me that there might be some problem in the `keras.engine.Layer` `keras.layer.merge._Merge` or maybe somewhere else? Any suggestions @mihaimaruseac @fchollet", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30357\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30357\">No</a>\n"]}, {"number": 30356, "title": "Compiling custom GPU operation on Microsoft Windows 10", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Microsoft Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 1.12.3\r\n- Python version: 3.5.0\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.17.2\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: 9\r\n- GPU model and memory: NVIDIA Quadro P100\r\n\r\n\r\n\r\n**Describe the problem**\r\nI've been trying to get a custom operation working with a Windows version of TensorFlow GPU version from the guide: https://www.tensorflow.org/guide/extend/op#compiling_the_kernel_for_the_gpu_device\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. Clone the repository:\r\n\r\n```powershell\r\ngit clone https://github.com/tensorflow/tensorflow.git \r\n```\r\n\r\nI put my C++ implementation of sqrt_kernels.cc, sqrt.h, and sqrt_kernels.cu.cc in tensorflow\\tensorflow\\core\\user_ops\r\n\r\n\r\n**sqrt_kerenls.cc**\r\n```c++\r\n#if GOOGLE_CUDA\r\n#define EIGEN_USE_GPU\r\n#endif\r\n\r\n#include \"sqrt.h\"\r\n#include <math.h>\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n\r\nusing namespace tensorflow;\r\n\r\nusing CPUDevice = Eigen::ThreadPoolDevice;\r\nusing GPUDevice = Eigen::GpuDevice;\r\n\r\n// CPU specialization of actual computation\r\ntemplate <typename T>\r\nstruct SqrtFunctor<CPUDevice, T>{\r\n\r\n    void operator()(const CPUDevice& d, int size, const T* in, T* out){\r\n        for(int i = 0; i < size; ++i){\r\n            out[i] = sqrt(in[i]);\r\n        }\r\n    }\r\n};\r\n\r\n//Opkernel definition\r\n//template paramter T is datatype of tensors\r\n\r\ntemplate <typename Device, typename T>\r\nclass SqrtOp : public OpKernel {\r\n    public:\r\n     explicit SqrtOp(OpKernelConstruction* context) : OpKernel(context) {}\r\n\r\n     void Compute(OpKernelContext* context) override {\r\n\r\n         const Tensor& input_tensor = context->input(0);\r\n\r\n         Tensor* output_tensor = NULL;\r\n\r\n         OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(), &output_tensor));\r\n\r\n         OP_REQUIRES(context, input_tensor.NumElements() <= tensorflow:kint32max, errors::InvalidArgument(\"Too many elements in tensor\"));\r\n\r\n         SqrtFunctor<Device, T>(){\r\n             context->eigen_device<Device>(),\r\n             static_cast<int>(input_tensor.NumElements()),\r\n             input_tensor.flat<T>().data(),\r\n             output_tensor->flat<T>().data());\r\n         }\r\n     }\r\n\r\n};\r\n\r\n//Register the Kernels\r\n#define REGISTER_CPU(T)                     \\\r\n REGISTER_KERNEL_BUILDER(                   \\\r\n        Name(\"Sqrt\").Device(DEVICE_CPU).TypeConstraint<T>(\"T\"), \\\r\n        SqrtOp<CPUDevice, T>);\r\n REGISTER_CPU(float);\r\n REGISTER_CPU(int32);\r\n\r\n#ifdef GOOGLE_CUDA\r\n#define REGISTER_GPU(T)\r\n extern template struct SqrtFunctor<GPUDevice, T>;   \\\r\nREGISTER_KERNEL_BUILDER(\r\n    Name(\"Sqrt\").Device(DEVICE_GPU).TypeConstraint<T>(\"T\"), \\\r\n    SqrtOp<GPUDevice, T>);\r\nREGISTER_GPU(float);\r\nREGISTER_GPU(int32);\r\n#endif\r\n}\r\n}\r\n```\r\n\r\n**sqrt_kenels.cu.cc**\r\n```c++\r\n#if GOOGLE_CUDA\r\n\r\n#define EIGEN_USE_GPU\r\n\r\n#include \"sqrt.h\"\r\n#include \"tensorflow/core/util/cuda_kernel_helper.h\"\r\n\r\n\r\nusing namespace tensorflow;\r\n\r\nusing GPUDevice = Eigen::GpuDevice;\r\n\r\ntemplate <typename T>\r\n\r\n__global__ void SqrtCudaKernel(const int size, const T* in, T* out){\r\n\r\n    for(int i = blockIdx.x * blockDim.x + threadIdx.x; i < size; i += blockDim.x * gridDim.x){\r\n\r\n        out[i] = sqrt(ldg(in + i));\r\n    }\r\n}\r\n\r\ntemplate <typename T>\r\nstruct SqrtFunctor<GPUDevice, T> {\r\n    void operator()(const GPUDevice& d, int size, const T* in, T* out){\r\n\r\n        int block_count = 1024;\r\n        int thread_per_block = 20;\r\n\r\n        SqrtCudaKernel<T> <<<block_count, thread_per_block, 0, d.stream()>>>(size,in,out);\r\n\r\n    }\r\n};\r\n\r\ntemplate struct SqrtFunctor<GPUDevice, float>;\r\ntemplate struct SqrtFunctor<GPUDevice, int32>;\r\n}\r\n}\r\n\r\n#endif\r\n```\r\n**sqrt.h**\r\n\r\n```c++\r\n#ifndef SQRT_H_\r\n#define SQRT_H_\r\n\r\nnamespace tensorflow {\r\n\r\nnamespace functor{\r\n\r\n\r\ntemplate <typename Device, typename T>\r\n\r\nstruct SqrtFunctor{\r\n    void operator()(const Device&d, int size, const T* in, T* out);\r\n};\r\n\r\n#if GOOGLE_CUDA\r\n\r\ntemplate <typename Eigen::GpuDevice, typename T>\r\nstruct SqrtFunctor{\r\n    void operator()(const Eigen::GpuDevice & d, int size, const T* in, T* out);\r\n};\r\n#endif\r\n\r\n}\r\n\r\n}\r\n\r\n#endif\r\n```\r\n\r\n2. Afterwards I configure and built the tensorflow library by using bazel:\r\n\r\n```powershell\r\npython ./configure.py\r\n```\r\n\r\n3.  This is where I get my error after running this command:\r\n\r\n```powershell\r\nnvcc -std=c++11 -c -o cuda_op_kernel.cu.o cuda_op_kernel.cu.cc \\\r\n  ${TF_CFLAGS[@]} -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC\r\nnvcc warning : The -std=c++11 flag is not supported with the configured host compiler. Flag will be ignored.\r\nsqrt_kernels.cu.cc\r\ncl : Command line warning D9002 : ignoring unknown option '-fPIC'\r\nsqrt_kernels.cu.cc\r\nC://Users//alayu//AppData//Local//Programs//Python//Python35//lib//site-packages//tensorflow//include\\third_party/eigen3/unsupported/Eigen/CXX11/Tensor(1): fatal error C1083: Cannot open include file: 'unsupported/Eigen/CXX11/Tensor': No such file or directory\r\n```\r\n\r\nI found the file \"unsupported/Eigen/CXX11/Tensor'\" in the path that it said it couldn't find it in, so I don't know what the error could be. Does nvcc require a specific gcc version? If it requires me to downgrade, I had originally used cygwin to install the latest version and it doesn't seem like they include older versions in their installer.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Reassigning this to @gunan, as I'm not familiar with the current state of the Windows build.", "Yifei is actively working on custom op support on windows.\r\nShe may have some updates.", "@alayu,\r\n\r\nWe are checking to see if you still need help on this issue.You can checkout this detailed [guide](https://www.tensorflow.org/guide/create_op?hl=lt_Dowloand) which helps you in creating custom operation.\r\n\r\nWe recommend that you upgrade to `2.6` which is latest stable version of TF and let us know if the issue still persists in newer versions. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30356\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30356\">No</a>\n"]}, {"number": 30355, "title": "[TF 2.0 keras] tf.keras.Concatenate Graph Disconnected when concatenating non-sequentially", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Binary, pip install \r\n- TensorFlow version (use command below): tensorflow-gpu==2.0.0-beta1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n**Describe the current behavior**\r\nError arises during Concatenate when I run the following code:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.layers import Conv2D, Concatenate\r\n\r\ninputs = keras.Input(shape=(256,256,3))\r\nx  = Conv2D(16,3, padding='same',activation='relu')(inputs)\r\nx_list = [x]\r\nfor i in range(3):\r\n    x = Conv2D(16,3, padding='same',activation='relu')(x)\r\n    x_list.append(x)\r\n    x = Concatenate(3)(x_list)\r\n\r\nmodel = keras.Model(inputs=inputs, outputs=x)\r\nmodel.summary()\r\n```\r\n\r\n`ValueError: Graph disconnected: cannot obtain value for tensor Tensor(\"conv2d_31/Identity:0\", shape=(None, 256, 256, 16), dtype=float32) at layer \"concatenate_8\". The following previous layers were accessed without issue: ['input_9', 'conv2d_29', 'conv2d_30']`\r\n\r\nThis issue does not occur in a Tensorflow 1.X environment, only TF 2.0\r\n\r\n**Describe the expected behavior**\r\nNow the Concatenate function works properly when using a sequential model. That is, if I swap in \"for i in range(1):\" rather than \"for i in range(3):\" above, the code executes cleanly. However, the non-sequential repeated Concatenation in the loop leaves the  a Graph disconnected error.\r\n\r\nFurthermore, the error is also eliminated when using tf.concat, so the following code also executes cleanly.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.layers import Conv2D, Concatenate\r\n\r\ninputs = keras.Input(shape=(256,256,3))\r\nx  = Conv2D(16,3, padding='same',activation='relu')(inputs)\r\nx_list = [x]\r\nfor i in range(3):\r\n    x = Conv2D(16,3, padding='same',activation='relu')(x)\r\n    x_list.append(x)\r\n    x = tf.concat(x_list, 3)\r\n\r\nmodel = keras.Model(inputs=inputs, outputs=x)\r\nmodel.summary()\r\n```\r\n\r\nTherefore, I do have a working alternative, but there does appear to be an issue with the keras Concatenate function", "comments": ["@mketcha Just to verify, Which Concatenate function of Keras did you use.I could find tf.keras.backend.concatenate and tf.keras.layers.Concatenate on Tensorflow website. Thanks! ", "@mketcha If it is tf.keras.layers.Concatenate then Concatenate function takes at least two list and syntax will be\r\n `x = Concatenate(3)([list1,list2])` \r\nLet us know if this helps. Thanks!", "It is the tf.keras.layers.Concatenate.\r\n\r\n I don't believe it is an issue with my arguments as analagous code works in tensorflow 1.13, and the argument passes cleanly if I use \"for i in range(1):\"\r\n\r\nIt seems to be an issue with passing an argument that had previously already been used in a concatenation call", "same error here,\r\nbut the tf.concat works as a workaround as @mketcha mentioned", "@mketcha I changed one line in your code to append all the three layers to `x_list` and then applied `concatenation` and the [gist is here](https://colab.sandbox.google.com/gist/jvishnuvardhan/d6f7c83b0315f3051023e8f915cce1ba/untitled527.ipynb). With that modification, it runs without any error. \r\n\r\n## I have also plotted the model and is shown below. \r\n![image](https://user-images.githubusercontent.com/46058173/66087634-00219280-e52d-11e9-9f42-f3c9531d1c34.png)\r\n\r\n## When I plot your workaround using tf.concat(), model looks like this shown below. Note that number of training params also increases (more than double)\r\n![image](https://user-images.githubusercontent.com/46058173/66088214-306a3080-e52f-11e9-829f-99276d476d3e.png)\r\n\r\nPlease let us know what you think. If this was resolved, please close the issue. Thanks!\r\n", "@jvishnuvardhan While your workaround may run, it does not achieve the same desired architecture", "Hi, this is a regression in TF Keras. In standalone Keras, it was fixed by https://github.com/keras-team/keras/pull/6035 .\r\n\r\nAnd it seems to be a duplicate of https://github.com/tensorflow/tensorflow/issues/32023 .\r\n\r\nThe workaround in user code is to copy the list at call time, e.g. by changing above example to:\r\n```\r\n    x = Concatenate(3)(x_list[:])\r\n```\r\n\r\nThe fix for TF would be to copy the list at call time of the layer object, to prevent later outside modification.", "Thanks for the issue!\r\n\r\nThis commit fixes the issue: \r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/816ec796ea6a96940188356628566ed11a11c186\r\n\r\nThe fix should be available in tomorrow's nightly build", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30355\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30355\">No</a>\n"]}, {"number": 30354, "title": "[ROCm] Adding ROCm support for the functional ops", "body": "This PR adds ROCm support for the functional ops\r\n\r\nThe changes in this PR are trivial, please review and merge\r\n\r\n------------------------------------------------\r\n\r\n@tatianashp @whchung @chsigg ", "comments": []}, {"number": 30353, "title": "Small changes", "body": "A few small changes:\r\n- Rename a variable.\r\n- Better error message.\r\n- More complete help message.\r\n- Small doc fix.\r\n\r\n@sanjoy  @thomasjoerg ", "comments": ["> We disable the signed vs. unsigned comparison warning in google since it is too noisy. Can you do the same in OSS instead of changing adding explicit casts to unsigned integers? Otherwise it is only a matter of time before these kinds of expressions creep back in.\r\n\r\nI only fixed a few. It is true that I saw tons of those warnings that I didn't fix.\r\n\r\nI didn't had the line 'build:opt --copt=-Wno-sign-compare' in my .tf_configure.bazelrc file. It is added by default by the configure script. So the problem was in my config. I'll make sure it stay fixed.\r\n\r\nIf you think of something else to change to this PR, tell me.", "> If you think of something else to change to this PR, tell me.\r\n\r\nCan you please revert the signed int -> unsigned changes in this PR?", "I removed the commit from the history.", "There was a conflict. I rebased and force-pushed to fix it. This needed small change in the last commit.", "The CI was failing. The rebase broked stuff that I didn't catched up before. Now it should be fine to start the CI again.", "The Mac CI failed, but I can't see why. I would be surprised that this is related to my PR. What should I do?", "> The Mac CI failed, but I can't see why. I would be surprised that this is related to my PR. What should I do?\r\n\r\nI'll manually merge this."]}, {"number": 30352, "title": "tensorflow.python.framework.errors_impl.NotFoundError", "body": "I have built an LSTM model and try to run that with python flask.\r\n\r\ntensorflow.python.framework.errors_impl.NotFoundError: Container localhost does not exist. (Could not find resource: localhost/embedding/embeddings)\r\n\t [[{{node embedding/embedding_lookup}}]]\r\nhere is the image of error.\r\n\r\n![screencapture-127-0-0-1-5002-predict-2019-07-03-18_18_18](https://user-images.githubusercontent.com/26489408/60592830-0cc40c80-9dbf-11e9-82a5-48ec7c835205.png)\r\n\r\n", "comments": ["can anyone help me out with this issue?", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "operation system: Windows 10\r\nanaconda \r\ntensorflow: 1.14.0\r\n\r\nCode: \r\n[first.zip](https://github.com/tensorflow/tensorflow/files/3366777/first.zip)\r\n", "Thank you, \r\nMy code is now running and I solved the issue.\r\nPlease see the new code to solve the issue.\r\n\r\n[first.zip](https://github.com/tensorflow/tensorflow/files/3368295/first.zip)"]}, {"number": 30351, "title": "Remove deprecated .where in math_grad.py", "body": "This PR substitutes array_ops.where with array_ops.whare_v2 in math_grad.py, removing the deprecated message.", "comments": ["@vcarpani Could you please address the build failures? Thanks!", "@vcarpani Could you please resolve the conflicts? Thanks!", "> @vcarpani Could you please resolve the conflicts? Thanks!\r\n\r\nYes, I will, had no spare time in the last weeks, as soon as i can I will get back on this ;)", "`bazel test //tensorflow/python`(which was failing before) is now passing", "@vcarpani can you please check build failures ?", "Can one of the admins verify this patch?"]}, {"number": 30350, "title": "How to get model predictions on all classes after applying Transfer Learning in TF2.0?", "body": "I have been following [transfer learning with TFHub](https://www.tensorflow.org/beta/tutorials/images/hub_with_keras) to implement transfer learning in my model for text classification. In the example, I do not understand how to get probabilities for all the classes (1000 classes before + 5 classes after implementing transfer learning). At the end of the tutorial, they only show predictions on the 5 classes on which the model was re-trained, but I want to have probabilities also on the 1000 classes. I can not find any documentation or explanation on how to achieve this. If anyone has any ideas or some useful links, please feel free to share. THANK YOU!\r\nThis was my main question.\r\n\r\n**About model, where I want to implement this idea of transfer learning.**\r\nI have a text dataset of 8000 classes, since the data size is too big so it can not be loaded into the RAM of my machine. So I want to train on small-small sizes of data i.e. 4000 classes and 4000 classes. I trained my model freshly on 4000 classes first, the learned model gives me good results. Now I want to retrain the saved model on the rest 4000 classes using Transfer Learning (I am also open to any other way), but after doing a lot of google searches seems like it is impossible to retrain a model on new classes, at least in text classification. If you have read this part and have some suggestions or ideas please share them, I will be very thankful. ", "comments": ["For faster resolution please post the issue on [TFHub](https://github.com/tensorflow/hub/issues)\r\nrepository.Thanks!\r\n", "@ravikyram thank you for your suggestion, I posted the issue on [TFHub](https://github.com/tensorflow/hub/issues) and here is the [issue #328](https://github.com/tensorflow/hub/issues/328#issuecomment-508732983). Just in case someone is also looking for it.", "@rishabhsahrawat Can i close this issue . We can follow-up the issue #328 on TFHub for the resolution. Thanks!"]}, {"number": 30349, "title": "Using pre trained tflite model on android", "body": "Hello,\r\nI have been trying to use your pre trained .tflite model on android app. Its giving me the following error:\r\njava.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite tensor with type FLOAT32 and a Java object of type [I (which is compatible with the TensorFlowLite type INT32)\r\nThis is when I use the command tfLite.runForMultipleInputsOutputs(inputArray, outputScoresNames); to run the model\r\nIf I use tfLite.run(inputs, outputScores); to run the model, it executes without error but always outputs only the first character in the list.\r\nCould anyone help me with this error??\r\nThanks in advance.", "comments": ["**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n# Copy and paste here\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n", "@ramrahu Please fill the above template to expedite the trouble-shooting process. \r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n\r\n", "**System information**\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 14.04\r\nTensorFlow installed from (source or binary): Source\r\nTensorFlow version (or github SHA if from source): 1.13\r\n\r\n**Provide the text output from tflite_convert**\r\nI didn't use tflite_convert. I was just trying a pre trained tflite model to check if the app is functioning fine, but it was causing the above mentioned problem.", "Which model are you using? Can you provide an exact link? And what is the type of `inputArray` and `outputScoresNames`? ", "These are the types:\r\n**Object[] inputArray = {floatInputBuffer, sampleRateList};\r\nMap<Integer, Object> outputScoresNames = new HashMap<>();\r\n    outputScoresNames.put(0, outputScores);**\r\nI have used the pretrained model mentioned in this link:\r\nhttps://github.com/mozilla/DeepSpeech\r\nI downloaded using this command mentioned in the link: **wget https://github.com/mozilla/DeepSpeech/releases/download/v0.5.1/deepspeech-0.5.1-models.tar.gz**", "Right, but what are the types of `floatInputBuffer`, `sampleRateList` and `outputScores`?", "floatInputBuffer is float, sampleRateList is int and outputScores is float", "I believe the sampleRateList should also be a float array. You can inspect the graph inputs/ouputs using our [visualization tool](https://www.tensorflow.org/lite/guide/faq#how_do_i_inspect_a_tflite_file) or with something like [Netron](https://electronjs.org/apps/netron).", "ok thanks will have a look at it. how about the run() function I mentioned in my first question? It doesn't give any error. It gives a result but the result contains only a single character repeated multiple times.", "> I believe the sampleRateList should also be a float array. You can inspect the graph inputs/ouputs using our [visualization tool](https://www.tensorflow.org/lite/guide/faq#how_do_i_inspect_a_tflite_file) or with something like [Netron](https://electronjs.org/apps/netron).\r\n\r\nHi this suggestion solved that error. But I am getting a new error now\r\n**java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/concatenation.cc:68 t->dims->size != t0->dims->size (1 != 2)Node number 27 (CONCATENATION) failed to prepare.**\r\nAny idea how to fix this??", "What are the dimensions of your inputs? I believe the model requires 4 inputs, but I see you only provide 2 (input_node[1,16,19,26], previous_state_c[1,2048], previous_state_h[1,2048], input_samples[512]).", "The ones that you mentioned are the dimensions. I've attached my code file. Maybe you will be able to help me better in clearing the error. And apologies I forgot to mention one thing. I changed the model from the pretrained model to my model. I trained the same architecture mentioned above with my data. The latest error I posted is from my model. I've commented the lines giving me the error.\r\n`\r\npublic class SpeechActivity extends Activity {\r\n  private static final float SAMPLE_RATE = 16000;\r\n  private static final int SAMPLE_DURATION_MS = 5000;\r\n  private static final int RECORDING_LENGTH = (int) (SAMPLE_RATE * SAMPLE_DURATION_MS / 1000);\r\n  private static final String MODEL_FILENAME = \"file:///android_asset/output_graph_240hrs.tflite\";\r\n\r\n    private static final char[] map = new char[]{'0', ' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g',\r\n            'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q',\r\n            'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z','\\''};\r\n\r\n  // UI elements.\r\n  private static final int REQUEST_RECORD_AUDIO = 13;\r\n  private Button startButton;\r\n  private TextView outputText;\r\n  private static final String LOG_TAG = SpeechActivity.class.getSimpleName();\r\n\r\n  // Working variables.\r\n  short[] recordingBuffer = new short[RECORDING_LENGTH];\r\n  int recordingOffset = 0;\r\n  boolean shouldContinue = true;\r\n  private Thread recordingThread;\r\n  boolean shouldContinueRecognition = true;\r\n  private Thread recognitionThread;\r\n  private final ReentrantLock recordingBufferLock = new ReentrantLock();\r\n  //private TensorFlowInferenceInterface inferenceInterface;\r\n  private Interpreter tfLite;\r\n\r\n  private static MappedByteBuffer loadModelFile(AssetManager assets, String modelFilename)\r\n          throws IOException {\r\n    AssetFileDescriptor fileDescriptor = assets.openFd(modelFilename);\r\n    FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());\r\n    FileChannel fileChannel = inputStream.getChannel();\r\n    long startOffset = fileDescriptor.getStartOffset();\r\n    long declaredLength = fileDescriptor.getDeclaredLength();\r\n    return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);\r\n  }\r\n\r\n  @Override\r\n  protected void onCreate(Bundle savedInstanceState) {\r\n    // Set up the UI.\r\n    super.onCreate(savedInstanceState);\r\n    setContentView(R.layout.activity_speech);\r\n    startButton = (Button) findViewById(R.id.start);\r\n    startButton.setOnClickListener(\r\n        new View.OnClickListener() {\r\n          @Override\r\n          public void onClick(View view) {\r\n\r\n              startRecording();\r\n          }\r\n        });\r\n    outputText = (TextView) findViewById(R.id.output_text);\r\n\r\n    String actualModelFilename = MODEL_FILENAME.split(\"file:///android_asset/\", -1)[1];\r\n\r\n    // Load the Pretrained WaveNet model.\r\n    //inferenceInterface = new TensorFlowInferenceInterface(getAssets(), MODEL_FILENAME);\r\n    try {\r\n      tfLite = new Interpreter(loadModelFile(getAssets(), actualModelFilename));\r\n    } catch (IOException e) {\r\n      e.printStackTrace();\r\n    }\r\n\r\n    tfLite.resizeInput(0, new int[] {RECORDING_LENGTH, 1});\r\n    //tfLite.resizeInput(1, new int[] {1});\r\n\r\n    requestMicrophonePermission();\r\n  }\r\n\r\n  private void requestMicrophonePermission() {\r\n    requestPermissions(\r\n        new String[] {android.Manifest.permission.RECORD_AUDIO}, REQUEST_RECORD_AUDIO);\r\n  }\r\n\r\n  @Override\r\n  public void onRequestPermissionsResult(\r\n      int requestCode, String[] permissions, int[] grantResults) {\r\n    if (requestCode == REQUEST_RECORD_AUDIO\r\n        && grantResults.length > 0\r\n        && grantResults[0] == PackageManager.PERMISSION_GRANTED) {\r\n    }\r\n  }\r\n\r\n  public synchronized void startRecording() {\r\n    if (recordingThread != null) {\r\n      return;\r\n    }\r\n    shouldContinue = true;\r\n    recordingThread =\r\n        new Thread(\r\n            new Runnable() {\r\n              @Override\r\n              public void run() {\r\n                record();\r\n              }\r\n            });\r\n    recordingThread.start();\r\n  }\r\n\r\n  private void record() {\r\n    android.os.Process.setThreadPriority(android.os.Process.THREAD_PRIORITY_AUDIO);\r\n\r\n    // Estimate the buffer size we'll need for this device.\r\n    int bufferSize =\r\n        AudioRecord.getMinBufferSize(\r\n                (int) SAMPLE_RATE, AudioFormat.CHANNEL_IN_MONO, AudioFormat.ENCODING_PCM_16BIT);\r\n    if (bufferSize == AudioRecord.ERROR || bufferSize == AudioRecord.ERROR_BAD_VALUE) {\r\n      bufferSize = (int) (SAMPLE_RATE * 2);\r\n    }\r\n    short[] audioBuffer = new short[bufferSize / 2];\r\n\r\n    AudioRecord record =\r\n        new AudioRecord(\r\n            MediaRecorder.AudioSource.DEFAULT,\r\n                (int) SAMPLE_RATE,\r\n            AudioFormat.CHANNEL_IN_MONO,\r\n            AudioFormat.ENCODING_PCM_16BIT,\r\n            bufferSize);\r\n\r\n    if (record.getState() != AudioRecord.STATE_INITIALIZED) {\r\n      Log.e(LOG_TAG, \"Audio Record can't initialize!\");\r\n      return;\r\n    }\r\n\r\n    record.startRecording();\r\n\r\n    Log.v(LOG_TAG, \"Start recording\");\r\n\r\n\r\n    while (shouldContinue) {\r\n      int numberRead = record.read(audioBuffer, 0, audioBuffer.length);\r\n        Log.v(LOG_TAG, \"read: \" + numberRead);\r\n      int maxLength = recordingBuffer.length;\r\n      recordingBufferLock.lock();\r\n      try {\r\n          if (recordingOffset + numberRead < maxLength) {\r\n              System.arraycopy(audioBuffer, 0, recordingBuffer, recordingOffset, numberRead);\r\n          } else {\r\n              shouldContinue = false;\r\n          }\r\n        recordingOffset += numberRead;\r\n      } finally {\r\n        recordingBufferLock.unlock();\r\n      }\r\n    }\r\n    record.stop();\r\n    record.release();\r\n    startRecognition();\r\n  }\r\n\r\n  public synchronized void startRecognition() {\r\n    if (recognitionThread != null) {\r\n      return;\r\n    }\r\n    shouldContinueRecognition = true;\r\n    recognitionThread =\r\n        new Thread(\r\n            new Runnable() {\r\n              @Override\r\n              public void run() {\r\n                recognize();\r\n              }\r\n            });\r\n    recognitionThread.start();\r\n  }\r\n\r\n  private void recognize() {\r\n    Log.v(LOG_TAG, \"Start recognition\");\r\n\r\n    short[] inputBuffer = new short[RECORDING_LENGTH];\r\n    float[][] floatInputBuffer = new float[1][RECORDING_LENGTH];\r\n    float[][] outputScores = new float[16][map.length];\r\n    //String[] outputScoresNames = new String[]{OUTPUT_SCORES_NAME};\r\n    float[] sampleRateList = new float[] {SAMPLE_RATE};\r\n    Map<Integer, Object> outputScoresNames = new HashMap<>();\r\n    outputScoresNames.put(0, outputScores);\r\n\r\n\r\n      recordingBufferLock.lock();\r\n      try {\r\n        int maxLength = recordingBuffer.length;\r\n          System.arraycopy(recordingBuffer, 0, inputBuffer, 0, maxLength);\r\n      } finally {\r\n        recordingBufferLock.unlock();\r\n      }\r\n\r\n      // We need to feed in float values between -1.0 and 1.0, so divide the\r\n      // signed 16-bit inputs.\r\n      for (int i = 0; i < RECORDING_LENGTH; ++i) {\r\n        floatInputBuffer[0][i] = inputBuffer[i] / 32767.0f;\r\n      }\r\n\r\n      //MFCC java library.\r\n      /*MFCC mfccConvert = new MFCC();\r\n      float[] mfccInput = mfccConvert.process(floatInputBuffer);\r\n      Log.v(LOG_TAG, \"MFCC Input======> \" + Arrays.toString(mfccInput));*/\r\n\r\n      Object[] inputArray = {floatInputBuffer, sampleRateList};\r\n\r\n      // Run the model.\r\n      //inferenceInterface.feed(INPUT_DATA_NAME, mfccInput, 1, 157, 20);\r\n      //inferenceInterface.run(outputScoresNames);\r\n      //inferenceInterface.fetch(OUTPUT_SCORES_NAME, outputScores);\r\n      //tfLite.runForMultipleInputsOutputs(inputArray, outputScoresNames); //error\r\n      tfLite.run(floatInputBuffer, outputScores); //error\r\n      Log.v(LOG_TAG, \"OUTPUT======> \" + Arrays.toString(outputScores));\r\n      //Output the result.\r\n      String result = \"\";\r\n      for (int i = 0; i<outputScores.length; i++) {\r\n          for (int j = 0; j<outputScores[i].length; j++) {\r\n              if (outputScores[i][j] == 0)\r\n                  break;\r\n              result += map[Math.round(outputScores[i][j])];\r\n          }\r\n      }\r\n      final String r = result;\r\n      this.runOnUiThread(new Runnable() {\r\n          @Override\r\n          public void run() {\r\n              outputText.setText(r);\r\n          }\r\n      });\r\n      Log.v(LOG_TAG, \"End recognition: \"+result);\r\n    }\r\n}\r\n`", "> . I changed the model from the pretrained model to my model. I trained the same architecture mentioned above with my data. \r\n\r\nAre your inputs/outputs identical to the DeepSpeech model?", "Part of the problem here is that you're trying to plug in the DeepSpeech model into the SpeechRecognition sample, which is using a *very* different model. These models take different inputs and provide different outputs.", "This question would be better served asking on StackOverflow. It doesn't appear to be an issue with TensorFlow Lite, but in how you're using TensorFlow Lite.", "> > . I changed the model from the pretrained model to my model. I trained the same architecture mentioned above with my data.\r\n> \r\n> Are your inputs/outputs identical to the DeepSpeech model?\r\n\r\nSorry for late response. Yes it is the same.\r\n"]}, {"number": 30348, "title": "TensorFlow 1.14 artifacts on Maven Central?", "body": "The latest version of [tensorflow](https://mvnrepository.com/artifact/org.tensorflow/tensorflow) and [tensorflow-android](https://mvnrepository.com/artifact/org.tensorflow/tensorflow-android) artifacts is 1.13.1. Will version 1.14.0 be published?", "comments": ["@maksim-m ,\r\nThe Artifacts of Official Tensorflow for 1.14 is present in this [Github link](https://github.com/tensorflow/tensorflow/tree/r1.14/tensorflow). \r\n\r\nArtifacts for Tensorflow for Java and Andriod can be found in this [Github link](https://github.com/tensorflow/tensorflow/tree/r1.14/tensorflow/java)", "Thanks for the reply, but the links you provide only contain the source code and build instructions,  not the artifacts (.jar and .aar files) for Java and Android. Thus, the users have to build TensorFlow themselves first. \r\n\r\nPrior to the release of version 1.14 there was always an official release of TensorFlow for Java and Android on Maven Central. Even [official documentation suggests](https://www.tensorflow.org/install/lang_java#tensorflow_with_apache_maven) to install TensorFlow for Java from Maven Central, but the artifacts for 1.14 are not uploaded there.", "I Maxim, \r\nI have this message when I test my installation :\r\nYour CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\nIt's mean that i cannot use TensorFlow  on my system ?\r\nThx\r\nRick", "> \r\n> \r\n> Thanks for the reply, but the links you provide only contain the source code and build instructions, not the artifacts (.jar and .aar files) for Java and Android. Thus, the users have to build TensorFlow themselves first.\r\n> \r\n> Prior to the release of version 1.14 there was always an official release of TensorFlow for Java and Android on Maven Central. Even [official documentation suggests](https://www.tensorflow.org/install/lang_java#tensorflow_with_apache_maven) to install TensorFlow for Java from Maven Central, but the artifacts for 1.14 are not uploaded there.\r\n\r\nI join the @maksim-m 's request. I think its neccesary to upload the artifacts to Maven Central.", "Hi, \r\nThx\r\nI join the @maksim-m 's request. I think its neccesary to upload the artifacts to Maven Central.\r\n(1.14.0)", "I'm working on resolving a missing `android_buildinfo.json` artifact which is blocking the release to Maven.", "I've uploaded artifacts for tensorflow 1.14.0 to Maven Central, with the exception of tensorflow-android. There are some changes to how that artifact is released which are causing some delays, sorry about that!", "Hi @sjamesr, thank you for adding 1.14.0 version to Maven. \r\n\r\nWhen will the tensorflow-android 1.14.0 be released on Maven Central? I'd like to use some operations in my application which are supported only in 1.14.0, such as GatherV2.\r\n\r\nThanks! ", "I found some issues with the 1.14.0 release of TF Java, I'm working on a 1.14.0.1 right now to fix those. This means 1.14.0 is not usable in its current form (at least on Mac and Linux).\r\n\r\nAlso working on an answer for tensorflow-android, no ETA right now.", "> \r\n> \r\n> I found some issues with the 1.14.0 release of TF Java, I'm working on a 1.14.0.1 right now to fix those. This means 1.14.0 is not usable in its current form (at least on Mac and Linux).\r\n> \r\n> Also working on an answer for tensorflow-android, no ETA right now.\r\n\r\nHi @sjamesr are there any updates about the Android package? \r\n\r\nI'm waiting for the release.", "There will be a 1.15 release soon with the usual release process (release candidates, final release) instead of trying to patch the broken 1.14. Sorry about that", "Hi @mihaimaruseac \r\nThanks for the information! May I also ask the expected release date for tensorflow-android 1.15?\r\nThanks a lot", "They should be released at around the same time", "Hi @ssrp. There has been a release candidate for 1.15, can you please check that tensorflow-android is also released?", "@mihaimaruseac it's not released on Maven Central. The latest version of tensorflow for Java is [1.14.0](https://mvnrepository.com/artifact/org.tensorflow/tensorflow) (which is not [usable in its current form](https://github.com/tensorflow/tensorflow/issues/30348#issuecomment-512922088)) and the latest version of tensorflow-android is [1.13.1](https://mvnrepository.com/artifact/org.tensorflow/tensorflow-android).", "Hi @mihaimaruseac, it is not released on Maven Central yet.\r\nAlso, I found the 1.15 artifacts of TF for Java and Android [here](https://github.com/tensorflow/tensorflow/tree/v1.15.0-rc1/tensorflow/java/) but I couldn't tell if it is updated (comparing with [1.14 Artifacts](https://github.com/tensorflow/tensorflow/tree/r1.14/tensorflow/java/)).", "Assigning @jdduke in this case", "Apologies for any confusion, we are no longer releasing TensorFlow Mobile Android artifacts on Maven. This can still be built from source, but it will not be part of the official release. TensorFlow Lite Maven artifacts for Android are available at the [usual link](https://mvnrepository.com/artifact/org.tensorflow/tensorflow-lite?repo=bt-google-tensorflow). Standard (desktop) TensorFlow artifacts for 1.14 [have been published](https://mvnrepository.com/artifact/org.tensorflow/tensorflow), and should be published with future releases.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30348\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30348\">No</a>\n"]}, {"number": 30347, "title": "Can tensorflow support alluxio as storage filesystem?", "body": "hello\uff0c\r\n   As a storage middle tier, alluxio can mount file systems such as hdfs, nfs, etc., while providing efficient caching capabilities. If tensorflow supports reading data directly from alluxio, it would be very beneficial.", "comments": ["Hi @myg821561935 , you can find instruction about how to do this here: https://docs.alluxio.io/os/user/stable/en/compute/Tensorflow.html", "@myg821561935  Did you get a chance to see the instructions suggested by haoyuan. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 30346, "title": "tf.keras.datasets not batched correctly", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution Linux Mint 19\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\nUsing the return values of `tf.keras.datasets.Cifar10` in `model.fit` seems to process the entire dataset in one batch, independent of the given batch size.\r\n\r\n**Describe the expected behavior**\r\nBoth versions should take the same amount of time.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nfrom tensorflow import keras\r\n\r\n(x, y), _ = keras.datasets.cifar10.load_data()\r\nx = (x / 255.0).reshape(x.shape[0], -1)\r\n\r\nmodel = keras.Sequential([keras.layers.Dense(10)])\r\n\r\n# commenting out these lines result in way slower training\r\nx = x[0:10]\r\ny = y[0:10]\r\n\r\nmodel.compile(\"sgd\", \"sparse_categorical_crossentropy\")\r\nmodel.fit(x, y, epochs=1, batch_size=1, steps_per_epoch=10)\r\n\r\n```\r\n\r\n**Other info / logs**\r\nThe example as written:\r\n```\r\n10/10 [==============================] - 0s 5ms/step - loss: 7.4634\r\n```\r\n\r\nand with the marked lines commented out:\r\n```\r\n10/10 [==============================] - 8s 786ms/step - loss: 5.7648\r\n```", "comments": ["I could reproduce the issue on Colab with Tensorflow version 2.0.0.beta1. ", "@ngc92 the keras.datasets return NumPy arrays, by doing `x = x[0:10]` you are telling it to only look at 10 samples, otherwise it is looking at the whole data, `steps_per_epoch` is ignored for NumPy arrays and only used for Datasets", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30346\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30346\">No</a>\n"]}, {"number": 30345, "title": "Keras Reshape Layer in Functional API Seems Bugged", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux 5.1.15 kernel\r\n- TensorFlow installed from (source or binary): Arch repository\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version: N/A, using CPU\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nI am getting the following error: Input to reshape is a tensor with 24576 values, but the requested shape has 1536\r\n\r\n**Describe the expected behavior**\r\nThis sample code should run.\r\nEditing out the skip connection makes the code run fine.\r\nDo note that I have successfully used the Reshape functionality just fine in straight feedforward models. For some reason, this bug crops up when using Reshape in a skip connection as shown in the sample code below.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef Model():\r\n    \r\n    x = tf.keras.layers.Input((4,4,3)) # 4x4 image with 3 channels\r\n    y = tf.keras.layers.Conv2DTranspose(3, 4, 2, padding='same') (x) # Creates a 8x8 images with 3 channels by upsampling with stride 2\r\n    \r\n    linear = tf.keras.layers.Dense(8*8*3) (x) # Linear transformation of the input\r\n    linear = tf.keras.layers.Reshape([8, 8, 3]) (linear) # Reshapes the output of the linear transformation to the same shape as the upsampled image\r\n    y = tf.keras.layers.Add() ([y, linear]) # adds them together\r\n    \r\n    return tf.keras.models.Model(inputs=x, outputs=y)\r\n\r\n# model\r\nmodel = Model()\r\n\r\n# input data\r\narray_range = np.random.randn(128, 4, 4,  3).astype(np.float32)\r\ndataset = tf.data.Dataset.from_tensor_slices(array_range).batch(8)\r\niterator = dataset.make_one_shot_iterator()\r\nnext_element = iterator.get_next()\r\ndataset_output = model(next_element)\r\n\r\n# session\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\n\r\n# evaluate\r\nprint(sess.run(dataset_output))\r\nprint(sess.run(dataset_output))\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nrunfile('/home/jaap/Dropbox/Python Projects/Code/Preprocessing/testing.py', wdir='/home/jaap/Dropbox/Python Projects/Code/Preprocessing')\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-20-0a014c5b096e>\", line 1, in <module>\r\n    runfile('/home/jaap/Dropbox/Python Projects/Code/Preprocessing/testing.py', wdir='/home/jaap/Dropbox/Python Projects/Code/Preprocessing')\r\n\r\n  File \"/usr/lib/python3.7/site-packages/spyder_kernels/customize/spydercustomize.py\", line 827, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"/usr/lib/python3.7/site-packages/spyder_kernels/customize/spydercustomize.py\", line 110, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"/home/jaap/Dropbox/Python Projects/Code/Preprocessing/testing.py\", line 40, in <module>\r\n    print(sess.run(dataset_output))\r\n\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 950, in run\r\n    run_metadata_ptr)\r\n\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1173, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\r\n    run_metadata)\r\n\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1370, in _do_call\r\n    raise type(e)(node_def, op, message)\r\n\r\nInvalidArgumentError: Input to reshape is a tensor with 24576 values, but the requested shape has 1536\r\n\t [[node model_8/reshape_8/Reshape (defined at /home/jaap/Dropbox/Python Projects/Code/Preprocessing/testing.py:33) ]]\r\n\r\nOriginal stack trace for 'model_8/reshape_8/Reshape':\r\n  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/usr/lib/python3.7/site-packages/spyder_kernels/console/__main__.py\", line 11, in <module>\r\n    start.main()\r\n  File \"/usr/lib/python3.7/site-packages/spyder_kernels/console/start.py\", line 310, in main\r\n    kernel.start()\r\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 505, in start\r\n    self.io_loop.start()\r\n  File \"/usr/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 132, in start\r\n    self.asyncio_loop.run_forever()\r\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 539, in run_forever\r\n    self._run_once()\r\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1775, in _run_once\r\n    handle._run()\r\n  File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\r\n    self._context.run(self._callback, *self._args)\r\n  File \"/usr/lib/python3.7/site-packages/tornado/ioloop.py\", line 758, in _run_callback\r\n    ret = callback()\r\n  File \"/usr/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/lib/python3.7/site-packages/tornado/gen.py\", line 1233, in inner\r\n    self.run()\r\n  File \"/usr/lib/python3.7/site-packages/tornado/gen.py\", line 1147, in run\r\n    yielded = self.gen.send(value)\r\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\r\n    yield gen.maybe_future(dispatch(*args))\r\n  File \"/usr/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\r\n    yielded = next(result)\r\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 272, in dispatch_shell\r\n    yield gen.maybe_future(handler(stream, idents, msg))\r\n  File \"/usr/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\r\n    yielded = next(result)\r\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 542, in execute_request\r\n    user_expressions, allow_stdin,\r\n  File \"/usr/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\r\n    yielded = next(result)\r\n  File \"/usr/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/usr/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2848, in run_cell\r\n    raw_cell, store_history, silent, shell_futures)\r\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2874, in _run_cell\r\n    return runner(coro)\r\n  File \"/usr/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\r\n    coro.send(None)\r\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3049, in run_cell_async\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3220, in run_ast_nodes\r\n    if (yield from self.run_code(code, result)):\r\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-20-0a014c5b096e>\", line 1, in <module>\r\n    runfile('/home/jaap/Dropbox/Python Projects/Code/Preprocessing/testing.py', wdir='/home/jaap/Dropbox/Python Projects/Code/Preprocessing')\r\n  File \"/usr/lib/python3.7/site-packages/spyder_kernels/customize/spydercustomize.py\", line 827, in runfile\r\n    execfile(filename, namespace)\r\n  File \"/usr/lib/python3.7/site-packages/spyder_kernels/customize/spydercustomize.py\", line 110, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n  File \"/home/jaap/Dropbox/Python Projects/Code/Preprocessing/testing.py\", line 33, in <module>\r\n    dataset_output = model(next_element)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 634, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\", line 751, in call\r\n    return self._run_internal_graph(inputs, training=training, mask=mask)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\", line 893, in _run_internal_graph\r\n    output_tensors = layer(computed_tensors, **kwargs)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 634, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py\", line 467, in call\r\n    (array_ops.shape(inputs)[0],) + self.target_shape)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 7715, in reshape\r\n    \"Reshape\", tensor=tensor, shape=shape, name=name)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n```\r\n", "comments": ["I could reproduce this issue in Jupyter Notebook with TF Version 1.14.", "Weirdly enough this issue does not happen if you use flatten before the Dense layer in the skip connection. I was under the impression Flatten was not required before using a Dense layer. But apparently it does work around this bug. Code snippet that does work below:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef Model():\r\n    \r\n    x = tf.keras.layers.Input((4,4,3)) # 4x4 image with 3 channels\r\n    y = tf.keras.layers.Conv2DTranspose(3, 4, 2, padding='same') (x) # Creates a 8x8 images with 3 channels by upsampling with stride 2\r\n    \r\n    linear = tf.keras.layers.Flatten() (x)\r\n    linear = tf.keras.layers.Dense(8*8*3) (linear) # Linear transformation of the input\r\n    linear = tf.keras.layers.Reshape([8, 8, 3]) (linear) # Reshapes the output of the linear transformation to the same shape as the upsampled image\r\n    y = tf.keras.layers.Add() ([y, linear]) # adds them together\r\n    \r\n    return tf.keras.models.Model(inputs=x, outputs=y)\r\n\r\n# model\r\nmodel = Model()\r\n\r\n# input data\r\narray_range = np.random.randn(128, 4, 4,  3).astype(np.float32)\r\ndataset = tf.data.Dataset.from_tensor_slices(array_range).batch(8)\r\niterator = dataset.make_one_shot_iterator()\r\nnext_element = iterator.get_next()\r\ndataset_output = model(next_element)\r\n\r\n# session\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\n\r\n# evaluate\r\nprint(sess.run(dataset_output))\r\nprint(sess.run(dataset_output))\r\n```", "@Mushoz ,\r\nPlease confirm if we can close this issue, as it is resolved.", "It depends. If the behavior that Dense requires a Flatten layer before is the intended behavior, then this definitely can be closed. If this is a bug, then this should probably fixed I guess?", "@Mushoz ,\r\nFrom the information provided in this [Stack Overflow Issue](https://stackoverflow.com/questions/43237124/role-of-flatten-in-keras) and in this [Keras Site](https://keras.io/layers/core/#dense), it is intended behavior. \r\n\r\nHence closing the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30345\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30345\">No</a>\n"]}, {"number": 30344, "title": "[Lite]Bugfix System.loadLibrary exception handle", "body": "Exception handle when application couldn't able to load the prebuilt smartreply_jni library.", "comments": ["Under what conditions do you find that the library fails to load?", "@Dayananda-V Did you get a chance to look on jdduke's comments? Please let us know on the update. Thanks! "]}, {"number": 30343, "title": " convolution VS correlation convolution ", "body": "\r\nI think the \r\n\r\n> tensorflow.nn.conv2d \r\n\r\nuse the correlation convolution, but i need convolution in my layer. So do you have any advice?\r\n", "comments": ["@lannisite110 Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 30342, "title": "{Deeplab}{mobile}using deeplab on mobile", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):macOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:iPhone6+\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):1.12.0\r\n- Python version:3.7\r\n- Bazel version (if compiling from source):0.27.0\r\n- GCC/Compiler version (if compiling from source):Nan\r\n- CUDA/cuDNN version:Nan\r\n- GPU model and memory:Nan\r\n\r\n**Describe the current behavior**\r\nDeeplab does not predict correctly on ios application\r\n**Describe the expected behavior**\r\nreal-time fast and good detection \r\n\r\n**Other info / logs**\r\nHi, i have trained deeplab on my custom dataset(400*300) with 513 as crop size and during the test it detects for crop with crop size 3041 (the reason is that my data are iris images and the model used for iris detection on face images) .\r\nnow what I need is to integrate my model on ios application, i was able to successfully convert the model to tflite with GPU delegate but i notice that on mobile the crop size that i should use to get a good a fast detection is 257 but my problem that my model does not detect with 257 as crop size i need to use 3041 \r\ndoes any on have an idea what should i do ??\r\n\r\n", "comments": []}, {"number": 30341, "title": "session still referable after sess.close()", "body": "From my intuition, a session will be removed from memory once a close() method is called. However, you can still refer to a session even after explicitly closing it. Here is a little example:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nsess = tf.Session()\r\nprint(sess)\r\nsess.close()\r\nprint(sess)\r\n```\r\n\r\nAbove code gives me such result:\r\n\r\n`<tensorflow.python.client.session.Session object at 0x00000000029E9DD8>\r\n<tensorflow.python.client.session.Session object at 0x00000000029E9DD8>`\r\n\r\nMy question is, since a session is not supposed to be reopened, what's the point of preserving it in memory? Is it that the closed session is ready for reopening or is it because it's just a reference and occupys no resource so it makes no difference wether or not the session is removed?", "comments": ["I guess the first reason is that in Python you don't have an object \"self-destruct\" (at least not to my knowledge) - see for example the objects returned by the `open` function, the `sqlite.connect` one, etc. which all have a `close` method but remain referable objects afterwards.\r\n\r\nThe second one is that the session object still retains information once close, which may be useful in some cases - see the `graph_def` attribute (although this is actually stored elsewhere and can be obtained by re-opening a session with the same string name - which is stored under the `str_name` attribute, also preserved after closing).\r\n\r\nFinally, note that if you want to remove the session once closed, you can always use `del sess`.\r\n\r\nP-S : no offence, but this is more of a question to ask on a website like Stack Overflow than an actual Issue.", "> I guess the first reason is that in Python you don't have an object \"self-destruct\" (at least not to my knowledge) - see for example the objects returned by the `open` function, the `sqlite.connect` one, etc. which all have a `close` method but remain referable objects afterwards.\r\n> \r\n> The second one is that the session object still retains information once close, which may be useful in some cases - see the `graph_def` attribute (although this is actually stored elsewhere and can be obtained by re-opening a session with the same string name - which is stored under the `str_name` attribute, also preserved after closing).\r\n> \r\n> Finally, note that if you want to remove the session once closed, you can always use `del sess`.\r\n> \r\n> P-S : no offence, but this is more of a question to ask on a website like Stack Overflow than an actual Issue.\r\n\r\nThanks for commenting, learned a lot!\r\nWill close this issue."]}, {"number": 30340, "title": "[INTEL MKL] Bugfix: 3d-dilated convolution crash on MKL-DNN build of tensorflow", "body": "Code to reproduce this bug:\r\n<pre>\r\nimport tensorflow as tf\r\n\r\nimage_shape = (1,224,224,224,1)\r\nkernel_1_shape = (5,5,5,1,1)\r\n\r\n#Dummy image:\r\ndummy_image = tf.truncated_normal(\r\n                     image_shape,\r\n                     dtype=tf.float32,\r\n                     mean=0,\r\n                     stddev=1,\r\n                     name='3Dconv_input')\r\ndummy_kernel_1 = tf.truncated_normal(\r\n                     kernel_1_shape,\r\n                     dtype=tf.float32,\r\n                     mean=0,\r\n                     stddev=1,\r\n                     name='3Dconv_kernel_1')\r\n\r\nconv = tf.nn.conv3d(dummy_image, dummy_kernel_1, [1, 1, 1, 1, 1], dilations=[1, 2, 2, 2, 1], \r\n                                 padding='SAME', data_format='NDHWC')\r\n\r\nwith tf.Session() as sess:\r\n       sess.run(conv)\r\n</pre>\r\n\r\nWe fix the bug in this PR, and also add testcases to make sure it won't happen again.", "comments": []}, {"number": 30339, "title": "[LITE]Changed notebooks kernel to python3", "body": "All other in lite folder is python3 based kernel", "comments": ["Check out this pull request on ReviewNB: https://app.reviewnb.com/tensorflow/tensorflow/pull/30339 \n\n You'll be able to see visual diffs and write comments on notebook cells. Powered by <a href='https://www.reviewnb.com'>ReviewNB</a>.", "@siju-samuel Could you please resolve the conflicts? Thanks!", "Can one of the admins verify this patch?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 30338, "title": "[LITE]BugFix non-string type causes memleak for two outputs", "body": "", "comments": []}, {"number": 30337, "title": "[LITE]Fix array leak during resize o/p", "body": "", "comments": []}, {"number": 30336, "title": "Xla int nearestneighbor resize", "body": "Make nearest neighbor resize always use float to do convolution for GPU/XLA, even if the inputs are integers, because convolution with most integer types is not supported for GPU/XLA.", "comments": ["Probably @ijkilchenko is the right person to review this.", "Requesting a review from @blakehechtman because even though I've played around with this file and did `ResizeNearestNeighborOp`, I am not familiar enough to review other's work. ", "@yongfeng-nv Could you please resolve the conflicts? Thanks!", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30336) for more info**.\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30336) for more info**.\n\n<!-- ok -->", "@yongfeng-nv Could you please resolve the conflicts? Thanks!", "@ijkilchenko, I see you have committed the changes in tensorflow/compiler/tf2xla/kernels/image_resize_ops.cc (https://github.com/tensorflow/tensorflow/commit/fd34d066ec5f514d737d771efaf0f6cf93925cf7#diff-e46056ad1b86847e16150ab7906bab3b).  Do you want me to push the test change or just drop it?  Thank you.", "@yongfeng-nv Yes, please push them. Thanks! ", "@yongfeng-nv Could you please resolve the conflicts? Thanks!", "The fix and test have been committed in https://github.com/tensorflow/tensorflow/commit/fd34d066ec5f514d737d771efaf0f6cf93925cf7#diff-e46056ad1b86847e16150ab7906bab3b and https://github.com/tensorflow/tensorflow/pull/31156, respectively.  Close this PR.\r\n"]}]