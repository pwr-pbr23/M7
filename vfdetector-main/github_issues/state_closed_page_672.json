[{"number": 33438, "title": "UpSample2D INT8 quantization not supported", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (or github SHA if from source): 2.0\r\n\r\n\r\n**Provide the text output from ~tflite_convert~ TFLiteConverter**\r\n\r\n```\r\n2019-10-16 13:10:02.543859: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-10-16 13:10:02.551468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-16 13:10:02.551740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645\r\npciBusID: 0000:01:00.0\r\n2019-10-16 13:10:02.551828: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-10-16 13:10:02.552513: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-10-16 13:10:02.553132: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-10-16 13:10:02.553261: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-10-16 13:10:02.554087: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-10-16 13:10:02.554716: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-10-16 13:10:02.556627: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-10-16 13:10:02.556687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-16 13:10:02.556988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-16 13:10:02.557237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-10-16 13:10:02.557401: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-10-16 13:10:02.580750: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 4008000000 Hz\r\n2019-10-16 13:10:02.581738: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e194a15110 executing computations on platform Host. Devices:\r\n2019-10-16 13:10:02.581748: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-10-16 13:10:02.624356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-16 13:10:02.624717: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e194aa7ab0 executing computations on platform CUDA. Devices:\r\n2019-10-16 13:10:02.624728: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2019-10-16 13:10:02.624836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-16 13:10:02.625102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645\r\npciBusID: 0000:01:00.0\r\n2019-10-16 13:10:02.625122: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-10-16 13:10:02.625130: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-10-16 13:10:02.625137: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-10-16 13:10:02.625143: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-10-16 13:10:02.625149: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-10-16 13:10:02.625155: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-10-16 13:10:02.625162: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-10-16 13:10:02.625189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-16 13:10:02.625459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-16 13:10:02.625708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-10-16 13:10:02.625725: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-10-16 13:10:02.626255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-10-16 13:10:02.626262: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-10-16 13:10:02.626265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-10-16 13:10:02.626314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-16 13:10:02.626592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-16 13:10:02.626856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9051 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2019-10-16 13:10:02.739500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-16 13:10:02.739771: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2019-10-16 13:10:02.739810: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-10-16 13:10:02.740219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-16 13:10:02.740510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645\r\npciBusID: 0000:01:00.0\r\n2019-10-16 13:10:02.740538: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-10-16 13:10:02.740545: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-10-16 13:10:02.740551: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-10-16 13:10:02.740557: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-10-16 13:10:02.740563: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-10-16 13:10:02.740568: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-10-16 13:10:02.740574: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-10-16 13:10:02.740601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-16 13:10:02.740873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-16 13:10:02.741124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-10-16 13:10:02.741138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-10-16 13:10:02.741142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-10-16 13:10:02.741145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-10-16 13:10:02.741185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-16 13:10:02.741460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-16 13:10:02.741715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9051 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2019-10-16 13:10:02.742758: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize\r\n2019-10-16 13:10:02.742766: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.002ms.\r\n2019-10-16 13:10:02.742770: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2019-10-16 13:10:02.749704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-16 13:10:02.749989: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2019-10-16 13:10:02.750037: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-10-16 13:10:02.750314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-16 13:10:02.750591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645\r\npciBusID: 0000:01:00.0\r\n2019-10-16 13:10:02.750609: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-10-16 13:10:02.750615: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-10-16 13:10:02.750621: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-10-16 13:10:02.750627: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-10-16 13:10:02.750633: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-10-16 13:10:02.750639: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-10-16 13:10:02.750645: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-10-16 13:10:02.750670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-16 13:10:02.750974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-16 13:10:02.751224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-10-16 13:10:02.751237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-10-16 13:10:02.751241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-10-16 13:10:02.751244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-10-16 13:10:02.751285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-16 13:10:02.751587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-16 13:10:02.751843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9051 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2019-10-16 13:10:02.757033: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize\r\n2019-10-16 13:10:02.757044: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 10 nodes (0), 10 edges (0), time = 0.65ms.\r\n2019-10-16 13:10:02.757047: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 10 nodes (0), 10 edges (0), time = 0.14ms.\r\nINFO: Initialized TensorFlow Lite runtime.\r\nTraceback (most recent call last):\r\n  File \"/home/wd_ai/git-pkgs/yolov3-tf2/test_upsample2d_tflite.py\", line 35, in <module>\r\n    tflite_model = convert2tflite_int8(model)\r\n  File \"/home/wd_ai/git-pkgs/yolov3-tf2/test_upsample2d_tflite.py\", line 28, in convert2tflite_int8\r\n    tflite_model = converter.convert()\r\n  File \"/home/wd_ai/miniconda3/envs/tf2-n-pytorch/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py\", line 450, in convert\r\n    constants.FLOAT)\r\n  File \"/home/wd_ai/miniconda3/envs/tf2-n-pytorch/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py\", line 239, in _calibrate_quantize_model\r\n    inference_output_type, allow_float)\r\n  File \"/home/wd_ai/miniconda3/envs/tf2-n-pytorch/lib/python3.7/site-packages/tensorflow_core/lite/python/optimize/calibrator.py\", line 78, in calibrate_and_quantize\r\n    np.dtype(output_type.as_numpy_dtype()).num, allow_float)\r\n  File \"/home/wd_ai/miniconda3/envs/tf2-n-pytorch/lib/python3.7/site-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py\", line 115, in QuantizeModel\r\n    return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, input_py_type, output_py_type, allow_float)\r\nRuntimeError: Quantization not yet supported for op: RESIZE_NEAREST_NEIGHBOR\r\n\r\nProcess finished with exit code 1\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible. (source code)\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nimport numpy as np\r\n\r\n\r\ndef build_upsample_model():\r\n    inputs = keras.Input(shape=(28, 28, 32), name='input')\r\n    outputs = layers.UpSampling2D()(inputs)\r\n    model = keras.models.Model(inputs=inputs, outputs=outputs, name='upsample_model')\r\n    return model\r\n\r\n\r\ndef convert2tflite_int8(model):\r\n    def repr_data_gen(shape):\r\n\r\n        def gen_random_data():\r\n            for _ in range(10):\r\n                yield [np.array(np.random.random_sample(shape), dtype=np.float32)]\r\n\r\n        return gen_random_data\r\n\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n    converter.representative_dataset = repr_data_gen((1, 28, 28, 32))\r\n\r\n    tflite_model = converter.convert()\r\n\r\n    return tflite_model\r\n\r\n\r\nif __name__ == '__main__':\r\n    model = build_upsample_model()\r\n    tflite_model = convert2tflite_int8(model)\r\n```\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["see comment: https://github.com/tensorflow/tensorflow/issues/33397#issuecomment-542880236", "I submitted a PR to address the issue: https://github.com/tensorflow/tensorflow/pull/33492 .\r\n\r\nWill close this one for now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33438\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33438\">No</a>\n"]}, {"number": 33437, "title": "IPython Tab Completion causes logged warnings", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below):  `.1.15.0` (git version `v1.15.0-rc3-22-g590d6ee`, `1.15.0`)\r\n- Python version: 3.6.6\r\n\r\n**Describe the current behavior**\r\nTab-completions in IPython spam the console with `The name ___ is deprecated` logging warnings.\r\n\r\n**Code to reproduce the issue**\r\nI am using Python 3.6.6 with IPython 7.8.0. If you do:\r\n\r\n```\r\n$ ipython\r\nimporPython 3.6.6 | packaged by conda-forge | (default, Oct 12 2018, 14:43:46) \r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 7.8.0 -- An enhanced Interactive Python. Type '?' for help.\r\n\r\nIn [1]: import tensorflow as tf                                                                                                                                                                                                        \r\n\r\nIn [2]: tf.\r\n```\r\n\r\nand press `tab` after the `.`, you get dozens of warnings like:\r\n\r\n```\r\nWARNING:tensorflow:From /opt/anaconda/envs/ctrldev/lib/python3.6/site-packages/jedi/evaluate/compiled/access.py:347: The name tf.parse_example is deprecated. Please use tf.io.parse_example instead.\r\n\r\nWARNING:tensorflow:From /opt/anaconda/envs/ctrldev/lib/python3.6/site-packages/jedi/evaluate/compiled/access.py:347: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\r\n\r\nWARNING:tensorflow:From /opt/anaconda/envs/ctrldev/lib/python3.6/site-packages/jedi/evaluate/compiled/access.py:347: The name tf.parse_single_sequence_example is deprecated. Please use tf.io.parse_single_sequence_example instead.\r\n\r\nWARNING:tensorflow:From /opt/anaconda/envs/ctrldev/lib/python3.6/site-packages/jedi/evaluate/compiled/access.py:347: The name tf.parse_tensor is deprecated. Please use tf.io.parse_tensor instead.\r\n```\r\n\r\nYou can actually reproduce this with the docker images, although that produces fewer deprecation warnings than the released package does:\r\n\r\n```\r\n$ docker run -it tensorflow/tensorflow:1.15.0rc1-py3-jupyter ipython\r\nPython 3.6.8 (default, Aug 20 2019, 17:12:48) \r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 7.8.0 -- An enhanced Interactive Python. Type '?' for help.\r\n\r\nIn [1]: import tensorflow as tf                                                                                                                                                                                                        \r\ntf.\r\nIn [2]: tf.WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/jedi/evaluate/compiled/access.py:347: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.  \r\n```\r\n\r\n(Unfortunately, I wasn't able to find a more up-to-date official docker image).", "comments": ["@alanhdu, \r\nI tried to replicate the issue on Ipython IDE 7.8.0. But it worked as expected with Tf 2.0.0beta.\r\nPlease look at the screenshot below. You want to try Tf2.0.0.Thanks!\r\n![Screenshot from 2019-10-17 13-46-55](https://user-images.githubusercontent.com/48476109/66990995-d8156000-f0e4-11e9-999b-61c34b08bbca.png)\r\n \r\n", "I could reproduce the issue on Ipython with Tf 1.15.0. Thanks!\r\n![Screenshot from 2019-10-17 13-55-35](https://user-images.githubusercontent.com/48476109/66991503-d1d3b380-f0e5-11e9-976d-b8ab5c5fe842.png)\r\n", "Yes -- this is a TF 1.15.0 problem. We have a pretty large Tensorflow 1 codebase, so a migration to TensorFlow 2 will take some time and isn't currently feasible.", "This can help you suppress the deprecation warning messages;\r\n```python\r\nfrom tensorflow.python.util import deprecation\r\ndeprecation._PRINT_DEPRECATION_WARNINGS = False\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33437\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33437\">No</a>\n", "@ymodak: Is there a reason this issue got closed? You posted a work-around, but AFAICT the issue still exists.\r\n\r\nIn particular, unless the work around is activated by default, TensorFlow 1.15 is currently a *terrible* interactive experience with all the deprecation warnings. And for quick exploratory sessions, it's really common to forget to explicitly silence deprecations.", "This seems related to the way deprecation warnings are printed. Unfortunately, I don't think we have a quick fix to this.\r\n\r\nOne idea for a workaround would be to try to evaluate `len(tf.__dict__)` but I'm not sure this will hide all deprecation warnings.", "The other option is to start by following the code in\r\nhttps://github.com/tensorflow/tensorflow/issues/35550#issuecomment-570661108 ", "Hi @mihaimaruseac . Coming from that different issue. I recently upgraded to TensorFlow 2.1 and am now having a slightly different (but still related issue).\r\n\r\nLooks like in 2.1, all those deprecation errors are gone, which is awesome! Unfortunately, what happens now is:\r\n\r\n1. I get a few errors of the line of \r\n```\r\nimport tensorflow.2020-03-13 16:17:02.351299: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\r\n2020-03-13 16:17:02.352383: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\r\n2020-03-13 16:17:02.352454: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n```\r\n2. At this point, my IPython session either hangs, or actually let's me interact with the top level directory and choose something (albeit after all the warning messages). Having trouble replicating the first; I think I must have CTRL+C'ed or something during the warning dump.\r\n3. Assuming I was able to keep control of the IPython session. I can now `exit` out...but now I see a new error that I've never seen before:\r\n```\r\nError in atexit._run_exitfuncs:\r\nTraceback (most recent call last):\r\n  File \"/home/cheinich/anaconda3/lib/python3.7/logging/__init__.py\", line 2034, in shutdown\r\n    h.close()\r\n  File \"/home/cheinich/anaconda3/lib/python3.7/site-packages/absl/logging/__init__.py\", line 864, in close\r\n    self.stream.close()\r\nAttributeError: 'StdoutProxy' object has no attribute 'close'\r\n```\r\n\r\nSo long story short, looks like things are improved...but I'm not sure what's causing the new issues.\r\n\r\n", "For 1, there's nothing to worry about. They are just warnings and tell you that GPU couldn't be found so it won't be used.\r\n\r\n2 and 3 look more like IPython issues than TF, but we can look at them sometimes.", "Thanks for the response. I would argue that it's still a bug because I don't expect to see any messages at all...since I haven't done anything other than try to tab complete! \r\n\r\nPlus, to be frank, I don't need the messages telling me it can't use the GPU. I know it won't use the GPU because I installed the CPU version of TensorFlow! I get the need to let users know, but I'd much rather look up the information in some config file when I need it (or global variable, or whatever makes sense), rather than have it given to me every time TensorFlow is imported.", "As for the other, my first thought honestly was that it was my IPython terminal, not TensorFlow...but I tried it with a few other libraries (pandas, numpy) and didn't see it show up. It might still be my IPython, but appreciate you looking into it!", "If you only want the CPU version of TensorFlow, you have to install `tensorflow_cpu`. `tensorflow` is a combined package which contains both CPU and GPU", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33437\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33437\">No</a>\n"]}, {"number": 33436, "title": "Add initial minimal ArmNN delegate plugin.", "body": "- Adds ArmNN delegate interface\r\n- Allows to specify backend id\r\n- Supports the following operators: Conv2d, DepthwiseCond2d, Pool2d, Softmax, Squeeze", "comments": ["@jdduke could you take a look at this, since it's in the mobile TensorFlow Lite code?", "Thanks George, per offline discussion we'll sync up on this directly to discuss next steps.", "@GeorgeARM, @jdduke Any update on this PR, please. Thanks!", "Tentatively closing per offline discussions, we can reopen after further dialog around hosting and distribution."]}, {"number": 33435, "title": "mixed_precision_graph_rewrite behaviours", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: V10.0.130\r\n- GPU model and memory: GeForce RTX 2080ti, 10G\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nSee the results below. Basically, there no clear evidence mix precision speeds up the training in this case. \r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nimport os\r\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]= '3'\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n\r\nimport tensorflow\r\nimport tensorflow.keras as keras\r\nfrom tensorflow.keras.datasets import mnist\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Input\r\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\r\nfrom tensorflow.keras import backend as K\r\nimport time\r\n\r\n\r\noptimizer1 = keras.optimizers.Adadelta()\r\noptimizer2 = tensorflow.train.experimental.enable_mixed_precision_graph_rewrite(optimizer1)\r\nopts_dict = {'fp32': optimizer1, 'mix': optimizer2}\r\nbatch_sizes = [256, 512, 1024, 2048, 4096, 8192]\r\nnum_classes = 10\r\nepochs = 30\r\n\r\ndataset = 'mnist'\r\n\r\nfor batch_size in batch_sizes:\r\n    for precision in opts_dict:\r\n\r\n        optimizer = opts_dict[precision]\r\n        start_time = time.time()\r\n\r\n        # the data, split between train and test sets\r\n        if dataset == 'mnist':\r\n            (x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n            if K.image_data_format() == 'channels_first':\r\n                x_train = x_train.reshape(x_train.shape[0], 1, 28, 28)\r\n                x_test = x_test.reshape(x_test.shape[0], 1, 28, 28)\r\n                input_shape = (1, 28, 28)\r\n            else:\r\n                x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\r\n                x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\r\n                input_shape = (28, 28, 1)\r\n\r\n        if dataset == 'cifar10':\r\n            (x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\n            input_shape = x_train.shape[1:]\r\n\r\n\r\n        x_train = x_train.astype('float32')\r\n        x_test = x_test.astype('float32')\r\n        x_train /= 255\r\n        x_test /= 255\r\n\r\n        y_train = keras.utils.to_categorical(y_train, num_classes)\r\n        y_test = keras.utils.to_categorical(y_test, num_classes)\r\n\r\n        input = Input(shape=input_shape)\r\n        x = Conv2D(32, kernel_size=(3, 3),\r\n                         activation='relu',\r\n                         input_shape=input_shape)(input)\r\n        x = Conv2D(64, (3, 3), activation='relu')(x)\r\n        x = MaxPooling2D(pool_size=(2, 2))(x)\r\n        x = Dropout(0.25)(x)\r\n        x = Flatten()(x)\r\n        x = Dense(128, activation='relu')(x)\r\n        x = Dropout(0.5)(x)\r\n        x = Dense(num_classes, activation='softmax')(x)\r\n        model = Model(inputs=input, outputs=x)\r\n        model.compile(loss=keras.losses.categorical_crossentropy,\r\n                      optimizer=optimizer,\r\n                      metrics=['accuracy'])\r\n\r\n        model.fit(x_train, y_train,\r\n                  batch_size=batch_size,\r\n                  epochs=epochs,\r\n                  verbose=0,\r\n                  validation_data=(x_test, y_test))\r\n        score = model.evaluate(x_test, y_test, verbose=0)\r\n        print('Batch size = %s' % batch_size)\r\n        print('Precision = %s' % precision)\r\n        print('Test loss:', score[0])\r\n        print('Test accuracy:', score[1])\r\n        dur = time.time() - start_time\r\n        print('Run time = %s s' % dur)\r\n        print('================================')\r\n\r\n\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nResults on mnist:\r\n![Screenshot from 2019-10-17 13-58-01](https://user-images.githubusercontent.com/32969920/67010775-33941d80-f0e6-11e9-944a-fccb1ec7b933.png)\r\n\r\nI also tried running on cifar10:\r\n![Screenshot from 2019-10-17 13-56-01](https://user-images.githubusercontent.com/32969920/67010652-f6c82680-f0e5-11e9-930c-e5f58bf7e0b7.png)\r\n", "comments": ["You are running with mixed precision even for the fp32 runs. Mixed precision is enabled when you call the line:\r\n\r\n```\r\noptimizer2 = tensorflow.train.experimental.enable_mixed_precision_graph_rewrite(optimizer1)\r\n```\r\n\r\nEven if you do not use `optimizer2`, mixed precision is still being used. `optimizer2` will additionally use a technique called loss scaling to avoid numeric underflow, but mixed precision will still be used without it. \r\n\r\nThe easiest way to fix this is running the Python script twice: once where you call `enable_mixed_precision_graph_rewrite` and once where you do not. Typically you do not want to run with both fp32 and mixed precision in a single process. Alternatively, you can start by running the mixed precision benchmarks, then call `tf.train.experimental.disable_mixed_precision_graph_rewrite()` before doing the fp32 benchmarks.\r\n", "@reedwm Hi,  I got these log infos after starting mix precision training my model on T4 GPU card\r\n\r\n`2019-11-07 16:51:44.992350: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1767] Running auto_mixed_precision graph optimizer\r\n2019-11-07 16:51:44.997218: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1241] No whitelist ops found, nothing to do\r\n2019-11-07 16:51:50.326678: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1767] Running auto_mixed_precision graph optimizer\r\n2019-11-07 16:51:50.434888: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1723] Converted 1299/8694 nodes to float16 precision using 73 cast(s) to float16 (excluding Const and Variable casts)\r\n2019-11-07 16:51:56.493836: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1767] Running auto_mixed_precision graph optimizer\r\n2019-11-07 16:51:56.494237: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1241] No whitelist ops found, nothing to do\r\n2019-11-07 16:51:56.957105: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1767] Running auto_mixed_precision graph optimizer\r\n2019-11-07 16:51:56.957657: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1241] No whitelist ops found, nothing to do`\r\n\r\nIt seems nothing is wrong.\r\nThen I follow the steps below to do inference \r\n1 load the checkpoint with tf. train.Saver() and restored the graph\r\n2  export to savedmodel using saved_model_builder\r\n3  deploying savedmodel files to tf-serving\r\n\r\n Everything is ok. But the inference performance is not impoved...\r\n It that ok? ", "@superhg2012, `enable_mixed_precision_graph_rewrite` will have no effect on SavedModels when served with TensorFlow Serving. The reason is that the function causes TensorFlow to rewrite the graph behind the scenes to use mixed precision, but the SavedModel will still be in float32. \r\n\r\nIn general, `enable_mixed_precision_graph_rewrite` is designed to be a training API (although it will work in inference if you do not export your model). The reason is that for training, more work must be done to ensure certain parts of the model stay in float32, while during inference, almost everything can be done in float16.\r\n\r\n@suharshs, any pointers to using float16 for inference?\r\n\r\n", "@reedwm thanks a lot for kindly explaination. if I want to run inference with tf.float16 dtype.  savedmodel can not be used. Do you mean I can only run inference with  the checkpoint after training?", "Yes, if you want to run inference in float16 with `enable_mixed_precision_graph_rewrite`, you must use the checkpoint after training. You can still save a SavedModel, but the SavedModel will not use mixed precision.\r\n\r\nThere might be an alternative way of using float16 with SavedModel. @christisg @netfs, do either of you know if you can choose to run a grappler pass when serving a SavedModel? Or use some other mechanism to make the model float16?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "> @superhg2012, `enable_mixed_precision_graph_rewrite` will have no effect on SavedModels when served with TensorFlow Serving. The reason is that the function causes TensorFlow to rewrite the graph behind the scenes to use mixed precision, but the SavedModel will still be in float32.\r\n> \r\n> In general, `enable_mixed_precision_graph_rewrite` is designed to be a training API (although it will work in inference if you do not export your model). The reason is that for training, more work must be done to ensure certain parts of the model stay in float32, while during inference, almost everything can be done in float16.\r\n> \r\n> @suharshs, any pointers to using float16 for inference?\r\n\r\nWe also want to known is there any pointers to using float16 for inference?", "Adding --platform_config_file=platform_config_file.cfg for tensorflow_model_server to enable float16 for inference.\r\n```\r\nplatform_configs {\r\n  key: \"tensorflow\"\r\n  value {\r\n    source_adapter_config {\r\n      [type.googleapis.com/tensorflow.serving.SavedModelBundleSourceAdapterConfig] {\r\n        legacy_config {\r\n          #batching_parameters {\r\n          #  max_batch_size { value: 100 }\r\n          #  batch_timeout_micros { value: 200000000 }\r\n          #  max_enqueued_batches { value: 10000 }\r\n          #  num_batch_threads { value: 36 }  \r\n          #}\r\n          session_config {\r\n            allow_soft_placement: true\r\n            gpu_options {\r\n              per_process_gpu_memory_fraction: 0.9\r\n              allow_growth: true\r\n            }\r\n            graph_options {\r\n              rewrite_options {\r\n                auto_mixed_precision: 1 # DEFAULT = 0; ON = 1; OFF = 2;\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n```"]}, {"number": 33434, "title": "tf.function tracing when input tensor varies", "body": "Dear experts,\r\n  In tf 2.0, when using the tf.function decorator, if the shape of the input tensors varies, it seems TF will create a new graph every single time. Is there a way to get around this?\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef add(a, b):\r\n    print('Addition')\r\n    return a + b\r\nadd(tf.constant(1), tf.constant(2))\r\nadd(tf.constant([1, 2]), tf.constant([2, 4]))\r\nadd(tf.constant([333, 2]), tf.constant([2, 4444]))\r\n```\r\n\r\nOutput:\r\n```\r\nAddition\r\nAddition\r\n```\r\n", "comments": ["Closing.\r\n\r\nFound input_signature to solve this. "]}, {"number": 33433, "title": "Merge pull request #1 from tensorflow/master", "body": "fork merge", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33433) for more info**.\n\n<!-- need_sender_cla -->", "Fat-finger error PR, sorry."]}, {"number": 33432, "title": "Update batchnorm doc and change default.", "body": "I'm hoping this triggers a build/test or something.\r\n\r\nWill have to read more about if not.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/33027#issuecomment-542402972", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33432) for more info**.\n\n<!-- need_sender_cla -->", "Seems you have checked out lot of files , it will be very hard to resolve the conflicts , please submit a new PR.", "@googlebot I signed it! ", "@rthadur weird ... there should only be one small change in one file. I'll have a look."]}, {"number": 33431, "title": "internal compiler error: in assign_temp, at function.c:968", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: \r\n- Bazel version (if compiling from source): Build label: 0.26.1\r\n- GCC/Compiler version (if compiling from source): \r\n```gcc (Ubuntu 5.5.0-12ubuntu1~16.04) 5.5.0 20171010```\r\nand\r\n```gcc (Ubuntu 7.4.0-1ubuntu1~16.04~ppa1) 7.4.0```\r\n\r\n- CUDA/cuDNN version: off\r\n- GPU model and memory: off\r\n\r\n\r\n\r\n**Describe the problem**\r\n```\r\nERROR: /home/ubuntu/tensorflow/tensorflow/compiler/mlir/xla/BUILD:226:1: C++ compilation of rule '//tensorflow/compiler/mlir/xla:hlo' failed (Exit 1)\r\ntensorflow/compiler/mlir/xla/ir/hlo_ops.cc: In function 'mlir::Type {anonymous}::GetBroadcastType(mlir::Builder*, mlir::Type, mlir::Type, mlir::Type, mlir::DenseIntElementsAttr)':\r\ntensorflow/compiler/mlir/xla/ir/hlo_ops.cc:622:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     for (int i = 0; i < shape_x.size(); i++) {\r\n                     ~~^~~~~~~~~~~~~~~~\r\nIn file included from ./tensorflow/compiler/mlir/xla/ir/hlo_ops.h:22:0,\r\n                 from tensorflow/compiler/mlir/xla/ir/hlo_ops.cc:18:\r\nexternal/local_config_mlir/include/mlir/IR/Attributes.h: In member function 'T mlir::DenseElementsAttr::getValue(llvm::ArrayRef<long unsigned int>) const [with T = mlir::IntegerAttr]':\r\nexternal/local_config_mlir/include/mlir/IR/Attributes.h:783:20: internal compiler error: in assign_temp, at function.c:968\r\n     auto castFn = [](Attribute attr) { return attr.template cast<T>(); };\r\n                    ^\r\nPlease submit a full bug report,\r\n```\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\n$ bazel build --config=opt  //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I also trying configuring Tensorflow to use the experimental clang option.\r\n\r\nThis lead to a compiler error \r\n```\r\n#include <climits>\r\n```\r\nShould I try to build with gcc or clang on Ubuntu 16.04?", "@dbl001, Please include `./configure` output. Thanks!", "```\r\n(ai) ubuntu@ip-10-0-1-71:~/tensorflow$ ./configure\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.26.1 installed.\r\nPlease specify the location of python. [Default is /home/ubuntu/anaconda2/envs/ai/bin/python]:\r\n\r\n\r\nFound possible Python library paths:   \r\n  /home/ubuntu/anaconda2/envs/ai/lib/python3.6/site-packages\r\n  /home/ubuntu/opencog/build/opencog/cython\r\n  /home/ubuntu/opencog/opencog/python/ \r\n  /home/ubuntu/deepdist\r\nPlease input the desired Python library path to use.  Default is [/home/ubuntu/anaconda2/envs/ai/lib/python3.6/site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: n\r\nClang will not be downloaded.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]:\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n  --config=mkl          # Build with MKL support.\r\n  --config=monolithic   # Config for mostly static monolithic build.\r\n  --config=ngraph       # Build with Intel nGraph support.\r\n  --config=numa         # Build with NUMA support.\r\n  --config=dynamic_kernels    # (Experimental) Build kernels into separate shared objects.\r\n  --config=v2           # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n  --config=noaws        # Disable AWS S3 filesystem support.\r\n  --config=nogcp        # Disable GCP support.\r\n  --config=nohdfs       # Disable HDFS support.\r\n  --config=nonccl       # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n```", "@dbl001, Can you try compiling with GCC 4.8. Let us know how it progresses. Thanks!", "Same error with gcc 4.8.5:\r\n\r\n```\r\nERROR: /home/ubuntu/tensorflow/tensorflow/compiler/mlir/xla/BUILD:226:1: C++ compilation of rule '//tensorflow/compiler/mlir/xla:hlo' failed (Exit 1)\r\nIn file included from ./tensorflow/compiler/mlir/xla/ir/hlo_ops.h:22:0,\r\n                 from tensorflow/compiler/mlir/xla/ir/hlo_ops.cc:18:\r\nexternal/local_config_mlir/include/mlir/IR/Attributes.h: In member function 'T mlir::DenseElementsAttr::getValue(llvm::ArrayRef<long unsigned int>) const [with T = mlir::IntegerAttr]':\r\nexternal/local_config_mlir/include/mlir/IR/Attributes.h:783:20: internal compiler error: in assign_temp, at function.c:968\r\n     auto castFn = [](Attribute attr) { return attr.template cast<T>(); };\r\n                    ^\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <https://gcc.gnu.org/bugs/> for instructions.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 739.561s, Critical Path: 91.36s\r\nINFO: 541 processes: 541 local.\r\nFAILED: Build did NOT complete successfully\r\n(ai) ubuntu@ip-10-0-1-71:~/tensorflow$ gcc --version\r\ngcc (Ubuntu 4.8.5-4ubuntu8~16.04.1) 4.8.5\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n```", "Any news on this issue?\r\n\r\nI did a 'git pull', installed bazel 27.1, did bazel clean, ./configure and I am getting the same error.", "Same error with gcc-9\r\n```\r\n$ gcc --version\r\ngcc (Ubuntu 9.2.1-17ubuntu1~16.04) 9.2.1 20191102\r\n```", "It appears to be using an Anaconda gcc version:\r\n```\r\nERROR: /home/ubuntu/tensorflow/tensorflow/compiler/mlir/xla/BUILD:247:1: C++ compilation of rule '//tensorflow/compiler/mlir/xla:hlo' failed (Exit 1): x86_64-conda_cos6-linux-gnu-cc failed: error executing command                             (cd /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/usr/local/cuda/lib64 \\\r\n    PATH=/home/ubuntu/anaconda2/envs/ai/bin:/home/ubuntu/apps/maven/apache-maven-3.3.x-SNAPSHOT/bin:/home/ubuntu/anaconda2/bin:/usr/local/cuda-7.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/ubuntu/caffe/build/tools:/home/ubuntu/gnulib:.:.:/usr/lib/jvm/java-8-oracle/bin:/usr/local/hadoop/bin:/usr/local/hadoop/sbin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHONPATH=:/home/ubuntu/anaconda2/envs/ai/lib/python3.6/site-packages:/home/ubuntu/opencog/opencog/python/:/home/ubuntu/opencog/build/opencog/cython:/home/ubuntu/deepdist:/home/ubuntu/anaconda2/envs/ai/lib/python3.6/site-packages:/home/ubuntu/opencog/opencog/python/:/home/ubuntu/opencog/build/opencog/cython:/home/ubuntu/deepdist \\\r\n    PYTHON_BIN_PATH=/home/ubuntu/anaconda2/envs/ai/bin/python \\\r\n    PYTHON_LIB_PATH=/home/ubuntu/anaconda2/envs/ai/lib/python3.6/site-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n  /home/ubuntu/anaconda2/envs/ai/bin/x86_64-conda_cos6-linux-gnu-cc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-py2-opt/bin/tensorflow/compiler/mlir/xla/_objs/hlo/hlo_ops.pic.d '-frandom-seed=bazel-out/k8-py2-opt/bin/tensorflow/compiler/mlir/xla/_objs/hlo/hlo_ops.pic.o' -fPIC -DLLVM_ENABLE_STATS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -DLLVM_BUILD_GLOBAL_ISEL -iquote . -iquote bazel-out/k8-py2-opt/bin -iquote external/local_config_mlir -iquote bazel-out/k8-py2-opt/bin/external/local_config_mlir -iquote external/llvm -iquote bazel-out/k8-py2-opt/bin/external/llvm -iquote external/zlib_archive -iquote bazel-out/k8-py2-opt/bin/external/zlib_archive -Ibazel-out/k8-py2-opt/bin/external/local_config_mlir/_virtual_includes/CallOpInterfacesIncGen -Ibazel-out/k8-py2-opt/bin/external/local_config_mlir/_virtual_includes/DialectSymbolRegistry -Ibazel-out/k8-py2-opt/bin/external/local_config_mlir/_virtual_includes/InferTypeOpInterfaceIncGen -Ibazel-out/k8-py2-opt/bin/external/local_config_mlir/_virtual_includes/AffineOpsIncGen -Ibazel-out/k8-py2-opt/bin/external/local_config_mlir/_virtual_includes/LoopLikeOpInterfaceIncGen -Ibazel-out/k8-py2-opt/bin/external/local_config_mlir/_virtual_includes/StandardOpsIncGen -Ibazel-out/k8-py2-opt/bin/external/local_config_mlir/_virtual_includes/LoopOpsIncGen -Ibazel-out/k8-py2-opt/bin/external/local_config_mlir/_virtual_includes/VectorOpsIncGen -isystem tensorflow/compiler/mlir/xla/include -isystem bazel-out/k8-py2-opt/bin/tensorflow/compiler/mlir/xla/include -isystem external/local_config_mlir/include -isystem bazel-out/k8-py2-opt/bin/external/local_config_mlir/include -isystem external/llvm/include -isystem bazel-out/k8-py2-opt/bin/external/llvm/include -isystem external/zlib_archive -isystem bazel-out/k8-py2-opt/bin/external/zlib_archive '-march=native' -Wno-sign-compare '-std=c++14' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/compiler/mlir/xla/ir/hlo_ops.cc -o bazel-out/k8-py2-opt/bin/tensorflow/compiler/mlir/xla/_objs/hlo/hlo_ops.pic.o)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\n[4,817 / 5,026] 3 actions running\r\nIn file included from tensorflow/compiler/mlir/xla/ir/hlo_ops.cc:113:0:\r\nbazel-out/k8-py2-opt/bin/tensorflow/compiler/mlir/xla/transforms/generated_canonicalize.inc:68:6: warning: 'void {anonymous}::populateWithGenerated(mlir::MLIRContext*, mlir::OwningRewritePatternList*)' defined but not used [-Wunused-function]\r\n void populateWithGenerated(MLIRContext *context, OwningRewritePatternList *patterns) {\r\n      ^~~~~~~~~~~~~~~~~~~~~\r\nIn file included from ./tensorflow/compiler/mlir/xla/ir/hlo_ops.h:22:0,\r\n                 from tensorflow/compiler/mlir/xla/ir/hlo_ops.cc:18:\r\nexternal/local_config_mlir/include/mlir/IR/Attributes.h: In member function 'T mlir::DenseElementsAttr::getValue(llvm::ArrayRef<long unsigned int>) const [with T = mlir::IntegerAttr]':\r\nexternal/local_config_mlir/include/mlir/IR/Attributes.h:783:20: internal compiler error: in assign_temp, at function.c:968\r\n     auto castFn = [](Attribute attr) { return attr.template cast<T>(); };\r\n                    ^\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <https://gcc.gnu.org/bugs/> for instructions.\r\n[4,817 / 5,026] 3 actions running\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 2670.079s, Critical Path: 191.81s\r\nINFO: 1583 processes: 1583 local.                                               FAILED: Build did NOT complete successfully\r\n(ai) ubuntu@ip-10-0-1-71:~/tensorflow$  /home/ubuntu/anaconda2/envs/ai/bin/x86_64-conda_cos6-linux-gnu-cc --version\r\nx86_64-conda_cos6-linux-gnu-cc (crosstool-NG 1.23.0.452-d158) 7.3.0\r\n\r\n```", "I tried setting these environment variables but bagel is still using the Anaconda version of 'gcc':\r\n```\r\n$ !env\r\nenv | grep GCC\r\nGCC_NM=/usr/bin/nm\r\nGCC_HOST_COMPILER_PREFIX=/usr/bin\r\nGCC_RANLIB=/usr/bin/ranlib\r\nGCC_HOST_COMPILER_PATH=/usr/bin/gcc\r\nGCC=/usr/bin/gcc\r\nGCC_AR=/usr/bin/ar\r\n```", "I am having the same issues and I am sure that anaconda's GCC is not being used.", "When I set CC to:\r\n\r\nexport CC=/usr/bin/g++\r\n```\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/zlib_archive/BUILD.bazel:5:1: C++ compilation of rule '@zlib_archive//:zlib' failed (Exit 1): g++ failed: error executing command\r\n  (cd /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/usr/local/cuda/lib64 \\\r\n    PATH=/home/ubuntu/anaconda2/envs/ai/bin:/home/ubuntu/apps/maven/apache-maven-3.3.x-SNAPSHOT/bin:/home/ubuntu/anaconda2/bin:/usr/local/cuda-7.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/ubuntu/caffe/build/tools:/home/ubuntu/gnulib:.:.:/usr/lib/jvm/java-8-oracle/bin:/usr/local/hadoop/bin:/usr/local/hadoop/sbin \\\r\n    PWD=/proc/self/cwd \\\r\n  /usr/bin/g++ -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/host/bin/external/zlib_archive/_objs/zlib/adler32.d '-frandom-seed=bazel-out/host/bin/external/zlib_archive/_objs/zlib/adler32.o' -iquote external/zlib_archive -iquote bazel-out/host/bin/external/zlib_archive -isystem external/zlib_archive -isystem bazel-out/host/bin/external/zlib_archive -g0 '-march=native' -Wno-shift-negative-value -DZ_HAVE_UNISTD_H -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/zlib_archive/adler32.c -o bazel-out/host/bin/external/zlib_archive/_objs/zlib/adler32.o)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nexternal/zlib_archive/adler32.c:63:25: error: 'uLong adler32_z' redeclared as different kind of symbol\r\n uLong ZEXPORT adler32_z(adler, buf, len)\r\n                         ^~~~~\r\nIn file included from external/zlib_archive/zutil.h:22:0,\r\n                 from external/zlib_archive/adler32.c:8:\r\nexternal/zlib_archive/zlib.h:1707:23: note: previous declaration 'uLong adler32_z(uLong, const Bytef*, z_size_t)'\r\n ZEXTERN uLong ZEXPORT adler32_z OF((uLong adler, const Bytef *buf,\r\n                       ^~~~~~~~~\r\nexternal/zlib_archive/adler32.c:63:25: error: 'adler' was not declared in this scope\r\n uLong ZEXPORT adler32_z(adler, buf, len)\r\n                         ^~~~~\r\nexternal/zlib_archive/adler32.c:63:32: error: 'buf' was not declared in this scope\r\n uLong ZEXPORT adler32_z(adler, buf, len)\r\n                                ^~~\r\nexternal/zlib_archive/adler32.c:63:37: error: 'len' was not declared in this scope\r\n uLong ZEXPORT adler32_z(adler, buf, len)\r\n                                     ^~~\r\nexternal/zlib_archive/adler32.c:67:1: error: expected unqualified-id before '{' token\r\n {\r\n ^\r\nexternal/zlib_archive/adler32.c:10:13: warning: 'uLong adler32_combine_(uLong, uLong, off64_t)' declared 'static' but never defined [-Wunused-function]\r\n local uLong adler32_combine_ OF((uLong adler1, uLong adler2, z_off64_t len2));\r\n\r\n```", "Add '--verbose_failures' to your bazel command to see the g++ command:\r\nE.g. -\r\n```\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n\r\n```", "I am already using `verbose_failures`", "```\r\nSUBCOMMAND: # @llvm//:scalar [action 'Compiling external/llvm/lib/Transforms/Scalar/LoopUnrollAndJamPass.cpp']\r\n(cd /home/uujjwal/.cache/bazel/_bazel_uujjwal/e8964cdbc60ece81d1ab77b05142a52e/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/misc/opt/cuda/10.0 \\\r\n    GCC_HOST_COMPILER_PATH=/misc/opt-7.4/gcc/7.3.0/bin/gcc \\\r\n    GCC_HOST_COMPILER_PREFIX=/misc/opt-7.4/gcc/7.3.0/bin \\\r\n    LD_LIBRARY_PATH=/misc/opt-7.4/gcc/7.3.0/lib64:/misc/opt/cudnn/7.4-cuda-10.0/lib64:/misc/opt/cuda/10.0/lib64/:/misc/opt/cuda/10.0/extras/CUPTI/lib64:/data/stars/user/uujjwal/collection-stars/nccl_2.3.7-1+cuda10.0_x86_64/lib:/data/stars/user/uujjwal/collection-stars/TensorRT-5.0.2.6/lib:/data/stars/user/uujjwal/collection-stars/openmpi-4.0.0/lib:/data/stars/user/uujjwal/collection-stars/mkl-dnn/lib:/data/stars/user/uujjwal/collection-stars/intel/mkl/lib/intel64:/data/stars/user/uujjwal/collection-stars/intel/lib/intel64:/data/stars/user/uujjwal/collection-stars/nccl_2.3.7-1+cuda10.0_x86_64/lib:/data/stars/user/uujjwal/collection-stars/jemalloc/lib:/data/stars/user/uujjwal/collection-stars/mxnet/lib:/data/stars/user/uujjwal/collection-stars/mxnet/lib64:/data/stars/user/uujjwal/collection-stars/nccl_2.3.7-1+cuda10.0_x86_64/lib: \\\r\n    PATH=/misc/opt-7.4/gcc/7.3.0/bin:/data/stars/user/uujjwal/collection-stars/sources/bazel-0.26.1-dist/output:/misc/opt/gcc/7.3.0/bin:/misc/opt/gcc/7.3.0/include:/misc/opt/cuda/10.0/bin:/home/uujjwal/bin:/home/uujjwal/.cargo/bin:/home/uujjwal/go/bin:/data/stars/user/uujjwal/collection-stars/git/bin:/data/stars/user/uujjwal/collection-stars/sources/bazel-0.26.1-dist/output:/home/uujjwal/bin:/data/stars/user/uujjwal/collection-stars/nasm2.14.02/bin:/data/stars/user/uujjwal/collection-stars/yasm1.3.0/bin:/home/uujjwal/anaconda3/envs/instance-segmentation-tensorflow/bin:/home/uujjwal/anaconda3/condabin:/data/stars/user/uujjwal/collection-stars/TensorRT-5.0.2.6/bin:/data/stars/user/uujjwal/collection-stars/openmpi-4.0.0/bin:/usr/lib/oar/oardodo:/usr/lib/oar/oardodo:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/uujjwal/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/home/uujjwal/anaconda3/envs/instance-segmentation-tensorflow/bin/python \\\r\n    PYTHON_LIB_PATH=/home/uujjwal/anaconda3/envs/instance-segmentation-tensorflow/lib/python3.6/site-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=5.2,6.1,7.0,7.5 \\\r\n    TF_CUDA_PATHS=/misc/opt/cuda/10.0,/misc/opt/cudnn/7.4-cuda-10.0,/data/stars/user/uujjwal/collection-stars/TensorRT-5.0.2.6,/data/stars/user/uujjwal/collection-stars/nccl_2.3.7-1+cuda10.0_x86_64 \\\r\n    TF_CUDA_VERSION=10 \\\r\n    TF_CUDNN_VERSION=7 \\\r\n    TF_NCCL_VERSION=2.3 \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_TENSORRT=1 \\\r\n    TF_TENSORRT_VERSION=5 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-py2-opt/bin/external/llvm/_objs/scalar/LoopUnrollAndJamPass.pic.d '-frandom-seed=bazel-out/k8-py2-opt/bin/external/llvm/_objs/scalar/LoopUnrollAndJamPass.pic.o' -DLLVM_ENABLE_STATS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -DLLVM_BUILD_GLOBAL_ISEL -iquote external/llvm -iquote bazel-out/k8-py2-opt/bin/external/llvm -iquote external/zlib_archive -iquote bazel-out/k8-py2-opt/bin/external/zlib_archive -isystem external/llvm/include -isystem bazel-out/k8-py2-opt/bin/external/llvm/include -isystem external/zlib_archive -isystem bazel-out/k8-py2-opt/bin/external/zlib_archive -isystem external/llvm/lib/IR -isystem bazel-out/k8-py2-opt/bin/external/llvm/lib/IR -isystem external/llvm/include/llvm/IR -isystem bazel-out/k8-py2-opt/bin/external/llvm/include/llvm/IR -isystem external/llvm/lib/Transforms/InstCombine -isystem bazel-out/k8-py2-opt/bin/external/llvm/lib/Transforms/InstCombine '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections '-march=native' -Wno-sign-compare '-std=c++14' '-D_GLIBCXX_USE_CXX11_ABI=0' -c external/llvm/lib/Transforms/Scalar/LoopUnrollAndJamPass.cpp -o bazel-out/k8-py2-opt/bin/external/llvm/_objs/scalar/LoopUnrollAndJamPass.pic.o)\r\nERROR: /local/ujjwal/tensorflow/tensorflow/compiler/mlir/xla/BUILD:246:1: C++ compilation of rule '//tensorflow/compiler/mlir/xla:hlo' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/uujjwal/.cache/bazel/_bazel_uujjwal/e8964cdbc60ece81d1ab77b05142a52e/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/misc/opt/gcc/7.3.0/lib64:/misc/opt/cudnn/7.4-cuda-10.0/lib64:/misc/opt/cuda/10.0/lib64/:/misc/opt/cuda/10.0/extras/CUPTI/lib64:/data/stars/user/uujjwal/collection-stars/nccl_2.3.7-1+cuda10.0_x86_64/lib:/data/stars/user/uujjwal/collection-stars/TensorRT-5.0.2.6/lib:/data/stars/user/uujjwal/collection-stars/openmpi-4.0.0/lib:/data/stars/user/uujjwal/collection-stars/mkl-dnn/lib:/data/stars/user/uujjwal/collection-stars/intel/mkl/lib/intel64:/data/stars/user/uujjwal/collection-stars/intel/lib/intel64:/data/stars/user/uujjwal/collection-stars/nccl_2.3.7-1+cuda10.0_x86_64/lib:/data/stars/user/uujjwal/collection-stars/jemalloc/lib:/data/stars/user/uujjwal/collection-stars/mxnet/lib:/data/stars/user/uujjwal/collection-stars/mxnet/lib64:/data/stars/user/uujjwal/collection-stars/nccl_2.3.7-1+cuda10.0_x86_64/lib: \\\r\n    PATH=/misc/opt-7.4/gcc/7.3.0/bin:/data/stars/user/uujjwal/collection-stars/sources/bazel-0.26.1-dist/output:/misc/opt/gcc/7.3.0/bin:/misc/opt/gcc/7.3.0/include:/misc/opt/cuda/10.0/bin:/home/uujjwal/bin:/home/uujjwal/.cargo/bin:/home/uujjwal/go/bin:/data/stars/user/uujjwal/collection-stars/git/bin:/data/stars/user/uujjwal/collection-stars/sources/bazel-0.26.1-dist/output:/home/uujjwal/bin:/data/stars/user/uujjwal/collection-stars/nasm2.14.02/bin:/data/stars/user/uujjwal/collection-stars/yasm1.3.0/bin:/home/uujjwal/anaconda3/envs/instance-segmentation-tensorflow/bin:/home/uujjwal/anaconda3/condabin:/data/stars/user/uujjwal/collection-stars/TensorRT-5.0.2.6/bin:/data/stars/user/uujjwal/collection-stars/openmpi-4.0.0/bin:/usr/lib/oar/oardodo:/usr/lib/oar/oardodo:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/uujjwal/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/compiler/mlir/xla/_objs/hlo/hlo_ops.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/compiler/mlir/xla/_objs/hlo/hlo_ops.pic.o' -DLLVM_ENABLE_STATS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -DLLVM_BUILD_GLOBAL_ISEL -iquote . -iquote bazel-out/host/bin -iquote external/llvm -iquote bazel-out/host/bin/external/llvm -iquote external/zlib_archive -iquote bazel-out/host/bin/external/zlib_archive -iquote external/local_config_mlir -iquote bazel-out/host/bin/external/local_config_mlir -Ibazel-out/host/bin/external/local_config_mlir/_virtual_includes/AffineOpsIncGen -Ibazel-out/host/bin/external/local_config_mlir/_virtual_includes/CallOpInterfacesIncGen -Ibazel-out/host/bin/external/local_config_mlir/_virtual_includes/DialectSymbolRegistry -Ibazel-out/host/bin/external/local_config_mlir/_virtual_includes/InferTypeOpInterfaceIncGen -Ibazel-out/host/bin/external/local_config_mlir/_virtual_includes/LoopLikeOpInterfaceIncGen -Ibazel-out/host/bin/external/local_config_mlir/_virtual_includes/StandardOpsIncGen -Ibazel-out/host/bin/external/local_config_mlir/_virtual_includes/LoopOpsIncGen -Ibazel-out/host/bin/external/local_config_mlir/_virtual_includes/VectorOpsIncGen -isystem tensorflow/compiler/mlir/xla/include -isystem bazel-out/host/bin/tensorflow/compiler/mlir/xla/include -isystem external/llvm/include -isystem bazel-out/host/bin/external/llvm/include -isystem external/zlib_archive -isystem bazel-out/host/bin/external/zlib_archive -isystem external/local_config_mlir/include -isystem bazel-out/host/bin/external/local_config_mlir/include '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 '-march=native' -g0 '-std=c++14' -c tensorflow/compiler/mlir/xla/ir/hlo_ops.cc -o bazel-out/host/bin/tensorflow/compiler/mlir/xla/_objs/hlo/hlo_ops.pic.o)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\ntensorflow/compiler/mlir/xla/ir/hlo_ops.cc: In function 'mlir::Type {anonymous}::GetBroadcastType(mlir::Builder*, mlir::Type, mlir::Type, mlir::Type, mlir::DenseIntElementsAttr)':\r\ntensorflow/compiler/mlir/xla/ir/hlo_ops.cc:740:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     for (int i = 0; i < shape_x.size(); i++) {\r\n                     ~~^~~~~~~~~~~~~~~~\r\nIn file included from tensorflow/compiler/mlir/xla/ir/hlo_ops.cc:112:0:\r\nbazel-out/host/bin/tensorflow/compiler/mlir/xla/transforms/generated_canonicalize.inc: At global scope:\r\nbazel-out/host/bin/tensorflow/compiler/mlir/xla/transforms/generated_canonicalize.inc:68:6: warning: 'void {anonymous}::populateWithGenerated(mlir::MLIRContext*, mlir::OwningRewritePatternList*)' defined but not used [-Wunused-function]\r\n void populateWithGenerated(MLIRContext *context, OwningRewritePatternList *patterns) {\r\n      ^~~~~~~~~~~~~~~~~~~~~\r\nIn file included from ./tensorflow/compiler/mlir/xla/ir/hlo_ops.h:22:0,\r\n                 from tensorflow/compiler/mlir/xla/ir/hlo_ops.cc:18:\r\nexternal/local_config_mlir/include/mlir/IR/Attributes.h: In member function 'T mlir::DenseElementsAttr::getValue(llvm::ArrayRef<long unsigned int>) const [with T = mlir::IntegerAttr]':\r\nexternal/local_config_mlir/include/mlir/IR/Attributes.h:783:20: internal compiler error: in assign_temp, at function.c:968\r\n     auto castFn = [](Attribute attr) { return attr.template cast<T>(); };\r\n                    ^\r\n0x8a4e91 assign_temp(tree_node*, int, int)\r\n\t../.././gcc/function.c:968\r\n0x7674af initialize_argument_information\r\n\t../.././gcc/calls.c:1825\r\n0x7674af expand_call(tree_node*, rtx_def*, int)\r\n\t../.././gcc/calls.c:3278\r\n0x85a4dd expand_expr_real_1(tree_node*, rtx_def*, machine_mode, expand_modifier, rtx_def**, bool)\r\n\t../.././gcc/expr.c:10889\r\n0x863caa store_expr_with_bounds(tree_node*, rtx_def*, int, bool, bool, tree_node*)\r\n\t../.././gcc/expr.c:5592\r\n0x864a0e expand_assignment(tree_node*, tree_node*, bool)\r\n\t../.././gcc/expr.c:5361\r\n0x776971 expand_call_stmt\r\n\t../.././gcc/cfgexpand.c:2656\r\n0x776971 expand_gimple_stmt_1\r\n\t../.././gcc/cfgexpand.c:3571\r\n0x776971 expand_gimple_stmt\r\n\t../.././gcc/cfgexpand.c:3737\r\n0x777b4f expand_gimple_basic_block\r\n\t../.././gcc/cfgexpand.c:5744\r\n0x77ccb6 execute\r\n\t../.././gcc/cfgexpand.c:6357\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nPlease include the complete backtrace with any bug report.\r\nSee <https://gcc.gnu.org/bugs/> for instructions.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 17.194s, Critical Path: 14.25s\r\nINFO: 164 processes: 164 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n```", "I don\u2019t see the \u2018g++\u2019 command. \r\nE.g. - \r\n```\r\n/usr/bin/g++ -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/host/bin/external/zlib_archive/_objs/zlib/adler32.d '-frandom-seed=bazel-out/host/bin/external/zlib_archive/_objs/zlib/adler32.o' -iquote external/zlib_archive -iquote bazel-out/host/bin/external/zlib_archive -isystem external/zlib_archive -isystem bazel-out/host/bin/external/zlib_archive -g0 '-march=native' -Wno-shift-negative-value -DZ_HAVE_UNISTD_H -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/zlib_archive/adler32.c -o bazel-out/host/bin/external/zlib_archive/_objs/zlib/adler32.o)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\n```", "Here is the correct one : \r\n\r\n```\r\nSUBCOMMAND: # @local_config_mlir//:Analysis [action 'Compiling external/local_config_mlir/lib/Analysis/InferTypeOpInterface.cpp', configuration: 0f73095a64644035ba855e855c65da5d]\r\n(cd /home/uujjwal/.cache/bazel/_bazel_uujjwal/e8964cdbc60ece81d1ab77b05142a52e/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/misc/opt/cuda/10.0 \\\r\n    GCC_HOST_COMPILER_PATH=/misc/opt-7.4/gcc/7.3.0/bin/g++ \\\r\n    GCC_HOST_COMPILER_PREFIX=/misc/opt-7.4/gcc/7.3.0/bin \\\r\n    LD_LIBRARY_PATH=/misc/opt/gcc/7.3.0/lib64:/misc/opt/cudnn/7.4-cuda-10.0/lib64:/misc/opt/cuda/10.0/lib64/:/misc/opt/cuda/10.0/extras/CUPTI/lib64:/data/stars/user/uujjwal/collection-stars/nccl_2.3.7-1+cuda10.0_x86_64/lib:/data/stars/user/uujjwal/collection-stars/TensorRT-5.0.2.6/lib:/data/stars/user/uujjwal/collection-stars/openmpi-4.0.0/lib:/data/stars/user/uujjwal/collection-stars/mkl-dnn/lib:/data/stars/user/uujjwal/collection-stars/intel/mkl/lib/intel64:/data/stars/user/uujjwal/collection-stars/intel/lib/intel64:/data/stars/user/uujjwal/collection-stars/nccl_2.3.7-1+cuda10.0_x86_64/lib:/data/stars/user/uujjwal/collection-stars/jemalloc/lib:/data/stars/user/uujjwal/collection-stars/mxnet/lib:/data/stars/user/uujjwal/collection-stars/mxnet/lib64:/data/stars/user/uujjwal/collection-stars/nccl_2.3.7-1+cuda10.0_x86_64/lib: \\\r\n    PATH=/home/uujjwal/bin:/misc/opt-7.4/gcc/7.3.0/bin:/data/stars/user/uujjwal/collection-stars/sources/bazel-0.26.1-dist/output:/misc/opt/gcc/7.3.0/bin:/misc/opt/gcc/7.3.0/include:/misc/opt/cuda/10.0/bin:/home/uujjwal/bin:/home/uujjwal/.cargo/bin:/home/uujjwal/go/bin:/data/stars/user/uujjwal/collection-stars/git/bin:/data/stars/user/uujjwal/collection-stars/sources/bazel-0.26.1-dist/output:/home/uujjwal/bin:/data/stars/user/uujjwal/collection-stars/nasm2.14.02/bin:/data/stars/user/uujjwal/collection-stars/yasm1.3.0/bin:/home/uujjwal/anaconda3/envs/instance-segmentation-tensorflow/bin:/home/uujjwal/anaconda3/condabin:/data/stars/user/uujjwal/collection-stars/TensorRT-5.0.2.6/bin:/data/stars/user/uujjwal/collection-stars/openmpi-4.0.0/bin:/usr/lib/oar/oardodo:/usr/lib/oar/oardodo:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/uujjwal/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/home/uujjwal/anaconda3/envs/instance-segmentation-tensorflow/bin/python \\\r\n    PYTHON_LIB_PATH=/home/uujjwal/anaconda3/envs/instance-segmentation-tensorflow/lib/python3.6/site-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=5.2,6.1,7.0,7.5 \\\r\n    TF_CUDA_PATHS=/misc/opt/cuda/10.0,/misc/opt/cudnn/7.4-cuda-10.0,/data/stars/user/uujjwal/collection-stars/nccl_2.3.7-1+cuda10.0_x86_64,/data/stars/user/uujjwal/collection-stars/TensorRT-5.0.2.6 \\\r\n    TF_CUDA_VERSION=10.0 \\\r\n    TF_CUDNN_VERSION=7.4 \\\r\n    TF_ENABLE_XLA=1 \\\r\n    TF_NCCL_VERSION=2.3 \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_TENSORRT=1 \\\r\n    TF_TENSORRT_VERSION=5 \\\r\n  /misc/opt-7.4/gcc/7.3.0/bin/gcc -MD -MF bazel-out/k8-py2-opt/bin/external/local_config_mlir/_objs/Analysis/InferTypeOpInterface.pic.d '-frandom-seed=bazel-out/k8-py2-opt/bin/external/local_config_mlir/_objs/Analysis/InferTypeOpInterface.pic.o' -DLLVM_ENABLE_STATS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -DLLVM_BUILD_GLOBAL_ISEL -iquote external/local_config_mlir -iquote bazel-out/k8-py2-opt/bin/external/local_config_mlir -iquote external/llvm -iquote bazel-out/k8-py2-opt/bin/external/llvm -iquote external/zlib_archive -iquote bazel-out/k8-py2-opt/bin/external/zlib_archive -Ibazel-out/k8-py2-opt/bin/external/local_config_mlir/_virtual_includes/AffineOpsIncGen -Ibazel-out/k8-py2-opt/bin/external/local_config_mlir/_virtual_includes/CallOpInterfacesIncGen -Ibazel-out/k8-py2-opt/bin/external/local_config_mlir/_virtual_includes/DialectSymbolRegistry -Ibazel-out/k8-py2-opt/bin/external/local_config_mlir/_virtual_includes/InferTypeOpInterfaceIncGen -Ibazel-out/k8-py2-opt/bin/external/local_config_mlir/_virtual_includes/LoopLikeOpInterfaceIncGen -Ibazel-out/k8-py2-opt/bin/external/local_config_mlir/_virtual_includes/StandardOpsIncGen -Ibazel-out/k8-py2-opt/bin/external/local_config_mlir/_virtual_includes/LoopOpsIncGen -Ibazel-out/k8-py2-opt/bin/external/local_config_mlir/_virtual_includes/VectorOpsIncGen -isystem external/local_config_mlir/include -isystem bazel-out/k8-py2-opt/bin/external/local_config_mlir/include -isystem external/llvm/include -isystem bazel-out/k8-py2-opt/bin/external/llvm/include -isystem external/zlib_archive -isystem bazel-out/k8-py2-opt/bin/external/zlib_archive '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -w '-march=native' -Wno-sign-compare '-std=c++14' '-D_GLIBCXX_USE_CXX11_ABI=0' -c external/local_config_mlir/lib/Analysis/InferTypeOpInterface.cpp -o bazel-out/k8-py2-opt/bin/external/local_config_mlir/_objs/Analysis/InferTypeOpInterface.pic.o)\r\nERROR: /local/ujjwal/tensorflow/tensorflow/compiler/mlir/xla/BUILD:250:1: C++ compilation of rule '//tensorflow/compiler/mlir/xla:hlo' failed (Exit 1)\r\nIn file included from ./tensorflow/compiler/mlir/xla/ir/hlo_ops.h:22:0,\r\n                 from tensorflow/compiler/mlir/xla/ir/hlo_ops.cc:18:\r\nexternal/local_config_mlir/include/mlir/IR/Attributes.h: In member function 'T mlir::DenseElementsAttr::getValue(llvm::ArrayRef<long unsigned int>) const [with T = mlir::IntegerAttr]':\r\nexternal/local_config_mlir/include/mlir/IR/Attributes.h:875:20: internal compiler error: in assign_temp, at function.c:968\r\n     auto castFn = [](Attribute attr) { return attr.template cast<T>(); };\r\n                    ^\r\n0x8a4e91 assign_temp(tree_node*, int, int)\r\n\t../.././gcc/function.c:968\r\n0x7674af initialize_argument_information\r\n\t../.././gcc/calls.c:1825\r\n0x7674af expand_call(tree_node*, rtx_def*, int)\r\n\t../.././gcc/calls.c:3278\r\n0x85a4dd expand_expr_real_1(tree_node*, rtx_def*, machine_mode, expand_modifier, rtx_def**, bool)\r\n\t../.././gcc/expr.c:10889\r\n0x863caa store_expr_with_bounds(tree_node*, rtx_def*, int, bool, bool, tree_node*)\r\n\t../.././gcc/expr.c:5592\r\n0x864a0e expand_assignment(tree_node*, tree_node*, bool)\r\n\t../.././gcc/expr.c:5361\r\n0x776971 expand_call_stmt\r\n\t../.././gcc/cfgexpand.c:2656\r\n0x776971 expand_gimple_stmt_1\r\n\t../.././gcc/cfgexpand.c:3571\r\n0x776971 expand_gimple_stmt\r\n\t../.././gcc/cfgexpand.c:3737\r\n0x777b4f expand_gimple_basic_block\r\n\t../.././gcc/cfgexpand.c:5744\r\n0x77ccb6 execute\r\n\t../.././gcc/cfgexpand.c:6357\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nPlease include the complete backtrace with any bug report.\r\nSee <https://gcc.gnu.org/bugs/> for instructions.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 738.260s, Critical Path: 112.10s\r\nINFO: 9280 processes: 9280 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n```", "In your case it is using`/usr/bin/g++` and not the anaconda GCC", "I switched it to see the results \n\n> On Nov 17, 2019, at 11:46 AM, indranaut <notifications@github.com> wrote:\n> \n> \ufeff\n> In your case it is using/usr/bin/g++ and not the anaconda GCC\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n", "I am hitting the same issue on TF2.1 with gcc 7.3.0. I wonder if anyone was able to figure out a solution for this?", "the same problem", "The same error on TF2.1(e5bf8de410005de06a7ff5393fafdf832ef1d4ad) wiith gcc 7.3.0", "same for me TF2.1 gcc 7.3.0.\r\n\r\n\r\n", "Looks like this is related to a GCC bug:\r\n\r\nhttps://gcc.gnu.org/bugzilla/show_bug.cgi?format=multiple&id=79085\r\n\r\nI changed to gcc7.5.0 and it works.", "any one solve this promble? ", "same for me TF2.1 gcc 7.3.0.", "@dbl001 ,\r\n\r\nIs this still an issue?\r\nCould you please update TensorFlow to the latest stable version v.2.6 and let us know if you are facing the same error. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33431\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33431\">No</a>\n"]}, {"number": 33430, "title": "estimator for tf serving error '_lower_using_switch_merge'", "body": "\r\n**System information**\r\n- OS Platform and Distribution :\r\nwin10 subsystem Ubuntu 18.04 LTS\r\n- TensorFlow version: 2.0\r\n- Python version: 3.6\r\n\r\n\r\n**Describe the current behavior**\r\nwhen I use tensorflow_model_server, I get a error:\r\n```python\r\nfailed: Internal: Node {{node dnn/zero_fraction/cond}} of type StatelessIf has '_lower_using_switch_merge' attr set but it does not support lowering.\r\n```\r\n\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport pandas as pd\r\nfrom sklearn import datasets\r\nimport tensorflow as tf\r\nimport itertools\r\n\r\nCOLUMNS = [\"crim\", \"zn\", \"indus\", \"nox\", \"rm\", \"age\",\r\n           \"dis\", \"tax\", \"ptratio\", \"medv\"]\r\n\r\ntraining_set = pd.read_csv(r'./data/boston/boston_train.csv', skipinitialspace=True, skiprows=1, names=COLUMNS)\r\ntest_set = pd.read_csv(r'./data/boston/boston_test.csv', skipinitialspace=True, skiprows=1, names=COLUMNS)\r\npredict_set = pd.read_csv(r'./data/boston/boston_predict.csv', skipinitialspace=True, skiprows=1, names=COLUMNS)\r\n\r\nFEATURES = [\"crim\", \"zn\", \"indus\", \"nox\", \"rm\",\r\n                 \"age\", \"dis\", \"tax\", \"ptratio\"]\r\nLABEL = \"medv\"\r\n\r\nfeature_cols = [tf.feature_column.numeric_column(k) for k in FEATURES]\r\n\r\nestimator = tf.estimator.LinearRegressor(    \r\n        feature_columns=feature_cols\r\n)\r\n\r\ndef get_input_fn(data_set, num_epoch=None, n_batch=128, shuffle=True):\r\n    return tf.compat.v1.estimator.inputs.pandas_input_fn(\r\n        x=pd.DataFrame({k: data_set[k].values for k in FEATURES}),\r\n        y=pd.Series(data_set[LABEL].values),\r\n        batch_size=n_batch,\r\n        num_epochs=num_epoch,\r\n        shuffle=shuffle\r\n    )\r\n\r\n\r\nestimator.train(input_fn=get_input_fn(training_set, num_epoch=None, n_batch=128, shuffle=False), steps=1000)\r\n\r\nfeature_spec = tf.feature_column.make_parse_example_spec(feature_cols)\r\nerving_input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)\r\nestimator.export_saved_model(r'./saved/boston', erving_input_receiver_fn)\r\n```\r\nafter that, when I try tf serving:\r\n```bash\r\ntensorflow_model_server --rest_api_port=8501 --model_name=saved_model --model_base_path='/mnt/c/Study/tensorflow/saved/boston'\r\n```\r\nI get this error:\r\n```bash\r\n SavedModel load for tags { serve }; Status: fail. Took 59577 microseconds.\r\nLoading servable: {name: saved_model version: 1571232515} failed: Internal: Node {{node zero_fraction/total_zero/zero_count/else/_1/zero_fraction/cond}} of type StatelessIf has '_lower_using_switch_merge' attr set but it does not support lowering.\r\n```\r\n", "comments": ["I face the same problem when serving for model from TF2.0 ,  and I found that tensorflow/serving:1.15.0-rc2 (docker image) can fix this problem .\r\n\r\nPS: At this time , tensorflow/serving:latest == tensorflow/serving:1.14.0 , so you should try specific tag 1.15.0-rc2. \r\n\r\nHope this help", "@siyi8088 \r\n\r\nThis issue is more suitable for TensorFlow Serving repo. Please post it on Serving repo from [here.](https://github.com/tensorflow/serving/issues) Thanks!\r\n", "@ravikyram\r\nOk. Thanks for your attention.", "@siyi8088 \r\n\r\nI am closing the issue here.Please, post the issue in serving repo and we can track there. Thanks!"]}, {"number": 33429, "title": "Tensorflow lite conversion problem", "body": "\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: FULLY_CONNECTED, RESHAPE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: EmptyTensorList, TensorListFromTensor, TensorListReserve, TensorListStack, While.\r\n\r\n```\r\nGraph code:\r\nself.inputs = tf.placeholder(tf.float32, [None, None, num_features], name='inputs')\r\n\r\n        # # Here we use sparse_placeholder that will generate a\r\n        # # SparseTensor required by ctc_loss op.\r\n        # # https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor\r\n        # # https://www.tensorflow.org/api_docs/python/tf/nn/ctc_loss\r\n        self.targets = tf.sparse_placeholder(tf.int32, name='targets')\r\n\r\n        # # 1d array of size [batch_size]\r\n        self.seq_len = tf.placeholder(tf.int32, [None], name='seq_len')\r\n        self.lstm_cell = tf.lite.experimental.nn.TFLiteLSTMCell(num_hidden)\r\n        self.outputs, _ =tf.lite.experimental.nn.dynamic_rnn(self.lstm_cell,self.inputs,dtype='float32')\r\n\r\n        self.shape = tf.shape(self.inputs)\r\n        self.batch_s, self.max_time_steps = self.shape[0], self.shape[1]\r\n\r\n        # Reshaping to apply the same weights over the timesteps\r\n        self.outputs = tf.reshape(self.outputs, [-1, num_hidden])\r\n\r\n        # Truncated normal with mean 0 and stdev=0.1\r\n        # Tip: Try another initialization\r\n        # see https://www.tensorflow.org/versions/r0.9/api_docs/python/contrib.layers.html#initializers\r\n        self.W = tf.Variable(tf.truncated_normal([num_hidden,\r\n                                             num_classes],\r\n                                            stddev=0.1))\r\n        # Zero initialization\r\n        # Tip: Is tf.zeros_initializer the same?\r\n        self.b = tf.Variable(tf.constant(0., shape=[num_classes]))\r\n\r\n        # Doing the affine projection\r\n        self.logits = tf.matmul(self.outputs, self.W) + self.b\r\n\r\n        # Reshaping back to the original shape\r\n        self.logits = tf.reshape(self.logits, [self.batch_s, -1, num_classes])\r\n\r\n        # Time major\r\n        self.logits = tf.transpose(self.logits, (1, 0, 2))\r\n\r\n        self.logits = tf.identity(self.logits,name=\"output\")\r\n\r\n        self.loss = tf.nn.ctc_loss(self.targets, self.logits, self.seq_len)\r\n        self.cost = tf.reduce_mean(self.loss)\r\n\r\n        with tf.variable_scope(\"gs\"):\r\n            self.global_step = tf.Variable(0, name='global_step', trainable=False)\r\n\r\n        # optimizer = tf.train.AdamOptimizer().minimize(cost)\r\n        # optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.9).minimize(cost)\r\n        self.optimizer = tf.train.AdamOptimizer(learning_rate=5e-4)\r\n        self.gvs = self.optimizer.compute_gradients(self.cost)\r\n        self.clipped = []\r\n        for grad, var in self.gvs:\r\n            grad = tf.clip_by_value(grad, -1., 1.)\r\n            self.clipped.append((grad, var))\r\n            self.train_op = self.optimizer.apply_gradients(self.clipped, global_step=self.global_step)\r\n\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hi,\r\n\r\nThe conversion fails because it has a dynamic rnn in your model, and the ops used by the rnn are 'TensorList' ops and while op, which is not supported by TOCO converter.\r\n\r\nWe are currently working on a new converter that could handle all these use cases. Please stay tuned!", "But I am using tf.lite.experimental.dynamic_rnn which is supported as TFLite Op as given in the documentation. Can you please check this link https://github.com/tensorflow/tensorflow/tree/r1.14/tensorflow/lite/experimental/examples/lstm/g3doc\r\nAccording to this doc we can convert dynamic_rnn to tensorflow lite if we use experimental dynamic_rnn op instead of tf.nn.dynamic_rnn.\r\nAlso please check this tutorial given by tensorflow team https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/lite/experimental/examples/lstm/TensorFlowLite_LSTM_Keras_Tutorial.ipynb. They succesfully converted to TFLite model", "Which version of tensorflow are you using?\r\n\r\ncould you try repeating the steps here:\r\nhttps://github.com/tensorflow/tensorflow/tree/r1.14/tensorflow/lite/experimental/examples/lstm/g3doc#0-turn-on-control_flow_v2\r\n\r\nCould you also share the converter command you use?", "Tensorflow Version - 1.14.\r\n\r\nFor saving the model I used simple_save() function.\r\n\r\nFor freezing the graph I used :\r\npython -m tensorflow.python.tools.freeze_graph --input_saved_model=saved_model.pb --output_node_names=\"output\"\r\n\r\nThis is the code I used to converted the frozen graph into tflite model.\r\n\r\n        graph_def_file = pbfile_path\r\n        input_arrays = [\"inputs\"]\r\n        output_arrays = [\"output\"]\r\n        converter = tf.lite.TFLiteConverter.from_frozen_graph(\r\n          graph_def_file, input_arrays, output_arrays,input_shapes = {\"inputs\":[1,100,13]})\r\n        converter.allow_custom_ops=False\r\n        converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS]\r\n        #converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n        #converter.post_training_quantize=True\r\n        #converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n        tflite_model = converter.convert()\r\n\r\n        open(\"output.tflite\", \"wb\").write(tflite_model)\r\n", "Thanks for the info.\r\n\r\nI'm wondering if there are certain corner cases caused the conversion to fail. Renjie, could you help also take a look?", "Hi, you don't need to manually freeze the graph, you can just \r\n\r\nuse converter = tf.lite.TFLiteConverter.from_session(sess, input_arrays, output_arrays,input_shapes = {\"inputs\":[1,100,13]})\r\n\r\nlike here\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/examples/lstm/g3doc#3-lets-define-the-export-to-tensorflow-lite-model-function\r\n\r\nalso please note tf.lite.experimental.nn.dynamic_rnn only accepts time_major=True (and it's the default value).", "Yeah its working I can directly convert to tensorflow lite model from session.\r\nBut what is the problem with above process I did?", "Great to know it's working. :)\r\n\r\nIf you converted from frozen graph, you need to use tf.lite.experimental.convert_op_hints_to_stubs\r\n\r\nlike here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/op_hint.py#L36-L40\r\n\r\nusing 'tf.lite.TFLiteConverter.from_session' is much simpler. :)", "Ok. Thanks for the info.", "Hey @renjie-liu  tf.lite.TFLiteConverter.from_session doesn't have any parameter as input_shapes. Can you please check it again.", "@renjie-liu   #33429 Can you please reopen this issue", "are you trying to use a different shape? from_session takes in tensors, which have shapes implicitly ", "Yes I am using different shapes. Because at training stage I am having shape of [None,None,13] which is not accepted by tflite converter as it is having shape of None in 2nd dimension.", "I see.\r\n\r\nI think you have two options:\r\n\r\n1) freeze the graph & call tf.lite.experimental.convert_op_hints_to_stubs then pass to tfliteconverter\r\n\r\n2) refactor your model building structure, yes you have [None,None,13] for training, but export a fixed-shape graph for inference ", "How to use tf.lite.experimental.convert_op_hints_to_stubs with tf.lite.TFLiteConverter. Can you give me a sample code.\r\nI will be trying the 2nd option mean while", "Second option worked for me. Its fine. It will be better if you share some details regarding how to use 1st option.", "please check here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/op_hint.py#L38-L41\r\n\r\nbut ignore the toco_convert (should be tfliteconverter)", "Ok Thanks."]}, {"number": 33428, "title": "TF2 Keras functional API error", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.6 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0 (most recent from pip)\r\n- Python version: Python 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CPU version\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nShow OperatorNotAllowedInGraphError error. \r\n\r\n**Describe the expected behavior**\r\n\r\nShow model summary.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.keras.backend.clear_session()\r\n\r\nclass Manager(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Manager, self).__init__()\r\n\r\n        self.inputs = tf.keras.Input(shape=(None,))\r\n\r\nmodel = Manager()\r\nmodel.build(input_shape=(16,16)).summary()\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py\", line 630, in build\r\n    if input_shape and not self.inputs:\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 765, in __bool__\r\n    self._disallow_bool_casting()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 534, in _disallow_bool_casting\r\n    self._disallow_in_graph_mode(\"using a `tf.Tensor` as a Python `bool`\")\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 523, in _disallow_in_graph_mode\r\n    \" this function with @tf.function.\".format(task))\r\ntensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\r\n```\r\n", "comments": ["Issue replicating for TF-2.0, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/84becccc850298e4e7e67e96512279b2/33428.ipynb) of colab.Thanks!", "Is ```tf.keras.Model``` support ```build```?", "`inputs` is a property on the `tf.keras.Model` class. Also you wouldn't need to create Input layer in the sub-classed model. It is more for the keras Functional API.\r\n\r\nPlease checkout the following guide on sub-classed models: https://www.tensorflow.org/guide/keras/custom_layers_and_models#building_models", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33428\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33428\">No</a>\n"]}, {"number": 33427, "title": "Colab TPU cannot use dataset from GCS", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS (Colab)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Colab\r\n- TensorFlow version (use command below): 1.15\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: - \r\n\r\n**Describe the current behavior**\r\nupload credential to TPU is failed. Therefore, cannot train on TPU using dataset from google cloud service. I also tried to downgrade to tf 1.14 but still not working.\r\n\r\n**Describe the expected behavior**\r\nSame code was working few days ago, also the example codes in seedbank are using same code and they were working fine. Now all are not working.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport os\r\nimport tensorflow as tf\r\n\r\nimport json\r\nbucket = 'kurnianggoro ' #@param {type:\"string\"}\r\n\r\nassert bucket, 'Must specify an existing GCS bucket name'\r\nprint('Using bucket: {}'.format(bucket))\r\n\r\n\r\nfrom google.colab import auth\r\nauth.authenticate_user()\r\n\r\nTF_MASTER = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\r\n\r\n# Upload credentials to TPU.\r\nwith tf.Session(TF_MASTER) as sess:    \r\n    with open('/content/adc.json', 'r') as f:\r\n        auth_info = json.load(f)\r\n    tf.contrib.cloud.configure_gcs(sess, credentials=auth_info)\r\n```\r\n\r\nsimilar code in the official examples of google colab also not working\r\nhttps://colab.research.google.com/github/GoogleCloudPlatform/cloudml-samples/blob/master/tpu/templates/tpu_estimator/trainer.ipynb\r\n\r\n**Other info / logs**\r\n```\r\n\r\nInternalError                             Traceback (most recent call last)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)\r\n   1364     try:\r\n-> 1365       return fn(*args)\r\n   1366     except errors.OpError as e:\r\n\r\n9 frames\r\n\r\nInternalError: From /job:tpu_worker/replica:0/task:0:\r\nThe filesystem registered under the 'gs://' scheme was not a tensorflow::RetryingGcsFileSystem*.\r\n\t [[{{node GcsConfigureCredentials}}]]\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInternalError                             Traceback (most recent call last)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)\r\n   1382                     '\\nsession_config.graph_options.rewrite_options.'\r\n   1383                     'disable_meta_optimizer = True')\r\n-> 1384       raise type(e)(node_def, op, message)\r\n   1385 \r\n   1386   def _extend_graph(self):\r\n\r\nInternalError: From /job:tpu_worker/replica:0/task:0:\r\nThe filesystem registered under the 'gs://' scheme was not a tensorflow::RetryingGcsFileSystem*.\r\n\t [[node GcsConfigureCredentials (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]\r\n```", "comments": ["Duplicate of #33426 ", "> Duplicate of #33426\r\n\r\nWell, i can access the gcs bucket, but the TPU training cannot get access since the authentication on TPU machine is broken.\r\n\r\nI guess this problem is related to this one \r\nhttps://github.com/googlecolab/colabtools/issues/808\r\nbut, i am not sure it is problem from colab or tensorflow\r\n\r\nwell..it is fixed now"]}, {"number": 33426, "title": "TPU/Colab auth to google cloud bucket failing", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.15rc\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: TPU\r\n\r\n**Describe the current behavior**\r\nCalling this code on a TPU notebook on colab generate this issue :\r\n```\r\nfrom google.colab import auth\r\nauth.authenticate_user()\r\n```\r\n\r\nWARNING:tensorflow:\r\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\n  * https://github.com/tensorflow/io (for I/O related ops)\r\nIf you depend on functionality not listed there, please file an issue.\r\n\r\n---------------------------------------------------------------------------\r\nInternalError                             Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)\r\n   1364     try:\r\n-> 1365       return fn(*args)\r\n   1366     except errors.OpError as e:\r\n\r\n9 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1349       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\r\n-> 1350                                       target_list, run_metadata)\r\n   1351 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1442                                             fetch_list, target_list,\r\n-> 1443                                             run_metadata)\r\n   1444 \r\n\r\nInternalError: From /job:tpu_worker/replica:0/task:0:\r\nThe filesystem registered under the 'gs://' scheme was not a tensorflow::RetryingGcsFileSystem*.\r\n\t [[{{node GcsConfigureCredentials}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInternalError                             Traceback (most recent call last)\r\n<ipython-input-3-1f759c1655bd> in <module>()\r\n      1 from google.colab import auth\r\n----> 2 auth.authenticate_user()\r\n\r\n/usr/local/lib/python3.6/dist-packages/google/colab/auth.py in authenticate_user(clear_output)\r\n    155         with open(_get_adc_path()) as auth_info:\r\n    156           tf.contrib.cloud.configure_gcs(\r\n--> 157               sess, credentials=_json.load(auth_info))\r\n    158   if _check_adc():\r\n    159     return\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/cloud/python/ops/gcs_config_ops.py in configure_gcs(session, credentials, block_cache, device)\r\n    180     with ops.device(device):\r\n    181       return configure(credentials, block_cache)\r\n--> 182   return configure(credentials, block_cache)\r\n    183 \r\n    184 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/cloud/python/ops/gcs_config_ops.py in configure(credentials, block_cache)\r\n    169       placeholder = array_ops.placeholder(dtypes.string)\r\n    170       op = gen_gcs_config_ops.gcs_configure_credentials(placeholder)\r\n--> 171       session.run(op, feed_dict={placeholder: credentials})\r\n    172     if block_cache:\r\n    173       op = gen_gcs_config_ops.gcs_configure_block_cache(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    954     try:\r\n    955       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 956                          run_metadata_ptr)\r\n    957       if run_metadata:\r\n    958         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1178     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1179       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1180                              feed_dict_tensor, options, run_metadata)\r\n   1181     else:\r\n   1182       results = []\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1357     if handle is None:\r\n   1358       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1359                            run_metadata)\r\n   1360     else:\r\n   1361       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)\r\n   1382                     '\\nsession_config.graph_options.rewrite_options.'\r\n   1383                     'disable_meta_optimizer = True')\r\n-> 1384       raise type(e)(node_def, op, message)\r\n   1385 \r\n   1386   def _extend_graph(self):\r\n\r\nInternalError: From /job:tpu_worker/replica:0/task:0:\r\nThe filesystem registered under the 'gs://' scheme was not a tensorflow::RetryingGcsFileSystem*.\r\n\t [[node GcsConfigureCredentials (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]\r\n\r\n\r\n**Describe the expected behavior**\r\nAuth must work\r\n\r\n**Code to reproduce the issue**\r\n```\r\nfrom google.colab import auth\r\nauth.authenticate_user()\r\n```\r\n", "comments": ["It works on GPU/CPU mode", "It seems to be an issue with the Tensorflow version < 2.x. I had the same error with 1.13.1 but changing to Tensorflow 2.0.0 solved the issue. ", "With tensorflow 2.0.0, i've got the following error : \r\n```\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-1-131e482caf09> in <module>()\r\n      2 \r\n      3 from google.colab import auth\r\n----> 4 auth.authenticate_user()\r\n\r\n/usr/local/lib/python3.6/dist-packages/google/colab/auth.py in authenticate_user(clear_output)\r\n    154       with tf.compat.v1.Session('grpc://{}'.format(colab_tpu_addr)) as sess:\r\n    155         with open(_get_adc_path()) as auth_info:\r\n--> 156           tf.contrib.cloud.configure_gcs(\r\n    157               sess, credentials=_json.load(auth_info))\r\n    158   if _check_adc():\r\n\r\nAttributeError: module 'tensorflow' has no attribute 'contrib'\r\n```", "So far as a workaround, i skip the google auth, just comment the two lines : \r\n```\r\nfrom google.colab import auth\r\nauth.authenticate_user()\r\n```\r\n\r\nwhen ever a call (like model.fit) required access to a bucket, it generates me a message like service-XYZ@cloud-tpu.iam.gserviceaccount.com doesn't have access to gs path.\r\n\r\nI just add this account as a 'reader' of the gs and it works.", "For me it doesn't show account name, just says 'Anonymous caller':\r\n`\"message\": \"Anonymous caller does not have storage.objects.get access to...`", "i got similar problem yesterday but it is work now, colab team said they fixed the problem few hours ago", "@anhmeow,\r\nLooks like the issue is resolved, can you please check [this](https://github.com/googlecolab/colabtools/issues/808#issuecomment-542910815) comment and the [issue](https://github.com/googlecolab/colabtools/issues/808) ?Thanks!", "i confirm, it's solved !"]}, {"number": 33425, "title": "Tensorflow eager execution not working with tf.math.unsorted_segment_max, Gradient output is null", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, Professional Edition\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary, installed using conda\r\n- TensorFlow version (use command below): unknown, 1.14.0\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version: 10.0, 7.6\r\n- GPU model and memory: T1000, 4GB VRAM\r\n\r\n**Describe the current behavior**\r\nWhen using tf.math.unsorted_segment_max with Tensorflow eager execution and Gradient Tape, the source code (see below) produces following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:/Projects/iotmap/py/segmented_max_error.py\", line 80, in <module>\r\n    grads = tape.gradient(loss_value, model.trainable_weights)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\", line 980, in gradient\r\n    unconnected_gradients=unconnected_gradients)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\", line 76, in imperative_grad\r\n    compat.as_str(unconnected_gradients.value))\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\", line 137, in _gradient_function\r\n    return grad_fn(mock_op, *out_grads)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\", line 349, in _UnsortedSegmentMaxGrad\r\n    return _UnsortedSegmentMinOrMaxGrad(op, grad)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\", line 326, in _UnsortedSegmentMinOrMaxGrad\r\n    _GatherDropNegatives(op.outputs[0], op.inputs[1])\r\nTypeError: 'NoneType' object is not subscriptable\r\n```\r\nOperations tf.math.segment_max, tf.math.segment_mean and tf.math.unsorted_segment_mean are working ok, though. \r\nI need the unsorted version because in more complex code bases, I a using several segmented aggregations and concatenating them, so I need to have fixed sizes.\r\n\r\n**Describe the expected behavior**\r\nIt should work without throwing error.\r\n\r\n**Code to reproduce the issue**\r\nThe code is here:\r\nhttps://gist.github.com/racinmat/9a95cac7db36d5f0b6b33e9c35678ca2\r\n\r\n**Other info / logs**\r\nException thrown is mentioned above.", "comments": ["@racinmat \r\nI tried reproducing the issue with TF 1.14 on colab. However i am seeing the different error.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/2f6897da2b30cbaba0bdd43ec9a84aa3/untitled279.ipynb). Thanks!", "Yes, my bad, it is fixed now in the gist and produces the abovementioned error.\r\nHere is the google colab notebook with fixed code: https://colab.research.google.com/gist/racinmat/057bb526253484884f3f484f62cb1f0a/untitled279.ipynb", "I have tried on colab with TF 1.14,1.15.0-rc3 and was able to reproduce the issue\r\nPlease, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/c69ad2af68dbb3ff0747b0d4ed0e7df4/untitled279.ipynb#scrollTo=YVFBL130Y2w7). Thanks!", "Just a small question, should this still have the awaiting response label?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33425\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33425\">No</a>\n", "I see the issue is fixed in master, will there be a 1.X version with the fix released sometimes?", "I'm seeing this same problem in TensorFlow 2.1.  My model uses `tf.math.unsorted_segment_max()`.  When I call `tape.gradient()` in eager mode I get the error\r\n\r\n```\r\n  File \"/Users/peastman/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\", line 1014, in gradient\r\n    unconnected_gradients=unconnected_gradients)\r\n  File \"/Users/peastman/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/imperative_grad.py\", line 76, in imperative_grad\r\n    compat.as_str(unconnected_gradients.value))\r\n  File \"/Users/peastman/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\", line 138, in _gradient_function\r\n    return grad_fn(mock_op, *out_grads)\r\n  File \"/Users/peastman/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py\", line 455, in _UnsortedSegmentMaxGrad\r\n    return _UnsortedSegmentMinOrMaxGrad(op, grad)\r\n  File \"/Users/peastman/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py\", line 432, in _UnsortedSegmentMinOrMaxGrad\r\n    _GatherDropNegatives(op.outputs[0], op.inputs[1])\r\nTypeError: 'NoneType' object is not subscriptable\r\n```\r\n\r\nWhich versions is this supposed to be fixed in?", "The issue is still not resolved in v1.15.2, @ravikyram  why has it been closed? The bug is still there.", "Also, in TensorFlow 2.1 it appears this is no longer restricted to eager mode.  Even if I wrap the calculation in `tf.function` it still fails.  Can someone reopen this so it will get fixed?", "Actually I cannot reproduce this against nightly TF, so I think it has been fixed since. Sorry for the noise.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33425\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33425\">No</a>\n", "@ravikyram This problem still happened in tensorflow-gpu==2.0.0. But, it's ok in tensorflow-gpu==2.1.0. "]}, {"number": 33424, "title": "Loading optimizer variables from checkpoint not working in tf.keras", "body": "**System information**\r\n- Custom code\r\n- Ubuntu 18\r\n- TensorFlow version 2.0.0\r\n- Python version 3.7\r\n\r\n**Describe the current behavior**\r\n\r\nRestoring a checkpoint with saved optimizer variables is not working. \r\n\r\n**Describe the expected behavior**\r\n\r\nWhen saving a checkpoint using `tf.keras.callbacks.ModelCheckpoint(..., save_weights_only=False)`, restoring the checkpoint should also recover the optimizer variables.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nmodel = get_model()\r\nmodel.fit(\r\n    train_images,\r\n    train_labels,\r\n    epochs=1,\r\n    callbacks=[tf.keras.callbacks.ModelCheckpoint(filepath=model_path, save_weights_only=False)]\r\n)\r\nw0 = model.trainable_weights[0].numpy()\r\nopt_w0 = model.optimizer.weights[1].numpy()\r\n\r\ntf.keras.backend.clear_session()\r\n\r\n# this throws an error when `save_weights_only=True`. If `save_weights_only=False` it doesn't restore optimizer variables\r\nmodel = tf.keras.models.load_model(model_path)\r\n\r\n# this throws an error when `save_weights_only=False`.\r\nmodel = get_model()\r\nmodel.load_weights(model_path)\r\n\r\nw1 = model.trainable_weights[0].numpy()\r\ntry:\r\n    opt_w1 = model.optimizer.weights[1].numpy()\r\nexcept:\r\n    opt_w1 = None\r\n    \r\nprint(f'Weights correctly loaded: {np.all(w0 == w1)}') \r\nprint(f'Optimizer weights correctly loaded: {np.all(opt_w0 == opt_w1) if opt_w1 is not None else False}') \r\n```\r\n\r\n**Outputs**\r\n\r\n`save_weights_only=True` with `tf.keras.load_model()`:\r\n\r\n```\r\nOSError: SavedModel file does not exist at: /tmp/m79804149/model/{saved_model.pbtxt|saved_model.pb}\r\n```\r\n\r\n`save_weights_only=False` with `tf.keras.load_model()`:\r\n\r\n```\r\nTrain on 60000 samples\r\n59136/60000 [============================>.] - ETA: 0s - loss: 0.4933 - accuracy: 0.8269INFO:tensorflow:Assets written to: /tmp/m41299667/model/assets\r\n60000/60000 [==============================] - 3s 55us/sample - loss: 0.4923 - accuracy: 0.8273\r\nWeights correctly loaded: True\r\nOptimizer weights correctly loaded: False\r\n```\r\n\r\n`save_weights_only=True` with `model.load_weights()` (expected behaviour):\r\n\r\n```\r\nTrain on 60000 samples\r\n60000/60000 [==============================] - 3s 50us/sample - loss: 0.4945 - accuracy: 0.8266\r\nWeights correctly loaded: True\r\nOptimizer weights correctly loaded: False\r\n```\r\n\r\n\r\n`save_weights_only=False` with `model.load_weights()`:\r\n\r\n```\r\nOSError: Unable to open file (file read failed: time = Wed Oct 16 13:16:20 2019\r\n, filename = '/tmp/m88520022/model', file descriptor = 79, errno = 21, error message = 'Is a directory', buf = 0x7ffd683b9c80, total read size = 8, bytes this sub-read = 8, bytes actually read = 18446744073709551615, offset = 0)\r\n```", "comments": ["@koenhelwegen,\r\nAs per the description in [doc](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint#arguments), `save_weights_only `is True only model weights will be saved else full model is saved. Please let me know if i misunderstood your issue. Thanks! ", "Yes, so I would like to set `save_weights_only=False`, but then loading the model fails or optimizer variables are not loaded (see 2nd and 4th output above).", "@koenhelwegen, Will it be possible to provide the complete code to replicate the issue. Thanks!", "Sure, here you go:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\nimport os\r\n\r\ndef get_model():\r\n    model = keras.Sequential([\r\n        keras.layers.Flatten(input_shape=(28, 28)),\r\n        keras.layers.Dense(128, activation=tf.nn.relu),\r\n        keras.layers.Dense(10, activation=tf.nn.softmax)\r\n    ])\r\n    model.compile(optimizer=tf.keras.optimizers.Adam(),\r\n                  loss='sparse_categorical_crossentropy',\r\n                  metrics=['accuracy'])\r\n    return model\r\n\r\ndef get_model_path():\r\n    model_dir = '/tmp/m' + str(np.random.randint(0, 1000000))\r\n    os.makedirs(model_dir)\r\n    model_path = os.path.join(model_dir, 'model')\r\n    return model_path\r\n\r\ndef attempt_save_and_reload(model_path, restore_method=\"model.load_weights()\", save_weights_only=False,):\r\n    assert restore_method in [\"model.load_weights()\", \"tf.keras.models.load_model()\"]\r\n    \r\n    fashion_mnist = keras.datasets.fashion_mnist\r\n    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\r\n    train_images = train_images / 255.0\r\n    test_images = test_images / 255.0\r\n    \r\n    tf.keras.backend.clear_session()\r\n    \r\n    model = get_model()\r\n    model.fit(\r\n        train_images,\r\n        train_labels,\r\n        epochs=1,\r\n        callbacks=[tf.keras.callbacks.ModelCheckpoint(filepath=model_path, save_weights_only=save_weights_only)]\r\n    )\r\n    w0 = model.trainable_weights[0].numpy()\r\n    opt_w0 = model.optimizer.weights[1].numpy()\r\n\r\n    tf.keras.backend.clear_session()\r\n\r\n    # this throws an error when `save_weights_only=True`. If `save_weights_only=False` it doesn't restore optimizer variables\r\n    if restore_method == \"tf.keras.models.load_model()\":\r\n        model = tf.keras.models.load_model(model_path)\r\n\r\n    # this throws an error when `save_weights_only=False`.\r\n    elif restore_method == \"model.load_weights()\":\r\n        model = get_model()\r\n        model.load_weights(model_path)\r\n\r\n    w1 = model.trainable_weights[0].numpy()\r\n    try:\r\n        opt_w1 = model.optimizer.weights[1].numpy()\r\n    except:\r\n        opt_w1 = None\r\n\r\n    print(f'Weights correctly loaded: {np.all(w0 == w1)}') \r\n    print(f'Optimizer weights correctly loaded: {np.all(opt_w0 == opt_w1) if opt_w1 is not None else False}')\r\n    \r\nif __name__ == '__main__':\r\n    for restore_method in [\"model.load_weights()\", \"tf.keras.models.load_model()\"]:\r\n        print('Restore method: ', restore_method)\r\n        model_path = get_model_path()\r\n        try:\r\n            attempt_save_and_reload(model_path, restore_method)\r\n        except Exception as e:\r\n            print('Exception raised: \\n', e)\r\n        print()\r\n```\r\n\r\nOutput for me:\r\n\r\n```\r\nRestore method:  model.load_weights()\r\nTrain on 60000 samples\r\n59392/60000 [============================>.] - ETA: 0s - loss: 0.5015 - accuracy: 0.8250INFO:tensorflow:Assets written to: /tmp/m2539/model/assets\r\n60000/60000 [==============================] - 3s 53us/sample - loss: 0.5008 - accuracy: 0.8252\r\nException raised: \r\n Unable to open file (file read failed: time = Fri Oct 18 14:43:23 2019\r\n, filename = '/tmp/m2539/model', file descriptor = 79, errno = 21, error message = 'Is a directory', buf = 0x7ffe7d545130, total read size = 8, bytes this sub-read = 8, bytes actually read = 18446744073709551615, offset = 0)\r\n\r\nRestore method:  tf.keras.models.load_model()\r\nTrain on 60000 samples\r\n59104/60000 [============================>.] - ETA: 0s - loss: 0.5011 - accuracy: 0.8255INFO:tensorflow:Assets written to: /tmp/m183568/model/assets\r\n60000/60000 [==============================] - 3s 56us/sample - loss: 0.5003 - accuracy: 0.8257\r\nWeights correctly loaded: True\r\nOptimizer weights correctly loaded: False\r\n```", "Could reproduce the issue with Tf 2.0.0. Please see the [gist](https://colab.sandbox.google.com/gist/gadagashwini/1dcfd9ad579df7586d62a0176a11feb6/untitled215.ipynb). Thanks!", "@koenhelwegen,\r\nThere are couple of Observations from your code. \r\n\r\n1. For you to Load the Model using `tf.keras.models.load_model()`, model should be saved using the command, `model.save` as shown in [this link](https://www.tensorflow.org/tutorials/keras/save_and_load#savedmodel_format) but you are using `Call Backs`. \r\n2. Once the Model is Saved using `model.save` and loaded using `tf.keras.models.load_model()`, the way we are extracting the Weights using the code, `model.optimizer.weights[1].numpy()` might not be correct. That might be the reason for `Optimizer weights correctly loaded: False`. \r\n\r\nPlease find the [Gist](https://colab.sandbox.google.com/gist/rmothukuru/85a439b4d65c592280f009f48e97436b/33424_loading_optimizer_vars.ipynb#scrollTo=KyaLvV9wUGPN) in which, the `Weights` are Same, when using `model.save_weights(model_path)` and  `model.load_weights(model_path)` but different when using `model.save(model_path)` and `model = tf.keras.models.load_model(model_path)`, which might be expected, as per point (2) above.\r\n\r\nPlease let me know your thought about the same. Thanks!", "@rmothukuru thanks for your response!\r\n\r\nIt seems that what I need is to use the `.h5` model format. If `model_path` in my code above points to a `.h5` file both weights and optimizer variables are restored correctly. This works fine with `tf.keras.callbacks.ModelCheckpoint(model_path, save_weights_only=False)` on both single and multi-gpu.\r\n\r\nI looked at your gist and it seems that you are not resetting the model between calling `model.save_weights()` and `model.load_weights()`. If I insert a `model = get_model()` optimizer variables are not restored (which I suppose is expected, as save and load functions explicitly target weights). The SavedModel format seems broken to me, but I'm happy to rely on the .h5 format for now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33424\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33424\">No</a>\n", "This appears to be an issue when trying to use ModelCheckpoint with distributed models. Specifically, the ModelCheckpoint callback only supports h5 format, but the h5 format cannot save the distribution strategy properly for resuming training later. Using \"with strategy.scope(): load_model(XXX)\" doesn't work either. One partial (but not good) workaround would be to rebuild the model, then set its weights from a saved checkpoint. This restores the scope as desired, but loses the optimizer state.", "From this post, I've understood that by model.save(file.h5), and loading the model again solves the problem of loading of the Optimizer weights. \r\nHowever, this contradicts with my case:\r\n**Code to Reproduce:**\r\n\r\n\r\n```\r\ndef get_model():\r\n    model = keras.Sequential([\r\n        keras.layers.Flatten(input_shape=(28, 28)),\r\n        keras.layers.Dense(128, activation=tf.nn.relu),\r\n        keras.layers.Dense(10, activation=tf.nn.softmax)\r\n    ])\r\n    model.compile(optimizer=tf.keras.optimizers.Adam(),\r\n                  loss='sparse_categorical_crossentropy',\r\n                  metrics=['accuracy'])\r\n    return model\r\n\r\nmodel = get_model()\r\n\r\nsteps_per_epoch = round(len(train_labels))//BATCH_SIZE\r\nval_steps = 20\r\n\r\nbaseline_history = model.fit(\r\n    train_data.repeat(),\r\n    epochs=60,\r\n    steps_per_epoch = steps_per_epoch,\r\n    callbacks = [cp_callback],\r\n    validation_data=val_data.repeat(),\r\n    validation_steps=val_steps,\r\n    verbose=1\r\n    )\r\n\r\n\r\nmodel.save('model_03Mar.h5')\r\n\r\n\r\nresampled_model = tf.keras.models.load_model(\"model_03Mar.h5\")\r\n\r\nfine_tune_oversample_epochs = 30\r\ninitial_epoch = 30\r\ntotal_epochs =  initial_epoch + fine_tune_oversample_epochs\r\n\r\nresampled_steps_per_epoch =int(np.ceil(2.0*neg/BATCH_SIZE))\r\n\r\nresampled_history = resampled_model.fit(\r\n    resampled_ds.repeat(),\r\n    initial_epoch = 30,\r\n    epochs=total_epochs,\r\n    steps_per_epoch=resampled_steps_per_epoch,\r\n    validation_data=val_data.repeat(),\r\n    validation_steps = 20\r\n)\r\n```\r\n\r\n**Outcome:**\r\n\r\n> W0302 10:16:43.084619 140023726221120 hdf5_format.py:192] Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\r\n\r\n```\r\nTrain for 194 steps, validate for 20 steps\r\nEpoch 31/60\r\n194/194 [==============================] - 96s 495ms/step - loss: 0.0151 - tp: 3102.0000 - fp: 13.0000 - tn: 3078.0000 - fn: 15.0000 - accuracy: 0.9955 - precision: 0.9958 - recall: 0.9952 - auc: 0.9997 - val_loss: 0.5462 - val_tp: 56.0000 - val_fp: 36.0000 - val_tn: 522.0000 - val_fn: 22.0000 - val_accuracy: 0.9088 - val_precision: 0.6087 - val_recall: 0.7179 - val_auc: 0.8619\r\n\r\n```\r\nAnd then training continues, I am not able to understand why the optimizer weights are not loaded."]}, {"number": 33423, "title": "\"Tensor.op is meaningless when eager execution is enabled\",  when using tf.compat.v1 RNN cells", "body": "\r\n**System information**\r\n- Have I written custom code : Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): openSUSE Leap 15.0\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from : binary\r\n- TensorFlow version: v2.0.0-rc2-26-g64c3d38( 2.0.0)\r\n- Python version: Python 3.6\r\n- Bazel version (if compiling from source): - \r\n- GCC/Compiler version (if compiling from source): - \r\n- CUDA/cuDNN version: CUDA Version 10.2\r\n- GPU model and memory: GeForce RTX 2080\r\n\r\n**Describe the current behavior**\r\n[The final goal is convert the saved_model to .tflite to port to an Android device. My model must use LSTM layers. My references are from : [Tensorflow Lite Experimental Examples](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/examples/lstm/g3doc/README.md) and [Supported functions to convert RNNs](https://www.tensorflow.org/lite/convert/rnn) ]\r\n\r\nI have used a single BasicLSTMCell with Static RNN with compat.v1.*.\r\nMNIST dataset is used to provide a stand-alone code example.\r\n\r\n**Describe the expected behavior**\r\noperations in compat.V1 for RNNs for TF2.0 should be supported for eager execution.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nfrom tensorflow.keras import Sequential\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.layers import LSTM, Dense, Input, Layer\r\n\r\n\r\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\r\n\r\nn_hidden = 8\r\ntime_steps = x_train.shape[1]\r\nn_input = x_train.shape[2]\r\nn_classes = 10\r\nbatch_size  = 20\r\n\r\nloss_object = tf.keras.losses.CategoricalCrossentropy()\r\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\r\n\r\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\r\ntrain_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\r\n\r\nval_loss = tf.keras.metrics.Mean(name='val_loss')\r\nval_accuracy = tf.keras.metrics.CategoricalAccuracy(name='val_accuracy')\r\n\r\n\r\n@tf.function\r\ndef train_step(model, features, labels):\r\n    with tf.GradientTape() as tape:\r\n        predictions = model(features)\r\n        loss = loss_object(labels, predictions)\r\n\r\n    gradients = tape.gradient(loss, model.trainable_variables)\r\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n    train_loss(loss)\r\n    train_accuracy(labels, predictions)\r\n\r\n\r\nweights = {\r\n    'out': tf.Variable(tf.random.normal([n_hidden, n_classes]), trainable = True, dtype='float32')\r\n}\r\nbiases = {\r\n    'out': tf.Variable(tf.random.normal([n_classes]), trainable = True, dtype='float32')\r\n}\r\n\r\n\r\nclass CompatV1LSTM(tf.keras.layers.Layer):\r\n    def __init__(self, n_hidden = 8):\r\n        super(CompatV1LSTM, self).__init__()\r\n        print('Layer Init')\r\n\r\n    def call(self, x):\r\n        x = tf.dtypes.cast(x, tf.dtypes.float32, name='Converted_floats')\r\n        _X = tf.unstack(x, time_steps, 1)\r\n\r\n        print('Shape of x', x.shape)\r\n        print('Shape of input after unstack',len(_X))\r\n        print('Shape of first element', _X[0].shape)\r\n\r\n        lstm_cell = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\r\n        output, state = tf.compat.v1.nn.static_rnn(cell=lstm_cell, inputs=_X, dtype=tf.float32)\r\n\r\n        return tf.matmul(output[-1], weights['out']) + biases['out']\r\n\r\nbatch_dataset = train_dataset.batch(20)\r\nx, y = next(iter(batch_dataset))\r\nlabels = tf.one_hot(y, depth = 10)\r\n\r\nmodel = CompatV1LSTM()\r\nprint(model)\r\ntrain_step(model, x, labels)\r\nprint(train_accuracy.result())\r\nprint(train_loss.result())\r\n```\r\n\r\n**Other info / logs** : Error log :\r\n```\r\nTraceback (most recent call last):\r\n  File \"<input>\", line 1, in <module>\r\n  File \"/opt/pycharm-community-2019.1.3/helpers/pydev/_pydev_bundle/pydev_umd.py\", line 197, in runfile\r\n    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script\r\n  File \"/opt/pycharm-community-2019.1.3/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"/localdata/d1300/HAR_RW_Linse10/Models/DeployModels/tflite2.py\", line 123, in <module>\r\n    train_step(model, x, labels)\r\n  File \"/home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 520, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1822, in __call__\r\n    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n  File \"/home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2150, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2041, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 358, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 905, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nAttributeError: in converted code:\r\n    /localdata/d1300/HAR_RW_Linse10/Models/DeployModels/tflite2.py:31 train_step  *\r\n        predictions = model(features)\r\n    /home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:847 __call__\r\n        outputs = call_fn(cast_inputs, *args, **kwargs)\r\n    /localdata/d1300/HAR_RW_Linse10/Models/DeployModels/tflite2.py:118 call  *\r\n        x = self.block_1(inputs)\r\n    /home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:847 __call__\r\n        outputs = call_fn(cast_inputs, *args, **kwargs)\r\n    /localdata/d1300/HAR_RW_Linse10/Models/DeployModels/tflite2.py:87 call  *\r\n        output, state = tf.compat.v1.nn.static_rnn(cell=lstm_cell, inputs=_X, dtype=tf.float32)\r\n    /home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py:324 new_func\r\n        return func(*args, **kwargs)\r\n    /home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn.py:1438 static_rnn\r\n        (output, state) = call_cell()\r\n    /home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn.py:1425 <lambda>\r\n        call_cell = lambda: cell(input_, state)\r\n    /home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:386 __call__\r\n        self, inputs, state, scope=scope, *args, **kwargs)\r\n    /home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/layers/base.py:548 __call__\r\n        outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n    /home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:817 __call__\r\n        self._maybe_build(inputs)\r\n    /home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:2141 _maybe_build\r\n        self.build(input_shapes)\r\n    /home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/tf_utils.py:306 wrapper\r\n        output_shape = fn(instance, input_shape)\r\n    /home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:735 build\r\n        shape=[input_depth + h_depth, 4 * self._num_units])\r\n    /home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py:324 new_func\r\n        return func(*args, **kwargs)\r\n    /home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:1702 add_variable\r\n        return self.add_weight(*args, **kwargs)\r\n    /home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/layers/base.py:461 add_weight\r\n        **kwargs)\r\n    /home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:522 add_weight\r\n        aggregation=aggregation)\r\n    /home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py:744 _add_variable_with_custom_getter\r\n        **kwargs_for_getter)\r\n    /home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/ops/variable_scope.py:1504 get_variable\r\n        aggregation=aggregation)\r\n    /home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/ops/variable_scope.py:1247 get_variable\r\n        aggregation=aggregation)\r\n    /home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/ops/variable_scope.py:567 get_variable\r\n        aggregation=aggregation)\r\n    /home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/ops/variable_scope.py:519 _true_getter\r\n        aggregation=aggregation)\r\n    /home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/ops/variable_scope.py:861 _get_single_variable\r\n        tb = var.op.traceback[::-1]\r\n    /home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:555 op\r\n        return self._handle.op\r\n    /home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1080 op\r\n        \"Tensor.op is meaningless when eager execution is enabled.\")\r\n    AttributeError: Tensor.op is meaningless when eager execution is enabled.\r\n```\r\n", "comments": ["I could reproduce the issue with TF 2.0.0 on colab. Please see the [gist](https://colab.sandbox.google.com/gist/gadagashwini/fd5270ec7c179952716861d70e75853e/untitled204.ipynb). Thanks!", "The code is bit messy, but I think the error is caused by creating the lstm cell within the call body(), which is executed in the graph context(). The correct way to do it is create the cell at init(), and have a build() method, which build the weights for cell (it will run in eager context), and use the cell in the call() body, like below:\r\n\r\n```python\r\n\r\nclass CompatV1LSTM(tf.keras.layers.Layer):\r\n  def __init__(self, n_hidden = 8):\r\n    super(CompatV1LSTM, self).__init__()\r\n    self.lstm_cell = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\r\n    print('Layer Init')\r\n\r\n  def build(self, input_shape):\r\n    self.lstm_cell.build(input_shape)\r\n\r\n\r\n  def call(self, x):\r\n    x = tf.dtypes.cast(x, tf.dtypes.float32, name='Converted_floats')\r\n    _X = tf.unstack(x, time_steps, 1)\r\n\r\n    print('Shape of x', x.shape)\r\n    print('Shape of input after unstack',len(_X))\r\n    print('Shape of first element', _X[0].shape)\r\n\r\n    output, state = tf.compat.v1.nn.static_rnn(cell=self.lstm_cell,\r\n                                               inputs=_X, dtype=tf.float32)\r\n\r\n    return tf.matmul(output[-1], weights['out']) + biases['out']\r\n\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33423\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33423\">No</a>\n"]}, {"number": 33422, "title": "Specifying output_shape is not working in tf.keras Lambda Layer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version: 10.0/7\r\n\r\n**Describe the current behavior**\r\nWhen creating a keras Model using a lambda function with specified output shape, the shape is not assigned to the resulting tensor: \r\ndense_net from the example below:\r\n`<tf.Tensor 'lambda_6/Identity:0' shape=(None, None, None) dtype=float32>`\r\nIf used before another layer like Dense the error appears:\r\n`ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.`\r\n\r\n**Describe the expected behavior**\r\ndense_net should have shape information:\r\n`<tf.Tensor 'lambda_6/Identity:0' shape=(None, 10, 5) dtype=float32>`\r\n\r\n**Code to reproduce the issue**\r\n```\r\nfrom tensorflow.keras.layers import Input, Dense, Lambda\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.backend import to_dense\r\n\r\ntest_input = Input((10, 5), sparse=True)\r\ndense_net = Lambda(to_dense, output_shape=(10, 5))(test_input)\r\ntest_net = Dense(50)(dense_net)\r\n```\r\n", "comments": ["Edit: Found a workaround:\r\n\r\n```\r\nfrom tensorflow.keras.layers import Input, Dense, Lambda, Layer, Reshape\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.backend import to_dense\r\n\r\ntest_input = Input((10, 5), sparse=True)\r\ndense_net = Lambda(to_dense)(test_input)\r\nreshape_net = Reshape((10, 5))(dense_net)\r\ntest_net = Dense(50)(reshape_net)\r\n```\r\n\r\nBug still remains though.", "Issue replicating for TF-2.0, please find the [gist](https://colab.sandbox.google.com/gist/oanush/2071cc1a42c3c1991d99da279c98b051/33422.ipynb) for the same.Thanks!", "Trying a custom Layer instead of the Lambda Layer:\r\nWhile this version throws the same error:\r\n```\r\nclass ToDenseLayer(Layer):\r\n    def __init__(self, out_shape):\r\n        super(ToDenseLayer, self).__init__()\r\n        self.out_shape = out_shape\r\n\r\n    def call(self, inputs, **kwargs):\r\n        return tf.sparse.to_dense(inputs)\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return None, None, self.out_shape\r\n```\r\nthis version is working:\r\n```\r\nclass ToDenseLayer(Layer):\r\n    def __init__(self, out_shape):\r\n        super(ToDenseLayer, self).__init__()\r\n        self.out_shape = out_shape\r\n\r\n    def call(self, inputs, **kwargs):\r\n        dense_tensor = tf.sparse.to_dense(inputs)\r\n        return tf.ensure_shape(dense_tensor, [None, None, self.out_shape])\r\n```", "`output_shape` in the Lambda Layer  is used to help Keras do shape inference when in eager execution (or otherwise when shape information is not available), but it does not override TF shape inference on tensors, so it does not affect the tensor.shape attribute of the outputs.\r\n\r\nTo set the shape of a symbolic tensor with partial shape information, you should use the `set_shape` method.", "Received this error in TF 2.4.1. Used keras Lambda with tf.squeeze and `output_shape` defined, but got the error within next layer which is Dense. I checked internals of sequential model, it did not infer output shape of the Lambda layer (`output_tensor` is\r\n`<KerasTensor: shape=<unknown> dtype=float32 (created by layer 'lambda')>`). Now I look for \"clean\" solution not to speak about the workarounds.\r\n\r\nMy question - when `output_shape` is defined, and inference for output shape is unsuccessful, can the `output_shape` defined by user be used?"]}, {"number": 33421, "title": "Added CMSIS-NN specialization for int8 conv op.", "body": "Change-Id: I0b15db09fc168d8d9abee9989c0f50e1f2cd21fd", "comments": ["Rebase issue fixed. Ready for review.", "@freddan80 thanks for your contribution. I downloaded and merged your changes wih my local branch, substituted the original micro/kernels/conv.cc with your version and resolved all the dependencies (mainly the CMSIS/NN files).\r\nHowever, I see no real speedup on cortex M4 (like I would have by manually implementing my network with CMSIS_NN).\r\nTo quantize the network I did\r\n```python\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\ntflite_quant_model = converter.convert()\r\n```\r\n\r\nI also tried by providing a representative dataset as explained in [this page](https://www.tensorflow.org/lite/performance/post_training_integer_quant) in order to have a fully quantized model. In that case, however, as generated CONV operations have version=3 they cannot be executed by the tflite micro interpreter.\r\nam I missing something? How did you menaged to exploit the speedup coming from using cmsis?", "Hi @antofara ,\r\n\r\nThanks for your input! See my comments below.\r\n\r\n> @freddan80 thanks for your contribution. I downloaded and merged your changes wih my local branch, substituted the original micro/kernels/conv.cc with your version and resolved all the dependencies (mainly the CMSIS/NN files).\r\n\r\nAre you using the cmsis-nn TAG? I added instructions how to build with CMSIS-NN here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/micro/README.md\r\nunder \"CMSIS-NN optimized kernels\". Note that this is still under construction. Any feedback is appreciated.\r\n\r\n> However, I see no real speedup on cortex M4 (like I would have by manually implementing my network with CMSIS_NN).\r\n\r\nIn our internal measurements we see 2-4x speedup for the CONV kernel on an M4 system as compared to reference kernels.\r\n\r\n> To quantize the network I did\r\n> \r\n> ```python\r\n> converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\n> tflite_quant_model = converter.convert()\r\n> ```\r\n> \r\n> I also tried by providing a representative dataset as explained in [this page](https://www.tensorflow.org/lite/performance/post_training_integer_quant) in order to have a fully quantized model. In that case, however, as generated CONV operations have version=3 they cannot be executed by the tflite micro interpreter.\r\n> am I missing something? How did you menaged to exploit the speedup coming from using cmsis?\r\n\r\nI don't know about the version. However, CMSIS-NN optimizations will work only with int8 quantized networks, which yours seem to be.\r\n\r\nCheers!\r\n", "Thanks for your reply! I think I found the issue, it never passes the condition\r\n```C\r\ncase kTfLiteInt8:\r\n      return EvalQuantizedPerChannel(context, node, params, &data, input,\r\n                                     filter, bias, output, nullptr);\r\n```\r\nsince the input type is kTfLiteFloat32, thus it falls back to EvalFloat().\r\nTherefore I think my model isn't correctly quantized (I am not following any of the examples, is a new project). Have you tried your code with a different network? Do you remember how you quantized it?\r\nI tried also the tflite_convert cli tool as explained in [this tutorial](https://www.tensorflow.org/lite/convert/cmdline_examples) but it doesn't quantize at all. \r\nI am using TF2, but I also tried the converter from TF1\r\n```python\r\nconverter=tf.compat.v1.lite.TFLiteConverter.from_keras_model_file('model.h5')\r\nconverter.inference_type=tf.uint8\r\nconverter.quantized_input_stats={'input_3':(0,1)}\r\nconverter.default_ranges_stats=(-1,1)\r\nconvmodel=converter.convert()\r\n```\r\nBy doing so, the model is correctly quantized but inputs are in unsigned int format (kTfLiteUInt8), thus calling EvalQuantized (which does not use cmsis) instead of EvalQuantizedPerChannel\r\nmany thanks in advance!", "I would suggest using the following conversion parameters:\r\n\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\nconverter.inference_input_type = tf.int8\r\nconverter.inference_output_type = tf.int8\r\nconverter.representative_dataset = representative_dataset_gen\r\n\r\nand define a representitive_dataset generator method.  Here's a dummy one for a simple single-input model (you should use an actual dataset representative of the data you expect, otherwise the quantization parameters will be incorrect and your model will not work well):\r\ndef representative_dataset():                                                                                                                                                                                                                                                             \r\n  calibration_inputs = []\r\n  calibration_inputs.append(np.random.rand(1, 1).astype(np.float32))\r\n  return calibration_inputs\r\n\r\ndef representative_dataset_gen():\r\n  for _ in range(100):\r\n    yield representative_dataset()\r\n\r\nThis should get you a signed fully-8-bit quantized model.  I have a change in the pipeline to fix the conv and depthwise_conv version 3 issue, but you'll see an error running this model before that fix lands.", "@antofara, I verified the implementation on an int8 version of the person detect model. Let me know what improvement you see on your model. Note that the level of optimization depends on the e.g. dimension of the input.\r\n\r\n@njeffrie, thanks for the input!", "Ready to merge"]}, {"number": 33420, "title": "Added CMSIS-NN specialization for int8 depthwise conv op.", "body": "Change-Id: Icc8b933363677eca7cc444078ff15721c734ca8f", "comments": ["@petewarden, ready for review"]}, {"number": 33419, "title": "save_weights does not work for scalars in keras subclass when using h5 format", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nI tried three cases and they all result in the same problem:\r\n\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nimport tempfile\r\nimport glob\r\n\r\nclass A(tf.keras.models.Model):\r\n    def __init__(self):\r\n        self.something = tf.keras.backend.variable(name='something', value=1.0)\r\n        super().__init__()\r\n\r\n        \r\nfor fmt in ['tf', 'h5']:\r\n    print(f'fmt={fmt}')\r\n    filename_a = os.path.join(tempfile.mkdtemp(), 'data_{fmt}')\r\n    filename_b = os.path.join(tempfile.mkdtemp(), 'data_{fmt}')\r\n\r\n    a = A()\r\n    a.save_weights(filename_a, save_format=fmt)\r\n    a.something.assign(1.2)\r\n    a.save_weights(filename_b, save_format=fmt)\r\n    print('trainable_variables', a.trainable_variables)\r\n\r\n    b = A()\r\n    b.load_weights(filename_b)\r\n    value = float(b.something.numpy())\r\n    check = np.abs(value - 1.2) < 1e-4\r\n    print(value, f'fmt={fmt}. class var PASS={check}. The value should be 1.2')\r\n    \r\n    value = float(b.trainable_variables[0].numpy())\r\n    check = np.abs(value - 1.2) < 1e-4\r\n    print(value, f'fmt={fmt}. trainable_variables PASS={check}. The value should be 1.2')\r\n\r\n\r\n```\r\n\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nimport tempfile\r\nimport glob\r\n\r\nclass A(tf.keras.models.Model):\r\n    def __init__(self):\r\n        self.something = tf.Variable(1.0, dtype='float32', trainable=True)\r\n        super().__init__()\r\n\r\n        \r\nfor fmt in ['tf', 'h5']:\r\n    print(f'fmt={fmt}')\r\n    filename_a = os.path.join(tempfile.mkdtemp(), 'data_{fmt}')\r\n    filename_b = os.path.join(tempfile.mkdtemp(), 'data_{fmt}')\r\n\r\n    a = A()\r\n    a.save_weights(filename_a, save_format=fmt)\r\n    a.something.assign(1.2)\r\n    a.save_weights(filename_b, save_format=fmt)\r\n    print('trainable_variables', a.trainable_variables)\r\n\r\n    b = A()\r\n    b.load_weights(filename_b)\r\n    value = float(b.something.numpy())\r\n    check = np.abs(value - 1.2) < 1e-4\r\n    print(value, f'fmt={fmt}. class var PASS={check}. The value should be 1.2')\r\n    \r\n    value = float(b.trainable_variables[0].numpy())\r\n    check = np.abs(value - 1.2) < 1e-4\r\n    print(value, f'fmt={fmt}. trainable_variables PASS={check}. The value should be 1.2')\r\n```\r\n\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nimport tempfile\r\nimport glob\r\n\r\nclass A(tf.keras.models.Model):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n    def build(self, input_shape):\r\n        self.something = self.add_weight(initializer=tf.keras.initializers.Ones(), dtype=tf.float32, shape=(1,), name='something')\r\n\r\n    def call(self, x):\r\n        return x\r\n\r\n\r\nfor fmt in ['tf', 'h5']:\r\n    print(f'fmt={fmt}')\r\n    filename_a = os.path.join(tempfile.mkdtemp(), 'data_{fmt}')\r\n    filename_b = os.path.join(tempfile.mkdtemp(), 'data_{fmt}')\r\n\r\n    a = A()\r\n    a(np.random.randn(3, 4).astype(np.float32))\r\n    a.save_weights(filename_a, save_format=fmt)\r\n    a.something.assign([1.2])\r\n    a.save_weights(filename_b, save_format=fmt)\r\n    print('trainable_variables', a.trainable_variables)\r\n\r\n    b = A()\r\n    b(np.random.randn(3, 4).astype(np.float32))\r\n    b.load_weights(filename_b)\r\n    value = float(b.something.numpy())\r\n    check = np.abs(value - 1.2) < 1e-4\r\n    print(value, f'fmt={fmt}. class var PASS={check}. The value should be 1.2')\r\n    \r\n    value = float(b.trainable_variables[0].numpy())\r\n    check = np.abs(value - 1.2) < 1e-4\r\n    print(value, f'fmt={fmt}. trainable_variables PASS={check}. The value should be 1.2')\r\n```\r\n\r\n\r\nOutput is:\r\n\r\n```\r\nfmt=tf\r\ntrainable_variables [<tf.Variable 'something:0' shape=() dtype=float32, numpy=1.2>]\r\n1.2000000476837158 fmt=tf. class var PASS=True. The value should be 1.2\r\n1.2000000476837158 fmt=tf. trainable_variables PASS=True. The value should be 1.2\r\nfmt=h5\r\ntrainable_variables [<tf.Variable 'something:0' shape=() dtype=float32, numpy=1.2>]\r\n1.0 fmt=h5. class var PASS=False. The value should be 1.2\r\n1.0 fmt=h5. trainable_variables PASS=False. The value should be 1.2\r\n```\r\n\r\n\r\nThe value of 1.2 is not coming back in the h5 case\r\n\r\n\r\n", "comments": ["I have tried on colab with TF version 2.0 and was able to reproduce the issue.Please, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/99e3b759bcb03008a038e49f8db272e4/untitled280.ipynb) Thanks!", "I think the problem is that model.layers is not model.weights.\r\n\r\nThe h5 routine uses layers not weights for some reasons:\r\n\r\n\r\n```\r\n    model_weights_group = f.create_group('model_weights')\r\n    model_layers = model.layers\r\n    save_weights_to_hdf5_group(model_weights_group, model_layers)\r\n\r\n\r\n```\r\n\r\nlayers does not have the thing:\r\n\r\n```\r\nIn [17]: w.b.weights\r\nOut[17]: [<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>]\r\n\r\nIn [18]: w.b.layers\r\nOut[18]: []\r\n```", "The following is a workaround, however I think the existing behaviour is extremely dangerous.\r\n\r\n\r\n```\r\nimport numpy as np\r\nimport os\r\nimport tensorflow as tf\r\nimport tempfile\r\nimport glob\r\n\r\nclass ScalarLayer(tf.keras.layers.Layer):\r\n    def __init__(self, value, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.value = tf.Variable(value, dtype='float32', trainable=True)\r\n\r\nclass A(tf.keras.models.Model):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.hack = ScalarLayer(1.0)\r\n        self.something = self.hack.value\r\n\r\nfor fmt in ['tf', 'h5']:\r\n    print(f'fmt={fmt}')\r\n    filename_a = os.path.join(tempfile.mkdtemp(), 'data_{fmt}')\r\n    filename_b = os.path.join(tempfile.mkdtemp(), 'data_{fmt}')\r\n\r\n    a = A()\r\n    a.save_weights(filename_a, save_format=fmt)\r\n    a.something.assign(1.2)\r\n    a.save_weights(filename_b, save_format=fmt)\r\n    print('trainable_variables', a.trainable_variables)\r\n\r\n    b = A()\r\n    b.load_weights(filename_b)\r\n    value = float(b.something.numpy())\r\n    check = np.abs(value - 1.2) < 1e-4\r\n    print(value, f'fmt={fmt}. class var PASS={check}. The value should be 1.2')\r\n\r\n    value = float(b.trainable_variables[0].numpy())\r\n    check = np.abs(value - 1.2) < 1e-4\r\n    print(value, f'fmt={fmt}. trainable_variables PASS={check}. The value should be 1.2')\r\n```\r\n\r\n\r\nProbably this should change:\r\n\r\nin tensorflow/tensorflow/tensorflow/python/keras/saving/hdf5_format.py\r\n```\r\n\r\n107     model_weights_group = f.create_group('model_weights')\r\n108     model_layers = model.layers\r\n109     save_weights_to_hdf5_group(model_weights_group, model_layers)\r\n```", "Also, using a Dense layer with no bias and calling it once with an array of dim [1, 1] and then using the kernel as a variable \"works\". ", "Was able to reproduce the issue with TF v2.2.0-rc3. Please find the gist [here](https://colab.research.google.com/gist/amahendrakar/34b87b5dae09d70149d9c7cec162e230/33419.ipynb). Thanks!", "@cottrell I think this is a stale issue. If you are still looking for solution, [here](https://colab.research.google.com/gist/jvishnuvardhan/771b76e690d26a61165af3b2980f945b/33419.ipynb) is a gist that works.\r\n\r\nTwo modifications I implemented are based on the error.\r\n1) In case of subclass model, we need to call the model before loading the weights (error was clear)\r\n2) Need to implement a call method in the subclass model\r\n\r\nPlease check [this tutorial](https://www.tensorflow.org/guide/keras/save_and_serialize#saving_loading_only_the_models_weights_values) for more information. Thanks!\r\n\r\nPlease verify once and close the issue if this resolved your issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33419\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33419\">No</a>\n"]}, {"number": 33418, "title": "(TF 2.0)My custom loss doesn't works.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0 / 7.6.0\r\n- GPU model and memory: 8G\r\n\r\n\r\n\r\nI have made my custom loss like below:\r\n```\r\ndef inner_loss(model, penalty=0.001):\r\n    for i in range(model.layers[3].trainable_variables[1].shape[3]):\r\n         for j in range(i + 1, model.layers[3].trainable_variables[1].shape[3]):\r\n            pw_kernel_i = tf.squeeze(model.layers[3].trainable_variables[1][:, :, :, i])\r\n            pw_kernel_j = tf.squeeze(model.layers[3].trainable_variables[1][:, :, :, j])\r\n            model.add_loss(lambda: penalty * tf.tensordot(pw_kernel_i, pw_kernel_j, 1))\r\n    return\r\n\r\n@tf.function\r\ndef train_one_step(model, x, y, optimizer):\r\n    with tf.GradientTape() as tape:\r\n        logits = model(x, training=True)\r\n        loss = _criterion(y_true=y, y_pred=logits)\r\n\r\n        inner_loss(model, 0.001)\r\n        loss += sum(model.losses)\r\n\r\n    grads = tape.gradient(loss, model.trainable_variables)\r\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n    return loss, logits\r\n```\r\n\r\nAnd I have this error code:\r\n\r\n**TypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  @tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\nThe graph tensor has name: Squeeze_4030:0**\r\n\r\n\r\nWhy does this error occur? \r\n\r\n", "comments": ["@jyh2378, Looks like code is incomplete to replicate the reported issue. Please provide the complete code. Thanks!", "@gadagashwini This is my full code:\r\n\r\n```\r\nimport time\r\nfrom data_pipe import *\r\n\r\ntrain_dataset, test_dataset = load_tensorflow_dataset(\"D:/DataSet/cifar10\", batch_size=128)\r\n\r\nmodel = tf.keras.Sequential([tf.keras.layers.Conv2D(32, (3, 3), strides=1, padding='same', kernel_regularizer=tf.keras.regularizers.l2(l=0.001), input_shape=(32,32,3)),\r\n                             tf.keras.layers.BatchNormalization(),\r\n                             tf.keras.layers.ReLU(),\r\n                             tf.keras.layers.SeparableConv2D(64, (3, 3), strides=1, padding='same', depthwise_regularizer=tf.keras.regularizers.l2(l=0.001)),\r\n                             tf.keras.layers.BatchNormalization(),\r\n                             tf.keras.layers.ReLU(),\r\n                             tf.keras.layers.SeparableConv2D(128, (3, 3), strides=2, padding='same', depthwise_regularizer=tf.keras.regularizers.l2(l=0.001)),\r\n                             tf.keras.layers.BatchNormalization(),\r\n                             tf.keras.layers.ReLU(),\r\n                             tf.keras.layers.SeparableConv2D(128, (3, 3), strides=1, padding='same', depthwise_regularizer=tf.keras.regularizers.l2(l=0.001)),\r\n                             tf.keras.layers.BatchNormalization(),\r\n                             tf.keras.layers.ReLU(),\r\n                             tf.keras.layers.GlobalAveragePooling2D(),\r\n                             tf.keras.layers.Dense(10)\r\n])\r\n\r\noptimizer = tf.keras.optimizers.Adam(lr=0.001)\r\n\r\n_criterion = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n\r\n\r\ndef inner_loss(model, penalty=0.001):\r\n    for i in range(model.layers[3].trainable_variables[1].shape[3]):\r\n         for j in range(i + 1, model.layers[3].trainable_variables[1].shape[3]):\r\n            pw_kernel_i = tf.squeeze(model.layers[3].trainable_variables[1][:, :, :, i])\r\n            pw_kernel_j = tf.squeeze(model.layers[3].trainable_variables[1][:, :, :, j])\r\n            model.add_loss(lambda: penalty * tf.tensordot(pw_kernel_i, pw_kernel_j, 1))\r\n    return\r\n\r\n\r\n@tf.function\r\ndef train_one_step(model, x, y, optimizer):\r\n    with tf.GradientTape() as tape:\r\n        logits = model(x, training=True)\r\n        loss = _criterion(y_true=y, y_pred=logits)\r\n\r\n        inner_loss(model, penalty=0.001)\r\n        loss += sum(model.losses)\r\n\r\n    grads = tape.gradient(loss, model.trainable_variables)\r\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n    return loss, logits\r\n\r\n\r\n@tf.function\r\ndef test_one_step(model, x, y):\r\n    logits = model(x, training=False)\r\n    loss = _criterion(y_true=y, y_pred=logits)\r\n    return loss, logits\r\n\r\n\r\ntemplete = (\"\\r[{0:3}/{1}]\\t\"\r\n            \"Time per epoch: {epoch_time:.3f}\\t\"\r\n            \"Time per batch: {batch_time:.3f}\\t\"\r\n            \"Loss: {loss:.4f}\\t\"\r\n            \"Top1 Acc: {top1:.4f}\")\r\n\r\n\r\ndef train(model, train_dataset, optimizer):\r\n    train_batch_time = tf.keras.metrics.Mean()\r\n    train_loss = tf.keras.metrics.Mean()\r\n    train_acc = tf.keras.metrics.SparseCategoricalAccuracy()\r\n\r\n    epoch_time = 0\r\n    for batch_idx, (x, y) in enumerate(train_dataset):\r\n        start_time = time.time()\r\n        loss, logits = train_one_step(model, x, y, optimizer)\r\n        train_loss(loss), train_acc(y_true=y, y_pred=logits)\r\n\r\n        elapsed_time = time.time() - start_time\r\n        train_batch_time(time.time() - start_time)\r\n        epoch_time += elapsed_time\r\n        print(templete.format(batch_idx + 1, 391,\r\n                              epoch_time=epoch_time,\r\n                              batch_time=train_batch_time.result().numpy(),\r\n                              loss=train_loss.result().numpy(),\r\n                              top1=train_acc.result().numpy()),\r\n              end='')\r\n    print(templete.format(batch_idx + 1, 391,\r\n                          epoch_time=epoch_time,\r\n                          batch_time=train_batch_time.result().numpy(),\r\n                          loss=train_loss.result().numpy(),\r\n                          top1=train_acc.result().numpy()),\r\n          end='\\n')\r\n    return\r\n\r\n\r\ndef validation(model, test_dataset):\r\n    test_batch_time = tf.keras.metrics.Mean()\r\n    test_loss = tf.keras.metrics.Mean()\r\n    test_acc = tf.keras.metrics.SparseCategoricalAccuracy()\r\n\r\n    epoch_time = 0\r\n    for batch_idx, (x, y) in enumerate(test_dataset):\r\n        start_time = time.time()\r\n        loss, logits = test_one_step(model, x, y)\r\n        test_loss(loss), test_acc(y_true=y, y_pred=logits)\r\n\r\n        elapsed_time = time.time() - start_time\r\n        test_batch_time(time.time() - start_time)\r\n        epoch_time += elapsed_time\r\n\r\n    print(\"(validation acc)\", test_acc.result().numpy())\r\n    return test_acc.result().numpy()\r\n\r\n\r\n\r\n# training!\r\nmax_acc = 0\r\nfor epoch in range(250):\r\n    if epoch % 5 == 4:\r\n        optimizer.lr = optimizer.lr * 0.9\r\n        print(optimizer.get_config())\r\n    print(\"epoch\", epoch)\r\n    train(model, train_dataset, optimizer)\r\n    val_acc = validation(model, test_dataset)\r\n    if val_acc > max_acc:\r\n        max_acc = val_acc\r\n        model.save_weights(\"saved_model/mobilenet/5/experiment_l1l2inner.h5\")\r\n        print(epoch, \"epoch model was saved\")\r\n```", "@jyh2378, Thanks for the code. \r\nI tried to replicate the issue, i am unable to import the data_pipe. \r\n`ModuleNotFoundError: No module named 'data_pipe'`. Please help us to replicate the issue. Thanks!", "@gadagashwini  Oh, 'data_pipe' is my custom loading data code.\r\nThis code return tf.data.dataset of cifar10.\r\nSo you can reproduce this by using below code:\r\n```\r\nimport tensorflow as tf\r\n(x_tr, y_tr), ( x_te, y_te) = tf.keras.datasets.cifar10.load_data()\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_tr, y_tr))\r\ntest_dataset = tf.data.Dataset.from_tensor_slices((x_te, y_te))\r\n```", "@jyh2378, I tried replicating the issue on colab but got different error message. Please take a look at the [gist](https://colab.sandbox.google.com/gist/gadagashwini/13cd9a4427adcef6e779d99074e7ce96/untitled211.ipynb). Thanks!", "@gadagashwini I have changed the code in [gist](https://colab.research.google.com/gist/jyh2378/5b4b82e6177aa3c13de4d299515bd1e6/untitled211.ipynb#scrollTo=WNIKldqaQOTa). You can look at the code. Thanks!", "Based on this [comment ](https://github.com/tensorflow/tensorflow/issues/32889#issuecomment-541985576) you have to strip @tf.function decorator from def test_one_step(model, x, y) and try running the training again and it should work. ", "@jyh2378 Did the above comment help you solve your issue?", "@gowthamkpr Sorry for late reply. Yes, it works well. But I didn't use keras.fit() method, so I think @tf.function decorator is be needed to fast training up.", "So I think your issue is caused by the model.add_loss() in the inner_loss function. Note that model is a stateful object, and the current code is adding inner loss tensor in each training steps to the model, which is not correct.\r\n\r\nThe alternative can be like below, and your model just trains fine.\r\n```python\r\ndef inner_loss(model, penalty=0.001):\r\n  losses = []\r\n  for i in range(model.layers[3].trainable_variables[1].shape[3]):\r\n    for j in range(i + 1, model.layers[3].trainable_variables[1].shape[3]):\r\n      pw_kernel_i = tf.squeeze(model.layers[3].trainable_variables[1][:, :, :, i])\r\n      pw_kernel_j = tf.squeeze(model.layers[3].trainable_variables[1][:, :, :, j])\r\n      # model.add_loss(lambda: penalty * tf.tensordot(pw_kernel_i, pw_kernel_j, 1))\r\n      losses.append(penalty * tf.tensordot(pw_kernel_i, pw_kernel_j, 1))\r\n  return losses\r\n\r\n@tf.function\r\ndef train_one_step(model, x, y, optimizer):\r\n  with tf.GradientTape() as tape:\r\n    logits = model(x, training=True)\r\n    loss = _criterion(y_true=y, y_pred=logits)\r\n\r\n    inner_losses = inner_loss(model, penalty=0.001)\r\n    loss += sum(inner_losses)\r\n    loss += sum(model.losses)\r\n```\r\n"]}, {"number": 33417, "title": "[r1.15-CherryPick]:Use correct casts to get right dimensions on s390x", "body": "This PR is exact copy of [33111](https://github.com/tensorflow/tensorflow/pull/33111) for merging the code changes in to `r1.15` branch.", "comments": ["@rthadur: We unfortunately missed the cut-off for 1.15. Unless this is a critical fix it's unlikely to be pulled into the release at the moment.", "@jaingaurav thanks for info, @shahidhs-ibm we will not be accepting this fix , thank you for your contribution."]}, {"number": 33416, "title": "TensorFlow Lite: TensorListFromTensor, TensorListReserve, TensorListStack, While", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\n- TensorFlow installed from (source or binary): Binaries from pacman\r\n- TensorFlow version (or github SHA if from source): 2.0.0\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, EXPAND_DIMS, FILL, LOGISTIC, MAXIMUM, MAX_POOL_2D, MINIMUM, MUL, PACK, PAD, RELU, RESHAPE, RESIZE_NEAREST_NEIGHBOR, SHAPE, STRIDED_SLICE, SUB, TILE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.\r\n\r\n```\r\n\r\nThe model is very similar to https://github.com/fizyr/keras-retinanet.\r\n\r\nI am converting with target specs `[tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]`, so my problem could be solved by adding TensorListFromTensor, TensorListReserve, TensorListStack, While to the whitelist [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/flex/whitelisted_flex_ops.cc) and use the standard tensorflow implementation.\r\n\r\n", "comments": ["Is it OK to just add TensorListFromTensor, TensorListReserve, TensorListStack, While to the whitelist .  Should I build it from source to get a binary version?  Just add these operators to whitelist and I don't do anything else?  These operators will use standard tensorflow implementation automatically\uff1f", "Hi,\r\n\r\nSorry for late reply. Can you try out our new TF Lite converter by setting:\r\nconverter.experimental_new_converter = True \r\n\r\nPlease use the tf-nightly pip package so that you can get this working.\r\n\r\nThanks.", "> Hi,\r\n> \r\n> Sorry for late reply. Can you try out our new TF Lite converter by setting:\r\n> converter.experimental_new_converter = True\r\n> \r\n> Please use the tf-nightly pip package so that you can get this working.\r\n> \r\n> Thanks.\r\n\r\nThanks for you reply.  I have solved it. please keep working on it. It is really a nice framework. By the way, tf1 is very diffcult to use, but tf2.0 is much bettter. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33416\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33416\">No</a>\n", "> **System information**\r\n> \r\n> * OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\n> * TensorFlow installed from (source or binary): Binaries from pacman\r\n> * TensorFlow version (or github SHA if from source): 2.0.0\r\n> \r\n> **Provide the text output from tflite_convert**\r\n> \r\n> ```\r\n> Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, EXPAND_DIMS, FILL, LOGISTIC, MAXIMUM, MAX_POOL_2D, MINIMUM, MUL, PACK, PAD, RELU, RESHAPE, RESIZE_NEAREST_NEIGHBOR, SHAPE, STRIDED_SLICE, SUB, TILE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.\r\n> ```\r\n> \r\n> The model is very similar to https://github.com/fizyr/keras-retinanet.\r\n> \r\n> I am converting with target specs `[tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]`, so my problem could be solved by adding TensorListFromTensor, TensorListReserve, TensorListStack, While to the whitelist [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/flex/whitelisted_flex_ops.cc) and use the standard tensorflow implementation.\r\n@vcarpani \r\nI am trying to convert EfficientDet in https://github.com/xuannianz/EfficientDet, which is based on fizyr/keras-retinanet in https://github.com/fizyr/keras-retinanet.\r\nMy tensorflow is 1.15.0.\r\nI have converted the model to tflite.When call interpreter.allocate_tensors(), I still get the same error as you said.\r\nMy code as follows.\r\ncustom_objects = {\r\n'BatchNormalization': layers_new.BatchNormalization,\r\n'swish' : efficientnet.get_swish(backend=keras.backend,layers=keras.layers,models=keras.models,utils=keras.utils),\r\n'FixedDropout' : efficientnet.get_dropout(backend=keras.backend,layers=keras.layers,models=keras.models,utils=keras.utils),\r\n'wBiFPNAdd' : layers_new.wBiFPNAdd,\r\n'PriorProbability' : initializers.PriorProbability,\r\n'RegressBoxes' : layers_new.RegressBoxes,\r\n'FilterDetections' : layers_new.FilterDetections,\r\n'ClipBoxes' : layers_new.ClipBoxes,\r\n'_smooth_l1' : losses.smooth_l1(),\r\n'_focal' : losses.focal(),\r\n}\r\ninput_shapes = {'input_2':[None,768,768,3],'input_5':[None,184140,4]}\r\ntf.enable_control_flow_v2()\r\nconverter = tf.lite.TFLiteConverter.from_keras_model_file('E:/object_detection/EfficientDet-region_anchor_opt_mbconv-head-ckpts/inference_ckpts/ckpts_B0_image-size-768/mbconv-se-head_1e-5_unfreeze-backbone_freeze-bn/csv_04_0.6736_0.7484.h5',\r\ncustom_objects=custom_objects,\r\ninput_shapes=input_shapes,\r\n)\r\nconverter.experimental_new_converter = True\r\nconverter.allow_custom_ops=True\r\ntflite_model = converter.convert()\r\nopen(\"E:/object_detection/EfficientDet-region_anchor_opt_mbconv-head-ckpts/tflites/ckpts_B0_image-size-768/mbconv-se-head_1e-5_unfreeze-backbone_freeze-bn/csv_04_0.6736_0.7484.tflite\", \"wb\").write(tflite_model)\r\ninterpreter = tf.lite.Interpreter(model_path=\"E:/object_detection/EfficientDet-region_anchor_opt_mbconv-head-ckpts/tflites/ckpts_B0_image-size-768/mbconv-se-head_1e-5_unfreeze-backbone_freeze-bn/csv_04_0.6736_0.7484.tflite\")\r\ninterpreter.allocate_tensors()\r\n`\r\nAnd the error is about TensorListFromTensor as follows.\r\n`\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-13-ca8eb7ec6089> in <module>()\r\n      1 # interpreter = tf.lite.Interpreter(model_content=tflite_model)\r\n----> 2 interpreter.allocate_tensors()\r\n      3 # help(tf.lite.Interpreter)\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\lite\\python\\interpreter.py in allocate_tensors(self)\r\n    242   def allocate_tensors(self):\r\n    243     self._ensure_safe()\r\n--> 244     return self._interpreter.AllocateTensors()\r\n    245 \r\n    246   def _safe_to_run(self):\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\lite\\python\\interpreter_wrapper\\tensorflow_wrap_interpreter_wrapper.py in AllocateTensors(self)\r\n    104 \r\n    105     def AllocateTensors(self):\r\n--> 106         return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\n    107 \r\n    108     def Invoke(self):\r\n\r\nRuntimeError: Encountered unresolved custom op: TensorListFromTensor.Node number 710 (TensorListFromTensor) failed to prepare.\r\n`\r\n\r\nI am new to tflite.Could you share your custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While. Thanks a lot!", "@hzk7287 Can you please help that How you have resolved it ?", "> @hzk7287 Can you please help that How you have resolved it ?\r\n\r\ntake a look at https://github.com/tensorflow/tensorflow/issues/33490 \r\nI use tensorflow2.0 ", "As a summary, here is a minimal example for tensorflow 2.0 with an RNN that produced the same error, including one line from the solution mentioned by @hzk7287 in #33490:\r\n\r\nadding `converter.allow_custom_ops = True` was enough to solve the problem in my case. However, for using the converted model with tensorflow lite, one would need to implemented the custom operations, correct? @hzk7287 would you share yours?\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.Sequential()\r\n\r\nmodel.add(tf.keras.layers.Input(shape=(1, 1,)))\r\n\r\ncell = tf.keras.layers.GRUCell(10)\r\n\r\nmodel.add(tf.keras.layers.RNN(cell))\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\r\nconverter.allow_custom_ops = True # does the trick\r\n#converter.experimental_new_converter = True # does not help\r\n\r\ntflite_model = converter.convert()\r\n```", "According to the [RNN Lite conversion documentation](https://www.tensorflow.org/lite/convert/rnn), only `static_rnn` without `sequence_length` is supported by Tensorflow Lite. According to the [static_rnn documentation](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/nn/static_rnn), `keras.layers.RNN(cell, unroll=True)` is the equivalent in Tensorflow 2.\r\n\r\nSo I added `unroll=True` and it works. However, unfortunately, the model does not work for Tensorflow Lite Micro.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.Sequential()\r\n\r\nmodel.add(tf.keras.layers.Input(shape=(1, 1,)))\r\n\r\ncell = tf.keras.layers.GRUCell(10)\r\n\r\nmodel.add(tf.keras.layers.RNN(cell, unroll=True))\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\r\ntflite_model = converter.convert()\r\n\r\n# for testing if operations are implemented by Tensorflow Lite\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\ninterpreter.allocate_tensors()\r\ninterpreter.invoke()\r\n```\r\n", "I had a similar error as above using tensorflow 2.2:\r\n\r\n```\r\nerror: failed to legalize operation 'tf.TensorListReserve'\r\n```\r\n\r\nand\r\n\r\n```\r\nerror: requires element_dtype to be 1-bit/8-bit/16-bit/32-bit/64-bit integer or 16-bit/32-bit/64-bit float type during TF Lite transformation pass\r\n```\r\n\r\nI fixed it by using:\r\n\r\n```python\r\nconverter = TFLiteConverter.from_saved_model(model_dir)\r\nconverter.experimental_new_converter = False\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.allow_custom_ops=True\r\ntflite_model = converter.convert()\r\n```\r\n\r\nCompared to the fix by @hzk7287 in https://github.com/tensorflow/tensorflow/issues/33490#issuecomment-544847214, this also disables the new experimental converter. "]}, {"number": 33415, "title": "TensorFlow now builds with Bazel 1.0", "body": "Fix cc_library targets by adding 'alwayslink = 1'\r\nso targets depending on them build with\r\n--incompatible_remove_legacy_whole_archive, which\r\nis enabled by default in Bazel 1.0.\r\n\r\nInspired by c3619ceb0200bc1eb9db7565e78bbf645e1f6cb0\r\n\r\nFixes https://github.com/tensorflow/tensorflow/issues/32835", "comments": ["ping", "I don't think `alwayslink = 1` is the right solution here. @meteorcloudy is working on enabling TF to build with Bazel 1.0.", "> I don't think `alwayslink = 1` is the right solution here.\r\n\r\nCan you explain why?", "@mihaimaruseac I think specifying `alwayslink = 1` for every cc_library that need to be force linked is the correct way to migrate for [--incompatible_remove_legacy_whole_archive](https://github.com/bazelbuild/bazel/issues/7362), @laszlocsomor is helping us to make TF build with 1.0. I'll help upgrading Bazel on CI later.", "In this case approving. I was thinking that `alwayslink = 1` will result in more bloat in the binaries, but happy to be proven wrong and learn that it is useful. Thanks.", "@laszlocsomor , I did a rerun with TF@HEAD, but the build failure doesn't seem to be fixed. \r\nhttps://buildkite.com/bazel/tensorflow/builds/3755\r\n\r\nAre we missing anything?", "Well this is embarrassing, but it seems I was actually using 0.26.1 because of the `.bazelversion` and because I used Bazelisk.\r\n\r\nLooking into failures with the real Bazel 1.1.0 now, then again with Bazel 1.0.0. I'll keep this thread updated.", "Fortunately, @scentini has also been working on this same issue and made far more progress than I did.", "> Fortunately, @scentini has also been working on this same issue and made far more progress than I did.\r\n\r\nIs that progress publicly available?", "Also would be very interested in trying this out :-)", "The commit in question is 52167ad. TF at HEAD currently builds with Bazel 1.0: https://buildkite.com/bazel/tensorflow/\r\n\r\nThere are still a couple of tests that are failing with Bazel 1.0, and I'm working on fixing them. Then, I will set --incompatible_remove_legacy_whole_archive to True in [.bazelrc](https://github.com/tensorflow/tensorflow/blob/master/.bazelrc), to prevent regression.", "Is there some kind of hotfix one can apply to compile the current releases (1.15.0 and 2.0.0) with bazel 1.0?", "@timokau : Can you patch https://github.com/tensorflow/tensorflow/pull/33415/commits/b4e690154f96a1242b4d5562cb8e59b1dcffe58b and https://github.com/tensorflow/tensorflow/commit/52167ad65887b4d5f433b719d9c3654e02daafd5 onto your source tree? I think that should do it.", "@laszlocsomor unfortunately neither of those apply on the 1.15.0 or 2.0.0 trees. Is there any other option?", "@timokau : Sorry, I'm not aware of any. Can you try resolving the merge conflicts? I just tried cherry-picking the commits onto r1.15: some conflicting targets don't yet exist (I deleted those), some files don't exist (deleted those too). There are some genuine conflicts too that I now cannot inspect closer, but the goal of these two cherry-picked commits was to add `alwayslink = 1` to `cc_library` rules that contain source files that define methods that the linker would strip without the `alwayslink`. So if you try building TensorFlow and the linker complains about missing functions, find the .cc files where they are defined, find the cc_library where those .cc files are in the srcs, and add `alwayslink = 1`. Hope that helps -- if you're still stuck, I can help again on Monday next week.", "Yeah, unfortunately I'm still stuck. I've gotten this far with cherry-picks: https://github.com/timokau/tensorflow/commit/cec52a85a9de50c03840175458e8d05cc455e17f\r\n\r\nBut the build still fails with\r\n\r\n```\r\nbazel-out/host/bin/tensorflow/contrib/ignite/gen_gen_igfs_ops_py_wrappers_cc: symbol lookup error: /build/output/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/contrib/ignite/../../../_solib_k8/_U_S_Stensorflow_Scontrib_Signite_Cgen_Ugen_Uigfs_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so.1: undefined symbol: scc_info_DoubleValue_google_2fprotobuf_2fwrappers_2eproto\r\n```\r\n\r\nand I don't know where this symbol is coming from. Any help would be much appreciated from me and maybe also other distro maintainers.\r\n", "@scentini: does this error look familiar?", "@scentini @laszlocsomor  any idea which package causes this:\r\n```\r\nERROR: /build/tensorflow/src/tensorflow-2.0.0/tensorflow/python/BUILD:2158:1: Executing genrule //tensorflow/python:parsing_ops_pygenrule failed (Exit 127)\r\nbazel-out/host/bin/tensorflow/python/gen_parsing_ops_py_wrappers_cc: symbol lookup error: /build/.cache/bazel/_bazel_builduser/539918b59a0d8a3726d6fc114e247083/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/../../_solib_k8/_U_S_Stensorflow_Spython_Cgen_Uparsing_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so.2: undefined symbol: scc_info_DoubleValue_google_2fprotobuf_2fwrappers_2eproto\r\n```\r\n? from the log it points to something in `tensorflow/python` but not sure what.", "Can you try patching [2d2d6cf](https://github.com/tensorflow/tensorflow/commit/2d2d6cffbda0fce31672beabffabb2dfacf76cd5) ?\r\nThe missing symbol is a protobuf one.\r\nI'll share a list of all the changes we made regarding `--incompatible_remove_legacy_whole_archive` later today. Note that we are still encountering issues though.", "Here it is:\r\n[c3619ce](https://github.com/tensorflow/tensorflow/commit/c3619ceb0200bc1eb9db7565e78bbf645e1f6cb0)\r\n[afb7639](https://github.com/tensorflow/tensorflow/commit/afb76393811a7e46ffab256c29116492bfd08e31)\r\n[9760bc2](https://github.com/tensorflow/tensorflow/commit/9760bc2f6a657ca58019116062b96ac6c60f1b43)\r\n[b4e6901](https://github.com/tensorflow/tensorflow/commit/b4e690154f96a1242b4d5562cb8e59b1dcffe58b)\r\n[52167ad](https://github.com/tensorflow/tensorflow/commit/52167ad65887b4d5f433b719d9c3654e02daafd5)\r\n[2d2d6cf](https://github.com/tensorflow/tensorflow/commit/2d2d6cffbda0fce31672beabffabb2dfacf76cd5)\r\n\r\nAlso, see https://github.com/tensorflow/tensorflow/pull/34044, not yet merged.", "Thank you! Unfortunately with all those patches cherry-picked (to the best of my conflict-resolution ability), the build still fails:\r\n\r\n```\r\nERROR: /build/source/tensorflow/compiler/tf2xla/ops/BUILD:25:1: Executing genrule //tensorflow/compiler/tf2xla/ops:gen_xla_ops_pygenrule failed (Exit 127)\r\n[2,139 / 2,289] 3 actions running\r\n    Compiling external/pcre/pcre_exec.c [for host]; 2s local\r\n    Compiling external/pcre/pcre_compile.c [for host]; 0s local\r\n    Compiling external/swig/Source/Modules/python.cxx [for host]; 0s local\r\nbazel-out/host/bin/tensorflow/compiler/tf2xla/ops/gen_gen_xla_ops_py_wrappers_cc: symbol lookup error: /build/output/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/compiler/tf2xla/ops/../../../../_solib_k8/_U_S_Stensorflow_Scompiler_Stf2xla_Sops_Cgen_Ugen_Uxla_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so.1: undefined symbol: _ZN10tensorflow16ProtoDebugStringB5cxx11ERKNS_16DeviceAttributesE\r\n[2,139 / 2,289] 3 actions running\r\n```\r\n\r\nShould I give up on trying to get 1.15 to build?", "@scentini thanks. I managed to port the patches you listed. On `v2.0.0` it still fails with the following error:\r\n```\r\nERROR: /build/tensorflow/src/tensorflow-2.0.0/tensorflow/python/BUILD:2212:1: Executing genrule //tensorflow/python:state_ops_pygenrule failed (Exit 127)\r\nbazel-out/host/bin/tensorflow/python/gen_state_ops_py_wrappers_cc: symbol lookup error: /build/.cache/bazel/_bazel_builduser/539918b59a0d8a3726d6fc114e247083/execroot/org_tens\r\norflow/bazel-out/host/bin/tensorflow/python/../../_solib_k8/_U_S_Stensorflow_Spython_Cgen_Ustate_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so.2: undefined s\r\nymbol: _ZN10tensorflow16ProtoDebugStringB5cxx11ERKNS_16DeviceAttributesE\r\n```\r\nIs there an easy way of identifying which `cc_library` is the culprit, so that I can put `alwayslink = 1` accordingly? Cheers", "Sometimes you can just search where the method is defined, and then find the cc_library that contains it. This case was tricky though, we managed to track it to [this](https://github.com/tensorflow/tensorflow/blob/aaa42634af9bdd316f65de64759ff31f42d97345/tensorflow/tensorflow.bzl#L2328) library. I sent out a PR internally to add `alwayslink=1`.", "In case anybody else is interested in this, with [this patch](https://github.com/timokau/nixpkgs/blob/8e382a7ca71fdbb35efe037f1bb26e021408981d/pkgs/development/python-modules/tensorflow/tf-1.15-bazel-1.0.patch) I got the 1.15.0 build to work with bazel 1.1. Thanks @scentini!"]}, {"number": 33414, "title": "Allow to specify the type of the tf.data.Dataset.range function", "body": "**System information**\r\n- TensorFlow version (you are using): *v2.0.0*\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n[`tf.data.Dataset.range`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#range) return a dataset of `tf.dtypes.int64` tensors.\r\n\r\nIt could be nice if the `range` method accept a `dtype` argument (as many other TF methods) to choose the type of the dataset elements.\r\n\r\n**Will this change the current api? How?**\r\n\r\nThis will change the API, adding an accepted `dtype` parameter to the `range` method.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAny person that uses the `Dataset.range` method to generate some index to use with `TensorArray.write` which only accept a `tf.dtypes.int32` tensor as its first argument.\r\n\r\n**Any Other info.**\r\n\r\nIt could be possible to update `TensorArray.write` to let it accept `int32` or `int64` (or even `int8`) as its `index` argument.", "comments": ["This sounds like a useful functionality and I would be happy to review a PR that implements it.", "I think this is a good issue for my first PR for TensorFlow. I'll add dtype as an argument and set `tf.dtypes.int64` as the default value.", "Thank you @ksslng . Please add me as a reviewer when you PR is ready.", "Can I contribute to this project?\r\n", "Hi @jsimsa, \r\n\r\nSince this issue has gone silent in a couple of days, I tried to solve it. I have tried adding optional output_types argument and later assigning `self._structure` variable with the corresponding dtypes in [RangeDataset class](https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/data/ops/dataset_ops.py#L2891). It worked well with the default `dtypes.int64`, but when I tried to change it into `dtypes.int32` it shows:\r\n\r\n`\r\nInvalidArgumentError: Data type mismatch at component 0: expected int32 but got int64. [Op:MakeIterator]\r\n`\r\n\r\nDid I miss something? Looking into  [_create_iterator](https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/data/ops/iterator_ops.py#L597), it should be fine since `self._element_spec = dataset.element_spec` right?\r\n\r\nThank you.\r\n\r\n[Here's a colab notebook showing the updated code and the problem.](https://colab.research.google.com/drive/1gWdPla-YnER6Z8k4qf2kpca8T096egNW)", "From what I can see, in the `_build_tensor` method, you still pass `dtypes.int64` to the `dtypes` argument (whatever the required `output_type`).\r\nI tried to update using `self.element_spec.dtype` but stumble upon another error:\r\n```\r\nAttributeError: 'RangeDataset' object has no attribute '_structure'\r\n```\r\nWhich seems strange as `self._structure` is defined in the `__init__` method...\r\nMaybe you have to investigate a little bit deeper, in the `DatasetSource` class.", "Hi @jsimsa,  this issue has gone silent for a while  .  If it is open, I would like to take on this issue. ", "Sounds good. Once you have a working PR, please add me as a reviewer.", "OK, I am on it. ", "Hi @jsimsa , I dug a little bit and found that the underlying `RangeDatasetOp` operation does not support `int32`. In fact, it does not parse `output_types` at all, yet.  If I extend `RangeDatasetOp`, then the problem can be solved?  Am I right? Thanks \r\n\r\n\r\n"]}, {"number": 33413, "title": "tflite 2.0 gpu delegate error when inputs resized", "body": "Hi,\r\n\r\nFor gpu delegate if resizeInput is called and then runForMultipleInputsOutputs, there is an exception.\r\n\r\n- Mobile Samsung S9\r\n- Mali G72 GPU\r\n- Tensorflow lite installed from binary version 2.0\r\n- Development env, android studio on fedora 29\r\n\r\nExample code:\r\n```\r\nfun resizeInput() {\r\n            val options: Interpreter.Options = Interpreter.Options()\r\n            val del = GpuDelegate()\r\n            //options.addDelegate(del)\r\n            val inter = Interpreter(AssetLoader.loadMappedBytes(\"best_model_shape_4x416x224_float32.tflite\"), options)\r\n            val sh = inter.getInputTensor(0).shape()\r\n            inter.resizeInput(0, intArrayOf(1, 224, 416, 4))\r\n            var inNumBytes = inter.getInputTensor(0).numBytes()\r\n            var inputs = Array<ByteBuffer>(1) {\r\n                ByteBuffer.allocateDirect(inNumBytes).order(ByteOrder.nativeOrder())\r\n            }\r\n            val outputs1 = Array<ByteBuffer>(inter.outputTensorCount) {\r\n                val tensor = inter.getOutputTensor(it)\r\n                ByteBuffer.allocateDirect(tensor.numBytes()).order(ByteOrder.nativeOrder())\r\n            }\r\n            val outputBuffers = outputs1.mapIndexed { index, byteBuffer -> index to byteBuffer }.toMap()\r\n\r\n            inter.modifyGraphWithDelegate(del)\r\n            inter.runForMultipleInputsOutputs(inputs, outputBuffers)\r\n            DefaultLogger.verbose { \"TfLite inference: ${inter.lastNativeInferenceDurationNanoseconds} ns\" }\r\n\r\n            inter.resetVariableTensors()\r\n            inter.resizeInput(0, intArrayOf(1, 416, 224, 4))\r\n            inNumBytes = inter.getInputTensor(0).numBytes()\r\n            inputs = Array<ByteBuffer>(1) {\r\n                ByteBuffer.allocateDirect(inNumBytes).order(ByteOrder.nativeOrder())\r\n            }\r\n            inter.runForMultipleInputsOutputs(inputs, outputBuffers)\r\n            DefaultLogger.verbose { \"After resize TfLite inference: ${inter.lastNativeInferenceDurationNanoseconds} ns\" }\r\n    }\r\n```\r\n```java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: TfLiteGpuDelegate Init: Index is out of range\r\nTfLiteGpuDelegate Prepare: delegate is not initialized\r\nNode number 70 (TfLiteGpuDelegateV2) failed to prepare.\r\n\r\nRestored previous execution plan after delegate application failure.\r\nat org.tensorflow.lite.NativeInterpreterWrapper.allocateTensors(Native Method)\r\n```\r\n\r\nIs this a bug or this sequence of operations is not supported for gpu delegate? I am using the Java API.\r\n\r\nRegards,\r\n\r\nNaveen", "comments": ["@karopawil I see two `resizeInput`.  Can you elaborate which of those are giving you the problem?", "The second one. Basically once you have set GpuDelegate and then you call resizeInput, it leads to the exception.\r\n\r\nNaveen", "The problem is not directly due to resize, but with allocate Tensor call afterwards.", "Hi, is there any update on this? Regards.", "Sorry for the late reply; I was out on a conference & business trip.\r\n\r\n`resizeInput` must be called before `modifyGraphWithDelegate`.  Tensor dimensions must be known upfront for GPU delegates, and the failure is expected.  For this to work (unless we make some bigger changes around the delegate), you would have to re-create an interpreter object.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33413\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33413\">No</a>\n", "Thanks for the clarification."]}, {"number": 33412, "title": "tf2.0 takes more than twice memory than 1.14", "body": "I have 2 environments. tf 1.14 installed for python 3.5 and tf 2 for python 3.7.\r\n\r\nI run the same code and load the same keras model in both environments. in py3.5, after model loaded, the process takes 560MB memory. while in py3.7, it takes 1.2GB.\r\n\r\nis there any way to reduce the memory for tf 2 ?", "comments": ["Could you try providing a reproducer?", "> Could you try providing a reproducer?\r\n\r\nsorry, I don't understand. how to provide a reproducer?", "@saintthor \r\nIn order to expedite the trouble-shooting process, please provide a minimal standalone code to reproduce the issue reported here. Thanks!", "> @saintthor\r\n> In order to expedite the trouble-shooting process, please provide a minimal standalone code to reproduce the issue reported here. Thanks!\r\n\r\nI tested with a simple code as:\r\n\r\n`\r\nimport tensorflow as tf\r\n\r\nfrom time import sleep\r\n\r\nModel = tf.keras.models.load_model( 'models/mo_fghijk' )\r\n\r\nprint( 'loaded' )\r\n\r\nsleep( 30 )\r\n`\r\n\r\nIt takes 13.9% memory run by py3.7 and 10.8% by py3.5. the gap is not so much as running my real code. but it is obvious.", "with psutil:\r\n```python\r\nimport psutil\r\n\r\nimport os\r\n\r\nprint( 'start', psutil.Process(os.getpid()).memory_info())\r\n\r\nimport tensorflow as tf\r\n\r\nprint( 'imported', psutil.Process(os.getpid()).memory_info())\r\n\r\nModel = tf.keras.models.load_model( 'models_1/mo_fghijk' )\r\n\r\nprint( 'loaded', psutil.Process(os.getpid()).memory_info())\r\n```\r\n\r\npy3.5\r\nstart pmem(rss=12574720, vms=46747648)\r\nimported pmem(rss=234987520, vms=1003446272)\r\nloaded pmem(rss=442376192, vms=2402095104)\r\n\r\npy3.7\r\nstart pmem(rss=11087872, vms=59379712, shared=5435392, text=2658304, lib=0, data=5623808, dirty=0)\r\nimported pmem(rss=195751936, vms=701423616, shared=90480640, text=2658304, lib=0, data=224903168, dirty=0)\r\nloaded pmem(rss=573812736, vms=2255093760, shared=98611200, text=2658304, lib=0, data=1778573312, dirty=0)\r\n\r\nwe can see after model loaded, pmem.rss increased about 380 MB in py3.7 while about 210MB in py3.5.", "In my real code, the model is not used frequently. I get the data once every 5 minutes, predict, then sleep to next time.\r\nThe model file stored on the disk is only 4MB. I hope there is a minimum loading mode to save the memory even run slower.", "@saintthor Thanks sharing the memory usage stats. However at this point we cannot reproduce the reported behavior since we don't have access to your local model.  \r\nPerhaps you can build a similar model using predefined [tf.keras.datasets](https://www.tensorflow.org/api_docs/python/tf/keras/datasets) and see if you can repro the bug or attach your model if possible. Thanks!", "> @saintthor Thanks sharing the memory usage stats. However at this point we cannot reproduce the reported behavior since we don't have access to your local model.\r\n> Perhaps you can build a similar model using predefined [tf.keras.datasets](https://www.tensorflow.org/api_docs/python/tf/keras/datasets) and see if you can repro the bug or attach your model if possible. Thanks!\r\n\r\nthanks. please test with this file.\r\n\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/3756628/model.zip)\r\n", "I was able to reproduce reported behavior TF 2.X consumes more than twice the memory taken by TF 1.X\r\n\r\nFor TF '1.15.0' [GitHub Gist](https://colab.sandbox.google.com/gist/ymodak/4d66c77e8c807b8da3614d6919da6cd2/untitled18.ipynb)\r\n```python\r\nstart pmem(rss=407224320, vms=1446768640, shared=173322240, text=3883008, lib=0, data=493367296, dirty=0)\r\nimported pmem(rss=407224320, vms=1446768640, shared=173322240, text=3883008, lib=0, data=493367296, dirty=0)\r\n```\r\n\r\nFor TF '2.1.0-dev20191029' [GitHub Gist](https://colab.sandbox.google.com/gist/ymodak/a91ea97d92eca586ac8c577db3c96fe5/untitled19.ipynb#scrollTo=x_pzvzWWTI3S)\r\n```python\r\nstart pmem(rss=154517504, vms=658960384, shared=62566400, text=3883008, lib=0, data=211365888, dirty=0)\r\nimported pmem(rss=431603712, vms=2538975232, shared=221831168, text=3883008, lib=0, data=392503296, dirty=0)\r\nloaded pmem(rss=604594176, vms=2801840128, shared=235962368, text=3883008, lib=0, data=636260352, dirty=0)\r\n```", "> I was able to reproduce reported behavior TF 2.X consumes more than twice the memory taken by TF 1.X\r\n> \r\n> For TF '1.15.0' [GitHub Gist](https://colab.sandbox.google.com/gist/ymodak/4d66c77e8c807b8da3614d6919da6cd2/untitled18.ipynb)\r\n> \r\n> ```python\r\n> start pmem(rss=407224320, vms=1446768640, shared=173322240, text=3883008, lib=0, data=493367296, dirty=0)\r\n> imported pmem(rss=407224320, vms=1446768640, shared=173322240, text=3883008, lib=0, data=493367296, dirty=0)\r\n> ```\r\n> \r\n> For TF '2.1.0-dev20191029' [GitHub Gist](https://colab.sandbox.google.com/gist/ymodak/a91ea97d92eca586ac8c577db3c96fe5/untitled19.ipynb#scrollTo=x_pzvzWWTI3S)\r\n> \r\n> ```python\r\n> start pmem(rss=154517504, vms=658960384, shared=62566400, text=3883008, lib=0, data=211365888, dirty=0)\r\n> imported pmem(rss=431603712, vms=2538975232, shared=221831168, text=3883008, lib=0, data=392503296, dirty=0)\r\n> loaded pmem(rss=604594176, vms=2801840128, shared=235962368, text=3883008, lib=0, data=636260352, dirty=0)\r\n> ```\r\nthanks.\r\n\r\nFor TF '1.15.0'\uff0cthe start and imported logs seems same. because you imported tensorflow at the beginning. We may mainly compare the imported and loaded logs. \r\n", "Related issue with reproducible code : #31871", "In 2.0, we have extended the loading and saving functionality to cover a number of use cases and to use a different format, so this is not entirely surprising. Can you clarify how the model is being saved in each case? Is the saved file the same size in each case?", "> In 2.0, we have extended the loading and saving functionality to cover a number of use cases and to use a different format, so this is not entirely surprising. Can you clarify how the model is being saved in each case? Is the saved file the same size in each case?\r\n\r\nI saved the model by tf.keras.Model.save method.\r\n\r\nI loaded the same model file in the two envs for the above testing.  thanks.", "We are facing a similar issue with some LSTM-based models we are working with. For our full model architecture, we are seeing memory usage of `2.8GB` for TF 2.x, vs `1.4GB` for TF 1.15. Combined with the memory leak issue [here](https://github.com/tensorflow/tensorflow/issues/40171), this makes TF 2.x unusable for our production models - both due to significantly higher memory usage (and cost) and the inability to delete/reload models due to the memory leaks.\r\n\r\nI've created a simple reproduction to illustrate the memory consumption issue:\r\n\r\n### Model load \r\n\r\n* simple model architecture - single `Bidirectional LSTM` layer -> `Dense` layer -> `Softmax Activation`. Save model as SavedModel (no training, just initialized weights).\r\n* loading this model into memory from SavedModel takes up to **17x** more memory in TF 2.x vs TF 1.15.\r\n  * Colab notebook: [load SavedModel TF 1.15](https://colab.research.google.com/drive/1ZLFvDZr7cVA8m8DTQxZWR-vnPB5ah7eL?usp=sharing) - memory used **14MB**\r\n  * Colab notebook: [load SavedModel TF 2.2](https://colab.research.google.com/drive/1Q7Dx-NQo9F2Ghbrmo77-l9jL_eF8VaO1?usp=sharing) - memory used **249MB**\r\n* size of SavedModel on disk is also around **17x** more for TF 2.x vs TF 1.15 (also shown in the notebooks above).\r\n* I confirmed the same order of magnitude differences for loading the SavedModels in TF-Serving.\r\n\r\n### Model save\r\n\r\nIt's worth noting that saving this model as SavedModel also sees memory usage of similar magnitude (and differential between versions) to the model load scenario.\r\n\r\nThe following notebooks illustrate this (and can be used to re-create the SavedModel files used in the model load notebooks above):\r\n\r\n* Colab notebook: [save model to SavedModel TF 1.15](https://colab.research.google.com/drive/1vV3hH2gUCdFR44sioNmpmDOh5xwxr95K?usp=sharing) - memory used **30MB**\r\n* Colab notebook: [save model to SavedModel TF 2.2](https://colab.research.google.com/drive/1QJvxORQSuG4DnRBd6pMRfWsQRKKP08n8?usp=sharing) - memory used **339MB**\r\n\r\n### General issue\r\n\r\nWhile I've shown the LSTM case here (as the memory usage differential is particularly large), this also applies to even the simplest models (such as 2 or 3 Dense layers), where I saw memory usage (and size on disk) around 2-5x more in TF 2.x vs TF 1.15.\r\n\r\ncc @frreiss @kmh4321", "@karmel @ymodak @mihaimaruseac any chance to take another look at this issue? Above ^^ I've provided more info and repro Colab notebooks. The memory usage difference seems particularly large for LSTM layers and this is impacting our production use cases for LSTM models. ", "@karmel @ymodak @mihaimaruseac ping on the above - any idea to move this forward?", "+1, we are also facing @MLnick's issue. This makes TF 2.x unusable for us as well in TF Serving due to the high memory usage.", "There seems to be a significant improvement in thein imported prem and loaded prem TF 2.x code after testing it in  Tensorflow 2.7, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/8ceadb513b0f722ad1cc31da44fc0e73/33412.ipynb) and confirm. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "> \r\n\r\nYes, In tf2.7 this issue is fixed."]}, {"number": 33411, "title": "Saving model containing sequence_numeric_column fails", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v1.12.1-15925-g2e1e8ecea2 2.1.0-dev20191015\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nSaving a keras model containing a `sequence_numeric_column` feature_column results in the error:\r\n\r\n`TypeError: Input must be a SparseTensor.`\r\n\r\nHowever, saving a model using a `sequence_categorical_column_with_*` together with `indicator_column` or `embedding_column` works as expected. For example, the following works as expected:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ncols = [\r\n    tf.feature_column.indicator_column(\r\n        tf.feature_column.sequence_categorical_column_with_vocabulary_list(\r\n            \"a\", vocabulary_list=[\"one\", \"two\"]\r\n        )\r\n    ),\r\n    tf.feature_column.embedding_column(\r\n        tf.feature_column.sequence_categorical_column_with_hash_bucket(\r\n            \"b\", hash_bucket_size=10\r\n        ),\r\n        dimension=2,\r\n    ),\r\n]\r\ninput_layers = {\r\n    \"a\": tf.keras.layers.Input(\r\n        shape=(None, 1), sparse=True, name=\"a\", dtype=\"string\"\r\n    ),\r\n    \"b\": tf.keras.layers.Input(\r\n        shape=(None, 1), sparse=True, name=\"b\", dtype=\"string\"\r\n    ),\r\n}\r\n\r\nfc_layer, _ = tf.keras.experimental.SequenceFeatures(cols)(input_layers)\r\nx = tf.keras.layers.GRU(32)(fc_layer)\r\noutput = tf.keras.layers.Dense(10)(x)\r\n\r\nmodel = tf.keras.models.Model(input_layers, output)\r\n\r\nmodel.compile(\r\n    loss=tf.keras.losses.MSE,\r\n    optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\r\n    metrics=[tf.keras.metrics.categorical_accuracy],\r\n)\r\n\r\ntf.saved_model.save(model, \"model\")\r\n```\r\n\r\n**Describe the expected behavior**\r\nIt should be possible to save a model containing all types of sequence_feature_columns.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ncols = [\r\n    tf.feature_column.sequence_numeric_column('a'),\r\n]\r\ninput_layers = {\r\n    'a':\r\n        tf.keras.layers.Input(shape=(None, 1), sparse=True, name='a'),\r\n}\r\n\r\nfc_layer, _ = tf.keras.experimental.SequenceFeatures(cols)(input_layers)\r\nx = tf.keras.layers.GRU(32)(fc_layer)\r\noutput = tf.keras.layers.Dense(10)(x)\r\n\r\nmodel = tf.keras.models.Model(input_layers, output)\r\n\r\nmodel.compile(\r\n    loss=tf.keras.losses.MSE,\r\n    optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\r\n    metrics=[tf.keras.metrics.categorical_accuracy])\r\n\r\ntf.saved_model.save(model, \"model\")\r\n```\r\n**Other info / logs**\r\nFull traceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"test2.py\", line 22, in <module>\r\n    tf.saved_model.save(model, \"model\")\r\n  File \"/Users/emla2805/.pyenv/versions/bug/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py\", line 894, in save\r\n    checkpoint_graph_view)\r\n  File \"/Users/emla2805/.pyenv/versions/bug/lib/python3.6/site-packages/tensorflow_core/python/saved_model/signature_serialization.py\", line 64, in find_function_to_export\r\n    functions = saveable_view.list_functions(saveable_view.root)\r\n  File \"/Users/emla2805/.pyenv/versions/bug/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py\", line 142, in list_functions\r\n    self._serialization_cache)\r\n  File \"/Users/emla2805/.pyenv/versions/bug/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 2414, in _list_functions_for_serialization\r\n    .list_functions_for_serialization(serialization_cache))\r\n  File \"/Users/emla2805/.pyenv/versions/bug/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/base_serialization.py\", line 91, in list_functions_for_serialization\r\n    fns = self.functions_to_serialize(serialization_cache)\r\n  File \"/Users/emla2805/.pyenv/versions/bug/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/layer_serialization.py\", line 80, in functions_to_serialize\r\n    serialization_cache).functions_to_serialize)\r\n  File \"/Users/emla2805/.pyenv/versions/bug/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/layer_serialization.py\", line 95, in _get_serialized_attributes\r\n    serialization_cache)\r\n  File \"/Users/emla2805/.pyenv/versions/bug/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/model_serialization.py\", line 47, in _get_serialized_attributes_internal\r\n    default_signature = save_impl.default_save_signature(self.obj)\r\n  File \"/Users/emla2805/.pyenv/versions/bug/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py\", line 212, in default_save_signature\r\n    fn.get_concrete_function()\r\n  File \"/Users/emla2805/.pyenv/versions/bug/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 891, in get_concrete_function\r\n    self._initialize(args, kwargs, add_initializers_to=initializers)\r\n  File \"/Users/emla2805/.pyenv/versions/bug/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 497, in _initialize\r\n    *args, **kwds))\r\n  File \"/Users/emla2805/.pyenv/versions/bug/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2365, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/Users/emla2805/.pyenv/versions/bug/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2673, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/Users/emla2805/.pyenv/versions/bug/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2563, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/Users/emla2805/.pyenv/versions/bug/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 958, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/Users/emla2805/.pyenv/versions/bug/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/Users/emla2805/.pyenv/versions/bug/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saving_utils.py\", line 143, in _wrapped_model\r\n    outputs_list = nest.flatten(model(inputs=inputs, training=False))\r\n  File \"/Users/emla2805/.pyenv/versions/bug/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 771, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"/Users/emla2805/.pyenv/versions/bug/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\", line 713, in call\r\n    convert_kwargs_to_constants=base_layer_utils.call_context().saving)\r\n  File \"/Users/emla2805/.pyenv/versions/bug/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\", line 869, in _run_internal_graph\r\n    output_tensors = layer(computed_tensors, **kwargs)\r\n  File \"/Users/emla2805/.pyenv/versions/bug/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 771, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"/Users/emla2805/.pyenv/versions/bug/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 292, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/Users/emla2805/.pyenv/versions/bug/lib/python3.6/site-packages/tensorflow_core/python/feature_column/sequence_feature_column.py\", line 144, in call\r\n    transformation_cache, self._state_manager)\r\n  File \"/Users/emla2805/.pyenv/versions/bug/lib/python3.6/site-packages/tensorflow_core/python/feature_column/sequence_feature_column.py\", line 559, in get_sequence_dense_tensor\r\n    sp_tensor, default_value=self.default_value)\r\n  File \"/Users/emla2805/.pyenv/versions/bug/lib/python3.6/site-packages/tensorflow_core/python/ops/sparse_ops.py\", line 1488, in sparse_tensor_to_dense\r\n    sp_input = _convert_to_sparse_tensor(sp_input)\r\n  File \"/Users/emla2805/.pyenv/versions/bug/lib/python3.6/site-packages/tensorflow_core/python/ops/sparse_ops.py\", line 69, in _convert_to_sparse_tensor\r\n    raise TypeError(\"Input must be a SparseTensor.\")\r\nTypeError: Input must be a SparseTensor.\r\n```\r\n", "comments": ["@emla2805 \r\n\r\nLooks like code is incomplete. Can you please help us with reproducible code .It will be easy for localizing the issue faster.Thanks!", "@ravikyram Running the code in \"Code to reproduce the issue\" reproduces the issue for me.\r\n\r\nMaybe it wasn't clear from the original report that tensorflow had been installed through `pip install tf-nightly`? e.g.\r\n\r\n1. Create new Python 3.6 virtualenv.\r\n2. Run\r\n   ```console\r\n   $ pip install tf-nightly\r\n   Collecting tf-nightly\r\n     Downloading https://files.pythonhosted.org/packages/e6/ec/83b404a83db81b133baf609e35030356234bbe9015078941c243684ff03e/tf_nightly-2.1.0.dev20191015-cp36-cp36m-manylinux2010_x86_64.whl (397.9MB)\r\n        |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 397.9MB 58kB/s \r\n   Collecting protobuf>=3.6.1\r\n     Downloading https://files.pythonhosted.org/packages/a8/52/d8d2dbff74b8bf517c42db8d44c3f9ef6555e6f5d6caddfa3f207b9143df/protobuf-3.10.0-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\r\n        |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.3MB 11.0MB/s \r\n   Collecting absl-py>=0.7.0\r\n     Using cached https://files.pythonhosted.org/packages/3b/72/e6e483e2db953c11efa44ee21c5fdb6505c4dffa447b4263ca8af6676b62/absl-py-0.8.1.tar.gz\r\n   Collecting astor>=0.6.0\r\n     Using cached https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\r\n   Collecting tf-estimator-nightly\r\n     Downloading https://files.pythonhosted.org/packages/b8/40/c96062a2ca7ac8a4e417168433e29c059d6f9a42ab50eb670edac864f65d/tf_estimator_nightly-2.0.0.dev2019101701-py2.py3-none-any.whl (450kB)\r\n        |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 460kB 14.1MB/s \r\n   Collecting six>=1.10.0\r\n     Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\r\n   Collecting wrapt>=1.11.1\r\n     Using cached https://files.pythonhosted.org/packages/23/84/323c2415280bc4fc880ac5050dddfb3c8062c2552b34c2e512eb4aa68f79/wrapt-1.11.2.tar.gz\r\n   Collecting keras-applications>=1.0.8\r\n     Using cached https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl\r\n   Collecting termcolor>=1.1.0\r\n     Using cached https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\r\n   Collecting keras-preprocessing>=1.0.5\r\n     Using cached https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl\r\n   Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/joar/.virtualenvs/tf-33411/lib/python3.6/site-packages (from tf-nightly) (0.33.6)\r\n   Collecting grpcio>=1.8.6\r\n     Downloading https://files.pythonhosted.org/packages/e8/cb/ebf7b54c5d4ad521d88ee7826dfa0fc3ac84502361ad7e5cb739ea5057a4/grpcio-1.24.1-cp36-cp36m-manylinux1_x86_64.whl (2.3MB)\r\n        |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.3MB 16.0MB/s \r\n   Collecting tb-nightly<2.2.0a0,>=2.1.0a0\r\n     Downloading https://files.pythonhosted.org/packages/4d/27/25b0674acb6775ca55cd072270b613e2b2ad8c4667040c80a3aed9d041e6/tb_nightly-2.1.0a20191016-py3-none-any.whl (3.8MB)\r\n        |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.8MB 9.2MB/s \r\n   Collecting numpy<2.0,>=1.16.0\r\n     Downloading https://files.pythonhosted.org/packages/e5/e6/c3fdc53aed9fa19d6ff3abf97dfad768ae3afce1b7431f7500000816bda5/numpy-1.17.2-cp36-cp36m-manylinux1_x86_64.whl (20.4MB)\r\n        |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20.4MB 13.9MB/s \r\n   Collecting google-pasta>=0.1.6\r\n     Using cached https://files.pythonhosted.org/packages/d0/33/376510eb8d6246f3c30545f416b2263eee461e40940c2a4413c711bdf62d/google_pasta-0.1.7-py3-none-any.whl\r\n   Collecting gast==0.2.2\r\n     Using cached https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\r\n   Collecting opt-einsum>=2.3.2\r\n     Downloading https://files.pythonhosted.org/packages/b8/83/755bd5324777875e9dff19c2e59daec837d0378c09196634524a3d7269ac/opt_einsum-3.1.0.tar.gz (69kB)\r\n        |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 1.9MB/s \r\n   Requirement already satisfied: setuptools in /home/joar/.virtualenvs/tf-33411/lib/python3.6/site-packages (from protobuf>=3.6.1->tf-nightly) (41.4.0)\r\n   Collecting h5py\r\n     Downloading https://files.pythonhosted.org/packages/60/06/cafdd44889200e5438b897388f3075b52a8ef01f28a17366d91de0fa2d05/h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\r\n        |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.9MB 12.8MB/s \r\n   Collecting werkzeug>=0.11.15\r\n     Using cached https://files.pythonhosted.org/packages/ce/42/3aeda98f96e85fd26180534d36570e4d18108d62ae36f87694b476b83d6f/Werkzeug-0.16.0-py2.py3-none-any.whl\r\n   Collecting markdown>=2.6.8\r\n     Using cached https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl\r\n   Building wheels for collected packages: absl-py, wrapt, termcolor, gast, opt-einsum\r\n     Building wheel for absl-py (setup.py) ... done\r\n     Created wheel for absl-py: filename=absl_py-0.8.1-cp36-none-any.whl size=121167 sha256=32f7bf2fb9913b688e7ab74e284a22fba09793783a0a02df186282efda5eceb3\r\n     Stored in directory: /home/joar/.cache/pip/wheels/a7/15/a0/0a0561549ad11cdc1bc8fa1191a353efd30facf6bfb507aefc\r\n     Building wheel for wrapt (setup.py) ... done\r\n     Created wheel for wrapt: filename=wrapt-1.11.2-cp36-cp36m-linux_x86_64.whl size=67537 sha256=384d7ab58996e6ba3b5746a45a45cffd5d4d0c8365c99de0efa186c8182813ed\r\n     Stored in directory: /home/joar/.cache/pip/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd\r\n     Building wheel for termcolor (setup.py) ... done\r\n     Created wheel for termcolor: filename=termcolor-1.1.0-cp36-none-any.whl size=4832 sha256=d8c9176957cfb829be0e0f4f8085005a8b4bd2ab15a0702aaf885ac24d3e6bca\r\n     Stored in directory: /home/joar/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\r\n     Building wheel for gast (setup.py) ... done\r\n     Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=554185d043315585e3d038666310d14ab790638a37cf0ce500015f1b41d6a283\r\n     Stored in directory: /home/joar/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\r\n     Building wheel for opt-einsum (setup.py) ... done\r\n     Created wheel for opt-einsum: filename=opt_einsum-3.1.0-cp36-none-any.whl size=61682 sha256=c9ea2616c390fe28f6585dea4c4be07096bf988ad69f3869c51fa2bcc206afbf\r\n     Stored in directory: /home/joar/.cache/pip/wheels/2c/b1/94/43d03e130b929aae7ba3f8d15cbd7bc0d1cb5bb38a5c721833\r\n   Successfully built absl-py wrapt termcolor gast opt-einsum\r\n   Installing collected packages: six, protobuf, absl-py, astor, tf-estimator-nightly, wrapt, numpy, h5py, keras-applications, termcolor, keras-preprocessing, grpcio, werkzeug, markdown, tb-nightly, google-pasta, gast, opt-einsum, tf-nightly\r\n   Successfully installed absl-py-0.8.1 astor-0.8.0 gast-0.2.2 google-pasta-0.1.7 grpcio-1.24.1 h5py-2.10.0 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.1.1 numpy-1.17.2 opt-einsum-3.1.0 protobuf-3.10.0 six-1.12.0 tb-nightly-2.1.0a20191016 termcolor-1.1.0 tf-estimator-nightly-2.0.0.dev2019101701 tf-nightly-2.1.0.dev20191015 werkzeug-0.16.0 wrapt-1.11.2\r\n   $ python -c\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n   v1.12.1-15925-g2e1e8ec 2.1.0-dev20191015\r\n   ```\r\n3. Run the example code in the original report", "@ravikyram Same here, I don't see how the code is incomplete, could you please elaborate?", "@emla2805 \r\nI am able to reproduce the issue with` tf.feature_column.sequence_numeric_column` But you said  `saving a model using a sequence_categorical_column_with_* together with indicator_column or embedding_column works as expected`,is where i am facing the issue. Thanks!", "@ravikyram Ah, got it. I have updated the original description with an example of working code using two feature columns, one `indicator_column` wrapping a `sequence_categorical_column_with_vocabulary_list` column and one `embedding_column` wrapping a `sequence_categorical_column_with_hash_bucket` column.  ", "I have tried on colab with TF version 2.1.0-dev20191015, 2.1.0-dev20191022 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/d563809afcdf9878e04257ba037ffb9b/untitled289.ipynb).Thanks!", "This is fixed with the latest tf-nightly. [colab gist](https://colab.sandbox.google.com/gist/goldiegadde/20e648fa5e3e9cc7f8457ca251287530/untitled289.ipynb)\r\nMarking this as closed. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33411\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33411\">No</a>\n"]}, {"number": 33410, "title": "tfliteGpuDelegate Invoke write to buffer failed source data is larger than buffer", "body": "\r\n\r\nI followed the steps in https://www.tensorflow.org/lite/performance/gpu and added gpu to tflite interpreter. The mobilenet model given by an android project called tensorflow/lite/jave/demo can work using this project on Pixel 2. However, when it was put into another project with the follwing code. Here comes the errors: \r\n```\r\nTfLiteGpuDelegate Invoke: Write to buffer failed. Source data is larger than buffer\r\nNode number 31 (TfLiteGpuDelegateV2) failed to invoke.\r\n```\r\nimgData, outputClasses are already checked and have the same size with those in tensorflow/lite/jave/demo project.\r\n\r\nVersions of TfLite and TfLiteGpu are also kept the same with that in tensorflow/lite/jave/demo project.\r\n\r\n```\r\nimport  org.tensorflow.lite.TensorFlowLite;\r\nimport org.tensorflow.lite.gpu.GpuDelegate;\r\n\r\n/**\r\n * Wrapper for frozen detection models trained using the Tensorflow Object Detection API:\r\n * github.com/tensorflow/models/tree/master/research/object_detection\r\n */\r\npublic class TFLiteObjectDetectionAPIModel implements Classifier {\r\n  private static final Logger LOGGER = new Logger();\r\n\r\n  // Only return this many results.\r\n  private static final int NUM_DETECTIONS = 1001; // hand landmark ,21, mobilenet, 1001\r\n  // Float model\r\n  private static final float IMAGE_MEAN = 128.0f;\r\n  private static final float IMAGE_STD = 128.0f;\r\n  private boolean isModelQuantized;\r\n  private int inputSize;\r\n  private Vector<String> labels = new Vector<String>();\r\n  private int[] intValues;\r\n  private float[][][] outputLocations;\r\n  private float[][] outputClasses;\r\n  private float[][] outputScores;\r\n  private float[] numDetections;\r\n\r\n  private GpuDelegate gpuDelegate = new GpuDelegate();\r\n  private final Interpreter.Options options = (new Interpreter.Options()).addDelegate(gpuDelegate);\r\n  private ByteBuffer imgData;\r\n\r\n  private Interpreter tfLite;\r\n  private ArrayList<Recognition> recognitions = new ArrayList<Recognition>();\r\n\r\n  private TFLiteObjectDetectionAPIModel() {}\r\n\r\n  /** Memory-map the model file in Assets. */\r\n  private static MappedByteBuffer loadModelFile(AssetManager assets, String modelFilename)\r\n      throws IOException {\r\n    FileInputStream inputStream = new FileInputStream(new File(\"/sdcard/sunny/data/\"+modelFilename+\".tflite\"));\r\n\r\n    FileChannel fileChannel = inputStream.getChannel();\r\n    long startOffset = fileChannel.position();\r\n    long declaredLength = fileChannel.size();\r\n    return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);\r\n  }\r\n\r\n  /**\r\n   * Initializes a native TensorFlow session for classifying images.\r\n   *\r\n   * @param assetManager The asset manager to be used to load assets.\r\n   * @param modelFilename The filepath of the model GraphDef protocol buffer.\r\n   * @param labelFilename The filepath of label file for classes.\r\n   * @param inputSize The size of image input\r\n   * @param isQuantized Boolean representing model is quantized or not\r\n   */\r\n  public static Classifier create(\r\n      final AssetManager assetManager,\r\n      final String modelFilename,\r\n      final String labelFilename,\r\n      final int inputSize,\r\n      final boolean isQuantized)\r\n      throws IOException {\r\n    final TFLiteObjectDetectionAPIModel d = new TFLiteObjectDetectionAPIModel();\r\n\r\n    InputStream labelsInput = null;\r\n    String actualFilename = labelFilename.split(\"file:///android_asset/\")[1];\r\n    labelsInput = assetManager.open(actualFilename);\r\n    BufferedReader br = null;\r\n    br = new BufferedReader(new InputStreamReader(labelsInput));\r\n    String line;\r\n    while ((line = br.readLine()) != null) {\r\n      LOGGER.w(line);\r\n      d.labels.add(line);\r\n    }\r\n    br.close();\r\n\r\n    d.inputSize = inputSize;\r\n    try {\r\n      d.tfLite = new Interpreter(loadModelFile(assetManager, modelFilename),d.options);\r\n    } catch (Exception e) {\r\n      throw new RuntimeException(e);\r\n    }\r\n\r\n    d.isModelQuantized = isQuantized;\r\n    // Pre-allocate buffers.\r\n    int numBytesPerChannel;\r\n    if (isQuantized) {\r\n      numBytesPerChannel = 1; // Quantized\r\n    } else {\r\n      numBytesPerChannel = 4; // Floating point\r\n    }\r\n    d.imgData = ByteBuffer.allocateDirect(1 * d.inputSize * d.inputSize * 3 * 4);\r\n    d.imgData.order(ByteOrder.nativeOrder());\r\n    d.intValues = new int[d.inputSize * d.inputSize];\r\n\r\n    d.outputClasses = new float[1][NUM_DETECTIONS];\r\n\r\n    return d;\r\n  }\r\n\r\n  @Override\r\n  public List<Recognition> processImage(final AssetManager assetManager, Classifier.Recognition.inputFormat imageFormat, int[] intValues){\r\n    Trace.beginSection(\"preprocessBitmap\");\r\n\r\n    imgData.rewind();\r\n    for (int i = 0; i < inputSize; ++i) {\r\n      for (int j = 0; j < inputSize; ++j) {\r\n        int pixelValue = intValues[i * inputSize + j];\r\n        if (isModelQuantized) {\r\n          // Quantized model\r\n          imgData.put((byte) ((pixelValue >> 16) & 0xFF));\r\n          imgData.put((byte) ((pixelValue >> 8) & 0xFF));\r\n          imgData.put((byte) (pixelValue & 0xFF));\r\n        } else { // Float model\r\n          imgData.putFloat((((pixelValue >> 16) & 0xFF) - IMAGE_MEAN) / IMAGE_STD);\r\n          imgData.putFloat((((pixelValue >> 8) & 0xFF) - IMAGE_MEAN) / IMAGE_STD);\r\n          imgData.putFloat(((pixelValue & 0xFF) - IMAGE_MEAN) / IMAGE_STD);\r\n        }\r\n      }\r\n    }\r\n    Trace.endSection(); // preprocessBitmap\r\n\r\n    // Copy the input data into TensorFlow.\r\n    Trace.beginSection(\"feed\");\r\n    outputClasses = new float[1][NUM_DETECTIONS];\r\n    Trace.endSection();\r\n\r\n    // Run the inference call.\r\n    Trace.beginSection(\"run\");\r\n    tfLite.run(imgData, outputClasses);\r\n    Trace.endSection();\r\n\r\n    return recognitions;\r\n  }\r\n\r\n\r\n  @Override\r\n  public void enableStatLogging(final boolean logStats) {}\r\n\r\n  @Override\r\n  public String getStatString() {\r\n    return \"\";\r\n  }\r\n\r\n  @Override\r\n  public void close() {\r\n    tfLite.close();\r\n    tfLite = null;\r\n    recognitions.clear();\r\n    recognitions=null;\r\n    gpuDelegate.close();\r\n    gpuDelegate = null;\r\n}\r\n```", "comments": ["@manchengfenxu The error message is printed by `GlBuffer`.  I think you're trying to write more than what you can.  MobileNet IIRC has the input dimensions of size 224x224.  Is `inputSize = 224` ?", "> \r\n> \r\n> @impjdi  The error message is printed by `GlBuffer`. I think you're trying to write more than what you can. MobileNet IIRC has the input dimensions of size 224x224. Is `inputSize = 224` ?\r\n\r\nYes\uff0cinputSize = 224\uff0cthe same with that in tensorflow/lite/jave/demo. I also found this message \"Write to buffer failed. Source data is larger than buffer\" in source code of GlBuffer. That is why I checked the size of imgData, output Classes carefully and compared them with those in tensorflow/lite/jave/demo but didn't found any clue. Is it the problem of the wrong usage of openGl? I didn't find any code related to invoking OpenGL in tensorflow/lite/jave/demo, so no code was for invoking OpenGL in this project with errors. ", "I am having a similar issue. My model runs fine on CPU and GPU on my Galaxy S8 but only works on the CPU of my Nexus 5X.\r\n\r\nI assume it is an issue with part of the OpenGL buffer.\r\n\r\n``` Process: xyz.hutt.meng, PID: 12866\r\n    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: OpenCL library not loaded - dlopen failed: library \"libOpenCL-pixel.so\" not found\r\n    Falling back to OpenGL\r\n    TfLiteGpuDelegate Invoke: Write to buffer failed. Source data is larger than buffer.\r\n    Node number 51 (TfLiteGpuDelegateV2) failed to invoke.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:154)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:314)\r\n        at org.tensorflow.lite.Interpreter.run(Interpreter.java:275)\r\n        at xyz.hutt.meng.models.SonoNet.recognizeBuffer(SonoNet.kt:98)\r\n        at xyz.hutt.meng.models.SonoNet.recognizeImage(SonoNet.kt:136)\r\n        at xyz.hutt.meng.models.SonoNet.process(SonoNet.kt:74)\r\n        at xyz.hutt.meng.BitmapProcessor.processBitmap(BitmapProcessor.kt:82)\r\n        at xyz.hutt.meng.BitmapProcessor.processBitmap(BitmapProcessor.kt:19)\r\n        at xyz.hutt.meng.BitmapCapture$start$1.invoke(BitmapCapture.kt:62)\r\n        at xyz.hutt.meng.BitmapCapture$start$1.invoke(BitmapCapture.kt:15)\r\n        at xyz.hutt.meng.BitmapCapture$ImageListener.onImageAvailable(BitmapCapture.kt:81)\r\n        at android.media.ImageReader$ListenerHandler.handleMessage(ImageReader.java:812)\r\n        at android.os.Handler.dispatchMessage(Handler.java:106)\r\n        at android.os.Looper.loop(Looper.java:164)\r\n        at android.os.HandlerThread.run(HandlerThread.java:65)```", "You could print out and see what the actual values are (how much it's trying to write vs how much you have allocated).  Maybe it can give you a hint what went wrong where.", "@impjdi It works perfectly fine on all my other devices as well as when running fine on the CPU though. It's ```tflite.run(input, output)``` which is erroring. I am not really sure how to debug this.", "Such \"TfLiteGpuDelegate Invoke: Write to buffer failed. Source data is larger than buffer\" also appears in more recent issues like https://github.com/tensorflow/tensorflow/issues/41839, https://github.com/tensorflow/tensorflow/issues/44659. Just wondering does more recent tflite build (tflite 2.3.0, tf-nightly etc.) fix the issue? Thx!", "@manchengfenxu,\r\n\r\nWe are checking to see if this is still an issue, Here's a [comment](https://github.com/tensorflow/tensorflow/issues/41839#issuecomment-896148989) from a similar issue where the user confirmed that it works fine on TF 2.5. One more similar issue [here](https://github.com/tensorflow/tensorflow/issues/44659) for your reference. Can you update your tensorflow to latest stable version i.e `2.6.0` and let us know if the issue persists?  Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33410\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33410\">No</a>\n"]}, {"number": 33409, "title": "Improve tf.make_ndarray() docstring", "body": "Added missing usage example to improve documentation", "comments": ["Please make the changes against master, we do not update release branches after a final release except for doing patch releases."]}]