[{"number": 26843, "title": "SVD segfaults when given a 0x0 matrix", "body": "**System information**\r\n- Have I written custom code: Yes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 on WSL, also Colab\r\n- TensorFlow installed from (source or binary): binary (pip, no GPU)\r\n- TensorFlow version (use command below): b'v1.13.1-0-g6612da8951' 1.13.1\r\n- Python version: 3.6.8 (anaconda)\r\n\r\n**Describe the current behavior**\r\nRunning `tf.svd([[]])` leads to a segfault.\r\n\r\n**Describe the expected behavior**\r\nThis should probably raise an exception. Numpy returns something meaningless in this case, so is not a very good role model!\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\ntf.enable_v2_behavior()\r\ntf.svd([[]])\r\n```\r\n", "comments": ["@amilsted This is a stale issue. `TF1.x` is no more supported. \r\n\r\nHowever, I couldn't reproduce the segfault when I ran your code with `TF2.5`. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/28bed80469ead33f617f80e1d05680cd/untitled.ipynb). Thanks!\r\n\r\nI am closing this issue as this was resolved in recent `TF 2.5` version. Please feel free to reopen I am mistaken. Thanks!\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26843\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26843\">No</a>\n"]}, {"number": 26841, "title": "Terribly slow conditional updates", "body": "I'm trying to implement conditional  tensor updates such as:\r\n\r\n\r\n#######################################################\r\nGiven i:\r\n\r\ntf.scatter_update(self.vOut,i,[tf.cond( tf.less(tf.cast(tf.random_uniform([1])[0],tf.float64),tf.minimum(tf.cast(1.,tf.float64),  tf.exp(-self.dUk(vv,hh,vIn,hIn,i,j,krepl)))),\r\n                 lambda:tf.slice(vv,[j,0],[1,self.n_v])[0],lambda:tf.slice(vIn,[i,j,0],[1,1,self.n_v])[0][0]) for j in kk])\r\n\r\n\r\n def U(self,x,y,x1,y1):\r\n            return tf.multiply(tf.cast(self.P1,tf.float64),tf.exp(-((tf.norm(x-y)+tf.norm(x1-y1))/self.P2)**2.))\r\n\r\n  def dU(self,x,y,v,z,x1,y1,v1,z1):    \r\n          return    tf.subtract(self.U(x,y,x1,y1),self.U(v,z,v1,z1))\r\n\r\n def dUk(self,vv,hh,vIn,hIn,i,j,krepl):    \r\n          return    tf.reduce_sum([tf.cond(tf.not_equal(p,i),\r\n             lambda: self.dU(tf.slice(vv,[j,0],[1,self.n_v])[0],tf.slice(vIn,[p,j,0],[1,1,self.n_v])[0][0], \r\n             tf.slice(vIn,[i,j,0],[1,1,self.n_v])[0][0],tf.slice(vIn,[p,j,0],[1,1,self.n_v])[0][0],\r\n             tf.slice(hh,[j,0],[1,self.n_h])[0],tf.slice(hIn,[p,j,0],[1,1,self.n_h])[0][0],\r\n             tf.slice(hIn,[i,j,0],[1,1,self.n_h])[0][0],tf.slice(hIn,[p,j,0],[1,1,self.n_h])[0][0]\r\n             ),\r\n             lambda: tf.cast(0., tf.float64)) for p in  krepl])\r\n #############################################################\r\n\r\nThe problem i'm encountering is being  terribly slow compared to just numpy computation, i.e.\r\n\r\npure dUk computation:\r\ndUk time (TF): 2.5306854248046875\r\ndUk time (Numpy) time: 0.0\r\n\r\nUpdate computation:\r\nTime for Scatter (TF): 12.936962366104126\r\nTime for Scatter (Numpy): 0.015621185302734375\r\n\r\nEverything is done on CPU.\r\n\r\nIs it normal to be that slow ?", "comments": ["@phquanta Could you provide system details and TF version [here](https://github.com/tensorflow/tensorflow/issues/new?template=00-bug-performance-issue.md). Thanks!", "Tensorflow: version 1.7.0 (CPU based installation)\r\nPython: 3.6\r\nOS: Windows 8.1\r\nOS Name\tMicrosoft Windows 8.1\r\nVersion\t6.3.9600 Build 9600\r\nOther OS Description \tNot Available\r\nSystem Model\tXPS 8700\r\nProcessor\tIntel(R) Core(TM) i7-4790 CPU @ 3.60GHz, 3601 Mhz, 4 Core(s), 8 Logical \r\nInstalled Physical Memory (RAM)\t24.0 GB\r\nGPU: Nvidia GTX 1050Ti\r\nIDE: Spyder 3.3.1\r\n\r\n\r\nAslo this operation\r\n\r\nj=1\r\n\r\naa=self.sess.run(tf.map_fn(lambda x: (tf.tensordot(x[0],x[1],0)),elems=tf.convert_to_tensor(self.vt[:,j,:]), tf.convert_to_tensor(self.ht[:,j,:])),dtype=tf.float64))\r\n\r\nis twice as slow compared  to:\r\n\r\n[np.outer(self.vt[i,j,:],self.ht[i,j,:])*expF[i] for i in self.lRepls]\r\n\r\n\r\nself.lRepls=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\r\nnp.shape(self.vt)=(16,10,784)\r\nnp.shape(self.ht)=(16,10,340)\r\n", "@phquanta Could you check with latest version of TF and check whether the bug persists? Thanks!", "Thanks. Will do.", "Did check, it still persists. \r\n\r\nNumpy:\r\nstart=time.time()\r\nself.wNegJ[j,:,:]+=np.add.reduce([np.outer(self.vt[i,j,:],self.ht[i,j,:])*expF[i] for i in self.lRepls],axis=0)                        \r\nprint(\"Time for Average step0:\",time.time()-start) \r\n\r\nTime for Average step0: 0.26468753814697266\r\n\r\nTensorflow:\r\n               bb1=tf.convert_to_tensor(self.vt[:,j,:])\r\n               bb2=tf.convert_to_tensor(self.ht[:,j,:])\r\n                        star1t=time.time()\r\n                        with tf.device(self.device):  \r\n                         \r\n                         aa=self.sess.run(tf.map_fn(lambda x: (tf.tensordot(x[0],x[1],0)),elems=(bb1,\r\n                                                                                bb2),dtype=tf.float64))\r\n                        star1t=time.time()\r\n\r\n                        self.wNegJ[j,:,:]+=np.add.reduce([aa[i]*expF[i]\r\n                                    for i in self.lRepls],axis=0)                        \r\n                        \r\n                                      print(\"Time for Average step0:\",time.time()-star1t)   \r\n\r\n\r\nTime for Average step0: 0.4763360023498535", "Sorry, TF version is 1.11.0", "Hi @phquanta ,\r\nWe see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade the code base to 2.6 version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26841\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26841\">No</a>\n"]}, {"number": 26840, "title": "[ROCm] Add ROCm macro for Google upstream avgpooling op", "body": "This PR added the ROCm macro for avgpooling_op\r\n\r\nThis PR depends on #26722 and #26774 ", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26840) for more info**.\n\n<!-- need_author_consent -->", "Oops, looks like I can't do merge from an on-going PR request. Working on removing those merge nodes.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26840) for more info**.\n\n<!-- ok -->", "Nagging Reviewer @chsigg: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 29 days with no activity and the `awaiting review` label has been applied."]}, {"number": 26839, "title": "Numpy like slicing on Tensors", "body": "I was wondering if it is possible to implement Numpy like slicing annd updating a[1:10,2:20....]  in Tensorflow. It would make life much easier. Right now it code just gets bigger and uglier and bug prone.\r\n\r\n", "comments": ["I would like to take up this task, if at all it's possible. Need your comments @mihaimaruseac @fchollet ", "I'm not familiar with what that would entail, but it looks like a good project.", "@phquanta Could you provide more details on the feature? May be take a simple example code and demonstrate what was missing with the current TF and what would be achieved with this new feature. Thanks! ", "Sure , basically instead of writing down :\r\n\r\n1) aa=tf.slice(hIn,[p,j,0],[1,1,self.n_h])[0][0], One would write aa=hIn[p,j,:]\r\n2) tf.scatter_update(self.vOut,i,[[1. for i in range(self.n_v)] for j in kk])], One would write self.vOut[i,:,:]=[[1. for i in range(self.n_v)] for j in kk])] \r\n\r\nThose type of operations.", "Most numpy slicing is already supported in tensorflow; tensor[p, q, :] for example is perfectly supported.\r\n\r\nWhat isn't currently supported is slice assignment. TensorFlow tensors are immutable, so we'd need an operation which takes a tensor and returns another one. It should be relatively straightforward to repurpose the existing kernel for StridedSlice to do this, similar to how TensorScatterAdd etc repurposed the ScatterNdAdd kernel.\r\n\r\nI'm happy to provide pointers to the C++ code if anyone is interested in contributing this.", "If i'm not mistaken, that type is supported but not tensor[p, :, q] or tensor[:, :, q] or tensor[:, p,q] or other more fancier slices.\r\n\r\nAlso, i was wondering if it is possible to make tensors mutable at some point ?\r\n\r\nThanks. ", "We'll not make tensors mutable (as this will make compilation substantially\nharder) but we can do a better job at hiding the immutability of tensors\nafter tf 2.0 goes out.\n\nIf you can enumerate specific slicing formats which currently do not work\nwe can try to add them to the strided slice operation.\n\nOn Tue, Mar 19, 2019 at 6:52 PM Andrei Buin <notifications@github.com>\nwrote:\n\n> If i'm not mistaken, that type is supported but not tensor[p, :, q] or\n> tensor[:, :, q] or tensor[:, p,q] or other more fancier slices.\n>\n> Also, i was wondering if it is possible to make tensors mutable at some\n> point ?\n>\n> Thanks.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26839#issuecomment-474651731>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxTG66CgsMvM3tXqbhfq0JVBSYyORks5vYZRfgaJpZM4b6Nv9>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp, I'm interested in contributing this, and hope you could give me some reference. Thanks.", "@a6802739 happy to help!\r\n\r\nThe current logic for numpy-style slicing in tensorflow is spread out over two functions: boolean_mask and strided_slice.\r\n\r\ntf.boolean_mask is implemented in python in terms of gather here: https://github.com/tensorflow/tensorflow/blob/49414867df30277c7ea7839837cc8e3397c712cb/tensorflow/python/ops/array_ops.py#L1275\r\n\r\nwhen slicing we first build a spec here for the strided_slice op https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py#L526 and then we call the strided_slice op (or boolean_mask), which is implemented in C in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/slice_op.h and the corresponding .cc files in the same directory.\r\n\r\nDepending on what extra types of slicing you want to add support for we can do this in python by reducing to the existing ops or by adding a new op for that type of slicing and plumbing it through tf.\r\n\r\nI can help you more if you tell me which other features of numpy slicing are important to you.", "@alextp , Thanks for your reference.\r\nI have tried the slice operation, I found it could support operation like `tensor[p, q, :]`, `tensor[p, :, q]`, `tensor[:, p, q]`, etc\r\nBut it could not support fetch multi elements one time, just like in `numpy`, we could done it like this:\r\n```\r\nimport numpy as np\r\n \r\narr = np.array([\r\n    [1, 2, 3, 4],\r\n    [2, 4, 6, 8],\r\n    [3, 6, 9, 12],\r\n    [4, 8, 12, 16]\r\n])\r\n \r\nprint arr[[0, 2], [3, 1]]\r\n```\r\nSo I will first try to add this operation, how about that?\r\n\r\nAnd for now we could not support update tensor like `tensor[p, q, j] = xxx`, if you think we could support this, I could have a try either.", "That sounds good. Fetching multiple elements like that sounds like\nsomething which we can support with tf.gather_nd. So I'd try implementing\nthis by adding code to _slice_helper to detect that the slice expression\nlooks like this and then forward it to gather_nd in that case.\n\nOn Wed, Mar 27, 2019 at 3:30 AM songhao <notifications@github.com> wrote:\n\n> @alextp <https://github.com/alextp> , Thanks for your reference.\n> I have tried the slice operation, I found it could support operation like tensor[p,\n> q, :], tensor[p, :, q], tensor[:, p, q], etc\n> But it could not support fetch multi elements one time, just like in numpy,\n> we could done it like this:\n>\n> import numpy as np\n>\n> arr = np.array([\n>     [1, 2, 3, 4],\n>     [2, 4, 6, 8],\n>     [3, 6, 9, 12],\n>     [4, 8, 12, 16]\n> ])\n>\n> print arr[[0, 2], [3, 1]]\n>\n> So I will first add this operation, how about that?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26839#issuecomment-477083309>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxWfM_Xc00CboIRIhBDrgEk42imBKks5va0hZgaJpZM4b6Nv9>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp, Thanks. I have come up with the PR #27238, please take a loot at that.\r\n\r\nAnd would us have any plan to support this kind of operation `tensor[p, q, j] = xxx`?\r\n\r\n", "We cannot support mutating assignments because tf tensors are immutable. Not at least until a future release where we can hide the mutation somewhat better.", "Thanks! @alextp ", "In order to be compatible with numpy array operation as much as possible, I list the items that we could consider:\r\nfor example for numpy array `arr = np.array([[1,2,3], [4,5,6], [7,8,9]])`.\r\n- [ ] arr[[0, 1], [1, 0]], #27238\r\n- [ ]  arr[[0], 1]\r\n- [ ] arr[[0, 1], 0]\r\n- [ ] arr[np,array(1), 0]\r\n\r\n@alextp, what do you think of the above?\r\n\r\nAnd there is also a thing that I found is confused,  here is the example:\r\n```\r\n>>> import tensorflow as tf\r\n>>> tf.enable_eager_execution()\r\n>>> foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])\r\n>>> print (foo[[0, 1]])\r\ntf.Tensor(2, shape=(), dtype=int32)\r\n>>> import numpy as np\r\n>>> arr = np.array([[1,2,3], [4,5,6], [7,8,9]])\r\n>>> print(arr[[0, 1]])\r\n[[1 2 3]\r\n [4 5 6]]\r\n```\r\nFor above, the result of `foo[[0, 1]]` is different from `arr[[0, 1]]`, did that match our expectation?", "@a6802739 can you find an exhaustive list of the operations supported by numpy slicing somewhere?", "@alextp, I'll have a try.", "@alextp  From this line within `array_ops.py`\r\n[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py#L653](url)\r\nWe restrict the slice_spec must be `integers` or `slices (`:`)`, `ellipsis ` or  `tf.newaxis ` or `scalar`.\r\nIn fact from the tutorial [https://github.com/numpy/numpy/blob/master/doc/source/reference/arrays.indexing.rst](url), we found the slice_spec could also be mixture of `list`(or `tensor` or  `ndarray`) with the restricted slice_spec above. So it could be like, `foo[[i, j], 0]` or `foo[..., [i, j]]` or `foo[0:2, [i, j]]` etc.\r\n\r\nAnd first I think we should fix the  inconsistent behavior between tensor and ndarrray for `foo[[i, j]]`, for tensor, it would be `foo[i][j]`, for ndarray, it would be `[foo[i], foo[j]]`.", "yes we should fix this inconsistent behavior", "@alextp, Okay, I could have have a try for the inconsistent behavior.", "Hi guys,\r\n\r\nafter reviewing tf.gather, tf.gather_nd and advanced indexing using tf.stack and then tf.gather_nd, I got to know that the advanced indexing like this numpy nparray[ arr_rows, arr_cols] is not supported in tensorflow.\r\n\r\nany plan to add this feature?\r\nthe reproducible code you can find here:\r\nhttps://stackoverflow.com/questions/56636994/non-continuous-index-slicing-on-tensor-object-in-tensorflow-advanced-indexing-l\r\n\r\na simple version is being supported, but for example in case the number of columns is different from the source tensor is not been supported.\r\n\r\nWe can do it easily in numpy but tensorflow no!", "@phquanta,\r\n\r\nCould you please refer [Introduction to tensor slicing](https://www.tensorflow.org/guide/tensor_slicing) guide, may help you. Thanks!\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 26838, "title": "Trying to play alarm when it detects a person on android using tesnorflow", "body": "\r\nI am trying to play a sound when i detect a person and it does not process more images while sound is playing and wait n number of seconds after the sound stops playing before it processes any more images. this is to prevent overlapping sounds being played. \r\nI made changes to the line number below , it plays the sound when it detects an object that i tell it to, but problem is it does not detect anything after that it just freezes. \r\n\r\nhttps://github.com/rojanulak/MarsRoverPhoto/blob/master/tensorflow-master/tensorflow/examples/android/src/org/tensorflow/demo/DetectorActivity.java#L331\r\nhttps://github.com/rojanulak/MarsRoverPhoto/blob/master/tensorflow-master/tensorflow/examples/android/src/org/tensorflow/demo/CameraActivity.java#L215\r\n\r\n", "comments": ["@rojanulak Interesting application. However, this is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 26837, "title": "Update gradient_descent.py", "body": "Python 3 allows this kind of syntax. As of now, the old syntax is used, where you specify the child's name. Should supercharge be referred to in this way, or does this change break functionality somehow? I can update statements in other files as well, if this change is accepted and meaningful. My apologies for this silly commit!", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26837) for more info**.\n\n<!-- need_sender_cla -->", "I signed it! ", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26837) for more info**.\n\n<!-- ok -->", "I'd love it if we could do this but we still have to support python 2 :-/", "I see, I thought this might be the case after I'd submitted the pull. Thank for the reply anyway!"]}, {"number": 26836, "title": "ERROR!!! AttributeError: module '_pywrap_tensorflow_internal' has no attribute 'TF_ImportGraphDefOptionsSetDefaultDevice'", "body": "my programm error!\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-2-658d4b6817ec>\", line 1, in <module>\r\n    runfile('C:/Users/svm2717/Desktop/test.py', wdir='C:/Users/svm2717/Desktop')\r\n\r\n  File \"D:\\ANACONDA\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 705, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"D:\\ANACONDA\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 102, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"C:/Users/svm2717/Desktop/test.py\", line 8, in <module>\r\n    from keras.datasets import mnist\r\n\r\n  File \"D:\\ANACONDA\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from . import utils\r\n\r\n  File \"D:\\ANACONDA\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n\r\n  File \"D:\\ANACONDA\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n\r\n  File \"D:\\ANACONDA\\lib\\site-packages\\keras\\backend\\__init__.py\", line 89, in <module>\r\n    from .tensorflow_backend import *\r\n\r\n  File \"D:\\ANACONDA\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n\r\n  File \"D:\\ANACONDA\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n\r\n  File \"D:\\ANACONDA\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\n  File \"D:\\ANACONDA\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n\r\n  File \"D:\\ANACONDA\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 1393, in <module>\r\n    TF_ImportGraphDefOptionsSetDefaultDevice = _pywrap_tensorflow_internal.TF_ImportGraphDefOptionsSetDefaultDevice\r\n\r\nAttributeError: module '_pywrap_tensorflow_internal' has no attribute 'TF_ImportGraphDefOptionsSetDefaultDevice'\r\n\r\nCUDA:8.0\r\n", "comments": ["Decided to himself! For those who have the same error, just reinstall Tensorflow and everything will work!", "Closing as it seems solved. Please reopen if that's not the case"]}, {"number": 26835, "title": "[TF2.0] KerasLayer cannot be loaded from .h5", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\nRun on Colab example from copied from tensorflow/hub: https://colab.research.google.com/drive/1ymmzlWHieCuXR7UjXezULnYsNsDeHc9m\r\n\r\nVersion:  2.0.0-alpha0\r\nEager mode:  True\r\nHub version:  0.3.0\r\nGPU is available\r\n\r\n**The issue**\r\nWhen I try to run Colab example from tensorflow/hub for image retraining and then save/load model according to [the documentation](https://www.tensorflow.org/alpha/guide/keras/saving_and_serializing), there are some issues with it.\r\n\r\nAfter retraining, when we try to save the model::\r\n```\r\n# Save the model\r\nmodel.save('path_to_my_model.h5')\r\n```\r\nThis error occurs (already described here: #26811 )\r\n```\r\n//...skipped, see ticket above for details ^\r\nh5py/_objects.pyx in h5py._objects.with_phil.wrapper()\r\nh5py/_objects.pyx in h5py._objects.with_phil.wrapper()\r\nh5py/h5o.pyx in h5py.h5o.link()\r\nRuntimeError: Unable to create link (name already exists)\r\n```\r\n\r\nBut actually `path_to_my_model.h5` file is created. Then when I try to load it via:\r\n```\r\n# Recreate the exact same model purely from the file\r\nnew_model = keras.models.load_model('path_to_my_model.h5')\r\nnew_model.summary()\r\n```\r\nI get an error:\r\n```\r\nValueError: Unknown layer: KerasLayer\r\n```\r\n\r\nEntire stacktrace:\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-15-0ceb2f1e6ad5> in <module>()\r\n----> 1 new_model = keras.models.load_model('path_to_my_model.h5')\r\n      2 \r\n      3 new_model.summary()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/hdf5_format.py in load_model(filepath, custom_objects, compile)\r\n    213     model_config = json.loads(model_config.decode('utf-8'))\r\n    214     model = model_config_lib.model_from_config(model_config,\r\n--> 215                                                custom_objects=custom_objects)\r\n    216 \r\n    217     # set weights\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/model_config.py in model_from_config(config, custom_objects)\r\n     53                     '`Sequential.from_config(config)`?')\r\n     54   from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top\r\n---> 55   return deserialize(config, custom_objects=custom_objects)\r\n     56 \r\n     57 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/serialization.py in deserialize(config, custom_objects)\r\n     93       module_objects=globs,\r\n     94       custom_objects=custom_objects,\r\n---> 95       printable_module_name='layer')\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)\r\n    190             custom_objects=dict(\r\n    191                 list(_GLOBAL_CUSTOM_OBJECTS.items()) +\r\n--> 192                 list(custom_objects.items())))\r\n    193       with CustomObjectScope(custom_objects):\r\n    194         return cls.from_config(cls_config)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py in from_config(cls, config, custom_objects)\r\n    349     for layer_config in layer_configs:\r\n    350       layer = layer_module.deserialize(layer_config,\r\n--> 351                                        custom_objects=custom_objects)\r\n    352       model.add(layer)\r\n    353     if not model.inputs and build_input_shape:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/serialization.py in deserialize(config, custom_objects)\r\n     93       module_objects=globs,\r\n     94       custom_objects=custom_objects,\r\n---> 95       printable_module_name='layer')\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)\r\n    179     config = identifier\r\n    180     (cls, cls_config) = class_and_config_for_serialized_keras_object(\r\n--> 181         config, module_objects, custom_objects, printable_module_name)\r\n    182 \r\n    183     if hasattr(cls, 'from_config'):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py in class_and_config_for_serialized_keras_object(config, module_objects, custom_objects, printable_module_name)\r\n    164     cls = module_objects.get(class_name)\r\n    165     if cls is None:\r\n--> 166       raise ValueError('Unknown ' + printable_module_name + ': ' + class_name)\r\n    167   return (cls, config['config'])\r\n    168 \r\n\r\nValueError: Unknown layer: KerasLayer\r\n```\r\n\r\nEntire colab to review: https://colab.research.google.com/drive/1ymmzlWHieCuXR7UjXezULnYsNsDeHc9m", "comments": ["same problem here.", "Same for Unknown layer: DenseFeatures", "same for Unknown layer: DenseFeatures", "Same issue for LSTM layers. When using 'ModelCheckpoint' callback.\r\n File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\r\n File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\r\n File \"h5py/h5o.pyx\", line 202, in h5py.h5o.link\r\nRuntimeError: Unable to create link (name already exists)", "Same here. \r\n\r\n```\r\nValueError: Unknown layer: DenseFeatures\r\n```\r\n\r\nIs there any update on this? ", "I'm wondering if it's a good idea to use tf.keras.applications.MobileNetV2 model instead of the module loaded from TF Hub. \r\nIt seems working after quick checks - tested model saving in this example: https://www.tensorflow.org/alpha/tutorials/images/transfer_learning", "Not perfect, but there is a workaround:\r\n\r\nSImply create the model from scratch every time (instead of loading from JSON/YAML) and then load the weights. \r\n\r\nWhen using the DenseFeatures layer, the model will not be built after calling compile, so you will not be able to load the weights right away. You must either manually call the build() method or run a single batch through the model and then load the weights.", "I think I found the solution (or another workaround).\r\n\r\nInstead of:\r\n```\r\nmodel.save('path_to_my_model.h5')\r\n```\r\n\r\nCall:\r\n```\r\ntf.keras.experimental.export_saved_model(model, 'path_to_my_model.h5')\r\n```\r\n\r\nInstead of:\r\n```\r\nnew_model = keras.models.load_model('path_to_my_model.h5')\r\nnew_model.summary()\r\n```\r\n\r\nCall:\r\n```\r\nreloaded_model = tf.keras.experimental.load_from_saved_model('path_to_my_model.h5', custom_objects={'KerasLayer':hub.KerasLayer})\r\nprint(reloaded_model.get_config())\r\n\r\n#Get input shape from model.get_config()\r\nreloaded_model.build((None, 224, 224, 3))\r\nreloaded_model.summary()\r\n```\r\n\r\nWorks like a charm. :) \r\n\r\nColaboratory with the code above: https://colab.research.google.com/drive/17YerjXGUwqp48mB3-WSTipv4itizuOCP", "I use follow this code it work\r\n\r\n```\r\nmodel.save('./model/saved.h5')\r\nnew_model = tf.keras.models.load_model('./model/saved.h5',custom_objects='KerasLayer':hub.KerasLayer})\r\n new_model.summary()\r\n```\r\nhttps://colab.research.google.com/drive/1JLkVWpXhDwImuF930i5PWstJvu4ENDoc", "The crash with hub.KerasLayer likely needs to be fixed in tensorflow-hub, not tensorflow. I've filed issue https://github.com/tensorflow/hub/issues/287 for that.\r\n\r\nCan this issue here be closed then?", "\ud83d\udc4d Same problem.", "The crash with hub.KerasLayer and an image module from tfhub.dev should be fixed by updating to their latest versions:\r\nhttps://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/3\r\nhttps://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/3\r\nFor details, see issue https://github.com/tensorflow/hub/issues/287\r\n\r\nCan this issue be closed now, or is anyone using it to track the DenseLayer and LSTM problems with similar error messages but probably different root causes?", "Any updates on ValueError: Unknown layer: DenseFeatures ?", "Have the same issue when do model.save('model.h5') under tensorflow 1.13.1", "> Have the same issue when do model.save('model.h5') under tensorflow 1.13.1\r\n\r\n\r\nRecommended to install tf-nightly or tf-nightly-gpu also\r\nand test follow https://github.com/tensorflow/tensorflow/issues/26835#issuecomment-488972247", "resolved by updating tf version to 2.0\r\n\r\n\u53d1\u81ea\u6211\u7684 iPhone\r\n\r\n\u5728 2019\u5e745\u670820\u65e5\uff0c17:47\uff0cOooO <notifications@github.com<mailto:notifications@github.com>> \u5199\u9053\uff1a\r\n\r\n\r\nHave the same issue when do model.save('model.h5') under tensorflow 1.13.1\r\nRecommended to install tf-nightly or tf-nightly-gpu also\r\n\r\nand test follow #26835 (comment)<https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F26835%23issuecomment-488972247&data=02%7C01%7C%7C7e2e94c948854a0bf13508d6dd08346b%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636939424643466257&sdata=M5tDagIFSyWvX%2Bh%2BX1eWmEPBeT8Xeaqrb7HyaTHTWHI%3D&reserved=0>\r\n\r\n\u2014\r\nYou are receiving this because you commented.\r\nReply to this email directly, view it on GitHub<https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F26835%3Femail_source%3Dnotifications%26email_token%3DAGN6QFXOJJJ5TODYI7LYTWLPWJXT5A5CNFSM4G7HUWZKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVYIHMY%23issuecomment-493913011&data=02%7C01%7C%7C7e2e94c948854a0bf13508d6dd08346b%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636939424643476268&sdata=LbB7PLQ68j1qcnPjTiwa67qxvU80XsE%2B9jCNdXeEPL8%3D&reserved=0>, or mute the thread<https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAGN6QFRPEUTENZOMTXRUTH3PWJXT5ANCNFSM4G7HUWZA&data=02%7C01%7C%7C7e2e94c948854a0bf13508d6dd08346b%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636939424643486273&sdata=8UOTn%2B%2Bu6IBnXmjw0omM6PKA1gLIqASEi%2BexYO5LsmQ%3D&reserved=0>.\r\n", "Same for Unknown layer: DenseFeatures", "I find a solution [https://github.com/tensorflow/tensorflow/issues/26496#issuecomment-477246896](https://github.com/tensorflow/tensorflow/issues/26496#issuecomment-477246896)\r\n\r\n```py\r\n# Load the Keras model into a session.\r\nkeras_model = tf.keras.models.load_model(\r\n    keras_file, custom_objects={'VladPooling': VladPooling})\r\nsess = tf.keras.backend.get_session()\r\n\r\n# Convert the model.\r\nconverter = tf.lite.TFLiteConverter.from_session(sess, keras_model.inputs,\r\n                                                 keras_model.outputs)\r\nconverter.convert()\r\n```", "Has anyone found a solution for how I can save a model with a feature layer. Right now I saved the weights of my model and then I rebuild the model and fit the model with epochs= 0; seems to do the trick for now. The results are repeatable, but this seems like it is not the most professional way to do it. ", "This is fixed in latest tf-nightly build '2.0.0-dev20190802'.\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26835\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26835\">No</a>\n", "> I think I found the solution (or another workaround).\r\n> \r\n> Instead of:\r\n> \r\n> ```\r\n> model.save('path_to_my_model.h5')\r\n> ```\r\n> \r\n> Call:\r\n> \r\n> ```\r\n> tf.keras.experimental.export_saved_model(model, 'path_to_my_model.h5')\r\n> ```\r\n> \r\n> Instead of:\r\n> \r\n> ```\r\n> new_model = keras.models.load_model('path_to_my_model.h5')\r\n> new_model.summary()\r\n> ```\r\n> \r\n> Call:\r\n> \r\n> ```\r\n> reloaded_model = tf.keras.experimental.load_from_saved_model('path_to_my_model.h5', custom_objects={'KerasLayer':hub.KerasLayer})\r\n> print(reloaded_model.get_config())\r\n> \r\n> #Get input shape from model.get_config()\r\n> reloaded_model.build((None, 224, 224, 3))\r\n> reloaded_model.summary()\r\n> ```\r\n> \r\n> Works like a charm. :)\r\n> \r\n> Colaboratory with the code above: https://colab.research.google.com/drive/17YerjXGUwqp48mB3-WSTipv4itizuOCP\r\n\r\n Thank you, It is worked for me in this way:\r\nMy error was: `ValueError: Unknown layer: BatchNorm`. As I have used `KL.BatchNormalization` class to compose this custom layer, I solved the error as follows:\r\n\r\n```\r\nfrom keras.models import load_model\r\nimport keras.layers as KL\r\ntrained_model_path = \"./RESULTS/2019_10_30_10_17_53/bestmodel/MultiLabel_BIODI_weights.best.hdf5\"\r\nBIODI_model = load_model(trained_model_path, custom_objects={'BatchNorm':KL.BatchNormalization})\r\nBIODI_model.summary()\r\n```", "> Not perfect, but there is a workaround:\r\n> \r\n> SImply create the model from scratch every time (instead of loading from JSON/YAML) and then load the weights.\r\n> \r\n> When using the DenseFeatures layer, the model will not be built after calling compile, so you will not be able to load the weights right away. You must either manually call the build() method or run a single batch through the model and then load the weights.\r\n\r\nThis is what I do every time but is there a faster way?", "same unknown layer: kerasLayer", "> I use follow this code it work\r\n> \r\n> ```\r\n> model.save('./model/saved.h5')\r\n> new_model = tf.keras.models.load_model('./model/saved.h5',custom_objects='KerasLayer':hub.KerasLayer})\r\n>  new_model.summary()\r\n> ```\r\n> \r\n> https://colab.research.google.com/drive/1JLkVWpXhDwImuF930i5PWstJvu4ENDoc\r\n\r\ndon't miss '{' after 'custom_objects='\r\n", " Oh, thanks for pointing out. I was also wondering why my code didn't run last time.\u00a0\n    On Wednesday, 27 May 2020, 05:20:00 pm GMT+8, AlAlaa Tashkandi <notifications@github.com> wrote:  \n \n \n\n\n\nI use follow this code it work\nmodel.save('./model/saved.h5')\nnew_model = tf.keras.models.load_model('./model/saved.h5',custom_objects='KerasLayer':hub.KerasLayer})\n new_model.summary()\n\nhttps://colab.research.google.com/drive/1JLkVWpXhDwImuF930i5PWstJvu4ENDoc\n\n\ndon't miss '{' after 'custom_objects='\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or unsubscribe.\n  ", "> _Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template_\r\n> \r\n> **System information**\r\n> Run on Colab example from copied from tensorflow/hub: https://colab.research.google.com/drive/1ymmzlWHieCuXR7UjXezULnYsNsDeHc9m\r\n> \r\n> Version: 2.0.0-alpha0\r\n> Eager mode: True\r\n> Hub version: 0.3.0\r\n> GPU is available\r\n> \r\n> **The issue**\r\n> When I try to run Colab example from tensorflow/hub for image retraining and then save/load model according to [the documentation](https://www.tensorflow.org/alpha/guide/keras/saving_and_serializing), there are some issues with it.\r\n> \r\n> After retraining, when we try to save the model::\r\n> \r\n> ```\r\n> # Save the model\r\n> model.save('path_to_my_model.h5')\r\n> ```\r\n> \r\n> This error occurs (already described here: #26811 )\r\n> \r\n> ```\r\n> //...skipped, see ticket above for details ^\r\n> h5py/_objects.pyx in h5py._objects.with_phil.wrapper()\r\n> h5py/_objects.pyx in h5py._objects.with_phil.wrapper()\r\n> h5py/h5o.pyx in h5py.h5o.link()\r\n> RuntimeError: Unable to create link (name already exists)\r\n> ```\r\n> \r\n> But actually `path_to_my_model.h5` file is created. Then when I try to load it via:\r\n> \r\n> ```\r\n> # Recreate the exact same model purely from the file\r\n> new_model = keras.models.load_model('path_to_my_model.h5')\r\n> new_model.summary()\r\n> ```\r\n> \r\n> I get an error:\r\n> \r\n> ```\r\n> ValueError: Unknown layer: KerasLayer\r\n> ```\r\n> \r\n> Entire stacktrace:\r\n> \r\n> ```\r\n> ---------------------------------------------------------------------------\r\n> ValueError                                Traceback (most recent call last)\r\n> <ipython-input-15-0ceb2f1e6ad5> in <module>()\r\n> ----> 1 new_model = keras.models.load_model('path_to_my_model.h5')\r\n>       2 \r\n>       3 new_model.summary()\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/hdf5_format.py in load_model(filepath, custom_objects, compile)\r\n>     213     model_config = json.loads(model_config.decode('utf-8'))\r\n>     214     model = model_config_lib.model_from_config(model_config,\r\n> --> 215                                                custom_objects=custom_objects)\r\n>     216 \r\n>     217     # set weights\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/model_config.py in model_from_config(config, custom_objects)\r\n>      53                     '`Sequential.from_config(config)`?')\r\n>      54   from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top\r\n> ---> 55   return deserialize(config, custom_objects=custom_objects)\r\n>      56 \r\n>      57 \r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/serialization.py in deserialize(config, custom_objects)\r\n>      93       module_objects=globs,\r\n>      94       custom_objects=custom_objects,\r\n> ---> 95       printable_module_name='layer')\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)\r\n>     190             custom_objects=dict(\r\n>     191                 list(_GLOBAL_CUSTOM_OBJECTS.items()) +\r\n> --> 192                 list(custom_objects.items())))\r\n>     193       with CustomObjectScope(custom_objects):\r\n>     194         return cls.from_config(cls_config)\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py in from_config(cls, config, custom_objects)\r\n>     349     for layer_config in layer_configs:\r\n>     350       layer = layer_module.deserialize(layer_config,\r\n> --> 351                                        custom_objects=custom_objects)\r\n>     352       model.add(layer)\r\n>     353     if not model.inputs and build_input_shape:\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/serialization.py in deserialize(config, custom_objects)\r\n>      93       module_objects=globs,\r\n>      94       custom_objects=custom_objects,\r\n> ---> 95       printable_module_name='layer')\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)\r\n>     179     config = identifier\r\n>     180     (cls, cls_config) = class_and_config_for_serialized_keras_object(\r\n> --> 181         config, module_objects, custom_objects, printable_module_name)\r\n>     182 \r\n>     183     if hasattr(cls, 'from_config'):\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py in class_and_config_for_serialized_keras_object(config, module_objects, custom_objects, printable_module_name)\r\n>     164     cls = module_objects.get(class_name)\r\n>     165     if cls is None:\r\n> --> 166       raise ValueError('Unknown ' + printable_module_name + ': ' + class_name)\r\n>     167   return (cls, config['config'])\r\n>     168 \r\n> \r\n> ValueError: Unknown layer: KerasLayer\r\n> ```\r\n> \r\n> Entire colab to review: https://colab.research.google.com/drive/1ymmzlWHieCuXR7UjXezULnYsNsDeHc9m\r\n\r\nuse this\r\ntf.keras.models.load_model('model.h5',custom_objects={'KerasLayer':tfhub.KerasLayer})", "Use this code instead :\r\ntf.keras.models.load_model('DogVisionModel.h5',custom_objects={'KerasLayer':tfhub.KerasLayer})", " Thanks! I will update the code according to your post. I really appreciate your effort.\n    On Sunday, 31 May 2020, 04:52:21 pm GMT+8, Philip Purwoko <notifications@github.com> wrote:  \n \n \n\n\nUse this code instead :\ntf.keras.models.load_model('DogVisionModel.h5',custom_objects={'KerasLayer':tfhub.KerasLayer})\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or unsubscribe.\n  ", "> I use follow this code it work\r\n> \r\n> ```\r\n> model.save('./model/saved.h5')\r\n> new_model = tf.keras.models.load_model('./model/saved.h5',custom_objects='KerasLayer':hub.KerasLayer})\r\n>  new_model.summary()\r\n> ```\r\n> \r\n> https://colab.research.google.com/drive/1JLkVWpXhDwImuF930i5PWstJvu4ENDoc\r\n\r\nAdding the KeralLayer inside the model_from_json command also did the trick for me.\r\n```\r\nwith open(os.path.join(os.getcwd(), 'model_three\\\\model_english_google_news_without_OOV_{0}batchsize_{1}.json'.format(str(dense_layer_batch_size), version_data_control)),'r') as f:\r\n        model_json = json.load(f)\r\n\r\nmodel_imported = model_from_json(model_json, custom_objects={'KerasLayer':hub.KerasLayer})\r\n\r\nmodel_imported.load_weights(os.path.join(os.getcwd(), 'model_three\\\\model_english_google_news_without_OOV_{0}batchsize_{1}.h5'.format(str(dense_layer_batch_size), version_data_control)))\r\n\r\nmodel_imported.compile(optimizer=optimizer_adam_v2(x_train_data_shape, training_validation_split_ratio, dense_layer_batch_size),\r\n                           loss=model_loss_function,\r\n                           metrics=model_metric_function)\r\nprint(type(model_imported))\r\nprint(\"\\nModel is loaded successfully\\n\")\r\n```", " Thanks for the effort Nikos Spanos. I'll be updating my colab code.    On Sunday, July 5, 2020, 06:33:15 PM GMT+8, Nikos Spanos <notifications@github.com> wrote:  \n \n \n\n\n\nI use follow this code it work\nmodel.save('./model/saved.h5')\nnew_model = tf.keras.models.load_model('./model/saved.h5',custom_objects='KerasLayer':hub.KerasLayer})\n new_model.summary()\n\nhttps://colab.research.google.com/drive/1JLkVWpXhDwImuF930i5PWstJvu4ENDoc\n\n\nAdding the KeralLayer inside the model_from_json command also did the trick for me.\nwith open(os.path.join(os.getcwd(), 'model_three\\\\model_english_google_news_without_OOV_{0}batchsize_{1}.json'.format(str(dense_layer_batch_size), version_data_control)),'r') as f:\n        model_json = json.load(f)\n\nmodel_imported = model_from_json(model_json, custom_objects={'KerasLayer':hub.KerasLayer})\n\nmodel_imported.load_weights(os.path.join(os.getcwd(), 'model_three\\\\model_english_google_news_without_OOV_{0}batchsize_{1}.h5'.format(str(dense_layer_batch_size), version_data_control)))\n\nmodel_imported.compile(optimizer=optimizer_adam_v2(x_train_data_shape, training_validation_split_ratio, dense_layer_batch_size),\n                           loss=model_loss_function,\n                           metrics=model_metric_function)\nprint(type(model_imported))\nprint(\"\\nModel is loaded successfully\\n\")\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or unsubscribe.\n  ", "> I'm wondering if it's a good idea to use tf.keras.applications.MobileNetV2 model instead of the module loaded from TF Hub.\r\n> It seems working after quick checks - tested model saving in this example: https://www.tensorflow.org/alpha/tutorials/images/transfer_learning\r\n\r\nAgree, I'm discarding TF Hub for the moment because of that issue,  tf.keras.applications.MobileNetV2 worked just fine.", "In my case, I could solve this problem by updating tensorflow from 2.2.0 to 2.3.0", "# SAVE MODEL\r\nMODEL_BASE_PATH = \"MODEL\"\r\nPROJECT_NAME = model_name\r\nSAVE_MODEL_NAME = 'model.h5'\r\nsave_model_path = os.path.join(MODEL_BASE_PATH, PROJECT_NAME, SAVE_MODEL_NAME)\r\nif os.path.exists(os.path.join(MODEL_BASE_PATH, PROJECT_NAME)) == False:\r\n    os.makedirs(os.path.join(MODEL_BASE_PATH, PROJECT_NAME))\r\n    \r\nprint('Saving Model At {}...'.format(save_model_path))\r\nmodel.save(save_model_path,include_optimizer=False)\r\n\r\n\r\n\r\n#LOAD MODEL\r\nfrom tensorflow.keras.models import load_model\r\nMODEL_PATH = 'MODEL/model_name/model.h5'\r\nmodel = load_model(MODEL_PATH,compile=False, custom_objects={'KerasLayer': hub.KerasLayer})\r\n\r\n\r\n\r\n\r\n"]}, {"number": 26834, "title": "Lite: Kernel Util bug fix", "body": "1:> Bug fix in \r\n\r\n> PopulateConvolutionQuantizationParams()\r\n\r\n * Bias is an optional parameter in case of per-tensor quantization.\r\n\r\n2:> New test cases added to improve coverage", "comments": ["@karimnosseir : Gentle Reminder!", "Nagging Reviewer @karimnosseir: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "Sorry for the delay as i was Out of office past few weeks.\r\nAssigning to Jian"]}, {"number": 26833, "title": "TOCO/TFLite Converter not working", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 1.14.1-dev20190314\r\n- Python version: Python3.5.2\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI am trying to convert a yolo model to tflite, I have generated the .pb file. I am able to get predictions working on Tensorflow using a session run. While converting the same .pb file to tflite through the python api it throws the following error \r\n\r\nConverterError: TOCO failed. See console for info.\r\n2019-03-18 16:34:02.329321: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1012 operators, 1562 arrays (0 quantized)\r\n2019-03-18 16:34:02.392648: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1012 operators, 1562 arrays (0 quantized)\r\n2019-03-18 16:34:03.052789: F tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc:722] Check failed: start_array.data_type == ArrayDataType::kInt32 Range op inputs must be int32.\r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x00007f254cb60700 (most recent call first):\r\n File \"/usr/local/lib/python3.5/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33 in execute\r\n File \"/usr/local/lib/python3.5/dist-packages/absl/app.py\", line 251 in _run_main\r\n File \"/usr/local/lib/python3.5/dist-packages/absl/app.py\", line 300 in run\r\n File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 40 in run\r\n File \"/usr/local/lib/python3.5/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59 in main\r\n File \"/usr/local/bin/toco_from_protos\", line 11 in <module>\r\nAborted (core dumped)\r\n\r\nIf i do it using the tflite_convert commandline function I am able to generate the tflite file however the predictions are horrible and they are not working at all. \r\n\r\nWhen I am predicting using tensorflow graph the output tensor is as follows:\r\n\r\narray([[[-9.8978394e+01, -1.3161835e+01,  1.3285416e+02, ...,\r\n         8.2985699e-02,  7.4172002e-01,  3.4630752e-01],\r\n       [-4.7924599e+01, -1.0267001e+02,  9.4346962e+01, ...,\r\n         3.8488835e-02,  7.7538919e-01,  4.5305699e-01],\r\n       [-1.7301196e+02, -1.9800723e+02,  2.0602176e+02, ...,\r\n         1.3685286e-02,  7.9602063e-01,  5.4424524e-01],\r\n       ...,\r\n       [ 3.9877176e+02,  4.0956854e+02,  4.2418051e+02, ...,\r\n         5.5837166e-01,  4.6688780e-01,  3.5518968e-01],\r\n       [ 4.0362198e+02,  3.9286102e+02,  4.1983481e+02, ...,\r\n         7.3692095e-01,  4.5420480e-01,  5.1171225e-01],\r\n       [ 3.8272733e+02,  4.0869794e+02,  4.4242825e+02, ...,\r\n         6.1386418e-01,  6.0733950e-01,  3.9755249e-01]]], dtype=float32)\r\n\r\nOutput tensor from tensorflow lite for same image and same configuration\r\narray([[[-7.69360809e+01, -9.79493904e+00,  1.13176025e+02, ...,\r\n         4.96306121e-02,  5.01996398e-01,  5.42349815e-01],\r\n       [-4.04162636e+01, -8.99354248e+01,  8.99407501e+01, ...,\r\n         1.81048810e-02,  4.99900073e-01,  7.85123646e-01],\r\n       [-1.43075928e+02, -1.83144836e+02,  1.79637024e+02, ...,\r\n         4.17280197e-03,  6.15250051e-01,  8.26103389e-01],\r\n       ...,\r\n       [ 3.99175842e+02,  4.08667114e+02,  4.26062622e+02, ...,\r\n         5.26856482e-01,  5.87077022e-01,  3.69019330e-01],\r\n       [ 4.06181152e+02,  3.69634338e+02,  4.14379700e+02, ...,\r\n         8.10274899e-01,  4.71839100e-01,  6.61600232e-01],\r\n       [ 3.91825989e+02,  4.08976501e+02,  4.34132080e+02, ...,\r\n         5.45497298e-01,  6.76900625e-01,  4.91856068e-01]]],\r\n     dtype=float32)\r\n\r\nCode to convert graph to tflite [not working]: \r\ntflite_yolo_model='test.tflite'\r\nwith tf.Session(graph=frozenGraph,config=config) as sess:\r\n   frozen_graph_def = tf.graph_util.convert_variables_to_constants(\r\n     sess, sess.graph_def, ['output_boxes'])\r\nconverter=tf.lite.TFLiteConverter.from_frozen_graph(frozen_model,['inputs'],['output_boxes'])\r\nconverter.target_ops=[tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model=converter.convert()\r\n#tflite_model=tf.lite.toco_convert(frozen_graph_def, input_tensors=[img_resized],output_tensors=[boxes])\r\nopen(tflite_yolo_model, \"wb\").write(tflite_model)\r\n", "comments": ["Which YOLO model is this? v2? v3? Also is it the full YOLO model or Tiny YOLO?", "> Which YOLO model is this? v2? v3? Also is it the full YOLO model or Tiny YOLO?\r\n\r\nFull YOLOv3", "And what did you do to initially convert it from Darknet to Tensorflow?", "You can try tensorflow==1.10.0. ", "> And what did you do to initially convert it from Darknet to Tensorflow?\r\n\r\nI tried using tools from \r\n- https://github.com/mystic123/tensorflow-yolo-v3\r\n- https://github.com/jinyu121/DW2TF\r\n\r\nand used the .pb files generated from these two tools. ", "> You can try tensorflow==1.10.0.\r\n\r\nI was able to convert to tflite format with 1.12.0 after a lot of issues however if i try to use the interpreter using the same version it says that some operations are not suppored 'ResizeNearest' ", "> > You can try tensorflow==1.10.0.\r\n> \r\n> I was able to convert to tflite format with 1.12.0 after a lot of issues however if i try to use the interpreter using the same version it says that some operations are not suppored 'ResizeNearest'\r\n\r\nYou can try to quickly create a tensorflow1.10.0 environment with the virtual environment under Linux. I have tried to use TF1.13.0 for a mistake. I am not very clear about whether it is possible to use TF1.12.0.", "Can you try converting your model using [```TFLITE_BUILTINS```, ```SELECT_TF_OPS```](https://www.tensorflow.org/lite/guide/ops_select) if haven't already?", "Hello, I am doing exactly the same and I have that same issue also? Did you find any solution to it?", "I already tried and with tensorflow 1.12.0 it is not possible since the library tf.lite is not included in this version, it is in tensorflow 1.13.0 only", "i am using the nightly build of tensorflow\r\npython 3.6.6\r\nos mac\r\n\r\ni've got a .h5 saved model\r\nusing \r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model_file('model.h5')\r\n    converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                        tf.lite.OpsSet.SELECT_TF_OPS]\r\n    tflite_model = converter.convert()\r\n    open(\"model.tflite\",'wb').write(tflite_model)\r\n```\r\n\r\n\r\nit fails while trying to convert\r\nand i get\r\n\r\n\r\n\r\n ```\r\nFile \"loadtrain.py\", line 51, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/Users/darnelle/anaconda3/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 729, in convert\r\n    **converter_kwargs)\r\n  File \"/Users/darnelle/anaconda3/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 410, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"/Users/darnelle/anaconda3/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 176, in toco_convert_protos\r\n    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n2019-03-25 01:02:13.314209: F tensorflow/lite/toco/import_tensorflow.cc:2599] Check failed: status.ok() Unexpected value for attribute 'data_format'. Expected 'NHWC'\r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x00000001194505c0 (most recent call first):\r\n  File \"/Users/darnelle/anaconda3/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33 in execute\r\n  File \"/Users/darnelle/anaconda3/lib/python3.6/site-packages/absl/app.py\", line 251 in _run_main\r\n  File \"/Users/darnelle/anaconda3/lib/python3.6/site-packages/absl/app.py\", line 300 in run\r\n  File \"/Users/darnelle/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40 in run\r\n  File \"/Users/darnelle/anaconda3/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59 in main\r\n  File \"/Users/darnelle/anaconda3/bin/toco_from_protos\", line 10 in <module>\r\n\r\n```", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. \r\nWe encourage other users facing similar problems to post a separate issue by providing all the information asked by the template. Thanks!", "> Can you try converting your model using [`TFLITE_BUILTINS`, `SELECT_TF_OPS`](https://www.tensorflow.org/lite/guide/ops_select) if haven't already?\r\n\r\nI have tried this but it still does not work properly.", "> Hello, I am doing exactly the same and I have that same issue also? Did you find any solution to it?\r\n\r\nI've not found a solution to this yet.", "> i am using the nightly build of tensorflow\r\n> python 3.6.6\r\n> os mac\r\n> \r\n> i've got a .h5 saved model\r\n> using\r\n> \r\n> ```\r\n> converter = tf.lite.TFLiteConverter.from_keras_model_file('model.h5')\r\n>     converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n>                         tf.lite.OpsSet.SELECT_TF_OPS]\r\n>     tflite_model = converter.convert()\r\n>     open(\"model.tflite\",'wb').write(tflite_model)\r\n> ```\r\n> \r\n> it fails while trying to convert\r\n> and i get\r\n> \r\n> ```\r\n> File \"loadtrain.py\", line 51, in <module>\r\n>    tflite_model = converter.convert()\r\n>  File \"/Users/darnelle/anaconda3/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 729, in convert\r\n>    **converter_kwargs)\r\n>  File \"/Users/darnelle/anaconda3/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 410, in toco_convert_impl\r\n>    input_data.SerializeToString())\r\n>  File \"/Users/darnelle/anaconda3/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 176, in toco_convert_protos\r\n>    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n> tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n> 2019-03-25 01:02:13.314209: F tensorflow/lite/toco/import_tensorflow.cc:2599] Check failed: status.ok() Unexpected value for attribute 'data_format'. Expected 'NHWC'\r\n> Fatal Python error: Aborted\r\n> \r\n> Current thread 0x00000001194505c0 (most recent call first):\r\n>  File \"/Users/darnelle/anaconda3/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33 in execute\r\n>  File \"/Users/darnelle/anaconda3/lib/python3.6/site-packages/absl/app.py\", line 251 in _run_main\r\n>  File \"/Users/darnelle/anaconda3/lib/python3.6/site-packages/absl/app.py\", line 300 in run\r\n>  File \"/Users/darnelle/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40 in run\r\n>  File \"/Users/darnelle/anaconda3/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59 in main\r\n>  File \"/Users/darnelle/anaconda3/bin/toco_from_protos\", line 10 in <module>\r\n> ```\r\n\r\nHow did you get the .h5 file? You need to change the data format while converting from yolo weights to other format", "> > > You can try tensorflow==1.10.0.\r\n> > \r\n> > \r\n> > I was able to convert to tflite format with 1.12.0 after a lot of issues however if i try to use the interpreter using the same version it says that some operations are not suppored 'ResizeNearest'\r\n> \r\n> You can try to quickly create a tensorflow1.10.0 environment with the virtual environment under Linux. I have tried to use TF1.13.0 for a mistake. I am not very clear about whether it is possible to use TF1.12.0.\r\n\r\nI am not able to get it to work on TF 1.10.0", "> > i am using the nightly build of tensorflow\r\n> > python 3.6.6\r\n> > os mac\r\n> > i've got a .h5 saved model\r\n> > using\r\n> > ```\r\n> > converter = tf.lite.TFLiteConverter.from_keras_model_file('model.h5')\r\n> >     converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n> >                         tf.lite.OpsSet.SELECT_TF_OPS]\r\n> >     tflite_model = converter.convert()\r\n> >     open(\"model.tflite\",'wb').write(tflite_model)\r\n> > ```\r\n> > it fails while trying to convert\r\n> > and i get\r\n> > ```\r\n> > File \"loadtrain.py\", line 51, in <module>\r\n> >    tflite_model = converter.convert()\r\n> >  File \"/Users/darnelle/anaconda3/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 729, in convert\r\n> >    **converter_kwargs)\r\n> >  File \"/Users/darnelle/anaconda3/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 410, in toco_convert_impl\r\n> >    input_data.SerializeToString())\r\n> >  File \"/Users/darnelle/anaconda3/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 176, in toco_convert_protos\r\n> >    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n> > tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n> > 2019-03-25 01:02:13.314209: F tensorflow/lite/toco/import_tensorflow.cc:2599] Check failed: status.ok() Unexpected value for attribute 'data_format'. Expected 'NHWC'\r\n> > Fatal Python error: Aborted\r\n> > \r\n> > Current thread 0x00000001194505c0 (most recent call first):\r\n> >  File \"/Users/darnelle/anaconda3/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33 in execute\r\n> >  File \"/Users/darnelle/anaconda3/lib/python3.6/site-packages/absl/app.py\", line 251 in _run_main\r\n> >  File \"/Users/darnelle/anaconda3/lib/python3.6/site-packages/absl/app.py\", line 300 in run\r\n> >  File \"/Users/darnelle/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40 in run\r\n> >  File \"/Users/darnelle/anaconda3/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59 in main\r\n> >  File \"/Users/darnelle/anaconda3/bin/toco_from_protos\", line 10 in <module>\r\n> > ```\r\n> \r\n> How did you get the .h5 file? You need to change the data format while converting from yolo weights to other format\r\n\r\ni used model.save('model.h5') method\r\n\r\nhow do i change the data format im new to machine learning and A.i\r\n\r\ni noticed the error said the data format is NHWC WHICH since im new i have no idea what ", "> > > i am using the nightly build of tensorflow\r\n> > > python 3.6.6\r\n> > > os mac\r\n> > > i've got a .h5 saved model\r\n> > > using\r\n> > > ```\r\n> > > converter = tf.lite.TFLiteConverter.from_keras_model_file('model.h5')\r\n> > >     converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n> > >                         tf.lite.OpsSet.SELECT_TF_OPS]\r\n> > >     tflite_model = converter.convert()\r\n> > >     open(\"model.tflite\",'wb').write(tflite_model)\r\n> > > ```\r\n> > > it fails while trying to convert\r\n> > > and i get\r\n> > > ```\r\n> > > File \"loadtrain.py\", line 51, in <module>\r\n> > >    tflite_model = converter.convert()\r\n> > >  File \"/Users/darnelle/anaconda3/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 729, in convert\r\n> > >    **converter_kwargs)\r\n> > >  File \"/Users/darnelle/anaconda3/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 410, in toco_convert_impl\r\n> > >    input_data.SerializeToString())\r\n> > >  File \"/Users/darnelle/anaconda3/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 176, in toco_convert_protos\r\n> > >    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n> > > tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n> > > 2019-03-25 01:02:13.314209: F tensorflow/lite/toco/import_tensorflow.cc:2599] Check failed: status.ok() Unexpected value for attribute 'data_format'. Expected 'NHWC'\r\n> > > Fatal Python error: Aborted\r\n> > > \r\n> > > Current thread 0x00000001194505c0 (most recent call first):\r\n> > >  File \"/Users/darnelle/anaconda3/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33 in execute\r\n> > >  File \"/Users/darnelle/anaconda3/lib/python3.6/site-packages/absl/app.py\", line 251 in _run_main\r\n> > >  File \"/Users/darnelle/anaconda3/lib/python3.6/site-packages/absl/app.py\", line 300 in run\r\n> > >  File \"/Users/darnelle/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40 in run\r\n> > >  File \"/Users/darnelle/anaconda3/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59 in main\r\n> > >  File \"/Users/darnelle/anaconda3/bin/toco_from_protos\", line 10 in <module>\r\n> > > ```\r\n> > \r\n> > \r\n> > How did you get the .h5 file? You need to change the data format while converting from yolo weights to other format\r\n> \r\n> i used model.save('model.h5') method\r\n> \r\n> how do i change the data format im new to machine learning and A.i\r\n> \r\n> i noticed the error said the data format is NHWC WHICH since im new i have no idea what\r\n\r\nThis might help [freeze graph](https://stackoverflow.com/questions/47014306/freeze-graph-with-different-data-format)", "Same Issue .\r\nModel saves succesfully in tensorflow\r\nWhile converting using code:\r\n\r\n`import tensorflow.compat.v1 as tf\r\n\r\ntf.disable_v2_behavior()\r\ntf.enable_eager_execution\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('C:/Users/ajayb/OneDrive/Desktop/roj/SM/1557730360',signature_key='predict')\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n`\r\n\r\n> \r\n\r\nCurrent thread 0x000023d8 (most recent call first):\r\n  File \"c:\\users\\ajayb\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 33 in execute\r\n  File \"c:\\users\\ajayb\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\absl\\app.py\", line 251 in _run_main\r\n  File \"c:\\users\\ajayb\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\absl\\app.py\", line 300 in run\r\n  File \"c:\\users\\ajayb\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40 in run\r\n  File \"c:\\users\\ajayb\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 59 in main\r\n  File \"C:\\Users\\ajayb\\AppData\\Local\\Programs\\Python\\Python36\\Scripts\\toco_from_protos.exe\\__main__.py\", line 9 in <module>\r\n  File \"c:\\users\\ajayb\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py\", line 85 in _run_code\r\n  File \"c:\\users\\ajayb\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py\", line 193 in _run_module_as_main\r\n\r\n> ", "@SirCastic1,@reorder-cv \r\n\r\nsimilar issue, do you have any ideas? Thanks!\r\n\r\n  File \"/root/.local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 205, in toco_convert_protos\r\n    **\"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n/bin/sh: 1: toco_from_protos: not found**\r\n\r\nenvironment and codes\r\nprint(tensorflow.__version__)\r\n1.13.1\r\n\r\n```\r\nimport tensorflow\r\nfrom tensorflow import lite\r\n\r\nmodel_name = \"tf_serving_keras_mobilenetv2\"\r\nmodel_path = f\"models/{model_name}.h5\"\r\nconverter = lite.TFLiteConverter.from_keras_model_file(model_path)\r\ntfmodel = converter.convert()\r\nopen (\"model.tflite\" , \"wb\").write(f\"models/{tfmodel}\")\r\n```", "> \r\n> \r\n> @SirCastic1,@reorder-cv\r\n> \r\n> similar issue, do you have any ideas? Thanks!\r\n> \r\n> File \"/root/.local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 205, in toco_convert_protos\r\n> **\"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr)) tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info. /bin/sh: 1: toco_from_protos: not found**\r\n> \r\n> environment and codes\r\n> print(tensorflow.**version**)\r\n> 1.13.1\r\n> \r\n> ```\r\n> import tensorflow\r\n> from tensorflow import lite\r\n> \r\n> model_name = \"tf_serving_keras_mobilenetv2\"\r\n> model_path = f\"models/{model_name}.h5\"\r\n> converter = lite.TFLiteConverter.from_keras_model_file(model_path)\r\n> tfmodel = converter.convert()\r\n> open (\"model.tflite\" , \"wb\").write(f\"models/{tfmodel}\")\r\n> ```\r\n\r\nTried the tflite converter using commandline?\r\nFor me it says \r\n\r\n    `ValueError: No 'serving_default' in the SavedModel's SignatureDefs. Possible values are 'predict'.`\r\n\r\nUnfortunately I am not sure how to set serving_default as 'predict' in command line. I am only able  to set using python api which is failing me.\r\n\r\nIf anyone can help me out with this:\r\nhttps://stackoverflow.com/questions/56112190/how-to-succesfully-convert-my-tensorflow-savedmodel-to-tflite-using-tflite-conve\r\n", "@SirCastic1,\r\nsolved with correction at the bold text\r\n\r\nprint(tensorflow.__version__)\r\n1.14.1-dev20190513\r\n\r\nimport tensorflow\r\nfrom tensorflow import lite\r\n\r\nmodel_name = \"tf_serving_keras_mobilenetv2\"\r\ndir=\"/N/share/project/weisheng/build/mobilenet_v2/keras/models\"\r\nmodel_path = f\"{dir}/{model_name}.h5\"\r\nconverter = lite.TFLiteConverter.from_keras_model_file(model_path)\r\ntfmodel = converter.convert()\r\nopen (f\"{dir}/model.tflite\" , \"wb\").write(**tfmodel**)\r\n", "What happened with this, did anyone find the solution? I am encountering the same issue while converting the .pb model to .tflite using `TFLiteConverter.from_frozen_graph`.", "> What happened with this, did anyone find the solution? I am encountering the same issue while converting the .pb model to .tflite using `TFLiteConverter.from_frozen_graph`.\r\n\r\nthis was a small personal project i was interested in. and since i couldnt find any solution or help about NHWC. its on a pause", "> \u53d1\u751f\u4e86\u4ec0\u4e48\u4e8b\uff0c\u6709\u4eba\u627e\u5230\u89e3\u51b3\u65b9\u6848\u4e86\u5417\uff1f\u5728\u4f7f\u7528\u5c06.pb\u6a21\u578b\u8f6c\u6362\u4e3a.tflite\u65f6\u9047\u5230\u76f8\u540c\u7684\u95ee\u9898`TFLiteConverter.from_frozen_graph`\u3002\r\n\r\ni meet this problem too, but i can't solve it", "@reorder-cv I am also facing a similar problem while converting my tensorflowModel.pb to tflite using toco 1.13.1.\r\n\r\nI was going through the converted model and found that tflite converted model has some difference compared with the original model.pb(saved before conversion to tflite).\r\nIt seems the range op conversion has some problem. And  this seems to be the input for the expand dims.\r\n_Tensorflow Model(original)_ [Model Link](https://drive.google.com/file/d/1DLzCOb9p8c_s86qNW_7QIsyikfRjkhK9/view?usp=sharing)\r\n![tensorflow_range](https://user-images.githubusercontent.com/5344072/85119479-a3454400-b243-11ea-946f-ad6ece115221.png)\r\n\r\n\r\n_Tflite Model(converted)_ [Model Link](https://drive.google.com/file/d/1Naomfft5yA7nbto_8aD72S2bC-GCW6rM/view?usp=sharing)\r\n![tflite_range](https://user-images.githubusercontent.com/5344072/85119483-a50f0780-b243-11ea-835d-9fa2c77deba9.png)\r\n\r\nCan anyone help why is this happening? \r\nP.S the model conversion showed no error while using toco api. \r\n[My conversion code](https://gist.github.com/bmabir17/754a6e0450ec4fd5e25e462af949cde6)\r\n\r\nN.B I have also opened [another issue](https://github.com/tensorflow/tensorflow/issues/39179) for running the model on andorid"]}, {"number": 26832, "title": "Update README.md file of TFLite Android Example", "body": "Please check. @perfinion ", "comments": ["@kyscg please check now.", "@arp95 could you please resolve the conflicts? Thanks!", "@arp95 Did you get a chance to check build failures? Please let us know on the update. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 26831, "title": "Update README.md file of Tensorflow Lite Android example", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: 1.7.1\r\n- Doc Link: - \r\n\r\n\r\n**Describe the documentation issue**\r\n\r\nUpdate the README.md file of Tensorflow Lite Android Example.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n\r\nYes.", "comments": ["Can you please provide the doc link and elaborate on the issue? Thanks!", "@ymodak i have updated the readme as seen in the pr. Please review.", "@arp95 \r\n\r\nDocumentation has been updated in latest TF versions.Closing the issue.Please, feel free to reopen if necessary.Thanks!\r\n"]}, {"number": 26830, "title": "[micro] change --std=c++11 to -std=c++11 in Makefile.", "body": "", "comments": []}, {"number": 26829, "title": "Saving and Restoring SingularMoniteredSession in tensorflow 1.4.1", "body": "I want to save and restore a SingularMoniteredSession in my code \r\n\r\n```\r\nwith g.as_default():\r\n    .......\r\n    with tf.train.SingularMoniteredSession() as sess:\r\n        child_ops = .......\r\n        sess.run(child_ops)\r\n        controller_ops=..........\r\n        sess.run(controller_ops)\r\n```\r\nI want to save and restore the controller ops session here(second session). but i wasn't able to find any documentation regarding it.", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n"]}, {"number": 26828, "title": "Is it possible to do inference using a pre-trained tensorflow model from inside cuda kernel?", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 26827, "title": "Migrate TF for https://github.com/bazelbuild/bazel/issues/6942", "body": "Since Bazel 0.22 static_runtime_libs and dynamic_runtime_libs attributes\r\nare not mandatory, and since 0.23 they are replaced by their singluar\r\nsiblings.\r\n\r\nDetails: https://github.com/bazelbuild/bazel/issues/6942", "comments": ["CC @meteorcloudy ", "We need to bump the minimal Bazel version to 0.22.0", "I'm upgrading TF Bazel version using on CI to 0.23.2, after that you can bump the minimal Bazel version here:\r\nhttps://github.com/tensorflow/tensorflow/blob/5bc5a4b562170fd922b9e6cb56cd5e0b15429928/configure.py#L1624", "@hlopko can you please check failed tests"]}, {"number": 26826, "title": "1.13.1 wheels for Python 3.7 do not work on RHEL/CentOS7", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RedHat Enterprise Linux 7.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.13.1\r\n- Python version: 2.7.2\r\n- Installed using virtualenv? pip? conda?: virtualenv+pip\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the problem**\r\n\r\nWe noticed that TensorFlow 1.13.1 now comes with wheels for Python 3.7 as well:\r\n\r\ntensorflow-1.13.1-cp37-cp37m-manylinux1_x86_64.whl\r\n\r\nWe first upgraded to 1.13.1 on our existing Python 3.6, which worked\r\nfine. Then we tried upgrading to Python 3.7, but even importing\r\ntensorflow fails there, because the shared libraries need a newer\r\nlibstdc++:\r\n\r\nE   ImportError: Traceback (most recent call last):\r\nE     File \"/home/anon/git/sensors/venv3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\nE       from tensorflow.python.pywrap_tensorflow_internal import *\r\nE     File \"/home/anon/git/sensors/venv3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\nE       _pywrap_tensorflow_internal = swig_import_helper()\r\nE     File \"/home/anon/git/sensors/venv3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\nE       _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nE     File \"/opt/anon/python-3.7.2-master-201903180831/lib/python3.7/imp.py\", line 242, in load_module\r\nE       return load_dynamic(name, filename, file)\r\nE     File \"/opt/anon/python-3.7.2-master-201903180831/lib/python3.7/imp.py\", line 342, in load_dynamic\r\nE       return _load(spec)\r\nE   ImportError: /lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /home/anon/git/sensors/venv3/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\nE   \r\nE   \r\nE   Failed to load the native TensorFlow runtime.\r\nE   \r\nE   See https://www.tensorflow.org/install/errors\r\nE   \r\nE   for some common reasons and solutions.  Include the entire stack trace\r\nE   above this error message when asking for help.\r\n\r\nThis is error is logical, because RHEL/CentOS 7 only has support for 1.3.7 and earlier:\r\n\r\n(venv3) [anon@vdesk sensors]$ uname -a \r\nLinux vdesk.localdomain 3.10.0-957.5.1.el7.x86_64 #1 SMP Wed Dec 19 10:46:58 EST 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n(venv3) [anon@vdesk sensors]$ cat /etc/redhat-release \r\nRed Hat Enterprise Linux Server release 7.6 (Maipo)\r\n(venv3) [anon@vdesk sensors]$ strings /lib64/libstdc++.so.6 | grep CXXABI\r\nCXXABI_1.3\r\nCXXABI_1.3.1\r\nCXXABI_1.3.2\r\nCXXABI_1.3.3\r\nCXXABI_1.3.4\r\nCXXABI_1.3.5\r\nCXXABI_1.3.6\r\nCXXABI_1.3.7\r\nCXXABI_TM_1\r\n(venv3) [anon@vdesk sensors]$ gcc --version\r\ngcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\nIf you unzip the 3.6 and 3.7 wheels for TF 1.13.1, you can see that the 3.6 wheel is compiled on Ubuntu 14.04 with GCC 4.8:\r\n\r\n[anon@vdesk python]$ strings python36/tensorflow-1.13.1.data/purelib/tensorflow/libtensorflow_framework.so | grep -i ubuntu\r\nGCC: (Ubuntu 4.8.5-4ubuntu8~14.04.2) 4.8.5\r\n\r\nand the 3.7 wheel is compiled on Ubuntu 16.04 with GCC 5.4:\r\n\r\n[anon@vdesk python]$ strings python37/tensorflow-1.13.1.data/purelib/tensorflow/libtensorflow_framework.so | grep -i ubuntu\r\nGCC: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\n\r\nRHEL/CentOS 7 also has GCC 4.8.x, so that's why the Python 3.6 wheels work.\r\n\r\nIs there any chance you could build the Python 3.7 wheels on Ubuntu 14.04 as well?\r\n\r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nJust importing tensorflow; works fine with the Python3.6 wheels.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I also got this issue (TF 1.13, CentOS 7, CUDA 10, python 3.7), which is caused by a old glibc version. \r\nAre you sure it works on python 3.6?", "Yes, it works with python 3.6, because those binaries are built on Ubuntu 14.04 with the same version of GCC as on RHEL7. I use the CPU version of TF though, so perhaps it's different for the GPU version ...", "It seems that TF for Python 3.7 are built on Ubuntu 16, and TF for Python 3.6 and lower are built on Ubuntu 14. \r\nUbuntu 16's glibc is higher than Ubuntu 14, which is the same as CentOS 7.", "Correct, we had to build our python 3.7 binaries on ubuntu 16, because we were not able to setup python 3.7 fully on ubuntu 14. We had to disable ssl, which caused our builds to fail.\r\nWe are exploring builds on CentOS, but I cannot offer an immediate solution for prebuilt binaries.\r\nYou should be able to build TF from sources on CentOS for a quick solution.", "@dverstap Is this still an issue? Did you try the workaround suggested by @gunan. Please close the issue, if it was already resolved. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 26825, "title": "[doc] fix an error in the comment.", "body": "", "comments": ["@csukuangfj could you please resolve the conflicts? Thanks!", "@gbaned\r\nIt takes so long to get a reply that I deleted my repository some time ago.\r\n\r\nI will close this pullrequest and reopen it at #28594 with all conflicts solved."]}, {"number": 26824, "title": "The contents of Transfer learning tutorial and its github source are not the same.", "body": "https://www.tensorflow.org/alpha/tutorials/images/transfer_learning\r\nThis is the tutorial for transfer learning for tensorflow 2.0\r\nbut if you click on run on colab or view source on github buttons,\r\nthen the contents are not the same \r\n(colab link : https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb\r\ngithub link :\r\nhttps://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb\r\n)\r\n\r\nwhich one is the latest one and which one I should follow?\r\n\r\nThank you in advance!", "comments": ["@wongongv  is correct.\r\nBoth colab link and github link are wrong. The current link is not for tensorflow 2.0\r\nThey should direct to this link: https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/images/transfer_learning.ipynb\r\n", "Thanks.\r\nThese links have been fixed upstream: https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/images/transfer_learning.ipynb\r\nAnd will be reflected on the website when the next site publish happens.\r\n\r\nIf you're learning, I'd go with the alpha/r2 docs (though some of them are still in alpha condition)", "Hi, @lamberta  \r\nI would like to know if the website team have a plan to check all the link on TF2 tutorial or not?\r\nBecause I just found another case in [\"Load images with tf.data\"](https://www.tensorflow.org/alpha/tutorials/load_data/images) and [\"Using TFRecords and tf.Example\"](https://www.tensorflow.org/alpha/tutorials/load_data/tf_records). \r\nThey should link to https://github.com/tensorflow/docs/tree/master/site/en/r2/tutorials/load_data \r\nI know that TF team are all busy so I just want to remind.\r\nRegards."]}, {"number": 26823, "title": "Performance hit in WALSModel?", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): from conda\r\n- TensorFlow version (use command below): 1.10 gpu from conda\r\n- Python version:  3.7 from conda\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: GX 1070 maxq\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\nSuddenly running update row operations became laggy and never progressed...\r\n\r\n**Describe the expected behavior**\r\n\r\nupdate row operations should not run indefinetly \r\n\r\n```python\r\n\r\n\r\nfrom mymodule  import ex\r\nimport gc, resource, multiprocessing\r\n\r\ndef run_ex(print_usage=False, *args, **kwargs):\r\n    ex.run(config_updates=kwargs)\r\n    if print_usage:\r\n        usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\r\n        print('resources used: {}'.format(usage))\r\n    gc.collect()\r\n\r\ndef pool_run(*args, processes=1, print_usage=False, **kwargs):\r\n    with multiprocessing.Pool(processes=processes) as pool:\r\n        results = pool.apply(run_ex, (print_usage, *args), kwargs)\r\n    gc.collect()\r\n    return results\r\n\r\npool_run(processes=1, print_usage=True, **opts)\r\n\r\n\r\n# ex is a wrapper that calls\r\n\r\ndef train_fn(\r\n    model,                                   # <--- WALSModel\r\n    input_tensor,                         # tf.SparseTensor\r\n    iterations:int,\r\n    sp_sparse=None,                 # sp.sparse.coo_matrix\r\n    sacred_run=None,              \r\n    print_progress:bool=True,\r\n    logger=None\r\n):\r\n    os.environ['KMP_DUPLICATE_LIB_OK']='True'\r\n    # extract row and column factors\r\n    row_factor = model.row_factors[0]\r\n    col_factor = model.col_factors[0]\r\n\r\n    # update operations\r\n    row_update_op = model.update_row_factors(sp_input=input_tensor)#[1]\r\n    col_update_op = model.update_col_factors(sp_input=input_tensor)#[1]\r\n\r\n\r\n\r\n    with tf.Session() as sess:\r\n        # init model\r\n        sess.run(model.initialize_op)\r\n        sess.run(model.worker_init)\r\n\r\n        if print_progress:\r\n            status = Sil(total=iterations)\r\n        for i in range(iterations):\r\n            if logger is not None: logger.debug('Training iteration {}/{}'.format(str(i), str(iterations)))\r\n            # update rows\r\n            if logger is not None: logger.debug('Updating rows')   # <--- I see this in logs\r\n            sess.run(model.row_update_prep_gramian_op)\r\n            sess.run(model.initialize_row_update_op)\r\n            _, _, loss, reg, _ = sess.run(row_update_op)\r\n            if sacred_run is not None: sacred_run.log_scalar(\"loss.row\", loss, i)   \r\n\r\n            # update cols\r\n            if logger is not None: logger.debug('Updating columns')   # <--- never makes it here\r\n            sess.run(model.col_update_prep_gramian_op)\r\n            sess.run(model.initialize_col_update_op)\r\n            _, _, loss, reg, _ = sess.run(col_update_op)\r\n            if sacred_run is not None: sacred_run.log_scalar(\"loss.col\", loss, i)\r\n\r\n            # update status\r\n            if print_progress:\r\n                status.tick(prefix='iteration')\r\n\r\n            if sacred_run is not None and sp_sparse is not None:\r\n                if logger is not None: logger.debug('Calculating rmse.')\r\n                rf = row_factor.eval(session=sess)\r\n                cf = col_factor.eval(session=sess)\r\n                it_rmse = rmse(sp_sparse, rf, cf)\r\n                sacred_run.log_scalar(\"rmse\", it_rmse, i)\r\n\r\n\r\n        # eval row / col factors\r\n        output_row = row_factor.eval(session=sess)\r\n        output_col = col_factor.eval(session=sess)\r\n        sess.close()\r\n    os.environ['KMP_DUPLICATE_LIB_OK']='False'\r\n    return output_row, output_col\r\n\r\n```\r\n\r\n\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 26822, "title": "Provides example for tf.argsort and tf.sort", "body": "Fixes issue #26533", "comments": ["So, any more changes or it's good now?", "Nagging Reviewer : You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 15 days with no activity and the `awaiting review` label has been applied."]}, {"number": 26821, "title": "tf.cond on a variable. FailedPreconditionError in tf.global_variables_initializer()", "body": "I am running into  FailedPreconditionError error in tf.global_variables_initializer(). I have zeroed-in on the following part of the code to be the cultprit:\r\n\r\n    def __init__(...):\r\n        ...\r\n        self.global_step = tf.get_variable(initializer=tf.zeros_initializer(), trainable=False, shape=(), name='global_step')\r\n        ...\r\n        step_rampup_value = self.step_rampup(self.global_step, self.rampup_length)\r\n\r\n    def step_rampup(self, global_step, rampup_length):\r\n        result = tf.cond(global_step < rampup_length,\r\n                         lambda: tf.constant(0.0),\r\n                         lambda: tf.constant(1.0))\r\n        return tf.identity(result, name=\"step_rampup\")\r\n    session.run(tf.global_variables_initilizer())\r\n\r\nself.global_step is to be incremented by 1 by optimizer at each iteration. It's value has to change. So, that is the behavior i want.\r\n\r\nError message:\r\n\r\n    FailedPreconditionError ...\r\n    506         with tf.Session(graph=highgraph) as session:\r\n    --> 507             session.run(tf.global_variables_initializer())\r\n    ...\r\n    FailedPreconditionError: Attempting to use uninitialized value global_step\r\n\t [[node global_step/read (defined at NML_U/sNeural.py:103)  = Identity[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](global_step)]]\r\n\r\nWhy is that part of the code is culprit?\r\nBecause, The following code works\r\n\r\n    def __init__(...):\r\n        ...\r\n        self.global_step = tf.get_variable(initializer=tf.zeros_initializer(), trainable=False, shape=(), name='global_step')\r\n        ...\r\n        step_rampup_value = self.step_rampup(self.global_step, self.rampup_length)\r\n\r\n    def step_rampup(self, global_step, rampup_length):\r\n        result = tf.cond(global_step.initialized_value() < rampup_length,\r\n                         lambda: tf.constant(0.0),\r\n                         lambda: tf.constant(1.0))\r\n        return tf.identity(result, name=\"step_rampup\")\r\n    session.run(tf.global_variables_initilizer())\r\n\r\nbut that will evaluate the conditional with the initialized value of self.global_step(=0) each time which is not the intended behavior\r\n\r\nAlso,\r\n\r\nThis code works as well:\r\n\r\n    def __init__(...):\r\n        ...\r\n        self.global_step = tf.get_variable(initializer=tf.zeros_initializer(), trainable=False, shape=(), name='global_step')\r\n        self.global_step = tf.assign(self.global_step,0.)\r\n        ...\r\n        step_rampup_value = self.step_rampup(self.global_step, self.rampup_length)\r\n\r\n    def step_rampup(self, global_step, rampup_length):\r\n        result = tf.cond(global_step < rampup_length,\r\n                         lambda: tf.constant(0.0),\r\n                         lambda: tf.constant(1.0))\r\n        return tf.identity(result, name=\"step_rampup\")\r\n    session.run(tf.global_variables_initilizer())\r\n\r\nBut (maybe) this will again not lead to the dependency on global_step but  instead on assign op which will keep assigning 0 to self.global_step\r\n\r\nHow do i go about achieving the behavior \r\n\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 26820, "title": "Added New scenarios in the File for the TCs", "body": "Added new scenarios in the file.", "comments": ["Nagging Reviewer @karimnosseir: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 15 days with no activity and the `awaiting review` label has been applied.", "Closing this PR as a similar one is already merged.\r\n\r\nRegards\r\nAmit"]}, {"number": 26819, "title": "how to assign trainable variable to a tensor to form another trainable variable", "body": "### System information\r\n\r\nTensorFlow version : 1.12\r\nAre you willing to contribute it : Yes\r\n\r\n### Describe the feature and the current behavior/state.\r\n\r\nThis issue occurs in my nerual network. The data A stored in TFrecord file is with shape [batch_size,  A_size * A_size]. I want to use a fully connected layer to increase the dimension to get a bigger tensor B with shape [batch_size, B_size * B_size]. Then zero-padding B to get a bigger C with shape [batch_size, C_size, C_size]. \r\n\r\n> Note: B[i,:,:] is a block extracted from A[i,:,:][position_height_index * B_size:(position_height_index + 1) * B_size, position_width_index * B_size:(position_width_index + 1) * B_size]\r\n \r\nHere is my code:\r\n```python\r\ninput_size = A_size * A_size\r\noutput_size = B_size * B_size\r\n\r\n w = tf.Variable(tf.random_normal([input_size, output_size]))\r\n b = tf.Variable(0.0)\r\n\r\n# A is a data tensor with shape [batch_size,  A_size * A_size]\r\n\r\n prediction = tf.matmul(A,w)+b\r\n\r\n C = tf.Variable(np.zeros([batch_size, C_size * C_size]), True, dtype = tf.float32)\r\n \r\nfor i in range(batch_size):\r\n            \r\n            # initialize temp\r\n            temp = tf.Variable(np.zeros([C_size, C_size]), True, dtype=tf.float32)\r\n            \r\n            # assign reshaped prediction to temp with corresponding positions\r\n            temp[position_height_index[i] * B_size:(position_height_index[i] + 1) * B_size,\r\n            position_width_index[i] * B_size:(position_width_index[i] + 1) * \r\n            B_size].assign(tf.reshape( prediction[i], [B_size, B_size]))\r\n            \r\n           # assign reshaped temp to C\r\n           C[i].assign(tf.reshape(temp, [1, C_size * C_size]))\r\n     \r\n  ```\r\n### My Question: \r\nWhen i print C, all elements in C are still zero. It should be non-zero because i assign prediction to C, what's wrong. I assign trainable variable prediction to a zero tesnor C to zero-padding predcition. I hope C is trainable. Is it allowed?\r\n\r\n### Will this change the current api? How?\r\nNo\r\n### Who will benefit with this feature?\r\nSome researchers of image processing\r\nAny Other info.", "comments": ["I don't see in your code where you use session.run to make the assignments happen. \r\n\r\nAre you using eager execution?", "Thanks for your reply!\r\n\r\nI'm not using eager execution.\r\n\r\nThe above code is the neural network i defined, it ruturns C as the prediction of the neural network. \r\nI name the network as NT, the input of NT is A and the output of NT is C:\r\nC=NT(A)\r\n\r\n\r\nI assign the prediction to a zero tensor C, the C should be non-zero and trainable. But when i run the following code to print C, all elements in C are still zero.\r\n\r\n\r\nHere is the code:\r\n\r\nwith tf.Session() as sess:\r\n       sess.run(tf.global_variables_initializer())\r\n       C_example= sess.run(C, feed_dict={A: train_data_batch})\r\n       C_sum=sess.run(tf.reduce_sum(C_example))\r\nprint(C_sum)\r\nprint(C)", "@alextp ", "Right but when you do c[i].assign without using the return value you create an assignment operation in the graph but never actually execute it.\r\n\r\nYou have two options to make this code work. The first is in the TF 1.x style adding a lot of control dependencies to build an update op which depends on all assignments; the second is to use [tf.function](https://www.tensorflow.org/alpha/tutorials/eager/tf_function) (which is available in 1.13 but with fewer bugs in nightly) to build your graph and automatically add control dependencies for you.", "Sorry for not response promptly. \r\nThanks for your reply\uff01\r\n\r\nI tried your first options\u2014\u2014add a lot of control dependencies. But it still doesn't work. Here is the code:\r\n\r\ninput_size = A_size * A_size\r\noutput_size = B_size * B_size\r\n\r\nw = tf.Variable ( tf.random_normal ( [input_size , output_size ] ) )\r\nb = tf.Variable ( 0.0 )\r\n\r\n\r\n\r\nprediction = tf.matmul ( A , w ) + b\r\n\r\nC = tf.Variable ( np.zeros ( [ batch_size , C_size * C_size ] ) , True, dtype = tf.float32 )\r\n \r\nfor i in range ( batch_size ) :\r\n         \r\n            temp = tf.Variable ( np.zeros ( [ C_size , C_size ] ) , True , dtype = tf.float32 )\r\n          \r\n            ops_1= tf.assign ( temp [ position_height_index [ i ] * B_size : ( position_height_index [ i ] + 1 ) * B_size, position_width_index [ i ] * B_size : ( position_width_index [ i ] + 1 ) * B_size ] , ( tf.reshape ( prediction [ i ] , [ B_size , B_size ] ) ) )\r\n            \r\n \r\n           with tf.control_dependencies ( [ ops_1 ] ) :\r\n                      tf.assign ( C [ i ] , ( tf.reshape ( temp, [ 1, C_size * C_size ] ) ) )\r\n\r\n\r\nGrateful for helping me!\r\n\r\n", "there is still nothing depending on the last tf.assign on your code, so it won't execute.\r\n\r\nYou should switch to tf2 alpha as eager execution fits the style of code you're trying to write much better."]}, {"number": 26818, "title": "[TF2.0] Unknown TensorShape error when convert tf.keras model to tflite", "body": "*Environment*\r\nWin 7 64bit\r\nPyCharm\r\nAnaconda Python 3.6\r\ntensorflow-gpu 2.0.0a0\r\n\r\n\r\n*issue*\r\nI trained a tf.keras model, now I want to convert it to tflite file.\r\nThe following is my implement:\r\n\r\nsaved_model.save(model, 'testM')\r\nmodel = tf.saved_model.load('testM')\r\nconcrete_func = model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_concrete_function(concrete_func)\r\ntflite_model = converter.convert()\r\nopen(\"new_classificaton.tflite\", \"wb\").write(tflite_model)\r\n\r\nWhen I execute it with PyCharm, the following error log is reported:\r\n\r\nTraceback (most recent call last):\r\n  File \"E:/Keras/PycharmProjects/7241/HelloTF2/HelloWorld.py\", line 73, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\HelloTF2\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 261, in convert\r\n    shape_list = tensor.get_shape().as_list()\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\HelloTF2\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\", line 1128, in as_list\r\n    raise ValueError(\"as_list() is not defined on an unknown TensorShape.\")\r\nValueError: as_list() is not defined on an unknown TensorShape.\r\n\r\nIs this a bug? Or I misunderstand the api usage?", "comments": ["Apologies for the delay in response. You have to create a concrete function before the model conversion to tf lite. Please take a look at an [example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r2/convert/python_api.md#exporting-a-keras-model-).", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Same for me: `tf.__version__ == '2.0.0-alpha0'`\r\n\r\n```\r\npretrained_model = tf.keras.applications.MobileNet()\r\ntf.saved_model.save(pretrained_model, \"tmp_mobilenet/1/\")\r\nloaded = tf.saved_model.load(\"tmp_mobilenet/1/\")\r\ninfer = loaded.signatures[\"serving_default\"]\r\nconverter = tf.lite.TFLiteConverter.from_concrete_function(infer)\r\nconverter.convert()\r\n```\r\n\r\nresults in \r\n`ValueError: as_list() is not defined on an unknown TensorShape`"]}, {"number": 26817, "title": "Fix: typo in gpu_hlo_support_checker.h", "body": "There is a comment typo in gpu_hlo_support_checker.h.  \r\nThis patch fixes it.", "comments": []}, {"number": 26816, "title": "AttributeError: 'module' object has no attribute 'Session' in tensorflow/tensorflow:latest-gpu-jupyter  TF 2.0-alpah0 in Docker Container ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04, \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary/From official Docker Container\r\n- TensorFlow version (use command below);\r\n- Python version:2.0.0-alpha0\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: Using TF GPU Docker/Official\r\n- GPU model and memory:NVIDIA V100 ,32 Gb,NVIDIA-SMI 384.145\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I run a simple code snippet I get the following error\r\n`AttributeError: 'module' object has no attribute 'Session'\r\n\r\n```\r\n# Creates a graph.\r\nwith tf.device('/gpu:0'):\r\n  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\n  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\nc = tf.matmul(a, b)\r\n# Creates a session with log_device_placement set to True.\r\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n# Runs the op.\r\nprint(sess.run(c))\r\n```\r\n*Output*\r\n```\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-7-9c6adda4f265> in <module>()\r\n      5 c = tf.matmul(a, b)\r\n      6 # Creates a session with log_device_placement set to True.\r\n----> 7 sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n      8 # Runs the op.\r\n\r\n**AttributeError: 'module' object has no attribute 'Session'**\r\n\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe basic code should work. The TF in the Docker container should be updated and  build\r\n\r\n**Code to reproduce the issue**\r\nGiven\r\n\r\n**Other info / logs**\r\nReinstalling TF in Docker container works\r\npip install --upgrade --force-reinstall tensorflow-gpu\r\n(this installs TF 1.13 version and unistalls the TF 2.0)\r\n\r\n", "comments": ["To clarify things, by using cuda 8.0 and cuDNN 7, tensorflow extracts the correct output for the operations you specified. That is:\r\n[[22. 28.]\r\n [49. 64.]]\r\n\r\nDoes this imply that this be a problem of Tensorflow Docker specifically? ", "I'm not sure, but I'm afraid Session will be removed ? https://github.com/tensorflow/community/pull/20 cc @alextp ", "It's by design that TF 2.0 does not have tf.Session.", "@alextp What replaced it? This is really hard to find!", "Nothing replaced it!\n\nIn TF2 you have eager execution on by default, so running a TF operation\nlike tf.matmul will immediately schedule the computation to compute the\nresults. If you want you can trace a sequence of operations using\ntf.function to get a thing you can call many times with better performance\nthan executing things eagerly.\n\nPlease look at any documentation or tutorial about TF2 to see more details.\n\nOn Fri, Aug 2, 2019 at 7:27 PM Russell Jurney <notifications@github.com>\nwrote:\n\n> @alextp <https://github.com/alextp> What replaced it? This is really hard\n> to find!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26816?email_source=notifications&email_token=AAABHROHOUKW5OSEUCQ33DDQCTUKNA5CNFSM4G7DYGE2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3PFHJY#issuecomment-517886887>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRIZ7M2V4ES34CPOJULQCTUKNANCNFSM4G7DYGEQ>\n> .\n>\n\n\n-- \n - Alex\n"]}, {"number": 26815, "title": "Added Activation,padding and stride scenarios.", "body": "This is one of the TODO in the file.", "comments": ["Nagging Reviewer @karimnosseir: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 15 days with no activity and the `awaiting review` label has been applied.", "@karimnosseir , would be great of you can review the PR and provide your feedback.\r\n\r\nRegards\r\nAmit", "Sorry for late reply as i was out of office past few weeks."]}, {"number": 26814, "title": "[TF2.0] Loading a Saved Model failed with `AttributeError: '_UserObject' object has no attribute '_create_or_restore_slot_variable'`", "body": "I am using the latest TF2.0 pip package.\r\n\r\nConsider the following code:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass ToyModel(tf.keras.Model):\r\n    \"\"\"A simple linear model.\"\"\"\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.l1 = tf.keras.layers.Dense(5)\r\n\r\n    @tf.function(input_signature=[tf.TensorSpec([None, 1], tf.float32, name=\"input_func\")])\r\n    def call(self, x):\r\n        return self.l1(x)\r\n\r\ndef toy_dataset():\r\n    inputs = tf.range(10.)[:, None]\r\n    outputs = inputs * 5. + tf.range(5.)[None, :]\r\n    # TODO: switch `tuple` to `dict`.\r\n    dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\r\n    return dataset.repeat(10).batch(2).shuffle(buffer_size=5)\r\n\r\n\r\ndataset = toy_dataset()\r\noptimizer = tf.keras.optimizers.Adam(0.1)\r\n\r\nmodel = ToyModel()\r\nmodel.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\r\nhistory = model.fit(dataset, epochs=5)\r\n\r\n# Inference works\r\ninputs = np.array([[0, 5]], dtype=np.float32).T\r\nprint(model(inputs))\r\n\r\n# Export to Saved Model\r\nmodel_path = \"/tmp/saved_model\"\r\ntf.saved_model.save(model, model_path)\r\n\r\n# Load model\r\nsaved_model = tf.saved_model.load(model_path)\r\n\r\n# Inference from Saved Model\r\n#inputs = np.array([[0, 5]], dtype=np.float32).T\r\n#saved_model(inputs)\r\n```\r\n\r\nIt fails with:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-2-4a1364881187> in <module>\r\n     37 \r\n     38 # Load model\r\n---> 39 saved_model = tf.saved_model.load(model_path)\r\n     40 \r\n     41 # Inference from Saved Model\r\n\r\n~/local/conda/envs/tf/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in load(export_dir, tags)\r\n    322       loader = _Loader(object_graph_proto,\r\n    323                        saved_model_proto,\r\n--> 324                        export_dir)\r\n    325       root = loader.get(0)\r\n    326   else:\r\n\r\n~/local/conda/envs/tf/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in __init__(self, object_graph_proto, saved_model_proto, export_dir)\r\n     62     self._setup_functions_structures()\r\n     63     self._setup_functions_captures()\r\n---> 64     self._restore_checkpoint()\r\n     65 \r\n     66     for node in self._nodes:\r\n\r\n~/local/conda/envs/tf/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in _restore_checkpoint(self)\r\n    151     saver = util.TrackableSaver(graph_view.ObjectGraphView(self.get(0)))\r\n    152     saver._file_prefix_placeholder = constant_op.constant(variables_path)\r\n--> 153     load_status = saver.restore(variables_path)\r\n    154     load_status.assert_existing_objects_matched()\r\n    155     checkpoint = load_status._checkpoint\r\n\r\n~/local/conda/envs/tf/lib/python3.7/site-packages/tensorflow/python/training/tracking/util.py in restore(self, save_path)\r\n   1094         graph_view=self._graph_view)\r\n   1095     base.CheckpointPosition(checkpoint=checkpoint, proto_id=0).restore(\r\n-> 1096         self._graph_view.root)\r\n   1097     load_status = CheckpointLoadStatus(\r\n   1098         checkpoint,\r\n\r\n~/local/conda/envs/tf/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py in restore(self, trackable)\r\n    207         # This object's correspondence with a checkpointed object is new, so\r\n    208         # process deferred restorations for it and its dependencies.\r\n--> 209         restore_ops = trackable._restore_from_checkpoint_position(self)  # pylint: disable=protected-access\r\n    210         if restore_ops:\r\n    211           self._checkpoint.new_restore_ops(restore_ops)\r\n\r\n~/local/conda/envs/tf/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py in _restore_from_checkpoint_position(self, checkpoint_position)\r\n    777           ._single_restoration_from_checkpoint_position(\r\n    778               checkpoint_position=current_position,\r\n--> 779               visit_queue=visit_queue)))\r\n    780     return restore_ops\r\n    781 \r\n\r\n~/local/conda/envs/tf/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py in _single_restoration_from_checkpoint_position(self, checkpoint_position, visit_queue)\r\n    804             child_position)\r\n    805       else:\r\n--> 806         if child_position.bind_object(trackable=local_object):\r\n    807           # This object's correspondence is new, so dependencies need to be\r\n    808           # visited. Delay doing it so that we get a breadth-first dependency\r\n\r\n~/local/conda/envs/tf/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py in bind_object(self, trackable)\r\n    249                       slot_name=slot_restoration.slot_name))\r\n    250         else:\r\n--> 251           optimizer_object._create_or_restore_slot_variable(  # pylint: disable=protected-access\r\n    252               slot_variable_position=CheckpointPosition(\r\n    253                   checkpoint=checkpoint,\r\n\r\nAttributeError: '_UserObject' object has no attribute '_create_or_restore_slot_variable'\r\n```", "comments": ["Maybe related to https://github.com/tensorflow/tensorflow/issues/25235 ?", "Sorry you ran into this; should be fixed by https://github.com/tensorflow/tensorflow/commit/147e6777068ea569cbecbd270b8b07c648bae09b\r\n\r\nI checked with 2.0.0-dev20190318 and the save and load works. \"call\" isn't there with that name; we should make subclassed Models export any explicitly-decorated methods. [This method](https://github.com/tensorflow/tensorflow/blob/ac3faa26869258b6149f154618feb08dccd810ff/tensorflow/python/keras/engine/training.py#L1801) just needs to be modified to include [something like AutoTrackable's search for attributes containing decorated functions](https://github.com/tensorflow/tensorflow/blob/ac3faa26869258b6149f154618feb08dccd810ff/tensorflow/python/training/tracking/tracking.py#L94). We could have Layer inherit from AutoTrackable and call super().", "Or we could just add a copy of the traced function under \"call\" instead of just \"_default_save_signature\".", "I confirm `2.0.0.dev20190318` fix the issue. Thanks @allenlavoie.", "I let you close since you want to address something with `call`.", "Yep, leaving open to track the call issue. Thanks for confirming.", "Models export all their methods now ([example](https://github.com/tensorflow/tensorflow/blob/a37f3e3f88e1a91f9418ace165e59986164a942e/tensorflow/python/saved_model/load_test.py#L1554)). So well and truly fixed now.", "use `tf.keras.models.load_model(path)` instead.", "I am having a similar issue. I recently installed anaconda 2020.02 on my Windows 10 laptop which comes with Spyder 4.0.1. .... then installed Tensorflow 2.1.0\r\n\r\n`import tensorflow as tf\r\ntrained_model=tf.saved_model.load(export_dir='model_folder')\r\ntrained_model.summary()`\r\n\r\nit fails with:\r\n\r\n> AttributeError: '_UserObject' object has no attribute 'summary'\r\n\r\nAny suggestions please?", "@LatinAmericanProgramer if `summary` is from a Keras Model, I'd use the Keras save/load wrappers (`tf.keras.Model.save(..., save_format='tf')`, `tf.keras.load_model`). That will revive Models as Models rather than just an object with traced `tf.function`s. Otherwise pure Python methods are not saved with the SavedModel.", "Thank you @allenlavoie ... I fix it uninstalling Tensorflow 2.1.0 and installing the version 2.2.0-dev20200327 using `pip install tf-nightly-gpu` . This fixed the problem for me.\r\n\r\nThank you", "I believe I have a similar problem... \r\n\r\nPython 3.7.6\r\nTensorFlow 2.0.0\r\n\r\nI am trying to save NN model ('Policy') with tf.keras.models.save_model and it raises AttributeError: 'Policy' object has no attribute 'built'.\r\n\r\nShould I save it using another method?\r\n\r\nThanks!"]}, {"number": 26813, "title": "can't import tensorflow.keras properly", "body": "I,m writing my code in vscode edit with tensorflow=1.13.1 version and anaconda virtual environment. But when I write 'from tensorflow.keras import layers',it give me a warning:\r\n\"unresolved import 'tensorflow.keras'(unresolved import)\". \r\nThe code can run as I expected,no errors. But because tensorflow.keras can't be imported properly,the auto-completion and intelligent hint function can't work,I need to search the function's usage everytime. I have thought it's the problem of vscode, but the problem came as well when I use pycharm IDE. Have anyone has the same problem? Is there anyone can help me?\r\n", "comments": ["The problem is present also in 2.0. Apparently, Jedi (or Pyls) cannot resolve Keras import.", "Works fine for me, with both ways of importing\r\n\"from tensorflow.python.keras import layers\"\r\nand\r\n\"from tensorflow.keras import layers\"\r\n\r\nCan you give us a little more info about your configuration?", "In my case I am using the 2.0 Nightly, the issue is related to autocomplete either using Jedi or Pyls (Python Language Server) with either Vim/Neovim/Pycharm or VS Code.\r\nThe issue is present also if linting with pylint as it cannot see Keras marking it as an erroneous import. The problem is common to all 4 machines we use in our lab. The only peculiar thing is that we run Arch Linux but Tensorflow is installed in a separate virtualenv. Every other module seems to work fine, Keras is the only problematic one.", "> Works fine for me, with both ways of importing\r\n> \"from tensorflow.python.keras import layers\"\r\n> and\r\n> \"from tensorflow.keras import layers\"\r\n> \r\n> Can you give us a little more info about your configuration?\r\n\r\nI works on windows10 platform, and I use the python plugin for vscode. It seems that just tensorflow's keras module can't be imported properly, i.e. vscode's automatic completion function didn't work but other modules works fine. ", "If the problem is related to the autocomplete functions of your IDEs, then maybe this isn't a tensorflow issue.\r\nCan you import from a python interpreter that does NOT use any autocomplete method? If you already tried that, does the problem persist?", "Importing keras in an interpreter works but all linters and autocomplete providers cannot resolve it. Unless it's something shared by all of them I would guess it's TF", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "The autocomplete never worked for me either. I'm using PyCharm (professional edition) and tf-nightly-gpu-2.0-preview version 2.0.0.dev20190314 (anyway, never worked since the very first TF 2.0 preview). \t\r\n\r\n![keras_autocomplete](https://user-images.githubusercontent.com/7030076/55887070-2d5c3c00-5bad-11e9-907e-be9539274664.png)", "You can use the v1 api like so:\r\n```python\r\n\r\nfrom tensorflow._api.v1.keras import Sequential\r\nfrom tensorflow._api.v1.keras.layers import Dense\r\n```\r\nor the tensorflow.python api like so:\r\n\r\n```python\r\nfrom tensorflow.python.keras import Sequential\r\nfrom tensorflow.python.keras.layers import Dense\r\n```", "Closing this issue since it is IDE related. Feel free to reopen if have any questions. Thanks!", "It solved my problem, thank you!\n\nAt 2019-04-17 18:00:06, \"Raphael Neumann\" <notifications@github.com> wrote:\n\n\nYou can use the v1 api like so:\n\nfrom tensorflow._api.v1.keras import Sequential\nfrom tensorflow._api.v1.keras.layers import Dense\n\nor the tensorflow.python api like so:\n\nfrom tensorflow.python.keras import Sequential\nfrom tensorflow.python.keras.layers import Dense\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.", "It seems because auto-complete won't scan the whole code of a library. I open `__init__.py` file whose path is `/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py` in my computer, and add one line `from tensorflow.python import keras`. It works for me. And you can find the path of the file by using finding definition of your IDE. I think it's a temporary way.", "this is still a problem for me, it cann't auto-complete the code.", "this is reproducible outside of IDEs using pylint\r\n```\r\npython -m pylint --disable=all --enable=E0401 model.py\r\n\r\n\r\n************* Module models\r\nmodels.py:1:0: E0401: Unable to import 'tensorflow.python.keras' (import-error)\r\nmodels.py:2:0: E0401: Unable to import 'tensorflow.keras.models' (import-error)\r\nmodels.py:3:0: E0401: Unable to import 'tensorflow.keras.layers' (import-error)\r\nmodels.py:5:0: E0401: Unable to import 'tensorflow.keras.layers' (import-error)\r\nmodels.py:6:0: E0401: Unable to import 'tensorflow.keras.activations' (import-error)\r\nmodels.py:7:0: E0401: Unable to import 'tensorflow.keras.optimizers' (import-error)\r\nmodels.py:8:0: E0401: Unable to import 'tensorflow.keras' (import-error)\r\nmodels.py:9:0: E0401: Unable to import 'tensorflow.keras' (import-error)\r\nmodels.py:11:0: E0401: Unable to import 'tensorflow.keras.layers' (import-error)\r\n```", "Please reopen. This is a real issue.", "Please reopen. This is a real issue.", "I third @astier. Getting `unresolved import 'tensorflow.keras.layers'` with\r\n\r\n```py\r\nfrom tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPool2D\r\n```\r\n\r\nImporting from `tensorflow.python.keras.layers` makes no difference. This is in TF r2.1.", "Using tensorflow 2.1.0 in VScode.\r\nI'm having the same issue.", "Having the same issue. Please reopen this ", "Having the same issue. Please reopen this", "same issue with VScode 1.42.1 & tensorflow 2.0.1", "Same issue TensorFlow 2.0 in PyCharm. ", "If you upgrade pycharm to 2019.3 version, this issue may can be solved.\n\nSoheila Zangeneh <notifications@github.com>\u4e8e2020\u5e742\u670820\u65e5 \u5468\u56db\u4e0a\u534810:25\u5199\u9053\uff1a\n\n> Same issue TensorFlow 2.0 in PyCharm.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26813?email_source=notifications&email_token=ALBFLYHYG7P7W7JRZOJDBETRDXLXFA5CNFSM4G7DFINKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMKL5IQ#issuecomment-588562082>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ALBFLYDITBG5QDCR5A67EMLRDXLXFANCNFSM4G7DFINA>\n> .\n>\n", "Same issue here. All other packages work fine.", "> If you upgrade pycharm to 2019.3 version, this issue may can be solved. Soheila Zangeneh <notifications@github.com>\u4e8e2020\u5e742\u670820\u65e5 \u5468\u56db\u4e0a\u534810:25\u5199\u9053\uff1a\r\n> [\u2026](#)\r\n\r\nWorked for me! Thanks!", "When I switched to use flake8 instead of pylint, all error message related to importing keras has gone. If you try, open VScode setting page and serach `Linting Enabled`, and\r\n\r\n1. Uncheck `Python \u203a Linting: Pylint Enabled`\r\n2. Check `Python \u203a Linting: Flake8 Enabled`\r\n\r\n-----\r\nOther solution?\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/15736#issuecomment-354642939", "I opened an issue for this on the vscode-python page: https://github.com/microsoft/vscode-python/issues/10598", "> \r\n> \r\n> When I switched to use flake8 instead of pylint, all error message related to importing keras has gone. If you try, open VScode setting page and serach `Linting Enabled`, and\r\n> \r\n>     1. Uncheck `Python \u203a Linting: Pylint Enabled`\r\n> \r\n>     2. Check `Python \u203a Linting: Flake8 Enabled`\r\n> \r\n> \r\n> Other solution?\r\n> \r\n> [#15736 (comment)](https://github.com/tensorflow/tensorflow/issues/15736#issuecomment-354642939)\r\n\r\nThat finally worked for me! Thanks a lot for that hint!", "Working with the 2.1.0. The problem persists only with the TensorFlow modules.  ", "uninstall extension Visual Studio IntelliCode works fine for me ", "Locking conversation to drive people encountering new issues with newer code to open new issue reports (filling in template) and to help people encountering the same issue find the solution here"]}]