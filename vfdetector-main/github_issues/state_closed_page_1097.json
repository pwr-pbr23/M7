[{"number": 20355, "title": "Gradient of tf.where returns NaN when it should not.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Centos 7\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: v1.8.0-3463-g39ea5a7044 1.10.0-dev20180620\r\n- **Python version**: Python 3.6.5\r\n- **Bazel version (if compiling from source)**: \r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: Cuda 9.0\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: \r\n\r\n```\r\nfrom __future__ import absolute_import, division, print_function\r\nimport tensorflow as tf\r\n\r\ntransfer = lambda x: tf.where(x < 0, 1./(1.-x), tf.log(x+1)) \r\n\r\ndef f(x):\r\n    xt = tf.placeholder(tf.float32, shape=(1,))\r\n    yt  = transfer(xt)\r\n    dydx = tf.gradients(ys=yt, xs=xt)[0]\r\n    with tf.Session() as sess:\r\n        rval = sess.run([xt, yt, dydx], feed_dict={xt:[x]})\r\n    return rval\r\n\r\nprint(f(1.))  # [1, 2, nan]\r\nprint(f(-1.)) # [-1, 0.5, nan]\r\nprint(f(0.)) # [0, 0, 1] (no nan here, interestingly)\r\n```\r\n\r\n### Problem\r\nGradients computed through the tf.where command erroneously returns nans. Presumably this is because the gradient computation computes the gradient through both conditions, which may result in a division by zero or log of zero.", "comments": ["The behavior is expected, see #2540", "Closing as duplicate. [#2540](https://github.com/tensorflow/tensorflow/issues/2540)"]}, {"number": 20354, "title": "No instructions for NDK setup", "body": "The setup steps for the NDK don't include how to install from an older version\r\n\r\nStep 2 on https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android#bazel", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: NA\r\nOS Platform and Distribution: NA\r\nTensorFlow installed from: NA\r\nTensorFlow version: NA\r\nBazel version: NA\r\nCUDA/cuDNN version: NA\r\nGPU model and memory: NA\r\nExact command to reproduce: NA", "Hi\r\nAndroid Studio:\r\nhttps://developer.android.com/ndk/guides/", "or download directly https://developer.android.com/ndk/downloads/", "Android Studio installs a version that is incompatible with Bazel (according to the guide. \r\n\r\nYou can download it directly, but the guide doesn't provide steps to the reader on how they should then install the locally downloaded NDK. Providing those steps for the reader or a link to those steps make it easier for them to complete the task. ", "http://dl.google.com/android/repository/android-ndk-r14b-linux-x86_64.zip", "This Dockerfile should also help\r\nhttps://github.com/tensorflow/models/blob/master/research/object_detection/dockerfiles/android/Dockerfile\r\nor use Android Studio"]}, {"number": 20353, "title": "convert frozon pb file to tflite but cannot find Output array which may be caused by that the frozen pb still have raining ops like Assign.", "body": "I cannot generate tflite file which may be caused that frozon pb file still have training ops.\r\n\r\nThe operation like below:\r\nuse \r\nbazel-bin/tensorflow/python/tools/freeze_graph \\\r\n  --input_graph=/data/frankzhu/tmp/t2t_train/mnist/graph.pbtxt\\\r\n  --input_checkpoint=/data/frankzhu/tmp/t2t_train/mnist/model.ckpt-2000 \\\r\n  --input_binary=false \\\r\n  --output_graph=/data/frankzhu/tmp/mnist.pb \\\r\n  --output_node_names=save/restore_all\r\n\r\nto get .tflite file\r\n\r\nthen use \r\n\r\nbazel run --config=opt \\\r\n  //tensorflow/contrib/lite/toco:toco -- \\\r\n  --input_file=/data/frankzhu/tmp/mnist.pb \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --output_file=/data/frankzhu/tmp/mnist_q8.tflite \\\r\n  --inference_type=QUANTIZED_UINT8 \\\r\n  --input_type=FLOAT \\\r\n  --input_arrays=input \\\r\n  --output_arrays=save/restore_all/NoOp --input_shapes=1,28,28,1\r\n\r\nBut get error:\r\n018-06-11 09:42:39.760033: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Assign\r\n2018-06-11 09:42:39.760052: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Assign\r\n2018-06-11 09:42:39.774575: F tensorflow/contrib/lite/toco/tooling_util.cc:806] Check failed: model.HasArray(output_array) Output array not found: save/restore_all/NoOp\r\n\r\nHelp from Andre Hentz' via TensorFlow Lite <tflite@tensorflow.org>, I was told that frozen graph may have training ops.\r\n\r\nBasically I would like to freeze the graph and convert it to quantized int8 tflite. If you need more details, could you please let me know?\r\n\r\n\r\n[mnist.tar.gz](https://github.com/tensorflow/tensorflow/files/2142432/mnist.tar.gz)\r\n\r\n\r\n\r\n\r\n\r\n  ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "not much related to environment but I can give my working environment:\r\n1.  I don't write my custom code\r\n2.  OS Platform and Distribution: Linux ubuntu 16.04\r\n3.  git clone https://github.com/tensorflow/tensorflow.git to get latest tensorflow  (origin/master)\r\n4.  bazel release 0.13.0\r\n5. CUDA 9.1\r\n6. GTX 1080 Ti, 11G\r\n7  mnist from https://github.com/tensorflow/tensor2tensor", "command to generate \r\nt2t-trainer   --generate_data   --data_dir=~/t2t_data   --output_dir=~/t2t_train/mnist   --problem=image_mnist   --model=shake_shake   --hparams_set=shake_shake_quick   --train_steps=1000   --eval_steps=100\r\n\r\n/home/frankzhu/TestPrj/tensor2tensor/tensor2tensor/bin/t2t-trainer \\\r\n  --generate_data \\\r\n  --data_dir=~/t2t_data \\\r\n  --output_dir=/data/frankzhu/tmp/t2t_train/mnist \\\r\n  --problem=image_mnist \\\r\n  --model=shake_shake \\\r\n  --hparams_set=shake_shake_quick \\\r\n  --train_steps=1000 \\\r\n  --eval_steps=100 \\\r\n  --profile", "Could you make your frozen graphdef available? From the message it looks like \"save/restore_all/NoOp\" is not an output of the model, but that might be misleading.", "file size too big like  32225296 Jul  5 09:51 mnist.zip so not allow me to upload. \r\nhere is where how I build\r\n#https://github.com/tensorflow/tensor2tensor#walkthrough\r\n1.\r\npip install tensor2tensor or use setup.py if you have problem\r\n\r\n\r\nt2t-trainer   --generate_data   --data_dir=~/t2t_data   --output_dir=~/t2t_train/mnist   --problem=image_mnist   --model=shake_shake   --hparams_set=shake_shake_quick   --train_steps=1000   --eval_steps=100\r\n\r\n/home/frankzhu/TestPrj/tensor2tensor/tensor2tensor/bin/t2t-trainer \\\r\n  --generate_data \\\r\n  --data_dir=~/t2t_data \\\r\n  --output_dir=/data/frankzhu/tmp/t2t_train/mnist \\\r\n  --problem=image_mnist \\\r\n  --model=shake_shake \\\r\n  --hparams_set=shake_shake_quick \\\r\n  --train_steps=1000 \\\r\n  --eval_steps=100 \\\r\n  --profile\r\n\r\nbazel-bin/tensorflow/python/tools/freeze_graph \\\r\n  --input_graph=/data/frankzhu/tmp/t2t_train/mnist/graph.pbtxt\\\r\n  --input_checkpoint=/data/frankzhu/tmp/t2t_train/mnist/model.ckpt-1000 \\\r\n  --input_binary=false \\\r\n  --output_graph=/data/frankzhu/tmp/mnist.pb \\\r\n  --output_node_names=save/restore_shard_1 \r\n\r\nhere I don't know if  \"--output_node_names=save/restore_shard_1\" is correct but without that, there will be error like \"You need to supply the name of a node to --output_node_names.\"\r\n\r\n\r\n\r\n\r\n", "Hi Andrew:\r\n\r\nIf you still need that file, do you any place to upload large size file?\r\nOr anything you want me to update from my side.\r\n\r\nThanks and best regards\r\nFrank", "@fzhu129 Please try \"--output_array=save/restore_all/NoOp\" instead of \"--output_arrays=save/restore_all/NoOp\"", "@myth01 I tried this before and have tried today but not working.\r\nbazel run --config=opt   //tensorflow/contrib/lite/toco:toco --   --input_file=/data/frankzhu/tmp/mnist.pb   --input_format=TENSORFLOW_GRAPHDEF   --output_format=TFLITE   --output_file=/data/frankzhu/tmp/mnist_q8.tflite   --inference_type=QUANTIZED_UINT8   --input_type=FLOAT   --input_arrays=input   **--output_array=save/restore_all/NoOp** --input_shapes=1,28,28,1 \r\n\r\nThe result is the same:\r\n2018-07-17 11:54:12.054096: F tensorflow/contrib/lite/toco/tooling_util.cc:806] Check failed: model.HasArray(output_array) Output array not found: save/restore_all/NoOp", "@fzhu129 In your freeze_graph command you use output_node_names=save/restore_shard_1. Did you try toco using the output node name instead of save/restore_all/NoOp", "@myth01 have tried output_node_names=save/restore_shard_1 already and  it does not work.", "Hi Suharsh:\r\nAny update from your side?\r\nThanks \r\nFrank", "Hi Frank,\r\n You cannot currently convert a training graph using TFLite. Note that TFLite only converts eval graphs via toco. For this purpose, you will need to create an eval mnist graph (without the optimizer)  as a pb and use it along with the checkpoint to call freeze_graph.", "Hi Raghuraman:\r\n\r\nfor tensor2tensor, \r\n\r\nDo you have an example upon creating eval mnist graph (without the optimizer) as a pb with tensor2tensor?\r\n\r\nSo far I use this \r\n/home/frankzhu/TestPrj/tensor2tensor/tensor2tensor/bin/t2t-trainer \r\n--generate_data \r\n--data_dir=~/t2t_data \r\n--output_dir=/data/frankzhu/tmp/t2t_train/mnist \r\n--problem=image_mnist \r\n--model=shake_shake \r\n--hparams_set=shake_shake_quick \r\n--train_steps=1000 \r\n--eval_steps=100 \r\n--profile\r\n\r\nhow to evaluate it I have not found the command.\r\n\r\nThank you very much in advance\r\nFrank\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "Hi I am not super familiar with tensor2tensor, but after a quick scan of their code, it seems they support saved model https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/bin/t2t_trainer.py#L171\r\n\r\nThis can then be converted with tflite_convert https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/python/tflite_convert.py\r\n\r\nYou should convert to FLOAT before trying quantized because currently you have to train models with Fakequant operations as described here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantize which can be more complicated to start with.", "Nagging Assignees @suharshs, @raghuraman-k: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 20352, "title": "Added Linux CPU with Intel\u00ae MKL-DNN\u00ae community supported build", "body": "Hi @gunan. This PR adds a link to the README.md community supported build table for Linux CPU with MKL. The broken link for the badge is because the  HTTPS certs on the server haven't been pushed yet. I'll ping you when they're in place; but I wanted to get this to you beforehand.", "comments": []}, {"number": 20351, "title": "Update version strings for TF 1.9.0-rc2.", "body": "", "comments": []}, {"number": 20350, "title": "TRT 4.0 update", "body": "  updated feature support for TRT 4.0 layers\r\n  disabled broken shape inference (added TODO)\r\n  removed unused code\r\n  code is compatible with TRT 3.0.4", "comments": ["Ping @aaroey @benbarsdell @samikama ", "Sorry for not clarifying this before.\r\nI have python scripts doing unit tests + functioning test. Trying to split them into 2 PRs. Will follow right after this one", "@jjsjann123 Please add the tests in this PR then, I'm happy to review them at the same time.", "even better! Will push it later today.\r\njust FYI, unit tests are totally separate from the code so there's no need to hold back the review waiting for my push\r\n\r\nreally appreciate the review :)", "Thanks for the fix. The c++ changes looks good to me. I read the python tests and they have duplicate logic comparing to tf_trt_integration_test.py. I would prefer to remove them in this PR and put them in a new PR, and I can merge them with tf_trt_integration_test.py. What do you think?", "Thanks so much for the patient, LGTM."]}, {"number": 20349, "title": "TF TRT Integration removes the input nodes for an Object Detection Model", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**:  3.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: cuda 9, cudnn 7.1.3\r\n- **GPU model and memory**: dual GTX 1080Ti\r\n- **Exact command to reproduce**:\r\n`from tensorflow.contrib import tensorrt as trt \r\ntrt_graph = trt.create_inference_graph(\r\n        input_graph_def=g.as_graph_def(),\r\n        outputs=nodenames,\r\n        max_batch_size=1,\r\n        max_workspace_size_bytes=1 << 25,\r\n        precision_mode=\"FP32\",  # TRT Engine precision \"FP32\",\"FP16\" or \"INT8\"\r\n        minimum_segment_size=2  # minimum number of nodes in an engine\r\n        )`\r\n\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nWith the TF-TRT integration for my object detection model, some of the input nodes are removed/fused. Is there a way to freeze some nodes from optimization?\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["/CC @aaroey, any idea if this is possible?", "Hi @dhingratul,\r\n\r\nYes, at this point it's possible for any nodes to get fuzed into TRT, regardless of the nodes placement and other settings. This is a bug and we'll be addressing it. But there is a hack to prevent this by adding your node names to the `outputs` parameter of `create_inference_graph()` in the short term.\r\n\r\nPlease let me know if you have any questions. Thanks.", "I am getting another error after I use the workaround, \r\n`2018-06-28 16:17:56.152587: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2660] Max batch size= 1 max workspace size= 151830\r\n2018-06-28 16:17:56.152618: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2666] starting build engine\r\n*** Error in python: munmap_chunk(): invalid pointer: 0x00007ffc3d3f1940 ***\r\n`\r\nI saw a similar issue, which was resolved, #https://github.com/tensorflow/tensorflow/issues/19142 , but I am using 3.0.4 and not running out of GPU memory", "@dhingratul , If you are using pip tensorflow-gpu, you *should be using TensorRT 3.0.4 for Ubuntu 14.04*. Are you sure that you are not installed TensorRT for Ubuntu 16.04?", "The installed version was 16.04, resolved after reverting to 14.04 version. "]}, {"number": 20348, "title": "tf.contrib.ffmpeg.decode_video error", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: ('v1.8.0-4015-g44a1f241bc', '1.9.0-rc0')\r\n- **Python version**: 2.7.15rc1\r\n- **Bazel version (if compiling from source)**: 0.15.0\r\n- **GCC/Compiler version (if compiling from source)**: 6.4.0\r\n- **CUDA/cuDNN version**: 9.0.176/7.0.5\r\n- **GPU model and memory**: GeForce GTX 1080 Ti (11GB)\r\n- **Exact command to reproduce**:\r\n```python\r\nimport tensorflow as tf\r\nwith tf.Session() as sess:\r\n    movie_bin = tf.read_file('5205acf1-3d30b48d.mov')\r\n    movie = tf.contrib.ffmpeg.decode_video(movie_bin)\r\n    movie_ev = movie.eval()\r\n    print(\"****\", len(movie_ev))\r\n```\r\n### Describe the problem\r\n\r\nThe `tf.contrib.ffmpeg.decode_video` op throws an error and causes a core dump.\r\n\r\nMy ffmpeg version (`apt get install ffmpeg` on ubuntu 18.04):\r\n```shell\r\nffmpeg version 3.4.2-2 Copyright (c) 2000-2018 the FFmpeg developers\r\n  built with gcc 7 (Ubuntu 7.3.0-16ubuntu2)\r\n  configuration: --prefix=/usr --extra-version=2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\r\n  libavutil      55. 78.100 / 55. 78.100\r\n  libavcodec     57.107.100 / 57.107.100\r\n  libavformat    57. 83.100 / 57. 83.100\r\n  libavdevice    57. 10.100 / 57. 10.100\r\n  libavfilter     6.107.100 /  6.107.100\r\n  libavresample   3.  7.  0 /  3.  7.  0\r\n  libswscale      4.  8.100 /  4.  8.100\r\n  libswresample   2.  9.100 /  2.  9.100\r\n  libpostproc    54.  7.100 / 54.  7.100\r\n```\r\n### Source code / logs\r\n```shell\r\n2018-06-27 09:15:22.489692: F tensorflow/contrib/ffmpeg/default/ffmpeg_lib.cc:405] Non-OK-status: ReadInfoFile(stderr_filename, width, height, frames) status: Unknown: Not enough video info returned by FFmpeg [0, 720, 1280, 3]Could not read FFmpeg stderr file: /tmp/tmp_file_tensorflow_3_ILW8Ht.err\r\nAborted (core dumped)\r\n```", "comments": ["May be similar to #17533. @yongtang ", "Here is the err file:\r\n```shell\r\nInput #0, mov,mp4,m4a,3gp,3g2,mj2, from '/tmp/tmp_file_tensorflow_1_RNEIEt':\r\n  Metadata:\r\n    major_brand     : qt  \r\n    minor_version   : 512\r\n    compatible_brands: qt  \r\n    encoder         : Lavf57.71.100\r\n  Duration: 00:00:40.06, start: 0.000000, bitrate: 4036 kb/s\r\n    Stream #0:0(eng): Video: h264 (High) (avc1 / 0x31637661), yuv420p(tv, smpte170m/bt709/bt709), 720x1280, 4034 kb/s, 30.16 fps, 59.94 tbr, 19200 tbn, 38400 tbc (default)\r\n    Metadata:\r\n      rotate          : 90\r\n      handler_name    : DataHandler\r\n      encoder         : H.264\r\n    Side data:\r\n      displaymatrix: rotation of -90.00 degrees\r\nStream mapping:\r\n  Stream #0:0 -> #0:0 (h264 (native) -> rawvideo (native))\r\nOutput #0, image2pipe, to '/tmp/tmp_file_tensorflow_2_q9AqGY.raw':\r\n  Metadata:\r\n    major_brand     : qt  \r\n    minor_version   : 512\r\n    compatible_brands: qt  \r\n    encoder         : Lavf57.83.100\r\n    Stream #0:0(eng): Video: rawvideo (RGB[24] / 0x18424752), rgb24, 1280x720, q=2-31, 1325778 kb/s, 59.94 fps, 59.94 tbn, 59.94 tbc (default)\r\n    Metadata:\r\n      encoder         : Lavc57.107.100 rawvideo\r\n      handler_name    : DataHandler\r\n    Side data:\r\n      displaymatrix: rotation of -0.00 degrees\r\nMore than 1000 frames duplicated 5370300kB time=00:00:33.18 bitrate=1325778.2kbits/s dup=992 drop=5 speed=6.61x    \r\nframe= 2401 fps=352 q=-0.0 Lsize= 6482700kB time=00:00:40.05 bitrate=1325778.2kbits/s dup=1198 drop=5 speed=5.88x    \r\nvideo:6482700kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000000%\r\n```", "@rongou Thanks for the report, I will take a look.", "Created a PR #20366 for the fix.", "@yongtang sent you an alternative fix: #20388. Tested it on 70 .mov files from the [BDD100K dataset](http://bdd-data.berkeley.edu/).", "ping @yifeif", "I was blocked by this also on my side.\r\nI'm sharing a dirty workaround here (might be useful to someone else), that is \"patching\" ffmpeg (no need to recompile tensorflow).\r\n\r\n**Warning** this is dirty and this may broke your ffmpeg install so you should revert the patch after you did it\r\n\r\n### Save ffmpeg binary as `ffmpeg-unpatched`\r\n```\r\nffmpeg_path=$(which ffmpeg)\r\nmv $ffmpeg_path ${ffmpeg_path}-unpatched\r\n```\r\n\r\n### Create a patched-ffmpeg script\r\n\r\nsave a file named `ffmpeg` somewhere in your `$PATH`\r\n\r\n\r\n```\r\n#!/bin/bash\r\n\r\n# create a temporary file to put ffmpeg input\r\nrand=$(shuf -i1-100000 -n1)\r\nfilename=/tmp/$rand.ffmpegout\r\n\r\n# call ffmpeg and redirect stderr to temporary file\r\nffmpeg-unpatched \"$@\" > /dev/null 2> $filename\r\n\r\necho \"This ffmpeg is a bash script calling ffmpeg in order to patch it for tensorflow/issues/20348\"\r\n\r\n# output ffmpeg results in an order that is matching tensorflow expectations\r\ncat $filename | grep -A 100 \"Output\" >&2\r\ncat $filename | grep -A 1 \"Stream mapping\" >&2\r\ncat $filename | grep \"frame=\" >&2\r\n\r\n# Uncomment this line for bugfixing (to visualize your frames numbers line)\r\n# cat $filename | grep \"frame=\"\r\n\r\nrm -f $filename\r\n\r\nexit\r\n```\r\n\r\n### Run tensorflow again\r\n\r\nnow it should work \r\n\r\n### Revert\r\n```\r\nmv ${ffmpeg_path}-unpatched $ffmpeg_path\r\n```", "`tf.contrib` module is no longer supported.\r\nPlease test with `TF_IO` module https://www.tensorflow.org/io/api_docs/python/tfio/experimental/ffmpeg/decode_video\r\nand raise a new issue if problem still persists. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20348\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20348\">No</a>\n"]}, {"number": 20347, "title": "Bug on  create a Dataset from a DataFrame when  index its not ordered", "body": "This code generate a  **segmentation fault**\r\n\r\n```python\r\nimport pandas     as pd\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\ntf.enable_eager_execution()\r\n\r\ntrain = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]}, index=[2, 1, 4])\r\nlabel = pd.Series([7, 8, 9], index=[2, 1, 4])\r\n\r\nds = tf.data.Dataset.from_tensor_slices((dict(train), label))\r\n```\r\n\r\nBut this not,  (only index  it's changed) from **[2, 1, 4]** to **[0, 1, 4]**:\r\n```python\r\nimport pandas     as pd\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\ntf.enable_eager_execution()\r\n\r\n\r\ntrain = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]}, index=[0, 1, 4])\r\nlabel = pd.Series([7, 8, 9], index=[0, 1, 4])\r\n\r\nds = tf.data.Dataset.from_tensor_slices((dict(train), label))\r\n```\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Kubuntu 17.10 (Artful Aardvark), Kernel 4.13.0-21-generic\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**:  3.6.3\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I updated the missing information.", "Thanks for the report @marcelino-m - it surely shouldn't crash, so I'll look into that.\r\n\r\nIn the mean time, a workaround would be to explicitly convert to tensors before creating the dataset, with something like this:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport pandas as pd\r\nimport numpy as np\r\ntf.enable_eager_execution()\r\n\r\ntrain = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]}, index=[2, 1, 4])\r\nlabel = pd.Series([7, 8, 9], index=[2, 1, 4])\r\n\r\nnp_train  = {k: np.array(v) for k, v in dict(train).items()}\r\nnp_label = np.array(label)\r\nds = tf.data.Dataset.from_tensor_slices((np_train, np_label))\r\n```\r\n\r\n", "thanks for the tips. I did something like that.\r\n"]}, {"number": 20346, "title": "Tensorflow Program Hangs on pthread_cond_wait", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.4 LTS, xenial\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**:  tensorflow-gpu (1.8.0)\r\n- **Python version**: Python 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0.176/7.1.3.16\r\n- **GPU model and memory**: GeForce GTX 850M with 2GB of memory\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI have a program using Tensorflow and it consistently hangs after a week or so, arbitrarily.  The GPU utilization goes to 0%, and the program stops training.  A stack trace shows that the program is stuck waiting on pthread_cond_wait.\r\n\r\n### Source code / logs\r\nI don't know that the source code will be of much use since it's not a concise example that reproduces the problem, but the code is available here: https://github.com/benbotto/bsy-dqn-atari/tree/breakout-best  My code is single threaded.\r\n\r\nWhen last the program hung I took a stack trace.  That's available here: https://pastebin.com/LiPhz2CE\r\n\r\nThis looks similar to this report: https://github.com/tensorflow/tensorflow/issues/1947\r\n \r\nLet me know if more information is needed.", "comments": ["the same problem, no solution now.", "Looked at the pastebin and nothing obvious jumped out to me. @mrry any ideas?\r\n\r\nIf the problem happened again, could you paste a newer version of the stack trace?", "The hang happens reliably on one of my machines.  I have another machine that's running the same software on the same version of Tensorflow--albeit different hardware--and I haven't had any hangs on that one.  The hang is sporadic, though, and often takes a week or more.  If I really need to I can take another stack trace, but that will take some time.", "Looking at this again, and there's no obvious answer in the stack trace, but I have a few suggestions:\r\n\r\n1.  Can you try creating your `tf.Session` with `config=tf.ConfigProto(inter_op_parallelism_threads=1, intra_op_parallelism_threads=1)`? This shouldn't fix anything... if anything it makes it more likely that buggy code will deadlock sooner... but it should give a more compact thread dump to analyze.\r\n2. Are you using `tf.py_func()` in your code? It looks like \"Thread 1\" (which appears twice in the pastebin link) is blocked in some NumPy code, in what appears to be OpenBLAS (`exec_blas_async_wait()`). It's possible that if your other machine has a different version of NumPy, then it might have different behavior for this method.\r\n3. Related to (2), experimenting with the `OMP_NUM_THREADS` variable (e.g. setting it to 1?) might reveal something about the behavior.\r\n\r\nIf you can reproduce the hang and capture another (ideally smaller) stack trace, that would be useful!", "It could be related to this issue in OpenBLAS: https://github.com/xianyi/OpenBLAS/issues/660\r\n\r\n[This comment](https://github.com/xianyi/OpenBLAS/issues/660#issuecomment-277033954) suggests that setting the environment variable `OPENBLAS_NUM_THREADS=1` is a possible workaround, but may slow things down.", "Thanks for the response, @mrry.\r\n\r\n1. Right now I'm not actually creating a session explicitly as I'm using tf.keras which evidently creates a session behind the scenes.  It looks like Keras has the option of using a manually defined session (`tf.keras.backend.set_session`), so I'll try your config recommendation and see if I can get a better stack dump.\r\n2. I'm not using `tf.py_func` anywhere, no.  As far as I know both machines are using identical version of everything (Tensorflow, Keras, numpy, and all other dependencies).  I'm using virtualenv, and I set up both machines simultaneously.  I'll double check the np version, though.\r\n\r\nI wish I knew how to set up a smaller test case that would reproduce the issue--as a programmer I certainly understand how important that is--but I'm not sure how in this instance.  I don't think my code is doing anything out of the ordinary: I'm generating images, doing some basic processing on those (crop and grayscale and such), then feeding them through a rather trivial network.  I'm not _directly_ using any threading in my code or anything else that would cause a deadlock.  All that is to say I wish I could be of more help (and furthermore I wish the code would run reliably on all my machines!).\r\n\r\nAnyway, I'll try again with the session configured as recommended and see if I can get another stack trace.  It may take a few weeks to lock up again.", "@mrry The similar problem occurs at the end of an iteration when using `DataSet` api, while the old `Queue` based data api is fine. \r\n\r\n  TF version: 1.12 build from source\r\n  Linux kernel: 3.10.0\r\n  Distributed running on yarn, only cpu. Every time, several random nodes will hang forever.\r\n  \r\n```\r\nThread 217 (Thread 0x7fce01b3e700 (LWP 100195)):\r\n#0  0x00007fce7ea14995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n#1  0x00007fcdf8b3082c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /lib64/libstdc++.so.6\r\n#2  0x00007fce5be86d30 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /data9/hadoop/yarn/nm-local-dir/usercache/hadoop-user/appcache/application_1538038835235_2238029/container_e10_1538038835235_2238029_01_000361/tf.tgz/tf/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#3  0x00007fce5be87784 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) () from /data9/hadoop/yarn/nm-local-dir/usercache/hadoop-user/appcache/application_1538038835235_2238029/container_e10_1538038835235_2238029_01_000361/tf.tgz/tf/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#4  0x00007fce5be86482 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /data9/hadoop/yarn/nm-local-dir/usercache/hadoop-user/appcache/application_1538038835235_2238029/container_e10_1538038835235_2238029_01_000361/tf.tar.gz/tf/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#5  0x00007fcdf8b34070 in ?? () from /lib64/libstdc++.so.6\r\n#6  0x00007fce7ea10e25 in start_thread () from /lib64/libpthread.so.0\r\n#7  0x00007fce7e031bad in clone () from /lib64/libc.so.6\r\nThread 3 (Thread 0x7fc99ffff700 (LWP 104238)):\r\n#0  0x00007fce7ea14995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n#1  0x00007fcd7b5aa187 in gpr_cv_wait () from /data9/hadoop/yarn/nm-local-dir/usercache/hadoop-user/appcache/application_1538038835235_2238029/container_e10_1538038835235_2238029_01_000361/tf.tgz/tf/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#2  0x00007fcd7b58680c in executor_thread(void*) () from /data9/hadoop/yarn/nm-local-dir/usercache/hadoop-user/appcache/application_1538038835235_2238029/container_e10_1538038835235_2238029_01_000361/tf.tgz/tf/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#3  0x00007fcd7b5aae8c in grpc_core::(anonymous namespace)::ThreadInternalsPosix::ThreadInternalsPosix(char const*, void (*)(void*), void*, bool*)::{lambda(void*)#1}::_FUN(void*) () from /data9/hadoop/yarn/nm-local-dir/usercache/hadoop-user/appcache/application_1538038835235_2238029/container_e10_1538038835235_2238029_01_000361/tf.tgz/tf/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#4  0x00007fce7ea10e25 in start_thread () from /lib64/libpthread.so.0\r\n#5  0x00007fce7e031bad in clone () from /lib64/libc.so.6\r\nThread 2 (Thread 0x7fc99f7fe700 (LWP 119076)):\r\n#0  0x00007fce7e02bec9 in syscall () from /lib64/libc.so.6\r\n#1  0x00007fcd7b4625d4 in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) () from /data9/hadoop/yarn/nm-local-dir/usercache/hadoop-user/appcache/application_1538038835235_2238029/container_e10_1538038835235_2238029_01_000361/tf.tgz/tf/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#2  0x00007fcd7b461db1 in nsync::nsync_sem_wait_with_cancel_(nsync::waiter*, timespec, nsync::nsync_note_s_*) () from /data9/hadoop/yarn/nm-local-dir/usercache/hadoop-user/appcache/application_1538038835235_2238029/container_e10_1538038835235_2238029_01_000361tf.tgz/tf/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#3  0x00007fcd7b45f2e4 in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) () from /data9/hadoop/yarn/nm-local-dir/usercache/hadoop-user/appcache/application_1538038835235_2238029/container_e10_1538038835235_2238029_01_000361/tf.tgz/tf/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#4  0x00007fcd7b45f805 in nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*) () from /data9/hadoop/yarn/nm-local-dir/usercache/hadoop-user/appcache/application_1538038835235_2238029/container_e10_1538038835235_2238029_01_000361/tf.tgz/tf/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#5  0x00007fcd78af538b in tensorflow::BlockingCounter::Wait() () from /data9/hadoop/yarn/nm-local-dir/usercache/hadoop-user/appcache/application_1538038835235_2238029/container_e10_1538038835235_2238029_01_000361/tf.tgz/tf/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#6  0x00007fcd78b02a66 in tensorflow::Status tensorflow::MasterSession::ReffedClientGraph::RunPartitionsHelper<std::vector<std::string, std::allocator<std::string> >, tensorflow::RunStepRequestWrapper, tensorflow::MutableRunStepResponseWrapper>(std::unordered_map<absl::string_view, unsigned long, tensorflow::hash<absl::string_view, void>, std::equal_to<absl::string_view>, std::allocator<std::pair<absl::string_view const, unsigned long> > > const&, std::vector<std::string, std::allocator<std::string> > const&, tensorflow::MasterEnv const*, long long, long long, tensorflow::MasterSession::PerStepState*, tensorflow::CallOptions*, tensorflow::RunStepRequestWrapper const&, tensorflow::MutableRunStepResponseWrapper*, tensorflow::CancellationManager*, bool) () from /data9/hadoop/yarn/nm-local-dir/usercache/hadoop-user/appcache/application_1538038835235_2238029/container_e10_1538038835235_2238029_01_000361/tf.tgz/tf/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#7  0x00007fcd78b03a5b in tensorflow::MasterSession::ReffedClientGraph::RunPartitions(tensorflow::MasterEnv const*, long long, long long, tensorflow::MasterSession::PerStepState*, tensorflow::CallOptions*, tensorflow::RunStepRequestWrapper const&, tensorflow::MutableRunStepResponseWrapper*, tensorflow::CancellationManager*, bool) () from /data9/hadoop/yarn/nm-local-dir/usercache/hadoop-user/appcache/application_1538038835235_2238029/container_e10_1538038835235_2238029_01_000361/tf.tgz/tf/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#8  0x00007fcd78b0c530 in tensorflow::MasterSession::DoRunWithLocalExecution(tensorflow::CallOptions*, tensorflow::RunStepRequestWrapper const&, tensorflow::MutableRunStepResponseWrapper*) () from /data9/hadoop/yarn/nm-local-dir/usercache/hadoop-user/appcache/application_1538038835235_2238029/container_e10_1538038835235_2238029_01_000361/tf.tgz/tf/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#9  0x00007fcd78b0d93e in tensorflow::MasterSession::Run(tensorflow::CallOptions*, tensorflow::RunStepRequestWrapper const&, tensorflow::MutableRunStepResponseWrapper*) () from /data9/hadoop/yarn/nm-local-dir/usercache/hadoop-user/appcache/application_1538038835235_2238029/container_e10_1538038835235_2238029_01_000361/tf.tgz/tf/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#10 0x00007fcd78ae88ec in std::_Function_handler<void (), tensorflow::Master::RunStep(tensorflow::CallOptions*, tensorflow::RunStepRequestWrapper const*, tensorflow::MutableRunStepResponseWrapper*, std::function<void (tensorflow::Status const&)>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /data9/hadoop/yarn/nm-local-dir/usercache/hadoop-user/appcache/application_1538038835235_2238029/container_e10_1538038835235_2238029_01_000361/tf.tgz/tf/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#11 0x00007fcdf8b34070 in ?? () from /lib64/libstdc++.so.6\r\n#12 0x00007fce7ea10e25 in start_thread () from /lib64/libpthread.so.0\r\n#13 0x00007fce7e031bad in clone () from /lib64/libc.so.6\r\n```", "@formath That sounds like it could be a different problem. However, there isn't enough information in that one stack to diagnose the cause: it just tells us that one call to `sess.run()` is hanging somewhere. Can you please open a new issue with a minimal reproducible example and/or a full set of thread stacks? Also, to remove a potential source of complexity, please try to reproduce it using `tf.Session()` instead of `tf.Session(rpc_server)` (although if it only reproduces in an RPC-based session, then that would tell us something useful).", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 20345, "title": "tf.train.init_from_checkpoint should add reshape parameter like the tf.train.Saver", "body": "Have I written custom code . Yes, with Estimator API\r\nOS Platform and Distribution. Ubuntu 16.04\r\nTensorFlow installed from: pip\r\nTensorFlow version: 1.8\r\nBazel version: N/A\r\nCUDA/cuDNN version: 9.0\r\nGPU model and memory: 1080Ti with 11G\r\nExact command to reproduce: N/A\r\n\r\n\r\nSometimes pretrained weights are squeezed while the weights for training have extra dimensions. For example, the shape of V1 in checkpoint is [128], while shape of V1 in target net is [1,1,1,128].  The reshape parameter in tf.train.Saver can solve this issue. But when I use Estimator to load pretrained weights, it will be very difficult to load weights with mismatch shapes.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "You might try tf.contrib.framework.assign_from_checkpoint_fn", "@bignamehyp  Thanks for your suggestion. But the tf.contrib.framework.assign_from_checkpoint_fn return a  function which needs a session to run it. I have to use hook or scaffold to implement it instead of directly add tf.contrib.framework.init_from_checkpoint operation after net building. By the way, I can't  find any document  that describes how to load part of pretrained weights in Estimator,  it often takes a lot of time on finding the right API  for doing this since many API seems to implement similar function. More examples of how to deploy the API should be included in future documents, I hope. ", "As this issue has invited community support, please remove the assignee. Otherwise, remove the `community support` label. Thank you.", "@Flowerfan did you manage to find a solution for this? I'm interested in any hints, please. Thanks!", "@AVCarreiro  I used scaffold and tf.train.Saver to deal with it. First create a scaffold object for initialization, and then pass it to EstimatorSpec.\r\nHere is my solution: \r\n```\r\nsaver = tf.train.Saver(var_list=varlist, reshape=True)`\r\ndef init_fn(scaffold, session):\r\n   saver.restore(session, model_pth)\r\nscaffold = tf.train.Scaffold(init_fn=init_fn)\r\nreturn tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op, scaffold=scaffold)\r\n```\r\n"]}, {"number": 20344, "title": "When I save a tensorflow session in windows the file(e.g 1.ckpt.data-00000-of-00001) size becomes around 1.9 GB whereas in linux it is around 270 MB. The file generated in windows gives 2% accuracy(supposed to be 82%) when restored in testing.  So the weights are not getting saved properly and many extra info gets stored in Windows I think.  Kindly help to solve the problem in Windows.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["System information\r\n-Not custom code\r\n-windows server 2016 essentials\r\n-python 3.6.5\r\n-jupyter notebook 5.5.0\r\n-tensorflow 1.2.0\r\n-Not using GPU", "I have used tf.train.Saver().restore(sess, \"1.ckpt\") to restore and for saving the model I have used tf.train.Saver().save(sess, \"./model/\"+str(epoch)+\".ckpt\")\r\nafter restoration the model gives 0.032 accuracy where after training it is giving around 95% ", "Any update on this?", "@ani31101993 Tensorflow 1.2.0 is pretty old. Can you please see if your program continues with a more recent version? ", "It has been 34 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 20343, "title": "Please, invest into profiler documentation", "body": "After trying to figure out how to use tensorflow profiler to test my model I am giving up. Current `README.md`  does not only lack details but is actually quite confusing. ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@av8ramit Can we add [FR] in the title or something like this to let the bot skip the template check for cases like this one?", "We are working on a solution where we have separate templates.", "\r\nIt also has a g3doc/ directory with more info.\r\n\r\nBut I can't find an active maintainer for the profiler.\r\n\r\n@asimshankar, I've heard you mention this tool before. Do you know what the current status, or what future-plans for it are?", "Would be wonderful to use profiler with GradientTape", "I think this profiler is dead. Closing."]}, {"number": 20341, "title": "SWIGing tensorflow/contrib/lite/toco/python/toco.i failed", "body": "\r\nHi There..\r\nWhile installing tensorflow-1.4.0 usning bazel-0.9.0 getting the error as follows.Can any one help to solve this.\r\nmy environmet\r\n-----------------\r\ngcc-4.8.3 java-1.8.0 bazel-0.9.0\r\n\r\n\r\nERROR: /remote/us01home34/ids_cm/rgatti/temp/RHEL6/tensorflow/tensorflow/contrib/lite/toco/python/BUILD:22:1: SWIGing tensorflow/contrib/lite/toco/python/toco.i failed (Exit 1)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1172.854s, Critical Path: 45.81s\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\nThanks in advance", "comments": ["when i try to build Using \"bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures\" It throwed another error as follows.\r\n\r\nERROR: /remote/us01home34/ids_cm/rgatti/temp/RHEL6/tensorflow/tensorflow/python/BUILD:1426:1: Linking of rule '//tensorflow/python:gen_state_ops_py_wrappers_cc' failed (Exit 1): gcc failed: error executing command\r\nbazel-out/host/bin/_solib_k8/_U_S_Stensorflow_Spython_Cgen_Ustate_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `clock_gettime'\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 363.612s, Critical Path: 25.37s\r\nFAILED: Build did NOT complete successfully\r\n", "Hi aselle,\r\nCan you provide the solution for this error...\r\nAlso i have installed swig-3.0.12 after seeing this error, but there is no result.\r\nPlease help ASAP", "Could you consider upgrading bazel? 0.9.0 is somewhat old. It seems wrong that clock_gettime() is undefined as it is a base thing in the C library on linux.", "I had the same issue using bazel 0.11.1 and different version combinations of python, gcc, and tensorflow.\r\nFixed by this comment: https://github.com/bazelbuild/bazel/issues/4053#issuecomment-343134886", "Hi aselle,\r\nEven if I upgrade my bazel to 1.10.0, issue still remains.", "What version of Linux are you using?", "Also, did @dsalaj's comment help you?", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 20340, "title": "reg. tensorflow-gpu in amd windows", "body": "**I have Win 10/ AMD Raedon R5 M335\r\nI am using Python 3.5 and tensorflow 1.0.0\r\nI am also, working through Spyder as using Python Console would make handling the code very difficult**\r\n\r\nPlease guide me how should i use my gpu in the training of deep learning model i am working on. Currently it uses my CPU.\r\n\r\n**PS: No solutions to previously made issues worked for me.**", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "OS Platform and Distribution-Win 10\r\nTensorFlow installed from-pip\r\nTensorFlow version-1.0.0\r\nBazel version-Don't have\r\nCUDA/cuDNN version-for Nvidia so didn't install\r\nGPU model and memory-AMD Raedon R5 M335\r\nExact command to reproduce-deep learning model , training the data set on qns ans basis, for a chatbot", "No official binaries for tf on AMD GPUs are available yet.\r\nYou can check out [tensorflow with SYCL](https://www.codeplay.com/portal/tensorflow%E2%84%A2-for-opencl%E2%84%A2-using-sycl%E2%84%A2) or [tensorflow with ROCM](https://github.com/ROCmSoftwarePlatform/tensorflow)"]}, {"number": 20339, "title": "A/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0x30 in tid 28963 (flitecamerademo).I found this error while compiling the TfliteCameraDemo app", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Please describe the steps you took and the error that resulted. Its hard to debug with this little information.", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 20338, "title": "Keras Model functional API with custom submodel not working in eager execution?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nDarwin localhost 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64\r\nMac OS X 10.13.4\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.8\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\nSee below\r\n\r\n\r\n### Describe the problem\r\nConsider the following keras model with a custom submodel:\r\n\r\n```\r\ntf.enable_eager_execution()\r\n\r\nclass SubModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(SubModel, self).__init__()\r\n        self.layer = tf.keras.layers.Dense(3) \r\n    def call(self, inputs):\r\n        return self.layer(inputs)\r\n\r\ndef MyModel():\r\n    input = tf.keras.Input(shape=(3, 3))\r\n    m = SubModel()\r\n    output = m(input)\r\n    return tf.keras.Model(input, output)\r\n\r\nm = MyModel()\r\nm(tf.constant(tf.ones([3, 3])))\r\n```\r\n\r\nwhere the SubModel is a custom model, and MyModel() uses it in the functional API. The code raises error:\r\n\r\n> File \"/Library/Python/2.7/site-packages/tensorflow/python/keras/_impl/keras/engine/network.py\", line 639, in compute_output_shape\r\n>     raise NotImplementedError\r\n\r\nI think it might be because the submodel cannot calculate the shape. So I added a compute_output_shape method for the SubModel class as if it's a custom layer:\r\n\r\n```\r\nclass SubModel(tf.keras.Model):\r\n    ...\r\n    def compute_output_shape(self, input_shape):\r\n        return (input_shape[0], 3)\r\n```\r\n\r\nNow the NotImplementedError disappeared, but we have a new error when running the model:\r\n\r\n> AssertionError: Could not compute output DeferredTensor('None', shape=(3,), dtype=float32)\r\n\r\nNow I don't know what to do. The code actually works in non-eager mode, so I guess it's a bug for keras functional API in eager execution?\r\n\r\n", "comments": ["Actually it's not a bug,  it is only suitable for graph mode, `tf.keras.Input` does not work in eager mode.", "@zakizhou \r\nAs I read from a comment of @fchollet in [a different thread](https://github.com/tensorflow/models/issues/3755), he said that \"In general all Model APIs work the same way with or without eager execution.\" So I guess at least the intention of Keras's author is to make the functional API work in eager mode.", "@David-Mao I know that, but that's impossible for your situation. Essentially you should understand why nearly all apis in keras are compatible with eager and graph, that is because that classes like `tf.keras.layers.Con2D` are just a combination of internal tensorflow operations like `tf.get_variable` and `tf.nn.conv2d` which are naturally compatible with eager and graph. But there are exceptions, the most typical one is `tf.placeholder` which stands for a empty and symbolic tensor in graph mode, in eager mode, each operation should immediately have an output tensor with exact value, so operations like `tf.placeholder` makes no sense in eager  mode and will never be compatible with eager, so is `tf.keras.Input` which make use of it.", "I would like to explain on why I need the functional API. I actually prefer the model subclassing API, but it seems to have a problem: it cannot hold multiple layers defined by a loop. Say I would like to have a Model that contains 20 same layers with different weights, and I cannot use `Sequential()` model because my layer is a non-trivial one with multiple inputs. For example my model is something like:\r\n```\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(MyModel, self).__init__()\r\n        self.layers = []\r\n        for i in range(20):\r\n            self.layers.append(MyLayer())\r\n    def call(self, inputs):\r\n        x = inputs[0]\r\n        y = inputs[1]\r\n        for i in range(20):\r\n            x = self.layers[i](x, y)\r\n        return x\r\n```\r\nThis model would not work correctly. In fact `MyModel().variables` would give an empty list. Apparently Keras is not smart enough to recognize the list member containing layers. (The `call()` method actually works, but others, e.g. `Save()` won't work.) So in this case I have to use the function API to define such a model (or I will need to repeat one same line 20 times in the code. Edit: as discussed below, we can also use `setattr` here. )", "@David-Mao \r\n```\r\nclass MyModel(Model):\r\n    def __init__(self):\r\n        super(MyModel, self).__init__()\r\n        # self.layers = []\r\n        for i in range(20):\r\n            setattr(self, \"layer%i\" % i, MyLayer())\r\n\r\n    def call(self, inputs):\r\n        x = inputs[0]\r\n        y = inputs[1]\r\n        for i in range(20):\r\n            x = getattr(self, \"layer%i\" % i)(x, y)\r\n        return x\r\n```", "@zakizhou \r\nBut if I'm not using a custom submodel, say, e.g. I'm using a standard layer:\r\n```\r\ntf.enable_eager_execution()\r\ndef MyModel():\r\n    input = tf.keras.Input(shape=(3, 3))\r\n    m = tf.keras.layers.Dense(3)\r\n    # replace the above line to m = SubModel() would make this code not working.\r\n    output = m(input)\r\n    return tf.keras.Model(input, output)\r\nm = MyModel()\r\nm(tf.constant(tf.ones([3, 3])))\r\n```\r\nThis works pretty well. So at least in this case keras.Input is compatible with eager execution. If this can work, I don't see any fundamanetal reason to prevent the custom submodel from working.\r\n\r\nReply to your other comment on the suggestion of the subclassing code:\r\n\r\nYeah that's basically what I'm doing now (which I don't like). I don't believe that in Keras's vision using `setattr` is the best practice...", "A hack way? Have you read the source code of [Network](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/network.py#L365)? `__setattr__` is overriden to collect variblaes in `tf.keras.layers.Layer`. Actually when you call `self.xxx = xxxx`, you are implicitly calling `__setattr__`, why can't we directly use it?", "@zakizhou Well I believe that you can do whatever dirty work under the hood but it would be nice if the users of the API are not forced to use `setattr ` to finish the high level job. But this debate is very subjective and totally beyond the point of this thread, which is about the functional API. Anyway thank you very much for that suggestion :)", "Yes, the intention is for the Keras API to work smoothly with or without eager execution enabled. Thanks for the report.\r\n\r\n@David-Mao : Having list-valued attributes should be functional soon (CC @allenlavoie who was adding support for that). So the example you mentioned in https://github.com/tensorflow/tensorflow/issues/20338#issuecomment-400887101 will be supported in a future release (@allenlavoie can correct me if I'm wrong).\r\n\r\n@fchollet @pavithrasv : Could you comment on the issue here?\r\n", "Yep, just sent auto list tracking for review, then we'll do auto dict and tuple/namedtuple tracking.", "@David-Mao As previously mentioned, saving subclass models is not currently supported. Is there a reason that you want to use subclass models? What issues do you hit if you use a Functional model instead?", "@anj-s\r\nAs mentioned in the very beginning of this thread, the functional API won't work if it contains a custom submodel. It's not that it cannot save. It just won't work (in eager execution).", "@David-Mao I guess my question is if it is possible to have a Functional model instead of a custom Subclass model. Sorry I wasn't clear before. Would it be possible to do that?  \r\n\r\nI think someone as already mentioned this in the thread above but you cannot call a subclass model with a symbolic tensor in eager mode. From what I understand you want to call a layer multiple times in a model without sharing weights. I believe this is possible with a Functional model. Hence I want to understand if there is a use case for using subclass models here that I am missing. ", "@anj-s \r\nYes in my particular case there is a workaround. But in general it's quite possible that we have to use a subclass model. For example, I might need to use a 3rd party model written by others as a part in my model,  which I cannot control how they wrote.\r\n\r\nAs @asimshankar mentioned, the ideal vision is that Keras API should work smoothly with or without eager execution enabled. It would be very frustrated for a user to tell (and remember) in which case which API is working or not. \r\n\r\nI just tested  the following 4 possible use cases (all in eager execution):\r\n\r\nA) A model written as subclass calling another model written as subclass\r\nB) A model written by functional API calling another model written by functional API\r\nC) A model written as subclass calling another model written by functional API\r\nD) A model written by functional API calling another model written as subclass\r\n\r\nIt seems A, B, C work but only D doesn't. It's quite unexpected and confusing.\r\n", "@David-Mao there's an issue with how the output shapes are used right now and is being looked at. However, the workaround right now is to simply wrap your output shape with a TensorShape, e.g. doing this will work. \r\n\r\n```\r\n    class SubModel(keras.Model):\r\n\r\n      def __init__(self):\r\n        super(SubModel, self).__init__()\r\n        self.layer = keras.layers.Dense(3)\r\n\r\n      def call(self, inputs):\r\n        return self.layer(inputs)\r\n\r\n      def compute_output_shape(self, input_shape):\r\n        # wrap (input_shape[0], 3, 3) with tensorshape \r\n        return tf.TensorShape((input_shape[0], 3, 3))\r\n\r\n    def func_model(input_shape):\r\n      func_input = keras.layers.Input(shape=input_shape)\r\n      m = SubModel()\r\n      output = m(func_input)\r\n      return keras.Model(func_input, output)\r\n\r\n    input_shape = (3, 3)\r\n    m = func_model(input_shape)\r\n    input_vals = tf.ones((32,) + input_shape)\r\n    m(input_vals)\r\n```", "I have the same problem ([posted on SO](https://stackoverflow.com/questions/51806852/cant-save-custom-subclassed-model)) but even w/o eager execution.\r\nAny update?", "On the keras 2.2.2\r\nFound comment (for model saving) \r\nfile: keras/engine/network.py  \r\nFunction: get_config   \r\n\r\n>             # Subclassed networks are not serializable\r\n>             # (unless serialization is implemented by\r\n>             # the author of the subclassed network).", "I have a same issue. I am trying to use the tf.estimator for the training of the network. However, model.fit works perfectly fine...\r\n\r\nHere is the code for what I am trying to do:\r\nmodel_params = {\r\n        \"batch_size\": args.batch_size,\r\n        \"num_conv_filters\" : [8, 16, 32],\r\n        \"conv_filter_sizes\" : [5, 5, 3],\r\n        \"code_layer_size\" : 16,\r\n        \"num_deconv_filters\" : [8, 8, 4, 1],\r\n        \"deconv_filter_sizes\" : [9, 7, 5, 4],\r\n        \"deconv_start_shape\" : [3,3,3,3],\r\n        \"useMaxPool\" : True,\r\n        \"useBN\" : True,\r\n        \"activation\" : tf.nn.relu,\r\n        \"epochs\": args.epochs,\r\n        \"backend\": args.backend,\r\n        \"padded_resolution\":args.padded_resolution,\r\n        \"resolution\": args.resolution,\r\n        \"do_plot\": args.do_plot,\r\n        \"gpu\": args.gpu,\r\n        \"input_shape\":data.inputs.shape,\r\n        \"output_shape\":data.outputs.shape,\r\n        }\r\n\r\n\r\ntf.logging.info(\"loading model\")\r\nae = AutoEncoder(**model_params)\r\n\r\ntf.logging.info(\"compiling model\")\r\nae.compile(optimizer = tf.train.AdamOptimizer(),\r\n    loss = tf.losses.mean_squared_error, metrics=['accuracy'])\r\ntf.logging.info(\"training model\")\r\nae.fit(data.inputs, data.outputs, args.batch_size, epochs=args.epochs)\r\nae.summary()\r\n\r\nstrategy = tf.contrib.distribute.MirroredStrategy()\r\nconfig = tf.estimator.RunConfig(train_distribute=strategy)\r\nkeras_estimator = tf.keras.estimator.model_to_estimator(\r\n  keras_model=ae,\r\n  config=config,\r\n  model_dir='/tmp/model_dir')\r\n\r\nkeras_estimator.train(input_fn=input_fn(), steps=10)\r\n", "Hi @David-Mao,\r\n\r\nI met the same issue that loop of layers will met \"AttributeError: can't set attribute\", however, I figure out that it seem \"self.layer\" is conflict with existing attribute \"layer/layers\"\r\n\r\n```python\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(MyModel, self).__init__()\r\n        self.layers = []\r\n        for i in range(20):\r\n            self.layers.append(tf.keras.layers.Dense(units=10))\r\n    def call(self, inputs):\r\n        for i in range(20):\r\n            x = self.layers[i](inputs)\r\n        return x\r\ninput_layer = tf.keras.layers.Input(shape=(10,))\r\nmodules = MyModel()(input_layer)\r\nmodel = tf.keras.Model(inputs=input_layer, outputs=modules)\r\nmodel.summary()\r\n```\r\nwhich gives you \"AttributeError\"\r\n\r\nHowver,\r\n```python\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(MyModel, self).__init__()\r\n        self.iamlist = []\r\n        for i in range(20):\r\n            self.iamlist.append(tf.keras.layers.Dense(units=10))\r\n    def call(self, inputs):\r\n        for i in range(20):\r\n            x = self.iamlist[i](inputs)\r\n        return x\r\ninput_layer = tf.keras.layers.Input(shape=(10,))\r\nmodules = MyModel()(input_layer)\r\nmodel = tf.keras.Model(inputs=input_layer, outputs=modules)\r\nmodel.summary()\r\n```\r\nthis case will pass.\r\nThe only different is \"self.layer\" -> \"self.iamlist\"\r\n\r\nSo... I think the conclusion is you should not define attributes that will conflict with tf.keras.Model. as far as I know, so common attributes like \"self.layer\", \"self.layers\", \"self.output\" ... (and it works for both enable_eager_execution or not)\r\n\r\nHope it helps", "How can I specify input shape in Imperative (or Model Subclassing) API without compile so that I can convert it to SavedModel format? If it is not possible then it's super strange to position it as main way to create new models...", "> How can I specify input shape in Imperative (or Model Subclassing) API without compile so that I can convert it to SavedModel format? If it is not possible then it's super strange to position it as main way to create new models...\r\n\r\nDid you find an answer?", "Wow, this issue is still open. 2 years ago was the first my comment here.\r\nThen I switched primarily to pytorch.", "@David-Mao,\r\nThe code mentioned in [your first comment](https://github.com/tensorflow/tensorflow/issues/20338#issue-336150944) is working fine with the `latest version of Tensorflow (2.4)`.\r\n\r\nPlease find [the Gist](https://colab.research.google.com/gist/rmothukuru/9124919e9b209ffd5838ba5753ab2992/gh_20338.ipynb) of working code. \r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20338\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20338\">No</a>\n"]}, {"number": 20337, "title": "profiler add_step make import_meta_graph error", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Centos 7\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: \r\n- **Python version**: \r\ntf.VERSION = 1.8.0\r\ntf.GIT_VERSION = v1.8.0-0-g93bc2e2072\r\ntf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072\r\n- **GPU**\uff1a**as i test, \uff08gpu-tf, gpu-sess\uff09or (cpu-tf,cpu-sess)\uff0c    this error will always occur.**\r\n\r\n### Produce\r\n1. prepare data\r\ncp ner_data /ceph_ai/xiahong/data/ -rf\r\n2. python train.py -i example_configs/ner_config.py\r\nnotice MODEL_PATH\r\n3. test follow 2 commands\r\n-* python predict.py -i {MODEL_PATH}-0\r\n-* python predict.py -i {MODEL_PATH}-34   (should fail load model if run add step, else works)\r\n\r\ncomments train.py 86 \"profiler.add_step(step=step, run_meta=run_metadata)\" and try 2,3 again.\r\n\r\n### Describe the problem\r\nafter run profiler add_step, \r\nthe saver.save file cannot import_meta_graph \r\n\r\n### Source code / logs\r\n```\r\nTraceback (most recent call last):\r\n  File \"predict.py\", line 126, in <module>\r\n    tag_model=Classifier(args.model_path)\r\n  File \"predict.py\", line 50, in __init__\r\n    self.tf_model=TFModel(sess,model_path,inputs,outputs)\r\n  File \"/dockerdata/xiahong/tf_text_modeling-master/text_modeling/tf_utils/predict.py\", line 8, in __init__\r\n    saver = tf.train.import_meta_graph(\"{}.meta\".format(model_path), clear_devices=True)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1955, in import_meta_graph\r\n    **kwargs)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/meta_graph.py\", line 743, in import_scoped_meta_graph\r\n    producer_op_list=producer_op_list)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 493, in import_graph_def\r\n    raise ValueError(str(e))\r\nValueError: Can not squeeze dim[1], expected a dimension of 1, got 200 for 'cond_2/Squeeze' (op: 'Squeeze') with input shapes: [500,200,19].\r\n```\r\n without train.py line 86, it works well.\r\n\r\n[ner_data.zip](https://github.com/tensorflow/tensorflow/files/2140517/ner_data.zip)\r\n[text_modeling.zip](https://github.com/tensorflow/tensorflow/files/2140519/text_modeling.zip)\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler related-info had been added. ", "@hxsnow10 is predict.py your code? If so, could you please provide it?", "It has been 20 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 35 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 20336, "title": "Tensorflow failed to build on x64 when build with MSVC", "body": "System information\r\n\u2022\tHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nN/A\r\n\u2022\tOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows server 2016\r\n\u2022\tTensorFlow installed from (source or binary):\r\nSource\r\n\u2022\tTensorFlow version (use command below):\r\nMaster branch latest revison\r\n\u2022\tPython version:\r\nAnaconda 4.1.1 (Python 3.5 64-bit)\r\n\u2022\tBazel version (if compiling from source):\r\nN/A\r\n\u2022\tGCC/Compiler version (if compiling from source):\r\nVS2017 15.5.7\r\n\u2022\tCUDA/cuDNN version:\r\nNVidia CUDA Toolkit 8.0\r\nNVidia CUDNN 5.1\r\n\u2022\tGPU model and memory:\r\nN/A\r\n\u2022\tExact command to reproduce:\r\nN/A\r\n\r\n**Describe the problem:**\r\nTensorflow failed to build on x64. This issue can be reproduced from the master revision 68bb435. This should be tensorflow source issue, could you please help take a look at this? Thanks!\r\n\r\n**The failures like:**\r\nThe whole log file please see attachment.\r\n[tensorflow_x64_build.log](https://github.com/tensorflow/tensorflow/files/2140452/tensorflow_x64_build.log)\r\n\r\n**Repro steps:**\r\n1. git clone https://github.com/tensorflow/tensorflow D:\\Tensorflow\\src\r\n2. pushd D:\\Tensorflow\r\n3. set PreferredToolArchitecture=x64\r\n4. set rel=Release\r\n5. set CUDNN_HOME=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\cuda\"\r\n6. set PY=C:\\ProgramData\\Anaconda3\r\n7. set CL=/FS /permissive-\r\n8. cmake D:\\Tensorflow\\src\\tensorflow\\contrib\\cmake -A x64 -DCMAKE_BUILD_TYPE=Release -DPYTHON_EXECUTABLE=C:\\ProgramData\\Anaconda3\\python.exe -DPYTHON_LIBRARIES=C:\\ProgramData\\Anaconda3\\libs\\python36.lib -DSWIG_EXECUTABLE=D:\\Tensorflow\\swigwin-3.0.12\\swig.exe -Dtensorflow_BUILD_PYTHON_TESTS=ON -Dtensorflow_BUILD_SHARED_LIB=ON\r\n9. MSBuild /m /p:Configuration=Release;Platform=x64 /p:WindowsTargetPlatformVersion=10.0.17134.0 tensorflow.sln /t:Rebuild", "comments": ["Hello,\r\nAny update for this?", "\u4e3a\u4ec0\u4e48\u4e0d\u7f16\u8bd1 Release \u7248\uff1f\r\n\r\n> Why not compile the Release version?", "Here's the error:\r\n\r\n```\r\n   526>CustomBuild:\r\n         Generating __init__.py files for Python API.\r\n         Traceback (most recent call last):\r\n           File \"D:/Tensorflow/build_x64/tf_python/tensorflow/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n             from tensorflow.python.util import tf_decorator\r\n           File \"D:\\Tensorflow\\build_x64\\tf_python\\tensorflow\\python\\__init__.py\", line 52, in <module>\r\n             from tensorflow.core.framework.graph_pb2 import *\r\n           File \"D:\\Tensorflow\\build_x64\\tf_python\\tensorflow\\core\\framework\\graph_pb2.py\", line 15, in <module>\r\n             from tensorflow.core.framework import node_def_pb2 as tensorflow_dot_core_dot_framework_dot_node__def__pb2\r\n           File \"D:\\Tensorflow\\build_x64\\tf_python\\tensorflow\\core\\framework\\node_def_pb2.py\", line 15, in <module>\r\n             from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\r\n           File \"D:\\Tensorflow\\build_x64\\tf_python\\tensorflow\\core\\framework\\attr_value_pb2.py\", line 15, in <module>\r\n             from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\r\n           File \"D:\\Tensorflow\\build_x64\\tf_python\\tensorflow\\core\\framework\\tensor_pb2.py\", line 15, in <module>\r\n             from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\r\n           File \"D:\\Tensorflow\\build_x64\\tf_python\\tensorflow\\core\\framework\\resource_handle_pb2.py\", line 22, in <module>\r\n             serialized_pb=_b('\\n/tensorflow/core/framework/resource_handle.proto\\x12\\ntensorflow\\\"r\\n\\x13ResourceHandleProto\\x12\\x0e\\n\\x06\\x64\\x65vice\\x18\\x01 \\x01(\\t\\x12\\x11\\n\\tcontainer\\x18\\x02 \\x01(\\t\\x12\\x0c\\n\\x04name\\x18\\x03 \\x01(\\t\\x12\\x11\\n\\thash_code\\x18\\x04 \\x01(\\x04\\x12\\x17\\n\\x0fmaybe_type_name\\x18\\x05 \\x01(\\tBn\\n\\x18org.tensorflow.frameworkB\\x0eResourceHandleP\\x01Z=github.com/tensorflow/tensorflow/tensorflow/go/core/framework\\xf8\\x01\\x01\\x62\\x06proto3')\r\n         TypeError: __new__() got an unexpected keyword argument 'serialized_options'\r\n```\r\n\r\nIt looks like there is a mismatch between the installed version of the `protobuf` package for your Python environment and the version used for generating code in the TensorFlow build. Try upgrading to protobuf 3.6.0 using `pip install -U protobuf==3.6.0` and then rebuilding.", "There's also another issue when importing compiled protobuf, see my response to #20669 ", "It has been 30 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Hi folks, since we are not maintaining the CMake build after TF 1.10 (https://github.com/tensorflow/tensorflow/releases/tag/v1.10.0), I'd recommend trying to reproduce this using the Bazel for Windows build. This page has instructions for building with Bazel on Windows:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/install/install_sources_windows.md\r\n\r\nIf you continue to have problems building using Bazel for Windows, please open a new issue with details of the problem.", "This link does not work"]}, {"number": 20335, "title": "Keras Model functional API not working in eager execution?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nDarwin localhost 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64\r\nMac OS X 10.13.4\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.8\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\nSee below\r\n\r\n\r\n### Describe the problem\r\nConsider the following keras model with a custom submodel:\r\n\r\n```\r\ntf.enable_eager_execution()\r\n\r\nclass SubModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(SubModel, self).__init__()\r\n        self.layer = tf.keras.layers.Dense(3) \r\n    def call(self, inputs):\r\n        return self.layer(inputs)\r\n\r\ndef MyModel():\r\n    input = tf.keras.Input(shape=(3, 3))\r\n    m = SubModel()\r\n    output = m(input)\r\n    return tf.keras.Model(input, output)\r\n\r\nm = MyModel()\r\nm(tf.constant(tf.ones([3, 3])))\r\n```\r\n\r\nwhere the SubModel is a custom model, and MyModel() uses it in the functional API. The code raises error:\r\n\r\n> File \"/Library/Python/2.7/site-packages/tensorflow/python/keras/_impl/keras/engine/network.py\", line 639, in compute_output_shape\r\n>     raise NotImplementedError\r\n\r\nI think it might be because the submodel cannot calculate the shape. So I added a compute_output_shape method for the SubModel class as if it's a custom layer:\r\n\r\n```\r\nclass SubModel(tf.keras.Model):\r\n    ...\r\n    def compute_output_shape(self, input_shape):\r\n        return (input_shape[0], 3)\r\n```\r\n\r\nNow the NotImplementedError disappeared, but we have a new error when running the model:\r\n\r\n> AssertionError: Could not compute output DeferredTensor('None', shape=(3,), dtype=float32)\r\n\r\nNow I don't know what to do. The code actually works in non-eager mode, so I guess it's a bug for keras functional API in eager execution?\r\n\r\n", "comments": []}, {"number": 20334, "title": "Incompatible shapes between op input and calculated input gradient.", "body": "When I try to implement a SegNet with tensorflow, I met up with the following issue:  \r\n\r\n```\r\nIncompatible shapes between op input and calculated input gradient.\r\n```\r\n\r\nMy net structure is here below:  \r\n\r\n```\r\n def buildNet(self):\r\n        #TODO: Implemente this method\r\n        self.X = tf.placeholder(dtype=tf.float32, shape=(self.batchsize, self.width, self.height, self.channel))\r\n        self.Y = tf.placeholder(dtype=tf.int32, shape=(self.batchsize, self.width, self.height))\r\n\r\n        norm = tf.nn.lrn(self.X, depth_radius=5, bias=1.0, alpha=1e-4, beta=0.75)\r\n        # Encoder of segnet\r\n        self.enconv1 = batchnorm(conv2d(norm, [3,3,self.channel,64], [1,1,1,1]))\r\n        self.enconv2 = batchnorm(conv2d(self.enconv1, [3,3,64,64], [1,1,1,1]))\r\n        self.enpool1 = maxpooling2d(self.enconv2, [1,2,2,1], [1,2,2,1])\r\n\r\n        self.enconv3 = batchnorm(conv2d(self.enpool1, [3,3,64,128], [1,1,1,1]))\r\n        self.enconv4 = batchnorm(conv2d(self.enconv3, [3,3,128,128], [1,1,1,1]))\r\n        self.enpool2 = maxpooling2d(self.enconv4, [1,2,2,1], [1,2,2,1])\r\n\r\n        self.enconv5 = batchnorm(conv2d(self.enpool2, [3,3,128,256], [1,1,1,1]))\r\n        self.enconv6 = batchnorm(conv2d(self.enconv5, [3,3,256,256], [1,1,1,1]))\r\n        self.enpool3 = maxpooling2d(self.enconv6, [1,2,2,1], [1,2,2,1])\r\n\r\n        self.enconv7 = batchnorm(conv2d(self.enpool3, [3,3,256,512], [1,1,1,1]))\r\n        self.enconv8 = batchnorm(conv2d(self.enconv7, [3,3,512,512], [1,1,1,1]))\r\n        self.enpool4 = maxpooling2d(self.enconv8, [1,2,2,1], [1,2,2,1])\r\n\r\n        self.enconv9 = batchnorm(conv2d(self.enpool4, [3,3,512,512], [1,1,1,1]))\r\n        self.enconv10 = batchnorm(conv2d(self.enconv9, [3,3,512,512], [1,1,1,1]))\r\n        self.enpool5 = maxpooling2d(self.enconv10, [1,2,2,1], [1,2,2,1])\r\n\r\n        # Decoder of segnet\r\n        self.depool1 = deconv2d(self.enpool5, [3,3,512,512], self.enconv10.shape, [1,1,1,1])\r\n        self.deconv1 = batchnorm(conv2d(self.depool1, [3,3,512,512], [1,1,1,1]))\r\n        self.deconv2 = batchnorm(conv2d(self.deconv1, [3,3,512,512], [1,1,1,1]))\r\n\r\n        self.depool2 = deconv2d(self.deconv2, [3,3,512,512], self.enconv8.shape, [1,1,1,1])\r\n        self.deconv3 = batchnorm(conv2d(self.depool2, [3,3,512,256], [1,1,1,1]))\r\n        self.deconv4 = batchnorm(conv2d(self.deconv3, [3,3,256,256], [1,1,1,1]))\r\n\r\n        self.depool3 = deconv2d(self.deconv4, [3,3,256,256], self.enconv6.shape, [1,1,1,1])\r\n        self.deconv5 = batchnorm(conv2d(self.depool3, [3,3,256,128], [1,1,1,1]))\r\n        self.deconv6 = batchnorm(conv2d(self.deconv5, [3,3,128,128], [1,1,1,1]))\r\n\r\n        self.depool4 = deconv2d(self.deconv6, [3,3,128,128], self.enconv4.shape, [1,1,1,1])\r\n        self.deconv7 = batchnorm(conv2d(self.depool4, [3,3,128,64], [1,1,1,1]))\r\n        self.deconv8 = batchnorm(conv2d(self.deconv7, [3,3,64,64], [1,1,1,1]))\r\n\r\n        self.depool5 = deconv2d(self.deconv8, [3,3,64,64], self.enconv2.shape, [1,1,1,1])\r\n        self.deconv9 = batchnorm(conv2d(self.depool5, [3,3,64,32], [1,1,1,1]))\r\n        self.deconv10 = batchnorm(conv2d(self.deconv9, [3,3,32,self.classes], [1,1,1,1]))\r\n\r\n        self.pred = tf.nn.softmax(self.deconv10)\r\n        self.oneHotY = tf.one_hot(self.Y, depth=2, on_value=1.0, off_value=0.0)\r\n        self.loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(onehot_labels=self.oneHotY, logits=self.pred))\r\n        print self.loss.shape\r\n        self.optimizer = tf.train.AdamOptimizer(self.learningrate).minimize(self.loss)\r\n```\r\n\r\nI think it is the problem of **tf.conv2d_trainspose**. Does anyone have idea about it?\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Could you try to simplify your program to the minimum program that exhibits the error?", "OS Platform: MacOS High Sierra\r\nNo GPU, just on CPU, for testing\r\nTensorflow Version: 1.7\r\nTerminal Output: ValueError: Incompatible shapes between op input and calculated input gradient.  Forward operation: conv2d_transpose_4.  Input index: 2. Original input shape: (4, 256, 256, 64).  Calculated input gradient shape: (4, 512, 512, 64)", "encountered same issue on older tensorflow version: 1.4", "Does this still occur in the latest version?", "U mean tensorflow r1.9? I am not sure and will check it later.", "I have the same error. My message looks liek this:\r\n\r\n `ValueError: Incompatible shapes between op input and calculated input gradient.  Forward operation: decoder/conv3d_transpose/conv3d_transpose.  Input index: 2. Original input shape: (30, 149, 149, 1, 8).  Calculated input gradient shape: (30, 298, 298, 1, 8)`.\r\n\r\n It seems that the calculated shape of the gradient does not match with the output shape that one has to give the function conv_transpose. In my case the error only occurs in my code with a stride > 1 and a deconv -> conv  -> deconv -> conv ... architecture for the decoder (For an AE). Iam not sure if this is a bug or If I did something wrong. But since I have a symmetric AE it could be a bug.\r\n\r\n**EDIT**\r\n\r\nAfter further investigation it looks like a bug: \r\nI perform a 2d convolution with stride = 2 and padding =\"SAME\" and tf reduces the size from 298 to 149. I checked my connections in tensorboard and did nothing wrong. This is the output of the specific node:\r\n```\r\n\r\n`decoder/conv_same_3/Conv2D\r\nOperation: Conv2D\r\nAttributes (6)\r\nT {\"type\":\"DT_FLOAT\"}\r\ndata_format {\"s\":\"NHWC\"}\r\ndilations {\"list\":{\"i\":[1,1,1,1]}}\r\npadding {\"s\":\"SAME\"}\r\nstrides {\"list\":{\"i\":[1,2,2,1]}}\r\nuse_cudnn_on_gpu {\"b\":true}\r\nInputs (2) decoder/conv2d_transpose_3/Relu 30\u00d7298\u00d7298\u00d78\r\n    default_variable_decoder/conv2d_transpose_3_w/read 2\u00d72\u00d78\u00d78 \r\nOutputs (1) decoder/conv_same_3/BiasAdd 30\u00d7149\u00d7149\u00d78\r\n```\r\n`", "Nagging Assignee @drpngx: It has been 18 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@Aaron19960821 I think we're at `1.10` now.", "@2649 thanks, that seems strange. Do you have a simpler repro case, maybe with just one call to the conv operator?", "I extracted the variables of this layer and performed only one convolution. Now I get a different error, but the behaviour is the same. It falsely uses valid padding although I pass \"`SAME\"` as padding.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ninput_tensor = tf.placeholder(tf.float32, (30, 298, 298, 8))\r\ny_tensor = tf.placeholder(tf.float32, (30, 298, 298, 8))\r\nx = np.random.normal(size=(30, 298, 298, 8))\r\n\r\nweight = tf.get_variable(\"weight\", [2, 2, 8, 8])\r\nbias = tf.get_variable(\"bias\", [8])\r\nstride = [1, 2, 2, 1]\r\n\r\nconv = tf.nn.conv2d(input_tensor, weight, stride, padding=\"SAME\")\r\n\r\nloss = tf.losses.mean_squared_error(y_tensor, conv)\r\noptimizer = tf.train.GradientDescentOptimizer(0.001)\r\ntrain = optimizer.minimize(loss)\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(train, \r\n        feed_dict={\r\n            input_tensor: x,\r\n            y_tensor: x\r\n        })\r\n\r\n```\r\nThe error:\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-23-fe1f6d95be60> in <module>()\r\n     12 conv = tf.nn.conv2d(input_tensor, weight, stride, padding=\"SAME\")\r\n     13 \r\n---> 14 loss = tf.losses.mean_squared_error(y_tensor, conv)\r\n     15 optimizer = tf.train.GradientDescentOptimizer(0.001)\r\n     16 train = optimizer.minimize(loss)\r\n\r\nC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py in mean_squared_error(labels, predictions, weights, scope, loss_collection, reduction)\r\n    627     predictions = math_ops.to_float(predictions)\r\n    628     labels = math_ops.to_float(labels)\r\n--> 629     predictions.get_shape().assert_is_compatible_with(labels.get_shape())\r\n    630     losses = math_ops.squared_difference(predictions, labels)\r\n    631     return compute_weighted_loss(\r\n\r\nC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py in assert_is_compatible_with(self, other)\r\n    842     \"\"\"\r\n    843     if not self.is_compatible_with(other):\r\n--> 844       raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\r\n    845 \r\n    846   def most_specific_compatible_shape(self, other):\r\n\r\nValueError: Shapes (30, 149, 149, 8) and (30, 298, 298, 8) are incompatible\r\n\r\n```\r\n\r\nI use version 1.8, but on my workstation 1.6 and it yields the same error.", "@drpngx  Ok I thought about it again and it is not a bug. When using a stride > 1 with same padding the image will automatically scale down. Otherwise the padding must happen between pixels in the image to maintain the original size. I think you can close this thread.", "Thanks for looping back! Closing.", "> \r\n> \r\n> Ok I thought about it again and it is not a bug. When using a stride > 1 with same padding the image will automatically scale down. Otherwise the padding must happen between pixels in the image to maintain the original size. I think you can close this thread.\r\n\r\n@2649\r\ndid you change padding to \"VALID\" to fix the shape error?", "I experience the same issue, already implementing the padding as specified by @2649 \r\n\r\nHave I written custom code\r\nNo\r\nOS Platform and Distribution\r\nWindows 10, 64 bit \r\nTensorFlow installed from\r\nAnaconda https://anaconda.org/anaconda/tensorflow-gpu\r\nTensorFlow version\r\n1.10.0 (result from print(tf.__version__))\r\n\r\nAny help please!", "@2649 did you solve this issue? \r\n\r\nI'm using Python3.6 with TensorFlow 1.5.1 and my issue is [Report Error]ValueError: Incompatible shapes between op input and calculated input gradient. conv2d_transpose any idea how to solve it?"]}, {"number": 20333, "title": "Tensorflow build issue in RHEL6 machine", "body": "-----------------------\r\n\r\n### System information\r\n- [root@abinaya-tensorflow ~]$ cat /etc/redhat-release\r\nRed Hat Enterprise Linux Server release 6.8 (Santiago)\r\n\r\n-TensorFlow installed from source\r\n- TensorFlow version \r\n- Python 2.7.11 \r\n- Bazel version :\r\n\r\n[root@abinaya-tensorflow tensorflow-r1.6]$ bazel version\r\nBuild label: 0.14.1- (@non-git)\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Jun 27 06:45:12 2018 (1530081912)\r\nBuild timestamp: 1530081912\r\nBuild timestamp as int: 1530081912\r\n\r\n- GCC/Compiler version :\r\n\r\n[root@abinaya-tensorflow ~]$ gcc --version\r\ngcc (GCC) 8.1.0\r\nCopyright (C) 2018 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n### Describe the problem\r\n\r\nWe are trying to build Tensorflow 1.6 from source code as per the help of below guide:\r\n\r\nhttps://www.tensorflow.org/install/install_sources\r\n\r\nplease find below steps which i tried:\r\n\r\n***********************************************************************************************************\r\n[root@abinaya-tensorflow tensorflow-r1.6]$ bazel version\r\n**Build label: 0.14.1- (@non-git)**\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Jun 27 06:45:12 2018 (1530081912)\r\nBuild timestamp: 1530081912\r\nBuild timestamp as int: 1530081912\r\n\r\n[root@abinaya-tensorflow tensorflow-r1.6]$ ./configure\r\nTraceback (most recent call last):\r\n  File \"configure.py\", line 1459, in <module>\r\n    main()\r\n  File \"configure.py\", line 1346, in main\r\n    check_bazel_version('0.5.4')\r\n  File \"configure.py\", line 459, in check_bazel_version\r\n    curr_version = run_shell(['bazel', '--batch', 'version'])\r\n  File \"configure.py\", line 154, in run_shell\r\n    output = subprocess.check_output(cmd)\r\nAttributeError: 'module' object has no attribute 'check_output'\r\n\r\nAlready available bazel version is 0.14.1 but i couldn't run the \"configure\" script of tensorflow.\r\n\r\nCan you please help us to build the tensorflow from source code.\r\n\r\n\r\n### Source code / logs\r\n\r\nAvailable version of gcc and JAVA and bazel in our machine:\r\n\r\n#################################################################\r\n[root@abinaya-tensorflow ~]$ which gcc\r\n/opt/gcc/x86_64/8.1.0/bin/gcc\r\n\r\n[root@abinaya-tensorflow ~]$ echo $JAVA_HOME\r\n/opt/j2se/x86_64/1.8.0_172\r\n\r\n[root@abinaya-tensorflow ~]$ bazel version\r\nWARNING: ignoring http_proxy in environment.\r\nBuild label: 0.14.1- (@non-git)\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Jun 27 06:45:12 2018 (1530081912)\r\nBuild timestamp: 1530081912\r\nBuild timestamp as int: 1530081912\r\n#################################################################\r\n\r\nIssue which we are facing while run the configure script of tensorflow is:\r\n\r\n################################################################\r\n[root@abinaya-tensorflow tensorflow-r1.6]$ ls\r\nACKNOWLEDGMENTS     AUTHORS             CODEOWNERS    CONTRIBUTING.md    models.BUILD  tensorflow   util\r\nADOPTERS.md         BUILD               configure     ISSUE_TEMPLATE.md  README.md     third_party  WORKSPACE\r\narm_compiler.BUILD  CODE_OF_CONDUCT.md  configure.py  LICENSE            RELEASE.md    tools\r\n[root@abinaya-tensorflow tensorflow-r1.6]$ ./configure\r\nTraceback (most recent call last):\r\n  File \"configure.py\", line 1459, in <module>\r\n    main()\r\n  File \"configure.py\", line 1346, in main\r\n    check_bazel_version('0.5.4')\r\n  File \"configure.py\", line 459, in check_bazel_version\r\n    curr_version = run_shell(['bazel', '--batch', 'version'])\r\n  File \"configure.py\", line 154, in run_shell\r\n    output = subprocess.check_output(cmd)\r\nAttributeError: 'module' object has no attribute 'check_output'\r\n####################################################################\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I suspect you are running into this:\r\nhttps://stackoverflow.com/questions/26894024/subprocess-check-output-module-object-has-out-attribute-check-output\r\n\r\nCould you print the output of `python --version` just to make sure that your main python is a python 27 binary?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 20332, "title": "An attempt at Tensorflow and Bazel CPU/GPU build (v1.8.0)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Well, I added patches\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10, 64 bit\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**:  3.5, 3.6\r\n- **Bazel version (if compiling from source)**: 1.12.0, 1.14.0\r\n- **GCC/Compiler version (if compiling from source)**:  MSVC 2015 Update v3\r\n- **CUDA/cuDNN version**: 9.0/7.1\r\n- **GPU model and memory**: GeForce GT 430\r\n- **Exact command to reproduce**: ;-)\r\n\r\n### Description\r\nI wanted to compile tensorflow with GPU support on Windows and I went through the ordeal (I was finally able to build it, but I'll come to that later). I always followed the stuff done in: https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/tools/ci_build/\r\n\r\nThis is how it went:\r\n\r\nFirst, I attempted the CPU build.\r\nInitial attempt was with CMake. It took ~3hours, all went fine upto the final linking phase, there link.exe died with:\r\n```\r\n(Lib target) ->\r\n  tf_core_kernels.dir\\Release\\tf_core_kernels.lib : fatal error LNK1248: image size (100039B2C) exceeds maximum allowable size (FFFFFFFF) \r\n```\r\nI was lost here, I tried tinkering with some CMake build options, but all resulted into the same error. So, I gave up CMake.\r\n\r\nThen, I attempted to build with bazel 1.12.0.\r\nI was using the mingw64 shell, so first, I had to patch `tensorflow/tools/pip_package/build_pip_package.sh` . I also sent a PR https://github.com/tensorflow/tensorflow/pull/18953 \r\n\r\nSurprisingly, the build went fine after that, and then I tried to run the tests. All were successful, except `tensorflow/python/kernel_tests/boosted_trees:training_ops_test`,  probably because it was not being run serially. Even for running the tests, I had to patch `tensorflow/contrib/tensorboard/plugins/trace/trace.py` and I also sent a PR https://github.com/tensorflow/tensorflow/pull/18954\r\n\r\nNow comes the real deal. The GPU build.\r\n\r\nAgain, I attempted the build with CMake. It took ~3hours and then link.exe failed with the same error as before. I spent quite some time reading about it and couldn't figure it out as most solutions said 'break your huge lib into smaller libs'. I am still baffled as to why the CI job at http://ci.tensorflow.org/job/tf-master-win-gpu-cmake/ doesn't die with the same problem. I am using the same compiler, same linker, \r\nand my machine is powerful enough. So, I just gave up again on CMake.\r\n\r\nGoing back to bazel now.  By this time, bazel 1.14.0 was was released. So I decided to use that instead. I had to apply the two patches as before, but life isn't so easy. \r\n\r\nNext big hurdle: https://github.com/tensorflow/tensorflow/issues/17067 .\r\n-  So I got @dtrebbien 's patch from https://github.com/dtrebbien/protobuf/commit/50f552646ba1de79e07562b41f3999fe036b4fd0 and made changes to `tensorflow/workspace.bzl` to apply it to all protobuf checkouts.\r\n\r\nNext big hurdle: NCCL doesn't seem to be officially supported on Windows. \r\n- But bazel is still trying to build nccl related stuff. I had to patch stuff again, to disable all references to nccl in the BUILD files all over the place. :crying_cat_face: \r\n\r\nNext big hurdle:\r\n-  Shared libraries under `tensorflow/contrib/rnn/python/ops/` failed at link phase with the error:\r\n```\r\nCreating library bazel-out/host/bin/tensorflow/contrib/rnn/python/ops/_gru_ops.ifso and object bazel-out/host/bin/tensorflow/contrib/rnn/python/ops/_gru_ops.exp\r\nblas_gemm.o : error LNK2019: unresolved external symbol \"public: class perftools::gputools::Stream & __cdecl perftools::gputools::Stream::ThenBlasGemm(enum perftools::gputools::blas::Transpose,enum perftools::gputools::blas::Transpose,unsigned __int64,unsigned __int64,unsigned __int64,float,class perftools::gputools::DeviceMemory<float> const &,int,class perftools::gputools::DeviceMemory<float> const &,int,float,class perftools::gputools::DeviceMemory<float> *,int)\" (?ThenBlasGemm@Stream@gputools@perftools@@QEAAAEAV123@W4Transpose@blas@23@0_K11MAEBV?$DeviceMemory@M@23@H2HMPEAV623@H@Z) referenced in function \"public: void __cdecl tensorflow::functor::TensorCuBlasGemm<float>::operator()(class tensorflow::OpKernelContext *,bool,bool,unsigned __int64,unsigned __int64,unsigned __int64,float,float const *,int,float const *,int,float,float *,int)\" (??R?$TensorCuBlasGemm@M@functor@tensorflow@@QEAAXPEAVOpKernelContext@2@_N1_K22MPEBMH3HMPEAMH@Z)\r\nblas_gemm.o : error LNK2019: unresolved external symbol \"public: class perftools::gputools::Stream & __cdecl perftools::gputools::Stream::ThenBlasGemm(enum perftools::gputools::blas::Transpose,enum perftools::gputools::blas::Transpose,unsigned __int64,unsigned __int64,unsigned __int64,double,class perftools::gputools::DeviceMemory<double> const &,int,class perftools::gputools::DeviceMemory<double> const &,int,double,class perftools::gputools::DeviceMemory<double> *,int)\" (?ThenBlasGemm@Stream@gputools@perftools@@QEAAAEAV123@W4Transpose@blas@23@0_K11NAEBV?$DeviceMemory@N@23@H2HNPEAV623@H@Z) referenced in function \"public: void __cdecl tensorflow::functor::TensorCuBlasGemm<double>::operator()(class tensorflow::OpKernelContext *,bool,bool,unsigned __int64,unsigned __int64,unsigned __int64,double,double const *,int,double const *,int,double,double *,int)\" (??R?$TensorCuBlasGemm@N@functor@tensorflow@@QEAAXPEAVOpKernelContext@2@_N1_K22NPEBNH3HNPEANH@Z)\r\n```\r\n- Then I stumbled upon https://github.com/tensorflow/tensorflow/issues/15013 and saw that another person had faced the same issue quite some time ago and the OP's solution was `add all the .lib from tensorflow and cuda` . I can't do that. So, I went looking for where this symbol comes from, and where should it be. Took me a while to realize what's happening and that's when I came across the workaround \r\nof using intermediate interface shared object file at https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/tensorflow.bzl#L1228-L1237. Apparently, this didn't have enough symbols for the kernels in `tensorflow/contrib/rnn/python/ops/`. So I started looking for where I could find them and ended up adding:\r\n`clean_dep(\"//tensorflow/stream_executor:stream_executor_impl\"),` to that list and the link error went away.\r\n\r\nNext hurdle: https://github.com/tensorflow/tensorflow/issues/20088\r\n-  The compiler doesn't like\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/contrib/fused_conv/kernels/fused_conv2d_bias_activation_op.cc#L46 when it was already being done at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops_gpu.h#L189 . So I commented out the typdef in `fused_conv2d_bias_activation_op.cc` and tried building it. The error went away, but link.exe cried again at the end, because it didn't know where to get the symbol for `GetCudnnWorkspaceLimit` at https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/contrib/fused_conv/kernels/fused_conv2d_bias_activation_op.cc#L519 . After some searching, I found out that this symbol comes from https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/core/kernels/conv_ops.cc#L456-L471 . So I tried making it part of the intermediary additional_deps_impl.ifso import lib, but no matter what, it always ended up bloating it with symbols > 64k (it went upto ~72k, when I tried adding some deps which might have that symbol). So I gave up on this contrib kernel at this point and when I looked at the CMake build (I could be wrong here) I didn't find it building this kernel either. So I just ended up commenting out all references to `fused_conv` in the BUILD files.\r\n\r\nNext hurdle: Bazel had made some breaking changes in 1.13.0\r\n-  I came across at https://github.com/bazelbuild/bazel/issues/4583 and then applied two other patches locally to get around that problem.\r\n\r\nNext hurdle: Tests won't run.\r\n- Most of them want to import nccl. :sob: So I ended up faking it by creating a blank module and then the tests ran fine. I did have to tell bazel to ignore the `contrib/lite` subset `-//${PY_TEST_DIR}/tensorflow/contrib/lite/...`.\r\n\r\nFor the lost souls who attempt at building tensorflow with GPU support, I hope this issue might be helpful. All the patches and the entire build can be found here: https://github.com/AnacondaRecipes/aggregate/blob/459dee0989e0c7cc4ab66c49d3d7605cddbb1bc3/tensorflow-gpu-base-feedstock/recipe/meta.yaml", "comments": ["@meteorcloudy this is a useful report. I think most of this is known, but it could point to some valuable things we can do (or bazel can do) to improve the experience.", "Did anyone from the tensorflow team face the link error with the CMake builds? How did they overcome it?", "@nehaljwani Thank you so much for the investigations and all the PRs to make the Windows build better!\r\n\r\nI was working on making the Bazel GPU build work on Windows a few days ago and went through exactly the same issues you mentioned. I have submitted a change to fix all of them, it will soon be pushed to github.\r\n\r\n@martinwicke I think our first priority is to step up pre&postsubmits to make sure the GPU build work constantly. Then with people trying the build in different environment (eg. with mingw64), we'll see how robust the build is and improve it.\r\n", "That's great news @meteorcloudy . Will your changes make it to the 1.9.0 release?", "I'm not sure it can make into 1.9.0. @gunan @case540 Do you think it's possible?", "1.9 is already almost final. So these will make into 1.10.", "The official switch to bazel is now planned for 1.11 release.\r\nWindows CPU build is now enabled and enforced for master, and it should now just work\r\nWe are working on the documentation, and will make sure to push the documents before 1.11 release.", "Found a related commit: https://github.com/tensorflow/tensorflow/commit/11157efc4e94a7c70ff7532d7bb835fb5d9d19da", "I believe this should be closed? @gunan?", "Let's wait for the docs update, and close with a reference to the docs.", "A ran a build for v1.10.0 today. Faced https://github.com/tensorflow/tensorflow/issues/19198 and then after patching it, I hit another wall at the end:\r\n```\r\nINFO: From PythonZipper tensorflow/tools/pip_package/simple_console_for_windows.zip:\r\nUncompressed input jar has size 509583118, which exceeds the maximum supported output size 4294967295.\r\nAssuming that ijar will be smaller and hoping for the best.\r\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  C:/users/nwani/_bazel_nwani/oemicc5z/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/tensorflow/tools/pip_package/build_pip_package\r\n  C:/users/nwani/_bazel_nwani/oemicc5z/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/tensorflow/tools/pip_package/build_pip_package.exe\r\nINFO: Elapsed time: 9993.655s, Critical Path: 1593.79s\r\nINFO: 6841 processes: 6841 local.\r\nINFO: Build completed successfully, 8973 total actions\r\n+ PY_TEST_DIR=py_test_dir\r\n+ rm -fr py_test_dir\r\n+ mkdir -p py_test_dir\r\n+ cmd /c 'mklink /J py_test_dir\\tensorflow .\\tensorflow'\r\nJunction created for py_test_dir\\tensorflow <<===>> .\\tensorflow\r\n+ ./bazel-bin/tensorflow/tools/pip_package/build_pip_package /c/ci/tensorflow-base_1535197690648/work/py_test_dir\r\nSat, Aug 25, 2018 9:37:13 AM : === Preparing sources in dir: /c/t/tmp.YD7O3ACI0A\r\nUnzipping simple_console_for_windows.zip to create runfiles tree...\r\n[./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip]\r\n  End-of-central-directory signature not found.  Either this file is not\r\n  a zipfile, or it constitutes one disk of a multi-part archive.  In the\r\n  latter case the central directory and zipfile comment will be found on\r\n  the last disk(s) of this archive.\r\nunzip:  cannot find zipfile directory in one of ./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip or\r\n        ./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip.zip, and cannot find ./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip.ZIP, period.\r\n```\r\n\r\nI was using:\r\n```\r\nexport TF_CUDA_COMPUTE_CAPABILITIES=\"3.0,3.5,5.2,6.0,6.1,7.0\"\r\n```", "Upon further investigation, I found that the file passed as an argument to Zipper.exe (provided by bazel as third_party) had a whole bunch of unnecessary zip files, which I got from:\r\n```\r\ngrep zip= /c/users/nwani/_bazel_nwani/oemicc5z/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/tensorflow/tools/pip_package/simple_console_for_windows.zip-2.params\r\n```\r\n\r\nAfter removing all these zip files from the list of files to be zipped, zipper was successful and the build/test process proceeded. \r\n\r\nI said that these zip files in the list of files to be zipped are redundant, because their contents were also listed as files to be zipped. The 18 zip files which I removed manually from the params file were:\r\n```\r\nmnist.zip\r\nlinear_regression.zip\r\nrnn_colorbot.zip\r\nrnn_ptb.zip\r\nknown_anomaly.zip\r\nlstm.zip\r\nmultivariate.zip\r\npredict.zip\r\noffline_analyzer.zip\r\nfreeze_graph.zip\r\nimport_pb_to_tensorboard.zip\r\ninspect_checkpoint.zip\r\noptimize_for_inference.zip\r\nprint_selective_registration_header.zip\r\nsaved_model_cli.zip\r\nstrip_unused.zip\r\ncreate_python_api.zip\r\ngrpc_tensorflow_server.zip\r\n```", "Nagging Assignee @meteorcloudy: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Any word on this last issue (with Zipper)? Do we need to fix the zip in bazel itself or is there some way to maybe split the zips into a manageable size?", "I could be wrong, but from whatever time I could spend on this issue, it seemed like the fixing needs to be done in bazel itself. The zips are already splitted into manageable sizes, but the tflow build system tries to create a bigger one which comprised of all these zip files along with their extracted contents, which seemed odd to me and I couldn't figure out how to fix that, so I manually edited the file in the middle of the build.", "The original issue has been resolved. The new issue is a duplicate of #22390, I'm closing this one. Let's follow the progress in #22390.", "who have solved this problem? help me"]}, {"number": 20331, "title": "update protobuf requirement to 3.6.0 for pip", "body": "since protobuf dependence was update 3.6.0 in 3bfd3aeb (this was actually a part of @gunan's 3bfd3aeb. Probably, it's reverted when merging 1.9.0 back to master), there are incompatible\r\nproblems on machines with 3.4.0 < protobuf version < 3.6.0. \r\n\r\nE.g., I got something like\r\n```\r\nTraceback (most recent call last):\r\n  File \"/tmp/foobar.py\", line 26, in <module>\r\n    from tensorflow.contrib.lite.python import interpreter as interpreter_wrapper\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 52, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py\", line 15, in <module>\r\n    from tensorflow.core.framework import node_def_pb2 as tensorflow_dot_core_dot_framework_dot_node__def__pb2\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/node_def_pb2.py\", line 15, in <module>\r\n    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/attr_value_pb2.py\", line 15, in <module>\r\n    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/tensor_pb2.py\", line 15, in <module>\r\n    from tensorflow.core.framework import resource_handle_pb2 as3bfd3aeb tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/resource_handle_pb2.py\", line 22, in <module>\r\n    serialized_pb=_b('\\n/tensorflow/core/framework/resource_handle.proto\\x12\\ntensorflow\\\"r\\n\\x13ResourceHandleProto\\x12\\x0e\\n\\x06\\x64\\x65vice\\x18\\x01 \\x01(\\t\\x12\\x11\\n\\tcontainer\\x18\\x02 \\x01(\\t\\x12\\x0c\\n\\x04name\\x18\\x03 \\x01(\\t\\x12\\x11\\n\\thash_code\\x18\\x04 \\x01(\\x04\\x12\\x17\\n\\x0fmaybe_type_name\\x18\\x05 \\x01(\\tBn\\n\\x18org.tensorflow.frameworkB\\x0eResourceHandleP\\x01Z=github.com/tensorflow/tensorflow/tensorflow/go/core/framework\\xf8\\x01\\x01\\x62\\x06proto3')\r\nTypeError: __new__() got an unexpected keyword argument 'serialized_options'\r\n```", "comments": []}, {"number": 20330, "title": "make benchmark_model for tflite build", "body": "benchmark_model doesn't build for desktop as reported at https://github.com/tensorflow/tensorflow/issues/20313", "comments": ["This fix worked for me\r\nUbuntu 16.04\r\nBazel 0.10.0", "Unfortunately merging this will have to wait, see \r\nhttps://groups.google.com/a/tensorflow.org/forum/#!topic/announce/6ojJ7ykrmZ0"]}, {"number": 20329, "title": "Fix checkpoints link in keras guide", "body": "", "comments": []}, {"number": 20328, "title": "Add a corrected link to Eager Guide", "body": "", "comments": ["Good to know thanks.", "Thanks for bringing this up. I've added some temp redirects for tensorflow.org/guide/... to get through this in-between time."]}, {"number": 20327, "title": "warning: no files found matching '*.dll' under directory '*'", "body": "Hi all, this problem has troubled me for a long time and I have not found a suitable solution.\r\n\r\nMy system informations:\r\n- **Have I written custom code**: No, I just try to make a building wheel by Bazel.\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from**: source\r\n- **TensorFlow version**: latest\r\n- **Python version**: 3.5 \r\n- **Bazel version**: 0.14.1\r\n- **GCC/Compiler version**: 5.0+\r\n- **CUDA/cuDNN version**: 8.0, 7.0.5\r\n- **GPU model and memory**: GeForce GTX 1050 Ti (4GB)\r\n- **Exact command to reproduce**:`bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg`\r\n\r\nWhen I make a building wheel:\r\n\r\n`bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg`\r\n\r\n I get some warnings like these:\r\n\r\n```\r\n2018\u5e74 06\u6708 26\u65e5 \u661f\u671f\u4e8c 14:22:53 CST : === Preparing sources in dir: /tmp/tmp.DJDLaJRILD\r\n~/tensorflow ~/tensorflow\r\n~/tensorflow\r\n2018\u5e74 06\u6708 26\u65e5 \u661f\u671f\u4e8c 14:24:22 CST : === Building wheel\r\nwarning: no files found matching '*.dll' under directory '*'\r\nwarning: no files found matching '*.lib' under directory '*'\r\nwarning: no files found matching '*.h' under directory 'tensorflow/include/tensorflow'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/Eigen'\r\nwarning: no files found matching '*.h' under directory 'tensorflow/include/google'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/third_party'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/unsupported'\r\n2018\u5e74 06\u6708 26\u65e5 \u661f\u671f\u4e8c 14:26:01 CST : === Output wheel file is in: /tmp/tensorflow_pkg\r\n```\r\n\r\nThen, I check the **Output wheel files**, **but I did not find any tmp/tensorflow folder or .whl file.** \r\nI think many people are also facing the same confusion, anyone can help me?\r\n\r\nThanks in advance!\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nGPU model and memory\nExact command to reproduce", "Thank you for the reminder, and  I have updated the issue.", "Those are just warning messages. Not error messages. Everything seems to be working as expected.\r\nYou can try creating a different folder and creating your pip package there.\r\nOr you can simply try running\r\n```\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package .\r\n```\r\nWhich should create the whl file in the folder you are running the command in."]}, {"number": 20326, "title": "codelabs tutorial poets doesn't work", "body": "Your tutorial does not work on a variety of fronts. You cannot complete the tutorial from start to end just trying to use your instructions. I dare you to do so. \r\n\r\nI tried this using Anaconda AND Pycharm\r\n\r\nhttps://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@Jvalal I completed the tutorial just by following the instructions.  Refer [this](http://nilhcem.com/android/custom-tensorflow-classifier) if you want. Where are you facing issue?", "Answers *INLINE*\n\nOn Wed, Jun 27, 2018 at 5:44 AM Alfred Sorten Wolf <notifications@github.com>\nwrote:\n\n> Thank you for your post. We noticed you have not filled out the following\n> field in the issue template. Could you update them if they are relevant in\n> your case, or leave them as N/A? Thanks.\n> Have I written custom code *NO*\n> OS Platform and Distribution - *Mac High Sierra 10.13.4 (17E199)*\n> TensorFlow installed from - *Tried through Anaconda and PIP*\n> TensorFlow version - 1.8.x\n> Bazel version - n/a\n> CUDA/cuDNN version n/a\n> GPU model and memory - *AMD Radeon Pro 460 and Intel HD Graphics 530*\n> Exact command to reproduce - *There are no commands. Just try and follow\n> the tutorial. It doesn't work. If you want to watch a 15 minute video of me\n> trying to go through it I'm more than happy to record so you can see the\n> isssues. *\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20326#issuecomment-400657341>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADoHrc_9IFejUgwbwOLTLSxjNypAzoeiks5uA34YgaJpZM4U48Cb>\n> .\n>\n", "@Jvalal I don't know what `Answers *INLINE*` means. Tell me at what step in the tutorial are you facing the issue.", "If you look at the thread. It means I responded to you with answers inline\r\nto your questions. did you look at my response?\r\n![screen shot 2018-06-27 at 10 19 02 am](https://user-images.githubusercontent.com/3803053/41989264-9139ec6e-79f3-11e8-9330-98e159dfeb23.png)\r\n\r\n\r\nOn Wed, Jun 27, 2018 at 10:16 AM Sumanth <notifications@github.com> wrote:\r\n\r\n> @Jvalal <https://github.com/Jvalal> I don't know what Answers *INLINE*\r\n> means. Tell me at what step in tutorial are you facing issue?\r\n>\r\n> \u2014\r\n> You are receiving this because you were mentioned.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/20326#issuecomment-400760801>,\r\n> or mute the thread\r\n> <https://github.com/notifications/unsubscribe-auth/ADoHraaKW1Me32EVKxyAYSYV-BMuH2ePks5uA73hgaJpZM4U48Cb>\r\n> .\r\n>\r\n", "```\r\nJust try and follow the tutorial. It doesn't work. If you want to watch a 15-minute video of me trying to go through it I'm more than happy to record so you can see the issues.\r\n```\r\nAt what step are you struck?", "I'm getting errors at Step 2. See attached. I tried installing via terminal AND Anaconda. Same error. \r\n![screen shot 2018-06-28 at 4 40 39 pm](https://user-images.githubusercontent.com/3803053/42065815-103f25f2-7af2-11e8-9253-1c1ba7fa81fa.png)\r\n", "Hello???", "Try in a virtual environment. Afaik, that output isn't an error. It's saying that an upgraded version of tf has already been installed. Also pip is pointing to your system python. Not anaconda python. Execute `which pip` to confirm this. Also, very less people use python2 now.", "If none of these work, uninstall tf completely. Ignore that command to install tf in codelabs. Install the latest version of tf in anaconda preferably and you are good to go.", "the red ones seem like errors. It's pointing to the Anaconda install. \r\n\r\n<img width=\"476\" alt=\"screen shot 2018-06-30 at 12 25 53 pm\" src=\"https://user-images.githubusercontent.com/3803053/42128451-d0e8e864-7c60-11e8-8e73-226b6095ab82.png\">\r\n", "Still get those red errors when I install it in Anaconda. ", "@tatatodd BUMP", "Hi @Jvalal! \r\nWe are checking to see whether you still need help in this issue . Please create a new issue if the issue is replicating in Latest version TF 2.6 . Have you checked this [thread](https://www.qwiklabs.com/focuses/1095?parent=catalog) on Tensorflow for Poets yet?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 20325, "title": "Fix code block formatting", "body": "", "comments": []}]