[{"number": 7417, "title": "Crashes in tf.sparse_tensor_dense_matmul", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nPossibly related to #4282?\r\n\r\nAlso, [SO seems to suggest](https://stackoverflow.com/questions/29401116/abort-trap-6-in-c-program) that \"abort trap 6\" is related to accessing memory that TF doesn't own.\r\n\r\nBoth problems boil down to bounds errors crashing the Python session instead of throwing an error.\r\n\r\n### Environment info\r\nOperating System: macOS Sierra 10.12.3\r\n\r\nTF version 0.12.1. Pretty sure I'm using a CPU-only build, and my graphics card is not CUDA-compatible.\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nI've managed to crash Python with sparse_tensor_dense_matmul in two different ways.  I've included a reproducible example for each.  Let me know if you'd prefer them split into two separate issues.\r\n\r\n***Crash 1***\r\n\r\nAn off-by-one error crashes my Python session.\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nsess = tf.InteractiveSession()\r\n\r\nnrow = 3\r\nncol = 10\r\n\r\nX = tf.sparse_placeholder(tf.float32, shape=[None, ncol])\r\nW = tf.ones([ncol,1])\r\n\r\n# Note: largest allowable column index is ncol-1 because of zero-indexing\r\ncol_indices = [1, 2, ncol] \r\nindices = np.transpose(np.array([range(nrow),col_indices]))\r\n\r\nsess.run(tf.sparse_tensor_dense_matmul(X,W), feed_dict={X:(indices, [1] * nrow, [nrow, ncol])})\r\n```\r\n\r\nThis kills my Python session, and I get the following at my bash prompt:\r\n\r\n```\r\nF tensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc:242] Check failed: k < lhs_right (10 vs. 10)\r\nAbort trap: 6\r\n```\r\n\r\n***Crash 2***\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nsess = tf.InteractiveSession()\r\n\r\nn = 3\r\nm = 3324\r\np = 49\r\n\r\na_sparse = tf.sparse_placeholder(tf.float32, shape=[None, m])\r\nb = tf.Variable(tf.random_normal([m, p]))\r\n\r\n\r\nindices = [[0,0],[1, 2]]\r\nvalues = [1, 1]\r\nshape = [n,m]\r\n\r\nfeed={a_sparse:(np.array([[3828,  135],\r\n        [ 320,   11]]), [1, 1], [2, 3324])}\r\n\r\n\r\ninit = tf.global_variables_initializer()\r\nsess.run(init)\r\nsess.run(tf.sparse_tensor_dense_matmul(a_sparse, b), feed_dict=feed)\r\n```\r\n\r\n```\r\nF tensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc:273] Check failed: m < out.dimension(0) (3828 vs. 2)\r\nAbort trap: 6\r\n```\r\n\r\nIt looks like I'm exceeding the number of rows this time, and getting a crash instead of an error.\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nN/A", "comments": ["(updated with a second reproducible example and more information)", "Yep, this is a bug.  @ebrevdo: Can you take a look?  Pure Python shouldn't be able to get a CHECK crash like this.", "Thanks for reporting this. I'll take a look likely after end of next week", "@ebrevdo Which week were you referring to?", "Indeed.  This op has been rewritten since the original report. Let's close\nit and op can reopen if it still happens.\n\nOn Jun 16, 2017 11:24 AM, \"Geoffrey Irving\" <notifications@github.com>\nwrote:\n\n> @ebrevdo <https://github.com/ebrevdo> Which week were you referring to?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7417#issuecomment-309099384>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimx-RmqumLzMk8aVyxc930I4pmaNLks5sEshagaJpZM4L9iNG>\n> .\n>\n"]}, {"number": 7416, "title": "Bazel build fails on SUSE Enterprise due to too many open files", "body": "This could also be a bazel issue, I don't know.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nNone\r\n\r\n### Environment info\r\nOperating System: SUSE Linux Enterprise Server 12 SP1\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n```\r\n$ ls /usr/local/cuda/lib/libcuda*\r\n/usr/local/cuda/lib/libcudadevrt.a  /usr/local/cuda/lib/libcudart.so.7.0     /usr/local/cuda/lib/libcudart_static.a\r\n/usr/local/cuda/lib/libcudart.so    /usr/local/cuda/lib/libcudart.so.7.0.28\r\n$ ls ~/.local/lib64/libcudnn*\r\n/home/student/r/rdiederichse/.local/lib64/libcudnn.so    /home/student/r/rdiederichse/.local/lib64/libcudnn.so.4.0.7\r\n/home/student/r/rdiederichse/.local/lib64/libcudnn.so.4  /home/student/r/rdiederichse/.local/lib64/libcudnn_static.a\r\n```\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`): `d86cadcdf03ae6fef63f34c90150a4518a121de8`\r\n2. The output of `bazel version`\r\n```\r\nWarning: ignoring http_proxy in environment.\r\nBuild label: 0.4.4- (@non-git)\r\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Feb 10 14:39:20 2017 (1486737560)\r\nBuild timestamp: 1486737560\r\nBuild timestamp as int: 1486737560\r\n```\r\n\r\nThe error I get when  running `bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package` is (after some time)\r\n```\r\nERROR: /home/student/r/rdiederichse/.cache/bazel/_bazel_rdiederichse/e5cce820cc082410b4fcc604db349066/external/jpeg/BUILD:37:1: C++ compilation of rule '@jpeg//:jpeg' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 33 argument(s) skipped): java.io.IOException: Cannot run program \"/home/student/r/rdiederichse/.cache/bazel/_bazel_rdiederichse/e5cce820cc082410b4fcc604db349066/execroot/tensorflow/_bin/process-wrapper\" (in directory \"/home/student/r/rdiederichse/.cache/bazel/_bazel_rdiederichse/e5cce820cc082410b4fcc604db349066/execroot/tensorflow\"): error=24, Too many open files.\r\n```", "comments": ["@damienmg Have you seen this before?  `Too many open files` during the build is odd.", "The limit as given by `ulimit -n` is 1024 btw.", "Yes we have seen that on OS X with high number of --jobs, try forcefully setting --jobs to the number of cores.", "Closing here since this seems to be a Bazel issue, and not TensorFlow specific.  @damienmg Is it worth filing a Bazel bug, or is this impractical to fix?", "We already have a bug for that :( Our resource estimation is quite wrong\nunfortunately (and even it was right, we would still have those issue on\nsome edge case systems).\n\nOn Fri, Feb 10, 2017 at 6:29 PM Geoffrey Irving <notifications@github.com>\nwrote:\n\n> Closed #7416 <https://github.com/tensorflow/tensorflow/issues/7416>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7416#event-957635854>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADjHf7fFBOTa4D88UuEkrM5DcVcCSyn7ks5rbJ58gaJpZM4L9fID>\n> .\n>\n"]}, {"number": 7415, "title": "Scale-out performance limitation for distributed session", "body": "\r\n![image](https://cloud.githubusercontent.com/assets/433170/22829003/8b585f4e-efa0-11e6-9ddc-34a6b7e65cfb.png)\r\n\r\nScale-out performance for scatter-gather dataflow pipelines is limited.\r\n\r\nI use Tensorflow to build custom pipelines (i.e. I write my own OpKernels) of \"embarrassingly parallel\" problems (no coordination between \"local\" pipelines required). Typically these involve the _local_ pipeline being replicated across all the machines in my cluster, and having a source and sink queue to feed input and receive output, respectively.\r\n\r\nThe issue: the performance degrades linearly with the number of nodes in the cluster, compared to a perfect scale-out performance.\r\n\r\nI am not sure if this is a fundamental limitation of the distributed session, or if there is a way within Tensorflow to build such graphs better (with less coordination needed between independent _local_ pipelines).\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nNot much guidance. When speaking with other folks at conferences, it seems common to sidestep the distributed session in Tensorflow for custom, non-TF solutions (e.g. MPI). I personally use ZeroMQ to facilitate these sort of scatter-gather patterns.\r\n\r\n### Environment info\r\n\r\n* Operating System: Linux (Ubuntu 16.04, 4.4.0-22)\r\n* installed from `pip` following the default [download and setup instructions](https://www.tensorflow.org/get_started/os_setup)\r\n* output of `python -c \"import tensorflow; print(tensorflow.__version__)\"` -> `0.12.1`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n* [here is a gist with vanilla tensorflow](https://gist.github.com/samwhitlock/3b099f8489909a3293b3e4239a7188d0), with the versions I specified above. My local pipelines that I stamp out across all nodes in my cluster has custom ops, which I replicated in standard Tensorflow with a while loop to imitate the delay of my custom pipeline. I'm not sure if there is a better way to simulate the delay.\r\n* [here is the data I collected on my cluster of 10 machines (only up to 9 to keep the source/sink queue machine unloaded)](https://docs.google.com/spreadsheets/d/1Ht8O9OT4csfYVOxSDybojXnZ-_uV2A60CF9Jt0jhps0/edit). Note that the workload scales up linearly with _only_ changes with the number of nodes; if the scale-out was perfect, the time should be the same regardless of the number of nodes.\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n* I get around this by limitation by using ZeroMQ as a higher-performance substitute for the source/sink queues (the equivalent of `\"source_queue\"` and `\"sink_queue\"` in the example script)", "comments": ["Please ask this kind of question on StackOverflow, as Github issues are for bug reports and feature requests.  It is probably a lack of parallelism in your graph or bad distribution of ops across workers.", "discussion moved to http://stackoverflow.com/questions/42175033/scale-out-performance-limitations-for-scatter-gather-pattern-in-tensorflow?noredirect=1#comment71548725_42175033\r\n\r\ntldr; driving k workers from a single client thread is not efficient"]}, {"number": 7414, "title": "extract_image_patches gradient only works with float32", "body": "`tf.extract_image_patches()` works well with `tf.float64`, however its gradient requires that `tf.float32`s are passed. Otherwise `tf.gradients()` raises:\r\n`TypeError: Input 'b' of 'SparseTensorDenseMatMul' Op has type float64 that does not match type float32 of argument 'a_values'.`.\r\n\r\nHere is an example of the offending code. If I take the gradient with respect to `X` without casting, I get the error above.\r\n```\r\ncastX = tf.cast(X, tf.float32, name=\"castX\")  # This is needed to get it working\r\npatches = tf.extract_image_patches(tf.reshape(castX, [-1, self.img_size[0], self.img_size[1], 1], name=\"rX\"),\r\n                                   [1, self.patch_size[0], self.patch_size[1], 1],\r\n                                   [1, 1, 1, 1],\r\n                                   [1, 1, 1, 1], \"VALID\")\r\nshp = tf.shape(patches)\r\nreturn tf.cast(tf.reshape(patches, [shp[0], shp[1] * shp[2], shp[3]]), float_type)\r\n```", "comments": ["The problem appears to be this line: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_grad.py#L641.  It explicitly sets a dtype to be `float32`.  Can you change `tf.float32` to `grad_flat.dtype` and see if it solves your problem?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I faced the same issue with calculating gradients for a graph containing `extract_image_patches`, using TF 1.10.0. \r\n\r\nAfter upgrading TF to 1.13.0, the error stack is slightly different but fundamentally the problem remains the same:\r\n\r\n```\r\n[0:01:29] No OpKernel was registered to support Op 'SparseTensorDenseMatMul' used by node gradients/ExtractImagePatches_3_grad/SparseTensorDenseMatMul/SparseTensorDenseMatMul (defined at /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:2757) with these attrs: [Tindices=DT_INT64, T=DT_HALF, _class=[\"loc:@ExtractImagePatches_3\"], adjoint_a=false, adjoint_b=false]\r\nRegistered devices: [CPU, GPU, XLA_CPU, XLA_GPU]\r\nRegistered kernels:\r\n  device='CPU'; T in [DT_COMPLEX128]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_COMPLEX128]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_COMPLEX64]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_COMPLEX64]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_INT32]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_INT32]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]\r\n  device='GPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]\r\n  device='GPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]\r\n\r\n\t [[node gradients/ExtractImagePatches_3_grad/SparseTensorDenseMatMul/SparseTensorDenseMatMul (defined at /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:2757) ]]\r\n\r\nCaused by op 'gradients/ExtractImagePatches_3_grad/SparseTensorDenseMatMul/SparseTensorDenseMatMul', ...\r\n```\r\n\r\nThis issue happens while running official NVidia docker container on a Tesla V100. Code runs fine in single precision, only crashes with half precision.", "> I faced the same issue with calculating gradients for a graph containing `extract_image_patches`, using TF 1.10.0.\r\n> \r\n> After upgrading TF to 1.13.0, the error stack is slightly different but fundamentally the problem remains the same:\r\n> \r\n> ```\r\n> [0:01:29] No OpKernel was registered to support Op 'SparseTensorDenseMatMul' used by node gradients/ExtractImagePatches_3_grad/SparseTensorDenseMatMul/SparseTensorDenseMatMul (defined at /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:2757) with these attrs: [Tindices=DT_INT64, T=DT_HALF, _class=[\"loc:@ExtractImagePatches_3\"], adjoint_a=false, adjoint_b=false]\r\n> Registered devices: [CPU, GPU, XLA_CPU, XLA_GPU]\r\n> Registered kernels:\r\n>   device='CPU'; T in [DT_COMPLEX128]; Tindices in [DT_INT32]\r\n>   device='CPU'; T in [DT_COMPLEX128]; Tindices in [DT_INT64]\r\n>   device='CPU'; T in [DT_COMPLEX64]; Tindices in [DT_INT32]\r\n>   device='CPU'; T in [DT_COMPLEX64]; Tindices in [DT_INT64]\r\n>   device='CPU'; T in [DT_INT32]; Tindices in [DT_INT32]\r\n>   device='CPU'; T in [DT_INT32]; Tindices in [DT_INT64]\r\n>   device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT32]\r\n>   device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT64]\r\n>   device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]\r\n>   device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]\r\n>   device='GPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]\r\n>   device='GPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]\r\n> \r\n> \t [[node gradients/ExtractImagePatches_3_grad/SparseTensorDenseMatMul/SparseTensorDenseMatMul (defined at /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:2757) ]]\r\n> \r\n> Caused by op 'gradients/ExtractImagePatches_3_grad/SparseTensorDenseMatMul/SparseTensorDenseMatMul', ...\r\n> ```\r\n> \r\n> This issue happens while running official NVidia docker container on a Tesla V100. Code runs fine in single precision, only crashes with half precision.\r\n\r\nI come across the same issue as yours when I use `tf.image.extract_patches`. Do you have ways to solve it?"]}, {"number": 7413, "title": "Python : simplify computecpp.tpl", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it! (again)", "@JoanThibault Can you check that you used the same email address to sign the CLA and author the change ? Our bot can't seem to find a CLA for you. ", "CLAs look good, thanks!\n\n<!-- ok -->", ":+1:  Looks good to me! Thanks!", "Jenkins, test this please."]}, {"number": 7412, "title": "AttributeError: module 'tensorflow.contrib.tfprof' has no attribute 'model_analyzer'", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem? No\r\n\r\n### Environment info\r\nOperating System: Win10\r\n\r\nThe output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`. 0.12.1\r\n\r\n### What other attempted solutions have you tried?\r\nI installed tensorflow on Win10 platform. The version is 0.12.1. I tried to run the model zoo's example: resnet:\r\n`python ./resnet/resnet_main.py`\r\nIt occured the error:\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n```\r\n\"D:\\Program Files (x86)\\Miniconda3\\python.exe\" G:/codes/tensorflow/models-master/resnet/resnet_main.py\r\nTraceback (most recent call last):\r\n  File \"G:/codes/tensorflow/models-master/resnet/resnet_main.py\", line 210, in <module>\r\n    tf.app.run()\r\n  File \"D:\\Program Files (x86)\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"G:/codes/tensorflow/models-master/resnet/resnet_main.py\", line 204, in main\r\n    train(hps)\r\n  File \"G:/codes/tensorflow/models-master/resnet/resnet_main.py\", line 57, in train\r\n    param_stats = tf.contrib.tfprof.model_analyzer.print_model_analysis(\r\nAttributeError: module 'tensorflow.contrib.tfprof' has no attribute 'model_analyzer'\r\n```", "comments": ["It actually appears in my 0.12.0-rc1. Do you mind upgrading to 1.0?\r\n\r\npython -c \"import tensorflow; print(tensorflow.__version__)\"\r\n0.12.0-rc1\r\n\r\n>>> print tensorflow.contrib.tfprof.model_analyzer.print_model_analysis\r\n<function print_model_analysis at 0x7fe5949d4b18>", "Found this: https://github.com/tensorflow/tensorflow/issues/6731\r\nIt seems the packages under tf.contrib is currently missing for Windows\r\n\r\n", "Thanks! No, I have not upgrated to 1.0. When the packages will supplement? ", "It is still not available for windows. Can anyone suggest alternatives to deal with this error:\r\n\"module 'tensorflow.contrib.tfprof' has no attribute 'model_analyzer'\"", "@jart Any updates?", "@jart Co-Ask. Will this problem in Windows been solved soon? I am already using the 1.1.0rc0 version, but the problem still exists.", "Thank you for your patience. We've only recently introduced Windows support. Bringing contrib support to Windows is something we're still working to accomplish. Please follow #6791 for updates."]}, {"number": 7411, "title": "bazel build tensorflow/python/tools:optimize_for_inference doesn't work", "body": "I want to build the Android Camera Demo using a custom classifier.\r\nHowever when I run \r\n\r\n`bazel build tensorflow/python/tools:optimize_for_inference`\r\n\r\nin order to optimize my graph I get these errors:\r\n\r\n```\r\nERROR: /home/davide/android/tensorflow/tensorflow/core/BUILD:1259:1: no such target '//tensorflow/tools/\r\ngit:gen/spec.json': target 'gen/spec.json' not declared in package 'tensorflow/tools/git' defined by /home/david\r\ne_biraghi/android/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: /home/davide/android/tensorflow/tensorflow/core/BUILD:1259:1: no such target '//tensorflow/tools/\r\ngit:gen/head': target 'gen/head' not declared in package 'tensorflow/tools/git' defined by /home/davide_biraghi/\r\nandroid/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: /home/davide/android/tensorflow/tensorflow/core/BUILD:1259:1: no such target '//tensorflow/tools/\r\ngit:gen/branch_ref': target 'gen/branch_ref' not declared in package 'tensorflow/tools/git' defined by /home/dav\r\nide_biraghi/android/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'\r\n.\r\nERROR: Analysis of target '//tensorflow/python/tools:optimize_for_inference' failed; build aborted.\r\n```\r\n\r\nSuggestion?", "comments": ["hi, @davideb91, I'm trying to integrate my retrained model to iOS app and having the same problem, have you resolved it? ", "hi, do you know how to fix this problem", "@ainurb @davideb91 Hi~I also have this problem.Do you know how to fix it?"]}, {"number": 7410, "title": "How to update model parameters with accumulated gradients?", "body": "Hi,\r\n\r\nDue to some reason, my model has limited batch size, then this limited batch-size will make the model has a high variance.\r\n\r\nSo, I want to use some trick to make the batch size larger. My idea is to store the gradients of each mini-batch, for example 64 mini-batches, and then sum the gradients together, use the mean gradients of this 64 mini batches of training data to update the model's parameters. \r\n\r\nThis means that for the first 63 mini-batches, do not update the parameters, and after the 64 mini batch, update the model's parameters only once.\r\n\r\nBut as TensorFlow is graph based, do anyone know how to implement this wanted feature?\r\n\r\nThanks very much.", "comments": ["Please ask questions like this on StackOverflow.  Github issues are for bug reports and feature requests."]}, {"number": 7409, "title": "Pass required input_length to batch_sequences_with_states() in example", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks!  Can you make the example:\n\nlength = tf.shape(sequences['inputs'])[0]\n\nOr something of the sort?", "Like this?\r\n", "@tensorflow-jenkins test this please", "Yes, thanks!  Lgtm."]}, {"number": 7408, "title": "Cannot achieve the reported accuracy 0.844 with wide_n_deep_tutorial", "body": "After running the code in \"wide_n_deep_tutorial.py\", I achieved accuracy 0.834 for wide-only model, but the achieved accuracy for wide-and-deep model was only about 0.82.\r\n\r\nI didn't change anything in the source code and just couldn't achieve the accuracy 0.844 as reported on the website. Can anyone help me to tell where could sth be wrong?", "comments": ["@ispirmustafa Is that within noise?", "@Sean-Git I tried that and I get about 84.7% accuracy when train steps = 200, it will fluctuate in small range because of randomness, and when I increase train steps to 800, 1000, or larger, it even can up to 85.5%. so, I think you might check your code and ensure there are no modifications on key code.  ", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "closing based on answer from @whatbeg "]}, {"number": 7407, "title": "Estimator does not catch OutOfRange error", "body": "I created a [4, 1, 5] tensor and use \r\n\r\n```\r\nfeature_a = tf.train.limit_epochs(testing_data_a, num_epochs=1, name=\"feature_a_limit\")\r\ntf.train.batch([feature_a], batch_size=2, enqueue_many=True,\r\n                                                                allow_smaller_final_batch=True, name=\"feature_a_batch\")\r\n```\r\n\r\nto create a batch queue that produces the tensor only once, and I then use this in the input_fn function of `model.evaluate` with steps=2. Because the batch size is 2 and the steps is 2, I expect this could use up the batch queue without an OutOfRange error (or the Estimator will catch this error and use it as an indicator to stop the evaluation), however the Estimator does not catch it and the program ends.\r\n\r\nIs this an expected behavior? If I remember correctly Estimator used to catch such error. Another question is, why the FIFO queue still throw the OutOfRange even if I just request the exact number of elements in the queue?\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nhttp://stackoverflow.com/questions/42127447/tensorflows-estimator-can-only-get-n-1-batches-from-tf-train-limit-epochs\r\n\r\n### Environment info\r\nOperating System:\r\n\r\n```\r\nLinux DOMAIN 3.13.0-107-generic #154-Ubuntu SMP Tue Dec 20 09:57:27 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n```\r\n1536a84f32f1fe77efd3fee6e5933a1dfe4e10bb\r\n```\r\n2. The output of `bazel version`\r\n\r\n```\r\nBuild label: 0.4.3\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Dec 22 12:31:25 2016 (1482409885)\r\nBuild timestamp: 1482409885\r\nBuild timestamp as int: 1482409885\r\n```\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.layers.python.layers.optimizers import optimize_loss\r\nfrom tensorflow.contrib.learn.python.learn.estimators import model_fn\r\nfrom tensorflow.contrib.learn.python.learn.estimators.estimator import Estimator\r\nfrom tensorflow.python import debug as tf_debug\r\nfrom tensorflow.python.framework import ops\r\n\r\n\r\ndef main(_):\r\n    hooks = [tf_debug.LocalCLIDebugHook()]\r\n\r\n    def func(features, targets, mode, params):\r\n        idx = tf.concat([features['a'], features['b']], axis=1)\r\n\r\n        embedding = tf.get_variable(\"embed\", [10, 20], dtype=tf.float32)\r\n\r\n        pred = tf.reduce_sum(tf.nn.embedding_lookup(embedding, idx))\r\n\r\n        train_op = optimize_loss(loss=pred,\r\n                                 global_step=tf.train.get_global_step(),\r\n                                 learning_rate=0.001,\r\n                                 optimizer='Adam',\r\n                                 variables=tf.trainable_variables(),\r\n                                 name=\"training_loss_optimizer\")\r\n\r\n        eval_metric_dict = dict()\r\n        eval_metric_dict['metric'] = pred\r\n\r\n        return model_fn.ModelFnOps(mode=mode,\r\n                                   predictions=pred,\r\n                                   loss=pred,\r\n                                   train_op=train_op,\r\n                                   eval_metric_ops=eval_metric_dict)\r\n\r\n    model = Estimator(func, params={})\r\n\r\n    model.fit(\r\n        input_fn=lambda: (\r\n            {'a': ops.convert_to_tensor([[1, 2, 3, 4, 5]]), 'b': ops.convert_to_tensor([[2, 3, 4, 3, 5]])},\r\n            None), max_steps=10)\r\n\r\n    testing_data_a = [[1, 2, 3, 4, 5], [1, 2, 3, 4, 5] , [1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]\r\n    testing_data_b = [[2, 3, 4, 3, 5], [2, 3, 4, 3, 5] , [2, 3, 4, 3, 5], [2, 3, 4, 3, 5]]\r\n\r\n    def test_input_fn():\r\n        print(\"test_input_fn entered\")\r\n        feature_a = tf.train.limit_epochs(testing_data_a, num_epochs=1, name=\"feature_a_limit\")\r\n        feature_b = tf.train.limit_epochs(testing_data_b, num_epochs=1, name=\"feature_b_limit\")\r\n\r\n        feature_a_producer, feature_b_producer = tf.train.batch([feature_a, feature_b], batch_size=2, enqueue_many=True,\r\n                                                                allow_smaller_final_batch=True, name=\"feature_a_batch\")\r\n\r\n        print(\"test_input_fn exit\")\r\n        return {'a': feature_a_producer, 'b': feature_b_producer}, None\r\n\r\n    for i in range(10):\r\n        print(model.evaluate(input_fn=test_input_fn, steps=2))\r\n        print(\"one iteration done\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    tf.app.run()\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n```\r\nssh://bshi@dsg1.crc.nd.edu:22/data/bshi/py3env/bin/python -u /data/bshi/ProjC/estimator_test.py --debug\r\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpxtkow_oq\r\ntest_input_fn entered\r\ntest_input_fn exit\r\nW tensorflow/core/framework/op_kernel.cc:993] Out of range: FIFOQueue '_0_feature_a_batch/fifo_queue' is closed and has insufficient elements (requested 2, current size 0)\r\n\t [[Node: feature_a_batch = QueueDequeueUpToV2[component_types=[DT_INT32, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](feature_a_batch/fifo_queue, feature_a_batch/n)]]\r\nTraceback (most recent call last):\r\n  File \"/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 1022, in _do_call\r\n    return fn(*args)\r\n  File \"/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 1004, in _run_fn\r\n    status, run_metadata)\r\n  File \"/usr/lib/python3.4/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: FIFOQueue '_0_feature_a_batch/fifo_queue' is closed and has insufficient elements (requested 2, current size 0)\r\n\t [[Node: feature_a_batch = QueueDequeueUpToV2[component_types=[DT_INT32, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](feature_a_batch/fifo_queue, feature_a_batch/n)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/data/bshi/ProjC/estimator_test.py\", line 62, in <module>\r\n    tf.app.run()\r\n  File \"/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/data/bshi/ProjC/estimator_test.py\", line 57, in main\r\n    print(model.evaluate(input_fn=test_input_fn, steps=2))\r\n  File \"/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/util/deprecation.py\", line 280, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 514, in evaluate\r\n    log_progress=log_progress)\r\n  File \"/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 836, in _evaluate_model\r\n    hooks=hooks)\r\n  File \"/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/contrib/training/python/training/evaluation.py\", line 430, in evaluate_once\r\n    session.run(eval_ops, feed_dict)\r\n  File \"/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py\", line 478, in __exit__\r\n    self._close_internal(exception_type)\r\n  File \"/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py\", line 508, in _close_internal\r\n    h.end(self._coordinated_creator.tf_sess)\r\n  File \"/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 647, in end\r\n    feed_dict=self._final_ops_feed_dict)\r\n  File \"/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 767, in run\r\n    run_metadata_ptr)\r\n  File \"/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 965, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: FIFOQueue '_0_feature_a_batch/fifo_queue' is closed and has insufficient elements (requested 2, current size 0)\r\n\t [[Node: feature_a_batch = QueueDequeueUpToV2[component_types=[DT_INT32, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](feature_a_batch/fifo_queue, feature_a_batch/n)]]\r\n\r\nCaused by op 'feature_a_batch', defined at:\r\n  File \"/data/bshi/ProjC/estimator_test.py\", line 62, in <module>\r\n    tf.app.run()\r\n  File \"/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/data/bshi/ProjC/estimator_test.py\", line 57, in main\r\n    print(model.evaluate(input_fn=test_input_fn, steps=2))\r\n  File \"/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/util/deprecation.py\", line 280, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 514, in evaluate\r\n    log_progress=log_progress)\r\n  File \"/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 800, in _evaluate_model\r\n    features, labels = input_fn()\r\n  File \"/data/bshi/ProjC/estimator_test.py\", line 51, in test_input_fn\r\n    allow_smaller_final_batch=True, name=\"feature_a_batch\")\r\n  File \"/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/training/input.py\", line 872, in batch\r\n    name=name)\r\n  File \"/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/training/input.py\", line 665, in _batch\r\n    dequeued = queue.dequeue_up_to(batch_size, name=name)\r\n  File \"/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/ops/data_flow_ops.py\", line 510, in dequeue_up_to\r\n    self._queue_ref, n=n, component_types=self._dtypes, name=name)\r\n  File \"/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 1402, in _queue_dequeue_up_to_v2\r\n    timeout_ms=timeout_ms, name=name)\r\n  File \"/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/data/bshi/py3env/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nOutOfRangeError (see above for traceback): FIFOQueue '_0_feature_a_batch/fifo_queue' is closed and has insufficient elements (requested 2, current size 0)\r\n\t [[Node: feature_a_batch = QueueDequeueUpToV2[component_types=[DT_INT32, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](feature_a_batch/fifo_queue, feature_a_batch/n)]]\r\n\r\n\r\nProcess finished with exit code 1\r\n```\r\n", "comments": ["@ilblackdragon Should learn catch this?", "Is there an alternative way to feed batches into `model.evaluate`?", "I have the same issue on Windows 10. I saw that when I use the batched inputs for the fit operation and setting `steps=None` it works and terminates the training after the epoch is finished. But when I use the same input queue for evaluate and also provide `steps=None` as parameter, it crashes with the OutOfRangeError.", "Current workaround is setting `eval_steps` parameter to a concrete integer value but not `None`.\r\nI think it should catch every `OutOfRangeError` because developers usually want to use whole validation/test dataset to get the best distribution. ", "Ouch. I'm currently experiencing the same problem with the same parameters to `tf.train.batch()` and with different batch_sizes. Using TensorFlow 1.0.1 and I'm parsing TFRecords.", "Same problem here on TensorFlow 1.1 (Windows build).", "Seems to be solved on TensorFlow 1.1. (at least on Linux).\r\n@sunsided It's rather odd because Estimators are written in Python. Did you make sure that you are using 1.1 release and not 1.1rcX?", "@pkubik Yep, 1.1.0 as per `pip install -U tensorflow-gpu`. (Further, calling the input function in a validation monitor (despite deprecated) stops my training when the \"validation\" input queue reaches its end. That might be a different issue, but it definitely is not smooth.)", "@pkubik Just tried on Linux (built from `r1.1` branch) and have the same problem with \"contrib\"  `Estimator` and the `tf.estimator.Estimator`. Looking at the code of the latter, there also seems to be no `try` at all ... am I missing something obvious?\r\n\r\n```\r\n2017-05-24 20:52:38.532029: W tensorflow/core/framework/op_kernel.cc:1152] Out of range: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 16, current size 0)\r\n\t [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]\r\n2017-05-24 20:52:38.532066: W tensorflow/core/framework/op_kernel.cc:1152] Out of range: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 16, current size 0)\r\n\t [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]\r\n2017-05-24 20:52:38.532262: W tensorflow/core/framework/op_kernel.cc:1152] Out of range: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 16, current size 0)\r\n\t [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]\r\n2017-05-24 20:52:38.694023: W tensorflow/core/framework/op_kernel.cc:1152] Out of range: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 16, current size 0)\r\n\t [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]\r\n2017-05-24 20:52:38.694249: W tensorflow/core/framework/op_kernel.cc:1152] Out of range: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 16, current size 0)\r\n\t [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]\r\n2017-05-24 20:52:38.695189: W tensorflow/core/framework/op_kernel.cc:1152] Out of range: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 16, current size 0)\r\n\t [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]\r\n2017-05-24 20:52:38.695218: W tensorflow/core/framework/op_kernel.cc:1152] Out of range: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 16, current size 0)\r\n\t [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]\r\n2017-05-24 20:52:38.695568: W tensorflow/core/framework/op_kernel.cc:1152] Out of range: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 16, current size 0)\r\n\t [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]\r\n2017-05-24 20:52:38.695924: W tensorflow/core/framework/op_kernel.cc:1152] Out of range: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 16, current size 0)\r\n\t [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]\r\nTraceback (most recent call last):\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1039, in _do_call\r\n    return fn(*args)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1021, in _run_fn\r\n    status, run_metadata)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/contextlib.py\", line 89, in __exit__\r\n    next(self.gen)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 16, current size 0)\r\n\t [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/training/evaluation.py\", line 182, in _evaluate_once\r\n    session.run(eval_ops, feed_dict)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 484, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 820, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 776, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 930, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 776, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 778, in run\r\n    run_metadata_ptr)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 982, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1032, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1052, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 16, current size 0)\r\n\t [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]\r\n\r\nCaused by op 'shuffle_batch', defined at:\r\n  File \"/opt/pycharm-2017.1.2/helpers/pydev/pydevd.py\", line 1585, in <module>\r\n    globals = debugger.run(setup['file'], None, None, is_module)\r\n  File \"/opt/pycharm-2017.1.2/helpers/pydev/pydevd.py\", line 1015, in run\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"/opt/pycharm-2017.1.2/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"/home/mmayer/dev/foobar.py\", line 206, in <module>\r\n    main()\r\n  File \"/home/mmayer/dev/foobar.py\", line 40, in main\r\n    run_training(nn, args, model_params)\r\n  File \"/home/mmayer/dev/foobar.py\", line 58, in run_training\r\n    ev = run_evaluation(nn, args, model_params, best_f=None, use_best_checkpoint=True)\r\n  File \"/home/mmayer/dev/foobar.py\", line 82, in run_evaluation\r\n    ev = nn.evaluate(input_fn=lambda: input_fn(args.eval_dir, params=model_params, epochs=1), steps=None)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 268, in evaluate\r\n    name=name)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 604, in _evaluate_model\r\n    features, labels = input_fn()\r\n  File \"/home/mmayer/dev/foobar.py\", line 82, in <lambda>\r\n    ev = nn.evaluate(input_fn=lambda: input_fn(args.eval_dir, params=model_params, epochs=1), steps=None)\r\n  File \"/home/mmayer/dev/input_pipeline/input_fn.py\", line 31, in input_fn\r\n    allow_smaller_final_batch=True)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/training/input.py\", line 1214, in shuffle_batch\r\n    name=name)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/training/input.py\", line 782, in _shuffle_batch\r\n    dequeued = queue.dequeue_up_to(batch_size, name=name)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/ops/data_flow_ops.py\", line 499, in dequeue_up_to\r\n    self._queue_ref, n=n, component_types=self._dtypes, name=name)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 1420, in _queue_dequeue_up_to_v2\r\n    timeout_ms=timeout_ms, name=name)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nOutOfRangeError (see above for traceback): RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 16, current size 0)\r\n\t [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1039, in _do_call\r\n    return fn(*args)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1021, in _run_fn\r\n    status, run_metadata)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/contextlib.py\", line 89, in __exit__\r\n    next(self.gen)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 16, current size 0)\r\n\t [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/opt/pycharm-2017.1.2/helpers/pydev/pydevd.py\", line 1585, in <module>\r\n    globals = debugger.run(setup['file'], None, None, is_module)\r\n  File \"/opt/pycharm-2017.1.2/helpers/pydev/pydevd.py\", line 1015, in run\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"/opt/pycharm-2017.1.2/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"/home/mmayer/dev/foobar.py\", line 206, in <module>\r\n    main()\r\n  File \"/home/mmayer/dev/foobar.py\", line 40, in main\r\n    run_training(nn, args, model_params)\r\n  File \"/home/mmayer/dev/foobar.py\", line 58, in run_training\r\n    ev = run_evaluation(nn, args, model_params, best_f=None, use_best_checkpoint=True)\r\n  File \"/home/mmayer/dev/foobar.py\", line 82, in run_evaluation\r\n    ev = nn.evaluate(input_fn=lambda: input_fn(args.eval_dir, params=model_params, epochs=1), steps=None)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 268, in evaluate\r\n    name=name)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 632, in _evaluate_model\r\n    config=config_pb2.ConfigProto(allow_soft_placement=True))\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/training/evaluation.py\", line 182, in _evaluate_once\r\n    session.run(eval_ops, feed_dict)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 500, in __exit__\r\n    self._close_internal(exception_type)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 532, in _close_internal\r\n    h.end(self._coordinated_creator.tf_sess)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 663, in end\r\n    feed_dict=self._final_ops_feed_dict)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 778, in run\r\n    run_metadata_ptr)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 982, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1032, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1052, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 16, current size 0)\r\n\t [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]\r\n\r\nCaused by op 'shuffle_batch', defined at:\r\n  File \"/opt/pycharm-2017.1.2/helpers/pydev/pydevd.py\", line 1585, in <module>\r\n    globals = debugger.run(setup['file'], None, None, is_module)\r\n  File \"/opt/pycharm-2017.1.2/helpers/pydev/pydevd.py\", line 1015, in run\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"/opt/pycharm-2017.1.2/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"/home/mmayer/dev/foobar.py\", line 206, in <module>\r\n    main()\r\n  File \"/home/mmayer/dev/foobar.py\", line 40, in main\r\n    run_training(nn, args, model_params)\r\n  File \"/home/mmayer/dev/foobar.py\", line 58, in run_training\r\n    ev = run_evaluation(nn, args, model_params, best_f=None, use_best_checkpoint=True)\r\n  File \"/home/mmayer/dev/foobar.py\", line 82, in run_evaluation\r\n    ev = nn.evaluate(input_fn=lambda: input_fn(args.eval_dir, params=model_params, epochs=1), steps=None)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 268, in evaluate\r\n    name=name)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 604, in _evaluate_model\r\n    features, labels = input_fn()\r\n  File \"/home/mmayer/dev/foobar.py\", line 82, in <lambda>\r\n    ev = nn.evaluate(input_fn=lambda: input_fn(args.eval_dir, params=model_params, epochs=1), steps=None)\r\n  File \"/home/mmayer/dev/input_pipeline/input_fn.py\", line 31, in input_fn\r\n    allow_smaller_final_batch=True)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/training/input.py\", line 1214, in shuffle_batch\r\n    name=name)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/training/input.py\", line 782, in _shuffle_batch\r\n    dequeued = queue.dequeue_up_to(batch_size, name=name)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/ops/data_flow_ops.py\", line 499, in dequeue_up_to\r\n    self._queue_ref, n=n, component_types=self._dtypes, name=name)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 1420, in _queue_dequeue_up_to_v2\r\n    timeout_ms=timeout_ms, name=name)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/mmayer/.conda/envs/tensorflow-xla/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nOutOfRangeError (see above for traceback): RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 16, current size 0)\r\n\t [[Node: shuffle_batch = QueueDequeueUpToV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, shuffle_batch/n)]]\r\n```\r\n\r\nThe line in question in my code is a\r\n\r\n```python\r\nimage_batch = tf.train.shuffle_batch([file_names, image_bytes],\r\n                                                 batch_size=params['batch_size'],\r\n                                                 capacity=1000, min_after_dequeue=100,\r\n                                                 num_threads=params['input_threads'],\r\n                                                 enqueue_many=True,\r\n                                                 allow_smaller_final_batch=True)\r\n```", "I found that my code using `Estimator` works well when it is passed no `eval_metric_ops`. Does someone experience the same?", "@raviqqe I have partially reproduced your error. It occurs after passing an invalid metric to the `eval_metric_ops`. Each metric should be a tuple of `(value_tensor, update_op)`. It worked for me until I made a quick hack (don't ask why) and passed a `(value_tensor, value_tensor)` to the `eval_metric_ops`.\r\nI'm not sure how is it related to the queues though. Anyway, please make sure that your metric ops are correct.\r\n\r\nEdit:\r\nI should add that you'd probably want to use `tf.metrics.*`.", "@bxshi, did @pkubik's suggestion to fix your metrix solve your problem?", "@pkubik's suggestion fixed the problem for me.\r\n\r\nAfter replacing my homemade metric\r\n```tf.reduce_mean(tf.cast(tf.equal(predicted_classes, labels), tf.float32))```\r\nwith\r\n```tf.metrics.accuracy(labels, predicted_classes)```\r\nit runs smoothly.\r\n\r\nMaybe something could throw a more useful exception that indicates how to solve the problem, when people try to use improper metrics?", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!", "I have a workaround that makes use of a homemade metric. I just wrapped around the homemade metric with `tf.metrics.mean` like so:\r\n\r\n`tf.metrics.mean(tf.reduce_mean(tf.cast(tf.equal(predicted_classes, labels), tf.float32)))`\r\n\r\nIt works but I'm not sure if it outputs the same thing as using tf.metrics.accuracy. If it does output the same thing, it would be possible to use homemade metrics."]}, {"number": 7406, "title": "Should check whether n_class is zero before calling sample_n() in mixture.py", "body": "### Problem Description\r\nMixture model first use categorical to sample how much samples it need for each mixture components (this is variable `n_class` at [line 308](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distributions/python/ops/mixture.py#L308), but it actually means **number of samples per component**), and then it pass `n_class` to `sample_n()`.\r\n\r\nThe problem is `n_class` could be 0 and you can't pass `shape=0` to `tf.random_gamma(shape, alpha, ...)`, which is used in Beta distribution. (see [line 310](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distributions/python/ops/mixture.py#L310) in mixture.py)\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nIt's easy to reproduce: just create a mixture of Beta + Uniform with 50/50 probability. Half of the time it'll sample from uniform, and half of the time it'll sample from Beta.\r\n```python\r\n#!/usr/bin/python\r\nimport tensorflow as tf\r\nds = tf.contrib.distributions\r\n\r\n# Create mixture distribution of Beta + Uniform\r\ncomponents = [ds.Beta(2., 2.), ds.Uniform(a=0., b=1.)]\r\ncat = ds.Categorical(p=[0.5, 0.5])\r\nmix = ds.Mixture(cat=cat, components=components)\r\n\r\n# get ONLY 1 sample\r\nx = mix.sample_n(1)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())    \r\n    # repeats until crash\r\n    for i in range(1000):\r\n        print sess.run(x)\r\n```\r\n### What other attempted solutions have you tried?\r\n\r\nTwo possible solutions:\r\n\r\n1. Add a conditional branch in **mixture.py** like this (tested with the above script):\r\n    ```python\r\n    # INSTEAD OF DOING\r\n    # samples_class_c = self.components[c].sample_n(n_class, seed=seed)\r\n    # DO THIS\r\n    samples_class_c = control_flow_ops.cond(\r\n        math_ops.equal(n_class, 0),\r\n        lambda: array_ops.zeros(0, self.components[c].dtype),\r\n        lambda: self.components[c].sample_n(n_class, seed=seed)\r\n    )\r\n    ```\r\n    Just create a zero tensor with shape 0 when `n_class` is **0** and let the reshape operator at [line 330](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distributions/python/ops/mixture.py#L330) to worry about the shape.\r\n\r\n2. Support `shape=0` in `random_gamma(shape, alpha, ...)`. Personally I think it's a bad idea. It already caused `InvalidArgumentError` exception, which means the one who implemented this might already considered this problem before.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n```bash\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: TITAN X (Pascal)\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.531\r\npciBusID 0000:02:00.0\r\nTotal memory: 11.90GiB\r\nFree memory: 337.50MiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x4190110\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: \r\nname: GeForce GTX 980 Ti\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\r\npciBusID 0000:01:00.0\r\nTotal memory: 5.93GiB\r\nFree memory: 5.08GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 980 Ti, pci bus id: 0000:01:00.0)\r\n[ 0.32805401]\r\n[ 0.2802822]\r\nW tensorflow/core/framework/op_kernel.cc:975] Invalid argument: Input shape should have non-zero element count, got: 0\r\n\t [[Node: Beta/sample_n/random_gamma_1/RandomGamma = RandomGamma[S=DT_INT32, T=DT_FLOAT, seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Beta/sample_n/random_gamma_1/shape/_25, Beta/sample_n/random_gamma_1/add/_27)]]\r\nTraceback (most recent call last):\r\n  File \"bug.py\", line 21, in <module>\r\n    print sess.run(x)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Input shape should have non-zero element count, got: 0\r\n\t [[Node: Beta/sample_n/random_gamma_1/RandomGamma = RandomGamma[S=DT_INT32, T=DT_FLOAT, seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Beta/sample_n/random_gamma_1/shape/_25, Beta/sample_n/random_gamma_1/add/_27)]]\r\n\r\nCaused by op u'Beta/sample_n/random_gamma_1/RandomGamma', defined at:\r\n  File \"bug.py\", line 15, in <module>\r\n    x = mix.sample_n(1)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distributions/python/ops/distribution.py\", line 574, in sample_n\r\n    x = self._sample_n(n, seed, **condition_kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distributions/python/ops/mixture.py\", line 313, in _sample_n\r\n    samples_class_c = self.components[c].sample_n(n_class, seed=seed)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distributions/python/ops/distribution.py\", line 574, in sample_n\r\n    x = self._sample_n(n, seed, **condition_kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distributions/python/ops/beta.py\", line 205, in _sample_n\r\n    seed=distribution_util.gen_new_seed(seed, \"beta\"))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/random_ops.py\", line 437, in random_gamma\r\n    seed2=seed2) / beta\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_random_ops.py\", line 122, in _random_gamma\r\n    seed=seed, seed2=seed2, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Input shape should have non-zero element count, got: 0\r\n\t [[Node: Beta/sample_n/random_gamma_1/RandomGamma = RandomGamma[S=DT_INT32, T=DT_FLOAT, seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Beta/sample_n/random_gamma_1/shape/_25, Beta/sample_n/random_gamma_1/add/_27)]]\r\n```\r\n\r\nP.S. The variable name `n_class` confused me for a while.", "comments": ["The bug here is that `RandomGamma` doesn't allow zero samples.  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/random_op.cc#L306 should be fixed to exit early in that case rather than crashing.  @brianwa84: Can you confirm this interpretation?\r\n\r\n@botonchou Pending Brian's ack, would you be interested in submitting a pull request?", "Returning a zero shape tensor is OK for the zero sample use-case. Feel free\r\nto submit the pull request.\r\n\r\n\r\n\r\nOn Fri, Feb 10, 2017 at 11:49 AM, Geoffrey Irving <notifications@github.com>\r\nwrote:\r\n\r\n> The bug here is that RandomGamma doesn't allow zero samples.\r\n> https://github.com/tensorflow/tensorflow/blob/master/\r\n> tensorflow/core/kernels/random_op.cc#L306 should be fixed to exit early\r\n> in that case rather than crashing. @brianwa84\r\n> <https://github.com/brianwa84>: Can you confirm this interpretation?\r\n>\r\n> @botonchou <https://github.com/botonchou> Pending Brian's ack, would you\r\n> be interested in submitting a pull request?\r\n>\r\n> \u2014\r\n> You are receiving this because you were mentioned.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/7406#issuecomment-278996301>,\r\n> or mute the thread\r\n> <https://github.com/notifications/unsubscribe-auth/AVJZI8fwfqaPDIeg0TDDN-fBUXmQdIpAks5rbJURgaJpZM4L8_pY>\r\n> .\r\n>\r\n", "@brianwa84 Not to pry, but did you really intend to post your phone number in a Github comment?", "heh thx. email signature fail"]}, {"number": 7405, "title": "tf.complex_abs not supported in r1.0.0 ", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\nHI,\r\nI installed tf from [this link](https://www.tensorflow.org/versions/r1.0/get_started/os_setup) on Mac OS Sierra 10.12. However, tf.complex_abs() module is returning an error \"'module' object has no attribute 'complex_abs'\".\r\n\r\n The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`. :: 1.0.0-rc2", "comments": ["It has been deprecated and is now called tf.abs(). PrettyTensor doesn't support this apparently. Closing it, since it is not a tf issue."]}, {"number": 7404, "title": "No attribute 'outer_context' when calculating gradient from imported graph", "body": "It seems when you import a graph with a \"while\" loop, you can't calculate gradients as you could on the original graph. e.g.\r\n\r\n```python\r\nimport tensorflow as tf\r\ni=tf.constant(0, name=\"input\")\r\nout=tf.while_loop(lambda i: tf.less(i,5), lambda i: [tf.add(i,1)], [i], name=\"output\")\r\ngraph_def = tf.get_default_graph().as_graph_def()\r\n\r\ng = tf.Graph()\r\nwith g.as_default():\r\n    tf.import_graph_def(graph_def)\r\ns = tf.Session(graph=g)\r\ni_imported = g.get_tensor_by_name(\"import/input:0\")\r\nout_imported = g.get_tensor_by_name(\"import/output/Exit:0\")\r\ntf.gradients(out_imported, i_imported)\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-12-e7e2b78684d3> in <module>()\r\n----> 1 tf.gradients(out_imported, i_imported)\r\n\r\n/Users/malmaud/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.pyc in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)\r\n    439     pending_count, loop_state = _PendingCount(ops.get_default_graph(), to_ops,\r\n    440                                               from_ops,\r\n--> 441                                               colocate_gradients_with_ops)\r\n    442 \r\n    443     # Iterate over the collected ops.\r\n\r\n\r\n/Users/malmaud/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.pyc in _PendingCount(graph, to_ops, from_ops, colocate_gradients_with_ops)\r\n    184   # 'loop_state' is None if there are no while loops.\r\n    185   loop_state = control_flow_ops.MaybeCreateControlFlowState(\r\n--> 186       between_op_list, between_ops, colocate_gradients_with_ops)\r\n    187 \r\n    188   # Initialize pending count for between ops.\r\n\r\n/Users/malmaud/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc in MaybeCreateControlFlowState(between_op_list, between_ops, colocate_gradients_with_ops)\r\n   1293           loop_state.AddWhileContext(op, between_op_list, between_ops)\r\n   1294       else:\r\n-> 1295         loop_state.AddWhileContext(op, between_op_list, between_ops)\r\n   1296   return loop_state\r\n   1297 \r\n\r\n/Users/malmaud/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc in AddWhileContext(self, op, between_op_list, between_ops)\r\n   1102     if grad_state is None:\r\n   1103       # This is a new while loop so create a grad state for it.\r\n-> 1104       outer_forward_ctxt = forward_ctxt.outer_context\r\n   1105       if outer_forward_ctxt:\r\n   1106         outer_forward_ctxt = outer_forward_ctxt.GetWhileContext()\r\n\r\nAttributeError: 'NoneType' object has no attribute 'outer_context'\r\n```", "comments": ["@ebrevdo Any idea what would be happening here?  Do while loop gradients depend on unserialized information?", "Yes, though I thought Yuan and Sherry had added the appropriate serialization to the metagraph.  Is this on master branch of tf?", "Ya, this is master.", "OK, I see if I save and restore the metagraph (via `tf.train.export_meta_graph` and `tf.train.import_meta_graph`, then I can take the gradient in the restored session. So it's just a problem if you try serializing and importing the `GraphDef` itself. ", "@ebrevdo Is there an easy way to produce a nicer error message here, so that users don't have to guess the graph vs. metagraph issue?", "Yes; I believe that the forward_ctxt object here:\n\nforward_ctxt.outer_context\n\nshould have a @property outer_context; and if no such internal object is\nset, it can raise a ValueError/InternalError that maybe you're trying to\ncall gradients on a while loop without properly serializing your graph via\nMetaGraphDef.\n\nOn Mon, Feb 13, 2017 at 7:51 AM, Geoffrey Irving <notifications@github.com>\nwrote:\n\n> @ebrevdo <https://github.com/ebrevdo> Is there an easy way to produce a\n> nicer error message here, so that users don't have to guess the graph vs.\n> metagraph issue?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7404#issuecomment-279431704>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimxlSpTQAQlztIVyjpp6fmXDWkZ60ks5rcHvvgaJpZM4L89DK>\n> .\n>\n", "Hi @ebrevdo , is there any update on this issue? Thanks!", "@ebrevdo, any update on this issue or any workaround? Thanks!", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@ebrevdo should we mark this contributions welcome?", "Yes", "Created a PR #18052 to fix this.", "Hello, I came up with the same issue when I use **tf.import_graph_def**:\r\nAttributeError: 'NoneType' object has no attribute 'outer_context'\r\n\r\nFollowing the instructions above I switched to **import_meta_graph** like this:\r\n\r\n`\r\ndef _make_model_and_ops(self, patch_val):\r\n        start = time.time()\r\n        \r\n        self.image_input_ = tf.placeholder(tf.float32, shape=(None, None, None, 3), name='image_input')\r\n        saver = tf.train.import_meta_graph(PATH_TO_CKPT + '.meta',\r\n                                           import_scope=\"detection\",\r\n                                           input_map={\r\n                                               'ToFloat_3': self.image_input_\r\n                                           })\r\n        saver.restore(self.sess, PATH_TO_CKPT)\r\n        self.graph = self.sess.graph\r\n        \r\n        with self.sess.graph.as_default():\r\n            tf.set_random_seed(1234)\r\n            \r\n            # Tensors are post-fixed with an underscore!\r\n            #self.image_input_shape_ = tf.placeholder(tf.int32, shape=(1,3), name=\"image_input_shape\")\r\n            \r\n            self.eps_ = tf.placeholder(tf.float32, shape=(1), name='eps')\r\n            \r\n            # The following commented part is the original code using import_graph_def\r\n            '''\r\n            detection_graph_def = tf.GraphDef()\r\n            with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\r\n                serialized_graph = fid.read()\r\n                detection_graph_def.ParseFromString(serialized_graph)\r\n                tf.import_graph_def(detection_graph_def, name='detection',\r\n                                    input_map={\r\n                                               'Preprocessor/map/TensorArrayStack/TensorArrayGatherV3': self.image_input_,\r\n                                               'Preprocessor/map/TensorArrayStack_1/TensorArrayGatherV3': self.image_input_shape_\r\n                                              })\r\n            '''\r\n            \r\n            # Second-stage Class Loss\r\n            self.second_stage_cls_scores_ = self.graph.get_tensor_by_name('detection/SecondStagePostprocessor/convert_scores:0')\r\n            second_stage_cls_logits_ = self.graph.get_tensor_by_name('detection/SecondStagePostprocessor/scale_logits:0')\r\n            self.second_stage_cls_labels_ = tf.placeholder(tf.float32, shape=second_stage_cls_logits_.shape, name='second_stage_cls_labels')\r\n            \r\n            self.second_stage_cls_losses_ = tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf.reshape(self.second_stage_cls_labels_, (-1, self.second_stage_cls_labels_.shape[2])),\r\n                                                                                       logits=tf.reshape(second_stage_cls_logits_, (-1, second_stage_cls_logits_.shape[2]))) \r\n           \r\n            # Second-stage bounding boxes\r\n            self.second_stage_loc_bboxes_ = self.graph.get_tensor_by_name('detection/SecondStagePostprocessor/Reshape_4:0')\r\n            \r\n            grads = tf.gradients(self.second_stage_cls_losses_, [self.image_input_])\r\n            print(grads)\r\n            self.unclipped_adv_images_ = self.image_input_ + self.eps_ * tf.sign(grads)\r\n            self.adv_images_ = tf.clip_by_value(self.unclipped_adv_images_, clip_value_min=0, clip_value_max=255)\r\n    \r\n    \r\n        elapsed = time.time() - start\r\n        print(\"Finished loading the model, took {:.0f}s\".format(elapsed))           \r\n`\r\n\r\nBut the problem still exist.\r\nDoes this mean that the information required for tf.gradients does not appear in the check point? If so, what else can I do?\r\n\r\nThanks, and sorry for my poor English....", "Having the same problem as @ProjectDimlight, tried the suggestions above with no luck. Any updates on this?"]}, {"number": 7403, "title": "Incorrect second derivative of softmax cross entropy loss", "body": "### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nlogits = tf.Variable([0.5, 0.5])\r\ny = tf.constant([1,0], dtype=tf.float32)\r\n\r\n# direct computation\r\nloss1 = -tf.reduce_sum(y * tf.log(tf.nn.softmax(logits)))\r\n\r\n# optimized version\r\nloss2 = tf.nn.softmax_cross_entropy_with_logits(logits, y)\r\n\r\nfeed = {logits: [0.5, 0.5]}\r\n\r\nwith tf.Session() as sess:\r\n    for i, loss in enumerate([loss1, loss2]):\r\n        g = tf.gradients(loss, [logits])[0]\r\n        \r\n        h0 = tf.gradients(g[0], [logits])[0]\r\n        h1 = tf.gradients(g[1], [logits])[0]\r\n        h = tf.pack([h0, h1])\r\n        \r\n        print 'loss%d:' % (i+1)\r\n        print 'gradient:'\r\n        print g.eval(feed_dict=feed)\r\n        print 'hessian:'\r\n        print h.eval(feed_dict=feed)\r\n        print \r\n```\r\n\r\nproduces the output\r\n\r\n```\r\nloss1:\r\ngradient:\r\n[-0.5  0.5]\r\nhessian:\r\n[[ 0.25 -0.25]\r\n [-0.25  0.25]]\r\n\r\nloss2:\r\ngradient:\r\n[-0.5  0.5]\r\nhessian:\r\n[[-0.  0.]\r\n [-0.  0.]]\r\n```\r\n\r\nAlthough the gradient is correct in both cases, the hessian computed using ```loss1``` is correct while the one computed from ```softmax_cross_entropy_with_logits``` is not. \r\nI have not figured out where the issue arises from.", "comments": ["I'm guessing you're using an old version of TensorFlow.  HEAD explicitly bails if you try to do this: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_grad.py#L334.\r\n\r\nWe'd be happy to accept a fix to do the Hessian correctly  @alextp Is there any fundamental issue here, or did we just not get to it?", "There is no fundamental issue, simply no one has implemented it yet. We should figure out a way fo back off from these fused implementations when things like second derivatives are requested.\r\n\r\nIn this case I think the right answer for now is to not use tf.softmax_cross_entropy_with_logits", "@girving @alextp Hi guys. I want to contribute something to tensorflow and I think this issue is good starting point for me. Could someone to push me in right direction?\r\n\r\nI've followed the tutorial on creating new op in TF, so I have \"some\" knowlegde about the core of the library.", "@persiyanov You shouldn't need to create any new ops.  Instead, you'll need to write a more functional version of the registered gradient function at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_grad.py#L334\r\n\r\nIt may help to look at other gradient-of-derivative registrations, such as https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_grad.py#L501.", "@girving Yes, I understand that I need not create new ops, it's just for my proficiency.\r\n\r\nSo, you say that there is no need to dive into C++ core in order to fix this issue?\r\n\r\nIn `_SoftmaxCrossEntropyWithLogitsGrad(op, grad_0, _)`, how could we know whether second derivative will be requested or not? I mean, is there some convenient way? Maybe instead of `prevent_gradient` there is some register function where we can specify how gradients of softmax gradients should be computed?\r\n\r\nAs I see, if we overcome this trouble, gradient of trivial crossentropy error (`-tf.reduce_sum(y * tf.log(tf.nn.softmax(logits)))`) can be returned.", "@persiyanov Ah, sorry, I misread the Python stub I pointed you to.  I thought it was more like the gradient of `SigmoidGrad` second example I linked, where the answer to your question would be obvious.\r\n\r\nIn the softmax case, the native C++ op produces one output for the softmax loss and one for the gradient, so you're right that you can't know \"in this routine\" whether the second derivative is actually used.  However, you can know \"outside of this routine\", since normal users who ask for the second derivative will ask for the derivative of the `_BroadcastMul(grad_0, softmax_grad_without_gradient)`.  Thus, I see two ways to proceed:\r\n\r\n1. Compute the second derivative unconditionally: always take grad_1 and process it appropriately.  This is the sanest option.  Unfortunately, unless an optimization is performed, nearly all classification models will get a bit slower.\r\n\r\n2. Continue to use `array_ops.prevent_gradient`, but wrap the returned `mul` in a complicated gradient override block to \"un-ban the gradients\".  If the overridden gradients happen, you'll know the second derivative was actually requested, so there's no slowdown.\r\n\r\nThe optimization that would make (1) perfect would be detecting that `grad_1` is zeros, and skipping the second derivative calculation if so.  This is harder than it looks, unfortunately, since the zeros appear to be originate from potentially nasty looking control flow code.  Sometimes it's just `ZerosLike`, sometimes it's weird switch logic.\r\n\r\n@mrry @ebrevdo Do we have a nice way of checking whether a tensor is known to be zero that will work despite crazy loop logic?  Something as simple as `tensor.is_zero()` or `tf.is_zero(tensor)` would be lovely, and presumably useful elsewhere.\r\n\r\nDepressing how complicated this all is.  Or maybe I've misunderstood it?", "@girving \r\nWhat do you mean in (1) when you say \"always take grad_1 and process it appropriately\"? What grad_1?\r\n\r\n\r\nP.S. I have noticed that there is `_ReluGradGrad` in `nn_grad.py` which is derivative of the relu's gradient. Maybe there is a way to do something similar with softmax xentropy?\r\n\r\nP.S.S. Have I understood correctly that I can find all ops' signatures in `gen_nn_ops.py::_InitOpDefLibrary.op_list_ascii`?", "@girving I thought if the graph doesn't use the first derivative in the forward pass the second derivative won't be computed in the backward pass since there will be no edges connecting it with anything anyone will run, as a consequence of how we prune graphs for feeds and fetches. Or am I missing something?\r\n\r\nIf I'm not missing anything, option (1) is a good way to go.", "@alextp You should correct me if I'm wrong, but I believe that `tf.gradients` will feed zeros to `grad_1`.  None of the gradient routines handle `None` inputs, so it can't be `None` for a similar absence of input.  The gradient routine is then supposed to add the various gradient contributions together, which will produce a TensorFlow graph where the result of the zero computation is live downstream.", "@persiyanov The `SoftmaxCrossEntropyWithLogits` op produces two outputs.  One is the tensor of values, and the other is the gradient.  Thus, the unused second argument to the registered gradient routine currently called `_`, or `grad_1` for short`, is the gradient of the loss w.r.t. the gradient, and is what you need to compute second order gradients.\r\n\r\nP.S.  No, that's the simpler case where the gradient is a different op.\r\n\r\nP.P.S. Yes.  Alternatively, you can grep the code for `REGISTER_OP`.", "@girving Ah, okay, I see.\r\n\r\n`SoftmaxCrossentropyWithLogits(logits, labels) -> (loss, grad of loss w.r.t. logits)`\r\n\r\nTherefore,\r\n\r\n`SoftmaxCrossentropyWithLogitsGrad(op, grad_0, grad_1) -> (grad w.r.t logits, grad w.r.t labels)`\r\n\r\nWhere, `grad_0 -- backprop w.r.t loss, grad_1 -- backprop w.r.t (grad w.r.t logits)` and `grad w.r.t labels` is actually ``None`.\r\n\r\n\r\nI need to know ``grad_1`` for computing hessian. \r\n\r\nBut you say that I need to compute second derivative myself. So where do I need to compute it? I can't do this in `SoftmaxCrossentropyWithLogitsGrad`.\r\n\r\nWhen you said about optimization and checking `grad_1` on zero values, I understood it as \"check grad_1 in `SoftmaxCrossentropyWithLogitsGrad`, and if it equals zeroes, then return gradient of softmax loss without fusion. Otherwise, use `op.outputs[1]` for computing gradient.\"\r\n\r\n\r\nP.S. Of course, when I say \"equals zeroes\" I mean your hacky checking.", "Setting aside the optimization, what do you mean by \"I can't do this in `SoftmaxCrossEntropyWithLogitsGrad`\"?  That's where you'd do it.\r\n\r\nIt may be simpler to ignore the fact that second derivatives are involved.  Consider the op as an arbitrary smooth function that takes in some inputs and produces some outputs.  You can write down the formulas for everything involved, so you can take all the required derivatives.\r\n\r\nDoes that make sense?  We should make sure that part is clear before moving on to the optimization bit.", "@girving I can't do this because `SoftmaxCrossentropyWithLogitsGrad` must return first derivatives w.r.t logits and labels, not second ones. Or there is something that I missed and that is why I seem stupid:)", "@persiyanov I think I don't quite understand your question.  Consider the following function from R -> R^2:\r\n\r\n    def f(x):\r\n      return x**2, 2*x\r\n\r\n    def grad_f(x, dy, dz):\r\n      return 2*x*dy + 2*dz\r\n\r\n`f` returns both a function and its first derivative.  It's gradient computes only \"first derivatives\", but clearly contains enough information to compute the second derivative of `f`'s first result.\r\n\r\n**Note**: In the TensorFlow equivalent we're discussing, the occurrence of `2*x` in `grad_f` is a reference to the `2*x` produced by `f`, so that everything is linked together explicitly.", "One of the outputs of SoftmaxCrossEntropyWithLogits is the gradient, so the\ngradient wrt this second output will be the second derivative.\n\nOn Tue, Feb 21, 2017 at 9:16 AM, Dmitry Persiyanov <notifications@github.com\n> wrote:\n\n> I can't do this because SoftmaxCrossentropyWithLogitsGrad must return\n> first derivatives w.r.t logits and labels, not second ones. Or there is\n> something that I missed and that is why I seem stupid:)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7403#issuecomment-281411077>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxfyR4CjKNOtJk88myoTlZLgyTYkhks5rexvngaJpZM4L86cM>\n> .\n>\n\n\n\n-- \n - Alex\n", "@girving why `grad_f` returns `2*x*dy + 2*dz`, not just `2*x*dy` (it is the first derivative of f(x) w.r.t x multiplied by backprop)?\r\n\r\n\r\nI understand that I CAN compute second derivative in `_SoftmaxCrossEntropyWithLogitsGrad`. I do not understand WHERE should I return it.\r\n\r\nI see the solution where I return first derivative (just as it is returned now), but compute it using `-tf.reduce_sum(y * tf.log(tf.nn.softmax(logits)))`. And if user needs second derivative, it could get it through `tf.gradients` and there will be no error.", "```python\r\n@ops.RegisterGradient(\"SoftmaxCrossEntropyWithLogits\")\r\ndef _SoftmaxCrossEntropyWithLogitsGrad(op, grad_0, _):\r\n  ### OLD VERSION ###\r\n  # softmax_grad_without_gradient = array_ops.prevent_gradient(op.outputs[1])\r\n\r\n  ### NEW VERSION ###\r\n  logits, labels = op.inputs\r\n  loss = -tf.reduce_sum(labels * tf.log(tf.nn.softmax(logits)))\r\n  grad_loss_wrt_logits = tf.gradients(loss, logits)\r\n\r\n  ### OLD VERSION ###\r\n  # return _BroadcastMul(grad_0, softmax_grad_without_gradient), None\r\n\r\n  ### NEW VERSION ###\r\n  return _BroadcastMul(grad_0, grad_loss_wrt_logits), None\r\n```\r\n\r\n\r\nI am talking about something like this. I cant return second derivative from this function, I only can provide a way to compute it properly from outside the function.", "@girving sry, in your example if `f` is just function from R to R^2. That's ok, the derivative is correct. But in softmax case, second output is a hack, as I suppose, for better efficiency (there is a fused implementation in C++ core). Why can we simply consider it as an operator from some space K to K^2?", "@persiyanov How does it being a hack change the fact that it's a well defined mathematical function?", "@girving \r\nMathematically, softmax_xentropy returns one output, not two. If we inject gradient as second output and simply compute gradients as always, they will not be correct, will they?", "@persiyanov No, the C++ op returns two inputs, not one.  Note the `unused_backprop` output here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_ops.py#L1618.", "@girving We talk about mathemtical correctness, I know that C++ op returns two outputs=)\r\n\r\nYou say that in `_SoftmaxCrossEntropyWithLogitsGrad` I need to return something like this:\r\n\r\n`_BroadcastMul(grad_0, grad_loss_wrt_logits) + _BroadcastMul(grad_1, grad_grad_wrt_logits)`?\r\n\r\nI think it's definitely incorrect...", "@persiyanov Unfortunately to me it looks obviously correct.  Clearly one of us is wrong!  It may well be me, but why would have to be explained to me.  For starters, you could explain how the xent case is different from my simple `f` example.", "@girving I suppose that I am wrong but I did not realize it yet:)\r\n\r\nI thought that `_SoftmaxCrossEntropyWithLogitsGrad` must return gradient of real (mathematical) xentropy, which returns one output. In this case, current implementation looks to me obvious. But I can't understand why these two variants of returned values are equivalent:\r\n\r\nOld version: `_BroadcastMul(grad_0, grad_loss_wrt_logits)`\r\n\r\nNew version: `_BroadcastMul(grad_0, grad_loss_wrt_logits) + _BroadcastMul(grad_1, grad_grad_wrt_logits)`\r\n\r\nIf I call \r\n> `tf.gradients(_BroadcastMul(grad_0, grad_loss_wrt_logits) + _BroadcastMul(grad_1, grad_grad_wrt_logits), logits)`\r\n\r\nand\r\n\r\n> `tf.gradients(_BroadcastMul(grad_0, grad_loss_wrt_logits), logits)`,\r\n\r\nI will get different values, unless `grad_0 == 0` when I request second derivative AND `grad_1 == 0` when I request first derivative.", "@persiyanov I think to understand this you need to understand better how the tensorflow gradients code works.\r\n\r\nRight now when you request a gradient for a loss wrt to a variable tf will walk back the graph calling the gradient functions for all ops. The gradient of an op is a function of the gradient of its outputs (by the chain rule). For an op with multiple outputs the gradient will depend on both of them, additively. If one output was not used in the model, TF will put a zero tensor in there, so we can compute the value of the gradient function correctly.\r\n\r\nSo if you have a model which only uses the normal output of the softmax cross entropy and you try to take the gradient, because the gradient of the softmax cross entropy was not used in the graph TF will put a zero in there. When you try to take just the second derivative, similarly a zero will be provided as the downstream gradient of the first output, and the function is again mathematically correct.", "@alextp Yes! Only with these assumptions everything will be okay. \r\n\r\nSo when `g = tf.gradients(loss, [logits])[0]` is called, `_SoftmaxCrossEntropyWithLogitsGrad` is immediately called, because `loss` is an output of xentropy op. And `grad_0==1` and `grad_1==0` in this case.\r\n\r\nBut when I call `h = tf.gradients(g, [logits])[0]`, what really happens in TF? I suppose that `_SoftmaxCrossEntropyWithLogitsGrad` will be called with `grad_0==0` and `grad_1==1`, but not immediately. So when? Or TF will recognize that `g` is actually second output from softmax_xentropy op, and will call immediately its gradient function?", "Yes, tf will recognize that g is the second output from softmax_cross_entropy and do the right thing.\r\n\r\nThought note that tf only takes gradients WRT scalars, not WRT arbitrary tensors so this won't give you the full hessian matrix.", "@alextp @girving Thank you guys, I finally got it.\r\n\r\nSo what optimization do you propose?\r\n\r\nThe draft of solution can look like this:\r\n\r\n```python\r\n@ops.RegisterGradient(\"SoftmaxCrossEntropyWithLogits\")\r\ndef _SoftmaxCrossEntropyWithLogitsGrad(op, grad_0, grad_1):\r\n  \"\"\"Gradient function for SoftmaxCrossEntropyWithLogits.\"\"\"\r\n  # grad_0 is the backprop for cost, and we multiply it with the gradients\r\n  # (which is output[1])\r\n  # There is no gradient for the labels\r\n  softmax_grad = op.outputs[1]\r\n   \r\n  ## HERE WE COMPUTE SECOND DERIVATIVE\r\n  cost = op.outputs[1]\r\n  logits = op.inputs[0]\r\n  second_deriv = tf.gradients(cost, tf.gradients(cost, logits)[0])[0]\r\n  return _BroadcastMul(grad_0, softmax_grad) + _BroadcastMul(grad_1, second_deriv), None\r\n```\r\n\r\nbut \r\n(1) I think there is a trouble with using `tf.gradients` directly.\r\n(2) I will be slower as @girving pointed.\r\n\r\n", "@persiyanov Your solution will actually be infinitely slow, since instead of defining the second derivative you're just calling the same function again and hoping that someone else will have defined the second derivative for you in the meantime.  I.e., it's an infinite loop.", "@girving I meant smth like this:\r\n\r\n```python\r\n@ops.RegisterGradient(\"SoftmaxCrossEntropyWithLogits\")\r\ndef _SoftmaxCrossEntropyWithLogitsGrad(op, grad_0, grad_1):\r\n  \"\"\"Gradient function for SoftmaxCrossEntropyWithLogits.\"\"\"\r\n  # grad_0 is the backprop for cost, and we multiply it with the gradients\r\n  # (which is output[1])\r\n  # There is no gradient for the labels\r\n  softmax_grad = op.outputs[1]\r\n   \r\n  ## HERE WE COMPUTE SECOND DERIVATIVE\r\n  logits, labels = op.inputs\r\n  cost = -tf.reduce_sum(labels * tf.log(tf.nn.softmax(logits)))\r\n  second_deriv = tf.gradients(cost, tf.gradients(cost, logits)[0])[0]\r\n\r\n  return _BroadcastMul(grad_0, softmax_grad) + _BroadcastMul(grad_1, second_deriv), None\r\n```", "@persiyanov You are basically doing something like this:\r\n\r\n    def f(x):\r\n      return f(x)\r\n\r\nIt won't work.  If it's not clear why, I would suggest reading through the code more or implementing it and looking at the resulting periodic stack overflow.", "@girving That's because I am calling `tf.gradients`? If so, I understand this. I can compute second derivative without calling `tf.gradients` and other grad functions if it is okay.", "@girving Hi, I've implemented first version, using the fact that second derivative of xentropy is actually gradient of softmax w.r.t logits.\r\n\r\nThis code works with minimal example which was introduced above.\r\n\r\n```python\r\n@ops.RegisterGradient(\"SoftmaxCrossEntropyWithLogits\")\r\ndef _SoftmaxCrossEntropyWithLogitsGrad(op, grad_0, grad_1):\r\n  \"\"\"Gradient function for SoftmaxCrossEntropyWithLogits.\"\"\"\r\n  # grad_0 is the backprop for cost, and we multiply it with the gradients\r\n  # (which is output[1])\r\n  # There is no gradient for the labels\r\n  #\r\n  # Second derivative is just softmax derivative w.r.t. logits.\r\n  softmax_grad = op.outputs[1]\r\n\r\n  logits = op.inputs[0]\r\n\r\n  softmax = nn_ops.softmax(logits)\r\n\r\n  second_deriv = ((grad_1 - array_ops.reshape(\r\n      math_ops.reduce_sum(grad_1 * softmax, [1]), [-1, 1])) * softmax)\r\n\r\n  return _BroadcastMul(grad_0, softmax_grad) + second_deriv, None\r\n```\r\n\r\n\r\nCan we now proceed to optimization?\r\n\r\nAlso, though it is a rare situation but I think we need to check if (grad_0 != 0 and grad_1 != 0) then we need to raise some warning or exception, because the returned value will not be correct.", "Cool!  Yep, assuming that's correct, onwards to optimization.  The two important optimizations are\r\n\r\n1. Use `matmul` instead of multiplication followed by `reduce_sum`.  Some reshapes may be required.  Side note: you can also use `tensor[:, None]` to insert a unit dimension in a shape.\r\n2. Don't add `second_deriv` if `grad_1.op.type` is `Zeros` or `ZerosLike` (I believe those are the right 2).  You should have a unit test that verifies the extra computation is skipped if a second derivative is not requested.  Note that this op check type is a graph construction time thing; don't do it based on values at runtime.", "@girving Hi! Is something like that looks okay?\r\n\r\n```python\r\n@ops.RegisterGradient(\"SoftmaxCrossEntropyWithLogits\")\r\ndef _SoftmaxCrossEntropyWithLogitsGrad(op, grad_0, grad_1):\r\n  \"\"\"Gradient function for SoftmaxCrossEntropyWithLogits.\"\"\"\r\n  # grad_0 is the backprop for cost, and we multiply it with the gradients\r\n  # (which is output[1])\r\n  # grad_1 is the backprop for softmax gradient.\r\n  # There is no gradient for the labels\r\n  #\r\n  # Second derivative is just softmax derivative w.r.t. logits.\r\n  softmax_grad = op.outputs[1]\r\n  first_deriv = _BroadcastMul(grad_0, softmax_grad)\r\n\r\n  if grad_1.op.type == 'ZerosLike':  # or `in ('Zeros', 'ZerosLike')`\r\n      return first_deriv, None\r\n\r\n  logits = op.inputs[0]\r\n  softmax = nn_ops.softmax(logits)\r\n\r\n  second_deriv = ((grad_1 - gen_array_ops.diag_part(\r\n      math_ops.matmul(grad_1, softmax, transpose_b=True)\r\n  )[:, None]) * softmax)\r\n\r\n  return first_deriv + second_deriv, None\r\n```\r\n\r\n\r\n**Question** about unit test: How could I check the graph nodes after its construction? If second derivative is requested, then `_SoftmaxCrossEntropyWithLogitsGrad` will return tensor with 'add' at the end of the name (e.g., `Tensor(\"hessians_second_derivative_2/gradients_1/SoftmaxCrossEntropyWithLogits_grad/add:0\", shape=(1, 2), dtype=float32)`), and otherwise not. But if look at the result of `tf.hessians`, it will look like `Tensor(\"hessians_second_derivative_2/hessians:0\", shape=(2, 2), dtype=float32)`.", "@persiyanov That looks correct, but I'd write it as\r\n\r\n    grad = ...  # First derivative\r\n    if grad_1.op.type not in ('Zeros', 'ZerosLike'):\r\n      grad += ... # Second derivative\r\n    return grad, None\r\n\r\nYou should check for both `Zeros` and `ZerosLike`.  Also, the `diag_part` code is worse than your previous version.  The point of switching to `matmul` is that if you do it correctly it will do the reduction for you; if you make a larger tensor partway through it's bad.  Please change the shapes going into the `matmul` so that `diag_part` becomes unnecessary.\r\n\r\nFor the unit test, I think it suffices to check that\r\n\r\n1. There's a matmul somewhere in the graph if you use the second derivative.\r\n2. There isn't a matmul if you don't.\r\n\r\nAs long as the rest of the graph doesn't include a `matmul` you should be set, although the test will need a comment explaining what's going on."]}, {"number": 7402, "title": "Unable to build pip package from sources. Error: invalid command 'bdist_wheel' in pip_package/build_pip_package", "body": "Hi, trying to build tensorflow for ppce64le got stuck with the issue. No CUDA (yet), trying to build binaries for CPU\r\n\r\n```\r\n$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\nFri Feb 10 01:47:35 CET 2017 : === Using tmpdir: /tmp/tmp.6Ayjop9NY1\r\n~/dmitry/tf/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles ~/dmitry/tf/tensorflow\r\n~/dmitry/tf/tensorflow\r\n/tmp/tmp.6Ayjop9NY1 ~/dmitry/tf/tensorflow\r\nFri Feb 10 01:47:36 CET 2017 : === Building wheel\r\nusage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\r\n   or: setup.py --help [cmd1 cmd2 ...]\r\n   or: setup.py --help-commands\r\n   or: setup.py cmd --help\r\n\r\nerror: invalid command 'bdist_wheel'\r\n```\r\n\r\n```\r\n$ uname -a\r\nLinux hostname.domainname 3.10.0-514.2.2.el7.ppc64le #1 SMP Wed Dec 7 17:03:53 GMT 2016 ppc64le ppc64le ppc64le GNU/Linux\r\n\r\n$ git rev-parse HEAD\r\n2180bb97de1fedfe249523bb8fcc2144d97fd00e\r\n\r\n$ bazel version\r\nBuild label: 0.4.4- (@non-git)\r\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Feb 9 23:48:24 2017 (1486684104)\r\nBuild timestamp: 1486684104\r\nBuild timestamp as int: 1486684104\r\n\r\n```\r\n", "comments": ["For the one who will struggle, need to install lates wheel package manually first.\r\n```\r\n# pip3.4 install wheel\r\n```\r\nafter that it will be able to build:\r\n\r\n```\r\n$ ls /tmp/tensorflow_pkg/\r\ntensorflow-1.0.0rc2-cp34-cp34m-linux_ppc64le.whl\r\n```", "@dchirikov Glad you found the fix.  Closing since it's fixed by the latest `wheel`."]}, {"number": 7401, "title": "Branch 147093777", "body": "", "comments": []}, {"number": 7400, "title": "Better Variable.__repr__", "body": "Closes #6539 ", "comments": ["Can one of the admins verify this patch?", "Looks good to me, thanks!\r\n\r\n@tensorflow-jenkins test this please."]}, {"number": 7399, "title": "Tensorflow GPU on Windows Python 3.5 CUDA_ERROR_OUT_OF_MEMORY", "body": "I am working on\r\n\r\n    Notebook Lenova Z50-70\r\n    GPU:Nvidia Geforce 840M\r\n    Python 3.5\r\n    Windows 7\r\n    pip install tensorflow-gpu\r\n\r\nI open windows and try to test how tensorflow use GPU.\r\n\r\n    import tensorflow as tf\r\n>>> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA libr\r\nary cublas64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA libr\r\nary cudnn64_5.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA libr\r\nary cufft64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA libr\r\nary nvcuda.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA libr\r\nary curand64_80.dll locally\r\n\r\n    from __future__ import print_function\r\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\r\n    sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\r\n\r\n>>>I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:885] Found device 0 with p\r\nroperties:\r\nname: GeForce 840M\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.124\r\npciBusID 0000:03:00.0\r\nTotal memory: 4.00GiB\r\nFree memory: 3.93GiB\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:906] DMA: 0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:916] 0: Y\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:975] Creating TensorFlow d\r\nevice (/gpu:0) -> (device: 0, name: GeForce 840M, pci bus id: 0000:03:00.0)\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:1002] failed to allocate 1.3\r\n3G (1430223872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:1002] failed to allocate 1.2\r\n0G (1287201536 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:1002] failed to allocate 1.0\r\n8G (1158481408 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:1002] failed to allocate 994\r\n.33M (1042633216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:1002] failed to allocate 894\r\n.90M (938370048 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:1002] failed to allocate 805\r\n.41M (844532992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:1002] failed to allocate 724\r\n.87M (760079872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:1002] failed to allocate 652\r\n.38M (684071936 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:1002] failed to allocate 587\r\n.14M (615664896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:1002] failed to allocate 528\r\n.43M (554098432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:1002] failed to allocate 475\r\n.59M (498688768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n>>> hello = tf.constant('Hello, TensorFlow!')\r\n>>> print(sess.run(hello))\r\n\r\nAnd got window error that it's not enought memory and OS closed python.\r\nYes also Gchrome was open and use a lot of system memory.\r\nMy question.\r\nWhy such error appear, why when i close GChrome system starts to works better, as i understand tensorflow gpu, use gpu memory and my Nvidia Geforce 840M has Dedicated Memory. I also have integrated video IntelHD.\r\nWhy tensorflow start to use system resources not gpu resources? and how to fix it? i don't have error in theano framework\r\n", "comments": ["Presumably another process is using more than 0.66 of your GPU memory.  Do you have evidence that this isn't the case?\r\n\r\n@mrry: Have you unexpected out-of-GPU-memory issues on Windows?", "@girving Not that I'm aware of. I don't think this issue is Windows specific.", "I think Gchrome not use GPU memory, because when i closed GChrome, and start use Firefox for Jupiter, everything was ok.\r\nAlso i check info, that my tensorflow use GPU dedicated memory and it's ok, but not use GPU processor, and use CPU processor, is it normal?", "I think this is better as a StackOverflow question, since it's not at all clear it's a TensorFlow bug.", "@vladimircape you could use nvidia-smi to check where is your GPU memory going for.\r\nhttps://developer.nvidia.com/nvidia-system-management-interface"]}, {"number": 7398, "title": "Cuda 8.0 .deb file installation \"E: Unable to locate package cuda\"", "body": "I am following the instructions from https://developer.nvidia.com/cuda-downloads for the Ubuntu 16.04 setup but the `cuda` package is not available after\r\n\r\n```\r\nsudo dpkg -i cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.deb\r\nsudo apt-get update\r\nsudo apt-get install cuda\r\n```\r\n\r\nHere is my output:\r\n\r\n```\r\nstefan@stefan-pc:~/Downloads$ sudo dpkg -i cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.deb\r\n(Reading database ... 272242 files and directories currently installed.)\r\nPreparing to unpack cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.deb ...\r\nUnpacking cuda-repo-ubuntu1604-8-0-local-ga2 (8.0.61-1) over (8.0.61-1) ...\r\nSetting up cuda-repo-ubuntu1604-8-0-local-ga2 (8.0.61-1) ...\r\nOK\r\nstefan@stefan-pc:~/Downloads$ sudo apt-get update\r\nIgn:1 http://dl.google.com/linux/chrome/deb stable InRelease\r\nHit:2 http://dl.google.com/linux/chrome/deb stable Release                 \r\nHit:3 https://deb.nodesource.com/node_6.x xenial InRelease                 \r\nGet:4 http://security.ubuntu.com/ubuntu xenial-security InRelease [102 kB]\r\nHit:6 http://at.archive.ubuntu.com/ubuntu xenial InRelease                          \r\nHit:7 http://at.archive.ubuntu.com/ubuntu xenial-updates InRelease                  \r\nHit:8 http://at.archive.ubuntu.com/ubuntu xenial-backports InRelease               \r\nFetched 102 kB in 0s (115 kB/s)                                                    \r\nReading package lists... Done\r\nstefan@stefan-pc:~/Downloads$ sudo apt-get install cuda\r\nReading package lists... Done\r\nBuilding dependency tree       \r\nReading state information... Done\r\nE: Unable to locate package cuda\r\n```", "comments": []}, {"number": 7396, "title": "Branch 147051664", "body": "", "comments": ["Makefile issue is a transient failure.\r\npython3 test is a known issue which is rolled back internally."]}, {"number": 7395, "title": "Remove leading slash from data directory", "body": "To allow embedding of TensorBoard, the data directories need to be relative.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "@geromueller  are you able to sign the CLA?", "I consent to contribute to this project with my commits", "Can one of the admins verify this patch?", "I think you have to sign the CLA using \"post@geromueller.de\" and then comment here, since that's the github email you used.", "I already made this change on the src file (router.ts), so it will already be fixed when we make the next update of the generated tf-tensorboard.html (should be this week)."]}, {"number": 7394, "title": "Segfault when calling TF_OperationGetAttrTensor on malformed tensor", "body": "On 1.0.0-rc1 (and earlier versions), this segfaults:\r\n\r\n```c\r\nvoid dealloc(void* data, size_t len, void* arg) {\r\n}\r\n\r\nint main() {\r\n  TF_Tensor* empty = TF_NewTensor(TF_FLOAT, NULL, 0, NULL, 0, dealloc, NULL);\r\n  TF_Graph* graph = TF_NewGraph();\r\n  TF_OperationDescription* desc = TF_NewOperation(graph, \"Const\", \"empty\");\r\n  TF_Status* status = TF_NewStatus();\r\n  TF_SetAttrTensor(desc, \"value\", empty, status);\r\n  TF_SetAttrType(desc, \"dtype\", TF_FLOAT);\r\n  TF_Operation* op = TF_FinishOperation(desc, status);\r\n  TF_Tensor* value;\r\n  TF_OperationGetAttrTensor(op, \"value\", &value, status); //Segfaults\r\n  return 0;\r\n}\r\n```\r\n\r\nNote that `TF_Message(status)` is `TF_OK` after the calls to `TF_SetAttrTensor` and `TF_FinishOperation`. ", "comments": ["We should probably error our more safely, but could you elabore on what you mean by an \"empty\" tensor? A tensor with `ndims=0` is intended for representing a scalar, so a `TF_FLOAT` tensor with `ndims=0` should be backed by 4 bytes of storage. So, the `empty` `TF_Tensor` in the snippet above seems malformed.\r\n\r\nPerhaps `TF_NewTensor` should return `nullptr` in that case and we should update the documentation for it. Does that make sense? Or is there something about an empty tensor that I'm missing?\r\n\r\n", "Yes thanks, that makes sense. It just seems problematic that a malformed tensor should cause `TF_OperationGetAttrTensor`to crash instead of safely error-ing out. ", "@asimshankar do we plan to add a check or this is expected?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "ping @asimshankar is this resolved?\r\nShould we close the issue?", "Ah, I hadn't gotten to this, but it seems like a simple fix. I'll try to get this out soon."]}, {"number": 7393, "title": "Have Docker Build not Install via Pip", "body": "In the [Development Dockerfile](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel), I see this note,\r\n\r\n```\r\n# TODO(craigcitro): Don't install the pip package, since it makes it\r\n# more difficult to experiment with local changes. Instead, just add\r\n# the built directory to the path.\r\n```\r\n\r\nDue to the [lack of important header files](https://github.com/tensorflow/tensorflow/issues/1419) in mainstream Tensorflow, I need to patch the TF build so I can get those headers. Unfortunately, the current Dockerfile clears the Bazel cache. I am requesting that the above TODO, be resolved soon, so I can patch Tensorflow's Bazel BUILD files in order to make local changes and build quicker. Rebuilding TF without a cache in the container takes a pretty long time.", "comments": ["This is the patch I do, in case it is important\r\n```\r\ndiff --git a/tensorflow/tools/pip_package/BUILD b/tensorflow/tools/pip_package/BUILD\r\nindex 6a3f66b..c88840a 100644\r\n--- a/tensorflow/tools/pip_package/BUILD\r\n+++ b/tensorflow/tools/pip_package/BUILD\r\n@@ -26,6 +26,8 @@ transitive_hdrs(\r\n         \"//tensorflow/core:core_cpu\",\r\n         \"//tensorflow/core:framework\",\r\n         \"//tensorflow/core:lib\",\r\n+        \"//tensorflow/core:proto_text\",\r\n+        \"//tensorflow/core:all_kernels\",\r\n         \"//tensorflow/core:protos_all_cc\",\r\n         \"//tensorflow/core:stream_executor\",\r\n         \"//third_party/eigen3\",\r\n```", "@caisq Can you comment on this Docker request?", "I think we started to recently run bazel clean, as bazel cache can be in the order of Giga bytes.\r\nThis was a move to limit the size of docker containers for people with bandwidth concerns.\r\nIn the US this is not a very big concern, but this can be very important in other countries where ISPs have strict data caps.\r\n\r\nIn any changes you make, you should only need to build once.\r\nYou can reuse the container you previously built in by running `docker restart <containername>`, and then `docker  attach <containername>`, so you do not have to lose any of the build files you accumulated. So you should not need to build more than once.\r\n\r\nAnother option you have, you can always pull our docker image, then start a container with it, create the bazel cache and create your own image with the bazel cache intact on top of our image.\r\nYou simply need to start another terminal, check the name of the container running using `docker ps`, then use `docker commit` command to create your own image: https://docs.docker.com/engine/reference/commandline/commit/\r\n\r\nPlease let me know if the above explanation and workarounds make sense.\r\n\r\n", "Yeah, as @gunan pointed out, our devel docker images (especially the GPU one) used to be so large that they caused download and disk space issues to users, so we removed the bazel cache, which reduced the image size by about 50%.", "I think @gunan's solution should work for @aidan-plenert-macdonald . Another alternative is to simply run `docker run` without the `--rm` flag and then restart the container with `docker start -i` later. So I'll close the issue for now. Feel free to comment or reopen the issue in case you have more questions."]}, {"number": 7392, "title": "docs: Broken link/missing py file in Model Files guide", "body": "On the page [\"A Tool Developer's Guide to TensorFlow Model Files\"](https://www.tensorflow.org/how_tos/tool_developers/)\r\n([GitHub md link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/tool_developers/index.md))\r\n\r\nIn the GraphDef section, there's a link to `graph_metrics.py`, pointing to the url https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/graph_metrics.py\r\n\r\nThis url does not exist. Perhaps it's intended to point at a different file in that folder? I don't see an obvious suitable replacement from scanning the filenames listed: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/tools\r\n\r\n\r\n", "comments": ["@petewarden This one of your doc files?", "graph_metrics.py was removed, so we should update the docs."]}, {"number": 7391, "title": "Change `dim` to `axis` for neural network classification ops to match others for the TF 1.0.0 API?", "body": "It looks like the neural network classification ops still use `dim` whereas the rest of the API is being moved to `axis` in v1.0.0. Is this purposeful, or was this bit just missed?", "comments": ["<img width=\"1284\" alt=\"screen shot 2017-02-09 at 10 56 02 am\" src=\"https://cloud.githubusercontent.com/assets/5296416/22794074/70921e78-eeb7-11e6-914c-61977aee574b.png\">\r\n", "@aselle Looks like we missed one.  I imagine this is too late for 1.0, but maybe we could add the *axis* parameter as an alternative name?", "Yes, for most of the axis changes (except reverse and concat) we actually just kept the old name at the end of the argument list so both work. We can do the same here.", "Yay! Thank you!", "Added a PR #12716 for the fix."]}, {"number": 7390, "title": "Expose param_regularizers in python batch_norm layer", "body": "Closes #7367", "comments": ["Can one of the admins verify this patch?", "@sguada tests added", "ping?", "@tensorflow-jenkins Test this please", "Thanks cancan!"]}, {"number": 7389, "title": "Add unsorted segment ops for prod, min, mean, sqrt_n", "body": "As requested by @girving in #7362 I'll file a separate issue.\r\n\r\nThe sorted segment reduction ops feature  prod, min, mean, sqrt_n while the unsorted ops don't include those. So I suggest to add the following functions:\r\n```\r\ntf.unsorted_segment_prod(data, segment_ids, num_segments, name=None)\r\ntf.unsorted_segment_min(data, segment_ids, num_segments, name=None)\r\ntf.unsorted_segment_mean(data, segment_ids, num_segments, name=None)\r\ntf.unsorted_segment_sqrt_n(data, segment_ids, num_segments, name=None\r\n```\r\n\r\nIf you mark this as contributions welcome, I'd start working on this.", "comments": ["Thank you for splitting this out!", "I'd recommend spelling it `num_segments`, though.", "Sorry, of course! Stupid typo, I'll change the issue description.", "I am implementing batch k-means. In the update phase, new centers have to be computed according to the points re-assignments. It would make thing easier if there is unsorted_segment_mean.", "Hi,\r\nsorry, I've been rather busy in the last few weeks. I'm 95% done implementing the CPU Kernels, with a bug remaining. Thanks for reminding me, I'll have a look at it tomorrow!\r\nCheers,\r\nPhil", "For now you could just compute the mean with existing tensorflow functions, e.g.\r\n\r\n```\r\nimport tensorflow as tf\r\nx = tf.constant([1.0, 2.0, 3.0, 4.0])\r\nids = tf.constant([0, 1, 0, 1])\r\nones = tf.ones_like(x)\r\ncount = tf.unsorted_segment_sum(ones, ids, 2)\r\nsums = tf.unsorted_segment_sum(x, ids, 2)\r\nmean = tf.divide(sums, count)\r\ns = tf.Session()\r\ns.run(mean)\r\n\r\n```\r\n\r\nThe main advantage is that this runs on GPU.", "This is great! Thanks a lot!", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 7388, "title": "error with bazel build in windows", "body": "I'm trying to build Tensorflow android camera demo in Windows.\r\nWhen I try to build the application with \r\n\r\n`bazel build -c opt //tensorflow/examples/android:tensorflow_demo`\r\n\r\nI have these errors:\r\n\r\n```\r\nERROR: missing input file '@androidsdk//:build-tools/25.0.1/aapt'.\r\nERROR: C:/Users/DAVIDE/AppData/Local/Temp/_bazel_davide.biraghi/55PhBU2g/external/androidsdk/BUILD:5:1: Executing genrule @androidsdk//:zipalign_runner failed: bazel failed: error executing command C:/ProgramData/chocolatey/lib/bazel -c ... (remaining 1 argument(s) skipped): java.io.IOException: CreateProcess(): Access is denied.\r\n.\r\nERROR: C:/Users/DAVIDE/AppData/Local/Temp/_bazel_davidee/55PhBU2g/external/androidsdk/BUILD:5:1: @androidsdk//:aapt_binary: missing input file '@androidsdk//:build-tools/25.0.1/aapt'.\r\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: C:/Users/DAVIDE/AppData/Local/Temp/_bazel_davide/55PhBU2g/external/androidsdk/BUILD:5:1 1 input file(s) do not exist.\r\n```\r\n\r\nI have android sdk installed with the 25.0.1 build-tools version and the aapt file. I have also modified my WORKSPACE with the correct path (and version. I tried also with version 24.0.1 with same error).  I'm working on Windows 10. Any suggestion? ", "comments": ["@andrewharp Can you comment?  Haven't seen many Android + Windows bugs yet.", "Unfortunately Bazel doesn't support Android builds on Windows yet -- that seems to be [planned for 0.8](https://bazel.build/roadmap.html)\r\n\r\nIn the meantime I'd suggest using the [prebuilt libraries](https://ci.tensorflow.org/view/Nightly/job/nightly-android/) or waiting for the upcoming cmake demo build support."]}, {"number": 7387, "title": "Update shape checking logic in einsum", "body": "In #6830, I modified einsum function to check whether new shape has two or more unknown dimensions.\r\nHowever, according to the comment on #6824, it is proven to be not sufficient to check only the last reshape call.\r\nThis PR fixes the issue.", "comments": ["Can one of the admins verify this patch?", "I updated the codes as you suggested, but there are some differences to keep its coverage on shape checks before evaluated.\r\nSpecifically, I modified the codes in the below ways:\r\n1) Instead of calling `get_shape()`, I added `_get_shape()` helper function that returns a result of `get_shape().to_list()` if the result does not contain `None` values, and else the helper function explicitly queries the dimension value using `array_ops.shape` function.\r\n2) `_total_size` helper function is not deleted but modified a bit. Previously it returned `-1` if the shape contains `None` dimensions. I modified the function not to return `-1` even when some of dimensions are unknown.\r\nIf the shape is fully known, it returns an integer, and if the shape contains unknown dimensions, it returns a scalar tensor.\r\nThe reason I didn't obtain the total size by using just `math_ops.reduce_prod` is to keep its capability to check the shape before actually evaluated if available. (See `test_dim_mismatch` test.)", "Thanks for fixing this!", "@tensorflow-jenkins test this please"]}]