[{"number": 25916, "title": "Update bigtable_lib.cc", "body": "grpc_code was assigned and not used, probably a small bug.", "comments": []}, {"number": 25915, "title": "Update split_handler_ops.cc", "body": "Removed unused variables best_dimension_idx & dimension_id from split_handler_ops.cc", "comments": []}, {"number": 25914, "title": "TF Lite kernels package test build error fix", "body": "This PR solve the several test compilation and warning problem while runing kernels package.", "comments": ["@nutsiepully  \r\n\r\nPlease review the changes, thanks.", "Nagging Reviewer @nutsiepully: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 36 days with no activity and the `awaiting review` label has been applied."]}, {"number": 25913, "title": "glog macros are redefined by platform/logging.h, included by refcount.h", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): `42c4f4ab6b53bce8639c203d7839d27eac11bd2f`\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source): 5.4.0-6ubuntu1~16.04.10\r\n- CUDA/cuDNN version: 9.0 / 7\r\n- GPU model and memory: 8x GTX 1080 Ti 12GB\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\n```\r\nIn file included from /tmp/tensorflow/execroot/org_tensorflow/bazel-out/k8-opt/genfiles/tensorflow/include/tensorflow/core/platform/logging.h:25:0,\r\n                 from /tmp/tensorflow/execroot/org_tensorflow/bazel-out/k8-opt/genfiles/tensorflow/include/tensorflow/core/lib/core/refcount.h:22,\r\n                 from /tmp/tensorflow/execroot/org_tensorflow/bazel-out/k8-opt/genfiles/tensorflow/include/tensorflow/core/platform/tensor_coding.h:21,\r\n                 from /tmp/tensorflow/execroot/org_tensorflow/bazel-out/k8-opt/genfiles/tensorflow/include/tensorflow/core/framework/resource_handle.h:19,\r\n                 from /tmp/tensorflow/execroot/org_tensorflow/bazel-out/k8-opt/genfiles/tensorflow/include/tensorflow/core/framework/allocator.h:24,\r\n                 from /tmp/tensorflow/execroot/org_tensorflow/bazel-out/k8-opt/genfiles/tensorflow/include/tensorflow/core/framework/tensor.h:22,\r\n                 from /tmp/tensorflow/execroot/org_tensorflow/bazel-out/k8-opt/genfiles/tensorflow/include/tensorflow/core/public/session.h:24,\r\n                 from bug.cc:2:\r\n/tmp/tensorflow/execroot/org_tensorflow/bazel-out/k8-opt/genfiles/tensorflow/include/tensorflow/core/platform/default/logging.h:95:0: warning: \"LOG\" redefined\r\n #define LOG(severity) _TF_LOG_##severity\r\n ^\r\nIn file included from bug.cc:1:0:\r\n/usr/local/include/glog/logging.h:506:0: note: this is the location of the previous definition\r\n #define LOG(severity) COMPACT_GOOGLE_LOG_ ## severity.stream()\r\n ^\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nNo leaking `platform/logging.h` and no those warnings about redefining `glog` macros.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```c++\r\n#include <glog/logging.h>\r\n#include \"tensorflow/core/public/session.h\"\r\n\r\nint main() {\r\n    tensorflow::SessionOptions gpu_option;\r\n}\r\n```\r\n\r\n```bash\r\nexport TF_INC_DIR=/tmp/tensorflow/execroot/org_tensorflow/bazel-out/k8-opt/genfiles/tensorflow/include\r\ng++ bug.cc -std=c++11 -I$TF_INC_DIR -I$TF_INC_DIR/external/com_google_absl\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\nI know TensorFlow has tried to avoid leaking the logging macros (#7480), but [`core/lib/core/refcount.h`](https://github.com/tensorflow/tensorflow/blob/42c4f4ab6b53bce8639c203d7839d27eac11bd2f/tensorflow/core/lib/core/refcount.h) is leaking them again. It seems that one way to fix it is to remove `#include \"tensorflow/core/platform/logging.h\"` from the headers and add them back in the `.cc` files. However, as said in `core/lib/core/refcount.h`, those are \"Inlined routines, since these are performance critical\" thus cannot be moved to a `.cc` file. But I'm sure there must be a way to stop leaking the logging macros.\r\n\r\n", "comments": ["duplicate of #7480", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25913\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25913\">No</a>\n"]}, {"number": 25912, "title": "Freezing a model with Exponential Moving Average gives different probabilities", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.9.0\r\n- Python version: 2.7.12\r\n- Bazel version (if compiling from source): 0.15.0\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version: 9.0.176\r\n- GPU model and memory:\r\n\r\nI'm trying to freeze a model using Exponential Moving Average (EMA) variables for training and inference.\r\n\r\nThe freezing and inference code work normally, but the inferred probabilities are slightly different (about 0.05) with the ones from the original code. These results become identical (differnce < 1e-6) if I turn off the EMA in both models.\r\n\r\nAm I doing the freezing incorrectly, or the freeze_graph tool is just unable to treat EMA?\r\n\r\n\r\nCode I am using for freezing graph:\r\n\r\n    from __future__ import print_function\r\n    import tensorflow as tf\r\n    from nets.inception_v3 import inception_v3, inception_v3_arg_scope\r\n    from tensorflow.python.framework import graph_util\r\n    import sys \r\n    slim = tf.contrib.slim\r\n\r\n    checkpoint_file = '/my/model'\r\n\r\n    with tf.Graph().as_default() as graph:\r\n\r\n        images = tf.placeholder(shape=[None, 100, 221, 6], dtype=tf.float32, name = 'input')\r\n\r\n        with slim.arg_scope(inception_v3_arg_scope()):\r\n            logits, end_points = inception_v3(images, num_classes = 3, create_aux_logits = False, is_training = False)\r\n\r\n        variables_to_restore = slim.get_variables_to_restore()\r\n\r\n        MOVING_AVERAGE_DECAY = 0.9999\r\n        variable_averages = tf.train.ExponentialMovingAverage(\r\n            MOVING_AVERAGE_DECAY)\r\n        for var in variables_to_restore:\r\n            tf.add_to_collection(tf.GraphKeys.MOVING_AVERAGE_VARIABLES, var)\r\n        variables_to_restore = variable_averages.variables_to_restore()        #This line is commented if EMA is turned off\r\n\r\n        saver = tf.train.Saver(variables_to_restore)\r\n\r\n        #Setup graph def\r\n        input_graph_def = graph.as_graph_def()\r\n        output_node_names = \"InceptionV3/Predictions/Reshape_1\"\r\n        output_graph_name = \"./frozen_inception_v3_new_100_221_ema.pb\"\r\n\r\n        with tf.Session() as sess:\r\n            saver.restore(sess, checkpoint_file)\r\n\r\n            #Exporting the graph\r\n            print (\"Exporting graph...\")\r\n            output_graph_def = graph_util.convert_variables_to_constants(\r\n                    sess,\r\n                    input_graph_def,\r\n                    output_node_names.split(\",\"))\r\n\r\n            with tf.gfile.GFile(output_graph_name, \"wb\") as f:\r\n                f.write(output_graph_def.SerializeToString())\r\n\r\nThis code is modified from https://gist.github.com/kwotsin/8e43f5db4815e1f1af37da70d0933d8b . The EMA part is the same as the original code.\r\n\r\n", "comments": ["The problem is solved.\r\nThe EMA part in the code I use is not correct.\r\n\r\n    MOVING_AVERAGE_DECAY = 0.9999\r\n    variable_averages = tf.train.ExponentialMovingAverage(\r\n        MOVING_AVERAGE_DECAY)\r\n    for var in variables_to_restore:\r\n        tf.add_to_collection(tf.GraphKeys.MOVING_AVERAGE_VARIABLES, var)\r\n    variables_to_restore = variable_averages.variables_to_restore()\r\n\r\nIf I remove\r\n\r\n    for var in variables_to_restore:\r\n        tf.add_to_collection(tf.GraphKeys.MOVING_AVERAGE_VARIABLES, var)\r\n\r\nThe results become the same with the original code."]}, {"number": 25911, "title": "Fixed the warning in the file", "body": "In file tpu_embedding_optimization_parameters_utils.cc removed warning", "comments": []}, {"number": 25910, "title": "Fixed warning in Tpu embedding optimization", "body": "Warning fixed in the file", "comments": []}, {"number": 25909, "title": "Fixed warning in the file", "body": "Initialized the variable to fix the warning", "comments": []}, {"number": 25908, "title": "Reserved the space for shape for possibly speed up", "body": "This will speed up the push_back() call", "comments": ["@shahzadlone , sorry by the time i could see your comments that PR got merged so opened a new one with the suggestion you had asked for.", "Nagging Reviewer @aselle: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "closing the PR as similar is already merged"]}, {"number": 25907, "title": "Warning fix dependency_optimizer.cc", "body": "", "comments": ["@joyalbin  please rebase your branch.", "> @joyalbin please rebase your branch.\r\n\r\ngentle ping", "@rthadur The related source code is modified with commit 831addfea46847836fa47e76055ec4ed4cd880ff, and this PR is not valid now. I will close this"]}, {"number": 25906, "title": "Removed redundant code from layout_optimizer.cc", "body": "", "comments": ["@andyly Thank you so much for making me understand this logic more clearly.\r\nI have added one line of comment to make the code more understandable.\r\nCould you please review the change.", "@andyly , I have reworked on the comments, could you please review  the PR"]}, {"number": 25905, "title": "TFTRT: Override new TRT 5.1 pure virtual functions for FakeITensor", "body": "Forgot to also override these in convert_nodes_test.cc for FakeITensor.\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/25898", "comments": []}, {"number": 25904, "title": "benchmark_model from r1.13.0rc0 perform slower that r1.11.0 ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.01\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N.A.\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): r1.13.0rc0 and r1.11.0\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.21.0 and 0.17.2\r\n- GCC/Compiler version (if compiling from source): N.A.\r\n- CUDA/cuDNN version: N.A.\r\n- GPU model and memory: N.A.\r\n\r\n\r\nI'm testing the performance of my mobilenetv2_ssdlite model using benchmark_model on my android device( android8.1. aarch64). I built benchmark_model with the bazel command in your readme files.\r\n\r\nbenchmark command:\r\n./benchmark_model \\\r\n--graph=./mobilenetv2_ssdlite.tflite \\\r\n--use_nnapi=true \\\r\n--num_threads=1\r\n\r\n**Result( the same device, the same model):\r\nbuilt from r1.13.0rc0**\r\nInitialized session in 31.021ms\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds\r\ncount=1 curr=4385333\r\nRunning benchmark for at least 50 iterations and at least 1 seconds\r\ncount=50 first=105224 curr=151561 min=87415 max=197387 avg=**116375** std=28964\r\nAverage inference timings in us: Warmup: 4.38533e+06, Init: 31021, no stats: 116375\r\n\r\n**built from r1.\uff11\uff11\uff0e\uff10**\r\nInitialized session in 29.833ms\r\nRunning benchmark for 1 iterations\r\ncount=1 curr=4344146\r\nRunning benchmark for 50 iterations\r\ncount=50 first=88549 curr=87301 min=87271 max=104938 avg=**91616.2** std=4073\r\nAverage inference timings in us: Warmup: 4.34415e+06, Init: 29833, no stats: 91616.2\r\n\r\nWe can observe that the r1.13.0rc0 version is about 20% slower that the r1.11.0.\r\nSimilar result can be obtained while running other public ssdlite models.\r\nDoes anyone know why and how fix the problem?", "comments": []}, {"number": 25903, "title": "Please release the Raspberry Pi Camera Follower Demo", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: 1.12\r\n- Doc Link: https://youtu.be/FAMfy7izB6A?t=930\r\n\r\n\r\n**Describe the documentation issue**\r\nAndrew Selle showed off a cool demo, and said, \"I'm not an electrical engineer or mechanical engineer, so you can do this too.\" It's been 10.5 months. Maybe he forgot to release the code so that we can do that too? He also mentioned in the comment section that it would be released as a demo.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\nI don't have access to the code in question\r\n", "comments": ["@dynamicwebpaige @aselle Can I please work on this as a project for this year's GSOC.", "Hi y'all! @Ayush517 @dynamicwebpaige @aselle \r\n\r\n@Ayush517, did you end up working on this project for Google's Summer of Code? \r\n", "@leigh-johnson No,  I am working on \"Tutorials and Guides for Swift for TensorFlow\" for Google Summer of Code.\r\nA similar thing was I think mentioned in the mailing list. You can find it [here](https://groups.google.com/a/tensorflow.org/forum/?nomobile=true#!topic/tflite/Lx6VrFmX_4s)", "Hi @ArriFerrari, are you still interested in getting access to this demo?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hi Nikita,\nYes! It would still be helpful!\n-ArriFerrari\n\nOn Fri, Mar 12, 2021 at 9:11 AM Nikita ***@***.***> wrote:\n\n> Hi @ArriFerrari <https://github.com/ArriFerrari>, are you still\n> interested in getting access to this demo?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/25903#issuecomment-797551341>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AJH6BYHR4VS3V4SHS5FWKOTTDIOJJANCNFSM4GYQV6BQ>\n> .\n>\n", "Hi @ArriFerrari ! I have attached few relevant examples on camera based tracking in Rasbperry pi  for reference. Link [1](https://medium.com/augmented-startups/raspberry-pi-pan-tilt-tracking-camera-using-opencv-ai-kit-tutorial-app-4-49bf122401a4), [2](https://www.pyimagesearch.com/2019/04/01/pan-tilt-face-tracking-with-a-raspberry-pi-and-opencv/), [3](https://www.instructables.com/Automatic-Vision-Object-Tracking/) ,[4](https://opensource.com/article/20/1/object-tracking-camera-raspberry-pi) Hope it helps address this issue.Thank you!", "Thank you so much! I\u2019ve been wanting to try this for a while.\n\nOn Thu, Jan 13, 2022 at 2:20 AM mohantym ***@***.***> wrote:\n\n> Hi @ArriFerrari <https://github.com/ArriFerrari> ! I have attached few\n> relevant examples on camera based tracking in Rasbperry pi for reference.\n> Link 1\n> <https://medium.com/augmented-startups/raspberry-pi-pan-tilt-tracking-camera-using-opencv-ai-kit-tutorial-app-4-49bf122401a4>,\n> 2\n> <https://www.pyimagesearch.com/2019/04/01/pan-tilt-face-tracking-with-a-raspberry-pi-and-opencv/>,\n> 3 <https://www.instructables.com/Automatic-Vision-Object-Tracking/> ,4\n> <https://opensource.com/article/20/1/object-tracking-camera-raspberry-pi>\n> Hope it helps address this issue.Thank you!\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/25903#issuecomment-1011863722>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AJH6BYAJRA3TFAJSTJN4JMDUVZ4NLANCNFSM4GYQV6BQ>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n", "At this point I think the code has not been updated, and so it is unfortunately unlikely it will be released. I'm happy to answer any questions if people are trying to do something similar.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 25902, "title": "[INTEL MKL] Add support for mkl_quantize_op. fp32->{uint8, int8}", "body": "This PR adds support for MKL quantize operator to convert activations in  fp32 to 8 bits precision", "comments": ["Hi @penpornk . Gentle reminder for review.", "@penpornk gentle reminder.", "Hi @penpornk . Thank you for the review. I have addressed your comments. Please let me know if the changes are okay.", "Hi @penpornk , I have done the changes above too. Thank you!"]}, {"number": 25901, "title": "Batch Normalization renorm with tensorflow_hub", "body": "Hi,\r\n\r\nI have observed an issue in Tensorflow hub and batch normalization. It seems that I cannot have a batch normalization layer with renorm in a module. Consider the following short script:\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\n\r\n\r\n\r\ndef myfunc():\r\n    input = tf.placeholder(tf.float32, [5, 10])\r\n    output = tf.layers.batch_normalization(input, renorm = True)\r\n\r\n    outputs = dict(default=output)\r\n    inputs = dict(input_txt=input)\r\n    hub.add_signature(inputs=inputs, outputs=outputs)\r\n\r\ng_spec = hub.create_module_spec(myfunc)\r\n```\r\nwhich gives me the following error:\r\n\r\n> Traceback (most recent call last):\r\n  File \"..../codes/5/test.py\", line 16, in <module>\r\n    g_spec = hub.create_module_spec(myfunc)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_hub/native_module.py\", line 189, in create_module_spec\r\n    if err: raise ValueError(err)\r\nValueError: A state-holding node x of a module's graph (e.g., a Variable op) must not be subject to a tf.colocate_with(y) constraint unless y is also a state-holding node.\r\nDetails: node 'batch_normalization/renorm_mean_weight' has op 'VarHandleOp', which counts as state-holding, but Operation.colocation_groups() == [b'loc:@batch_normalization/Read/ReadVariableOp']. \r\n\r\nThe problem does NOT exist with ```renorm=False```.\r\n\r\nIs there anything that I am missing? I have not found similar problem on Stackoverflow.\r\n\r\n", "comments": ["Thanks for your interest in using TensorFlow Hub. I think these issue can be better addressed in TensorFlow Hub repo. Please post it in TF Hub repo from [here](https://github.com/tensorflow/hub/issues). Thanks!"]}, {"number": 25900, "title": "TFTRT: Don't allow empty tensors", "body": "Tensorflow allows \"empty tensors\", which have shapes that look like `[128, 0, 512]`. These can be created by a `tf.where(x)` in the case that only False is present in `x`. Since TRT doesn't appear to support anything like this, we don't allow tensors with a dim size of 0 to convert.", "comments": []}, {"number": 25899, "title": "BestExporter exporting multiple models.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nhttps://cloud.google.com/ml-engine/docs/tensorflow/runtime-version-list#1.10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n1.10\r\n- Python version:\r\n3.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\nNvidia Tesla K80\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nMultiple (8) models are exported in `export/best_exporter` directory.\r\n\r\n**Describe the expected behavior**\r\nBestExporter exports single best model in `export/best_exporter` directory.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nhttps://github.com/conversationai/conversationai-models/pull/246\r\nhttps://github.com/conversationai/conversationai-models/blob/6b383cfa6d8f3c42fc09d7c9496069a13fcad48d/experiments/tf_trainer/common/model_trainer.py#L226\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n[ml_job_job_id_tf_trainer_tf_cnn_civil_comments_glove_msushkov_20190215_163340__logs__2019-02-15T08-04.log](https://github.com/tensorflow/tensorflow/files/2881829/ml_job_job_id_tf_trainer_tf_cnn_civil_comments_glove_msushkov_20190215_163340__logs__2019-02-15T08-04.log)\r\n", "comments": ["Apologies for the delay in response. I think this issue is better asked on conversationai repo since its not a bug/feature request on TF side.\r\nPlease post it on conversationai repo from [here](https://github.com/conversationai/conversationai-models/issues). Thanks!", "Thanks for the response, apologies for the confusion. Submitted the issue here because BestExporter does not exhibit the documented behavior (exports multiple models instead of one).", "In order to expedite the trouble-shooting process, please provide a minimal code snippet to reproduce the issue reported here. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 25898, "title": "TFTRT: Override new 5.1.2 ITensor pure virtual functions", "body": "TRT 5.1.2 added new functions to ITensor. We have to define them for SimpleITensor to prevent compiler errors.\r\n\r\nAccording to TRT team, users are not meant to inherit from ITensor, so we should modify our code at some point to avoid having these issues every time the definition changes.", "comments": []}, {"number": 25897, "title": "correctly initialize scratch descriptor for gemm autotuning", "body": "Fixes #25761. Scratch should be initialized with the same descriptor as output_matrix, and not assume that output_matrix is column major. ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\nGooglers can find more info about SignCLA and this PR by [following this link](go/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F25897).\n\n<!-- need_sender_cla -->", "@ngimel please sign CLA", "I signed it!", "CLAs look good, thanks!\n\nGooglers can find more info about SignCLA and this PR by [following this link](go/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F25897).\n\n<!-- ok -->", "Thanks!\r\n\r\nI verified this also fixes JAX issue https://github.com/google/jax/issues/304 ."]}, {"number": 25896, "title": "Issues while compiling tensorflow v1.12.0 with intel 19 compilers", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.6.1810 (Core)\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: compiling from source (will use pip for .whl)\r\n- Bazel version (if compiling from source): 0.19.1\r\n- GCC/Compiler version (if compiling from source): gcc 4.8.5 +  intel/19.0.0.117\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA (CPU build)\r\n\r\nwhile compiling tensorflow using \r\n` CC=icc bazel build -s --config=mkl --define=grpc_no_ares=true --copt=-xHOST //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nthe build fails with intel compilers. Compilation error messages are attached - TF12_intel_log.txt\r\n\r\n\r\nCompilation of tensorflow v12 was succesful with pure gcc compilers.\r\nI am trying to build a TF version compiled with intel + mkl .\r\n\r\nPlease let me know if any further information is required.\r\n[TF12_intel_log.txt](https://github.com/tensorflow/tensorflow/files/2880703/TF12_intel_log.txt)\r\n\r\nAs the issue is related to intel compilers, i have posted the issue at intel forum - https://software.intel.com/en-us/forums/intel-c-compiler/topic/805280\r\n", "comments": ["I retried using CC=icpc as - \r\n\r\n`CC=icpc bazel build -s --config=mkl --jobs=1 --define=grpc_no_ares=true --copt=-xHOST //tensorflow/tools/pip_package:build_pip_package `\r\n\r\nBreif error message - \r\n \r\n\r\n`/usr/include/string.h(402): error: linkage specification is incompatible with previous \"strnlen\" (declared at line 160 of \"external/nasm/include/compiler.h\")\r\n  extern size_t strnlen (const char *__string, size_t __maxlen)\r\n                ^\r\n`\r\nError log file attached.\r\n[build_serial2.txt](https://github.com/tensorflow/tensorflow/files/2884393/build_serial2.txt)\r\n", "I tried manual compilation of a file from bazel cache directory - \r\n/home/puneet/.cache/bazel/_bazel_puneet/000cc71254e2a8499cd324ebddae445d/execroot/org_tensorflow\r\nusing the command line which fails during bazel build .\r\n\r\n`/opt/intel/compilers_and_libraries_2019.0.117/linux/bin/intel64/icpc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/opt/intel/compilers_and_libraries_2019.0.117/linux/bin/intel64 -B/usr/bin -Wunused-but-set-parameter -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/external/boringssl/_objs/ssl/s3_pkt.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/boringssl/_objs/ssl/s3_pkt.pic.o' -fPIC -iquote external/boringssl -iquote bazel-out/k8-opt/genfiles/external/boringssl -iquote bazel-out/k8-opt/bin/external/boringssl -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -iquote bazel-out/k8-opt/bin/external/bazel_tools -isystem external/boringssl/src/include -isystem bazel-out/k8-opt/genfiles/external/boringssl/src/include -isystem bazel-out/k8-opt/bin/external/boringssl/src/include -xHOST -Wa,--noexecstack '-D_XOPEN_SOURCE=700' -Wall -Werror '-Wformat=2' -Wsign-compare -Wmissing-field-initializers -Wwrite-strings -Wshadow -fno-common '-std=c++11' -Wmissing-declarations -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/boringssl/src/ssl/s3_pkt.cc -o bazel-out/k8-opt/bin/external/boringssl/_objs/ssl/s3_pkt.pic.o`\r\n\r\nhere are the terminal logs -\r\n```\r\nicpc: command line warning #10006: ignoring unknown option '-frandom-seed=bazel-out/k8-opt/bin/external/boringssl/_objs/ssl/s3_pkt.pic.o'\r\nIn file included from external/boringssl/src/ssl/s3_pkt.cc(122):\r\nexternal/boringssl/src/ssl/internal.h(191): error #1418: external function definition with no prior declaration\r\n  T *New(Args &&... args) {\r\n     ^\r\n\r\nIn file included from external/boringssl/src/ssl/s3_pkt.cc(122):\r\nexternal/boringssl/src/ssl/internal.h(204): error #1418: external function definition with no prior declaration\r\n  void Delete(T *t) {\r\n       ^\r\n\r\nIn file included from external/boringssl/src/ssl/s3_pkt.cc(122):\r\nexternal/boringssl/src/ssl/internal.h(223): error #1418: external function definition with no prior declaration\r\n  UniquePtr<T> MakeUnique(Args &&... args) {\r\n```\r\n.c and .h files are attached.\r\n\r\nUPDATE:  **-Werror** flag seems to be creating the issue, manual compilation now succeeds. Will try to remove the same from build process config.\r\n\r\n[internal.h.txt](https://github.com/tensorflow/tensorflow/files/2888579/internal.h.txt)\r\n[s3_pkt.cc.txt](https://github.com/tensorflow/tensorflow/files/2888581/s3_pkt.cc.txt)\r\n\r\n\r\n\r\n\r\n\r\n", "compilation of another .cc file failed\r\n```\r\ncd /home/puneet/.cache/bazel/_bazel_puneet/000cc71254e2a8499cd324ebddae445d/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/home/puneet/MySoftwares/COMPILER/JDK/1.8.0.201_precompiled/lib:/home/puneet/MySoftwares/COMPILER/JDK/1.8.0.201_precompiled/lib64:/home/puneet/MySoftwares/UTILS/BAZEL/0.19.1_gcc4.8.5/lib:/home/puneet/MySoftwares/UTILS/BAZEL/0.19.1_gcc4.8.5/lib64:/home/puneet/MySoftwares/COMPILER/PYTHON/3.6.8/lib:/home/puneet/MySoftwares/COMPILER/PYTHON/3.6.8/lib64:/opt/intel/compilers_and_libraries_2019.0.117/linux/compiler/lib/intel64_lin:/opt/intel/compilers_and_libraries_2019.0.117/linux/mpi/intel64/libfabric/lib:/opt/intel/compilers_and_libraries_2019.0.117/linux/mpi/intel64/lib/release:/opt/intel/compilers_and_libraries_2019.0.117/linux/mpi/intel64/lib:/opt/intel/compilers_and_libraries_2019.0.117/linux/ipp/lib/intel64:/opt/intel/compilers_and_libraries_2019.0.117/linux/mkl/lib/intel64_lin:/opt/intel/compilers_and_libraries_2019.0.117/linux/tbb/lib/intel64/gcc4.7:/opt/intel/debugger_2019/libipt/intel64/lib:/opt/intel/compilers_and_libraries_2019.0.117/linux/daal/lib/intel64_lin:/opt/intel/compilers_and_libraries_2019.0.117/linux/daal/../tbb/lib/intel64_lin/gcc4.4 \\\r\n    PATH=/home/puneet/MySoftwares/COMPILER/JDK/1.8.0.201_precompiled/bin:/home/puneet/MySoftwares/UTILS/BAZEL/0.19.1_gcc4.8.5/bin:/home/puneet/MySoftwares/PYTHONPACKAGES/3.6.8_gnu4.8.5_intel19.0.0.117/PACKAGESUITE/1/bin:/home/puneet/MySoftwares/COMPILER/PYTHON/3.6.8/bin:/opt/intel/compilers_and_libraries_2019.0.117/linux/bin/intel64:/opt/intel/compilers_and_libraries_2019.0.117/linux/mpi/intel64/libfabric/bin:/opt/intel/compilers_and_libraries_2019.0.117/linux/mpi/intel64/bin:/opt/intel/debugger_2019/gdb/intel64/bin:/opt/xcat/bin:/opt/xcat/sbin:/opt/xcat/share/xcat/tools:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/puneet/.local/bin:/home/puneet/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    /opt/intel/compilers_and_libraries_2019.0.117/linux/bin/intel64/icc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/opt/intel/compilers_and_libraries_2019.0.117/linux/bin/intel64 -B/usr/bin -Wunused-but-set-parameter -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/host/bin/tensorflow/core/kernels/_objs/non_max_suppression_op/non_max_suppression_op.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/core/kernels/_objs/non_max_suppression_op/non_max_suppression_op.pic.o' -fPIC -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DTF_USE_SNAPPY -iquote . -iquote bazel-out/host/genfiles -iquote bazel-out/host/bin -iquote external/nsync -iquote bazel-out/host/genfiles/external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -iquote bazel-out/host/bin/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/host/genfiles/external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/genfiles/external/local_config_sycl -iquote bazel-out/host/bin/external/local_config_sycl -iquote external/com_google_absl -iquote bazel-out/host/genfiles/external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/gif_archive -iquote bazel-out/host/genfiles/external/gif_archive -iquote bazel-out/host/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/host/genfiles/external/jpeg -iquote bazel-out/host/bin/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/host/genfiles/external/protobuf_archive -iquote bazel-out/host/bin/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/genfiles/external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/genfiles/external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/genfiles/external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/genfiles/external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/zlib_archive -iquote bazel-out/host/genfiles/external/zlib_archive -iquote bazel-out/host/bin/external/zlib_archive -iquote external/png_archive -iquote bazel-out/host/genfiles/external/png_archive -iquote bazel-out/host/bin/external/png_archive -isystem external/nsync/public -isystem bazel-out/host/genfiles/external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem third_party/eigen3/mkl_include -isystem bazel-out/host/genfiles/third_party/eigen3/mkl_include -isystem bazel-out/host/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/host/genfiles/external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/host/genfiles/external/gif_archive/lib -isystem bazel-out/host/bin/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/host/genfiles/external/protobuf_archive/src -isystem bazel-out/host/bin/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/host/genfiles/external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/zlib_archive -isystem bazel-out/host/genfiles/external/zlib_archive -isystem bazel-out/host/bin/external/zlib_archive -isystem external/png_archive -isystem bazel-out/host/genfiles/external/png_archive -isystem bazel-out/host/bin/external/png_archive -g0 -g0 -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-DINTEL_MKL=1' -DEIGEN_USE_VML -DENABLE_MKL -fopenmp -msse3 -pthread '-DINTEL_MKL=1' -DENABLE_MKL -Wno-builtin-macro-redefined -Wno-error '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/core/kernels/non_max_suppression_op.cc -o bazel-out/host/bin/tensorflow/core/kernels/_objs/non_max_suppression_op/non_max_suppression_op.pic.o)\r\n```\r\n\r\nerror messages are as follows - \r\n```\r\n\r\nERROR: /home/puneet/MySoftwares/INSTALLATION_ROOT/python3/intel/tensorflow-1.12.0/tensorflow/core/kernels/BUILD:2220:1: C++ compilation of rule '//tensorflow/core/kernels:non_max_suppression_op' failed (Exit 2)\r\n\r\ntensorflow/core/kernels/non_max_suppression_op.cc(120): error: type name is not allowed\r\n    typename TTypes<float, 2>::ConstTensor overlaps_data =\r\n    ^\r\n\r\ntensorflow/core/kernels/non_max_suppression_op.cc(120): error: expected a \";\"\r\n    typename TTypes<float, 2>::ConstTensor overlaps_data =\r\n                                           ^\r\n\r\ntensorflow/core/kernels/non_max_suppression_op.cc(122): error: identifier \"overlaps_data\" is undefined\r\n    return std::bind(&OverlapsGreaterThanThreshold, overlaps_data,\r\n\r\n\r\ntensorflow/core/kernels/non_max_suppression_op.cc(112): error: expected a \";\"\r\n    typename TTypes<T, 2>::ConstTensor boxes_data = boxes.tensor<T, 2>();\r\n                                       ^\r\n\r\ntensorflow/core/kernels/non_max_suppression_op.cc(113): error: identifier \"boxes_data\" is undefined\r\n    return std::bind(&IOUGreaterThanThreshold<T>, boxes_data,\r\n               \r\n\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 135.802s, Critical Path: 119.45s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 2777 processes: 2777 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n\r\n\r\n\r\n\r\nI am attaching files involved in this issue.\r\n[non_max_suppression_op.cc.txt](https://github.com/tensorflow/tensorflow/files/2889131/non_max_suppression_op.cc.txt)\r\n[non_max_suppression_op.h.txt](https://github.com/tensorflow/tensorflow/files/2889132/non_max_suppression_op.h.txt)\r\n\r\nI noticed that intel compilers are failing to compile the non_max_suppression_op.cc file. gcc compilers are able to compile the file perfectly. Terminal logs - intelv19_issue_non_max_suppression_op.txt. Seems i now need to give up on intel compilers or downgrade version.\r\n\r\n[intelv19_issue_non_max_suppression_op.txt](https://github.com/tensorflow/tensorflow/files/2889297/intelv19_issue_non_max_suppression_op.txt)\r\n\r\n\r\n\r\n", "We recommend you to follow the standard gcc process, as TF w/ Intel compiler to believed to be very fragile. We don't have full engineering support to accommodate this request as of now. However, if you are  still interested to use intel compiler, you can submit PRs to TensorFlow to support Intel compiler. ", "closing this issue, as mentioned due to limited support", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25896\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25896\">No</a>\n"]}, {"number": 25895, "title": "keras ConvLSTM2d not working with eager execution", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.0-dev20190130\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 10/7.4.2.24\r\n- GPU model and memory: GeForce GTX 1050 Ti, 4096 MB\r\n\r\n**Describe the current behavior**\r\nWhile using Eager Execution keras ConvLSTM2D seems like not initialized.\r\nAfter that we tried implement our model using gradient tape and it seemed fine but after loading checkpoint results was different than expected.\r\nThe second issue probably has something in common with the first, so i'm posting code to reproduce first one and logs. \r\n\r\n**Describe the expected behavior**\r\nConvLSTM2D works with Eager Execution\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nDATA_PATH = 'path/to/directory/with/grayscale/jpg/images\"\r\n\r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers, Sequential\r\n\r\ntf.enable_eager_execution()\r\nfilenames = [os.path.join(DATA_PATH, x) for x in sorted(os.listdir(DATA_PATH))]\r\ntf_dataset = tf.data.Dataset.from_tensor_slices(filenames).map(lambda x: tf.image.decode_jpeg(tf.read_file(x), channels=0))\r\ntf_dataset = tf_dataset.batch(10).batch(10)\r\ntf_dataset = tf.data.Dataset.zip((tf_dataset, tf_dataset)).prefetch(tf.contrib.data.AUTOTUNE)\r\n\r\nmodel =  Sequential([\r\n    layers.ConvLSTM2D(filters=3, kernel_size=3, padding='same')\r\n])\r\nmodel.compile(optimizer=tf.train.AdamOptimizer(), loss='categorical_crossentropy', metrics=['accuracy'])\r\nmodel.fit(tf_dataset)\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0219 14:04:10.508448 140324682260608 training_utils.py:1249] Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\r\n\r\n2019-02-19 14:04:13.426855: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Failed precondition: Error while reading resource variable _AnonymousVar4 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar4/N10tensorflow3VarE does not exist.\r\n         [[{{node convlstm1/convolution/ReadVariableOp}}]]\r\n         [[training/TFOptimizer/gradients/convlstm1/while_grad/convlstm1/while_grad/LoopCond/_705/_52]]\r\n\r\n2019-02-19 14:04:13.426894: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Failed precondition: Error while reading resource variable _AnonymousVar4 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar4/N10tensorflow3VarE does not exist.\r\n         [[{{node convlstm1/convolution/ReadVariableOp}}]]\r\n\r\n2019-02-19 14:04:13.426968: E tensorflow/core/common_runtime/process_function_library_runtime.cc:735] Component function execution failed: Failed precondition: Error while reading resource variable _AnonymousVar4 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar4/N10tensorflow3VarE does not exist.\r\n         [[{{node convlstm1/convolution/ReadVariableOp}}]]\r\n         [[training/TFOptimizer/gradients/convlstm1/while_grad/convlstm1/while_grad/LoopCond/_705/_52]]\r\n\r\n2019-02-19 14:04:13.427323: E tensorflow/core/common_runtime/process_function_library_runtime.cc:735] Component function execution failed: Failed precondition: Error while reading resource variable _AnonymousVar4 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar4/N10tensorflow3VarE does not exist.\r\n         [[{{node convlstm1/convolution/ReadVariableOp}}]]\r\n```", "comments": ["I encountered the same preblem in tensorflow-gpu-1.9", "I think the issue should be the fixed by the recent commit https://github.com/tensorflow/tensorflow/commit/391bee73641f1c23ef90cc40a9e8893f343c30fc. Can u try the same snippet with tf-nightly and see if it works?", "> I think the issue should be the fixed by the recent commit [391bee7](https://github.com/tensorflow/tensorflow/commit/391bee73641f1c23ef90cc40a9e8893f343c30fc). Can u try the same snippet with tf-nightly and see if it works?\r\nI install  tf-nightly -1.14.1-dev20190304, I re-run the code to find it started wording.\r\nthank you!", "I install tf-nightly -1.14.1-dev20190304, I re-run the code to find it started wording.\r\nthank you!", "SG, glad its fixed."]}, {"number": 25894, "title": "TF Lite lstm.cc warnings fix", "body": "Warning[warning: control reaches end of non-void function] fix.", "comments": ["This is duplicate PR#25719, so closing this PR."]}, {"number": 25893, "title": "Fix test sharding version 2", "body": "Test sharding appears to have been broken by commit [87cc788](https://github.com/tensorflow/tensorflow/commit/87cc788e1cc04036e89d87bbe71df7f449d0e934). The result\r\nbeing that many tests are simply not run when sharding is enabled.\r\nThe issue seems to be that the filtering for sharding is happening\r\ntwice, once in tensorflow/python/platform/googletest.py and then once\r\nagain in external/absl_py/absl/testing/absltest.py. This commit fixes\r\nthe issue by removing the shard filtering code in googletest.py.\r\n\r\nFixes: #25594", "comments": ["This PR replaces https://github.com/tensorflow/tensorflow/pull/25595 which for some reason is proving difficult to merge.", "@pragyaak  Please help proceeding with the next steps as I've some access issues(I'm trying to resolve).", "This commit seems to reliably cause //tensorflow/lite/python:lite_test to fail one of four sharded runs. Looking into what's going on.", "The failure of //tensorflow/lite/python:lite_test is revealed by rather than caused by this patch. The bug that causes the failure must have snuck past the CI system while sharding was not working correctly, and the failing test was simply not being run.  For example,  if I run the test on a build of master from yesterday without this patch the test passes.\r\n\r\n```\r\n$ bazel test --config opt -- //tensorflow/lite/python:lite_test\r\nDEBUG: /home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/external/build_bazel_rules_apple/apple/repositories.bzl:35:5: \r\nWARNING: `build_bazel_rules_apple` depends on `bazel_skylib` loaded from https://github.com/bazelbuild/bazel-skylib.git (tag 0.6.0), but we have detected it already loaded into your workspace from None (tag None). You may run into compatibility issues. To silence this warning, pass `ignore_version_differences = True` to `apple_rules_dependencies()`.\r\n\r\nINFO: Build options have changed, discarding analysis cache.\r\nWARNING: /home/user/src/tensorflow/tensorflow/python/BUILD:3178:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nINFO: Analysed target //tensorflow/lite/python:lite_test (1 packages loaded, 18590 targets configured).\r\nINFO: Found 1 test target...\r\nTarget //tensorflow/lite/python:lite_test up-to-date:\r\n  bazel-bin/tensorflow/lite/python/lite_test\r\nINFO: Elapsed time: 18.035s, Critical Path: 13.55s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 140 processes: 140 local.\r\nINFO: Build completed successfully, 208 total actions\r\n//tensorflow/lite/python:lite_test                                       PASSED in 3.5s\r\n  Stats over 4 runs: max = 3.5s, min = 3.1s, avg = 3.3s, dev = 0.1s\r\n\r\nExecuted 1 out of 1 test: 1 test passes.\r\nINFO: Build completed successfully, 208 total actions\r\n```\r\n\r\nHowever, if I run the test on the same build with sharding disabled it fails\r\n\r\n```\r\n$ bazel test --config=opt --test_sharding_strategy=disabled --cache_test_results=no -- //tensorflow/lite/python:lite_test\r\nDEBUG: /home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/external/build_bazel_rules_apple/apple/repositories.bzl:35:5: \r\nWARNING: `build_bazel_rules_apple` depends on `bazel_skylib` loaded from https://github.com/bazelbuild/bazel-skylib.git (tag 0.6.0), but we have detected it already loaded into your workspace from None (tag None). You may run into compatibility issues. To silence this warning, pass `ignore_version_differences = True` to `apple_rules_dependencies()`.\r\n\r\nINFO: Build options have changed, discarding analysis cache.\r\nWARNING: /home/user/src/tensorflow/tensorflow/python/BUILD:3178:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nINFO: Analysed target //tensorflow/lite/python:lite_test (0 packages loaded, 18589 targets configured).\r\nINFO: Found 1 test target...\r\nFAIL: //tensorflow/lite/python:lite_test (see /home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/lite/python/lite_test/test.log)\r\nTarget //tensorflow/lite/python:lite_test up-to-date:\r\n  bazel-bin/tensorflow/lite/python/lite_test\r\nINFO: Elapsed time: 17.584s, Critical Path: 15.87s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 1 process: 1 local.\r\nINFO: Build completed, 1 test FAILED, 2 total actions\r\n//tensorflow/lite/python:lite_test                                       FAILED in 15.8s\r\n  /home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/lite/python/lite_test/test.log\r\n\r\nINFO: Build completed, 1 test FAILED, 2 total actions\r\n```\r\n\r\nand this failure occurs on a branch without this patch.\r\n", "Aye, I've disabled the test internally and routed it to the right person. We should be clear to submit this now (without being immediately rolled back!)\r\n\r\n@pragyaak can we get this submitted?", "Hi @rthadur, can we get this submitted?", "> Hi @rthadur, can we get this submitted?\r\n\r\nSure , @markdryan can you please check test failures.", "> > Hi @rthadur, can we get this submitted?\r\n> \r\n> Sure , @markdryan can you please check test failures.\r\n\r\nThe test failures should be unrelated, as Mark explained above.", "TF 1.13 was released in between. Does this suggest that some bugs could have sneaked into 1.13 without being tested?", "I don't think that's a concern. The change that introduced this bug was from January, while 1.13 cut from master in December 2018.\r\n\r\nAsides, all tests were still run internally for the duration, as this bug only affected sharding in OSS. Two tests broke during this time for OSS only, which we're now tracing.", "Awesome! Thanks for the explanations!"]}, {"number": 25892, "title": "TF Lite kernel_test package test build error fix", "body": "This PR solve the several test compilation problem while runing kernel_test package.", "comments": ["@nutsiepully  \r\n\r\nPlease review the changes, thanks.", "Thanks a lot for your contribution.\r\n\r\nUnfortunately, this happened because of a missing config for some of the modified files. We don't need a new #define flag for this. There is a tool which strips out all the \"third_party\" references. We'll add the missing files to that tool to rectify this.\r\n\r\nThanks!\r\n"]}, {"number": 25891, "title": "TF Lite micro_speech package test build error fix", "body": "Test micro_speech package run encounter \"undefined reference to symbol 'floor@@GLIBC_2.2.5'\", this PR solve the problem.", "comments": ["@nutsiepully  \r\n\r\nPlease review the changes, thanks.", "@Dayananda-V - Could you please describe what build/test is this fixing? The linkopts options should be passed into the bazel from the specific test you are trying to execute.\r\n\r\nThis also looks like it might be a fix local to your setup.\r\n", "@nutsiepully \r\n\r\nbazel  test -j 8 -c opt --linkopt=\"-lm\"  -- //tensorflow/lite/experimental/micro/...\r\n\r\nwith above command line issue is solved, if don't pass \"linkopt\" still compilation problem exists. So would recomended to use bazel build with linkopt or this solution?", "@Dayananda-V - Can you paste the exact error you are receiving? Our builds/tests have not detected any failure in this, so it's likely the failure is specific on your local setup.\r\n\r\nIf it's a more general issue, we can fix it in the build definitions. But more likely, if it's specific to your local environment you can just use the linkopt as an argument.", "@nutsiepully \r\n\r\nBelow is the error am facing, i configured cuda environment and android configuration on top of default configure.\r\n`ERROR: /opt/work/work_tf/tensorflow/tensorflow/lite/experimental/micro/kernels/BUILD:49:1: Linking of rule '//tensorflow/lite/experimental/micro/kernels:depthwise_conv_test_binary' failed (Exit 1)\r\n/usr/bin/x86_64-linux-gnu-ld: bazel-out/k8-opt/bin/tensorflow/lite/kernels/internal/libquantization_util.a(quantization_util.o): undefined reference to symbol 'floor@@GLIBC_2.2.5'\r\n//lib/x86_64-linux-gnu/libm.so.6: error adding symbols: DSO missing from command line\r\ncollect2: error: ld returned 1 exit status`\r\n\r\n Hope this helps to rectify the problem...", "So judging by the error, the code is unable to link to the `floor` function. The `-lm` option asks the linker to link in `libm` explicitly which contains an implementation of `floor`.\r\n\r\nHowever, on a regular configuration this error does not occur. I'm wondering if there is some issue in the way your environment is set up which is causing this. Could you mention the exact steps you used to configure your environment? Perhaps it'll help me replicate the error.\r\n\r\nRegardless though, if we actually find an issue with the environment, the solution won't be to add a default link option to the macro. That changes the build for all targets that use the macro.", "@nutsiepully \r\n\r\ni agree with your detail explanation, am just using below configure and able to reproduce this problem on my workspace.I also not recommended but this problem is realy local build issue or any macro we need to fix for this problem?\r\n\r\nConfigure :\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version (use command below): 1.13.1\r\nPython version: 3.6.7\r\nBazel version (if compiling from source): 0.21.0\r\nGCC/Compiler version (if compiling from source): 6.5.0\r\nCUDA/cuDNN version:cuda-9.2\r\nGPU model and memory: GTX 1060 6GB GDDR5 "]}, {"number": 25890, "title": "TF Lite main.cc warnings fix", "body": "Warnign[unused-but-set-variable]] fix.", "comments": ["@nutsiepully  \r\n\r\nPlease review the changes, thanks."]}, {"number": 25889, "title": "tensorflow r1.6 use Eigen or intel mkl for matrix operation?", "body": "tensorflow r1.6 use Eigen or intel mkl for matrix operation?\r\nand we user java api of tensorFlow 1.6 and try to optimize the time cost ", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 25888, "title": "New tflite operator ELU added", "body": "Based on some of the recent issues reported by people added this operator", "comments": ["@nutsiepully i have resolved the merge conflict, can you pls review the PR", "@amitsrivastava78 - The Elu operator has been merged into master. I don't think this is needed.\r\n\r\nThanks for sending the pull request."]}, {"number": 25887, "title": "Lite: Sparse_to_dense Operator performance improvement", "body": "1:> Code Optimized for performance, remove unnecessary std::vector usage\r\n2:> Removed hard coding or bottleneck to 4-Dim\r\n3:> Bug fix in wrong error condition\r\n4:> New Test cases added for uncovered scenarios\r\n5:> Bug fix for num_indices fetch for 0-Dim Indices", "comments": ["@jdduke , @rthadur , I have one more PR pending(Raise once this PR is merged) for one more feature in sparse_to_dense(tflite) , would appreciate if this PR can be reviewed and concluded as soon as possible. Thanks in advance!", "Adding a few other folks to review while I'm out of the office.", "Can you quantify what the performance improvement is (when executed on an arm device with `-c opt`)? Ideally when run against an actual model with our [benchmark_model tool and operator profiling enabled](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark).\r\n\r\nWe generally try to avoid performance optimizations in reference_ops.h if we can avoid it, preferring optimized_ops.h for such cases. However, this looks like a straightforward change and doesn't really hurt readability, so it's probably fine.", "@jdduke, thanks for your feedback. Will try to get performance data.", "@jdduke , @miaout17 , @karimnosseir : Please help conclude this PR, it is been long pending, TIA!", "@karimnosseir can you take a look?", "ANSHUMAN87, you left comment\r\n\"@jdduke, thanks for your feedback. Will try to get performance data.\" \r\n\r\nI don't see any numbers posted. This doesn't look ready to me.\r\nCan you please provide the data, so we can proceed further.\r\n\r\nThanks", "@ANSHUMAN87 gentle ping to address above review comments.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}]