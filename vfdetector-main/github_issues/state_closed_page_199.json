[{"number": 48732, "title": "Custom gradient tape-based fit() function is slower than Tf2 fit() and has memory leaking", "body": "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.1 LTS\r\n- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: Python 3.8.5\r\n- CUDA/cuDNN version: CUDA Version 11.0.207\r\n- GPU model and memory: NVIDIA Quadro RTX 5000, 16 GB GDDR6\r\n\r\nI wrote a custom `fit()` function using gradient tape in TF2. The `@tf.function` decorator has been used in a few specific locations to disable eager execution, which is supposed to speed up the whole code.\r\n\r\nMy custom fit has three main issues:\r\n\r\n1-It's a lot slower than its TF2 counterpart (decorator `@tf.function` on the `train_batch` function does not improve the speed, while the decorator placed above the `fit()` function does). The time indicated on the output (see image attached below) of the custom `fit()` is wrong for some reason, in reality it is constant, around 10 seconds per each epoch.\r\n\r\n2-Memory usage does increase linearly (see plot at the bottom).\r\n\r\n3-Code gets killed even before reaching the maximum amount of GPU memory usage (see code output at the bottom).\r\n\r\nBelow you can find the complete code, you should be able to run it directly, as it does not require any data or any additional line of code.\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Conv1D, Flatten, MaxPooling1D\r\nfrom tensorflow.keras.models import Model\r\nimport numpy as np\r\nimport warnings\r\nimport psutil\r\nimport time\r\nimport matplotlib.pyplot as plt\r\n\r\n\r\n### DEFINE NEURAL NETWORK\r\ndef baseline_network():\r\n    input = Input(shape=(128, 11))\r\n    layer = Conv1D(64, 5, activation='relu', padding='same')(input) \r\n    layer = Flatten()(layer)\r\n    layer = Dense(1024, activation='relu')(layer)\r\n    output = Dense(257, activation=None)(layer)\r\n                  \r\n    model = Model(inputs=input, outputs=output)\r\n    model.compile(optimizer='adam',\r\n                  loss='mse', \r\n                  metrics=['mse'])\r\n   \r\n    return model\r\n\r\n\r\n### DATA GENERATOR\r\ndef generator():\r\n    \r\n    # initialize numpy tensors\r\n    X_out = np.zeros([6000, 128, 11])\r\n    Y_out = np.zeros([6000, 257])\r\n\r\n    ### CREATE DATA BATCHES ###\r\n    while True:\r\n        for i in range(32):\r\n        \r\n            yield X_out, Y_out\r\n\r\n\r\n### CUSTOM FIT USING GRADIENT TAPE\r\nclass runGradientTape:\r\n    \r\n    def __init__(self, model, cost_function, iterator_train, steps_train, max_epochs, batch_size):\r\n        \r\n        self.model = model\r\n        self.cost_function = cost_function\r\n        self.iterator_train = iterator_train\r\n        self.steps_train = steps_train\r\n        self.max_epochs = max_epochs\r\n        self.batch_size = batch_size\r\n\r\n\r\n    def lossWrapper(self):\r\n        \r\n        #@tf.function\r\n        def lossFunction(y_true, y_pred):\r\n            # calculating loss and squeezing single dimensions away\r\n            loss = tf.squeeze(self.cost_function(y_pred, y_true))\r\n            # calculate mean over batches\r\n            loss = tf.reduce_mean(loss)\r\n            # return the loss\r\n            return loss\r\n        # returning the loss function as handle\r\n        return lossFunction\r\n    \r\n    \r\n    @tf.function\r\n    def train_batch(self, x_batch_train, y_batch_train):\r\n        # run gradient taping\r\n        with tf.GradientTape() as tape:\r\n            y_hat = self.model(x_batch_train, training=True)\r\n            loss_value = self.loss_fn(y_batch_train, y_hat)\r\n\r\n        # calculate gradients\r\n        grads = tape.gradient(loss_value, self.model.trainable_variables)\r\n        self.model.optimizer.apply_gradients(zip(grads, self.model.trainable_variables)) \r\n        \r\n        return loss_value\r\n    \r\n    \r\n    @tf.function\r\n    def fit(self):\r\n        \r\n        # others\r\n        warnings.filterwarnings(\"ignore\")\r\n\r\n        self.loss_fn = self.lossWrapper()\r\n        progbar = tf.keras.utils.Progbar(self.steps_train)\r\n        \r\n        memory_used = []\r\n        \r\n        # ITERATE OVER EPOCHS \r\n        for epoch in range(self.max_epochs):\r\n            \r\n            print('Epoch = %d/%d' %(epoch+1, self.max_epochs))\r\n            \r\n            # iterate over steps in training generator\r\n            for i in range(self.steps_train):\r\n                \r\n                x_batch_train, y_batch_train = next(self.iterator_train)\r\n                \r\n                loss_value = self.train_batch(x_batch_train, y_batch_train)\r\n                progbar.update(i+1)\r\n                \r\n                memory_used.append(psutil.virtual_memory().used / 2 ** 30)\r\n                #print('   memory used: ', memory_used[-1])\r\n        \r\n        #plt.plot(memory_used)\r\n        #plt.title('Memory usage vs batch')\r\n        #plt.savefig('mem_usage')\r\n\r\n                \r\n# parameters\r\nsteps_train = 100       \r\nn_epochs = 5\r\nbatch_size = 32\r\nbaseline =  baseline_network()   \r\niterator_train = generator()\r\n\r\n# TF2 fit()\r\nprint(\"TF2 fit\")\r\nstart_time = time.time()\r\nhistory = baseline.fit(iterator_train, steps_per_epoch=steps_train, epochs=n_epochs)\r\nstop_time = time.time()\r\nprint(\"time elapsed (TF2 fit): \", stop_time-start_time)\r\n\r\n#reset parameters\r\ndel baseline, iterator_train\r\nbaseline =  baseline_network()   \r\niterator_train = generator()\r\n\r\n# custom fit\r\nprint(\"Custom fit\")\r\nstart_time = time.time()\r\nrun_gradient_tape = runGradientTape(baseline, keras.losses.MSE, iterator_train, steps_train, n_epochs, batch_size)  \r\nrun_gradient_tape.fit()\r\nstop_time = time.time()\r\nprint(\"time elapsed (custom fit): \", stop_time-start_time)\r\n```\r\n\r\nSome of the suggestions I tried so far:\r\n\r\n1- `gs.collect()` does not give any improvement in this case.\r\n\r\n2- profiling the code using tensorboard (check profile and memory usage below).\r\n\r\nCould you please give me any suggestion on how to solve the three previously mentioned problems?\r\n\r\nCode output\r\n![](https://i.stack.imgur.com/LEZkN.png)\r\n\r\nMemory usage\r\n![](https://i.stack.imgur.com/IA4wr.png)\r\n![](https://i.stack.imgur.com/Oj8Tc.png)\r\n\r\nTiming:\r\n![](https://i.stack.imgur.com/OpaD3.png)\r\n\r\n\r\n", "comments": ["@ymodak\r\n\r\nI have tried in colab with TF version 2.4, 2.5.orc1 and nightly version and noticed that session is being crashed. Please, find the [gist](https://colab.research.google.com/gist/tilakrayal/8d0d45222b42d0f9ae4b7af8a412affc/48732.ipynb) here. Thanks!", "> \r\n> \r\n> @ymodak\r\n> \r\n> I have tried in colab with TF version 2.4, 2.5.orc1 and nightly version and noticed that session is being crashed. Please, find the [gist](https://colab.research.google.com/gist/tilakrayal/8d0d45222b42d0f9ae4b7af8a412affc/48732.ipynb) here. Thanks!\r\n\r\nHi, I believe it has something to do with the RAM available in Colab. I do not use this service but, as far as I understood, maximum RAM should be around 11-12 GB, please correct me if I am wrong. \r\nThe code crashes around batch 121-143. If you see the linear memory usage plot, we should be around that value.\r\nHowever, my code does not crash on my GPU, but it gets killed at the very end, after the very last batch is fed. You may want to just reduce the number of epochs to 1, and see if this problem still occurs in Colab.", "Custom fit took `16 seconds` for 1 epoch vs TF2 fit which took `26 seconds` for 1 epoch. At the end the end session gets killed. [Here](https://colab.research.google.com/gist/sachinprasadhs/52f386c3cee8c2f7e00a2662d4000243/48732.ipynb) is the gist for reference. If you are satisfied with this performance, please move this issue to closed. Thanks!", "Thanks @sachinprasadhs for the updated benchmarks for 2.8 -- the memory usage still linearly increases per call, indicating there is a leak.\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Tried to reproduce this and saw that @tf.function is not marked correctly:\r\nBecause @tf.function's first execution is expensive (which is used to do [function tracing](https://www.tensorflow.org/guide/intro_to_graphs#when_is_a_function_tracing)), marking the custom loop function as a @tf.function will cause the whole training process to be under tracing, which is slow and uses extra mem, eventually killing the training process when the memory are used up.\r\n\r\nThe convention is to mark the single training step as a @tf.function, so only the first training batch will have the graph tracing overhead (if the data type and shape remain the same for the remaining data). \r\n\r\nPlease see [this colab](https://colab.research.google.com/gist/tomytsai1/e958398428a786edaea3ebff9340b42d/48732.ipynb#scrollTo=vOVrfKCFSbJo) as an example. We see that the custom fit runs about the same as (or slightly faster than) the tf2 fit, and no apparent memory increase after the custom training.\r\n\r\n"]}, {"number": 48731, "title": "dow", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["I apologize, but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template. Please provide all the information it asks. Thank you.\r\n"]}, {"number": 48730, "title": "Tensorflow", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Traceback (most recent call last):\r\n  File \"C:\\Users\\User\\PycharmProjects\\wheelchair\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/User/PycharmProjects/wheelchair/Training/Training.py\", line 5, in <module>\r\n    from utlis import *\r\n  File \"C:\\Users\\User\\PycharmProjects\\wheelchair\\Training\\utlis.py\", line 8, in <module>\r\n    from tensorflow.keras.models import Sequential\r\n  File \"C:\\Users\\User\\PycharmProjects\\wheelchair\\venv\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\User\\PycharmProjects\\wheelchair\\venv\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 39, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n  File \"C:\\Users\\User\\PycharmProjects\\wheelchair\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\User\\PycharmProjects\\wheelchair\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n", "@athira-ajith ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the following information\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version:\r\nPython version:\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\n \r\nand the exact sequence of commands / steps that you executed before running into the problem\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@athira-ajith ,\r\n\r\nDo you have any update on this?\r\nBased on the error you may also refer to:#47517,#43915,#46788\r\n\r\nThanks!\r\n\r\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 48729, "title": "[Cherrypicke:2.5]Disable pad_op_test for macos. Test times out during 2.5 release", "body": "PiperOrigin-RevId: 370178263\nChange-Id: I57989213810709d73d9e503b40b0f871e9006a05", "comments": []}, {"number": 48728, "title": "[CherryPick 2.5] 3.9 compatibility fix for AST unparsing", "body": null, "comments": []}, {"number": 48727, "title": "Port micro op GATHER_ND and its test code from lite", "body": "Issue #46268. This PR aims to finish porting GATHER_ND from TFL to TFLM.\r\n(This PR combines the originally planned PRS 3, 4, and 5)\r\n\r\nNotes:\r\n1. For params/output tensors, only the float and int8_t data types are supported;\r\n2, For indices tensors, only the int32_t data type is supported;\r\n3. The reference implementation in lite/kernels/internal/reference/reference_ops.h was not used, due to the vector used in the reference (possible dynamic allocation);\r\n4. As with the TFL kernel Gather_ND, the TFLM kernel does not yet support batch_dims as of April 23, 2021.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "For some reason, maybe the branch name too long? Only three tests are running. Re-created the same PR here https://github.com/tensorflow/tensorflow/pull/48812. Closing this PR."]}, {"number": 48726, "title": "Tensorflow not working on Windows (Parallel) VM on Mac with M1 Chip", "body": "**Error: ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.**\r\n\r\nSystem info:\r\nCuda V11.3\r\ncudnn V11.3\r\nPycharm\r\nPython 3.7\r\nVS2019 installed as well\r\nInstalled tensorflow by : `pip install tensorflow` (tensorflow version: 1.15.0) and also tried tensorflow 2.4.1\r\n\r\nSystem is: windows10 installed by parallel on Mac with M1 Chip\r\n\r\nWhen I test tensorflow by following commends, it gave me these error\r\n`import tensorflow as tf`\r\n\r\nTraceback (most recent call last):\r\nFile \"C:\\Users\\XXXXXXXXX\\Documents\\XXXX\\Development\\Application\\WebVersion-Desktop\\XXXX\\XXXXvenv\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in\r\nfrom tensorflow.python.pywrap_tensorflow_internal import *\r\nFile \"C:\\Users\\XXXX\\Documents\\XXXX\\Development\\Application\\WebVersion-Desktop\\XXXX\\XXXX\\venv\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in\r\n_pywrap_tensorflow_internal = swig_import_helper()\r\nFile \"C:\\Users\\XXXX\\Documents\\XXXX\\Development\\Application\\WebVersion-Desktop\\XXXX\\XXXX\\venv\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nFile \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\imp.py\", line 242, in load_module\r\nreturn load_dynamic(name, filename, file)\r\nFile \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\imp.py\", line 342, in load_dynamic\r\nreturn _load(spec)\r\n**ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:**\r\n\r\nTraceback (most recent call last):\r\nFile \"C:/Users/XXXX/Documents/XXXX/Development/Application/WebVersion-Desktop/XXXX/XXXX/XXXXapp.py\", line 39, in\r\nfrom model import KeyPointClassifier\r\nFile \"C:\\Users\\XXXX\\Documents\\XXXX\\Development\\Application\\WebVersion-Desktop\\XXXX\\XXXX\\model_init_.py\", line 1, in\r\nfrom model.keypoint_classifier.keypoint_classifier import KeyPointClassifier\r\nFile \"C:\\Users\\XXXX\\Documents\\XXXX\\Development\\Application\\WebVersion-Desktop\\XXXX\\XXXXr\\model\\keypoint_classifier\\keypoint_classifier.py\", line 4, in\r\nimport tensorflow as tf\r\nFile \"C:\\Users\\XXXX\\Documents\\XXXX\\Development\\Application\\WebVersion-Desktop\\XXXX\\XXXX\\venv\\lib\\site-packages\\tensorflow_init_.py\", line 99, in\r\nfrom tensorflow_core import *\r\nFile \"C:\\Users\\XXXX\\Documents\\XXXX\\Development\\Application\\WebVersion-Desktop\\XXXX\\XXXX\\venv\\lib\\site-packages\\tensorflow_core_init_.py\", line 28, in\r\nfrom tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import\r\nFile \"C:\\Users\\XXXX\\Documents\\XXXX\\Development\\Application\\WebVersion-Desktop\\XXXX\\XXXX\\venv\\lib\\site-packages\\tensorflow_init_.py\", line 50, in getattr\r\nmodule = self.load()\r\nFile \"C:\\Users\\XXXX\\Documents\\XXXX\\Development\\Application\\WebVersion-Desktop\\XXXX\\XXXX\\venv\\lib\\site-packages\\tensorflow_init.py\", line 44, in _load\r\nmodule = importlib.import_module(self.name)\r\nFile \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\importlib_init.py\", line 127, in import_module\r\nreturn _bootstrap.gcd_import(name[level:], package, level)\r\nFile \"C:\\Users\\XXXX\\Documents\\XXXX\\Development\\Application\\WebVersion-Desktop\\XXXX\\XXXX\\venv\\lib\\site-packages\\tensorflow_core\\python_init.py\", line 49, in\r\nfrom tensorflow.python import pywrap_tensorflow\r\nFile \"C:\\Users\\XXXX\\Documents\\XXXX\\Development\\Application\\WebVersion-Desktop\\XXXX\\XXXX\\venv\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in\r\nraise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\nFile \"C:\\Users\\XXXX\\Documents\\XXXX\\Development\\Application\\WebVersion-Desktop\\XXXX\\XXXX\\venv\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in\r\nfrom tensorflow.python.pywrap_tensorflow_internal import *\r\nFile \"C:\\Users\\XXXX\\Documents\\XXXX\\Development\\Application\\WebVersion-Desktop\\XXXXr\\XXXX\\venv\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in\r\n_pywrap_tensorflow_internal = swig_import_helper()\r\nFile \"C:\\Users\\XXXX\\Documents\\XXXX\\Development\\Application\\WebVersion-Desktop\\XXXXXXXX\\venv\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nFile \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\imp.py\", line 242, in load_module\r\nreturn load_dynamic(name, filename, file)\r\nFile \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\imp.py\", line 342, in load_dynamic\r\nreturn _load(spec)\r\n**ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nFailed to load the native TensorFlow runtime.**\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions. Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nProcess finished with exit code 1\r\n\r\nCan someone please give us some help? thanks a lot!\r\n\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.5.0  | 11.2 |\n| 2.4.0  | 11.0 |\n \n| 2.1.0 - 2.3.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "Cuda and Cudnn are correctly installed and added into system path, \r\nwe installed win10 on MacBook, and it is M1 Chip one", "@Johnny-liqiang \r\nAs mentioned in the above comment, please downgrade to cuda 11.0 for tf 2.4 and let us know, if you still face the issue.\r\nBased on the error you may also refer to:#47517,#43915,#46788\r\n\r\n\r\n", "After long time Stackoverflow and debugging , the following solution works for me:\r\nIf you want to run tensorflow on Win10 (VM) installed by parallel, and you Macbook is latest M1 chip,\r\nAnd you came cross DLL problem, maybe try following command:\r\n`pip uninstall tensorflow `and also go to check **venv/lib/sit-packages/** whether or not tensorflow folder deleted completely , make sure deleted it completely, then run command,\r\n\r\n> Bascially, M1 CPU doesn't support AVX instructions, you will get ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed. (Win 10) or ImportError: DLL load failed with error code -1073741795 (Win 7) when using tensorflow official release 1.6.0 and up (pip install tensorflow)\r\n> You can use pip install <filename.whl> which file download from sse2 folder instead of using official AVX binary.\r\n\r\nThanks a lot for this [github](https://github.com/fo40225/tensorflow-windows-wheel)\r\n\r\nThen download 7Zip to unzip the whole file, [Combine_several_zip_files](http://mwiki.gichd.org:8090/IM/Combine_several_zip_files) into whl file\r\nFinally you can use `pip install filename.whl` to install tensorflow!\r\n\r\nCheers ! Solved!\r\n\r\n- someone also suggested not using pip install, but conda install tensorflow, it doesnt work for this system set.:(\r\n- someone also suggested downgrading the u5py version, not work:(\r\n- downgrading cuda not help:(\r\n- **pip install --upgrade tensorflow-gpu==2.2.0** not work :(\r\n\r\nBut if my way not helping you, the above mentioned methods maybe be helpful for your case.!\r\n\r\n`>>> import tensorflow\r\n2021-04-26 15:34:59.397734: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n>>>\r\n`", "@Johnny-liqiang \r\nThank you for the update and information, glad you issue is solved. Please move this to closed status.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48726\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48726\">No</a>\n"]}, {"number": 48725, "title": "Added negative parameter validation to Core/Recurrent/Convolutional Keras layers.", "body": "This PR adds validation and custom errors for negative inputs in various Keras layers throughout the `core`, `convolutional`, and `recurrent` modules.", "comments": ["Ok, I've made the changes as have been requested. Should I add tests for these different cases?"]}, {"number": 48724, "title": "Support for complex weight initializers", "body": "Adds support for complex weights to tf.keras's initializers. I am currently experiencing problems with Bazel on my local machine, so I was unable to run the tests, but only minor changes have been made to them, so they should, ideally, work out of the box.\r\n\r\nAims to support [#48454](https://github.com/tensorflow/tensorflow/issues/48454), [#25218](https://github.com/tensorflow/tensorflow/issues/25218),  in the future\r\nFixes [#17097](https://github.com/tensorflow/tensorflow/issues/17097)", "comments": ["@gbaned Any updates on this?", "/cc @nikitamaia Connect the mentioned ticket/Assign/In progress", "@DarshanDeshpande What is missing to full fix https://github.com/tensorflow/tensorflow/issues/17097 ?", "@bhack It's fully fixed now. I forgot to update the ones initializer in my first commit so I edited the main message to partially fixes but forgot to edit it again when I merged the second commit. I have changed it to fixes. Thanks :)", "Thanks for your first contribution.\r\nWe will assign that ticket to you and we will move it `in progress`.  \r\nIn the meantime If you are interested in other contributions please check the TODO column in https://github.com/orgs/tensorflow/projects/14. \r\nComment on the ticket that you like to work on and we will assign it to you before the PR so that we will not have potentially duplicated coding activities.", "@bhack @gbaned @deeb02 I have fixed the failing tests. Please check and let me know if any other changes are to be made.", "@gbaned @deeb02 Any update yet?", "Hello! I am on the Keras team and am taking a look at this, though am still trying to grok the overall complex number support story for tf.\r\n\r\nCan you explain more how this change would fit into the broader picture for complex support. Is there a particular use case you are trying to enable? And what in particular is broken?\r\n\r\nMade a colab to play around a bit with this. It looks like a number of keras initializers already work fine with complex dtypes, and at a higher level any tf.Variable can be used immediately as a keras weight. If there are gaps with tf.Variable support for complex numbers that might be the place to start, rather than Keras.\r\n\r\nhttps://colab.research.google.com/gist/mattdangerw/8ab4a8fc6217843fa5ab42ad41f75397/keras-complex-initializers.ipynb", "Thanks for the review @mattdangerw! The initial idea for this pull request was to help expand complex number support for tf.keras. \r\n\r\nIf you take a look at this [colab ](https://colab.research.google.com/drive/1zWAvg3KbLUflAvKPPsWaR4dJvqugYL5-?usp=sharing) notebook, you can see that if the `Input` layer is of type tf.complex64/128 then the `Dense` layer weights following it are automatically cast to float (depending on backend.floatxx). If you try to specify a custom initializer, it throws an error saying complex dtype is not supported. A simple fix for this is to pass a dtype parameter which then prevents the overriding/autocast. Initially, when I created this PR, I thought that these errors were because of a lack of support for weight initializers but then it looks like `tensorflow/core/ops` don't support complex numbers for most layers like Convolution (see notebook for a possible implementation example). I am working on creating a couple more PRs to add support for these ops as we speak but till then the addition of proper support for all complex weight initializers to test those new changes would make it a bit easier to work with and will be important once the layers are made compatible. This addition will also help solve the confusion [here](https://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/keras/engine/base_layer.py#L601). The reason for not adding support for complex dtype to `random_op` directly was because the most efficient and logical way to do this is to generate two randomly distributed arrays and parse them as real and imaginary components of a complex array (this [test](https://github.com/tensorflow/tensorflow/issues/17097#issuecomment-477651162) supports my argument). This, I felt, was better if it was done in Python instead of C++ because I thought it could break backward compatibility somewhere that I wasn't sure about. Though I believe that doing it this way makes better sense and does not cause any performance problems, if the team believes that this change should be made explicitly to `random ops`, we can try doing so as well.\r\n\r\nKeras already has optimizer support for complex gradients and so this PR will act as a small step towards achieving complete complex variable support, which is the bigger picture you spoke about :)", "@mattdangerw If there are any thoughts or suggestions regarding the same or if there are any updates related to this PR then please let me know!", "Thanks for the response!\r\n\r\nI guess maybe a better specific question is what do the changes to the ones, zeros and constant intializers aim to accomplish? You can call these today with complex dtypes and they work. You can use these in a Dense layer and they seem to work:\r\n`tf.keras.layers.Dense(40, kernel_initializer='ones', dtype=tf.complex64)`\r\n\r\nI can see the point about a default initializer that is inferred for in `add_weight` not handling complex dtypes (the note you linked), but it looks like really the way this is solving that issue is by supporting complex number with glorot_uniform.\r\n\r\nI am also a little confused at why these changes seem to initialize everything with an equivalent real and imaginary value. That does not seem to be correct--I would expect ones to return `1 + 0i` as it does today. I would also expect random initializers to not constraint the real and imaginary part to be equal, but that could definitely be my own ignorance on the subject matter. Do we have references here?\r\n\r\nFinally, there is a layering issue. We are working to remove all private usage of lower level tf APIs from keras.  `gen_math_ops._complex` would qualify, we should avoid that usage.", "Thanks for the clarification @mattdangerw!\r\n\r\n1. I haven't touched the zeros and constant initializers since they already support complex numbers. Regarding the ones initializer, the distribution of ones should logically be in both real and imaginary (or amplitude and phase components of the weights) instead of just the real domain. That is the reason why the ones initializer was updated in later commits.\r\n\r\n2. The note that I referred to was just to show that currently, by default, the code throws an error if the initializer is purposely set to None. This PR will add support for Glorot, He and Lecun initializers and so you need not worry about it as it will be handled on its own.\r\n\r\n3. You brought up a very important question that I was meaning to ask. I had even mailed about 10-15 members from the Keras and Tensorflow team but got no response. The weight initialization that is implemented follows the intuition that both the real and imaginary components should have same distributions since both are in separate vector spaces. I had done my research on this and I could only find [this](https://arxiv.org/abs/1705.09792v1) paper whose initializer code is present [here](https://github.com/ChihebTrabelsi/deep_complex_networks/blob/f7065cea654c8b2a049c40fcc40fd76817c15096/complexnn/init.py#L202). This implementation uses Rayleigh distribution but I couldn't implement it because the only rayleigh functionality I could find for tensorflow was present in TFP ([here](https://www.tensorflow.org/probability/api_docs/python/tfp/random/rayleigh)) which is a completely separate branch of Tensorflow. Most other papers tend to use zeros or ones or something similar, so I followed the next best implementation according to me which was having the same distributions (uniform/normal) in both domains. About the usage of same seed values for both domains, this can be changed if you want but I'm not sure if it will have any significant improvement in terms of providing better loss convergence.\r\n\r\n4.  I didn't know about the layering issue. Thanks for letting me know! I will change it to `math_ops.complex` instead", "I do not see a good reason why the ones initializer should have different behavior than `tf.ones` with a complex dtype. If you want `1 + i` you could use a constant initializer right? `1 + i` is not even norm one. I think this will surprise people.\r\n\r\nRe point 3) having the same distribution does mean having equal value I think. We should be careful to make sure we are doing the right thing here, but sadly I'm probably not the best person to ask. There is a project out there trying to do this  https://github.com/NEGU93/cvnn/blob/master/cvnn/initializers.py. It looks like some care was taken to handle the limits of the distribution differently, and it looks like will only return a non-zero real component. I will try to ask around to find if someone has done more thinking on this.", "@mattdangerw I have updated the `Ones` initializer and changed to `math_ops.complex` instead of `gen_math_ops` as per your advice. \r\n\r\nThe repository that you linked to has a similar weight initialization implementation, but with different seeds for real and imaginary kernels. Rather than initializing complex-valued weights, the author initializes two real-valued kernels, one for each real and imaginary domains and handles their operations separately. So every time `add_weight` is called, it results in two different-valued real and imaginary domain arrays. This is the current best way of handling complex numbers in tensorflow without changing the C++ ops but is inefficient since it doubles the cost of operations like convolution (which as I had mentioned in my notebook can be done much more efficiently by using complex-valued weights). \r\n\r\nLet me know if you think we should use different seeds for initialization", "Hey @DarshanDeshpande, I wanted to take a step back and ask about the motivation. Is it neural networks use case or something else? Is it to be able to handle special types of inputs for some problem domains? I took a look at #48454, #25218 but still couldn't figure out the main use case.", "@deeb02 Okay so if you're familiar with the signal and audio processing domains, you probably know about the widespread use of complex numbers in the field. I will provide a very basic example to explain the need for this PR:\r\nIf anyone wants to train a ML model for an audio task, whether it be classification or generation based, the usual method is to use a FFT, STFT or wavelet based transformation to convert the data into a time-frequency representation. These methods project the timeseries data into complex domains and this data is usually converted to real spectrograms which causes loss of individuality of phase and magnitude that is irrecoverable. To avoid this, we usually split the STFT output as two real tensors and stack them up, thereby changing their shape to (2, frames, freq bins) instead of the original (frames, freq bins). This is extremely wasteful and feeds the dimensionality curse. Also, as I had mentioned in my conversation with @mattdangerw, the current best way to process this is to use two kernels, one for each component which is computationally expensive. The best and the most efficient way is to use complex numbers directly while training since complex gradients are already supported. But to be able to do this, we will need to expand tensorflow's support for complex numbers and this PR will act as a starting step for that by adding complete weight initializer support, upon which the support for individual layers can be added.", "Thank you for the PR!\r\n\r\nIn general, we want to make sure that use cases that involve complex numbers are *possible* in Keras, but we have no intention to support them *out of the box* given that this is a very niche space.\r\n\r\nBasically, it should be possible for you to easily roll out your own complex-valued initializers and complex-valued layers, and have them work. However, we don't intend to add complex support to existing real-valued initializers and layers.\r\n\r\nIn the case of this PR, I recommend that you create your own `Initializer` subclasses that return complex values (we won't add them to the core API). If there's a significant amount of interest in such initializers, then we can consider making them available in the TF Addons package.", "/cc @nikitamaia we have still the other two mentioned issues in https://github.com/tensorflow/tensorflow/pull/48724#issue-622246567 as `contribution welcome`.\r\nProbably it Is better to evaluate/close these before another PR will land in the repo."]}, {"number": 48723, "title": "Updated dso_loader.cc, removed redundant log", "body": "Removed `VLOG(1) << \"Successfully opened dynamic library \" << filename;` because it was annoying. No reason to log loading libcudart successfully. \r\n\r\nFix for issue referenced in Issue #45214 ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48723) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "#45214 should be fixed after https://github.com/tensorflow/tensorflow/commit/73f56e99958893bbe536e8282688cfc6080d4034 already.\r\n\r\nThe original LOG(INFO) gives the \"I\" in the original report. After that commit you'd only get the logging if you set vlog higher than the default.\r\n\r\nI think having the (opt-in) vlog is useful, since if it's enabled for that file someone is probably debugging an issue with dso loading. So maybe we should try it this way for a bit?", "@krivokuca Can you please check @allenlavoie's comments and keep us posted ? Thanks!", "Please leave a comment / let me know if it's still an issue. Looks like it won't be."]}, {"number": 48722, "title": "Obsolete NVIDIA driver check for CUDA 11.2", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Ubuntu 20.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.5.0\r\n- Python version: 3.8\r\n- CUDA/cuDNN version: 11.2/8.1.1 (Docker image: `tensorflow/tensorflow:2.5.0-gpu`)\r\n\r\n**Describe the current behavior**\r\n\r\n```bash\r\ndocker run --gpus all --entrypoint python tensorflow/tensorflow:2.5.0-gpu -c \"import tensorflow as tf; tf.zeros([50])\"\r\n```\r\n\r\nWhen running the Docker command above and GPU driver 455 installed on the host, the following error message is logged:\r\n\r\n> `E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected`\r\n> `I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: f7b7a8f85319`\r\n> `I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: f7b7a8f85319`\r\n> `I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 460.73.1`\r\n> `I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 455.32.0`\r\n> `E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 455.32.0 does not match DSO version 460.73.1 -- cannot find working devices in this configuration`\r\n\r\nHowever, this driver version check is no longer accurate. Starting with CUDA 11.1, minor CUDA versions no longer require a driver update. So all CUDA 11.x versions are compatible with driver versions >= 450.\r\n\r\nThis change is referred as the \"CUDA Enhanced Compatibility\" in the [CUDA Compatibility](https://docs.nvidia.com/deploy/cuda-compatibility/index.html) guide.\r\n\r\n**Describe the expected behavior**\r\n\r\nTensorFlow binaries should comply with the [CUDA Compatibility](https://docs.nvidia.com/deploy/cuda-compatibility/index.html) guide. So TensorFlow with CUDA 11.2 should work with GPU drivers >= 450 instead of >= 460.\r\n\r\nSee in particular the Table 1 that specifies the driver requirement for each CUDA version:\r\n\r\nCUDA Toolkit | Linux x86_64 Required Driver Version\r\n-- | --\r\nCUDA 11.3 | >= 450.80.02\r\nCUDA 11.2 | >= 450.80.02\r\nCUDA 11.1 (11.1.0) | >= 450.80.02\r\nCUDA 11.0 (11.0.3) | >= 450.36.06\r\nCUDA 10.2 (10.2.89) | >= 440.33\r\nCUDA 10.1 (10.1.105) | >= 418.39\r\nCUDA 10.0 (10.0.130) | >= 410.48", "comments": ["I updated the issue to clarify it was executed in a Docker container and the logs also report an error from `cuInit`.", "I believe we print the warning about driver/DSO version mismatch only if `cuInit` fails.  I.e. you're seeing the warning because there was a definite failure, though the message could be incorrect about the root cause.\r\n\r\nI don't see why `cuInit` itself fails -- as you said, by the docs, 11.2 should work fine with 455.  Do things work ok if you upgrade the host driver to 460?", "@sanjoy I confirm that it works with driver 460 but not 455. I tested with the official TensorFlow 2.5.0 GPU Docker image:\r\n\r\n```\r\ndocker run --gpus all --entrypoint python tensorflow/tensorflow:2.5.0-gpu -c \"import tensorflow as tf; tf.zeros([50])\"\r\n```\r\n\r\nWith driver 455.45.01 the GPU is not detected and the driver mismatch is logged:\r\n\r\n```\r\nI tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\r\nI tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 538e7c2fb724\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 538e7c2fb724\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 460.73.1\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 455.45.1\r\nE tensorflow/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 455.45.1 does not match DSO version 460.73.1 -- cannot find working devices in this configuration\r\n```\r\n\r\nWith driver 460.73.01 the GPU is successfully detected:\r\n\r\n```\r\nI tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\r\nI tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \r\npciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\r\n```", "@nluehr Any idea (TLDR: `cuInit` fails with CUDA 11.2 and 455 and we don't know why).", "Note that we only mentioned driver 455 here, but there is probably the same error for all drivers in range >=450,<460 even though they should be compatible with CUDA 11.2.", "I am unable to reproduce the failure on my machine with Ubuntu 20.04 and 455.45.01 running the tensorflow/tensorflow:2.5.0-gpus Docker image. On a failing system what is the output of `nvidia-smi` within the container?", "Thanks for the feedback. Here's the `nvidia-smi` output within the container:\r\n\r\n```text\r\n$ docker run --gpus all --entrypoint nvidia-smi tensorflow/tensorflow:2.5.0-gpu\r\nThu Jun 17 08:09:26 2021 \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 455.45.01    Driver Version: 455.45.01    CUDA Version: 11.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\r\n| N/A   24C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nFor reference, this was run on a fresh `g4dn.xlarge` EC2 instance with \"Deep Learning Base AMI (Ubuntu 18.04) Version 39.0\". I just ran the following initialization steps before running the Docker commands:\r\n\r\n* Uninstall current GPU driver\r\n* Reboot\r\n* Install GPU driver 455.45.01\r\n* Reboot", "This is probably a nvidia-docker issue, not a TensorFlow issue. It works after manually changing the `libcuda.so` simlink within the container:\r\n\r\n```bash\r\nln -sf /usr/lib/x86_64-linux-gnu/libcuda.so.455.45.01 /usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48722\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48722\">No</a>\n", "Note: extending the TensorFlow Docker image and removing the package `cuda-compat-11-2` is also a way to workaround this issue."]}, {"number": 48721, "title": "tf.RaggedTensor.bounding_shape: out_type argument is ignored", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.5.0-rc0-36-g0d1805aede0 2.5.0-rc1\r\n- Python version: 3.9.2\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\n`RaggedTensor.bounding_shape` has an argument out_type which is supposed to control the type of the output tensor. But it does not seem to have any effect:\r\n```\r\n>>> tf.RaggedTensor.from_tensor([[1,2,3]]).bounding_shape(1, out_type=tf.int32)  # output is not int32\r\n<tf.Tensor: shape=(), dtype=int64, numpy=3>\r\n```\r\n\r\n**Describe the expected behavior**\r\n```\r\n>>> tf.RaggedTensor.from_tensor([[1,2,3]]).bounding_shape(1, out_type=tf.int32)\r\n<tf.Tensor: shape=(), dtype=int32, numpy=3>\r\n```\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\n>>> tf.RaggedTensor.from_tensor([[1,2,3]]).bounding_shape(1, out_type=tf.int32)\r\n<tf.Tensor: shape=(), dtype=int64, numpy=3>\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Was able to reproduce the issue with TF v2.4.1, TF v2.5.0-rc1 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/9640e7286e9edde8f7a32339625dc7cb/48721.ipynb#scrollTo=VGMsyaf7P_J4). Thanks!", "@Andreas5739738, I tried to run your code on `Colab` using `TF v2.6` and `tf-nightly` and this issue was resolved. Please find the [gist](https://colab.research.google.com/gist/chunduriv/91da87b9e05efcb3eba7d0c6456ebefa/48721.ipynb) here for reference. Please feel free to close the issue if it is resolved ? Thanks! ", "Thank you! Confirming that this is resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48721\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48721\">No</a>\n"]}, {"number": 48718, "title": "IOS version of TensorFlowLiteC expose symbol(s) not found error", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nWhen attempting to use TensorFlow Lite on iOS  as the documentation, the linker raises an error that it found:\r\n\r\n```\r\nUndefined symbols for architecture arm64:\r\n  \"_OBJC_CLASS_$_NSProcessInfo\", referenced from:\r\n      objc-class-ref in TensorFlowLiteC\r\n  \"_CFBundleGetVersionNumber\", referenced from:\r\n      l5794 in TensorFlowLiteC\r\n  \"___CFConstantStringClassReference\", referenced from:\r\n      CFString in TensorFlowLiteC\r\n  \"_objc_release\", referenced from:\r\n      l4515 in TensorFlowLiteC\r\n  \"_objc_msgSend\", referenced from:\r\n      l4506 in TensorFlowLiteC\r\nld: symbol(s) not found for architecture arm64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n```\r\n\r\nI build my app with cmake with cmakelist.txt and include \"tensorflow/lite/c/c_api.h\" in model.h as:\r\n```\r\ncmake_minimum_required(VERSION 3.5)\r\nproject(myapp)\r\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -O2 -fPIC\")\r\ninclude_directories(${CMAKE_SOURCE_DIR}/tflib/include)\r\nlink_directories(${CMAKE_SOURCE_DIR}/tflib/lib)\r\nadd_library(mylib_ios SHAREDsrc/model.cc src/model.h)\r\nfind_library(TFLiteC NAMES TensorFlowLiteC HINTS ${CMAKE_SOURCE_DIR}/tflib/lib)\r\ntarget_link_libraries(mylib_ios ${TFLiteC})\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@Mrlyk423 \r\n\r\nCould you please fill the issue template\r\nIn order to expedite the trouble-shooting process, could you please provide the following details\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version:\r\nPython version:\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory\r\n\r\nand the exact sequence of commands / steps that you executed before running into the problem. Thanks!\r\n\r\n\r\n", "@UsharaniPagadala  Thanks! Below is my system env.\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.7 (19H2)\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version: both r2.5 and commit 1d4451e42c6ce44bd9ebb89d117636e4960df7c3 of the master branch have the problem\r\nPython version: 3.7.6\r\nInstalled using virtualenv? pip? conda?: conda\r\nBazel version (if compiling from source): 3.7.2\r\nGCC/Compiler version (if compiling from source):\r\n      Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include/c++/4.2.1\r\n      Apple clang version 12.0.0 (clang-1200.0.32.29)\r\n      Target: x86_64-apple-darwin19.6.0\r\n      Thread model: posi\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory\uff1a Intel UHD Graphics 630 1536 MB (No compile with Cuda support)", " @yyoon @thaink could you take a look?", "@UsharaniPagadala Can you share the exact command used for building the iOS?", "> @UsharaniPagadala Can you share the exact command used for building the iOS?\r\n\r\n@yyoon The command for build tflite is \r\n`bazel build --config=ios_fat -c opt \\  //tensorflow/lite/ios:TensorFlowLiteC_framework` \r\nAnd the command for build my own lib is: \r\n```\r\ncmake .. -G Xcode -DCMAKE_TOOLCHAIN_FILE=../ios.toolchain.cmake -DPLATFORM=SIMULATOR64\r\ncmake --build . --config Release\r\n```", "Can you try building it for `-DPLATFORM=OS64` in the same way and see if it works?\r\nThe `TensorFlowLiteC.framework` is a multi-arch framework, but it does not contain the recent arm64 simulator platform.", "@yyoon I have also tried to build it for `-DPLATFORM=OS64` but the problem still exists. \r\n\r\nMeanwhile, I also tried the command `bazel build --config=ios_x86_64 -c opt //tensorflow/lite/c:tensorflowlite_c` to directly build it similar to the darwin/Linux/Android, and got a libtensorflowlite_c.dylib file. My app can build with this file in the arm64 simulator platform. But it fails in the TensorFlowLiteC.framework.", "@yyoon I finally add Foundation.framework and CoreFoundation.framework when compiling my own app, and it seems work. The usage way is different from the dynamic lib libtensorflowlite_c.dylib.", "I see. Usually those frameworks are included by default when you create an Xcode project, but maybe they need to be manually added when using CMake. Good to know.\r\n\r\nIs there anything else we can help you with? Otherwise, could you close the issue?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48718\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48718\">No</a>\n"]}, {"number": 48717, "title": "boosted tree classifier h-params", "body": "<em>I am working with the [boosted tree classifier](https://github.com/tensorflow/estimator/blob/781c0d30c6bf100aa174591dd97cb70fc39d294d/tensorflow_estimator/python/estimator/canned/boosted_trees.py#L1933)</em>\r\n\r\n**System information**\r\n- TensorFlow installed from: pip\r\n- TensorFlow version: 2.4.1\r\n- Python version: 3.6.5\r\n\r\n\r\n**Describe the current behavior**\r\nThere is a hyper-parameter in the model called `n_trees` which its \r\ndefault value is 100, also, there is a step parameter `max_step=100` in the training method.\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\nBased on the gradient boosting classifier, `n_trees` refers to the number of trees, which we add at each boosting iteration.\r\nThe question is, what is the difference between these two parameters in the model?\r\n\r\n**Standalone code to reproduce the issue**\r\nI tried to plot the boosting accuracy and have the accuracy by adding a new tree at each iteration [here](https://github.com/samanemami/TFBoostedTree/blob/main/examples/Accuracy_plot.ipynb). \r\nI used `max_step` to have boosting iteration, so what is the **role of `max_step`** (in train method)?\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n[Boosted tree](https://github.com/tensorflow/estimator/blob/781c0d30c6bf100aa174591dd97cb70fc39d294d/tensorflow_estimator/python/estimator/canned/boosted_trees.py#L1933).\r\n[train method](https://github.com/tensorflow/estimator/blob/781c0d30c6bf100aa174591dd97cb70fc39d294d/tensorflow_estimator/python/estimator/canned/boosted_trees_test.py#L403).", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48717\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48717\">No</a>\n"]}, {"number": 48716, "title": "Fix invalid string formatting in merge.py", "body": "", "comments": ["@fvollmer Can you please check @deeb02's comments and keep us posted ? Thanks!", "This is not a stylistic change. The variable `input_shape` is a tuple and for this reason it can cause the error `TypeError: not all arguments converted during string formatting`. Here a simple python example to demonstrate the difference:\r\n\r\n```python\r\nmy_tuple = (1, 2, 3)\r\nmy_string = (\"abc %s\" % (my_tuple,))  # works fine\r\nmy_string = (\"abc %s\" % my_tuple)  # TypeError: not all arguments converted during string formatting\r\n```\r\n", "Thank you for the clarification. Makes sense.", "What happend here? Seems like my commit f444f868df477470747305debd8bccc757c9e50f was rolled back by the tensorflow gardener 980ed0664ce73bcb1ba4f7edcef3706679a3e482? What was the reason?"]}, {"number": 48715, "title": "style(log) : pretty up a debug log string", "body": "Here is a example, it changes from\r\n\"Available kernels for Castare   device='CPU'\"\r\nto\r\n\"Available kernels for Cast are  device='CPU'\"", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48715) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent"]}, {"number": 48714, "title": "Adding a utility to penalize majority class pixels in the Segmentation tutorial", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/tutorials/images/segmentation\r\n\r\n## Description of issue (what needs changing):\r\n\r\nIt's not really an issue, a suggestion rather. \r\n\r\n### Clear description\r\n\r\nSemantic segmentation datasets can be highly imbalanced meaning that particular class pixels can be present more inside images than that of other classes. Since segmentation problems can be treated as per-pixel classification problems we can deal with the imbalance problem by weighing the loss function to account for this. It's a simple and elegant way to deal with this problem. Other solutions include always ensuring that a batch of samples (during training) always contain some proportion (which is prefixed) of positive classes. \r\n\r\nHowever, TensorFlow does not yet support the `class_weight` argument in `model.fit()` for targets that are 3D (for segmentation problems, we are essentially predicting a map of shape `[batch_size, height, width, nb_channels]`). One way to get around this problem is to use `sample_weight` instead. But then again, it's not very clear as to how to do that properly particularly with `tf.data` pipelines. \r\n\r\nMultiple folks have tried several hacks to get around this problem but it keeps coming back (see [here](https://github.com/keras-team/keras/issues/3653)). Therefore, I think the tutorial under question is a perfect opportunity to demonstrate the use case. \r\n\r\nCc: @MarkDaoust ", "comments": ["That sounds like a reasonable idea at face value. Why not submit a PR with that change?", "Since I don't have the workaround yet, I wanted to pass this on to the team responsible for authoring the tutorial. If that was not the case I would have submitted the PR. ", "I closed it as a Keras issue, but it may make sense as a documentation issue.", "I'll take  look. It seems easy to implement a workaround.", "Looking forward to it. FWIW, I don't think it's a docs issue. In fact, the tutorial is very clear and well written. It's rather an incremental suggestion to accomplish a use case. ", "@MarkDaoust I think the thread is alive since 2016 at https://github.com/keras-team/keras/issues/3653", "^ which is what I mentioned in my initial post as well. ", "I wonder why the \"Fix:\" comment on the commit didn't close the bug.\r\n\r\n", "@MarkDaoust  Cross repositories is not in the format: https://github.blog/2013-03-18-closing-issues-across-repositories/?", "As I don't see your direct commits in this repo other than an old commit in 2015 but with another email.  Do you have push permission here?", "Ok I've checked with your current email and all seems fine. Very strange.", "Yeah, I think you're right that it's just a permissions issue.\r\n\r\nIt may have something to do with all the bots moving commits around.", "Yes probably is ` MarkDaoust authored and Copybara-Service`", "We don't have that commiter bot here `git log --committer=\"Copybara-Service\"`. it is used in the Docs repository. This is why Github doesn't close your cross ticket as the permission check it is related to the committer not to the author (you).", "@MarkDaoust thank you for your contribution for the section:\r\n\r\nhttps://www.tensorflow.org/tutorials/images/segmentation#optional_imbalanced_classes_and_class_weights\r\n\r\nWe now at least have a section in the official docs providing a way to get around this important problem. "]}, {"number": 48713, "title": "`Adadelta` optimizer throws error on GPU in apply_gradients", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: \r\n- GPU model and memory:\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport random\r\n\r\n\r\ndef test_optimizer(optimizer):\r\n  class Layer1(tf.keras.layers.Layer):\r\n\r\n      def __init__(self, shape):\r\n          super(Layer1, self).__init__()\r\n          self.weight1 = self.add_weight(shape=shape)\r\n          \r\n      def call(self, inputs):\r\n          return tf.gather(self.weight1, axis=0, indices=inputs)\r\n\r\n\r\n  x = tf.keras.Input(shape=[1,])\r\n  ind = tf.cast(x, \"int64\")\r\n  y = Layer1(shape=(10, 2))(ind)\r\n  model = tf.keras.Model(x, y)\r\n  model.compile(optimizer=optimizer, loss='mse')\r\n\r\n  x_data = np.array([[random.randint(0, 9),],])\r\n\r\n  out = model(x_data)\r\n  y_data = np.zeros(out.shape)\r\n  with tf.GradientTape() as tape:\r\n    y_pred = model(x_data)\r\n    loss = tf.keras.losses.MeanSquaredError()(y_data, y_pred)\r\n\r\n    gradients = tape.gradient(loss, model.trainable_weights)\r\n\r\n  optimizer.apply_gradients(zip(gradients, model.trainable_weights))\r\n\r\ntest_optimizer(tf.optimizers.Adam())\r\nprint('Adam Passed')\r\ntest_optimizer(tf.optimizers.Adadelta())\r\n\r\n```\r\n\r\n**Outputs / logs** \r\n```\r\nAdam Passed\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-16-69f396904fc3> in <module>()\r\n      1 test_optimizer(tf.optimizers.Adam())\r\n      2 print('Adam Passed')\r\n----> 3 test_optimizer(tf.optimizers.Adadelta())\r\n      4 print('Adadelta Passed')\r\n\r\n16 frames\r\n<ipython-input-13-fbaf6e8a2eb4> in test_optimizer(optimizer)\r\n     31     gradients = tape.gradient(loss, model.trainable_weights)\r\n     32 \r\n---> 33   optimizer.apply_gradients(zip(gradients, model.trainable_weights))\r\n\r\n\r\nNotFoundError: No registered 'ResourceSparseApplyAdadelta' OpKernel for 'GPU' devices compatible with node {{node ResourceSparseApplyAdadelta}}\r\n\t.  Registered:  device='CPU'; T in [DT_COMPLEX128]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_COMPLEX128]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_COMPLEX64]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_COMPLEX64]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_BFLOAT16]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_BFLOAT16]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT32]\r\n [Op:ResourceSparseApplyAdadelta]\r\n```\r\n\r\n**Describe the current behavior**\r\n\r\nI am using `Adadelta` optimizer for training on GPU. During backpropagation `optimizer.apply_gradients()` I encounter this `NotFoundError`: No registered 'ResourceSparseApplyAdadelta' OpKernel for 'GPU' devices compatible with node {{node ResourceSparseApplyAdadelta}}.\r\n\r\nHowever, if `Adam` optimizer is used, no error occurs.\r\n\r\n\r\n**Describe the expected behavior**\r\nThe `Adadelta` optimizer works on CPU. I expect `Adamdelta` optimizer to work also on GPU.\r\n\r\n", "comments": ["There are some relevant issues, e.g. https://github.com/tensorflow/tensorflow/issues/38779, but I didn't find any solution to this problem.", "@lugalUrim \r\n\r\nI reproduced the issue in tf-nightly did not face any error.Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/b7b992805d254cc67d9cc69fb4bfb279/-48713.ipynb) here.Thanks", "@UsharaniPagadala Yes, the error is in tensorflow 2.4.1, and it seems to be fixed in tf 2.5 and tf-nightly.\r\n\r\nThere is a small bug in the code (should be `random.randint(0, 9)` instead of `random.randint(0, 10)`), I have edited the issue description to fix it. ", "Currently Google colab hosts cuda 11.0 where as TF 2.5 and nightlies support cuda 11.2 so I suspect that in those cases the computation is falling back on cpu. We may want to test it with cuda 11.2 to be sure.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48713\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48713\">No</a>\n"]}, {"number": 48712, "title": "Issue: Invalid argument:  assertion failed: [len requires non-zero rank, got 0]", "body": "I am experiencing the below issue on random occurrences.\r\n\r\nI checked my data and it looks good. Please advise, as this is a production issue.\r\n\r\nThank you.\r\n\r\nNektarios\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Custom**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows and Linux**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **2.3.1 and 2.4.1** \r\n- Python version: **3.8**\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: **11.0/8.05**\r\n- GPU model and memory: **NVIDIA Titan Volta**\r\n- \r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Development\\Python\\Python386\\lib\\contextlib.py\", line 131, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"C:\\Development\\Python\\Python386\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2825, in variable_creator_scope\r\n    yield\r\n  File \"C:\\Development\\Python\\Python386\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1100, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n  File \"C:\\Development\\Python\\Python386\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\Development\\Python\\Python386\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 855, in _call\r\n    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n  File \"C:\\Development\\Python\\Python386\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2942, in __call__\r\n    return graph_function._call_flat(\r\n  File \"C:\\Development\\Python\\Python386\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1918, in _call_flat\r\n    return self._build_call_outputs(self._inference_function.call(\r\n  File \"C:\\Development\\Python\\Python386\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 555, in call\r\n    outputs = execute.execute(\r\n  File \"C:\\Development\\Python\\Python386\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument:  assertion failed: [len requires non-zero rank, got 0]\r\n\t [[{{node cond/else/_1157/cond/Assert/Assert}}]]\r\n\t [[cond_1/pivot_f/_1168/_61]]\r\n  (1) Invalid argument:  assertion failed: [len requires non-zero rank, got 0]\r\n\t [[{{node cond/else/_1157/cond/Assert/Assert}}]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_110455]\r\nFunction call stack:\r\ntrain_function -> train_function\r\n \r\n\r\n```\r\n\r\n<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@nectario ,\r\n\r\nIn order to reproduce the issue reported here, could you please provide the complete code and the dataset you are using. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48712\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48712\">No</a>\n"]}, {"number": 48711, "title": "Failed to load the native TensorFlow runtime.", "body": "### System information\r\n\r\n-   **OS Platform and Distribution (macOS 10.15.7)**:\r\n-   **TensorFlow installed from (pip3 install tensorflow)**:\r\n-   **TensorFlow version (2.4.1)**:\r\n-   **Python version(3.8)**:\r\n\r\n### Describe the problem\r\nI tried install and re-install tensorflow many times. Including manually remove all the package folders. But I still could not import tensorflow in my terminal.\r\n\r\n### Source code / logs\r\nimport tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"/Users/zmengfan/Library/Python/3.8/lib/python/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: dlopen(/Users/zmengfan/Library/Python/3.8/lib/python/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _PyThread_tss_alloc\r\n  Referenced from: /Users/zmengfan/Library/Python/3.8/lib/python/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n  Expected in: flat namespace\r\n in /Users/zmengfan/Library/Python/3.8/lib/python/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/zmengfan/Library/Python/3.8/lib/python/site-packages/tensorflow/__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"/Users/zmengfan/Library/Python/3.8/lib/python/site-packages/tensorflow/python/__init__.py\", line 39, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n  File \"/Users/zmengfan/Library/Python/3.8/lib/python/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/Users/zmengfan/Library/Python/3.8/lib/python/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: dlopen(/Users/zmengfan/Library/Python/3.8/lib/python/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _PyThread_tss_alloc\r\n  Referenced from: /Users/zmengfan/Library/Python/3.8/lib/python/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n  Expected in: flat namespace\r\n in /Users/zmengfan/Library/Python/3.8/lib/python/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["> Symbol not found: _PyThread_tss_alloc\r\n\r\nThat message looks suspicious, like there's problem with pip pulling wrong versions. I have been running TF 2.4.1 on macOS using the [conda](https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/) virtual environment. Is that a possibility for you to try instead?", "I installed [miniconda](https://docs.conda.io/en/latest/miniconda.html), and it works.", "@Zhang-Mengfan  Glad to hear that issue is resolved for you. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48711\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48711\">No</a>\n"]}, {"number": 48710, "title": "Python model to javascript model always return same prediction", "body": "Hi and thanks in advance,\r\n\r\nI create my own model in python and I get a 93% of accuracy,\r\nthen I convert my model to a js model using the command \"tensorflowjs_converter --input_format keras customModel_gs.h5 model/\", \r\n\r\nI add the model file to my angular project and then I try this:\r\n![imagen](https://user-images.githubusercontent.com/43205522/115792840-e2dc3f80-a3c2-11eb-80a7-0a3d9f0e7861.png)\r\n\r\nThe model charge I can do a summary all is correct except the predition, doesn't care the img a upload, always return the same number 2.\r\n\r\nMy input_shape is a null,128,128,1 (grayscale), I have 4 classes\r\n![imagen](https://user-images.githubusercontent.com/43205522/115793218-a78e4080-a3c3-11eb-9df2-20f3388335dc.png)\r\n![imagen](https://user-images.githubusercontent.com/43205522/115793289-c391e200-a3c3-11eb-9b31-19ec655ea870.png)\r\n\r\n", "comments": ["@danielreyes9756 \r\n\r\nCould you please fill the issue template\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the following details\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version:\r\nPython version:\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory\r\nand the exact sequence of commands or a colab gist/ steps that you executed before running into the problem. \r\nThanks!\r\n\r\n\r\n", "Yeah of course thanks,\r\nOS Windows 10\r\nissue happens on PC device, im working on angular 11\r\nmodel crearted on google colaboratory so:\r\nTensorFlow default version and intallation of google colaboratory \r\nSame with Python 3.7.10 default version I didn't install it\r\nBazel version (I don't know what is Bazel)\r\nNot need CUDA\r\nGPU model and memory, I workerd with the colab gpu acelerator but my own GPU is a GTX 1650 4GB GDDR6\r\n\r\nIn colab the only command I use where \r\nmodel.save(\"route/(customModel_gs.h5\")\r\n!pip install tensorflowjs\r\n!tensorflowjs_converter --input_format keras customModel_gs.h5 model/\",", "@danielreyes9756 \r\nAs this is tfjs issue, could you please open this in the tfjs repo and move this to closed status.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48710\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48710\">No</a>\n"]}, {"number": 48709, "title": "Update version numbers for TensorFlow 2.5.0-rc2", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 5 -> 5\nPatch: 0 -> 0\n\nNo lingering old version strings \"2.5.0-rc1\" found in source directory \n\"tensorflow/\". Good.\nNo lingering old version strings \"2.5.0rc1\" found in source directory \n\"tensorflow/\". Good.\n```", "comments": []}, {"number": 48707, "title": "[CHerryPick:r2.5] Keras Training API: Make sure to update the stateful metrics in the Progbar once the MetricsContainer is built so the metrics are not averaged over an epoch.", "body": "Fixes https://github.com/tensorflow/tensorflow/issues/47099\n\nUnit test element courtesy of the reporter of https://github.com/tensorflow/tensorflow/issues/47099.\n\nPiperOrigin-RevId: 369572207\nChange-Id: I308fcccac5164ab92a70e74d27ecd0f0cbba275a", "comments": []}, {"number": 48706, "title": "Iterative horizontal fusion.", "body": "1. Extend horizontal fusion to support non-fusion instructions.\r\n2. Enable iterative optimization for horizontal fusion. After each iteration,\r\n   new horizontal fusion opportunities are exposed because the producers to\r\n   the previously generated horizontally fused instructions will become\r\n   fusion candidates. See `IterativeHorizontalFusion` in the unittest as an example.", "comments": ["@cheshire could you help to review this PR? Thanks!", "BTW I'm currently working on trying to change the calling convention to allow more than N arguments for calls. If successful, that should enable a lot more horizontal fusion opportunities (in many cases I have seen it hits the # of arguments to the kernel boundary)", "> BTW I'm currently working on trying to change the calling convention to allow more than N arguments for calls. If successful, that should enable a lot more horizontal fusion opportunities (in many cases I have seen it hits the # of arguments to the kernel boundary)\r\n\r\nCool! As it is a limitation in the CUDA kernel signature, (wondering if) are you going to pack the arguments into a struct and pass the struct?", "I've addressed the review comments, please help to take a look again. Thanks!", "> wondering if) are you going to pack the arguments into a struct and pass the struct?\r\n\r\nThis would not work, as the limit is on the argument size and not the number of arguments. I'm experimenting with passing the buffer table directly instead."]}, {"number": 48705, "title": "[tf.data] graduate experimental save and load APIs to tf.data", "body": "UPDATED: This PR graduates the `tf.data.experimental.save` and `tf.data.experimental.load` APIs into `tf.data.Dataset` by making the following changes:\r\n\r\n- [x] Adds the deprecation decorator for the experimental API and export the new API.\r\n- [x] Adds the necessary `save` and `load` methods to `DatasetV2` class.\r\n- [x] Updates example in documentation with new API.\r\n- [x] Regenerate golden API's.\r\n- [x] Moved and updated the `io_test` target from experimental/kernel_tests to kernel_tests\r\n- [x] Updated the RELEASE.md file\r\n\r\nTEST LOG\r\n```\r\nINFO: Build completed successfully, 349 total actions\r\n//tensorflow/python/data/kernel_tests:io_test                            PASSED in 1.9s\r\n```\r\n\r\ncc: @jsimsa ", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48705) for more info**.\n\n<!-- need_author_cla -->", "@jsimsa made the changes. PTAL.", "@kvignesh1420 upon further review, I think we need to let these operations incubate in experimental longer. In particular, the operations do not support checkpointing their progress, which is something that should be implemented before graduating them out of the experimental namespace.", "@kvignesh1420 Can you please resolve conflicts? Thanks!", "@jsimsa I see. We can come back to this after we graduate the other APIs then. Will close this PR as of now.\r\ncc: @gbaned "]}, {"number": 48704, "title": " Error printed when calling tf.data.experimental.load", "body": "**System information**\r\n- OS Platform:  WSL2 Linux Ubuntu 20.04\r\n- TensorFlow version: 2.5.0-rc1\r\n- Python version: 3.9\r\n- CUDA/cuDNN version: 11.2 \r\n- GPU model and memory: RTX 3090\r\n\r\n**Describe the current behavior**\r\n\r\nWhen calling ` tf.data.experimental.load` the bellow error is printed, however, the dataset appears to load without issue\r\n\r\n`Unimplemented: Cannot merge options for dataset of type LoadDataset, because the dataset does not implement 'InputDatasets'.`\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nNo error message should be printed\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\nds = tf.data.Dataset.from_tensor_slices([1, 2, 3])\r\nfor i in ds:\r\n    print(i)\r\n\r\ntf.data.experimental.save(ds, \"/mnt/d/load_issue.dataset\", compression=\"GZIP\")\r\nload_ds = tf.data.experimental.load(\"/mnt/d/load_issue.dataset\", compression=\"GZIP\")\r\n\r\nfor e in load_ds:\r\n    print(e)\r\n```\r\n", "comments": ["It is solved on master. Can you try with `pip install tf-nightly`? Thanks", "@bhack, ahh looks like It's fixed in the nightly builds. I will close the issue!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48704\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48704\">No</a>\n", "Did the fix actually land in the TensorFlow 2.5.0 stable?\r\n\r\nI'm still experiencing this error message with TensorFlow. It looks like [the commit solving the problem](https://github.com/tensorflow/tensorflow/commit/d96b20aa303b9a90d407cd80b94288626bd8e22e) missed the v2.5.0 release and landed in v2.6.0-rc0.\r\n\r\nWould it be possible to cherry-pick the commit into a patch release for the 2.5.x branch?", "Patch releases are only for security issues."]}, {"number": 48702, "title": "About biLSTM model  different sequence lenght use padding & mask.", "body": "About \u300cVariable Length Sequence Support for CUDNN LSTM #23269\u300d\r\n\r\ni have three question about biLSTM model have different sequence lenght that i will use padding & mask.\r\n\r\n>In 2.0 alpha, the normal keras.layer.LSTM will use cudnn kernel if possible, so you don't need to use keras.layers.CuDNNLSTM layer any more.\r\n\r\n(1)So in tf2 I can use lstm  don't need to use keras.layers.CuDNNLSTM layer,but why is the GPU running speed different tf2 LSTM slow then tf1 CuDNNLSTM?\r\n\r\n> The cudnn kernel only support seqence_length parameter, so that it can skip the padding data on the tail. If your data has padding at the beginning of the sequence (left padded), then cudnn kernel will not be able to work correctly in this situation. The comment here is trying to point out that specific use case.\r\n\r\n (2)So if I use  tf2  layer\u2192 tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM ....),input data already padding on the tail,then cudnn kernel will be able to work correctly in this situation? But how can i prove it about masked,about model runing loss not consider padded location  ? \r\n\r\n(3)If padding data then model output result I need to drop padding location  right? ", "comments": ["@BeccaHuang ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the TensorFlow version,the  complete code and the dataset you are using. Thanks!", "@BeccaHuang ,\r\n\r\nPlease share all the dependencies to replicate the issue or share a colab gist with the reported error.\r\n\r\nThanks!", "Thank you Tilakrayal. \r\nI don't have error about code,but i just want to know if add mask layer how to run in model....\r\nabout Variable Length Sequence Support for CUDNN LSTM #23269\r\nin #23269 say if padding in tail  will be able to work correctly in this situation?", "\r\n>  In 2.0 alpha, the normal keras.layer.LSTM will use cudnn kernel if possible, so you don't need to use keras.layers.CuDNNLSTM layer any more.\r\n (1)So in tf2 I can use lstm don't need to use keras.layers.CuDNNLSTM layer,but why is the GPU running speed different tf2 LSTM slow then tf1 CuDNNLSTM?\r\n\r\nFor above question, you need to make sure the configuration is correct for using `tf.keras.layer.LSTM` layer. Since you are experiencing slower performance we should first verify if the gpu setup is done correctly.\r\nMake sure you have compatible cuda, cudnn and nvidia versions installed and the system recognizes gpu.\r\nYou may refer this [table](https://www.tensorflow.org/install/source#gpu) to know more.\r\nIf you can test with later TF versions such as 2.4 or 2.5 that will be great since there are high chances that performance must have improved since 2.0 alpha.\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hi @ymodak,\r\nthanks you vary much about you reply\r\nI will print(\"GPU Available: \", tf.test.is_gpu_available()) and  print(tf.__version__) to check\r\n\r\nGPU Available:  True\r\ntf version: 2.3.1\r\ncuda:11.1\r\n\r\ni need to change version? or i need to check my model layer,\r\nbut i run the same layer in tf1 cudnnLSTM  and tf2 LSTM run speed different tf1 is better.\r\n", "For TF 2.3.1 you should have cuda 10.1 installed for TF to recognize your gpu else it will fallback on cpu.\r\nYou may refer this table https://www.tensorflow.org/install/source#gpu", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hi @ymodak,\r\nthanks you vary much about you reply,\r\nalthough my system version does not match but test GPU available: TRUE,\r\nsee the system still use GPU run. Still compare I recommend that I use the corresponding version?\r\nor can run basically effectiveness does not differ too much?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48702\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48702\">No</a>\n"]}, {"number": 48701, "title": "Function used in many augmentations (convert_image_dtype) has an issue", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab default\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Colab default\r\n- TensorFlow version (use command below): v2.4.1-0-g85c8b2a817f 2.4.1\r\n- Python version: 3\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\n\r\n**Describe the current behavior**\r\nThere is a function convert_image_dtype that used in many augmentation ops like tf.image.stateless_random_brightness and etc.\r\nThat function has a rounding issue here https://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/ops/image_ops_impl.py#L2290\r\nMaybe other cases also have this bug (not checked).\r\n\r\nWhen we casting float to int, we should use rounding. In convert_image_dtype rounding implemented as shift by 0.5 which is correct for max value, but is not correct for 0.\r\nSee example by link below.\r\n\r\n**Describe the expected behavior**\r\nEvery time convert_image_dtype changes value scale following casting to int, it should use rounding before casting.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1xZqyuAlZu_xkDZZHNsr7K8g6MyapcNPi?usp=sharing\r\n", "comments": ["Was able to reproduce the issue in TF 2.4 and Nightly version as well. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/5bbf7fed3fe931f4cc07306c91276f88/convert_image_dtype.ipynb).", "Thanks for reporting the issue!", "Hi, Can I take this issue?", "Yes, please feel free to do so.", "@kkimdev thank you. I did changes and try to check it by unit test, but have one small problem. Could you please tell me or send the link where I can find how to start the exact unit test? e.g ConvertImageTest.testNoConvert.\r\nCould you also tell me please how to check what tests failed because of me? And do you have something like Jenkins or whatever where I can check that I don't break something?", "@pointhex I think you can put unit test here https://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/ops/image_ops_test.py and once you open a Github PR, it will kick off pre-submit testings.  Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48701\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48701\">No</a>\n", "The issue is still not resolved in TF master branch. It seems that PR https://github.com/tensorflow/tensorflow/pull/49868 was not merged.", "Here is a commit where this issue fix was removed https://github.com/tensorflow/tensorflow/commit/56ab2308f72da337865cf765a1844ed9e990d02e#diff-a5a22434f0c18768fc2e10c0e0420ac6f111a5802f67e3df54f155bfefc7094f"]}, {"number": 48700, "title": "Added closure cancelation on worker stop", "body": "I have added one change to Worker class which makes possible to remove Worker from cluster. I wasn't able to add tests because yours unit test script doesn't work for me.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48700) for more info**.\n\n<!-- need_author_cla -->", "This should be opened against master."]}, {"number": 48698, "title": "Cannot run tflite model on GPU after converting it into using Float16 quantization.", "body": "### 1. System information\r\n\r\n- Google Colab\r\n-Tensorflow 2\r\n\r\n### 2. Code\r\nhttps://github.com/Akhilesh64/Image-Segmentation-U-2-Net/blob/main/U2-net.py\r\n\r\n#### Option A: Reference code\r\n1)  Reference \r\nhttps://www.tensorflow.org/lite/performance/post_training_quantization#float16_quantization\r\n\r\n2) The warning i get while converting this model to tflite is.\r\n```\r\nWARNING:absl:Found untraced functions such as _defun_call, _defun_call, _defun_call, _defun_call, _defun_call while saving (showing 5 of 63). These functions will not be directly callable after loading.\r\nWARNING:absl:Found untraced functions such as _defun_call, _defun_call, _defun_call, _defun_call, _defun_call while saving (showing 5 of 63). These functions will not be directly callable after loading.\r\n```\r\n\r\n### 3. Failure after conversion\r\nBut when i use this tflite model in my app using gpu delegate it shows the error as \r\n```\r\njava.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.\r\n\r\n```\r\n\r\n- Model is not getting loaded after converting to float16 quantizaton.\r\n- Also, can anyone help me in implementing this model for real time inference.\r\n\r\n\r\n\r\n### 5. (optional) Any other info / logs\r\nI also found some stackoverflow answers regarding the same https://stackoverflow.com/questions/61406595/layer-up-sampling2dclass-tensorflow-python-keras-layers-convolutional-upsampl. But couldn't understand this in detail.", "comments": ["@impjdi could you take a look?", "Is this really a problem of FP16?  The error message \r\n\r\n> java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.\r\n\r\ntells me that it's the model itself not having set static tensor dimensions, but have dynamic tensor shapes.  That has nothing to do with FP16 or FP32.  Does this run in FP32 without problems?", "@impjdi how can i know which layers are making dynamic tensors. ", "Note that I don't know anything about neural net (ok, it's a bit exaggerated, but roughly captures my point), so I can't suggest a more efficient way than checking every input/output of an op using a visualizer like Netron or checking it programmatically, e.g. print all input/output tensors' dimensions.  There might be a much better way in Python land that I am not aware of.", "@impjdi thanks, will see if it works !", "@impjdi i checked the whole model summary, there was no dynamic tensor present, every layer has a fixed shape\r\n\r\n```\r\nModel: \"functional_1\"\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_1 (InputLayer)            [(None, 256, 256, 3) 0                                            \r\n__________________________________________________________________________________________________\r\nconv2d (Conv2D)                 (None, 256, 256, 64) 1792        input_1[0][0]                    \r\n__________________________________________________________________________________________________\r\nbatch_normalization (BatchNorma (None, 256, 256, 64) 256         conv2d[0][0]                     \r\n__________________________________________________________________________________________________\r\nactivation (Activation)         (None, 256, 256, 64) 0           batch_normalization[0][0]        \r\n__________________________________________________________________________________________________\r\nconv2d_1 (Conv2D)               (None, 256, 256, 16) 9232        activation[0][0]                 \r\n__________________________________________________________________________________________________\r\nbatch_normalization_1 (BatchNor (None, 256, 256, 16) 64          conv2d_1[0][0]                   \r\n__________________________________________________________________________________________________\r\nactivation_1 (Activation)       (None, 256, 256, 16) 0           batch_normalization_1[0][0]      \r\n__________________________________________________________________________________________________\r\nmax_pooling2d (MaxPooling2D)    (None, 128, 128, 16) 0           activation_1[0][0]               \r\n__________________________________________________________________________________________________\r\nconv2d_2 (Conv2D)               (None, 128, 128, 16) 2320        max_pooling2d[0][0]              \r\n__________________________________________________________________________________________________\r\nbatch_normalization_2 (BatchNor (None, 128, 128, 16) 64          conv2d_2[0][0]                   \r\n__________________________________________________________________________________________________\r\nactivation_2 (Activation)       (None, 128, 128, 16) 0           batch_normalization_2[0][0]      \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 16)   0           activation_2[0][0]               \r\n__________________________________________________________________________________________________\r\nconv2d_3 (Conv2D)               (None, 64, 64, 16)   2320        max_pooling2d_1[0][0]            \r\n__________________________________________________________________________________________________\r\nbatch_normalization_3 (BatchNor (None, 64, 64, 16)   64          conv2d_3[0][0]                   \r\n__________________________________________________________________________________________________\r\nactivation_3 (Activation)       (None, 64, 64, 16)   0           batch_normalization_3[0][0]      \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_3[0][0]               \r\n__________________________________________________________________________________________________\r\nconv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        max_pooling2d_2[0][0]            \r\n__________________________________________________________________________________________________\r\nbatch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \r\n__________________________________________________________________________________________________\r\nactivation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 16)   0           activation_4[0][0]               \r\n__________________________________________________________________________________________________\r\nconv2d_5 (Conv2D)               (None, 16, 16, 16)   2320        max_pooling2d_3[0][0]            \r\n__________________________________________________________________________________________________\r\nbatch_normalization_5 (BatchNor (None, 16, 16, 16)   64          conv2d_5[0][0]                   \r\n__________________________________________________________________________________________________\r\nactivation_5 (Activation)       (None, 16, 16, 16)   0           batch_normalization_5[0][0]      \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 16)     0           activation_5[0][0]               \r\n__________________________________________________________________________________________________\r\nconv2d_6 (Conv2D)               (None, 8, 8, 16)     2320        max_pooling2d_4[0][0]            \r\n__________________________________________________________________________________________________\r\nbatch_normalization_6 (BatchNor (None, 8, 8, 16)     64          conv2d_6[0][0]                   \r\n__________________________________________________________________________________________________\r\nactivation_6 (Activation)       (None, 8, 8, 16)     0           batch_normalization_6[0][0]      \r\n__________________________________________________________________________________________________\r\nconv2d_7 (Conv2D)               (None, 8, 8, 16)     2320        activation_6[0][0]               \r\n__________________________________________________________________________________________________\r\nbatch_normalization_7 (BatchNor (None, 8, 8, 16)     64          conv2d_7[0][0]                   \r\n__________________________________________________________________________________________________\r\nactivation_7 (Activation)       (None, 8, 8, 16)     0           batch_normalization_7[0][0]      \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat (TensorFlowO (None, 8, 8, 32)     0           activation_7[0][0]               \r\n                                                                 activation_6[0][0]               \r\n__________________________________________________________________________________________________\r\nconv2d_8 (Conv2D)               (None, 8, 8, 16)     4624        tf_op_layer_concat[0][0]         \r\n__________________________________________________________________________________________________\r\nbatch_normalization_8 (BatchNor (None, 8, 8, 16)     64          conv2d_8[0][0]                   \r\n__________________________________________________________________________________________________\r\nactivation_8 (Activation)       (None, 8, 8, 16)     0           batch_normalization_8[0][0]      \r\n__________________________________________________________________________________________________\r\nup_sampling2d (UpSampling2D)    (None, 16, 16, 16)   0           activation_8[0][0]               \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_1 (TensorFlo (None, 16, 16, 32)   0           up_sampling2d[0][0]              \r\n                                                                 activation_5[0][0]               \r\n__________________________________________________________________________________________________\r\nconv2d_9 (Conv2D)               (None, 16, 16, 16)   4624        tf_op_layer_concat_1[0][0]       \r\n__________________________________________________________________________________________________\r\nbatch_normalization_9 (BatchNor (None, 16, 16, 16)   64          conv2d_9[0][0]                   \r\n__________________________________________________________________________________________________\r\nactivation_9 (Activation)       (None, 16, 16, 16)   0           batch_normalization_9[0][0]      \r\n__________________________________________________________________________________________________\r\nup_sampling2d_1 (UpSampling2D)  (None, 32, 32, 16)   0           activation_9[0][0]               \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_2 (TensorFlo (None, 32, 32, 32)   0           up_sampling2d_1[0][0]            \r\n                                                                 activation_4[0][0]               \r\n__________________________________________________________________________________________________\r\nconv2d_10 (Conv2D)              (None, 32, 32, 16)   4624        tf_op_layer_concat_2[0][0]       \r\n__________________________________________________________________________________________________\r\nbatch_normalization_10 (BatchNo (None, 32, 32, 16)   64          conv2d_10[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_10 (Activation)      (None, 32, 32, 16)   0           batch_normalization_10[0][0]     \r\n__________________________________________________________________________________________________\r\nup_sampling2d_2 (UpSampling2D)  (None, 64, 64, 16)   0           activation_10[0][0]              \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_3 (TensorFlo (None, 64, 64, 32)   0           up_sampling2d_2[0][0]            \r\n                                                                 activation_3[0][0]               \r\n__________________________________________________________________________________________________\r\nconv2d_11 (Conv2D)              (None, 64, 64, 16)   4624        tf_op_layer_concat_3[0][0]       \r\n__________________________________________________________________________________________________\r\nbatch_normalization_11 (BatchNo (None, 64, 64, 16)   64          conv2d_11[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_11 (Activation)      (None, 64, 64, 16)   0           batch_normalization_11[0][0]     \r\n__________________________________________________________________________________________________\r\nup_sampling2d_3 (UpSampling2D)  (None, 128, 128, 16) 0           activation_11[0][0]              \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_4 (TensorFlo (None, 128, 128, 32) 0           up_sampling2d_3[0][0]            \r\n                                                                 activation_2[0][0]               \r\n__________________________________________________________________________________________________\r\nconv2d_12 (Conv2D)              (None, 128, 128, 16) 4624        tf_op_layer_concat_4[0][0]       \r\n__________________________________________________________________________________________________\r\nbatch_normalization_12 (BatchNo (None, 128, 128, 16) 64          conv2d_12[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_12 (Activation)      (None, 128, 128, 16) 0           batch_normalization_12[0][0]     \r\n__________________________________________________________________________________________________\r\nup_sampling2d_4 (UpSampling2D)  (None, 256, 256, 16) 0           activation_12[0][0]              \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_5 (TensorFlo (None, 256, 256, 32) 0           up_sampling2d_4[0][0]            \r\n                                                                 activation_1[0][0]               \r\n__________________________________________________________________________________________________\r\nconv2d_13 (Conv2D)              (None, 256, 256, 64) 18496       tf_op_layer_concat_5[0][0]       \r\n__________________________________________________________________________________________________\r\nbatch_normalization_13 (BatchNo (None, 256, 256, 64) 256         conv2d_13[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_13 (Activation)      (None, 256, 256, 64) 0           batch_normalization_13[0][0]     \r\n__________________________________________________________________________________________________\r\ntf_op_layer_AddV2 (TensorFlowOp (None, 256, 256, 64) 0           activation_13[0][0]              \r\n                                                                 activation[0][0]                 \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_5 (MaxPooling2D)  (None, 128, 128, 64) 0           tf_op_layer_AddV2[0][0]          \r\n__________________________________________________________________________________________________\r\nconv2d_14 (Conv2D)              (None, 128, 128, 64) 36928       max_pooling2d_5[0][0]            \r\n__________________________________________________________________________________________________\r\nbatch_normalization_14 (BatchNo (None, 128, 128, 64) 256         conv2d_14[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_14 (Activation)      (None, 128, 128, 64) 0           batch_normalization_14[0][0]     \r\n__________________________________________________________________________________________________\r\nconv2d_15 (Conv2D)              (None, 128, 128, 16) 9232        activation_14[0][0]              \r\n__________________________________________________________________________________________________\r\nbatch_normalization_15 (BatchNo (None, 128, 128, 16) 64          conv2d_15[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_15 (Activation)      (None, 128, 128, 16) 0           batch_normalization_15[0][0]     \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_6 (MaxPooling2D)  (None, 64, 64, 16)   0           activation_15[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_16 (Conv2D)              (None, 64, 64, 16)   2320        max_pooling2d_6[0][0]            \r\n__________________________________________________________________________________________________\r\nbatch_normalization_16 (BatchNo (None, 64, 64, 16)   64          conv2d_16[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_16 (Activation)      (None, 64, 64, 16)   0           batch_normalization_16[0][0]     \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_7 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_16[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_17 (Conv2D)              (None, 32, 32, 16)   2320        max_pooling2d_7[0][0]            \r\n__________________________________________________________________________________________________\r\nbatch_normalization_17 (BatchNo (None, 32, 32, 16)   64          conv2d_17[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_17 (Activation)      (None, 32, 32, 16)   0           batch_normalization_17[0][0]     \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_8 (MaxPooling2D)  (None, 16, 16, 16)   0           activation_17[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_18 (Conv2D)              (None, 16, 16, 16)   2320        max_pooling2d_8[0][0]            \r\n__________________________________________________________________________________________________\r\nbatch_normalization_18 (BatchNo (None, 16, 16, 16)   64          conv2d_18[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_18 (Activation)      (None, 16, 16, 16)   0           batch_normalization_18[0][0]     \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_9 (MaxPooling2D)  (None, 8, 8, 16)     0           activation_18[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_20 (Conv2D)              (None, 8, 8, 16)     2320        max_pooling2d_9[0][0]            \r\n__________________________________________________________________________________________________\r\nconv2d_19 (Conv2D)              (None, 8, 8, 16)     2320        max_pooling2d_9[0][0]            \r\n__________________________________________________________________________________________________\r\nbatch_normalization_20 (BatchNo (None, 8, 8, 16)     64          conv2d_20[0][0]                  \r\n__________________________________________________________________________________________________\r\nbatch_normalization_19 (BatchNo (None, 8, 8, 16)     64          conv2d_19[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_20 (Activation)      (None, 8, 8, 16)     0           batch_normalization_20[0][0]     \r\n__________________________________________________________________________________________________\r\nactivation_19 (Activation)      (None, 8, 8, 16)     0           batch_normalization_19[0][0]     \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_6 (TensorFlo (None, 8, 8, 32)     0           activation_20[0][0]              \r\n                                                                 activation_19[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_21 (Conv2D)              (None, 8, 8, 16)     4624        tf_op_layer_concat_6[0][0]       \r\n__________________________________________________________________________________________________\r\nbatch_normalization_21 (BatchNo (None, 8, 8, 16)     64          conv2d_21[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_21 (Activation)      (None, 8, 8, 16)     0           batch_normalization_21[0][0]     \r\n__________________________________________________________________________________________________\r\nup_sampling2d_5 (UpSampling2D)  (None, 16, 16, 16)   0           activation_21[0][0]              \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_7 (TensorFlo (None, 16, 16, 32)   0           up_sampling2d_5[0][0]            \r\n                                                                 activation_18[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_22 (Conv2D)              (None, 16, 16, 16)   4624        tf_op_layer_concat_7[0][0]       \r\n__________________________________________________________________________________________________\r\nbatch_normalization_22 (BatchNo (None, 16, 16, 16)   64          conv2d_22[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_22 (Activation)      (None, 16, 16, 16)   0           batch_normalization_22[0][0]     \r\n__________________________________________________________________________________________________\r\nup_sampling2d_6 (UpSampling2D)  (None, 32, 32, 16)   0           activation_22[0][0]              \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_8 (TensorFlo (None, 32, 32, 32)   0           up_sampling2d_6[0][0]            \r\n                                                                 activation_17[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_23 (Conv2D)              (None, 32, 32, 16)   4624        tf_op_layer_concat_8[0][0]       \r\n__________________________________________________________________________________________________\r\nbatch_normalization_23 (BatchNo (None, 32, 32, 16)   64          conv2d_23[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_23 (Activation)      (None, 32, 32, 16)   0           batch_normalization_23[0][0]     \r\n__________________________________________________________________________________________________\r\nup_sampling2d_7 (UpSampling2D)  (None, 64, 64, 16)   0           activation_23[0][0]              \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_9 (TensorFlo (None, 64, 64, 32)   0           up_sampling2d_7[0][0]            \r\n                                                                 activation_16[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_24 (Conv2D)              (None, 64, 64, 16)   4624        tf_op_layer_concat_9[0][0]       \r\n__________________________________________________________________________________________________\r\nbatch_normalization_24 (BatchNo (None, 64, 64, 16)   64          conv2d_24[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_24 (Activation)      (None, 64, 64, 16)   0           batch_normalization_24[0][0]     \r\n__________________________________________________________________________________________________\r\nup_sampling2d_8 (UpSampling2D)  (None, 128, 128, 16) 0           activation_24[0][0]              \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_10 (TensorFl (None, 128, 128, 32) 0           up_sampling2d_8[0][0]            \r\n                                                                 activation_15[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_25 (Conv2D)              (None, 128, 128, 64) 18496       tf_op_layer_concat_10[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_25 (BatchNo (None, 128, 128, 64) 256         conv2d_25[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_25 (Activation)      (None, 128, 128, 64) 0           batch_normalization_25[0][0]     \r\n__________________________________________________________________________________________________\r\ntf_op_layer_AddV2_1 (TensorFlow (None, 128, 128, 64) 0           activation_25[0][0]              \r\n                                                                 activation_14[0][0]              \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_10 (MaxPooling2D) (None, 64, 64, 64)   0           tf_op_layer_AddV2_1[0][0]        \r\n__________________________________________________________________________________________________\r\nconv2d_26 (Conv2D)              (None, 64, 64, 64)   36928       max_pooling2d_10[0][0]           \r\n__________________________________________________________________________________________________\r\nbatch_normalization_26 (BatchNo (None, 64, 64, 64)   256         conv2d_26[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_26 (Activation)      (None, 64, 64, 64)   0           batch_normalization_26[0][0]     \r\n__________________________________________________________________________________________________\r\nconv2d_27 (Conv2D)              (None, 64, 64, 16)   9232        activation_26[0][0]              \r\n__________________________________________________________________________________________________\r\nbatch_normalization_27 (BatchNo (None, 64, 64, 16)   64          conv2d_27[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_27 (Activation)      (None, 64, 64, 16)   0           batch_normalization_27[0][0]     \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_11 (MaxPooling2D) (None, 32, 32, 16)   0           activation_27[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_28 (Conv2D)              (None, 32, 32, 16)   2320        max_pooling2d_11[0][0]           \r\n__________________________________________________________________________________________________\r\nbatch_normalization_28 (BatchNo (None, 32, 32, 16)   64          conv2d_28[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_28 (Activation)      (None, 32, 32, 16)   0           batch_normalization_28[0][0]     \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_12 (MaxPooling2D) (None, 16, 16, 16)   0           activation_28[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_29 (Conv2D)              (None, 16, 16, 16)   2320        max_pooling2d_12[0][0]           \r\n__________________________________________________________________________________________________\r\nbatch_normalization_29 (BatchNo (None, 16, 16, 16)   64          conv2d_29[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_29 (Activation)      (None, 16, 16, 16)   0           batch_normalization_29[0][0]     \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_13 (MaxPooling2D) (None, 8, 8, 16)     0           activation_29[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_31 (Conv2D)              (None, 8, 8, 16)     2320        max_pooling2d_13[0][0]           \r\n__________________________________________________________________________________________________\r\nconv2d_30 (Conv2D)              (None, 8, 8, 16)     2320        max_pooling2d_13[0][0]           \r\n__________________________________________________________________________________________________\r\nbatch_normalization_31 (BatchNo (None, 8, 8, 16)     64          conv2d_31[0][0]                  \r\n__________________________________________________________________________________________________\r\nbatch_normalization_30 (BatchNo (None, 8, 8, 16)     64          conv2d_30[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_31 (Activation)      (None, 8, 8, 16)     0           batch_normalization_31[0][0]     \r\n__________________________________________________________________________________________________\r\nactivation_30 (Activation)      (None, 8, 8, 16)     0           batch_normalization_30[0][0]     \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_11 (TensorFl (None, 8, 8, 32)     0           activation_31[0][0]              \r\n                                                                 activation_30[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_32 (Conv2D)              (None, 8, 8, 16)     4624        tf_op_layer_concat_11[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_32 (BatchNo (None, 8, 8, 16)     64          conv2d_32[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_32 (Activation)      (None, 8, 8, 16)     0           batch_normalization_32[0][0]     \r\n__________________________________________________________________________________________________\r\nup_sampling2d_9 (UpSampling2D)  (None, 16, 16, 16)   0           activation_32[0][0]              \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_12 (TensorFl (None, 16, 16, 32)   0           up_sampling2d_9[0][0]            \r\n                                                                 activation_29[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_33 (Conv2D)              (None, 16, 16, 16)   4624        tf_op_layer_concat_12[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_33 (BatchNo (None, 16, 16, 16)   64          conv2d_33[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_33 (Activation)      (None, 16, 16, 16)   0           batch_normalization_33[0][0]     \r\n__________________________________________________________________________________________________\r\nup_sampling2d_10 (UpSampling2D) (None, 32, 32, 16)   0           activation_33[0][0]              \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_13 (TensorFl (None, 32, 32, 32)   0           up_sampling2d_10[0][0]           \r\n                                                                 activation_28[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_34 (Conv2D)              (None, 32, 32, 16)   4624        tf_op_layer_concat_13[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_34 (BatchNo (None, 32, 32, 16)   64          conv2d_34[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_34 (Activation)      (None, 32, 32, 16)   0           batch_normalization_34[0][0]     \r\n__________________________________________________________________________________________________\r\nup_sampling2d_11 (UpSampling2D) (None, 64, 64, 16)   0           activation_34[0][0]              \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_14 (TensorFl (None, 64, 64, 32)   0           up_sampling2d_11[0][0]           \r\n                                                                 activation_27[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_35 (Conv2D)              (None, 64, 64, 64)   18496       tf_op_layer_concat_14[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_35 (BatchNo (None, 64, 64, 64)   256         conv2d_35[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_35 (Activation)      (None, 64, 64, 64)   0           batch_normalization_35[0][0]     \r\n__________________________________________________________________________________________________\r\ntf_op_layer_AddV2_2 (TensorFlow (None, 64, 64, 64)   0           activation_35[0][0]              \r\n                                                                 activation_26[0][0]              \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_14 (MaxPooling2D) (None, 32, 32, 64)   0           tf_op_layer_AddV2_2[0][0]        \r\n__________________________________________________________________________________________________\r\nconv2d_36 (Conv2D)              (None, 32, 32, 64)   36928       max_pooling2d_14[0][0]           \r\n__________________________________________________________________________________________________\r\nbatch_normalization_36 (BatchNo (None, 32, 32, 64)   256         conv2d_36[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_36 (Activation)      (None, 32, 32, 64)   0           batch_normalization_36[0][0]     \r\n__________________________________________________________________________________________________\r\nconv2d_37 (Conv2D)              (None, 32, 32, 16)   9232        activation_36[0][0]              \r\n__________________________________________________________________________________________________\r\nbatch_normalization_37 (BatchNo (None, 32, 32, 16)   64          conv2d_37[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_37 (Activation)      (None, 32, 32, 16)   0           batch_normalization_37[0][0]     \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_15 (MaxPooling2D) (None, 16, 16, 16)   0           activation_37[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_38 (Conv2D)              (None, 16, 16, 16)   2320        max_pooling2d_15[0][0]           \r\n__________________________________________________________________________________________________\r\nbatch_normalization_38 (BatchNo (None, 16, 16, 16)   64          conv2d_38[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_38 (Activation)      (None, 16, 16, 16)   0           batch_normalization_38[0][0]     \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_16 (MaxPooling2D) (None, 8, 8, 16)     0           activation_38[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_40 (Conv2D)              (None, 8, 8, 16)     2320        max_pooling2d_16[0][0]           \r\n__________________________________________________________________________________________________\r\nconv2d_39 (Conv2D)              (None, 8, 8, 16)     2320        max_pooling2d_16[0][0]           \r\n__________________________________________________________________________________________________\r\nbatch_normalization_40 (BatchNo (None, 8, 8, 16)     64          conv2d_40[0][0]                  \r\n__________________________________________________________________________________________________\r\nbatch_normalization_39 (BatchNo (None, 8, 8, 16)     64          conv2d_39[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_40 (Activation)      (None, 8, 8, 16)     0           batch_normalization_40[0][0]     \r\n__________________________________________________________________________________________________\r\nactivation_39 (Activation)      (None, 8, 8, 16)     0           batch_normalization_39[0][0]     \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_15 (TensorFl (None, 8, 8, 32)     0           activation_40[0][0]              \r\n                                                                 activation_39[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_41 (Conv2D)              (None, 8, 8, 16)     4624        tf_op_layer_concat_15[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_41 (BatchNo (None, 8, 8, 16)     64          conv2d_41[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_41 (Activation)      (None, 8, 8, 16)     0           batch_normalization_41[0][0]     \r\n__________________________________________________________________________________________________\r\nup_sampling2d_12 (UpSampling2D) (None, 16, 16, 16)   0           activation_41[0][0]              \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_16 (TensorFl (None, 16, 16, 32)   0           up_sampling2d_12[0][0]           \r\n                                                                 activation_38[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_42 (Conv2D)              (None, 16, 16, 16)   4624        tf_op_layer_concat_16[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_42 (BatchNo (None, 16, 16, 16)   64          conv2d_42[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_42 (Activation)      (None, 16, 16, 16)   0           batch_normalization_42[0][0]     \r\n__________________________________________________________________________________________________\r\nup_sampling2d_13 (UpSampling2D) (None, 32, 32, 16)   0           activation_42[0][0]              \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_17 (TensorFl (None, 32, 32, 32)   0           up_sampling2d_13[0][0]           \r\n                                                                 activation_37[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_43 (Conv2D)              (None, 32, 32, 64)   18496       tf_op_layer_concat_17[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_43 (BatchNo (None, 32, 32, 64)   256         conv2d_43[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_43 (Activation)      (None, 32, 32, 64)   0           batch_normalization_43[0][0]     \r\n__________________________________________________________________________________________________\r\ntf_op_layer_AddV2_3 (TensorFlow (None, 32, 32, 64)   0           activation_43[0][0]              \r\n                                                                 activation_36[0][0]              \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_17 (MaxPooling2D) (None, 16, 16, 64)   0           tf_op_layer_AddV2_3[0][0]        \r\n__________________________________________________________________________________________________\r\nconv2d_44 (Conv2D)              (None, 16, 16, 64)   36928       max_pooling2d_17[0][0]           \r\n__________________________________________________________________________________________________\r\nbatch_normalization_44 (BatchNo (None, 16, 16, 64)   256         conv2d_44[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_44 (Activation)      (None, 16, 16, 64)   0           batch_normalization_44[0][0]     \r\n__________________________________________________________________________________________________\r\nconv2d_45 (Conv2D)              (None, 16, 16, 16)   9232        activation_44[0][0]              \r\n__________________________________________________________________________________________________\r\nbatch_normalization_45 (BatchNo (None, 16, 16, 16)   64          conv2d_45[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_45 (Activation)      (None, 16, 16, 16)   0           batch_normalization_45[0][0]     \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_18 (MaxPooling2D) (None, 8, 8, 16)     0           activation_45[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_46 (Conv2D)              (None, 8, 8, 16)     2320        max_pooling2d_18[0][0]           \r\n__________________________________________________________________________________________________\r\nbatch_normalization_46 (BatchNo (None, 8, 8, 16)     64          conv2d_46[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_46 (Activation)      (None, 8, 8, 16)     0           batch_normalization_46[0][0]     \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_19 (MaxPooling2D) (None, 4, 4, 16)     0           activation_46[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_48 (Conv2D)              (None, 4, 4, 16)     2320        max_pooling2d_19[0][0]           \r\n__________________________________________________________________________________________________\r\nconv2d_47 (Conv2D)              (None, 4, 4, 16)     2320        max_pooling2d_19[0][0]           \r\n__________________________________________________________________________________________________\r\nbatch_normalization_48 (BatchNo (None, 4, 4, 16)     64          conv2d_48[0][0]                  \r\n__________________________________________________________________________________________________\r\nbatch_normalization_47 (BatchNo (None, 4, 4, 16)     64          conv2d_47[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_48 (Activation)      (None, 4, 4, 16)     0           batch_normalization_48[0][0]     \r\n__________________________________________________________________________________________________\r\nactivation_47 (Activation)      (None, 4, 4, 16)     0           batch_normalization_47[0][0]     \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_18 (TensorFl (None, 4, 4, 32)     0           activation_48[0][0]              \r\n                                                                 activation_47[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_49 (Conv2D)              (None, 4, 4, 16)     4624        tf_op_layer_concat_18[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_49 (BatchNo (None, 4, 4, 16)     64          conv2d_49[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_49 (Activation)      (None, 4, 4, 16)     0           batch_normalization_49[0][0]     \r\n__________________________________________________________________________________________________\r\nup_sampling2d_14 (UpSampling2D) (None, 8, 8, 16)     0           activation_49[0][0]              \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_19 (TensorFl (None, 8, 8, 32)     0           up_sampling2d_14[0][0]           \r\n                                                                 activation_46[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_50 (Conv2D)              (None, 8, 8, 16)     4624        tf_op_layer_concat_19[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_50 (BatchNo (None, 8, 8, 16)     64          conv2d_50[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_50 (Activation)      (None, 8, 8, 16)     0           batch_normalization_50[0][0]     \r\n__________________________________________________________________________________________________\r\nup_sampling2d_15 (UpSampling2D) (None, 16, 16, 16)   0           activation_50[0][0]              \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_20 (TensorFl (None, 16, 16, 32)   0           up_sampling2d_15[0][0]           \r\n                                                                 activation_45[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_51 (Conv2D)              (None, 16, 16, 64)   18496       tf_op_layer_concat_20[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_51 (BatchNo (None, 16, 16, 64)   256         conv2d_51[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_51 (Activation)      (None, 16, 16, 64)   0           batch_normalization_51[0][0]     \r\n__________________________________________________________________________________________________\r\ntf_op_layer_AddV2_4 (TensorFlow (None, 16, 16, 64)   0           activation_51[0][0]              \r\n                                                                 activation_44[0][0]              \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_20 (MaxPooling2D) (None, 8, 8, 64)     0           tf_op_layer_AddV2_4[0][0]        \r\n__________________________________________________________________________________________________\r\nconv2d_52 (Conv2D)              (None, 8, 8, 64)     36928       max_pooling2d_20[0][0]           \r\n__________________________________________________________________________________________________\r\nbatch_normalization_52 (BatchNo (None, 8, 8, 64)     256         conv2d_52[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_52 (Activation)      (None, 8, 8, 64)     0           batch_normalization_52[0][0]     \r\n__________________________________________________________________________________________________\r\nconv2d_53 (Conv2D)              (None, 8, 8, 16)     9232        activation_52[0][0]              \r\n__________________________________________________________________________________________________\r\nbatch_normalization_53 (BatchNo (None, 8, 8, 16)     64          conv2d_53[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_53 (Activation)      (None, 8, 8, 16)     0           batch_normalization_53[0][0]     \r\n__________________________________________________________________________________________________\r\nconv2d_54 (Conv2D)              (None, 8, 8, 16)     2320        activation_53[0][0]              \r\n__________________________________________________________________________________________________\r\nbatch_normalization_54 (BatchNo (None, 8, 8, 16)     64          conv2d_54[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_54 (Activation)      (None, 8, 8, 16)     0           batch_normalization_54[0][0]     \r\n__________________________________________________________________________________________________\r\nconv2d_55 (Conv2D)              (None, 8, 8, 16)     2320        activation_54[0][0]              \r\n__________________________________________________________________________________________________\r\nbatch_normalization_55 (BatchNo (None, 8, 8, 16)     64          conv2d_55[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_55 (Activation)      (None, 8, 8, 16)     0           batch_normalization_55[0][0]     \r\n__________________________________________________________________________________________________\r\nconv2d_56 (Conv2D)              (None, 8, 8, 16)     2320        activation_55[0][0]              \r\n__________________________________________________________________________________________________\r\nbatch_normalization_56 (BatchNo (None, 8, 8, 16)     64          conv2d_56[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_56 (Activation)      (None, 8, 8, 16)     0           batch_normalization_56[0][0]     \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_21 (TensorFl (None, 8, 8, 32)     0           activation_56[0][0]              \r\n                                                                 activation_55[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_57 (Conv2D)              (None, 8, 8, 16)     4624        tf_op_layer_concat_21[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_57 (BatchNo (None, 8, 8, 16)     64          conv2d_57[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_57 (Activation)      (None, 8, 8, 16)     0           batch_normalization_57[0][0]     \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_22 (TensorFl (None, 8, 8, 32)     0           activation_57[0][0]              \r\n                                                                 activation_54[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_58 (Conv2D)              (None, 8, 8, 16)     4624        tf_op_layer_concat_22[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_58 (BatchNo (None, 8, 8, 16)     64          conv2d_58[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_58 (Activation)      (None, 8, 8, 16)     0           batch_normalization_58[0][0]     \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_23 (TensorFl (None, 8, 8, 32)     0           activation_58[0][0]              \r\n                                                                 activation_53[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_59 (Conv2D)              (None, 8, 8, 64)     18496       tf_op_layer_concat_23[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_59 (BatchNo (None, 8, 8, 64)     256         conv2d_59[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_59 (Activation)      (None, 8, 8, 64)     0           batch_normalization_59[0][0]     \r\n__________________________________________________________________________________________________\r\ntf_op_layer_AddV2_5 (TensorFlow (None, 8, 8, 64)     0           activation_59[0][0]              \r\n                                                                 activation_52[0][0]              \r\n__________________________________________________________________________________________________\r\nup_sampling2d_16 (UpSampling2D) (None, 16, 16, 64)   0           tf_op_layer_AddV2_5[0][0]        \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_24 (TensorFl (None, 16, 16, 128)  0           up_sampling2d_16[0][0]           \r\n                                                                 tf_op_layer_AddV2_4[0][0]        \r\n__________________________________________________________________________________________________\r\nconv2d_60 (Conv2D)              (None, 16, 16, 64)   73792       tf_op_layer_concat_24[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_60 (BatchNo (None, 16, 16, 64)   256         conv2d_60[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_60 (Activation)      (None, 16, 16, 64)   0           batch_normalization_60[0][0]     \r\n__________________________________________________________________________________________________\r\nconv2d_61 (Conv2D)              (None, 16, 16, 16)   9232        activation_60[0][0]              \r\n__________________________________________________________________________________________________\r\nbatch_normalization_61 (BatchNo (None, 16, 16, 16)   64          conv2d_61[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_61 (Activation)      (None, 16, 16, 16)   0           batch_normalization_61[0][0]     \r\n__________________________________________________________________________________________________\r\nconv2d_62 (Conv2D)              (None, 16, 16, 16)   2320        activation_61[0][0]              \r\n__________________________________________________________________________________________________\r\nbatch_normalization_62 (BatchNo (None, 16, 16, 16)   64          conv2d_62[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_62 (Activation)      (None, 16, 16, 16)   0           batch_normalization_62[0][0]     \r\n__________________________________________________________________________________________________\r\nconv2d_63 (Conv2D)              (None, 16, 16, 16)   2320        activation_62[0][0]              \r\n__________________________________________________________________________________________________\r\nbatch_normalization_63 (BatchNo (None, 16, 16, 16)   64          conv2d_63[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_63 (Activation)      (None, 16, 16, 16)   0           batch_normalization_63[0][0]     \r\n__________________________________________________________________________________________________\r\nconv2d_64 (Conv2D)              (None, 16, 16, 16)   2320        activation_63[0][0]              \r\n__________________________________________________________________________________________________\r\nbatch_normalization_64 (BatchNo (None, 16, 16, 16)   64          conv2d_64[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_64 (Activation)      (None, 16, 16, 16)   0           batch_normalization_64[0][0]     \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_25 (TensorFl (None, 16, 16, 32)   0           activation_64[0][0]              \r\n                                                                 activation_63[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_65 (Conv2D)              (None, 16, 16, 16)   4624        tf_op_layer_concat_25[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_65 (BatchNo (None, 16, 16, 16)   64          conv2d_65[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_65 (Activation)      (None, 16, 16, 16)   0           batch_normalization_65[0][0]     \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_26 (TensorFl (None, 16, 16, 32)   0           activation_65[0][0]              \r\n                                                                 activation_62[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_66 (Conv2D)              (None, 16, 16, 16)   4624        tf_op_layer_concat_26[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_66 (BatchNo (None, 16, 16, 16)   64          conv2d_66[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_66 (Activation)      (None, 16, 16, 16)   0           batch_normalization_66[0][0]     \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_27 (TensorFl (None, 16, 16, 32)   0           activation_66[0][0]              \r\n                                                                 activation_61[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_67 (Conv2D)              (None, 16, 16, 64)   18496       tf_op_layer_concat_27[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_67 (BatchNo (None, 16, 16, 64)   256         conv2d_67[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_67 (Activation)      (None, 16, 16, 64)   0           batch_normalization_67[0][0]     \r\n__________________________________________________________________________________________________\r\ntf_op_layer_AddV2_6 (TensorFlow (None, 16, 16, 64)   0           activation_67[0][0]              \r\n                                                                 activation_60[0][0]              \r\n__________________________________________________________________________________________________\r\nup_sampling2d_17 (UpSampling2D) (None, 32, 32, 64)   0           tf_op_layer_AddV2_6[0][0]        \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_28 (TensorFl (None, 32, 32, 128)  0           up_sampling2d_17[0][0]           \r\n                                                                 tf_op_layer_AddV2_3[0][0]        \r\n__________________________________________________________________________________________________\r\nconv2d_68 (Conv2D)              (None, 32, 32, 64)   73792       tf_op_layer_concat_28[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_68 (BatchNo (None, 32, 32, 64)   256         conv2d_68[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_68 (Activation)      (None, 32, 32, 64)   0           batch_normalization_68[0][0]     \r\n__________________________________________________________________________________________________\r\nconv2d_69 (Conv2D)              (None, 32, 32, 16)   9232        activation_68[0][0]              \r\n__________________________________________________________________________________________________\r\nbatch_normalization_69 (BatchNo (None, 32, 32, 16)   64          conv2d_69[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_69 (Activation)      (None, 32, 32, 16)   0           batch_normalization_69[0][0]     \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_21 (MaxPooling2D) (None, 16, 16, 16)   0           activation_69[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_70 (Conv2D)              (None, 16, 16, 16)   2320        max_pooling2d_21[0][0]           \r\n__________________________________________________________________________________________________\r\nbatch_normalization_70 (BatchNo (None, 16, 16, 16)   64          conv2d_70[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_70 (Activation)      (None, 16, 16, 16)   0           batch_normalization_70[0][0]     \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_22 (MaxPooling2D) (None, 8, 8, 16)     0           activation_70[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_72 (Conv2D)              (None, 8, 8, 16)     2320        max_pooling2d_22[0][0]           \r\n__________________________________________________________________________________________________\r\nconv2d_71 (Conv2D)              (None, 8, 8, 16)     2320        max_pooling2d_22[0][0]           \r\n__________________________________________________________________________________________________\r\nbatch_normalization_72 (BatchNo (None, 8, 8, 16)     64          conv2d_72[0][0]                  \r\n__________________________________________________________________________________________________\r\nbatch_normalization_71 (BatchNo (None, 8, 8, 16)     64          conv2d_71[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_72 (Activation)      (None, 8, 8, 16)     0           batch_normalization_72[0][0]     \r\n__________________________________________________________________________________________________\r\nactivation_71 (Activation)      (None, 8, 8, 16)     0           batch_normalization_71[0][0]     \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_29 (TensorFl (None, 8, 8, 32)     0           activation_72[0][0]              \r\n                                                                 activation_71[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_73 (Conv2D)              (None, 8, 8, 16)     4624        tf_op_layer_concat_29[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_73 (BatchNo (None, 8, 8, 16)     64          conv2d_73[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_73 (Activation)      (None, 8, 8, 16)     0           batch_normalization_73[0][0]     \r\n__________________________________________________________________________________________________\r\nup_sampling2d_18 (UpSampling2D) (None, 16, 16, 16)   0           activation_73[0][0]              \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_30 (TensorFl (None, 16, 16, 32)   0           up_sampling2d_18[0][0]           \r\n                                                                 activation_70[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_74 (Conv2D)              (None, 16, 16, 16)   4624        tf_op_layer_concat_30[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_74 (BatchNo (None, 16, 16, 16)   64          conv2d_74[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_74 (Activation)      (None, 16, 16, 16)   0           batch_normalization_74[0][0]     \r\n__________________________________________________________________________________________________\r\nup_sampling2d_19 (UpSampling2D) (None, 32, 32, 16)   0           activation_74[0][0]              \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_31 (TensorFl (None, 32, 32, 32)   0           up_sampling2d_19[0][0]           \r\n                                                                 activation_69[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_75 (Conv2D)              (None, 32, 32, 64)   18496       tf_op_layer_concat_31[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_75 (BatchNo (None, 32, 32, 64)   256         conv2d_75[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_75 (Activation)      (None, 32, 32, 64)   0           batch_normalization_75[0][0]     \r\n__________________________________________________________________________________________________\r\ntf_op_layer_AddV2_7 (TensorFlow (None, 32, 32, 64)   0           activation_75[0][0]              \r\n                                                                 activation_68[0][0]              \r\n__________________________________________________________________________________________________\r\nup_sampling2d_20 (UpSampling2D) (None, 64, 64, 64)   0           tf_op_layer_AddV2_7[0][0]        \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_32 (TensorFl (None, 64, 64, 128)  0           up_sampling2d_20[0][0]           \r\n                                                                 tf_op_layer_AddV2_2[0][0]        \r\n__________________________________________________________________________________________________\r\nconv2d_76 (Conv2D)              (None, 64, 64, 64)   73792       tf_op_layer_concat_32[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_76 (BatchNo (None, 64, 64, 64)   256         conv2d_76[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_76 (Activation)      (None, 64, 64, 64)   0           batch_normalization_76[0][0]     \r\n__________________________________________________________________________________________________\r\nconv2d_77 (Conv2D)              (None, 64, 64, 16)   9232        activation_76[0][0]              \r\n__________________________________________________________________________________________________\r\nbatch_normalization_77 (BatchNo (None, 64, 64, 16)   64          conv2d_77[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_77 (Activation)      (None, 64, 64, 16)   0           batch_normalization_77[0][0]     \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_23 (MaxPooling2D) (None, 32, 32, 16)   0           activation_77[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_78 (Conv2D)              (None, 32, 32, 16)   2320        max_pooling2d_23[0][0]           \r\n__________________________________________________________________________________________________\r\nbatch_normalization_78 (BatchNo (None, 32, 32, 16)   64          conv2d_78[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_78 (Activation)      (None, 32, 32, 16)   0           batch_normalization_78[0][0]     \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_24 (MaxPooling2D) (None, 16, 16, 16)   0           activation_78[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_79 (Conv2D)              (None, 16, 16, 16)   2320        max_pooling2d_24[0][0]           \r\n__________________________________________________________________________________________________\r\nbatch_normalization_79 (BatchNo (None, 16, 16, 16)   64          conv2d_79[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_79 (Activation)      (None, 16, 16, 16)   0           batch_normalization_79[0][0]     \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_25 (MaxPooling2D) (None, 8, 8, 16)     0           activation_79[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_81 (Conv2D)              (None, 8, 8, 16)     2320        max_pooling2d_25[0][0]           \r\n__________________________________________________________________________________________________\r\nconv2d_80 (Conv2D)              (None, 8, 8, 16)     2320        max_pooling2d_25[0][0]           \r\n__________________________________________________________________________________________________\r\nbatch_normalization_81 (BatchNo (None, 8, 8, 16)     64          conv2d_81[0][0]                  \r\n__________________________________________________________________________________________________\r\nbatch_normalization_80 (BatchNo (None, 8, 8, 16)     64          conv2d_80[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_81 (Activation)      (None, 8, 8, 16)     0           batch_normalization_81[0][0]     \r\n__________________________________________________________________________________________________\r\nactivation_80 (Activation)      (None, 8, 8, 16)     0           batch_normalization_80[0][0]     \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_33 (TensorFl (None, 8, 8, 32)     0           activation_81[0][0]              \r\n                                                                 activation_80[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_82 (Conv2D)              (None, 8, 8, 16)     4624        tf_op_layer_concat_33[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_82 (BatchNo (None, 8, 8, 16)     64          conv2d_82[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_82 (Activation)      (None, 8, 8, 16)     0           batch_normalization_82[0][0]     \r\n__________________________________________________________________________________________________\r\nup_sampling2d_21 (UpSampling2D) (None, 16, 16, 16)   0           activation_82[0][0]              \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_34 (TensorFl (None, 16, 16, 32)   0           up_sampling2d_21[0][0]           \r\n                                                                 activation_79[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_83 (Conv2D)              (None, 16, 16, 16)   4624        tf_op_layer_concat_34[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_83 (BatchNo (None, 16, 16, 16)   64          conv2d_83[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_83 (Activation)      (None, 16, 16, 16)   0           batch_normalization_83[0][0]     \r\n__________________________________________________________________________________________________\r\nup_sampling2d_22 (UpSampling2D) (None, 32, 32, 16)   0           activation_83[0][0]              \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_35 (TensorFl (None, 32, 32, 32)   0           up_sampling2d_22[0][0]           \r\n                                                                 activation_78[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_84 (Conv2D)              (None, 32, 32, 16)   4624        tf_op_layer_concat_35[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_84 (BatchNo (None, 32, 32, 16)   64          conv2d_84[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_84 (Activation)      (None, 32, 32, 16)   0           batch_normalization_84[0][0]     \r\n__________________________________________________________________________________________________\r\nup_sampling2d_23 (UpSampling2D) (None, 64, 64, 16)   0           activation_84[0][0]              \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_36 (TensorFl (None, 64, 64, 32)   0           up_sampling2d_23[0][0]           \r\n                                                                 activation_77[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_85 (Conv2D)              (None, 64, 64, 64)   18496       tf_op_layer_concat_36[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_85 (BatchNo (None, 64, 64, 64)   256         conv2d_85[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_85 (Activation)      (None, 64, 64, 64)   0           batch_normalization_85[0][0]     \r\n__________________________________________________________________________________________________\r\ntf_op_layer_AddV2_8 (TensorFlow (None, 64, 64, 64)   0           activation_85[0][0]              \r\n                                                                 activation_76[0][0]              \r\n__________________________________________________________________________________________________\r\nup_sampling2d_24 (UpSampling2D) (None, 128, 128, 64) 0           tf_op_layer_AddV2_8[0][0]        \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_37 (TensorFl (None, 128, 128, 128 0           up_sampling2d_24[0][0]           \r\n                                                                 tf_op_layer_AddV2_1[0][0]        \r\n__________________________________________________________________________________________________\r\nconv2d_86 (Conv2D)              (None, 128, 128, 64) 73792       tf_op_layer_concat_37[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_86 (BatchNo (None, 128, 128, 64) 256         conv2d_86[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_86 (Activation)      (None, 128, 128, 64) 0           batch_normalization_86[0][0]     \r\n__________________________________________________________________________________________________\r\nconv2d_87 (Conv2D)              (None, 128, 128, 16) 9232        activation_86[0][0]              \r\n__________________________________________________________________________________________________\r\nbatch_normalization_87 (BatchNo (None, 128, 128, 16) 64          conv2d_87[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_87 (Activation)      (None, 128, 128, 16) 0           batch_normalization_87[0][0]     \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_26 (MaxPooling2D) (None, 64, 64, 16)   0           activation_87[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_88 (Conv2D)              (None, 64, 64, 16)   2320        max_pooling2d_26[0][0]           \r\n__________________________________________________________________________________________________\r\nbatch_normalization_88 (BatchNo (None, 64, 64, 16)   64          conv2d_88[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_88 (Activation)      (None, 64, 64, 16)   0           batch_normalization_88[0][0]     \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_27 (MaxPooling2D) (None, 32, 32, 16)   0           activation_88[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_89 (Conv2D)              (None, 32, 32, 16)   2320        max_pooling2d_27[0][0]           \r\n__________________________________________________________________________________________________\r\nbatch_normalization_89 (BatchNo (None, 32, 32, 16)   64          conv2d_89[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_89 (Activation)      (None, 32, 32, 16)   0           batch_normalization_89[0][0]     \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_28 (MaxPooling2D) (None, 16, 16, 16)   0           activation_89[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_90 (Conv2D)              (None, 16, 16, 16)   2320        max_pooling2d_28[0][0]           \r\n__________________________________________________________________________________________________\r\nbatch_normalization_90 (BatchNo (None, 16, 16, 16)   64          conv2d_90[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_90 (Activation)      (None, 16, 16, 16)   0           batch_normalization_90[0][0]     \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_29 (MaxPooling2D) (None, 8, 8, 16)     0           activation_90[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_92 (Conv2D)              (None, 8, 8, 16)     2320        max_pooling2d_29[0][0]           \r\n__________________________________________________________________________________________________\r\nconv2d_91 (Conv2D)              (None, 8, 8, 16)     2320        max_pooling2d_29[0][0]           \r\n__________________________________________________________________________________________________\r\nbatch_normalization_92 (BatchNo (None, 8, 8, 16)     64          conv2d_92[0][0]                  \r\n__________________________________________________________________________________________________\r\nbatch_normalization_91 (BatchNo (None, 8, 8, 16)     64          conv2d_91[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_92 (Activation)      (None, 8, 8, 16)     0           batch_normalization_92[0][0]     \r\n__________________________________________________________________________________________________\r\nactivation_91 (Activation)      (None, 8, 8, 16)     0           batch_normalization_91[0][0]     \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_38 (TensorFl (None, 8, 8, 32)     0           activation_92[0][0]              \r\n                                                                 activation_91[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_93 (Conv2D)              (None, 8, 8, 16)     4624        tf_op_layer_concat_38[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_93 (BatchNo (None, 8, 8, 16)     64          conv2d_93[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_93 (Activation)      (None, 8, 8, 16)     0           batch_normalization_93[0][0]     \r\n__________________________________________________________________________________________________\r\nup_sampling2d_25 (UpSampling2D) (None, 16, 16, 16)   0           activation_93[0][0]              \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_39 (TensorFl (None, 16, 16, 32)   0           up_sampling2d_25[0][0]           \r\n                                                                 activation_90[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_94 (Conv2D)              (None, 16, 16, 16)   4624        tf_op_layer_concat_39[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_94 (BatchNo (None, 16, 16, 16)   64          conv2d_94[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_94 (Activation)      (None, 16, 16, 16)   0           batch_normalization_94[0][0]     \r\n__________________________________________________________________________________________________\r\nup_sampling2d_26 (UpSampling2D) (None, 32, 32, 16)   0           activation_94[0][0]              \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_40 (TensorFl (None, 32, 32, 32)   0           up_sampling2d_26[0][0]           \r\n                                                                 activation_89[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_95 (Conv2D)              (None, 32, 32, 16)   4624        tf_op_layer_concat_40[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_95 (BatchNo (None, 32, 32, 16)   64          conv2d_95[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_95 (Activation)      (None, 32, 32, 16)   0           batch_normalization_95[0][0]     \r\n__________________________________________________________________________________________________\r\nup_sampling2d_27 (UpSampling2D) (None, 64, 64, 16)   0           activation_95[0][0]              \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_41 (TensorFl (None, 64, 64, 32)   0           up_sampling2d_27[0][0]           \r\n                                                                 activation_88[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_96 (Conv2D)              (None, 64, 64, 16)   4624        tf_op_layer_concat_41[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_96 (BatchNo (None, 64, 64, 16)   64          conv2d_96[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_96 (Activation)      (None, 64, 64, 16)   0           batch_normalization_96[0][0]     \r\n__________________________________________________________________________________________________\r\nup_sampling2d_28 (UpSampling2D) (None, 128, 128, 16) 0           activation_96[0][0]              \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_42 (TensorFl (None, 128, 128, 32) 0           up_sampling2d_28[0][0]           \r\n                                                                 activation_87[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_97 (Conv2D)              (None, 128, 128, 64) 18496       tf_op_layer_concat_42[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_97 (BatchNo (None, 128, 128, 64) 256         conv2d_97[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_97 (Activation)      (None, 128, 128, 64) 0           batch_normalization_97[0][0]     \r\n__________________________________________________________________________________________________\r\ntf_op_layer_AddV2_9 (TensorFlow (None, 128, 128, 64) 0           activation_97[0][0]              \r\n                                                                 activation_86[0][0]              \r\n__________________________________________________________________________________________________\r\nup_sampling2d_29 (UpSampling2D) (None, 256, 256, 64) 0           tf_op_layer_AddV2_9[0][0]        \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_43 (TensorFl (None, 256, 256, 128 0           up_sampling2d_29[0][0]           \r\n                                                                 tf_op_layer_AddV2[0][0]          \r\n__________________________________________________________________________________________________\r\nconv2d_98 (Conv2D)              (None, 256, 256, 64) 73792       tf_op_layer_concat_43[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_98 (BatchNo (None, 256, 256, 64) 256         conv2d_98[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_98 (Activation)      (None, 256, 256, 64) 0           batch_normalization_98[0][0]     \r\n__________________________________________________________________________________________________\r\nconv2d_99 (Conv2D)              (None, 256, 256, 16) 9232        activation_98[0][0]              \r\n__________________________________________________________________________________________________\r\nbatch_normalization_99 (BatchNo (None, 256, 256, 16) 64          conv2d_99[0][0]                  \r\n__________________________________________________________________________________________________\r\nactivation_99 (Activation)      (None, 256, 256, 16) 0           batch_normalization_99[0][0]     \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_30 (MaxPooling2D) (None, 128, 128, 16) 0           activation_99[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_100 (Conv2D)             (None, 128, 128, 16) 2320        max_pooling2d_30[0][0]           \r\n__________________________________________________________________________________________________\r\nbatch_normalization_100 (BatchN (None, 128, 128, 16) 64          conv2d_100[0][0]                 \r\n__________________________________________________________________________________________________\r\nactivation_100 (Activation)     (None, 128, 128, 16) 0           batch_normalization_100[0][0]    \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_31 (MaxPooling2D) (None, 64, 64, 16)   0           activation_100[0][0]             \r\n__________________________________________________________________________________________________\r\nconv2d_101 (Conv2D)             (None, 64, 64, 16)   2320        max_pooling2d_31[0][0]           \r\n__________________________________________________________________________________________________\r\nbatch_normalization_101 (BatchN (None, 64, 64, 16)   64          conv2d_101[0][0]                 \r\n__________________________________________________________________________________________________\r\nactivation_101 (Activation)     (None, 64, 64, 16)   0           batch_normalization_101[0][0]    \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_32 (MaxPooling2D) (None, 32, 32, 16)   0           activation_101[0][0]             \r\n__________________________________________________________________________________________________\r\nconv2d_102 (Conv2D)             (None, 32, 32, 16)   2320        max_pooling2d_32[0][0]           \r\n__________________________________________________________________________________________________\r\nbatch_normalization_102 (BatchN (None, 32, 32, 16)   64          conv2d_102[0][0]                 \r\n__________________________________________________________________________________________________\r\nactivation_102 (Activation)     (None, 32, 32, 16)   0           batch_normalization_102[0][0]    \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_33 (MaxPooling2D) (None, 16, 16, 16)   0           activation_102[0][0]             \r\n__________________________________________________________________________________________________\r\nconv2d_103 (Conv2D)             (None, 16, 16, 16)   2320        max_pooling2d_33[0][0]           \r\n__________________________________________________________________________________________________\r\nbatch_normalization_103 (BatchN (None, 16, 16, 16)   64          conv2d_103[0][0]                 \r\n__________________________________________________________________________________________________\r\nactivation_103 (Activation)     (None, 16, 16, 16)   0           batch_normalization_103[0][0]    \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_34 (MaxPooling2D) (None, 8, 8, 16)     0           activation_103[0][0]             \r\n__________________________________________________________________________________________________\r\nconv2d_104 (Conv2D)             (None, 8, 8, 16)     2320        max_pooling2d_34[0][0]           \r\n__________________________________________________________________________________________________\r\nbatch_normalization_104 (BatchN (None, 8, 8, 16)     64          conv2d_104[0][0]                 \r\n__________________________________________________________________________________________________\r\nactivation_104 (Activation)     (None, 8, 8, 16)     0           batch_normalization_104[0][0]    \r\n__________________________________________________________________________________________________\r\nconv2d_105 (Conv2D)             (None, 8, 8, 16)     2320        activation_104[0][0]             \r\n__________________________________________________________________________________________________\r\nbatch_normalization_105 (BatchN (None, 8, 8, 16)     64          conv2d_105[0][0]                 \r\n__________________________________________________________________________________________________\r\nactivation_105 (Activation)     (None, 8, 8, 16)     0           batch_normalization_105[0][0]    \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_44 (TensorFl (None, 8, 8, 32)     0           activation_105[0][0]             \r\n                                                                 activation_104[0][0]             \r\n__________________________________________________________________________________________________\r\nconv2d_106 (Conv2D)             (None, 8, 8, 16)     4624        tf_op_layer_concat_44[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_106 (BatchN (None, 8, 8, 16)     64          conv2d_106[0][0]                 \r\n__________________________________________________________________________________________________\r\nactivation_106 (Activation)     (None, 8, 8, 16)     0           batch_normalization_106[0][0]    \r\n__________________________________________________________________________________________________\r\nup_sampling2d_30 (UpSampling2D) (None, 16, 16, 16)   0           activation_106[0][0]             \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_45 (TensorFl (None, 16, 16, 32)   0           up_sampling2d_30[0][0]           \r\n                                                                 activation_103[0][0]             \r\n__________________________________________________________________________________________________\r\nconv2d_107 (Conv2D)             (None, 16, 16, 16)   4624        tf_op_layer_concat_45[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_107 (BatchN (None, 16, 16, 16)   64          conv2d_107[0][0]                 \r\n__________________________________________________________________________________________________\r\nactivation_107 (Activation)     (None, 16, 16, 16)   0           batch_normalization_107[0][0]    \r\n__________________________________________________________________________________________________\r\nup_sampling2d_31 (UpSampling2D) (None, 32, 32, 16)   0           activation_107[0][0]             \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_46 (TensorFl (None, 32, 32, 32)   0           up_sampling2d_31[0][0]           \r\n                                                                 activation_102[0][0]             \r\n__________________________________________________________________________________________________\r\nconv2d_108 (Conv2D)             (None, 32, 32, 16)   4624        tf_op_layer_concat_46[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_108 (BatchN (None, 32, 32, 16)   64          conv2d_108[0][0]                 \r\n__________________________________________________________________________________________________\r\nactivation_108 (Activation)     (None, 32, 32, 16)   0           batch_normalization_108[0][0]    \r\n__________________________________________________________________________________________________\r\nup_sampling2d_32 (UpSampling2D) (None, 64, 64, 16)   0           activation_108[0][0]             \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_47 (TensorFl (None, 64, 64, 32)   0           up_sampling2d_32[0][0]           \r\n                                                                 activation_101[0][0]             \r\n__________________________________________________________________________________________________\r\nconv2d_109 (Conv2D)             (None, 64, 64, 16)   4624        tf_op_layer_concat_47[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_109 (BatchN (None, 64, 64, 16)   64          conv2d_109[0][0]                 \r\n__________________________________________________________________________________________________\r\nactivation_109 (Activation)     (None, 64, 64, 16)   0           batch_normalization_109[0][0]    \r\n__________________________________________________________________________________________________\r\nup_sampling2d_33 (UpSampling2D) (None, 128, 128, 16) 0           activation_109[0][0]             \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_48 (TensorFl (None, 128, 128, 32) 0           up_sampling2d_33[0][0]           \r\n                                                                 activation_100[0][0]             \r\n__________________________________________________________________________________________________\r\nconv2d_110 (Conv2D)             (None, 128, 128, 16) 4624        tf_op_layer_concat_48[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_110 (BatchN (None, 128, 128, 16) 64          conv2d_110[0][0]                 \r\n__________________________________________________________________________________________________\r\nactivation_110 (Activation)     (None, 128, 128, 16) 0           batch_normalization_110[0][0]    \r\n__________________________________________________________________________________________________\r\nup_sampling2d_34 (UpSampling2D) (None, 256, 256, 16) 0           activation_110[0][0]             \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_49 (TensorFl (None, 256, 256, 32) 0           up_sampling2d_34[0][0]           \r\n                                                                 activation_99[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_111 (Conv2D)             (None, 256, 256, 64) 18496       tf_op_layer_concat_49[0][0]      \r\n__________________________________________________________________________________________________\r\nbatch_normalization_111 (BatchN (None, 256, 256, 64) 256         conv2d_111[0][0]                 \r\n__________________________________________________________________________________________________\r\nactivation_111 (Activation)     (None, 256, 256, 64) 0           batch_normalization_111[0][0]    \r\n__________________________________________________________________________________________________\r\ntf_op_layer_AddV2_10 (TensorFlo (None, 256, 256, 64) 0           activation_111[0][0]             \r\n                                                                 activation_98[0][0]              \r\n__________________________________________________________________________________________________\r\nzero_padding2d_1 (ZeroPadding2D (None, 130, 130, 64) 0           tf_op_layer_AddV2_9[0][0]        \r\n__________________________________________________________________________________________________\r\nzero_padding2d_2 (ZeroPadding2D (None, 66, 66, 64)   0           tf_op_layer_AddV2_8[0][0]        \r\n__________________________________________________________________________________________________\r\nzero_padding2d_3 (ZeroPadding2D (None, 34, 34, 64)   0           tf_op_layer_AddV2_7[0][0]        \r\n__________________________________________________________________________________________________\r\nzero_padding2d_4 (ZeroPadding2D (None, 18, 18, 64)   0           tf_op_layer_AddV2_6[0][0]        \r\n__________________________________________________________________________________________________\r\nzero_padding2d_5 (ZeroPadding2D (None, 10, 10, 64)   0           tf_op_layer_AddV2_5[0][0]        \r\n__________________________________________________________________________________________________\r\nzero_padding2d (ZeroPadding2D)  (None, 258, 258, 64) 0           tf_op_layer_AddV2_10[0][0]       \r\n__________________________________________________________________________________________________\r\nconv2d_113 (Conv2D)             (None, 128, 128, 1)  577         zero_padding2d_1[0][0]           \r\n__________________________________________________________________________________________________\r\nconv2d_114 (Conv2D)             (None, 64, 64, 1)    577         zero_padding2d_2[0][0]           \r\n__________________________________________________________________________________________________\r\nconv2d_115 (Conv2D)             (None, 32, 32, 1)    577         zero_padding2d_3[0][0]           \r\n__________________________________________________________________________________________________\r\nconv2d_116 (Conv2D)             (None, 16, 16, 1)    577         zero_padding2d_4[0][0]           \r\n__________________________________________________________________________________________________\r\nconv2d_117 (Conv2D)             (None, 8, 8, 1)      577         zero_padding2d_5[0][0]           \r\n__________________________________________________________________________________________________\r\nconv2d_112 (Conv2D)             (None, 256, 256, 1)  577         zero_padding2d[0][0]             \r\n__________________________________________________________________________________________________\r\nup_sampling2d_35 (UpSampling2D) (None, 256, 256, 1)  0           conv2d_113[0][0]                 \r\n__________________________________________________________________________________________________\r\nup_sampling2d_36 (UpSampling2D) (None, 256, 256, 1)  0           conv2d_114[0][0]                 \r\n__________________________________________________________________________________________________\r\nup_sampling2d_37 (UpSampling2D) (None, 256, 256, 1)  0           conv2d_115[0][0]                 \r\n__________________________________________________________________________________________________\r\nup_sampling2d_38 (UpSampling2D) (None, 256, 256, 1)  0           conv2d_116[0][0]                 \r\n__________________________________________________________________________________________________\r\nup_sampling2d_39 (UpSampling2D) (None, 256, 256, 1)  0           conv2d_117[0][0]                 \r\n__________________________________________________________________________________________________\r\nactivation_112 (Activation)     (None, 256, 256, 1)  0           conv2d_112[0][0]                 \r\n__________________________________________________________________________________________________\r\nactivation_113 (Activation)     (None, 256, 256, 1)  0           up_sampling2d_35[0][0]           \r\n__________________________________________________________________________________________________\r\nactivation_114 (Activation)     (None, 256, 256, 1)  0           up_sampling2d_36[0][0]           \r\n__________________________________________________________________________________________________\r\nactivation_115 (Activation)     (None, 256, 256, 1)  0           up_sampling2d_37[0][0]           \r\n__________________________________________________________________________________________________\r\nactivation_116 (Activation)     (None, 256, 256, 1)  0           up_sampling2d_38[0][0]           \r\n__________________________________________________________________________________________________\r\nactivation_117 (Activation)     (None, 256, 256, 1)  0           up_sampling2d_39[0][0]           \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_50 (TensorFl (None, 256, 256, 6)  0           activation_112[0][0]             \r\n                                                                 activation_113[0][0]             \r\n                                                                 activation_114[0][0]             \r\n                                                                 activation_115[0][0]             \r\n                                                                 activation_116[0][0]             \r\n                                                                 activation_117[0][0]             \r\n__________________________________________________________________________________________________\r\nconv2d_118 (Conv2D)             (None, 256, 256, 1)  7           tf_op_layer_concat_50[0][0]      \r\n__________________________________________________________________________________________________\r\nactivation_118 (Activation)     (None, 256, 256, 1)  0           conv2d_118[0][0]                 \r\n__________________________________________________________________________________________________\r\ntf_op_layer_stack (TensorFlowOp (7, None, 256, 256,  0           activation_118[0][0]             \r\n                                                                 activation_112[0][0]             \r\n                                                                 activation_113[0][0]             \r\n                                                                 activation_114[0][0]             \r\n                                                                 activation_115[0][0]             \r\n                                                                 activation_116[0][0]             \r\n                                                                 activation_117[0][0]             \r\n==================================================================================================\r\nTotal params: 1,136,877\r\nTrainable params: 1,131,181\r\nNon-trainable params: 5,696\r\n\r\n```", "@abattery can you help ?", "In the above summary, every shape is dynamic and the batch dimensions are None. Fixing the batch size in the model may help running your model on the GPU delegate.", "Hey @abattery thanks for the suggestion,\r\nhowever after converting the model to tflite the input details showed batch_size of 1., so maybe that's not the issue.\r\n", "You can wrap the keras model with the concrete function to override the input signature, for example:\r\n\r\n```\r\nclass Model(tf.Module):\r\n  def __init__(self);\r\n   self.keras_model = ...\r\n\r\n  @tf.function(input_signature=[1, 256, 256, 3])\r\n  def serving_default(self, x):\r\n    return {\"result\": self.keras_model(x)}\r\n\r\nm = Model()\r\ntf.saved_model.save(\r\n   m, tmp_dir, signatures=m.serving_default.get_concrete_function())\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(tmp_dir)\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48698\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48698\">No</a>\n", "A better and easy solution would be to load the weights of this model, to an identical model with batch_size fixed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48698\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48698\">No</a>\n"]}]