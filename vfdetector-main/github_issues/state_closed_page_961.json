[{"number": 24598, "title": "Potential tf.boolean_mask bug when the mask array is empty", "body": "**System information**\r\n- OS Platform and Distribution : Windows 10\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 1.11.0\r\n- Python version: 3.6.6\r\n- CUDA/cuDNN version: V8.0.60\r\n- GPU model and memory: Geforce GTX 1070 8GB\r\n\r\n**Describe the current behavior**\r\nI have actually experiencing almost the similar problem like in thread:  https://github.com/tensorflow/tensorflow/issues/24585\r\nAgain, I want to partition a minibatch into different parts, process them in parallel using different computation units and then stitch them back together. However this time I used `tf.boolean_mask` instead of `tf.dynamic_partition` for the partition operation, since the latter runs into problems when one of the partitions is empty. This code is below (it is copy&paste reproducible):\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\ndef build_conv_layer(input, filter_size, num_of_input_channels, num_of_output_channels, name_suffix=\"\"):\r\n    # OK\r\n    conv_weights = tf.Variable(\r\n        tf.truncated_normal([filter_size, filter_size, num_of_input_channels, num_of_output_channels],\r\n                            stddev=0.1, dtype=tf.float32))\r\n    # OK\r\n    conv_biases = tf.Variable(\r\n        tf.constant(0.1, shape=[num_of_output_channels], dtype=tf.float32))\r\n    conv = tf.nn.conv2d(input, conv_weights, strides=[1, 1, 1, 1], padding='SAME')\r\n    relu = tf.nn.relu(tf.nn.bias_add(conv, conv_biases))\r\n    pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\r\n    return pool\r\n\r\n\r\nbatch_size = 250\r\nchild_count = 3\r\nchannel_count = 32\r\n\r\ndataTensor = tf.placeholder(tf.float32, shape=(None, 28, 28, 1), name=\"dataTensor\")\r\nindices_tensor = tf.placeholder(name=\"indices_tensor\", dtype=tf.int32)\r\nbatch_size_tensor = tf.placeholder(name=\"batch_size_tensor\", dtype=tf.int32)\r\n\r\ncondition_indices_list = []\r\npartition_list = []\r\nmask_list = []\r\nfor child_index in range(child_count):\r\n    mask_indices = tf.reshape(indices_tensor[:, child_index], [-1])\r\n    condition_indices = tf.boolean_mask(tf.range(batch_size_tensor), mask_indices)\r\n    partition = tf.boolean_mask(dataTensor, mask_indices)\r\n    mask_list.append(mask_indices)\r\n    condition_indices_list.append(condition_indices)\r\n    partition_list.append(partition)\r\n\r\ntransformed_list = [build_conv_layer(input=part, filter_size=5, num_of_input_channels=1, num_of_output_channels=32)\r\n                    for part in partition_list]\r\nsquared_list = [tf.square(part) for part in partition_list]\r\nstitched_conv_transform = tf.dynamic_stitch(indices=condition_indices_list, data=transformed_list)\r\nstitched_square_transform = tf.dynamic_stitch(indices=condition_indices_list, data=squared_list)\r\nsum = tf.reduce_sum(stitched_square_transform)\r\ngrads = tf.gradients(sum, dataTensor)\r\n\r\nsess = tf.Session()\r\nsamples = np.random.uniform(size=(batch_size, 28, 28, 1))\r\nindices_arr = np.zeros(shape=(batch_size, child_count), dtype=np.int32)\r\nindices_arr[:, 0] = 1\r\nindices_arr[-2] = np.array([0, 1, 0])\r\nindices_arr[-1] = np.array([0, 1, 0])\r\n\r\nfeed_dict = {dataTensor: samples,\r\n             batch_size_tensor: batch_size,\r\n             # indices_tensor: np.argmax(np.random.uniform(size=(GlobalConstants.EVAL_BATCH_SIZE, child_count)), axis=1)}\r\n             indices_tensor: indices_arr}\r\noutputs = []\r\noutputs.extend(mask_list)\r\noutputs.extend(transformed_list)\r\noutputs.extend(squared_list)\r\noutputs.append(stitched_conv_transform)\r\noutputs.append(stitched_square_transform)\r\noutputs.append(sum)\r\noutputs.append(grads)\r\n\r\ninit = tf.global_variables_initializer()\r\nsess.run(init)\r\nfor i in range(10000):\r\n    results = sess.run(outputs, feed_dict=feed_dict)\r\n    assert np.allclose(results[-1][0], 2.0*samples)\r\n    print(\"{0} runned.\".format(i))\r\n```\r\n\r\nTo my disappointment, `tf.boolean_mask` runs into a similar problem, when `indices_arr ` contains no references to at least one partition and it produces an empty array for that partition as the result. The for loop in the end runs correctly a few times but then the program crashes with the following error:\r\n\r\n> InternalError (see above for traceback): WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true / nonzero indices.  temp_storage_bytes: 1, status: invalid configuration argument\r\n\t [[{{node boolean_mask/Where}} = Where[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](boolean_mask/Reshape_1/_9)]]\r\n\t [[{{node DynamicStitch/_49}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_259_DynamicStitch\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nI think this is the same error underlying the problem in https://github.com/tensorflow/tensorflow/issues/24585 where it crashes when `tf.dynamic_partition` receives an empty index array since they could be using the same mechanism in the cub library (or whatever cub is). The `tf.dynamic_partition` error also occurs after a few succesfull iterations like this one. What could be the reason here?", "comments": ["I was able to run your code snippet successfully on cpu however interestingly it failed computing on gpu.", "Indeed this is a bug, fixed by pinning Where to the CPU. I'm submitting a patch soon.", "Thank you @alextp . How can I get the fix now?"]}, {"number": 24597, "title": "Fix: typo in mkl_layout_pass.cc", "body": "This patch fixes the typo in mkl_layout_pass.cc", "comments": []}, {"number": 24596, "title": "Failded to build libtensorflow_inference.so", "body": "<em>Please Help,I try to build libtensorflow_inference.so ,but i get an error.  I guess the problem is my sytem ,I use window10, I don't know</em>\r\n\r\n``bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain  --cpu=armeabi-v7a\r\n``\r\n\r\nthis is my error information:\r\n\r\n``\r\nINFO: Analysed target //tensorflow/contrib/android:libtensorflow_inference.so (0 packages loaded, 2701 targets configured).\r\n``\r\n``\r\nINFO: Found 1 target...\r\n``\r\n``\r\nERROR: C:/users/cuiwanxin/_bazel_cuiwanxin/obpmm2rb/external/com_google_absl/absl/numeric/BUILD.bazel:25:1: C++ compilation of rule '@com_google_absl//absl/numeric:int128' failed (Exit -1). Note: Remote connection/protocol failed with: execution failed: false failed: error executing command\r\n``\r\n``\r\n  cd C:/users/cuiwanxin/_bazel_cuiwanxin/obpmm2rb/execroot/org_tensorflow\r\n``\r\n``\r\n  SET PATH=D:\\mysy64\\usr\\bin\\bash.exe;D:\\mysy64\\usr\\share\r\n``\r\n``\r\n    SET PWD=/proc/self/cwd\r\n``\r\n``\r\n  /bin/false -MD -MF bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/numeric/_objs/int128/int128.pic.d -frandom-seed=bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/numeric/_objs/int128/int128.pic.o -fPIC -iquote external/com_google_absl -iquote bazel-out/armeabi-v7a-opt/genfiles/external/com_google_absl -iquote bazel-out/armeabi-v7a-opt/bin/external/com_google_absl -Wall -Wextra -Wcast-qual -Wconversion-null -Wmissing-declarations -Woverlength-strings -Wpointer-arith -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla -Wwrite-strings -Wno-sign-compare -c external/com_google_absl/absl/numeric/int128.cc -o bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/numeric/_objs/int128/int128.pic.o\r\n``\r\n``\r\nExecution platform: @bazel_tools//platforms:host_platform\r\n``\r\n``\r\nAction failed to execute: java.io.IOException: ERROR: src/main/native/windows/processes-jni.cc(383): CreateProcessW(\"C:\\users\\cuiwanxin\\_bazel_cuiwanxin\\obpmm2rb\\execroot\\org_tensorflow\\bin\\false\" -MD -MF bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/numeric/_objs/int128/int128.pic.d -frandom-see(...)):\r\n``\r\n``\r\nTarget //tensorflow/contrib/android:libtensorflow_inference.so failed to build\r\n``\r\n``\r\nINFO: Elapsed time: 11.720s, Critical Path: 0.07s\r\n``\r\n``\r\nINFO: 0 processes.\r\n``\r\n``\r\nFAILED: Build did NOT complete successfully\r\n``\r\n", "comments": ["Hi @Cuiwanxin1998 , as it is not clear to find the root-cause of the issue, could you fill the following build/installation template.  Also, please report the installation process and commands used.  Here is the [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md).", "Hi @Cuiwanxin1998 @jvishnuvardhan @ymodak \r\nI am facing similar issues while building libtensorflow_inference.so. Is there a standard/detailed procedure for building...?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@Cuiwanxin1998 and @vinayonchip Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in the Github new issue [template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Compiling TF v1.12.0 from sources with:\r\nbazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a --verbose_failures\r\n\r\nGetting exactly same error.", "@hokmund Could you try the solutions provided [here](https://github.com/tensorflow/tensorflow/issues/6356) that worked for someone. Thanks!"]}, {"number": 24595, "title": "Multi-tower support on each GPU in MirroredStrategy and estimator", "body": "\r\n**System information**\r\n- TensorFlow version (you are using): 1.12\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\n*Feature*:\r\nAdd an argument like `num_towers_per_gpu`  in `tf.contrib.distribute.MirroredStrategy` to enable multi-tower computation on each GPU.\r\n*Current state*:\r\nCurrent `tf.contrib.distribute.MirroredStrategy`'s design is one GPU one tower. It does not support mult-tower on one GPU. It has args `devices`, `num_gpus`, `num_gpus_per_worker`, `cross_tower_ops`, `prefetch_on_device`, `auto_shard_dataset`. We can only set the number of GPUs, and the mirrored strategy will assign one tower to each GPU.\r\n\r\n**Will this change the current api? How?**\r\nYes. It will add a new argument like `num_towers_per_gpu` to `tf.contrib.distribute.MirroredStrategy`.\r\n\r\n**Who will benefit with this feature?**\r\nThis feature can make it available to assign multiple towers on each GPU. It can maximize GPU utilization.\r\n\r\n**Any Other info.**\r\n", "comments": ["Why you want multiple replicas per GPU? We'll probably add multiple GPUs per replica for model parallelism but not the opposite.\r\n\r\nThere are many other ways to increase your GPU utilization like increasing your batch size.", "It is difficult to apply the batch training approach to some models with dynamic structures. The computation of each tower in some reinforcement learning tasks is small. I think multiple towers per GPU would be a nice feature for that case.", "@ZhouYuChen,\r\nSorry for the delayed response. In the **`Tensorflow Version 2.x`**, since we use [TF Keras](https://www.tensorflow.org/api_docs/python/tf/keras) predominantly and don't use [Estimators](https://www.tensorflow.org/guide/estimator) much, can you please let us know if this Feature is still relevant? Thanks!", "@rmothukuru \r\nIt is not relevant any more. Thanks!"]}, {"number": 24594, "title": "tensorflow parameter server start with no worker_hosts specific in  cluster def", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (1.8.0):\r\n- Are you willing to contribute it (Yes):\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI'm setting up a distributed tensorflow with ps and worker. Currently tf.train.Server must accept a ClusterDef to define the total cluster before program start.\r\n\r\nIf I only use ps to store and update variables, use worker to train. Why PS must need to know the worker hosts addrs before it starts.\r\n\r\nSuppose I want to setup 1 ps and 2 worker, the cluster_def shows below\r\n\r\nFor PS:\r\n\r\n```\r\n  cluster = tf.train.ClusterSpec({'ps': {0: 'ps0:2222'}})\r\n```\r\nFor worker0:\r\n\r\n```\r\n  cluster = tf.train.ClusterSpec({'ps': {0: 'ps0:2222'}, 'worker': {0: 'localhost:0'}})\r\n```\r\nFor worker1:\r\n\r\n```\r\n cluster = tf.train.ClusterSpec({'ps': {0: 'ps0:2222'}, 'worker': {1: 'localhost:0'}})\r\n```\r\nI don't need to communicate between worker1 and worker0, If i config this, PS and worker can start, but when call sess.run with train_op ,the following fault will cause:\r\n\r\n```\r\ntensorflow.python.framework.errors_impl.InternalError: No worker known as \r\n/job:worker/replica:0/task:0\r\n     [[Node: gradients/LinearRegression/xw_plus_b_grad/BiasAddGrad_S29 = _Recv[client_terminated=false, recv_device=\"/job:ps/replica:0/task:0/device:CPU:0\", send_device=\"/job:worker/replica:0/task:0/device:CPU:0\", send_device_incarnation=-801089107413104296, tensor_name=\"edge_78_gradients/LinearRegression/xw_plus_b_grad/BiasAddGrad\", tensor_type=DT_FLOAT, _device=\"/job:ps/replica:0/task:0/device:CPU:0\"]()]]\r\n```\r\n\r\nSeem when GrpcServer::Init, PS did not know any worker info, its channel_cache is empty, RpcRemoteRendezvous::RecvFromRemoteAsync can not find Worker channel and throw errors. The interesting is worker can connect to ps to fetch variable for forward computing.\r\n\r\nI don't know if we have any plan to support this usage. I don't know whether the error here is a bug or by-design?\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n**Who will benefit with this feature?**\r\nWe want to when any worker host crash, we can re-launch another worker to continue train, but the new worker may change ip and port. In this case, If we support this feature, we can do worker FO simply\r\n**Any Other info.**\r\n", "comments": ["It\u2019s by design. We use the name \u201cps\u201d in TensorFlow in its nominal sense, without a specialized PS construct. The current SendRecvOps require both side to be aware of each other\u2019s endpoint for duplex communication. \r\n\r\nFor example, before each iteration, the worker needs to call RecvTensor RPC to fetch model parameters. It does not use the same channel to push gradients; instead, the PS calls RecvTensor and fetch gradient tensors from the workers (which requires PS to be aware of workers\u2019 endpoint). Compared to a specialized PS that provides PULL/PUSH semantics, the SendRecvOps gets handy in more complex case like model parallelism.\r\n\r\nAs for FO, @saeta posted an excellent explanation here: https://github.com/tensorflow/tensorflow/issues/18880#issuecomment-392842564", "@byronyi I see this commit, but there is not example how to use in ps/server async sgd mode.\r\nhttps://git.codingcafe.org/Mirrors/tensorflow/tensorflow/commit/f28935a7d280b6ba75fe93fe35783d87b9cc2ec9\r\nDo you have any example code for it?", "@piaoyats,\r\nSorry for the delayed response. There is a detailed example for using [Parameter Server Training in asynchronous mode](https://www.tensorflow.org/tutorials/distribute/parameter_server_training). Can you please take a look and let us know if this is what you are looking for? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 24593, "title": "Keras model evaluate() progress bar randomly stops before 100%", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMac OS X 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nVERSION=1.13.0-dev20181225 (note: this is the 2.0-preview)\r\nGIT_VERSION=b'v1.12.0-5131-gc6f3c5dc48'\r\n- Python version:\r\n3.6.6\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nWhen evaluating a Keras model, the progress bar randomly stops before 100% (however, the loss and metrics returned by the function are correct). Also, it does not end with a newline.\r\n\r\n**Describe the expected behavior**\r\nI expect the progress bar to go up to 100% and display a newline.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nnp.random.seed(42)\r\ntf.random.set_seed(42)\r\n\r\n(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.Flatten(input_shape=[28, 28]),\r\n    tf.keras.layers.Dense(10, activation=\"softmax\"),\r\n])\r\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\r\n              optimizer=\"sgd\", metrics=[\"accuracy\"])\r\n\r\nmodel.fit(X_train, y_train, epochs=2)\r\nprint(model.evaluate(X_test, y_test))\r\n```\r\n\r\n**Other info / logs**\r\nHere is the output of this program:\r\n\r\n```\r\nEpoch 1/2\r\n60000/60000 [==============================] - 2s 28us/sample - loss: 32.8388 - acc: 0.8403\r\nEpoch 2/2\r\n60000/60000 [==============================] - 2s 27us/sample - loss: 26.3895 - acc: 0.8683\r\n 9792/10000 [============================>.] - ETA: 0s - loss: 33.9531 - acc: 0.8363[33.969303797870886, 0.8358]\r\n```\r\n\r\nNotice that the evaluation progress bar (last line) does not go up to 100% (it stops at 9792/10000). Moreover, there is no newline at the end, so the function's returned values (`[33.969303797870886, 0.8358]`) are printed on the same line.\r\n\r\nMoreover, when I run the same code again, I get a different output (only the last line differs). This time the progress bar stopped at 9088/10000, but notice that the function's results are the same as above:\r\n\r\n```\r\nEpoch 1/2\r\n60000/60000 [==============================] - 2s 29us/sample - loss: 32.8388 - acc: 0.8403\r\nEpoch 2/2\r\n60000/60000 [==============================] - 2s 29us/sample - loss: 26.3895 - acc: 0.8683\r\n 9088/10000 [==========================>...] - ETA: 0s - loss: 34.8416 - acc: 0.8327[33.969303797870886, 0.8358]\r\n```\r\n", "comments": ["I think the issue was that, `on_batch_end()` was relying on `on_epoch_end()` for the last step processing:\r\nhttps://github.com/tensorflow/tensorflow/blob/8f60a381d210478f21762a6cf14f547a05e98878/tensorflow/python/keras/callbacks.py#L724-L727\r\nbut `on_epoch_end()` was only called for train mode:\r\nhttps://github.com/tensorflow/tensorflow/blob/8f60a381d210478f21762a6cf14f547a05e98878/tensorflow/python/keras/engine/training_arrays.py#L375-L378\r\n", "Created a PR #24633 for the fix.", "Once again, thanks @yongtang , and happy New Year! \ud83d\udc4d ", "Happy New Year\ud83c\udf89\ud83d\udc4d!", "This issue can be closed, because some commit already fixed the problem according to  #24633.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=24593\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=24593\">No</a>\n"]}, {"number": 24592, "title": "Estimators + tf.data iterators incompatible with eager execution enabled", "body": "> Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n\r\nYes\r\n\r\n> OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n\r\nMacOS Mojave version 10.14.2\r\n\r\n> Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n\r\nN/A\r\n\r\n> TensorFlow installed from (source or binary):\r\n\r\nbinary\r\n\r\n> TensorFlow version (use command below):\r\n\r\nv1.12.0-rc2-3-ga6d8ffae09 1.12.0\r\n\r\n> Python version:\r\n\r\nPython 3.6.0\r\n\r\n> Bazel version (if compiling from source):\r\n\r\nN/A\r\n\r\n> GCC/Compiler version (if compiling from source):\r\n\r\nN/A\r\n\r\n> CUDA/cuDNN version:\r\n\r\nN/A\r\n\r\n> GPU model and memory:\r\n\r\nN/A\r\n\r\n**Describe the current behavior**\r\n\r\nSay you train an estimator using [tf.estimator.train_and_evaluate](https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate). The `input_fn` to your estimator returns a [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset). you may want to write a small program to make predictions over a subset of data to sanity-check model training and gain more insight into how your estimator makes predictions, without having to go through the trouble of deploying your model. You may want to, for simplicity, use eager execution to do this. Something like:\r\n\r\n```python\r\nimport tensorflow as tf; tf.enable_eager_execution()\r\n\r\nestimator = tf.estimator.Estimator(\r\n        model.model_fn,\r\n        warm_start_from=tf.train.latest_checkpoint(args.job_dir))\r\nit = model_input_fn().make_one_shot_iterator()\r\n\r\nbatch = it.get_next() # Returns <features>, <label>\r\npredictions = estimator.predict(lambda: tf.data.Dataset.from_tensor_slices(batch))\r\nprint('Prediction:', next(predictions))\r\nprint('Label:', batch[1])\r\n```\r\n\r\nThis currently throws the following error:\r\n\r\n```\r\nRuntimeError: Attempting to capture an EagerTensor without building a function.\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nIdeally this would just work? Unless I'm missing something \ud83d\ude04 \r\n\r\n**Code to reproduce the issue**\r\n\r\n1. Train a toy estimator using `tf.estimator.train_and_evaluate`. Have its input function return a dataset.\r\n1. Write a script similar to the one written above, which basically creates an estimator warm-started from the latest checkpoint in your training job, calls the input function to get the actual labels, then tries to compare that to prediction output from an estimator.\r\n\r\n**Other info / logs**\r\n\r\nThe problem _appears_ to be something that `from_tensor_slices` is doing. Relevant part of my traceback is below\r\n\r\n```\r\n File \"WORKDIR/model_evaluation.py\", line 61, in <lambda>\r\n    predictions = estimator.predict(lambda: tf.data.Dataset.from_tensor_slices(batch))\r\n  File \"PYTHONDIR/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 289, in from_tensor_slices\r\n    return TensorSliceDataset(tensors)\r\n  File \"PYTHONDIR/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1565, in __init__\r\n    for i, t in enumerate(nest.flatten(tensors))\r\n  File \"PYTHONDIR/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1565, in <listcomp>\r\n    for i, t in enumerate(nest.flatten(tensors))\r\n  File \"PYTHONDIR/site-packages/tensorflow/python/framework/ops.py\", line 1050, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"PYTHONDIR/site-packages/tensorflow/python/framework/ops.py\", line 1106, in internal_convert_to_tensor\r\n    raise RuntimeError(\"Attempting to capture an EagerTensor without \"\r\nRuntimeError: Attempting to capture an EagerTensor without building a function.\r\n```", "comments": ["@traviskaufman Would you mind to provide a full reproducible code sample that triggers the error? The code snippet you pasted may not be complete.", "I ran into the same issue. I think the problem is that `predict()` runs a new session, and in that context, accessing the EagerTensor is not supported. A workaround I found works is to convert the tensor into a numpy array and convert it with `numpy_input_fn()`:\r\n```\r\npredictions = estimator.predict(tf.estimator.inputs.numpy_input_fn(\r\n  {\"foo\": features[\"foo\"].numpy()}, shuffle=False))\r\n```\r\n\r\nMy first attempt was to pass the numpy array into `Dataset.from_tensor_slices()`:\r\n```\r\npredictions = estimator.predict(\r\n  lambda: tf.data.Dataset.from_tensor_slices({\"x\": features[\"foo\"].numpy()}))\r\n```\r\nBut that leads to an infinitely yielding `predictions`. I don't know why.", "The problem is in these lines:\r\n\r\n```\r\nit = model_input_fn().make_one_shot_iterator()\r\n\r\nbatch = it.get_next() # Returns <features>, <label>\r\npredictions = estimator.predict(lambda: tf.data.Dataset.from_tensor_slices(batch))\r\n```\r\n\r\nIf you really want to predict on this single batch, going through numpy is the way to go:\r\n\r\n```\r\npredictions = estimator.predict(lambda: tf.data.Dataset.from_tensor_slices(batch.numpy()))\r\n```\r\n\r\nbut this is unusably slow so I think you want to do\r\n\r\n\r\n```\r\npredictions = estimator.predict(model_input_fn)\r\n```\r\n\r\nright?\r\n", "I have the same issue. If I load a dataset from tensorflow_datasets, then how should I define the input-function. Here is my code:\r\n\r\n```python\r\nmnist, info = tfds.load('mnist', with_info=True)\r\n\r\nds_train_orig, ds_test = mnist['train'], mnist['test']\r\n\r\n## Step 1: define the input-function\r\ndef train_input_fn(dataset, batch_size):\r\n    dataset = dataset.map(lambda x:({'image-pixels':tf.reshape(x['image'], (-1,))}, \r\n                                    x['label']))\r\n    return dataset.shuffle(1000).repeat().batch(batch_size)\r\n\r\n## Step 2: define the feature_column:\r\nimage_feature_column = tf.feature_column.numeric_column(key='image-pixels',\r\n                                                        shape=(28*28))\r\n\r\n# image_feature_column\r\n# NumericColumn(key='image-pixels', shape=(784,), default_value=None, dtype=tf.float32, normalizer_fn=None)\r\n\r\n\r\n## Step 3:\r\ndnn_classifier = tf.estimator.DNNClassifier(\r\n    feature_columns=image_feature_column,\r\n    hidden_units=[16, 16],\r\n    n_classes=10)\r\n\r\n## Step 4:\r\ndnn_classifier.train(\r\n    input_fn=lambda:train_input_fn(ds_train_orig, batch_size=32),\r\n    #lambda:iris_data.train_input_fn(train_x, train_y, args.batch_size),\r\n    steps=20)\r\n```\r\n\r\nBut this gives me the same error as given in this thread:\r\n```\r\nRuntimeError: Attempting to capture an EagerTensor without building a function.\r\n```\r\n\r\nI tried to convert the data (step-1) to NumPy using the `map()` method like this:\r\n```python\r\ndef train_input_fn(dataset, batch_size):\r\n    dataset = dataset.map(lambda x:({'features':x['image'].numpy().flatten()}, \r\n                                    x['label']))\r\n    return dataset.shuffle(1000).repeat().batch(batch_size)\r\n```\r\nbut it says that Tensor object does not have attribute numpy.\r\n\r\nIs there any example to use a pre-existing dataset object to use with an estimator? "]}, {"number": 24591, "title": "list index out of range in freeze_graph.py", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):**NO**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**Linux Ubuntu 16.04.5**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:**No**\r\n- TensorFlow installed from (source or binary):**binary**\r\n- TensorFlow version (use command below):**1.12.0**\r\n- Python version:**3.5.2**\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:**9.0/7.0**\r\n- GPU model and memory:**GTX 1080 11GB**\r\n\r\n**Describe the current behavior**\r\nI try to freeze graph generated by tf.esitimator. esitimator will generate *.pbtxt and ckpt file automatically, so I use following command:\r\n`python freeze_graph.py --input_graph=path/to/.pbtxt --input_checkpoint=path/to/model.ckpt-0 --output_graph=path/to/saved/.pb --output_node_name=OutPutOp`\r\n\r\n**Other info / logs**\r\n`Traceback (most recent call last):\r\n  File \"model_freeze.py\", line 494, in <module>\r\n    run_main()\r\n  File \"model_freeze.py\", line 490, in run_main\r\n    app.run(main=my_main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"model_freeze.py\", line 489, in <lambda>\r\n    my_main = lambda unused_args: main(unused_args, flags)\r\n  File \"model_freeze.py\", line 382, in main\r\n    flags.saved_model_tags, checkpoint_version)\r\n  File \"model_freeze.py\", line 364, in freeze_graph\r\n    checkpoint_version=checkpoint_version)\r\n  File \"model_freeze.py\", line 191, in freeze_graph_with_def_protos\r\n    var_list=var_list, write_version=checkpoint_version)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 1102, in __init__\r\n    self.build()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 1114, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 1151, in _build\r\n    build_save=build_save, build_restore=build_restore)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 773, in _build_internal\r\n    saveables = self._ValidateAndSliceInputs(names_to_saveables)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 680, in _ValidateAndSliceInputs\r\n    for converted_saveable_object in self.SaveableObjectsForOp(op, name):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 654, in SaveableObjectsForOp\r\n    variable, \"\", name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 128, in __init__\r\n    self.handle_op = var.op.inputs[0]\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2128, in __getitem__\r\n    return self._inputs[i]\r\nIndexError: list index out of range\r\n`\r\n", "comments": ["@burui11087, \r\nCould you provide a code to reproduce the issue to find root-cause of the issue? \r\nThank you\r\n", "@jvishnuvardhan Thank you for checking on this.  It links to couple other issues #22029 #5387 #4363\r\n\r\nHere I provides a minimal code using tf hub:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\n\r\nmodule = hub.Module(\"https://tfhub.dev/google/imagenet/resnet_v2_50/classification/1\")\r\nsaver = tf.train.Saver()\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    tf.train.write_graph(sess.graph, \"resnet\", \"model.pb\", as_text=False)\r\n    saver.save(sess,'resnet/model')\r\n```\r\n\r\nThen I encountered the IndexError as I run\r\n\r\n`python -m tensorflow.python.tools.freeze_graph --input_graph=resnet/model.pb --input_checkpoint=resnet/model --output_graph=resnet/frozen_graph.pb --output_node_names=module/resnet_v2_50/predictions/Reshape_1  --input_binary`\r\n\r\nAt the same time, i have successfully frozen a plain-vanilla single layer CNN.  Could you point some directions to check?\r\nThanks!", "I have the same problem when using keras advanced layers. ", "same here. ", "Encountered the same error while freezing transformer from tensorflow official models", "I'm getting the same error on a keras implementation of yolo. \r\n\r\nThe error occurs in `__init__` of a `ResourceVariableSaveable` when trying to access `var.op.inputs[0]` on a tensor:\r\n```\r\nTensor(\"batch_normalization_1/beta:0\", shape=(), dtype=resource)\r\n```\r\nop is a \"VarHandleOp\" and `var.op.inputs` is a `tensorflow.python.framework.ops.Operation._InputList` object.\r\n\r\nThe input list in this case is empty. Does the resource variable have to have an input? What should handle_op be in this case?\r\n\r\nAny suggestions for how to proceed?\r\n", "@stevehawley  - have you tried with the tf.keras rather than the native keras?", "@jiayiliu - yes, I am using the internal `tensorflow.keras`.\r\n\r\nMy workaround has been to use `tf.saved_model.simple_save` instead of a `Saver` and have freeze_graph use input_saved_model_dir. \r\n\r\nIt gets me past this problem but I don't consider the issue resolved. If freeze_graph is not going to support working on normal graphs it should deprecate that api and update the docs, or at least put a note that for keras models you have to used saved_model.\r\n\r\n", "Thanks for sharing the workaround @stevehawley .  I agree with you that the problem is not solved, especially the `simple_save` is marked as DEPRECATED in the documentation.   I switched to keras.applications rather than using the tf.hub.", "Unclear why this was assigned to me. TensorFlow Hub was used to produce a failing case, but IIUC the issue exists even without using hub.\r\n", "I am hitting this same problem too.\r\nIs there any updates ? \r\nIs freeze_graph.py still officially supported ? \r\nIs there an alternative way of generating a protobuf out of a tf.graph ?", "I also meet this porblem when using mnasnet and efficientnet to freeze, where I download models from tensorflow/tpu models: https://github.com/tensorflow/tpu/tree/master/models/official. I use tensorflow 1.11.0 and I have checked my input/output node names which are right.\r\n\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/tools/freeze_graph.py\", line 363, in freeze_graph\r\n    checkpoint_version=checkpoint_version)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/tools/freeze_graph.py\", line 190, in freeze_graph_with_def_protos\r\n    var_list=var_list, write_version=checkpoint_version)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 1094, in __init__\r\n    self.build()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 1106, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 1143, in _build\r\n    build_save=build_save, build_restore=build_restore)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 765, in _build_internal\r\n    saveables = self._ValidateAndSliceInputs(names_to_saveables)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 672, in _ValidateAndSliceInputs\r\n    for converted_saveable_object in self.SaveableObjectsForOp(op, name):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 646, in SaveableObjectsForOp\r\n    variable, \"\", name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 128, in __init__\r\n    self.handle_op = var.op.inputs[0]\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2126, in __getitem__\r\n    return self._inputs[i]\r\nIndexError: list index out of range\r\n\r\nCan any one help me? Thx!\r\n", "I got same issue. Here is the [actual script](https://gist.github.com/thepulkitagarwal/36bc45aa43ae43f83baa5111a89be73e#file-freezekerasmodel-py.) help me to freeze the model. Note that you need to modify the output node name manually.  And I successfully loaded the frozen model onto android.", "I get the same issue when I try to freeze the bidirectional LSTM network from tensor2tensor (https://github.com/tensorflow/tensor2tensor/blob/v.1.12.0/tensor2tensor/models/lstm.py).", "I managed to freeze my tensorflow model with `tf.keras` layers in it by using `tf.SavedModel` in conjuntion with `freeze_graph.freeze_graph_with_def_protos`. Below is a generic snippet to convert a `tf.keras` based tensorflow checkpoint into a frozen checkpoint.\r\n\r\nI am working in `tensorflow==1.13.1`\r\n```\r\n<create evaluation graph/model here>\r\n\r\nexport_dir = <SavedModel serving dir>\r\nwith <eval_graph>.as_default() as graph:\r\n    saver = tf.train.Saver()\r\n    init_op = tf.global_variables_initializer()\r\n    \r\n    with tf.Session(graph=graph) as sess:\r\n        sess.run(init_op)\r\n        saver.restore(sess, <checkpoint_path>)\r\n\r\n        builder = tf.saved_model.Builder(export_dir)\r\n        builder.add_meta_graph_and_variables(sess,\r\n            [tf.saved_model.tag_constants.SERVING],\r\n            strip_default_attrs=True)\r\n    builder.save()\r\n\r\n    from tensorflow.python.tools import freeze_graph\r\n    freeze_graph.freeze_graph_with_def_protos(\r\n        ...\r\n        input_saved_model_dir=export_dir,\r\n        saved_model_tags=[tf.saved_model.tag_constants.SERVING]\r\n    )\r\n```\r\n\r\nI was able to freeze a SSD model with Keras mobilenet V2 feature extractor from [tensorflow_models](https://github.com/tensorflow/models/blob/master/research/object_detection/models/ssd_mobilenet_v2_keras_feature_extractor.py)", "@ymodak Should we expect any further updates to graph freezing? freeze_graph.py is still in master. Tensorflow has moved to Keras and the incompatibility should probably be either documented or fixed. ", "Please test with the latest version of tensorflow for using `freeze_graph` tool. \r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24591\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24591\">No</a>\n", "I believe this is still not working with the latest version of tensorflow 2.1.0. The error becomes: `self.handle_op = var.op.inputs[0]\r\nIndexError: tuple index out of range`\r\n@ymodak Any updates on this?\r\n\r\nThe full stack error is:\r\n```\r\n File \"/path/to/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py\", line 491, in <module>\r\n    run_main()\r\n  File \"/path/to/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py\", line 487, in run_main\r\n    app.run(main=my_main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/path/to/python3.6/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/path/to/python3.6/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/path/to/python3.6/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/path/to/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py\", line 486, in <lambda>\r\n    my_main = lambda unused_args: main(unused_args, flags)\r\n  File \"/path/to/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py\", line 378, in main\r\n    flags.saved_model_tags, checkpoint_version)\r\n  File \"/path/to/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py\", line 361, in freeze_graph\r\n    checkpoint_version=checkpoint_version)\r\n  File \"/path/to/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py\", line 190, in freeze_graph_with_def_protos\r\n    var_list=var_list, write_version=checkpoint_version)\r\n  File \"/path/to/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 828, in __init__\r\n    self.build()\r\n  File \"/path/to/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 840, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"/path/to/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 878, in _build\r\n    build_restore=build_restore)\r\n  File \"/path/to/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 482, in _build_internal\r\n    names_to_saveables)\r\n  File \"/path/to/python3.6/site-packages/tensorflow_core/python/training/saving/saveable_object_util.py\", line 349, in validate_and_slice_inputs\r\n    for converted_saveable_object in saveable_objects_for_op(op, name):\r\n  File \"/path/to/python3.6/site-packages/tensorflow_core/python/training/saving/saveable_object_util.py\", line 210, in saveable_objects_for_op\r\n    variable, \"\", name)\r\n  File \"/path/to/python3.6/site-packages/tensorflow_core/python/training/saving/saveable_object_util.py\", line 84, in __init__\r\n    self.handle_op = var.op.inputs[0]\r\nIndexError: tuple index out of range\r\n```", "> I believe this is still not working with the latest version of tensorflow 2.1.0. The error becomes: `self.handle_op = var.op.inputs[0] IndexError: tuple index out of range`\r\n> @ymodak Any updates on this?\r\n> \r\n> The full stack error is:\r\n> \r\n> ```\r\n>  File \"/path/to/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py\", line 491, in <module>\r\n>     run_main()\r\n>   File \"/path/to/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py\", line 487, in run_main\r\n>     app.run(main=my_main, argv=[sys.argv[0]] + unparsed)\r\n>   File \"/path/to/python3.6/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n>     _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n>   File \"/path/to/python3.6/site-packages/absl/app.py\", line 300, in run\r\n>     _run_main(main, args)\r\n>   File \"/path/to/python3.6/site-packages/absl/app.py\", line 251, in _run_main\r\n>     sys.exit(main(argv))\r\n>   File \"/path/to/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py\", line 486, in <lambda>\r\n>     my_main = lambda unused_args: main(unused_args, flags)\r\n>   File \"/path/to/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py\", line 378, in main\r\n>     flags.saved_model_tags, checkpoint_version)\r\n>   File \"/path/to/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py\", line 361, in freeze_graph\r\n>     checkpoint_version=checkpoint_version)\r\n>   File \"/path/to/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py\", line 190, in freeze_graph_with_def_protos\r\n>     var_list=var_list, write_version=checkpoint_version)\r\n>   File \"/path/to/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 828, in __init__\r\n>     self.build()\r\n>   File \"/path/to/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 840, in build\r\n>     self._build(self._filename, build_save=True, build_restore=True)\r\n>   File \"/path/to/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 878, in _build\r\n>     build_restore=build_restore)\r\n>   File \"/path/to/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 482, in _build_internal\r\n>     names_to_saveables)\r\n>   File \"/path/to/python3.6/site-packages/tensorflow_core/python/training/saving/saveable_object_util.py\", line 349, in validate_and_slice_inputs\r\n>     for converted_saveable_object in saveable_objects_for_op(op, name):\r\n>   File \"/path/to/python3.6/site-packages/tensorflow_core/python/training/saving/saveable_object_util.py\", line 210, in saveable_objects_for_op\r\n>     variable, \"\", name)\r\n>   File \"/path/to/python3.6/site-packages/tensorflow_core/python/training/saving/saveable_object_util.py\", line 84, in __init__\r\n>     self.handle_op = var.op.inputs[0]\r\n> IndexError: tuple index out of range\r\n> ```\r\n\r\nDo you have any progress\uff1fI have the same problem.", "Same here. Tensorflow 2.1.0 and keras 2.3.1.", "@cmehrshad Maybe because of TF2.x use resource variables by default. My job is make export_inference_graph.py working in TF_models/slim/ with TF2.x. Then I try print var_list.value before the code line :`saver = saver_lib.Saver(var_list,write_version=checkpoint_version)`, the tensor like 'shape=(), dtype=resource' . when I use `tf.compat.v1.disable_resource_variables()`, error occurs  `NameError: gobal name 'distribute_strategy' is not defined`. so I haven't any solution now.", "I guess the main reason is because of the use of keras\uff0ci change \u201ctf.keras.layers.BatchNormalization \u201d to \"tf.layers.batch_normalization\"\uff0cand the freeze_graph is work.", "Any updates on this? \r\nI have the exact same callstack as @qipengh.\r\nAny updates on the issue? @ymodak ", "Same here. Tensorflow 2.2.0", "Is it fixed? Same here : Tensorflow 2.2", "### **_root cause:_**\r\nmaybe \"list index out of range\" in freeze_graph.py is caused by a bug in saveable_object_util.py\r\n\r\nstack is:\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.14.0/tensorflow/python/tools/freeze_graph.py#L189\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.14.0/tensorflow/python/training/saver.py#L481\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.14.0/tensorflow/python/training/saving/saveable_object_util.py#L82 \r\n\r\n![image](https://user-images.githubusercontent.com/33881637/127797539-b4032bab-d383-4176-80c3-48be6107e77e.png)\r\n\r\n### **_solution:_**\r\nuse tf.graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(add_shapes=True), FLAGS.output_names.split(\",\"))   instead of freeze_graph.py.\r\n\r\n"]}, {"number": 24590, "title": "  java.lang.IllegalArgumentException: ConcatOp : Dimensions of inputs should match: shape[0] = [1,1,1,256] vs. shape[13] = [0,1,1,256]", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):1.12\r\n- Python version:3.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:9.0/7.0\r\n- GPU model and memory:ZR390\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n   I use yolov2 to train my model,and transplant it to mobile phone,in office demo https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android\r\nit crash,output this bug:\r\n 2018-12-27 09:41:33.780 17423-17443/org.tensorflow.demo E/TensorFlowInferenceInterface: Failed to run TensorFlow inference with inputs:[input], outputs:[output]\r\n2018-12-27 09:41:33.783 17423-17443/org.tensorflow.demo E/AndroidRuntime: FATAL EXCEPTION: inference\r\n    Process: org.tensorflow.demo, PID: 17423\r\n    java.lang.IllegalArgumentException: ConcatOp : Dimensions of inputs should match: shape[0] = [1,1,1,256] vs. shape[13] = [0,1,1,256]\r\n    \t [[{{node concat_9}} = ConcatV2[N=19, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Reshape_152, Reshape_153, Reshape_154, Reshape_155, Reshape_156, Reshape_157, Reshape_158, Reshape_159, Reshape_160, Reshape_161, Reshape_162, Reshape_163, Reshape_164, Reshape_165, Reshape_166, Reshape_167, Reshape_168, Reshape_169, Reshape_170, concat_9/axis)]]\r\n        at org.tensorflow.Session.run(Native Method) \r\nIt confused me  long time,can anyone help me ,thanks a lot \r\n**Describe the expected behavior**\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@shenyingying Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support. Github is mainly for addressing bugs in installation and performance. Thanks!", "Closing due to lack of recent activity. Please open new ticket if you see similar issue. Thanks!\r\n"]}, {"number": 24589, "title": "Why can't Python 2.7 install tensorFlow with windows operating system ?  Does TensorFlow not support Python 27 now? Is there a kind person who can tell me, I really urgent!", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["I am afraid you are right. Unfortunately, TensorFlow is not supported on Windows with Python 2.7."]}, {"number": 24588, "title": "[INTEL MKL] Enabled filter caching for convolution.", "body": "When filter is a constant in convolution operations, we can avoid converting filter from TensorFlow format to MKL format by caching it after the first conversion. This cached filter can then be reused in subsequent iterations without having to perform the conversion again.\r\n\r\nThis PR also contains unit tests for the following cases:\r\n\r\n1. To check if the attribute `is_filter_const` is set to true (in the graph pass) when filter is a constant and false otherwise.\r\n2. To ensure that cached filter data is reused when `Conv2D` kernel is run more than once.\r\n\r\nFor quantized Conv2D kernels, we expect `is_filter_const` to be true in the compute kernel.", "comments": ["Hi @tatianashp, @penpornk can you please take a look at this PR? Thanks.", "@bhavani-subramanian Sorry for the delay! I'll look at it today.", "@penpornk, I have addressed your review comments.", "Thanks for the clarification, @penpornk. I have addressed your review comments. ", "Hi @caisq the 2 failing checks don't seem to be related to this PR.\r\n\r\nThese checks fail for another recently submitted PR as well: https://github.com/tensorflow/tensorflow/pull/25186", "@bhavani-subramanian There seems to be a merge conflict. Could you please help take a look if you can resolve it from your side? Thank you!", "@bhavani-subramanian Never mind. It finally got pulled in now. Sorry!", "Thanks @penpornk!"]}, {"number": 24587, "title": "Controlling Scheduling of Unrelated Ops", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.11\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n[tf.control_dependencies](https://www.tensorflow.org/api_docs/python/tf/control_dependencies) can be used to order the operations on same variables in TensorFlow. What's the alternative of control_dependencies if I want to force relative scheduling of completely unrelated operations i.e. operations that don't share input or output variables? A similar discussion:\r\nhttps://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/XFnL2WlnWd8\r\n\r\n**Will this change the current api? How?**\r\nNot Sure\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who wants to schedule operations to optimize performance when multiple GPUs are being used.\r\n**Any Other info.**\r\nSpecifically, if you look at [this gist](https://gist.github.com/xilenteyex/066c5218802c16f4b987c7d086f6f4a5), there are two matrix multiplications, mul1 and mul2, is there a way to force TensorFlow to always compute mul1 before computing mul2 if they are placed on the same device ? In case there is no way, how can I go about adding this functionality ? Any hints ?", "comments": ["The only way to tell TF to execute op A after it finishes executing op B if they have no data dependencies is by adding a control dependency. Adding control dependencies doesn't require anything about the ops, though.\r\n\r\nThe one thing you have to be aware of when adding control dependencies is that you cannot add control dependencies to an op after it's been created (as this can introduce cycles in the graph and all manner of sad things), which was the mistake in the thread you linked to.\r\n\r\nI'll close this issue for now, but please reopen if you have a concrete code example you'd like to know how to make work.", "Hi @alextp ,\r\nThanks a lot for looking into this.  I am not sure how to add control dependencies among low-level graph operations when created using high-level (contrib level) APIs. For example, I am working with [neural network for language model using PTB dataset](https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py) In this code, at [line 180](https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py#L180) `cudnn_rnn.CudnnLSTM` was created. This line will introduce multiple low level compute operations in the tensorflow graph which will have some dependencies. Similarly, at [line 144](https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py#L144) `seq2seq.sequence_loss` API is used to create another set of low-level interdependent compute nodes in the TF graph. How can I introduce control_dependencies among the low-level operations created as a result of single high-level API call as well as across low-level operations created as a result of multiple high-level APIs? Also, I want to do this for multiple models/code examples. Incase, there is no easy way out, I was thinking of modifying default TensorFlow scheduler or writing my own scheduler to execute the operations in the sequence I want. \r\n\r\nPlease let me know whats the best way to go about doing this.\r\nThanks a bunch for looking into this!", "@alextp, \r\nAlso, I am not seeing an option to re-open the issue, not sure why is this so. ", "You can always do\n\nwith tf.control_dependencies(...):\n  seq2seq.some_call_gere()\n\n\n\nOn Mon, Feb 4, 2019 at 2:29 PM xilenteyex <notifications@github.com> wrote:\n\n> @alextp <https://github.com/alextp>,\n> Also, I am not seeing an option to re-open the issue, not sure why is this\n> so.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/24587#issuecomment-460437931>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxbuJOm1XOr4Ou0lqlC1qPAN8nBfXks5vKLREgaJpZM4ZiMtm>\n> .\n>\n\n\n-- \n - Alex\n", "I am not sure, how will this enable me to add control dependencies among the multiple low level compute operations that were part of `seq2seq.some_call_here()`. Can you please elaborate ? For example, let's say `seq2seq.some_call_here()` has 3 low-level operations i.e. **a**, **b** and **c** and **c** depends on both **a** and **b** and there is no dependency between **a** and **b**. How can I make sure that **a** is always executed before **b** ?  ", "To do so you need to refactor your code in seq2seq to expose the building\nof the operations whose dependencies you want to control.\n\nOn Tue, Feb 5, 2019 at 8:34 AM xilenteyex <notifications@github.com> wrote:\n\n> I am not sure, how will this enable me to add control dependencies among\n> the multiple low level compute operations that were part of\n> seq2seq.some_call_here(). Can you please elaborate ? For example, let's\n> say seq2seq.some_call_here() has 3 low-level operations i.e. *a*, *b* and\n> *c* and *c* depends on both *a* and *b* and there is no dependency\n> between *a* and *b*. How can I make sure that *a* is always executed\n> before *b* ?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/24587#issuecomment-460704878>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxY8eg-VpTfmYA3i77LjK0TWJjZGWks5vKbKUgaJpZM4ZiMtm>\n> .\n>\n\n\n-- \n - Alex\n", "Can I also do this by modifying the TensorFlow scheduler ? I ask this because there are a lot of contrib level calls that I am using and in this way, I have to modify all of those. But, If I can control this by modifying the scheduler that will work for my case as well and I don't need to modify any of the contrib code.", "The scheduler should be generic, not specific.\n\nOn Tue, Feb 5, 2019 at 9:00 AM xilenteyex <notifications@github.com> wrote:\n\n> Can I also do this by modifying the TensorFlow scheduler ? I ask this\n> because there are a lot of contrib level calls that I am using and in this\n> way, I have to modify all of those. But, If I can control this by modifying\n> the scheduler that will work for my case as well and I don't need to modify\n> any of the contrib code.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/24587#issuecomment-460715708>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxQPbBsyLLObOLyzv4g0FBzuWw-hSks5vKbiogaJpZM4ZiMtm>\n> .\n>\n\n\n-- \n - Alex\n", "Can I do something like I give an input to the scheduler specific to the current model and if no input is given, it will just do simple topological sorting and execute the graph ? If you think its doable, can you guide me how can I get started on this ?\r\nThanks! ", "The current scheduler does not look like that at all. It is highly parallel and greedy (executes nodes as soon as they're ready) and has no explicit toposorting step inside of it. The only way to enforce execution order is control dependencies.", "Can you point me to the code-files that I should look into to understand scheduler implementation and see for myself if I can modify it to my use-case ? I found [another GitHub issue](https://github.com/tensorflow/tensorflow/issues/13763) very similar to what I am trying to achieve ", "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/executor.cc\n\nOn Tue, Feb 5, 2019 at 10:54 AM xilenteyex <notifications@github.com> wrote:\n\n> Can you point me to the code-files that I should look into to understand\n> scheduler implementation and see for myself if I can modify it to my\n> use-case ? I found another GitHub issue\n> <https://github.com/tensorflow/tensorflow/issues/13763> very similar to\n> what I am trying to achieve\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/24587#issuecomment-460756698>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxXrsSnRlP6YqgHJsgDdv5T2XJEWmks5vKdNfgaJpZM4ZiMtm>\n> .\n>\n\n\n-- \n - Alex\n", "Hi @alextp , thanks for the link.  I looked through https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/executor.cc I am kind of stuck at issues similar to reported in https://github.com/tensorflow/tensorflow/issues/13763#issuecomment-337113301. It will be great if you can have a look and advise how should I proceed.\r\nThanks! ", "As I told you, I believe what you want to do is infeasible.\n\nOn Wed, Feb 6, 2019 at 9:53 AM xilenteyex <notifications@github.com> wrote:\n\n> Hi @alextp <https://github.com/alextp> , thanks for the link. I looked\n> through\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/executor.cc\n> I and am kind of stuck at issues similar to reported in #13763 (comment)\n> <https://github.com/tensorflow/tensorflow/issues/13763#issuecomment-337113301>.\n> It will be great if you can have a look and advise how should I proceed.\n> Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/24587#issuecomment-461118406>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxeLkTIO-H506fN_5-ew8Dr42kfv0ks5vKxZ7gaJpZM4ZiMtm>\n> .\n>\n\n\n-- \n - Alex\n", "Oh okay. Then I am thinking of doing something like add control_flow dependencies after creating the graph. Save this new graph in a file. Reload the new graph into another TF session and execute this new graph with desired control-flow dependencies. Do you think this will work ? Or the control flow dependencies added after node creation will again be ignored in this case as well ?", "They won't be ignored, but you do risk generating invalid graphs.\n\nOn Wed, Feb 6, 2019 at 10:50 AM xilenteyex <notifications@github.com> wrote:\n\n> Oh okay. Then I am thinking of doing something like add control_flow\n> dependencies after creating the graph. Save this new graph in a file.\n> Reload the new graph into another TF session and execute this new graph\n> with desired control-flow dependencies. Do you think this will work ? Or\n> the control flow dependencies added after node creation will again be\n> ignored in this case as well ?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/24587#issuecomment-461139844>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxRpW14DMhjBr2cC_nAHlNbIlFMolks5vKyPzgaJpZM4ZiMtm>\n> .\n>\n\n\n-- \n - Alex\n", "Hi @alextp ,\r\nI tried adding control dependencies after creating the graph, but the graph is not updating. Specifically, I tried modifying the control_inputs field of the ops, but looks like its not mutable. [here is the link of the code that i tried](https://gist.github.com/xilenteyex/bb6b63bafc0691102920b4b6b6f1ad2b). Goal is to modify the graph by adding control dependencies and load this new graph into a new TF program. Do you know what is the right way to do this ?\r\nAlso, I tried adding the control_dependencies using the with `control_dependencies([control_inputs]) `decorator. If dependencies are added using this decorator, they are respected while execution, but the control_inputs field of the operations still stay empty. Am I doing something wrong here ?", "I told you this is not supported. You keep insisting on it, don't. Refactor\nthe code and add the dependencies at creation time. Anything else is\nunlikely to work in a reliable way.\n\nOn Fri, Feb 8, 2019 at 2:00 PM xilenteyex <notifications@github.com> wrote:\n\n> Hi @alextp <https://github.com/alextp> ,\n> I tried adding control dependencies after creating the graph, but the\n> graph is not updating. Specifically, I tried modifying the control_inputs\n> field of the ops, but looks like its not mutable. here is the link of the\n> code that i tried\n> <https://gist.github.com/xilenteyex/bb6b63bafc0691102920b4b6b6f1ad2b>.\n> Goal is to modify the graph by adding control dependencies and load this\n> new graph into a new TF program. Do you know what is the right way to do\n> this ?\n> Also, I tried adding the control_dependencies using the with control_dependencies([control_inputs])\n> decorator. If dependencies are added using this decorator, they are\n> respected while execution, but the control_inputs field of the operations\n> still stay empty. Am I doing something wrong here ?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/24587#issuecomment-461960690>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxRT0ttB0KMVBIeIAkRU1Csa6RT3Pks5vLfNzgaJpZM4ZiMtm>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp a lot of low-level operations are created when I create an [LSTM cell](https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/LSTMCell) for a neural network in my model. I don't know how to add dependencies to each low level operation individually.  Can you guide me with this ? In case, you think its hard to do so, can you tell where control_dependencies are stored for each operation in the protobuf for a saved graph or in the running TF program which can be modified?  ", "After exploring the meta_graph proto buf, I was able to get it to work. Now I can save a graph after modifying the control dependencies and then reload this new graph into another TF session and run it. this way, I am able to add control dependencies after the complete graph is created. For me, this is useful in improving the performance when doing training on multiple GPUs using model parallelism.\r\nI am adding the [code](https://gist.github.com/xilenteyex/97fafd210d73b30443db7dbdb73e6c80) for the toy example incase someone else wants to achieve what I did and closing the issue.\r\n`toy_mamtul_save.py` modifies the TF graph and adds dependencies in the graph of a simple toy matrix multiplication example while `toy_matmul_restore_run.py` uses the modified graph saved  and executes it and logs the timeline for the execution as well so that one can see that control dependencies added after the graph creation in `toy_mamtul_save.py `are respected when running `toy_matmul_restore_run.py`\r\n\r\n@alextp  Thanks a lot for your help!"]}, {"number": 24586, "title": "Using MirroredStrategy for Multi-GPU Training Fails with DNNLinearCombinedClassifier using default FTRL optimizer", "body": "Many similar issues using MirroredStrategy with other optimizers have been filed. Looks like FTRL optimizer needs to be updated as well.\r\n\r\n**Environment**\r\n== cat /etc/issue ===============================================\r\nLinux 1226-234618-hanks421-172-20-170-14 4.4.0-1072-aws #82-Ubuntu SMP Fri Nov 2 15:00:21 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.5 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux 1226-234618-hanks421-172-20-170-14 4.4.0-1072-aws #82-Ubuntu SMP Fri Nov 2 15:00:21 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy                              1.14.3                   \r\nprotobuf                           3.6.1                    \r\ntensorflow                         1.12.0                   \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.12.0\r\ntf.GIT_VERSION = unknown\r\ntf.COMPILER_VERSION = unknown\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /databricks/python/lib:\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nThu Dec 27 00:32:16 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 396.44                 Driver Version: 396.44                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla V100-SXM2...  Off  | 00000000:00:1B.0 Off |                    0 |\r\n| N/A   43C    P0    67W / 300W |    503MiB / 16160MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla V100-SXM2...  Off  | 00000000:00:1C.0 Off |                    0 |\r\n| N/A   42C    P0    54W / 300W |    503MiB / 16160MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  Tesla V100-SXM2...  Off  | 00000000:00:1D.0 Off |                    0 |\r\n| N/A   42C    P0    56W / 300W |    503MiB / 16160MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  Tesla V100-SXM2...  Off  | 00000000:00:1E.0 Off |                    0 |\r\n| N/A   42C    P0    57W / 300W |    503MiB / 16160MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart.so.9.2.148\r\n/usr/local/cuda-9.2/doc/man/man7/libcudart.7\r\n/usr/local/cuda-9.2/doc/man/man7/libcudart.so.7\r\n\r\n**Describe the current behavior**\r\nUsing MirroredStrategy for Multi-GPU Training fails with DNNLinearCombinedClassifier using default FTRL optimizer\r\n\r\n**Other info / logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"/databricks/python/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/databricks/python/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 177, in _call_for_each_tower\r\n    **merge_kwargs)\r\n  File \"/databricks/python/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 665, in _distributed_apply\r\n    self._create_slots(var_list)\r\n  File \"/databricks/python/lib/python2.7/site-packages/tensorflow/python/training/ftrl.py\", line 125, in _create_slots\r\n    with ops.colocate_with(v):\r\n  File \"/databricks/python/lib/python2.7/contextlib.py\", line 17, in __enter__\r\n    return self.gen.next()\r\n  File \"/databricks/python/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 4094, in _colocate_with_for_gradient\r\n    with self.colocate_with(op, ignore_existing):\r\n  File \"/databricks/python/lib/python2.7/contextlib.py\", line 17, in __enter__\r\n    return self.gen.next()\r\n  File \"/databricks/python/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 4146, in colocate_with\r\n    op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op\r\n  File \"/databricks/python/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1307, in internal_convert_to_tensor_or_indexed_slices\r\n    value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/databricks/python/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1146, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/databricks/python/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/values.py\", line 439, in _tensor_conversion_mirrored\r\n    assert not as_ref\r\nAssertionError\r\n```\r\n", "comments": ["I have the same issue trying to use FTRL optimizer with MirroredStrategy on multiple GPUs.", "Also failing for ProximalAdagradOptimizer and AdagradDAOptimizer.", "Does this optimizer https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/optimizers/Ftrl work for you?", "I am experiencing the same with `tf.train.RMSPropOptimizer()` and `tf.train.AdamOptimizer()`:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nfeatures = tf.data.Dataset.from_tensors([1.]).repeat(10000).batch(10)\r\nlabels = tf.data.Dataset.from_tensors([1.]).repeat(10000).batch(10)\r\ntrain_dataset = tf.data.Dataset.zip((features, labels))\r\n\r\ndistribution = tf.contrib.distribute.MirroredStrategy()\r\nwith distribution.scope():\r\n  inputs = tf.keras.layers.Input(shape=(1,))\r\n  predictions = tf.keras.layers.Dense(1)(inputs)\r\n  model = tf.keras.models.Model(inputs=inputs, outputs=predictions)\r\n  model.compile(loss='mean_squared_error', optimizer=tf.train.AdamOptimizer(learning_rate=0.2))\r\nmodel.fit(train_dataset, epochs=5, steps_per_epoch=10)\r\n```\r\nYeilding:\r\n\r\n```\r\n2019-05-23 16:56:03.965220: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nWARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\nIf you depend on functionality not listed there, please file an issue.\r\n\r\nWARNING:tensorflow:Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\nWARNING:tensorflow:From /Users/francis/virtualenvs/tf_venv/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\nWARNING:tensorflow:From /Users/francis/virtualenvs/tf_venv/lib/python3.6/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.cast instead.\r\nWARNING:tensorflow:From /Users/francis/virtualenvs/tf_venv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.cast instead.\r\nTraceback (most recent call last):\r\n  File \"/Users/francis/holograb/next_gen_training/test.py\", line 21, in <module>\r\n    model.fit(train_dataset, epochs=5, steps_per_epoch=10)\r\n  File \"/Users/francis/virtualenvs/tf_venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 880, in fit\r\n    validation_steps=validation_steps)\r\n  File \"/Users/francis/virtualenvs/tf_venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 195, in model_iteration\r\n    f = _make_execution_function(model, mode)\r\n  File \"/Users/francis/virtualenvs/tf_venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 122, in _make_execution_function\r\n    return model._make_execution_function(mode)\r\n  File \"/Users/francis/virtualenvs/tf_venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1983, in _make_execution_function\r\n    self._make_fit_function()\r\n  File \"/Users/francis/virtualenvs/tf_venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1926, in _make_fit_function\r\n    '_fit_function', [self.total_loss] + metrics_tensors)\r\n  File \"/Users/francis/virtualenvs/tf_venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1895, in _make_train_function_helper\r\n    params=self._collected_trainable_weights, loss=self.total_loss)\r\n  File \"/Users/francis/virtualenvs/tf_venv/lib/python3.6/site-packages/tensorflow/python/keras/optimizers.py\", line 763, in get_updates\r\n    grads, global_step=self.iterations)\r\n  File \"/Users/francis/virtualenvs/tf_venv/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 612, in apply_gradients\r\n    update_ops.append(processor.update_op(self, grad))\r\n  File \"/Users/francis/virtualenvs/tf_venv/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 171, in update_op\r\n    update_op = optimizer._resource_apply_dense(g, self._v)\r\n  File \"/Users/francis/virtualenvs/tf_venv/lib/python3.6/site-packages/tensorflow/python/training/adam.py\", line 164, in _resource_apply_dense\r\n    m = self.get_slot(var, \"m\")\r\n  File \"/Users/francis/virtualenvs/tf_venv/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 766, in get_slot\r\n    return mirrored_slot.get(device=var.device)\r\nAttributeError: 'ResourceVariable' object has no attribute 'get'\r\n```", "I solved all my issues with using` tf.keras `and `tf.contrib.distribute.MirroredStrategy()` by upgrading to tf 2.0 nightly. I would recommend the same. \r\nNote: Do not include `model.fit(..)` in the scope of `tf.contrib.distribute.MirroredStrategy()`. ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing this out since I understand it to be resolved, but please let me know if I'm mistaken. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=24586\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=24586\">No</a>\n"]}, {"number": 24585, "title": "tf.dynamic_stitch does not work when one of the partitions has zero elements.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution : Windows 10\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.6.6\r\n- CUDA/cuDNN version: V8.0.60\r\n- GPU model and memory: Geforce GTX 1070 8GB\r\n\r\n\r\n**Describe the current behavior**\r\nI am building a model which partitions a given minibatch into different parts, processes each part with different computation units in parallel and then stitch them back together. To implement this, I decided to use `tf.dynamic_partition` and `tf.dynamic_stitch` methods. I have the following code for testing purposes:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\ndef build_conv_layer(input, filter_size, num_of_input_channels, num_of_output_channels, name_suffix=\"\"):\r\n    # OK\r\n    conv_weights = tf.Variable(\r\n        tf.truncated_normal([filter_size, filter_size, num_of_input_channels, num_of_output_channels],\r\n                            stddev=0.1, dtype=tf.float32))\r\n    # OK\r\n    conv_biases = tf.Variable(\r\n        tf.constant(0.1, shape=[num_of_output_channels], dtype=tf.float32))\r\n    conv = tf.nn.conv2d(input, conv_weights, strides=[1, 1, 1, 1], padding='SAME')\r\n    relu = tf.nn.relu(tf.nn.bias_add(conv, conv_biases))\r\n    pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\r\n    return pool\r\n\r\nbatch_size = 250\r\nchild_count = 3\r\nchannel_count = 32\r\n\r\ndataTensor = tf.placeholder(tf.float32, shape=(None, 28, 28, 1), name=\"dataTensor\")\r\nindices_tensor = tf.placeholder(name=\"indices_tensor\", dtype=tf.int32)\r\nbatch_size_tensor = tf.placeholder(name=\"batch_size_tensor\", dtype=tf.int32)\r\n\r\ncondition_indices = tf.dynamic_partition(data=tf.range(batch_size_tensor), partitions=indices_tensor,\r\n                                         num_partitions=child_count)\r\npartition_list = tf.dynamic_partition(data=dataTensor, partitions=indices_tensor, num_partitions=child_count)\r\ntransformed_list = [build_conv_layer(input=part, filter_size=5, num_of_input_channels=1,\r\n                                     num_of_output_channels=channel_count)\r\n                    for part in partition_list]\r\nstitched = tf.dynamic_stitch(indices=condition_indices, data=transformed_list)\r\n\r\nsess = tf.Session()\r\nsamples = np.random.uniform(size=(batch_size, 28, 28, 1))\r\nindices_arr = np.zeros(shape=(batch_size, ), dtype=np.int32)\r\nindices_arr[-1] = 1\r\nindices_arr[-2] = 0\r\nfeed_dict = {dataTensor: samples,\r\n             batch_size_tensor: batch_size,\r\n             # indices_tensor: np.argmax(np.random.uniform(size=(GlobalConstants.EVAL_BATCH_SIZE, child_count)), axis=1)}\r\n             indices_tensor: indices_arr}\r\noutputs = []\r\noutputs.extend(transformed_list)\r\noutputs.append(stitched)\r\n\r\ninit = tf.global_variables_initializer()\r\nsess.run(init)\r\nfor i in range(10000):\r\n    results = sess.run(outputs, feed_dict=feed_dict)\r\n    print(\"{0} runned.\".format(i))\r\n\r\n\r\n```\r\n\r\nSo, I divide the `dataTensor` into 3 parts with `tf.dynamic_partition` and each part goes through a convolutional layer. After that, they are stitched back together into a single minibatch again, with `tf.dynamic_stitch`, using individual samples' original location information from `condition_indices`. This works without any visible problems when all three partitions have assigned at least one sample. But when at least one partition does have zero samples, like in code above (which is controlled with `indices_arr`), Tensorflow crashes at the `tf.dynamic_stitch` line with the following error:\r\n\r\n> InvalidArgumentError (see above for traceback): data[0].shape = [0,14,14,32] does not start with indices[0].shape = [249]\r\n\r\nIt seems like `tf.dynamic_partition` and  `tf.dynamic_stitch` stops working when one of the partitions is empty (receiving no samples) and **all partitions receive zero samples as the result**. The code above is reproducible with simple copy and paste. Am I missing something or doing something wrong or is this a bug in `tf.dynamic_stitch` implementation? Doesn't it support empty partitions which can occur in practice, if you determine partitions as the result of an algorithm ? \r\n", "comments": ["@ufukcbicici I tried your code example on Ubuntu (with the tf same version) but could not reproduce the error you mentioned. Wondering if the issue only exists on Windows?", "@yongtang Interestingly, again  on Windows but using a much older Tensorflow 1.7.0, I cannot reproduce the error as well. The newest Tensorflow seems to be affected.", "I think this does not reproduce on nightly, so closing it. Please reopen if I'm wrong."]}, {"number": 24584, "title": "Fix warning in losses_utils.py", "body": "While debugging #24577 (https://github.com/tensorflow/tensorflow/issues/24577#issuecomment-450024786) I noticed the following warning:\r\n```\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.cast instead.\r\n```\r\n\r\nThis fix fixes the warning.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 24583, "title": "Fix tf.version.GIT_VERSION to return str (instead of bytes)", "body": "\r\nThis fix fixes the issue raised in #24578 where `tf.version.GIT_VERSION` returns byte instead of string before this fix:\r\n```\r\nPython 3.6.7 (default, Oct 22 2018, 11:32:17)\r\n[GCC 8.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> print(tf.version.GIT_VERSION)\r\nb'v1.12.0-5133-gc343196842'\r\n>>> print(tf.version.COMPILER_VERSION)\r\n4.8.5\r\n>>> print(tf.version.VERSION)\r\n1.13.0-dev20181226\r\n```\r\n\r\nThis fix fixes #24578.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Ping @yifeif to take a look."]}, {"number": 24582, "title": "TF Identity - SparseTensor - Not Working", "body": "`tf.identity` seems to be not working with SparseTensor. This is a problem, because it makes quite impossible to use them with `tf.control_dependencies`.\r\n\r\nBelow a very simple example:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\na = tf.SparseTensor([[0,0]], [0], [800, 256])\r\n\r\nprint_op = tf.print(\"Hello world\")\r\n\r\nwith tf.control_dependencies([print_op]):\r\n    a = tf.identity(a)\r\n```\r\n\r\n```python\r\nTypeError: Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor. Contents: SparseTensor(indices=Tensor(\"SparseTensor_5/indices:0\", shape=(1, 2), dtype=int64), values=Tensor(\"SparseTensor_5/values:0\", shape=(1,), dtype=int32), dense_shape=Tensor(\"SparseTensor_5/dense_shape:0\", shape=(2,), dtype=int64)). Consider casting elements to a supported type.\r\n```\r\n", "comments": ["What is the tensorflow version you used ? Also please fill [this](https://github.com/tensorflow/tensorflow/issues/new/choose) template which helps us to look into the issue.", "I used the latest version of tensorflow.\nI will check with the nightly build version and I will update the post\n\n_Sent from my Galaxy S9+ using [FastHub](https://play.google.com/store/apps/details?id=com.fastaccess.github)_", "@hgadig so I just tested with the following containers:\r\n\r\n* `tensorflow/tensorflow:latest-gpu-py3 (413b9533f92a)` => TF 1.12.0, CUDA 9\r\n* `tensorflow/tensorflow:nightly-gpu-py3 (e06d8999a068)` => TF 1.13.0-dev20181228, CUDA 10\r\n\r\nThe error is reproducible on both. So it should be live on master. The code given above easily allows you to reproduce the error.", "Workaround for SparseTensor:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\na = tf.SparseTensor([[0,0]], [0], [800, 256])\r\nprint_op = tf.print(\"Hello world\")\r\nwith tf.control_dependencies([print_op]):\r\n  a = tf.SparseTensor(a.indices, tf.identity(a.values), a.dense_shape)\r\n```\r\nNote that you must use tf.identity somewhere (e.g., `tf.identity(a.values)`), otherwise TF optimizes away the statement.\r\n", "As you said it's a nice workaround, but this operation has a non negligible cost.\r\nWould it be just possible to make sure SparseTensors are supported by the identity operation ?", "Looks like this was already resolved in recent TF version. I checked it with TF2.8. [Here](https://colab.research.google.com/gist/jvishnuvardhan/4dadabb4bb5fd01305edb7c49fe17423/untitled1170.ipynb) is a gist for reference. \r\n\r\nI am closing this issue as this was resolved. Thanks!"]}, {"number": 24581, "title": "Failed to Build Tensorflow due to extension_dict.cc on Mac OSX", "body": "I am using the latest version of Mac 10.14.1, Python 3.7 (through Anaconda, I then later installed Python 3.6 in an environment to see if it would work, but the same error appears), and Bazel 0.15.0. \r\nWhen configuring the install, I put N for everything and used default for all others. \r\n\r\nI then executed the following to build a CPU-only version since I am on a Mac:\r\n\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\nAfter a long time of waiting for the build process, I've hit the following error that caused my build to fail:\r\n\r\n```\r\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (1 packages loaded).\r\nINFO: Found 1 target...\r\nERROR: /private/var/tmp/_bazel_maisora/ceb5b62c0dc483a80db02f088b1ce574/external/protobuf_archive/BUILD:659:1: C++ compilation of rule '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed (Exit 1)\r\nexternal/protobuf_archive/python/google/protobuf/pyext/extension_dict.cc:184:7: error: assigning to 'char *' from incompatible type 'const char *'\r\n  if (PyString_AsStringAndSize(arg, &name, &name_size) < 0) {\r\n      ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/extension_dict.cc:56:22: note: expanded from macro 'PyString_AsStringAndSize'\r\n       ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \\\r\n                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n1 error generated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 9.701s, Critical Path: 2.59s\r\nINFO: 17 processes: 17 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nHow do I get around this to successfully build tensorflow?", "comments": ["Is this still an issue? I know you used virtual env to try python 3.6 but can you uninstall python 3.7 instead?", "This is still an issue, I will try downgrading to Python 3.6.8 without virtualenv and update this post tomorrow.", "Were you able to get it running yet? Can you please share your progress? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "With exactly same build environment got exactly same error. \r\nAdded information, i tried with tensorflow v1.8.0 and v1.12.0 tags. \r\nIs there a fix for this?  "]}, {"number": 24580, "title": "Tf cast function not changing data type of the source variable", "body": "On using Tf.cast(source_variable,dtype=tf.int64), the datatype of the source variable remains unchanged. If I assign the result of tf.cast to a new_variable, the new_variable's datatype is equal to the \"dtype\" but source_variable.dtype remains the same.\r\n\r\nExample:\r\nsource_variable data-type is float32\r\nresult = tf.cast(source_variable,tf.int32)\r\nThe Data type of \"result\" is int32, but the datatype of \"source_variable\" is float32.\r\n\r\nHow do I change the datatype of the source_variable to int32?", "comments": ["@ram1897, tf.cast function is working as expected. tf.cast creates a new tensor with the specified data type and does not alter the original variable. Here is the solution for your question.  \r\n\r\nsource_variable = tf.cast(source_variable,tf.int32)     \r\n\r\nPlease understand that Github is primarily for addressing bugs in installation and performance. Stackoverflow is a better place to ask/discuss questions like these.  ", "@jvishnuvardhan I tried as your solution before: the problem is after performing:\r\n\r\nsource_variable = tf.cast(source_variable,tf.int32)\r\n\r\nThe modified \"source_variable\" does not have the same Name attribute as the original one i.e:\r\nold_source_variable.name = \"Variable:0\"\r\nmod_source_variable.name = \"some_other_name:0\"", "@ram1897 \r\nPlease post this @stockoverflow as there is a big community to answer questions like this.\r\nhttps://stackoverflow.com/questions/35596629/how-to-convert-tf-int64-to-tf-float32\r\n\r\n I think you can use \"name\" argument to control the name attribute to some extent as shown below.\r\n\r\nimport tensorflow as tf\r\nx=tf.constant(1)\r\nprint(x.dtype)\r\nprint(x.name)\r\nname=x.name\r\nname=name.split(':')[0]\r\nprint(name)\r\nx=tf.cast(x,tf.float32,name=name)\r\nx=tf.cast(x,tf.float32)\r\nprint(x.dtype)\r\nprint(x.name)\r\n\r\nPlease post any followup queries in stackoverflow. \r\n\r\n", "@jvishnuvardhan I have opened up a stackoverflow question 10 days back. No replies from the community there.\r\n**Did you execute the snippet in your comment?**\r\nThe two Printing Name functions in your snippet will give you:\r\nConst:0 and Const_1:0 which is the problem which I reported in the first place.\r\nI want both print statements to give the output as Const:0\r\n\r\nIs this a **Bug** in Tensorflow or the function is meant to behave like this.", "Yes. I executed it.\r\nThe function is performing as it was intended. \r\nI think the attribute is only referencing a variable. When you change the type of a variable, it's reference also changes. If you want to use the attribute for some comparison, you could read, split and compare.\r\nThanks again. This channel is not for support questions like this.  "]}, {"number": 24579, "title": "Unimplemented ops BatchMatMul, Erf, SquaredDifference.", "body": "- OS Platform and Distribution : Linux Ubuntu 16.04\r\n- TensorFlow installed from : binary\r\n- TensorFlow version : 1.12.0\r\n\r\n**Text output from tflite_convert**\r\n```\r\ntflite_convert \r\n--output_file=/tmp/lite_model.tflite \r\n--graph_def_file=frozen_graph.pb \r\n--input_shapes=1,100 \r\n--input_arrays=input_data \r\n--output_arrays=output_data\r\n\r\nRuntimeError: TOCO failed see console for info.\r\nb'2018-12-24 16:31:57.805610: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\\n2018-12-24 16:31:57.807588: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\\n2018-12-24 16:31:57.811170: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Erf\\n2018-12-24 16:31:57.814732: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\\n2018-12-24 16:31:57.816627: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\\n2018-12-24 16:31:57.820222: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Erf\\n2018-12-24 16:31:57.823765: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\\n2018-12-24 16:31:57.825625: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\\n2018-12-24 16:31:57.829252: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Erf\\n2018-12-24 16:31:57.832835: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\\n2018-12-24 16:31:57.834676: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\\n2018-12-24 16:31:57.838239: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Erf\\n2018-12-24 16:31:57.841768: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\\n2018-12-24 16:31:57.843573: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\\n2018-12-24 16:31:57.847116: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Erf\\n2018-12-24 16:31:57.850639: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\\n2018-12-24 16:31:57.852443: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\\n2018-12-24 16:31:57.856015: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Erf\\n2018-12-24 16:31:57.859547: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\\n2018-12-24 16:31:57.861375: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\\n2018-12-24 16:31:57.865034: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Erf\\n2018-12-24 16:31:57.868582: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\\n2018-12-24 16:31:57.870416: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\\n2018-12-24 16:31:57.873967: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Erf\\n2018-12-24 16:31:57.877478: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\\n2018-12-24 16:31:57.879385: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\\n2018-12-24 16:31:57.882895: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Erf\\n2018-12-24 16:31:57.886449: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\\n2018-12-24 16:31:57.888249: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\\n2018-12-24 16:31:57.891826: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Erf\\n2018-12-24 16:31:57.895390: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\\n2018-12-24 16:31:57.897229: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\\n2018-12-24 16:31:57.900830: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Erf\\n2018-12-24 16:31:57.904400: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\\n2018-12-24 16:31:57.906183: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\\n2018-12-24 16:31:57.909826: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Erf\\n2018-12-24 16:31:57.913387: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\\n2018-12-24 16:31:57.942135: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1110 operators, 1758 arrays (0 quantized)\\n2018-12-24 16:31:57.970531: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 1107 operators, 1749 arrays (0 quantized)\\n2018-12-24 16:31:58.007597: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1107 operators, 1749 arrays (0 quantized)\\n2018-12-24 16:31:58.302044:\r\n I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 702 operators, 1261 arrays (0 quantized)\\n2018-12-24 16:31:58.342334:\r\n I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 690 operators, 1250 arrays (0 quantized)\\n2018-12-24 16:31:58.366749:\r\n I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 677 operators, 1224 arrays (0 quantized)\\n2018-12-24 16:31:58.390442:\r\n I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 677 operators, 1224 arrays (0 quantized)\\n2018-12-24 16:31:58.420840:\r\n I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 922432 bytes, theoretical optimal value: 921600 bytes.\\n2018-12-24 16:31:58.425368: F tensorflow/contrib/lite/toco/tflite/export.cc:386] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime.\r\n If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.contrib.lite.TFLiteConverter(). Here is a list of operators for which  you will need custom implementations: BatchMatMul, Erf, SquaredDifference.\\nAborted (core dumped)\\n'\r\nNone\r\n```\r\n\r\n**Any other info / logs**\r\nWhy is the op SquaredDifference listed in the error ?\r\nThe https://www.tensorflow.org/lite/tf_ops_compatibility says tf.squared_difference is compatible.\r\n\r\nThanks in advance.!", "comments": ["Hi, we met SquaredDifference is not supporting operator as well and check from Tensorflow 1.9 to 1.12, all have this problem.\r\nCould you let us know if this is a bug? If yes, could you let us know do you have planning to fix this?\r\nThanks.", "@vinayonchip @samuallin Squared Difference is a supported op now. Can you please install tf-nightly version and check again?\r\nThanks!", "@ymodak , I tried tf-nightly about a week ago, I will try the latest tf-night\r\nThanks.", "@ymodak , Squared Difference support on tf-nightly 1.13.0.dev20190117, thanks.", "I will close this issue now. Thanks for confirming."]}, {"number": 24578, "title": "tf.version.GIT_VERSION looks like repr(GIT_VERSION)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMac OS X 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nVERSION=1.13.0-dev20181225\r\nGIT_VERSION=b'v1.12.0-5131-gc6f3c5dc48'\r\n- Python version:\r\n3.6.6\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nThe current GIT_VERSION is `b'v1.12.0-5131-gc6f3c5dc48'`.\r\n\r\nIt starts with `b'` and ends with `'`, this seems wrong. It may break scripts that rely on the `GIT_VERSION`. Note that the `GIT_VERSION` is fine in TF 1.12 (`v1.12.0-rc2-3-ga6d8ffae09`).\r\n\r\n**Describe the expected behavior**\r\nI expect `v1.12.0-5131-gc6f3c5dc48` rather than `b'v1.12.0-5131-gc6f3c5dc48'`\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nprint(tf.version.GIT_VERSION)\r\n```\r\n\r\n**Other info / logs**\r\nI checked the other versions in `tf.version`, they look fine.\r\n\r\n```python\r\n>>> print(tf.version.COMPILER_VERSION)\r\n4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.42)\r\n>>> print(tf.version.GRAPH_DEF_VERSION)\r\n27\r\n>>> print(tf.version.GRAPH_DEF_VERSION_MIN_CONSUMER)\r\n0\r\n>>> print(tf.version.GRAPH_DEF_VERSION_MIN_PRODUCER)\r\n0\r\n>>> print(tf.version.VERSION)\r\n1.13.0-dev20181225\r\n```", "comments": ["Added PR #24583 for a fix."]}, {"number": 24577, "title": "total_loss attribute not found when fitting a Sequential model using a validation dataset", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMac OS X 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nVERSION=1.13.0-dev20181225\r\nGIT_VERSION=b'v1.12.0-5131-gc6f3c5dc48'\r\n- Python version:\r\n3.6.6\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nI try to fit a `Sequential` model with both a training dataset and a validation dataset. Works fine in TF 1.12, but the same code now fails in 1.13.0-dev20181225 (`total_loss` attribute not found).\r\n\r\n**Describe the expected behavior**\r\nTraining should work fine.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\n\r\nX_train = np.random.rand(1000, 2).astype(np.float32)\r\ny_train = np.random.rand(1000).astype(np.float32)\r\nX_valid = np.random.rand(200, 2).astype(np.float32)\r\ny_valid = np.random.rand(200).astype(np.float32)\r\nbatch_size = 32\r\nlearning_rate = 0.01\r\n\r\ntrain_set = tf.data.Dataset.from_tensor_slices((X_train, y_train)).repeat().batch(batch_size)\r\nvalid_set = tf.data.Dataset.from_tensor_slices((X_valid, y_valid)).repeat().batch(batch_size)\r\n\r\nmodel = keras.models.Sequential([keras.layers.Dense(1)])\r\nmodel.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate))\r\n\r\nmodel.fit(train_set, epochs=5,\r\n          steps_per_epoch=len(X_train) // batch_size,\r\n          validation_data=valid_set,\r\n          validation_steps=len(X_valid) // batch_size)\r\n```\r\n\r\n**Other info / logs**\r\nHere is the stacktrace:\r\n\r\n```python\r\n>>> model.fit(train_set, epochs=5,\r\n...           steps_per_epoch=len(X_train) // batch_size,\r\n...           validation_data=valid_set,\r\n...           validation_steps=len(X_valid) // batch_size)\r\nWARNING:tensorflow:From /Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py:1732: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\r\nWARNING:tensorflow:From /Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 4, in <module>\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 829, in fit\r\n    initial_epoch=initial_epoch)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1492, in fit_generator\r\n    initial_epoch=initial_epoch)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\", line 149, in model_iteration\r\n    mode=mode)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 132, in configure_callbacks\r\n    callback_model._make_eval_function()\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 2063, in _make_eval_function\r\n    '_eval_function', [self.total_loss] + metrics_tensors)\r\nAttributeError: 'Sequential' object has no attribute 'total_loss'\r\n```\r\n", "comments": ["**Note**: to make the code work in TF 1.12, I added `tf.enable_eager_execution()` after importing TensorFlow, and I replaced the `SGD` optimizer with a `GradientDescentOptimizer`.", "@ageron I tried the same tf-nightly on Ubuntu and it looks to be fine:\r\n```\r\n# python3\r\nPython 3.6.7 (default, Oct 22 2018, 11:32:17) \r\n[GCC 8.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.VERSION\r\n'1.13.0-dev20181225'\r\n>>> tf.GIT_VERSION\r\n\"b'v1.12.0-5131-gc6f3c5dc48'\"\r\n>>> exit()\r\nroot@ubuntu:/v# python3 24577.py \r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1253: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:439: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py:1761: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\r\n2018-12-26 20:48:23.241652: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2018-12-26 20:48:23.275192: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2499995000 Hz\r\n2018-12-26 20:48:23.275687: I tensorflow/compiler/xla/service/service.cc:161] XLA service 0x25e4ce0 executing computations on platform Host. Devices:\r\n2018-12-26 20:48:23.275715: I tensorflow/compiler/xla/service/service.cc:168]   StreamExecutor device (0): <undefined>, <undefined>\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.cast instead.\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3067: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.cast instead.\r\nEpoch 1/5\r\n31/31 [==============================] - 0s 9ms/step - loss: 0.1204 - val_loss: 0.1172\r\nEpoch 2/5\r\n31/31 [==============================] - 0s 2ms/step - loss: 0.1114 - val_loss: 0.1122\r\nEpoch 3/5\r\n31/31 [==============================] - 0s 1ms/step - loss: 0.1080 - val_loss: 0.1068\r\nEpoch 4/5\r\n31/31 [==============================] - 0s 1ms/step - loss: 0.1061 - val_loss: 0.1065\r\nEpoch 5/5\r\n31/31 [==============================] - 0s 1ms/step - loss: 0.1056 - val_loss: 0.1040\r\nroot@ubuntu:/v# \r\n```\r\n\r\nI am wondering if you completely uninstall the tensorflow/tf-nightly then reinstall again (or delete packages from site-packages/tensorflow completely) might make a difference?\r\n\r\nOr this could be specific to Mac only issue?", "Hi @yongtang ,\r\nI'm not sure we are using the same version. I installed the 2.0 preview, but it says the version is 1.13.0-dev20181225.", "I was able to run the script successfully in TF 1.13.0-dev20190118. @ageron I will close this issue, please let me know if you still see the error using latest TF build. Thanks!"]}, {"number": 24576, "title": "Cannot import tensorflow.spectral", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMac OS X 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nVERSION=1.13.0-dev20181225\r\nGIT_VERSION=b'v1.12.0-5131-gc6f3c5dc48'\r\n- Python version:\r\n3.6.6\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nI cannot import `tensorflow.spectral`.\r\n\r\n**Describe the expected behavior**\r\nIt should be possible.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nfrom tensorflow import spectral\r\n```\r\n\r\n**Other info / logs**\r\nHere is the stacktrace:\r\n\r\n```python\r\n>>> from tensorflow import spectral\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nImportError: cannot import name 'spectral'\r\n```\r\n", "comments": ["We removed `tf.spectral` and replaced it with `tf.signal`, which also includes more functional related to signal processing. We should probably update the RFC, but it's probably better to do it once all these changes are solidified."]}, {"number": 24574, "title": "Redundant summary in KMeansClustering", "body": "The estimator produces two summaries: `loss` and `loss/raw`. The former is due to the Estimator API, and the latter is due to [this line](https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/contrib/factorization/python/ops/kmeans.py#L209). The summaries are identical, and I\u2019m wondering what the reason for keeping the latter is. Is it that the Estimator API can, in some cases, augment the loss passed to it, or is it a leftover from before the API was introduced? Thanks!", "comments": ["@theweiho Can you please take a look? Thanks!", "I suppose this one is no longer relevant, as `contrib` has been moved out."]}, {"number": 24573, "title": "Calling a Dense layer fails when it is created with kernel_initializer=tf.keras.initializers.Zeros()", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMac OS X 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nVERSION=1.13.0-dev20181225\r\nGIT_VERSION=b'v1.12.0-5131-gc6f3c5dc48'\r\n- Python version:\r\n3.6.6\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nI get a `TypeError` exception when I call a `Dense` layer which was created with `kernel_initializer=tf.keras.initializers.Zeros()`.\r\n\r\n**Describe the expected behavior**\r\nI expect no error, and the kernel should be initialized to zeros, just like when I set `kernel_initializer=\"zeros\"`.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\ninputs = tf.constant([[1., 2.], [3., 4.]])\r\nlayer_init0 = keras.layers.Dense(units=4, kernel_initializer=tf.keras.initializers.Zeros())\r\nprint(layer_init0(inputs))\r\nprint(layer_init0.get_weights())\r\n```\r\n\r\nIf you do not set the `kernel_initializer`, or if you set it to `\"zeros\"`, everything works fine.\r\n\r\n**Other info / logs**\r\nHere is the stacktrace:\r\n\r\n```python\r\n>>> print(layer_init0(inputs))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 541, in __call__\r\n    self._maybe_build(inputs)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1572, in _maybe_build\r\n    self.build(input_shapes)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py\", line 949, in build\r\n    trainable=True)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 355, in add_weight\r\n    aggregation=aggregation)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/base.py\", line 612, in _add_variable_with_custom_getter\r\n    **kwargs_for_getter)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\", line 145, in make_variable\r\n    aggregation=aggregation)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 213, in __call__\r\n    return cls._variable_v1_call(*args, **kwargs)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 176, in _variable_v1_call\r\n    aggregation=aggregation)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 155, in <lambda>\r\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 2489, in default_variable_creator\r\n    import_scope=import_scope, distribute_strategy=distribute_strategy)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 217, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 298, in __init__\r\n    constraint=constraint)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 410, in _init_from_args\r\n    initial_value() if init_from_fn else initial_value,\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\", line 127, in <lambda>\r\n    shape, dtype=dtype, partition_info=partition_info)\r\nTypeError: __call__() got an unexpected keyword argument 'partition_info'\r\n```\r\n\r\n", "comments": ["@ageron I tried with ubuntu but could not reproduce it:\r\n```\r\n# python3\r\nPython 3.6.7 (default, Oct 22 2018, 11:32:17) \r\n[GCC 8.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> from tensorflow import keras\r\n>>> \r\n>>> tf.VERSION\r\n'1.13.0-dev20181225'\r\n>>> tf.GIT_VERSION\r\n\"b'v1.12.0-5131-gc6f3c5dc48'\"\r\n>>> \r\n>>> inputs = tf.constant([[1., 2.], [3., 4.]])\r\n>>> layer_init0 = keras.layers.Dense(units=4, kernel_initializer=tf.keras.initializers.Zeros())\r\n>>> print(layer_init0(inputs))\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:439: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\nTensor(\"dense/BiasAdd:0\", shape=(2, 4), dtype=float32)\r\n>>> print(layer_init0.get_weights())\r\n2018-12-26 20:54:44.832443: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2018-12-26 20:54:44.863208: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2499995000 Hz\r\n2018-12-26 20:54:44.867255: I tensorflow/compiler/xla/service/service.cc:161] XLA service 0x43bd5c0 executing computations on platform Host. Devices:\r\n2018-12-26 20:54:44.867287: I tensorflow/compiler/xla/service/service.cc:168]   StreamExecutor device (0): <undefined>, <undefined>\r\n[array([[0., 0., 0., 0.],\r\n       [0., 0., 0., 0.]], dtype=float32), array([0., 0., 0., 0.], dtype=float32)]\r\n>>> \r\n```\r\n\r\nWondering if the issue is similar to #24577 as well?", "Hi @yongtang ,\r\nI'm not sure we are using the same version. I installed the 2.0 preview, but it says the version is 1.13.0-dev20181225.", "@ageron I used the following command to install tf-nightly on Ubuntu 18.04:\r\n```\r\npip3 install tf-nightly==1.13.0-dev20181225\r\n```\r\n\r\nMay I know the installation method you used?\r\n", "@ageron Think I found out where the issue is. The initializer could be v1 or v2 depending on the version (1.x vs. 2.0) the source code is built.\r\n\r\nFor 2.0 initializer, the signature is:\r\n```\r\ndef _initializer(shape, dtype=dtypes.float32):\r\n```\r\n\r\nFor 1.x initializer, the signature is:\r\n```\r\ndef _initializer(shape, dtype=dtypes.float32, partition_info=None):\r\n```\r\n\r\nHowever, the check of the initializer always assumes the initializer is 1.x:\r\nhttps://github.com/tensorflow/tensorflow/blob/a2f7f39d982682fa8de050e001522581570c510f/tensorflow/python/keras/engine/base_layer_utils.py#L124-L128\r\n\r\n", "@ageron Added a PR #24699 for the fix.", "Thanks once more @yongtang ! \ud83d\udc4d ", "Closing this issue since its resolved. Thanks!", "Hi all. I have the same problem but have no idea how this solution can be implemented."]}, {"number": 24572, "title": "The performance with profiler seems much slower than normal", "body": "\r\n    I found something uninterpretable when profiling with tensorflow profiler.\r\n    The time measured with profiler seems much slower than without it. \r\n            eg: I translate a sentence with the model Transformer, it takes 70ms normally, but it takes 220ms with profiler. code as follows:\r\n             with tf.contrib.tfprof.ProfileContext('/home/work/tmp/profile') as pctx\r\n        Does profiler take too much overhead? Which part costs so much time?? Does the overhead take place in every op or between ops??\r\n", "comments": ["Most of the overhead of tfprof are between steps. It takes a long time to parse the data it collect. So the result is more accurate than it appear.", "@venuswu Closing this out since I understand it to be resolved, but please let me know if I'm mistaken."]}, {"number": 24571, "title": "Request for function argument for tf.device in eager mode", "body": "\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No custom code.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: Using pip install.\r\n- **TensorFlow version (use command below)**: 1.12\r\n- **Python version**: 2.7.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n\r\n### Describe the problem\r\nIn graph mode, `tf.device()` accepts either a device specifier or a function that specifies ops that should run on a specific device. However in eager mode, there's no ability to pass in a function.\r\n\r\nIt's marked as a todo in the code base, so might be in the pipeline but I'd love to know if there's a rough time estimate/anything in the works?\r\n### Source code / logs\r\nHere's the place in the codebase where its marked as a TODO:\r\nhttps://github.com/tensorflow/tensorflow/blob/a6d8ffae097d0132989ae4688d224121ec6d8f35/tensorflow/python/framework/ops.py#L4928", "comments": ["This is not actively being pursued at this time. \r\n\r\nThat said, could you elaborate on the use case? Quitting from [the documentation](https://www.tensorflow.org/api_docs/python/tf/Graph#device):\r\n\r\n\r\n\"If it is a function, it will be treated as a function from Operation objects to device name strings, and invoked each time a new Operation is created. The Operation will be assigned to the device with the returned name.\"\r\n\r\nWith eager execution, there is no `Operation` object, as there is no symbolic execution. So, I'm curious about what function you would be using that doesn't just return a string that can be used.\r\n\r\nPerhaps I'm missing something.", "Ah ok I didn't realize there's no `Operation` object in eager mode.\r\n\r\nMy use case is the following: I have a LSTM network with a Gaussian distribution output. I'm using the TF probability library to create a distribution object parametrized by the outputs of my LSTM network. \r\n\r\nOne of the methods of this TF Probability distribution object invokes the op `MatrixTriangularSolve` at some point in the call stack. This op seems to be much slower on GPU than on CPU. I was hoping to use the function to force this op to run on CPU. In the forward pass, I can do this using the `tf.device` context manager. But in the backward pass, this seems to get run on the GPU which makes the backward pass painfully slow.\r\n\r\nI'd be happy with an alternate solution that forces this op to run on CPU during the backward pass. \r\n\r\n", "@asimshankar Checking to see if you have any inputs/updates regarding the above?", "@nikhil-dev : Sorry, somehow fell through the cracks.\r\nWill have to think about  this a bit more, but in the mean time you can  hack around it by monkey patching `matrix_triangular_solve` (yay Python!) using something like this:\r\n\r\n```python\r\ndef force_matrix_triangular_solve_on_cpu():\r\n  from tensorflow.python.ops import gen_linalg_ops\r\n  from tensorflow.python.ops import linalg_ops\r\n  orig = gen_linalg_ops.matrix_triangular_solve\r\n  def hack(*args, **kwargs):\r\n    with tf.device(\"/cpu:0\"):\r\n      return orig(*args, **kwargs)\r\n  gen_linalg_ops.matrix_triangular_solve = hack\r\n  linalg_ops.matrix_triangular_solve = hack\r\n  tf.linalg.triangular_solve = hack\r\n  tf.matrix_triangular_solve = hack\r\n\r\nforce_matrix_triangular_solve_on_cpu()\r\n```\r\n\r\nClearly not something to encourage :), but just something to work with in the interim.\r\n\r\nFor background: One reason why device functions are a bit iffy with eager is that the `Operation` object gives you access to the whole graph (you can traverse the inputs and output of the `Operation` object), allowing the placement to be a function of the outputs of the `Operation` (as a silly example, the device placement function could choose a device based on the inferred shape of the output of the operation). When eager execution is enabled, no information about the output is available till the kernel is executed. A restricted form of a device placement function may make sense (the device placement can be a function of the name of the operation, or the inputs, but not a function of metadata on the outputs). Which would be something to be explored.", "@asimshankar I just tried the above. By looking at the output from setting `log_device_placement=True`, I see that during the forward pass, it gets scheduled on CPU (even without the above hack). During the backward pass, `matrix_triangular_solve` still seems to be being placed on the GPU even with the hack.", "@nikhil-dev : I updated the hack in the comment above (https://github.com/tensorflow/tensorflow/issues/24571#issuecomment-453630352), I think it works on the backward pass now too.", "The hack works on the backward pass now - thank you!!", "@nikhil-dev,\r\nCan you please confirm if we can close this issue as your problem has been resolved with [this comment](https://github.com/tensorflow/tensorflow/issues/24571#issuecomment-454113321)? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 24570, "title": "Unexpected warning during GeneratorDataset iterator finalization", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 14.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): `('v1.12.0-0-ga6d8ffae09', '1.12.0')`\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory:\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 410.73       Driver Version: 410.73       CUDA Version: 10.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Quadro M2000M       Off  | 00000000:01:00.0  On |                  N/A |\r\n| N/A   57C    P0    N/A /  N/A |    832MiB /  4043MiB |      4%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n```\r\n\r\n**Describe the current behavior**\r\nWhen using `from_generator` to create a `tf.data.Dataset` instance, reading from a one_shot_iterator and not explicitly closing the session object, I observe a warning: \r\n```\r\n W tensorflow/core/kernels/data/generator_dataset_op.cc:78] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.\r\n```\r\n\r\n**Describe the expected behavior**\r\nWould expect the program to finish without a warning.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport itertools\r\n\r\nimport tensorflow as tf\r\n\r\ndef gen():\r\n    for i in itertools.count(1):\r\n        yield (i, [1] * i)\r\n\r\nds = tf.data.Dataset.from_generator(\r\n    gen, (tf.int64, tf.int64), (tf.TensorShape([]), tf.TensorShape([None])))\r\n\r\niterator = ds.make_one_shot_iterator()\r\n\r\nsess = tf.Session()\r\niterator = iterator.get_next()\r\n\r\nsess.run(iterator)\r\n# sess.close()   <-- no warning is shown if closing the session explicitly\r\n```\r\n\r\n**Other info / logs**\r\n```\r\n2018-12-25 23:43:21.645869: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-12-25 23:43:21.684610: W tensorflow/core/kernels/data/generator_dataset_op.cc:78] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.\r\n\t [[{{node PyFunc}} = PyFunc[Tin=[DT_INT64], Tout=[DT_INT64], token=\"pyfunc_2\"](arg0)]]\r\n```\r\n", "comments": ["I'm experiencing a very similar issue with Keras API:\r\n```python\r\nimport itertools\r\nimport tensorflow as tf\r\nimport tensorflow.keras.backend as K\r\n\r\nconfig = tf.ConfigProto(device_count={'GPU': 0})\r\nK.set_session(tf.Session(config=config))\r\n\r\ndef train_fn():\r\n    i = tf.keras.layers.Input(shape=(1,))\r\n    x = tf.keras.layers.Dense(100, activation='relu')(i)\r\n    o = tf.keras.layers.Dense(1, activation='sigmoid')(x)\r\n    model = tf.keras.Model(i, o)\r\n    model.compile('adam', 'mae')\r\n\r\n    def gen():\r\n        for i in itertools.count(1):\r\n            yield [float(i)], [float(i)]\r\n\r\n    ds = tf.data.Dataset.from_generator(\r\n        gen, (tf.float32, tf.float32), (tf.TensorShape([1]), tf.TensorShape([1])))\r\n\r\n    model.fit(ds, steps_per_epoch=10)\r\n\r\ntrain_fn()\r\n```\r\n\r\nOddly, if I in-line `train_fn()` or register model globally (e.g. via `globals()['_model'] = model`), the warning goes away.\r\n\r\nAffects uber/horovod#606.", "cc @mrry ", "@selitvin @alsrgv I could not reproduce the warning on tf-nightly (1.13.0.dev20190124).", "Is this fixed in TF 1.14?", "I can still reproduce this issue in TF 1.14.0.", "Same problem when using tf1.14.", "@rachellim can you please take a look?", "@alsrgv I cannot reproduce the issue in `tf-nightly`. Your code runs without any error. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/385acc9953aa16b441a84618a81a7727/tf-24570_dataset.ipynb). Thanks!", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=24570\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=24570\">No</a>\n", "@jvishnuvardhan we have this issue in 2.0b1", "@bionicles Could you open a new issue by filling issue template and also provide a standalone code to reproduce the issue? As the origianal issue with TF1.x was resolved, it is better to open a new issue so that it will help the community that are using TF2.0. Thanks!", "I meet the same problem in tf 2.2 and python 3.6", "@zhidaole, you can share a code snippet so we can reproduce this issue?", "> I meet the same problem in tf 2.2 and python 3.6\r\n\r\nI have the same issue as well", "@Manoj-M-97 can you share a code snippet to reproduce the issue? Thanks.", "I'm having the same issue using TF 2.2.0 and Python 3.8.4. I'm trying to run a simplified convnet with the following code:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport os\r\nfrom keras import backend as K\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Activation\r\nfrom keras.layers.core import Dense, Flatten\r\nfrom keras.optimizers import Adam\r\nfrom keras.metrics import categorical_crossentropy\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.layers.normalization import BatchNormalization\r\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\r\nfrom sklearn.metrics import confusion_matrix\r\nimport itertools\r\nimport matplotlib.pyplot as plt\r\n\r\ntrain_path = os.path.join(os.path.dirname(__file__), \"./CNN Data/Training Set\")\r\nvalid_path = os.path.join(os.path.dirname(__file__), \"./CNN Data/Validation Set\")\r\ntest_path = os.path.join(os.path.dirname(__file__), \"./CNN Data/Testing Set\")\r\n\r\ntrain_batches = ImageDataGenerator().flow_from_directory(train_path, target_size=(251,388), classes=['Cortex','Medulla', 'Pelvis'], batch_size=6)\r\nvalid_batches = ImageDataGenerator().flow_from_directory(valid_path, target_size=(251,388), classes=['Cortex','Medulla', 'Pelvis'], batch_size=7)\r\ntest_batches = ImageDataGenerator().flow_from_directory(test_path, target_size=(251,388), classes=['Cortex','Medulla', 'Pelvis'], batch_size=7)\r\n\r\nmodel = keras.Sequential([\r\n    Conv2D(64, (3,3), strides=(1, 1), activation='relu', padding='same', input_shape=(251,388, 1)),\r\n    Conv2D(64, (3,3), activation='relu', padding='same'),\r\n    MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'),\r\n    Flatten(),\r\n    Dense(3, activation='softmax')\r\n])\r\n\r\nEPOCHS = 20\r\nINIT_LR = 0.001\r\n\r\nmodel.compile(Adam(learning_rate=INIT_LR), loss='categorical_crossentropy', metrics=['accuracy'])\r\nmodel.fit(\r\n    train_batches, steps_per_epoch=36, epochs=EPOCHS, verbose=1,\r\n    validation_data=valid_batches, validation_steps=18\r\n)\r\n```\r\n\r\nRunning the above code yields the following error message:\r\n\r\n> tensorflow.python.framework.errors_impl.NotFoundError:  No algorithm worked!\r\n         [[node sequential/conv2d/Conv2D (defined at c:/.../TinyNet/Reproduce the Error.py:43) ]] [Op:__inference_train_function_684]\r\n> \r\n> Function call stack:\r\n> train_function\r\n> \r\n> 2020-07-30 14:39:37.905975: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.\r\n>          [[{{node PyFunc}}]]\r\n\r\nI've played around with this code some more and noticed that removing the input_shape argument from the first conv layer eliminates the \"Failed precondition\" error, allowing the program to run normally; likewise, putting another layer (e.g. MaxPooling2D) first and including input_shape causes the error to occur. Furthermore, if I run a single image through the network (which involves removing the ImageDataGenerator().flow_from_directory()), the program runs normally even with the input_shape argument in the first layer.", "Hi Any luck on this issue, I am having the same error.", "same error here:\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError:  OOM when allocating tensor with shape[12,1632,12,12] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n         [[node functional_1/block6h_dwconv/depthwise (defined at train_cod.py:149) ]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n [Op:__inference_train_function_36493]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node functional_1/block6h_dwconv/depthwise:\r\n functional_1/block6h_expand_activation/mul (defined at /usr/local/lib/python3.6/dist-packages/efficientnet/model.py:115)\r\n\r\nFunction call stack:\r\ntrain_function\r\n\r\n2020-10-09 08:28:48.954550: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.", "@emoen , I'd suspect that your issue is something different - looks like your error is an OOM with tensor allocation, and the GeneratorDataset warning is a red herring.", "@selitvin , I was able to run your code without any warning in Tensorflow 2.5 version, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/2d8c26de036ca88f3b6e05760eb21387/24570.ipynb).", "Closing the issue since it is not showing any error in the recent Tensorflow version, if you face any issue again feel free to open new issue by filling the issue template. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24570\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24570\">No</a>\n", "To anyone experiencing the same error, I was training a multi class network and got this error on the first epoch.\r\nMy problem was that the classes I needed to predict were 3 but in the last layer I had written 7 for some reason.\r\nPrediction classes =/= Output nodes count made my error. "]}, {"number": 24569, "title": "Fix the issue when PYTHON_LIB_PATH consists of multiple paths", "body": "This fix tries to address the issue raise in #24567 where `PYTHON_LIB_PATH` does not support specifying multiple paths for bazel.\r\n\r\nThis fix update python_configure.bzl so that it is possible to process multiple paths for `PYTHON_LIB_PATH`.\r\n\r\nThis fix fixes #24567.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 24568, "title": "TF failing to build on Mac OSX because of missing ares.h even after installing c-ares ", "body": "Delete.", "comments": ["@maisora, how did you solve this issue?", "never mind, fixed by downgrade bazel version.", "@PigApple Which bazel version did you use? "]}]