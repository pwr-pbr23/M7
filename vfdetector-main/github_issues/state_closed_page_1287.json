[{"number": 14513, "title": "Add missing conv1d in `tf.contrib.layers`", "body": "Currently, conv1d exists in `tf.layers` but does not in `tf.contrib.layers`.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 14512, "title": "gfile random access file seek implementation inappropriate", "body": "the call stack for tensorflow.python.platform.gfile.GFile.seek function is as follows:\r\nhttps://github.com/tensorflow/tensorflow/blob/294442996b2aeff00b1bfdc7e7169f7cb35bbf3d/tensorflow/python/lib/io/file_io.py#L118\r\nhttps://github.com/tensorflow/tensorflow/blob/18f36927160d05b941c056f10dc7f9aecaa05e23/tensorflow/core/lib/io/buffered_inputstream.cc#L153\r\nhttps://github.com/tensorflow/tensorflow/blob/18f36927160d05b941c056f10dc7f9aecaa05e23/tensorflow/core/lib/io/buffered_inputstream.cc#L126\r\nhttps://github.com/tensorflow/tensorflow/blob/d44d271c9da4d244ce4b2ffaf808adbe4cff759d/tensorflow/core/lib/io/random_inputstream.h#L27\r\nhttps://github.com/tensorflow/tensorflow/blob/d44d271c9da4d244ce4b2ffaf808adbe4cff759d/tensorflow/core/lib/io/inputstream_interface.cc#L27\r\n\r\nThe implementation of SkipNBytes for random access file is read out N bytes. It's not efficient if we seek through one huge file. I think we should override the implementation for random access file.\r\n\r\nI tried seek API for one large hdfs file, it took more than one hour to finish.\r\n", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "@fesun Are you looking for not reading through the data for RandomInputStream? It looks like RandomInputStream relies on RandomAccessFile which does not expose `SizeOfFile` so moving through the position marker will not be enough. We will need to know when EOF is hit.\r\n\r\nOn the other hand, I think it is possible to try reading 1 bytes to `expected_pos -1`, and if it complete, then we know EOF is not hit yet and we could (safely) move the position marker. Added #14557 for that.", "@yongtang Thanks for your fix! This is exactly what I want."]}, {"number": 14511, "title": "How can I have an Online Data Augmentation like Keras in Tensorflow (tf-slim)?", "body": "Hi all,\r\n\r\nI use tf-slim [(this network)][1] for my classification problem. I want to do online data augmentation like [Keras ImageDataGenerator][2] for my images. I know [these functions][3] in Tensorflow, but I need a tutorial or an example in Tensorflow or tf-slim that does this online data augmentation. Would you please, help me to find a way to do this?\r\n\r\n\r\n  [1]: https://github.com/pudae/tensorflow-densenet\r\n  [2]: https://keras.io/preprocessing/image/\r\n  [3]: https://www.tensorflow.org/api_guides/python/image", "comments": ["@sguada are there pointers we can pass along, or might this be something we would add in future documentation? \r\n\r\n@Ellie68 have you also asked on Stack Overflow?\r\n", "Hi @cy89 \r\n\r\nYes, I have already asked on [Stack Overflow](https://stackoverflow.com/q/47258519/8872136). But I didn't get any good answers. ", "Take a look at https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py", "Closing this out, since the `inception_preprocessing.py` example seems pretty good.\r\n\r\nFor future reference, note that this type of question is better asked (and kept) on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\nThe \"New issue\" template has more information on this:\r\n\r\n-----\r\n\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow."]}, {"number": 14510, "title": "index_to_string_table_from_file can use tf.string as vocabulary file", "body": "Fix #14505 \r\n\r\n### How to test\r\n\r\n+ [x] add test case\r\n+ [ ] pass all tests", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Thanks for your approval, @martinwicke. Since the master branch often is broken recently, it might be not easy to make PR pass all tests.", "I know. I'm making sure that failures are unrelated or pre-existing."]}, {"number": 14509, "title": "Function for explicit broadcasting", "body": "It would be convenient to have an explicit function for broadcasting in TensorFlow's Python API, like to [`numpy.broadcast_to`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.broadcast_to.html) or XLA's [Broadcast](https://www.tensorflow.org/performance/xla/operation_semantics#broadcast), and as [requested on StackOverFlow](https://stackoverflow.com/questions/34362193/how-to-explicitly-broadcast-a-tensor-to-match-anothers-shape-in-tensorflow). This would facilitate adding broadcasting-like behavior to the many TensorFlow operations that don't support it out of the box.\r\n\r\nI understand that in general TensorFlow does not implement NumPy's the strided N-dimensional array data model, so unlike the case for NumPy, broadcasting (e.g., with Eigen tensors) can require a copy. This is a good reason to not necessarily build such a version of broadcasting into ops. However, explicit broadcasting rather than using tile/expand_dims can still be very convenient.\r\n", "comments": ["@josh11b can you please comment on the possibility of an explicit broadcast op in future libraries, please? Might such a thing come from us in the future, or might I mark it \"contributions welcome\"? ", "I am not aware of any specific plans to provide this functionality in the short term, and I would expect it to be straightforward to implement (since we already have many ops that broadcast, this would have a subset of the functionality). So this would be a good fit for \"contributions welcome\".\r\n\r\nThe only question is the API -- would it take two tensors as input, the one to broadcast and one describing the shape?", "> The only question is the API -- would it take two tensors as input, the one to broadcast and one describing the shape?\r\n\r\nNumPy's `broadcast_to` API basically works they way. It takes an array to broadcast and a tuple describing the shape. In TensorFlow the shape could either be a TensorShape object or tuple/list (used for static shape analysis), or a 1D tensor (for dynamic shapes).", "There is a quick way to implement a broadcast_to(tensor, shape) function (though not super efficient);\r\n\r\n```\r\ndef broadcast_to(tensor, shape):\r\n  return tensor + tf.zeros(dtype=tensor.dtype, shape=shape)\r\n```", "> There is a quick way to implement a broadcast_to(tensor, shape) function (though not super efficient);\r\n\r\nThe version using `+` appears to work on many but not all dtypes:\r\n```\r\n>>> broadcast_to(tf.convert_to_tensor([True]), (2,)).eval()\r\nTypeError: Value passed to parameter 'x' has DataType bool not in list of allowed values: float16, float32, float64, uint8, int8, int16, int32, int64, complex64, complex128, string\r\n```", "It also does not work for string. I think you might be able to replace +\nwith logical_or to make it work on bool dtypes. Nothing I can come up with\nfor strings, sadly.\n\nOn Thu, Nov 16, 2017 at 12:25 PM, Stephan Hoyer <notifications@github.com>\nwrote:\n\n> There is a quick way to implement a broadcast_to(tensor, shape) function\n> (though not super efficient);\n>\n> The version using + appears to work on many but not all dtypes:\n>\n> >>> broadcast_to(tf.convert_to_tensor([True]), (2,)).eval()\n> TypeError: Value passed to parameter 'x' has DataType bool not in list of allowed values: float16, float32, float64, uint8, int8, int16, int32, int64, complex64, complex128, string\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/14509#issuecomment-345051046>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxbkKvxrFk5uuwIpOzbpwMIVWXLYtks5s3JpRgaJpZM4QbQ_2>\n> .\n>\n\n\n\n-- \n - Alex\n", "Strings were my first try, but to my surprise they work:\r\n```\r\n>>> broadcast_to(tf.convert_to_tensor(['foo']), (2,)).eval()\r\narray(['foo', 'foo'], dtype=object)\r\n```", "Added a PR #15243 for `tf.broadcast_to`. Please take a look."]}, {"number": 14508, "title": "PrefetchDatasetOp checks buffer_size in c++ side", "body": "alternative pr for #14447\r\n\r\ncheck `buffer_size` in c++ side\r\n\r\n### How to test\r\n\r\n+ [x] add test case\r\n+ [x] pass all tests\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please."]}, {"number": 14507, "title": "XLA reports error with 1000 steps of static_bidirectional_rnn", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.2.1 or 1.3\r\n- **Python version**: 2.7.4\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **GCC/Compiler version (if compiling from source)**: 4.8.5\r\n- **CUDA/cuDNN version**: 5.1\r\n- **GPU model and memory**: M40\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nThis issue can only be reproduced when XLA works with static_bidirectional_rnn with 1000 steps, and the \"seq_len\" of static_bidirectional_rnn must be assigned, which means it works with \"dynamic calculation\". When the issue is reproduced, it reports:\r\n\r\n```\r\n2017-11-01 18:47:16.497266: E tensorflow/stream_executor/cuda/cuda_driver.cc:731] failed to load PTX text as a module: CUDA_ERROR_NO_BINARY_FOR_GPU\r\n2017-11-01 18:47:16.497294: E tensorflow/stream_executor/cuda/cuda_driver.cc:736] error log buffer (163 bytes): ptxas application ptx input, line 7231; error   : Kernel '_fusion_1' exceeds parameter space limit of 4352 bytes\r\nptxas fatal   : Ptx assembly aborted due to error\r\n\r\n```\r\nFrom my analysis, a fused XLA instruction requires for more than 1000 input parameters. This further leads to a PTX kernel with 1000+ parameters, which is not accepted by the cuda driver.\r\n\r\nThis is what I found from the PTX ISA documents:\r\n`The maximum memory size supported by PTX for normal (non-opaque type) parameters is 4352 bytes. Prior to PTX ISA version 1.5, the maximum size was 256 bytes.`\r\n\r\nRead more at: http://docs.nvidia.com/cuda/parallel-thread-execution/index.html#ixzz4yGwCVOB7\r\nFollow us: @GPUComputing on Twitter | NVIDIA on Facebook\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@tatatodd could you please either comment or route to the appropriate TensorFlower?", "@jlebar might be able to provide some guidance here.\r\n\r\n@linearhit can you provide a minimal program that demonstrates the error?\r\n\r\nNote that the whole point of static_bidirectional_rnn is that it's fully unrolled, so it's not surprising to me that unrolling 1000 steps might encounter issues; you'll end up with a large graph!\r\n\r\nThat said, it might be useful to look at exactly why we're ending up with so many parameters.  E.g. I'm guessing we end up with one or more parameters per step into this fusion node, and we can probably pack these together into a single tensor.", "Well that's fun.\r\n\r\nWe'll have to change our calling convention in order to fix this.  Which is to say, this is a bug, we should fix it, but I'm not sure it will be simple.", "@linearhit, a reproducer would be appreciated.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Unassigning myself, although if anyone wants to try their hand at fixing this, please be in touch.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Hi @linearhit ! 1.x version are not supported any more . Can you please provide a simple standalone code  in the 2.8 version to replicate the issue?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 14506, "title": "poor performance when XLA works with dynamic control_flow ops", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.2.1\r\n- **Python version**:  2.7.14\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **GCC/Compiler version (if compiling from source)**: 4.8.5\r\n- **CUDA/cuDNN version**: 8.0/5.1.10\r\n- **GPU model and memory**: M40\r\n- **Exact command to reproduce**: \r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nWhen XLA works with:\r\n1, dynamic_rnn\r\n2, static_rnn with \"dynamic calculation\" enabled, specificaly, when the seq_len is assigned.\r\n\r\nIn these cases, the performance of XLA is poor, even result in negative performance optimization.\r\nFrom the time line it seems that the switch/merge ops are breaking the XLA fused instructions into pieces. But i still don't understand why it leads to negative optimization.\r\n\r\nPls let me know if this is a known issue, are there any special reasons for XLA not to support control flow ops? or if there's anything i can do to fix it. Thanks.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["As clarification, when you say negative optimization, do you mean that the performance when using XLA is worse than using vanilla TensorFlow or something else? For which backend are you seeing this?", "@jpienaar \r\nThe backend is GPU. And, yes the performance when using XLA is worse than using vanilla TensorFlow, which is what i called \"negative optimization\".", "Will leave it upto Eugene, but might be helpful to get the exact code that you're running in order to debug.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "The problem is probably that with XLA, we currently perform execution past `max(seq_len)`; and for `dynamic_rnn`, if your `shape(inputs)[1]` (time) changes between session.run calls, it'll re-JIT the graph.  @jpienaar @prb12 does one of you want to \"own\" this issue?  I'm making some changes to enable `dynamic_rnn` to use `max(seq_len)` as the loop bound but someone still needs to document that the input shape should not change too often, except by intelligent padded bucketing in the input pipeline.", "Now that we can compile `tf.while_loop`s with a fixed upper bound on the *max* number of iterations we can easily change `dynamic_rnn` to iterate only over the valid input data.  This change is pretty trivial (i.e. one line) and in progress but we wanted to test it separately since we don't want to break and existing users.\r\n\r\n`dynamic_rnn`s wlth **padded** input data then have performance that is proportional to the max sequence length in the minibatch.  The _memory_ usage for gradients will still be proportional to the padded input size, but this should't usually matter too much in most cases.\r\n\r\nBidirectional rnns should also be doable, but we need to implement a translation of the `ReverseSequence` op for XLA and that turns out to be a bit ugly ;-)", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 60 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 14505, "title": "index_to_string_table_from_file cannot use tf.string as vocabulary file", "body": "Running stock tensorflow on macOS I get:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nprint (tf.__version__)\r\n\r\nwith open('test', 'w') as f:\r\n  f.write('hello\\n')\r\n  f.write('world\\n')\r\n  f.write('!\\n')\r\n\r\nfile_path = tf.constant('test')\r\n#file_path = 'test' # this works\r\n\r\na = tf.constant(['world', 'hello'])\r\n\r\nwith tf.Session() as sess:\r\n  str2i = tf.contrib.lookup.index_table_from_file(vocabulary_file=file_path)\r\n  sess.run(str2i.init)\r\n  i = str2i.lookup(a)\r\n  print(sess.run(i))\r\n\r\n  # Other way doesn't work\r\n  i2str = tf.contrib.lookup.index_to_string_table_from_file(vocabulary_file=file_path)\r\n  sess.run(i2str.init)\r\n  s = i2str.lookup(i)\r\n  print(sess.run(s))\r\n```\r\n\r\nresults in:\r\n\r\n```\r\n1.4.0\r\n2017-11-12 23:29:51.316725: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n[1 0]\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 22, in <module>\r\n    i2str = tf.contrib.lookup.index_to_string_table_from_file(vocabulary_file=file_path)\r\n  File \"/Users/olanymoe/anaconda2/envs/tf3/lib/python3.5/site-packages/tensorflow/python/ops/lookup_ops.py\", line 1121, in index_to_string_table_from_file\r\n    if not vocabulary_file:\r\n  File \"/Users/olanymoe/anaconda2/envs/tf3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 528, in __bool__\r\n    raise TypeError(\"Using a `tf.Tensor` as a Python `bool` is not allowed. \"\r\nTypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.\r\n```\r\n\r\nExpected:\r\n\r\n```\r\n1.4.0\r\n2017-11-12 23:30:22.082767: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n[1 0]\r\n[b'world' b'hello']\r\n```\r\n\r\nDue to None check at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/lookup_ops.py#L1126\r\n\r\nThis is an issue because I can't seem to be able to make `index_to_string_table_from_file` read the vocabulary_file from a `GraphKeys.ASSET_FILEPATHS` collection when storing the graph as a saved_model", "comments": ["@ysuematsu can you comment or offer advice here? ", "Using the underlying ops of `index_to_string_table_from_file` (`tf.contrib.lookup.HashTable ` with `tf.contrib.lookup.TextFileStringTableInitializer`) which can take the file path as a tf.tensor seems to solve the issue of loading the file from the saved_model assets folder, so I assume the proposed patch from @facaiy for input validation will work for my issue.", "```\r\nvocabulary_file = \"vocab_target.txt\"\r\n\r\ntable = tf.contrib.lookup.index_table_from_file(\r\n    vocabulary_file,\r\n    vocab_size=None,\r\n    default_value=-1,\r\n    key_dtype=tf.string,\r\n    key_column_index=0,\r\n    value_column_index=1,\r\n    delimiter='\\t'\r\n)\r\n```\r\n\r\nThis results in,\r\n\r\n>  TypeError: index_table_from_file() got an unexpected keyword argument 'key_column_index'\r\n\r\nAlso, using `TextFileStringTableInitializer` and `HashTable` does not create any assets?\r\n\r\n```\r\nfile_name = tf.constant(\"vocab_target.txt\", tf.string)\r\nid_to_vocab_init = tf.contrib.lookup.TextFileStringTableInitializer(\r\n     file_name,\r\n     key_column_index=1,\r\n     value_column_index=0,\r\n     vocab_size=None,\r\n     delimiter='\\t',\r\n)\r\n        \r\nid_to_vocab_table = tf.contrib.lookup.HashTable(id_to_vocab_init, \"UNK\")\r\nindices = tf.constant([1, 2], tf.int64)\r\nvalues = id_to_vocab_table.lookup(indices)\r\n\r\ntf.tables_initializer().run()\r\nsess.run(init_op)\r\n\r\nsave_path = saver.save(sess, \"./model.ckpt\")\r\nbuilder = tf.saved_model.builder.SavedModelBuilder('./export/')\r\nbuilder.add_meta_graph_and_variables(sess,[\"serve\"])\r\nbuilder.save()\r\n```"]}, {"number": 14504, "title": "Cannot use keras estimator_from_model() in distributed cluster", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: tf.VERSION = 1.4.0 tf.GIT_VERSION = v1.4.0-rc1-11-g130a514\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 8.0.61/6.0.21\r\n- **GPU model and memory**: NVIDIA Tesla M60 8 GB\r\n- **Exact command to reproduce**: See Below\r\n\r\n### Describe the problem\r\nWhen trying to use an estimator that is derived from ```tf.keras.estimator.estimator_from_model()``` and training with ```tf.estimator.train_and_evaluate()```, it will work as expected if in a standalone non-distributed session. However, when in a distributed training cluster and the TF_CONFIG has the cluster information set, there is a an explicit device assignment of an op to a device that is not valid in the current cluster spec.\r\n\r\nBelow is code to reproduce this issue. When ```simulate_cluster``` is set to True an error is throws as shown in the log below. When ```simulate_cluster``` is set to False the network is constructed and trained as intended. It should be noted that the error occurs when calling ```tf.keras.estimator.model_to_estimator(keras_model=model)``` and not when doing the training, the cluster config is required for the distributed training to take place.\r\n\r\nThe TF_CONFIG that is set below is derived from calling the code using the gcloud SDK as follows:\r\n```gcloud ml-engine local train --distributed --parameter-server-count=1 --worker-count=2 --package-path=trainer --module-name=trainer.task --```\r\n\r\n### Source code / logs\r\nMinimal example:\r\n```python\r\nimport os\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\nsimulate_cluster = True\r\nif simulate_cluster:\r\n    os.environ[\"TF_CONFIG\"] = '{\"environment\": \"cloud\", \"cluster\": {\"worker\": [\"localhost:27184\", \"localhost:27185\"], \\\r\n               \"ps\": [\"localhost:27183\"], \"master\": [\"localhost:27182\"]}, \"job\": {\"args\": [\"\"], \\\r\n               \"job_name\": \"trainer.task\"}, \"task\": {\"index\": 0, \"type\": \"master\"}}'\r\nelse:\r\n    os.environ[\"TF_CONFIG\"] = ''\r\n\r\ninputs = tf.keras.layers.Input(shape=(10,))\r\noutputs = tf.keras.layers.Dense(10)(inputs)\r\nmodel = tf.keras.models.Model(inputs, outputs)\r\nmodel.compile(optimizer='Adam', loss='binary_crossentropy')\r\nest_keras = tf.keras.estimator.model_to_estimator(keras_model=model) # InvalidArgumentError thrown here if simulate_cluster is True\r\n\r\ninput_name = model.input_names[0]\r\ndata = np.random.rand(1000,10).astype(np.float32)\r\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn({input_name:data}, data, batch_size=10, num_epochs=None, shuffle=False)\r\n\r\ntrain_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=100)\r\neval_spec = tf.estimator.EvalSpec(input_fn=train_input_fn, steps=10)\r\ntf.estimator.train_and_evaluate(est_keras, train_spec, eval_spec)\r\n```\r\n\r\nInvalidArgumentError emitted when ```simulate_cluster = True```:\r\n```python\r\nTraceback (most recent call last):\r\n  File \"minimal.py\", line 19, in <module>\r\n    est_keras = tf.keras.estimator.model_to_estimator(keras_model=model)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/_impl/keras/estimator.py\", line 280, in model_to_estimator\r\n    _save_first_checkpoint(keras_model, est, custom_objects, keras_weights)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/_impl/keras/estimator.py\", line 217, in _save_first_checkpoint\r\n    model.set_weights(keras_weights)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/_impl/keras/engine/topology.py\", line 766, in set_weights\r\n    K.batch_set_value(tuples)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/_impl/keras/backend.py\", line 2406, in batch_set_value\r\n    get_session().run(assign_ops, feed_dict=feed_dict)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/_impl/keras/backend.py\", line 376, in get_session\r\n    _initialize_variables(session)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/_impl/keras/backend.py\", line 554, in _initialize_variables\r\n    [variables_module.is_variable_initialized(v) for v in candidate_vars])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'loss/dense_1_loss/sub': Operation was explicitly assigned to /job:master/task:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:GPU:0, /job:localhost/replica:0/task:0/device:GPU:1, /job:localhost/replica:0/task:0/device:GPU:2, /job:localhost/replica:0/task:0/device:GPU:3 ]. Make sure the device specification refers to a valid device.\r\n         [[Node: loss/dense_1_loss/sub = Sub[T=DT_FLOAT, _device=\"/job:master/task:0\"](loss/dense_1_loss/sub/x, loss/dense_1_loss/Const)]]\r\n\r\nCaused by op u'loss/dense_1_loss/sub', defined at:\r\n  File \"minimal.py\", line 19, in <module>\r\n    est_keras = tf.keras.estimator.model_to_estimator(keras_model=model)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/_impl/keras/estimator.py\", line 280, in model_to_estimator\r\n    _save_first_checkpoint(keras_model, est, custom_objects, keras_weights)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/_impl/keras/estimator.py\", line 209, in _save_first_checkpoint\r\n    custom_objects)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/_impl/keras/estimator.py\", line 124, in _clone_and_build_model\r\n    target_tensors=target_tensors)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/_impl/keras/engine/training.py\", line 840, in compile\r\n    output_loss = weighted_loss(y_true, y_pred, sample_weight, mask)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/_impl/keras/engine/training.py\", line 444, in weighted\r\n    score_array = fn(y_true, y_pred)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/_impl/keras/losses.py\", line 78, in binary_crossentropy\r\n    return K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/_impl/keras/backend.py\", line 3027, in binary_crossentropy\r\n    output = clip_ops.clip_by_value(output, epsilon_, 1 - epsilon_)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\", line 910, in r_binary_op_wrapper\r\n    return func(x, y, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 4636, in _sub\r\n    \"Sub\", x=x, y=y, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'loss/dense_1_loss/sub': Operation was explicitly assigned to /job:master/task:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:GPU:0, /job:localhost/replica:0/task:0/device:GPU:1, /job:localhost/replica:0/task:0/device:GPU:2, /job:localhost/replica:0/task:0/device:GPU:3 ]. Make sure the device specification refers to a valid device.\r\n         [[Node: loss/dense_1_loss/sub = Sub[T=DT_FLOAT, _device=\"/job:master/task:0\"](loss/dense_1_loss/sub/x, loss/dense_1_loss/Const)]]\r\n```\r\n\r\nFull logs, tf_env, and more are here: https://gist.github.com/droidicus/2abd4ddad81a1e9169a1c7a100057b15\r\n", "comments": ["Which instructions/examples are you following? Usually there are some extra steps needed to configure the cluster", "Primary reference for the Keras model -> Estimator: https://www.tensorflow.org/api_docs/python/tf/keras/estimator/model_to_estimator\r\nPrimary reference for the TF_CONFIG settings for cluster: https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate\r\nPrimary reference for gcloud ml-engine: https://cloud.google.com/sdk/gcloud/reference/ml-engine/\r\n\r\nI agree that the code snippet above will likely not proceed to completion when ```simulate_cluster``` is set to true even without the error shown above, since this is just simulating a single job (in this case master). It is rather intended to show that when the custer spec is set in the ```TF_CONFIG``` variable (as required by ```tf.estimator.train_and_evaluate()```) the call to ```tf.keras.estimator.model_to_estimator()``` fails with an ```InvalidArgumentError```.\r\n\r\nThe code after the call to ```tf.keras.estimator.model_to_estimator()``` is there only to show that the estimator created from the keras model works correctly when ```simulate_cluster``` is set to false, and is not intended to show a full working distributed training solution. I do have a full solution, but it is much larger and multi-file, this was an attempt at a minimal example of the keras estimator error only.\r\n\r\nThe full model training code is based on a working example in TFv1.3 using ```tf.estimator.EstimatorSpec()``` to manually create an estimator from a keras model ops, ```tf.contrib.learn.Experiment()```, and ```tf.contrib.learn.learn_runner.run()```. The hope here is to transition over to core functions since TFv1.4 now has them.", "That page seems to be for distributed training of estimator. Don't know if distributed training on Keras is supported, cc @fchollet \r\n\r\nFor distributed training, session needs to be created in a different way, ie, `tf.Session(\"grpc://....\")`. From your error it looks like keras creates just a regular local session `tf.Session()`", "I'm experimenting exactly the same issue.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Gentle ping, this is still an issue for me, and had been confirmed above.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "We are seeing this same issue with model_to_estimator, and it is currently blocking a large project. \r\n\r\nAny update?", "me too.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Probably also multi_gpu is not well supported. See https://github.com/tensorflow/tensorflow/issues/16907#issuecomment-366825713", "@yifeif Do you have coverage of tf.keras.utils.multi_gpu_mode with this commit?", "@bhack I suspect not.  _clone_and_build_model relies on keras.models.clone_model, which appears to copy the underlying keras model, but does not preserve device placement.  \r\n\r\nI had to hack model_to_estimator to call multi_gpu_model after the cloned model is created. I suspect a better fix is to have keras.models.clone_model respect device placement, or to remove the need to clone the model altogether.  It is unclear to me why this is needed, rather than to use the input keras_model directly in model_fn and _save_first_checkpoint.\r\n\r\nThis is critical to support distributed training + multi_gpu.  Ideally, this would be officially supported in tf 1.6.", "@JVOSU so is it related also to https://github.com/tensorflow/tensorflow/issues/16468?", "Any update on this?\r\nHaving an example that use keras + multi gpu +  model_to_estimator + distributed cluster seems in my opinion pretty important for anyone the want to do production work with google cloud ml.", "@zippeurfou This is now closed. Probably you can open a new Documentation/example issue  mentioning this one.", "The original issue should be resolved in TFv1.6, although I will admit to not having had the chance to test it yet. As for a comprehensive example, that would be nice, but to the best of my knowledge it doesn't exist currently.", "@droidicus Cloud ML is not yet on 1.6 [ source ](https://cloud.google.com/ml-engine/docs/runtime-version-list) but I guess it will come at some point or you can manually force the installation.\r\n@bhack As much as I agree with you I really think that just a gist would be enough for most people than a full tutorial especially since this is the first result when you google `distributed model_to_estimator`.\r\n", "I still think that is partially a counter sense to non having Highlevel api documented or with example cause are the first ones that a beginner is approaching. As @JVOSU confirmed probably `model_to_estimator` is not working with Distributed/Multi-gpu also If I think that users will try to use it also if undocumented :smile:"]}, {"number": 14503, "title": "Error: CUDNN_STATUS_INTERNAL_ERROR Any help is appreciated. ", "body": "Dears, I am struglling with tensorflow installation. \r\n\r\nI have GeForce GTX1080 , Ubuntu16.04 cuda 9 cudnn7 . nvidia driver 384.98. I try to run the mnist samle coud from cudd_samples v7 and I got this error: \r\nError: CUDNN_STATUS_INTERNAL_ERROR\r\nhere is nvidia-smi\r\n-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.98                 Driver Version: 384.98                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:03:00.0  On |                  N/A |\r\n| 30%   47C    P8    13W / 250W |    581MiB / 11171MiB |      1%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 108...  Off  | 00000000:04:00.0 Off |                  N/A |\r\n| 23%   36C    P8     9W / 250W |      2MiB / 11172MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1390      G   /usr/lib/xorg/Xorg                           331MiB |\r\n|    0      2158      G   compiz                                       196MiB |\r\n|    0      2878      G   ...-token=04563B517E2C68A632F4C6B61B846229    51MiB |\r\n\r\n\r\nnvcc -V\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2017 NVIDIA Corporation\r\nBuilt on Fri_Sep__1_21:08:03_CDT_2017\r\nCuda compilation tools, release 9.0, V9.0.176\r\n\r\n\r\nPlease help. ", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14502, "title": "Estimator does not not support sparse jobs", "body": "You need sparse jobs (https://github.com/tensorflow/tensorflow/commit/8177edd7700ccbe0c831e680a1acb5275819d762) if you want async training to proceed without having all workers available.\r\n\r\nThis currently doesn't work because estimator [uses json](https://github.com/tensorflow/tensorflow/blob/9ff1aebc2b3dcb07f15b92c0a58027b0438f502a/tensorflow/python/estimator/run_config.py#L379) to dump values into `TF_CONFIG` env var, and json does not allow numeric keys. `json.dumps` automatically converts numeric keys into strings.\r\n\r\nHowever, clusterspec for sparse job must have integers like `{\"local\": {37: \"localhost:0\"}}` and will crash if we have `\"37\"` instead of `37`.\r\n\r\nSuggestion 1: Modify sparse job support to allow strings as task indices\r\n\r\nSuggestion 2: use pickle + base16 encoding to transmit these values\r\n\r\nBase16 is shell-friendly, it allows you to copy paste the value into \"export TF_....\" in terminal\r\n\r\n```\r\n # saving\r\n cluster_config = {'cluster': cluster_spec, 'task': task_spec}\r\n pickle_string = pickle.dumps(sparse_cluster_config)\r\n pickle_string_encoded = base64.b16encode(pickle_string)\r\n pickle_string_encoded = pickle_string_encoded.decode('ascii')\r\n export_command = \"export TF_PICKLE_BASE16=%s\"%(pickle_string_encoded,)\r\n os.system(export_command)\r\n\r\n# loading\r\nconfig_dict = pickle.loads(base64.b16decode(os.environ[\"TF_PICKLE_BASE16\"]))\r\nconfig.task_type = config_dict[\"task\"][\"type\"]\r\nconfig.task_id = config_dict[\"task\"][\"index\"]\r\nconfig.cluster_spec = config_dict[\"cluster\"]\r\nreturn config\r\n```\r\n\r\ncc @ispirmustafa ", "comments": ["It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "PS, this is one reason why Estimator sucks for large training jobs -- dense job spec means training can't start until all the workers are up", "@martinwicke for priority. ", "Hi yaroslavvb, thanks for tracking this. \r\n\r\nAs the TF_CONFIG contains all the information required to populate the RunConfig properties, we need to be careful about how to support this. For example, the num_worker_replicas is calculated based on the number of the training jobs inside the TF_CONFIG. If we allow sparse cluster spec directly at environment variable TF_CONFIG level, num_workers_replicas will be filled with wrong value. User will be very hard to automatically figure out how to configure the SyncReplicaOptimizer, which replies on the number of the worker in the system. \r\n\r\nOne alternative option could be: We keep TF_CONFIG as is, but use an opt-in approach to turn on the sparse cluster spec when bring up the tf.train.Server. To be precise, at Estimator level, we still see the global information for the whole cluster, but when launching the tf.train.Server to connect to each other, we take the advantage of the sparse cluster spec. \r\n\r\nWhat's your thoughts on this? Are you willing to contribute here? \r\n\r\nIf yes, we could work on a plan how to change/test here. \r\n\r\n\r\n\r\n\r\n\r\n\r\n", "@xiejw not sure about contributing here because I'm not sure whether Estimator is going to fit this use-case. IE, I need high performance models at large scale and the official high performance models [guide](https://www.tensorflow.org/performance/performance_models) doesn't use Estimator. Also the advice on this list has been to use that instead of Estimator.", "So, @yaroslavvb, then what's your proposal here? Maybe I misread your comment, but feel like you do not need this feature, right?", "I guess I'm saying that it makes sense to implement this feature as part of concerted effort to make Estimator work on large-scale problems. I didn't want to do it since I don't know what Estimator plans are.", "Given this, I am going to close this and revisit/reopen once we have use case or clients. Maybe very soon."]}, {"number": 14501, "title": "Correct space in ValueError for input rank", "body": "The current error message is `ValueError: ('Inputs should have rank 4Received input shape:', '(?, n)')` where a space is missing, so the change would result in `ValueError: ('Inputs should have rank 4. Received input shape:', '(?, n)')`", "comments": ["Can one of the admins verify this patch?", "Another pull request fixes this error message. Closing in favor of the other one."]}, {"number": 14500, "title": "Sgdr fix", "body": "Proposed fix for SGDR implementation.\r\nDeveloped together with the author @loshchil\r\n", "comments": ["Can one of the admins verify this patch?", "Dear Vijay Vasudevan @vrv ,\r\n\r\nPlease consider to review this request given the following \r\ni) A new function linear_cosine_decay recently added to TensorFlow implements some features (minimum learning rate and restarts) that for an unknown reason are missing in cosine_decay function implemented by the same TensorFlow contributor. Note that these features **were proposed in SGDR and also implemented here https://github.com/tensorflow/tensorflow/pull/11749 in TensorFlow *before* linear_cosine_decay was implemented**. Thus, the contributor of linear_cosine_decay function didn't take into account both the existing code and academic sources.\r\nii) You reviewed and accepted our implementation of SGDR  https://github.com/tensorflow/tensorflow/pull/11749 in TensorFlow  which preceded the one where linear_cosine_decay was introduced.\r\niii) You are a co-author of the paper where linear cosine decay was introduced.\r\niv) The current implementation of linear_cosine_decay misleads the users of TensorFlow to think that the use of minimum learning rate and restarts together with cosine decay was introduced by linear_cosine_decay function and the corresponding paper.\r\n\r\nPlease consider our fix designed to avoid misleading TensorFlow users from industry and academia.\r\n\r\nThank you for your consideration.\r\nIlya", "This looks good to me.  If you want to have the cosine_decay_restarts in the public API , you'll need to also add the call out here: https://github.com/tensorflow/tensorflow/blob/e9d2b60ed3d94eef7a3cf139cc27ec629f510681/tensorflow/python/training/training.py#L41, and then I'll have to ask for someone from the API review team to approve the change as well.\r\n\r\nDo you feel strongly about the new parameter name 'first_decay_steps' ?  In the contrib version, it is named differently. \r\n\r\nLastly, I would also change the contrib version of the code to call this public version so there's not two copies of the implementation in the codebase that are largely the same.\r\n\r\nOtherwise, looks good, and thanks for contributing!\r\n\r\n", "We used the name **initial_period_steps** . We changed the name to **first_decay_steps** because  **linear_cosine_decay** and **cosine_decay** functions have a parameter **decay_steps** . We thought that this way we will keep the naming more consistent. I'm not sure which name is better, the current one or the previous one.", "Okay, sounds good.  Let's proceed with adding the callouts in the public API then?", "(I believe API review was skipped last week due to NIPS conference, I'm optimistic it'll happen this week).", "Thanks Asim!\r\n\r\nI like the idea of fewer public symbols, however, I am not sure whether cosine_decay should default to having restarts enabled or disabled.  The warm restarts idea was a big contribution of the original paper, but I kind of preferred the simplicity of removing hyperparameters that cosine_decay currently has.  I figured having two public APIs, one simple, one complex might be preferred.\r\n\r\nSimilarly, with linear_cosine_decay, etc; if we add num_periods and beta and other fields to cosine_decay, the documentation and code gets more complicated, and I thought it might make sense to keep things simple.\r\n\r\nWhat do you all think?", "@vrv : Sounds good. Rule of thumb: If it gets hard to explain the interaction between the arguments, then yeah, separate functions seem like a good idea. Thanks.", "I think it might be fine to just keep them as separate functions, though they could probably share implementations more in the future.\r\n\r\nI think you just need to resolve the conflicts in the file and we can get this in.", "@PatrykChrabaszcz fee free to pull rebase and push again.", "@drpngx \r\nIt should be done if I did not mess something up (first time I do this).", "Jenkins, test this please.", "@PatrykChrabaszcz could you look into the build failures?", "@drpngx I didn't remove one line that uses old \"sgdr_learning_rate_decay.py\" file from the BUILD file, fixed now", "Jenkins, test this please.", "Right, you need to update the goldens on the API. This adds a new operator in `tf.train`.", "@drpngx I'm afraid that I do not understand what \"update the goldens on the API\" means :) . I see that some checks for Python2 fail because of API changes. Is there something I should do now to fix it? ", "@PatrykChrabaszcz If you look at the logs here, you will see instructions about how to run the api compatibility golden API generation files, and then you can include the changed files in a subsequent push.\r\n\r\nhttps://source.cloud.google.com/results/invocations/08c9e185-a549-411a-9b11-0394502a8d7b/targets/%2F%2Ftensorflow%2Ftools%2Fapi%2Ftests:api_compatibility_test/log\r\n\r\nLet us know if that's not helpful enough!\r\n\r\n@drpngx perhaps we should have these instructions on CONTRIBUTIONS.md somewhere, rather than buried in a test file log.", "/CC @MarkDaoust @gunan +1 @vrv can't hurt to have it in the contributions doc.", "@vrv \r\nI had some errors while compiling with the first command.\r\nI had to add some additional options:\r\nbazel build --copt=-msse4.1 --copt=-msse4.2 tensorflow/tools/api/tests:api_compatibility_test\r\n\r\nI hope everything is ok now\r\n\r\n", "Jenkins, test this please.", "Jenkins, test this please.", "Timeout on `//tensorflow/python/keras:data_utils_test`, trying again."]}, {"number": 14499, "title": "fix assert_shallow_structure for dicts", "body": "The function `tensorflow.python.data.util.nest.flatten_up_to` used in `tf.data.Dataset.from_generator` does not compare the dict keys (only the length of the dict)\r\n```python\r\ntf_tree = dict(A=tf.placeholder(tf.float32), B=tf.placeholder(tf.float64))\r\nnp_tree = dict(A=np.zeros([1], np.float32), C=np.zeros([2], np.float64))\r\nnest.flatten_up_to(tf_tree, np_tree)  # Fails now\r\n```\r\nand ignores nested dict's (Bug, iterate over keys in `tensorflow.python.data.util.assert_shallow_structure`)\r\n```python\r\ntf_tree = dict(A=tf.placeholder(tf.float32), B=dict(C=tf.placeholder(tf.float64)))\r\nnp_tree = dict(A=np.zeros([1], np.float32), B=dict(D=np.zeros([2], np.float64)))\r\nnest.flatten_up_to(tf_tree, np_tree)  # Fails now\r\n```\r\n\r\nThis PR fix `tensorflow.python.data.util.assert_shallow_structure` that is used in `tensorflow.python.data.util.nest.flatten_up_to`, but there may also other function that have such a bug.", "comments": ["Can one of the admins verify this patch?", "Sure, I made the modification also in the other `nest.py`.\r\nI fixed also a wrong key the `nest_test.py`.", "@tensorflow-jenkins test this please."]}, {"number": 14498, "title": "The result of tf.control_dependencies is undetermined.", "body": "```python\r\nimport tensorflow as tf\r\n\r\nw = tf.Variable(0)\r\nop1 = tf.assign(w, 1)\r\nop2 = tf.assign(w, 2)\r\nop_1 = tf.constant('op1')\r\nop_2 = tf.constant('op2')\r\n\r\nop1 = tf.Print(op1, [op_1])\r\nop2 = tf.Print(op2, [op_2])\r\n\r\nwith tf.control_dependencies([op2]):\r\n    with tf.control_dependencies([op1]):\r\n        op = tf.no_op()\r\n\r\nwith tf.Session() as sess:\r\n    tf.global_variables_initializer().run()\r\n    sess.run(op)\r\n    print(sess.run(w))\r\n```\r\nIt sometimes prints `1` and sometimes prints `2`.\r\n\r\nAnother example:\r\n```python\r\nimport tensorflow as tf\r\n\r\nw = tf.Variable(0)\r\nop1 = tf.assign(w, 0)\r\nop2 = tf.assign(w, 2)\r\nop_1 = tf.constant('op1')\r\nop_2 = tf.constant('op2')\r\n\r\nop1 = tf.Print(op1, [op_1])\r\nop2 = tf.Print(op2, [op_2])\r\n\r\nwith tf.control_dependencies([op1]):\r\n    op3 = tf.no_op()\r\n\r\nwith tf.control_dependencies([op3, op2]):\r\n    op4 = tf.no_op()\r\n\r\n\r\nwith tf.Session() as sess:\r\n    tf.global_variables_initializer().run()\r\n    sess.run(op4)\r\n    print(sess.run(w))\r\n```\r\nSometimes `0` and sometimes `2`.", "comments": ["Same as https://github.com/tensorflow/tensorflow/issues/4663#issuecomment-336504782. Using ResourceVariable fixed the problem.", "@ppwwyyxx I am also confused, what difference between `Variable` ans `ResourceVariable` , can you explain why the result will change for my two examples above? And my problem may be different, because I don't use` tf.identity`.", "I tried your first snippet with ResourceVariable and it appears to work.\r\nI'll just quote https://github.com/tensorflow/tensorflow/issues/4663#issuecomment-336609536 that `tf.Variable` is semi-broken and `ResourceVariable` is better."]}, {"number": 14497, "title": "use with when calling TFRecordWriter", "body": "TFRecordWriter supports __enter__ and __exit__ . \r\nCalling it through with is more pythonic and does cleanup in case something throws an uncaught exception.", "comments": ["Can one of the admins verify this patch?", "@qmick Anything I can do to progress the this PR?\r\nIt seems that the TF Test Suite never finishes.", "Don't worry about that. You just need to wait until a reviewer with write access  approve this PR, then he/she will start the test."]}, {"number": 14496, "title": "Building with MKL reduces CPU performance", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: both\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.5.4\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 8\r\n- **GPU model and memory**: Tesla P100-PCIE-16GB\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nBuilding tensorflow with mkl (--config=mkl) prevents the system from using all its cores.\r\nCPU load remains always below 20% in my testcase. Using the same build flags but without mkl achieve 100% CPU load and a nearly 10 times faster execution.\r\n\r\nWhile playing with the MKL flags described here https://www.tensorflow.org/performance/performance_guide#optimizing_for_cpu\r\ni noticed some strange behavior:\r\n\r\nRunning the MKL-build with \r\n`OMP_NUM_THREADS=27 KMP_SETTINGS=1 KMP_AFFINITY=verbose `\r\nresults in the following print:\r\n\r\n```\r\nUser settings:\r\n\r\n   KMP_AFFINITY=verbose\r\n   KMP_SETTINGS=1\r\n   OMP_NUM_THREADS=27\r\n\r\nEffective settings:\r\n\r\n   KMP_ABORT_DELAY=0\r\n   KMP_ADAPTIVE_LOCK_PROPS='1,1024'\r\n   KMP_ALIGN_ALLOC=64\r\n   KMP_ALL_THREADPRIVATE=224\r\n   KMP_ATOMIC_MODE=2\r\n   KMP_BLOCKTIME=200\r\n   KMP_CPUINFO_FILE: value is not defined\r\n   KMP_DETERMINISTIC_REDUCTION=false\r\n   KMP_DEVICE_THREAD_LIMIT=2147483647\r\n   KMP_DISP_NUM_BUFFERS=7\r\n   KMP_DUPLICATE_LIB_OK=false\r\n   KMP_FORCE_REDUCTION: value is not defined\r\n   KMP_FOREIGN_THREADS_THREADPRIVATE=true\r\n   KMP_FORKJOIN_BARRIER='2,2'\r\n   KMP_FORKJOIN_BARRIER_PATTERN='hyper,hyper'\r\n   KMP_FORKJOIN_FRAMES=true\r\n   KMP_FORKJOIN_FRAMES_MODE=3\r\n   KMP_GTID_MODE=3\r\n   KMP_HANDLE_SIGNALS=false\r\n   KMP_HOT_TEAMS_MAX_LEVEL=1\r\n   KMP_HOT_TEAMS_MODE=0\r\n   KMP_INIT_AT_FORK=true\r\n   KMP_INIT_WAIT=2048\r\n   KMP_ITT_PREPARE_DELAY=0\r\n   KMP_LIBRARY=throughput\r\n   KMP_LOCK_KIND=queuing\r\n   KMP_MALLOC_POOL_INCR=1M\r\n   KMP_NEXT_WAIT=1024\r\n   KMP_NUM_LOCKS_IN_BLOCK=1\r\n   KMP_PLAIN_BARRIER='2,2'\r\n   KMP_PLAIN_BARRIER_PATTERN='hyper,hyper'\r\n   KMP_REDUCTION_BARRIER='1,1'\r\n   KMP_REDUCTION_BARRIER_PATTERN='hyper,hyper'\r\n   KMP_SCHEDULE='static,balanced;guided,iterative'\r\n   KMP_SETTINGS=true\r\n   KMP_SPIN_BACKOFF_PARAMS='4096,100'\r\n   KMP_STACKOFFSET=64\r\n   KMP_STACKPAD=0\r\n   KMP_STACKSIZE=4M\r\n   KMP_STORAGE_MAP=false\r\n   KMP_TASKING=2\r\n   KMP_TASKLOOP_MIN_TASKS=0\r\n   KMP_TASK_STEALING_CONSTRAINT=1\r\n   KMP_TEAMS_THREAD_LIMIT=56\r\n   KMP_TOPOLOGY_METHOD=all\r\n   KMP_USER_LEVEL_MWAIT=false\r\n   KMP_VERSION=false\r\n   KMP_WARNINGS=true\r\n   OMP_CANCELLATION=false\r\n   OMP_DEFAULT_DEVICE=0\r\n   OMP_DISPLAY_ENV=false\r\n   OMP_DYNAMIC=false\r\n   OMP_MAX_ACTIVE_LEVELS=2147483647\r\n   OMP_MAX_TASK_PRIORITY=0\r\n   OMP_NESTED=false\r\n   OMP_NUM_THREADS='27'\r\n   OMP_PLACES: value is not defined\r\n   OMP_PROC_BIND='false'\r\n   OMP_SCHEDULE='static'\r\n   OMP_STACKSIZE=4M\r\n   OMP_THREAD_LIMIT=2147483647\r\n   OMP_WAIT_POLICY=PASSIVE\r\n   KMP_AFFINITY='verbose,warnings,respect,granularity=core,none'\r\n\r\nOMP: Info #209: KMP_AFFINITY: decoding x2APIC ids.\r\nOMP: Info #207: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\r\nOMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55}\r\nOMP: Info #156: KMP_AFFINITY: 56 available OS procs\r\nOMP: Info #157: KMP_AFFINITY: Uniform topology\r\nOMP: Info #179: KMP_AFFINITY: 2 packages x 14 cores/pkg x 2 threads/core (28 total cores)\r\nOMP: Info #247: KMP_AFFINITY: pid 35537 tid 35708 thread 0 bound to OS proc set {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55}\r\nOMP: Info #247: KMP_AFFINITY: pid 35537 tid 35706 thread 1 bound to OS proc set {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55}\r\nOMP: Info #247: KMP_AFFINITY: pid 35537 tid 35707 thread 2 bound to OS proc set {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55}\r\nOMP: Info #247: KMP_AFFINITY: pid 35537 tid 35704 thread 3 bound to OS proc set {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55}\r\nOMP: Info #247: KMP_AFFINITY: pid 35537 tid 35701 thread 4 bound to OS proc set {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55}\r\nOMP: Info #247: KMP_AFFINITY: pid 35537 tid 35711 thread 5 bound to OS proc set {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55}\r\nOMP: Info #247: KMP_AFFINITY: pid 35537 tid 35712 thread 6 bound to OS proc set {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55}\r\nOMP: Info #247: KMP_AFFINITY: pid 35537 tid 35713 thread 7 bound to OS proc set {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55}\r\nOMP: Info #247: KMP_AFFINITY: pid 35537 tid 35705 thread 8 bound to OS proc set {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55}\r\nOMP: Info #247: KMP_AFFINITY: pid 35537 tid 35710 thread 9 bound to OS proc set {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55}\r\nOMP: Info #247: KMP_AFFINITY: pid 35537 tid 35702 thread 10 bound to OS proc set {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55}\r\nOMP: Info #247: KMP_AFFINITY: pid 35537 tid 35717 thread 11 bound to OS proc set {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55}\r\n...\r\nOMP: Info #247: KMP_AFFINITY: pid 35537 tid 35678 thread 78 bound to OS proc set {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55}\r\nOMP: Info #247: KMP_AFFINITY: pid 35537 tid 35672 thread 79 bound to OS proc set {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55}\r\nOMP: Info #247: KMP_AFFINITY: pid 35537 tid 35674 thread 80 bound to OS proc set {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55}\r\n....\r\n(continues with a higher thread number each time)\r\n```\r\n\r\nIf I use the same execution flags with a build without MKL or with the pip version I get the same ouput up to \r\n`\r\n...\r\nOMP: Info #247: KMP_AFFINITY: pid 36958 tid 37191 thread 27 bound to OS proc set {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55}\r\n`\r\nAfterwards no OMP prints are created. It seems like, if I build with mkl, tensorflow continues to create more and more threads but cant utilize them. \r\n\r\nIs this a configuration issue or a bug? \r\nIf its a known issue, please expand the performance guide :) \r\n\r\npinging @skye because of its help with the performance issue with while_loop \r\n\r\n", "comments": ["I also have this problem. I build tensorflow step by step as official tutorial says, but get a much slower version when MKL is enabled. It is quite strange,", "@vivek-rane Vivek, could you respond to this?", "Will get someone to take a look - thanks for tagging.", "@georgh Could you tell us what the inter/intra op settings and model is? ", "I didn't set inter or intra op settings for the example, but I just tested it with both set to 20 and it still behaves the same. \r\nThe model I used is quite big, the main parts uses two big tf.while_loops with a few operations.\r\nI will try to provide a simple test case tomorrow.", "Setting the inter op to 20 is a really bad idea. Try starting with an inter op of 1 and intra op of #cores, and OMP_NUM_THREADS set to #cores and tweak from there.", "Thank you for your advise. If that's a bad Idea, why does the performance guide state:\r\n> A common alternative optimization is to set the number of threads in both pools equal to the number of physical cores rather than logical cores.\r\n\r\nIs this only valid if you set it to the number of physical cores? \r\nI didn't do that much tests with these values, because I can't always use the complete cluster for my computations. Setting both values to 20 restricts tensorflow to 20 logical cores and leaves therefore enough room for other people to use the system.\r\n\r\nBut anyway - regarding the error using MKL the options to not show any effect.\r\nBuild with MKL and inter/intra set to 20 I only get a CPU usage of around 150% (1.5 cores) using a build without MKL I achieve 2000 % (20 cores).\r\n\r\n \r\n\r\n", "@tfboyd do we recommend running with intra=num_cores for mkl? that would give pretty bad perf, since TF will run num_cores number of ops in parallel, each with OMP_NUM_THREADS threads (heavy oversubscription).", "@georgh can you share the timelines for your run with and without MKL? It is hard to figure what the problem is without topology info.", "I created two timelines for a very small run, but I am not sure if they contain useful informations. \r\nI tried to create a timeline for a bigger run, but the resulting file is 1.3GB big and chrome does not show any information if I try to open it.\r\n\r\n\r\n\r\n[timelines.zip](https://github.com/tensorflow/tensorflow/files/1478931/timelines.zip)\r\n\r\n\r\n", "In my testing I had the best results with the following settings, cut and\npasted from the document\n<https://www.tensorflow.org/performance/performance_guide#optimizing_for_cpu>I\nwrote.  It can be a different for different models but for resnet and\ninception training this worked well and even for inference.  Intel\nmentioned that slightly higher levels of inter_op might work but my\nguidelines as # of \"sockets\" was reasonable.  If someone gets data for\nother models I am happy to try and find a way to share the info widely.\n\nThere are models and hardware platforms that benefit from different\nsettings. Each variable that impacts performance is discussed below.\n\n   -\n\n   *KMP_BLOCKTIME*: The MKL default is 200ms, which was not optimal in our\n   testing. 0 (0ms) was a good default for CNN based models that were tested.\n   The best performance for AlexNex was achieved at 30ms and both GoogleNet\n   and VGG11 performed best set at 1ms.\n   -\n\n   *KMP_AFFINITY*: The recommended setting is\n   granularity=fine,verbose,compact,1,0.\n   -\n\n   *OMP_NUM_THREADS*: This defaults to the number of physical cores.\n   Adjusting this parameter beyond matching the number of cores can have an\n   impact when using Intel\u00ae Xeon Phi\u2122 (Knights Landing) for some\nmodels. SeeTensorFlow*\n   Optimizations on Modern Intel\u00ae Architecture\n   <https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture>\nfor\n   optimal settings.\n   -\n\n   *intra_op_parallelism_threads*: Setting this equal to the number of\n*physical\n   cores* is recommended. Setting the value to 0, which is the default and\n   will result in the value being set to the number of logical cores, is an\n   option to try for some architectures. This value and OMP_NUM_THREADS should\n   be equal.\n   -\n\n   *inter_op_parallelism_threads*: Setting this equal to the number of\n   sockets is recommended. Setting the value to 0, which is the default,\n   results in the value being set to the number of logical cores.\n\n\nOn Thu, Nov 16, 2017 at 6:57 AM, georgh <notifications@github.com> wrote:\n\n> I created two timelines for a very small run, but I am not sure if they\n> contain useful informations.\n> I tried to create a timeline for a bigger run, but the resulting file is\n> 1.3GB big and chrome does not show any information if I try to open it.\n>\n> timelines.zip\n> <https://github.com/tensorflow/tensorflow/files/1478931/timelines.zip>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/14496#issuecomment-344946223>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AWZeshN8JsICk3B_IfU-v9WbQsIJZxkuks5s3E1WgaJpZM4Qa4-L>\n> .\n>\n", "I agree with Toby that the inter-op should be set to somewhere around the number of sockets on the system, which is typically 1. \r\n\r\nI looked at the timeline and there are no ops in there that are currently supported with MKL on TensorFlow. If this timeline is representative of your workload, you will not gain any benefit from switching to MKL. The top 5 ops here are listed below, and matmul is probably the only thing that stands to gain from MKL. If this timeline is off, we need to find a way of getting the correct timeline.\r\n\r\n- Pack\r\n- Mul\r\n- StridedSlice\r\n- Matmul\r\n- Add", "Btw setting the KMP_BLOCKTIME to 0 (as Toby suggested) also helps with oversubscription.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "@tatianashp any idea?", "@vivek-rane explained why using MKL does not help with performance for this network. I am closing the issue.\r\n\r\n@georgh If you have more questions related to MKL performance please re-open.\r\n\r\n"]}, {"number": 14495, "title": "FATAL EXCEPTION on running the custom app after including libraries. java.lang.UnsatisfiedLinkError", "body": "java.lang.UnsatisfiedLinkError: com.android.tools.fd.runtime.IncrementalClassLoader$DelegateClassLoader[DexPathList[[dex file \"/data/data/com.example.iiti.earcheck/files/instant-run/dex/slice-support-annotations-25.0.1_ef28e13c9736d79b0dc9b87816fdb5d73ab6b4a6-classes.dex\", dex file \"/data/data/com.example.iiti.earcheck/files/instant-run/dex/slice-slice_9-classes.dex\", dex file \"/data/data/com.example.iiti.earcheck/files/instant-run/dex/slice-slice_8-classes.dex\", dex file \"/data/data/com.example.iiti.earcheck/files/instant-run/dex/slice-slice_7-classes.dex\", dex file \"/data/data/com.example.iiti.earcheck/files/instant-run/dex/slice-slice_6-classes.dex\", dex file \"/data/data/com.example.iiti.earcheck/files/instant-run/dex/slice-slice_5-classes.dex\", dex file \"/data/data/com.example.iiti.earcheck/files/instant-run/dex/slice-slice_4-classes.dex\", dex file \"/data/data/com.example.iiti.earcheck/files/instant-run/dex/slice-slice_3-classes.dex\", dex file \"/data/data/com.example.iiti.earcheck/files/instant-run/dex/slice-slice_2-classes.dex\", dex file \"/data/data/com.example.iiti.earcheck/files/instant-run/dex/slice-slice_1-classes.dex\", dex file \"/data/data/com.example.iiti.earcheck/files/instant-run/dex/slice-slice_0-classes.dex\", dex file \"/data/data/com.example.iiti.earcheck/files/instant-run/dex/slice-org.tensorflow-tensorflow-android-1.4.0_d5862dbeef875a2fd03edd55b52916408a0fdae3-classes.dex\", dex file \"/data/data/com.example.iiti.earcheck/files/instant-run/dex/slice-libandroid_tensorflow_inference_java_8b98de511efe4c9135a05cbc7896de53b57dba53-classes.dex\", dex file \"/data/data/com.example.iiti.earcheck/files/instant-run/dex/slice-internal_impl-25.0.1_fe869e8b718f7d011b42911906bd8102f80b5a3e-classes.dex\", dex file \"/data/data/com.example.iiti.earcheck/files/instant-run/dex/slice-internal_impl-25.0.1_e684d856bf1ea2afc0785de8b2275aa12611114e-classes.dex\", dex file \"/data/data/com.example.iiti.earcheck/files/instant-run/dex/slice-internal_impl-25.0.1_aa673af4dcf22bcd43d00f80c8888302b9bc278f-classes.dex\", dex file \"/data/data/com.example.iiti.earcheck/files/instant-run/dex/slice-internal_impl-25.0.1_73c7d1a62ae7807eb7fd020f01cc2e0e5fd6f9f0-classes.dex\", dex file \"/data/data/com.example.iiti.earcheck/files/instant-run/dex/slice-internal_impl-25.0.1_2002db2ec303ce7ff6c3fba7cf2a064df60d63e4-classes.dex\", dex file \"/data/data/com.example.iiti.earcheck/files/instant-run/dex/slice-com.android.support-support-vector-drawable-25.0.1_6f89a35e510cd2b99a51cac7fbf50945d73d2434-classes.dex\", dex file \"/data/data/com.example.iiti.earcheck/files/instant-run/dex/slice-com.android.support-support-v4-25.0.1_7519131df48ff71670e81da58ca765ab77bce972-classes.dex\", dex file \"/data/data/com.example.iiti.earcheck/files/instant-run/dex/slice-com.android.support-support-media-compat-25.0.1_d42d2937b0925301d8bc24c4d82fa264d601340d-classes.dex\", dex file \"/data/data/com.example.iiti.earcheck/files/instant-run/dex/slice-com.android.support-support-fragment-25.0.1_130afa5cd91643b7e407f699e25e8953be2af0b8-classes.dex\", dex file \"/data/data/com.example.iiti.earcheck/files/instant-run/dex/slice-com.android.support-support-core-utils-25.0.1_39638966b982d2402411d64d18981a4ce8677c1f-classes.dex\", dex file \"/data/data/com.example.iiti.earcheck/files/instant-run/dex/slice-com.android.support-support-core-ui-25.0.1_a6406a5ff5aa24fae58de96cd3218c571f49607d-classes.dex\", dex file \"/data/data/com.example.iiti.earcheck/files/instant-run/dex/slice-com.android.support-support-compat-25.0.1_5b463be571b727968d5b5d68f9c9b17d4dd40809-classes.dex\", dex file \"/data/data/com.example.iiti.earcheck/files/instant-run/dex/slice-com.android.support-appcompat-v7-25.0.1_17a51234c0d6cd6e15ec23d8be2aa38790811b00-classes.dex\", dex file \"/data/data/com.example.iiti.earcheck/files/instant-run/dex/slice-com.android.support-animated-vector-drawable-25.0.1_df8c5ec44495c7c6a687ebfe83e9e46b0bcc5d42-classes.dex\"],nativeLibraryDirectories=[/data/app/com.example.iiti.earcheck\r\n\r\n\r\nHelp!\r\n", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "There was some issue with including the libraries manually. I fixed it just now. I will be posting the whole process after I finish up with my android application. Thanks a lot!", "@makarg1001 I am facing same issue. Can you post the solution here? Thanks"]}, {"number": 14494, "title": "Difference between distribute and local machine version when training by tensorflow ", "body": " I use a common DNN network with less than 4 layers for a regression task. When I test my codes in local single machine with CPU, everything is fine. But when I train it on distribute system, which uses 30 machines' CPU and in the asynchronous parameters update way, all the layers' output values, including the last prediction values, come into Nan. The learning rate in local and distribute training are both 0.01. I changed it to 0.0001 in distribute way, it seems fine that the outputs are in normal range. I am confused about why learning rate cause this phenomenon. Any friend can try to explain it ? Thanks.\r\n\r\n\r\n--\r\nThe label is a float, ranging from 0 to 1. Most of them falls into 0.1 to 0.3\r\n\r\n\r\n", "comments": ["To misquote Tolstoy -- \"Happy training jobs are all alike, every unhappy job is unhappy in its own way.\" \r\n\r\nAsync training sometimes gets problems when gradients are too stale -- stale worker arriving with yesterday's gradient can throw the whole procedure into NaN-land"]}, {"number": 14493, "title": "Feature Request: C++ gradient for Prod (PR done)", "body": "Pull request opened. https://github.com/tensorflow/tensorflow/pull/14422", "comments": ["Thanks! We can discuss directly on the PR."]}, {"number": 14492, "title": "Add a mutex in cuda_solvers GetrfImpl.", "body": "This should solve issue #13558. Because NVIDIA does not publish the code, it is not possible to prove\r\nthe bug is in the cuSolver function. However, I have removed / commented code from\r\nmatrix_inverse_op.cpp until essentially only the call to solver->Getrf remained, and from @yaroslavvb' \r\nproject mostly everything except the regularized_inverse calls, and the segmentation fault still\r\noccurred. It did not appear when there was a single thread calling regularized_inverse. The segmentation fault always occurs without my change, and never after introducing the\r\nlock (although that in itself does not mean the bug is solved).\r\n\r\nNote that GetrsImpl is already protected with a lock, so apparently this issue was known.\r\n\r\nFinally, @yaroslavvb has a second stacktrace which does not involve the Getrf function.\r\nI could not reproduce this error, but it may be a different bug.", "comments": ["Can one of the admins verify this patch?", "Impressive investigation!", "Hi @rmlarsen,\r\n\r\nPlease, would you have some time to look at this PR?", "Only one test fails: //tensorflow/core:util_saved_tensor_slice_util_test. It does not seem related to my change (it doesn't even use CUDA). Also, I can't reproduce this error on my computer.", "@codrut3 I have sadly had to add similar mutexes due to races in cusolver. NVIDIA is aware of the problem and I believe the situation should be improved in Cuda9.\r\n@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "OK, we are moving to cuda9. could you update and test with the new version?", "Hi @drpngx ,\r\n\r\nSomeone else added in the meantime the mutex, so the change is no longer needed. However, my PR also fixes a typo, at least this could merged. Because it's just a comment change, I don't think it requires testing.", "We have to test everything. You never know when something is going to break.\r\n\r\nJenkins, test this please.", "/CC @gunan Ran out of disk space on the Jenkins build here too.\r\n\r\n```\r\ndevmapper: Thin Pool has 968455 free data blocks which is less than minimum required 983040 free data blocks. Create more free space in thin pool or use dm.min_free_space option to change behavior\r\n```\r\nJenkins, test this please."]}, {"number": 14491, "title": "n 6", "body": "Had the wrong window highlighted, hit enter and opened this issue by mistake. Can be removed.", "comments": []}, {"number": 14490, "title": "tf.contrib.metrics.streaming_mean", "body": "```python\r\ndef streaming_mean_weight_test():\r\n    '''\r\n        Average by weight\r\n    '''\r\n    values = tf.constant([[1.0, 2.0], [3.3, 2.5]])\r\n    weights = tf.constant([[0.3, 3.1], [-1.3, 1.2]])\r\n    mean_value, update_op = tf.contrib.metrics.streaming_mean(values, weights=weights)\r\n    tf_weight_mean = tf.truediv(tf.reduce_sum(tf.multiply(values, weights)), tf.reduce_sum(weights))\r\n    with tf.Session() as sess:\r\n        tf.local_variables_initializer().run()\r\n        print(update_op.eval())\r\n        _mean_value = mean_value.eval()\r\n        _tf_weight_mean = tf_weight_mean.eval()\r\n        print(_mean_value)\r\n        print(_tf_weight_mean)\r\n    assert np.all(np.isclose(_mean_value, _tf_weight_mean)), 'streaming_mean_weight_test is wrong!'\r\n```\r\nIt works well. But when I use `weights = tf.constant([[0.3, -3.1], [-1.3, 1.2]])` instead of weights above,\r\nIt can't work, `_mean_value=0.0`, what's  wrong?\r\n", "comments": ["I've tried your code. It seems working. Could you please re-check?", "@ispirmustafa  I have re-checked code, it will output `assert` error.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef streaming_mean_weight_test():\r\n    '''\r\n        Average by weight\r\n    '''\r\n    values = tf.constant([[1.0, 2.0], [3.3, 2.5]])\r\n    weights = tf.constant([[0.3, -3.1], [-1.3, 1.2]])\r\n    #weights = tf.constant([[0.3, 3.1], [-1.3, 1.2]])\r\n    mean_value, update_op = tf.contrib.metrics.streaming_mean(values, weights=weights)\r\n    tf_weight_mean = tf.truediv(tf.reduce_sum(tf.multiply(values, weights)), tf.reduce_sum(weights))\r\n    with tf.Session() as sess:\r\n        tf.local_variables_initializer().run()\r\n        print(update_op.eval())\r\n        _mean_value = mean_value.eval()\r\n        _tf_weight_mean = tf_weight_mean.eval()\r\n        print(_mean_value)\r\n        print(_tf_weight_mean)\r\n    assert np.all(np.isclose(_mean_value, _tf_weight_mean)), 'streaming_mean_weight_test is wrong!'\r\n\r\n\r\nif __name__ == '__main__':\r\n    streaming_mean_weight_test()\r\n```\r\n\r\n", "@ispirmustafa Do you have any response?", "Metrics uses so called a `_safe_div`, which returns `0 if denominator <= 0, else numerator/denominator`. \r\nIn this example `reduce_sum(weights)` is < 0. \r\nHaving total weights negative seems very unusual. Could you please explain what is the use case?", "I just test the `tf.contrib.metrics.streaming_mean`,  maybe it is not useful."]}, {"number": 14488, "title": "Problem to install tensorflow", "body": "Hello, I am trying to install the tensorflow to use it in the digits of nvdia and am having this problem :/\r\n\r\n> Exception:\r\nTraceback (most recent call last):\r\n  File \"/home/felipe/.local/lib/python2.7/site-packages/pip/basecommand.py\", line 215, in main\r\n    status = self.run(options, args)\r\n  File \"/home/felipe/.local/lib/python2.7/site-packages/pip/commands/install.py\", line 342, in run\r\n    prefix=options.prefix_path,\r\n  File \"/home/felipe/.local/lib/python2.7/site-packages/pip/req/req_set.py\", line 784, in install\r\n    **kwargs\r\n  File \"/home/felipe/.local/lib/python2.7/site-packages/pip/req/req_install.py\", line 851, in install\r\n    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)\r\n  File \"/home/felipe/.local/lib/python2.7/site-packages/pip/req/req_install.py\", line 1064, in move_wheel_files\r\n    isolated=self.isolated,\r\n  File \"/home/felipe/.local/lib/python2.7/site-packages/pip/wheel.py\", line 345, in move_wheel_files\r\n    clobber(source, lib_dir, True)\r\n  File \"/home/felipe/.local/lib/python2.7/site-packages/pip/wheel.py\", line 316, in clobber\r\n    ensure_dir(destdir)\r\n  File \"/home/felipe/.local/lib/python2.7/site-packages/pip/utils/__init__.py\", line 83, in ensure_dir\r\n    os.makedirs(path)\r\n  File \"/usr/lib/python2.7/os.py\", line 157, in makedirs\r\n    mkdir(name, mode)\r\nOSError: [Errno 13] Permiss\u00e3o negada: '/usr/local/lib/python2.7/dist-packages/funcsigs-1.0.2.dist-info'", "comments": ["(1) Please follow issue guidelines and template style [ISSUE_TEMPLATE](https://github.com/tensorflow/tensorflow/blob/master/ISSUE_TEMPLATE.md) before raising an issue.\r\n(2) This is not a bug hence you can go to stack overflow where tf community can help you out. This [link](https://stackoverflow.com/questions/23870808/oserror-errno-13-permission-denied) might solve your issue. ", "Thanks, I solved the problem using the following command:\r\n\r\n`pip3 install --user tensorflow-gpu`"]}, {"number": 14487, "title": "Sync from internal More Limited Tests", "body": "@tensorflow-jenkins please test this.", "comments": ["Spot check looks like it doesn't revert any PRs. So far so good.", "Only commits from internal + aselle@, so all good."]}, {"number": 14486, "title": "TowerLoss(Multiple GPU) will hurt final accuracy/performance so much when doing image finetune, is it a bug?", "body": "From the paper \"Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge\"\r\nWriter say \"Training was\r\ndone using a single GPU (Nvidia K20), and step time was\r\nabout 3 seconds. Thus, training took over 3 weeks \u2013 **parallelizing\r\ntraining yielded somewhat worse results,** though it\r\nincreased the speed to convergence.\"\r\nFor my applications, I find tower loss is ok when you do anything without finetune image mode, even \r\nif you use image model(inception resnet nasnet etc.) is fine.\r\nBut if you do finetune, either for image caption or image classification, the performance will hurt \r\na lot(using mulitple gpu wether increase total batch size or keep total batch size the same as single gpu), might not convergent.  Is it a known bug, can we avoid this ?\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14485, "title": "Fixed incorrect documentation in tf.contrib.nn.deprecated_flipped_spa\u2026", "body": "\u2026rse_softmax_cross_entropy_with_logits where it referenced labels rather than logits.\r\n\r\nI was looking to fix #14450, but was unable to because it seems the site documentation is not the same as the in file documentation in master (which is correct in that case). It seems someone fixed it in master but the change was not made to 1.4. However, incidentally I noticed this identical error on `tf.contrib.nn.deprecated_flipped_sparse_softmax_cross_entropy_with_logits`. In this case, the error is on both the in file documentation and the website documentation. ", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it.", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 14484, "title": "My trained checkpoint doesn't work.", "body": "    $ python3 tests/test_snapshot.py lsp out/lsp_alexnet_imagenet_small/checkpoint-370000.data-00000-of-00001\r\n\r\nBut it shows this error\r\n\r\nData loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):\r\n\r\nDataLossError (see above for traceback): Unable to open table file out/lsp_alexnet_imagenet_small/checkpoint-370000.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\r\n\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14483, "title": "R0.10", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->"]}]