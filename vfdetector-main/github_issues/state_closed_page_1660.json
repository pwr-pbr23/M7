[{"number": 3105, "title": "Branch 126195202", "body": "Push of internal Google changes to TensorFlow.\n", "comments": ["@tensorflow-jenkins test this please\n", "@vrv @martinwicke the two test failures are known and tracked by TF bulid cops. This should be OK to merge.\n", "still waiting for GPU builds.\n", "right\n", "Adandoning this PR.\n"]}, {"number": 3104, "title": "Fixed dependency in makefile, and sped up iOS build", "body": "", "comments": ["@tensorflow-jenkins test this please\n"]}, {"number": 3103, "title": "Non-deterministic mean and sum reduction", "body": "I'm running Tensorflow 0.9.0 installed from wheel on Python 2.7 on a K40 with CUDA 7.0.\n\nThe following test case attempts to minimize the mean of a vector through gradient descent. The script finds that the vectors are equal at all steps, but the means are not. I believe the vectors being equal at all steps is pure numerical luck, since non-deterministic loss likely means non-deterministic gradient which means non-deterministic/reproducible iterative optimization. I've observed cases where training results in different final losses where the only source of non-determinism is from reduce_mean.\n\n``` python\nimport numpy as np\nimport tensorflow as tf\n\nn_dims = 1000\nn_steps = 50\n\nnp.random.seed(2016)\n\nvec = tf.Variable(np.random.randn(n_dims).astype(np.float32))\nmean = tf.reduce_mean(vec)\n\noptimizer = tf.train.GradientDescentOptimizer(0.01)\ntrain_step = optimizer.minimize(mean)\n\ndef generate():\n    data = []\n    with tf.Session() as sess:\n        sess.run(tf.initialize_all_variables())\n\n        for _ in xrange(n_steps):\n            _vec, _mean, _ = sess.run([vec, mean, train_step])\n            data.append((_vec, _mean))\n\n    return [np.array([f[i] for f in data]) for i in xrange(2)]\n\nfirst_vec, first_mean = generate()\nsecond_vec, second_mean = generate()\nprint 'vecs equal:', np.all(first_vec == second_vec)\n\nprint 'mean equal:', np.all(first_mean == second_mean)\nprint 'means not equal at idxs:', np.nonzero(first_mean != second_mean)[0]\n```\n\nExample output:\n\n```\nvecs equal: True\nmean equal: False\nmeans not equal at idxs: [ 4  5 11 18 34 38 44 49]\n```\n\nFrom looking through the code, it appears the GPU mean reduction is implemented with GPU sum reduction: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/reduction_ops_gpu.cu.cc#L43\nI've confirmed my test case still triggers when I replace \"reduce_mean\" with \"reduce_sum\".\n\nThe GPU sum reduction appears to be implemented using CUDA's atomicAdd: https://bitbucket.org/eigen/eigen/src/241472d2a52142e23b0b2ba5c301c6c146298fa9/unsupported/Eigen/CXX11/src/Tensor/TensorReductionCuda.h?at=default&fileviewer=file-view-default#TensorReductionCuda.h-97\n\nAtomic floating point adds on GPU are the problem. Having floating point adds to the same address in an undefined order is inherently non-deterministic due to non-associativity of floating point arithmetic.\n\nThis issue could be solved (and reduction performance improved) by using some sort of reduction tree to reduce within blocks, and then launching a second kernel (or doing some manual block synchronization tricks) to reduce across blocks.\n", "comments": ["I created a smaller test case.\n\n``` python\nimport numpy as np\nimport tensorflow as tf\n\nnp.random.seed(2016)\ndata = np.random.randn(100000).astype(np.float32)\n\nvec = tf.placeholder(tf.float32, data.shape)\navg = tf.reduce_mean(vec)\n\navgs = []\nwith tf.Session() as sess:\n    for _ in xrange(100):\n        avgs.append(sess.run(avg, feed_dict={vec: data}))\n\nprint min(avgs) == max(avgs)\nprint max(avgs) - min(avgs)\n```\n\nwith output\n\n```\nFalse\n6.98492e-10\n```\n", "@lightcatcher as you point out, our current implementation of sum reduction (and the various ops that depend on it) is not deterministic on either GPU or multi-threaded CPUs. It is primarily a speed/accuracy trade-off and if we could get comparable speed from one of the approaches you mention, we would be happy to switch the implementation. In short, this is working as intended for now, but contributions for a more accurate or even deterministic sum reduction (possibly as a separate op) would certainly be welcomed.\n", "I'm closing this as \"working as intended\".\n", "Which other ops are non-deterministic that are not non-deterministic by nature?\n", "@TimZaman Unless something has changed in the recent months, some of the cuDNN code is non-deterministic, for instance cudnn.SpatialConvolution. So, I guess that some of the CNN-related stuff in tensorflow may be non-reproducible (if run on GPU). Would probably be a bit of work, but it would be nice to have a flag or note in the TF docstrings of the affected functions.", "As @zheng-xq mentioned earlier, anything using cuda atomics is non-deterministic, so a way to narrow it down is to see which CuDNN algorithms use CUDA atomics. For CPU ops, the way to check might be to track down parallel ops (see which ops use tensorflow/core/util/work_sharder.cc) and check that result is independent of the order in which individual work shards complete. Note that there are more tricky cases of non-determinism, for instance same sequence of SSE instructions can give different results on rerun, so to get a stronger guarantee of determinism you need to disable multi-threading and special instruction sets: http://blog.nag.com/2011/02/wandering-precision.html\r\n\r\n", "@eamartin I ran you \"smaller test case\" from above and I am getting a fully deterministic behavior. I am just wondering what has changed since you ran it. I am using py 3.5 and tf 1.4.1.", "Are reductions still non-determinstic by default on GPU? If so, can this issue be re-opened? Determinstic computation is critical for reproducibility, and reductions are a critical part of neural nets.\r\n\r\nFinally:\r\nSeveral comments on this issue and other linked issues mention that \"reductions are non-determinstic for performance\". This is not the case. A reduction tree is both determinstic and generally faster than using atomic adds (which cause non-determinism). https://devblogs.nvidia.com/faster-parallel-reductions-kepler/ describes reduction trees, shows that reduction tree + very limited use of atomic add is the fastest option, but that reduction trees (determinstic) is only marginally slower.\r\nLast I checked (quite a while ago), the TF reduction implementation exclusively used atomics and no reduction tree, so a switch to a reduction tree only implementation would provide a performance boost. My guess is that atomics were used for the TF implementation not for performance but because the implementation with atomics is somewhat simpler to write.", "I believe there's been some work to make reductions deterministic", "@yaroslavvb \r\nI just re-ran my initial examples on TF 1.5.0 and found that that `reduce_mean` call produced consistent results across 10K trials with the mean kernel running on a K80 GPU. This observation agrees with @ahmedhosny 's observation.\r\n\r\nI dug through the source a little bit and I think the reduction logic happens in https://github.com/eigenteam/eigen-git-mirror/blob/master/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h . This (and the `TensorReductionGpu.h` file) are new since I opened this issue in 2016 and contain reduction tree logic. However, they also still contain atomic floating point adds. I think these are to reduce the values between the different GPU blocks.\r\n\r\nThis file has been recently modified by @rmlarsen . @rmlarsen , do you have any insight on how deterministic these reductions are? When is atomic floating point add used?", "maybe it's `num of cpu` related.\r\n\r\ntest with tf2, with docker, as:\r\n```\r\ndocker run -it --rm --cpuset-cpus=\"0-7\" ...\r\n```\r\n\r\ndifferent cpus will get results, different with the last two digits\r\n\r\ntesting code \r\n```\r\nimport tensorflow as tf\r\n\r\nx = tf.constant(0.3631, shape=[32, 100, 36804], dtype=tf.float32)\r\ny = tf.reduce_mean(x)\r\nprint(y)\r\n```"]}, {"number": 3102, "title": "Constant operation \"forgets\" about incoming shape", "body": "## Feature request\n\nFollowing this question in StackOverflow: [Why can't tensorflow determine the shape of this expression](http://stackoverflow.com/questions/37881917/why-cant-tensorflow-determine-the-shape-of-this-expression/37882501#37882501).\n\nWhen using a constant tensor (`tf.zeros`, `tf.ones`, `tf.constant`...), if one of the input dimension is unknown, the operation will forget every dimension of the input.  \nThe code will be clearer:\n\n``` python\ninput_tensor = tf.placeholder(tf.float32, [None, 32])\nbatch_size = tf.shape(input_tensor)[0]\n\nres = tf.zeros((batch_size, 128))\nprint res.get_shape()  # prints (?, ?) WHEREAS one could expect (?, 128)\n```\n\nThe easy fix is to set the output shape by hand:\n\n``` python\nres.set_shape([None, 128])\nprint res.get_shape()  # prints (?, 128)\n```\n\nHowever, this could be done directly in the TensorFlow code. Otherwise, it may confuse users that think their shape is fully defined.\n", "comments": []}, {"number": 3101, "title": "Softmax for multiple dimensions", "body": "Attention models get more popular. In many of my experiments with attention, I end up using Softmax over multiple dimensions. \n\ntf.nn.softmax() supports only one dimension. It would be nice to expand the API with another argument, softmax_dim, for softmax over multiple dimension, say over 2 (images) or 3 (CT-scans, videos)\n\nI;d like to here what you think? DO more people end up writing softmax code themselves?\n", "comments": ["I'm not clear if this is a feature request, or question. Perhaps you could start by asking this question on StackOverflow first, and tag it with the `tensorflow` tag.\n", "I think it's a feature request\n", "Yes, it's a feature request.\n\nFor exaple, squeeze(), reduce_sum() an any other reduce_() function, they all have arguments where you define the axes to be operated on.\n\nFor reduce_sum(), you define which axes ought to be reduced and summed. Why not generalize the SoftMax like that too?\n\nAt the moment, the softmax API is\n   `tf.nn.softmax(logits,name=None)`\n\nMy feature request is to to\n    `tf.nn.softmax(logits, softmax_dim = [1], name=None)`\n\nNow softmax_dim is the list of axes to be softmax-ed\n\nfor example, softmax_dim would be\n- [1] for a batch of features\n- [1,2] for a batch of images\n- [1,2,3] for a batch of volumes, like a batch of CT-scan masks\n- and so on\n", "@vrv @RobRomijnders stupid question, but where can I find the softmax function? I wanna work on this, but don't seem to be able to find the file..(sorry, first time contributing!)\n", "It's under the neural networks module, so tensorflow.nn.softmax().\n\nAnother engineer of the Google Brain team also contacted me. I think he's working on it too\n", "@RobRomijnders Sorry, but I still can't find it... Could you please give me a link? \n\nAlso, should I still work on it then? Perhaps, I can collaborate with the engineer? Not sure. \n", "Indeed, assigned to the person who is almost done with it, although I believe what is being done here is softmax over a single but specific dimension (with the thought being that if you need to softmax over multiple contiguous dimensions, you can just reshape the adjacent dims).\n", "@vrv ah, okay. For future reference, could you provide a link to where the softmax function is?\n", "https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/kernels (softmax_*) for implementations -- the python code is automatically generated from the Op specification here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/nn_ops.cc#L1261, because there's no extra logic needed in python.\n", "Superseded by #6590.", "I think #6590 does not address this. #210 is a duplicate, and offers that solution: https://gist.github.com/raingo/a5808fe356b8da031837\r\n\r\nI think it would be more coherent and nicer if it could be the default behaviour of tensorflow.", "I think this one slipped through the cracks. tf.nn.softmax still only works over 1 dimension.", "required +1"]}, {"number": 3100, "title": "tensorflow best practice model exploration record keeping", "body": "I understand that we're trying to keep questions to StackOverflow. But, since subjective questions are off-topic for SO, I wasn't sure were to put questions about best practices.\n\nto the point: As a matter of record keeping during model exploration, what are best (I'll settle for sustainable) practices for record keeping of tf graph and parameters of previously tried models?\n\nI'm doing a fair bit of model exploration, but this has resulted in an unsustainable number of model#.py files... Every time I tweak the model involving a graph or parameter change, I feel like I have to keep that .py file for eternity. I have a great workflow for checkpoint saving and loading, but I can't just load up my model from the checkpoint file; I need to have some documentation of what the graph structure and parameters are that actually generated this checkpoint file.\n\nThanks,\nChris Snyder\n", "comments": ["I understand that you feel like this might be too subjective for StackOverflow, but this does sound like a question that is better suited for StackOverflow than issues.  Please ask it there and tag it with the `tensorflow` tag.\n", "Alternatively you can try the discuss mailing list: https://groups.google.com/a/tensorflow.org/forum/#!forum/discuss\n", "Thank you for the feedback.\n"]}, {"number": 3099, "title": "Building Tensorflow in armhf (olimex a20)", "body": "I'm trying to build Tensorflow on an olimex A20 for an embedded NN application. \nI just need to build the C++ or C session API because i'm building the graph in a different box. \n\nSo far I've managed to build protobuf and bazel following this guide. \nhttps://github.com/samjabrahams/tensorflow-on-raspberry-pi/blob/master/GUIDE.md\n\nBut compiling Tensorflow with bazel i get the following error,\n\n`ERROR: /root/.cache/bazel/_bazel_root/8bbee47417ac55737a4a72fb4017df9b/external/highwayhash/BUILD:17:1: C++ compilation of rule '@highwayhash//:sip_hash' failed: gcc failed: error executing command\n  (cd /root/.cache/bazel/_bazel_root/8bbee47417ac55737a4a72fb4017df9b/tensorflow && \\\n  exec env - \\\n    PATH=/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\n  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-std=c++0x' -iquote external/highwayhash -iquote bazel-out/host/genfiles/external/highwayhash -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -isystem external/highwayhash -isystem bazel-out/host/genfiles/external/highwayhash -isystem external/bazel_tools/tools/cpp/gcc3 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/host/bin/external/highwayhash/_objs/sip_hash/external/highwayhash/highwayhash/sip_hash.o' -MD -MF bazel-out/host/bin/external/highwayhash/_objs/sip_hash/external/highwayhash/highwayhash/sip_hash.d -c external/highwayhash/highwayhash/sip_hash.cc -o bazel-out/host/bin/external/highwayhash/_objs/sip_hash/external/highwayhash/highwayhash/sip_hash.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nIn file included from external/highwayhash/highwayhash/sip_hash.cc:15:0:\nexternal/highwayhash/highwayhash/sip_hash.h:31:9: error: expected nested-name-specifier before 'Key'\nexternal/highwayhash/highwayhash/sip_hash.h:31:9: error: using-declaration for non-member at class scope\nexternal/highwayhash/highwayhash/sip_hash.h:31:13: error: expected ';' before '=' token\nexternal/highwayhash/highwayhash/sip_hash.h:31:13: error: expected unqualified-id before '=' token\nexternal/highwayhash/highwayhash/sip_hash.h:34:38: error: 'Key' does not name a type\nexternal/highwayhash/highwayhash/sip_hash.h: In constructor 'highwayhash::SipHashState::SipHashState(const int&)':\nexternal/highwayhash/highwayhash/sip_hash.h:35:39: error: invalid types 'const int[int]' for array subscript\nexternal/highwayhash/highwayhash/sip_hash.h:36:39: error: invalid types 'const int[int]' for array subscript\nexternal/highwayhash/highwayhash/sip_hash.h:37:39: error: invalid types 'const int[int]' for array subscript\nexternal/highwayhash/highwayhash/sip_hash.h:38:39: error: invalid types 'const int[int]' for array subscript\nexternal/highwayhash/highwayhash/sip_hash.h: At global scope:\nexternal/highwayhash/highwayhash/sip_hash.h:114:36: error: 'Key' in 'class highwayhash::SipHashState' does not name a type\nexternal/highwayhash/highwayhash/sip_hash.h: In function 'highwayhash::uint64 highwayhash::SipHash(const int&, const char*, highwayhash::uint64)':\nexternal/highwayhash/highwayhash/sip_hash.h:116:52: error: no matching function for call to 'ComputeHash(const int&, const char*&, const uint64&)'\nexternal/highwayhash/highwayhash/sip_hash.h:116:52: note: candidate is:\nexternal/highwayhash/highwayhash/state_helpers.h:63:8: note: template<class State> highwayhash::uint64 highwayhash::ComputeHash(const typename State::Key&, const char*, highwayhash::uint64)\nexternal/highwayhash/highwayhash/sip_hash.h: At global scope:\nexternal/highwayhash/highwayhash/sip_hash.h:120:46: error: 'Key' in 'class highwayhash::SipHashState' does not name a type\ncc1plus: warning: unrecognized command line option \"-Wno-free-nonheap-object\" [enabled by default]\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nINFO: Elapsed time: 1577.488s, Critical Path: 22.08s`\n\nSystem:\nLinux version 3.4.90+ (root@debian) (gcc version 4.7.1 (Debian 4.7.1-7) )\n\nBazel:\nBuild label: 0.2.1-2016-06-28 (@447f7f3)\nBuild target: bazel-out/local_linux-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Tue Jun 28 17:09:27 2016 (1467133767)\nBuild timestamp: 1467133767\nBuild timestamp as int: 1467133767\n", "comments": ["Have you looked at the makefile approach in tensorflow/contrib/makefile? I created this to make it easier to just build the C++ core inference library on platforms like iOS and the Raspberry Pi.\n", "I haven't. Let me see if i can find my way to it. Would appreciate some pointers though\n", "Ok i followed your guide here:\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile\n\nSkipped building protobuf since i already built protobuf in my earlier attempt.\nprotoc --version\nlibprotoc 3.0.0\n\nAfter running\nmake -f tensorflow/contrib/makefile/Makefile\ni got,\ngcc: error: unrecognized option \u2018--std=c++11\u2019\n\nSo i updated gcc to 4.7.2 (gcc version 4.7.2 (Debian 4.7.2-5))\nand i got,\n./tensorflow/core/lib/gtl/inlined_vector.h:383:10: error: \u2018is_trivially_destructible\u2019 is not a member of \u2018std\u2019\n\nso i changed \u2018is_trivially_destructible\u2019 to 'has_trivial_destructor' and now i'm getting\n./tensorflow/core/lib/gtl/array_slice_internal.h:232:38: error: \u2018tensorflow::gtl::array_slice_internal::ArraySliceImplBase<const T>::ArraySliceImplBase\u2019 names constructor\n\nI'm obviously trying to build with the wrong gcc version or something. help!\n", "i could move past the original issue compiling tensorflow since i updated gcc. but it looks like i'm gonna run out of disk space soon. so would appreciate help resolving the makefile method\n", "I would recommend using at least gcc 4.8, that's the first one that has proper C++11 support:\nhttps://gcc.gnu.org/projects/cxx-status.html\nThe errors you're seeing seem related to the C++11 features that 4.7 is missing.\n", "debian wheezy armhf only has 4.7. maybe 4.8 is in jessie. I'll try. thanks!\n"]}, {"number": 3098, "title": "Update os_setup.md for new tensorflow release", "body": "updated links to tensorflow 0.9.0\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins test this please\n"]}, {"number": 3097, "title": "solve the building issues \"Undefined reference to symbol 'ceil@@GLIBC_2.2.5'\"", "body": "This is to solve the bug `Undefined reference to symbol 'ceil@@GLIBC_2.2.5'`\nhttps://github.com/tensorflow/tensorflow/issues/3070,\nthe solution is inspired from https://github.com/bazelbuild/bazel/issues/934 and https://github.com/tensorflow/tensorflow/issues/1171\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "Sadly doesn't work on mac, let me find an example of how to fix this.\n", "https://github.com/tensorflow/tensorflow/blob/4c789e39bedf51ec0d8f966126356e470c229665/third_party/gpus/cuda/BUILD#L64\n", "I am facing the same error. Is there a fix/patch for it yet?\n", "This is the fix/patch, it just needs one more modification before it can go in.\n", "So the patch worked for me but gives an error:\n\n> virtual memory exhausted: Cannot allocate memory\n\nFull error O/P:\n`ERROR: /home/sarthak/tensorflow/tensorflow/core/kernels/BUILD:331:1: C++ compilation of rule '//tensorflow/core/kernels:tile_ops' failed: gcc failed: error executing command \n  (cd /home/sarthak/.cache/bazel/_bazel_sarthak/886c0f367c70255a45d3226b973459e4/tensorflow && \\\n  exec env - \\\n    PATH=/home/sarthak/.linuxbrew/bin:/home/sarthak/.linuxbrew/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\n  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-std=c++0x' -DOPENSSL_NO_ASM -iquote . -iquote bazel-out/host/genfiles -iquote external/protobuf -iquote bazel-out/host/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -iquote external/farmhash_archive -iquote bazel-out/host/genfiles/external/farmhash_archive -iquote external/jpeg_archive -iquote bazel-out/host/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/host/genfiles/external/png_archive -iquote external/highwayhash -iquote bazel-out/host/genfiles/external/highwayhash -iquote external/re2 -iquote bazel-out/host/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/host/genfiles/external/eigen_archive -iquote external/zlib_archive -iquote bazel-out/host/genfiles/external/zlib_archive -iquote external/boringssl_git -iquote bazel-out/host/genfiles/external/boringssl_git -iquote external/jsoncpp_git -iquote bazel-out/host/genfiles/external/jsoncpp_git -isystem external/protobuf/src -isystem bazel-out/host/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/farmhash_archive/farmhash-34c13ddfab0e35422f4c3979f360635a8c050260 -isystem bazel-out/host/genfiles/external/farmhash_archive/farmhash-34c13ddfab0e35422f4c3979f360635a8c050260 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/host/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/host/genfiles/external/png_archive/libpng-1.2.53 -isystem external/highwayhash -isystem bazel-out/host/genfiles/external/highwayhash -isystem external/re2 -isystem bazel-out/host/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/host/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-802d984ade26 -isystem bazel-out/host/genfiles/external/eigen_archive/eigen-eigen-802d984ade26 -isystem external/zlib_archive/zlib-1.2.8 -isystem bazel-out/host/genfiles/external/zlib_archive/zlib-1.2.8 -isystem external/boringssl_git/include -isystem bazel-out/host/genfiles/external/boringssl_git/include -isystem external/jsoncpp_git/include -isystem bazel-out/host/genfiles/external/jsoncpp_git/include -fno-exceptions -DEIGEN_AVOID_STL_ARRAY -pthread -no-canonical-prefixes -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/host/bin/tensorflow/core/kernels/_objs/tile_ops/tensorflow/core/kernels/tile_ops.pic.o' -MD -MF bazel-out/host/bin/tensorflow/core/kernels/_objs/tile_ops/tensorflow/core/kernels/tile_ops.pic.d -fPIC -c tensorflow/core/kernels/tile_ops.cc -o bazel-out/host/bin/tensorflow/core/kernels/_objs/tile_ops/tensorflow/core/kernels/tile_ops.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nvirtual memory exhausted: Cannot allocate memory\n`\n", "@vrv ,  I modified as the example does\n", "@tensorflow-jenkins test this please\n", "Fixes it for Ubuntu 16.04 too.\n", "@vrv  I'm still running into this issue on RHEL 6.7 using gcc 4.7.2. This patch is included in my source, since I just puled recently. Any suggestions?\n\nEDIT:\nThis did the trick for me. Modify `LINK_OPTS` in `bazel-tensorflow/external/protobuf/BUILD` by adding the `-lm` flag to `//conditions:default`:\n\n```\nLINK_OPTS = select({\n    \":android\": [],\n    \"//conditions:default\": [\"-lpthread\", \"-lm\"],\n})\n```\n", "Just fix it manually\nThe patched file\n\nOp 5 sep. 2016 om 23:52 heeft Rasmi <notifications@github.com<mailto:notifications@github.com>> het volgende geschreven:\n\n@vrvhttps://github.com/vrv I'm still running into this issue on RHEL 6.7 using gcc 4.7.2. This patch is included in my source, since I just puled recently. Any suggestions?\n\n## \n\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHubhttps://github.com/tensorflow/tensorflow/pull/3097#issuecomment-244814825, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AFjpIQ11LtJMNlUHXJQkYOJlPBg2MAJ3ks5qnI8WgaJpZM4JAxlt.\n"]}, {"number": 3096, "title": "Wide and Deep docs update", "body": "", "comments": ["Failure is infrastructure flake, rerun here: http://ci.tensorflow.org/job/tensorflow-pull-requests-mac/1021/\n", "This Wide and Deep tutorial looks really interesting, it makes use of `tf.contrib.layers.embedding_column`, which doesn't seem to be part of tensorflow yet. Is the code for the embedding column going to be added or made available somewhere else?\n", "It's on master, but not on 0.9. Arguably these docs should not be on 0.9 either. :/\n\nThat confusion should be resolved with the next release.\n"]}, {"number": 3095, "title": "Branch 126112363", "body": "Pushing for last minute website update.\n", "comments": []}, {"number": 3094, "title": "Fix path to boringssl_err_data_c in workspace.bzl.", "body": "Passing in an absolute path for `path_prefix` gave me this error:\n\n```\ninvalid package name '.../boringssl': package names may not start with '/'.\n```\n\nI believe the intent was to refer to the target within the TF repo, and\nI think that `tf_repo_name` is supposed to accomplish this.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "@kirilg you added this rule, can you make sure this works for you?\n", "Looks good, verified that this works for TensorFlow Serving.\n"]}, {"number": 3093, "title": "Branch 126125286", "body": "Extra push\n", "comments": []}, {"number": 3092, "title": "Compile fails with 'DSO missing from command line'", "body": "```\nERROR: /sw/tensorflow/tensorflow/tools/proto_text/BUILD:31:1: Linking of rule '//tensorflow/tools/proto_text:gen_proto_text_functions' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/host/bin/tensorflow/tools/proto_text/gen_proto_text_functions ... (remaining 26 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n/usr/bin/ld: bazel-out/host/bin/tensorflow/core/liblib_internal.a(numbers.o): undefined reference to symbol 'ceil@@GLIBC_2.2.5'\n//lib/x86_64-linux-gnu/libm.so.6: error adding symbols: DSO missing from command line\ncollect2: error: ld returned 1 exit status\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 123.174s, Critical Path: 86.34s\n\n```\n### Environment info\n\nOperating System:\nLinux ghost 4.4.0-24-generic #43-Ubuntu SMP Wed Jun 8 19:27:37 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n\nInstalled version of CUDA and cuDNN: \n\nUsing 8.0\n(output of `ls -l /path/to/cuda/lib/libcud*`):\nhttps://gist.github.com/delip/6a85c0750a216a8e5340dd29606860f2\n\nIf installed from sources, provide the commit hash:\ncommit 214ba59755791a4c4631e2bc0325d77451483740\n### Steps to reproduce\n\nFollow usual compile steps\n1. ./configure\n2. bazel build -c opt --verbose_failures --config=cuda //tensorflow/cc:tutorials_example_trainer\n### What have you tried?\n1. bazel clean, rebuild\n2. Fresh clone + build\n### Logs or other output that would be helpful\n\nlogs with --verbose_failures\n\n```\nERROR: /sw/tensorflow/tensorflow/tools/proto_text/BUILD:31:1: Linking of rule '//tensorflow/tools/proto_text:gen_proto_text_functions' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command \n  (cd /home/delip/.cache/bazel/_bazel_delip/197c8f9456af837dca196cbcaf2ed97a/tensorflow && \\\n  exec env - \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/host/bin/tensorflow/tools/proto_text/gen_proto_text_functions bazel-out/host/bin/tensorflow/tools/proto_text/_objs/gen_proto_text_functions/tensorflow/tools/proto_text/gen_proto_text_functions.o bazel-out/host/bin/tensorflow/tools/proto_text/libgen_proto_text_functions_lib.a bazel-out/host/bin/tensorflow/core/liblib_internal.a bazel-out/host/bin/external/farmhash_archive/libfarmhash.a bazel-out/host/bin/external/jpeg_archive/libjpeg.a bazel-out/host/bin/external/png_archive/libpng.a bazel-out/host/bin/external/highwayhash/libsip_hash.a bazel-out/host/bin/external/re2/libre2.a bazel-out/host/bin/tensorflow/core/libprotos_all_cc.a bazel-out/host/bin/external/protobuf/libprotobuf.a bazel-out/host/bin/external/protobuf/libprotobuf_lite.a bazel-out/host/bin/external/zlib_archive/libzlib.a -ldl -lz -pthread -lpthread -lstdc++ -B/usr/bin/ -pie -Wl,-z,relro,-z,now -no-canonical-prefixes -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,-S -Wl,--gc-sections): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n/usr/bin/ld: bazel-out/host/bin/tensorflow/core/liblib_internal.a(numbers.o): undefined reference to symbol 'ceil@@GLIBC_2.2.5'\n//lib/x86_64-linux-gnu/libm.so.6: error adding symbols: DSO missing from command line\ncollect2: error: ld returned 1 exit status\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nINFO: Elapsed time: 0.856s, Critical Path: 0.13s\n\n```\n", "comments": ["I experience the same error with the similar system setup\n", "btw, I saw an issue earlier where someone starting getting this error after upgrading glibc\n", "Looks like the solution is in this pull request #3097 \n", "It looks like that pull request has been committed. Can you please try it and close this issue if it resolves your problem.\n", "Automatically closing due to lack of recent activity. Please reopen when further information becomes available.\n", "https://github.com/tensorflow/models/issues/255 has a fix that appears to work for me and some other people.\n"]}, {"number": 3091, "title": "Resolve the libcudnn library to its SONAME, when possible,", "body": "and build the binary with that SONAME, which is supposed to be\nABI-compatible.\n\nWhen we build our pip packages, we will now build with precisely\nthe version of the SONAME that we intend to support with the\nbinary packages.  This is the solution recommended by @3XX0.\n\nTested by installing cudnn 5.1.3 RC and seeing that the source code\nmodifications show the use of version \"5\" (the SONAME version\npointed to by libcudnn.so symlink).\n\nIn addition, installing the pip package shows:\n\n> > > import tensorflow as tf\n> > > I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\n> > > I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.5 locally\n> > > I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\n> > > I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\n> > > I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n", "comments": ["@3XX0: is this what you were thinking? \n", "Ok, merging, hopefully this will help in the future.\n"]}, {"number": 3090, "title": "Fix the TensorBoard README link on the how to page.", "body": "Change: 125799092\n\nBackporting this change from master to r0.9 to fix the link in the r0.9 TensorBoard how-to too.\n", "comments": ["Can one of the admins verify this patch?\n", "LGTM\n"]}, {"number": 3089, "title": "400 bad request when creating pip package", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\nCentOS 6\nInstalled version of CUDA and cuDNN: 7.5, 4.0\n\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\nCUDA and cuDNN files are both present\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   NA\n\nIf installed from sources, provide the commit hash:\n`25023dffcf88f46777b5ddab457ac84a5bed5d2f`\n### Steps to reproduce\n1. Build package from source\n2. Run `bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\n### What have you tried?\n1. Have tried looking at #1817 which had same error. It is not intermittent as described in that issue, it fails every single time but works if you just try to clone the repo directly.\n2. However, running `git clone https://github.com/google/boringssl.git` results in:\n\n```\ngit clone https://github.com/google/boringssl.git\nCloning into 'boringssl'...\nremote: Counting objects: 26620, done.\nremote: Compressing objects: 100% (310/310), done.\nremote: Total 26620 (delta 164), reused 0 (delta 0), pack-reused 26299\nReceiving objects: 100% (26620/26620), 18.72 MiB | 5.79 MiB/s, done.\nResolving deltas: 100% (18032/18032), done.\nChecking connectivity... done.\n```\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment)\n\n```\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\nWARNING: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/protobuf/WORKSPACE:1: Workspace name in /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/protobuf/WORKSPACE (@__main__) does not match the name given in the repository's definition (@protobuf); this will cause a build error in future versions.\nWARNING: /tensorflow/util/python/BUILD:11:16: in includes attribute of cc_library rule //util/python:python_headers: 'python_include' resolves to 'util/python/python_include' not in 'third_party'. This will be an error in the future.\nWARNING: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/highwayhash/WORKSPACE:1: Workspace name in /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/highwayhash/WORKSPACE (@__main__) does not match the name given in the repository's definition (@highwayhash); this will cause a build error in future versions.\nWARNING: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/re2/WORKSPACE:1: Workspace name in /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/re2/WORKSPACE (@__main__) does not match the name given in the repository's definition (@re2); this will cause a build error in future versions.\nERROR: /tensorflow/tensorflow/core/distributed_runtime/rpc/BUILD:48:1: no such package '@grpc//': Error cloning repository: https://github.com/google/boringssl.git: 400 Bad Request caused by https://github.com/google/boringssl.git: 400 Bad Request and referenced by '//tensorflow/core/distributed_runtime/rpc:grpc_util'.\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\nINFO: Elapsed time: 49.467s\n```\n", "comments": ["FWIW I was able to fix this by editing the workspace to use a local repository of boringssl that I cloned manually.\n", "Unfortunately, this looks like bazel's inability to deal with connectivity issues with github, though not sure why it is persistent in your case. Do you have restricted internet connectivity? In either event, consider opening an issue against the bazel repo for this one.\n"]}, {"number": 3088, "title": "nvidia/cuda:7.5-cudnn4-devel + 0.9.0 fails after second epoch", "body": "### Environment info\n\nOperating System:\n\n```\n Ubuntu 14.04\n```\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\nnvidia/cuda:7.5-cudnn4-devel\nnvidia/cuda:7.5-cudnn5-devel\n\nDriver version: 361.42 (AWS)\n```\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   \n   ```\n   export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0-cp27-none-linux_x86_64.whl\n   ```\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   \n   ```\n   0.9.0\n   ```\n\nIf installed from sources, provide the commit hash:\n\n```\nN/A\n```\n### Steps to reproduce\n1. Train simple convnet for one epoch\n2. Fails on second epoch? (no new operations or changes between epochs)\n### What have you tried?\n1. Tried various CUDA/cuDNN nvidia-docker versions.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n\n```\nF tensorflow/stream_executor/cuda/cuda_dnn.cc:422] could not set cudnn tensor descriptor: CUDNN_STATUS_BAD_PARAM\nFailed to run user code: signal: aborted (core dumped)\n```\n\nThe docker container reports the expected CUDA/CUDNN version (7.5 and 4 respectively).\n", "comments": ["Closing as user error.\n"]}, {"number": 3087, "title": "tensorbaord graph broken for multi-gpu (cifar10 example)", "body": "![graph-run 1](https://cloud.githubusercontent.com/assets/15835199/16431922/c084e280-3d50-11e6-9b9d-5169d84ae865.png)\nGitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nCentOS 6 (64 bit) + NVidia K80\n\nInstalled version of CUDA and cuDNN: \nCUDA 7.5, cuDNN v4\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   The lateset (0.9 with GPU)\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   0.9.0\n### Steps to reproduce\n1. Locate the cifar10 example https://github.com/tensorflow/tensorflow/tree/master/tensorflow/models/image/cifar10\n2. run \"python cifar10_multi_gpu_train.py --num_gpus=2\"\n3. Check the tensorboard results from events. The \"GRAPHS\" doesn't look right\n### What have you tried?\n1. I tried to run the single GPU version of cifar10 example \"python cifar10_train.py\", Tensorboard graph looks normal\n2. I tried \"python cifar10_multi_gpu_train.py --num_gpus=1\", Tensorboard graph broken\n3. I didn't try a early version of tensorflow this time, but I remember the Tensorboard used to work with the cifar10 multi-gpu example\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["Same problem for imagenet multi-GPU training.\n", "Hi @JianbangZ ! Thanks for reporting.\n\nThe reason why the multi-GPU graph looks like that is because the namescoping in the multi-GPU version creates tower_N namespaces that have incoming edges (tensors) above a certain threshold, at which point we extract those nodes on the side since usually they end up being auxiliary and not part of the main net architecture.\n\nIf we don't extract nodes on the side, many graphs will be highly intertwined and unreadable. The current threshold for incoming edges was decided empirically, but as this example suggest, it is not ideal. We will try out different thresholds and heuristics and get back to you!\n", "Hi @dsmilkov  is this issue fixed in 0.10.0?\n", "Assuming this was fixed in later versions. Feel free to open a new issue if the problem persists with new code.", "Getting almost similar unconnected series. Is it fine for CIFAR10 multi GPU code? Please suggest. \r\n\r\n<img width=\"1390\" alt=\"screen shot 2019-01-16 at 11 25 47 am\" src=\"https://user-images.githubusercontent.com/17875749/51267948-e6372400-198c-11e9-91db-12becf7097a0.png\">\r\n"]}, {"number": 3086, "title": "from google.protobuf import descriptor as _descriptor ImportError: No module named google.protobuf", "body": "Hey guys I'm working on a project using tensorflow on the Amazon Web Services microservices platform and I've run into a few problems. I installed tensorflow using pip into a virtualenv. Then I took the contents of /env/lib/python2.7/site-packages/ and zipped it along with my source code into a development package for AWS Lambda (env is the name of my virtualenv). This is the process for using python libraries for an AWS Lambda process and I haven't run into any problems with other libraries. I've done this for numpy,scipy,Pillow, and a bunch of other far less supported libraries and they have all worked fine with Lambda.\n\nHowever when I attempt to use tensorflow it returns this:\n\nUnable to import module 'classify': Traceback (most recent call last): File `\"/var/task/tensorflow/python/__init__.py\",` line 52, in <module> from tensorflow.core.framework.graph_pb2 import \\* File \"/var/task/tensorflow/core/framework/graph_pb2.py\", line 6, in <module> from google.protobuf import descriptor as _descriptor ImportError: No module named google.protobuf   Error importing tensorflow. Unless you are using bazel, you should not try to import tensorflow from its source directory; please exit the tensorflow source tree, and relaunch your python interpreter from there.\n\nclassify is the source script AWS Lambda calls when it is invoked and was zipped along with the contents of  /env/lib/python2.7/site-packages/  as I described.\n\nThese errors have been addressed a couple of other places but none of the solutions I found on those pages have worked for me, probably because none of them were using lambda. Is it possible that tensorflow just will not work with lambda because of the way lambda imports libraries? If so is there anyway import the tensorflow package from just a source folder that I upload as a zip?\n\nSpecific Steps to Reproduce:\n- virtualenv env \n- env/bin/pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0-cp27-none-linux_x86_64.whl\n- zip contents of /env/lib/python2.7/site-packages with .py file classify\n- upload to aws lambda function\n- invoke aws lambda function which calls classify and runs into an issue with \"import tensorflow as tf\"\n\nIt's occurred to me that this probably isn't something tensorflow is probably supporting given that most people aren't using tensorflow inside AWS containers but hopefully someone can help or at least give it their best shot even if they don't specifically have experience with AWS.\n\nThanks!!\n", "comments": ["Okay I have no idea what this does but it fixed this whole issue::\n\ncd into site-packages\n`touch google/__init__.py` \ndo this before you zip up site-packages\n^bash\n", "That is fascinating. I have no idea what the problem is, but it may be endemic to AWS lambda. I'll close this issue.\n", "Stumbled over this on my travels. It's not Lambda, it's the `protobuf` module: https://github.com/google/protobuf/issues/1296\n"]}, {"number": 3085, "title": "TensorFlow r0.9 warning warns about Deprecation and recommends itself.", "body": "TensorFlow r0.9:\n\n```\nIn [3]: tf.logging.warning('asdf')\n/home/hholst/anaconda3/envs/tf-r0.9/bin/ipython:1: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n  #!/home/hholst/anaconda3/envs/tf-r0.9/bin/python\nWARNING:tensorflow:asdf\n```\n", "comments": ["That is pretty funny :)\n\nIt is really easy to fix if you want to fix it.\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/tf_logging.py#L56\nChange it to use logger._warning (and verify that everything still works)\n", "Sorry if this is a naive question:\n\nWhat is the standard way to verify that \"everything still works\"?\nIs there a test suite to validate changes?\n", "You can run bazel test //tensorflow/... to run every test, although that will be unnecessary since it will test a lot of code you haven't touched at all. bazel test //tensorflow/python/... is plenty.\n\nWhen you submit a PR, we will have Jenkins test it, and this will build and test the code in several different environments.\n", "@danmane I verified that there was no change in the amount of errors before and after the change.\n"]}, {"number": 3084, "title": "ImportError: No module named tensorflow", "body": "**I followed the steps for Anaconda installation for the CPU install of Tensorflow. (r0.9)**\n\n**Despite installation completing successfully in my conda environment on CentOS, I am unable to import tensorflow**\n### Environment info\n\nOperating System:  **CentOS 7.2.1511**\n                                **Python 2.7.11 Anaconda 4.0.0 (64-bit)**\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n\n```\n# Ubuntu/Linux 64-bit, CPU only, Python 2.7\n$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0-cp27-none-linux_x86_64.whl\n```\n1. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\n```\n(tensorflow)[hmanjunatha@curran ~]$ python -c \"import tensorflow; print(tensorflow.__version__)\"\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nImportError: No module named tensorflow\n```\n### What have you tried?\n1. Searched on Google, Stackoverflow, but to no avail\n2. **OS X installation of tensorflow worked perfectly**\n", "comments": ["Installation\n\n```\n(tensorflow)[hmanjunatha@curran ~]$ pip install --upgrade $TF_BINARY_URL\nCollecting tensorflow==0.9.0 from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0-cp27-none-linux_x86_64.whl\n  Downloading https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0-cp27-none-linux_x86_64.whl (27.6MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 27.6MB 39kB/s \nCollecting numpy>=1.8.2 (from tensorflow==0.9.0)\n  Using cached numpy-1.11.1-cp27-cp27mu-manylinux1_x86_64.whl\nRequirement already up-to-date: six>=1.10.0 in ./anaconda2/envs/tensorflow/lib/python2.7/site-packages (from tensorflow==0.9.0)\nCollecting protobuf==3.0.0b2 (from tensorflow==0.9.0)\n  Using cached protobuf-3.0.0b2-py2.py3-none-any.whl\nRequirement already up-to-date: wheel in ./anaconda2/envs/tensorflow/lib/python2.7/site-packages (from tensorflow==0.9.0)\nCollecting setuptools (from protobuf==3.0.0b2->tensorflow==0.9.0)\n  Using cached setuptools-23.1.0-py2.py3-none-any.whl\nInstalling collected packages: numpy, setuptools, protobuf, tensorflow\n  Found existing installation: numpy 1.11.0\n    Uninstalling numpy-1.11.0:\n      Successfully uninstalled numpy-1.11.0\n  Found existing installation: setuptools 23.0.0\nCannot remove entries from nonexistent file /home/hmanjunatha/anaconda2/envs/tensorflow/lib/python2.7/site-packages/easy-install.pth\n(tensorflow)[hmanjunatha@curran ~]$ \n```\n", "Paste output of  `python --version` and also `pip --version`\n", "@thammegowda \nPython 2.7.11 Anaconda 4.0.0 (64-bit)\n\n**Edit:**\n\n```\n$ pip --version\npip 8.1.2 from /home/hmanjunatha/anaconda2/envs/tensorflow/lib/python2.7/site-packages (python 2.7)\n```\n", "@harsham05  Looks good. I suspect anaconda and its PYTHONPATH \n", "Looks like for CentOS, tensorflow needs to be built from source.\nClosing this\n", "I am seeing the same problem with Ubuntu 14.04 and Anaconda. I have tried the install with python 2.7, 3.4 and 3.5, and all give the same error: ImportError: No module named tensorflow\n", "Same resolution as @harsham05 : build from source worked fine.\n", "@rob-rowe \ud83d\udc4d \n", "r0.9 works well, after I upgrade to r0.10, encounter the same issue, and my tensorflow built from source.\n", "I am r1.3 and have encounters the similar issue (built from source). I got a whl but can't import. (centOS)"]}, {"number": 3083, "title": "ValueError in tensorflow/examples/skflow/mnist_weights.py", "body": "### Environment info\n\nOS: OS X 10.11.5\n\nCuda version: 7.5.30\n\ncuDNN version: v5.1rc (or 5103)\n\nTensorflow version: 0.9.0rc0\n\nInstalled with cuda support using these [instructions](https://medium.com/@fabmilo/how-to-compile-tensorflow-with-cuda-support-on-osx-fd27108e27e1#.mf4zcsmu3).\n### Steps to reproduce:\n1. cd tensorflow/examples/skflow/\n2. $ python mnist_weights.py\n### Output:\n\n```\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.7.5.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.5.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.7.5.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.7.5.dylib locally\nExtracting MNIST_data/train-images-idx3-ubyte.gz\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\nWARNING:tensorflow:TensorFlowLinearClassifier class is deprecated. Please consider using LinearClassifier as an alternative.\nTraceback (most recent call last):\n  File \"mnist_weights.py\", line 37, in <module>\n    classifier.fit(mnist.train.images, mnist.train.labels)\n  File \"/Users/tcf/.virtualenvs/dnn/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/base.py\", line 458, in fit\n    batch_size=batch_size or self.batch_size, monitors=monitors)\n  File \"/Users/tcf/.virtualenvs/dnn/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 182, in fit\n    monitors=monitors)\n  File \"/Users/tcf/.virtualenvs/dnn/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 449, in _train_model\n    train_op, loss_op = self._get_train_ops(features, targets)\n  File \"/Users/tcf/.virtualenvs/dnn/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/linear.py\", line 97, in _get_train_ops\n    return super(LinearClassifier, self)._get_train_ops(features, targets)\n  File \"/Users/tcf/.virtualenvs/dnn/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py\", line 158, in _get_train_ops\n    targets, self._get_weight_tensor(features))]):\n  File \"/Users/tcf/.virtualenvs/dnn/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py\", line 282, in _centered_bias_step\n    loss = self._loss(logits, targets, weight_tensor)\n  File \"/Users/tcf/.virtualenvs/dnn/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py\", line 316, in _loss\n    loss_vec = self._loss_vec(logits, target)\n  File \"/Users/tcf/.virtualenvs/dnn/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py\", line 510, in _loss_vec\n    \"Instead got %s.\" % target.dtype)\nValueError: Target's dtype should be int32, int64 or compatible. Instead got <dtype: 'uint8'>.\n```\n### What have I tried?\n\nI used the ndarray method `.astype` to change the type appropriately. That is, I replaced `mnist.train.images` with `mnist.train.images.astype(np.float32)` and `mnist.train.labels` with `mnist.train.images.astype(int)`.\n##### I can make a PR for this if you want.\n", "comments": ["To sum up discussion with @martinwicke on the proposed PR: \"This is an outdated example, it's in line to be updated, just has to wait its turn. If the new [tflearn] code cannot handle uint8s it should.\"\n\n@ilblackdragon will you address this issue or assign to whoever will?\n", "This example was removed recently. Probably we will update when we write new example :)\n"]}, {"number": 3082, "title": "Branch 126082003", "body": "Push of internal Google updates to TensorFlow.\n", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "@tensorflow-jenkins test this please\n", "@willnorris it looks like the cla checker bot isn't checking the commits for these types of PRs -- just an FYI.\n", "@googlebot I signed it as Google Inc.\n", "@vrv @martinwicke : this looks ready to be merged, apart from the (bogus?) cla check.\n", "@rmlarsen: This PR is in a state that our CLA checker can't handle.  For this, and ones like it, it's safe to ignore googlebot (see [go/cla#It's okay to accept](http://go/cla#It's okay to accept)).  Also, [b/28148245](http://b/28148245) for a relevant bug that should help PRs like this.\n", "@willnorris thanks for the clarification.\n"]}, {"number": 3081, "title": "Check for Xcode 7.3 fails in compile_ios_tensorflow.sh", "body": "```\n++ xcodebuild -version\n++ head -n 1\n++ sed 's/Xcode //'\n+ ACTUAL_XCODE_VERSION=7.3\n+ REQUIRED_XCODE_VERSION=7.3.0\n+ '[' 73 -lt 730 ']'\n+ echo 'error: Xcode 7.3.0 or later is required.'\nerror: Xcode 7.3.0 or later is required.\n```\n\nThis should be easy to fix :)\n", "comments": ["Can you please provide the information from the template that is presented when you created the issue?\n\nGitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n\n### Environment info\n\nOperating System:\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from sources, provide the commit hash:\n\n### Steps to reproduce\n\n1.\n2.\n3.\n\n### What have you tried?\n\n1.\n\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "Automatically closing due to lack of recent activity. Please reopen when further information becomes available.\n"]}, {"number": 3080, "title": "Problem with meta file size when saving model initialized with pre-trained weights", "body": "### Context/Problem\n\nI have pretrained models (from elsewhere) with weights saved as numpy arrays. I want to create a tensorflow model where the weights and biases are initialized with numpy arrays rather than random tensors. I am able to do this (code snippet below), but run into problems when I try saving/loading my model.\n\nSpecifically, the problem involves using the `Saver` class to save `model` and `model.meta`. I expect `model.meta` to be much smaller in size than `model`, but this is not the case. In fact, using the code snippet below, you can see that the meta file is the same size as the model file.\n\nThis results in problems when importing the meta file with much larger models.\n### Environment info\n\nOperating System: Ubunutu 14.04\nTensorflow version: '0.9.0rc0'\n### Steps to reproduce\n\n```\nimport tensorflow as tf\nimport numpy as np\n\n# normally pretrained weights, but random arrays used for brevity\nweights = np.random.rand(1000, 500).astype(np.float32)\nbiases = np.random.rand(500).astype(np.float32)\n\nx = tf.placeholder(tf.float32, [None, 1000], name='input')\n\nW = tf.Variable(weights)\nb = tf.Variable(biases)\n\nfc = tf.matmul(x, W, name='fc1')\nfc = tf.nn.bias_add(fc, b)\ny_pred = tf.nn.softmax(fc, name='output')\n\n# saving model\nsess = tf.Session()\ninit_op = tf.initialize_all_variables()\nsess.run(init_op)\nsaver = tf.train.Saver()\nsaver.save(sess, \"model\")\n```\n", "comments": ["Looking at tensorflow/tensorflow/core/protobuf/meta_graph.proto, it's clear that the meta graph proto stores the entire graph def (in addition to the meta information). This is why the file is >= model file size.\nThis was done for use cases where they wanted to package the GraphDef and meta info into one blob.\nIf this does not fit your use case, you can configure saver.py to not produce the meta files.\n", "If I run this snippet instead of the one earlier, then `model` is 2.0M, but `model.meta` is 8.0K. My question regards why running the earlier code (when weights/biases were initialized with a numpy array) result in both `model` and `model.meta` size to be 2.0M, whereas that's not the case here.\n\n```\nimport tensorflow as tf\nimport numpy as np\n\nx = tf.placeholder(tf.float32, [None, 1000], name='input')\n\nW = tf.Variable(tf.random_normal([1000,500]))\nb = tf.Variable(tf.random_normal([500]))\n\nfc = tf.matmul(x, W, name='fc1')\nfc = tf.nn.bias_add(fc, b)\ny_pred = tf.nn.softmax(fc, name='output')\n\n# saving model\nsess = tf.Session()\ninit_op = tf.initialize_all_variables()\nsess.run(init_op)\nsaver = tf.train.Saver()\nsaver.save(sess, \"model\")\n```\n", "My guess here is that in your first example (where you initialize the variables from the output of numpy random), that this forces the GraphDef to have a large Constant tensor nodes (to contain the 2MB of random data needed to initialize the Variables). For the second case, it does not need to \"inline\" the Variable initialization data into the graph, because this data is generated when the graph is run.\n", "Ah, thanks. Do you happen to know a simple way to initialize the weight/bias variables with a numpy array while not forcing the GraphDef to have large Constant tensor nodes?\n", "You can try using a placeholder and feed_dict. Here is an example courtesy of @mrry:\n\n> > > bigarray = np.zeros((1024, 1024, 1024), dtype=np.float32)\n> > > p = tf.placeholder(tf.float32, shape=bigarray.shape)\n> > > v = tf.Variable(p)\n> > > sess = tf.Session()\n> > > sess.run(tf.initialize_all_variables(), feed_dict={p: bigarray})\n> > > sess.run(tf.reduce_sum(v))\n"]}, {"number": 3079, "title": "Wheel not supported installation problem: GPU Ubuntu 64 bit", "body": "My installation attempt failed on the last step:\n\n```\ndrake@sparky:~$ sudo pip3 install --upgrade $TF_BINARY_URL\n[sudo] password for drake: \nThe directory '/home/drake/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\nThe directory '/home/drake/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\ntensorflow-0.9.0-cp35-cp35m-linux_x86_64.whl is not a supported wheel on this platform.\ndrake@sparky:~$ whoami\ndrake\ndrake@sparky:~$ sudo -H pip3 install --upgrade $TF_BINARY_URL\ntensorflow-0.9.0-cp35-cp35m-linux_x86_64.whl is not a supported wheel on this platform.\n```\n\nWhat to do?\n### Environment info\n\nOperating System:\n\n```\ndrake@sparky:/usr/local/bin/cuda/include$ uname -a\nLinux sparky 3.13.0-88-generic #135-Ubuntu SMP Wed Jun 8 21:10:42 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n```\n\nInstalled version of CUDA and cuDNN: \n\nI've installed cuda-repo-ubuntu1404_7.5-18_amd64.deb.\n\nI _think_ I've installed cuDNN 5.0, but the instructions there are vague offer no way to test it.\n\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\ndrake@sparky:/usr/local/bin/cuda$ ls -l lib64/libcud*\nlrwxrwxrwx 1 root root       13 Jun 28 09:21 lib64/libcudnn.so -> libcudnn.so.5\nlrwxrwxrwx 1 root root       17 Jun 28 09:21 lib64/libcudnn.so.5 -> libcudnn.so.5.0.5\n-rwxr-xr-x 1 root root 59909104 Jun 28 09:21 lib64/libcudnn.so.5.0.5\n-rw-r--r-- 1 root root 58775484 Jun 28 09:21 lib64/libcudnn_static.a\n```\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n\nSee above\n1. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\n```\ndrake@sparky:/usr/local/bin/cuda$ python3 -c \"import tensorflow; print(tensorflow.__version__)\"\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nImportError: No module named 'tensorflow'\n```\n", "comments": ["I am getting same with a nearly identical setup.   When I did the python 3.4 and 2.7 install I got different errors bit none completed.\n\nWill try and update later with specifics\n", "this got it done for me \n\n```\n(tensorflow) tscharf@ch0ds01:~$ echo $TF_BINARY_URL\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0-cp35-cp35m-linux_x86_64.whl\n(tensorflow) tscharf@ch0ds01:~$ pip3 install --upgrade --ignore-installed $TF_BINARY_URL\ntensorflow-0.9.0-cp35-cp35m-linux_x86_64.whl is not a supported wheel on this platform.\nStoring debug log for failure in /home/tscharf/.pip/pip.log\n(tensorflow) tscharf@ch0ds01:~$ pip install --upgrade --ignore-installed $TF_BINARY_URL\n```\n\nI dropped the pip3  and added an --ignore-installed flag and it managed to install\n", "This didn't work for me. It claimed to install, and the first test worked, but when I tried to do the convolutional MNIST test at\n\nhttps://www.tensorflow.org/versions/r0.9/get_started/os_setup.html#run-a-tensorflow-demo-model\n\nI got:\n\n```\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:346] Loaded cudnn library: 5103 but source was compiled against 4007.  If using a binary install, upgrade your cudnn library to match.  If building from sources, make sure the library loaded matches the version you specified during compile configuration.\nF tensorflow/core/kernels/conv_ops.cc:459] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) \n```\n\nI also tried building from source, which got me to exactly the same point. I'm now caught in a bind:\n\n1) When I built from source, I definitely specified cuDNN 5.1.3, and that's what I installed, but the source (of what?) \"was compiled against 4007\".\n\n2) When I tried to use the binary as tim-scharf suggested, I got the same problem. I can't downgrade cuDNN to version 4, because THAT installation requires downgrading from CUDA 7.5 to 7.0, and I can't figure out how to do that.\n\nAny suggestions would be appreciated.\n", "Ok I still have the CuDNN error as well -- I have 5.0.5 installed and Tensorflow wants 4.0.4.\n\nApparently this is configurable \n\n```\n$ ./configure\nPlease specify the location of python. [Default is /usr/bin/python]:\nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\n\nPlease specify which gcc nvcc should use as the host compiler. [Default is\n/usr/bin/gcc]: /usr/bin/gcc-4.9\n\nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave\nempty to use system default]: 7.5\n\nPlease specify the location where CUDA 7.5 toolkit is installed. Refer to\nREADME.md for more details. [default is: /usr/local/cuda]: /usr/local/cuda\n\nPlease specify the Cudnn version you want to use. [Leave empty to use system\ndefault]: 4.0.4\n\nPlease specify the location where the cuDNN 4.0.4 library is installed. Refer to\nREADME.md for more details. [default is: /usr/local/cuda]: /usr/local/cudnn-r4-rc/\n\nPlease specify a list of comma-separated Cuda compute capabilities you want to\nbuild with. You can find the compute capability of your device at:\nhttps://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your\nbuild time and binary size. [Default is: \\\"3.5,5.2\\\"]: 3.5\n\nSetting up Cuda include\nSetting up Cuda lib64\nSetting up Cuda bin\nSetting up Cuda nvvm\nSetting up CUPTI include\nSetting up CUPTI lib64\nConfiguration finished\n```\n\nhttps://www.tensorflow.org/versions/r0.9/get_started/os_setup.html#gpu-related-issues\n", "Telling it to use cuDNN 4 did the trick! Thanks!\n", "Looks resolved. Closing for now...\n", "@PeterDrake Could you please tell me how to do that?\r\n\r\nI got\r\n\r\n```\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:390] Loaded runtime CuDNN library: 5005 (compatibility version 5000) but source was compiled with 5105 (compatibility version 5100).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.\r\nF c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\kernels\\conv_ops.cc:605] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\r\n```", "I'm afraid I don't have any notes other than what's in this thread. I'm guessing I followed the advice posted by tim-scharf above."]}, {"number": 3078, "title": "Allow use of gitter.im for tensorflow/tensorflow repo", "body": "As far as I could tell, there is no place for the community to discuss tensorflow in realtime; the recommendations all seem to be about asking specific questions in a detailed async fashion.\n\nIt would be nice if there was a gitter.im or IRC channel for tensorflow so that the community could interact more easily. not even as a dedicated support channel.\n\nThere is already a gitter.im for skflow, but since fewer people know about skflow, let alone the gitter.im for it, it's pretty small: https://gitter.im/tensorflow/skflow\n\nIt would be even nicer if the channel was linked from the repo, but gitter.im is nice since it has some automatic discoverability if you're already familiar with gitter\n", "comments": ["This is a duplicate of #132, and unfortunately our bandwidth is quite limited right now. Things have not changed in terms of our ability to support another official channel of communication, unfortunately.\n", "Is there a way to have an unofficial but discoverable place for real-time communication? I'm more interested in talking to members of the community than recieving official support.\n"]}, {"number": 3077, "title": "Remove explicit dependency on Python 2.7 from crosstool wrapper", "body": "Many superclusters need to compile TensorFlow from source due to an outdated glibc version (see #110). In @rdipietro's excellent workaround post (https://github.com/tensorflow/tensorflow/issues/110#issuecomment-219790730) he mentions issues with the referenced Python version in this file. I have issues as well, but of a different nature. In my case the build script is unable to find `libpython2.7.so.1.0`, since only Python 3 is present on my machine. The issue originates from `crosstool_wrapper_driver_is_not_gcc` where the only Python 2.7 exclusive feature is the `print` statement. By `import`ing `print_function from __future__` the explicit dependency can be dropped and both versions of Python are supported.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n"]}, {"number": 3076, "title": "Problem compiling version 0.9 for iOS", "body": "### Environment info\n\nOperating System: macOS 10.11.5\nxcode 7.3.1\nI tried installing on 2 different computers and got the exact same error. One system was very clean as it was freshly initialized out of the box and had not been polluted.\n### Steps to reproduce\n\nI follow the steps indicated here first for the Makefile and than the steps specific to install for iOS\n- clone the repository\n- install the dependencies\n- run build_all_ios.sh\n\nEverything run fine for a little while and I get the following error:\n\nconfigure: creating ./config.status\nconfig.status: creating Makefile\nconfig.status: creating scripts/gtest-config\nconfig.status: creating build-aux/config.h\nconfig.status: executing depfiles commands\nconfig.status: executing libtool commands\n- make\n  /Applications/Xcode.app/Contents/Developer/usr/bin/make  all-recursive\n  Making all in .\n  make[2]: Nothing to be done for `all-am'.\n  Making all in src\n  /Users/lcavalie/iOSdev/tensorflow/tensorflow/contrib/makefile/gen/protobuf-host/bin/protoc -I. --cpp_out=. google/protobuf/any_test.proto google/protobuf/compiler/cpp/cpp_test_bad_identifiers.proto google/protobuf/map_lite_unittest.proto google/protobuf/map_proto2_unittest.proto google/protobuf/map_unittest.proto google/protobuf/unittest_arena.proto google/protobuf/unittest_custom_options.proto google/protobuf/unittest_drop_unknown_fields.proto google/protobuf/unittest_embed_optimize_for.proto google/protobuf/unittest_empty.proto google/protobuf/unittest_enormous_descriptor.proto google/protobuf/unittest_import_lite.proto google/protobuf/unittest_import.proto google/protobuf/unittest_import_public_lite.proto google/protobuf/unittest_import_public.proto google/protobuf/unittest_lite_imports_nonlite.proto google/protobuf/unittest_lite.proto google/protobuf/unittest_mset.proto google/protobuf/unittest_mset_wire_format.proto google/protobuf/unittest_no_arena_lite.proto google/protobuf/unittest_no_arena_import.proto google/protobuf/unittest_no_arena.proto google/protobuf/unittest_no_field_presence.proto google/protobuf/unittest_no_generic_services.proto google/protobuf/unittest_optimize_for.proto google/protobuf/unittest_preserve_unknown_enum2.proto google/protobuf/unittest_preserve_unknown_enum.proto google/protobuf/unittest.proto google/protobuf/unittest_proto3_arena.proto google/protobuf/unittest_proto3_arena_lite.proto google/protobuf/unittest_proto3_lite.proto google/protobuf/unittest_well_known_types.proto google/protobuf/util/internal/testdata/anys.proto google/protobuf/util/internal/testdata/books.proto google/protobuf/util/internal/testdata/default_value.proto google/protobuf/util/internal/testdata/default_value_test.proto google/protobuf/util/internal/testdata/field_mask.proto google/protobuf/util/internal/testdata/maps.proto google/protobuf/util/internal/testdata/oneofs.proto google/protobuf/util/internal/testdata/struct.proto google/protobuf/util/internal/testdata/timestamp_duration.proto google/protobuf/util/json_format_proto3.proto google/protobuf/util/message_differencer_unittest.proto google/protobuf/compiler/cpp/cpp_test_large_enum_value.proto\n  make[2]: /Users/lcavalie/iOSdev/tensorflow/tensorflow/contrib/makefile/gen/protobuf-host/bin/protoc: No such file or directory\n  make[2]: **\\* [unittest_proto_middleman] Error 1\n  make[1]: **\\* [all-recursive] Error 1\n  make: **\\* [all] Error 2\n- make install\n  Making install in .\n  make[2]: Nothing to be done for `install-exec-am'.\n  ./install-sh -c -d '/Users/lcavalie/iOSdev/tensorflow/tensorflow/contrib/makefile/gen/protobuf_ios/lib/ios_arm64/lib/pkgconfig'\n  /usr/bin/install -c -m 644 protobuf.pc protobuf-lite.pc '/Users/lcavalie/iOSdev/tensorflow/tensorflow/contrib/makefile/gen/protobuf_ios/lib/ios_arm64/lib/pkgconfig'\n  Making install in src\n  /Users/lcavalie/iOSdev/tensorflow/tensorflow/contrib/makefile/gen/protobuf-host/bin/protoc -I. --cpp_out=. google/protobuf/any_test.proto google/protobuf/compiler/cpp/cpp_test_bad_identifiers.proto google/protobuf/map_lite_unittest.proto google/protobuf/map_proto2_unittest.proto google/protobuf/map_unittest.proto google/protobuf/unittest_arena.proto google/protobuf/unittest_custom_options.proto google/protobuf/unittest_drop_unknown_fields.proto google/protobuf/unittest_embed_optimize_for.proto google/protobuf/unittest_empty.proto google/protobuf/unittest_enormous_descriptor.proto google/protobuf/unittest_import_lite.proto google/protobuf/unittest_import.proto google/protobuf/unittest_import_public_lite.proto google/protobuf/unittest_import_public.proto google/protobuf/unittest_lite_imports_nonlite.proto google/protobuf/unittest_lite.proto google/protobuf/unittest_mset.proto google/protobuf/unittest_mset_wire_format.proto google/protobuf/unittest_no_arena_lite.proto google/protobuf/unittest_no_arena_import.proto google/protobuf/unittest_no_arena.proto google/protobuf/unittest_no_field_presence.proto google/protobuf/unittest_no_generic_services.proto google/protobuf/unittest_optimize_for.proto google/protobuf/unittest_preserve_unknown_enum2.proto google/protobuf/unittest_preserve_unknown_enum.proto google/protobuf/unittest.proto google/protobuf/unittest_proto3_arena.proto google/protobuf/unittest_proto3_arena_lite.proto google/protobuf/unittest_proto3_lite.proto google/protobuf/unittest_well_known_types.proto google/protobuf/util/internal/testdata/anys.proto google/protobuf/util/internal/testdata/books.proto google/protobuf/util/internal/testdata/default_value.proto google/protobuf/util/internal/testdata/default_value_test.proto google/protobuf/util/internal/testdata/field_mask.proto google/protobuf/util/internal/testdata/maps.proto google/protobuf/util/internal/testdata/oneofs.proto google/protobuf/util/internal/testdata/struct.proto google/protobuf/util/internal/testdata/timestamp_duration.proto google/protobuf/util/json_format_proto3.proto google/protobuf/util/message_differencer_unittest.proto google/protobuf/compiler/cpp/cpp_test_large_enum_value.proto\n  make[1]: /Users/lcavalie/iOSdev/tensorflow/tensorflow/contrib/makefile/gen/protobuf-host/bin/protoc: No such file or directory\n  make[1]: **\\* [unittest_proto_middleman] Error 1\n  make: **\\* [install-recursive] Error 1\n- lipo /Users/lcavalie/iOSdev/tensorflow/tensorflow/contrib/makefile/gen/protobuf_ios/lib/iossim_386/lib/libprotobuf.a /Users/lcavalie/iOSdev/tensorflow/tensorflow/contrib/makefile/gen/protobuf_ios/lib/iossim_x86_64/lib/libprotobuf.a /Users/lcavalie/iOSdev/tensorflow/tensorflow/contrib/makefile/gen/protobuf_ios/lib/ios_arm7/lib/libprotobuf.a /Users/lcavalie/iOSdev/tensorflow/tensorflow/contrib/makefile/gen/protobuf_ios/lib/ios_arm7s/lib/libprotobuf.a /Users/lcavalie/iOSdev/tensorflow/tensorflow/contrib/makefile/gen/protobuf_ios/lib/ios_arm64/lib/libprotobuf.a -create -output /Users/lcavalie/iOSdev/tensorflow/tensorflow/contrib/makefile/gen/protobuf_ios/lib/libprotobuf.a\n  fatal error: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: can't open input file: /Users/lcavalie/iOSdev/tensorflow/tensorflow/contrib/makefile/gen/protobuf_ios/lib/iossim_386/lib/libprotobuf.a (No such file or directory)\n- lipo /Users/lcavalie/iOSdev/tensorflow/tensorflow/contrib/makefile/gen/protobuf_ios/lib/iossim_386/lib/libprotobuf-lite.a /Users/lcavalie/iOSdev/tensorflow/tensorflow/contrib/makefile/gen/protobuf_ios/lib/iossim_x86_64/lib/libprotobuf-lite.a /Users/lcavalie/iOSdev/tensorflow/tensorflow/contrib/makefile/gen/protobuf_ios/lib/ios_arm7/lib/libprotobuf-lite.a /Users/lcavalie/iOSdev/tensorflow/tensorflow/contrib/makefile/gen/protobuf_ios/lib/ios_arm7s/lib/libprotobuf-lite.a /Users/lcavalie/iOSdev/tensorflow/tensorflow/contrib/makefile/gen/protobuf_ios/lib/ios_arm64/lib/libprotobuf-lite.a -create -output /Users/lcavalie/iOSdev/tensorflow/tensorflow/contrib/makefile/gen/protobuf_ios/lib/libprotobuf-lite.a\n  fatal error: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: can't open input file: /Users/lcavalie/iOSdev/tensorflow/tensorflow/contrib/makefile/gen/protobuf_ios/lib/iossim_386/lib/libprotobuf-lite.a (No such file or directory)\n### What have you tried?\n\nI looked into tensorflow/contrib/makefile/gen/ it contains only a protobuf-host folder that is empty and a protobuf_ios folder with the various architectures and on each case only 2 files /lib/ios_arm7/lib/pkgconfig/protobuf-lite.pc and  /lib/ios_arm7/lib/pkgconfig/protobuf.pc \n\nAssuming that there is a problem with the building of everything related to protobuf I tried to run compile_ios_protobuf.sh it resulted in the exact same error.\n### Logs or other output that would be helpful\n\nI can upload the full log if needed\n", "comments": ["Sorry you're hitting problems! I checked in a related fix to top-of-tree in #3075 a couple of hours ago. Can you give that a try and let me know if that fixes your issue?\n", "The inclusion of _make_host_protoc_ in the latest changes fixed this problem for me. \n", "Hi pete\nI can confirm that #3075 fixed the problem for me. The makefile can actually go through now.\nI think this issue can be close. Thanks for your help. (big fan of your blog BTW)\n"]}]