[{"number": 16808, "title": "Fix error message in record_reader", "body": "Corrected error message logged if unsupported compression type.", "comments": ["For XLA Internal CI build, there were 42 reported failures with 371 tests successful.  I navigated to [XLA details](https://source.cloud.google.com/results/invocations/ba938685-281a-447b-a398-b26d0ba193bf/artifacts) but was unable to download test.log or test.xml corresponding to failed test.", "The failure doesn't seem like it could be related. Rerunning test", "Thank you for merging and re-running tests."]}, {"number": 16807, "title": "TF 1.3 unable to create Session in the first time", "body": "I have install TF 1.3 GPU using anaconda. It is failed to create session as run TF in script file.\r\n\r\nAs run TF interactively or using spyder, same error messages were shown and it fail to create session in the first time. However, it able to create session if run \"sess = tf.Session()\" again. The TF will run either by input line by line script or using \"exec(compile(open(filename, \"rb\").read(), filename, 'exec'))\" \r\n\r\nThe error message as create session:\r\n\r\n$ source activiate tf13py36\r\n(tf13py36)$ python\r\n>>> import tensorflow as tf\r\n>>> sess = tf.Session()\r\n2018-02-05 17:44:25.343373: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-02-05 17:44:25.343398: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-02-05 17:44:25.540883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-02-05 17:44:25.541399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\r\nname: GeForce GTX 980 Ti\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.228\r\npciBusID 0000:03:00.0\r\nTotal memory: 5.94GiB\r\nFree memory: 5.83GiB\r\n2018-02-05 17:44:25.600695: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x225e110 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.\r\n2018-02-05 17:44:25.600988: E tensorflow/core/common_runtime/direct_session.cc:171] Internal: failed initializing StreamExecutor for CUDA device ordinal 1: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_DEVICE\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/tchen/anaconda3/envs/tf13py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1486, in __init__\r\n    super(Session, self).__init__(target, graph, config=config)\r\n  File \"/home/tchen/anaconda3/envs/tf13py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 621, in __init__\r\n    self._session = tf_session.TF_NewDeprecatedSession(opts, status)\r\n  File \"/home/tchen/anaconda3/envs/tf13py36/lib/python3.6/contextlib.py\", line 89, in __exit__\r\n    next(self.gen)\r\n  File \"/home/tchen/anaconda3/envs/tf13py36/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InternalError: Failed to create session.\r\n>>> sess = tf.Session()\r\n2018-02-05 17:45:19.371509: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x225e110 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.\r\n2018-02-05 17:45:19.371738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:03:00.0)\r\n2018-02-05 17:45:19.430913: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x225e110 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.\r\n>>> x1 = tf.constant([1,2,3,4])\r\n>>> x2 = tf.constant([5,6,7,8])\r\n>>> result = tf.multiply(x1, x2)\r\n>>> print(sess.run(result))\r\n[ 5 12 21 32]\r\n\r\nSYSTEM Infomation\r\nubuntu16.04\r\ncuda V8.061\r\ncudnn 6021\r\ngtx1060\r\ntensorflow1.3.0\r\npython3.6.1\r\nmemory 30G ,used 5.5GB\r\n\r\n$ nvidia-smi\r\nTue Feb  6 10:21:38 2018\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.111                Driver Version: 384.111                   |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 980 Ti  Off  | 00000000:03:00.0 Off |                  N/A |\r\n| 22%   31C    P8    19W / 250W |    110MiB /  6078MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Quadro 600          Off  | 00000000:04:00.0  On |                  N/A |\r\n| 36%   53C    P0    N/A /  N/A |    524MiB /   959MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0     19564      C   python                                        98MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\nI am grateful to anyone for helping me\r\nThank you very much!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Problem solved.\r\nAdd the CUDA_VISIBLE_DEVICES in the python file:\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\r\n\r\nThanks for the help.\r\n"]}, {"number": 16806, "title": "Update CONTRIBUTING.md", "body": "Edited a few grammar issues.", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!\n\n> On Feb 6, 2018, at 11:56 AM, googlebot <notifications@github.com> wrote:\n> \n> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n> \n> \ud83d\udcdd Please visit https://cla.developers.google.com/ to sign.\n> \n> Once you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.\n> \n> If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n> If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n> In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 16805, "title": "Feature Request: max_norm argument added to tf.nn.nce_loss ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n\r\n### Describe the problem\r\nI would like to request an additional argument to added to the tf.nn.nce_loss function. This function calls the tf.nn.embedding_lookup in order to build the matrix of negative sample embeddings. tf.nn.embedding_lookup has a max_norm argument, and I would like for the value of the max_norm argument supplied to tf.nn.nce_loss to be propagated to the tf.nn.embedding_lookup call.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Assigning to @xiejw, since he's touched some of this code, and may have a better idea than me.", "Re-assign to @ispirmustafa to find a proper owner. ", "Hi @dshieble,\r\nHave you tried max norm with nce_loss in your application?\r\nthanks", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 30 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to inactivity."]}, {"number": 16804, "title": "Add optimized gif support for decode_gif", "body": "While revisiting the issue of #15838, I noticed that currently optimized gif is not supported. However,\r\noptimized gif is actually possible to be processed as essentially the subsequent frame just adds the content on top of the previous frame on canvas.\r\n\r\nThis fix adds the support for optimized gif with decode_gif.\r\n\r\nAs is shown in the added test case, optimized gif (`optimized.gif`) could be handled the same way as original gif (`scan.gif`).\r\n\r\nThis fix fixes #15838.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 16803, "title": "transform_graph generates faulty model after optimization (quantisation).", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n1.4.1\r\n- **Bazel version (if compiling from source)**:\r\n0.8.1\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.6)\r\n\r\n### Describe the problem\r\nI am using a pre-trained style transfer model that I have converted  to .pb format. When I inference a single image through the fp32 model then I can see the styled image at the output of the network and everything works fine. Later on I optimized the model for inference using `transform_graph` tool with these parameters : \r\n\r\n```\r\n--transforms='\r\n  add_default_attributes\r\n  strip_unused_nodes(type=float)\r\n  remove_nodes(op=CheckNumerics)\r\n  fold_constants(ignore_errors=true)\r\n  fold_batch_norms\r\n  fold_old_batch_norms\r\n  quantize_weights\r\n  quantize_nodes\r\n  strip_unused_nodes\r\n  sort_by_execution_order'\r\n```\r\nthe quantised models (.pb) generated successfully but now I have problem with inference when I execute sess.run().\r\n\r\n### Source code / logs\r\n```\r\nTraceback (most recent call last):\r\n  File \"Inf_Image_pb.py\", line 89, in <module>\r\n    Session_out = sess.run(l_output, feed_dict={l_input: image})            \r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Shapes of all inputs must match: values[0].shape = [474,712,3] != values[2].shape = []\r\n\t [[Node: Reshape/shape = Pack[N=3, T=DT_INT32, axis=0, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_input_image_0_0, _arg_input_image_0_0, Reshape/shape/2)]]\r\n\r\nCaused by op u'Reshape/shape', defined at:\r\n  File \"Inf_Image_pb.py\", line 74, in <module>\r\n    producer_op_list=None\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 313, in import_graph_def\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): Shapes of all inputs must match: values[0].shape = [474,712,3] != values[2].shape = []\r\n\t [[Node: Reshape/shape = Pack[N=3, T=DT_INT32, axis=0, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_input_image_0_0, _arg_input_image_0_0, Reshape/shape/2)]]\r\n```\r\nI have tried other tensorflow versions, other bazel versions to rebuild the tool and the legacy quantisation tool 'quantize_graph' with mode --mode=eightbit but still fail to inference. In addition I tried other transform combinations using the `transform_graph` tool, other batch sizes and other input dimensions but still getting error. What is this error refers to ? I can't find any useful information online ..\r\n", "comments": ["This is the part that I am getting the error : \r\n\r\n![image](https://user-images.githubusercontent.com/3832904/35868951-dfd9674e-0b55-11e8-87e8-3a92a8931a18.png)\r\n", "@suharshs can you comment or redirect? Thanks!", "cc @petewarden \r\n\r\ntransform_graph has had some hard to debug issue for various models in the past. From your error, it seems that some transformation is screwing up shape information in the graph, but i am really not sure. \r\n\r\nWe have focused our 8-bit quantization efforts for mobile models on TensorFlow Lite : https://www.tensorflow.org/mobile/tflite/. Currently TFLite quantizations requires quantizing both weights and activations, though this may change.\r\n\r\nWe use fake quantized training (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantize) to get models that are compatible with TensorFlow Lite's converter, TOCO. TOCO can then convert these models to fully quantized models. We find that for many models, quantizing with training in the loop is necessary for good accuracy.\r\nWe are working on making these rewriters more general over various models, so expect improvements in this department.\r\n\r\nHere are some examples for how we train mobilenet_v1 for quantization : https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md\r\n\r\nThe graph resulting from https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1_eval.py is compatible with TensorFlow Lite.\r\n\r\nHope that helps!\r\n\r\nWhat model architecture are you trying to quantize? Are you ok with quantizing both weights and activations? ", "Hi @suharshs,\r\n\r\nThanks for the information. I am currently not interested on gaining back the quatnization error through re-training but only to manage to inference through the quantized graph. The model is a style transfer model and I want to quantize both weights and activations as I am showing above.", "Nagging Assignee @suharshs: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @suharshs: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @suharshs: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @suharshs: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @suharshs: It has been 78 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @suharshs: It has been 94 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @suharshs: It has been 110 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hello,\r\n\r\nI think that the activity is expected by you ...\r\n\r\nThanks.\r\n", "Hi @kmonachopoulos ,\r\n\r\nWe are working on putting together weight and activation quantization for TensorFlow Lite (that doesn't require training), will keep you posted once that is ready.\r\n\r\nIn the meantime can you inspect the inputs of the culprit \"Reshape/shape\" (looks like it is a Pack node) node before and after you call the transformation to see what exactly is changing. This may give you some hints as to what is going wrong with the shapes.", "Nagging Assignee @suharshs: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @suharshs: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n\r\nWe don't have any current plans for quantized support for transform tool."]}, {"number": 16802, "title": "[tflite] make calling NNAPI work again (resend)", "body": "for the previous one (https://github.com/tensorflow/tensorflow/pull/16256) somehow reverted/overwritten\r\n\r\ncalling PrepareOpsAndTensors() before using NN API looks\r\n  1. unnecessary\r\n  2. decrease next_execution_plan_index_to_prepare_ so that\r\n     the logic check in the next line\r\n     `next_execution_plan_index_to_prepare_ == execution_plan_.size`\r\n     will fail", "comments": ["it seems https://github.com/tensorflow/tensorflow/pull/16256 will be brought back in https://github.com/tensorflow/tensorflow/pull/16858. Will close this when https://github.com/tensorflow/tensorflow/pull/16858 is merged.", "close this since #16858 is merged"]}, {"number": 16801, "title": "Tensorflow installer assumes that the user uses CUDA 9.0 while CUDA 9.1 is out already.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["The decision to stay with cuda 9.0 was recommended to us by NVIDIA, as the new drivers to support cuda 9.1 is not available everywhere at the moment.\r\nTo run TF with cuda 9.1, you can build from sources, and it will definitely work.\r\n@tfboyd can comment more."]}, {"number": 16800, "title": "How to compile and use Opencv in tensorflow c++?", "body": "I want to implement a model inference in tensorflow c++ and have saved the model as .pb file. But I can't use opencv to process the image. I wonder how can I add the opencv lib to the bazel project? Are there any tricks to solve the problem? Thanks!\r\n\r\nThis is the code of the bazel BUILD file:\r\n\r\n    package(\r\n        default_visibility = [\"//tensorflow:internal\"],\r\n    )\r\n\r\n    licenses([\"notice\"])  # Apache 2.0\r\n\r\n    exports_files([\"LICENSE\"])\r\n\r\n    load(\"//tensorflow:tensorflow.bzl\", \"tf_cc_binary\")\r\n\r\n    tf_cc_binary(\r\n        name = \"mask_rcnn\",\r\n        srcs = [\r\n            \"main.cc\",\r\n        ],\r\n        #prefix = \"flower\",\r\n        linkopts = select({\r\n            \"//tensorflow:android\": [\r\n                \"-pie\",\r\n                \"-landroid\",\r\n                \"-ljnigraphics\",\r\n                \"-llog\",\r\n                \"-lm\",\r\n                \"-z defs\",\r\n                \"-s\",\r\n                \"-Wl,--exclude-libs,ALL\",\r\n            ],\r\n            \"//conditions:default\": [\"-lm\"],\r\n        }),\r\n        deps = select({\r\n            \"//tensorflow:android\": [\r\n                # cc:cc_ops is used to include image ops (for label_image)\r\n                # Jpg, gif, and png related code won't be included\r\n                \"//tensorflow/cc:cc_ops\",\r\n                \"//tensorflow/core:android_tensorflow_lib\",\r\n                # cc:android_tensorflow_image_op is for including jpeg/gif/png\r\n                # decoder to enable real-image evaluation on Android\r\n                \"//tensorflow/core/kernels:android_tensorflow_image_op\",\r\n            ],\r\n            \"//conditions:default\": [\r\n                \"//tensorflow/cc:cc_ops\",\r\n                \"//tensorflow/core:core_cpu\",\r\n                \"//tensorflow/core:framework\",\r\n                \"//tensorflow/core:framework_internal\",\r\n                \"//tensorflow/core:lib\",\r\n                \"//tensorflow/core:protos_all_cc\",\r\n                \"//tensorflow/core:tensorflow\",\r\n            ],\r\n        }),\r\n    )\r\n\r\n    filegroup(\r\n        name = \"all_files\",\r\n        srcs = glob(\r\n            [\"**/*\"],\r\n            exclude = [\r\n                \"**/METADATA\",\r\n                \"**/OWNERS\",\r\n                \"bin/**\",\r\n                \"gen/**\",\r\n            ],\r\n        ),\r\n        visibility = [\"//tensorflow:__subpackages__\"],\r\n    )\r\n\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "hello, are you have solved this problem ?   why are you  want use opencv process image not ByteBuffer ?"]}, {"number": 16799, "title": "Debug prompts use_default_colors() returned ERR", "body": "I was running a example code from [tensorpack](http://tensorpack.readthedocs.io/en/latest/index.html) on Pycharm, which runs properly, then I changed the session to \r\n`sess = tf_debug.LocalCLIDebugWrapperSession(sess) ` in order to debug\r\nas suggested by official example https://www.tensorflow.org/programmers_guide/debugger, but then I got:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/user/.pycharm_helpers/pydev/pydev_run_in_console.py\", line 53, in run_file\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"/home/user/prj/shufflenet_v1/shufflenet.py\", line 231, in <module>\r\n    launch_train_with_config(config, SyncMultiGPUTrainerParameterServer(nr_tower))\r\n  File \"/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorpack/train/interface.py\", line 96, in launch_train_with_config\r\n    config.steps_per_epoch, config.starting_epoch, config.max_epoch)\r\n  File \"/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorpack/train/base.py\", line 288, in train\r\n    self.main_loop(steps_per_epoch, starting_epoch, max_epoch)\r\n  File \"/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorpack/utils/argtools.py\", line 171, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorpack/train/base.py\", line 239, in main_loop\r\n    self.loop.update_global_step()\r\n  File \"/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorpack/train/base.py\", line 59, in update_global_step\r\n    self._global_step = get_global_step_value()\r\n  File \"/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorpack/tfutils/common.py\", line 77, in get_global_step_value\r\n    get_global_step_var())\r\n  File \"/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/training_util.py\", line 67, in global_step\r\n    return int(sess.run(global_step_tensor))\r\n  File \"/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/framework.py\", line 455, in run\r\n    is_callable_runner=bool(callable_runner)))\r\n  File \"/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py\", line 253, in on_run_start\r\n    self._prep_cli_for_run_start()\r\n  File \"/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py\", line 275, in _prep_cli_for_run_start\r\n    self._run_cli = ui_factory.get_ui(self._ui_type)\r\n  File \"/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/debug/cli/ui_factory.py\", line 56, in get_ui\r\n    return curses_ui.CursesUI(on_ui_exit=on_ui_exit)\r\n  File \"/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/debug/cli/curses_ui.py\", line 285, in __init__\r\n    self._screen_init()\r\n  File \"/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/debug/cli/curses_ui.py\", line 400, in _screen_init\r\n    self._screen_color_init()\r\n  File \"/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/debug/cli/curses_ui.py\", line 405, in _screen_color_init\r\n    curses.use_default_colors()\r\nerror: use_default_colors() returned ERR\r\n```\r\nwhich I have no clue for the reason, can someone help?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I run `tfdbg` in `pycharm`, switch to terminal solve the problem", "Thanks for sharing your solution! cc/ @caisq "]}, {"number": 16798, "title": "tf.nn.conv2d on GPU with data_format='NHWC' gives corrupted result for specific shapes", "body": "Basically what the title says.\r\n\r\nFor an image of size (1096, 2449) EXACTLY, not 1097 or 1095 or 2448 or 2450 (but 2451 for some reason produces the same effect). The bottom part of the convolution result gets corrupted. It only affects convolution done on the GPU with the 'NHWC' data format.\r\n\r\nHonestly, it feels more of a cuDNN bug than anything, but not sure where to post it otherwise.\r\n\r\n**Important note : I am on Ubuntu 14.04 hence I can not try tensorflow 1.5 which needs CUDA 9 which needs 16.04. So my test is done on 1.4 with cuda8 and cuDNNv6.**\r\n\r\n------------------------\r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: pip install 1.4 tensorflow-gpu\r\n- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1\r\n- **Python version**: 3.5\r\n- **CUDA/cuDNN version**: cuda 8 cudnn 6\r\n- **GPU model and memory**: TITAN X Pascal\r\n- **Exact command to reproduce**:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import layers\r\nsess=tf.InteractiveSession()\r\nh, w = 1096, 2449\r\ninput_img = tf.placeholder(tf.float32, shape=(None, h, w, 3))\r\nfilters = np.random.randn(1,1,3,2)\r\nwith tf.device('/cpu:0'):\r\n    out_cpu = tf.nn.conv2d(input_img, filter=filters, strides=(1,1,1,1), padding='VALID')\r\nwith tf.device('/gpu:0'):\r\n    out_gpu = tf.nn.conv2d(input_img, filter=filters, strides=(1,1,1,1), padding='VALID')\r\n    # The following actually works if you manually transpose to use the NCHW data_format\r\n    # tmp = tf.transpose(input_img, (0,3,1,2))\r\n    # out_gpu = tf.nn.conv2d(tmp, filter=filters, strides=(1,1,1,1), padding='VALID', data_format='NCHW')\r\n    # out_gpu = tf.transpose(out_gpu, (0,2,3,1))\r\n\r\nout1, out2 = sess.run((out_cpu, out_gpu), feed_dict={\r\n    input_img: np.random.randn(1, h, w, 3)\r\n})\r\nplt.imshow(np.linalg.norm(out1-out2 , axis=-1)[0])\r\nplt.colorbar()\r\n```\r\nPlotting the difference between the CPU conv and the GPU conv :\r\n\r\n<img width=\"359\" alt=\"capture d ecran 2018-02-06 a 12 31 03\" src=\"https://user-images.githubusercontent.com/7132817/35857501-a74bb66e-0b39-11e8-9edd-98e69eba5c48.png\">\r\n\r\n", "comments": ["/CC @chsigg is this a known issue?", "I'd have to doublecheck again, but I think with the current versions of cuda and tf, I was not able to reproduce it.", "OK in that case I'll close the issue. If you determine this is still an issue, I'll reopen.", "Quick update, I actually tried again with Ubuntu 18.04, Tensorflow 1.7, Cuda 9.0, cuDNN 7.1.2 (all installed with `conda install tensorflow-gpu`) and the problem is actually still present.\r\n\r\n", "To be more precise:\r\n\r\n- If I install with `conda install tensorflow-gpu=1.8.0`, conda cares of installing cudatoolkit=9.0 and cudnn=7.1.2. In that case, the bug seems to persist.\r\n\r\n- If I have cuda 9.0 and cudnn 7 installed at the system level, and run `pip install tensorflow-gpu` to get 1.8.0, things work as they should. Same if I try the relevant code snippet in a collab notebook with GPU.\r\n\r\nLong story short, I have no clue what is happening.", "/CC @chsigg can you look into this.", "Nagging Assignee @chsigg: It has been 79 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @chsigg: It has been 94 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi Seguin, I'm very sorry about the slow response time.\r\nI agree this looks like a cuDNN bug, and should have been fixed in cuDNN 7.1.3."]}, {"number": 16797, "title": "Fixes variable name", "body": "", "comments": ["@jhseu  "]}, {"number": 16796, "title": "Slim batch image prediciton?", "body": "I just finished training a model by following train_image_classifier.py\r\n\r\nCUDA_VISIBLE_DEVICE=0,1 python train_image_classifier.py --train_dir=train_logs --dataset_dir=../train --num_samples=15500 --num_classes=4 --labels_to_names_path=../labels.txt --model_name=inception_resnet_v2 --checkpoint_path=../checkpoints/inception_resnet_v2_2016_08_30.ckpt --checkpoint_exclude_scopes=InceptionResnetV2/Logits,InceptionResnetV2/AuxLogits --num_clones=2 --num_preprocessing_threads=8 --max_number_of_steps=100000 --batch_size=32 --learning_rate=0.0001 --learning_rate_decay_type=fixed --save_interval_secs=60 --save_summaries_secs=60 --log_every_n_steps=10 --optimizer=rmsprop --weight_decay=0.00004\r\n\r\nand evaluate the model by using eval_image_classifier.py. \r\n\r\nCUDA_VISIBLE_DEVICE=0,1 python eval_image_classifier.py --checkpoint_path=train_logs --eval_dir=eval_logs --dataset_dir=../val --num_samples=797 --num_classes=4 --model_name=inception_resnet_v2\r\n\r\nEverything seems great. But, how to test image classifier unfortunately is not provided, and I cannot really find an good example of how to use this model to predict testing images. Is there a good example that I can use my trained model to predict multiple images in a batch way? Thank you for helping. \r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 16795, "title": "Android C++ API on arm64-v8a", "body": "Hello,\r\n\r\nI try to compile tensorflow for android arm64-v8a. I found lots of issues on similar problem but no answer work for me. Here is the full description of my commands:\r\n\r\nInformations of my system:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.9\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: Not used\r\n- **GPU model and memory**: Not used\r\n- **Have I written custom code** : No\r\n- **Exact command to reproduce** : `bazel build --cxxopt=--std=c++11  -c opt //tensorflow:libtensorflow_cc.so    --verbose_failures    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=arm64-v8a\r\n`\r\n\r\n\r\n```\r\nERROR: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/kernels/BUILD:4276:1: C++ compilation of rule '//tensorflow/core/kernels:random_poisson_op' failed (Exit 1): clang failed: error executing command \r\n  (cd /home/xavier/.cache/bazel/_bazel_xavier/ef54af8645dec4f38c438d8e1c779747/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/opt/ros/kinetic/lib:/usr/local/lib: \\\r\n    PATH=/opt/ros/kinetic/bin:/home/xavier/Android/Sdk/ndk-bundle/toolchains/aarch64-linux-android-4.9/prebuilt/linux-x86_64/aarch64-linux-android/bin/ld:/home/xavier/bin:/home/xavier/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/lib/jvm/java-8-oracle/bin:/usr/lib/jvm/java-8-oracle/db/bin:/usr/lib/jvm/java-8-oracle/jre/bin:/home/xavier/Android/Sdk/platform-tools:/home/xavier/android-ndk-r14b:/home/xavier/Bureau/developpement/androidscreencast/bin:/home/xavier/Bureau/developpement/androidscreencast:/home/xavier/Bureau/developpement/gerrit_tools:/home/xavier/android-studio/bin:/home/xavier/Bureau/developpement:/home/xavier/dev/common.tools/bin:/home/xavier/dev/gcc-arm-none-eabi-6-2017-q2-update/bin:/home/xavier/dev/dump_Cyril:/home/xavier/opt/gnu-mcu-eclipse/openocd/0.10.0-3-20170826-1813-dev/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n  external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang -gcc-toolchain external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/linux-x86_64 -target aarch64-none-linux-android -ffunction-sections -funwind-tables -fstack-protector-strong -fpic -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -no-canonical-prefixes -O2 -g -DNDEBUG -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK '--std=c++11' -MD -MF bazel-out/arm64-v8a-opt/bin/tensorflow/core/kernels/_objs/random_poisson_op/tensorflow/core/kernels/random_poisson_op.d '-frandom-seed=bazel-out/arm64-v8a-opt/bin/tensorflow/core/kernels/_objs/random_poisson_op/tensorflow/core/kernels/random_poisson_op.o' -DEIGEN_MPL2_ONLY '-DS_IREAD=S_IRUSR' '-DS_IWRITE=S_IWUSR' '-DS_IEXEC=S_IXUSR' -DTF_USE_SNAPPY -iquote . -iquote bazel-out/arm64-v8a-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/arm64-v8a-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/arm64-v8a-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/arm64-v8a-opt/genfiles/external/local_config_sycl -iquote external/nsync -iquote bazel-out/arm64-v8a-opt/genfiles/external/nsync -iquote external/gif_archive -iquote bazel-out/arm64-v8a-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/arm64-v8a-opt/genfiles/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/arm64-v8a-opt/genfiles/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/arm64-v8a-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/arm64-v8a-opt/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/arm64-v8a-opt/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/arm64-v8a-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/arm64-v8a-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/arm64-v8a-opt/genfiles/external/zlib_archive -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/arm64-v8a-opt/genfiles/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/arm64-v8a-opt/genfiles/external/nsync/public -isystem external/gif_archive/lib -isystem bazel-out/arm64-v8a-opt/genfiles/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/arm64-v8a-opt/genfiles/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/arm64-v8a-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/arm64-v8a-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/arm64-v8a-opt/genfiles/external/zlib_archive -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-std=c++11' -DTF_LEAN_BINARY -Wno-narrowing -fomit-frame-pointer -O2 '--sysroot=external/androidndk/ndk/platforms/android-24/arch-arm64' -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/arm64-v8a/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward -c tensorflow/core/kernels/random_poisson_op.cc -o bazel-out/arm64-v8a-opt/bin/tensorflow/core/kernels/_objs/random_poisson_op/tensorflow/core/kernels/random_poisson_op.o)\r\nIn file included from tensorflow/core/kernels/random_poisson_op.cc:27:\r\nIn file included from ./tensorflow/core/framework/op_kernel.h:23:\r\nIn file included from ./tensorflow/core/framework/allocator.h:23:\r\nIn file included from ./tensorflow/core/framework/numeric_types.h:21:\r\nIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:31:\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/../SpecialFunctions:46:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:108:5: error: static_assert failed \"THIS_TYPE_IS_NOT_SUPPORTED\"\r\n    EIGEN_STATIC_ASSERT((internal::is_same<Scalar, Scalar>::value == false),\r\n    ^                   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/StaticAssert.h:32:40: note: expanded from macro 'EIGEN_STATIC_ASSERT'\r\n    #define EIGEN_STATIC_ASSERT(X,MSG) static_assert(X,#MSG);\r\n                                       ^             ~\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1509:47: note: in instantiation of member function 'Eigen::internal::lgamma_impl<double>::run' requested here\r\n  return EIGEN_MATHFUNC_IMPL(lgamma, Scalar)::run(x);\r\n                                              ^\r\ntensorflow/core/kernels/random_poisson_op.cc:234:58: note: in instantiation of function template specialization 'Eigen::numext::lgamma<double>' requested here\r\n            CT t = -rate + k * log_rate - Eigen::numext::lgamma(k + 1);\r\n                                                         ^\r\ntensorflow/core/kernels/random_poisson_op.cc:308:5: note: in instantiation of member function 'tensorflow::functor::PoissonFunctor<Eigen::ThreadPoolDevice, float, float>::operator()' requested here\r\n    functor::PoissonFunctor<CPUDevice, T, U>()(\r\n    ^\r\ntensorflow/core/kernels/random_poisson_op.cc:284:12: note: in instantiation of member function 'tensorflow::(anonymous namespace)::RandomPoissonOp<float, float>::Compute' requested here\r\n  explicit RandomPoissonOp(OpKernelConstruction* context) : OpKernel(context) {\r\n           ^\r\ntensorflow/core/kernels/random_poisson_op.cc:328:15: note: in instantiation of member function 'tensorflow::(anonymous namespace)::RandomPoissonOp<float, float>::RandomPoissonOp' requested here\r\nTF_CALL_float(REGISTER);\r\n              ^\r\n1 error generated.\r\nTarget //tensorflow:libtensorflow_cc.so failed to build\r\nINFO: Elapsed time: 1712.148s, Critical Path: 61.59s\r\nFAILED: Build did NOT complete successfully\r\n\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nExact command to reproduce", "I modify my first post. Is \"libtensorflow_cc.so\" well the project to build the C++ API of tensorflow on Android?", "You likely want to be running\r\n```\r\nbazel build -c dbg --verbose_failures //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a\r\n```", "Hello d4l3k, So building libtensorflow_inference.so works properly in armeabi-v7a works.\r\nBut the problem is I try to build tensorflow:libtensorflow_cc.so in arm64-v8a.\r\nTo summurize:\r\n\r\ntensorflow:libtensorflow_cc.so armeabi-v7: OK arm64-v8a: NOK\r\nlibtensorflow_inference.so armeabi-v7: OK arm64-v8a: NOK\r\n\r\n", "@asimshankar do you own android builds?", "What do you means by \" own Android builds\"?  Why should I have Android builds whereas un armeabiv7 I sont need it.\r\nWhat is the difference between tensorflow_cc and tensorfllw_inference?", "The `//tensorflow:libtensorflow_cc.so` bazel target includes dependencies that may not be Android friendly. As a general rule, not all build targets in the TensorFlow repository are compatible with Android and we will rely on the community to figure out how to build any target of interest if needed.\r\n\r\n`libtensorflow_inference.so` is specifically designed to be a version of the TensorFlow runtime suitable for Android and that exposes the [TensorFlow C API](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.h)\r\n\r\nIt should be possible to build that for arm64-v8a.\r\n@petewarden may have more details.", "Is there a cpp version of libtensorflow_inference ? Or what I had to  to that to use it on cpp on Android?\r\nIn that example:\r\nhttps://github.com/miyosuda/TensorFlowAndroidDemo/tree/master/jni-build/jni\r\n\r\nit also use the libtensorflow_inference but also LOCAL_PATH)/libs/$(TARGET_ARCH_ABI)/libandroid_tensorflow_lib.lo \\\r\n\t$(LOCAL_PATH)/libs/$(TARGET_ARCH_ABI)/libandroid_tensorflow_kernels.lo \\\r\n\t$(LOCAL_PATH)/libs/$(TARGET_ARCH_ABI)/libandroid_tensorflow_lib_lite.lo \\\r\n\t$(LOCAL_PATH)/libs/$(TARGET_ARCH_ABI)/libprotos_all_cc.a \\\r\n\t$(LOCAL_PATH)/libs/$(TARGET_ARCH_ABI)/libprotobuf.a \\\r\n\t$(LOCAL_PATH)/libs/$(TARGET_ARCH_ABI)/libprotobuf_lite.a \\\r\nThis files seems to be generate with libtensorflow_cc.   \r\n\r\nI would like to do the same un arm64-v8a but the there are errors.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I still have no solution to build Tensorflow for Cpp on Android ...\r\n", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 59 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I have a dockerfile builds tensorflow (not jni dependent) that uses the standalone ndk for arm64 with the changes needed to get all the way to the linking that I can share, but it fails due to gcc always choking on pthread, even though the -lpthread isn't being explicitly passed anywhere I can see, anyone know where it gets it? (initially I posted here about it https://github.com/snipsco/tensorflow-build/issues/24)\r\n\r\n```\r\nSUBCOMMAND: # //tensorflow:libtensorflow_framework.so [action 'Linking tensorflow/libtensorflow_framework.so']\r\n(cd /root/.cache/bazel/_bazel_root/b46121d0546718834d76554a8a3bea26/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n  /android-arm/bin/aarch64-linux-android-gcc -shared -o bazel-out/armeabi-opt/bin/tensorflow/libtensorflow_framework.so '-Wl,-rpath,$ORIGIN/' -Wl,-soname,libtensorflow_framework.so -pthread -Wl,-z,relro,-z,now -Wl,--gc-sections -v -L/android-arm/sysroot/usr/lib -L/android-arm/lib -L/android-arm/lib64 -L/android-arm/aarch64-linux-android/lib -L/android-arm/aarch64-linux-android/lib64 -Wl,@bazel-out/armeabi-opt/bin/tensorflow/libtensorflow_framework.so-2.params)\r\nERROR: /tmp/tensorflow/arm/tensorflow/BUILD:744:1: Linking of rule '//tensorflow:libtensorflow_framework.so' failed (Exit 1): aarch64-linux-android-gcc failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/b46121d0546718834d76554a8a3bea26/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n  /android-arm/bin/aarch64-linux-android-gcc -shared -o bazel-out/armeabi-opt/bin/tensorflow/libtensorflow_framework.so '-Wl,-rpath,$ORIGIN/' -Wl,-soname,libtensorflow_framework.so -pthread -Wl,-z,relro,-z,now -Wl,--gc-sections -v -L/android-arm/sysroot/usr/lib -L/android-arm/lib -L/android-arm/lib64 -L/android-arm/aarch64-linux-android/lib -L/android-arm/aarch64-linux-android/lib64 -Wl,@bazel-out/armeabi-opt/bin/tensorflow/libtensorflow_framework.so-2.params)\r\nUsing built-in specs.\r\nCOLLECT_GCC=/android-arm/bin/aarch64-linux-android-gcc\r\nCOLLECT_LTO_WRAPPER=/android-arm/bin/../libexec/gcc/aarch64-linux-android/4.9.x/lto-wrapper\r\nTarget: aarch64-linux-android\r\nConfigured with: /usr/local/google/buildbot/src/android/gcc/toolchain/build/../gcc/gcc-4.9/configure --prefix=/tmp/ec4cb553ef61611c1d388a938c56382a --target=aarch64-linux-android --host=x86_64-linux-gnu --build=x86_64-linux-gnu --with-gnu-as --with-gnu-ld --enable-languages=c,c++ --with-gmp=/buildbot/tmp/build/toolchain/temp-install --with-mpfr=/buildbot/tmp/build/toolchain/temp-install --with-mpc=/buildbot/tmp/build/toolchain/temp-install --with-cloog=/buildbot/tmp/build/toolchain/temp-install --with-isl=/buildbot/tmp/build/toolchain/temp-install --with-ppl=/buildbot/tmp/build/toolchain/temp-install --disable-ppl-version-check --disable-cloog-version-check --disable-isl-version-check --enable-cloog-backend=isl --with-host-libstdcxx='-static-libgcc -Wl,-Bstatic,-lstdc++,-Bdynamic -lm' --disable-libssp --enable-threads --disable-nls --disable-libmudflap --disable-libgomp --disable-libstdc__-v3 --disable-sjlj-exceptions --disable-shared --disable-tls --disable-libitm --enable-bionic-libs --enable-libatomic-ifuncs=no --enable-initfini-array --disable-nls --prefix=/tmp/ec4cb553ef61611c1d388a938c56382a --with-sysroot=/tmp/ec4cb553ef61611c1d388a938c56382a/sysroot --with-binutils-version=2.27 --with-mpfr-version=3.1.1 --with-mpc-version=1.0.1 --with-gmp-version=5.0.5 --with-gcc-version=4.9 --with-gdb-version=none --with-gxx-include-dir=/tmp/ec4cb553ef61611c1d388a938c56382a/include/c++/4.9.x --with-bugurl=http://source.android.com/source/report-bugs.html --enable-languages=c,c++ --disable-bootstrap --enable-plugins --enable-libgomp --enable-gnu-indirect-function --disable-libsanitizer --enable-gold --enable-ld=default --enable-threads --enable-eh-frame-hdr-for-static --enable-fix-cortex-a53-835769 --enable-graphite=yes --with-isl-version=0.11.1 --with-cloog-version=0.18.0 --program-transform-name='s&^&aarch64-linux-android-&' --enable-gold\r\nThread model: posix\r\ngcc version 4.9.x 20150123 (prerelease) (GCC) \r\nCOMPILER_PATH=/android-arm/bin/../libexec/gcc/aarch64-linux-android/4.9.x/:/android-arm/bin/../libexec/gcc/:/android-arm/bin/../lib/gcc/aarch64-linux-android/4.9.x/../../../../aarch64-linux-android/bin/\r\nLIBRARY_PATH=/android-arm/bin/../lib/gcc/aarch64-linux-android/4.9.x/:/android-arm/bin/../lib/gcc/:/android-arm/bin/../lib/gcc/aarch64-linux-android/4.9.x/../../../../aarch64-linux-android/lib/../lib64/:/android-arm/bin/../lib/gcc/aarch64-linux-android/4.9.x/../../../../aarch64-linux-android/lib/:/android-arm/bin/../sysroot/usr/lib/\r\nCOLLECT_GCC_OPTIONS='-shared' '-o' 'bazel-out/armeabi-opt/bin/tensorflow/libtensorflow_framework.so' '-pthread' '-v' '-L/android-arm/sysroot/usr/lib' '-L/android-arm/lib' '-L/android-arm/lib64' '-L/android-arm/aarch64-linux-android/lib' '-L/android-arm/aarch64-linux-android/lib64' '-mlittle-endian' '-mabi=lp64'\r\n /android-arm/bin/../libexec/gcc/aarch64-linux-android/4.9.x/collect2 -plugin /android-arm/bin/../libexec/gcc/aarch64-linux-android/4.9.x/liblto_plugin.so -plugin-opt=/android-arm/bin/../libexec/gcc/aarch64-linux-android/4.9.x/lto-wrapper -plugin-opt=-fresolution=/tmp/ccklAaJ3.res -plugin-opt=-pass-through=-lgcc -plugin-opt=-pass-through=-lc -plugin-opt=-pass-through=-ldl -plugin-opt=-pass-through=-lgcc --sysroot=/android-arm/bin/../sysroot --eh-frame-hdr -shared -dynamic-linker /system/bin/linker64 -X -EL -maarch64linux --fix-cortex-a53-835769 --fix-cortex-a53-843419 -z noexecstack -z relro -z now -o bazel-out/armeabi-opt/bin/tensorflow/libtensorflow_framework.so /android-arm/bin/../sysroot/usr/lib/crtbegin_so.o -L/android-arm/sysroot/usr/lib -L/android-arm/lib -L/android-arm/lib64 -L/android-arm/aarch64-linux-android/lib -L/android-arm/aarch64-linux-android/lib64 -L/android-arm/bin/../lib/gcc/aarch64-linux-android/4.9.x -L/android-arm/bin/../lib/gcc -L/android-arm/bin/../lib/gcc/aarch64-linux-android/4.9.x/../../../../aarch64-linux-android/lib/../lib64 -L/android-arm/bin/../lib/gcc/aarch64-linux-android/4.9.x/../../../../aarch64-linux-android/lib -L/android-arm/bin/../sysroot/usr/lib -rpath $ORIGIN/ -soname libtensorflow_framework.so -z relro -z now --gc-sections @bazel-out/armeabi-opt/bin/tensorflow/libtensorflow_framework.so-2.params -lgcc -lc -ldl -lgcc /android-arm/bin/../sysroot/usr/lib/crtend_so.o\r\n/android-arm/bin/../lib/gcc/aarch64-linux-android/4.9.x/../../../../aarch64-linux-android/bin/ld: cannot find -lpthread\r\n/android-arm/bin/../lib/gcc/aarch64-linux-android/4.9.x/../../../../aarch64-linux-android/bin/ld: cannot find -lpthread\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow:libtensorflow.so failed to build\r\nINFO: Elapsed time: 123.493s, Critical Path: 15.35s\r\n```", "Turning off supports_param_files I get\r\n```\r\nSUBCOMMAND: # //tensorflow:libtensorflow_framework.so [action 'Linking tensorflow/libtensorflow_framework.so']\r\n(cd /root/.cache/bazel/_bazel_root/b46121d0546718834d76554a8a3bea26/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n  /android-arm/bin/aarch64-linux-android-gcc -shared -o bazel-out/armeabi-opt/bin/tensorflow/libtensorflow_framework.so -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/libframework_internal_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/libversion_lib.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/liblib_internal_impl.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/liblib_hash_crc32c_accelerate_internal.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/liblib_proto_parsing.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/snappy/libsnappy.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/protobuf_archive/libprotobuf.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/protobuf_archive/libprotobuf_lite.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/libprotos_all_cc_impl.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/grappler/costs/libop_performance_data_cc_impl.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/stream_executor/libstream_executor_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/libgpu_runtime_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/libcore_cpu_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/libgpu_id.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/libgpu_init_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/libgpu_lib.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/nsync/libnsync_cpp.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/kernels/liblookup_util.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/kernels/libinitializable_lookup_table.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/util/tensor_bundle/libtensor_bundle.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/util/tensor_bundle/libnaming.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/libcore_cpu_base.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/libgraph.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/grappler/libgrappler_item.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/grappler/libop_types.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/grappler/libutils.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/com_google_absl/absl/base/libbase.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/com_google_absl/absl/base/libspinlock_wait.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/com_google_absl/absl/base/libdynamic_annotations.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/gif_archive/libgif.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/jpeg/libjpeg.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/jpeg/libsimd_none.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/com_googlesource_code_re2/libre2.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/farmhash_archive/libfarmhash.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/fft2d/libfft2d.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/highwayhash/libsip_hash.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/highwayhash/libarch_specific.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/png_archive/libpng.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/zlib_archive/libzlib.pic.a -Wl,-no-whole-archive '-Wl,-rpath,$ORIGIN/' -Wl,-soname,libtensorflow_framework.so -ldl -lm -lpthread -lm -lpthread -lm -ldl -ldl -pthread -lm -lm -Wl,-z,relro,-z,now -Wl,--gc-sections -v -mandroid -L/android-arm/sysroot/usr/lib -L/android-arm/lib -L/android-arm/lib64 -L/android-arm/aarch64-linux-android/lib -L/android-arm/aarch64-linux-android/lib64)\r\nERROR: /tmp/tensorflow/arm/tensorflow/BUILD:744:1: Linking of rule '//tensorflow:libtensorflow_framework.so' failed (Exit 1): aarch64-linux-android-gcc failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/b46121d0546718834d76554a8a3bea26/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n  /android-arm/bin/aarch64-linux-android-gcc -shared -o bazel-out/armeabi-opt/bin/tensorflow/libtensorflow_framework.so -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/libframework_internal_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/libversion_lib.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/liblib_internal_impl.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/liblib_hash_crc32c_accelerate_internal.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/liblib_proto_parsing.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/snappy/libsnappy.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/protobuf_archive/libprotobuf.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/protobuf_archive/libprotobuf_lite.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/libprotos_all_cc_impl.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/grappler/costs/libop_performance_data_cc_impl.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/stream_executor/libstream_executor_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/libgpu_runtime_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/libcore_cpu_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/libgpu_id.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/libgpu_init_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/libgpu_lib.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/nsync/libnsync_cpp.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/kernels/liblookup_util.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/kernels/libinitializable_lookup_table.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/util/tensor_bundle/libtensor_bundle.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/util/tensor_bundle/libnaming.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/libcore_cpu_base.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/libgraph.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/grappler/libgrappler_item.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/grappler/libop_types.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/grappler/libutils.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/com_google_absl/absl/base/libbase.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/com_google_absl/absl/base/libspinlock_wait.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/com_google_absl/absl/base/libdynamic_annotations.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/gif_archive/libgif.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/jpeg/libjpeg.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/jpeg/libsimd_none.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/com_googlesource_code_re2/libre2.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/farmhash_archive/libfarmhash.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/fft2d/libfft2d.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/highwayhash/libsip_hash.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/highwayhash/libarch_specific.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/png_archive/libpng.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-opt/bin/external/zlib_archive/libzlib.pic.a -Wl,-no-whole-archive '-Wl,-rpath,$ORIGIN/' -Wl,-soname,libtensorflow_framework.so -ldl -lm -lpthread -lm -lpthread -lm -ldl -ldl -pthread -lm -lm -Wl,-z,relro,-z,now -Wl,--gc-sections -v -mandroid -L/android-arm/sysroot/usr/lib -L/android-arm/lib -L/android-arm/lib64 -L/android-arm/aarch64-linux-android/lib -L/android-arm/aarch64-linux-android/lib64)\r\nUsing built-in specs.\r\nCOLLECT_GCC=/android-arm/bin/aarch64-linux-android-gcc\r\nCOLLECT_LTO_WRAPPER=/android-arm/bin/../libexec/gcc/aarch64-linux-android/4.9.x/lto-wrapper\r\nTarget: aarch64-linux-android\r\nConfigured with: /usr/local/google/buildbot/src/android/gcc/toolchain/build/../gcc/gcc-4.9/configure --prefix=/tmp/ec4cb553ef61611c1d388a938c56382a --target=aarch64-linux-android --host=x86_64-linux-gnu --build=x86_64-linux-gnu --with-gnu-as --with-gnu-ld --enable-languages=c,c++ --with-gmp=/buildbot/tmp/build/toolchain/temp-install --with-mpfr=/buildbot/tmp/build/toolchain/temp-install --with-mpc=/buildbot/tmp/build/toolchain/temp-install --with-cloog=/buildbot/tmp/build/toolchain/temp-install --with-isl=/buildbot/tmp/build/toolchain/temp-install --with-ppl=/buildbot/tmp/build/toolchain/temp-install --disable-ppl-version-check --disable-cloog-version-check --disable-isl-version-check --enable-cloog-backend=isl --with-host-libstdcxx='-static-libgcc -Wl,-Bstatic,-lstdc++,-Bdynamic -lm' --disable-libssp --enable-threads --disable-nls --disable-libmudflap --disable-libgomp --disable-libstdc__-v3 --disable-sjlj-exceptions --disable-shared --disable-tls --disable-libitm --enable-bionic-libs --enable-libatomic-ifuncs=no --enable-initfini-array --disable-nls --prefix=/tmp/ec4cb553ef61611c1d388a938c56382a --with-sysroot=/tmp/ec4cb553ef61611c1d388a938c56382a/sysroot --with-binutils-version=2.27 --with-mpfr-version=3.1.1 --with-mpc-version=1.0.1 --with-gmp-version=5.0.5 --with-gcc-version=4.9 --with-gdb-version=none --with-gxx-include-dir=/tmp/ec4cb553ef61611c1d388a938c56382a/include/c++/4.9.x --with-bugurl=http://source.android.com/source/report-bugs.html --enable-languages=c,c++ --disable-bootstrap --enable-plugins --enable-libgomp --enable-gnu-indirect-function --disable-libsanitizer --enable-gold --enable-ld=default --enable-threads --enable-eh-frame-hdr-for-static --enable-fix-cortex-a53-835769 --enable-graphite=yes --with-isl-version=0.11.1 --with-cloog-version=0.18.0 --program-transform-name='s&^&aarch64-linux-android-&' --enable-gold\r\nThread model: posix\r\ngcc version 4.9.x 20150123 (prerelease) (GCC) \r\nCOMPILER_PATH=/android-arm/bin/../libexec/gcc/aarch64-linux-android/4.9.x/:/android-arm/bin/../libexec/gcc/:/android-arm/bin/../lib/gcc/aarch64-linux-android/4.9.x/../../../../aarch64-linux-android/bin/\r\nLIBRARY_PATH=/android-arm/bin/../lib/gcc/aarch64-linux-android/4.9.x/:/android-arm/bin/../lib/gcc/:/android-arm/bin/../lib/gcc/aarch64-linux-android/4.9.x/../../../../aarch64-linux-android/lib/../lib64/:/android-arm/bin/../lib/gcc/aarch64-linux-android/4.9.x/../../../../aarch64-linux-android/lib/:/android-arm/bin/../sysroot/usr/lib/\r\nCOLLECT_GCC_OPTIONS='-shared' '-o' 'bazel-out/armeabi-opt/bin/tensorflow/libtensorflow_framework.so' '-pthread' '-v' '-mandroid' '-L/android-arm/sysroot/usr/lib' '-L/android-arm/lib' '-L/android-arm/lib64' '-L/android-arm/aarch64-linux-android/lib' '-L/android-arm/aarch64-linux-android/lib64' '-mlittle-endian' '-mabi=lp64'\r\n /android-arm/bin/../libexec/gcc/aarch64-linux-android/4.9.x/collect2 -plugin /android-arm/bin/../libexec/gcc/aarch64-linux-android/4.9.x/liblto_plugin.so -plugin-opt=/android-arm/bin/../libexec/gcc/aarch64-linux-android/4.9.x/lto-wrapper -plugin-opt=-fresolution=/tmp/ccIwFJZt.res -plugin-opt=-pass-through=-lgcc -plugin-opt=-pass-through=-lc -plugin-opt=-pass-through=-ldl -plugin-opt=-pass-through=-lgcc --sysroot=/android-arm/bin/../sysroot --eh-frame-hdr -shared -dynamic-linker /system/bin/linker64 -X -EL -maarch64linux --fix-cortex-a53-835769 --fix-cortex-a53-843419 -z noexecstack -z relro -z now -o bazel-out/armeabi-opt/bin/tensorflow/libtensorflow_framework.so /android-arm/bin/../sysroot/usr/lib/crtbegin_so.o -L/android-arm/sysroot/usr/lib -L/android-arm/lib -L/android-arm/lib64 -L/android-arm/aarch64-linux-android/lib -L/android-arm/aarch64-linux-android/lib64 -L/android-arm/bin/../lib/gcc/aarch64-linux-android/4.9.x -L/android-arm/bin/../lib/gcc -L/android-arm/bin/../lib/gcc/aarch64-linux-android/4.9.x/../../../../aarch64-linux-android/lib/../lib64 -L/android-arm/bin/../lib/gcc/aarch64-linux-android/4.9.x/../../../../aarch64-linux-android/lib -L/android-arm/bin/../sysroot/usr/lib -whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/libframework_internal_impl.pic.lo -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/libversion_lib.pic.a -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/liblib_internal_impl.pic.a -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/liblib_hash_crc32c_accelerate_internal.pic.a -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/liblib_proto_parsing.pic.a -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/external/snappy/libsnappy.pic.a -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/external/protobuf_archive/libprotobuf.pic.a -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/external/protobuf_archive/libprotobuf_lite.pic.a -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/libprotos_all_cc_impl.pic.a -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/grappler/costs/libop_performance_data_cc_impl.pic.a -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/tensorflow/stream_executor/libstream_executor_impl.pic.lo -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/libgpu_runtime_impl.pic.lo -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/libcore_cpu_impl.pic.lo -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/libgpu_id.pic.a -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/libgpu_init_impl.pic.lo -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/libgpu_lib.pic.a -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/external/nsync/libnsync_cpp.pic.a -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/kernels/liblookup_util.pic.a -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/kernels/libinitializable_lookup_table.pic.a -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/util/tensor_bundle/libtensor_bundle.pic.a -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/util/tensor_bundle/libnaming.pic.a -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/libcore_cpu_base.pic.lo -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/libgraph.pic.a -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/grappler/libgrappler_item.pic.a -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/grappler/libop_types.pic.a -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/tensorflow/core/grappler/libutils.pic.a -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/external/com_google_absl/absl/base/libbase.pic.a -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/external/com_google_absl/absl/base/libspinlock_wait.pic.a -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/external/com_google_absl/absl/base/libdynamic_annotations.pic.a -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/external/gif_archive/libgif.pic.a -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/external/jpeg/libjpeg.pic.a -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/external/jpeg/libsimd_none.pic.a -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/external/com_googlesource_code_re2/libre2.pic.a -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/external/farmhash_archive/libfarmhash.pic.a -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/external/fft2d/libfft2d.pic.a -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/external/highwayhash/libsip_hash.pic.a -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/external/highwayhash/libarch_specific.pic.a -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/external/png_archive/libpng.pic.a -no-whole-archive -whole-archive bazel-out/armeabi-opt/bin/external/zlib_archive/libzlib.pic.a -no-whole-archive -rpath $ORIGIN/ -soname libtensorflow_framework.so -ldl -lm -lpthread -lm -lpthread -lm -ldl -ldl -lm -lm -z relro -z now --gc-sections -lgcc -lc -ldl -lgcc /android-arm/bin/../sysroot/usr/lib/crtend_so.o\r\n/android-arm/bin/../lib/gcc/aarch64-linux-android/4.9.x/../../../../aarch64-linux-android/bin/ld: cannot find -lpthread\r\n/android-arm/bin/../lib/gcc/aarch64-linux-android/4.9.x/../../../../aarch64-linux-android/bin/ld: cannot find -lpthread\r\ncollect2: error: ld returned 1 exit status\r\n```\r\n\r\nCan anyone tell me what is responsible for those last -lpthread linker flags?", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This is similar to issue #20192  , let me know if the workaround mentioned doesn't work.", "@shashishekhar Unless I'm misunderstanding something, that issue is totally different. This issue isn't about NDK issues. This issue is that Tensorflow doesn't support building C++ library for use on android.", "@d4l3k : For building C++ on Android you will use the NDK toolchain, which is used by bazelunderneath. Tensorflow does support building armeabi-v7a as well as arm64 on Linux.\r\n\r\n@xav12358 : I should mention that we have Tensorflow lite : https://www.tensorflow.org/mobile/tflite/ which is better suitable for Android/iOS and mobile devices and has been designed specifically for mobile computationally constrained devices. Do let me know if you have difficulty using it.\r\n", "If I understand correctly you would like to use Tensorflow on Android via a c++ API? If so, check out [this repo](https://github.com/leeor-langer/Tensorflow-Android-Cpp) and the [following blog](https://medium.com/@leeor.langer/tensorflow-c-api-for-android-a0c55fe226e8). These links will explain how to build a dynamic library (.so file) step by step.", "Please pardon my ignorance (which I can assure you is vast), but considering the complexity of the build pipeline (and the fact that it's 2021) why aren't the tensorflow c and python libraries along with the various language bindings compiled / provided in cross platform docker images."]}, {"number": 16794, "title": "Feature Request: Modification of lstm2d.horizontal_lstm implementation", "body": "I noticed something in the documentation of `lstm2d.horizontal_lstm`. It says:\r\n\r\n> Run an LSTM bidirectionally over all the rows of each image.\r\n\r\nKinda looks like a bidirectional_lstm to me. I propose to change the implentation such that it will use bidirectional_lstm within the function replacing this:\r\n\r\n```\r\nwith variable_scope.variable_scope(\"lr\"):\r\n  hidden_sequence_lr = lstm1d.ndlstm_base(sequence, num_filters_out // 2)\r\nwith variable_scope.variable_scope(\"rl\"):\r\n  hidden_sequence_rl = (lstm1d.ndlstm_base(\r\n      sequence, num_filters_out - num_filters_out // 2, reverse=1))\r\noutput_sequence = array_ops.concat([hidden_sequence_lr, hidden_sequence_rl],\r\n                                       2)\r\n```\r\n\r\nWith this:\r\n\r\n```\r\ncell_fw = rnn_cell.BasicLSTMCell(num_filters_out // 2)\r\ncell_bw = rnn_cell.BasicLSTMCell(num_filters_out // 2)\r\noutput_sequence = rnn.bidirectional_dynamic_rnn(cell_fw, cell_bw, sequence, time_major=True, \r\n                                                                                 dtype=sequence.dtype)\r\n```\r\n  ", "comments": ["This code is considered legacy because the author is no longer maintaining it, and it doesn't have many users.  In fact, I hope to remove it for TF 1.7.  If you'd like to make these kinds of modifications, you may want to copy it under the Apache 2 license into your own repository and make them there?", "If you really intend to remove it, mind at least keeping some of the functions in there (like move them to contrib.layers or something)? I think `images_to_sequence` and `sequence_to_images` are pretty useful.", "Would you like to send a PR to copy these + unit tests over?  I'll be\nremoving them in the next day or two.\n\nOn Tue, Feb 6, 2018 at 3:27 PM, Jerome <notifications@github.com> wrote:\n\n> If you really intend to remove it, mind at least keeping some of the\n> functions in there (like move them to contrib.layers or something)? I think\n> images_to_sequence and sequence_to_images are pretty useful.\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/16794#issuecomment-363601446>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim4GDo6qgnqiuU8j8ZlNl45KYeB59ks5tSN_8gaJpZM4R6qoX>\n> .\n>\n", "Alright, I'll work on them.", "Pull request made."]}, {"number": 16793, "title": "tf.gradients(colocate_gradients_with_ops=True) set wrong device when using CPU param weight decay", "body": "**System information**\r\n\r\n- OS Platform and Distribution: CentOS Linux 7(x86-64)\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.4.1\r\n- Python version: 2.7.6\r\n- CUDA/cuDNN version: 8.0/6.0\r\n- Have I written custom code: YES\r\n- Bazel version: N/A\r\n- GPU model and memory: PS Toy model\r\n- Exact command to reproduce: List at the end\r\n\r\nI am trying to define a two-layered dnn for mnist classification, first fc on cpu second on gpu. The devices are set with replica_device_setter. I computed gradients with **colocate_gradients_with_ops=True**. If as expected, the gradient of first layer should be on **/job:worker/replica:0/task:0/device:CPU:0** and gradient of the second layer on **/job:worker/task:0/device:GPU:0**.\r\n\r\nHowever, i was confused that gradient of the first layer is on device **/job:ps/replica:0/task:0/device:CPU:0**! \r\n\r\nI noticed that this error can be avoided by omitting `loss += l2_loss * weight_decay`, BUT WHY? It's there some conflict between CPU param weight_decay and gradients with colocate_gradients_with_ops=True?\r\n\r\n**Source Code**\r\n\r\n    import tensorflow as tf\r\n\r\n    # cluster specification\r\n    parameter_servers = [\"10.194.43.100:2222\"]\r\n    workers = [\"10.194.43.100:%d\"%(2230+i) for i in range(2)]\r\n\r\n    cluster = tf.train.ClusterSpec({\"ps\":parameter_servers, \"worker\":workers})\r\n    worker_prefix = \"/job:worker/task:%d\"%0\r\n    cpu_device = tf.train.replica_device_setter(worker_device=worker_prefix+'/cpu:0', cluster=cluster)\r\n    gpu_device = tf.train.replica_device_setter(worker_device=worker_prefix+'/gpu:0', cluster=cluster, ps_strategy=tf.contrib.training.GreedyLoadBalancingStrategy(1, tf.contrib.training.byte_size_load_fn))\r\n\r\n    normal_initializer = tf.truncated_normal_initializer(stddev=0.1, dtype=\"float32\")\r\n    weight_decay = 1e-2\r\n\r\n    with tf.device(cpu_device):\r\n        W0 = tf.get_variable('W0', shape=[784, 100], initializer=normal_initializer, dtype=\"float32\")\r\n        b0 = tf.get_variable('b0', shape=[100], initializer=tf.constant_initializer(0), dtype=\"float32\")\r\n        x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x-input_%d\"%i)\r\n        y_ = tf.placeholder(tf.float32, shape=[None, 10], name=\"y-input_%d\"%i)\r\n\r\n        cpu_output = tf.add(tf.matmul(x, W0), b0)\r\n\r\n    with tf.variable_scope('v', reuse=False), tf.name_scope('tower_0') as name_scope:\r\n        with tf.device(gpu_device):\r\n            W1 = tf.get_variable('W1', shape=[100, 10], initializer=normal_initializer, dtype=\"float32\")\r\n            b1 = tf.get_variable('b1', shape=[10], initializer=tf.constant_initializer(0), dtype=\"float32\")\r\n\r\n            gpu_output = tf.add(tf.matmul(cpu_output, W1), b1)\r\n            loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=gpu_output)\r\n            loss = tf.reduce_mean(loss)\r\n\r\n            params = tf.trainable_variables()\r\n            l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in params])\r\n            loss += l2_loss * weight_decay\r\n            grads = tf.gradients(loss, params, colocate_gradients_with_ops=True, aggregation_method=tf.AggregationMethod.DEFAULT)\r\n\r\n    with tf.device(cpu_device):\r\n        for grad in grads:\r\n            print grad, grad.device\r\n    print 'done'\r\n\r\n**Logs**\r\n\r\n    Tensor(\"v/tower_0/gradients/AddN_3:0\", shape=(784, 100), dtype=float32, device=/job:ps/task:0) /job:ps/task:0\r\n    Tensor(\"v/tower_0/gradients/AddN_2:0\", shape=(100,), dtype=float32, device=/job:ps/task:0) /job:ps/task:0\r\n    Tensor(\"v/tower_0/gradients/AddN_1:0\", shape=(100, 10), dtype=float32, device=/job:worker/task:0/device:GPU:0) /job:worker/task:0/device:GPU:0\r\n    Tensor(\"v/tower_0/gradients/AddN:0\", shape=(10,), dtype=float32, device=/job:worker/task:0/device:GPU:0) /job:worker/task:0/device:GPU:0\r\n    done", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nBazel version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Love how you guys close relevant issues because no one knows the answer :)"]}, {"number": 16792, "title": "Branch 184622482", "body": "", "comments": []}, {"number": 16791, "title": "Tensorflow 1.5.0 Import Error on CUDA 9.0", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, I used the stock version.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: pip binary for windows with GPU support\r\n- **TensorFlow version (use command below)**: 1.5.0\r\n- **Python version**: Python 3.6.1 from Anaconda\r\n- **CUDA/cuDNN version**: 9.0/7.0.5\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: import tensorflow as tf\r\n\r\n### Describe the problem\r\nInstalled CUDA/CUDNN 8.0/6, 9.0/7, 9.1/7\r\nUsed pip to install tensorflow_gpu 1.5.0\r\n\r\nThe installation process finished normally.\r\n\r\nHowever, when import the module by \"import tensorflow as tf\", error messages raises and it says ImportError: Could not find 'cudart64_90.dll'.\r\n\r\nI double checked the CUDA_PATH and PATH environmental variables to make sure that CUDA/CUDNN 9.0/7 are being used. Later I removed the 8.0/6 and 9.1/7, and the problem still exists.\r\n\r\n### Source code / logs\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 30, in <module>\r\n    self_check.preload_check()\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\", line 82, in preload_check\r\n    % (build_info.cudart_dll_name, build_info.cuda_version_number))\r\nImportError: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit\r\n", "comments": ["A system reboot solved the issue.\r\n\r\nHowever, since the tensorflow installation page still refers to CUDA/CUDNN 8.0/6, it would be better if the page states the CUDA requirement for different versions of prebuilt binaries.\r\n", "cc/ @MarkDaoust ", "The docs on the TensorFlow site have been updated. r1.5, r1.6, and master all include this commit now:\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/f68404bdd10a4b6ef2a50439efac4614de024636", "Thanks so much! I've tried dozens of ways for hours. **A system reboot finally solved everything.**"]}, {"number": 16790, "title": "Save a numpy params as tensorflow model!", "body": "Hi, I have trained a model, and i save the params in a numpy file in dict type.\r\nNow i construct the network manually, and set the params as my trained model.\r\ni want to save the network as tensorflow model, but the session is empty, \r\nso how could i save the model?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 16789, "title": "Fix typo", "body": "", "comments": []}, {"number": 16788, "title": "Fix for GLSTMCell implementation", "body": "* Fix issue described here: https://github.com/tensorflow/tensorflow/issues/16703\r\n* Make it work correctly for input sizes!=num_units", "comments": ["case540 partially fixed it. I'll submit another PR for a smaller fix"]}, {"number": 16787, "title": "Bump the rtol in hmc_test", "body": "This test is flaky due to the rtol:\r\nhttps://source.cloud.google.com/results/invocations/e381cb7e-272e-41ac-b0da-931f6cf293bb/log", "comments": ["[+gunan] affects the r1.6 merge back to master too.", "Sorry for the flake.  I actually had thought this was rolled back? In either case we're currently submitting the fixed version of the original."]}, {"number": 16786, "title": "Fix wrong error message for beta distribution", "body": "This PR fixes the assertion failure message for Beta distribution's valid sample check, which is currently incorrect. For a sample with value `1.0`, the current error message is: `sample must be no larger than '1'`, wrongly suggesting that sample can be `1.0`. After this PR, the will say, `sample must be less than '1'`.", "comments": []}, {"number": 16785, "title": "Merging the 1.6 branch back into master.", "body": "", "comments": []}, {"number": 16784, "title": "Add broadcast support for softmax_cross_entropy_with_logits", "body": "This fix tries to address the issue raised in #11534 where there was no broadcast support for SoftmaxCrossEntropyWithLogits. This fix adds the broadcast support for SoftmaxCrossEntropyWithLogits, and adds test cases for it.\r\n\r\nThis fix fixes #11534.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@yongtang any followup on this? (also, please rebase)", "Thanks @rmlarsen for the review. The PR has been updated. Please take a look.", "@yongtang Thanks for the contribution.", "@yongtang This PR is showing a significant performance regression in our internal monitoring. Can you please take a look? \r\n\r\nCompare the benchmark before/after by running:\r\n\r\nbazel test tensorflow/core/kernels:xent_op_test --test_arg=--benchmarks=all\r\n\r\nThanks. ", "@yongtang To add to @rmlarsen's command, you can pass --test_output=streamed to print out benchmark outputs.\r\n\r\nI just tried to run the benchmarks at and before the commit in this PR and can see the regression. For e.g. BM_Xent_16_10000_gpu benchmark prints:\r\n570.9M items/s without this change\r\n373.9M items/s with this change\r\n\r\n(I was running with 4 Titan X gpus, so numbers are probably different with other configs)", "@yongtang It's OK. I have a fix for it. It will be pushed to open source sometime next week.", "@yongtang To be clear: I am improving Eigen's broadcasting to handle the trivial case better. We have to upstream this to Eigen before it will appear in TensorFlow, but expect it to land sometime next week.", "@rmlarsen @annarev Thanks a lot for the help and sorry for the trouble the PR was causing. Please let me know if there are anything else I could help.", "@yongtang no worries. It actually alerted me to a bunch of easy improvements in Eigen :-) So look out for improved performance of a bunch of stuff in the near future...\r\n", "@yongtang Here is the pull request: https://bitbucket.org/eigen/eigen/pull-requests/389/add-a-fast-path-for-the-trivial-case-in/diff\r\n"]}, {"number": 16783, "title": "Bump the required numpy version to 1.13.3 in r1.6", "body": "", "comments": []}, {"number": 16782, "title": "Module missing", "body": "\r\n", "comments": ["Please reopen with more information if this is a bug thanks!"]}, {"number": 16781, "title": "Feature request: Save optimizer variables under separate name scope", "body": "Tensorflow version b'v1.3.0-rc1-3011-gd86448938' 1.3.0\r\nHave I written custom code N/A\r\nOS Platform and Distribution N/A \r\nTensorFlow installed from N/A\r\nBazel version N/A\r\nCUDA/cuDNN version N/A\r\nGPU model and memory N/A\r\nExact command to reproduce N/A\r\n\r\nThe variables initialized by the Adam optimizer is not created under its own name scope, but inherits the name of trainable variables, e.g. `model/q_networks/online/conv2d/bias/Adam_1` (the Adam optimizer was not creater under the name scopes `model`, `q_networks` or `online`).\r\n\r\nThis creates an issue if you want to use another optimizer for a restored model, because you can't use the name scope to avoid restoring the variables related to the optimizer.\r\n\r\nHere's a workaround:\r\n```\r\n        build_model()  # Build model under namescope 'model', without optimizer\r\n        temp = set(tf.global_variables())\r\n        if save or restore:\r\n            saver = tf.train.Saver(\r\n                var_list=tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='model'))\r\n        sess.run(tf.global_variables_initializer())\r\n        if restore:\r\n            saver.restore(sess, model_path)\r\n        trainer = tf.train.AdamOptimizer(1e-4)\r\n        sess.run(tf.variables_initializer(set(tf.global_variables()) - temp))  # Initialize Adam variables\r\n```\r\nIt would be simpler (to figure out) if the optimizer variables had their own scope.\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Yes.", "We will consider it if we heard more demand for this feature.", "I am facing this issue as well when building GAN models where the pretraining uses a different optimizer than the adversarial training.", "I face this issue when trying to use weights from some model as part for a larger one. I need to manually filter out the optimizer variables from the scope in which the variables which should be restored live.\r\nAlternatively, one can place a `variable_scope` around the call to `minimize` and access the desired variables with `tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='my_prefix')`.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Yes, this is still an issue.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "yes\n\n2018-04-17 14:42 GMT+02:00 Alfred Sorten Wolf <notifications@github.com>:\n\n> It has been 14 days with no activity and the awaiting response label was\n> assigned. Is this still an issue?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/16781#issuecomment-381976966>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AGd9xNalGDApwv-qDRIRfvoXLw8myymeks5tpeM8gaJpZM4R6E3o>\n> .\n>\n", "@martinwicke do you know who owns AdamOptimizer? Thanks.", "I believe we would have trouble changing this behavior before 2.0 since this would be a breaking change. So sadly, I think we're stuck with your workaround until then.\r\n\r\n@allenlavoie has dealt with this particular issue before and correct me if I'm wrong.", "As of TensorFlow 1.5 you can use `optimizer.variables()` to get a list of variables associated with the optimizer. For Adam this is its `m` and `v` slot variables along with `beta1_power` and `beta2_power`. Note that this only works after a training step has been built, since optimizers create these variables lazily.\r\n\r\nI think this works for the use-case you've described? You can use `.variables()` to filter the `var_list` passed to a `Saver` for restore, and you can use it to initialize the variables.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nv = tf.get_variable(name=\"v\", shape=[], use_resource=True)\r\noptimizer = tf.train.AdamOptimizer(0.001)\r\ntrain_op = optimizer.minimize(lambda: v.read_value())\r\nprint(\"All variables\", tf.global_variables())\r\nprint(\"Non-optimizer variables\",\r\n      set(tf.global_variables()) - set(optimizer.variables()))\r\n```\r\nPrints:\r\n```\r\nAll variables [<tf.Variable 'v:0' shape=() dtype=float32>, <tf.Variable 'beta1_power:0' shape=() dtype=float32_ref>, <tf.Variable 'beta2_power:0' shape=() dtype=float32_ref>, <tf.Variable 'v/Adam:0' shape=() dtype=float32>, <tf.Variable 'v/Adam_1:0' shape=() dtype=float32>]\r\nNon-optimizer variables {<tf.Variable 'v:0' shape=() dtype=float32>}\r\n```\r\nThis should work for any optimizer.", "Good enough for me. Thanks!\n\n2018-04-25 18:33 GMT+02:00 Allen Lavoie <notifications@github.com>:\n\n> As of TensorFlow 1.5 you can use optimizer.variables() to get a list of\n> variables associated with the optimizer. For Adam this is its m and v\n> slot variables along with beta1_power and beta2_power. Note that this\n> only works after a training step has been built, since optimizers create\n> these variables lazily.\n>\n> I think this works for the use-case you've described? You can use\n> .variables() to filter the var_list passed to a Saver for restore, and\n> you can use it to initialize the variables.\n>\n> import tensorflow as tf\n>\n> v = tf.get_variable(name=\"v\", shape=[], use_resource=True)\n> optimizer = tf.train.AdamOptimizer(0.001)\n> train_op = optimizer.minimize(lambda: v.read_value())\n> print(\"All variables\", tf.global_variables())\n> print(\"Non-optimizer variables\",\n>       set(tf.global_variables()) - set(optimizer.variables()))\n>\n> Prints:\n>\n> All variables [<tf.Variable 'v:0' shape=() dtype=float32>, <tf.Variable 'beta1_power:0' shape=() dtype=float32_ref>, <tf.Variable 'beta2_power:0' shape=() dtype=float32_ref>, <tf.Variable 'v/Adam:0' shape=() dtype=float32>, <tf.Variable 'v/Adam_1:0' shape=() dtype=float32>]\n> Non-optimizer variables {<tf.Variable 'v:0' shape=() dtype=float32>}\n>\n> This should work for any optimizer.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/16781#issuecomment-384349990>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AGd9xEOq--B56Lkq3HNf5-wZk2snYBnNks5tsKVugaJpZM4R6E3o>\n> .\n>\n", "Cool. Will close for now, but feel free to comment if there are related issues.\r\n\r\nLonger-term we'd like names not to matter as much, so getting variables from an object is slightly closer to that. [Object-based checkpointing](https://github.com/tensorflow/tensorflow/blob/8fa93c0eb6e12438c3fa564698c0bbf2921def9b/tensorflow/python/training/checkpointable_utils.py#L827) (not yet in core for a release, but it's been around as `tf.contrib.eager.Checkpoint` for a bit) should make things like this easier too, since you can just skip a dependency on the Optimizer object."]}, {"number": 16780, "title": "Trying to allocate large output tensor in custom op leads to multiple evaluation of Compute method", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS X 10.13.2\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.5.0\r\n- **Python version**: 2.7.14\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: CPU-version of Tensorflow\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: make\r\n\r\n\r\n### Describe the problem\r\nI wrote a custom op. During debugging i've noticed then Compute method from my op fired multiple times during single op.eval() call.\r\nMy quest lead me to row:\r\n```c++\r\nOP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({output_size}), &values_tensor));\r\n```\r\nIf output_size is small (e.g. 1000), my op works well and executes one time.\r\nIf output_size is big (e.g. 15000000), my op executes multiple times.\r\n\r\n### Source code / logs\r\nThere are 3 files in archive:\r\n- custom op source code\r\n- demo eval script\r\n- makefile config to build op and run eval\r\n\r\n\r\n[issue.zip](https://github.com/tensorflow/tensorflow/files/1696503/issue.zip)\r\n", "comments": ["Do you get the same behavior with a simple type e.g. float or is it just string?", "Got same behavior with float", "OK I will take a look when I get a chance but it may be a day or two sorry!", "The first time your op is run is during a constant-folding optimization pass. When the output is sufficiently small the constant folding heuristic decides to replace the execution of the op by a constant, and when the 'real' step runs it doesn't need to execute your op again. When the output is larger, the heuristic decides it's better to keep re-running the op than fold it into a constant, so the 'real' step actually executes your op.\r\n\r\nIf you put SetIsStateful on your Op registration then the constant folder ignores it, and it will only execute once as you expect, regardless of the size of the output.\r\n\r\nClosing for now but please reopen if that doesn't make sense!"]}, {"number": 16778, "title": "Fix the incorrect link to vulnerability reporting", "body": "This fix fixes the incorrect link to vulnerability reporting.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}]