[{"number": 3165, "title": "AttributeError: 'Tensor' object has no attribute 'shape'", "body": "### Environment info\n\nOperating System:  BashOn Windows\n\nInstalled version of CUDA and cuDNN: \n(doesnt matter CPU version only)\n\nIf installed from binary pip package, provide:\n0.9.0\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n\n```\n    def create_classifier(self):\n        hiddenLayers = [self.num_points, self.num_points * 2, 10]\n        self.classifier = tf.contrib.learn.DNNClassifier(hidden_units=hiddenLayers)\n```\n\nlabel is a string\npoint list is a list of an array of x, y values\nex:\n\n```\n[[1,2],[2,3],[3,4],...]\n```\n\n```\n    def train(self, label, point_list):\n        points = self.resample(point_list, self.num_points)\n        utils.strip_ids_from_points(points)\n        value_class = 1 if label == self.label else 0\n        target = tf.reshape(tf.constant(value_class), [1])\n        print 'training classifier to recognize value as: [' + str(value_class) + '] label is ' + label + ' class is ' + self.label\n        point_tensor = tf.convert_to_tensor(points, dtype=tf.float32)\n        reshaped_tensor = tf.reshape(point_tensor, [1, self.num_points * 2])\n        print reshaped_tensor\n        print target\n        self.classifier.fit(x=reshaped_tensor, y=target, steps=1)\n```\n### What have you tried?\n\ngoogling why a tensor would not have a shape attribute\n### Logs or other output that would be helpful\n\n```\nTensor(\"Reshape_1:0\", shape=(1, 64), dtype=float32)\nTensor(\"Reshape:0\", shape=(1,), dtype=int32)\nTraceback (most recent call last):\n  File \"main.py\", line 6, in <module>\n    connection.start_socket(8089, callback=handler.message_processor)\n  File \"/mnt/d/workspace/SketchRecognitionWithTensorFlow/src/main/python/connection/python_socket_server.py\", line 13, in start_socket\n    process_message(connection, callback=callback)\n  File \"/mnt/d/workspace/SketchRecognitionWithTensorFlow/src/main/python/connection/python_socket_server.py\", line 38, in process_message\n    result = callback(general_proto)\n  File \"/mnt/d/workspace/SketchRecognitionWithTensorFlow/src/main/python/recognition/proto_handler.py\", line 39, in message_processor\n    return train_shape(general_proto.template)\n  File \"/mnt/d/workspace/SketchRecognitionWithTensorFlow/src/main/python/recognition/proto_handler.py\", line 23, in train_shape\n    rec.add_training_data(recognition_template.interpretation.label, recognition_template.shape)\n  File \"/mnt/d/workspace/SketchRecognitionWithTensorFlow/src/main/python/recognition/recognition_manager.py\", line 98, in add_training_data\n    self.recognizers[label].train(label, points)\n  File \"/mnt/d/workspace/SketchRecognitionWithTensorFlow/src/main/python/recognition/simple/recognizer.py\", line 78, in train\n    self.classifier.fit(x=reshaped_tensor, y=target, steps=1)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 173, in fit\n    input_fn, feed_fn = _get_input_fn(x, y, batch_size)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 67, in _get_input_fn\n    x, y, n_classes=None, batch_size=batch_size)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/io/data_feeder.py\", line 117, in setup_train_data_feeder\n    X, y, n_classes, batch_size, shuffle=shuffle, epochs=epochs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/io/data_feeder.py\", line 239, in __init__\n    self.X.shape, None if self.y is None else self.y.shape, n_classes,\nAttributeError: 'Tensor' object has no attribute 'shape'\n```\n", "comments": ["Okay figured it out.  Fit can not take a tensor as an input.  (it would be helpful if that was the error message instead)\n", "Actually there is a bug in data_feeder.py it should use get_shape() instead .shape.\n", "I'm also having this issue. I'm using LinearClassifier. I tried passing the X argument as a tensor, Pandas dataframe, and NumPy matrix. None work and all attempts result in the same AttributeError.\n\nIs there any progress with this? Thanks!\n", "Have you tried at head? We know there's a bug in 0.9, but it should work at head.\n", "Closing for now since @martinwicke says this is fixed.\n"]}, {"number": 3164, "title": "Add Float64 support to CTC Decoder Ops", "body": "**PR incomplete! Have question about best way to get around one line**\n\nChipping away at ops listed in #1140\n\nThis PR adds 64-bit floating point support for CTCGreedyDecoder and provides tests for it. However, CTCDecoderBeamSearch doesn't have support yet, as there's [a line that needs to be changed](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/ctc_decoder_ops.cc#L293), and I need help with it. What would be the best way to dynamically select the correct `DataType` here? Currently `DT_FLOAT` is hard-coded in, but it should be `DT_DOUBLE` when the template type is `double`. Thanks!\n", "comments": ["Can one of the admins verify this patch?\n", "`DataTypeToEnum<T>::value` from\nhttps://github.com/tensorflow/tensorflow/blob/b8652a2efea3a04cbaa00b5210ba58a6c5854307/tensorflow/core/framework/types.h#L150 should work.\n", "@tensorflow-jenkins test this please\n", "Is there a good reason for having a float64  version of CTC decoders, and possibly even loss?  Can you ought point to a reference it or example that shows this improves numerical stability/convergence speed?\n", "Honestly, there wasn't much thought in terms of the total benefit for adding double support for these specific Ops- it was pretty much a random draw from [this list](https://github.com/tensorflow/tensorflow/issues/1140#issuecomment-227012645) in #1140. You can follow the discussion there to see the context.\n\nThat said, I still have more work on this to implement and register a GPU kernel. If you think my efforts are better spent on something else, I'm happy to close this for now and move on to something more beneficial.\n", "Pulling this off for now- doesn't look like it's necessary.\n"]}, {"number": 3163, "title": "the results is not reproducible", "body": "the results is not reproducible due to some random factors, is there a way to reproduce the exactly same result in NN training\n", "comments": ["Please re-ask this question on stackoverflow. Issues are primarily for bugs.\n", "```\r\nimport numpy as np\r\nnp.random.seed(0)  # make sure results are reproducible\r\nimport tensorflow as tf\r\ntf.set_random_seed(0)  # make sure results are reproducible\r\n```\r\n\r\nThe numpy part might not be necessary in your case."]}, {"number": 3162, "title": "Update README.md", "body": "Fix markup for link to iOS examples\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins test this please.\n"]}, {"number": 3161, "title": "Sequence_Length for RNN Bug", "body": "I'm using the current up-to-date version of Tensorflow for Apple. For Long-Short Term Memory, there is an error which prevents me from specifying the sequence length [batch_size] Tensor. Code used to work until I updated Tensorflow:\n\n`seq_len = tf.placeholder(\"int64\", [batch_size])`\n\n...\n...\n...\n\n`lstm_fw1_cell = rnn_cell.BasicLSTMCell(n_LSTM_input, forget_bias=1.0)`\n\n`state_fw1 = tf.fill([batch_size,2*n_LSTM_input], constant(0, dtype=tf.float32))`\n\n`outputs, states = rnn.rnn(lstm_fw1_cell, _X, initial_state=state_fw1, scope=\"LSTM1\", sequence_length=_seq_len)`\n\nError:\nFile \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 112, in rnn\n    lambda: zero_output_state, output_state)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 642, in cond\n    res_f = context_f.BuildCondBranch(fn2)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 568, in BuildCondBranch\n    r = fn()\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 107, in output_state\n    return cell(input_, state)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell.py\", line 190, in **call**\n    concat = linear([inputs, h], 4 \\* self._num_units, True)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell.py\", line 667, in linear\n    shapes = [a.get_shape().as_list() for a in args]\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 722, in as_list\n    return [dim.value for dim in self._dims]\nTypeError: 'NoneType' object is not iterable\n", "comments": ["Could you please provide a reproducible test case?  Please also follow the instructions in new issue and provide us the PIP version you used if a binary, what version of the source if built as source, what version of MacOS you are using?\n", "Can you provide us with _X.get_shape()?\n", "Closing due to inactivity. Please reopen if there is still a problem.\n"]}, {"number": 3160, "title": "iOS: No OpKernel was registered to support Op 'Assign'", "body": "`Running model failed: Invalid argument: No OpKernel was registered to support Op 'Assign' with these attrs\n     [[Node: save/Assign_18 = Assign[T=DT_INT64, _class=[\"loc:@input_producer/limit_epochs/epochs\"], use_locking=true, validate_shape=true, _device=\"/device:CPU:0\"](input_producer/limit_epochs/epochs, save/restore_slice_18)]]`\n\nTensorflow 0.9 iOS build.\nModel+graph: https://cloud.mail.ru/public/EhFP/eN1dwHhQR\n\nI have added\n./ops/state_ops.cc\n./kernels/dense_update_ops.cc\n\nto the tensorflow/contrib/makefile/tf_cc_files.txt\n\nBut with no result.\n", "comments": ["Closing due to inactivity. Feel free to open a new issue if you are still interested in solving this."]}, {"number": 3159, "title": "How to remember the unit position of Dropout", "body": "I am using dropout in my neural network:\n\n```\nW   = tf.get_variable(\"W\", [hidden_unit, 50]) \n\ndef RNN_L1(x,  initial_state,  real_length):\n                x = tf.transpose(x, [1, 0, 2]) \n                x = tf.reshape(x, [-1, word_dim]) \n\n                lstm_cell = rnn_cell.LSTMCell(num_units = hidden_unit, input_size = word_dim)\n\n                x = tf.split(0, sequence_length, x)\n                outputs, _ = rnn.rnn(lstm_cell, x, initial_state=initial_state, sequence_length=real_length)\n\n                return outputs\n\n\n\noutputs_L1  =  RNN_L1(x_vec,      self.initial_stateL1,     self.real_length)\noutputs_L1  =  tf.pack(outputs_L1)\n\n\ntensor_shape = outputs_L1.get_shape()\n\nfor step_index in range(tensor_shape[0]):\n                output_relu= tf.matmul(outputs_L1[step_index,  :,  :], W) + B\n                output_relu= tf.nn.relu(output_relu)\n                output_relu = tf.nn.dropout(output_relu, self.dropout_keep_prob)\n```\n\n**outputs_L1** is the output of LSTM which has 3D-Tensor[sentence length, batchsize, hidden unit]. So, you can see the \"for loop\" code, I use \"for loop\" to do dropout for each **outputs_L1[step_index,  :,  :]**. But in this way, each outputs_L1[step_index,  :,  :] has it's own position of dropout. \n\nFor example:\n**outputs_L1[1,  :,  :]** and **outputs_L1[2,  :,  :]** have different position of dropout at \n`output_relu = tf.nn.dropout(output_relu, self.dropout_keep_prob)`.\n\nWhat I want is that they have same dropout position in \"for loop\". Can you help me?\n", "comments": []}, {"number": 3158, "title": "Update from original r0.9", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}, {"number": 3157, "title": "Revert \"Misc improvements to Dockerfile.devel-gpu\"", "body": "Reverts tensorflow/tensorflow#3143\n", "comments": []}, {"number": 3156, "title": "Updated tflearn examples copyright", "body": "- Updated copyright in examples\n", "comments": ["@ilblackdragon \n"]}, {"number": 3155, "title": "More clear error message required", "body": "Tensorflow 0.9.0\n\nTrying to load and execute that graph+checkpoint from C++ API: https://cloud.mail.ru/public/5yk3/No887jqS8\n\nModel was prepared using 'freeze_graph.py'\n\nCode based on Tensorflow examples(graph contains no input layer):\n`std::string output_layer = \"output_node\";\n    std::vector<tensorflow::Tensor> outputs;\n    tensorflow::Status run_status = session->Run({{}},{output_layer}, {}, &outputs);\n    if (!run_status.ok()) {\n        LOG(ERROR) << \"Running model failed: \" << run_status;\n        return;\n    }\n    tensorflow::string status_string = run_status.ToString();\n    NSLog(@\"%s\",status_string.c_str());\n`\n\nError:\n\n> Running model failed: Invalid argument: No OpKernel was registered to support Op 'PaddingFIFOQueue' with these attrs\n>      [[Node: batch/padding_fifo_queue = PaddingFIFOQueue[capacity=32, component_types=[DT_FLOAT], container=\"\", shapes=[[-1,-1,3]], shared_name=\"\"]()]]\n\nAnd that's all. Nothing that can help me to understand what to do with the error. The only information I was able to find is that the error might be due to GPU only operation, but same model runs fine from python in CPU only mode.\n\nClearly don't know what to do with this error message. Created stackoverflow question http://stackoverflow.com/questions/38155086/no-opkernel-was-registered-to-support-op-paddingfifoqueue-with-these-attrs And hope for some insights.\n\nI think that it would be very useful to elaborate on this kind of errors in run_status.\n", "comments": ["I have found out that iOS build of tensorflow lacks numerous files. FIFOQueue is one of them. Is there fully functional iOS tensorflow version ?\n", "Feel free to submit a PR clarifying the error message."]}, {"number": 3154, "title": "can't enable --config=cuda", "body": "I successfully built tensorflow 0.9.0 with bazel 0.3.0 on Redhat Enterprices 6.6 without cuda support. \n### but when I enable --config=cuda, the build process stopped at linking '@protobuf//:protoc'\n\nI suspected some settings associate with both cuda and python are incorrect (from the highlighted error logs) but have no idea how to reconfigure. Can you please help to find the problem?  I am using python2.7.11 from anaconda2\n\nThanks,\n-Wei\n\n$ bazel build --config=cuda @protobuf//:protoc --verbose_failures --keep_going\nWARNING: /root/.cache/bazel/_bazel_root/33ff58ac202e7ac02dc3ff3533effc70/external/protobuf/WORKSPACE:1: Workspace name in /root/.cache/bazel/_bazel_root/33ff58ac202e7ac02dc3ff3533effc70/external/protobuf/WORKSPACE (@__main__) does not match the name given in the repository's definition (@protobuf); this will cause a build error in future versions.\nINFO: Found 1 target...\nERROR: /root/.cache/bazel/_bazel_root/33ff58ac202e7ac02dc3ff3533effc70/external/protobuf/BUILD:331:1: Couldn't build file external/protobuf/protoc: Linking of rule '@protobuf//:protoc' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command\n  (cd /root/.cache/bazel/_bazel_root/33ff58ac202e7ac02dc3ff3533effc70/execroot/tensorflow.gpu && \\\n  exec env - \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/local_linux-fastbuild/bin/external/protobuf/protoc bazel-out/local_linux-fastbuild/bin/external/protobuf/_objs/protoc/external/protobuf/src/google/protobuf/compiler/main.pic.o bazel-out/local_linux-fastbuild/bin/external/protobuf/libprotoc_lib.a bazel-out/local_linux-fastbuild/bin/external/protobuf/libprotobuf.a bazel-out/local_linux-fastbuild/bin/external/protobuf/libprotobuf_lite.a -lpthread -lstdc++ -B/usr/local/bin/ -pie -fPIC -Wl,-z,relro,-z,now -no-canonical-prefixes -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,-S): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n### Could not find platform independent libraries <prefix>\n### Could not find platform dependent libraries <exec_prefix>\n### Consider setting $PYTHONHOME to <prefix>[:<exec_prefix>]\n### ImportError: No module named site\n\nTarget @protobuf//:protoc failed to build\nINFO: Elapsed time: 0.354s, Critical Path: 0.01s\n", "comments": ["all the output files include \"protoc\" already generated with third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc in the cache directory.  \nAnd I also tried adding pythonhome to LD_LIBRARY_PATH DYLD_LIBRARY_PATH and even PATH. problem still same\n", "shall I try r0.6?\n", "It appears the linking of the protobuf library failed. But there is no error message just an exit code one. You'll have to see if there is a more detailed error on why that failed. If you can't find it in the bazel logs you can maybe try running the command from the shell yourself (make sure to get the environment matchin)\n", "Automatically closing due to lack of recent activity. Please reopen when further information becomes available.\n"]}, {"number": 3153, "title": "R0.9", "body": "", "comments": []}, {"number": 3152, "title": "num_epochs does not work in modified cifar10", "body": "I am running cifar10 examples and have made some changes. I would like to evaluate every example only once, while the original example can only do mass evaluation. My question are described [here](http://stackoverflow.com/questions/38125436/tensorflow-how-to-evaluate-all-test-set-with-every-example-once-and-only-once). And was suggested by changing function 'tf.train.string_input_producer'.\n\nBut it failed and I also found a similar [question](http://stackoverflow.com/questions/35674073/compute-status-not-found-tensor-name-input-producer-limit-epochs-epochs-not) but with no final solution given, @mrry .\n\nAnd this problem can also solve other problems including this [one](http://stackoverflow.com/questions/36960457/tensorflow-evaluate-with-confusion-matrix) here.\n\nThank you in advance.\n", "comments": ["Dear All,\nI managed to solve it at the cost that I have to set every thread reads only one test example. I think the queues and threading mechanism of TensorFlow should re-design: the threading should be able to read the rest of queues automatically.\n\nThank you very much. And I would also appreciated if better solution to be given\n", "What version of tensorflow are you using on what operating system and version? Have you tried it with r0.9?\n\nOriginal Error was\n with NotFoundError: Tensor name \"input_producer/limit_epochs/epochs\" not found in checkpoint files\n", "I used 0.8 and I believe the problem, that evaluation on each example only once, need redesign. Although in the [post](http://stackoverflow.com/questions/38125436/tensorflow-how-to-evaluate-all-test-set-with-every-example-once-and-only-once), I proposed a go-round solution, but it can only applied to test. For real evaluation, each example should feed and evaluate only once, I still can not find a proper way.\n", "Can you confirm it is still a problem in 0.9?\n", "Automatically closing due to lack of recent activity. Please reopen if additional information becomes available.\n"]}, {"number": 3151, "title": "Branch 126416482", "body": "Merging internal Google changes.\n", "comments": []}, {"number": 3150, "title": "Small typo", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins test this please\n"]}, {"number": 3149, "title": "Fix link in tf.learn linear models overview", "body": "Adds '../' to a link in the inear models overview.\n", "comments": ["Can one of the admins verify this patch?\n", "Fixes https://github.com/tensorflow/tensorflow/issues/3123\n", "@tensorflow-jenkins test this please\n", "@vrv @martinwicke could you please merge this documentation-only PR?\n"]}, {"number": 3148, "title": "nan error: linear regression demo at tensorflow.org", "body": "There is a linear regression demo on [Here](https://www.tensorflow.org/versions/r0.9/get_started/index.html).\nThe x_data is generate by\n\n``` python\nx_data = np.random.rand(100).astype(np.float32)\n```\n\nbut when I change it to\n\n``` python\nx_data = np.arange(100).astype(np.float32)\n```\n\nthe \"w\" and \"b\" will become \"nan\".\n", "comments": ["This type of question is best asked on StackOverflow. I don't believe this is a bug. Your arange() version has a mean of 50ish. That means the learning rate provided to gradient descent will be too high for stable convergence. You can fix this by \nx_data = np.arange(100).astype(np.float32) / 100\nor if you don't want to change your data you'll need to reduce learning rate to .0001 and take 40000ish iterations.\n"]}, {"number": 3147, "title": "Better way to count number of zero elements across a dimension?", "body": "I was wondering whether there is a nicer way to count the number of zero elements across a given dimension? Looking through the API the closest result is `tf.unique_with_counts` ([link](https://www.tensorflow.org/versions/r0.9/api_docs/python/array_ops.html#unique_with_counts)) but this operation returns the values in order of occurrence so I first would need to find the index of the zero-element. The solution that I am currently using is not very elegant:\n\n``` python\nx = np.asarray([[0, 0, 1], [1, 0, 3], [1, 2, 3]])\n\na = tf.Variable(x)\nzero_elements = tf.equal(a, tf.zeros_like(a))\ncount_zeros_per_row = tf.reduce_sum(tf.to_int32(zero_elements), reduction_indices=1)\n\n# result => [2, 1, 0]\n```\n\nWould be nice to have operations to count the (non-)zero elements across a given dimension.\n", "comments": ["This is a question better suited for StackOverflow. Please ask it there and tag it with the `tensorflow` tag.\n"]}, {"number": 3146, "title": "eigen-eigen issue", "body": "Hi,\n\nI am trying to compile an example with C++ api, and I'm running into this issue:\n\n```\nIn file included from /Users/danieleghisi/max-sdk-7.0.3/source/fool/mains/dg/fool.rnn.c:53:\nIn file included from /Users/danieleghisi/LibrariesAndStuff/tensorflow/tensorflow/cc/ops/standard_ops.h:22:\nIn file included from /Users/danieleghisi/LibrariesAndStuff/tensorflow/tensorflow/cc/ops/array_ops.h:6:\nIn file included from /Users/danieleghisi/LibrariesAndStuff/tensorflow/tensorflow/core/framework/tensor.h:21:\nIn file included from /Users/danieleghisi/LibrariesAndStuff/tensorflow/tensorflow/core/framework/allocator.h:25:\nIn file included from /Users/danieleghisi/LibrariesAndStuff/tensorflow/tensorflow/core/framework/type_traits.h:22:\nIn file included from /Users/danieleghisi/LibrariesAndStuff/tensorflow/tensorflow/core/framework/types.h:23:\n/Users/danieleghisi/LibrariesAndStuff/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:10: fatal error: 'eigen-eigen-334b1d428283/unsupported/Eigen/CXX11/Tensor' file not found\n```\n\nI do not seem to have an \"eigen-eigen-334b1d428283\" folder anywhere. \nThere's an eigen.BUILD file at the root level, but I am unable to bazel it with \n`bazel build eigen`\nnor\n`bazel build tensorflow/eigen`\n(but it should be the first one, isn't it? eigen.BUILD being at the root level... Or how does bazel work in this respect?)\n\nShould I build that file via bazel? I guess I should. (but how do I build in general a xxxx.BUILD file, given that bazel build xxxx doesn't work?)\n\nThanks a lot,\ndaniele\n### Environment info\n\nOperating System: Mac OS X 10.10\nInstalled version of CUDA: 7.5\n", "comments": ["@danieleghisi Indeed, eigen-eigen-334b1d428283 is not a file, but refers to a commit timestamp in the Eigen repository.  bazel (magically) pulls a copy of Eigen when building tensorflow. The copy of Eigen ends up in bazel-tensorflow/external/eigen_archive/eigen-eigen-334b1d428283/.\n\nHowever, if you are trying to use the C++ TF API, I recommend that you start by cloning the code and build rules in tensorflow/cc.\n"]}, {"number": 3145, "title": "Add missing tf.tanh() tests for SparseTensor", "body": "More information in comment [here](https://github.com/tensorflow/tensorflow/pull/2998#issuecomment-229743288). Tested locally.\n", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 3144, "title": "Weird behavior with no skflow installation.", "body": "Environment info: Pycharm-Community Editon\nOperating System: Ubuntu-14.04-x64\nPip package: https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0-cp27-none-linux_x86_64.whl\nTensorflow version 0.9.0 (from python -c import tensorflow)\n\nWhen I was trying the text_classification examples from skflow repository (which are now moved to tensorflow), at line: `word_list = tf.unpack(word_vectors, axis=1)` it threw an error that says:\n`TypeError: unpack() got an unexpected keyword argument 'axis'`\n\nWhen I browsed the commit history on git, It says \n\n> 2 days ago: Switch from split_squeeze to tf.unpack where it makes sense.\n\nearlier it was working fine. Also it works fine when I run it on machine with skflow installed on it.\nI have used `tensorflow.contrib.learn as skflow` as directed on the tensorflow home page.But somehow it doesn't work.\n\nI have explored the `tf.unpack()` to check if it takes `axis` as an argument, but it doesn't. When I pass some other number that 1 to the `tf.unpack()` it says `num can not infer from shape (?,50,10)`\n", "comments": ["Have you had any luck on this?  I am having the same issue.\n", "Hard luck. It's not working. It's so worst that even after fresh installation it's still stuck there. And more importantly it works on the system with skflow installed on it. How's that possible?\n", "I'm having the same issue.  array_ops.py in the current install from pip differs from array_ops.py displayed here.  Any solution forthcoming?\n", "Update your tensorflow to 0.10.0 would help.\nJust follow the document of pip-installation:\nhttps://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#pip-installation\n", "It seems like this issue was just due to updated examples and old tensorflow. Updating tensorflow to any recent version would fix the issue.\nThanks!\n"]}, {"number": 3143, "title": "Misc improvements to Dockerfile.devel-gpu", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Can you please sign the CLA? I had to revert this one, my bad for not looking.\n", "I signed it! /CC @martinwicke \n", "Thanks!\n", "I'll revert the revert. \n"]}, {"number": 3142, "title": "i added my change", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}, {"number": 3141, "title": "fixed error in example syntax", "body": "Just a little nitpick...\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "oof never mind... \n"]}, {"number": 3140, "title": "update tensorflow learn readme", "body": "since `TensorFlowDNNClassifier`, `TensorFlowLinearClassifier`, `TensorFlowLinearRegressor` are all deprecated, use `DNNClassifier`, `LinearClassifier`, `LinearRegressor`\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks! They should be LinearRegressor instead of LinearRegression, same for Classifier. Could you change that? \n", "@terrytangyuan , I've changed as you mentioned\n", "@tensorflow-jenkins test this please\n"]}, {"number": 3139, "title": "Building tensorflow (makefile method) on arhmf Olimex A20", "body": "Building tensorflow using the makefile method, i get the below error, \n\n`PROTOC = \"protoc\"\nCC_PREFIX = \"\"\ngcc --std=c++11 -DIS_SLIM_BUILD -O0 -I/usr/local/include -I. -I/home/olimex/tf/tensorflow/tensorflow/contrib/makefile/downloads/ -I/home/olimex/tf/tensorflow/tensorflow/contrib/makefile/downloads/eigen-eigen-334b1d428283 -I/home/olimex/tf/tensorflow/tensorflow/contrib/makefile/gen/proto/ -I/home/olimex/tf/tensorflow/tensorflow/contrib/makefile/gen/proto_text/ \\\n        -o /home/olimex/tf/tensorflow/tensorflow/contrib/makefile/gen/bin/benchmark /home/olimex/tf/tensorflow/tensorflow/contrib/makefile/gen/obj/tensorflow/core/util/reporter.o /home/olimex/tf/tensorflow/tensorflow/contrib/makefile/gen/obj/tensorflow/tools/benchmark/benchmark_model.o /home/olimex/tf/tensorflow/tensorflow/contrib/makefile/gen/obj/tensorflow/tools/benchmark/benchmark_model_main.o \\\n         -Wl,--allow-multiple-definition -Wl,--whole-archive /home/olimex/tf/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a -L/usr/local/lib -lstdc++ -lprotobuf -lz -lm -ldl -lpthread\n/usr/lib/gcc/arm-linux-gnueabihf/4.8/libgcc.a(fp16.o): In function`__gnu_h2f_ieee':\n(.text+0x12a): relocation truncated to fit: R_ARM_THM_JUMP11 against symbol `__gnu_h2f_internal' defined in .text section in /usr/lib/gcc/arm-linux-gnueabihf/4.8/libgcc.a(fp16.o)\n/usr/lib/gcc/arm-linux-gnueabihf/4.8/libgcc.a(fp16.o): In function`__gnu_h2f_alternative':\n(.text+0x132): relocation truncated to fit: R_ARM_THM_JUMP11 against symbol `__gnu_h2f_internal' defined in .text section in /usr/lib/gcc/arm-linux-gnueabihf/4.8/libgcc.a(fp16.o)\n/usr/lib/gcc/arm-linux-gnueabihf/4.8/libgcc.a(fp16.o): In function`__gnu_h2f_ieee':\n(.text+0x12a): relocation truncated to fit: R_ARM_THM_JUMP11 against symbol `__gnu_h2f_internal' defined in .text section in /usr/lib/gcc/arm-linux-gnueabihf/4.8/libgcc.a(fp16.o)\n/usr/lib/gcc/arm-linux-gnueabihf/4.8/libgcc.a(fp16.o): In function`__gnu_h2f_alternative':\n(.text+0x132): relocation truncated to fit: R_ARM_THM_JUMP11 against symbol `__gnu_h2f_internal' defined in .text section in /usr/lib/gcc/arm-linux-gnueabihf/4.8/libgcc.a(fp16.o)\n/usr/lib/gcc/arm-linux-gnueabihf/4.8/libgcc.a(fp16.o): In function`__gnu_h2f_ieee':\n(.text+0x12a): relocation truncated to fit: R_ARM_THM_JUMP11 against symbol `__gnu_h2f_internal' defined in .text section in /usr/lib/gcc/arm-linux-gnueabihf/4.8/libgcc.a(fp16.o)\n/usr/lib/gcc/arm-linux-gnueabihf/4.8/libgcc.a(fp16.o): In function`__gnu_h2f_alternative':\n(.text+0x132): relocation truncated to fit: R_ARM_THM_JUMP11 against symbol `__gnu_h2f_internal' defined in .text section in /usr/lib/gcc/arm-linux-gnueabihf/4.8/libgcc.a(fp16.o)\ncollect2: error: ld returned 1 exit status`\n\nFrom what i understand this appears to be an issue with the linker trying to branch on a long address, \nhttps://lists.gnu.org/archive/html/bug-binutils/2007-06/msg00065.html\nBut this issue is back from 2007, any workaround i can do on the source to avoid this?\n\nSystem:\ngcc version 4.8.4 (Debian 4.8.4-1)\nGNU ld (GNU Binutils for Debian) 2.25\n", "comments": ["tried with gcc version 4.9.2\nsame result. can i not build the benchmark model? help!\n", "https://gcc.gnu.org/bugzilla/show_bug.cgi?id=71776\n", "Have you tried -mno-thumb in your compiler settings? From memory this helped when I was trying to cross-compile for the Pi (which I ended up giving up on in the end, and compiling natively on-device).\n", "i just tried -mno-long-calls and that appears to work so far. if it fails i'll try -mno-thumb. thanks!\n", "-mno-long-calls or -mlong-calls did not help. \n-mno-thumb is not an arm option (https://gcc.gnu.org/onlinedocs/gcc/ARM-Options.html)\n", "I managed to work around this by building gcc with --with-mode=arm\nIt appears to be a bug with [gcc](https://gcc.gnu.org/bugzilla/show_bug.cgi?id=71776) but it's not confirmed yet :/\nI did however run into a different issue right after #4172 :(\n", "Had some time to get to the bottom of this. The problem is with the use of **-Wl,--whole-archive** option without a **-Wl,--no-whole-archive** afterwards. \nEach library after **-Wl,--whole-archive** is included completely until **-Wl,--no-whole-archive** is specified and libgcc is automatically appended to the end of the library list, which means both libm.a and libgcc.a are included as a whole. \nThere are duplicates for a lot of functions between libm and libgcc. \nso when __gnu_h2f_ieee wants to link to __gnu_h2f_internal it has two options from libm and libgcc. This would typically cause the linking to fail, but since **-Wl,--allow-multiple-definitions** is specified the linker would link to the **first** definition. The first definition could be from the other library which would be further away from the program counter than the definition within the same library. \nThis would normally be alright with 64-bit instructions or even 32-bit instructions but with ARM 32-bit THUMB instructions the specific branch instruction only has a 12bit offset range, which is +-2k words. So it would fail trying to fit the offset into those 12bits. \nThe easy fix is to add **-Wl,--no-whole-archive** after libtensorflow-core.a. I believe this also removes the need for **-Wl,--allow-multiple-definitions**.\nOr if you're silly enough you could rebuild libgcc without thumb like i first did :/\nShould i send a PR for this?\n", "Thanks for your detective work on this, and for the update! It would make sense to add the -Wl,--no-whole-archive flag after every place where we use whole-archive to make sure other people don't hit this problem. A PR would be much appreciated.\n"]}, {"number": 3138, "title": "weird behavior on GPU ec2 - zero tensor instead of random one on local machine", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System: Ubuntu 14.04 \nLinux ip-172-31-7-130 3.13.0-88-generic #135-Ubuntu SMP Wed Jun 8 21:10:42 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n\nInstalled version of CUDA and cuDNN: cuda 7.5, cudnn 5.0.5\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n-rw-r--r-- 1 root root 189170 Jun 12 02:51 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Jun 12 02:51 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Jun 12 02:51 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Jun 12 02:51 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 Jun 12 02:51 /usr/local/cuda/lib/libcudart_static.a\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so.7.5 locally\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so.5.0.5 locally\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so.7.5 locally\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so.7.5 locally\n   0.8.0\n\nIf installed from sources, provide the commit hash:\n9b69ec3960cf9225df557fdc1ab673bd36bde4fb\n### Steps to reproduce\n\n1.\nimport tensorflow as tf\na=tf.Variable(tf.random_uniform([5], 0, 10, dtype=tf.int32, seed=10))\ninit = tf.initialize_all_variables()\nwith tf.Session() as sess:\n    sess.run(init)\n    result=sess.run(a)\n    print(result)\n    sess.close()\n\nthe result is: [0 0 0 0 0] \n2. If i run it locally (on my mac laptop where tensorflow was installed via pip - i dont have a gpu), i get: [ 33689671 570442370 201367012 671088640 671145444]\n3. I have python 3.5.1 , miniconda --version = \"conda 4.1.5\", run on EC2 , GPU K520\n### What have you tried?\n1. inside miniconda, outside it..\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["+1!\n", "switched to 0.9 and things worked\n", "Glad to hear that @dexter4455. Closing! We can reopen if this doesn't resolve it for @jarrydfillmore \n"]}, {"number": 3137, "title": "Fixes #2935 sklearn compatibility fix with cross_val_score", "body": "Fixed #2935 and added test. \n", "comments": ["Well, seems like we are not pursuing this anymore.\n", "If it's straightforward to fix - it may be worth it. I just thought there\nwas a lot of API changes that may have broken the compatibility.\n\nOn Sun, Jul 3, 2016 at 12:46 AM Yuan (Terry) Tang notifications@github.com\nwrote:\n\n> Closed #3137 https://github.com/tensorflow/tensorflow/pull/3137.\n> \n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/3137#event-711286398, or mute\n> the thread\n> https://github.com/notifications/unsubscribe/AAKtfq94PZs7sVOe0am5k3kYo89BR4wXks5qRupHgaJpZM4JCv7P\n> .\n", "Yeah this is straightforward. I've tested locally. Not sure why the test failed in Jenkins though. \n", "Seems like there's flaky test in learn/export_test under Linux CPU Python3 ([output](http://ci.tensorflow.org/job/tensorflow-pull-requests-cpu-python3/894/console)). Otherwise, seems ready to merge. @ilblackdragon \n", "@tensorflow-jenkins test this please\n", "Ping. Should be good to merge now. Seems like the flaky test has been fixed internally in #3241\n"]}, {"number": 3136, "title": "Fixed run_config example", "body": "- Switched to use `DNNClassifier`\n- Example runs forever if steps is not specified\n- Updated copyright info\n", "comments": ["Closing this since it's already changed from internal merge\n"]}]