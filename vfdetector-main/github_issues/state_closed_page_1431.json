[{"number": 10042, "title": "Warning for \\get_started TensorFlow Core tutorial - A custom model", "body": "### System information\r\n- **Have I used stock example script provided in TensorFlow from https://www.tensorflow.org/get_started/get_started\r\n\r\n- **OS Platform and Distribution : Ubuntu 16.04.2 LTS (GNU/Linux 4.4.0-78-generic x86_64)\r\n- **TensorFlow installed from binary\r\n- **TensorFlow version: ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')\r\n- **CUDA/cuDNN version**: Not using\r\n- **GPU model and memory**: Not using\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nKicks out an WARNING, \"WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\" when I am running one of the the getting started scripts.\r\n\r\nScript from https://www.tensorflow.org/get_started/get_started\r\n\r\n```import numpy as np\r\nimport tensorflow as tf\r\n# Declare list of features, we only have one real-valued feature\r\ndef model(features, labels, mode):\r\n  # Build a linear model and predict values\r\n  W = tf.get_variable(\"W\", [1], dtype=tf.float64)\r\n  b = tf.get_variable(\"b\", [1], dtype=tf.float64)\r\n  y = W*features['x'] + b\r\n  # Loss sub-graph\r\n  loss = tf.reduce_sum(tf.square(y - labels))\r\n  # Training sub-graph\r\n  global_step = tf.train.get_global_step()\r\n  optimizer = tf.train.GradientDescentOptimizer(0.01)\r\n  train = tf.group(optimizer.minimize(loss),\r\n                   tf.assign_add(global_step, 1))\r\n  # ModelFnOps connects subgraphs we built to the\r\n  # appropriate functionality.\r\n  return tf.contrib.learn.ModelFnOps(\r\n      mode=mode, predictions=y,\r\n      loss=loss,\r\n      train_op=train)\r\n\r\nestimator = tf.contrib.learn.Estimator(model_fn=model)\r\n# define our data set\r\nx = np.array([1., 2., 3., 4.])\r\ny = np.array([0., -1., -2., -3.])\r\ninput_fn = tf.contrib.learn.io.numpy_input_fn({\"x\": x}, y, 4, num_epochs=1000)\r\n\r\n# train\r\nestimator.fit(input_fn=input_fn, steps=1000)\r\n# evaluate our model\r\nprint(estimator.evaluate(input_fn=input_fn, steps=10))\r\n```\r\n\r\nAccording to page it should do this:\r\n\r\n```\r\nWhen run, it produces\r\n\r\n{'loss': 5.9819476e-11, 'global_step': 1000}\r\n\r\n```\r\n\r\nBut what I get is an error and an output that never appears the same...\r\n`WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp5JlIIa\r\n2017-05-19 13:24:03.968664: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-19 13:24:03.968718: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-19 13:24:03.968731: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nWARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\r\n{'loss': 2.003922e-11, 'global_step': 1000}\r\n`\r\n\r\nNext run:\r\n`\r\nWARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\r\n{'loss': 4.1125374e-11, 'global_step': 1000}\r\n`\r\nNext run:\r\n`\r\nWARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\r\n{'loss': 3.8334693e-11, 'global_step': 1000}\r\n`\r\nI am just learning machine learning and would like to continue on.  Not sure if this is a bug or a documentation error or what.", "comments": ["@wolffg @dr4b ", "You are not having a problem. From the documentation, \r\n```\r\nIf you run this program your loss will not be exactly the same, \r\nbecause the model is initialized with random values.\r\n```\r\n\r\nThat warning was unimportant, and it should have been removed via the current version of TensorFlow.\r\n\r\nThough an actual problem with the code you posted was that it was missing\r\n```\r\nimport numpy as np\r\n```\r\n\r\nThe TensorFlow documentations are in need for some updates. Too bad it is not open source for the community to fix.", "The documentation IS open source.  Please see this page for more information about contributing to TensorFlow's documentation: https://www.tensorflow.org/community/documentation\r\n\r\nNon API reference documentation can be found here to edit: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/docs_src", "Thank you!\r\n\r\nAnyway, I think this problem should be closed unless OP has any other questions or concerns.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly."]}, {"number": 10041, "title": "WIP: Python 3.6 build", "body": "", "comments": ["Experimental builds:\r\nhttp://ci.tensorflow.org/view/Experimental/job/experimental-cais-python36-linux-cpu/1/console\r\nhttp://ci.tensorflow.org/view/Experimental/job/experimental-cais-python36-linux-gpu/1/"]}, {"number": 10040, "title": "Virtual memory Leak when using gpu", "body": "Hello,\r\n\r\nI have little problem with virtual memory in tensorflow when running on GPU:\r\n\r\nWhen I run any network/graph, for example squeezenet:\r\n\r\n```\r\nargs = ...\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth=True\r\n\r\nwith tf.Session(config=config) as session:\r\n  model = SqueezeNet(args, session)\r\n  raw_input()\r\n```\r\n\r\nI get proper allocation when I look on nvidia-smi:\r\n\r\n```\r\n+-----------------------------------------------------------------------------+\r\n|  NVIDIA-SMI 367.48                 Driver Version: 367.48\r\n|  ....\r\n|   0  Quadro K620         Off  | 0000:11:00.0     Off |                  N/A |\r\n| 34%   35C    P8     1W /  30W |     65MiB /  1999MiB |      0%      Default |\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|    0     12128    C   python                                          63MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nHowever the top utility on the process looks very dangerous:\r\n```\r\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\r\n12128 xuser 20   0 64.696g 461928 162552 S   2.3  4.9   0:11.24 python\r\n\r\n```\r\n\r\n64 gigs in virtual memory is probably not normal, however when i run it on the CPU (just switching CUDA_VISIBLE_DEVICES to some big number):\r\n```\r\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\r\n12193 xuser 20   0 4468872 236988  74008 S   0.0  0.5   0:07.66 python\r\n\r\n```\r\nThen everything looks fine.\r\n\r\nDo you know where can be the problem?\r\n\r\nI am using tensorflow-gpu 1.1.0 on linux ubuntu16, cuda8, python2.7", "comments": ["@pbar can you comment or redirect to someone who can? Thanks!", "I found this on stackoverflow:\r\n\r\nNothing to do with scratch space, it is the result of the addressing system that allows unified andressing and peer to peer access between host and multiple GPUs. The CUDA driver registers all the GPU(s) memory + host memory in a single virtual address space using the kernel's virtual memory system. It isn't actually memory consumption, per se, it is just a \"trick\" to map all the available address spaces into a linear virtual space for unified addressing.\r\n\r\nSo probably it is just the behaviour of cuda....", "I'm closing this since it seems to be resolved, but please reopen if there is an issue."]}, {"number": 10039, "title": "Branch 155393864", "body": "", "comments": []}, {"number": 10038, "title": "Fix capitalization typo in release notes", "body": "`TensorFLowInferenceInterface` had an incorrectly capitalized \"L\" in it.\r\n\r\nWhile doing it, I formatted a couple of package names as code (since others seem to be).", "comments": ["Jenkins, test this please.", "@av8ramit FYI\r\nCherrypicking this as it is a small fix into RELEASES.md, but generally we block contributions into release branches."]}, {"number": 10037, "title": "Fix capitalization typo in release notes", "body": "`TensorFLowInferenceInterface` had an incorrectly capitalized \"L\" in it.\r\n\r\nWhile doing it, I formatted a couple of package names as code (since others seem to be).", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "D'oh, the commit was made while logged into the wrong account, retrying. :-("]}, {"number": 10036, "title": "Document functional differences between tf.stack and tf.parallel_stack", "body": "What if `tf.stack` had the API:\r\n\r\n```\r\ntf.stack(\r\n    values,\r\n    axis=0,\r\n    name='stack',\r\n    algorithm='sequential'\r\n)\r\n```\r\n\r\nOptions for algorithm could be `sequential`, `parallel` to start. This could apply broadly to many ops and ops like `parallel_stack` could be deprecated. \r\n\r\nAny ideas for a better parameter name than `algorithm`?", "comments": ["@yuanbyu ", "@ahundt `tf.parallel_stack` has strict limitations that `tf.stack` does not.  Not only do the shapes need to be known at graph build time for `tf.parallel_stack`, but this op ***does not support backprop*** (due to incompatibility between the in-place nature of its calculation and the functional requirements for enabling backprop on an op)!  In fact, this is more of a block temporary variable assign op than it is a stack op.  For this reason I prefer we keep this as an explicitly different operation.", "The fact that `parallel_stack` has no backprop support should be clearly documented, but it doesn't seem to be :(", "@alextp can you confirm?", "Yes, it should be documented that it has no backprop support.\r\n\r\nparallel_stack also has restrictions on what can be stacked (all shapes must be fully known I think)", "OK; so I'm going to rename this to \"document functional differences between tf.parallel_stack and tf.stack.\"", "PRs welcome.", "That's definitely important information, thanks! \r\n\r\nPerhaps either (1) my suggested rename, or (2) a rename to something like `block_until_data_arrives(layout='parallel_stack')` could still be made? \r\n\r\nIn addition to the clearer naming the limitation of the `parallel` vs `sequential` string option could of course still be documented.\r\n\r\nAlso, don't forget that I think there are still quite a few other pairs of ops which have similar names but differing functionality, I just chose `parallel_stack` as a good example!", "I submitted a PR #10593 to clarify things.\r\nTell me if you think the layout could direct the focus better.", "Despite the title change, I do still think specifying `algorithm='sequential'` and `algorithm='parallel'` params to `tf.stack()`, then deprecating `tf.parallel_stack()` would be cleaner. The description can of course specify all the useful information currently present in both docstrings, plus `'parallel' is faster for some use cases, but does not support backpropagation`.\r\n\r\nJust figured I'd plug the idea one more time, no worries if you decide to stick to a docstring only change. :-)", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Closing as this is resolved"]}, {"number": 10035, "title": "Fix capitalization typo in release notes", "body": "`TensorFLowInferenceInterface` had an incorrectly capitalized \"L\" in it.\r\n\r\nWhile doing it, I formatted a couple of package names as code (since others seem to be).", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "Can one of the admins verify this patch?", "Closing so I can reopen it while logged into the Google account which I have signed the Contributor License Agreement from."]}, {"number": 10034, "title": "Replace use of tensorflow::ops::ReadFile in label_image", "body": "Use tensorflow::Env instead of tensorflow::ops::ReadFile so that\r\nwe can avoid including whole_file_read_ops for Android.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 10033, "title": "ImportError: DLL load failed: The specified module could not be found.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: unknown, command fails (this is the problem)\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 8.0/5.1\r\n- **GPU model and memory**: GeForce GTX 970, 4 GB\r\n- **Exact command to reproduce**: import tensorflow as tf\r\n\r\n### Problem\r\nThis is something I've seen in a [previous issue report](https://github.com/tensorflow/tensorflow/issues/5949) added as a comment, but no one responded to it (except to say that you should be using cuDNN 5.1 instead of 6.0, which I am doing). I cannot even import tensorflow because it leads to the string of errors below. Someone in the linked issue suggested that this was caused by a missing MSVCP140.DLL, but this file is present in both my System32 and SysWOW64 folders (I have seen both mentioned). I have also installed the suggested Visual C++ Redistributable Update 3 with no luck.\r\n\r\n### Log\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Elijah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Elijah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Elijah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Elijah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Elijah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Elijah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Elijah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\Elijah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 51, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Elijah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Elijah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Elijah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Elijah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Elijah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Elijah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Elijah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'", "comments": ["Hey @mrry, any thoughts about this Windows build?", "It's almost certainly a missing directory from the `%PATH%` environment variable. The most common error is that the directory containing the cuDNN DLL is not in `%PATH%`, as it's different from where the rest of the CUDA DLLs are installed.", "@mrry, my `%PATH%` includes the directory I extracted my cuDNN download into (it contains nothing but the `cuda` folder at the root of the archive). I have also tried changing `%PATH%` to be this `cuda` subdirectory, but this produced the same result. Do I need to go even further and add the `cuda\\bin` subdirectory because that's what the DLL is in? Going to try that soon.", "Yes, you'll need to add the `cuda\\bin` subdirectory to get Windows to find the DLL (the DLL loading code doesn't search subdirectories, unfortunately). ", "I got the same error too. Was anyone able to fix this?\r\n\r\nEdit: I was able to fix this by downgrading cuDNN library to 5.1. My earlier install version was using cuDNN 6.0 release.", "I  have same problem . I have cudnn 5.1 and cuda-8.0. cudnn is copied in cuda's directory. I've also appended it's path to the system PATHs as:\r\n\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\bin\r\n\r\nAlso msvcp140.dll is included both in systemWow64 and system32 and I use Python 3.5.2.\r\n\r\nWhat is wrong? Have you faced the same problem? ", "Apologies for the late response, but what @mrry posted 8 days ago completely solved my issue. Thanks! Closing this now.", ":|", "5.1 instead of 6 worked for me as well. But what pisses me off big time is that it doesn't say which exact DLL didn't it find!", "Yes, it's unfortunate that the Win32 `LoadLibrary()` call doesn't produce a better error message! I've been mentioning this on some other issue threads, but I uploaded a script that can help to check common installation issues for TensorFlow on Windows. You can get it here:\r\n\r\nhttps://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c", "Same problem here, worked after changing to cuDNN5.1 instead of 6.0.\r\nwin10 x64\r\nPython 3.5\r\nTensorflow-gpu 1.1.0\r\n\r\n", "You do not need to downgrade from cuDNN 6.0 to 5.1. Making a symlink of cudnn64_6.dll, named as cudnn64_5.dll works just fine. It seems that TensorFlow hardcoded the dll file name.", "@jianglai Or how about keeping both versions? Symlink trick may be dangerous and misleading.", "@byronyi Sure I was just pointing out that 6.0 itself works. TensorFlow is hardcoded looking for the filename for 5.1.", "I solved this problem by duplicate a `cudnn64_6.dll` into `cudnn64_5.dll` in the `cuda\\bin` dictionary. Thx @jianglai ", "@QROST @jianglai Please be careful. You could download cuDNN v5.1 and place it in the same folder of cuDNN v6.0. Making alias of different versions is generally not considered as a good practice.", "In my case, `cudnn64_5.dll` wasn't found in the path. I did RTFM but TFM was wrong. i.e. [NVidia's instructions](http://developer2.download.nvidia.com/compute/machine-learning/cudnn/secure/v5.1/prod/doc/cudnn_install.txt) to install cuDNN are wrong (nvidia please fix if you see this)\r\n\r\nSo download cuDNN v5.1 (Jan 20, 2017), for CUDA 8.0 at https://developer.nvidia.com/rdp/cudnn-download \r\n\r\nThen do this:\r\n\r\n```\r\nALL PLATFORMS\r\n\r\n    Extract the cuDNN archive to a directory of your choice, referred to below as <installpath>.\r\n    Then follow the platform-specific instructions as follows.\r\n```\r\n\r\nBut then it's ... \r\n## Wrong\r\n```\r\nWINDOWS\r\n\r\n    Add <installpath> to the PATH environment variable.\r\n```\r\n\r\n## Correct\r\n```\r\nWINDOWS\r\n\r\n    Add <installpath>\\bin to the PATH environment variable.\r\n```\r\n\r\n", "I have the same problem too. I have done all the followings, but still does not work......\r\n\r\nInstalled Python 3.5.3\r\nInstalled CUDA 8.0\r\nInstalled cuDNN v5.1 (copied the cudnn64_5.dll into CUDA/bin)\r\nHave msvcp140.dll in both system32 and sysWOW64\r\nAdd the CUDA/bin path into %PATH%\r\nBoth dll files can be located using \"where\" in command prompt\r\n\r\nWhat do I miss?\r\n", "@mhyipa Assuming you just installed TensorFlow 1.3, you'll need to upgrade to cuDNN v6, and put `cudnn64_6.dll` in your `%PATH%`.\r\n", "@mrry Great thanks! I just struggled hard with the same issue.\r\nUpgraded TensorFlow (1.3) should go with cuDNN v6.0!", "Yes ... Tensorflow (1.3) does not go with cuDNN v7.", "@mrry Thank you so much for all the help and the self-check script!", "I have the same problem too. \r\nMy setup is as follows:\r\n\r\nInstalled Python 3.6.0\r\nInstalled CUDA 9.0 RC\r\nInstalled cuDNN v7\r\nAnd Installed TensorFlow version 1.3\r\n\r\nShould I downgrade?", "I did downgrade to CUDA 8.0.61 and cuDNN v6.0, tf 1.3", "Windows 10\r\nPython 3.6.3\r\nCUDA 8\r\ncuDNN v6\r\nTensorflow 1.3.0\r\nAdded CUDA/bin to path\r\nStill does not work. ", "@gauravk97 I saw the solution for CUDA 9 and cuDNN 7 on the internet. Please have a search if interested", "Currently this problem comes up when not using the latest version of tensorflow. Using 1.4.0rc0 fixed it for me.\r\nWindows 10,\r\nPython 3.5.0\r\ncuDNN v7\r\nTensorflow 1.4.0rc0\r\nworks, here.", "Hey @mrry I ran your script and got the following output:\r\n\r\nC:\\Users\\___\\Desktop>python tensorcheck.py\r\nERROR: Failed to import the TensorFlow module.\r\n\r\n- Python version is 3.6.\r\n\r\n- TensorFlow is installed at: C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\r\n\r\n- All required DLLs appear to be present. Please open an issue on the\r\n  TensorFlow GitHub page: https://github.com/tensorflow/tensorflow/issues\r\n\r\nIt says to open a new issue, but it's the same error as in this thread when I try to do import tensorflow.\r\n\r\nI have TF 1.3, CuDNN 5, 6, and 7 all on the path, and CUDA 8.0\r\n\r\ninstalled TF with pip instead of anaconda.", "@bopeton Can you try upgrading to the TF 1.4.0 release candidate and see if that fixes the problem (or gives a more useful error message)?", "@mrry \r\n\r\nThank you for responding!\r\n\r\nI solved the problem and it was much dumber than I thought. My SSD had run out of space while CUDA 8.0 was installing so there were problems with that. I re-installed CUDA and everything is fine now.", "I am beginner with tensorflow, having same problem. Can we run webcam detection with opencv without cuDNN?\r\n\r\nAlso, i am using tensorflow without GPU.\r\n\r\nLet me know please", "I had this problem and solved it thusly:\r\nAnaconda3 was installing tensorflow-gpu 1.1, while the latest version (with cuDNN 7 support) is 1.6. I had to manually update it and all of its dependencies, and that fixed it.", "just downgrade tensorflow version to 1.5...............wasted 4 days to get this right...................rest all worked fine automatically (all errors gone by this method)\r\nuse command \"pip install tensorflow==1.5\"", "I had similar issue when trying to run executable file built using pyinsatller. I resolved it by pasting the dll file and folder in the directory of the executable and it worked, I found which dll file inside ibm_db.py which was showing dll load failed: the specific module could not be found. Good luck", "Reinstalling nvidia driver fix this issue for me.\r\n", "tensorflow has some issues with python 3.6.\r\nTry installing using\r\npip install tensorflow in python 3.5.2.\r\nWorked for me", "Download and install numpy+mkl file. Currently you might have installed only the numpy file.\r\n\r\n1. Go to : https://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy\r\n\r\n2. Download the corresponding wheel file in which is suitable for your python installation\r\n\r\n3. pip install \"/path_to_thefile/numpy-1.14.6+mkl-cp36-cp36m-win_amd64.whl\"\r\n\r\nThis should help you", "I had this exact problem. I went to the TensorFlow GPU Support page and noticed that there is a required path variable I was missing `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin` After adding that path it worked perfectly for me.", "* Check out cuDNN & tensorflow versions\r\n* Include the path C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\<VERSION>\\bin", "Hello I am also facing same issues. For the first time the issue was of py_wrap module missing. I resolved it by installing visual c++ 2015. \r\nNow i am getting a different error \"\r\n  File \"C:\\Users\\vishw\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\"  \r\n\r\nand in the last this is what i got \"ImportError: DLL load failed: The specified module could not be found.\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\" . \r\nMy laptop has 1050gtx for which looked on nvidia supported cuda version and for my machine it was cuda 9.2 and cuDNN 7.4 version. Please help me out here. My nvidia driver version is 398.35. Any help will be appreciated.", "\r\n(TenKer) C:\\Users\\DELL7810>python\r\nPython 3.6.8 |Anaconda, Inc.| (default, Feb 21 2019, 18:30:04) [MSC v.1916 64 bi\r\nt (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\DELL7810\\Anaconda3\\envs\\TenKer\\lib\\site-packages\\tensorflow\\pyt\r\nhon\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\DELL7810\\Anaconda3\\envs\\TenKer\\lib\\site-packages\\tensorflow\\pyt\r\nhon\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\DELL7810\\Anaconda3\\envs\\TenKer\\lib\\site-packages\\tensorflow\\pyt\r\nhon\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\Users\\DELL7810\\Anaconda3\\envs\\TenKer\\lib\\imp.py\", line 243, in load_m\r\nodule\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\DELL7810\\Anaconda3\\envs\\TenKer\\lib\\imp.py\", line 343, in load_d\r\nynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\DELL7810\\Anaconda3\\envs\\TenKer\\lib\\site-packages\\tensorflow\\__i\r\nnit__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-im\r\nport\r\n  File \"C:\\Users\\DELL7810\\Anaconda3\\envs\\TenKer\\lib\\site-packages\\tensorflow\\pyt\r\nhon\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\DELL7810\\Anaconda3\\envs\\TenKer\\lib\\site-packages\\tensorflow\\pyt\r\nhon\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\DELL7810\\Anaconda3\\envs\\TenKer\\lib\\site-packages\\tensorflow\\pyt\r\nhon\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\DELL7810\\Anaconda3\\envs\\TenKer\\lib\\site-packages\\tensorflow\\pyt\r\nhon\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\DELL7810\\Anaconda3\\envs\\TenKer\\lib\\site-packages\\tensorflow\\pyt\r\nhon\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\Users\\DELL7810\\Anaconda3\\envs\\TenKer\\lib\\imp.py\", line 243, in load_m\r\nodule\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\DELL7810\\Anaconda3\\envs\\TenKer\\lib\\imp.py\", line 343, in load_d\r\nynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>>\r\n\r\nplease tell what is the issue...\r\n", "runfile('C:/Users/MG_14/Desktop/shiba kumar/MainEEGLearn/cnn_lstn_lib_eeg.py', wdir='C:/Users/MG_14/Desktop/shiba kumar/MainEEGLearn')\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-1-d0d50d859070>\", line 1, in <module>\r\n    runfile('C:/Users/MG_14/Desktop/shiba kumar/MainEEGLearn/cnn_lstn_lib_eeg.py', wdir='C:/Users/MG_14/Desktop/shiba kumar/MainEEGLearn')\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 786, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 110, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"C:/Users/MG_14/Desktop/shiba kumar/MainEEGLearn/cnn_lstn_lib_eeg.py\", line 4, in <module>\r\n    import matplotlib.pyplot as plt\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\", line 31, in <module>\r\n    import matplotlib.colorbar\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\colorbar.py\", line 32, in <module>\r\n    import matplotlib.artist as martist\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\artist.py\", line 16, in <module>\r\n    from .path import Path\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\path.py\", line 21, in <module>\r\n    from . import _path, rcParams\r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\n\r\n\r\n\r\n**_### This error was not solved after too many packages installed , anyone give me guidance to crush this error._** ", "> In my case, `cudnn64_5.dll` wasn't found in the path. I did RTFM but TFM was wrong. i.e. [NVidia's instructions](http://developer2.download.nvidia.com/compute/machine-learning/cudnn/secure/v5.1/prod/doc/cudnn_install.txt) to install cuDNN are wrong (nvidia please fix if you see this)\r\n> \r\n> So download cuDNN v5.1 (Jan 20, 2017), for CUDA 8.0 at https://developer.nvidia.com/rdp/cudnn-download\r\n> \r\n> Then do this:\r\n> \r\n> ```\r\n> ALL PLATFORMS\r\n> \r\n>     Extract the cuDNN archive to a directory of your choice, referred to below as <installpath>.\r\n>     Then follow the platform-specific instructions as follows.\r\n> ```\r\n> But then it's ...\r\n> \r\n> ## Wrong\r\n> ```\r\n> WINDOWS\r\n> \r\n>     Add <installpath> to the PATH environment variable.\r\n> ```\r\n> ## Correct\r\n> ```\r\n> WINDOWS\r\n> \r\n>     Add <installpath>\\bin to the PATH environment variable.\r\n> ```\r\n@mrry \r\nI'm using cuda 10.1 and cdnn 7.5 \r\nI have done this, I even changed the name of cudnn64_7.dll -> cudnn64_5.dll \r\nstill I am getting the following error \r\n**PLEASE HELP**\r\n\r\nPython 3.6.6 (v3.6.6:4cf1f54eb7, Jun 27 2018, 03:37:03) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"D:\\SETUP\\VISUAL STDS\\Python36_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\SETUP\\VISUAL STDS\\Python36_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\SETUP\\VISUAL STDS\\Python36_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"D:\\SETUP\\VISUAL STDS\\Python36_64\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"D:\\SETUP\\VISUAL STDS\\Python36_64\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"D:\\SETUP\\VISUAL STDS\\Python36_64\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"D:\\SETUP\\VISUAL STDS\\Python36_64\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"D:\\SETUP\\VISUAL STDS\\Python36_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"D:\\SETUP\\VISUAL STDS\\Python36_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\SETUP\\VISUAL STDS\\Python36_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\SETUP\\VISUAL STDS\\Python36_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"D:\\SETUP\\VISUAL STDS\\Python36_64\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"D:\\SETUP\\VISUAL STDS\\Python36_64\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "DLL Import Error can be resolved by installing latest c++\r\n[https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)\r\n\r\n\r\n\r\n", "Since the above is the solution, and to prevent future messages hiding it, I'm going to go ahead and lock the converstation."]}, {"number": 10032, "title": "add Cuda{2D,3D}LaunchConfig that maximizes occupancy", "body": "Add `Cuda2DLaunchConfig` and `Cuda3DLaunchConfig` that uses `cudaOccupancyMaxPotentialBlockSize` to calculate the best kernel launch parameters.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "I make some changes because I think it would also be necessary to check the physical limitation of each dimension:\r\n```c++\r\nint dev;\r\ncudaGetDevice(&dev);\r\ncudaDeviceProp deviceProp;\r\ncudaGetDeviceProperties(&deviceProp, dev);\r\nint xthreadlimit = deviceProp.maxThreadsDim[0];\r\nint ythreadlimit = deviceProp.maxThreadsDim[1];\r\nint zthreadlimit = deviceProp.maxThreadsDim[2];\r\nint xgridlimit = deviceProp.maxGridSize[0];\r\nint ygridlimit = deviceProp.maxGridSize[1];\r\nint zgridlimit = deviceProp.maxGridSize[2];\r\n```", "@zasdfgbnm Could you add a test for this?", "@rmlarsen I add and pass these tests. I also add some explanation of these `Cuda{2D,3D,}LaunchConfig` into `cuda_kernel_helper.h`. \r\n\r\nThe test is a `.cu.cc` file. To build it, I have to add something to `tensorflow.bzl`. I'm not sure if I'm doing the correct thing for this change.", "@zasdfgbnm thanks for the thorough test!\r\n@tensorflow-jenkins test this please", "@zasdfgbnm can you fix the build file formatting errors reported here:\r\n\r\n\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-sanity/4433/consoleFull", "@tensorflow-jenkins test this please", "Check return value of `cudaOccupancyMaxPotentialBlockSize`\r\nPlus some style fixes.\r\n\r\nAlthough github mark it as outdated, the issue https://github.com/tensorflow/tensorflow/pull/10032#discussion_r118041775 and my question https://github.com/tensorflow/tensorflow/pull/10032#discussion_r118129926  is still there.", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@zasdfgbnm can you please rebase to resolve conflicts?", "@rmlarsen done", "@tensorflow-jenkins test this please", "@zasdfgbnm The test failure looks like it's unrelated to your change, so I'll merge it as is. Thanks for the contribution."]}, {"number": 10031, "title": "[XLA] HLO Executor based backend example", "body": "Adding an example XLA device that uses the hlo evaluator to perform the execution.\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "@kayzhu @eliben \r\n\r\nDecided to make a new squashed version instead of the merge that included a whole lot of other stuff.\r\n\r\nIt passes some of the XLA unit tests (you would need pull request https://github.com/tensorflow/tensorflow/pull/9759 to be able to run the unit tests).\r\n\r\nIt crashes on a couple, and it hangs on the variable test, not sure why, breaking into it shows the evaluator executing binary ops.  but mostly it just complains errors out with unsupported ops.\r\n\r\nnevertheless - this pull request could be useful to you.\r\n\r\n", "Is \"plugin/example\" still the best name for this backend? Maybe call it something like \"evaluator\"?", "good point", "Very nice! I am still reading (sorry for the delay) so more to come :)\r\n\r\nOne immediate observation I have is I don't quite understand the meaning of \"plugin\" in this context. It seems me a more fitting place would be a executor/ backend similar to CPU/GPU, under service/ directory, but I will defer the final say to eliben@.", "It was originally a thing I created when I was trying to bootstrap my backend.  Originally we were hoping (as suggested by vrv) to make it into an actual loadable plugin.  However protobuf isn't really up to the job of working in that way, so I abandoned that earlier.  Nevertheless, the files remained in a directory called plugin.\r\n\r\nI quite like the isolation of the driver from the rest of XLA.\r\n\r\nBut it is as you wish.  Feel free to take this and run with it.  I've got plenty to do on my own company's backend really.  I don't suppose that they would be too happy for me to spend time on other things.", "@tensorflow-jenkins test this please", "@DavidNorman Thanks again for your contribution! I understand that this is a rather large, but otherwise self-contained change, so I'd like to reduce the burden for you as much as possible.\r\n\r\nIn that light, what do you think of we proceed as follows:\r\n1. fixing up the presubmit failures.\r\n2. fix potential major issues, if any.\r\n3. merge, and I can then fix up minor issues, if any.\r\n\r\n?\r\n\r\nIt's probably worth pointing out, we *may* later change the location/structure of this plugin, within reasonable bounds. If your internal code are dependents of them, they will eventually need to be modified as well to some degree.", "sure.   my backend doe not rely on this at all, although it is of a very similar structure in terms of file layout in the repo.\r\n", "I updated the API to match the current repo changes, and added your extra data types.", "Incidentally, when you run the CPU tests (or any test?) at the moment you get errors like these:\r\n\r\n```\r\n2017-05-26 09:25:47.991881: E tensorflow/compiler/xla/service/hlo_evaluator.cc:582] TryEvaluate failed:Unimplemented: unhandled HLO ops for HloEvaluator: pad.\r\n```\r\nI think that this is something to do with having it included in one of the optimizers?\r\n\r\nI don't think that the message should be listed as an error if it is going to appear regularly.\r\n", "Yes you are right, the error message that change has been reverted, and will be a VLOG going forward once it is re-committed.", "@tensorflow-jenkins test this please\r\n", "the error is in 'buildifier'.  does that mean that the BUILD file is not alphabetical or something similar?\r\n", "ah ha - i found the log (was hidden in a 'show whole log' thing) - will fix up", "@tensorflow-jenkins test this please\r\n", "As far as I can tell, there's only compile error to fix:\r\n\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-xla/1130/consoleFull\r\n\r\ntensorflow/compiler/plugin/executor/executable.cc: In function 'perftools::gputools::DeviceMemoryBase xla::executorplugin::AllocateOutputBuffer(perftools::gputools::executorplugin::ExecutorExecutor*, xla::Literal*)':\r\ntensorflow/compiler/plugin/executor/executable.cc:54:75: error: no matching function for call to 'xla::Literal::mutable_tuple_literals(tensorflow::int64&)'\r\n           AllocateSingleOutput(executor, literal->mutable_tuple_literals(n));\r\n                                                                           ^\r\n\r\n", "thanks.   not sure why this is failing", "but will get to the bottom of it\r\n", "oh i see.   will update to use the new Literal class", "Hi.  Can we go ahead with this now?  I suspect that the 'needs review' thing might be stopping some merging process from going ahead.\r\n\r\nCheers\r\n\r\n:)\r\n", "@tensorflow-jenkins test this please\r\n\r\n", "I don't believe that any of the failures generated by the last set of checks can be down to this code.  I have not changed the python API, I have not touched sparse tensors, and the other ones seem to be CI faults.", "Yea, they might be have been intermittent.\r\n\r\n@hawkinsp can we merge this change?", "@tensorflow-jenkins test this please\r\n", "you guys are up late", "ah - maybe the problem is that I have not integrated enough of the master to include some changes to the API\r\n", "Hi.\r\n\r\nThis commit:\r\n\r\n```\r\ncommit e6f58186363279496c46563e6f065ce7ea16c501\r\nAuthor: Bo Wang <david.b.wang@gmail.com>\r\nDate:   Mon Jun 5 11:41:32 2017 -0700\r\n```\r\n\r\ncontains addition to the public python API without updating the associated golden API list.  I don't know how it got through the smoke tests, but somehow it did.  ", "Not sure about the Windows CMAKE tests.  The failing ones are standard core python sparse ops tests.  I'm certain that your evaluator isn't being used as the device for these tests, because there is no XLA enabled. \r\n\r\nstrange", "the 'ci.tensorflow.org' is just a collection of the other ones - including the 2 failures.", "The Windows CMAKE test is failing for several other public pull requests (picked 3 at random).\r\n\r\nI have failed an Issue.\r\n", "I would say, looking at the history of the Windows CMAKE tests that it fails randomly, and has been doing so for quite a while.", "Yea, I'm currently looking at the failure in Linux CPU Tests:\r\n\r\n```\r\nERROR:tensorflow:TensorFlow API backwards compatibility test\r\nThis test ensures all changes to the public API of TensorFlow are intended.\r\n\r\nIf this test fails, it means a change has been made to the public API. Backwards\r\nincompatible changes are not allowed. You can run the test as follows to update\r\ntest goldens and package them with your change.\r\n\r\n    $ bazel build tensorflow/tools/api/tests:api_compatibility_test\r\n    $ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test \\\r\n          --update_goldens True\r\n\r\nYou will need an API approval to make changes to the public TensorFlow API. This\r\nincludes additions to the API.\r\n\r\nERROR:tensorflow:2 differences found between API and golden.\r\nERROR:tensorflow:Issue 1\t: Change detected in python object: tensorflow.\r\nERROR:tensorflow:Issue 2\t: New object tensorflow.LMDBReader found (added).\r\nF.\r\n======================================================================\r\nFAIL: testAPIBackwardsCompatibility (__main__.ApiCompatibilityTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/tools/api/tests/api_compatibility_test.py\", line 229, in testAPIBackwardsCompatibility\r\n    update_goldens=FLAGS.update_goldens)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/tools/api/tests/api_compatibility_test.py\", line 188, in _AssertProtoDictEquals\r\n    self.fail('%d differences found between API and golden.' % diff_count)\r\nAssertionError: 2 differences found between API and golden.\r\n```\r\n\r\nI do notice tensorflow/tools/api/tests:api_compatibility_test passing from the tip of the tree. I wonder if you could integrate the tip and re-run the tests? ", "@tensorflow-jenkins test this please\r\n", "Regarding the ABI compatibility failure: I was told it has been fixed this morning, so syncing to the tip and re-run should make that failure go away.", "@martinwicke the existing failures Linux CPU Tests (Python 3) and Windows Cmake Tests (https://ci.tensorflow.org/job/tensorflow-pull-requests-multijob/5415/) seems to be flaky.\r\n\r\nIs it safe to merge? And if so, could you merge? Thanks!"]}, {"number": 10030, "title": "Fix comments for dataset ops", "body": "@mrry ", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please."]}, {"number": 10029, "title": "about tf.train.ExponentialMovingAverage", "body": "Hi,I met a little problem when use this API.\r\n\"/biased does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\"\r\nAnd I saw someone written same problem in this issue https://github.com/tensorflow/tensorflow/issues/5827\r\nbut in comments of the issue,I did not find a efficient method to solve this problem.\r\nBy the way,a code ,such as \r\n```\r\ndef f(v):\r\n    ema = tf.train.ExponentialMovingAverage(0.9)\r\n    vema = ema.apply([v])\r\n    return vema\r\n\r\nwith tf.variable_scope('s'):\r\n    v1 = tf.get_variable('W', shape=[])\r\n    v1 = v1 + 1\r\n    f(v1)\r\nwith tf.variable_scope('s', reuse=True):\r\n    v2 = tf.get_variable('W', shape=[])\r\n    v2 = v2 + 2\r\n    `f(v2)```\r\n```\r\ncan normally run with tensorflow 0.11 but can't run with tensorflow 0.12 or higher version.\r\nI hope there will be a valid method to solve this problem.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 10028, "title": "s/training_op/train_op/", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 10027, "title": "Merge pull request #1 from tensorflow/master", "body": "roll on updates from main branch", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->"]}, {"number": 10026, "title": "Failed to load the native TensorFlow runtime : error while importing tensorflow", "body": "**I built TensorFlow from source on my Ubuntu 17.04 32bit**\r\n\r\nI got this message while importing tensorflow\r\n\r\npalash@ash:~$ python\r\nPython 3.6.0 |Anaconda 4.3.1 (32-bit)| (default, Dec 23 2016, 12:22:10) \r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\nFile \"/home/palash/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 61, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/palash/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"/home/palash/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\n  File \"/home/palash/anaconda3/lib/python3.6/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/palash/anaconda3/lib/python3.6/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /home/palash/anaconda3/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /home/palash/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow.so)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/palash/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/palash/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/palash/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 61, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/palash/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"/home/palash/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\n  File \"/home/palash/anaconda3/lib/python3.6/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/palash/anaconda3/lib/python3.6/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /home/palash/anaconda3/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /home/palash/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow.so)\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>> \r\n\r\nmy bazel info:\r\npalash@ash:~$ bazel version\r\nBuild label: 0.4.5- (@non-git)\r\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri May 19 01:31:26 2017 (1495157486)\r\nBuild timestamp: 1495157486\r\nBuild timestamp as int: 1495157486\r\n\r\nmy TensorFlow version : 1.0.1", "comments": ["Are you running on the same system you built TF on?\r\nUbuntu 17.04 should have a much more recent version of gcc, but this line in your python output is confusing:\r\n```\r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\r\n```\r\nThat is quite an old GCC version. Maybe the python coming with Anaconda is the problem. Did you try installing python through Ubuntu package manager?\r\n", "Yes. I am running it on same system i built it. \r\nI've gcc 6.3 installed on my Ubuntu ", "@jhseu Is this similar to the problems we saw in ubuntu 16.10, with newer glibc versions?", "The problem is that anaconda is including libstdc++:\r\n`/home/palash/anaconda3/bin/../lib/libstdc++.so.6`\r\n\r\nYou're building with your system libstdc++.so.6, but using an older version when trying to run it.", "how can i solve this?", "We recommend uninstalling anaconda, and installing python, numpy, scipy and all other python requirements through APT, as described in our installation documentation:\r\nhttps://www.tensorflow.org/install/install_linux#InstallingNativePip", "I have a similar issue when I run \"import tensor flow as tf\" command. I get the below message:\r\n\r\n(tensorflow) C:\\myAnaconda\\Anaconda3>python\r\nPython 3.5.3 |Anaconda custom (64-bit)| (default, May 15 2017, 10:43:23) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information. \r\n/>import tensorflow as tf\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\myAnaconda\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\myAnaconda\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 919, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\myAnaconda\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\myAnaconda\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\myAnaconda\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\myAnaconda\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\myAnaconda\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\myAnaconda\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\myAnaconda\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\myAnaconda\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\myAnaconda\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 919, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\myAnaconda\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\myAnaconda\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\myAnaconda\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\myAnaconda\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSomeone please point me in the right direction.", "I got the same error.\r\n\r\nimport tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /home/cloudera/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n", "this works for me: `conda install -c https://conda.anaconda.org/jjhelmus tensorflow\r\n`", "running python using sudo helped me.", "@sepideh68  thanks for the great solution. It saved hours of time for me. ", ">>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<pyshell#0>\", line 1, in <module>\r\n    import tensorflow\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "Thanks @amanrana20, using sudo was a quick fix!", "> conda install -c https://conda.anaconda.org/jjhelmus tensorflow\r\n\r\nThis worked for me too but all the time I have Warning message !! "]}, {"number": 10025, "title": "Fix some doc format for CropAndResize", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 10024, "title": "Text summary: Error encountered when serializing __tensorboard_plugin_asset__tensorboard_text.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux CentOS 7\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: `v1.1.0-rc2-1185-ge4f5123' 1.2.0-rc0`\r\n- **Bazel version (if compiling from source)**: `0.4.5`\r\n- **CUDA/cuDNN version**: `8.0` / `5.1.10`\r\n- **GPU model and memory**: GeForce GTX Titan X 12GB\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nlogdir = \"/output_dir\"\r\ntext = \"A text summary\"\r\ntext_tensor = tf.constant(text)\r\ntext_summary = tf.summary.text(\"Summary\", text_tensor)\r\nmerged = tf.summary.merge([text_summary])\r\nwriter = tf.summary.FileWriter(logdir, tf.get_default_graph())\r\ns = tf.Session()\r\ns.run(tf.global_variables_initializer())\r\nsummary = s.run(merged)\r\nwriter.add_summary(summary)\r\n```\r\n\r\n### Describe the problem\r\nTensorflow  gives the following error when I try to create a summary writer when the graph has a text summary:\r\n```\r\nWARNING:tensorflow:Error encountered when serializing __tensorboard_plugin_asset__tensorboard_text.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef.\r\n'TextSummaryPluginAsset' object has no attribute 'name'\r\n```\r\nOn my larger project I was able to output some text to the summary page on tensorboard, but the Markdown I put in there wasn't being parsed at all. I don't know if these problems are related but I feel like they probably are.\r\n\r\n### Source code / logs\r\nCode in first section is an MWE\r\n", "comments": ["@dandelionmane @jart ", "I have the same issue. It appears, when I store a checkpoint of my model.\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\nTensorFlow version (use command below): tensorflow_gpu-1.2.0-cp35-cp35m-win_amd64.whl  from PIP", "Ah, this is caused by an interaction between the way text summary was implemented and the metagraphdef. The warning doesn't actually break anything, it's just a warning. \r\n\r\nThis is unrelated to any markdown parsing issues.\r\n\r\n@chihuahua just submitted a PR internally that fixes this by using a different mechanism for the text summary. The fix will be in tf 1.3. Closing this since the issue is fixed at head."]}, {"number": 10023, "title": "Error while executing TF program with TF_CPP_MIN_VLOG_LEVEL=1", "body": "I got this error when I exported TF_CPP_MIN_VLOG_LEVEL=1 and TF_CPP_MIN_LOG_LEVEL=1 to see the LOG files . I got this error.\r\n> SetArgvFromEnv\r\n> [libprotobuf ERROR external/protobuf/src/google/protobuf/io/zero_copy_stream_impl_lite.cc:173] Cannot allocate buffer larger than kint32max for StringOutputStream.\r\n> 2017-05-19 10:13:31.716688: F tensorflow/compiler/tf2xla/dump_graph.cc:67] Non-OK-status: WriteTextProto(Env::Default(), path, graph_def) status: Failed precondition: Unable to convert proto to text.\r\n> Aborted\r\n", "comments": ["Your graph is too large to dump out. Just comment out that line or don't use VLOG."]}, {"number": 10022, "title": "Update 1_notmnist.ipynb", "body": "The url in the file will not redirect to https. i faced this problem so changing it for someone who will see it in future. \r\ni think port 80 is blocked on that server but 443 is going is going through. ", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "The url in the file will not redirect to https. i faced this problem so changing it for someone who will see it in future.\r\ni think port 80 is blocked on that server but 443 is going is going through.\r\nI changed and it working after it.", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "I signed it!", "I signed it!\n\nOn Fri, May 19, 2017 at 9:06 AM googlebot <notifications@github.com> wrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project. Before we can look at your\n> pull request, you'll need to sign a Contributor License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed, please reply here (e.g. I signed it!) and we'll\n> verify. Thanks.\n> ------------------------------\n>\n>    - If you've already signed a CLA, it's possible we don't have your\n>    GitHub username or you're using a different email address. Check your\n>    existing CLA data <https://cla.developers.google.com/clas> and verify\n>    that your email is set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - If you signed the CLA as a corporation, please let us know the\n>    company's name.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/10022#issuecomment-302600190>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AA9zFJFXbpMRGj6hrjCffr2PSJRc9H2iks5r7Q44gaJpZM4NgCpO>\n> .\n>\n", "@tensorflow-jenkins test this please"]}, {"number": 10021, "title": "Class weighting in tf.losses.softmax_cross_entropy", "body": "Feature request\r\n\r\nIn `tf.losses.softmax_cross_entropy` there's an optional field `weights`. I assumed this field was used for assigning a different weight to each class, but it actually is used to assign a weight to each sample in the batch. In my use case i have a `batch_size` of `128` and `30` classes, so I was passing a `[1, 30]` tensor and got this error:\r\n\r\n```\r\nInvalidArgumentError (see above for traceback): Incompatible shapes: [128] vs. [30]\r\n\t [[Node: optimizer/gradients/cross_entropy/softmax_cross_entropy_loss/Mul_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](optimizer/gradients/cross_entropy/softmax_cross_entropy_loss/Mul_grad/Shape, optimizer/gradients/cross_entropy/softmax_cross_entropy_loss/Mul_grad/Shape_1)]]\r\n```\r\n\r\nI looked at the implementation, confirmed that the function expects `batch_size` as the dimension of thensor and realized that my expected behavior cannot be achieved easily as `tf.nn.softmax_cross_entropy_with_logits` doesn't have a weight parameter.\r\n\r\nMy current workaround solution is to re-implement this function calculating the loss for each class and then multiplying for the weight vector that I pass, but that's inefficient compared to the optimized implementation of `tf.nn.softmax_cross_entropy_with_logits`.\r\n\r\nSo my request is:\r\n- provide an optimized `tf.nn.softmax_cross_entropy_with_logits` that also accepts `weights` for each class as a parameter\r\n- use it inside `tf.losses.softmax_cross_entropy` so that one can pass weights as a scalar, a `[batch_size, 1]` tensor, a `[1, num_classes]` tensor or a `[batch_size, num_classes]` tensor (the same dimension of  `onehot_labels`)", "comments": ["Thanks for the suggestion @w4nderlust. Marking this as \"Contributions Welcome\" for now.", "Personally I am opposed to such an extension -- the ``weights`` argument of ``tf.losses.softmax_cross_entropy`` can easily achieve class weighting (by using `tf.gather` on the class weights and class indices), using one additional line of code.\r\n\r\nYes, the `tf.gather` requires additional time and space, but linear in size of the output layer -- most networks spend several orders of magnitude more time in the computations before the output layer, so the effect of the suggested optimalization is minor (while requiring API change).", "@foxik my current solution is:\r\n```\r\nsample_weights = tf.reduce_sum(tf.multiply(self.one_hot_labels, class_weights), 1)\r\n```\r\nwhich is similar to `tf.gather`, but both are workarounds. I believe that allowing also for class weighting in the loss is a more general solution.\r\n", "@w4nderlust I see it the opposite way -- the current solution is a more general one. It allows class weighting as a simple case, but also allows weighting the instances individually (this is sometimes needed if you either are given weights beforehand, or if you for example want the same sum of positive/negative examples to be the same, for example).", "@foxik when I say general I refer to the interface, not the capabilities.\r\n\r\nMy proposed solution allows both class and instance weighting, so it allows exactly the same capabilities that are available now, but in simpler and more straightforward way, without requiring workarounds.\r\n\r\nMoreover, what's different in my proposal is the interface, it would become more general if it would accept all possible formats (scalar, [batch_size], [class_size] and [batch_size, class_size]) instead of accepting only [batch_size] (and I guess also scalar, haven't tried it) like it does now.", "I am not sure what the meaning of `[batch_size, class_size]` is (because there are only `batch_size` instances, but you are giving `batch_size * class_size` weights).\r\n\r\nFrom the API point of view, my personal oppinion is that you should not mix both instance-based weights and class-based weights in one argument -- I would instead keep `weights` as it is and add `class_weights` argument, if I were to proceed with the request.\r\n\r\nHowever, when designing APIs, there is a cost for extending it -- too rich API methods and arguments are not always better (simplicity is also an important factor). As a rule of thumb, if required functionality can be achieved by a line of code without harming performance, I personally do not believe the API should be extended (unless the functionality is used by everyone all the time).\r\n\r\nBut I respect that you have another oppinion -- lets wait for someone else to step in.", "@foxik [batch_size, class_size] is the size of both the logits and the onehot_labels we already passing to the softmax. I believe you should be able to pass weights with the same size. and if they happen to be just [bastch_size] then they should be broadcasted to each class, or if they are only [class_size] they should be broadcasted to each example in the batch, and finally if you pass a scalar it should be broadcasted in both batch_size and class_size. Hope this is more clear now.\r\n\r\nAdding an additional class weights parameter would be ok, but if you multiply the class weights and the batch weight parameter you get the [bach_size, class_size] weight matrix I'm talking about. Either way would be fine IMHO.\r\n\r\nI believe that an optional parameter, if useful, does no big harm and doesn't make the API less simple, it may actually make the model's code slightly simpler. Anyway, I understand that the effort to implement this may not be worth both the rather small advantage that it would give, but I'm proposing a feature I'd like to have, it's not up to me to decide how important or urgent it is :) I respect your point of view too anyway.", "@w4nderlust  implementing [`\"info gain\"](http://caffe.berkeleyvision.org/doxygen/classcaffe_1_1InfogainLossLayer.html#details) loss should give you the ability to have class weights as a special case of the more general infogain loss.", "@w4nderlust This solution works well! But I do not understand your code. My code is below:\r\n\r\n`class_weights = tf.multiply(one_hot_labels, class_weights)`\r\n`loss_crossentropy = tf.losses.softmax_cross_entropy(class_weights,tf.reshape(logits, (-1,classnum)))`\r\nwhere the shape of class_weights is [1, num_classes]", "@shaibagon: I already have something that works, but having something like that implemented inside tf would me more elegant I guess.\r\n\r\n@JackieLeeTHU11: thanks for the suggestion. I think you are computing something different anyway. Multiplying the weights for the one hots you're asking your model to predict those weights and penalize it if doesn't. In my case I still want the model to predict ones and zeros, but I want to weight the loss for some classes lower.", "In my loss function I would like to weight each sample differently and in each sample, each category should be weighted differently as well depending on the outcome. Meaning if in a cross entropy the one_hot is correctly specified, a different weight needs to be applied than when the output is incorrect. So I would need two weights per category. A tensor with rank 3. One dimension for the samples, a second dimension for the amount of classes (let's assume just one output neuron for simplicity), and a third dimension that differentiates between correct and incorrect match for that one binary output neuron.\r\n\r\nIs there any way to do that with one of the above loss functions?", "https://stackoverflow.com/a/46984951/8906149\r\nThis answer works fine for me~~\r\n", "@zihuaweng \uff0cHi zihua, \u540c\u6837\u6837\u672c\u52a0\u6743\u7684\u95ee\u9898\uff0c\u4e0d\u77e5\u9053\u5982\u4f55\u5c06sample_weight\u4f20\u7ed9custom model,\u611f\u8c22\uff01 https://stackoverflow.com/questions/49312839/how-to-set-parts-of-positive-samples-weight-in-tensorflow-for-binary-classficati\r\n", "This can be done in keras.", "If anything, that looks more like an additional reason to have it in the lower level APIs", "Currently while implementing weighted loss there is tf.nn.weighted_cross_entropy_with_logits which uses sigmoid so it is restricted to binary classification. For multiclass classification using softmax categorical cross entropy, there isn't any feature.\r\n\r\nAlso in many forums, while implementing the class weights people often suggest something like this:\r\n\r\nweighted_logits = tf.multiply(weights, logits)\r\nloss = tf.nn.softmax_cross_entropy_with_logits_v2\r\n\r\nIs this the correct method? Because we are messing around with the output of the model, while we should have been messing around with the calculated [batch_size, num_class] shape loss tensor.\r\n\r\nI think this is the correct way:\r\n        y_hat_softmax = tf.nn.softmax(logits)\r\n        y_cross = y * tf.log(y_hat_softmax)\r\n        weighted_cross = tf.multiply(class_weight, y_cross)\r\n        weighted_cross_reduced = - tf.reduce_sum(weighted_cross, 1)\r\n        loss = tf.reduce_mean(weighted_cross_reduced)\r\n\r\nHere we are changing the y_cross which should be actually changed, which then changes the gradient flow. But changing logits (by multiplying with weight) will make the model understand that the correct output is different thus biasing result.\r\n\r\neg. If a model has three classes and the output logit is [1, 4, 2] then it is making the correct decision if the actual class is the second one. But multiplying with weight [1, 1, 4] will make the result [1, 4, 8] thus saying that the third logit should have been high to the loss function, thus completely changing the supervision in supervised learning. But actually, we should have been messing around with the loss by scaling the loss according to the weights. \r\n\r\nCould you guys verify this?", "@bipinkc19 If you do the maths, you'll see that class weighting is strictly equivalent to sample weighting.  Cross entropy loss for a given sample with class `c` is defined as (in pseudocode):\r\n\r\n```loss = sum(w[i] * y[i] * log(p[i]))```\r\n\r\nwhere `w[i]` is the weight for the `i`th class, `y` is the one-hot encoding of `c`, and `p` is the softmax probabilities.  Knowing that the class of that sample is `c`, then we have that `y[c] = 1`, and `y[i] = 0` for `i != c`.  Hence:\r\n\r\n```loss = w[c] * log(p[c])```\r\n\r\nAll the other class weights have vanished.  The loss (and thus, its gradient) only depend on `w[c]`, and so class weights can be expressed as sample weights.", "@xlambein you are assuming that all datapoints from the same class should be weighted the same, which is not true in the most general case. Your math should be: `loss = sum(w_s[i] * w_c[y[i]] * y[i] * log(p[i]))` where `w_s[i]` is the weight of the i-th datapoint and `w_c[y[i]]` is the weight of the class of the i-th datapoint.\r\nAs already discussed several times in this thread you can combine class and sample weight together before providing them to the current api, but that doesn't need to be the case, the multiplication could be done inside the provided api loss computation. It's a matter of api design.", "@w4nderlust I think you did not understand the purpose or the content of my post.  I don't have an opinion about API design.  I was just pointing out that a \"weighted cross entropy\", as suggested by @bipinkc19, actually reduces to sample weights, since all but one weight end up disappearing from the expression.  Hence, the implementation they suggested is not more expressive than the current implementation in TensorFlow (and is slower, since it's not optimized).\r\n\r\nIf you read carefully, you'll see that I'm specifically talking about \"loss for a given sample\".  When I iterate over `i` in my sum, I iterate over the classes, not the data points.  If you generalize over multiple samples, you need to wrap that sum into another sum, but that doesn't change the validity of my statement.\r\n\r\nAs for what you are talking about: as you said, it's a matter of API design.", "@xlambein @w4nderlust Thanks for clearing out that class weights by the method mentioned by me converts to sample weights. But it still doesn't explain why most implementation are done this way:\r\n```weighted_logits = tf.multiply(weights, logits)```\r\n```loss = tf.nn.softmax_cross_entropy_with_logits_v2```\r\nBecause, we are messing around with the output logits and changing its scalar value. \r\n1. Isn't the method of class weight to sample weight okay to do?\r\n2. Even when we want the class weights to apply to all other classes like you said by multiplying it with the output logits. We are changing the ouput value for the model. I think it might disrupt the whole process like.\r\nIf a model has three classes and the output logit is [1, 4, 2] then it is making the correct decision if the actual class is the second one. But multiplying with weight [1, 1, 4] will make the result [1, 4, 8] thus saying that the third logit should have been high to the loss function, thus completely changing the supervision in supervised learning. But actually, we should have been messing around with the loss by scaling the loss according to the weights.\r\n\r\nCould you guys also mention how you do class weights implementation. :)", "@bipinkc19 I agree with you, weighting should be applied after the softmax to influence the gradients rather than at the logits / output level. In my implementation I multiply after computing the cross entropy and before aggregating (so the output of the cross entropy is not reduced, I multiply by weights for each datapoint based on class and specific datapoint, and finally I sum and divide by the batch size.\r\n\r\n@xlambein I understood both the purpose and the content of your post. The fact that the provided weighting mechanism is as expressive as needed for both class and sample weighting was clear from the beginning of this discussion. The discussion is about what is the best api design. As you said, multiplying outside the `softmax_cross_entropy_with_logits` function is non optimized, so a better interface providing a parameter for both weighting schemes could encapsulate a more optimized implementation imho.", "@w4nderlust Maybe it was clear for you, but it wasn't for me.  I encountered a situation where I wanted to weight my loss by a `[batch_size, n_classes]` tensor, and found this thread.  My first intuition was to think the current implementation was limited in expressiveness, and that intuition was compounded by the fact that you suggested a richer API.  Only when I did the maths myself did I realize it wasn't the case.  Hence, since @bipinkc19 seemed to have the same problem as me, I thought it would be useful to share the short math that explains why `[batch_size, 1]` weights are just as expressive as `[batch_size, n_classes]` weights.\r\n\r\n@bipinkc19 I would do the loss with class weight the way it's suggested at the start of the thread, with `tf.gather` (not tested):\r\n```python\r\nclass_weights = tf.constant([1., 2., 3., 4.])  # 4 classes\r\nsample_weights = tf.gather(class_weights, labels)\r\nloss = tf.losses.sparse_softmax_cross_entropy(labels, logits, weights=sample_weights)\r\n```\r\nOr alternatively, with one-hot labels, you can use matrix multiplication to compute the sample weights (multiply `[batch_size, n_classes]` one-hot labels with `[n_classes, 1]` class weights to get `[batch_size, 1]` sample weights), and compute the loss with `tf.losses.softmax_cross_entropy`.\r\n\r\nI'm not sure I understand what you're asking in (1).  As for (2), I think you're right that it's much better to change the loss than to mess with the logits.  That being said, I'm not sure how bad it would actually be.  In the case of your example, your model might just learn to output a smaller logit for the third class.  I'm too much of a novice to understand the impact of that."]}, {"number": 10020, "title": "undefined symbol: cuDevicePrimaryCtxRetain", "body": "Ubuntu 14.04    / NVIDIA GTX 1060\r\n\r\nI try nvidia 710 with Ubuntu 14.04,s ok!\r\nI try nvidia 1060 with Ubuntu 14.04, Fault! -- undefined symbol: cuDevicePrimaryCtxRetain\uff01\r\n\r\nNVIDIA-Linux-x86_64-375.66.run\r\ncuda_8.0.61_375.26_linux.run\r\nCuDNN5.1\r\nAnaconda3-4.3.1-Linux-x86_64.sh \uff08Python3.6\uff09\r\nTensorflowGPU1.1.0\r\n\r\nI have install this profile in one computer( not the same hardware) success.\r\nBut, today, another computer.\r\nI get that:\r\n\r\nimport tensorflow as tf\r\nTraceback (most recent call last):\r\nFile \"/home/student/miniconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in \r\nfrom tensorflow.python.pywrap_tensorflow_internal import *\r\nFile \"/home/student/miniconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in \r\n_pywrap_tensorflow_internal = swig_import_helper()\r\nFile \"/home/student/miniconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nFile \"/home/student/miniconda3/lib/python3.6/imp.py\", line 242, in load_module\r\nreturn load_dynamic(name, filename, file)\r\nFile \"/home/student/miniconda3/lib/python3.6/imp.py\", line 342, in load_dynamic\r\nreturn _load(spec)\r\nImportError: /home/student/miniconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: cuDevicePrimaryCtxRetain\r\n", "comments": ["Please provide the full information required by the issue template.\r\nAlso, just searching through issues with your error, I see some related issues.\r\nDid you search and try the suggestions in those issues?\r\nhttps://github.com/NVIDIA/nvidia-docker/issues/262", "This looks like it was previously reported in https://github.com/tensorflow/tensorflow/issues/9866", "Closing due to inactivity, and possible duplicate."]}, {"number": 10019, "title": "Java Api String tensors support", "body": "from [Tensor.java](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/java/src/main/java/org/tensorflow/Tensor.java#L90):\r\n**non-scalar DataType.STRING tensors are not supported yet**\r\n\r\nIs there a plan for adding them for the Java interface?", "comments": ["Closing this as a duplicate of #8531 \r\n@erranli was working on this, but if not, it's something we can add.\r\n"]}, {"number": 10018, "title": "Disable batch_ops_test on pip", "body": "", "comments": ["@alextp weird buildifier error. <tab> instead of space?", "Jenkins, test this please.", "@yifeif @wicke I don't have permission to merge this. Can you?", "Let's bring this back to master after release. cc @av8ramit "]}, {"number": 10017, "title": "tensorboard fails with `Fatal error in launcher: Unable to create process using '\"'`", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A, tensorboard command line issue\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 x64\r\n- **TensorFlow installed from (source or binary)**: binary with Python 3.5, installed via pip\r\n- **TensorFlow version (use command below)**: b'unknown' 1.1.0 (gpu version of TF)\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA 8, cuDNN 5.1.10\r\n- **GPU model and memory**: GTX 1080ti 11GB\r\n- **Exact command to reproduce**: run `tensorboard`, get `Fatal error in launcher: Unable to create process using '\"'`\r\n\r\n### Describe the problem\r\n\r\nRunning `tensorboard` fails with `Fatal error in launcher: Unable to create process using '\"'` no matter the command line arguments I pass.", "comments": ["Are you passing in any command line flags to tensorboard? It requires --logdir at the very least. Is there any additional output on standard out? Please provide the exact command you are running as well as any additional information that might help us debug this problem.", "@ali01 I've tried multiple ways, but they all result in the same error\r\n\r\n```\r\n>tensorboard\r\nFatal error in launcher: Unable to create process using '\"'\r\n>mkdir foo\r\n>tensorboard --logdir=foo\r\nFatal error in launcher: Unable to create process using '\"'\r\n>tensorboard --logdir foo\r\nFatal error in launcher: Unable to create process using '\"'\r\n```\r\n\r\nNot really sure what other information to provide. I didn't really do anything special and TF itself is working. It's only tensorboard I'm having trouble with.", "Turns out the problem was not actually caused by TF, but after moving the `conda` environment where it was installed (renaming the env), see https://github.com/winpython/winpython/issues/410. I didn't use tensorboard before the rename so I thought it just didn't work from the beginning.\r\n\r\nAnyway, removing the `conda` env and re-creating a new one and re-installing TF solved the issue.", "I had the same problem. Intsted of\r\n`tensorboard --logdir=/tmp/tboard --port=7007 &` and any of such comands this worked:\r\n`python -m tensorflow.tensorboard --logdir=/tmp/tboard --port=7007 &`", "Facing same issue. \r\n###### System information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A, tensorboard command line issue\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64\r\nTensorFlow installed from (source or binary): conda install -c anaconda tensorflow-tensorboard \r\nTensorFlow version (use command below): 1.4.0\r\nBazel version (if compiling from source): N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: run tensorboard, get Fatal error in launcher: Unable to create process using '\"'\r\n> (tensorflow) C:\\Users\\salee>tensorboard\r\n> Fatal error in launcher: Unable to create process using '\"'\r\n> \r\n> (tensorflow) C:\\Users\\salee> E:\r\n> \r\n> (tensorflow) E:\\>cd repos\r\n> \r\n> (tensorflow) E:\\repos>cd deep-siamese-text-similarity\r\n> \r\n> (tensorflow) E:\\repos\\deep-siamese-text-similarity>cd runs\r\n> \r\n> (tensorflow) E:\\repos\\deep-siamese-text-similarity\\runs>cd 1521406230\r\n> \r\n> (tensorflow) E:\\repos\\deep-siamese-text-similarity\\runs\\1521406230>cd summaries\r\n> \r\n> (tensorflow) E:\\repos\\deep-siamese-text-similarity\\runs\\1521406230\\summaries>cd train\r\n> \r\n> (tensorflow) E:\\repos\\deep-siamese-text-similarity\\runs\\1521406230\\summaries\\train>tensorboard --logdir events.out.tfevents.1521406241.MAUVE-JEDI\r\n> Fatal error in launcher: Unable to create process using '\"'\r\n> \r\n> (tensorflow) E:\\repos\\deep-siamese-text-similarity\\runs\\1521406230\\summaries\\train>tensorboard --logdir ./events.out.tfevents.1521406241.MAUVE-JEDI\r\n> Fatal error in launcher: Unable to create process using '\"'\r\n> \r\n> (tensorflow) E:\\repos\\deep-siamese-text-similarity\\runs\\1521406230\\summaries\\train>python -m tensorflow.tensorboard --logdir=./events.out.tfevents.1521406241.MAUVE-JEDI --port=7007\r\n> C:\\Users\\salee\\Anaconda3\\envs\\tensorflow\\python.exe: No module named tensorflow.tensorboard\r\n> \r\n> (tensorflow) E:\\repos\\deep-siamese-text-similarity\\runs\\1521406230\\summaries\\train>python -m tensorboard --logdir=./events.out.tfevents.1521406241.MAUVE-JEDI --port=7007\r\n> C:\\Users\\salee\\Anaconda3\\envs\\tensorflow\\python.exe: No module named tensorboard.__main__; 'tensorboard' is a package and cannot be directly executed\r\n> \r\n> (tensorflow) E:\\repos\\deep-siamese-text-similarity\\runs\\1521406230\\summaries\\train>tensorboard --logdir=./events.out.tfevents.1521406241.MAUVE-JEDI --port=7007\r\n> Fatal error in launcher: Unable to create process using '\"'\r\n> \r\n> (tensorflow) E:\\repos\\deep-siamese-text-similarity\\runs\\1521406230\\summaries\\train>tensorboard --logdir=./ --port=7007\r\n> Fatal error in launcher: Unable to create process using '\"'\r\n> \r\n> (tensorflow) E:\\repos\\deep-siamese-text-similarity\\runs\\1521406230\\summaries\\train>tensorboard --logdir=/ --port=7007\r\n> Fatal error in launcher: Unable to create process using '\"'\r\n> \r\n> (tensorflow) E:\\repos\\deep-siamese-text-similarity\\runs\\1521406230\\summaries\\train>tensorboard --logdir=./ --port=7007\r\n> \r\n> (tensorflow) E:\\repos\\deep-siamese-text-similarity\\runs\\1521406230\\summaries\\train>cd ..\r\n> \r\n> (tensorflow) E:\\repos\\deep-siamese-text-similarity\\runs\\1521406230\\summaries>tensorboard --logdir=train --port=7007\r\n> Fatal error in launcher: Unable to create process using '\"'\r\n> \r\n> (tensorflow) E:\\repos\\deep-siamese-text-similarity\\runs\\1521406230\\summaries>tensorboard --logdir=train --port=7007\r\n> \r\n> (tensorflow) E:\\repos\\deep-siamese-text-similarity\\runs\\1521406230\\summaries>cd ..\r\n> \r\n> (tensorflow) E:\\repos\\deep-siamese-text-similarity\\runs\\1521406230>cd ..\r\n> \r\n> (tensorflow) E:\\repos\\deep-siamese-text-similarity\\runs>cd ..\r\n> \r\n> (tensorflow) E:\\repos\\deep-siamese-text-similarity>tensorboard --logdir=runs\\1521480347\\summaries\\train --port=7007\r\n> Fatal error in launcher: Unable to create process using '\"'\r\n> \r\n> (tensorflow) E:\\repos\\deep-siamese-text-similarity>tensorboard --logdir=\"runs/1521480347/summaries/train\" --port=7007\r\n> Fatal error in launcher: Unable to create process using '\"'", "\"`python -m tensorflow.tensorboard ...` \"is not useful in the new version\r\nthe version I use :\r\n`tensorflow & tensorboard : 1.8.0`\r\nplatform :\r\n`windows 10`\r\n\r\nI found an [amazing way](http://enjoytools.net/xe/board_PZRP31/4816) to solve this issue\r\n\r\n1. backup the tensorboard.exe\r\n1. open tensorboard.exe with 010 Editor,I use [this](https://wxmedit.github.io/)\r\n1. search \"python.exe\"\r\n1. change all the ascil code between `#!` and `python.exe` to 20(whitespace)\r\n1. change the quotation marks after python.exe to whitespace\r\n\r\nIn my computer:\r\nbefore:\r\n`#!\"c:\\program files\\python35\\python.exe\"`\r\nafter:\r\n`#!\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000python.exe`", "@dreamhuan Amazing indeed!! Thanks a lot!", "@darthdeus This works, that means the problem probably is the space in \"program files\".  I usually has this problem while using powershell. Besides, adding space(\"\\x20\") doesn't work for me, I change mine to \"python3.exe\" plus some NULL char(\"\\x00\").", "I'm confused, why would the null char help?\n\nOn Thu, Jul 5, 2018, 11:50 AM YoungC2015 <notifications@github.com> wrote:\n\n> @darthdeus <https://github.com/darthdeus> This works, that means the\n> problem probably is the space in \"program files\". I usually has this\n> problem while using powershell. Besides, adding space(\"\\x20\") doesn't work\n> for me, I change mine to \"python3.exe\" plus some NULL char(\"\\x00\").\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/10017#issuecomment-402667901>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAHh7sfCI-ju4X606kL__hpMQunUI-1kks5uDeF8gaJpZM4Nf48q>\n> .\n>\n", "@dreamhuan you are a GENIUS"]}, {"number": 10016, "title": "Documentation: Typo confusing `tf.training.Coordinator.should_stop` w\u2026", "body": "\u2026ith `tf.training.Coordinator.request_stop`\r\n\r\nThe code makes it clear, but I wanted to remove the inconsistency between the discussion and the code.", "comments": ["@tensorflow-jenkins test this please"]}, {"number": 10015, "title": "unable to install for new versin tensorflow 1.2", "body": "Hi,\r\n already installed  tensorflow 1.1 in python3.5 and OS  win64. but now for upgrade to \"tensorflow-1.2.0rc0-cp35-cp35m-win_amd64.whl\" the python TAKE error \" unable to install tensorflow 1.2\"", "comments": ["Can please provide much more information. Please post full console output. What was the full pip command you used? Are you within an environment. Can you confirm you are only installing the CPU version too.", "i tried for installing with \"WinPython Control Panel\" this modulo install files with \".whl\"  format. i use CPU version. \r\npython= 3.5 , OS is windows. the message box was \"unable to install tensorflow 1.2\"", "That looks like an issue in WinPython, please try looking through product forums of that tool.\r\nThis google search tells me a lot of users of this tool faced similar issues:\r\nhttps://www.google.com/search?q=%22unable+to+install%22+winpython&oq=%22unable+to+install%22+winpython&aqs=chrome..69i64j46l3j69i57.1513j0j7&sourceid=chrome&ie=UTF-8\r\n\r\nI will close this issue, as it is not a TF bug.", "I mean that I have not installed it \"tensorflow1.2\" with pip . i installed with \"WinPython Control Panel\" \r\n\r\nalso already install tensorflow 1.1 with \"\"WinPython Control Panel\" "]}, {"number": 10014, "title": "feature request: make placeholder_with_default work in batch prediction", "body": "cc @qimingj\r\n\r\nI ran into an issue with building a model that has missing values. During training, I can fill in missing values via the options to tf.decode_csv or tf.parse_example. I want to make my exported saved model also handle missing values. I know how to do this if the model reads from tf.example string or csv string, but this is impossible if the model takes each input tensor individually through placeholders. \r\n\r\nI looked at tf.placeholder_with_default(), but this function has useless behavior in batch prediction. As an example:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nwith tf.Session() as sess:\r\n\tdefault = tf.placeholder_with_default([1], shape=[None])\r\n\tregular = tf.placeholder(tf.int32, shape=[None])\r\n\r\n\tid_default = tf.identity(default)\r\n\tid_regular = tf.identity(regular)\r\n\r\n\tprint('1) normal 1 example', sess.run([id_default, id_regular], feed_dict={default: [2], regular: [5]}))\r\n\tprint('2) normal batch example', sess.run([id_default, id_regular], feed_dict={default: [2, 2], regular: [5, 5]}))\r\n\tprint('3) missing 1 column in batch', sess.run([id_default, id_regular], feed_dict={regular: [5, 5]}))\r\n\tprint('4) missing some data', sess.run([id_default, id_regular], feed_dict={default: [2, None], regular: [5, 5]}))\t\r\n```\r\nThe output is\r\n```\r\n('1) normal 1 example', [array([2], dtype=int32), array([5], dtype=int32)])\r\n('2) normal batch example', [array([2, 2], dtype=int32), array([5, 5], dtype=int32)])\r\n('3) missing 1 column in batch', [array([1], dtype=int32), array([5, 5], dtype=int32)])\r\nTraceback (most recent call last):\r\n  File \"placeholders.py\", line 13, in <module>\r\n    print('4) missing some data', sess.run([id_default, id_regular], feed_dict={default: [2, None], regular: [5, 5]}))\r\n  File \"/usr/local/google/home/brandondutra/miniconda2/envs/cml/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 767, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/google/home/brandondutra/miniconda2/envs/cml/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 938, in _run\r\n    np_val = np.asarray(subfeed_val, dtype=subfeed_dtype)\r\n  File \"/usr/local/google/home/brandondutra/miniconda2/envs/cml/lib/python2.7/site-packages/numpy/core/numeric.py\", line 531, in asarray\r\n    return array(a, dtype, copy=False, order=order)\r\nTypeError: long() argument must be a string or a number, not 'NoneType'\r\n```\r\n\r\nNote how line 3 is incorrect: I expected a result of [1,1], [5,5]\r\n\r\nAlso, placeholder_with_default does not allow some values to be missing in a batch as line 4 shows. \r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04.1\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: ('v1.0.0-65-g4763edf-dirty', '1.0.1')\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: n/a\r\n", "comments": ["forgot to say, I do not know my batch size during prediction.", "FYI @mrry ", "Out of curiosity, why would you expect\r\n\r\n```python\r\nsess.run([id_default, id_regular], feed_dict={regular: [5, 5]})\r\n```\r\n\r\n...to return `[1, 1], [5, 5]`? The contract for `tf.placeholder_with_default()` is pretty simple: it produces its default value unless you feed it (and it offers some latitude for feeding values of different shapes). There's nothing in the program that associates `id_default` with the value of placeholder `regular` (no edge between them in the graph), so the value you feed for `regular` will never influence the value or `id_default`. \r\n\r\nI suspect you're looking for a higher-level input that maintains an association between two (or more?) tensors. That might well be a fine thing, but we'll need more information about the behavior you expect before we can tell if it's a reasonable thing to include.", "I was comparing them to how tf.parse_example would work. If instead I passed two tf.examples (that had features named 'default' and 'regular'), and they both were missing the 'default' feature, and the feature spec had a default value for the 'default' feature, both examples would inject the default value when I call session.run. Now, this default injection would happen at tf.parse_example and not the placeholder.\r\n\r\nI fully agree the values of regular do not influence id_default. During bath prediction of a trained model, I'm thinking of the line (note I changed the data)\r\n\r\n`sess.run([id_default, id_regular], feed_dict={regular: [5, 6]})`\r\n\r\nas a batch prediction request. I'm think of this as if there are two requests\r\n\r\nRequest 1) get id_default, id_regular given {regular=5} (so id_default would use the default value, and I would get id_default=1, id_regular=5\r\n\r\nRequest 2) get id_default, id_regular given {regular=6} (so id_default would use the default value, and I would get id_default=1, id_regular=6\r\n\r\nI guess the wall I'm running into is that I cannot tell TF I want to run batch prediction and it should replace None with that tensor's placeholder. For example, I would like \r\n\r\n`sess.run([id_default, id_regular], feed_dict={regular: [5, 6, 7], default: [None, 3, None]})`\r\n\r\nTo return [1,3,1], [5,6,7]. Is this possible using placeholders? Or is there some kind of 'session.batch_run'? I think the only way to get this working currently is to use csv input or tf.example input, and not tensor input.", "In the case 3): sess.run([id_default, id_regular], feed_dict={regular: [5, 5]})\r\n\r\nIn order to get it working as expected we'll need:\r\n\r\n  default = tf.placeholder_with_default(tf.ones_like(regular), shape=[None])\r\n\r\nIt connects \"regular\" with \"default\" that way. Otherwise, you do need a \"batch_run\" so TF can infer the batch size.\r\n\r\nIn the case 4): sess.run([id_default, id_regular], feed_dict={default: [2, None], regular: [5, 5]})\r\n\r\nIt doesn't complain if you use tf.float32, and you get:\r\n\r\n('4) missing some data', [array([  2.,  nan], dtype=float32), array([ 5.,  5.], dtype=float32)])\r\n\r\nDon't know why it doesn't work for int types. If it works, we may be able to use tf.scan to update \"nan\" values to default values.\r\n\r\n\r\n", "@brandondutra how is that workaround for you?", "I abandoned this and instead used 1 placeholder of a csv string, and I let decode_csv deal with the defaults. I still think placeholder_with_default is broken for batch prediction.", "Closing for now in the absence of a specific feature request for a different design."]}, {"number": 10013, "title": "GPU resources not released when session is closed", "body": "This is a possible duplicate of #1727. Posting here as there is no way to re-open or comment on the previous bug. \r\nThe comments suggests that the updating the driver would most probably fix the issue but its not the case. \r\n\r\n**Environment info**\r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 16.04.2 LTS\r\nRelease:\t16.04\r\nCodename:\txenial\r\n\r\n>`python3 -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n   `v1.1.0-rc0-61-g1ec6ed5 1.1.0`\r\n \r\n>`nvidia-smi`\r\n`NVIDIA-SMI 375.51                 Driver Version: 375.51`\r\n\r\n**Before starting the service**\r\n0     27825    C   /usr/bin/python3                                35MiB \r\n\r\n**After deleting the session**\r\nNote that I am creating the session and loading a checkpoint, running the session and then closing it explicitly with `session.close()`. I am also resetting the graph by calling `tf.reset_default_graph()` after closing the session. \r\n0     27825    C   /usr/bin/python3                              1435MiB\r\n\r\n\r\n    device_t='/gpu:0'\r\n    graph = tf.Graph()\r\n    soft_config = tf.ConfigProto(allow_soft_placement=True)\r\n    soft_config.gpu_options.allow_growth = True . #tried both True and False and doesn't seem to help\r\n    with graph.as_default(), graph.device(device_t), tf.Session(config=soft_config) as sess:\r\n        batch_shape = (batch_size,) + img.shape\r\n        img_placeholder = tf.placeholder(tf.float32, shape=batch_shape,\r\n                                         name=\"img_placeholder\")\r\n        preds = transform.net(img_placeholder)\r\n        saver = tf.train.Saver()\r\n        saver.restore(sess, checkpoint_dir)\r\n\r\n        content = np.zeros(batch_shape, dtype=np.float32)\r\n        content[0] = img\r\n\r\n        _preds = sess.run(preds, feed_dict={img_placeholder: content})\r\n        result = crop_img(_preds[0].astype(data_in.dtype))\r\n        sess.close()   #Explicitly closing the session and deleting does not help as well. \r\n        del sess\r\n    tf.reset_default_graph()\r\n    return result\r\n\r\nHere are some of the relevant logs. \r\n```\r\n2017-05-18 18:49:16.556719: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-18 18:49:16.556757: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-18 18:49:16.556775: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-18 18:49:16.584028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-05-18 18:49:16.584295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \r\nname: GRID K520\r\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.797\r\npciBusID 0000:00:03.0\r\nTotal memory: 3.94GiB\r\nFree memory: 3.87GiB\r\n2017-05-18 18:49:16.584326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 \r\n2017-05-18 18:49:16.584344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y \r\n2017-05-18 18:49:16.584368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)\r\n2017-05-18 18:49:16.585190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)\r\n\r\n\r\n```\r\n", "comments": ["I also have this issue with Windows 10 64 Bit. TF1.2-GPU. I have to restart my python kernel (i'm using jupyter) for the memory to be released.\r\n\r\nI think that I read that whilst the memory is not released, Tensorflow is managing it behind the scenes which is great if you're doing lots of TF stuff but not if you want to release it for something else. I may have mis-remembered this though so hopefully a Tensorflower can answer.", "Please provide more details about what platform you are using. We need your operating system, architecture, and TensorFlow version. What graphics card are you running? Also, did you compile from source or install a binary?  Make sure you also include the exact command(s)/code if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!"]}]