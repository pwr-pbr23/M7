[{"number": 5393, "title": "Error in `/home/software/anaconda2/bin/python': invalid fastbin entry (free): 0x00007f2fa8023940", "body": "**Tensorflow version: 0.11.0rc1 (compile from source)\r\nOS: CentOS Linux release 7.0.1406 (Core) 64bit\r\nmodel: models/inception**\r\n\r\nI am training inception model from scratch following [this](https://github.com/tensorflow/models/tree/master/inception), but after about 11500 steps got this error:\r\n\r\n```\r\n...\r\n...\r\n2016-11-03 22:37:06.142819: step 11540, loss = 9.38 (66.9 examples/sec; 0.957 sec/batch)\r\n2016-11-03 22:37:15.753609: step 11550, loss = 9.22 (67.4 examples/sec; 0.950 sec/batch)\r\n2016-11-03 22:37:25.332004: step 11560, loss = 9.51 (65.6 examples/sec; 0.975 sec/batch)\r\n*** Error in `/home/software/anaconda2/bin/python': invalid fastbin entry (free): 0x00007f2fa8023940 ***\r\n======= Backtrace: =========\r\n/lib64/libc.so.6(+0x7d19d)[0x7f315d7b919d]\r\n/home/software/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(+0x248ff48)[0x7f314baa2f48]\r\n/home/software/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(+0x244520f)[0x7f314ba5820f]\r\n/home/software/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN10tensorflow19LocalRendezvousImpl4SendERKNS_10Rendezvous9ParsedKeyERKNS1_4ArgsERKNS_6TensorEb+0xf9)[0x7f314bb9e7f9]\r\n/home/software/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN10tensorflow22IntraProcessRendezvous4SendERKNS_10Rendezvous9ParsedKeyERKNS1_4ArgsERKNS_6TensorEb+0xb4)[0x7f314ba57b74]\r\n/home/software/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN10tensorflow6SendOp7ComputeEPNS_15OpKernelContextE+0x346)[0x7f314baa3736]\r\n/home/software/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(+0x242ea59)[0x7f314ba41a59]\r\n/home/software/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(+0x2422e30)[0x7f314ba35e30]\r\n/home/software/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN5Eigen26NonBlockingThreadPoolTemplIN10tensorflow6thread16EigenEnvironmentEE10WorkerLoopEi+0x3c8)[0x7f314bc474a8]\r\n/home/software/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZNSt17_Function_handlerIFvvEZN10tensorflow6thread16EigenEnvironment12CreateThreadESt8functionIS0_EEUlvE_E9_M_invokeERKSt9_Any_data+0x22)[0x7f314bc46c72]\r\n/home/software/anaconda2/bin/../lib/libstdc++.so.6(+0xb4870)[0x7f3149153870]\r\n/lib64/libpthread.so.0(+0x7df3)[0x7f315e20ddf3]\r\n/lib64/libc.so.6(clone+0x6d)[0x7f315d8321ad]\r\n======= Memory map: ========\r\n00400000-00401000 r-xp 00000000 fd:02 34476856                           /home/software/anaconda2/bin/python2.7\r\n00600000-00601000 rw-p 00000000 fd:02 34476856                           /home/software/anaconda2/bin/python2.7\r\n0067e000-42ae4000 rw-p 00000000 00:00 0                                  [heap]\r\n200000000-200100000 rw-s 1026d71000 00:05 221089                         /dev/nvidiactl\r\n200100000-204100000 ---p 00000000 00:00 0 \r\n204100000-204200000 rw-s f70ee2000 00:05 221089                          /dev/nvidiactl\r\n204200000-204300000 ---p 00000000 00:00 0 \r\n204300000-204400000 rw-s f75483000 00:05 221089                          /dev/nvidiactl\r\n204400000-204500000 ---p 00000000 00:00 0 \r\n204500000-204600000 rw-s 1014d38000 00:05 221089                         /dev/nvidiactl\r\n204600000-208600000 ---p 00000000 00:00 0 \r\n208600000-208700000 rw-s f7735a000 00:05 221089                          /dev/nvidiactl\r\n208700000-208800000 ---p 00000000 00:00 0 \r\n208800000-208900000 rw-s f7777d000 00:05 221089                          /dev/nvidiactl\r\n208900000-208a00000 ---p 00000000 00:00 0 \r\n208a00000-208b00000 rw-s f77eaa000 00:05 221089                          /dev/nvidiactl\r\n208b00000-20cb00000 ---p 00000000 00:00 0 \r\n...\r\n...\r\n```", "comments": ["A few things to try:\n- Could you try this with tcmalloc?\n- Could you try this without the GPU?\n- Could you try this from master?\n", "Closing due to lack of activity.\n"]}, {"number": 5392, "title": "AttributeError: 'module' object has no attribute 'learn'", "body": "In your first code example you have the following function tf.contrib.learn being called:\r\n\r\n```\r\n  run_config = tf.contrib.learn.estimators.RunConfig(\r\n      num_cores=3, gpu_memory_fraction=0.6)\r\n```\r\n\r\nBut if you look at the options you won't see learn....  \r\n\r\n```\r\n>>> dir(tf.contrib)\r\n['__builtins__', '__doc__', '__file__', '__name__', '__package__', '__path__', 'absolute_import', 'division', 'layers', 'print_function', 'util']\r\n```\r\nSo you get the error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 2, in <module>\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\r\n    sys.exit(main(sys.argv))\r\n  File \"<stdin>\", line 6, in main\r\nAttributeError: 'module' object has no attribute 'learn'\r\n```\r\n", "comments": ["This is in your boston example as well, for the import:\n\nNameError: global name 'learn' is not defined\n", "@austintrombley It seems to work fine ! \n\n`>>>import tensorflow as tf`\n`>>>dir(tf.contrib)`\n`['__builtins__', '__doc__', '__file__', '__name__', '__package__', '__path__', 'absolute_import', 'bayesflow', 'copy_graph', 'distributions', 'division', 'factorization', 'framework', 'graph_editor', 'grid_rnn', 'layers', 'learn', 'linear_optimizer', 'lookup', 'losses', 'metrics', 'opt', 'print_function', 'quantization', 'rnn', 'session_bundle', 'slim', 'tensor_forest', 'testing', 'util']`\n", "@austintrombley, it is difficult to help you when you don't provide basic information about your environment. The template in the submit github issue is information that is very useful for us to know in order to help you. Please provide that information. Thanks!\n", "Thanks @Curios72 for trying this. What version/os are you using for reference so that @austintrombley can try your config?\n", "Closing due to lack of activity.\n", "I met similar issue, see #8794"]}, {"number": 5391, "title": "[Feature Request] Loop_function argument for seq2seq models", "body": "The RNN decoder has a [parameters](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L117) `loop_function` which allows to do some pre-processing on the output of the RNN before connecting the output on the next input.\r\n\r\nThat would be great to have this parameters for `embedding_rnn_seq2seq` and `embedding_rnn_decoder`. For now the only control we can have is no loop function at all when `feed_previous=False` or an argmax on the output called internally [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L274) .\r\n\r\nThe loop function parameters should be really useful to sample from the softmax distribution, instead of only taking the most likely output (as it is the case now with `feed_previous`) or to implement [scheduled sampling](https://arxiv.org/abs/1506.03099). \r\n\r\nFor now the only solution is to re-create custom versions of `embedding_rnn_decoder` `embedding_rnn_seq2seq` just to change this parameters.\r\n", "comments": ["@ebrevdo, could you consider this suggestion?\n", "We are working on a new seq2seq api in tf.contrib.seq2seq which should be flexible enough for you to do this.\n", "Great. Thanks!\n", "I close the issue as this can now be done with the new seq2seq API and the `decoder_fn` parameter."]}, {"number": 5390, "title": "tf.variable_scope() does not allow variables to be defined for individual towers under a scope, in tensorflow/tensorflow/contrib/slim/python/slim/nets/inception_v3.py", "body": "I have been working with inception_v3 model from past few weeks and I came across this issue with the inception model defined under tensorflow/tensorflow/contrib/slim/python/slim/nets/inception_v3.py. \r\n\r\nThis newer inception model cannot create variables for individual towers in a multi-gpu environment defined under a tower scope and fails with the following error:\r\n\r\n**ValueError: Variable tower_1//Conv2d_1a_3x3/weights does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?**\r\n\r\nThe source of this error stems from defining logits with scope, as follows,\r\n\r\n    logits = inception.inception_v3(inputs=images, num_classes=num_classes,\r\n                                              is_training=is_training,\r\n                                              dropout_keep_prob=dropout_rate,\r\n                                              reuse=None,\r\n                                              scope=scope)\r\n\r\nThe aforementioned error arises despite setting reuse=None. \r\n\r\nSuppose, scope isn't defined,\r\n\r\n    logits = inception.inception_v3(inputs=images, num_classes=num_classes,\r\n                                                    is_training=is_training,\r\n                                                    dropout_keep_prob=dropout_rate)\r\n\r\nlogits layer then builds for multiple GPUs, in my training environment without ending up with a ValueError. The model was indeed designed to be defined without a scope? My investigation of the code showed that the source of this error arises from the following 2 lines within ..slim/nets/inception_v3.py file, \r\n\r\n       with tf.variable_scope(scope, 'InceptionV3', [inputs, num_classes],\r\n                                                    reuse=reuse) as scope:\r\n      \r\n       with tf.variable_scope(scope, 'InceptionV3', [inputs]):\r\n\r\nTower variables are successfully created for the first tower (or a single GPU) whilst failing to creating the same variables for remaining towers (remainder of GPUs), with a 'scope' defined, the model ends up producing a ValueError. As one of the workarounds, I began using this model without defining a scope for tower creation in multi-gpu environment.\r\n\r\nOPTIONAL WORKAROUND:\r\n\r\nAs an optional workaround, I figured changing the way the variables are defined by changing variable definition from **with tf.variable_scope** to  **with tf.name_scope**. This seems to have fixed the ValueError issue. I'm now able to define logits for 'n' GPU setting, with \"scope\" as a functional parameter (the towers and their respective variables are successfully created for each of the several available GPUs, in a multi-GPU setting). Also, as an added bonus I did observe that the \"graph\" view on tensorboard looked more organized and nicely segmented in contrast to the default code (default code is the one that made use of **with tf.variable_scope**). Please let me know if this is indeed an existing issue with this newer inception_v3 model or am I doing something wrong in the way I am defining my logits layer. \r\n\r\nI am confident, I am following the right steps because,\r\n\r\n1. My model seems to train and test perfectly without a scope defined while using the default inception_v3.py to build the logits,\r\n2. My model seems to train and test perfectly after replacing variable_scope with name_scope and passing individual tower scope as an argument to build the logits layer.\r\n3. I am following the same methodology used in the following code, https://github.com/tensorflow/models/blob/master/inception/inception/inception_train.py to define my logits layers for multiple towers in the tower_loss function of my train file.\r\n\r\nAny inputs would be much appreciated,", "comments": ["Could you please provide a concrete example of what you are trying to do and what happens. Also, what version of tensorflow, what operating system. The template is provided so that we can help you, please don't ignore it.\n", "@aselle just wanted to know, if the information was sufficient to reproduce this issue? \n", "Hi , \nI have been studying the pinning of variables and ops in a multi-gpu enviroment. I am following the multi-gpu training methodology  as shown in [multi-gpu inception_train example](https://github.com/tensorflow/models/blob/master/inception/inception/inception_train.py) \n\nI observed different variable placement in two scenarios:\n1. As shown in the [example](https://github.com/tensorflow/models/blob/master/inception/inception/inception_train.py)  which uses a deprecated [inception.slim](https://github.com/tensorflow/models/tree/master/inception/inception/slim) #4247\n2.  Using the [tf.contrib.slim](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/inception_v3.py) , as mentioned in this thread  \n\n**Enviroment:**\nTensorflow version 0.11.0 (last stable binary)\nCUDA 8.0\nCUDnn 5.1\n2 \\* K20 GPU Cards\n\n**Scenario 1:**\nUsing the deprecated inception.slim library which is the same used in the inception_train example , i observed that the variables are placed on the CPU and all the ops are separately placed on the respective  GPU with a prefix 'TOWER_0' or 'TOWER_1' \nUsing the below **Test Code** this [Full Device Placement Log](http://www.filedropper.com/logdeprecated)  was generated: \n\n```\nimport tensorflow as tf\nfrom inception.slim import slim\nfrom inception.slim.inception_model import inception_v3\nwith tf.Graph().as_default(), tf.device('/cpu:0'):\n    imgPath=tf.placeholder(tf.string)\n    imageString=tf.read_file(imgPath)\n    imageJpeg=tf.image.decode_jpeg(imageString, channels=3)\n    inputImage=tf.image.resize_images(imageJpeg, [299,299])\n    images= tf.expand_dims(inputImage, 0)\n    for i in range(2):\n        with tf.device('/gpu:%d' % i):\n            with tf.name_scope('%s_%d' % ('TOWER', i)) as scope:\n                with slim.arg_scope([slim.variables.variable], device='/cpu:0'):\n                    # Parameters for BatchNorm.\n                    batch_norm_params = {\n                    # Decay for the moving averages.\n                        'decay': 0.9997,\n                    # epsilon to prevent 0s in variance.\n                        'epsilon': 0.001,\n                    }\n                    # Set weight_decay for weights in Conv and FC layers.\n                    with slim.arg_scope([slim.ops.conv2d, slim.ops.fc], weight_decay=0.00004):\n                        with slim.arg_scope([slim.ops.conv2d],\n                                stddev=0.1,\n                                activation=tf.nn.relu,\n                                batch_norm_params=batch_norm_params):\n                            logits, endpoints = slim.inception.inception_v3(\n                            images,\n                            dropout_keep_prob=0.8,\n                            num_classes=1001,\n                            is_training=True,\n                            restore_logits=False,\n                            scope=scope)\n            tf.get_variable_scope().reuse_variables()\n\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=True)) as sess:\n        tf.initialize_all_variables().run()\n    exit(0)\n```\n\n**Scenario 2:**\nUsing the [tf.contrib.slim] , the same '**VALUE ERROR**' occurs as stated by @pranavvm26  \n\n**Test Code:**\n\n```\nimport tensorflow as tf\nfrom tensorflow.contrib.slim.python.slim.nets.inception_v3 import inception_v3\nfrom tensorflow.contrib.slim.python.slim.nets.inception_v3 import inception_v3_arg_scope\nslim = tf.contrib.slim\nwith tf.Graph().as_default(), tf.device('/cpu:0'):\n    imgPath=tf.placeholder(tf.string)\n    labels= tf.placeholder(tf.float32,shape=([1,1]),name='labels')\n    imageString=tf.read_file(imgPath)\n    imageJpeg=tf.image.decode_jpeg(imageString, channels=3)\n    inputImage=tf.image.resize_images(imageJpeg, [299,299])\n    images= tf.expand_dims(inputImage, 0)\n    for i in range(2):\n        with tf.device('/gpu:%d' % i):\n            with tf.name_scope('%s_%d' % ('TOWER', i)) as scope:\n                with slim.arg_scope([slim.variable], device='/cpu:0'):\n                    with slim.arg_scope(inception_v3_arg_scope()):\n                            logits, endpoints = inception_v3(\n                              images,\n                              dropout_keep_prob=0.8,\n                              num_classes=1001,\n                              is_training=True,\n                              scope=scope)\n            tf.get_variable_scope().reuse_variables()\n\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=True)) as sess:\n        tf.initialize_all_variables().run()\n    exit(0)\n```\n\n**ERROR:**\n\n```\nValueError: Variable TOWER_1//Conv2d_1a_3x3/weights does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\n```\n\nOn not passing scope value to the inception_v3 model, the Value Error is gone but the device placement log shows variables pinned on GPU:0 and OPS on both GPU:0 and GPU:1\n[FULL DEVICE PLACEMENT LOG](http://www.filedropper.com/newlog)\n\nAs far as i understand and checking the cifar log device placement the variables should be pinned to the cpu.\nAny thoughts on the difference in behavior ?\n\nThanks \n", "The error comes because you are passing the name_scope TOWER_1 as a variable_scope to inception_v3, just don't pass any scope, it would use the proper inception_v3 scope by default.\n", "@sguada thanks for the reply.\nI have done that (please check last part of my post), but the device  placement log still places variables on gpu:0 rather than cpu\n", "to add : \nIf there is no P2P connectivity between the different GPU cards , placing the variables on gpu:0 does not work, since they cannot be shared\n", "@ashishkr23 I came across the same issue when training using the newer inception model. To enable variable placement on CPU rather than GPU:0 for the scenario you just mentioned, there might be a different workaround. Try the following,\n$  The conv, fc, etc ops are sourced from the following location:\n                          https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py\n\nThis script is responsible for the creation of variables for the ops. \n\nSide note: In the default case the variables lie in GPU:0 in my system too but my GPUs are in a P2P connectivity so the variables residing in the GPU:0 are supplied to all the other GPUs and also updated per session (this may be problematic in your case).\n\n$  To pin the variables to the CPU, add a param called **device** to to the ops in layers.py\n\nexample:\n\n```\nvariables.model_variable('weights',\n                                       shape=weights_shape,\n                                       dtype=dtype,\n                                       initializer=weights_initializer,\n                                       regularizer=weights_regularizer,\n                                       collections=weights_collections,\n                                       trainable=trainable)\n```\n\nchanged to \n\n```\nvariables.model_variable('weights',\n                                       shape=weights_shape,\n                                       dtype=dtype,\n                                       initializer=weights_initializer,\n                                       regularizer=weights_regularizer,\n                                       collections=weights_collections,\n                                       trainable=trainable,\n                                       device='/cpu:0')\n```\n\nwithin the layers.py (for all ops).\n\nThis would create the model with op variables pinned to the CPU instead of the GPU. The variable.model_variable is being called from the following location within tensorflow (@sguada please correct me if I am wrong),\n\nhttps://github.com/tensorflow/tensorflow/blob/beb10ceb086fe94a6b1247b45397aafddd47e05d/tensorflow/contrib/framework/python/ops/variables.py\n\n```\ndef model_variable(name, shape=None, dtype=dtypes.float32, initializer=None,\n                   regularizer=None, trainable=True, collections=None,\n                   caching_device=None, **device=None**)\n```\n\nIn my case it didnt make much difference because of my system architecture.\n\n$  To be able to pass tower as a name scope you could try the workaround suggested by copying the inception_v3.py file over to your project root and changing variable_scope to name_scope. If you do try this please let me know if these suggestion help with your case of distributed GPU system (without a P2P connectivity).\n", "@sguada \n\n> The error comes because you are passing the name_scope TOWER_1 as a variable_scope to inception_v3, just don't pass any scope, it would use the proper inception_v3 scope by default.\n\nIt does, but should the functionality of passing scope towards creation of logits layer be available to us ? Doesn't it help with the creation of separate op-variables under individual GPU tower ? Similar to the instance mentioned in the comments - a GPU system with the lack of P2P connectivity?\n\nAlso, the root cause of this issue seems to stem from the way the variables are created within the inception_v3.py under the umbrella of variable scope. Changing it from variable_scope to name_scope does not error out the same way and the \"net\" works as designed (from my testing).\n\nAlso, as I mentioned earlier the graph looks more modular too (similar to how the graph looked in the deprecated/older inception with slim).\n\nDo you foresee any issues with changing variable_scope to name_scope ?  \n\nThanks.\n", "Getting all the name_scope and variable_scope right can be a bit tricky. I would recommend you to take a look at https://github.com/tensorflow/models/tree/master/slim which has [model_deploy](https://github.com/tensorflow/models/blob/master/slim/deployment/model_deploy.py) that shows how to build and train a model using multi-GPU. It has several optimizations, like variables pinned into CPU and efficient gradient accumulation.\n\nFor reference, one can pin all the variables to CPU using arg_scope:\n`with slim.arg_scope(slim.variable, device='/CPU:0'):`\n", "@pranavvm26 thanks for the response.though i got it to work by resolving the P2P connectivity via firmware update.\ni will try  your solution too and let you know  \n", "@sguada so this solution didn't work for me. Despite adding **device** type within arg_scope, the variables still showed as pinned to the GPU:0. That was when I tried the workaround I mentioned in the earlier comment. Perhaps, @ashishkr23  could try the suggestion by sguada before making changes to the slim code and let us know if that worked in his case?\n\nThanks\n", "replaced \n`with slim.arg_scope([slim.variable], device='/cpu:0'):`\nwith \n`with slim.arg_scope([slim.model_variable, slim.variable],\n                         device='/cpu:0'):`  as shown in [model_deploy](https://github.com/tensorflow/models/blob/master/slim/deployment/model_deploy.py)    \n\nThe variables are now created on cpu:0 and ops on respective  gpu's\n\nThanks @pranavvm26  and @sguada  \n", "Any updates on this issue?", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 5389, "title": "[minor enhancement] Absolute vs. Relative import", "body": "I am raising a question if absolute imports should be changed to relative -- even if not in all the files, but at least in the files that have to be grouped together.\r\nAccording to [PEP 328](https://www.python.org/dev/peps/pep-0328/#rationale-for-relative-imports):\r\n> ... the most important [usage case ...] is being able to rearrange the structure of large packages without having to edit sub-packages. In addition, a module inside a package can't easily import itself without relative imports.\r\n\r\nI understand that absolute imports help avoid \"shadowing\" of the modules/methods with the same names, but if the naming convention of the relative imports is enforced to be protected (`_name`), the problem could be mitigated.", "comments": ["We are unlikely to switch to relative imports because we need to adhere to our internal conventions which mandate absolute paths. @wicke or @vrv can probably give a better explanation. I believe bazel also does things this way. Thus I am closing it for now unless  @wicke and @vrv think we could do it. Thanks for bringing it up though.\n", "Thanks! \n"]}, {"number": 5388, "title": "Revert IDCT jpeg change and nvcc flag change", "body": "IDCT change needs to be made backwards compatible until we can verify that the reduced quality for all applications (not just training) is sufficient.\r\n\r\nnvcc flag change doesn't work for all recent versions of nvcc, so reverting that too.", "comments": ["@vrv, thanks for your PR! By analyzing the history of the files in this pull request, we identified @mkolod, @gunan and @keveman to be potential reviewers.\n"]}, {"number": 5387, "title": "Branch 138101249", "body": "", "comments": []}, {"number": 5386, "title": "Add a .mention-bot config.", "body": "Notable configs:\r\n- Only mention people in \"tensorflow\" org\r\n- Our tensorflow-gardener catch-all github user is never mentioned.\r\n- Wait 10 minutes for mentioning (to avoid spam when we manually assign)\r\n- Skip assignment for internal pushes", "comments": []}, {"number": 5385, "title": "Create a script for building tensorflow pip package on windows using bazel.", "body": "Scripts created by @meteorcloudy ", "comments": ["Manually killed all tests, jenkins is overloaded and I needed the PR to start testing windows bazel build.\n", "This change looks good to me! \n", "http://ci.tensorflow.org/job/tensorflow-pull-requests-windows-bazel/4/console\n\n@meteorcloudy How about the failure above.\nIs it caused by this change, or something missing here?\nOr is it simply a breakage existing in the repo and we address it separately.\n", "Ah.. Looks like we are hitting the [Windows long path issue](http://stackoverflow.com/questions/1880321/why-does-the-260-character-path-length-limit-exist-in-windows).\nBasically, Windows doesn't support file path longer than 260 characters. Unfortunately, AFAIK, there's no workaround for cl.exe. \n\nThis error is because `tensorflow-pull-requests-windows-bazel` is long enough to make the file path exceed that limitation.\n\nWe'll have to work around this by setting `export TMPDIR=c:/temp`, so compare to `C:\\\\tools\\\\msys64\\\\var\\\\tmp` we can save some characters...\n", "I just noticed there's no need to use double back slash `\\\\`, this could save us a lot of characters, I'll fix it in the next Bazel release.\n", "```\n15:18:35 ERROR: C:/tmp/Bazel/3SovMC5L/external/protobuf/BUILD:579:1: Linking of rule '@protobuf//:internal/_api_implementation.so' failed: msvc_link.bat failed: error executing command \n15:18:35   cd C:/tmp/Bazel/3SovMC5L/execroot/tensorflow-pull-requests-windows-bazel\n15:18:35   SET PATH=external/local_config_cc/wrapper/bin\n15:18:35   external/local_config_cc/wrapper/bin/msvc_link.bat /DLL /WHOLEARCHIVE -m64 -Xcompilation-mode=opt -Wl,@bazel-out/vc_14_0_x64-py3-opt/bin/external/protobuf/internal/_api_implementation.so-2.params: com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1120\n15:18:35 LINK : warning LNK4001: no object files specified; libraries used\n15:18:35 LINK : error LNK2001: unresolved external symbol _DllMainCRTStartup\n15:18:35 bazel-out/vc_14_0_x64-py3-opt/bin/external/protobuf/internal/_api_implementation.so : fatal error LNK1120: 1 unresolved externals\n15:18:35 ERROR: C:/tmp/Bazel/3SovMC5L/external/zlib_archive/BUILD:5:1: output 'external/zlib_archive/libzlib.a' was not created\n15:18:35 ERROR: C:/tmp/Bazel/3SovMC5L/external/grpc/BUILD:69:1: output 'external/grpc/libgpr.a' was not created\n15:18:35 ERROR: C:/tf_jenkins/home/workspace/tensorflow-pull-requests-windows-bazel/tensorflow/core/BUILD:1112:1: declared output 'tensorflow/core/util/version_info.cc' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)\n```\n\nI saw the new error message on ci, I can reproduce the same error when using \"tensorflow-pull-requests-windows-bazel\" as the workspace folder name.\nAlthough the error message is not obvious, the underlying problem is still caused by long path. Is it ok to rename the job name to something like `tf-pr-win-bazel`?\nSo sorry for the trouble, I will fix this soon. (actually, we can only ease this by not using `\\\\` to save some characters)\n", "A-ha, now it is running:\nhttp://ci.tensorflow.org/job/tensorflow-pr-win-bazel/1/console\n", "Hehe, I don't think I can approve this PR :)\n@meteorcloudy with the new job name, do we need the TMPDIR environment variable?\nAlso, should I \"set -e\" in the script?\nNot sure how msys bash works.\n", "@gunan Probably yes, it's better we still have it for now.\nI can only confirm we don't need it when job name is as short as `tensorflow`\nSetting `set -e` sounds good to me, just add it in the beginning of the script, it should be the same as linux.\n\nWhy can't you approve this? What else need to be done?\n", "Since I am the author of the PR, it wont let me approve.\nI will add set -x and set -e to the bash piece.\n\nOn Thu, Nov 3, 2016 at 4:36 PM, Yun Peng notifications@github.com wrote:\n\n> @gunan https://github.com/gunan Probably yes, it's better we still have\n> it for now.\n> I can only confirm we don't need it when job name is as short as\n> tensorflow\n> Setting set -e sounds good to me, just add it in the beginning of the\n> script, it should be the same as linux.\n> \n> Why can't you approve this? What else need to be done?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/5385#issuecomment-258305588,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AHlCOf6Mw94f67kpaGRxRrNzSx7Q6mnAks5q6m_igaJpZM4KouOm\n> .\n", "Haha, I see~  :)\n", "@meteorcloudy Final question:\nhttp://ci.tensorflow.org/job/tensorflow-pr-win-bazel/2/consoleFull\n\nWe see this error, it does not seem to affect the outcome, but all bazel builds have it:\n\n00:00:36.893 ERROR: CreateFile(C:\\tmp\\Bazel\\hzVGLx29\\install): The system cannot find the file specified.\n\nAny idea what it is?\n", "@gunan It happens when running Bazel in a new workspace for the first time, Bazel tries to unzip itself to a install directory. But yes, we should get rid of this error message since it's not actually causing any problem.\nWe have an issue here: https://github.com/bazelbuild/bazel/issues/1744\n\nVijay just invited me to join TensorFlow organization, does that mean I can approve this? :)\n"]}, {"number": 5384, "title": "Gradle change in 2.2 and sdk update", "body": "There is a change in Gradle 2.2\nhttps://code.google.com/p/android/issues/detail?id=219732", "comments": ["Can one of the admins verify this patch?\n", "@Sadmansamee, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @andrewharp and @dsmilkov to be potential reviewers.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "You'll need to update [tensorflow/tools/ci_build/Dockerfile.android](https://github.com/tensorflow/tensorflow/blob/754048a0453a04a761e112ae5d99c149eb9910dd/tensorflow/tools/ci_build/Dockerfile.android) as well so it matches the new defaults you've selected, and then we can test with Jenkins.\n\n[tensorflow/tools/ci_build/builds/android.sh](https://github.com/tensorflow/tensorflow/blob/d44d271c9da4d244ce4b2ffaf808adbe4cff759d/tensorflow/tools/ci_build/builds/android.sh) pulls from it when auto-appending to the WORKSPACE file.\n", "@Sadmansamee ping :) ", "Friendly Ping!\r\n\r\nI'll close this as stale unless I hear from @Sadmansamee. ", "Closing due to inactivity. Feel free to reopen once ready."]}, {"number": 5383, "title": "GraphDef cannot be larger than 2GB using input pipelines", "body": "_ValueError: GraphDef cannot be larger than 2GB._\r\n\r\nI'm not loading my data into a constant. I'm using an input pipeline to prefetch small batches. My graph works with cifar10 but if I use my SVHN input pipeline I get the error posted above. Both input pipeline scripts are virtually the same. How can it be that my data becomes part of the graph?\r\n\r\n### Environment info\r\nCUDA 8.0.27\r\nccuDNN 5.1.5\r\nTensorflow 0.11 (manually compiled)\r\nbazel 0.3.2\r\nNvidia 1080 GTX\r\n\r\n### What other attempted solutions have you tried?\r\ncifar10 input pipeline works, but svhn gives 2GB error\r\n\r\nA working but slim version of the main script\r\n```\r\nimport tensorflow as tf\r\n\r\nfrom libs.components import conv2d, batch_norm, flatten, dense\r\nfrom datasets.cifar10 import cifar10_data\r\nfrom datasets.svhn import svhn_data\r\n\r\n## ----------------------------------------------------------------------------\r\n## CONFIGURATION\r\nBATCH_SIZE = 128\r\nLOGS_PATH = \"/home/chuck/MyStuff/cnn_test/logs/InitialTests/4/\"\r\nEPOCHS = 300  # max number of epochs if the network never converges\r\nlearning_rate = 0.01\r\n\r\n## ----------------------------------------------------------------------------\r\n## DATA INPUT\r\n#data = cifar10_data(batch_size=BATCH_SIZE)\r\ndata = svhn_data(batch_size=BATCH_SIZE)\r\n\r\nwith tf.device('/cpu:0'):\r\n  train_image_batch, train_label_batch = data.build_train_data_tensor(shuffle=True)\r\n  test_image_batch, test_label_batch = data.build_test_data_tensor(shuffle=False)\r\n\r\nNUMBER_OF_CLASSES = data.NUMBER_OF_CLASSES\r\nIMG_SIZE = data.IMAGE_SIZE\r\nNUM_CHANNELS = data.NUM_OF_CHANNELS\r\nTRAIN_SET_SIZE = data.TRAIN_SET_SIZE\r\nTEST_SET_SIZE = data.TEST_SET_SIZE\r\nTRAIN_BATCHES_PER_EPOCH = int(TRAIN_SET_SIZE / BATCH_SIZE)  # only used for training\r\n\r\n## ----------------------------------------------------------------------------\r\n## MODEL STRUCTURE\r\nis_training = tf.placeholder(tf.bool, name='is_training')\r\n\r\n\r\ndef conv_block(data, n_filter, scope, stride=1):\r\n  \"\"\"An ConvBlock is a repetitive composition used in the model.\"\"\"\r\n  with tf.variable_scope(scope):\r\n    conv = conv2d(data, n_filter, \"conv\",\r\n                  k_h=3, k_w=3,\r\n                  stride_h=stride, stride_w=stride,\r\n                  initializer=tf.random_normal_initializer(stddev=0.01),\r\n                  bias=True,\r\n                  padding='SAME')\r\n    norm = batch_norm(conv, n_filter, is_training, scope=\"bn\")\r\n    relu = tf.nn.relu(norm)\r\n\r\n    conv2 = conv2d(relu, n_filter, \"conv2\",\r\n                   k_h=3, k_w=3,\r\n                   stride_h=stride, stride_w=stride,\r\n                   initializer=tf.random_normal_initializer(stddev=0.01),\r\n                   bias=True,\r\n                   padding='SAME')\r\n    norm2 = batch_norm(conv2, n_filter, is_training, scope=\"bn2\")\r\n    relu2 = tf.nn.relu(norm2)\r\n\r\n    conv3 = conv2d(relu2, n_filter, \"conv3\",\r\n                   k_h=3, k_w=3,\r\n                   stride_h=stride, stride_w=stride,\r\n                   initializer=tf.random_normal_initializer(stddev=0.01),\r\n                   bias=True,\r\n                   padding='SAME')\r\n    norm3 = batch_norm(conv3, n_filter, is_training, scope=\"bn3\")\r\n    relu3 = tf.nn.relu(norm3)\r\n\r\n    return relu3\r\n\r\n\r\ndef model(x):\r\n  \"\"\"Defines the CNN architecture and returns its output tensor.\"\"\"\r\n  block1 = conv_block(x, 64, \"block1\")\r\n  pool1 = tf.nn.max_pool(block1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\r\n\r\n  block2 = conv_block(pool1, 128, \"block2\")\r\n  pool2 = tf.nn.max_pool(block2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\r\n\r\n  block3 = conv_block(pool2, 256, \"block3\")\r\n  pool3 = tf.nn.max_pool(block3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\r\n\r\n  flat = flatten(pool3)\r\n\r\n  dense1 = dense(flat, 4096, is_training, tf.nn.relu, scope=\"dense1\", dropout=True,\r\n                 initializer=tf.random_normal_initializer(stddev=0.01))\r\n  dense2 = dense(dense1, 4096, is_training, tf.nn.relu, scope=\"dense2\", dropout=True,\r\n                 initializer=tf.random_normal_initializer(stddev=0.01))\r\n  dense3 = dense(dense2, 4096, is_training, tf.nn.relu, scope=\"dense3\", dropout=True,\r\n                 initializer=tf.random_normal_initializer(stddev=0.01))\r\n  dense6 = dense(dense3, NUMBER_OF_CLASSES, is_training, tf.nn.relu, scope=\"dense6\", dropout=False,\r\n                 initializer=tf.random_normal_initializer(stddev=0.01))\r\n  return dense6\r\n\r\n\r\n## ----------------------------------------------------------------------------\r\n## LOSS AND ACCURACY\r\nwith tf.variable_scope(\"model\"):\r\n  batch_size = tf.placeholder(tf.float32, name=\"batch_size\")\r\n\r\n  input_image_batch = tf.placeholder(tf.float32, shape=[BATCH_SIZE, IMG_SIZE, IMG_SIZE, NUM_CHANNELS],\r\n                                     name=\"input_image_batch\")\r\n  input_label_batch = tf.placeholder(tf.float32, shape=[None, NUMBER_OF_CLASSES], name=\"input_label_batch\")\r\n\r\n  logits = model(input_image_batch)\r\n\r\nwith tf.variable_scope(\"loss\"):\r\n  cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\r\n    logits,\r\n    tf.cast(input_label_batch, tf.float32),\r\n    name=\"cross-entropy\")\r\n  loss = tf.reduce_mean(cross_entropy, name='loss')\r\n\r\nwith tf.variable_scope(\"accuracy\"):\r\n  top_1_correct = tf.nn.in_top_k(logits, tf.argmax(input_label_batch, 1), 1)\r\n  top_n_correct = tf.nn.in_top_k(logits, tf.argmax(input_label_batch, 1), 3)\r\n\r\npredictions = tf.argmax(logits, 1)\r\nlabel_batch_id = tf.argmax(input_label_batch, 1)\r\n\r\n## ----------------------------------------------------------------------------\r\n## OPTIMIZER\r\nglobal_step = tf.get_variable('global_step', [],\r\n                              initializer=tf.constant_initializer(0),\r\n                              trainable=False)\r\nlr = tf.placeholder(tf.float32, name=\"learning_rate\")\r\ntrain_op = tf.train.MomentumOptimizer(lr, 0.9).minimize(loss, global_step=global_step)\r\n\r\n## ----------------------------------------------------------------------------\r\n## SUMMARIES\r\nsummary_op = tf.merge_all_summaries()\r\n\r\n## ----------------------------------------------------------------------------\r\n## INITIALIZATION\r\ninit_op = tf.initialize_all_variables()\r\nwriter = tf.train.SummaryWriter(LOGS_PATH, graph=tf.get_default_graph())\r\nsaver = tf.train.Saver()\r\nsess = tf.Session()\r\n\r\n# initialize queue threads\r\ncoord = tf.train.Coordinator()\r\nthreads = tf.train.start_queue_runners(coord=coord, sess=sess)\r\n\r\n# initialize variables\r\nsess.run(init_op)\r\n\r\n## ----------------------------------------------------------------------------\r\n## HELPER FUNCTIONS\r\ndef next_feed_dic(image_batch, label_batch, train=True):\r\n  \"\"\"Fetches a mini-batch of images and labels and builds a feed-dictonary\"\"\"\r\n  with tf.device('/cpu:0'):\r\n    curr_image_batch, curr_label_batch = sess.run([image_batch, label_batch])\r\n\r\n    feed_dict = {\r\n      input_image_batch: curr_image_batch,\r\n      input_label_batch: curr_label_batch,\r\n      batch_size: curr_image_batch.shape[0],\r\n      is_training.name: train,\r\n      lr.name: learning_rate\r\n    }\r\n    return feed_dict\r\n\r\n## ----------------------------------------------------------------------------\r\n## PERFORM TRAINING\r\n\r\n# train cycles\r\nfor j in range(EPOCHS):\r\n  print(\"epoch \", j)\r\n  print(\"epoch.batches curr_loss (avg_loss)\")\r\n  for i in range(TRAIN_BATCHES_PER_EPOCH):\r\n    feed_dict = next_feed_dic(train_image_batch, train_label_batch, train=True)\r\n    _, curr_loss, summary, step = sess.run([train_op, loss, summary_op, global_step], feed_dict=feed_dict)\r\n\r\n    if i % 30 == 0:\r\n      print(\"{:3d}.{:03d} {:.5f}\".format(j, i, curr_loss))\r\n\r\nprint(\"done\")\r\n```\r\n\r\nThe cifar10 input pipeline (this one works fine)\r\n```\r\nimport tensorflow as tf\r\n\r\nfrom utils import cifar10\r\nfrom tensorflow.python.framework import ops\r\n\r\nclass cifar10_data:\r\n  \"\"\"\r\n  Downloads the CIFAR10 dataset and creates an input pipeline ready to be fed into a model.\r\n\r\n  - Reshapes flat images into 32x32\r\n  - converts [0 1] to [-1 1]\r\n  - shuffles the input\r\n  - builds batches\r\n  \"\"\"\r\n  NUM_THREADS = 8\r\n  NUMBER_OF_CLASSES = 10\r\n\r\n  TRAIN_SET_SIZE = 50000\r\n  TEST_SET_SIZE =  10000\r\n  IMAGE_SIZE = 32\r\n  NUM_OF_CHANNELS = 3\r\n\r\n  def __init__(self, batch_size):\r\n    \"\"\" Downloads the cifar10 data if necessary. \"\"\"\r\n    self.batch_size = batch_size\r\n    cifar10.maybe_download_and_extract()\r\n\r\n  def build_train_data_tensor(self, shuffle=False, augmentation=False):\r\n    images, _, targets = cifar10.load_training_data()\r\n    return self.__build_generic_data_tensor(images,\r\n                                            targets,\r\n                                            shuffle,\r\n                                            augmentation)\r\n\r\n  def build_test_data_tensor(self, shuffle=False, augmentation=False):\r\n    images, _, targets = cifar10.load_test_data()\r\n    return self.__build_generic_data_tensor(images,\r\n                                            targets,\r\n                                            shuffle,\r\n                                            augmentation)\r\n\r\n  def __build_generic_data_tensor(self, raw_images, raw_targets, shuffle, augmentation):\r\n    \"\"\" Creates the input pipeline and performs some preprocessing. \"\"\"\r\n\r\n    images = ops.convert_to_tensor(raw_images)\r\n    targets = ops.convert_to_tensor(raw_targets)\r\n\r\n    set_size, width, height, channels = raw_images.shape\r\n\r\n    images = tf.reshape(images, [set_size, width, height, channels])\r\n    image, label = tf.train.slice_input_producer([images, targets], shuffle=shuffle)\r\n\r\n    # Data Augmentation\r\n    if augmentation:\r\n      # TODO\r\n      # make sure after further preprocessing it is [0 1]\r\n      pass\r\n\r\n    # convert the given [0, 1] to [-1, 1]\r\n    image = tf.sub(image, 0.5)\r\n    image = tf.mul(image, 2.0)\r\n\r\n    images_batch, labels_batch = tf.train.batch([image, label], batch_size=self.batch_size, num_threads=self.NUM_THREADS)\r\n\r\n    return images_batch, labels_batch\r\n```\r\n\r\nThe not working SVHN input pipeline (virtually the same input pipeline script)\r\n```\r\nimport tensorflow as tf\r\n\r\nfrom utils import svhn\r\nfrom tensorflow.python.framework import ops\r\n\r\nclass svhn_data:\r\n  \"\"\"\r\n  Downloads the SVHN dataset and creates an input pipeline ready to be fed into a model.\r\n\r\n  - Reshapes flat images into 32x32\r\n  - converts [0 1] to [-1 1]\r\n  - shuffles the input\r\n  - builds batches\r\n  \"\"\"\r\n  NUM_THREADS = 8\r\n  NUMBER_OF_CLASSES = 100\r\n\r\n  TRAIN_SET_SIZE = 73257\r\n  TEST_SET_SIZE =  26032\r\n  IMAGE_SIZE = 32\r\n  NUM_OF_CHANNELS = 3\r\n\r\n  def __init__(self, batch_size):\r\n    \"\"\" Downloads the cifar100 data if necessary. \"\"\"\r\n    self.batch_size = batch_size\r\n    svhn.download_data()\r\n\r\n  def build_train_data_tensor(self, shuffle=False, augmentation=False):\r\n    images, _, targets = svhn.load_training_data()\r\n    return self.__build_generic_data_tensor(images,\r\n                                            targets,\r\n                                            shuffle,\r\n                                            augmentation)\r\n\r\n  def build_test_data_tensor(self, shuffle=False, augmentation=False):\r\n    images, _, targets = svhn.load_test_data()\r\n    return self.__build_generic_data_tensor(images,\r\n                                            targets,\r\n                                            shuffle,\r\n                                            augmentation)\r\n\r\n  def __build_generic_data_tensor(self, raw_images, raw_targets, shuffle, augmentation):\r\n    \"\"\" Creates the input pipeline and performs some preprocessing. \"\"\"\r\n\r\n    images = ops.convert_to_tensor(raw_images)\r\n    targets = ops.convert_to_tensor(raw_targets)\r\n\r\n    set_size, width, height, channels = raw_images.shape\r\n\r\n    images = tf.reshape(images, [set_size, width, height, channels])\r\n    image, label = tf.train.slice_input_producer([images, targets], shuffle=shuffle)\r\n\r\n    # Data Augmentation\r\n    if augmentation:\r\n      # TODO\r\n      # make sure after further preprocessing it is [0 1]\r\n      pass\r\n\r\n    # convert the given [0, 1] to [-1, 1]\r\n    image = tf.sub(image, 0.5)\r\n    image = tf.mul(image, 2.0)\r\n\r\n    images_batch, labels_batch = tf.train.batch([image, label],\r\n                                                batch_size=self.batch_size,\r\n                                                num_threads=self.NUM_THREADS)\r\n\r\n    return images_batch, labels_batch\r\n```", "comments": ["Note in the future this question is best asked on StackOverflow given that it concerns best usage. In fact, to get more help on feeding large data, that is the best place to look (or some of the documentation on the TensorFlow web page).\n\nThe key problem here is that tf.convert_to_tensor() creates tf.constant's behind the scenes. What you need to do is to create a placeholder that represents a batch that will be less than 2GB (likely you want way less than that). Then you can feed parts of your giant numpy array dataset at a time. For a simple reproducer of what's going on with your example look at this:\n\n``` python\nimport tensorflow as tf\nimport numpy as np\nsess=tf.Session()\n\n# make a np array that is 1.1 GB\nsize_in_gb = 1.1 # so that 2 of them will exceed 2GB\nfloat32_bytes = 4\nnelements = int(size_in_gb * float(1<<30) / float32_bytes)\nfoo = np.ones(nelements, dtype=np.float32)\n\n# convert np array to tensors (NOTE: this implicitly creates tf.constant's)\nt1 = tf.convert_to_tensor(foo)\nt2 = tf.convert_to_tensor(foo)\n# This bakes the graphdef and throws an error when it exceeds 2GB\nsess.run(t1 + t2)\n```\n"]}, {"number": 5382, "title": "Fix a potential bug for android demo", "body": "Size of the priority queue will decrease in every iteration, so the terminal condition will change,  but i increase in every iteration, it will lead to some problems. For example, when `MAX_RESULTS = 10 `and` pq.size() = 4` in the beginning, after 2 iterations,  `i = 2`, the  terminal condition will become `i < 2`, but it  is wrong.", "comments": ["@philokey, thanks for your PR! By analyzing the history of the files in this pull request, we identified @andrewharp to be a potential reviewer.\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Do you plan on signing the CLA?\n", "@vrv Yes, The application has just been submitted\u3002\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Jenkins, test this please.\n", "Thanks for the fix!\n"]}, {"number": 5381, "title": "Documentation For The New Version of Histogram on TensorBoard", "body": "Hello~\r\n\r\nI'm trying to interpret the new version of the histogram and it looks like the x-axis represents the value/bin, y-axis represent the step/time, and **z-axis represents the density since it is a continuous number. In addition, the color represents the standard deviation??** \r\n\r\nIs my interpretation correct? If so, I can help putting up the documentation for the new histogram tab.\r\n\r\nThanks!\r\n\r\nNOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["Thanks @TalhuaLI. FYI, @danmane, I assume documentation contributions are welcome. \n", "Yes, documentation contributions would be welcome. As you hypothesize, the axes are bin value, time in steps, and density. The color is actually just a function of which run it is from, and how far back in time it is.\n\nI label the axes x for value, y for density, and z for time (since the oldest steps seem to recede backwards into the distance)\n", "@danmane  Hi Daniel. Thank you for the clarification! I want to confirm one more thing before start writing the description. Is it accurate to state the density the \"probability density\"? Thanks!\n", "It's a normalized density. I wouldn't describe it as a probability density,\nalthough I think calling it one would be justifiable.\nOn Thu, Nov 10, 2016 at 3:03 PM Taihua Li notifications@github.com wrote:\n\n> @danmane https://github.com/danmane Hi Daniel. Thank you for the\n> clarification! I want to confirm one more thing before start writing the\n> description. Is it accurate to state the density the \"probability density\"?\n> Thanks!\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5381#issuecomment-259833998,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABVc19_pwKMXm8rkU7OxJQZy0_m4syRwks5q86LUgaJpZM4KohSg\n> .\n", "Did this get documented somewhere?  I'm trying to understand the tensorboard outputs in detail; a link at the end of this to where the updated docs are would be great. :D", "Take a look here. https://www.tensorflow.org/versions/master/get_started/tensorboard_histograms\r\n\r\nOr in the README for the dashboard, seen here: https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/histograms/README.md"]}, {"number": 5380, "title": "'tensorflow/core/public/session.h' file not found", "body": "I didn't find anything about this problem. I'm creating the python package from the source with bazel, the commands used are the same of the official guide in the section [create pip package](https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#create-the-pip-package-and-install).\r\n\r\nThe file *.whl* generated is working but doesn't have some include files that I use in a new op created in C++. The missing files are **session_options.h** and **session.h** which, however, are present in the official *.whl* of tensorflow that you can download (the binary package pre-compiled).\r\n\r\nTo add those files I had to insert some requirements in *tensorflow/core/BUILD*:\r\n\r\n```\r\n\n---\r\n tensorflow/core/BUILD | 5 +++++\r\n 1 file changed, 5 insertions(+)\r\n\r\ndiff --git a/tensorflow/core/BUILD b/tensorflow/core/BUILD\r\nindex 79546cc..7d9e9a2 100644\r\n--- a/tensorflow/core/BUILD\r\n+++ b/tensorflow/core/BUILD\r\n@@ -214,6 +214,9 @@ cc_library(\r\n         \"platform/strong_hash.h\",\r\n         \"platform/thread_annotations.h\",\r\n         \"platform/types.h\",\r\n+        \"public/version.h\",\r\n+        \"public/session.h\",\r\n+        \"public/session_options.h\",\r\n     ],\r\n     visibility = [\"//visibility:public\"],\r\n     deps = [\r\n@@ -299,6 +302,8 @@ tf_cuda_library(\r\n         \"framework/type_traits.h\",\r\n         \"framework/types.h\",\r\n         \"public/version.h\",\r\n+        \"public/session.h\",\r\n+        \"public/session_options.h\",\r\n         \"util/bcast.h\",\r\n         \"util/cuda_kernel_helper.h\",\r\n         \"util/device_name_utils.h\",\r\n-- \r\n2.10.2\r\n```\r\n\r\nIs it normal or I missed some configuration during the building? I could not fix it without this additions.\r\n\r\n### Environment info\r\nOperating System: macOS Sierra (10.12.1)\r\n\r\nInstalled version of CUDA and cuDNN: NO CUDA\r\n\r\nSource:\r\n\r\n1. The commit hash: \"eaa9dde98d95f843ad1d3d0f5956693991372e4a\"\r\n2. Bazel version:\r\n```\r\nBuild label: 0.3.2-homebrew\r\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Sat Oct 8 08:02:20 2016 (1475913740)\r\nBuild timestamp: 1475913740\r\nBuild timestamp as int: 1475913740\r\n```\r\n\r\n### Example to test\r\n\r\nIf you try to import a graph as you can see in the [guide](https://www.tensorflow.org/versions/r0.11/api_docs/cc/index.html) you need to require the header: `#include \"tensorflow/core/public/session.h\"`.\r\n\r\n\r\n### Logs or other output that would be helpful\r\n```\r\n... fatal error: 'tensorflow/core/public/session.h' file not found\r\n#include \"tensorflow/core/public/session.h\"\r\n         ^\r\n1 error generated.\r\nmake: *** [...] Error 1\r\n```\r\n", "comments": ["Will send a fix, thanks for the report!\n", "I'm getting this error, where is the fix? ", "I'm getting this error, where is the fix?"]}, {"number": 5379, "title": "C++ SetDefaultDevice does not appear to work properly", "body": "I'm using a simple C++ session that works fine otherwise. Trying to use `tensorflow::graph::SetDefaultDevice` to pin on either CPU or GPU (per id) does not appear to work:\r\n\r\n- without `SetDefaultDevice`:\r\n\r\n```\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 1:   Y Y   \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40c, pci bus i\r\nd: 0000:02:00.0)  \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K40c, pci bus i\r\nd: 0000:03:00.0) \r\n```\r\nall goes well beyond this point.\r\n\r\n- now trying to pin on `GPU0`:\r\n```\r\ntensorflow::graph::SetDefaultDevice(\"/gpu:0\", &graph_def);\r\n```\r\nyields\r\n```\r\nE tensorflow/core/common_runtime/direct_session.cc:132] Invalid argument: Could not parse entry in 'visible_device_list': '/gpu:0'.  visible_device_list = /gpu:0\r\n```\r\nSame with `/cpu:0`, no surprise.\r\n\r\n- looking at https://github.com/tensorflow/tensorflow/blob/f7ec99516ce0e0937e0b865e90aa02c748cd36c6/tensorflow/core/common_runtime/gpu/gpu_device.cc#L808 it seems the code is rather looking for an int ? Trying it out:\r\n\r\n```\r\ntensorflow::graph::SetDefaultDevice(\"0\",&graph_def);\r\n```\r\nyields\r\n```\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40c, pci bus id: 0000:02:00.0)\r\nE tensorflow/core/common_runtime/gpu/process_state.cc:108] Invalid allocator type: 0\r\n```\r\n\r\nAm I doing something wrong or has something changed (maybe due to 3e8c4fd7403659ec32b9fec90a78831043aa0786 ?)\r\n\r\nThe tutorial at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/tutorials/example_trainer.cc#L104 still calls on `SetDefaultDevice` the way I tried to use it, this seems odd :)\r\n", "comments": ["Check that this error doesn't occur when you don't call SetDefaultDevice at all. @keveman, do you have any inight?\n", "I'm going to go out on a limb and suggest that you are setting 'visible_device_list' incorrectly in your ConfigProto, and this has nothing to do with SetDefaultDevice.  SetDefaultDevice doesn't touch 'visible_device_list', and the documentation of 'visible_device_list' should clear up what the syntax is supposed to be.  Please comment if you disagree and I'll reopen!\n", "@aselle hi, see the initial report, there's no error when no call is made to `SetDefaultDevice`.\n\n@vrv hi, I can certify we are not touching anything within the `ConfigProto` in our code (it's here actually https://github.com/beniz/deepdetect/blob/tf/src/tflib.cc#L199) besides the `set_allow_growth` option that is set to `true`. \nI believe I understand what `SetDefaultDevice` does and that `visible_device_list` is outside its scope but if you look at the initial report you can see the initial parsing error can be avoided by setting up the device to `0` instead of `/gpu:0` and that with the parsing code `strings::safe_strto32(gpu_id_str, &gpu_id)` looks suspicious to me.\n\nPlease let me know if I can run tests somehow to drive closer to the root of the problem here.\n\nEDIT: could it be at graph `.pb` creation ?\n", "I just can't see a codepath that connects anything in the graph to \"visible_device_list\".  In fact, visible_device_list is _only_ populated here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.cc#L525, which comes from the ConfigProto.\n", "This issue is quite old and hasn't had recent activity. If it is still not working in the latest version of TensorFlow, and please create a new bug. Thank you.", "[ **Windows 10 x64** ] I have found a solution for using \"setDefaultDevice\" with no exception:\r\n\r\nedit **tensorflow/tf_version_script.lds** to:\r\n\r\n> this code is from another issue, but i cannot find it out now\r\n\r\n```\r\ntensorflow {\r\n  global:\r\n    *tensorflow*;\r\n    *perftools*gputools*;\r\n    *TF_*;\r\n    *TFE_*;\r\n    *nsync_*;\r\n    *protobuf*;\r\n  local:\r\n    *;\r\n};\r\n```\r\nthen compile the source( r1.2 is tested ok)\r\nyou will get tensorflow.dll and lib, but if use these two file in your project you will encounter the exception.\r\n please use ***.obj** file instead of ~~tensorflow.dll/lib~~\r\nEdit your **vcxproj** file (add a lot of Object Entries) just like:\r\n```\r\n<ItemGroup>\r\n        <Object Include=\"E:\\twd\\build_from_source\\buildgpu12_setdevice\\tf_core_lib.dir\\$(Configuration)\\arena.obj\" />\r\n    <Object Include=\"E:\\twd\\build_from_source\\buildgpu12_setdevice\\tf_core_lib.dir\\$(Configuration)\\bitmap.obj\" />\r\n    <Object Include=\"E:\\twd\\build_from_source\\buildgpu12_setdevice\\tf_core_lib.dir\\$(Configuration)\\coding.obj\" />\r\n    <Object Include=\"E:\\twd\\build_from_source\\buildgpu12_setdevice\\tf_core_lib.dir\\$(Configuration)\\status.obj\" />\r\n    <Object Include=\"E:\\twd\\build_from_source\\buildgpu12_setdevice\\tf_core_lib.dir\\$(Configuration)\\stringpiece.obj\" />\r\n    <Object Include=\"E:\\twd\\build_from_source\\buildgpu12_setdevice\\tf_core_lib.dir\\$(Configuration)\\threadpool.obj\" />\r\n...\r\n```\r\nmore details see [this link](https://pan.baidu.com/s/12OLFuQ4htdLbNQ-z57408g)\r\n\r\n---\r\nand the exception is gone.\r\nbut :\r\nexe file is larger  (10MB - > 150MB)\r\nno tensorflow.dll depends\r\n", "my solution:\r\ntensorflow1.8\r\nc++\r\n\r\nhttps://blog.csdn.net/cdknight_happy/article/details/81100554", "@knighthappy were you able to resolve the SetDefaultDevice issue?"]}, {"number": 5378, "title": "Issue Linking of rule '//tensorflow/python:_pywrap_tensorflow.so' failed while building", "body": "I have added a new Op kernel in \"user_ops\" folder of \"tensorflow\" source directory. I have also written a test to verify if the Op is successfully implemented. But while building and testing my Op kernel using the command, \"bazel test //tensorflow/user_ops:xxxx_test\", I am getting the following error:\r\nLinking of rule '//tensorflow/python:_pywrap_tensorflow.so' failed: gcc failed: error executing command /usr/bin/gcc -shared -o bazel-out/local-py3-fastbuild/bin/tensorflow/python/_pywrap_tensorflow.so -Wl,--version-script tensorflow/tf_version_script.lds -pthread -Wl,-no-as-needed -B/usr/bin -B/usr/bin ... (remaining 9 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.  \r\nI am unable to build and test my new Op kernel. Please help in resolving this issue as I am blocked because of this error.", "comments": ["Please provide more information (see the template for issue submission). OS? Python Version? Does /usr/bin/gcc exist? What version? What version of bazel? Thanks.\n", "Closing due to lack of activity.\n"]}, {"number": 5377, "title": "How to slice a tensor for custom loss?", "body": "Trying to implement a custom loss function that works as following in Theano:\r\n```\r\n        def sparse_sqe(y_true, y_pred):\r\n            ix = y_true.nonzero()\r\n            return (y_pred[ix] - y_true[ix]) ** 2\r\n```\r\ne.g. y_true = [[0, 0, 0, 0, 5], [9, 0, 0, 0, 0]]\r\ny_pred = [[0.2, 0.1, 0.2, 0, 4.8], [9.5, 0.3, 0.1, 0, 0]]\r\n\r\nwould return ([4.8, 9.5] - [5, 9]) ** 2 = 0.29\r\nThe loss should only be calculated for the y_pred values at the index of the nonzero y_true values.\r\n\r\nI tried\r\n\r\n```\r\nz = tf.constant(0, tf.float32)\r\nix = tf.where(tf.not_equal(y_true, z))\r\n(y_pred[ix] - y_true[ix]) ** 2\r\n```\r\nbut you cannot slice like a numpy array. I also tried tf.gather, but this applies to a collection of tensors, and it will slice the vectors, instead of the elements of those vectors.\r\n\r\n\r\n\r\nAny help would be greatly appreciated.", "comments": ["In the future you should ask usage questions like these on StackOverflow. However...\n\nYou probably are looking for gather_nd does indirect indexing,. On the other hand, that is not necessary and gather_nd does not yet have a gradient. So instead do\n\n``` python\n(y_true - tf.cast(tf.not_equal(y_true, 0), tf.float32) * y_pred)**2\n```\n"]}, {"number": 5376, "title": "Remove hard coded file system scheme checking in tensorboard code", "body": "This pull request addresses https://github.com/tensorflow/tensorflow/issues/5322.\r\n\r\nThis patch also fixes the case `run_name:gs://path` incorrect parsing issue.\r\n\r\nThere is still one file not cleaned up: [directory_watcher.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/summary/impl/directory_watcher.py#L181). However, this does not affecting adding other file systems.\r\n\r\nThat one can't be removed now because in GCS `Stat` for folder is not supported. However, according to [GCS API document](https://cloud.google.com/storage/docs/json_api/v1/objects/list), stat for a folder can be supported by summing sizes of all objects with that common prefix (In fact, our blob store has similar interface, and stat for folder is implemented likewise).", "comments": ["@llhe, thanks for your PR! By analyzing the history of the files in this pull request, we identified @danmane, @jhseu and @yuefengz to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "@jhseu I added a simple pure python fix.\n", "Jenkins, test this please\n"]}, {"number": 5375, "title": "Error cloning linenoise", "body": "This is a helpful note for anyone who might come across the following error, building tensorflow on arch linux (I may also have some funky ssh-agent settings causing this issue):\r\n\r\n\r\n```\r\nERROR: /home/user/testing/tensorflow/tensorflow/tools/tfprof/BUILD:22:1: no such package '@linenoise//': Error cloning repository: ssh://git@github.com/antirez/linenoise/archive/1.0.tar.gz: Auth cancel caused by ssh://git@github.com/antirez/linenoise/archive/1.0.tar.gz: Auth cancel caused by Auth cancel and referenced by '//tensorflow/tools/tfprof:tfprof'.\r\nERROR: /home/user/testing/tensorflow/tensorflow/tools/tfprof/BUILD:22:1: no such package '@linenoise//': Error cloning repository: ssh://git@github.com/antirez/linenoise/archive/1.0.tar.gz: Auth cancel caused by ssh://git@github.com/antirez/linenoise/archive/1.0.tar.gz: Auth cancel caused by Auth cancel and referenced by '//tensorflow/tools/tfprof:tfprof'.\r\nERROR: Evaluation of query \"deps((//tensorflow/... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\r\n```\r\n\r\nI was able to remedy this problem by changing the block for linenoise `tensorflow/workspace.bzl:181` to:\r\n\r\n```\r\n  native.new_local_repository(\r\n    name = \"linenoise\",\r\n    path = \"/path/to/local_repository\",\r\n    build_file = str(Label(\"//:linenoise.BUILD\")),\r\n  )\r\n```\r\n", "comments": ["Thanks for the helpful post.\n", "Had the same issue running on OSX, @Dannyzen's solution fixed it for me."]}, {"number": 5374, "title": "Configure requires user interaction in r0.11", "body": "The ./configure in the r0.11 branch requires user interaction if there is more than one PYTHON_LIBRARY_PATH found by the `util/python/python_config.sh` script. In r0.10 it was possible to run a ./configure without user interaction. Furthermore, in the PR #5135 the script was changed to make it possible to directly specify the PYTHON_LIBRARY_PATH to prevent the need of user interaction.\r\n\r\nI think this fix should also be merged into to the r0.11 branch to ensure tensorflow can be configured/build without user interaction (when there are multiple pythons installed). ", "comments": ["Thanks for bringing this up. Were you offering to contribute a patch to fix this. It looks like #5135 was already merged. Do you have a pointer to the code in ./configure that has regressed?\n", "#5135 was merged into master 12 days ago, while the `python_config.sh` file wasn't touched since September 20th in the r0.11 branch. That's why I think the change has never been merged to r0.11. Do you need a PR for this, or can you just merge/cherry pick the changes of #5135 and #5167? (#5167 is a small fix for #5135).\n", "@gunan, are we open to merging this change into r0.11?\n", "As long as the first python library path is valid, if you run \n`yes \"\" | ./configure` the script executes unattended, without any need for user interaction.\nTherefore, I am not too much in favor of merging this. @yifeif @martinwicke for their opinions.\n\n@andreas-eberle would the suggested execution method work for you?\n", "sgtm\n", "As this has been the default behaviour all along, I will close this issue.\nIt is a nice to have feature, but we have prepared the 0.11 final packages, and this issue currently has a workaround known to work for most cases.\n"]}, {"number": 5373, "title": "tf.assign_from_checkpoint_fn needs write_version argument to pass to tf.Saver", "body": "Hi,\r\naccording to [slim_walkthrough.ipython](https://github.com/tensorflow/models/blob/master/slim/slim_walkthough.ipynb), when we load from checkpoints we can use `assign_from_checkpoint_fn` function.\r\n\r\nBut (maybe i thought) inside of `slim.assign_from_checkpoint_fn`, it create `tf.Saver` but there is no `write_version` args. And in my case, I have some ckpt with saver V2 but default saver version is 1 and `slim.assign_from_checkpoint_fn` doesn't have any code about this version issue so I can't load model using this function.\r\n\r\nAny clue or plan to handle this issue?\r\nThanks.", "comments": ["It looks like this request is to get write_version added as an argument to tf.assign. Could you take this on @concretevitamin or comment on what a better way to solve this problem might be. Thanks!\n", "@nmhkahn: if I understand you correctly, I think you \n1. Used the master branch to produce a V2 checkpoint.\n2. Try to load it with some code (in this case, some contrib code) in an official release (or candidate).\n\nThis is not a supported use case.  If you do your second step also in master, it should work (I pushed out the default version switch this week). \n", "I see. thanks for comment!\n"]}, {"number": 5372, "title": "Option to check every write in TFRecordWriter.", "body": "Referencing this [post](http://stackoverflow.com/questions/40370866/tf-corrupted-record-while-training/40396754#40396754) on stackoverflow.\r\n\r\nThere may be situations when we write huge databases of images into a TFRecord (or at least a few TFRecords). It would be good if there's an option to check every write to prevent such cases of corrupted data happening during training.\r\n\r\n@allenlavoie ", "comments": ["@jkschin You mentioned you have run twice, so did you use the same TFRecord file or recreate it for each run?\n\nIf it's the first case, I guess you are using local disk. It might be just corruption in the IO path, like **broken disk**, bugs in IO controllers, non-ECC memory and bit rot etc. But this should be rare. Using checksum-ed auto recovery (by either duplication or EC code) file system like GFS, HDFS (or simply RAID) can avoid most of the problems. I think **check every write to prevent such cases of corrupted data happening** is not the responsibility of tensorflow.\n\nIf it's the second case, it may be a bug.\n", "@llhe, I could recreate the error. I used the same TFRecord file 4 times. I decided to recreate the TFRecord. It worked.\n", "@llhe the case of corrupted data is definitely something that happens locally. It's probably a black swan event. I'll close this as it's probably not wise to implement such a thing.\n", "@jkschin @allenlavoie May be adding an option **Ignore corrupted record silently** in the reader is a reasonable feature request? Looks like many TF users use poor unstable storage but processing **big** data which data corruption is not that rare.\n"]}, {"number": 5371, "title": "Is tensorflow threadsafe in feedforward prediction of trained models.", "body": "#### Question:\r\n\r\nI'ved trained a text classification model using tensorflow and also wrapped Java interface for feedward prediction. And I'm wondering if tensorflow is threadsafe in feedforward prediction. \r\n\r\nThanks", "comments": ["This question is better addressed on StackOverflow. Also, your question is ill-posed. Thread safety is not a simple concept. It is more a question of is it thread-safe to use something in a particular way. For someone to answer this question, you need to describe the context in which you are going to be setting up your feedforward prediction infrastructure.\n"]}, {"number": 5370, "title": "Change file system FileExists interface to return Status", "body": "The pull request addresses issue https://github.com/tensorflow/tensorflow/issues/5321.\r\n\r\nThe calling code are modified to handle error condition at best effort, if the outer caller function does not return error, it's simply changed to\r\n```c++\r\nFileExists(name, &result).ok() && result\r\n```\r\n\r\nTests:\r\nTests run except windows implementation. HDFS is tested with 2.7.3 pseudo-distributed mode.\r\n\r\nPS: The folowwing tests may fail when all tests run together, but pass when run standalone, should not related to this change\r\n```\r\n//tensorflow/contrib/session_bundle:exporter_test\r\n//tensorflow/tensorboard/backend:server_test\r\n//tensorflow/contrib/tensor_forest/hybrid:k_feature_routing_function_op_test\r\n//tensorflow/contrib/learn:random_forest_test\r\n```", "comments": ["Can one of the admins verify this patch?\n", "@llhe, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @rinugun and @caisq to be potential reviewers.\n", "Thanks for making this change!  It's going to be easier for us to make this change internally, since it changes a core public interface that a lot of people are using internally.  \n\ncc @jhseu @rohan100jain, perhaps use this change as a seed for the internal change, and make sure to credit @llhe ?\n\nAlternatively, we can keep the old interface around so we can pull this change internally without too many problems, and then get rid of it once all of the internal uses are changed.\n", "I actually already have an equivalent change internally that I was going to send out for review today. There are quite a few call sites internally, so it's easier to make the change there.\n\nNote that I'm going to change the signature to:\n`Status FileExists(const string& fname);` where it returns NOT_FOUND when it doesn't exist. That matches some existing conventions that we're using.\n", "Okay, cool, I will close this and we'll have the internal fix pushed out soon.  Thanks @llhe !\n"]}, {"number": 5369, "title": "Support reading data from HDFS with Kerberos", "body": "\r\nNow TensorFlow and read/write data from HDFS cluster. But we have tested that it doesn't support HDFS with Kerberos. The log looks like this.\r\n\r\n![screen shot 2016-11-03 at 15 25 37](https://cloud.githubusercontent.com/assets/2715000/19958692/3415c576-a1dc-11e6-906b-7e4d4af63574.png)\r\n\r\n### Environment info\r\n\r\nOperating System: CentOS 7.0\r\nTensorFlow Version: 0.11.0rc1\r\n\r\nRefer to https://github.com/tensorflow/tensorflow/issues/5316", "comments": ["@jhseu, can you comment on this issue?  Also, for future reference, posting textual errors logs is much more useful, because they get indexed, so people who see similar errors in the future can search for them.\n", "I'm not too familiar with Kerberos, so I don't plan on working on this anytime soon. Contributions are welcome:\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/platform/hadoop\n", "I'll provide a patch for kerberos-enabled HDFS.\n", "Looks like @llhe fixed this."]}, {"number": 5368, "title": "plural should be find", "body": "plural should be find", "comments": ["@leonardgithub, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @vrv and @oxtopus to be potential reviewers.\n", "Can one of the admins verify this patch?\n"]}, {"number": 5367, "title": "correct link to pcre", "body": "correct link is http://ftp.exim.llorien.org/pcre/pcre-8.39.tar.gz", "comments": ["Can one of the admins verify this patch?\n", "@qiaohaijun, thanks for your PR! By analyzing the history of the files in this pull request, we identified @caisq, @kirilg and @jart to be potential reviewers.\n", "@qiaohaijun can you please explain the reason behind this change? The old link seems to work fine, at east on my side.\n", "(Closing, since you have to sign the CLA and our clabot seems to have not checked this)\n"]}, {"number": 5366, "title": "Does tensorflow support structure of nested sequence like Paddle? Like \"each timestep of the input sequence is also a sequence\"?", "body": "For example, \r\n\r\nX=[[x11, x12, x13], [x21, x22], [x31, x32, x33, x34]], Y=[y1, y2, y3]. \r\nThis way the initial state of the sequence could be initialized to **0** vector.\r\n\r\nThanks", "comments": ["It's not really clear what you are asking. However, it looks somewhat like sparse ragged arrays. You might look at our sparse tensor format. Also, this question is more of a TensorFlow usage question, so please re-ask in StackOverflow. GitHub issues is primarily for bugs, feature requests, etc. If you find that nobody's suggestions on StackOverflow are reasonable, then you can come back and request a specific feature.\n"]}, {"number": 5365, "title": "Merge back changes from r0.11", "body": "", "comments": ["@yifeif, thanks for your PR! By analyzing the history of the files in this pull request, we identified @martinwicke, @ahundt and @tensorflower-gardener to be potential reviewers.\n", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "@tensorflow-jenkins test this please\n\n(CLA okay, this is merge from brranch)\n"]}, {"number": 5364, "title": "enable unittests for cmake builds", "body": "-Dtensorflow_BUILD_CC_TESTS=ON will build cc unit test\r\n-Dtensorflow_BUILD_PYTHON_TESTS=ON will enable python kernel tests\r\n\r\nThis might still need to work. Tested under windows, it should also work for other platforms.\r\n\r\nNot all tests are building and passing yet - the once that don't work are in the variable\r\ntf_test_src_simple_exclude and we'll deal with one at a time (my hope is that exposing the tests makes it easier to recruit some help).", "comments": ["Can one of the admins verify this patch?\n", "@guschmue, thanks for your PR! By analyzing the history of the files in this pull request, we identified @mrry, @ageron and @ebrevdo to be potential reviewers.\n", "@tensorflow-jenkins test this please\n", "Thanks for the contribution!\nI will need to create a new build for this, and test it manually on jenkins.\nI will do that in the afternoon.\n", "Cool. If you turn on the cc unit tests: be patient with Jenkins ... lots of binaries that it builds. Took 10+ hours for me, which is not practical for CI but very helpful for overall quality. @mrry suggested we could take a sensible subset of tests for CI.\nOr one could turn on only the python tests for CI - there are maybe 80 of those which is a ton better than nothing.\n", "Thanks for the warning, I almost enabled all tests :)\nI am starting a build now. I will see how the results look.\n", "So I tried to create a jenkins job, it ran the following:\n\n```\ncall \"C:\\program files (x86)\\Microsoft Visual Studio 14.0\\VC\\vcvarsall.bat\"\ncd tensorflow/contrib/cmake\nrmdir build /S /Q\nmkdir build\ncd build\n\"C:\\program files\\cmake\\bin\\cmake.exe\" .. -A x64 -DSWIG_EXECUTABLE=C:\\ProgramData\\chocolatey\\bin\\swig.exe -DPYTHON_EXECUTABLE=\"C:\\Program Files\\Anaconda3\\python.exe\" -DCMAKE_BUILD_TYPE=Release -DPYTHON_LIBRARIES=\"C:\\Program Files\\Anaconda3\\libs\\python35.lib -Dtensorflow_BUILD_PYTHON_TESTS=ON\"\n\"C:\\Program Files (x86)\\MSBuild\\14.0\\Bin\\msbuild\" /p:Configuration=Release /maxcpucount:32 tf_python_build_pip_package.vcxproj\nctest -C Release\n```\n\nHowever, the result is:\nhttp://ci.tensorflow.org/job/tensorflow-pull-requests-windows-cmake-python/1/\n\n```\n00:02:30.752 Test project C:/tf_jenkins/home/workspace/tensorflow-pull-requests-windows-cmake-python/tensorflow/contrib/cmake/build\n00:02:30.757 No tests were found!!!\n```\n\nhow should I modify my script?\n", "I think there is a \"\" thing in your command line.\nchange:\n-DPYTHON_LIBRARIES=\"C:\\Program Files\\Anaconda3\\libs\\python35.lib -Dtensorflow_BUILD_PYTHON_TESTS=ON\"\nto:\n-DPYTHON_LIBRARIES=\"C:\\Program Files\\Anaconda3\\libs\\python35.lib\" \"-Dtensorflow_BUILD_PYTHON_TESTS=ON\"\n", "Thanks for noticing that, you are right.\nRetrying build with the fixed script.\n\nOn Thu, Nov 3, 2016 at 2:51 PM, guschmue notifications@github.com wrote:\n\n> I think there is a \"\" thing in your command line.\n> change:\n> -DPYTHON_LIBRARIES=\"C:\\Program Files\\Anaconda3\\libs\\python35.lib\n> -Dtensorflow_BUILD_PYTHON_TESTS=ON\"\n> to:\n> -DPYTHON_LIBRARIES=\"C:\\Program Files\\Anaconda3\\libs\\python35.lib\"\n> \"-Dtensorflow_BUILD_PYTHON_TESTS=ON\"\n> \n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/5364#issuecomment-258284780,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AHlCObOEDm1TWiluQGHEcYJDIvPYKx_rks5q6ldtgaJpZM4Kn3d6\n> .\n", "yes, sure, that is better. I'll update the pr later today.\n", "Jenkins, test this please.\n", "Sorry for the prolonged review.\nNow this is what I am seeing:\nhttp://ci.tensorflow.org/job/tensorflow-pr-win-cmake-pytest/1/\n\nThe script I execute is this:\n\n```\ncall \"C:\\program files (x86)\\Microsoft Visual Studio 14.0\\VC\\vcvarsall.bat\"\ncd tensorflow/contrib/cmake\nrmdir build /S /Q\nmkdir build\ncd build\n\"C:\\program files\\cmake\\bin\\cmake.exe\" .. -A x64 -DSWIG_EXECUTABLE=C:\\ProgramData\\chocolatey\\bin\\swig.exe -DPYTHON_EXECUTABLE=\"C:\\Program Files\\Anaconda3\\python.exe\" -DCMAKE_BUILD_TYPE=Release -DPYTHON_LIBRARIES=\"C:\\Program Files\\Anaconda3\\libs\\python35.lib\" -Dtensorflow_BUILD_PYTHON_TESTS=ON\n\"C:\\Program Files (x86)\\MSBuild\\14.0\\Bin\\msbuild\" /p:Configuration=Release /maxcpucount:32 tf_python_build_pip_package.vcxproj\nctest -C Release -DPYTHON_EXECUTABLE=\"C:\\Program Files\\Anaconda3\\python.exe\"\n```\n\nAll tests seem to be failing, any idea what is wrong?\nAlso no logs are visible. Can we modify the tf tests cmake file to print out the test log in case of test failure?\n", "Is it possible we need to create a virtualenv, pip install the package, then run the tests?\n", "looked briefly over the output and didn't see something wrong. Don't think you need a virtual env, at least my jenkins is happy without.\nIs there a directory 'Testing' in the build directory ? (I think this is where ctest keeps the last result).\nGetting logs in case of failure: I can add a --output-on-failure.\nWe added the full python path to cmake ... did you use the pip that goes with that python ?\n", "--output-on-failure ... I meant you can call ctest with \n'ctest --output-on-failure' \n", "All I run is the above command I printed, as you see I was missing the pip install stage.\nHowever, I think we do need virtualenv, to make sure the builds stay isolated.\nWe do not want a pip package from a previous run to stay and cause some error messages on the machines.\n", "fair, its better to create an environment. I should do this for my jenkins as well. \n", "While I still see some test failures, the changes in this PR seem to be working fine.\nThe test failures can be addressed separately.\n", "thanks! I can take a look at the failing tests in a bit.\n", "I was not able to get the test failures in jenkins, but when I ran my script manually on a windows machine, I run into issues during \"import tensorflow\".\nIt tells me it cannot load some dll's\nMaybe we are not including all we should in the pip package?\n", "there is some stuff from contrib missing (its bugging me a lot and I'll look at that next) but a plain \nimport tensorflow\nwill work. I can download the wheel from your jenkins and see if it complains on my box. \n", "Let me try a few things, and see if it works.\nIf not, I will create an issue and we can continue discussion there.\n\nOn Fri, Nov 4, 2016 at 4:12 PM, guschmue notifications@github.com wrote:\n\n> there is some stuff from contrib missing (its bugging me a lot and I'll\n> look at that next) but a plain\n> import tensorflow\n> will work. I can download the wheel from your jenkins and see if it\n> complains on my box.\n> \n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/5364#issuecomment-258569682,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AHlCOZ0DKHrHzppG6LumEuzYsh9M1JAjks5q67v1gaJpZM4Kn3d6\n> .\n"]}]