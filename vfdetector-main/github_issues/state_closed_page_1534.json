[{"number": 6905, "title": "format code as code", "body": "Hard to read the very helpful `update_ops` snippet without the proper formatting :)", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "Closing on 3 days of inactivity. @EVaisman please feel free to reopen this PR if you want to work on it further. Thanks."]}, {"number": 6904, "title": "error in computing fft of a float variable", "body": "Python 3.5\r\ntensorflow GPU\r\n\r\nI am unable to compute the 1 D fft. My input values will be positive and negative float values not complex numbers.\r\n`x = tf.Variable([1.2,2,3,4.1,5.9],dtype=tf.float32)\r\nsess.run(tf.fft(x))`\r\nthe error that i get\r\n\r\n`TypeError:Input 'input' of 'FFT' Op has type int32 that does not match expected type of complex64`\r\n\r\nI have tried \r\n`sess.run(tf.batch_fft(x))`\r\n\r\nbut it gives an error as tensorflow has no module batch_fft. \r\ndid I miss any thing in the installation of tensorflow ??\r\n\r\nI tried declaring these variables dtype=tf.complex64\r\n\r\nit gave another error\r\n`InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'FFT' with  these attrs.  Registered kernels:\r\n  <no registered kernels>\r\n\t [[Node: FFT = FFT[](Variable/read)]]`\r\n", "comments": ["This question is probably better addressed on [stackoverflow](http://stackoverflow.com/questions/tagged/tensorflow) as we try to keep the github issues focused on bugs and feature requests, so closing it out here (also, please do provide all the details asked for in the new issue template, such as TensorFlow version, OS, steps-to-reproduce etc.)\r\n"]}, {"number": 6903, "title": "Feeding tensors doesn't stop gradients flowing through", "body": "Consider this code snippet:\r\n\r\n```\r\na = tf.constant(0)\r\nb = a + 1\r\nc = b * 2\r\ngrad, = tf.gradients(c, a)\r\ngrad.eval(feed_dict={b: 0})\r\n```\r\n\r\nThe result is `2`, instead of `0`. To obtain the true gradient, one has to wrap the tensor with an `tf.cond` and feed an additional indicator, which doesn't seem too elegant.\r\n\r\nApologize if I missed something.", "comments": ["It's counter-intuitive, but it would be hard to fix without significantly changing the underlying framework.\r\n\r\nTechnically `tf.gradients` creates a backprop graph, and the result of running this graph can be different from mathematical gradient. For instance, it can produce `None` values which indicate that given input is not connected to output (as opposed to 0 which means it's connected, but has no numeric effect on it).\r\n\r\nHere, the graph you get is below, and when you feed `b` that severs the forward prop chain of computation, but there are still connections from the backprop chain\r\n\r\n![gradients](https://cloud.githubusercontent.com/assets/23068/22034230/72b25b9c-dca0-11e6-989f-188533343e05.png)\r\n", "Is it possible to provide an `is_feed` op that indicates whether a tensor is feed or not? With the help of such op, feed-consistent gradients could be constructed without much difficulty.", "not sure where one would start for implementing `is_fed` op, maybe someone more familiar with executor internals can comments, @zffchen78 @mrry ", "Even if we have `is_fed`, it is not easy to eliminate redundant gradient computation. Suppose that `b = f(a); c = g(b)`, the `d_c / d_b` part will still get computed even if `f` has a feed-aware gradient when `b` is fed.", "So it seems the issue is that you are requesting TensorFlow to fetch `gradients` node which you know is 0 because you know what feeding part of the graph breaks connection. Meanwhile, TensorFlow has to do some computation before arriving to this conclusion. Could you just feed `0` into the gradients node to save the computation in such cases?\r\n\r\nAlso, wrapping gradient tensor in `cond` is not sufficient to remove redundant computation, the whole gradients subgraph must be created inside of a `tf.cond` statement", "Even when feeding 0 to gradients, `d_b / d_a` still sticks around while being redundant. The only way I can think of is to construct a copy of the graph and simply don't mix gradients with feeding.\r\n\r\nBTW, seems that the gradient of `cond` can be optimized harder.\r\n\r\n```\r\na = tf.constant(0., name='a')\r\np = tf.constant(True, name='p')\r\nb = tf.cond(p, lambda: tf.square(a), lambda: tf.constant(0.), name='b')\r\nc = tf.square(b, name='c')\r\ng, = tf.gradients(c, a, name='g')\r\n```\r\n\r\n![graph-run](https://cloud.githubusercontent.com/assets/10446514/22157412/8ca1aa10-df72-11e6-9780-5641eef6f9c3.png)\r\n\r\nThis is a trace from `g.eval(feed_dict={p: False})`. The `c_grad` subgraph is not actually useful, but anyway get executed (not shaded in tensorboard).", "If handling this issue in the framework is not practical, is there some way to throw a warning about this whenever a tensor is fed into a node that has an associated gradient computation (but if this is possible, maybe it's better to just automatically feed in 0 for any associated gradient nodes?). At the very least, I think a warning in the reading data documentation (https://www.tensorflow.org/programmers_guide/reading_data) would help. This was unexpected behavior to me.\r\n\r\nEdit: I see this was stated to be intended behavior in #1388.", "IMHO this is WAI. `feed_dict` is a low-level API and it requires understanding underlying graph and TensorFlow dataflow model. In future, the confusion may be removed by having higher level API's that remove the need to use `feed_dict`", "Agree that this is WAI, for better or worse."]}, {"number": 6902, "title": "Sliding Window Layer", "body": "Hi,\r\n\r\nIs there implementation the sliding window layer like in here: https://github.com/voidrank/caffe-fm/blob/2863b2a41e3f2114d24451f41eeed9ab618262ad/src/caffe/layers/sliding_window_layer.cu ?.\r\n\r\nThanks", "comments": ["This question is probably better asked on [stackoverflow](http://stackoverflow.com/questions/tagged/tensorflow) as we try to keep the github issues focused on bugs and feature requests. "]}, {"number": 6901, "title": "correcting index_to_string_table_from_tensor doc", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "cc @ysuematsu ", "Jenkins, test this please.", "Oops. Sorry for the delay. @tensorflow-jenkins test this please", "Jenkins, test this please.", "`http_util` errors are unrelated."]}, {"number": 6900, "title": "ImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory", "body": "Here is what happens when I try to import tensorflow. I installed using the compile from source method for Linux GPU. This is on Ubuntu 14.04. The file that it says there is no such file or directory is in the path file that I have listed. I've tried searching for help to no avail. \r\n\r\n\r\n(I apologize if this doesn't make a lot of sense. Inexperience, sleep-deprivation, and frustration are the main contributors to my inability to communicate clearly at the moment.)\r\n\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/__init__.py\", line 61, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\n  File \"/usr/lib/python3.4/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\nImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \"<pyshell#1>\", line 1, in <module>\r\n    import tensorflow\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/__init__.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/__init__.py\", line 61, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\n  File \"/usr/lib/python3.4/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\nImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory\r\nFailed to load the native TensorFlow runtime.\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["I encounter the same error, 64 bit machine and my solutions was :\r\n`export LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/your/local/cuda/lib64\"\r\n`\r\n`sudo ldconfig\r\n`\r\nHope my solution can give you some help.", "This question is probably better for [stackoverflow](http://stackoverflow.com/questions/tagged/tensorflow) as we try to keep the github issues focused on bugs and feature requests.\r\n\r\nSee also  https://www.tensorflow.org/get_started/os_setup#optional_linux_enable_gpu_support which is consistent with what @MalcolmSun was suggesting"]}, {"number": 6899, "title": "tf.select missing in v1.0?", "body": "Seems no tf.select in v1.0 and it exists in v0.12(also not marked as depreciated).\r\nIf no tf.select do we have corresponding ops to achieve select ?", "comments": ["Use `tf.where` instead of `tf.select`.\r\n(See [RELEASE.md](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md))\r\n\r\nApologies for the breaking changes, which should reduce considerably once 1.0 stable is released.\r\n\r\nAlso, FYI, there is a tool to help with these upgrades: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/compatibility\r\n\r\nClosing this out, feel free to reopen or file a new issue if your issue hasn't been addressed", "@asimshankar is there a reason this isn't in the docs? Putting the notice of deprecation somewhere in v0.12 or v1.0 docs might save people some time :)", "@mckeesh : As pointed out above, it is in the release notes file ([RELEASE.md](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md)). I'll look into making those notes even more accessible from www.tensorflow.org"]}, {"number": 6898, "title": "Merge master branch into 1.0", "body": "All(well, most) test failures are now addressed. Time to sync the release branch.", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Merge PR, cla OK."]}, {"number": 6897, "title": "compile failed at grpc_remote_worker.cc error: no matching function", "body": "Hi, I am trying to build tensorflow from source. Configure is OK, but bazel build is failed.\r\nEnviroment: Centos-6.5  gcc-4.8.0 bazel-0.42 tensorflow-0.12.1 cuda-8.0\r\n\r\nHere is the error log:\r\nERROR: /root/Downloads/tensorflow/tensorflow/core/distributed_runtime/rpc/BUILD:70:1: C++ compilation of rule '//tensorflow/core/distributed_runtime/rpc:grpc_remote_worker' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 134 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\ntensorflow/core/distributed_runtime/rpc/grpc_remote_worker.cc: In lambda function:\r\ntensorflow/core/distributed_runtime/rpc/grpc_remote_worker.cc:215:68: error: no matching function for call to 'grpc::ClientContext::TryCancel() const'\r\n         call_opts->SetCancelCallback([this]() { context_.TryCancel(); });\r\n                                                                    ^\r\ntensorflow/core/distributed_runtime/rpc/grpc_remote_worker.cc:215:68: note: candidate is:\r\nIn file included from external/grpc/include/grpc++/impl/codegen/call.h:43:0,\r\n                 from external/grpc/include/grpc++/impl/call.h:37,\r\n                 from external/grpc/include/grpc++/channel.h:39,\r\n                 from external/grpc/include/grpc++/grpc++.h:56,\r\n                 from ./tensorflow/core/distributed_runtime/rpc/grpc_util.h:21,\r\n                 from ./tensorflow/core/distributed_runtime/rpc/grpc_remote_worker.h:21,\r\n                 from tensorflow/core/distributed_runtime/rpc/grpc_remote_worker.cc:16:\r\nexternal/grpc/include/grpc++/impl/codegen/client_context.h:296:8: note: void grpc::ClientContext::TryCancel() <near match>\r\n   void TryCancel();\r\n        ^\r\nexternal/grpc/include/grpc++/impl/codegen/client_context.h:296:8: note:   no known conversion for implicit 'this' parameter from 'const grpc::ClientContext*' to 'grpc::ClientContext*'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 13.842s, Critical Path: 11.80s", "comments": ["The `GrpcRemoteWorker::InitContext()` method isn't const-qualified, so I'm not sure why the captured `this->context_` is const in your version of the code. Have you made any modifications?", "@mrry\uff0cno.   The code is git clone from master,  and than checkout to tags/0.12.1,  never change anything.  ", "@liuziwei : Could you share the steps to reproduce? And the output of `git status` on the repository? As @mrry pointed out, the line it's pointing to shouldn't be trying to use a const-qualified method. ", "@asimshankar :\r\nStep 1:  git clone\r\nStep 2: git checkout tags/0.12.1\r\nStep 3: fix issue #6950 -- so git status will show modified:  tensorflow/workspace.bzl\r\nStep 4: ./configure\r\ngoogle cloud: N\r\nHDFS: N\r\nOpenCL: N\r\nCUDA:Y\r\nOther options use default\r\nconfigure is complete\r\nStep 5:   bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\n```\r\ngit status:\r\n# Not currently on any branch.\r\n# Changed but not updated:\r\n#   (use \"git add <file>...\" to update what will be committed)\r\n#   (use \"git checkout -- <file>...\" to discard changes in working directory)\r\n#\r\n#\tmodified:   tensorflow/workspace.bzl\r\n#\r\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\r\n```\r\n\r\nBTW\uff1a I try pip install tensorflow-gpu,  it works!   Finally I give up \"build from source\" .\r\n\r\n", "@liuziwei I tried on a Ubuntu system instead, with `gcc-4.8.4`, and it worked. Closing since we can't repro. Please re-open if you're really sure it's a problem with the TF code."]}, {"number": 6896, "title": " tensorflow/core/platform/cpu_feature_guard.cc:35] The TensorFlow library was compiled to use AVX2 instructions, but these aren't available on your machine. ", "body": "# export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.1-cp27-none-linux_x86_64.whl\r\n# pip install --upgrade $TF_BINARY_URL\r\n\r\nthen got errors!", "comments": ["Could you please add some detail as to which OS / architecture etc. you're running on (the details asked for in the \"New Issue\" template)?\r\n\r\nThat said, it seems that your processor doesn't have AVX2 support and so your best bet is to [install from source](https://www.tensorflow.org/get_started/os_setup#installing_from_sources). Going forward, we haven't yet finalized what architectures are releases should be optimized for, but in general if the architecture is \"too old\", then you'll have to compile from source instead of using the release.\r\n\r\nCCing @petewarden and closing this out as a duplicate of #6809. That issue isn't precisely the same, but it deals with the same overarching problem - what set of optimizations should the releases be built for.\r\n\r\nHope that helps!\r\n", "**I got error even build from source . when run this project.https://github.com/koth/kcws**\r\n\r\nWelcome to Ubuntu 14.04.4 LTS (GNU/Linux 3.13.0-86-generic x86_64)\r\n\r\nLinux nlpDemo 3.13.0-86-generic #130-Ubuntu SMP Mon Apr 18 18:27:15 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n# dmidecode 2.12\r\nSMBIOS 2.8 present.\r\n10 structures occupying 495 bytes.\r\nTable at 0x000F0C80.\r\n\r\nHandle 0x0000, DMI type 0, 24 bytes\r\nBIOS Information\r\n\tVendor: SeaBIOS\r\n\tVersion: rel-1.7.5-0-ge51488c-20140602_164612-nilsson.home.kraxel.org\r\n\tRelease Date: 04/01/2014\r\n\tAddress: 0xE8000\r\n\tRuntime Size: 96 kB\r\n\tROM Size: 64 kB\r\n\tCharacteristics:\r\n\t\tBIOS characteristics not supported\r\n\t\tTargeted content distribution is supported\r\n\tBIOS Revision: 0.0\r\n\r\nHandle 0x0100, DMI type 1, 27 bytes\r\nSystem Information\r\n\tManufacturer: QEMU\r\n\tProduct Name: Standard PC (i440FX + PIIX, 1996)\r\n\tVersion: pc-i440fx-2.1\r\n\tSerial Number: 350d21c1-fd7c-4e8e-a596-bbbb031714a7\r\n\tUUID: 350D21C1-FD7C-4E8E-A596-BBBB031714A7\r\n\tWake-up Type: Power Switch\r\n\tSKU Number: Not Specified\r\n\tFamily: Not Specified\r\n\r\nHandle 0x0300, DMI type 3, 21 bytes\r\nChassis Information\r\n\tManufacturer: QEMU\r\n\tType: Other\r\n\tLock: Not Present\r\n\tVersion: pc-i440fx-2.1\r\n\tSerial Number: Not Specified\r\n\tAsset Tag: Not Specified\r\n\tBoot-up State: Safe\r\n\tPower Supply State: Safe\r\n\tThermal State: Safe\r\n\tSecurity Status: Unknown\r\n\tOEM Information: 0x00000000\r\n\tHeight: Unspecified\r\n\tNumber Of Power Cords: Unspecified\r\n\tContained Elements: 0\r\n\r\nHandle 0x0400, DMI type 4, 42 bytes\r\nProcessor Information\r\n\tSocket Designation: CPU 0\r\n\tType: Central Processor\r\n\tFamily: Other\r\n\tManufacturer: QEMU\r\n\tID: F2 06 03 00 FF FB 8B 0F\r\n\tVersion: pc-i440fx-2.1\r\n\tVoltage: Unknown\r\n\tExternal Clock: Unknown\r\n\tMax Speed: Unknown\r\n\tCurrent Speed: Unknown\r\n\tStatus: Populated, Enabled\r\n\tUpgrade: Other\r\n\tL1 Cache Handle: Not Provided\r\n\tL2 Cache Handle: Not Provided\r\n\tL3 Cache Handle: Not Provided\r\n\tSerial Number: Not Specified\r\n\tAsset Tag: Not Specified\r\n\tPart Number: Not Specified\r\n\tCore Count: 8\r\n\tCore Enabled: 8\r\n\tThread Count: 1\r\n\tCharacteristics: None\r\n\r\nHandle 0x1000, DMI type 16, 23 bytes\r\nPhysical Memory Array\r\n\tLocation: Other\r\n\tUse: System Memory\r\n\tError Correction Type: Multi-bit ECC\r\n\tMaximum Capacity: 16 GB\r\n\tError Information Handle: Not Provided\r\n\tNumber Of Devices: 1\r\n\r\nHandle 0x1100, DMI type 17, 40 bytes\r\nMemory Device\r\n\tArray Handle: 0x1000\r\n\tError Information Handle: Not Provided\r\n\tTotal Width: Unknown\r\n\tData Width: Unknown\r\n\tSize: 16384 MB\r\n\tForm Factor: DIMM\r\n\tSet: None\r\n\tLocator: DIMM 0\r\n\tBank Locator: Not Specified\r\n\tType: RAM\r\n\tType Detail: Other\r\n\tSpeed: Unknown\r\n\tManufacturer: QEMU\r\n\tSerial Number: Not Specified\r\n\tAsset Tag: Not Specified\r\n\tPart Number: Not Specified\r\n\tRank: Unknown\r\n\tConfigured Clock Speed: Unknown\r\n\tMinimum voltage:  Unknown\r\n\tMaximum voltage:  Unknown\r\n\tConfigured voltage:  Unknown\r\n\r\nHandle 0x1300, DMI type 19, 31 bytes\r\nMemory Array Mapped Address\r\n\tStarting Address: 0x00000000000\r\n\tEnding Address: 0x000BFFFFFFF\r\n\tRange Size: 3 GB\r\n\tPhysical Array Handle: 0x1000\r\n\tPartition Width: 1\r\n\r\nHandle 0x1301, DMI type 19, 31 bytes\r\nMemory Array Mapped Address\r\n\tStarting Address: 0x00100000000\r\n\tEnding Address: 0x0043FFFFFFF\r\n\tRange Size: 13 GB\r\n\tPhysical Array Handle: 0x1000\r\n\tPartition Width: 1\r\n\r\nHandle 0x2000, DMI type 32, 11 bytes\r\nSystem Boot Information\r\n\tStatus: No errors detected\r\n\r\nHandle 0x7F00, DMI type 127, 4 bytes\r\nEnd Of Table\r\n\r\n\r\noot@nlpDemo:~# cat /proc/cpuinfo\r\nprocessor\t: 0\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 63\r\nmodel name\t: Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz\r\nstepping\t: 2\r\nmicrocode\t: 0x1\r\ncpu MHz\t\t: 2494.224\r\ncache size\t: 30720 KB\r\nphysical id\t: 0\r\nsiblings\t: 8\r\ncore id\t\t: 0\r\ncpu cores\t: 8\r\napicid\t\t: 0\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl eagerfpu pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm xsaveopt\r\nbogomips\t: 4988.44\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 46 bits physical, 48 bits virtual\r\npower management:", "I have exactly same issue. My cpu does not support avx2 and it will cause core dumped each time when import tensorflow. ", "I had installed DeepSpeech and also a DeepSpeech server. Went to start the server and got an error message - \"2018-01-17 08:21:49.120154: F tensorflow/core/platform/cpu_feature_guard.cc:35] The TensorFlow library was compiled to use AVX2 instructions, but these aren't available on your machine.\r\nAborted (core dumped)\"\r\n\r\nApparently I need to compile TensorFlow on the same computer. Is there a list somewhere to match Kubuntu 17.10.1 and a HP Probook 4330S please ?", "I have exactly same issue", "I have exactly same issue\r\nSystem: Ubuntu 18.04\r\nCPU:  Intel(R) Xeon(R) CPU           E5620  @ 2.40GHz", "i have just installed tensorflow and run simple program by copy paste of correct program. I got this error. Does anyone know how to solve??", "This error ocuured because the computer cpu was too old to support the newer instructions. But we can [compile tensorflow from source](https://www.tensorflow.org/install/source#install_python_dependencies)."]}, {"number": 6895, "title": "Branch 144675800", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 6894, "title": "Branch 144669682", "body": "", "comments": []}, {"number": 6893, "title": "get_matching_files issue", "body": "In [saver.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/saver.py), if the argument of `get_matching_files` is a relative path with no ./ in the beginning, `get_matching_files` will return an empty list. e.g., `get_matching_files(\"filename\")` returns `[]`, while `get_matching_files(\"./filename\")` returns `['filename']`\r\n\r\nThus when using [freeze_graph](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py) with input checkpoint in the same directory, I found the issue below:\r\n\r\n```\r\npython freeze_graph.py \\\r\n--input_graph=input_graph.pb \\\r\n--input_checkpoint=input.ckpt \\\r\n--output_graph=output_graph.pb \\\r\n--output_node_names=Softmax\r\n\r\nInput checkpoint 'input.ckpt' doesn't exist!\r\n```\r\n\r\n", "comments": ["Treatment of `./` in `file_io.py` matches semantics of similar functionality in `io_ops.cc`\r\n\r\nIE, this works\r\n```\r\nfilename_queue = tf.train.string_input_producer([\"./file\"])\r\n_, serialized_example = reader.read(filename_queue)\r\n```\r\n\r\nThis finds nothing\r\n```\r\nfilename_queue = tf.train.string_input_producer([\"file\"])\r\n_, serialized_example = reader.read(filename_queue)\r\n```\r\n", "Got it. I think the PR was in the right direction, but depending on the `io_ops`, maybe in the wrong place. If it's a general bug in `io_ops`, then it should go there, otherwise, it should go in the wrapper code. The problem that I had was really about `./` which is not always portable. Do you want to resubmit the PR again?", "Seems related to this one: https://github.com/tensorflow/tensorflow/issues/4921. I'm not sure if it was fixed already.", "Thanks @ppwwyyxx for the cross-reference! Yes, it's a peculiarity in the lookup. It does look odd.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 6892, "title": "Branch 144656658", "body": "", "comments": []}, {"number": 6891, "title": "log_cdf of Gaussian distribution returns -inf", "body": "\r\nHi all! It seems that the log_cdf for normal distribution (tf.contrib.distributions.Normal.log_cdf) returns -inf for some extreme values: \r\n\r\n```\r\nfrom tensorflow.contrib.distributions import Normal\r\nsess = tf.Session()\r\nprint sess.run(Normal(0., 1.).log_cdf(-6.0))\r\n-inf \r\n```\r\nHowever, with the same mu and sigma for scipy.stats.norm.log_cdf, you get the correct value of -20.7367. Is there something wrong with the tensorflow implementation of log_cdf?\r\n\r\nThanks!\r\n", "comments": ["@ebrevdo : Any thoughts on this?", "It turns out that the issue is solved in the recent version of tensorflow. "]}, {"number": 6890, "title": "Use tf.get_variable() instead of tf.Variable, parameter did not update in every epoch?", "body": "I use the program in the page \r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials/mnist\r\n, just make tf.get_variable() instead of tf.Variable(), other parts are all the same, the result is very different, I doubt whether the parameter update in every epoch? \r\n# The result of using tf.get_variable():\r\nStep 0: loss = 449.45 (0.013 sec)\r\nStep 100: loss = 10.57 (0.004 sec)\r\nStep 200: loss = 3.80 (0.005 sec)\r\nStep 300: loss = 2.06 (0.004 sec)\r\nStep 400: loss = 1.99 (0.004 sec)\r\nStep 500: loss = 2.29 (0.004 sec)\r\nStep 600: loss = 1.77 (0.005 sec)\r\nStep 700: loss = 2.01 (0.004 sec)\r\nStep 800: loss = 2.39 (0.007 sec)\r\nStep 900: loss = 1.71 (0.004 sec)\r\nTraining Data Eval:\r\n  Num examples: 55000  Num correct: 21297  Precision @ 1: 0.3872\r\nValidation Data Eval:\r\n  Num examples: 5000  Num correct: 1886  Precision @ 1: 0.3772\r\nTest Data Eval:\r\n  Num examples: 10000  Num correct: 3780  Precision @ 1: 0.3780\r\nStep 1000: loss = 2.27 (0.009 sec)\r\nStep 1100: loss = 2.45 (0.073 sec)\r\nStep 1200: loss = 1.57 (0.004 sec)\r\nStep 1300: loss = 1.80 (0.004 sec)\r\nStep 1400: loss = 1.49 (0.004 sec)\r\nStep 1500: loss = 1.84 (0.004 sec)\r\nStep 1600: loss = 1.34 (0.004 sec)\r\nStep 1700: loss = 1.44 (0.004 sec)\r\nStep 1800: loss = 1.56 (0.004 sec)\r\nStep 1900: loss = 1.48 (0.004 sec)\r\nTraining Data Eval:\r\n  Num examples: 55000  Num correct: 26545  Precision @ 1: 0.4826\r\nValidation Data Eval:\r\n  Num examples: 5000  Num correct: 2336  Precision @ 1: 0.4672\r\nTest Data Eval:\r\n  Num examples: 10000  Num correct: 4785  Precision @ 1: 0.4785\r\nStep 2000: loss = 1.58 (0.011 sec)\r\nStep 2100: loss = 1.37 (0.004 sec)\r\nStep 2200: loss = 1.50 (0.070 sec)\r\nStep 2300: loss = 1.38 (0.004 sec)\r\nStep 2400: loss = 1.72 (0.004 sec)\r\nStep 2500: loss = 1.58 (0.004 sec)\r\nStep 2600: loss = 1.20 (0.004 sec)\r\nStep 2700: loss = 1.34 (0.004 sec)\r\nStep 2800: loss = 1.49 (0.004 sec)\r\nStep 2900: loss = 1.15 (0.004 sec)\r\nTraining Data Eval:\r\n  Num examples: 55000  Num correct: 30216  Precision @ 1: 0.5494\r\nValidation Data Eval:\r\n  Num examples: 5000  Num correct: 2685  Precision @ 1: 0.5370\r\nTest Data Eval:\r\n  Num examples: 10000  Num correct: 5481  Precision @ 1: 0.5481\r\nStep 3000: loss = 1.19 (0.010 sec)\r\nStep 3100: loss = 1.69 (0.004 sec)\r\nStep 3200: loss = 1.25 (0.004 sec)\r\nStep 3300: loss = 1.25 (0.071 sec)\r\nStep 3400: loss = 1.14 (0.004 sec)\r\nStep 3500: loss = 1.27 (0.004 sec)\r\nStep 3600: loss = 1.50 (0.004 sec)\r\nStep 3700: loss = 1.07 (0.004 sec)\r\nStep 3800: loss = 1.15 (0.004 sec)\r\nStep 3900: loss = 1.12 (0.004 sec)\r\nTraining Data Eval:\r\n  Num examples: 55000  Num correct: 29304  Precision @ 1: 0.5328\r\nValidation Data Eval:\r\n  Num examples: 5000  Num correct: 2684  Precision @ 1: 0.5368\r\nTest Data Eval:\r\n  Num examples: 10000  Num correct: 5234  Precision @ 1: 0.5234\r\nStep 4000: loss = 1.13 (0.008 sec)\r\nStep 4100: loss = 1.23 (0.004 sec)\r\nStep 4200: loss = 1.24 (0.004 sec)\r\nStep 4300: loss = 1.20 (0.004 sec)\r\nStep 4400: loss = 1.03 (0.069 sec)\r\nStep 4500: loss = 0.89 (0.004 sec)\r\nStep 4600: loss = 1.37 (0.004 sec)\r\nStep 4700: loss = 1.13 (0.004 sec)\r\nStep 4800: loss = 1.17 (0.004 sec)\r\nStep 4900: loss = 1.29 (0.004 sec)\r\nTraining Data Eval:\r\n  Num examples: 55000  Num correct: 32965  Precision @ 1: 0.5994\r\nValidation Data Eval:\r\n  Num examples: 5000  Num correct: 2963  Precision @ 1: 0.5926\r\nTest Data Eval:\r\n  Num examples: 10000  Num correct: 5944  Precision @ 1: 0.5944\r\nStep 5000: loss = 1.20 (0.008 sec)\r\nStep 5100: loss = 1.28 (0.004 sec)\r\nStep 5200: loss = 1.09 (0.004 sec)\r\nStep 5300: loss = 0.94 (0.004 sec)\r\nStep 5400: loss = 1.09 (0.004 sec)\r\nStep 5500: loss = 1.23 (0.069 sec)\r\nStep 5600: loss = 1.03 (0.004 sec)\r\nStep 5700: loss = 1.14 (0.004 sec)\r\nStep 5800: loss = 1.25 (0.004 sec)\r\nStep 5900: loss = 1.28 (0.004 sec)\r\nTraining Data Eval:\r\n  Num examples: 55000  Num correct: 33732  Precision @ 1: 0.6133\r\nValidation Data Eval:\r\n  Num examples: 5000  Num correct: 3071  Precision @ 1: 0.6142\r\nTest Data Eval:\r\n  Num examples: 10000  Num correct: 6170  Precision @ 1: 0.6170\r\nStep 6000: loss = 0.99 (0.007 sec)\r\nStep 6100: loss = 1.09 (0.004 sec)\r\nStep 6200: loss = 1.00 (0.004 sec)\r\nStep 6300: loss = 1.00 (0.004 sec)\r\nStep 6400: loss = 1.28 (0.004 sec)\r\nStep 6500: loss = 0.90 (0.004 sec)\r\nStep 6600: loss = 0.73 (0.069 sec)\r\nStep 6700: loss = 1.08 (0.004 sec)\r\nStep 6800: loss = 0.99 (0.004 sec)\r\nStep 6900: loss = 0.89 (0.004 sec)\r\nTraining Data Eval:\r\n  Num examples: 55000  Num correct: 32685  Precision @ 1: 0.5943\r\nValidation Data Eval:\r\n  Num examples: 5000  Num correct: 2972  Precision @ 1: 0.5944\r\nTest Data Eval:\r\n  Num examples: 10000  Num correct: 5963  Precision @ 1: 0.5963\r\nStep 7000: loss = 1.10 (0.009 sec)\r\nStep 7100: loss = 1.32 (0.004 sec)\r\nStep 7200: loss = 0.71 (0.004 sec)\r\nStep 7300: loss = 0.88 (0.004 sec)\r\nStep 7400: loss = 1.04 (0.004 sec)\r\nStep 7500: loss = 1.69 (0.004 sec)\r\nStep 7600: loss = 1.03 (0.004 sec)\r\nStep 7700: loss = 1.32 (0.069 sec)\r\nStep 7800: loss = 0.98 (0.004 sec)\r\nStep 7900: loss = 1.31 (0.004 sec)\r\nTraining Data Eval:\r\n  Num examples: 55000  Num correct: 36517  Precision @ 1: 0.6639\r\nValidation Data Eval:\r\n  Num examples: 5000  Num correct: 3351  Precision @ 1: 0.6702\r\nTest Data Eval:\r\n  Num examples: 10000  Num correct: 6659  Precision @ 1: 0.6659\r\nStep 8000: loss = 0.91 (0.009 sec)\r\nStep 8100: loss = 0.91 (0.004 sec)\r\nStep 8200: loss = 0.95 (0.004 sec)\r\nStep 8300: loss = 1.14 (0.004 sec)\r\nStep 8400: loss = 1.00 (0.004 sec)\r\nStep 8500: loss = 0.97 (0.004 sec)\r\nStep 8600: loss = 0.73 (0.004 sec)\r\nStep 8700: loss = 1.02 (0.004 sec)\r\nStep 8800: loss = 0.81 (0.069 sec)\r\nStep 8900: loss = 0.85 (0.004 sec)\r\nTraining Data Eval:\r\n  Num examples: 55000  Num correct: 37801  Precision @ 1: 0.6873\r\nValidation Data Eval:\r\n  Num examples: 5000  Num correct: 3456  Precision @ 1: 0.6912\r\nTest Data Eval:\r\n  Num examples: 10000  Num correct: 6916  Precision @ 1: 0.6916\r\nStep 9000: loss = 0.59 (0.010 sec)\r\nStep 9100: loss = 1.02 (0.004 sec)\r\nStep 9200: loss = 0.84 (0.004 sec)\r\nStep 9300: loss = 0.79 (0.004 sec)\r\nStep 9400: loss = 0.88 (0.004 sec)\r\nStep 9500: loss = 0.94 (0.004 sec)\r\nStep 9600: loss = 1.05 (0.004 sec)\r\nStep 9700: loss = 0.78 (0.004 sec)\r\nStep 9800: loss = 1.08 (0.004 sec)\r\nStep 9900: loss = 1.12 (0.070 sec)\r\nTraining Data Eval:\r\n  Num examples: 55000  Num correct: 38346  Precision @ 1: 0.6972\r\nValidation Data Eval:\r\n  Num examples: 5000  Num correct: 3526  Precision @ 1: 0.7052\r\nTest Data Eval:\r\n  Num examples: 10000  Num correct: 6997  Precision @ 1: 0.6997\r\n[Finished in 60.3s]\r\n\r\n# The result of using tf.Variable():\r\nExtracting Mnist_data/train-images-idx3-ubyte.gz\r\nExtracting Mnist_data/train-labels-idx1-ubyte.gz\r\nExtracting Mnist_data/t10k-images-idx3-ubyte.gz\r\nExtracting Mnist_data/t10k-labels-idx1-ubyte.gz\r\nWARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.\r\nStep 0: loss = 2.33 (0.013 sec)\r\nStep 100: loss = 2.16 (0.004 sec)\r\nStep 200: loss = 1.90 (0.004 sec)\r\nStep 300: loss = 1.56 (0.004 sec)\r\nStep 400: loss = 1.27 (0.004 sec)\r\nStep 500: loss = 0.84 (0.006 sec)\r\nStep 600: loss = 0.83 (0.004 sec)\r\nStep 700: loss = 0.82 (0.004 sec)\r\nStep 800: loss = 0.56 (0.004 sec)\r\nStep 900: loss = 0.50 (0.005 sec)\r\nTraining Data Eval:\r\n  Num examples: 55000  Num correct: 47541  Precision @ 1: 0.8644\r\nValidation Data Eval:\r\n  Num examples: 5000  Num correct: 4351  Precision @ 1: 0.8702\r\nTest Data Eval:\r\n  Num examples: 10000  Num correct: 8708  Precision @ 1: 0.8708\r\nStep 1000: loss = 0.56 (0.009 sec)\r\nStep 1100: loss = 0.49 (0.090 sec)\r\nStep 1200: loss = 0.63 (0.004 sec)\r\nStep 1300: loss = 0.38 (0.004 sec)\r\nStep 1400: loss = 0.35 (0.004 sec)\r\nStep 1500: loss = 0.46 (0.004 sec)\r\nStep 1600: loss = 0.40 (0.006 sec)\r\nStep 1700: loss = 0.47 (0.004 sec)\r\nStep 1800: loss = 0.33 (0.004 sec)\r\nStep 1900: loss = 0.30 (0.004 sec)\r\nTraining Data Eval:\r\n  Num examples: 55000  Num correct: 49230  Precision @ 1: 0.8951\r\nValidation Data Eval:\r\n  Num examples: 5000  Num correct: 4511  Precision @ 1: 0.9022\r\nTest Data Eval:\r\n  Num examples: 10000  Num correct: 8998  Precision @ 1: 0.8998\r\nStep 2000: loss = 0.33 (0.009 sec)\r\nStep 2100: loss = 0.33 (0.004 sec)\r\nStep 2200: loss = 0.34 (0.069 sec)\r\nStep 2300: loss = 0.45 (0.004 sec)\r\nStep 2400: loss = 0.31 (0.004 sec)\r\nStep 2500: loss = 0.29 (0.005 sec)\r\nStep 2600: loss = 0.17 (0.004 sec)\r\nStep 2700: loss = 0.23 (0.004 sec)\r\nStep 2800: loss = 0.34 (0.006 sec)\r\nStep 2900: loss = 0.39 (0.004 sec)\r\nTraining Data Eval:\r\n  Num examples: 55000  Num correct: 49923  Precision @ 1: 0.9077\r\nValidation Data Eval:\r\n  Num examples: 5000  Num correct: 4585  Precision @ 1: 0.9170\r\nTest Data Eval:\r\n  Num examples: 10000  Num correct: 9114  Precision @ 1: 0.9114\r\nStep 3000: loss = 0.18 (0.007 sec)\r\nStep 3100: loss = 0.25 (0.004 sec)\r\nStep 3200: loss = 0.41 (0.004 sec)\r\nStep 3300: loss = 0.67 (0.071 sec)\r\nStep 3400: loss = 0.28 (0.004 sec)\r\nStep 3500: loss = 0.27 (0.004 sec)\r\nStep 3600: loss = 0.34 (0.004 sec)\r\nStep 3700: loss = 0.50 (0.004 sec)\r\nStep 3800: loss = 0.27 (0.005 sec)\r\nStep 3900: loss = 0.54 (0.004 sec)\r\nTraining Data Eval:\r\n  Num examples: 55000  Num correct: 50419  Precision @ 1: 0.9167\r\nValidation Data Eval:\r\n  Num examples: 5000  Num correct: 4627  Precision @ 1: 0.9254\r\nTest Data Eval:\r\n  Num examples: 10000  Num correct: 9205  Precision @ 1: 0.9205\r\nStep 4000: loss = 0.43 (0.009 sec)\r\nStep 4100: loss = 0.31 (0.004 sec)\r\nStep 4200: loss = 0.31 (0.005 sec)\r\nStep 4300: loss = 0.20 (0.004 sec)\r\nStep 4400: loss = 0.24 (0.070 sec)\r\nStep 4500: loss = 0.40 (0.005 sec)\r\nStep 4600: loss = 0.27 (0.004 sec)\r\nStep 4700: loss = 0.32 (0.005 sec)\r\nStep 4800: loss = 0.30 (0.004 sec)\r\nStep 4900: loss = 0.24 (0.004 sec)\r\nTraining Data Eval:\r\n  Num examples: 55000  Num correct: 50727  Precision @ 1: 0.9223\r\nValidation Data Eval:\r\n  Num examples: 5000  Num correct: 4656  Precision @ 1: 0.9312\r\nTest Data Eval:\r\n  Num examples: 10000  Num correct: 9284  Precision @ 1: 0.9284\r\nStep 5000: loss = 0.40 (0.010 sec)\r\nStep 5100: loss = 0.22 (0.004 sec)\r\nStep 5200: loss = 0.29 (0.004 sec)\r\nStep 5300: loss = 0.21 (0.004 sec)\r\nStep 5400: loss = 0.29 (0.004 sec)\r\nStep 5500: loss = 0.27 (0.072 sec)\r\nStep 5600: loss = 0.17 (0.004 sec)\r\nStep 5700: loss = 0.14 (0.004 sec)\r\nStep 5800: loss = 0.21 (0.004 sec)\r\nStep 5900: loss = 0.32 (0.004 sec)\r\nTraining Data Eval:\r\n  Num examples: 55000  Num correct: 51058  Precision @ 1: 0.9283\r\nValidation Data Eval:\r\n  Num examples: 5000  Num correct: 4677  Precision @ 1: 0.9354\r\nTest Data Eval:\r\n  Num examples: 10000  Num correct: 9329  Precision @ 1: 0.9329\r\nStep 6000: loss = 0.17 (0.010 sec)\r\nStep 6100: loss = 0.35 (0.004 sec)\r\nStep 6200: loss = 0.21 (0.004 sec)\r\nStep 6300: loss = 0.23 (0.004 sec)\r\nStep 6400: loss = 0.15 (0.004 sec)\r\nStep 6500: loss = 0.34 (0.004 sec)\r\nStep 6600: loss = 0.17 (0.074 sec)\r\nStep 6700: loss = 0.29 (0.004 sec)\r\nStep 6800: loss = 0.20 (0.004 sec)\r\nStep 6900: loss = 0.21 (0.004 sec)\r\nTraining Data Eval:\r\n  Num examples: 55000  Num correct: 51370  Precision @ 1: 0.9340\r\nValidation Data Eval:\r\n  Num examples: 5000  Num correct: 4697  Precision @ 1: 0.9394\r\nTest Data Eval:\r\n  Num examples: 10000  Num correct: 9387  Precision @ 1: 0.9387\r\nStep 7000: loss = 0.23 (0.007 sec)\r\nStep 7100: loss = 0.25 (0.004 sec)\r\nStep 7200: loss = 0.16 (0.004 sec)\r\nStep 7300: loss = 0.22 (0.004 sec)\r\nStep 7400: loss = 0.12 (0.004 sec)\r\nStep 7500: loss = 0.31 (0.004 sec)\r\nStep 7600: loss = 0.21 (0.004 sec)\r\nStep 7700: loss = 0.16 (0.071 sec)\r\nStep 7800: loss = 0.18 (0.004 sec)\r\nStep 7900: loss = 0.21 (0.004 sec)\r\nTraining Data Eval:\r\n  Num examples: 55000  Num correct: 51623  Precision @ 1: 0.9386\r\nValidation Data Eval:\r\n  Num examples: 5000  Num correct: 4725  Precision @ 1: 0.9450\r\nTest Data Eval:\r\n  Num examples: 10000  Num correct: 9409  Precision @ 1: 0.9409\r\nStep 8000: loss = 0.26 (0.008 sec)\r\nStep 8100: loss = 0.23 (0.004 sec)\r\nStep 8200: loss = 0.32 (0.004 sec)\r\nStep 8300: loss = 0.16 (0.004 sec)\r\nStep 8400: loss = 0.18 (0.004 sec)\r\nStep 8500: loss = 0.21 (0.004 sec)\r\nStep 8600: loss = 0.26 (0.004 sec)\r\nStep 8700: loss = 0.17 (0.004 sec)\r\nStep 8800: loss = 0.24 (0.070 sec)\r\nStep 8900: loss = 0.12 (0.004 sec)\r\nTraining Data Eval:\r\n  Num examples: 55000  Num correct: 51853  Precision @ 1: 0.9428\r\nValidation Data Eval:\r\n  Num examples: 5000  Num correct: 4739  Precision @ 1: 0.9478\r\nTest Data Eval:\r\n  Num examples: 10000  Num correct: 9444  Precision @ 1: 0.9444\r\nStep 9000: loss = 0.26 (0.009 sec)\r\nStep 9100: loss = 0.13 (0.004 sec)\r\nStep 9200: loss = 0.18 (0.004 sec)\r\nStep 9300: loss = 0.21 (0.004 sec)\r\nStep 9400: loss = 0.13 (0.004 sec)\r\nStep 9500: loss = 0.17 (0.004 sec)\r\nStep 9600: loss = 0.22 (0.004 sec)\r\nStep 9700: loss = 0.10 (0.004 sec)\r\nStep 9800: loss = 0.23 (0.004 sec)\r\nStep 9900: loss = 0.14 (0.073 sec)\r\nTraining Data Eval:\r\n  Num examples: 55000  Num correct: 52047  Precision @ 1: 0.9463\r\nValidation Data Eval:\r\n  Num examples: 5000  Num correct: 4757  Precision @ 1: 0.9514\r\nTest Data Eval:\r\n  Num examples: 10000  Num correct: 9481  Precision @ 1: 0.9481\r\n[Finished in 61.5s]\r\n", "comments": []}, {"number": 6889, "title": "How to load a graph with tensorflow.so and c_api.h in c++ language?", "body": "I can not find any example about how to load graph with tensorflow.so and c_api.h in c++ language.  I read the c_api.h , however, the ReadBinaryProto function was not in it. how can I load a graph without ReadBinaryProto function?", "comments": ["@MisayaZ : This is probably better asked on [stackoverflow](http://stackoverflow.com/questions/tagged/tensorflow) as we try to keep the github issues focused on bugs and feature requests.\r\n\r\nPlease ask around there and I'm sure you'll find an answer."]}, {"number": 6888, "title": "add Erlang distribution", "body": "This is in the family of the Gamma distributions but with a=2.0. ", "comments": ["Can one of the admins verify this patch?", "This looks like the exponential distribution? It's really a special case of Erlang.", "Yes, is another special case of the Gamma distribution, but instead of alpha=1 is alpha=2.0. I am referencing the Machine Learning through a Probabilistic Perspective book.", "What's the difference with [exp](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distributions/python/ops/exponential.py#L37) then?", "On a second thought, the Gamma is a generalization of the Erlang, where instead of using the factorial it uses the Gamma function. The only advantage of adding the Erlang is to constrain the scale parameter to just integers and optimize the kernel to just compute the factorial instead of the gamma function. "]}, {"number": 6887, "title": "Add support for different feature signatures", "body": "Reopening #5546 on request of @martinwicke \r\n\r\nChanges were reverted back for `estimator.py` which allows different input signatures under different operating modes (`EVAL`, `INFER`, `TRAIN`). \r\n", "comments": ["Can one of the admins verify this patch?", "@martinwicke - merged with the master. Can you please review it before it get out of sync again?", "I don't see the zlib fix. Can you use `git pull --rebase` to clean up the history?", "@drpngx - There is no zlib fix, I had to do it before because previous version of master had problem download zlib package. After syncing with master, the problem got resolved. \r\n\r\nI tried doing `rebase` but it's incredibly hard as the conflicts keep happening. Is it not possible for you to review based on just `files changes` ? (there are 3 files changes from `master`.\r\n![image](https://cloud.githubusercontent.com/assets/12864026/22004111/f13f30e8-dc50-11e6-84b0-055ab4e7b6eb.png)\r\n\r\n", "Sure.\n\nOn Jan 16, 2017 5:06 PM, \"Abhi Agg\" <notifications@github.com> wrote:\n\n> @drpngx <https://github.com/drpngx> - There is no zlib fix, I had to do\n> it before because previous version of master had problem download zlib\n> package. After syncing with master, the problem got resolved.\n>\n> I tried doing rebase but it's incredibly hard as the conflicts keep\n> happening. Is it not possible for you to review based on just files\n> changes ? (there are 3 files changes from master.\n> [image: image]\n> <https://cloud.githubusercontent.com/assets/12864026/22004111/f13f30e8-dc50-11e6-84b0-055ab4e7b6eb.png>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/6887#issuecomment-272996066>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_Sbc4N4dnam7HIm4P3DW3jAfLKu7wMks5rTBQfgaJpZM4LlFlB>\n> .\n>\n", "@drpngx - Alright, I managed to `rebase` after all. Hope this helps.", "Thanks!\n\nOn Jan 16, 2017 5:27 PM, \"Abhi Agg\" <notifications@github.com> wrote:\n\n> @drpngx <https://github.com/drpngx> - Alright, I managed to rebase after\n> all. Hope this helps.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/6887#issuecomment-272998472>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbTZaX5JhtfwoaWjUMIDVSAMscVehks5rTBjqgaJpZM4LlFlB>\n> .\n>\n", "@drpngx, I'll have to test this internally reform we merge it.\n", "@martinwicke - would be great if you can test it sooner, as I am afraid it will go out of sync with master again. thanks!", "Jenkins, test this please.", "I believe kmeans_test will fail.", "Jenkins, test this please.", "@martinwicke - Thanks for the comments, Will have a look soon.", "Also please merge conflicts. Thanks!", "Jenkins, test this please!\r\n", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Jenkins, test this please!", "Jenkins? Test this please!", "@abhitopia please resolve the merge conflict.", "@abhitopia any updates? ", "@vrv - Will try to work on it today. Let's see how far I get.", "@martinwicke - regarding your comment on `create_example_parser_from_signatures`, I tried to backtrace it's usage and found that the only usage of this function is in `estimator.py` at \r\n\r\n![image](https://cloud.githubusercontent.com/assets/12864026/23106148/e57fea4a-f6e0-11e6-8ee6-3d3e82734c58.png)\r\n\r\nwhich is **deprecated**. Further, this deprecated function is used in `export.py` at one place \r\n\r\n![image](https://cloud.githubusercontent.com/assets/12864026/23106161/28b61492-f6e1-11e6-89a8-0cb0cff3afa8.png)\r\n\r\nwhich is **deprecated** as well. As such, I am not sure what is the reason to fix this? \r\n\r\nAnyways, I have made some changes that might fix the problem, but since there are no tests that fail, I have no way of testing.\r\n\r\nPlease let me know what you think.\r\n", "So all tests pass except those in under `tensorflow/compiler` which are probably unrelated with the changes I made.\r\n\r\nCC: @vrv, @rmlarsen , @drpngx ", "Jenkins, test this please.", "pylint failed, see below. Also, the CLA check failed, because you need to squash the commits that were merged from other people. Could you do that?\r\n\r\n```\r\n=== Sanity check step 1 of 7: do_pylint PYTHON2 (Python 2 pylint) ===\r\n\r\nERROR_WHITELIST=\"^tensorflow/python/framework/function_test\\.py.*\\[E1123.*noinline ^tensorflow/python/platform/default/_gfile\\.py.*\\[E0301.*non-iterator ^tensorflow/python/platform/default/_googletest\\.py.*\\[E0102.*function\\salready\\sdefined ^tensorflow/python/platform/gfile\\.py.*\\[E0301.*non-iterator\"\r\nRunning pylint on 1389 files with 32 parallel jobs...\r\n\r\nThis option u'required-attributes' will be removed in Pylint 2.0This option u'ignore-iface-methods' will be removed in Pylint 2.0\r\npylint took 89 s\r\n\r\n\r\nFAIL: Found 1 non-whitelited pylint errors:\r\ntensorflow/contrib/learn/python/learn/estimators/estimator_test.py:140: [E0102(function-redefined), extract] function already defined line 132\r\n\r\n\r\n\r\n=== Sanity check step 2 of 7: do_pylint PYTHON3 (Python 3 pylint) ===\r\n\r\nERROR_WHITELIST=\"^tensorflow/python/framework/function_test\\.py.*\\[E1123.*noinline ^tensorflow/python/platform/default/_gfile\\.py.*\\[E0301.*non-iterator ^tensorflow/python/platform/default/_googletest\\.py.*\\[E0102.*function\\salready\\sdefined ^tensorflow/python/platform/gfile\\.py.*\\[E0301.*non-iterator\"\r\nRunning pylint on 1389 files with 32 parallel jobs...\r\n\r\nThis option 'required-attributes' will be removed in Pylint 2.0This option 'ignore-iface-methods' will be removed in Pylint 2.0\r\npylint took 92 s\r\n\r\n\r\nFAIL: Found 1 non-whitelited pylint errors:\r\ntensorflow/contrib/learn/python/learn/estimators/estimator_test.py:140: [E0102(function-redefined), extract] function already defined line 132\r\n```", "@abhitopia Can you please address drpngx's comments? ", "Note: We're in the process of moving Estimator to core. It will lose a bunch of deprecated functionality, but it will also lose the restriction that this PR is addressing. ", "`tf.estimator.Estimator` is now a thing. It should not have this issue.\r\n\r\nWe'll stop supporting contrib.learn.Estimator shortly, so I'll close this issue. I'm sorry this has been such a saga. Thank you for your work, it has helped improve the final result."]}, {"number": 6886, "title": "fix comment misspell", "body": "", "comments": ["Can one of the admins verify this patch?", "Neither are actual words :-)"]}, {"number": 6885, "title": "Rename parameter in Exponential distribution", "body": "Current\r\n\r\n```python\r\ndistributions.Exponential(lam=2.0)\r\n```\r\nI had to think a bit before figuring out that `lam` stands for `lambda`. Following the MLP book I think a better name would be `rate` or `average_rate`. \r\n\r\n> This distribution describes the times between events in a process in which events occur continuously and independently at a constant average rate _lambda_ \r\n\r\n```python\r\ndistributions.Exponential(rate=2.0)\r\n```\r\n\r\nNumpy uses another parameterization: https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.exponential.html\r\n\r\n", "comments": ["@ebrevdo : Any thoughts on this?", "We're in the process of renaming all distribution params to English.", "Looks like the naming has been updated https://www.tensorflow.org/api_docs/python/tf/contrib/distributions/Exponential\r\nClosing this issue."]}, {"number": 6884, "title": "Dead link to Graph.control_dependencies()", "body": "\"tf.control_dependencies(control_inputs) {#control_dependencies}\r\n\r\nWrapper for Graph.control_dependencies() using the default graph.\r\nSee **Graph.control_dependencies()** for more details.\"\r\n\r\nThe link to **Graph.control_dependencies()** is dead, but I could really need the details. : 3\r\n\r\n(Hope this is the right place the post this. X: )\r\n", "comments": ["@GothaB : I'm guessing you were looking at some markdown files in github. Those may not have correct links, but the API docs on the TensorFlow website (www.tensorflow.org/api_docs/) are probably a better place to browse documentation anyway. \r\n\r\nSee [`tf.control_dependencies`](https://www.tensorflow.org/api_docs/python/framework/utility_functions#control_dependencies) linked there and the link to [`Graph.control_dependencies`](https://www.tensorflow.org/api_docs/python/framework/core_graph_data_structures#Graph.control_dependencies) from it.\r\n\r\nClosing this issue out since I believe it has been resolved. Feel free to reopen/create a new one if I misunderstood."]}, {"number": 6883, "title": "ERROR: Assign requires shapes of both tensors to match. lhs shape= [1000] rhs shape= [1001]", "body": "Searched for possible solutions, this page references using --Label-offsets=1 to use vgg or ResNet, here however I'm using InceptionV3 with imagenet TFRecord format, and the failure message occurs during initialization of the session.\r\n\r\nOperating System: Windows 10/64 or LINUX\r\n\r\nInstalled version of CUDA and cuDNN: \r\ncublas64_80.dll locally\r\ncudnn64_5.dll\r\ncufft64_80.dll\r\nnvcuda.dll locally\r\ncurand64_80.dll locally\r\n\r\nIf installed from binary pip package, provide:\r\n0.12.1\r\n\r\n\r\nCODE:\r\n    init_fn = slim.assign_from_checkpoint_fn(model_path,slim.get_model_variables('InceptionV3'))\r\n\r\n    with tf.Session() as sess:\r\n        # Load weights\r\n        init_fn(sess)\r\n\r\n\r\nERROR MESSAGE\r\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [1000] rhs shape= [1001]\r\n         [[Node: save/Assign_8 = Assign[T=DT_FLOAT, _class=[\"loc:@InceptionV3/AuxLogits/Conv2d_2b_1x1/biases\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](InceptionV3/AuxLogits/Conv2d_2b_1x1/biases, save/RestoreV2_8)]]\r\n", "comments": ["That doesn't seem like a bug in tensorflow -- that errors means you are a loading 1001 elements into variable with 1000 elements", "Thanks. W", "If you have a directory including the model trained before, you could try to remove it, then re-run.", "work\uff01", "thanks :+1:  it works!", "In my case, I got the error when distributing my training session into multiple worker threads. What solved it for me was surrounding the inference graph with a tf.variable_scope(name, reuse=tf.AUTO_REUSE). I guess tensorflow can't keep track of your variables when working in a distributed mode.", "It is absolutely insane that the error message has no relation whatsoever with the solution. Any insights on why does this happen?", "> If you have a directory including the model trained before, you could try to remove it, then re-run.\r\n\r\nIf I delete them, how can  my program load the trained model  ??? ", "> > If you have a directory including the model trained before, you could try to remove it, then re-run.\r\n> \r\n> If I delete them, how can my program load the trained model ???\r\n\r\nfinetune?", "> > > If you have a directory including the model trained before, you could try to remove it, then re-run.\r\n> > \r\n> > \r\n> > If I delete them, how can my program load the trained model ???\r\n> \r\n> finetune?\r\n\r\nyes, fine-tune Inception V3 model. what does it mean by removing pertained model?", "> > > > If you have a directory including the model trained before, you could try to remove it, then re-run.\r\n> > > \r\n> > > \r\n> > > If I delete them, how can my program load the trained model ???\r\n> > \r\n> > \r\n> > finetune?\r\n> \r\n> yes, fine-tune Inception V3 model. what does it mean by removing pertained model?\r\n\r\nif finetune, don's remove the pretrained model and get part of parameters", "Hi, @apiszcz You should look the function \"assign_from_checkpoint_fn ( ) \". Find code \r\n\"for var in var_dict:\", the below codes will load each layer weights. You can print \"var\" to see all the layer names, and wirte codes to let the sess not load the layer which has different shape.   ", "> If you have a directory including the model trained before, you could try to remove it, then re-run.\r\n\r\nwhy do this, what the trained model do???", "ImageNet defaults 1000 classes but the related weights downloaded has 1001 classes. It it the source of the conflict. \r\n\r\nIt is necessary to set \"labels_offset=1\". I know the following solution in the bazel environment. But I have not know where \"labels_offset=1\" can be inserted in the non-bazel environment.\r\n\r\nNotes:\r\n\r\nFor the bazel environment, please take the following reference. \r\n\r\n./bazel-bin/slim/train\r\n--train_dir=${TRAIN_DIR}\r\n--dataset_dir=${DATASET_DIR}\r\n--dataset_name=imagenet\r\n--dataset_split_name=train\r\n--model_name=resnet_v1_50\r\n--checkpoint_path=${CHECKPOINT_PATH}\r\n--labels_offset=1\r\n\r\n", "> > > > > If you have a directory including the model trained before, you could try to remove it, then re-run.\r\n> > > > \r\n> > > > \r\n> > > > If I delete them, how can my program load the trained model ???\r\n> > > \r\n> > > \r\n> > > finetune?\r\n> > \r\n> > \r\n> > yes, fine-tune Inception V3 model. what does it mean by removing pertained model?\r\n> \r\n> if finetune, don's remove the pretrained model and get part of parameters\r\n\r\nDid you get the solution? I also want to fine tune my model"]}, {"number": 6882, "title": "CPU version  AttributeError: 'module' object has no attribute \u2018pack\u2019", "body": "when I was trying training a fcn with the latest cpu version tensoflow,I got an error:\r\n\r\n> setting up vgg initialized conv layers ...\r\nTraceback (most recent call last):\r\n  File \"FCN.py\", line 285, in <module>\r\n    tf.app.run()\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"FCN.py\", line 154, in main\r\n    pred_annotation, logits = inference(image, keep_probability)\r\n  File \"FCN.py\", line 129, in inference\r\n    deconv_shape3 = tf.pack([shape[0], shape[1], shape[2], NUM_OF_CLASSESS])\r\nAttributeError: 'module' object has no attribute 'pack'\r\n\r\n\r\nMy tensorflow is installed from tensorflow-0.12.1-cp27-none-linux_x86_64.whl.  Seemingly there's no pack function of tf or in other module? ", "comments": ["@MalcolmSun : Are you sure you're using 0.12.1 and not a nightly, 1.0.0-alpha or head build (you can confirm with a `print tf.__version__`)?\r\n`tf.pack` should exist in 0.12.1 but has been replaced with `tf.stack` after that. \r\n\r\nSo, regardless, switching to `tf.stack` should work (it is available in 0.12.1).\r\nDo let us know if it really is 0.12.1 that is having this trouble.", "@asimshankar Thanks for your help, the tf.__version__ is 0.12.head. After replaced by stack, the code encoutered another error:\r\n`AttributeError: 'module' object has no attribute 'image_summary'`\r\nWhere can I find the API change log, so I can replace the old apis by myself. By the way, How can I install  tensorflow with a specified version. I only find the latest version in the install page.", "@MalcolmSun try https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/compatibility to automatically upgrade your files -- let us know if that works.", "@MalcolmSun \r\n\r\n- 0.12.head implies compiled from source, just FYI\r\n- As @vrv pointed out, we have some tools to help with the upgrade but we also list out the breaking changes in [RELEASE.md](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md). Note that there is a bit of churn these days as we work towards 1.0. Once 1.0 is out, as per [semver](https://www.semver.org) (our [versioning policy](https://www.tensorflow.org/resources/versions)) things will be much more stable\r\n- You can use `pip` support to install specific versions (e.g. `pip install \"tensorflow==0.12.0\"`)", "@asimshankar @vrv  Thanks for your help. @vrv The tool works,thank you."]}, {"number": 6881, "title": "Add custom operator code generator script", "body": "This script uses jinja2 templates to generate the following:\r\n* C++ Header file that defines the operator class, templated on Device.\r\n* C++ Header file that defines the CPU implementation of the operator.\r\n* C++ Source file with Shape Function, REGISTER_OP and  REGISTER_KERNEL_BUILDER constructs.\r\n* Cuda Header that defines the GPU implementation of the operator,  including a CUDA kernel.\r\n* Cuda Source file with GPU REGISTER_KERNEL_BUILDER's for the operator.\r\n* python unit test case, which constructs random input data, and calls   the operator.\r\n* Makefile for compiling the operator into a shared library, using g++   and nvcc.\r\n\r\nOperator inputs, outputs, polymorphic type attributes, other attributes and documentation can be specified.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I felt the content in the README.md was too large to be included in the [Adding a New Op howto](https://www.tensorflow.org/how_tos/adding_an_op/), but it may be appropriate to add a link to this tool in that file.", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Thanks! While it's nice, it'll take some effort to keep this in sync. I would suggest trying for contrib instead.", "Indeed, this is cool, but probably would be better as an external repo for now. GitHub doesn't provide great community tools for managing and supporting external code that isn't core to the codebase.  Can this live on your own repo on GitHub for now?  ", "Apologies for not getting back sooner -- other commitments required my attention\r\n\r\nI've packaged this up at https://github.com/sjperkins/tfopgen.\r\n\r\nIts also available on pypi -- `pip install tfopgen`"]}, {"number": 6880, "title": "tf.gfile.GFile issue on windows", "body": "When I use tf.gfile.GFile and set mode='rb' to open a file on windows, it will read as 'str' instead of 'byte'. For example,\r\n\r\nf = tf.gfile.GFile(\"d2f42068.ini\", mode='rb')\r\nf.readline()\r\nThe result shows\r\n'<KVList>\\n'.\r\nIt should be \r\nb'<KVList>\\n'\r\n\r\nI find the gfile.py replace the 'r' to empty.\r\ndef __init__(self, name, mode='r'):\r\n    mode = mode.replace('b', '')\r\n    super(FastGFile, self).__init__(name=name, mode=mode)\r\n\r\nThis issue will cause some error of data reading on windows.\r\n\r\nMy OS is win10 x64. The version of tensorflow is r0.12. ", "comments": ["Did you use python3? See #6669.", "@inflation Yes, my python version is 3.5.2", "Thanks for pointing out the other bug @inflation .\r\n\r\n@happytian : Closing this one out as a duplicate of #6669 . Let's have any further discussion/updates there."]}, {"number": 6879, "title": "TensorBoard variable name formatting", "body": "Variable names in TensorBoard going from 0.12rc0 to the latest 0.12.1 do not support spaces and special characters anymore. \r\n\r\nI would like to have spaces and percentage signs (for example) supported (again).\r\n\r\nBelow should say: `cpu->gpus FIFO capacity %`\r\n\r\n![image](https://cloud.githubusercontent.com/assets/7721540/21986760/5cbc0866-dc09-11e6-9f81-c4761af3cb90.png)\r\n", "comments": ["@dandelionmane @jart : Any thoughts on this?", "Do the following commit messages answer your question?\r\n\r\n- https://github.com/tensorflow/tensorflow/commit/af1c9a17a4ae1b2cf1feee2e81e102433f5d09c1\r\n- https://github.com/tensorflow/tensorflow/commit/02a3242748b73c43821900c01f0a3db3a2cf4c19\r\n\r\n@dandelionmane may wish to comment more.", "Closing due to lack of recent activity. Please update the issue if it persists and we will reopen.", "I'm going to add support for this back in, hopefully this quarter. Hang tight."]}, {"number": 6878, "title": "Symlink loop FATAL error", "body": "Hello all,\r\nI started teaching myself some tensor flow, and i faced a problem when trying to retrain an existing image classification model with a new class following that tutorial:\r\nhttps://www.tensorflow.org/how_tos/image_retraining/\r\n\r\nSince i installed tensorflow through pip and after reading a few issues, i had to download and install tensorflow from the source. when i got to configuring the install (calling ./configure) it gave me the following error:\r\n\r\n```\r\n____Loading package: tensorflow/contrib/lookup\r\n____Loading package: tensorflow/java/src/main/native\r\n____Loading package: tensorflow/tools/quantization\r\n____Loading package: tensorflow/compiler/aot\r\n____Loading package: tensorflow/core\r\n____Loading package: tensorflow/contrib/quantization\r\n____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 1,508,324 bytes\r\n____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 3,342,484 bytes\r\nERROR: infinite symlink expansion detected\r\n[start of symlink chain]\r\n/usr/local/Cellar/python/2.7.12_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages\r\n/usr/local/lib/python2.7/site-packages\r\n/usr/local/lib/python2.7/site-packages/tensorflow/util/python/python_lib\r\n[end of symlink chain]\r\n```\r\n\r\nIs there any particular way i can resolve this? I would really appreciate any input on the matter.\r\n\r\nThanks!\r\n\r\n\r\n", "comments": ["@evangello : Could you elaborate on your setup (ideally all the information asked for in the new issue template including OS, OS version, steps to reproduce etc.)? Without that, it is a bit a hard to figure out the problem since it isn't common in other setups.", "Hello @asimshankar, thanks for your reply. Im working on a mac El Capitan. I am running \"./configure\" in the main tensorflow directory and putting \"n\" to all the prompts except for when it prompts me about the location of the python bin.", "This seems like an issue with the location of the custom Python bin you are providing, possibly some issue with how Python is installed with homebrew.\r\n\r\nSince this seems like a custom setup, I'm going to close the issue here and suggest that you try [stackoverflow](http://stackoverflow.com/questions/tagged/tensorflow) for support as we try to keep the github issues focused on bugs and feature requests.\r\n\r\nIf I misunderstood (and this is a bug on our end) please feel free to reopen or file a new issue"]}, {"number": 6877, "title": "TF-slim: slim.evaluation.evaluation_loop results in code run that is stuck without progress.", "body": "Hi all, I am unsure why the evaluation loop would result in the code not running. My GPU memory is not consumed or whatsoever, and my CPU memory usage is at a minimum. This is what I get:\r\n\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\nStarting Evaluation of Validation Dataset\r\npredictions type: Tensor(\"Cast_2:0\", shape=(?,), dtype=float32)\r\nRunning evaluation Loop...\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/evaluation.py:426 in evaluation_loop.: __init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\r\nINFO:tensorflow:Waiting for new checkpoint at ../preprocessedData/model.ckpt-50\r\n```\r\n\r\nAll I did was to run this:\r\n\r\n```\r\nprint 'Starting Evaluation of Validation Dataset'\r\n#Load the validation dataset for evaluation while training\r\ntest_dataset = get_split('validation', train_dir)\r\ntest_images, _, test_labels = load_batch(test_dataset, height = image_size, batch_size = batch_size, width = image_size, is_training = False)\r\n\r\nwith slim.arg_scope(inception_resnet_v2_arg_scope()):\r\n\tlogits, _ = inception_resnet_v2(test_images, num_classes = test_dataset.num_classes, is_training = False)\r\n\r\npredictions = tf.argmax(logits, 1)\r\npredictions = tf.cast(predictions, tf.float32)\r\nprint 'predictions type:', predictions\r\n# print tf.Print(predictions)\r\n\r\n# Define the metrics:\r\nnames_to_values, names_to_updates = slim.metrics.aggregate_metric_map({\r\n\t'streaming_auc': slim.metrics.streaming_auc(predictions, test_labels),\r\n    # 'eval/Accuracy': slim.metrics.streaming_accuracy(predictions, test_labels),\r\n    # 'eval/Recall@5': slim.metrics.streaming_recall_at_k(logits, test_labels, 5),\r\n})\r\n\r\nprint'Running evaluation Loop...'\r\ncheckpoint_path = tf.train.latest_checkpoint(log_dir)\r\nmetric_values = slim.evaluation.evaluation_loop(\r\n    master='',\r\n    checkpoint_dir=checkpoint_path,\r\n    logdir=log_dir,\r\n    eval_op=names_to_updates.values(),\r\n    final_op=names_to_values.values())\r\n```\r\nIt's been a really long time since there is no progress. Is the function working?", "comments": ["Thanks @kwotsin ! First off, you probably want to pay heed to the warning and see if your problem disappears after that. Then, I would suggest using `tf.Print` the tf debugger, or asking for parts of the graph that are earlier than that. If that doesn't work for you, please ask piecemeal questions on stackoverflow, where we monitor all questions with the tag \"tensorflow\".", "Hi @kwotsin ,\r\nI am trying to evaluate InceptionV3 on Imagenet using the same script but with slim.evaluation.evaluate_once() and I see no usage of GPU (RTX 2080) or any significant CPU activity. Is there a solution to the issue?\r\nThank you!  "]}, {"number": 6876, "title": "Python 3.6 support", "body": "Run pip install tensorflow under python 3.6 get following:\r\n pip install tensorflow\r\nCollecting tensorflow\r\n  Could not find a version that satisfies the requirement tensorflow (from versions: )\r\nNo matching distribution found for tensorflow", "comments": ["I'm guessing you mean python 3.6?\r\n\r\nWe haven't finished testing with it just yet, so there are no releases for it. \r\n\r\nCC @martinwicke @gunan @yifeif - Will pips for py 3.6 be coming soon?", "Yes, you are right. I've modified the title. Thanks.", "Duplicate of #6533\r\nNot in immediate plans, but we are considering py 3.6\r\nPlease follow instructions in the above issue to install for python 3.6."]}]