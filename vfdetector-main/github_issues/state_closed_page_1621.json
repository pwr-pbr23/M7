[{"number": 4281, "title": "Seq2Seq example - shape error while decoding", "body": "When I try to run the tutorial available at [https://www.tensorflow.org/versions/r0.10/tutorials/seq2seq/index.html](https://www.tensorflow.org/versions/r0.10/tutorials/seq2seq/index.html) it works great. The model is training and the proximity is decreasing. \n\nFor training I used the default data and default command:\n`python translate.py  --data_dir [your_data_directory] --train_dir [checkpoints_directory]  --en_vocab_size=40000 --fr_vocab_size=40000` (substituting directory paths of course)\n\nHowever when I try using the model by running `python translate.py --decode\n  --data_dir [your_data_directory] --train_dir [checkpoints_directory]` I get the following error:\n\n```\nTraceback (most recent call last):\n  File \"translate.py\", line 290, in <module>\n    tf.app.run()\n  File \"/home/user/venv/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"translate.py\", line 285, in main\n    decode()\n  File \"translate.py\", line 248, in decode\n    target_weights, bucket_id, True)\n  File \"/www/data/user/tensorflow/tensorflow/models/rnn/translate/seq2seq_model.py\", line 244, in step\n    outputs = session.run(output_feed, input_feed)\n  File \"/home/user/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 382, in run\n    run_metadata_ptr)\n  File \"/home/user/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 640, in _run\n    % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\nValueError: Cannot feed value of shape (1,) for Tensor 'weight5:0', which has shape '(64,)'\n```\n\nIt looks like the script is expecting the whole batch (which is set to 64 by default) but when I type any input, it creates only 1 sample of data.\n\nVersion of tensorflow: `tensorflow==0.10.0rc0` (git commit I use `f71cc62282bf2e066f9ebd08cf3f605fc98c6e41`)\n\nThank you for any ideas\n", "comments": ["I managed to fix the problem. However, I believe that there has to be a better way how to do it.\n\nI changed the following rows of the `decode` method in `translate.py`.\n\n```\n# model.batch_size = 1 -> Changed the size of the batch to 64 (expected size)\nmodel.batch_size = 64\n\n# ...\n\n# encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n#         {bucket_id: [(token_ids, [])]}, bucket_id)  -> Extended the data list to 64 items\nencoder_inputs, decoder_inputs, target_weights = model.get_batch(\n          {bucket_id: [(token_ids, [])]*64}, bucket_id)\n\n# ...\n\n# outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits] -> Get only the first batch from the output\noutputs = [int(np.argmax(logit[:1,:], axis=1)) for logit in output_logits] \n```\n\nAs I have mentioned, it is not effective and I believe there is a better way how to fix the problem. My changes work as a hot fix, though.\n\nThanks for any additional advices, tips or ideas\n", "@lukaszkaiser might have some ideas.\n", "I tracked it down to the shape of placeholders in seq2seq_model (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/translate/seq2seq_model.py) being set to [batch_size] statically on init. It should be [None] so it can be changed dynamically. Corrected a day ago -- should all work ok now on head, no need for your tweak :).\n"]}, {"number": 4280, "title": "Fix type error in python_config.sh", "body": "In the getsitepackages code path,\nlibrary_paths is a list.\nMake the distutils code path a list as well.\nRemove extraneous whitespace while we're here.\n\nWithout this, running ./configure:\n\n$ ./configure \nPlease specify the location of python. [Default is /Users/josh/.virtualenvs/tf/bin/python]: \nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] n\nNo Google Cloud Platform support will be enabled for TensorFlow\nTraceback (most recent call last):\n  File \"<stdin>\", line 18, in <module>\nTypeError: can only concatenate list (not \"str\") to list\nFound possible Python library paths:\nPlease input the desired Python library path to use.  Default is []\n\nln: util/python/python_lib: Invalid argument\n", "comments": ["Can one of the admins verify this patch?\n", "@josharian, thanks for your PR! By analyzing the annotation information on this pull request, we identified @itsmeolivia, @vrv and @martinwicke to be potential reviewers\n", "@itsmeolivia PTAL\n", "LGTM\n", "@tensorflow-jenkins Test this please\n"]}, {"number": 4279, "title": "//tensorflow:libtensorflow.so target won't build", "body": "Trying to build libtensorflow.so from source on a Mac using Bazel 3.1, I get an error. Log is attached below.\n### Environment info\n\nOperating System: MacOS X 10.11.6, Xcode 7.3.1\n\nInstalled version of CUDA and cuDNN: \nNo CUDA\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n\n```\n2a6d7511f13a0387857081f1cf64d282d2816a62\n```\n1. The output of `bazel version`\n   \n   Build label: 0.3.1\n   Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\n   Build time: Fri Jul 29 09:05:18 2016 (1469783118)\n   Build timestamp: 1469783118\n   Build timestamp as int: 1469783118\n### Build log\n\n```\n$ bazel build //tensorflow:libtensorflow.so\nExtracting Bazel installation...\nSending SIGTERM to previous Bazel server (pid=8488)... done.\n.\nERROR: /Users/Projects/tensorflow/tensorflow/core/BUILD:942:1: no such target '//tensorflow/tools/git:gen/spec.json': target 'gen/spec.json' not declared in package 'tensorflow/tools/git' defined by /Users/Projects/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\nERROR: /Users/Projects/tensorflow/tensorflow/core/BUILD:942:1: no such target '//tensorflow/tools/git:gen/head': target 'gen/head' not declared in package 'tensorflow/tools/git' defined by /Users/Projects/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\nERROR: /Users/Projects/tensorflow/tensorflow/core/BUILD:942:1: no such target '//tensorflow/tools/git:gen/branch_ref': target 'gen/branch_ref' not declared in package 'tensorflow/tools/git' defined by /Users/Projects/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\nERROR: Analysis of target '//tensorflow:libtensorflow.so' failed; build aborted.\nINFO: Elapsed time: 5.724s\n```\n", "comments": ["It looks like you haven't run ./configure.\n", "Ugh. I'm an idiot. (I had run configure, but then I did pull to update my copy of the repository.)  I'm so sorry.\n", "@aselle Any chance we can add an error message mentioning ./configure here? We used to have a helpful prompt for idiots like me who forgot to run it, but I'm not sure where it went. I'm betting that would save quite a few engineer-hours for all our users. :)\n", "Do you know of an easy way to accomplish this in Bazel? We could always make a dependent filename \"please_rerun_configure\" but that is unintuitive to the uninitiated (and smells of the pre C++11 template static_assert  technique). Filed bug for that #4386.\n", "Can anybody add that I have to run `./configure` [here](https://www.tensorflow.org/api_guides/cc/guide)?", "I'll do that.", "I'm having the same issue, (gen/spec.json not declared in package) but I'm using Windows 10, so how do I run ./configure on windows? Or is there any other solution for the issue? This is my issue. #11279", "Hi, I am using Ubuntu 14.04. I tried to follow the instructions from [here](https://github.com/cjweeks/tensorflow-cmake) but i get an error saying \r\n\r\n`ERROR: no such target '//tensorflow:libtensorflow_all.so': target 'libtensorflow_all.so' not declared in package 'tensorflow' (did you mean 'libtensorflow_cc.so'?) defined by /home/muazzam/mywork/python/tensorflow/tensorflow/BUILD.\r\nINFO: Elapsed time: 0.047s`\r\n\r\nThis error occurs when I use the following command from the cjweeks/tensorflow repository documentation `bazel build tensorflow:libtensorflow_all.so`\r\n\r\nanybody has any clues ?", "I forgot exec `./configure`", "@mozi22 I ran into the same problem as you, and I executed./configure, how do you solve it, thank you", "@Tomhouxin did you solve it? I did the execution and still met the problem"]}, {"number": 4278, "title": "Minor documentation / code mismatch in Optimizer", "body": "The `Optimizer` class has the [following method](https://github.com/tensorflow/tensorflow/blob/7e7dff529fd35edea443580d58e95f5de39b5356/tensorflow/python/training/optimizer.py#L388):\n\n``` python\n  def _valid_dtypes(self):\n    \"\"\"Valid types for loss, variables and gradients.\n    Defaults to `float32`. Subclasses should override to allow other types.\n    Returns:\n      Valid types for loss, variables and gradients.\n    \"\"\"\n    return set([dtypes.float16, dtypes.float32, dtypes.float64])\n```\n\nThe docstring says that `_valid_dtypes` defaults only to `float32`, but the code also allows `float16` and `float64`. The docstring should be updated to reflect that the default allowed set includes these other float types.\n", "comments": ["@gibiansky Thanks for the comment!  Would you like to send a PR?\n", "I can send a PR. If it is okay with @gibiansky ?\n", "Go ahead! \n", "Hi, @agrawalnishant  Are you still working on this? If you are not available, I would like to fix this.\n", "Hi @haosdent , please go ahead. Thank you!\n", "Oh, I saw it fixed at https://github.com/tensorflow/tensorflow/pull/4351/files cc @aselle \n", "It looks like this was fixed in [this PR](https://github.com/tensorflow/tensorflow/pull/4351/files) so I'm going to go ahead and close this to avoid cluttering the issue tracker.\n"]}, {"number": 4277, "title": "Compilation of the C API in version 0.10", "body": "In version 0.10, the C API was [moved](https://github.com/tensorflow/tensorflow/commit/788f359b7218ad46696c15459c89688ffe70955e) from `tensorflow/core` to `tensorflow/c`. The master branch contains a [commit](https://github.com/tensorflow/tensorflow/commit/d03f2545ecc3012e6c941a3a1e957f7d7f8d5040) adding a new target to `tensorflow/BUILD` for building the C API in the new location. However, this commit and the corresponding target are not present on branch `r0.10`. I\u2019m wondering how one is supposed to build the C API in version 0.10. Thank you.\n\nRegards,\nIvan\n", "comments": ["@jhseu, could you give a quick response of if this is expected to work in 0.10? My inclination is to use master if you are planning to start building on the C-api, because much more work is done there and 0.10's C-api was much less complete.\n", "The C API is a doorway to TensorFlow for other languages, and it has been extensively exploited. If there is no a convenient way to compile the API as a shared library in 0.10, the existing bindings will have be able to upgrade to the new version.\n", "The C API was not yet finished as of 0.10. Users need to wait until 0.11 is out (or use master) to really use it.\n"]}, {"number": 4276, "title": "Tensor name \"hiddenlayer_2/biases\" not found in checkpoint files model", "body": "I have created my model on one machine and then sent the zipped folder 'model' by email to myself so I would be able to use this model on my second PC. \n\nHere is the code how I'm recreating a model:\n\n```\nfeature_columns = learn.infer_real_valued_columns_from_input(x_train)\nclassifier = learn.DNNClassifier(feature_columns=feature_columns, hidden_units=[40, 80, 40], n_classes=3, model_dir='model')\n```\n\nAs I understood, to reuse a model I need to make 'fit' 1-3 steps. (This fact I found here in TF issues)\n\n`classifier.fit(x_train, y_train, steps=3)`\n\nThen when I call `predicted = classifier.predict(x_test)` , it throws an exception:  \ntensorflow.python.framework.errors.NotFoundError: Tensor name \"hiddenlayer_2/biases\" not found in checkpoint files model/model.ckpt-30015-?????-of-00001\n\nThis model is working well on the machine where it has been created. \nHow can I make it work on any PC?\n\nThanks!\n", "comments": ["@eymar I'd start by narrowing down the problem:\n1. Are you running the same setup on both machines (OS / tensorflow version / python version / etc)?\n2. Do things work when run on each machine individually?  So is the only problem the transfer of the model from one machine to the other?\n", "@tatatodd Sorry, I didn't mention that.  \n\n1) The configuration of machines is almost identical. The difference is only in OS - first machine is Ubuntu 14.04, the second one is Ubuntu 12.04. The rest are the same: Python 2.7.6, tensorflow  - r0.10. \n\nAlso, tested this case on Mac OS and the result is similar: Tensor name \"dnn_logits/biases\" not found in checkpoint files\n\n2) Yes, when I run on each machine individually, everything work. The problem is transferring model from one machine to the other.\n\nI tried to do on the contrary: created and fit the model (iris example) on the Mac, then copied the directory with model to the Ubuntu and again the same error has been shown up: _Tensor name \"dnn/hiddenlayer_0/biases\" not found in checkpoint_. \n\nThe model on Mac stays reusable and works.\n\nP.S. Actually, I'm using skflow, but seems people are posting skflow's issues here too.\n", "I'm not sure what problem you're running in to.  For reference you can take a look at the general howto regarding saving and restoring variables:\nhttps://www.tensorflow.org/versions/r0.10/how_tos/variables/index.html\nhttps://www.tensorflow.org/versions/r0.10/how_tos/variables/index.html#checkpoint-files\n\nAnd use inspect_checkpoint to see whether the checkpoint looks valid before and after you've transferred to different machines:\nhttps://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/python/tools/inspect_checkpoint.py\n\nLet me know if that helps!\n", "@eymar Does your problem solved\uff1f recently, I meet the similar problem\u3002when i use a pretrained model in another machine, there something wrong happened. \r\nthe problem is :\r\n`2017-09-13 10:10:03.396626: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-09-13 10:10:03.396681: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-09-13 10:10:03.396694: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-09-13 10:10:03.396705: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-09-13 10:10:03.396714: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-09-13 10:10:03.649258: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key bi_lstm/bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias/Adam not found in checkpoint\r\n2017-09-13 10:10:03.691347: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key bi_lstm/bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam_1 not found in checkpoint\r\n2017-09-13 10:10:03.691625: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key bi_lstm/bidirectional_rnn/bw/multi_rnn_cell/cell_1/lstm_cell/bias/Adam_1 not found in checkpoint\r\n2017-09-13 10:10:03.693400: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key bi_lstm/bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias/Adam_1 not found in checkpoint\r\n2017-09-13 10:10:03.695818: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key bi_lstm/bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam_1 not found in checkpoint\r\n2017-09-13 10:10:03.697568: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key bi_lstm/bidirectional_rnn/fw/multi_rnn_cell/cell_1/lstm_cell/bias not found in checkpoint\r\n2017-09-13 10:10:03.697820: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key bi_lstm/bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel not found in checkpoint\r\n2017-09-13 10:10:03.698046: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key bi_lstm/bidirectional_rnn/bw/multi_rnn_cell/cell_1/lstm_cell/kernel not found in checkpoint\r\n2017-09-13 10:10:03.698193: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key bi_lstm/bidirectional_rnn/bw/multi_rnn_cell/cell_1/lstm_cell/bias not found in checkpoint\r\n2017-09-13 10:10:03.698306: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key bi_lstm/bidirectional_rnn/fw/multi_rnn_cell/cell_1/lstm_cell/kernel/Adam not found in checkpoint\r\n2017-09-13 10:10:03.700091: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key bi_lstm/bidirectional_rnn/bw/multi_rnn_cell/cell_1/lstm_cell/kernel/Adam not found in checkpoint\r\n2017-09-13 10:10:03.700892: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key bi_lstm/bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias not found in checkpoint\r\n2017-09-13 10:10:03.701265: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key bi_lstm/bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam not found in checkpoint\r\n2017-09-13 10:10:03.702148: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key bi_lstm/bidirectional_rnn/bw/multi_rnn_cell/cell_1/lstm_cell/bias/Adam not found in checkpoint\r\n2017-09-13 10:10:03.704113: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key bi_lstm/bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias not found in checkpoint\r\n2017-09-13 10:10:03.704663: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key bi_lstm/bidirectional_rnn/fw/multi_rnn_cell/cell_1/lstm_cell/bias/Adam not found in checkpoint\r\n2017-09-13 10:10:03.704809: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key bi_lstm/bidirectional_rnn/bw/multi_rnn_cell/cell_1/lstm_cell/kernel/Adam_1 not found in checkpoint\r\n2017-09-13 10:10:03.705010: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key bi_lstm/bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias/Adam not found in checkpoint\r\n2017-09-13 10:10:03.705397: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key bi_lstm/bidirectional_rnn/fw/multi_rnn_cell/cell_1/lstm_cell/kernel not found in checkpoint\r\n2017-09-13 10:10:03.705792: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key bi_lstm/bidirectional_rnn/fw/multi_rnn_cell/cell_1/lstm_cell/kernel/Adam_1 not found in checkpoint\r\n2017-09-13 10:10:03.706879: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key bi_lstm/bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias/Adam_1 not found in checkpoint\r\n2017-09-13 10:10:03.707018: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key bi_lstm/bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam not found in checkpoint\r\n2017-09-13 10:10:03.707096: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key bi_lstm/bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel not found in checkpoint\r\n2017-09-13 10:10:03.707426: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key bi_lstm/bidirectional_rnn/fw/multi_rnn_cell/cell_1/lstm_cell/bias/Adam_1 not found in checkpoint\r\n`", "solved? \r\n`NotFoundError (see above for traceback): Key aconv1d_0/variance/Adam_1 not found in checkpoint\r\n\t [[Node: save/RestoreV2_14 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_14/tensor_names, save/RestoreV2_14/shape_and_slices)]]\r\n\t [[Node: save/RestoreV2_95/_497 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_1926_save/RestoreV2_95\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]`", "I have solved this problem, so you can reference to this website \r\n [tensorflow.python.framework.errors_impl.NotFoundError](https://sthsf.github.io/wiki/Algorithm/DeepLearning/%E4%BD%BF%E7%94%A8Tensorflow%E7%88%AC%E8%BF%87%E7%9A%84%E5%9D%91/tensorflow.python.framework.errors_impl.NotFoundError.html). Hope it can help you! @wantongtang "]}, {"number": 4275, "title": "uint16 was missing", "body": "", "comments": ["@jonasrauber, thanks for your PR! By analyzing the annotation information on this pull request, we identified @keveman, @vrv and @samjabrahams to be potential reviewers\n", "Can one of the admins verify this patch?\n", "Thanks!\n"]}, {"number": 4274, "title": "convolution2d", "body": "explicit use \"is not None\" for activation_fn, normalize_fn and provide more detailed description\n", "comments": ["Can one of the admins verify this patch?\n", "@alrojo, thanks for your PR! By analyzing the annotation information on this pull request, we identified @tensorflower-gardener, @xodus7 and @lukaszkaiser to be potential reviewers\n", "@sguada PTAL\n", "LGTM,\njenkins, test this please\n", "@danmane do you know why Jenkins is not running?\n", "@alrojo would you mind updating the other layers that have activation_fn and normalizer_fn too?\n", "@tensorflow-jenkins test this please\n", "@sguada I'll look into the other layers, why did the test fail?\n", "It looks like the test failure was a flake, since it was a timeout in code clearly unrelated to your change. I would consider that a clean build, but I will re-run the tests anyway.\n@tensorflow-jenkins test this please.\n", "@sguada updated all of the other layers with `activation_fn` and `normalizer_fn`. I also updated a typo in the docs: `normalize_fn` -> `normalizer_fn`\n", "Thanks @alrojo \n\n@tensorflow-jenkins test this please\n", "Merged, the remaining test failure was a flake.\n"]}, {"number": 4273, "title": "Crashing when parsing names with special character [] in saver.restore(): bug or expected behavior ?", "body": "Disclaimer: I have tensorflow 0.7.0 (a bit outdated I suppose). I have not tested it on more recent versions.\nWhen I try:\n\n``` python\nimport tensorflow as tf\nx=tf.placeholder(tf.float32,[])\nw=tf.Variable(2.)\ny=tf.mul(x,w)\nsaver=tf.train.Saver()\ninit=tf.initialize_all_variables()\nsess1=tf.Session()\nsess1.run(init)\nname_model=\"./\"+\"model\"\nsaver.save(sess,name_model)\nsess1.close()\nsess2=tf.Session()\npath_meta=name_model+\".meta\"\npath_model=name_model\nnew_saver=tf.train.import_meta_graph(path_meta)\nnew_saver.restore(sess2,path_model)\nprint sess2.run(y, feed_dict={x: 1.})==2\n```\n\nIt runs perfectly fine but if I change name_model to \"./model[1]\" for example it crashes with message: `Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./model[1].`\nI was wondering if this is expected or if it is a bug. If this is buggy what would be aworkaround ?\n", "comments": ["@jeandut each filesystem has differing behavior wrt special characters.  I'd recommend the best thing is to avoid special characters in your filenames.  Hope this helps!  If you have any other follow-up questions, just add them here.\n", "Ok I had no idea ! Thanks !\n"]}, {"number": 4272, "title": "Training time for CIFAR-10 is higher on multi GPU cards compared to single", "body": "When running CIFAR-10 example on two GPU cards in a single machine, the training time is higher compared to a single GPU card.\n\nIt is utilizing both cards but overall execution time is higher compared to single GPU.\n\n![multi-gpu-cards](https://cloud.githubusercontent.com/assets/7776101/18342617/48dfeb66-75cd-11e6-83cb-473be9746f3a.png)\n\nOne more thing I noticed is that the queue is filled twice. I got following messages in log.\n`Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.`\n`Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.`\n\nIdeally, It should be filled once and distribute across two GPUs but that is not happening. \nCould anyone please help me out to resolve this?\n", "comments": ["@dashrathc Please fill out the full information from the template that appears when you click on the green \"New issue\" button, including OS, libcuda info, versions, etc.  Thanks!\n", "### Environment info\n\nOperating System: Ubuntu 16.04.1 LTS\n\nInstalled version of CUDA and cuDNN: \n![libcudnn](https://cloud.githubusercontent.com/assets/7776101/18408249/11279cf6-7748-11e6-9044-3c7eb081b7fb.png)\n\nTensorFlow version:\n![tensorflow_version](https://cloud.githubusercontent.com/assets/7776101/18408261/51d09c12-7748-11e6-9f74-bc4a24003c93.png)\n\ngit rev-parse HEAD\n8a3107801d15bf8af36221ff5bca0b94bf44d6d3\n\nbazel version:\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp as int: 0\n\n@tatatodd  let me know if need anything else.\n", "@dashrathc \nHi, having two queues is fine. It may consume CPU a little bit more, but from my experience, at least for two GPU cards, it works equivalent as one queue.\nOne thing to remember is too adjust the batch_size correctly. For the cifar10 example, batch_size is the size for each GPU card not the overall batch size. So if you are using batch_size=128 for one GPU, you should change batch_size=64 for two GPUs to notice the speed difference. \n", "@JianbangZ Thanks for reply.\nI have updated 128 to 64 cifar10.py file.\ntf.app.flags.DEFINE_integer('batch_size', 128, \"\"\"Number of images to process in a batch.\"\"\")\n\nIs it how batch_size should be changed? Please do let me know if there is any other way to update it.\n", "@dashrathc It is correct\n", "@JianbangZ \nI have run it again for 100k iterations and following is the result,\nbatch_size=128, Elapsed Time: 18188.31 seconds\nbatch_size=64, Elapsed Time: 18072.72 seconds\n\nThe elapsed time difference is very minimal though batch_size decreased to by 64.\nCould you please share your view?\n", "@dashrathc : Are you still looking into this?\n\nYou might find this comment: https://github.com/tensorflow/tensorflow/issues/4744#issuecomment-252088066 interesting. There are many factors to consider when using multiple GPUs.\n\nHave you seen this tutorial: https://www.tensorflow.org/tutorials/deep_cnn/index.html?\nIn particular, it talks about both single GPU and multi GPU training. The training is slightly different between the two as seen in the files [cifar10_train.py](https://github.com/tensorflow/tensorflow/blob/r0.11/tensorflow/models/image/cifar10/cifar10_train.py) and [cifar10_multi_gpu_train.py](https://github.com/tensorflow/tensorflow/blob/r0.11/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py). In particular, see the comment at the top of  [cifar10_multi_gpu_train.py](https://github.com/tensorflow/tensorflow/blob/r0.11/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py) which outlines the observed training time when using 1 GPU and 2 GPUs.\n\nLet us know if this was helpful or not.\n", "Closing due to inactivity, feel free to reopen if your concerns haven't been addressed and the pointers above didn't help.\n", "Thanks! This was helpful :)"]}, {"number": 4271, "title": "Update OS setup instructions with the new protobuf pip wheel files.", "body": "", "comments": ["@gunan, thanks for your PR! By analyzing the annotation information on this pull request, we identified @keveman, @vrv and @tensorflower-gardener to be potential reviewers\n"]}, {"number": 4270, "title": "Fix the issue#4138", "body": "Fix the issue #4138 \nRedefine the `w` and `w_t` for improving the efficiency in case of handling sparse `IndexedSlices` objects\n", "comments": ["@DjangoPeng, thanks for your PR! By analyzing the annotation information on this pull request, we identified @benoitsteiner, @vrv and @keveman to be potential reviewers\n", "Can one of the admins verify this patch?\n", "@lukaszkaiser Please take a look.\n", "Looks good to me, thanks! Does it really give a speedup? (We should merge it anyway, I think, just curious).\n", "@lukaszkaiser The issue shows it gives a speedup and set a `contribution welcome` label. So, I think it maybe helpful to do this. At least, it can't be worse.\n", "Let's merge this! Is anything blocking it?\n", "@lukaszkaiser I don't know, what happened when you merge?  My branch is still has the commit\n", "Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "@lukaszkaiser, based on your comments I think you might have meant to merge this and closed the PR instead? I'm re-opening it and testing it :)\n", "@danmane ooh..I got your point! \ud83d\udc4d   @lukaszkaiser Could you help merge it? :)\n", "I'm not authorized to merge. vrv -- could you merge?\n", "Please merge.\n"]}, {"number": 4269, "title": "bazel build failing - not generating bazel-bin directory when  build from source with CUDA 8.0 error ", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\n\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nhttp://stackoverflow.com/questions/38773402/tensorflow-bazel-build-failing-not-generating-bazel-bin-directory\n\nhttps://github.com/tensorflow/tensorflow/issues/1498\n### Environment info\n\nOperating System:\n\nozzie@debian:~$ uname -a\nLinux debian 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt25-2 (2016-04-08) x86_64 GNU/Linux\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nozzie@debian:~$ ls /usr/local/cuda-8.0/lib64/libcud*\n/usr/local/cuda-8.0/lib64/libcudadevrt.a\n/usr/local/cuda-8.0/lib64/libcudart.so\n/usr/local/cuda-8.0/lib64/libcudart.so.7.5\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.27\n/usr/local/cuda-8.0/lib64/libcudart_static.a\n/usr/local/cuda-8.0/lib64/libcudnn.so\n/usr/local/cuda-8.0/lib64/libcudnn.so.4\n/usr/local/cuda-8.0/lib64/libcudnn.so.4.0.7\n/usr/local/cuda-8.0/lib64/libcudnn.so.5\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\n\n(note: libcudart.so.7.5 is a link of libcudart.so.8.0)\n\nUsing Nvidia Quadro K4000 when I run \n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n   ozzie@debian:~/working/work/ML/tensorflow$ git rev-parse HEAD\n   2a6d7511f13a0387857081f1cf64d282d2816a62\n2. The output of `bazel version`\n\nozzie@debian:~/working/work/ML/tensorflow$ bazel version\nBuild label: 0.3.1-jdk7\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Jul 29 09:07:03 2016 (1469783223)\nBuild timestamp: 1469783223\nBuild timestamp as int: 1469783223\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nozzie@debian:~/working/work/ML/tensorflow$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\n### What other attempted solutions have you tried?\n\n try to modify GPU architecture to compute_30 (Nvidia Quadro K4000 code),  but can not sure which file contain this GPU architecture info?\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n\n'/usr/local/cuda-8.0/include/curand_discrete2.h'.\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 22.435s, Critical Path: 21.92s\n", "comments": ["@ztianjin can you include more of the logs from the failure, or attach the entire log as an attachment?  The snippet you provided above only shows a couple of warnings, but not the actual error.  Thanks!\n", "Automatically closing due to lack of recent activity. Please reopen when further information becomes available.\n", "now I can compile tensorflow from source and training net model\n\nMinibatch loss: 1.595, learning rate: 0.006302\nMinibatch error: 0.0%\nValidation error: 0.7%\nStep 8500 (epoch 9.89), 35.4 ms\nMinibatch loss: 1.621, learning rate: 0.006302\nMinibatch error: 3.1%\nValidation error: 0.9%\nTest error: 0.8%\n"]}, {"number": 4268, "title": "resize_image_with_crop_or_pad loses channel information", "body": "``` python\npng = tf.image.resize_image_with_crop_or_pad(png_raw, 100, 100)\n```\n\nbefore:\n\n```\nTensorShape([Dimension(None), Dimension(None), Dimension(3)])\n```\n\nafter:\n\n```\nTensorShape([Dimension(100), Dimension(100), Dimension(None)]\n```\n\n`tf.image.pad_to_bounding_box` has the same problem.\nThe combination of these two lines seems questionable:\n\n```\nheight, width, depth = _ImageDimensions(image, static_only=False)\npadded_shape = [None if is_tensor(i) else i\n                  for i in [target_height, target_width, depth]]\n```\n\nwhy set `static_only=False`? The docs for `_ImageDimensions` say:\n\n>    list of integers `[batch, height, width, channels]`, when static shape is\n>     fully defined or `static_only` is `True`.\n>     list of integer scalar tensors `[batch, height, width, channels]`, when\n>     static shape is not fully defined.\n\nso unless all dimensions of the image are defined you will get tensors which will then be changed to `None`.\n", "comments": ["Thanks for the report @cancan101!  Please add additional information (e.g. tensorflow version) - try to fill out the template that appears when you click the green \"New issue\" button, and add it to this issue.  Thanks!\n", "here, though the code snippet I reference is (still) in master:\n\n```\n bazel version\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\nExtracting Bazel installation...\nBuild label: 0.3.0\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Jun 10 11:38:23 2016 (1465558703)\nBuild timestamp: 1465558703\nBuild timestamp as int: 1465558703\n```\n\n```\n git rev-parse HEAD\n58b37cf745c6c30d44878ddf5987c4283ed12c3c\n```\n\n```\nls -l /usr/lib/x86_64-linux-gnu/libcud*\nlrwxrwxrwx 1 root root       29 Aug 12 05:11 /usr/lib/x86_64-linux-gnu/libcudnn.so -> /etc/alternatives/libcudnn_so\nlrwxrwxrwx 1 root root       17 Jun 10 13:07 /usr/lib/x86_64-linux-gnu/libcudnn.so.5 -> libcudnn.so.5.1.3\n-rw-r--r-- 1 root root 60696704 Jun 10 13:07 /usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.3\nlrwxrwxrwx 1 root root       32 Aug 12 05:11 /usr/lib/x86_64-linux-gnu/libcudnn_static.a -> /etc/alternatives/libcudnn_stlib\n-rw-r--r-- 1 root root 59715990 Jun 10 13:07 /usr/lib/x86_64-linux-gnu/libcudnn_static_v5.a\n```\n\n```\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally\n```\n", "@cancan101 I see what you're saying, and I agree there seems to be a bug here.\n\nWRT the details, I don't think calling `_ImageDimensions(image, static_only=True)` sounds right, since the sizes are not actually statically known.  Instead I think we want to treat the depth separately from the width and height, but I'm not sure what the right fix is.\n\n@gaohuazuo I believe you wrote the current version in PR #2737 - can you comment on whether you agree there's a problem here, and how we might fix things?\n\nThanks all!\n", "Ok then perhaps the bug is here:\n\n```\npadded_shape = [None if is_tensor(i) else i\n                  for i in [target_height, target_width, depth]]\n```\n\nwhich ignore tensors even if they are constant values.\n", "I think the problem is that `_ImageDimensions` returns tensors even if integer is possible for some dimensions.\n\nPossible fix (not tested) https://github.com/gaohuazuo/tensorflow/commit/85ce1fbb7d7d190b9cb5c6d8c87b986fb1358467\n", "Thanks @gaohuazuo!  I think that's right.  I'll add some tests and update this bug when the fix is in.\n", "I committed the fix and it's now visible in the public repo:\nhttps://github.com/tensorflow/tensorflow/commit/2e7b5c3ee82673fe5affd8de32eed0b350a451bf\n\nFYI the basic idea behind the fix is exactly what @gaohuazuo said; we now return python integers rather than tensors for known dimensions.\n\nThanks again for filing the bug @cancan101!\n"]}, {"number": 4267, "title": "libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program", "body": "i am trying to install tensorflow on my laptop LenovoY700, so that Ubuntu16.04 is more suitable with laptop.\nI installed cuda7.5+cudnn4 with GTX960M and nvidia-version is 367.\n\nWhen I was running 'convolutional.py', something goes wrong.\n\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:102] Couldn't open CUDA library libcuda.so.1. LD_LIBRARY_PATH: /usr/local/cuda-7.5/lib64:\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:160] hostname: Y700\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:185] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:356] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.44  Wed Aug 17 22:24:07 PDT 2016\nGCC version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.2) \n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] kernel reported version is: 367.44.0\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1077] LD_LIBRARY_PATH: /usr/local/cuda-7.5/lib64:\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1078] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so.1; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n\nCould somebody give me a hand?\nThanks for your help.\nSimon\n", "comments": ["@forhonourlx please provide the following details:\n\n```\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n\n1. A link to the pip package you installed:\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n\n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n```\n\nIn general it looks like libcuda.so couldn't be found on LD_LIBRARY_PATH, so that's the direction you want to investigate.  For more details, see this related issue:\nhttps://github.com/tensorflow/tensorflow/issues/4078#issuecomment-243065693\n", "Hi Todd,\nThank you for your help, I installed tensorflow from pip of this link:\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\n\nsimon@Y700:~$ ls -l /usr/local/cuda/lib/libcud*\n-rw-r--r-- 1 root root 189170 Sep  4 00:48 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Sep  4 00:48 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Sep  4 00:48 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Sep  4 00:48 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 Sep  4 00:48 /usr/local/cuda/lib/libcudart_static.a\n\nsimon@Y700:~$ python -c \"import tensorflow; print(tensorflow.**version**)\"\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:102] Couldn't open CUDA library libcuda.so.1. LD_LIBRARY_PATH: /usr/local/cuda/lib64:\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:160] hostname: Y700\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:185] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:356] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.44  Wed Aug 17 22:24:07 PDT 2016\nGCC version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.2) \n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] kernel reported version is: 367.44.0\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1077] LD_LIBRARY_PATH: /usr/local/cuda/lib64:\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1078] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so.1; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n0.10.0rc0\n\nsimon@Y700:~$ echo $LD_LIBRARY_PATH\n/usr/local/cuda/lib64:\n", "simon@Y700:~$ ls -l /usr/local/cuda/lib/libcud*\nso...\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/lib\n", "I encountered a similar problem when trying to use cuda in a [singularity](http://singularity.lbl.gov/) container.  It turns out that the cuda .run installer did not set up a few symbolic links in the appropriate directories.  Easy fix. \n\n```\n$ export NVID_VER=367.44 # or whatever version you have\n$ cd /path/to/libcuda.so.$NVID_VER\n$ ln -s libcuda.so.$NVID_VER libcuda.so\n$ ln -s libcuda.so.$NVID_VER libcuda.so.1\n```\n", "Automatically closing due to lack of recent activity by submitter. If the suggestions do not solve the problem, please provide information when available, and we can reopen.\n", "I'm also having this problem, and its actually very strange.\nSame as @forhonourlx, once I tried importing tensorflow, it said it can't open CUDA library libcuda.so and libcuda.so.1.\n\nSo I found the libraries within the machine as the following.\nUnder /usr/lib, /usr/lib64, the symbolic links are set up like this:\n\n```\nlibcuda.so -> libcuda.so.1\nlibcuda.so.1 -> libcuda.so.352.93\nlibcuda.so.352.93\n```\n\nwhich is pretty much the way it is expected to be.\n\nIt will be natural to export LD_LIBRARY_PATH with /usr/lib and /usr/lib64 added, but when I import tensorflow for testing, it still somehow isn't able to find it.\n\nCan anyone give a hint in what's going wrong?\nBy the way, the machine is CentOS 6 (I hate it)\n\nHere is the full error log with some censoring (not critical)\n\n```\n>>> import tensorflow\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:102] Couldn't open CUDA library libcuda.so.1. LD_LIBRARY_PATH: $HOME/.local/lib:$HOME/.local2/lib:/usr/lib:/usr/lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib:$HOME/.local/lib:/usr/local/cuda/lib64:/usr/local/cuda/lib:\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:160] hostname: (_______)\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:185] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:356] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  352.93  Tue Apr  5 18:18:24 PDT 2016\nGCC version:  gcc version 4.4.7 20120313 (Red Hat 4.4.7-17) (GCC)\n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] kernel reported version is: 352.93.0\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1077] LD_LIBRARY_PATH: $HOME/.local/lib:$HOME/.local2/lib:/usr/lib:/usr/lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib:$HOME/.local/lib:/usr/local/cuda/lib64:/usr/local/cuda/lib:\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1078] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so.1; dlerror: libcuda.so.1: cannot enable executable stack as shared object requires: Operation not permitted\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n```\n", "i think you install package libcuda1-370 or libcuda1-xxx\n\n![image](https://cloud.githubusercontent.com/assets/9005755/19687168/9e9a5a64-9aee-11e6-86d1-132d19de57a6.png)\n", "For some reason an nvidia driver update on Ubuntu 16.10 uninstalled the virtual `libcuda-XXX` package for me. Fixed it by manually installing it, then the error went away.", "@sunsided \r\nThank you very much!\r\nGot this problem when installing tensorflow on cuda8.0 in a Ubuntu16.04. After installing libcuda367, now, it can run on gpu.", "@sunsided : I am having the same problem with Ubuntu 16.10. Could you please tell me how you install it manually?", "I has been struggling for this problem for a whole day with docker. \r\nFinally I found a decent solution which is to use nvidia-docker: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/docker. ", "@damvantai thanks. Your answer solved my problem.", "\r\nHello, \r\nI have asme problem\r\n\r\n python -c \"import tensorflow; print(tensorflow.__version__)\"\r\n\r\n\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:119] Couldn't open CUDA library libcuda.1.dylib. LD_LIBRARY_PATH: :/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: RamakriidassMBP\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: Invalid argument: expected %d.%d or %d.%d.%d form for driver version; got \"\"\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1092] LD_LIBRARY_PATH: :/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1093] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.1.dylib; dlerror: dlopen(libcuda.1.dylib, 6): image not found\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.dylib locally\r\n0.12.1\r\n\r\n\r\n\r\n\r\n bazel version\r\nExtracting Bazel installation...\r\nBuild label: 0.5.2-homebrew\r\nBuild target: bazel-out/darwin_x86_64-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Jun 27 20:22:57 2017 (1498594977)\r\nBuild timestamp: 1498594977\r\nBuild timestamp as int: 1498594977", "@damvantai Yeah, this helped! Thanks. I don't know why it wasn't installed along with package`cuda` or `nvidia-384`.\r\n\r\n```\r\nsudo apt install libcuda1-384\r\n```\r\n\r\nJust for completeness the file should be located at `/usr/lib/x86_64-linux-gnu/libcuda.so.1`.", "@GodloveD I meet this also, and I can't find the file you referred to ,how can I solve this problem??", "Heya @zhangwewenwen.  Are you running into this problem in a Singularity container?  Did you run your container with the `--nv` option?  \r\n\r\n`--nv` will find the appropriate nvidia libraries on the host system (including `libcudo.so*`) and bind mount them into your container at runtime.  This will give you access to the host system GPU.\r\n\r\nThis is very similar to how nvidia-docker works afaik. ", "On following `sudo apt install libcuda1-390`\r\n\r\n```python3\r\nSome packages could not be installed. This may mean that you have\r\nrequested an impossible situation or if you are using the unstable\r\ndistribution that some required packages have not yet been created\r\nor been moved out of Incoming.\r\nThe following information may help to resolve the situation:\r\n\r\nThe following packages have unmet dependencies:\r\n libcuda1-390 : Depends: nvidia-compute-390 but it is not installable\r\nE: Unable to correct problems, you have held broken packages.\r\n```\r\n\r\nGetting this beautiful error on Ubuntu 18.04, nvidia drivers - 390.77.\r\n\r\nSuggestions?"]}, {"number": 4266, "title": "How to change the model and label file in \"assets\" of Android Demo?", "body": "When I change these files in \"assets\" folder , the Demo App will crash. How can I change the .txt and .pb file to change the verify model?\n", "comments": ["@YoWhatever If you change the names of the asset files you need to also edit the corresponding values in TensorFlowImageListener.java.\n", "@andrewharp Thank you\uff01 When I replace the asset files, I also rename my model file as the former corresponding file's name. But it didn't work. So I 'm wondering if there is something else need to be changed.  \n", "@YoWhatever There are a lot of possible things that could be going on; can you use adb logcat and post the exact error message that you get please?\n", "@andrewharp Sorry I'm a novice.Is there any useful information in these pictures\uff1f\n![image](https://cloud.githubusercontent.com/assets/17775886/18348161/537814ca-75fc-11e6-9a14-7380a6613020.jpeg)\n![image](https://cloud.githubusercontent.com/assets/17775886/18348190/8f031e68-75fc-11e6-8488-282f880bf657.jpeg)\n", "@YoWhatever I'm guessing the root cause information comes just above the lines in the 2nd picture you provided. You can save the entire log to a text file with `adb logcat | tee log.txt`, and then just upload log.txt after you've captured an instance of it crashing.\n\nWhat is odd to me is that you only have 7 nodes in your graph. Are the input and output nodes named the same? Possibly you're trying to use an operator that isn't supported on mobile?\n", "@andrewharp Sorry for late reply. I found these sentences in my logcat:\n\nI/native (21039): tensorflow_jni.cc:299 End computing.Ran in 3ms(3ms avg over 1 runs)\nF/native (21039): tensorflow_jni.cc:304 Error during inference: Not found: Feed Inputs: unable to find feed output Mul:0\nF/libc (21039):Fatal signal 6 (SIGABRT), code -6 in tid 21064 (InferenceThread)\nSince I change the TensorFlowImageListener.java to:\n\nprivate static final int NUM_CLASSES = 10; // number of categories (0-9)\nprivate static final int INPUT_SIZE = 299;\nprivate static final int IMAGE_MEAN = 128;\nprivate static final float IMAGE_STD = 128;\nprivate static final String INPUT_NAME = \"Mul:0\";\nprivate static final String OUTPUT_NAME = \"final_result:0\";\n\nI think that may means something.\n", "@YoWhatever Where did the graph you're trying to use come from? Are you trying to follow the TF for poets codelab? Otherwise do you know that the graph you're using has or should have a \"Mul:0\" input?\n", "Closing; please reopen with more details if the issue persists.\n"]}, {"number": 4265, "title": "ERROR: no such package '@local_config_cuda//crosstool': BUILD file not found on package path.", "body": "Operating System: \nUbuntu 16.04\n\nInstalled version of CUDA and cuDNN: \nserver@server:~$ ls -l /usr/local/cuda/lib64/libcud*\n-rw-r--r-- 1 root root 560184 9\u6708   7 09:56 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 9\u6708   7 09:56 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root     19 9\u6708   7 09:56 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\n-rwxr-xr-x 1 root root 394472 9\u6708   7 09:56 /usr/local/cuda/lib64/libcudart.so.8.0.27\n-rw-r--r-- 1 root root 737516 9\u6708   7 09:56 /usr/local/cuda/lib64/libcudart_static.a\n\nThe commit hash (`git rev-parse HEAD`)\nserver@server:~/tensorflow$ git rev-parse HEAD\n2ab7e6326296987ea0ce975afb3434a16d1aa21a\n\nThe output of `bazel version`\nserver@server:~$ bazel version\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp as int: 0\n\nHere is my bazel build result:\nINFO: From Compiling tensorflow/core/kernels/tile_ops_gpu.cu.cc:\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nINFO: From Compiling tensorflow/core/kernels/tile_ops_gpu.cu.cc:\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nTarget //tensorflow/cc:tutorials_example_trainer up-to-date:\nbazel-bin/tensorflow/cc/tutorials_example_trainer\nINFO: Elapsed time: 985.075s, Critical Path: 952.03s\n\nHere is my bazel-bin result:\n000008/000006 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]\n000007/000009 lambda = 39.000000 x = [3.605551 -0.000000] y = [10.816654 -3.605551]\n000006/000006 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]\n000001/000002 lambda =      nan x = [     nan      nan] y = [     nan      nan]\n000002/000006 lambda =      nan x = [     nan      nan] y = [     nan      nan]\n000006/000008 lambda =      nan x = [     nan      nan] y = [     nan      nan]\n000004/000004 lambda = 1.200000 x = [-0.894427 0.447214] y = [-0.894427 0.894427]\n000006/000007 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]\n000005/000005 lambda = 249281724416.000000 x = [0.894427 -0.447214] y = [278705438720.000000 -130.000000]\n000008/000003 lambda =      nan x = [     nan      nan] y = [     nan      nan]\n000007/000009 lambda = 2.400000 x = [0.948683 -0.316228] y = [2.213594 -0.948683]\n000008/000004 lambda =      nan x = [     nan      nan] y = [     nan      nan]\n000003/000009 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]\n000000/000004 lambda = 2.255962 x = [0.930408 -0.366524] y = [2.058176 -0.930408]\n000001/000002 lambda =      nan x = [     nan      nan] y = [     nan      nan]\n000004/000004 lambda =      nan x = [     nan 0.707107] y = [     nan      nan]\n000008/000000 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]\n000003/000008 lambda =      nan x = [     nan      nan] y = [     nan      nan]\n000006/000007 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]\n000007/000009 lambda = 2.172414 x = [0.919145 -0.393919] y = [1.969597 -0.919145]\n000008/000006 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]\n000008/000003 lambda =      nan x = [     nan      nan] y = [     nan      nan]\n000002/000006 lambda =      nan x = [     nan      nan] y = [     nan      nan]\n000000/000004 lambda = 2.115613 x = [0.911220 -0.411921] y = [1.909816 -0.911220]\n000003/000008 lambda =      nan x = [     nan      nan] y = [     nan      nan]\n000008/000004 lambda =      nan x = [     nan      nan] y = [     nan      nan]\n000003/000009 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]\n000006/000006 lambda = 1786627917493567488.000000 x = [715827904.000000 348406848.000000] y = [2844297216.000000 -715827904.000000]\n000008/000000 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]\n000007/000009 lambda = 2.080292 x = [0.906183 -0.422885] y = [1.872779 -0.906183]\n000008/000003 lambda =      nan x = [     nan      nan] y = [     nan      nan]\n000003/000008 lambda =      nan x = [     nan      nan] y = [     nan      nan]\n000008/000004 lambda =      nan x = [     nan      nan] y = [     nan      nan]\n000006/000007 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]\n000008/000006 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]\n000004/000004 lambda =      nan x = [     nan      nan] y = [     nan      nan]\n000005/000005 lambda = 3.000000 x = [1.000000 -0.000000] y = [3.000000 -1.000000]\n000003/000009 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]\n000008/000000 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]\n000008/000006 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]\n000006/000006 lambda = 2.584623 x = [0.969760 -0.244061] y = [2.421158 -0.969760]\n000004/000004 lambda =      nan x = [     nan      nan] y = [     nan      nan]\n000007/000009 lambda = 2.038786 x = [0.900159 -0.435561] y = [1.829356 -0.900159]\n\nI've searched the nan problem, according to https://github.com/tensorflow/tensorflow/issues/2037, i've tried \"--num_concurrent_sessions=1\" and \"--num_concurrent_steps=1\", and it seems to be fine\nHere is the output:\nserver@server:~/tensorflow$ bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu --num_concurrent_sessions=1 --num_concurrent_steps=1\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:105] Couldn't open CUDA library libcudnn.so.5. LD_LIBRARY_PATH: /usr/local/cuda-8.0/lib64:\nI tensorflow/stream_executor/cuda/cuda_dnn.cc:3304] Unable to load cuDNN DSO\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.8.0 locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: \nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:02:00.0\nTotal memory: 11.92GiB\nFree memory: 11.40GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\n000000/000000 lambda = 1.841570 x = [0.669396 0.742906] y = [3.493999 -0.669396]\n000000/000000 lambda = 2.708984 x = [0.982138 -0.188162] y = [2.570089 -0.982138]\n000000/000000 lambda = 2.284280 x = [0.934118 -0.356965] y = [2.088423 -0.934118]\n000000/000000 lambda = 2.127152 x = [0.912847 -0.408302] y = [1.921937 -0.912847]\n000000/000000 lambda = 2.060266 x = [0.903291 -0.429029] y = [1.851815 -0.903291]\n000000/000000 lambda = 2.029356 x = [0.898775 -0.438410] y = [1.819504 -0.898775]\n000000/000000 lambda = 2.014491 x = [0.896580 -0.442881] y = [1.803979 -0.896580]\n000000/000000 lambda = 2.007199 x = [0.895499 -0.445064] y = [1.796367 -0.895499]\n000000/000000 lambda = 2.003588 x = [0.894962 -0.446143] y = [1.792599 -0.894962]\n000000/000000 lambda = 2.001791 x = [0.894694 -0.446679] y = [1.790724 -0.894694]\n000000/000000 lambda = 2.000895 x = [0.894561 -0.446947] y = [1.789788 -0.894561]\n000000/000000 lambda = 2.000447 x = [0.894494 -0.447080] y = [1.789321 -0.894494]\n000000/000000 lambda = 2.000224 x = [0.894460 -0.447147] y = [1.789088 -0.894460]\n000000/000000 lambda = 2.000112 x = [0.894444 -0.447180] y = [1.788971 -0.894444]\n000000/000000 lambda = 2.000056 x = [0.894436 -0.447197] y = [1.788913 -0.894436]\n000000/000000 lambda = 2.000028 x = [0.894431 -0.447205] y = [1.788883 -0.894431]\n000000/000000 lambda = 2.000014 x = [0.894429 -0.447209] y = [1.788869 -0.894429]\n\nWhen I build the pip-packages. i got another error.\nHere is the output:\nserver@server:~/tensorflow$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\nERROR: no such package '@local_config_cuda//crosstool': BUILD file not found on package path.\nERROR: no such package '@local_config_cuda//crosstool': BUILD file not found on package path.\nINFO: Elapsed time: 0.037s\n\nWhere am I wrong?\n", "comments": ["This is a duplicate of #4105, please follow that thread for possible workarounds and hopefully an eventual fix.  Thanks for reporting the issue!\n", "Yes! I've fixed this problem through setting LD_LIBRARY_PATH and cuda version carefully, thanks!\n"]}, {"number": 4264, "title": "WIP: Update OSS protobuf git depedency version to 3.0.2", "body": "from 3.0.0-beta-2\n\nAdd a level of indirection to the int64 type in\ncore/example/feature_util.h, feature_util.cc and the places where the\ntype is used. This is for dealing with the variability of \"int64\" type\ndefinition among different platforms. On some systems, int64 is \"long\nint\", while on others it is \"long long int\". See GitHub Issue for more\ndetails: https://github.com/tensorflow/tensorflow/issues/3626\n", "comments": ["@caisq, thanks for your PR! By analyzing the annotation information on this pull request, we identified @vrv, @kirilg and @tensorflower-gardener to be potential reviewers\n"]}, {"number": 4263, "title": "Update OSS protobuf git depedency version to 3.0.0", "body": "from 3.0.0-beta-2\n\nAdd a level of indirection to the int64 type in\ncore/example/feature_util.h, feature_util.cc and the places where the\ntype is used. This is for dealing with the variability of \"int64\" type\ndefinition among different platforms. On some systems, int64 is \"long\nint\", while on others it is \"long long int\". See GitHub Issue for more\ndetails: https://github.com/tensorflow/tensorflow/issues/3626\n", "comments": ["@caisq, thanks for your PR! By analyzing the annotation information on this pull request, we identified @vrv, @kirilg and @tensorflower-gardener to be potential reviewers\n"]}, {"number": 4262, "title": "Exclude external/local_config_cuda from being built into the PIP package", "body": "Fixes #4261\n", "comments": ["@davidzchen, thanks for your PR! By analyzing the annotation information on this pull request, we identified @vrv, @keveman and @girving to be potential reviewers\n", "LGTM\n"]}, {"number": 4261, "title": "PIP package contains CUDA libraries", "body": "As discussed offline: the PIP package size has greatly increased since the `cuda_configure` change because the CUDA libraries under `external/local_config_cuda` are now being included by `build_pip_package.sh`.\n\nThe simplest fix for this is to special-case `local_config_cuda` to exclude it from being copied in `build_pip_package.sh` since there does not seem to be a way to specifically exclude all files under `@local_config_cuda//` in the `sh_binary` rule itself.\n\nThe reason why we did not experience this previously is because the CUDA libraries were symlinked under `third_party`, and `build_pip_package.sh` rsyncs `third_party/eigen3` specifically.\n", "comments": []}, {"number": 4260, "title": "Fix Make build under Linux (Ubuntu)", "body": "- Add missing files to the list of source to build.\n- Add the -fPIC flag under Linux (ignored when no necessary).\n", "comments": ["@ic, thanks for your PR! By analyzing the annotation information on this pull request, we identified @martinwicke, @tensorflower-gardener and @vrv to be potential reviewers\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Thanks @ic! Those changes look good now, I appreciate the patch.\n", "@petewarden Thank you for the link---I missed that part, sorry.\n\nChange done and tested on Mac OS X (10.10) and Linux Ubuntu 16_04.\n\nI did not find any note in the contribution guideline on rebasing or squashing commits, so I left these as-is. Please let me know if there's something necessary.\n", "@ic I believe the commiter will squash the commits, so that should be no problem, thanks again!\n", "@petewarden Ok, thanks.\n\nOne side note: When I run `tensorflow/contrib/makefile/gen_file_lists.sh`, I have four files modified, and pretty unrelated to the change in this PR.\n\n```\nmodified:   tensorflow/contrib/makefile/proto_text_cc_files.txt\nmodified:   tensorflow/contrib/makefile/proto_text_pb_cc_files.txt\nmodified:   tensorflow/contrib/makefile/proto_text_pb_h_files.txt\nmodified:   tensorflow/contrib/makefile/tf_proto_files.txt\n```\n\nThe contents remove and add `.cc` files and `.proto`.\n\nI think this is unrelated, but perhaps an important cleaning stage. Another similar issue has just popped up with the dynamic RNN API, and I wonder if the `gen_file_lists` script could not help keeping in sync with Bazel more easily. Working on this, so more issues to come~\n", "@tensorflow-jenkins test this please\n", "@ic can you rebase please?\n", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "@martinwicke Rebase worked locally. After a surprising result, it looks good now.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins test this please.\n"]}, {"number": 4259, "title": "Command line processor fails silently on mistyped args", "body": "### Problem:\n\nThe command line processor doesn't raise exceptions when the existing flags that are in common use throughout TensorFlow (at the very least by convention) are mistyped. For example, let's take\n\n```\n--num_gpus\n```\n\nwhich is frequently encountered, e.g. in Inception v3, CIFAR-10, etc. If I type a single dash instead of two, the CLI arg processor just picks up whatever the default value is defined in my app and silently ignores the misformed CLI argument. I realize that one can't restrict the args that are supported because they are pulled from sys.argv and there may be other args passed on to the user's app code, so it's not the same level of strictness as say in processing CLI switches for Unix apps which have a fixed set of options. However, perhaps the solution is to look for small typos such as Damerau-Levenshtein distance of 1 (one deletion, insertion, substitution or transposition, with 1 transposition still treated as edit distance of 1) from the arg specified as recognizable in FLAGS that should raise an exception unless the users forces literal interpretation of the args?\n### Affected versions:\n\nlatest 0.10, probably master top of tree and others as well\n", "comments": ["My personal suggestion is to not use tf.flags and to just use argparse / your own solution.  As far as I know, it was provided for convenience, but it's more of a maintenance burden for us than it's worth, and we might remove it.  We don't want to be in the flags library business ;)\n"]}, {"number": 4258, "title": "issue #4252", "body": "tensorflow/contrib/makefile/tf_pb_text_files.txt missing a couple of items.\n[Issue #4252](https://github.com/tensorflow/tensorflow/issues/4252)\n", "comments": ["@aaalgo, thanks for your PR! By analyzing the annotation information on this pull request, we identified @tensorflower-gardener, @keveman and @martinwicke to be potential reviewers\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@mrry / @keveman  / @martinwicke would any of you be suited to review this?\n", "Makefile issues are usually @petewarden's speciality, so I'm deferring to his wisdom :).\n", "Yeah don't be fooled by the bot! :) I think the ifdef in core is something\nto look at though. I don't know what the IS_SLIM_BUILD is, but I expect it\nto be mobile related.\nOn Wed, Sep 7, 2016 at 17:02 Derek Murray notifications@github.com wrote:\n\n> Makefile issues are usually @petewarden https://github.com/petewarden's\n> speciality, so I'm deferring to his wisdom :).\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4258#issuecomment-245456048,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAjO_b4QqylNQr1Qrw_qWTHn-yiiOsGAks5qn1CdgaJpZM4J3b_I\n> .\n", "Thanks! I've actually been trying to keep the zlib-related stuff out of the mobile builds entirely at the source file level, so on my local build I've actually got `$(wildcard tensorflow/core/lib/io/zlib*) \\` added to the `CORE_CC_EXCLUDE_SRCS` definition in the Makefile. Unfortunately it's part of a larger change that I haven't been able to check in yet, but if that's enough information to reproduce would you be able to try that approach?\n\nThanks for your help maintaining this, I do appreciate it!\n", "@petewarden is this redundant with your other PR?\n", "Yes, I believe #4287 fixed this issue, so I'm closing this.\n"]}, {"number": 4257, "title": "Move all model implementations to tensorflow/models ?", "body": "If I am correct, tensorflow.models is being deprecated.\nBut some model implementations are still kept inside this directory because of tensorflow tutorials. \nI think it would be more straight forward and relevant to keep all the model implementations in tensorflow/models repository.\n\n(This is a same issue discussed in [tensorflow/models issue 361](https://github.com/tensorflow/models/issues/361).)\n\nI made a [pull request](https://github.com/tensorflow/models/pull/393) that moves all tensorflow/tensorflow model implementations to tensorflow/models without losing commit histories.\n\nI can make another pull request to tensorflow/tensorflow to fully deprecate the model implementations and update tutorials.\n", "comments": ["I'll let @martinwicke comment on this.\n", "This is the right thing to do. We need to update the docs though, and our website. Maybe we can do that here, but I'm not sure. @wolffg @random-forests @dr4b what do you think? Kick start our tutorials/docs reorg?\n", "@nealwu Is this all complete?\r\nDid we also update the docs?", "Yep, this is complete. The models are now in https://github.com/tensorflow/models/tree/master/tutorials. The docs are also mostly updated, but I'm still working on a few issues with them now."]}, {"number": 4256, "title": "Tensorflow in Azure ML", "body": "I've so far seen people using tensorflow in Azure using in this [link](http://www.mikelanzetta.com/tensorflow-on-azure-using-docker.html).\nAlso using the advantage of ubuntu in windows tensorflow can be run on\nwindows pc as well.Here is the [link](http://www.hanselman.com/blog/PlayingWithTensorFlowOnWindows.aspx).\nHowever during a conversation with Windows Azure engineer Hai Ning it came out\nthat \"Azure ML PaaS VMs use Windows OS; TensorFlow is not supported on Windows as of now.\"\nHence there is no direct way of running tensorflow in Azure ML.\nIs there any work around anyone figured out that allows running tensorflow in Azure ML.\n\nNOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\n\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n### Environment info\n\nOperating System:\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n### What other attempted solutions have you tried?\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n", "comments": ["Others probably have more insight on this, but there is a windows version forthcoming (see #17).\n", "Just a small observation: [Azure ML](https://azure.microsoft.com/en-us/services/machine-learning/) is a fully managed service. Even when TensorFlow runs on Windows, you'd have to ask the people who manage that service to add TensorFlow to the set of modules that Azure ML exports.\n\nIf you want a managed cloud service that does offer TensorFlow support, the [Google Cloud ML](https://cloud.google.com/ml/) product is in limited preview, and you can [apply](https://services.google.com/fb/forms/machinelearningpreview/) to join the alpha.\n", "Agree with @mrry . Google Cloud ML is the better choice for managed TensorFlow service or you can implement the similar one in bare servers with TensorFlow. \n\nAzure ML is not the open-source project and I'm not sure if TensorFlow community could help. Maybe asking Microsoft to support TensorFlow could be more helpful.\n", "This is an interesting thread.  @tanayz can you expand on what you plan or wish to do?\n", "You might want to take a look at this new blog post\nhttps://blogs.msdn.microsoft.com/uk_faculty_connection/2016/09/26/tensorflow-on-docker-with-microsoft-azure/\n", "Thanks @aselle seems like accessing it from AML is doable.However I already found that keras can run in AML,hence keeping this in back burner for now.\n", "Automatically closing due to lack of recent activity. Please file a new issue referring to this issue, if the issue is still relevant and important. If this is documenting some kind of bug, be sure to use the latest version of TensorFlow. Thank you so much.", "Here is another blog to run tensorflow in Azure App Services \r\nhttps://prmadi.com/playing-piano-python-app-in-azure-app-services-linux/ "]}, {"number": 4255, "title": "iOS memory warnings", "body": "I tried posting this to SO first and it's been there a week with no activity: https://stackoverflow.com/questions/39255211/tensorflow-ios-memory-warnings\n\nWe are building an iOS app to perform image classification using the TensorFlow library.\n\nUsing our machine learning model (91MB, 400 classes) and the TensorFlow 'simple' example, we get memory warnings on any iOS device with 1GB of RAM. 2GB models do not experience any warnings, while < 1GB models completely run out of memory and crash the app.\n\nWe are using the latest TensorFlow code from the master branch that includes [this iOS memory performance commit](https://github.com/tensorflow/tensorflow/commit/459c2fed498530b794c4871892fd68d1e6834ac6), which we thought might help but didn't.\n\nWe have also tried setting various GPU options on our TF session object, including `set_allow_growth(true)` and `set_per_process_gpu_memory_fraction()`.\n\nOur only changes to the TF 'simple' example code is a `wanted_width` and `wanted_height` of 299, and an `input_mean` and `input_std` of 128.\n\nAny thoughts? Is our model simply too big?\n", "comments": ["@petewarden might have some ideas.\n", "I am working on an approach that uses memory mapping to avoid this problem. It works by mmap-ing the weight parameters of large files into read-only pages that don't count towards the memory limits on iOS. You can see the work-in-progress here:\nhttps://github.com/petewarden/tensorflow/blob/master/tensorflow/contrib/ios_examples/camera/tensorflow_utils.mm#L162\n\nI'll be finishing this off and writing proper documentation, but for now take a look at how we use that function, and here's how you create a memory-mappable file from the standard Inception v3 model:\n\n```\nbazel build tensorflow/contrib/util:convert_graphdef_memmapped_format && \\\nbazel-bin/tensorflow/contrib/util/convert_graphdef_memmapped_format \\\n--in_graph=tensorflow/contrib/ios_examples/camera/data/tensorflow_inception_graph.pb \\\n--out_graph=tensorflow/contrib/ios_examples/camera/inception_mmap.pb\n```\n\nDoes that help?\n", "Yes, that seems like it should help us. Thanks Pete.\n", "Just wanted to give an update on this. We mmapped our model as Pete showed above (FYI, this took quite awhile on a modern MacBook Pro) and implemented the changes from https://github.com/tensorflow/tensorflow/pull/4457 in our code.\n\nResult is no more memory warnings on 1GB iOS devices plus a much faster classification (2-3 seconds, down from several). Still need to test a 512MB device but I think we'll be OK.\n", "Just mmapped a quantized and unquantized graph, both resulting in a broken graph\r\n\r\n- 8bit quantized graph says 0 nodes converted, and can't load resulting `pb` graph (error `Out of range: Read less bytes than requested`)\r\n- unquantized graph says 16 nodes converted but prints error `Session was not created with a graph before Run()` when calling `Run()`", "@eaigner do you have info? The second problem looks like you just have to create a default session.\r\n", "The code is the same that works with the unmapped graph. A Session is definitely created (just no valid one).", "I see. Is this iOS only or does it fail on linux too? If so, do you have a simple repro script?", "Closing due to lack of response.  Please reopen if necessary.", "Follow-up question: is it possible to mmap a quantized graph? I've been trying and encountering the same issue that @eaigner was encountering above. Without quantization, my graph is huge but runs into memory issues on IOS. With quantization, it's small and I think it will work but I cannot mmap now."]}, {"number": 4254, "title": "Bug in decode", "body": "Need to change the batch_size before create the model. Right now, the model has placeholders for decode to be [batch_size]. It should be 1.\n", "comments": ["Can one of the admins verify this patch?\n", "@chiphuyen, thanks for your PR! By analyzing the annotation information on this pull request, we identified @vrv, @benoitsteiner and @keveman to be potential reviewers\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I think overriding the FLAG is not a good pattern. The FLAGS are intended to be a way for the user to specify options to the program, not as a way for the program to communicate arbitrary global state within itself. Please take a different approach.\n"]}, {"number": 4253, "title": "How can I use dropout on input layer?", "body": "Hi everyone:\nI try to combine softmax classifier and dropout algorithm, because there are only input layer and classfier. According tutorial, I want to use dropout on input layer like following:\n![image](https://cloud.githubusercontent.com/assets/12611573/18319468/6cd13298-7558-11e6-997d-0f87349b0015.png)\nBut I got the following error:\nTraceback (most recent call last):\n  File \"softmax.py\", line 255, in <module>\n    Start_building_and_training(num_sample, x_data_merged_dnaid[:], y_label[:], num_batch)\n  File \"softmax.py\", line 213, in Start_building_and_training\n    prob = sess.run(y, feed_dict={x: batch_xs, y_: batch_ys})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 717, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 915, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 965, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 985, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder_1' with dtype float\n     [[Node: Placeholder_1 = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n     [[Node: Softmax/_5 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_24_Softmax\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nCaused by op u'Placeholder_1', defined at:\n  File \"softmax.py\", line 255, in <module>\n    Start_building_and_training(num_sample, x_data_merged_dnaid[:], y_label[:], num_batch)\n  File \"softmax.py\", line 194, in Start_building_and_training\n    keep_x = tf.placeholder(tf.float32)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 1270, in placeholder\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 1530, in _placeholder\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2334, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1253, in __init__\n    self._traceback = _extract_stack()\n\nCan anyone help me ? Thanks a lot!\n", "comments": ["Hi @JohnnyY8!  The concept behind dropout is to randomly drop some nodes _from the hidden layer_ while training.  In the code you've highlighted above, you're applying dropout _to the input layer_, which is the problem.\n\nFor an example of how to apply dropout see this:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py#L109\n\nHope this helps!  FYI we primarily use github issues to track bugs and installation issues; in the future please ask these types of questions on StackOverflow and tag it with the `tensorflow` tag.  Thanks!\n", "From the error message and the code, I think the problem is rather that the `keep_x` placeholder is not fed into session.run. Giving a name to the placeholder should make the error message more helpful.\n", "@tatatodd Hi:\nThanks for your answer. But applying dropout to the input layer maybe not the root problem, I think it is good for input and hidden layer. Thank you for your help again.\n", "@fwalch Hi:\nThank you, I am just a beginner for tensorflow and do not understand how to feed the placeholder into session.run? Do you mean by feed_dict argument?\n", "@JohnnyY8 \r\nyes\r\ne.g.\r\nsess.run(train, feed_dict={x: x_train, y_true: y_train, dropout: TRAIN_DROPOUT})", "@tomermeged Truly thanks!"]}, {"number": 4252, "title": "make -f tensorflow/contrib/makefile/Makefile for iOS is failing", "body": "Probably related to [issue 2896](https://github.com/tensorflow/tensorflow/issues/2896)\n\nI am trying to build tensorflow on a mac for iOS. The first two commands work well:\n`tensorflow/contrib/makefile/download_dependencies.sh`\nand\n`compile_ios_protobuf.sh`\n\nHowever, when I call\n`make -f tensorflow/contrib/makefile/Makefile \\\n TARGET=IOS \\\n IOS_ARCH=ARM64`\n\nI receive a linker error:\n\n> Undefined symbols for architecture x86_64:\n>   \"tensorflow::io::InputStreamInterface::SkipNBytes(long long)\", referenced from:\n>       vtable for tensorflow::io::ZlibInputStream in zlib_inputstream.o\n>   \"tensorflow::io::RandomAccessInputStream::RandomAccessInputStream(tensorflow::RandomAccessFile_)\", referenced from:\n>       tensorflow::io::RecordReader::RecordReader(tensorflow::RandomAccessFile_, tensorflow::io::RecordReaderOptions const&) in record_reader.o\n>   \"typeinfo for tensorflow::io::InputStreamInterface\", referenced from:\n>       typeinfo for tensorflow::io::ZlibInputStream in zlib_inputstream.o\n>   \"vtable for tensorflow::io::InputStreamInterface\", referenced from:\n>       tensorflow::io::InputStreamInterface::InputStreamInterface() in zlib_inputstream.o\n>   NOTE: a missing vtable usually means the first non-inline virtual member function has no definition.\n> ld: symbol(s) not found for architecture x86_64\n> clang: error: linker command failed with exit code 1 (use -v to see invocation)\n> make: **\\* [/Users/senad/repos/tensorflow2/tensorflow/contrib/makefile/gen/host_bin/proto_text] Error 1\n> - '[' 2 -ne 0 ']'\n> - echo 'armv7 compilation failed.'\n>   armv7 compilation failed.\n> - exit 1\n\nI tried it with the current master commit `f71cc62f71cc62`. After reading the discussion of issue 2896, I also tried it with the commit `582e6c8582e6c8`. This is the one that supposedly fixed the issue in the other ticket.\n", "comments": ["I see the same issue when using makefile to produce Linux binary.  Indeed similar to issue 2896.\n", "Indeed this looks similar to #2896\n\n@petewarden might have clues as to why this is happening.\n", "Another confusing thing I noticed after looking through the source code: \n`RandomAccessInputStream` class does not call the method `SkipNBytes(long long)` from anywhere in the code.\n\nHow can it then appear like that in the call stack in the error message?\n", "I have a fix on my local branch of the Makefile at https://github.com/petewarden/tensorflow/blob/master/tensorflow/contrib/makefile/Makefile#L435, but unfortunately it's part of a bigger change. Adding that line to exclude zlib source files from the build should fix this though.\n", "I added the line to my makefile that excludes `zlib*`. I still get the same error though. I think the class causing the error is in `random_inputstream.ccrandom_inputstream.cc`?\n\nThat one will still get built.\n", "Having exactly the same problem here, the solution @petewarden suggested didn't work, unfortunately.\n\nThe logs are:\n\n```\nUndefined symbols for architecture x86_64:\n  \"tensorflow::io::InputStreamInterface::SkipNBytes(long long)\", referenced from:\n      vtable for tensorflow::io::ZlibInputStream in zlib_inputstream.o\n  \"tensorflow::io::RandomAccessInputStream::RandomAccessInputStream(tensorflow::RandomAccessFile*)\", referenced from:\n      tensorflow::io::RecordReader::RecordReader(tensorflow::RandomAccessFile*, tensorflow::io::RecordReaderOptions const&) in record_reader.o\n  \"typeinfo for tensorflow::io::InputStreamInterface\", referenced from:\n      typeinfo for tensorflow::io::ZlibInputStream in zlib_inputstream.o\n  \"vtable for tensorflow::io::InputStreamInterface\", referenced from:\n      tensorflow::io::InputStreamInterface::InputStreamInterface() in zlib_inputstream.o\n  NOTE: a missing vtable usually means the first non-inline virtual member function has no definition.\nld: symbol(s) not found for architecture x86_64\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\nmake: *** [/Users/alexandrolferuk/Documents/Projects/ML/tensorflow/tensorflow/contrib/makefile/gen/host_bin/proto_text] Error 1\n```\n", "I have a pending fix in PR #4287, could you confirm that fixes your issue?\n", "Yes, sir, it does help, thank you!\n", "Excellent, closing this out.  If you still have issues just ping this thread.\n"]}]