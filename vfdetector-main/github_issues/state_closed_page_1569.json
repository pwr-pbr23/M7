[{"number": 5844, "title": "[CMake] Add TensorBoard dependencies to PIP package.", "body": "Currently, TensorBoard external dependencies (JS scripts, css, images etc.) are not part of PIP package built by CMake. As a result, when you navigate to TensorBoard on Windows, it shows a blank screen. Added CMake scripts to download TensorBoard dependencies and make them part of PIP package.", "comments": ["Can one of the admins verify this patch?", "@vit-stepanovs, thanks for your PR! By analyzing the history of the files in this pull request, we identified @mrry, @danmane and @guschmue to be potential reviewers.", "Jenkins, test this please.", "Looks like build failed because multiple dependencies have the same downloadable archive name... Not sure why this passed for me locally. Will make a change to download each archive in a separate directory.", "@tensorflow-jenkins test this please.", "Mr. Jenkins: test this please", "It looks like test failures are unrelated...", "For good measure, let me rerun tests, than we can merge right away.\r\nJenkins, test this please.", "How do I install this new PIP package or how can I create/build one with this fix?", "@DomenicD, for now you can build a PIP package manually by following [these instructions](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md). Otherwise, it looks like this fix will be included in 0.12rc1 release, which may be available some time soon.", "@DomenicD if you don't want to build manually you can also get a whl of the nightly build from here: http://ci.tensorflow.org/view/Nightly/job/nightly-win/lastSuccessfulBuild/DEVICE=gpu,OS=windows/", "@ferrouswheel thank you, nightly worked great."]}, {"number": 5843, "title": "Smaller dataset to use for the seq2seq translate example?", "body": "I'm running the translate/seq2seq_model.py example code, and the dataset is rather large and I'm running out of space in my allocated remote node. Is there a smaller dataset that could be compatible with the code? \r\n\r\nMany thanks!", "comments": ["From https://www.tensorflow.org/versions/r0.11/resources/index.html\r\n> For help and support, technical or algorithmic questions, please submit your questions to Stack Overflow: https://stackoverflow.com/questions/tagged/tensorflow. You may also find answers in our FAQ, our glossary, or in the shapes, sizes and types guide. Please do not use the mailing list or issue tracker for support.\r\n\r\n", "Trying my hands on sequenct2sequence model for language translation and I'm curious to know, can we predict any translations using small datasets? like with 100 or 200 strings. Please share your findings"]}, {"number": 5842, "title": "Implementing SVD rank threshold?", "body": "It might be useful to use Eigen SVD's `Eigen::BDCSVD::setThreshold` function in order to cull the effectively zero singular values, perhaps by introducing a new parameter to TensorFlow's SVD so that `tf.svd(tensor, compute_uv=True, full_matrices=False, thresholded=True, name=None)` uses\r\n- the default Eigen threshold for `thresholded=True`\r\n- the full P = min(M, N) singular values for `thresholded=False`\r\n- whatever threshold is specified with a float, such as `thresholded=1e-7`\r\n\r\nI can get started on a PR if this is something that might be useful.", "comments": ["@rmlarsen, do you think this would be good. It doesn't look like NumPy has anything equivalent. I think if you are willing to accept this, we should invite @AidanGG to contribute it.", "@AidanGG The setThreshold() method in Eigen does not affect the main SVD computation itself. Rather, it is mostly a convenient way to post-filter the SVD when using it to determine numerical rank or using it for applying a pseudo-inverse via the solve() method.\r\n\r\nhttps://bitbucket.org/eigen/eigen/src/41ef39502e7e9aab985bb5b51894d1f87106b67e/Eigen/src/SVD/SVDBase.h?at=default&fileviewer=file-view-default#SVDBase.h-143\r\n\r\nAs such, I would prefer that such filtering be implemented in TensorFlow, rather than relying on this feature in the underlying Eigen C++ library. \r\n\r\nAnother concern is that such thresholding is not widely supported in the same manner by other high performance SVD libraries. Should we decide to replace the Eigen \"backend\" for this op, it would be preferable to have the thresholding logic in TensorFlow itself.\r\n\r\nIt would be awesome if you would be interested in contributing functions that, e.g., apply the pseudo-inverse or filter the SVD based on a threshold. ", "Hi @rmlarsen, I have been using\r\n```\r\nrank = tf.shape(tf.where(tf.greater_equal(s, np.finfo(dtype.real_dtype.as_numpy_dtype).eps * s[0] * tf.cast(tf.size(s), dtype.real_dtype))))[0]\r\n```\r\nand slicing `U`, `s` and  `V` by this `rank`. This works well enough for my purpose."]}, {"number": 5841, "title": "fix typo in `ScatterUpdate` doc", "body": "", "comments": ["@RustingSword, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @keveman and @ebrevdo to be potential reviewers.", "Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please.", "Test failure is unrelated. Merging this PR now. Thank you, @RustingSword "]}, {"number": 5840, "title": "Move Torch to TensorFlow", "body": "Hi, I try to translate a torch code to TensorFlow, but I cannot find the corresponding tensorflow function of `SpatialFullConvolution` which is able to apply transpose convoluation on vector (not only image).\r\n\r\nHow can I deal with it?\r\nhere is an example https://github.com/soumith/dcgan.torch/blob/master/main.lua", "comments": ["From https://www.tensorflow.org/versions/r0.11/resources/index.html\r\n> For help and support, technical or algorithmic questions, please submit your questions to Stack Overflow: https://stackoverflow.com/questions/tagged/tensorflow. You may also find answers in our FAQ, our glossary, or in the shapes, sizes and types guide. Please do not use the mailing list or issue tracker for support.\r\n"]}, {"number": 5839, "title": "Add sess.close() in document (g3doc/get_started/index.md)", "body": "I read from tensorflow Session management document that \r\n\r\nA session may own resources, such as variables, queues, and readers. It is important to release these resources when they are no longer required. \r\nTo do this, either **invoke the close() method on the session**, or **use the session as a context manager.** The following two examples are equivalent:\r\n\r\nbut I found g3doc/get_started/index.md didn't\r\n\r\nso I made the pull request for add sess.close() at the end of code.\r\n\r\nThank you for making tensorflow\r\n", "comments": ["@jangsoopark, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @vrv and @ilblackdragon to be potential reviewers.", "Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed CLA!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please."]}, {"number": 5838, "title": "better documentation of return value of boolean_mask", "body": "It cost me 2 hours of life to debug problems I had because of not understanding how exactly boolean_mask() reshapes the tensor...", "comments": ["Can one of the admins verify this patch?", "@ptakopysk, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @tensorflower-gardener and @aselle to be potential reviewers.", "Jenkins, test this please!", "Jenkins, test this please.\r\n\r\nThat was an unrelated failure."]}, {"number": 5837, "title": "Streaming Median Metric", "body": "I'd like to contribute a streaming median metric to:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/metrics/python/ops/metric_ops.py\r\n\r\nUsing something like the min/max heap streaming median algorithm:\r\nhttp://stackoverflow.com/questions/10657503/find-running-median-from-a-stream-of-integers\r\n\r\nThis method requires some kind of heap/list of tensors. To my knowledge you can't define a tf.Variable to be a list. Can you suggest how to solve this, or a better way to approach this problem? Thanks.", "comments": ["Take a look at `tf.contrib.metrics.streaming_concat` for an example of storing a dynamically resized array in tf.Variable.\r\n\r\nNote that in many cases it's actually fine to just use `streaming_concat` and then take the median in memory (e.g., with numpy).", "oh, that's perfect, thank you!", "Note for future: \r\n> For help and support, technical or algorithmic questions, please submit your questions to Stack Overflow: https://stackoverflow.com/questions/tagged/tensorflow. You may also find answers in our FAQ, our glossary, or in the shapes, sizes and types guide. Please do not use the mailing list or issue tracker for support.\r\n\r\n"]}, {"number": 5836, "title": "Fix: remove the leftover delta addition in cwise_ops_test.", "body": "small fix for #5810.\r\nShould fix mac tests on master.", "comments": ["@gunan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @lukeiwanski, @keveman and @vrv to be potential reviewers.", "CC @prb12 for approval, if online.\r\nThis PR fixed the mac test failures we have been seeing.", "@gunan All the others _compare methods for float32 and true_divide op are adding 0.1 to y component. We assumed that this one was missing. Isn't that the case?\r\n( eg. https://github.com/gunan/tensorflow/blob/101ca9e5530ca47d81d36ba99c1bd800a969d90c/tensorflow/python/kernel_tests/cwise_ops_test.py#L602 )", "@girving seems to have added some of the 0.1's.\r\nGeoffrey, it looks like it has been a long time since yu made the changes, but do you remember why we have the 0.1's added as @lukeiwanski described above?\r\nDo we need the `+ 0.1` Which I removed here? ", "@gunan Presumably the choice of rounding was different when the result was exactly a half integer.  I'm not sure this is different, but the only way I'd be able to tell would be removing them and trying.  \r\n\r\n@aselle Do you think we should match exactly now?  It might depend on processor flags, unfortunately."]}, {"number": 5835, "title": "Fixing bug in Adadelta implementation", "body": "The updated accum vector should be computed as:\r\n\\rho * accum + (1.0 - \\rho) * grad^2,\r\naccording to Algorithm1 in Zeiler 2012 (https://arxiv.org/pdf/1212.5701v1.pdf).", "comments": ["Can one of the admins verify this patch?", "@luheng, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @keveman and @tensorflower-gardener to be potential reviewers.", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Thanks!  I suspect you'll have to update the tests in adadelta_test.py, but I'll kick off the test anyway.\r\n\r\n@tensorflow-jenkins test this please", "Thanks! Weirdly the updates in adadelta_test.py are implemented correctly. Do I still need to update the tests?", "Do you know why the tests were passing before and after?  Should we come up with a better test that would have failed before and passes with your change?", "I updated the precision in adadelta_test.py so it will be able to test the difference.\r\n\r\nSo before the bug fix, testing would give the following assertion error:\r\n\r\n```\r\nAssertionError:\r\nNot equal to tolerance rtol=0.001, atol=0.001\r\n\r\n(mismatch 100.0%)\r\n x: array([ 0.003901,  0.003901], dtype=float16)\r\n y: array([ 0.001991,  0.001991], dtype=float16)\r\n```\r\n\r\nThe original precision `atol=1e-2, rtol=1e-2` for the `accum` variable wasn't able to detect the bug.", "That's great!  Gives us more confidence it's doing the right thing now.\r\n\r\n@tensorflow-jenkins test this please", "PR merged. Thanks, @luheng "]}, {"number": 5834, "title": "TensorForestEstimator with input_fn results in infinite loop for evaluate and predict.", "body": "When using `TensorForestEstimator`, using the previous implementation with x= and y= works. However, when trying to convert to the soon-to-be standard of input_fn, I get strange behaviors. Given \r\n\r\n```\r\nparams = tf.contrib.tensor_forest.python.tensor_forest.ForestHParams(\r\nnum_trees=10,\r\nmax_nodes=100,\r\nnum_classes=2,\r\nnum_features=features_count)\r\nclassifier = TensorForestEstimator(params)\r\n```\r\n\r\ncalling \r\n```\r\nclassifier.fit(input_fn=(lambda: training_input_fn(trainset)))\r\n```\r\nworks and completes correctly. The input_fn used is the following :\r\n\r\n```\r\nfeatures_name = [] # ordered list of all features in the dataset\r\ndef training_input_fn(dataset):\r\n        data, target = dataset\r\n        # 'data' is a numpy array where data[:,i] returns all observations for a given feature\r\n        # It is reshaped because otherwise I had a concat error\r\n        features = {features_name[i]: tf.constant(data[:, i], shape=(data.shape[0], 1))\r\n                    for i in range(len(features_name)) if features_name[i] in DEFAULT_FEATURES}\r\n        labels = tf.constant(target)\r\n        return features, labels\r\n```\r\n\r\n\r\nHowever, calling \r\n```\r\nclassifier.evaluate(input_fn=(lambda: training_input_fn(validset)))\r\n```\r\nafterwards results in an apparently neverending loop in `tf.contrib.learn.python.learn.graph_actions` in function `run_feeds_iter` in the following snippet :\r\n```\r\n      try:\r\n        threads = queue_runner.start_queue_runners(session, coord=coord)\r\n        for f in feed_dicts:\r\n          yield session.run(output_dict, f)\r\n      finally:\r\n        coord.request_stop()\r\n        if threads:\r\n          coord.join(threads, stop_grace_period_secs=120)\r\n```\r\nThe loop seems like never stopping. Also, memory starts increasing slowly but steadily from there (had to stop at 10 GB for this fairly small dataset). The same goes if I try predict on it. \r\nMy Training dataset is composed of 3633 entries with 183 features and my validation dataset is composed of 2180 entries. \r\n\r\n### Environment info\r\nOperating System: \r\nUbuntu 16.04, running on Python3.5\r\n\r\nInstalled version of CUDA and cuDNN: \r\n```\r\n-rw-r--r-- 1 root root   558720 Nov 22 10:52 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Nov 22 10:52 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 Nov 22 10:52 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rwxr-xr-x 1 root root   415432 Nov 22 10:52 /usr/local/cuda/lib64/libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   775162 Nov 22 10:52 /usr/local/cuda/lib64/libcudart_static.a\r\nlrwxrwxrwx 1 root root       13 Nov 22 12:47 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5\r\nlrwxrwxrwx 1 root root       17 Nov 22 12:47 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5\r\n-rwxr-xr-x 1 root root 79337624 Nov 22 12:47 /usr/local/cuda/lib64/libcudnn.so.5.1.5\r\n-rw-r--r-- 1 root root 69756172 Nov 22 12:47 /usr/local/cuda/lib64/libcudnn_static.a\r\n```\r\n\r\nIf installed from source, provide \r\n1. The commit hash (`git rev-parse HEAD`) : `a26a5925b0500d0e9cd259989fc0e113fa29e27f`\r\n2. The output of `bazel version` : \r\n\r\n`Build label: 0.4.0\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Nov 2 17:54:14 2016 (1478109254)\r\nBuild timestamp: 1478109254\r\nBuild timestamp as int: 1478109254`", "comments": ["@gilberthendry Is this the inteneded use of `input_fn`? - I can't tell from the documentation!\r\n\r\nI don't know anything about `TensorForestEstimator`, but in other places where an `input_fn` is used in TensorFlow it has often been to specify code to read a mini-batch from the file system using some sort of IO op, preprocess the samples and insert into a `Queue` of some kind.  In these other scenarios, the sub-graph created by `input_fn` is executed repeatedly in a `QueueRunner` thread.\r\n\r\nIn your example, you seem to be reading the entire dataset into a `tf.constant` and adding it to the graph.  This is probably fine if input_fn is only called once.\r\n\r\nIf `input_fn` was called repeatedly, then you would a) have a huge memory leak by adding large constants to the graph, and b) get dreadful performance by repeatedly changing the graphdef causing all the cached optimized datastructures to be thrown away.\r\n\r\nYou could probably check this quickly by adding a print/log statement to `training_input_fn`.", "As @prb12 points out, this isn't the usual usage of an input function (returning a tf.constant).  For starters, you won't get any mini-batching.  It looks like you'd be processing the whole dataset on every step during training (which is ok if that's what you want, but is a not very efficient thing to do for online extremely random forests). \r\n\r\nWithout trying to reproduce this myself, I suspect that evaluate might never terminate because it never throws the exception that indicates that the input is out of data (I think it still works this way).  You can verify this by passing max_steps=1 to evaluate.  \r\n\r\nStill, it seems strange that the input_fn is being called more than once, resulting in your memory leak and such.  \r\n\r\nI don't know what the migration plan is for other estimators to switch to input_fn, but TensorForestEstimator will continue to support x= and y= for datasets that easily fit in memory.  We will do the conversion to the right kind of tf.learn.Estimator under the hood (there will be an explicit class for this).  So my advice is to keep using x= and y= unless you want to use the IO ops mentioned by @prb12, which is really only necessary for large datasets.", "> Still, it seems strange that the input_fn is being called more than once, resulting in your memory leak and such.\r\n\r\nNote - that was just my hypothesis - we haven't yet verified whether this is what's happening.", "@prb12 Thanks for the feedback. My `training_input_fn` is only called once for fit and once for evaluate. However, if I add an print statement with and increasing integer in  `run_feeds_iter` mentioned earlier, it just prints, _ad infinitum_, which leads me to believe that @gilberthendry might be right about the exception never being raised.\r\n\r\n@gilberthendry Thanks for the clarification. I tried passing \"max_steps=1\" to evaluate, but the argument does not exist for evaluate. Also, I believe you may be right about the exception never being raised. What is the proper way of raising that exception in an `input_fn`? And the strangest, why does my training ( `fit` function) works while it fails on `predict` and `evaluate` ... They are using the same `input_fn`, with the exception of the data used being different. There is something I am missing here... \r\n\r\nOn a side note, I was using x and y inputs before, but now I get a Warning message saying the x= and y= inputs will be deprecated and to use input_fn instead (or SKCompat). The resulting input_fn came from this [tutorial](https://www.tensorflow.org/versions/r0.11/tutorials/input_fn/index.html#building-the-input-fn). As I want to be able to export my trained model on Android, I want to use the [export](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard2/tf.contrib.learn.BaseEstimator.md#tfcontriblearnbaseestimatorexportargs-kwargs-baseestimatorexport) function to save my model. Since this function is not part of the SKCompat wrapper, I thought I had to convert my `x` and `y` to an `input_fn` to be compatible with future versions and with the `export` function itself which requires an `input_fn`. I still am struggling at understanding how I am suppose to pass an input to get a prediction out of the trained model once it is exported on Android, but I understand if this might be better posted on stackoverflow for support!", "My bad, the parameter is called just 'steps', not max_steps. Try that.\r\n\r\nFit works fine because TensorForestEstimator (like many estimators) doesn't rely on the input being exhausted to complete.  It uses TensorForestLossHook (or a custom hook) to terminate training when the forest stops growing.\r\n\r\nThe way to indicate that input is exhausted is to raise StopIteration.  The problem with your input_fn is that there's no place to raise StopIteration, because it isn't based on a get_feed_fn like data_feeder.py uses. \r\n\r\nYou might be able to pass a separate input_fn for export (that maybe returns an appropriately typed and sized placeholder?), and use x=, y= for training and evaluation.  Not sure about that, just something to think about.\r\n\r\n\r\nOverall, one way to go is to convert your data into tf.Examples, then using input_fn's is much more natural. Another way is to always evaluate with steps=1, and maybe exporting/predict will work fine. ", "I had exactly the same problem, with a custom estimator, and an `input_fn`, that returns a tensor, and just looping forever.\r\n\r\nThe fix turned out to be easy in the end, though whether it should really be necessary is an open question.  Instead of eg:\r\n\r\n```\r\nfor y in estimator.predict(input_fn=...):\r\n    print('y', y)\r\n```\r\nor:\r\n```\r\ny = list(estimator.predict(input_fn=...))\r\n```\r\n... use itertools.islice to only retrieve as many predictions as you inputed input data, ie/eg:\r\n```\r\ny_generator = estimator.predict(input_fn=...)\r\ny = list(itertools.islice(y_generator, X.shape[0])\r\n```", "Thanks for all the feedback. I will consider the issue as closed for the moment because of all the proposed fixes that I have yet to test. Thanks for the quick support!", "I came here stuck at the same issue and the comment by @hughperkins solves it for me. Good catch! Thanks \ud83d\ude04 ", "I found that this issue happens in windows but when I run the exact same code in linux server this issue will not persist. "]}, {"number": 5833, "title": "Switch back to --relaxed-constexpr for cuda 7.0 compatibility.", "body": "Fixes #5799", "comments": ["@vrv, thanks for your PR! By analyzing the history of the files in this pull request, we identified @meteorcloudy, @keveman and @tensorflower-gardener to be potential reviewers.", "test breakage known issue on mac. merging."]}, {"number": 5832, "title": "Still 0.11.RC0 after upgrading to RC2", "body": "I've used Anaconda-Pip installation in the tutorial to upgrade my TensorFlow\r\n\r\n```\r\n# Ubuntu/Linux 64-bit, CPU only, Python 3.5\r\n(tensorflow)$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0rc2-cp35-cp35m-linux_x86_64.whl\r\n\r\n# Python 3\r\n(tensorflow)$ pip3 install --ignore-installed --upgrade $TF_BINARY_URL\r\n\r\n....> ipython\r\n\r\nIn [1]: import tensorflow as tf\r\nIn [2]: tf.__version__\r\nOut[2]: '0.11.0rc0'\r\n\r\n```\r\n\r\nBut it seems it is still in RC0, in which doesn't include new method such as `tf.summary.merge_all()`\r\n\r\n", "comments": ["Could you please check the TF version from python3 directly?\r\n", "@prb12 \r\n```\r\n# Ubuntu/Linux 64-bit, CPU only, Python 3.5\r\n(tf) omtcyfz@omtcyfz-VW:~$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0rc2-cp35-cp35m-linux_x86_64.whl\r\n(tf) omtcyfz@omtcyfz-VW:~$ pip3 install --ignore-installed --upgrade $TF_BINARY_URL\r\nCollecting tensorflow==0.11.0rc2 from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0rc2-cp35-cp35m-linux_x86_64.whl\r\n(tf) omtcyfz@omtcyfz-VW:~$ python3\r\nPython 3.5.2+ (default, Sep 22 2016, 12:18:14) \r\n[GCC 6.2.0 20160927] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\n>>> tensorflow.__version__\r\n'0.11.0rc2'\r\n```\r\nMy `__version__` is just fine.", "@prb12 \r\n\r\n\r\n```\r\n...> python3\r\nPython 3.5.2 |Continuum Analytics, Inc.\r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'0.11.0rc0'\r\n>>> \r\n```\r\n\r\nI've tried to show TF version directly on python3, it still returns RC0\r\n\r\nBy the way, I am using Anaconda virtual environment, all command above are used after `source activate tensorflow`. Does it have anything to do with this ? And it shows above that my GCC version is 4.4.7, and in @omtcyfz 's comment, it shows GCC 6.2.0 ", "@omtcyfz \r\n\r\nAre you using Anaconda virtual environment ?", "@zuoxingdong No, I am using `virtualenv` installed via pip. python3 is installed via apt-get.", "@zuoxingdong \r\n\r\nAs for\r\n\r\n> And it shows above that my GCC version is 4.4.7, and in @omtcyfz 's comment, it shows GCC 6.2.0\r\n\r\nThis shouldn't have any effect. Basically, 6.2.0 is up-to-date GCC on Ubuntu 16.10 and 4.4.7 is what Anaconda comes with.", "@omtcyfz  @prb12 \r\n\r\nUnder `..../anaconda3/envs/tensorflow/lib/python3.5/site-packages`, I found 3 related folders\r\n```\r\n\r\n/tensorflow\r\n/tensorflow-0.11.0.dist-info\r\n/tensorflow-0.11.0rc2.dist-info\r\n```\r\n\r\nAnd under `anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/include/tensorflow/core/public`, there is a file `version.h` which contains in the beginning\r\n\r\n```\r\n// TensorFlow uses semantic versioning, see http://semver.org/.\r\n\r\n#define TF_MAJOR_VERSION 0\r\n#define TF_MINOR_VERSION 11\r\n#define TF_PATCH_VERSION head\r\n\r\n// TF_VERSION_SUFFIX is non-empty for pre-releases (e.g. \"-alpha\", \"-alpha.1\",\r\n// \"-beta\", \"-rc\", \"-rc.1\")\r\n#define TF_VERSION_SUFFIX \"\"\r\n```\r\n\r\nIt should be up to date, but `tf.__version__` still shows RC0\r\n\r\nUnder `conda list`, it contains \r\n\r\n```\r\ntensorflow                0.11.0                    <pip>\r\ntensorflow                0.11.0rc2                 <pip>\r\n```\r\n\r\nSo it is pretty confused for me, RC0 is not shown, but it is returned from the command above. And `tf.summary.merge_all` still not available, even though `.../site-packages/tensorflow/python/summary/summary.py` contains `merge_all` method\r\n", "@prb12 @omtcyfz \r\n\r\nUPDATE: \r\nI found there is directory `~/.local/lib/python3.5/site-packages/tensorflow-0.11.0rc0.dist-info`. It seems the import will use this folder for tensorflow. Can I just delete this folder ? I am not sure if it is base file or shouldn't be deleted (if `.local` a system folder ?)", "FINAL UPDATE:\r\n\r\n   Problem resolved. When using Anaconda virtual environment, I need to `pip3 uninstall tensorflow` under vanilla terminal environment, and then `import tensorflow` will search the correct place within the virtual environment. The reason might be because I was installing tensorflow once in vanilla environment. So it is given priority to import. \r\n\r\nFeel free to close this issue if nobody else facing similar problem. ", "@zuoxingdong Just to check - did you use the `--system-site-packages` flag for `virtualenv`?  \r\n\r\n**EDIT:** Sorry - ignore that, I reread the thread and you're using Anaconda.", "OK - So I'm guessing that the Anaconda method picks up system site packages, but `virtualenv --system-site-packages` doesn't.  \r\n \r\nI'm going to close this issue now.", "@prb12 Thanks for the replies. I guess maybe if it is helpful to add a simple statement in the **Installation tutorial -> Anaconda Installation** as a note that if anyone has installed tensorflow previously on vanilla terminal, one should `pip(3) uninstall tensorflow`  for Anaconda to search the correct path to import tensorflow within the environment ?"]}, {"number": 5831, "title": "Is there a backpropagation-method for all math-layers i.e. tf.fft2d?", "body": "When building a deep NN, where one of the layer is represented by a fouriertransform (i.e. tf.fft), does tensorflow provide a backpropagation method for it? In this case it could/would be complex I guess. \r\nHow does the optimizer work on that? Are there any documents which could explain this behavior? \r\n\r\nIn theory, I understand how the gradient of a multiplication is carried out, but doing this for a more complicated function makes things more difficult for me. \r\n\r\nThank you very much :) ", "comments": ["From https://www.tensorflow.org/versions/r0.11/resources/index.html\r\n> For help and support, technical or algorithmic questions, please submit your questions to Stack Overflow: https://stackoverflow.com/questions/tagged/tensorflow. You may also find answers in our FAQ, our glossary, or in the shapes, sizes and types guide. Please do not use the mailing list or issue tracker for support.\r\n\r\n"]}, {"number": 5830, "title": "RNN support in tensorflow slim", "body": "Is there any plan to build RNN support into tf.slim layers? Or is there a road map for tf.slim development that you could please share?", "comments": ["See also https://github.com/tensorflow/models/issues/70", "There will be RNN layers in tf.layers. They will likely look like Keras' layers, and similar to the RNNCell objects in contrib/ now.\r\n\r\nYou may also be interested in DynamicRNNEstimator in contrib/learn.\r\n\r\nI guess the answer is that we're working on it. I will close this -- if you have a specific feature request you can open another issue. "]}, {"number": 5829, "title": "sampled_softmax souldn't be linear with respect to vocabulary size, but actually is", "body": "Hi,\r\n\r\n[tf.nn.sampled_softmax_loss API doc page](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard1/tf.nn.sampled_softmax_loss.md) tells us to get into Section 3 of [Jean et al., 2014](https://arxiv.org/abs/1412.2007) ([pdf](https://arxiv.org/pdf/1412.2007v2.pdf)) for more information about it.\r\n\r\nAnd actually, they quite start sec. 3.1saying:\r\n> \"With  the  proposed  approach,  the  computational complexity of training becomes constant with respect to the size of the target vocabulary\"\r\n\r\nWhich, kept my attention.   \r\nI ran a benchmark using my [custom RNN LM script](https://github.com/pltrdy/tf_rnnlm) (derived from [tensorflow 0.11 ptb_word_lm.py](https://github.com/tensorflow/tensorflow/blob/282823b877f173e6a33bbc9d4b9ad7dd8413ada6/tensorflow/models/rnn/ptb/ptb_word_lm.py).    \r\n\r\n[Results shows](https://github.com/pltrdy/tf_rnnlm/blob/master/benchmark.md) that, only changing vocab_size increases computation time linearly. (the benchmark is using ['SmallConfig'](https://github.com/tensorflow/tensorflow/blob/282823b877f173e6a33bbc9d4b9ad7dd8413ada6/tensorflow/models/rnn/ptb/ptb_word_lm.py#L189) with `vocab_size` of either 10k or 150k.\r\n\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04.\r\n\r\nInstalled version of CUDA and cuDNN: \r\n```\r\n$ ls -l /usr/local/cuda-8.0/lib64/libcud*\r\n-rw-r--r-- 1 root root 546K oct.  21 10:20 /usr/local/cuda-8.0/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root   16 oct.  21 10:20 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0*\r\nlrwxrwxrwx 1 root root   19 oct.  21 10:20 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44*\r\n-rwxr-xr-x 1 root root 406K oct.  21 10:20 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.44*\r\n-rw-r--r-- 1 root root 757K oct.  21 10:20 /usr/local/cuda-8.0/lib64/libcudart_static.a\r\n```\r\n\r\n\r\n#### TensorFlow Version:\r\n0.11RC2\r\n\r\n\r\n\r\nThx for your reading & feebacks\r\n`pltrdy`\r\n", "comments": []}, {"number": 5828, "title": "tf.train.range_input_producer computes \"limit\" argument more than once", "body": "Hello, I have an issue with unexpected behavior of range_input_producer. It recomputes its \"limit\" argument more times than needed.\r\n\r\n`\r\nimport tensorflow as tf\r\n\r\nlimit = tf.Print(tf.constant(5, dtype=tf.int32), [0], \"compute limit\")\r\nrange = tf.train.range_input_producer(limit, shuffle=False, num_epochs=1)\r\nidx = range.dequeue()\r\n\r\nwith tf.Session() as ss:\r\n    ss.run(tf.initialize_all_variables())\r\n    ss.run(tf.initialize_local_variables())\r\n    coord = tf.train.Coordinator()\r\n    threads = tf.train.start_queue_runners(sess=ss, coord=coord)\r\n    while True:\r\n        print(ss.run([idx]))\r\n`\r\n\r\nWhen num_epoch=1, the output will have \"compute limit\" twice. I expect it to appear only once.\r\nWhen num_epoch=None, the output will have infinitely many \"compute limit\" messages. But I am expecting, that it should read limit only once and then cycle through generated range infinitely.\r\n\r\nThings get important when \"limit\" is coming from previous queue, which after getting empty produces OutOfRange and for some reasons it is not handled by range_input_producer.\r\n\r\n\r\n\r\n### Environment info\r\nOperating System: Fedora 24\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n/usr/local/cuda/lib64/libcudadevrt.a       /usr/local/cuda/lib64/libcudnn.so\r\n/usr/local/cuda/lib64/libcudart.so         /usr/local/cuda/lib64/libcudnn.so.5\r\n/usr/local/cuda/lib64/libcudart.so.8.0     /usr/local/cuda/lib64/libcudnn.so.5.1.5\r\n/usr/local/cuda/lib64/libcudart.so.8.0.44  /usr/local/cuda/lib64/libcudnn_static.a\r\n/usr/local/cuda/lib64/libcudart_static.a\r\n\r\nIf installed from binary pip package, provide:\r\n1. A link to the pip package you installed:\r\n    https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp35-cp35m-linux_x86_64.whl\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n    0.11.0", "comments": ["Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 5827, "title": "Should variables in ExponentialMovingAverage use \"get_variable\"", "body": "zero_debias for moving average was introduced recently on master branches.  (tensorflow/tensorflow/python/training/moving_averages.py)\r\nVariables in func \u201c_zero_debias\u201d is created by \"variable_scope.get_variable\", while slot_creator creates moving variable with \"variables.Variable\". This causes some problem when outer scope set \u201creuse=True\u201d, if we only want to reuse variables of the network but not the moving averages.\r\nAt least, the new feature introduces inconsistency on how to maintain moving variables.", "comments": ["An example I put in #2740:\r\n```python\r\ndef f(v):\r\n    ema = tf.train.ExponentialMovingAverage(0.9)\r\n    vema = ema.apply([v])\r\n    return vema\r\n\r\nwith tf.variable_scope('s'):\r\n    v1 = tf.get_variable('W', shape=[])\r\n    v1 = v1 + 1\r\n    f(v1)\r\nwith tf.variable_scope('s', reuse=True):\r\n    v2 = tf.get_variable('W', shape=[])\r\n    v2 = v2 + 2\r\n    f(v2)\r\n```\r\n```\r\nValueError: Variable s/s_1/s_1/add/ExponentialMovingAverage/biased does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n```\r\n\r\nAlso, tf.contrib.layers.batch_norm doesn't use debias, thus doesn't have this problem. But should it use it to have a more accurate moving average?", "1) The issue should be fixed currently; zero_debias is no longer the default in ExponentialMovingAverage.\r\n2) There is a strong possibility that is *will* be the default in the future.\r\n3) ExponentialMovingAverage creates variables, so for future safety you might consider using it as if it respected variable scopes (ie don't put it in resuse=True variable scopes that it doesn't need to be in)", "To the 3rd point, EMA always creates variables -- the average itself, why should we make a compromise to the bias variable because it causes problem.\r\n\r\nIt seems like these variables are created differently: the average is with `create_slot`, and the bias/local_step with `get_variable` which causes the problem. Should we make changes so that it doesn't cause problems anymore, instead of not using it?", "Either I'm not understanding your objection, or you've missed my point: if you accept that ExponentialMovingAverage.apply creates variables, and you believe that variables should respect variable scopes, then your example *should* throw an error.\r\n\r\nThe reason EMAs haven't been broken in this case prior to this because they used the legacy `slot` system, which ignores variable scopes (incorrectly). We might be moving away from this, or incorporating variable scopes into slots (which would also break the code above).\r\n\r\nDoes that clarify my comment?", "@joel-shor Thanks, now I understand. I didn't know that the slot system is a legacy.\r\nThen the problem would become how to nicely avoid using `reuse=True`. Because currently the paradigm which causes problems is like:\r\n```python\r\ndef inference(input):\r\n   # some operations including BatchNorm(with EMA)\r\n   pass\r\nwith tf.variable_scope('scope'):\r\n    inference(x1)\r\nwith tf.variable_scope('scope', reuse=True):\r\n    inference(x2)\r\n``` \r\nIf we're not supposed to use EMA inside `reuse=True` any more, for `inference(x2)` we'll have to surround a `reuse=True` scope for every operation inside `inference` function except for EMA, right? This could be a lot of work.\r\nOne way to work around might be to force `reuse=False` even inside a `reuse=True` scope, which I currently did by\r\n```python\r\nwith tf.variable_scope(tf.get_variable_scope(), reuse=False)\r\n```\r\nBut I suppose this is not an expected feature? ", "ExponentialMovingAverage could be applied either on original variables or on output tensors. It's different from user-defined variables.\r\nIf the slot_creator will be replaced in design, there should be a direction for how to maintain averages on mid-output first. (as ppwwyyxx mentioned case)", "Hi!\r\nI'm also having this issue when reusing batch normalization layer, and in particular ema:\r\n```\r\nValueError: Variable deep_net/blabla/moments_6/moments_1/mean/ExponentialMovingAverage/biased does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n```\r\n\r\nMy objective is to duplicate some variables of a model, to insert them in another model, and then to load them from a check point. Everything works fine except the ema. When I use the trick explained by @ppwwyyxx , by imposing not to reuse variables, I have an issue when I load a model from a checkpoint:\r\n\r\n```\r\nNotFoundError (see above for traceback): Key deep_net/blabla/moments_6/moments_1/mean/ExponentialMovingAverage/biased not found in checkpoint\r\n\t [[Node: save_1/RestoreV2_57 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_1/Const_0, save_1/RestoreV2_57/tensor_names, save_1/RestoreV2_57/shape_and_slices)]]\r\n\t [[Node: save_1/RestoreV2_32/_137 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_349_save_1/RestoreV2_32\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n```\r\n\r\nHere is the code I use for the BN:\r\n\r\n```python\r\nwith tf.name_scope(None):\r\n  #with tf.variable_scope():# FIX?\r\n    beta = tf.constant(0.0, shape=[n_out])\r\n    gamma = tf.constant(1.0, shape=[n_out])\r\n    batch_mean, batch_var = tf.nn.moments(x, [0,1,2], name='moments')\r\n    ema=[]\r\n    with tf.variable_scope(tf.get_variable_scope(), reuse=False):\r\n      ema = tf.train.ExponentialMovingAverage(decay=0.99)\r\n\r\n    def mean_var_with_update():\r\n      ema_apply_op = ema.apply([batch_mean, batch_var])\r\n      with tf.control_dependencies([ema_apply_op]):\r\n        return tf.identity(batch_mean), tf.identity(batch_var)\r\n    mean, var = tf.cond(phase_train,\r\n      mean_var_with_update,\r\n      lambda: (ema.average(batch_mean), ema.average(batch_var)))\r\n\r\n    normed = tf.nn.batch_norm_with_global_normalization(x, mean, var, \r\n      beta, gamma, 1e-3, affine)\r\n    return normed\r\n```\r\nI guess I'm doing a very newbie error and I would really appreciate any help to fix this!\r\n\r\nThank you very much in advance.", "how to solve the problem?\r\nValueError: Variable deep_net/blabla/moments_6/moments_1/mean/ExponentialMovingAverage/biased does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n", "I think this problem should be fixed now. Specifically, in `ExponentialMovingAverage`, `zero_debias=False` by default. If you'd like to experience improved accuracy at the cost of breaking backwards compatibility with old checkpoints, set `zero_bias=True`.", "I get the following error when I use `tf.train.ExponentialMovingAverage` to update `batch_mean` and `batch_var` in batch normalization. This error pops up specifically when I try to restore a checkpoint.\r\n\r\n```bash\r\nNotFoundError (see above for traceback): Key D4/cond/D4/moments/moments_1/mean/ExponentialMovingAverage not found in checkpoint \r\n[[Node: save/RestoreV2_36 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_36/tensor_names, save/RestoreV2_36/shape_and_slices)]] \r\n[[Node: save/RestoreV2_117/_105 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_561_save/RestoreV2_117\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n```\r\n\r\nDo I have to update TensorFlow to the latest version to solve this problem?", "Also, I get this error when I try to train the network in a multi-GPU setting where one would ideally have to reuse variables. How do you solve the issue as specified by @hzchenlixin in https://github.com/tensorflow/tensorflow/issues/5827#issuecomment-280538373?", "Are you synced to head? If so, please copy the code that doesn't work for you.\r\n\r\nAlso, not that you might experience issues if you are trying to reload checkpoints from a model that was trained with the debias into a model that is not set up to use them.", "I have seen the same issue when I try to train a new model with ExponentialMovingAverage. I am using tensorflow 1.1.0. ", "I'm running into this problem as well while trying to use batch norm on a model across two GPUs.  My use case is basically identical to @ppwwyyxx.  Does anyone have a suggested workaround?\r\n", "Oh my solution was just to use `moving_averages.assign_moving_average` without debias factor. But I'd like to see a way with debias as well.", "On tensorflow 1.12.0, I had the same problem and fixed it by adding the line: \r\n\r\n            with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\r\n\r\nbefore ema.apply", "Hello all,\r\n\r\njust follow the below video and export your own model with in a 10 seconds\r\n\r\nhttps://youtu.be/w0Ebsbz7HYA"]}, {"number": 5826, "title": "[Windows/CMake] Some more C++ test fixes", "body": "A new batch of fixes to be able to build some C++ tests on Windows.", "comments": ["Can one of the admins verify this patch?", "@vit-stepanovs, thanks for your PR! By analyzing the history of the files in this pull request, we identified @concretevitamin, @guschmue and @vrv to be potential reviewers.", "@tensorflow-jenkins test this please.", "MacOS tests fixed. We should also check if GPU test is a flake.\r\nJenkins, test this please."]}, {"number": 5825, "title": "fix typo in `tf.scatter_update` doc", "body": "", "comments": ["@RustingSword, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener to be a potential reviewer.", "Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed the CLA.", "@RustingSword, the file you changed in this PR is an auto-generated doc file. Please edit this file instead: tensorflow/core/ops/state_ops.cc"]}, {"number": 5824, "title": "Be explicit about setting up CUDA", "body": "Now that OPENCL support is initiated I think makes sense to be explicit about the configuration option.", "comments": ["@Mistobaan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @lukeiwanski and @meteorcloudy to be potential reviewers.", "Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 5823, "title": "[FEATURE REQUEST] expand decode_csv to work with whole csv files.", "body": "I'm a bit new to tensorflow, so forgive me if this exists and I just missed it, but it seems decode_csv treats csv files exclusively as a set of records, returning a 1D tensor. This makes decoding 2D samples unintuitive (I've yet to figure out how to do so without simply telling numpy to get them and throw them into a placeholder). How much work would it be to either expand the current function or add a new one that works in a similar manner to decode_png?", "comments": ["`decode_csv` returns what I think you mean by 2-D data as a list of 1-D tensors (two dimensions in total).  I think it should be sufficient, but in any case we can't change it to return a 2-D tensor since (1) it wouldn't handle variable types and (2) it wouldn't be backwards compatible."]}, {"number": 5822, "title": "Branch 140088698", "body": "Manually resolved conflicts in:\r\ntensorflow/contrib/learn/python/learn/datasets/base.py\r\ntensorflow/contrib/training/BUILD\r\ntensorflow/core/kernels/cwise_op_floor_div.cc\r\ntensorflow/python/platform/test.py", "comments": ["@tensorflow-jenkins test this please", "The issue related to the extraneous file in third_party/llvm is now resolved. However, the windows cmake build shows another error:\r\n```\r\n18:58:53     import tensorflow as tf\r\n18:58:53   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n18:58:53     from tensorflow.python import *\r\n18:58:53   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 85, in <module>\r\n18:58:53     from tensorflow.python import layers\r\n18:58:53 ImportError: cannot import name 'layers'\r\n```\r\n\r\nAny thoughts? ", "@tensorflow-jenkins test this please.", "@aselle, @lukeiwanski, there were some conflicts in tensorflow/core/kernels/cwise_op_floor_div.cc that I resolved as a part of this push.\r\n\r\nIs it possible that CL/139922734 seems somehow interacted with PR https://github.com/tensorflow/tensorflow/commit/6a0263970dd05edfa080931cc0d2c202dfba1976 to cause the test failures seen at: \r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-gpu/2740/consoleFull\r\n\r\nExample error message:\r\n```\r\nInvalidArgumentError (see above for traceback): Multiple OpKernel registrations match NodeDef 'FloorDiv = FloorDiv[T=DT_INT64, _device=\"/device:CPU:0\"](FloorDiv/x, FloorDiv/y)': 'op: \"FloorDiv\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT64 } } }' and 'op: \"FloorDiv\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT64 } } }'\r\n\t [[Node: FloorDiv = FloorDiv[T=DT_INT64, _device=\"/device:CPU:0\"](FloorDiv/x, FloorDiv/y)]]\r\n```", "@caisq It looks like this PR adds one or more new Python modules. The file [tf_python.cmake](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/tf_python.cmake) has a sequence of `add_python_module(\u2026)` commands, one per module, and I guess we'll need a new one:\r\n\r\n```\r\nadd_python_module(\"tensorflow/python/layers\")\r\n```\r\n\r\n\u2026and maybe others if there are submodules within that directory.", "@caisq Upstream file ( https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cwise_op_div.cc#L27 ) looks okay.. Seems like this ( https://github.com/caisq/tensorflow/commit/72e85c594000667dd1b3718da0f2bf7e2d003d74#diff-b6091d29f9b8efa4283ca6937ee6ceaaR35 ) merge went bad.", "mac tests are flaking. but all the others seem to be legitimate failures.", "As @lukeiwanski says, it looks like there are now two GPU entries for the integer version of floordiv. Delete the first one and it should help.", "Windows failure is legit:\r\n00:42:28.542         Start 170: C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/tensorboard/backend/handler_test.py\r\n00:42:31.948 170/172 Test #170: C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/tensorboard/backend/handler_test.py .............................***Failed    3.41 sec\r\n00:42:31.948 Traceback (most recent call last):\r\n00:42:31.948   File \"C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/tensorboard/backend/handler_test.py\", line 28, in <module>\r\n00:42:31.948     from tensorflow.tensorboard.backend import handler\r\n00:42:31.948   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\tensorboard\\backend\\handler.py\", line 43, in <module>\r\n00:42:31.948     from tensorflow.tensorboard.plugins import REGISTERED_PLUGINS\r\n00:42:31.948   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\tensorboard\\plugins\\__init__.py\", line 20, in <module>\r\n00:42:31.948     from tensorflow.tensorboard.plugins.projector.plugin import ProjectorPlugin\r\n00:42:31.948   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\tensorboard\\plugins\\projector\\plugin.py\", line 27, in <module>\r\n00:42:31.948     from tensorflow.contrib.tensorboard.plugins.projector import PROJECTOR_FILENAME\r\n00:42:31.948   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\__init__.py\", line 36, in <module>\r\n00:42:31.948     from tensorflow.contrib import linalg\r\n00:42:31.948 ImportError: cannot import name 'linalg'\r\n\r\ntf.contrib.linalg seems to be a new package, and I bet they forgot to add it to the cmake files.", ":|\r\n```\r\nImportError: No module named 'tensorflow.contrib.linalg.python.ops'\r\n```\r\n", "All right. I pushed the following updates to fix the test failures\r\n1) Removed the duplicate GPU kernel registration lines in cwise_op_floor_div.cc\r\n2) Added new modules tensorflow/python/layers and tensorflow/contrib/linalg to tf_python.cmake\r\n3) Disabled complex data types (complex64 and complex128) in cwise_ops_test.py _compareBCast() for now.\r\n\r\n@aselle, I made a TODO item for you in cwise_ops_test.py to restore those data types once the question around the \"+ 0.1\" is resolved.\r\n\r\nI'm merging this PR now.\r\n", "I fixed the cwise op test on mac by removing +0.1\nLooks like it used to not be there, and was added during a\nmosunderstanding, it looks like.\n\nOn Nov 24, 2016 6:37 PM, \"Shanqing Cai\" <notifications@github.com> wrote:\n\n> Merged #5822 <https://github.com/tensorflow/tensorflow/pull/5822>.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/5822#event-871598396>, or mute\n> the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOUD3tv2lFwYz7D38roQkJQZZ6jV6ks5rBkoBgaJpZM4K7OFS>\n> .\n>\n", "@caisq  T want to use cmake build the libtensorflow-core.a which enable gpu for c++ predict program\u3002 But i saw the cmakelist can only work in windows that enable the gpu . What can i do?", "@shenyuhit Maybe you can take a look at the makefile build that we have? See: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile", "@caisq  \r\nI have tried to use makefile  and successfully acquired the  libtensorflow-core.a ,but it only supports  cpu, does not  support gpu!  In CPU to predict the time efficiency is not high enough ,so i want to predict in GPU!\r\n", "@mrry: see the question from @shenyuhit. Is there a way to make the GPU cmake build work on noo-windows platforms?", "@caisq There's some discussion of using CMake to build the GPU version on Ubuntu in #5465. However, it's not a configuration that we're planning to support, but we would accept contributions to make it work."]}, {"number": 5821, "title": "Make windows gpu build project name tensorflow_gpu. (#5794)", "body": "* Make windows gpu build project name tensorflow_gpu.\r\n(cherry picked from commit f8037317ba5e772f59086e3ba098d07cf83363cf)", "comments": ["@yifeif, thanks for your PR! By analyzing the history of the files in this pull request, we identified @andrewharp, @jhseu and @mrry to be potential reviewers."]}, {"number": 5820, "title": "[Windows/CMake] Enable DepthwiseConv2DNative.", "body": "Fixes #5818. Also enables saver_test, which was previously disabled.", "comments": ["@mrry, thanks for your PR! By analyzing the history of the files in this pull request, we identified @guschmue, @tensorflower-gardener and @zheng-xq to be potential reviewers.", "Jenkins is just not happy today.\r\nJenkins, test this please.", "I had this one on my list for a pr but you bet me to it. \r\nThis was broken in the past but I think a change in eigenpool.h fixed this one as well.", "@guschmue That's good to know! Was pleased by how easy the \"fix\" was... :).", "Gah, I clicked \"Update branch\" and that blew away the clean tests :(.\r\n\r\n@tensorflow-jenkins test this please.", "Mac failure looks like an unrelated flake!", "Looks great! merging!\r\n\r\nDo we want to merge this into the release?\r\nI can still accept small fixes into the release, but the next hour is the last chance.", "oh, this is already the release branch :)", "I'm sneaky like that. Happy Thanksgiving!"]}, {"number": 5819, "title": "[CMake] Build Python protos after core protos", "body": "Ensures that the generated code for the core protos has been generated before compiling the generated code for the Python-specific protos.", "comments": ["@gunan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @mrry and @danmane to be potential reviewers.", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->"]}, {"number": 5818, "title": "[Windows] DepthwiseConv2DNative not available", "body": "The DepthwiseConv2DNative op (and some related ops for backprop) are currently excluded from the Windows build ([CMake reference](https://github.com/tensorflow/tensorflow/blob/e297257e458654c7743c59c7f37154b7f6118c16/tensorflow/contrib/cmake/tf_core_kernels.cmake#L87), [Bazel reference](https://github.com/tensorflow/tensorflow/blob/e297257e458654c7743c59c7f37154b7f6118c16/tensorflow/core/kernels/BUILD#L1719)). This is causing issues for models that use these ops (e.g. https://github.com/fchollet/keras/issues/4478).\r\n\r\nWe should figure out why it doesn't build and fix it.", "comments": []}, {"number": 5817, "title": "Passing exit code of the test back to bazel", "body": "Script used for parallel GPU execution does not pass the exit code of the test to bazel (always return with exit 0), so failures are shown as PASS in the summary.", "comments": ["@ptrendx, thanks for your PR! By analyzing the history of the files in this pull request, we identified @gunan to be a potential reviewer.", "Can one of the admins verify this patch?", "Jenkins, test this please.", "Interesting catch. It seems like our CI was able to report failures fine without this though.\r\nYour analysis makes sense, but I wonder how our CI reported failures.", "This is strange indeed - even running false gives exit code 0 when running under old version of the script.", "The GPU tests ended up on a faulty machine, rerunning:\r\nJenkins, test this please."]}, {"number": 5816, "title": "MaxPool3DGrad - Out of Memory Issue", "body": "I am training a fairly big network with many 3D convolutions that almost fills up all the GPU memory (Titan X). When settings the batch size to a small amount, e.g. 32 examples the training process crashes after a number of steps with an out of memory issue caused by `MaxPool3DGrad`. I lowered the batch size to 20, which makes the training run fine for >2000 training steps but then at some point the model crashes again with the same error. It seems like some operations are not freeing memory, maybe the `MaxPool3DGrad` kernel? Full error message is given below. \r\n\r\n**Configuration**: Linux Mint, checkout of the TensorFlow master 5 days ago (`dfc5cd48a095b133ece9caff663e3cc512e8a268`) with CUDA 8.0 and CuDNN 5.1.  \r\n\r\nThis might be relevant: https://github.com/tensorflow/tensorflow/issues/3696\r\n\r\n```\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 3 Chunks of size 1722368000 totalling 4.81GiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 2674491392 totalling 2.49GiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 3444734720 totalling 3.21GiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use chunks: 11.21GiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats:\r\nLimit:                 12051264308\r\nInUse:                 12032303616\r\nMaxInUse:              12032303616\r\nNumAllocs:                 2860074\r\nMaxAllocSize:           3444734976\r\n\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:274] ******************************xxxxxxxxxxxxx**************************************************xxxxxxx\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:275] Ran out of memory trying to allocate 205.32MiB.  See logs for memory state.\r\nW tensorflow/core/framework/op_kernel.cc:975] Resource exhausted: OOM when allocating tensor with shape[20,64,50,29,29]\r\nTraceback (most recent call last):\r\n  File \"train_c3d.py\", line 324, in <module>\r\n    is_training: True\r\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[20,64,50,29,29]\r\n         [[Node: gradients/3D_CNN/conv-relu-0/MaxPool3D_grad/MaxPool3DGrad = MaxPool3DGrad[T=DT_FLOAT, ksize=[1, 1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 2, 1], _device=\"/job:localhost/repl\r\nica:0/task:0/gpu:0\"](3D_CNN/conv-relu-0/Relu, 3D_CNN/conv-relu-0/MaxPool3D, gradients/3D_CNN/conv-relu-1/conv-1_grad/tuple/control_dependency)]]\r\n\r\nCaused by op u'gradients/3D_CNN/conv-relu-0/MaxPool3D_grad/MaxPool3DGrad', defined at:\r\n  File \"train_c3d.py\", line 145, in <module>\r\n    grads_and_vars = optimizer.compute_gradients(total_loss)\r\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 335, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py\", line 482, in gradients\r\n    in_grads = grad_fn(op, *out_grads)\r\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_grad.py\", line 130, in _MaxPool3DGrad\r\n    padding=op.get_attr(\"padding\"))\r\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1657, in max_pool3d_grad\r\n    name=name)\r\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2259, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1130, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\n...which was originally created as op u'3D_CNN/conv-relu-0/MaxPool3D', defined at:\r\n  File \"train_c3d.py\", line 128, in <module>\r\n```", "comments": ["These errors looks completely random, using the same C3D model (and dataset) training crashes after ~2000 in my first run and the next one crashes after ~5500 training steps. I've tried this on multiple machines (all with Titan X) and same happens everywhere. No other processes are using the GPU.\r\n\r\nEverythime the error message is similar and contains the `MaxPool3DGrad` operation.", "Hey @tomrunia \r\nI'm also having exactly the similar problem as you described...I also work on C3D. I was wondering any update on this?\r\nThank you very much. ", "It's possible to have non-deterministic \"out of memory\" because you are unlucky. TensorFlow has non-deterministic order of execution, so depending on timing, you may have things scheduling in different order and needing different amounts of memory.\r\n\r\nTo verify this is indeed a bug, you could use memory profiling as detailed in https://github.com/yaroslavvb/notebooks/blob/master/mnist-memory.ipynb\r\n\r\nThat notebook shows how to examine timeline with Tensor allocation + de-allocation events and confirm that tensors are deallocated quickly after they are no longer needed.\r\n\r\nFor an example of unlucky scheduling, consider computational graph below\r\n![snake-graph](https://cloud.githubusercontent.com/assets/23068/21414260/a605697e-c7b2-11e6-87f9-e7b1682ba705.png)\r\n\r\nTensorFlow schedules ops asynchronously as soon as they are ready, so if \"circle\" execute faster than any \"square\" ops then TF could schedule them first and hence allocate memory for 5 circle ops.\r\n\r\n![snake2](https://cloud.githubusercontent.com/assets/23068/21414406/fe679a46-c7b3-11e6-9e7b-3d38403f5ea3.png)\r\n\r\nBut if circle ops take longer to execute, you may end up going in sequence left to right, and hence requiring less memory\r\n\r\n![snake3](https://cloud.githubusercontent.com/assets/23068/21414408/06033508-c7b4-11e6-939b-f0607d8e2a3a.png)\r\n\r\n\r\nYou could use control dependencies to force a particular execution order. IE, with `tf.control_dependencies` or if your graph has already been constructed such as when you use `tf.gradients`, you could use `graph_editor`. IE, something like this\r\n\r\n```\r\nimport tensorflow.contrib.graph_editor as ge\r\ndef run_after(a_tensor, b_tensor):\r\n    \"\"\"Force a to run after b\"\"\"\r\n    ge.reroute.add_control_inputs(a_tensor.op, [b_tensor.op])\r\n\r\n ge.reroute.add_control_inputs(a_tensor.op, [b_tensor.op])\r\n\r\n\r\n\r\n```", "@yaroslavvb can you please have a look at this?\r\nhttps://github.com/endernewton/tf-faster-rcnn/issues/7", "It looks pretty good!", "@yaroslavvb I didn't know if Jupyter notebook is open I'll get this error, even though GPU wasn't being used there (was used a day ago for a training using TF).", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 5815, "title": "Update pip file name for gpu builds.", "body": "", "comments": ["@yifeif, thanks for your PR! By analyzing the history of the files in this pull request, we identified @gunan, @mrry and @keveman to be potential reviewers.", "How about the links to nightly builds?"]}]