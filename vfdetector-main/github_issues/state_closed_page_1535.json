[{"number": 6875, "title": "Tensorflow require Protobuf 3.1 while attempting to use incompatible 3.0 function calls", "body": "**Environment:**\r\nArch Linux with CUDA 8.0 and cuDNN 5.1\r\n\r\n```\r\n/opt/cuda/lib64/libcudadevrt.a\r\n/opt/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\n/opt/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n/opt/cuda/lib64/libcudart.so.8.0.44\r\n/opt/cuda/lib64/libcudart_static.a\r\n/opt/cuda/lib64/libcudnn.so -> libcudnn.so.5\r\n/opt/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5\r\n/opt/cuda/lib64/libcudnn.so.5.0.5\r\n/opt/cuda/lib64/libcudnn.so.5.1.5\r\n/opt/cuda/lib64/libcudnn_static.a\r\n```\r\n\r\nTensorflow was succesfully compiled from source for a C++ pipeline using\r\n**Bazel** 0.4.3\r\n**Protobuf** 3.1.x (also tried 3.2.x)\r\n**Tensorflow** r1.0 (as well as master <9830ed87d46245a72a9d0c2dce854e6732c5542a>)\r\n\r\nwith the command: \r\n`bazel build -c opt --config=cuda --copt=-march=native --verbose_failures //tensorflow/core:tensorflow //tensorflow/cc:cc_ops tensorflow:libtensorflow_cc.so`\r\n\r\nIn _my_ CPP pipeline, the following include breaks the compile process\r\n`#include <tensorflow/core/public/session.h>`\r\n\r\n```\r\n/srv/builds/tensorflow/bazel-genfiles/tensorflow/core/protobuf/config.pb.h:1490:95: error: no matching function for call to \u2018google::protobuf::internal::ArenaStringPtr::Get(const string*) const\u2019\r\n return visible_device_list_.Get(&::google::protobuf::internal::GetEmptyStringAlreadyInited());\r\n                                                                                             ^\r\nIn file included from /srv/builds/tensorflow/bazel-genfiles/tensorflow/core/framework/graph.pb.h:23:0,\r\n                 from /srv/builds/tensorflow/tensorflow/core/public/session.h:22,\r\n                 from src/fragments/tensorflow.h:48,\r\n                 from src/blocks/detector.h:27,\r\n                 from src/json_deserializer.h:22,\r\n                 from src/blocks/assign_camera.cpp:2:\r\n/usr/include/google/protobuf/arenastring.h:66:31: note: candidate: const string& google::protobuf::internal::ArenaStringPtr::Get() const\r\n   inline const ::std::string& Get() const { return *ptr_; }\r\n                               ^~~\r\n/usr/include/google/protobuf/arenastring.h:66:31: note:   candidate expects 0 arguments, 1 provided\r\n```\r\n\r\n----------------------------------------------\r\n\r\nDigging in the documentation, I see for the python part that Protobuf 3.1.x is required, however 3.1.x had an API change from 3.0.X for the ArenaStringPtr::Get(), where the former takes a string as argument while the newer doesn't. I could downgrade to Protobuf 3.0.X, but macro's in e.g. error_codes.pb.h specifies a requirement for 3.1.X `#if 3001000 < GOOGLE_PROTOBUF_MIN_PROTOC_VERSION`", "comments": ["Just now, I had success compiling Tensorflow r0.12 (and presumably also r1.0, although it remain untested). \r\n\r\nFor others that might have these issues, the problem is indeed related to protobuf versioning, and inspecting the bazel-genfiles external parts, I figured that tensorflow is pulling an unnamed branch of Protobuf that doesn't have the problems described above. Installing 3.0.x, 3.1.x or even 3.2.x will not work, only the one in `~/.cache/bazel/_bazel_allan/83...59/external/protobuf` will work. Compiling from this folder did the trick. ", "Thank you for putting up this response to your own issue, you may have just saved me several hours of head scratching."]}, {"number": 6874, "title": "Support relative path head with no `./`.", "body": "`tf.gfile.Glob` will fail if the argument is a relative path with no `./` in the beginning in current version.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Why do we need this? This won't work on windows.", "I have posted an issue [#6893](https://github.com/tensorflow/tensorflow/issues/6893). I closed this pull request."]}, {"number": 6873, "title": "dynamic_rnn issue:", "body": "when I run a copy of code based on tensorflow, it gives following error, what should I do next ?\r\n\r\nCaused by op u'RNN/Assert/AssertGuard/Assert', defined at:\r\n  File \"lstm_and_ctc_ocr_train.py\", line 188, in <module>\r\n    train()\r\n  File \"lstm_and_ctc_ocr_train.py\", line 78, in train\r\n    logits, inputs, targets, seq_len, W, b = model.get_train_model()\r\n  File \"/home/sanjie/projects/tensorflow_lstm_ctc_ocr_ywh/model.py\", line 111, in get_train_model\r\n    outputs, _ = tf.nn.dynamic_rnn(cell, inputs, seq_len, dtype=tf.float32)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 823, in dynamic_rnn\r\n    [_assert_has_shape(sequence_length, [batch_size])]):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 818, in _assert_has_shape\r\n    packed_shape, \" but saw shape: \", x_shape])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 140, in Assert\r\n    condition, no_op, true_assert, name=\"AssertGuard\")\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1717, in cond\r\n    _, res_f = context_f.BuildCondBranch(fn2)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1613, in BuildCondBranch\r\n    r = fn()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 137, in true_assert\r\n    condition, data, summarize, name=\"Assert\")\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_logging_ops.py\", line 37, in _assert\r\n    summarize=summarize, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 756, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): assertion failed: [Expected shape for Tensor sequence_length:0 is ] [64] [ but saw shape: ] [85]\r\n", "comments": ["Thanks for reaching out. We try to keep this issue tracker limited to bugs and feature requests. We recommend posting on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) for community-driven support."]}, {"number": 6872, "title": "Added possible responses for uname on Windows, fixes #6871", "body": "According to https://en.wikipedia.org/wiki/Uname#Examples, other responses\r\nare possible on Windows, added mingw, cygwin, and uwin", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "If you put \"Fixes #6871\" in the commit description, the it'll auto-close the issue.", "Oh, got it -- I thought it was sufficient to put it in the merge description -- will keep in mind next time, thanks!", "Oh, I don't think I modified those files:\r\n\r\n```\r\n62 - C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/python/kernel_tests/inplace_ops_test.py (Failed)\r\n106 - C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/python/kernel_tests/scatter_nd_ops_test.py (Failed)`\r\n```\r\n", "Yeah, it's an unrelated failure.", "I wonder if it's transient.\r\n\r\nJenkins, test this please.", "OK, I looked into the code, and I think there is a potential bug (I didn't introduce it though).\r\n\r\nIn line [`inplace_ops_test.py#L102`](https://github.com/tensorflow/tensorflow/blob/76d596002/tensorflow/python/kernel_tests/inplace_ops_test.py#L102), there is a `d0 / 10`, which potentially might return a float, while `numpy.random.choice` expects an integer. This is supposed to be `d0 // 10`, given that there is a `from __future__ import division`. The same story is on line [`inplace_ops_test.py#L122`](https://github.com/tensorflow/tensorflow/blob/76d596002/tensorflow/python/kernel_tests/inplace_ops_test.py#L122). I am not really sure about the other error -- need to read the code more \ud83d\ude04 ", "Yeah, we're removing that test. Thanks for looking into it. Don't worry\nabout that.\n\nOn Mon, Jan 16, 2017 at 5:45 PM, Zafar Takhirov <notifications@github.com>\nwrote:\n\n> OK, I looked into the code, and I think there is a potential bug (I didn't\n> introduce it though).\n>\n> In line inplace_ops_test.py#L102\n> <https://github.com/tensorflow/tensorflow/blob/76d596002/tensorflow/python/kernel_tests/inplace_ops_test.py#L102>,\n> there is a d0 / 10, which potentially might return a float, while\n> numpy.random.choice expects an integer. This is supposed to be d0 // 10,\n> given that there is a from __future__ import division. The same story is\n> on line inplace_ops_test.py#L122\n> <https://github.com/tensorflow/tensorflow/blob/76d596002/tensorflow/python/kernel_tests/inplace_ops_test.py#L122>.\n> I am not really sure about the other error -- need to read the code more\n> \ud83d\ude04\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/6872#issuecomment-273000664>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbYt5OK7vJvOlOvd-fOS8Ep9zJ2Pdks5rTB1JgaJpZM4LkNWR>\n> .\n>\n", "Oh OK... \r\n\r\nWell, anyway, for future reference --  I think I found what causes the second error. In line [`scatter_nd_ops_test.py#L55`](https://github.com/tensorflow/tensorflow/blob/76d596002/tensorflow/python/kernel_tests/scatter_nd_ops_test.py#L55), there is a `num_updates = indices.size / ixdim`. The problem is that, because the header of the file has `from __future__ import division`, this line will return a float (it needs a `//` division). This in return will cause [`scatter_nd_ops_test.py#L61`](https://github.com/tensorflow/tensorflow/blob/76d596002/tensorflow/python/kernel_tests/scatter_nd_ops_test.py#L61) to fail, because `numpy.reshape` expects an integer or tuple of ints.\r\n\r\nJust thought it might be useful, because this bug will eventually resurface in other builds as well", "Jenkins, test this please.", "@tensorflow-jenkins test this please"]}, {"number": 6871, "title": "Windows `uname -s` might return different values", "body": "[`configure#L14`](https://github.com/tensorflow/tensorflow/blob/73bc42890/configure#L14) checks if the system is Windows, but doesn't check `uname -s` values correctly according to the [wiki page](https://en.wikipedia.org/wiki/Uname#Examples).\r\n\r\nUnless not supported, I think other Windows SDE's should be included in the check.", "comments": ["@drpngx could you please check if #6872 solves it", "Yeah, I think that's fine. I'm not sure how far you can go with the build after that. The configuration that works is the cmake build.", "I wonder if there is a Windows system with `cygwin` and `uwin` in the Jenkins, because if there are none, bugs will not be caught even if enabled in the `configure`", "No, there isn't. We only check the cmake build.", "Assigning to @drpngx since it seems that he's reviewing the PR\r\n(Thanks @zafartahirov for the report and PR)", "For windows, we currently do native compilation with cmake, (no cygwin).\r\nBazel setup for windows uses msys2, which is the closest thing we have to cygwin, but that also uses visual studio to compile TF.", "Fix submitted for this particular check."]}, {"number": 6870, "title": "POSIX compliance", "body": "This is a non-urgent issue that might require a lot of changes\r\n\r\nI was looking into modifying some TF files to make it work on BSD, and noticed that a lot of files ignore the POSIX compliance. Quick example, `configure` is written in Bourne shell, which is not POSIX.\r\n\r\nI think it is important to write everything with POSIX in mind, because later on if TF would need to add support for other OS's, it might be harder.\r\n\r\nFor example if you run `checkbashisms` on `configure`, that what you get:\r\n\r\n```\r\npossible bashism in ./configure line 7 ((push|pop)d):\r\npushd `dirname $0` > /dev/null\r\npossible bashism in ./configure line 9 ((push|pop)d):\r\npopd > /dev/null\r\npossible bashism in ./configure line 12 ('function' is useless):\r\nfunction is_windows() {\r\npossible bashism in ./configure line 14 (alternative test command ([[ foo ]] should be [ foo ])):\r\n  if [[ \"${PLATFORM}\" =~ msys_nt* ]]; then\r\npossible bashism in ./configure line 21 ('function' is useless):\r\nfunction bazel_clean_and_fetch() {\r\npossible bashism in ./configure line 40 (read with option other than -r):\r\n    read -p \"Please specify the location of python. [Default is $default_python_bin_path]: \" PYTHON_BIN_PATH\r\npossible bashism in ./configure line 64 (should be 'b = a'):\r\nwhile [ \"$TF_NEED_JEMALLOC\" == \"\" ]; do\r\npossible bashism in ./configure line 66 (read with option other than -r):\r\n  read -p \"Do you wish to use jemalloc as the malloc implementation? \"\\\r\n\"(Linux only) [Y/n] \" INPUT\r\npossible bashism in ./configure line 75 (should be 'b = a'):\r\nif [ \"$TF_NEED_JEMALLOC\" == \"1\" ]; then\r\npossible bashism in ./configure line 81 (should be 'b = a'):\r\nwhile [ \"$TF_NEED_GCP\" == \"\" ]; do\r\npossible bashism in ./configure line 83 (read with option other than -r):\r\n  read -p \"Do you wish to build TensorFlow with \"\\\r\n\"Google Cloud Platform support? [y/N] \" INPUT\r\npossible bashism in ./configure line 95 (should be 'b = a'):\r\nif [ \"$TF_NEED_GCP\" == \"1\" ]; then\r\npossible bashism in ./configure line 98 (alternative test command ([[ foo ]] should be [ foo ])):\r\n  if [[ $(uname -a) =~ Linux ]] && [[ ! -f \"/usr/include/curl/curl.h\" ]]; then\r\npossible bashism in ./configure line 111 (should be 'b = a'):\r\nwhile [ \"$TF_NEED_HDFS\" == \"\" ]; do\r\npossible bashism in ./configure line 113 (read with option other than -r):\r\n  read -p \"Do you wish to build TensorFlow with \"\\\r\n\"Hadoop File System support? [y/N] \" INPUT\r\npossible bashism in ./configure line 125 (should be 'b = a'):\r\nif [ \"$TF_NEED_HDFS\" == \"1\" ]; then\r\npossible bashism in ./configure line 134 (should be 'b = a'):\r\nwhile [ \"$TF_ENABLE_XLA\" == \"\" ]; do\r\npossible bashism in ./configure line 135 (read with option other than -r):\r\n  read -p \"Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] \" INPUT\r\npossible bashism in ./configure line 144 (should be 'b = a'):\r\nif [ \"$TF_ENABLE_XLA\" == \"1\" ]; then\r\npossible bashism in ./configure line 163 (should be 'b = a'):\r\nwhile [ \"$TF_NEED_OPENCL\" == \"\" ]; do\r\npossible bashism in ./configure line 164 (read with option other than -r):\r\n  read -p \"Do you wish to build TensorFlow with OpenCL support? [y/N] \" INPUT\r\npossible bashism in ./configure line 175 (should be 'b = a'):\r\nwhile [ \"$TF_NEED_CUDA\" == \"\" ]; do\r\npossible bashism in ./configure line 176 (read with option other than -r):\r\n  read -p \"Do you wish to build TensorFlow with CUDA support? [y/N] \" INPUT\r\npossible bashism in ./configure line 187 (alternative test command ([[ foo ]] should be [ foo ])):\r\nif [[ \"$TF_NEED_CUDA\" == \"0\" ]] && [[ \"$TF_NEED_OPENCL\" == \"0\" ]]; then\r\npossible bashism in ./configure line 187 (should be 'b = a'):\r\nif [[ \"$TF_NEED_CUDA\" == \"0\" ]] && [[ \"$TF_NEED_OPENCL\" == \"0\" ]]; then\r\npossible bashism in ./configure line 193 (should be 'b = a'):\r\nif [ \"$TF_NEED_CUDA\" == \"1\" ]; then\r\npossible bashism in ./configure line 200 (read with option other than -r):\r\n    read -p \"Please specify which gcc should be used by nvcc as the host compiler. [Default is $default_gcc_host_compiler_path]: \" GCC_HOST_COMPILER_PATH\r\npossible bashism in ./configure line 224 (read with option other than -r):\r\n    read -p \"Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: \" TF_CUDA_VERSION\r\npossible bashism in ./configure line 237 (read with option other than -r):\r\n    read -p \"Please specify the location where CUDA $TF_CUDA_VERSION toolkit is installed. Refer to README.md for more details. [Default is $default_cuda_path]: \" CUDA_TOOLKIT_PATH\r\npossible bashism in ./configure line 244 (alternative test command ([[ foo ]] should be [ foo ])):\r\n  if [[ -z \"$TF_CUDA_VERSION\" ]]; then\r\npossible bashism in ./configure line 252 (should be 'b = a'):\r\n  elif [ \"$OSNAME\" == \"Linux\" ]; then\r\npossible bashism in ./configure line 254 (should be 'b = a'):\r\n  elif [ \"$OSNAME\" == \"Darwin\" ]; then\r\npossible bashism in ./configure line 277 (read with option other than -r):\r\n    read -p \"Please specify the Cudnn version you want to use. [Leave empty to use system default]: \" TF_CUDNN_VERSION\r\npossible bashism in ./configure line 283 (read with option other than -r):\r\n    read -p \"Please specify the location where cuDNN $TF_CUDNN_VERSION library is installed. Refer to README.md for more details. [Default is $default_cudnn_path]: \" CUDNN_INSTALL_PATH\r\npossible bashism in ./configure line 293 (alternative test command ([[ foo ]] should be [ foo ])):\r\n  if [[ -z \"$TF_CUDNN_VERSION\" ]]; then\r\npossible bashism in ./configure line 300 (should be 'b = a'):\r\n    elif [ \"$OSNAME\" == \"Linux\" ]; then\r\npossible bashism in ./configure line 303 (should be 'b = a'):\r\n    elif [ \"$OSNAME\" == \"Darwin\" ]; then\r\npossible bashism in ./configure line 320 (alternative test command ([[ foo ]] should be [ foo ])):\r\n    if [[ \"$REALVAL\" =~ .so[.]+([0-9]*) ]]; then\r\npossible bashism in ./configure line 321 ($BASH_SOMETHING):\r\n      TF_CUDNN_EXT=\".\"${BASH_REMATCH[1]}\r\npossible bashism in ./configure line 321 (bash arrays, ${name[0|*|@]}):\r\n      TF_CUDNN_EXT=\".\"${BASH_REMATCH[1]}\r\npossible bashism in ./configure line 322 ($BASH_SOMETHING):\r\n      TF_CUDNN_VERSION=${BASH_REMATCH[1]}\r\npossible bashism in ./configure line 322 (bash arrays, ${name[0|*|@]}):\r\n      TF_CUDNN_VERSION=${BASH_REMATCH[1]}\r\npossible bashism in ./configure line 324 (alternative test command ([[ foo ]] should be [ foo ])):\r\n    elif [[ \"$REALVAL\" =~ ([0-9]*).dylib ]]; then\r\npossible bashism in ./configure line 325 ($BASH_SOMETHING):\r\n      TF_CUDNN_EXT=${BASH_REMATCH[1]}\".dylib\"\r\npossible bashism in ./configure line 325 (bash arrays, ${name[0|*|@]}):\r\n      TF_CUDNN_EXT=${BASH_REMATCH[1]}\".dylib\"\r\npossible bashism in ./configure line 326 ($BASH_SOMETHING):\r\n      TF_CUDNN_VERSION=${BASH_REMATCH[1]}\r\npossible bashism in ./configure line 326 (bash arrays, ${name[0|*|@]}):\r\n      TF_CUDNN_VERSION=${BASH_REMATCH[1]}\r\npossible bashism in ./configure line 330 (should be 'b = a'):\r\n    if [ \"$OSNAME\" == \"Darwin\" ]; then\r\npossible bashism in ./configure line 340 (should be 'b = a'):\r\n  elif [ \"$OSNAME\" == \"Linux\" ]; then\r\npossible bashism in ./configure line 343 (should be 'b = a'):\r\n  elif [ \"$OSNAME\" == \"Darwin\" ]; then\r\npossible bashism in ./configure line 354 (should be 'b = a'):\r\n  if [ \"$OSNAME\" == \"Linux\" ]; then\r\npossible bashism in ./configure line 365 (should be 'b = a'):\r\n  if [ \"$OSNAME\" == \"Linux\" ]; then\r\npossible bashism in ./configure line 388 (read with option other than -r):\r\n    read -p \"[Default is: \\\"3.5,5.2\\\"]: \" TF_CUDA_COMPUTE_CAPABILITIES\r\npossible bashism in ./configure line 395 (${parm/?/pat[/str]}):\r\n  COMPUTE_CAPABILITIES=${TF_CUDA_COMPUTE_CAPABILITIES//,/ }\r\npossible bashism in ./configure line 398 (alternative test command ([[ foo ]] should be [ foo ])):\r\n    if [[ ! \"$CAPABILITY\" =~ [0-9]+.[0-9]+ ]]; then\r\npossible bashism in ./configure line 404 (should be 'b = a'):\r\n  if [ \"$ALL_VALID\" == \"0\" ]; then\r\npossible bashism in ./configure line 430 (should be 'b = a'):\r\nif [ \"$TF_NEED_OPENCL\" == \"1\" ]; then\r\npossible bashism in ./configure line 437 (read with option other than -r):\r\n    read -p \"Please specify which C++ compiler should be used as the host C++ compiler. [Default is $default_cxx_host_compiler]: \" HOST_CXX_COMPILER\r\npossible bashism in ./configure line 460 (read with option other than -r):\r\n    read -p \"Please specify which C compiler should be used as the host C compiler. [Default is $default_c_host_compiler]: \" HOST_C_COMPILER\r\npossible bashism in ./configure line 485 (read with option other than -r):\r\n    read -p \"Please specify the location where ComputeCpp for SYCL $TF_OPENCL_VERSION is installed. [Default is $default_computecpp_toolkit_path]: \" COMPUTECPP_TOOLKIT_PATH\r\npossible bashism in ./configure line 492 (should be 'b = a'):\r\n  if [ \"$OSNAME\" == \"Linux\" ]; then\r\n```\r\n", "comments": ["This is something that I agree would be a good thing to do. It would be the right thing to do. However it would be the most difficult task imaginable.\r\n\r\nWriting scripts in bourne shell that are correct is easier said than done and oftentimes requires making a lot of tradeoffs. It's nice having features like arrays, set -o pipefail, $PIPESTATUS, $(...), etc. It's also nice being able to do things like [[ ... ]] that don't spawn subprocesses. I've heard some of these things have been added to the bourne shell by the posix spec. But I don't have much knowledge of which.\r\n\r\nGoogle style actually forbids the use of any shell except the bourne again shell. https://google.github.io/styleguide/shell.xml So the rabbit hole runs pretty deep when it comes to bash. In addition to the configure script, bashisms can be found all throughout our build system in things like genrule. All of our institutional wisdom about writing shell scripts is built on bash. That's something that would be hard to change.\r\n\r\nEventually, our hope is that we can get rid of the configure script. Then we can maybe vendor bash inside of Bazel so we don't have to depend on the system to provide it. See: https://github.com/bazelbuild/bazel/issues/1797. Thoughts on that?\r\n\r\nRight now our support matrix for operating systems is: Ubuntu Linux 14 LTE to 16. Mac OS X El Capitan. CentOS 7 and above.", "Would an acceptable alternative be for us to replace the configure script with a Python script?", "@jart I agree that bash is much more convenient, the problem is that not all OSs have it. I checked the style guide, and it looks like there are exceptions (like Solaris) \ud83d\ude09 \r\n\r\nI think replacing the `configure` with py script would be fine. Or [`autoconf` flow](https://en.wikipedia.org/wiki/Autoconf#/media/File:Autoconf-automake-process.svg) could be written.", "Anyway, I think this is just for future discussion -- and there are waaaaayyy too many things that require changing. As of now, I can just install bash on any system that doesn't have it, and modify the source where required \ud83d\ude04 ", "What are some other things that need changing? I know I can think of a few. I'd like to hear some of your ideas.", "Well, the first thing that comes to mind is that because `FreeBSD` is similar to OS X, most probably there must be some checks for it in the `*.cc` codes. \r\nFor example, quick grep find shows that [this line](https://github.com/tensorflow/tensorflow/blob/73bc42890/tensorflow/stream_executor/rng.cc#L44) might need an include for `BSD` or any other supported Unix system check in addition to `__APPLE__`. \r\n\r\nI guess the documentation will also need addition of other OSs. For example, on some compilers, `libdl` is part of `libc`, which means that a quick hack might be required (like `ln -s libc.so libdl.so`)\r\n\r\nThere might be more -- I am just sticking to using TF on OS X for now :)", "Closing this out for now as it seems unlikely that we will get to this in the near or medium-term future.\r\n\r\nIf there is sufficient interest, contributions will be welcome!"]}, {"number": 6869, "title": "Confusion regarding `jemalloc`", "body": "@jhseu In one of the recent commits (83c6e0c), `jemalloc` was introduced. The [blame comment](https://github.com/tensorflow/tensorflow/commit/83c6e0c63acdcab2c58c4ed7220bfa58879b1d57) says that it is \"Only enabled on Linux for now\". However, it defaults to `enabled` for any OS. If it is intended to work for Linux only, I think it should default to `disabled`, or should ask only if detected Linux-based system (i.e. `$(uname -a | tr 'A-Z' 'a-z') =~ linux`).\r\n\r\nThe main confusion is that if I run the `configure` from Unix-based (e.g. OS X) or Windows, and just zoom through the config requests, it will try enabling `jemalloc`, and doesn't have a check for OS in the [`configure`](https://github.com/tensorflow/tensorflow/blob/73bc42890/configure).", "comments": ["I think it is enabled/disabled right here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/default/build_config.bzl#L186\r\n\r\nI can confirm that in macos and windows, It does not build.\r\nHowever, in macos, configure script prints using jemalloc on linux, which might be confusing.\r\n\r\n@jhseu, maybe we can make configure print some more information based on OS?", "Is jemalloc better than tcmalloc? I'm using tcmalloc before.", "In our benchmarks, jemalloc was slightly faster, but looks like tcmalloc has some changes planned that can make it faster.\r\n@jhseu can comment more on that.", "Our intention is to choose the right default, and we run a lot of numbers internally, and right now jemalloc is better. It's possible that we'll update the external tcmalloc and switch later on, but we're not sure yet."]}, {"number": 6868, "title": "Remove overly restrictive check on gradient types.", "body": "Fixes #6858 \r\n\r\nCurrently gradient computation `input.is_compatible_with(backprop)` for every input. This fails if we have float16 activations and float32 backprops (ie `tf.float32.is_compatible_with(tf.float16)` returns False), this PR removes this check.", "comments": ["Can one of the admins verify this patch?", "Should we check that we have at least numeric types?\r\n\r\nJenkins, test this please.", "Makes sense, the backprops should be numeric (inputs could still be non-numeric)", "Added another place which enforces that gradient type matches activation type. With these changes we can run scott's mixed precision backprop kernels", "It would also be nice to check that the gradient is complex iff the value is.  It might be safer to rephase the check using whitelists of similar ops to avoid missing cases like this.", "So checking if backprops are real-valued for real-valued inputs and complex valued for complex valued inputs is more restrictive without ruling out mixed precision. I added `is_real` method to mirror `is_complex`, since the existing `is_floating` doesn't seem to cover variables or quantized types. What did you mean by whitelists of similar ops? What whitelist should I add?", "It's plausible to have quantized gradients for floating point parameters, I\nthink.\n\nOn Jan 17, 2017 8:26 AM, \"Yaroslav Bulatov\" <notifications@github.com>\nwrote:\n\n> So checking if backprops are real-valued for real-valued inputs and\n> complex valued for complex valued inputs is more restrictive without ruling\n> out mixed precision. I added is_real method to mirror is_complex, since\n> the existing is_floating doesn't seem to cover variables or quantized\n> types. What did you mean by whitelists of similar ops? What whitelist\n> should I add?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/6868#issuecomment-273218654>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbdQYqXzY1UMpKLTopMuH2Mv0Rzs9ks5rTOufgaJpZM4LkKOe>\n> .\n>\n", "added checks that real-valued inputs must have real-valued backprops, allowing for quantized representation, same restriction for complex-valued.\r\n\r\nwhile testing, found that some tensors (TopKV2, TensorArrayReadV3) produce integer valued gradients, so relaxed checks to allow for integer values, PTAL", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.  Maybe this time it won't abort partway through?", "Tensorflow-sanity job is failing. Once it fails, it aborts all other jobs.", "```\r\nFAIL: Found 2 non-whitelited pylint errors:\r\ntensorflow/python/ops/gradients_impl.py:232: [W0311(bad-indentation), ] Bad indentation. Found 10 spaces, expected 8\r\n\r\ntensorflow/python/ops/gradients_impl.py:239: [W0311(bad-indentation), ] Bad indentation. Found 10 spaces, expected 8\r\n```", "Ooops, that's what I get for doing edits in Github edit window", "Jenkins, test this please.", "Ok, so we have an issue here. We may have to revert this as is, or maybe we can patch it.\r\n\r\n@yaroslavvb, why did you introduce a `is_real`, instead of using the `is_floating` property? More specifically, `bfloat16` is really an internal type, is it necessary to expose it?", "Since there was discussion on this earlier, the bfloat16 quantization is (should be) an implementation detail for network transfer only (@josh11b, @vrv do you agree?). I don't understand the comment about Variables: in what way does is_floating not work for variables?", "bfloat16 was indeed supposed to be a hidden optimization.  Whether that should remain that way is up to others.", "@martinwicke I can send a PR that changes this to `is_floating`. The reason I added `is_real` is because future work may go a step further and use float32 backprop + quantized activation, which makes sense mathematically since both represent real-valued number", "That's true, but let's not get ahead of ourselves, especially not with something as hacky as bfloat16.  I doubt that we'd want that type in particular for quantized backprop. A proper fix-point or short float type (half) is much more likely. \r\n\r\nCan you make a PR changing this to use is_floating, and remove the is_real function? Mention me so we can get this in quickly.\r\n\r\nThanks!"]}, {"number": 6867, "title": "Fix quantize_graph invocation in docs", "body": "Fixes #6181 ", "comments": ["Hmm flaky test.\r\n\r\n```\r\nFileExistsError: [Errno 17] File exists: '/tmp/tensorflow_dataframe_test'\r\n```"]}, {"number": 6866, "title": "Update zlib URL in r0.11 branch", "body": "Fixes #6865 ", "comments": ["Jenkins, test this please.\r\n\r\nNot sure which one it'll test. This looks fine.", "Looks like we have errors, unrelated to this. @yifeif do we maintain 0.11?"]}, {"number": 6865, "title": "zlib removed from repository url mentioned in build", "body": "I was building docker container for tensorflow r0.11 branch and it throws error because it repository url for package zlib mentioned in build file is no longer available on that url. Here is snippet for error.\r\n\r\n```ERROR: /tensorflow/tensorflow/core/BUILD:903:1: no such package '@zlib_archive//': Error downloading \r\nfrom http://zlib.net/zlib-1.2.8.tar.gz to /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4\r\ncff9/external/zlib_archive: Error downloading http://zlib.net/zlib-1.2.8.tar.gz to /root/.cache/bazel\r\n/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/zlib_archive/zlib-1.2.8.tar.gz: 404 Not Found:\r\n <!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">                                                  \r\n<html><head>                                                                                         \r\n<title>404 Not Found</title>                                                                         \r\n</head><body>                                                                                        \r\n<h1>Not Found</h1>                                                                                   \r\n<p>The requested URL /zlib-1.2.8.tar.gz was not found on this server.</p>                            \r\n<p>Additionally, a 404 Not Found                                                                     \r\nerror was encountered while trying to use an ErrorDocument to handle the request.</p>                \r\n</body></html>                                                                                       \r\n and referenced by '//tensorflow/core:lib_internal'.                                                 \r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.  \r\n____Elapsed time: 3.516s                                                                             \r\n```", "comments": ["I just hit the same thing in r0.11 running ./configure", "Thanks for bringing this to our attention. This was fixed in the r0.12 branch in https://github.com/tensorflow/tensorflow/commit/1e317b1f7dc5ccf04fc51ac96d97f5bdaefa9af9 but not the r0.11 branch, which I'll do now. In the meantime you can change the URL in tensorflow/workspace.bzl to http://zlib.net/fossils/zlib-1.2.8.tar.gz.\r\n\r\nI'll also mention that at HEAD we now using redundant mirroring when downloading external dependencies. So problems like these should hopefully never arise again.", "Should be resolved now. ", "Thanks!"]}, {"number": 6864, "title": "Add ProfilerHook for capturing CPU/GPU profiling information with MonitoredSession", "body": "It would be good to see if this is the preferred way to get profiling when using tf.contrib.training. If so, I can add the missing tests and doc updates.", "comments": ["Can one of the admins verify this patch?", "@ispirmustafa could you take a look?", "This is pretty useful, I was wondering how to do the same thing myself", "I've added tests and updated the docs.\r\n\r\nOne test does have a sleep in it which isn't great, but I don't see an existing utility (eg. faking time) to get around that. The test does at least try to follow the existing conventions in that file.", "Excellent. I spotted @gunan just put in the use of `@test.mock.patch`, which I gladly used. The ProfilerHook test now takes < 1s rather than nearly 7s, which is a bit more like it. Thanks.\r\n\r\nI also fixed up the merge conflict.", "@tensorflow-jenkins test this please\r\n@ispirmustafa PTAAL?", "Thanks for the review @gunan. I've fixed the lines you spotted, but haven't figured out yet how to lint the TF code so hopefully it'll pass now:\r\n\r\nI tried:\r\n\r\n    bazel test -c opt --copt=\"-march=native\" --config=cuda //tensorflow/python:basic_session_run_hooks_test\r\n\r\nand:\r\n\r\n    PYLINTRC=./tensorflow/tools/ci_build/pylintrc pylint tensorflow/python/training/basic_session_run_hooks_test.py", "Jenkins, test this please.", "btw, to lint things:\n\npip install pylint\nwget -O /tmp/pylintrc\nhttps://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/tools/ci_build/pylintrc\npylint tensorflow/get_string_ops.py --rcfile=/tmp/pylintrc\n\n\nOn Mon, Jan 23, 2017 at 2:32 PM, gunan <notifications@github.com> wrote:\n\n> Jenkins, test this please.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/6864#issuecomment-274639226>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABaHNITeZVxzyUaATZBlmdodGIGOsEZks5rVSqPgaJpZM4Lj-qh>\n> .\n>\n", "@ispirmustafa @gunan are we good with the amended commit?", "I am ok, but Id like @ispirmustafa to take one final look at the PR.\r\nJenkins, test this please.", "Thanks for the reviews.\r\n\r\nThe Windows CMake test [failed](https://ci.tensorflow.org/job/tensorflow-pr-win-cmake-py/896/consoleFull), but that looks like an unrelated race condition in the [input_test.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/input_test.py#L987). A vague look seems like ``all_a`` and ``seen_b`` should be getting checked rather than ``which_a`` and ``which_b``, [possibly](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/input_test.py#L979).", "Jenkins, test this please.", "I'm good with the PR.\r\nThanks @darrengarvey!", "I think this involves an API change. Could you comment about the change @martinwicke ", "\u200bYes. We will get a decision next Tuesday. Should be fine.\n", "Thanks for the reviews all.\r\n\r\nI'll leave it up to you guys. Let me know if I should update the PR to not expose it to the public API and call the class `_ProfilerHook` like I've noticed other hooks - like `FeedFnHook` - did while they \"bed in\".", "Yes, can you move this to contrib for now? You can make a new `contrib/hooks` of if you have another good spot it fits into, that would be fine too.", "@darrengarvey @martinwicke it looks like we are waiting for this to be moved.", "I've had a go at moving it to contrib.hooks. I couldn't find an easier place to put it.", "BTW, locations in contrib mirror official locations for easy of moving between the two, so\r\n\r\n`<gitroot>/tensorflow/<somepath>` is translated to `<gitroot>/tensorflow/contrib/projectname/<somepath>`\r\n\r\nSo in your case \"hooks\" is project name, but `<somepath>` part remains the same. https://github.com/tensorflow/tensorflow/commit/37fbebdd is a recent example of moving something to contrib. Sorry this is taking so much work, it's faster the second time you do it!\r\n", "Thanks for the pointers @yaroslavvb, but I'm not sure I entirely follow. :) Do you mean:\r\n\r\n`<gitroot>/tensorflow/<somepath>` is translated to `<gitroot>/tensorflow/contrib/projectname/<somepath>`\r\n\r\n(note the contrib)?\r\n\r\nDid you notice I had pushed my attempt to the same branch? I have only moved `ProfilerHook` which was previously alongside the other hooks in `basic_session_run_hooks.py` so I had to give it a new file. I don't mind moving the whole of `basic_session_run_hooks.py` to `contrib/hooks`, but since the other hooks are already available I didn't take @martinwicke's comment to mean that.\r\n\r\nI don't mind getting this in the right place; I'm getting quite a lot back from TF so I've got a fair amount of patience. :)", "Yes, that's the path I meant, updated. Correct, I don't think Martin meant for you to move existing functionality to `contrib`, just the new addition.\r\n\r\nFor example of existing placement consider locations of existing hooks:\r\n\r\nOfficial \"basic_session_run_hooks\"\r\n`tensorflow/python/training/basic_session_run_hooks.py`\r\n\r\nUnofficial hooks that should at some point be merged into file above\r\n`tensorflow/contrib/learn/python/learn/basic_session_run_hooks.py `\r\n\r\nCurrently your path is `tensorflow/contrib/hooks/python/hooks/profiler_hook.py` which means that for integration it would be moved to `tensorflow/python/hooks/profiler_hook.py`, which would create a new package `python/hooks` in the TensorFlow repo.\r\n\r\nTo keep hooks under `python/learn`, you could use following locations\r\n\r\n1. Add to existing `learn` project, modify `basic_session_run_hooks.py`\r\n`tensorflow/contrib/learn/python/learn/basic_session_run_hooks.py `\r\n\r\n2. Move to `hooks` project, reuse `basic_session_run_hooks.py` filename\r\n`tensorflow/contrib/hooks/python/learn/basic_session_run_hooks.py `\r\n\r\n3. Move to `hooks` project, use new filename\r\n`tensorflow/contrib/hooks/python/learn/profiler_hook.py `\r\n\r\nI think your current solution is closest to 3.", "I have some preference for making this a `contrib/hooks` project. I think the hooks should probably live in the directory `python/training` once it's in core, next to `MonitoredSession`.\r\n\r\nSo, the path to use would be: `contrib/hooks/python/training/profiler_hook.py`. I think we want to have one file per hook in the end, so no need to use `basic_run_hook.py`.", "Ah I hadn't picked up on that mirroring magic. Good to know.\r\n\r\nI'm away for the weekend but I'll do this tweak to put it under `contrib/hooks/python/training/` probably on Monday.", "Right, pushed as per suggestions, plus a couple of fixes. The hooks aren't specifically for ``contrib.learn``, so putting them under ``contrib/hooks/python/training/`` made sense.", "I'm not sure if it's OK for me to ask Jenkins to test this, to check I didn't miss anything?", "Jenkins, test this please.", "Could you check the test? It might be related.\r\n\r\n```\r\n15:44:05 151/201 Test #168: C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/python/training/coordinator_test.py .........................................***Failed   15.53 sec\r\n15:44:05 ....Exception in thread Thread-15:\r\n15:44:05 Traceback (most recent call last):\r\n15:44:05   File \"C:\\Program Files\\Anaconda3\\lib\\threading.py\", line 914, in _bootstrap_inner\r\n15:44:05     self.run()\r\n15:44:05   File \"C:\\Program Files\\Anaconda3\\lib\\threading.py\", line 862, in run\r\n15:44:05     self._target(*self._args, **self._kwargs)\r\n15:44:05   File \"C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/python/training/coordinator_test.py\", line 39, in RaiseInN\r\n15:44:05     raise ex\r\n15:44:05 ValueError: Clean stop\r\n15:44:05 \r\n15:44:05 .Exception in thread Thread-16:\r\n15:44:05 Traceback (most recent call last):\r\n15:44:05   File \"C:\\Program Files\\Anaconda3\\lib\\threading.py\", line 914, in _bootstrap_inner\r\n15:44:05     self.run()\r\n15:44:05   File \"C:\\Program Files\\Anaconda3\\lib\\threading.py\", line 862, in run\r\n15:44:05     self._target(*self._args, **self._kwargs)\r\n15:44:05   File \"C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/python/training/coordinator_test.py\", line 39, in RaiseInN\r\n15:44:05     raise ex\r\n15:44:05 tensorflow.python.framework.errors_impl.OutOfRangeError: First\r\n15:44:05 \r\n15:44:05 .F............\r\n15:44:05 ======================================================================\r\n15:44:05 FAIL: testJoinRaiseReportExcInfo (__main__.CoordinatorTest)\r\n15:44:05 ----------------------------------------------------------------------\r\n15:44:05 RuntimeError: Too late\r\n15:44:05 \r\n15:44:05 During handling of the above exception, another exception occurred:\r\n15:44:05 \r\n15:44:05 Traceback (most recent call last):\r\n15:44:05   File \"C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/python/training/coordinator_test.py\", line 162, in testJoinRaiseReportExcInfo\r\n15:44:05     coord.join(threads)\r\n15:44:05 AssertionError: \"First\" does not match \"Too late\"\r\n15:44:05 \r\n15:44:05 ----------------------------------------------------------------------\r\n15:44:05 Ran 19 tests in 6.751s\r\n15:44:05 \r\n15:44:05 FAILED (failures=1)\r\n```\r\n\r\n", "Thanks @drpngx. ``coordinator_test.py`` failed, which looks unrelated. It's after midnight here so I'll try have a look tomorrow.", "The failure was certainly unrelated.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/58201a058853de647b37ddb0ccf63d89b2357f03/tensorflow/python/training/coordinator_test.py#L152\r\n\r\nThat test looks like it'll fail under heavy load - ie. when starting a thread #1 takes more than 40ms more than starting thread #2. I can't replicate the failure here under heavy load but I might try it again with some more exotic cgroup settings if I manage to get time, but that's not likely this week. The test just below looks equally susceptible to thread starting times.\r\n\r\nThe same time mocking trick we did here should work for this test, but that's for another PR AFAICT.", "Just for the peace of mind\r\nJenkins, test this please.", "Hmm, a link-time failure on Linux cmake builds this time. Gadzooks... I'll have a better look in the morning, but for reference:\r\n\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-cpu-cmake/1610/console\r\n\r\n``\r\n[98%] Linking CXX executable compare_graphs\r\nCMakeFiles/tf_core_kernels.dir/workspace/tensorflow/core/kernels/hexagon/graph_transfer_utils.cc.o: In function `tensorflow::GraphTransferUtils::BuildFusedGraphDef(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<tensorflow::GraphTransferer::InputNodeInfo, std::allocator<tensorflow::GraphTransferer::InputNodeInfo> > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, tensorflow::GraphDef const&, tensorflow::GraphTransferer*)':\r\ngraph_transfer_utils.cc:(.text+0x7c8): undefined reference to `tensorflow::Scope::NewRootScope()'\r\ngraph_transfer_utils.cc:(.text+0x88f): undefined reference to `tensorflow::Scope::WithOpName(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'\r\ngraph_transfer_utils.cc:(.text+0x8d4): undefined reference to `tensorflow::Scope::GetUniqueNameForOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'\r\ngraph_transfer_utils.cc:(.text+0x10ba): undefined reference to `tensorflow::Scope::UpdateBuilder(tensorflow::NodeBuilder*) const'\r\ngraph_transfer_utils.cc:(.text+0x10e0): undefined reference to `tensorflow::Scope::UpdateStatus(tensorflow::Status) const'\r\ngraph_transfer_utils.cc:(.text+0x112e): undefined reference to `tensorflow::Operation::Operation(tensorflow::Node*)'\r\ngraph_transfer_utils.cc:(.text+0x15b9): undefined reference to `tensorflow::Scope::WithOpName(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'\r\ngraph_transfer_utils.cc:(.text+0x19b8): undefined reference to `tensorflow::ops::AsNodeOutList(tensorflow::Scope const&, tensorflow::InputList const&)'\r\ngraph_transfer_utils.cc:(.text+0x1aef): undefined reference to `tensorflow::Scope::GetUniqueNameForOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'\r\ngraph_transfer_utils.cc:(.text+0x236a): undefined reference to `tensorflow::Scope::UpdateBuilder(tensorflow::NodeBuilder*) const'\r\ngraph_transfer_utils.cc:(.text+0x239b): undefined reference to `tensorflow::Scope::UpdateStatus(tensorflow::Status) const'\r\ngraph_transfer_utils.cc:(.text+0x23f8): undefined reference to `tensorflow::Scope::ToGraphDef(tensorflow::GraphDef*) const'\r\ncollect2: error: ld returned 1 exit status\r\nmake[2]: *** [compare_graphs] Error 1\r\nCMakeFiles/compare_graphs.dir/build.make:1470: recipe for target 'compare_graphs' failed\r\nCMakeFiles/Makefile2:3735: recipe for target 'CMakeFiles/compare_graphs.dir/all' failed\r\nmake[1]: *** [CMakeFiles/compare_graphs.dir/all] Error 2\r\n``", "I've been averaging 1-3 unrelated failing tests for my PR submissions. Back when I was on TensorFlow team I tried to make sure that all tests were 100% passing all the time but it was hard. You can make internal changes blocking on all tests passing only if tests are super-fast. But the tests are only super-fast if people take extra care to make their tests efficient/parallelized. So eventually end-to-end testing time gets slow and by the time failure is noticed, the person who broke them went to lunch/home/etc", "Both failures are known issues.\r\nWe have fixes available for both of them, just waiting for merges.\r\nhttps://ci.tensorflow.org/job/tensorflow-master-cpu-cmake/595/console", "@gunan this is ready for merging, right?", "Jenkins, test this please.", "Can one of the admins verify this patch?", "Just to confirm, this is an API change so I'd like @ispirmustafa to confirm it's OK.", "This is fine to merge. The added API is in contrib, so no problem. ", "Oh, right, good to go then."]}, {"number": 6863, "title": "Protobuf import issue", "body": "I am trying to run tensorboard but I am getting the following import error on mac osx;\r\n\r\n`AttributeError: module 'pkg_resources' has no attribute 'declare_namespace'`\r\n\r\nI have tried reinstalling setuptools and distribute. This is coming up as an error in python 3.5 even though my tensorflow is installed in 2.7; I.e. \r\n\r\n```\r\n File \"/Library/Frameworks/Python.framework/Versions/3.5/bin/tensorboard\", line 7, in <module>\r\n    from tensorflow.tensorboard.tensorboard import main\r\n```", "comments": ["Thank you for bringing this to our attention @mattdns100689. What version of TensorFlow are you using? Can you share the full backtrace?", "I have installed it for both 2.7 and 3.5 and get the same error on both.\r\n\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.1.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.dylib locally\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/bin/tensorboard\", line 7, in <module>\r\n    from tensorflow.tensorboard.tensorboard import main\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 63, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/google/protobuf/__init__.py\", line 37, in <module>\r\n    __import__('pkg_resources').declare_namespace(__name__)\r\nAttributeError: module 'pkg_resources' has no attribute 'declare_namespace'\r\n```", "This stacktrace is going into the Protobuf codebase so this doesn't look like a bug in TensorFlow. It looks like something is wrong with the setuptools on your system. Maybe try reinstalling it? You can also try installing TensorFlow inside a virtualenv."]}, {"number": 6862, "title": "Error when tensorflow installed with conda virtual environment in windows 10", "body": "### Environment info\r\nOperating System:\r\n    windows 10\r\n\r\nI was trying to install tensorflow in `windows 10` with the guidance of the `readme.md`.\r\nBecause the version of `python` in my pc is `2.7.X` which is not compatiable with the guidance as it say  `python version=3.5`. Therefore, I use `conda create --name tensorflow python=3.5` to create a virtual environmnt and install tensorflow.\r\n\r\nHowever, an error occurs:\r\nTraceback (most recent call last):\r\n  File \"D:\\Program Files\\anaconda\\envs\\tensorflow\\Scripts\\pip-script.py\", line 5, in <module>\r\n    sys.exit(pip.main())\r\n  File \"D:\\Program Files\\anaconda\\envs\\tensorflow\\lib\\site-packages\\pip\\__init__.py\", line 249, in main\r\n    return command.main(cmd_args)\r\n  File \"D:\\Program Files\\anaconda\\envs\\tensorflow\\lib\\site-packages\\pip\\basecommand.py\", line 252, in main\r\n    pip_version_check(session)\r\n  File \"D:\\Program Files\\anaconda\\envs\\tensorflow\\lib\\site-packages\\pip\\utils\\outdated.py\", line 102, in pip_version_check\r\n    installed_version = get_installed_version(\"pip\")\r\n  File \"D:\\Program Files\\anaconda\\envs\\tensorflow\\lib\\site-packages\\pip\\utils\\__init__.py\", line 838, in get_installed_version\r\n    working_set = pkg_resources.WorkingSet()\r\n  File \"D:\\Program Files\\anaconda\\envs\\tensorflow\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 644, in __init__\r\n    self.add_entry(entry)\r\n  File \"D:\\Program Files\\anaconda\\envs\\tensorflow\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 700, in add_entry\r\n    for dist in find_distributions(entry, True):\r\n  File \"D:\\Program Files\\anaconda\\envs\\tensorflow\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 1949, in find_eggs_in_zip\r\n    if metadata.has_metadata('PKG-INFO'):\r\n  File \"D:\\Program Files\\anaconda\\envs\\tensorflow\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 1463, in has_metadata\r\n    return self.egg_info and self._has(self._fn(self.egg_info, name))\r\n  File \"D:\\Program Files\\anaconda\\envs\\tensorflow\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 1823, in _has\r\n    return zip_path in self.zipinfo or zip_path in self._index()\r\n  File \"D:\\Program Files\\anaconda\\envs\\tensorflow\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 1703, in zipinfo\r\n    return self._zip_manifests.load(self.loader.archive)\r\n  File \"D:\\Program Files\\anaconda\\envs\\tensorflow\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 1643, in load\r\n    mtime = os.stat(path).st_mtime\r\nFileNotFoundError: [WinError 2] \u7cfb\u7edf\u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6587\u4ef6\u3002: 'D:\\\\Program Files\\\\anaconda\\\\envs\\\\tensorflow\\\\lib\\\\site-packages\\\\setuptools-27.2.0-py3.5.egg'\r\n\r\nBesides, the ananconda is installed in `'D:\\\\Program Files\\\\anaconda`, while there is no file folder `D:\\\\Program Files\\\\anaconda\\\\envs\\\\tensorflow\\\\lib`.\r\nAfter searching in stackoverflow, I didn't find the same problem, since most guys just install tf in other systems. \r\n\r\nIs it that conda virtual environment of `python=3.5` not  compatiable with tf in windows?? And why tf in windows doesn't support python 2.7?? \r\n\r\nThanks!", "comments": ["No idea about how to solve this.\r\nI installed Anaconda both 2 and 3 (They can coexist with each other, see [Install Anaconda both 2 and 3](http://blog.csdn.net/infin1te/article/details/50445217)) on win7 and use the Python 3.5 environment for Tensorflow.  It works very well.", "What version of conda are you running? Do you think this issue is being caused by https://github.com/conda/conda/issues/3494?", "@jart the version of my anaconda is 4.2.0 python 2.7.\r\n#3494 is quite similiar to my issue.\r\n\r\nSince the  windows version only support `python 3.5`, I am thinking change my python from version 2.7 to 3.5.", "So the problem with your setup was that you were using Python 3.5 inside a Python 2.7 environment? In that case, it sounds like your issue has been resolved. If things end up not being resolved, let me know and I'll re-open this issue."]}, {"number": 6861, "title": "Strange bug with tf.nn.nce_loss", "body": "So for tf.nn.nce_loss for word2vec_basic, if I explicitly use keyword arguments, such as:\r\n      tf.nn.nce_loss(weights=nce_weights,\r\n                     biases=nce_biases,\r\n                     labels=train_labels,\r\n                     inputs=embed,\r\n                     num_sampled=num_sampled,\r\n                     num_classes=vocabulary_size))\r\n\r\neverything is fine. But if I get rid of the keywords:\r\n      tf.nn.nce_loss(nce_weights, nce_biases, train_labels, embed, num_sampled, vocabulary_size))\r\n\r\nI run into the bug: \r\n`  File \".../tensorflow/python/ops/nn.py\", line 1336, in nce_loss\r\n    name=name)\r\n  File \".../tensorflow/python/ops/nn.py\", line 1198, in _compute_sampled_logits\r\n    array_ops.reshape(true_w, new_true_w_shape))\r\n  File \".../tensorflow/python/ops/gen_math_ops.py\", line 1613, in mul\r\n    result = _op_def_lib.apply_op(\"Mul\", x=x, y=y, name=name)\r\n  File \".../tensorflow/python/framework/op_def_library.py\", line 521, in apply_op\r\n    inferred_from[input_arg.type_attr]))\r\nTypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.`\r\n\r\nI looked into the source code for nce_loss but couldn't seem to find the cause of this bug. Can anyone help me with this? Thanks a lot!", "comments": ["If you try to look into the documentation of this function, say, run \r\n`print(tf.nn.nce_loss.__doc__)`\r\n, you'll get:\r\n```\r\nweights: A `Tensor` of shape `[num_classes, dim]`, or a list of `Tensor`\r\n        objects whose concatenation along dimension 0 has shape\r\n        [num_classes, dim].  The (possibly-partitioned) class embeddings.\r\n    biases: A `Tensor` of shape `[num_classes]`.  The class biases.\r\n    inputs: A `Tensor` of shape `[batch_size, dim]`.  The forward\r\n        activations of the input network.\r\n    labels: A `Tensor` of type `int64` and shape `[batch_size,\r\n        num_true]`. The target classes.\r\n    num_sampled: An `int`.  The number of classes to randomly sample per batch.\r\n    num_classes: An `int`. The number of possible classes.\r\n    and something more balabala...\r\n```\r\nSo if you pass arguments by position instead of by keyword, the 3rd argument is inputs, and the 4th is labels. Try to switch these 2 parameters.", "Thank you for helping our friend @soloice. For future reference @chiphuyen we try to keep this issue tracker limited to bugs and feature requests. We recommend posting on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) for community-driven support.", "@soloice Thank you so much, I have met this problem and I struggled it for a long time until I saw \"Try to switch these 2 parameters.\""]}, {"number": 6860, "title": "Problems with generate_batch() in word2vec_basic.py", "body": "Does the generate_batch() function miss some possible (word, context) pairs? The sliding window always skips the end of a batch.\r\n\r\nFor example, if I change the demonstration codes followed by this function into\r\n```\r\nbatch, labels = generate_batch(batch_size=4, num_skips=2, skip_window=1)\r\nfor i in range(4):\r\n  print(batch[i], reverse_dictionary[batch[i]],\r\n        '->', labels[i, 0], reverse_dictionary[labels[i, 0]])\r\n\r\nprint('-'*40)\r\nbatch, labels = generate_batch(batch_size=4, num_skips=2, skip_window=1)\r\nfor i in range(4):\r\n  print(batch[i], reverse_dictionary[batch[i]],\r\n        '->', labels[i, 0], reverse_dictionary[labels[i, 0]])\r\n```\r\n, the sampled pairs would be:\r\n```\r\n3083 originated -> 5242 anarchism\r\n3083 originated -> 12 as\r\n12 as -> 3083 originated\r\n12 as -> 6 a\r\n----------------------------------------\r\n3134 abuse -> 2 of\r\n3134 abuse -> 46 first\r\n46 first -> 3134 abuse\r\n46 first -> 59 used\r\n```\r\n\r\nNote that the sampled sentence is:\r\n`Sample data [5242, 3083, 12, 6, 195, 2, 3134, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']`\r\n, pairs such like `a -> term`, `term -> of`, `of -> abuse` are skipped.", "comments": ["Thanks for reaching out @soloice. I'm triaging bugs this week. I'm not deeply familiar with this tutorial, but I'm reluctant to triage this as a bug in TensorFlow since it's modified example code and skipping pairs when training appears to be an intended behavior of the skip-gram model. Maybe the following resources will help?\r\n\r\n- https://www.quora.com/How-does-word2vec-work\r\n- http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\r\n- http://mccormickml.com/2016/04/27/word2vec-resources/\r\n- https://arxiv.org/pdf/1301.3781.pdf", "Thanks for your reply.\r\n\r\nThough, you might have some misunderstanding about word2vec.  I'm familiar with this model, and have published a paper on it.  I think this is a minor bug, not a desired behavior.  For example, this will cause failure of some theoretical analysis, i.e.: [neural-word-embedding-as-implicit-matrix-factorization](https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf).\r\n\r\nBut it's not too serious, because the effect is to subsample the corpus.  In practice, it still works well for word embedding learning.\r\n\r\nA simple fix is to let the pointer `data_index` to backtrack a little bit at the end of generate_batch(), see the next-to-last line below:\r\n```\r\n# Step 3: Function to generate a training batch for the skip-gram model.\r\ndef generate_batch(batch_size, num_skips, skip_window):\r\n  global data_index\r\n  assert batch_size % num_skips == 0\r\n  assert num_skips <= 2 * skip_window\r\n  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\r\n  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\r\n  span = 2 * skip_window + 1  # [ skip_window target skip_window ]\r\n  buffer = collections.deque(maxlen=span)\r\n  for _ in range(span):\r\n    buffer.append(data[data_index])\r\n    data_index = (data_index + 1) % len(data)\r\n  for i in range(batch_size // num_skips):\r\n    target = skip_window  # target label at the center of the buffer\r\n    targets_to_avoid = [skip_window]\r\n    for j in range(num_skips):\r\n      while target in targets_to_avoid:\r\n        target = random.randint(0, span - 1)\r\n      targets_to_avoid.append(target)\r\n      batch[i * num_skips + j] = buffer[skip_window]\r\n      labels[i * num_skips + j, 0] = buffer[target]\r\n    buffer.append(data[data_index])\r\n    data_index = (data_index + 1) % len(data)\r\n  data_index = (data_index + len(data) - span) % len(data)\r\n  return batch, labels\r\n```\r\n\r\nFYI.", "In that case I will re-open and put you in touch with someone who can help.\r\n\r\n@gouwsmeister Our friend @soloice believes he's found a minor bug in the word2vec_basic.py tutorial.  Since you've done work on this tutorial in the past, I'm hoping you can take a look into the information he's presented us.", "Good catch. Yes, this seems to be a (very subtle) off-by-one error. Since it only seems to affect the end of every batch, it's effect should almost be negligible (especially for larger batches). \r\n\r\n@soloice, could you please post the output on the same example sentence with your fix, and if it solves the problem we can accept a pull request to fix it?", "@gouwsmeister Sure. With my fix, the output on the same example will be:\r\n```\r\nSample data [5234, 3081, 12, 6, 195, 2, 3135, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\r\n3081 originated -> 5234 anarchism\r\n3081 originated -> 12 as\r\n12 as -> 6 a\r\n12 as -> 3081 originated\r\n----------------------------------------\r\n6 a -> 195 term\r\n6 a -> 12 as\r\n195 term -> 2 of\r\n195 term -> 6 a\r\n```\r\nNow all possible pairs are recalled, and words at the end of a batch (`term`, for instance) will appear in the next batch.", "@gouwsmeister  I've made a pull request about this issue. \r\nThis is the first time that I made a pull request on Github. Not sure if I'm getting it correctly.\r\nThanks for your tracking."]}, {"number": 6859, "title": "The type of `seq_len_max` should be int64", "body": "If the `sequence_length` is not given, `max_seq_len` will be assigned to `time_len`.\r\nBy default, the type of `time_len` is `tf.int32`. But we need `tf.int64`.\r\nSo we need to cast it to `int64`, too. (Just like line 640)", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 6858, "title": "Support for mixed precision gradients", "body": "To save memory while keeping precision it's useful to be allow functions that store activations as fp16, but compute gradients in fp32. We are working on custom ops that compute gradients in mixed mode, but it requires hacks on Python gradient type checks side to working. The feature request is to relax these checks.\r\n\r\nHere's an example computation graph with mixed precision gradient. Forward propagation is done in fp16, and backprop is in fp32 as in the graph below \r\n![mixed-grads](https://cloud.githubusercontent.com/assets/23068/21962339/52c99a62-dad8-11e6-99a8-d034abd6c1af.png)\r\n\r\n\r\nHere's a toy example that creates such a g\r\n\r\nGraph:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import function\r\n\r\n@function.Defun(tf.float16, tf.float32)\r\ndef custom_grad(x, grad):\r\n    return 2*tf.cast(x, tf.float32)*grad\r\n\r\n@function.Defun(tf.float16, grad_func=custom_grad)\r\ndef custom(x):\r\n    return tf.square(x)\r\n\r\nx = tf.Variable(1., dtype=tf.float16)\r\n\r\n# override cast to keep first backprop in fp32 \r\nwith tf.get_default_graph().gradient_override_map({\"Cast\": \"Identity\"}):\r\n    loss = tf.cast(custom(x, name=\"mycustom_apply\"), tf.float32)\r\n    \r\ngradient = tf.gradients(loss, x)\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(gradient)\r\n```\r\n\r\nCurrently the example above crashes because various places in code assumes that grad function output matches type of incoming activation.\r\n\r\nin particular:\r\n\r\n- [tensorflow/python/ops/gradients_impl.py](https://github.com/tensorflow/tensorflow/blob/cb17d1b0e2b581b5da1d9597b7929ba764749d38/tensorflow/python/ops/gradients_impl.py#L260), line 264, in _VerifyGeneratedGradients: enforces that gradient output type is the same as activation input type", "comments": ["Assigning this gradient feature request to @girving. ", "I could send a PR that implements this if someone would review it, it's a simple change to these two functions"]}, {"number": 6857, "title": "Remove unused flag reading data example", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks!\r\n\r\nJenkins, test this please.", "@drpngx Some tests timeouts, It is possible to rerun the tests? Or we can just ignore those? This pull request should not cause any test failure.", "Yeah these are known."]}, {"number": 6856, "title": "Remove unused variable in reading data examples", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "This is close because I commit with wrong email address"]}, {"number": 6855, "title": "Installation error from source code with cuda7.5", "body": "For bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### Environment info\r\nOperating System:\r\n     Ubuntu 14.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n~$ ls -l /usr/local/cuda/lib64/libcud*\r\n-rw-r--r-- 1 root root 322936  8\u6708 16  2015 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root     16  8\u6708 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\r\nlrwxrwxrwx 1 root root     19  8\u6708 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\r\n-rwxr-xr-x 1 root root 383336  8\u6708 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\r\n-rw-r--r-- 1 root root 720192  8\u6708 16  2015 /usr/local/cuda/lib64/libcudart_static.a\r\n\r\n\u3010cudnn\u3011:\r\n/usr/local/include/cudnn.h\r\n/usr/local/lib/libcudnn.so.5\r\n/usr/local/lib/libcudnn.so.5.0.5\r\n\r\n/home/guorui/soft/cudnn_v5/cuda/include/cudnn.h\r\n/home/guorui/soft/cudnn_v5/cuda/lib64/libcudnn.so\r\n/home/guorui/soft/cudnn_v5/cuda/lib64/libcudnn.so.5\r\n/home/guorui/soft/cudnn_v5/cuda/lib64/libcudnn.so.5.0.5\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\ntensorflow version: download 2017.1.12\r\n2. The output of `bazel version`\r\nbazel-0.4.3-installer-linux-x86_64.sh\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n$ ./configure\r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] N\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] N\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] N\r\nNo XLA JIT support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\nUsing python library path: /usr/local/lib/python2.7/dist-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] N\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: \r\nPlease specify the location where CUDA  toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: \r\nPlease specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /home/guorui/soft/cudnn_v5/cuda\r\nlibcudnn.so resolves to libcudnn.5\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 5.2\r\n\r\n#Create the pip package and install\r\nbazel build -c opt --config=cuda --local_resources 2048,.5,1.0 //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\u3010error message\u3011:\r\nINFO: From Compiling tensorflow/core/kernels/cwise_op_gpu_mul.cu.cc:\r\nKilled\r\nERROR: /home/guorui/soft/code/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2081:1: output 'tensorflow/core/kernels/_objs/cwise_op_gpu/tensorflow/core/kernels/cwise_op_gpu_mul.cu.pic.o' was not created.\r\nERROR: /home/guorui/soft/code/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2081:1: not all outputs were created or valid.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1555.053s, Critical Path: 1466.64s\r\n\r\n\r\nThank you very much.\r\n", "comments": ["I have found the problem.\r\n\r\nJust add: --local_resources 2048,.5,1.0"]}, {"number": 6854, "title": "XLA: random numbers are the same across `session.run` calls", "body": "Not sure if that's intended, but that changes behavior of training pipelines:\r\n\r\n```\r\nfrom tensorflow.contrib.compiler import jit\r\njit_scope = jit.experimental_jit_scope\r\nwith jit_scope(compile_ops=True):\r\n    x = tf.random_uniform(())\r\n    \r\nsess = tf.Session()\r\nprint(sess.run(x))\r\nprint(sess.run(x))\r\n```\r\n\r\nOutput:\r\n```\r\n0.768917\r\n0.768917\r\n```", "comments": ["@leary for comment if that's normal", "That's definitely not working as intended. We don't seem to be setting the RNG seed correctly, which is why you get the same result.\r\n\r\nI'll check in a change disabling JIT compilation of the RNG ops until they are fixed.", "Actually, it turns out that Tensorflow is constant-folding away the _XlaLaunch ops containing the random-number generation code. Fix coming shortly (mark _XlaLaunch as stateful).", "Good to know it's an easy fix. BTW, if you setup the internal github/google email it'll add you to TensorFlow org automatically and I'll be able to assign future XLA issues to you (martin knows the place for the mapping)", "Hry folks,\n  Must have been some typo when you launched this thread. I am not related\nto the tensor flow project. You aren't communicating with who you think you\nare.\n\nOn Jan 15, 2017 9:53 AM, \"Yaroslav Bulatov\" <notifications@github.com>\nwrote:\n\n> Good to know it's an easy fix. BTW, if you setup the internal\n> github/google email it'll add you to TensorFlow org automatically and I'll\n> be able to assign future XLA issues to you (martin knows the place for the\n> mapping)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6854#issuecomment-272703655>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ACV3mIOcM-ZI890MJSfNxuvalDtmuLkbks5rSkESgaJpZM4LjvWp>\n> .\n>\n", "Ah, I see, I can't be assigned. I emailed Martin to add me to the Github org.", "@hawkinsp invited", "Also it is @leary-g I think", "@hawkinsp btw, if you include string like below in CL public description, it'll automatically close the issue when commit is merged:\r\nFixes issue: #6854\r\n(\"Fixes Github issue: #6854\" does not trigger it)\r\n"]}, {"number": 6853, "title": "Propagate seed in parallel_read to readers. Fixes #6735", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 6851, "title": "Have `write_graph` return the output path of file", "body": "Since the function is performing `os.path.join()` for the user, it makes sense to return the location of the final file, or else the user will have to call `os.path.join()` again on their own.", "comments": ["Can one of the admins verify this patch?", "Could we modify the test to check for that?", "Done- I think I've added them in the right place.", "Should I also assert `os.path.exists` is True?", "Hopefully that'll work under windows. Yes, `os.path.exists` is probably a good idea. Once you push that, I'll trigger a test.\r\n", "How's this look? Added in the path existence test, and changed `\"/\".join()` to `os.path.join()` (both in existing code and newly added code) for Windows compatibility.", "Jenkins, test this please.\r\n\r\nLooks good. It would be nice to specify if it's binary/text proto", "Can one of the admins verify this patch?", "NM the comment about text/binary, it's in the arguments."]}, {"number": 6850, "title": "cudnn.h header not found on Ubuntu when using NVidia's CUDNN .debs", "body": "The official libcudnn5-dev package from NVidia (developers.nvidia.com), version 5.1.5-1+cuda-8.0, places the cudnn.h header at\r\n`/usr/include/x86_64-linux-gnu/cudnn_v5.h`\r\n\r\nbut Tensorflow's config script looks for it at:\r\n`/usr/lib/x86_64-linux-gnu/include/cudnn.h`\r\n\r\nwhich means that running Tensorflow's `configure` script fails.\r\n\r\nA workaround is to add a symlink:\r\n```\r\nsudo mkdir /usr/lib/x86_64-linux-gnu/include\r\nsudo ln -s /usr/include/x86_64-linux-gnu/cudnn_v5.h /usr/lib/x86_64-linux-gnu/include/cudnn.h\r\n```\r\nafter which Tensorflow's `configure` script completes successfully.", "comments": ["@davidzchen would it be possible to make cuda_configure.bzl compatible with the NVidia Debian packages?\r\n\r\ncc: @gunan ", "deb package for cudnn is new. We do handle this for CUDA deb packages, we probably need a very similar mechanism for cudnn.", "We are also facing the same issue on powerpc64le. Could anyone please let us know when can we get the fix for it?", "Ping!\r\n@davidzchen @damienmg \r\nhow difficult would it be to add this functionality to cuda_configure.bzl?", "It would probably be a small change like 2 or 3 lines.", "Another possible workaround:\r\n@hawkinsp would it work if you add the path `/usr/lib/x86_64-linux-gnu` to your `LD_LIBRARY_PATH` ?", "@gunan Given the problem is finding the header file, not the .so libraries, I doubt setting LD_LIBRARY_PATH would help.\r\n\r\nNote also that it's not just the path that is different, the header is called \"cudnn_v5.h\" not \"cudnn.h\".\r\n\r\nThis is a configuration we should support without needing workarounds (Ubuntu with NVidia's CUDA/CUDNN packages).", "Submitted a pull request with a possible fix. Works for me on POWER Ubuntu with both tarball and deb package cuDNN installs.", "In my tests, on ubuntu the installed package redirects the header to `/usr/local/cuda/include/cudnn.h` using `/etc/alternatives`.\r\nWhich specific OS are you running into this problem?", "The OS is Ubuntu 16.04 for IBM POWER (ppc64le).  The cuDNN .deb packages are:\r\n\r\nlibcudnn5_5.1.5-1+cuda8.0_ppc64el.deb\r\nlibcudnn5-dev_5.1.5-1+cuda8.0_ppc64el.deb\r\nlibcudnn5-doc_5.1.5-1+cuda8.0_ppc64el.deb\r\n\r\nThe redirect with those packages doesn't involve /usr/local/cuda* at all:\r\n\r\n/usr/include/cudnn.h -> /etc/alternatives/libcudnn -> /usr/include/powerpc64le-linux-gnu/cudnn_v5.h\r\n\r\nAnd update-alternatives offers no other option.", "I am not sure with the above link structure why we would have failures.\r\n@hawkinsp could you confirm after #7151 issue is resolved?\r\nAlso, do you have the link through etc/alternatives as @hartb described (I can confirm that link structure gets built for me when I try in docker)", "Seems fixed for me at head. Closing.\r\n\r\nI also have the the /usr/include/cudnn.h -> /etc/alternatives/libcudnn -> /usr/include/x86_64-linux-gnu/cudnn_v5.h symlink chain as @hartb described.", "Just a heads up. I don't think this is fixed with the newest tag 1.5.0-rc1, you've still gotta make the symbolic link before using bazel to build.\r\n\r\nThis is on ubuntu16.04 with:\r\nBazel 0.9 \r\nCudaNN 7.0.5\r\nCuda9.1", "I am new to linking. I got confused. I did several linking and I think I messed it up. Could anyone give me the exact commands for link chain of /usr/include/cudnn.h -> /etc/alternatives/libcudnn -> /usr/include/x86_64-linux-gnu/cudnn_v5.h?", "@PouriaCh If you can use the .deb packages for the cuDNN install (available for cuDNN 5.1 at https://developer.nvidia.com/rdp/cudnn-archive) then I think the install process should create the links for you. You shouldn't have to do it by hand (and you'd probably want to remove any manually-created links before doing the .deb install).\r\n\r\nWith recent versions of TF, we've been using the cuDNN tarball install. That doesn't create any of those links.  We unpack the tarball so the cuDNN stuff lands under /usr/local/cuda:\r\n\r\n`$ sudo tar -C /usr/local --no-same-owner -xvf cudnn...tgz`\r\n\r\nAnd then inform TF build of the location during the configure step:\r\n\r\n`CUDNN_INSTALL_PATH=/usr/local/cuda-x.y`\r\n\r\nThat's been working for us (on Power, rather than x86, but I think should be the same)."]}, {"number": 6849, "title": "Regression error (worked in 0.8): DNNRegressor no longer supports multiple output neurons ", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nhttp://stackoverflow.com/questions/34224826/skflow-regression-predict-multiple-values\r\n\r\n### Environment info\r\nOperating System:\r\nMac\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\nTried in both:\r\n0.12.1\r\n1.0.0-rc1\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nI had implemented code based in TF 0.8 that used multiple output neurons.  I based it on this stack overflow question:\r\n\r\n http://stackoverflow.com/questions/34224826/skflow-regression-predict-multiple-values\r\n\r\nIt looks like this has been broken since at least 0.10 based on comments above.  I accounted for all of the TF breaking changes and upgraded the code provided by @ilblackdragon and came up with:\r\n```\r\nimport numpy as np\r\nimport tensorflow.contrib.learn as skflow\r\nimport tensorflow as tf\r\nfrom sklearn.metrics import mean_squared_error\r\n\r\n# Create random dataset.\r\nrng = np.random.RandomState(1)\r\nX = np.sort(200 * rng.rand(100, 1) - 100, axis=0)\r\ny = np.array([np.pi * np.sin(X).ravel(), np.pi * np.cos(X).ravel()]).T\r\n\r\n# Fit regression DNN model.\r\nfeature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=X.shape[0])]\r\nregressor = skflow.DNNRegressor(hidden_units=[5, 5],feature_columns=feature_columns)\r\nregressor.fit(X, y)\r\nscore = mean_squared_error(regressor.predict(X), y)\r\nprint(\"Mean Squared Error: {0:f}\".format(score))\r\n```\r\n\r\nBut this results in:\r\n\r\nValueError: Shapes (?, 1) and (?, 2) are incompatible\r\n\r\nHas the important feature of multiple outputs been removed mistakenly somehow?\r\n\r\nAlso it looks like the example that @ilblackdragon was removed, the link in GitHub 404's.\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nUpgraded code to account for breaking changes in TF, but still get an error.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["Thanks for reaching out. A lot of time has passed since 0.8. As you know, the API is not going to become stable until 1.0. I'm not sure what version you're currently using. The release history should document the breaking changes that have occurred since. https://github.com/tensorflow/tensorflow/releases Beyond that, you may want to consider [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) for community-driven support. If you're confident this is a bug in TensorFlow, I will re-open this issue provided more information.", "I do believe it to be a bug.  As stated above, I am using 0.12.1.  I did look at the breaking changes and did not see anything that covers this.\r\n\r\nMultiple output neurons is an important feature for neural networks.  One of the core maintainers of **contrib.learn** added this feature, as discussed on stackoverflow here:  \r\n\r\n[http://stackoverflow.com/questions/34224826/skflow-regression-predict-multiple-values](http://stackoverflow.com/questions/34224826/skflow-regression-predict-multiple-values)\r\n\r\nOthers have also asked about this on the stack overflow link above, but there has been no additional information given.  Basically I'd like to know if this feature (multiple regression outputs) has been dropped, or more likely, other changes have broken DNNRegressor.  ", "Thanks for the additional information. As promised, I'm re-opening, and bringing your concerns to the attention of @sandersk who should be able to take a closer look.", "Thanks!  It might be worth pinging @ilblackdragon  , as he implemented the original feature, in response to the stack overflow question above.  As you can see from the question, several others have run into issues after additional checks were added to DNNRegressor, which broke this feature.  It is nice to be able to have multiple outputs without the need to setup a custom graph.  ", "I tested this in 1.0.0-rc1 and has the problem as well.", "`DNNRegressor` has been completely rewritten since then, so I'm closing this as obsolete.", "No problem... I moved on to Keras."]}, {"number": 6848, "title": "temporarily add /sbin to PATH for ldconfig call", "body": "For issue #6762 (keeping the PATH changes local to where it is used instead of making it global as initially suggested).", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "This is a trivial code change addressing a minor problem. It does not have anywhere near the depth to be protected by copyright or requiring a licence to be copied or used.\r\n\r\nIf you insist on your CLA, I'm sorry I don't have time to review it at this time. You can easily implement an alternative solution such as the one paraphrased in issue #6762 .\r\n\r\nGood luck! ", "CLA is required for all contributions. Feel free to reopen if this is solved."]}, {"number": 6847, "title": "Better documentation for tf.extract_image_patches", "body": "In this [issue](https://github.com/tensorflow/tensorflow/issues/6743#issuecomment-272525783) someone requested a reverse operation to `tf.extract_image_patches`. The comments suggest that there exists a [gradient operation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_grad.py#L575) for this purpose which was added with [this PR](https://github.com/tensorflow/tensorflow/pull/3672).\r\n\r\nUnfortunately it is not obvious how to apply this gradient operation and there are no information on this in the [api docs](https://www.tensorflow.org/versions/master/api_docs/python/array_ops/slicing_and_joining#extract_image_patches) therefor I request to add an example to [tf.extract_image_patches](https://github.com/tensorflow/tensorflow/blob/a4c8df209d7413068f4ed3e71c43eb798fbd5580/tensorflow/g3doc/api_docs/python/functions_and_classes/shard4/tf.extract_image_patches.md) where one transforms an `image` tensor of shape `(image_height, image_width, channels)` to `(patch_num, patch_height, patch_width, channels)` and vice versa.\r\n\r\nThank you in advance!", "comments": ["@dr4b @vrv Want to bring to your attention that two users have requested better documentation for ExtractImagePatches, which currently has a one-liner doc. I think more accessible documentation is always a good thing. I've marked it \"contributions welcome\" but feel free to self-assign if it's something you're interested in doing.", "@wolffg This still seems to be an issue. Could you take a look?", "@wolffg Is it possible to use tf.extract_image_patches as a not-trainable 2d convolution layer? how will the loss be computed with mixed tf.extract_image_patches and 2d convolutions?", "I think @vrv is more likely to be able to clear this up than I.", "@agniszczotka: if you don't want to have backprop through extract_image_patches, wrap the output of tf.extract_image_patches() in tf.stop_gradient().\r\n\r\nI think there's two issues being conflated here:\r\n1) extract_image_patches doesn't have good documentation, and should be improved.  I would love one of the authors or maintainers of this code to add some.  @gpapan and @benoitsteiner ?\r\n\r\n2) It sounds like people want access to the gradient function of tf.extract_image_patches() without having to use it as a gradient (essentially a reduction) as part of the API with corresponding documentation.  This is similar to people who want 'deconvolution' (the backward pass of convolution) as its own API.  Such a feature is possible just like with deconvolution, it just requires writing the code and exposing it in the API after getting API approval.", "@vrv  \"2. It sounds like people want access to the gradient function of tf.extract_image_patches() without having to use it as a gradient (essentially a reduction). This is similar to people who want 'deconvolution' (the backward pass of convolution) as its own API. \"\r\n\r\nThis is a quite common use-case for many ops even  beyond 'deconvolution'  and 'extract_image_patches()'. Do you think it would be possible to great a general API, which lets you (easily) evaluate any op  (or even subgraph) backwards? \r\n\r\nThe api could work similar to pytorchs `.backward()` call which can be applied to subgraphs. \r\n\r\n\r\n", "@alextp What do you think?\r\n\r\nThe equivalent of .backward() might be exposing ops.get_gradient_function(), and would provide a mechanism to get the gradient of an op, but not as part of a documented API.  Perhaps that is good enough though?", "You can get the equivalent of .backward() now for any tf op by doing something like tfe.gradients_function(tf.extract_image_patches)(arguments_to_image_patches ,..., dy=arguments_to_the_gradient_function) if tf.extract_image_patches has a gradient defined. When executing graphs this will only compute the forward pass if the backward function needs it (like how the gradient of exp or the gradient of relu uses the output of relu).", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Closing this issue, as `tf.extract_image_patches` is no longer supported in TensorFlow 2.0.", "@dynamicwebpaige The reason for closing it is not valid because `tf.extract_image_patches` is not removed but just renamed to `tf.image.extract_patches`. I suggest edit the tile to \"reverse op for `tf.image.extract_patches`\" rather than closing it. This is not solved yet."]}, {"number": 6846, "title": "tf.contrib.losses.sparse_softmax_cross_entropy not normal", "body": "When I tried to use tf.contrib.losses.sparse_softmax_cross_entropy with weights for losses, I got the error:\r\n\r\nValueError: weight.get_shape().ndims cannot be None\r\n\r\nLater, I used tf.contrib.losses.softmax_cross_entropy and onehot labels,  everything was ok.  Is there anything wrong with tf.contrib.losses.sparse_softmax_cross_entropy?\r\n\r\nThanks a lot!\r\n\r\n", "comments": ["I have encountered the issue before.\r\n\r\nThe problem is that if `weights` has unknown shape (for example if it is `[None]`), the following line (referenced current head):\r\nhttps://github.com/tensorflow/tensorflow/blob/333dc32ff79af21484695157f3d141dc776f7c02/tensorflow/contrib/losses/python/losses/loss_ops.py#L424\r\ncauses the resulting shape to be `None` (because the number of dimensions after the `squeeze` is unknown at construction time). Then the code in `compute_weighted_loss` throws an error.\r\n\r\nThe solution is to remove the referenced line (you can see that it is not in `softmax_cross_entropy` call), because it interferes with the code for dealing with different shapes of `weights` present in `compute_weighted_loss`.", "Thank you for helping our friend @foxik.", "BTW, are you planning to fix the issue, i.e., remove the offending line from tensorflow/contrib/losses/python/losses/loss_ops.py? I could not find any corresponding pull request (I suppose it would be referenced here automatically if it was created).", "Re-opening for triage again.", "@foxik , thanks for your detailed explanation!", "@foxik it looks like you have a good handle on this, would you mind sending a PR to fix this?", "Working on it."]}, {"number": 6845, "title": "input pipeline spends all time in QueueDequeueMany", "body": "My training process use tfrecord format for train&eval dataset .\r\nI test the benchmark of reader , only 8000records/second.  and io speed(see from iotop command) just 400KB-500KB/s. \r\nI'm using the cpp version of protobuf here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#protobuf-library-related-issues\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttps://github.com/tensorflow/tensorflow/issues/4467\r\nhttps://github.com/tensorflow/tensorflow/issues/4732\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n```\r\ndef read_and_decode(filename_queue):\r\n     reader = tf.TFRecordReader()\r\n    _, serialized_example = reader.read(filename_queue)\r\n    return serialized_example\r\n```\r\n\r\n```\r\n  serialized_example = read_and_decode(filename_queue)\r\n  batch_serialized_example = tf.train.shuffle_batch(\r\n      [serialized_example],\r\n      batch_size=batch_size,\r\n      num_threads=thread_number,\r\n      capacity=capacity,\r\n      min_after_dequeue=min_after_dequeue)\r\n  features = tf.parse_example(\r\n      batch_serialized_example,\r\n      features={\r\n          \"label\": tf.FixedLenFeature([], tf.float32),\r\n          \"ids\": tf.VarLenFeature(tf.int64),\r\n          \"values\": tf.VarLenFeature(tf.float32),\r\n      })\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nI try to set num_threads in  tf.train.shuffle_batch but not working. It seems that when set to 2 threads, it work at 8000records/s, when enlarge the thread number, it get slower. (I remove all ops that cost cpus. Just read data.) \r\nMy sever are 24 core cpus.  \r\n", "comments": ["here is my benchmark script and timeline result (timeline.json original file inlcude)\r\nhttps://gist.github.com/ericyue/7705407a88e643f7ab380c6658f641e8", "https://github.com/tensorflow/tensorflow/issues/4732\r\nI read this issue that you guys talk about, seems the same problem. I'am using the latest version of tensorflow , but it's still unsolved.\r\n@gunan  @ebrevdo  @yaroslavvb ", "![screen shot 2017-01-14 at 11 15 56 pm](https://cloud.githubusercontent.com/assets/918889/21956027/ec463fbe-dab1-11e6-9670-9f75f1b21e6d.png)\r\n", "It's not clear if it's a bug in TensorFlow or somewhere else....can you give a reproducible example? (ie, I can't run your example because it's missing the input files). \r\n\r\nAlso, the time Dequeue op takes is surprisingly similar to the time in https://github.com/tensorflow/tensorflow/issues/4740 . The issue there was queue starvation. Python thread scheduler always scheduled compute thread ahead of data prefetching thread, so at every step the compute thread was blocked in Dequeue, and the time to context switch, read single example and switch back to compute was about 10 ms.\r\n\r\nSome explanation [here](http://stackoverflow.com/a/39842628/419116)\r\n\r\nCan you rule out queue starvation? (ie, increase queue size to fit the entire dataset, start queue runners, do time.sleep in main thread, and print(sess.run(queue.size())) to make sure queue is filled up before running computation)", "@yaroslavvb    I upload a part of my dataset here.\r\n[20161230_part-00390.output.pb.zlib.zip](https://github.com/tensorflow/tensorflow/files/706423/20161230_part-00390.output.pb.zlib.zip)\r\n\r\n1) increase queue size to fit the entire dataset\r\nthe \"queue size \" you mentioned is `capacity` in `tf.train.shuffle_batch()` ?\r\nin my script, `capacity = thread_number * batch_size + min_after_dequeue`, I tried increase min_after_dequeue or batchsize to a large value but it didn't help.\r\n2) `print(sess.run(queue.size()))`   how to get queue variable in my script?", "You could just add time.sleep(30) after starting queue runners and checking\nif it makes things faster. If not and get timeline for one of the first few\niterations\n\nOn Jan 14, 2017 8:27 PM, \"Eric Yue\" <notifications@github.com> wrote:\n\n> @yaroslavvb <https://github.com/yaroslavvb> I upload a part of my dataset\n> here.\n> 20161230_part-00390.output.pb.zlib.zip\n> <https://github.com/tensorflow/tensorflow/files/706423/20161230_part-00390.output.pb.zlib.zip>\n>\n>    1. increase queue size to fit the entire dataset\n>    the \"queue size \" you mentioned is capacity in tf.train.shuffle_batch()\n>    ?\n>    in my script, capacity = thread_number * batch_size + min_after_dequeue,\n>    I tried increase min_after_dequeue or batchsize to a large value but it\n>    didn't help.\n>    2. print(sess.run(queue.size())) how to get queue variable in my\n>    script?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6845#issuecomment-272672950>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABaHJ2DAlp948wIIJ3YozsdPQPlaBTaks5rSaAYgaJpZM4Ljje7>\n> .\n>\n", "@yaroslavvb   I do as you said.    \r\nAfter I add `time.sleep` and increase `capacity` in tf.batch, the speed changed from 7k/s to 70k/s. \r\nBut I find two things :   \r\n1) the speed become slower with the time elapse.  \r\n```\r\n[2017-01-15 12:54:40] time[  0.11] step[        30] speed[265345]\r\n[2017-01-15 12:54:40] time[  0.12] step[        60] speed[256539]\r\n[2017-01-15 12:54:40] time[  0.11] step[        90] speed[274768]\r\n[2017-01-15 12:54:40] time[  0.12] step[       120] speed[257424]\r\n[2017-01-15 12:54:40] time[  0.11] step[       150] speed[262851]\r\n[2017-01-15 12:54:41] time[  0.11] step[       180] speed[262207]\r\n[2017-01-15 12:54:41] time[  0.12] step[       210] speed[255981]\r\n[2017-01-15 12:54:41] time[  0.25] step[       240] speed[118329]\r\n[2017-01-15 12:54:42] time[  1.17] step[       270] speed[ 25723]\r\n[2017-01-15 12:54:46] time[  3.86] step[       300] speed[  7762]\r\n[2017-01-15 12:54:50] time[  3.99] step[       330] speed[  7524]\r\n```\r\n2) when I increase the thread number , the speed become slow.  the best number is  2.\r\nMy server are 24cores cpu, and the disk read speed are only 400KB/s in average (see from iotop command )\r\n```\r\n[2017-01-15 12:56:04] time[  0.26] step[        30] speed[115136]\r\n[2017-01-15 12:56:04] time[  0.27] step[        60] speed[109339]\r\n[2017-01-15 12:56:04] time[  0.27] step[        90] speed[110254]\r\n[2017-01-15 12:56:05] time[  0.32] step[       120] speed[ 92811]\r\n[2017-01-15 12:56:05] time[  0.29] step[       150] speed[102403]\r\n[2017-01-15 12:56:05] time[  0.27] step[       180] speed[109489]\r\n[2017-01-15 12:56:07] time[  2.08] step[       210] speed[ 14409]\r\n[2017-01-15 12:56:12] time[  4.84] step[       240] speed[  6195]\r\n[2017-01-15 12:56:17] time[  4.75] step[       270] speed[  6316]\r\n[2017-01-15 12:56:22] time[  4.78] step[       300] speed[  6282]\r\n```", "and if the reason why it's slow is queue starvation, how to solve this? manually time.sleep every batch? is there any better solution ?", "I have read the post you answered http://stackoverflow.com/questions/39840323/benchmark-of-howto-reading-data/39842628#39842628\r\nyou said \r\n```\r\nThe solution is to increase number of Python threads,\r\n increase queue size to fit the entire dataset, \r\nstart queue runners, \r\nand then pause main thread for a couple of seconds to give queue runners to pre-populate the queue.\r\n```\r\nso my question is : \r\n1\u3001 The solution is to increase number of Python threads\r\ndo you mean increase the thread number in `tf.train.shuffle_batch()` ? I tried it . but not working . increase the number of thread can descrease the speed.\r\n\r\n2\u3001 increase queue size to fit the entire dataset\r\nincrease the capacity in `tf.train.shuffle_batch()` ?  It seems that I didn't use queue directly, so i confused that what is the `queue size` you mean.\r\n\r\n3\u3001pause main thread for a couple of seconds\r\nthis maybe work, but only a few second later it's slow again.\r\n", "BTW in iotop command , I can only see `one` tensorflow `thread` reading disk io when I set thread number in tf.shufle_batch larger than 2 (only a few times it show 2 threads. ) is there a bug in multi-threading with tf.shuffle_batch ?   @yaroslavvb ", "I suppose this is a \"bug\" in how Python works. What happens when main compute thread session.run call is too fast (ie, <2ms), then Python does not switch to the other reading read and will keep running your compute thread until it runs out of data to consume. At which point it'll switch to \"read one example-run one train step-read another example\", which is much less efficient than reading many examples before switching threads to do computation on them.\r\n\r\nThis is not a common scenario because usually: train_op is slow/compute-intensive, and the pipeline is bottlenecked by compute part rather than IO part.\r\n\r\nIf adding `time.sleep` helps at first, and then training slows down again, suggests that enqueing part of the pipeline has lower throughput than your dequeing half.\r\n\r\nIt could mean that your \"dequeing\" (ie training part) is fundamentally easy, and decoding can't keep up. It could mean that decoding examples is inefficient and could be sped up. So you could try benchmarking just the decoding part (ie, start queue runners, and see how fast they can add things to queue), and try to estimate if the speed at which this happens is close to maximum attainable", "sorry I can not understand. You means the main reason it slows is because the dequeing half too fast?  In my acutal script , the training process is a 4-layer network each with 128 hidden unit. I think it's complex enough .    \r\nI'm confused. Increase the thread number can not speed up , so it maybe that the queue is always full, and the dequeing part is slow ?  it's not match with your thought about dequeing is too fast.\r\n\r\nis there any other solution can I use to speed up ?", "I'll try running your example on Monday, but meanwhile you could try benchmarking just the enqueuing part of the pipeline (no training, just adding data to queue) and seeing how fast it is.", "thanks.  The script I provide is just without training. it's just read from file and get from queue. \r\nI didn't know how to measure the time only adding data because I cannot get the timestamp when the queue is full. I'm not using the `Queue` directly.", "You can call queue size operation directly from graph, ie\r\n\r\nprint(tf.get_default_graph().as_graph_def())\r\n...\r\nnode {\r\n  name: \"batch/fifo_queue_Size\"\r\n  op: \"QueueSize\"\r\n  input: \"batch/fifo_queue\"\r\n...\r\nthen you can do `sess.run(\"batch/fifo_queue_Size\")` to get number of elements in that queue", "there's a \"shuffle_batch/random_shuffle_queue_Size\", but value keeps `None`.\r\n\r\n", "Correction, sess.run(\"batch/fifo_queue_Size:0\"). Also getting timeline for actual enqueue step will tell where true slowness is, maybe also combined with CPU profile from Google perf tools", "BTW, when I run your example on MacBook everything seems to be running consistently between 13k and 17k examples per second\r\n\r\n```\r\n[2017-01-15 21:03:09] time[  0.12] step[        20] speed[ 17000]\r\n[2017-01-15 21:03:09] time[  0.14] step[        40] speed[ 14747]\r\n[2017-01-15 21:03:10] time[  0.14] step[        60] speed[ 14788]\r\n[2017-01-15 21:03:10] time[  0.14] step[        80] speed[ 14053]\r\n[2017-01-15 21:03:10] time[  0.14] step[       100] speed[ 14386]\r\n[2017-01-15 21:03:10] time[  0.13] step[       120] speed[ 14964]\r\n[2017-01-15 21:03:10] time[  0.14] step[       140] speed[ 14594]\r\n[2017-01-15 21:03:10] time[  0.13] step[       160] speed[ 15027]\r\n[2017-01-15 21:03:10] time[  0.14] step[       180] speed[ 14646]\r\n```", "I thinks the speed on your mbp is still slow because add a time.sleep will increase a lot.\r\nAnd I try on my macbook pro too, without time.sleep (MacBook Pro (Retina, 13-inch, Early 2015))\r\nI'm wondering why the different come from.  \r\nI'm using tensorflow 0.12.1 and install from pip. And using the cpp-implemention protobuf in python.\r\n\r\n```\r\n[2017-01-15 21:45:32] time[  0.26] step[        20] speed[  7772]\r\n[2017-01-15 21:45:32] time[  0.38] step[        40] speed[  5326]\r\n[2017-01-15 21:45:33] time[  0.50] step[        60] speed[  3962]\r\n[2017-01-15 21:45:33] time[  0.37] step[        80] speed[  5388]\r\n[2017-01-15 21:45:34] time[  0.54] step[       100] speed[  3703]\r\n[2017-01-15 21:45:34] time[  0.62] step[       120] speed[  3251]\r\n[2017-01-15 21:45:35] time[  0.34] step[       140] speed[  5893]\r\n[2017-01-15 21:45:35] time[  0.27] step[       160] speed[  7454]\r\n[2017-01-15 21:45:35] time[  0.27] step[       180] speed[  7323]\r\n[2017-01-15 21:45:36] time[  0.48] step[       200] speed[  4145]\r\n[2017-01-15 21:45:36] time[  0.34] step[       220] speed[  5958]\r\n[2017-01-15 21:45:37] time[  0.42] step[       240] speed[  4734]\r\n[2017-01-15 21:45:37] time[  0.49] step[       260] speed[  4100]\r\n```\r\n\r\n", "@yaroslavvb   hi, I tried as you said to try benchmarking just the enqueuing part of the pipeline, the result is 7058records/s on my macbook.\r\nI'm using a large min_after_dequeue to ensure 5 seconds can not load all the records.\r\n```\r\n    num1 = sess.run(\"shuffle_batch/random_shuffle_queue_Size:0\")\r\n    start_time = datetime.datetime.now(pytz.timezone('Asia/Shanghai'))\r\n    threads = tf.train.start_queue_runners(coord=coord, sess=sess)\r\n    time.sleep(5)\r\n    end_time = datetime.datetime.now(pytz.timezone('Asia/Shanghai'))\r\n    sec = (end_time - start_time).total_seconds()\r\n    num2 = sess.run(\"shuffle_batch/random_shuffle_queue_Size:0\")\r\n    speed = (num2-num1)/sec\r\n```", "I tried on my server , the enqueuing speed is 7567/s . \r\nSo it seems that the bottleneck  is read speed ?   Why increase thread number `can not` speed up ?", "@yaroslavvb You've offered some really excellent guidance in this thread. The problem appears to be resolved from my perspective. I'm going to close this out since I'm not seeing evidence of a bug in TensorFlow. But by all means, feel free to continue discussing.", "I'm sorry but where the solution above ? \r\nThe every solution in stackoverflow or older issue @yaroslavvb  provided  is not possible. I try all of them.  \r\nI do speed up by add a time.sleep in the beginning , but a seconds later it slower again. So this is the solution ? \r\nI think a kind of pool performance is a bug. @yart", "I didn't know why increase thread number can not speed up  enqueuing speed .  \r\ncan you try to use my script on your macbook ? \r\njust try thread number 1 and 5.  I do this on 3 server and 2 macbooks. \r\nThe result shows that increase thread do decrease the speed. \r\n@yaroslavvb ", "@yaroslavvb was unable to reproduce the performance decline you describe. The problem seems related to how TensorFlow is being used w.r.t. Python scheduling rather than a bug in TensorFlow itself. As for increasing threads decreasing speed, that's because GIL causes Python threads to have negatively scalability. For many types of computations, particularly CPU-bound ones, 1 + 1 < 1.", "Thanks for reply. \r\nHe reproduced speed at 13k-17k , but I think if he add `time.sleep` in the main thread it may speed up to 60k or higher.  \r\nAfter many talks, I do thinks the bottleneck is enqueuing speed. The only thing I want to know is how to speed up enqueuing.  \r\nSo do you think the enqueuing speed at 13k-17k is a reasonable speed?  If so I will accept it .\r\nAnd I am wondering that if it is a python-gil issue , Why tensorflow support multi-thread or something to support a useless (it seems slower than single thread) function ? ", "ok, forget it. even though can not solved the problem, but I do realized more about the tf. \r\nI will try to dig more and see if I can find the reason. \r\n\r\n@yaroslavvb  @jart   Thanks for your help :) ", "The TensorFlow team is glad to help. If you manage to pinpoint the root cause of a performance issue, and are able to help us understand what specific part needs to be improved, please report back and we'll be more than happy to listen.", "@ericyue this issue is that your pipeline is dominated by overhead of `session.run` call. The solution is to use `tf.batch(...dequeue_many=True)` to have less `.run` calls being generated. With this change, your benchmark gets 10x speed-up, lets move discussion to stackoverflow, put more details [here](http://stackoverflow.com/questions/41647784/tfrecordreader-seems-extremely-slow-and-multi-threads-reading-not-working/41688401#41688401)", "@mrry -- this is an interesting use-case to keep in mind for your input pipelines work. To summarize, with current situation, if you have an input pipeline that produces >10k items/second in an intermediate stage, that's going to cause 10k `session.run(enqueue_op)` calls/second which is too much for Python to handle. An automatic solution would batch this up into fewer `enqueue_many` calls", "Sorry to comment on a closed thread. I believe I'm running into a similar issue since I have a relatively simple feed-forward network with lots of examples that I want to feed through it. I've generated a trace file which shows my process dominated by QueueDequeueMany, and the GPUs oscillate between 0% and 30-60% usage. I have tried different `time.sleep` settings to give the queue time to fill, increased the number of preprocess threads, and increased the batch size, but none have had a noticeable effect.\r\n\r\nI am trying the `tf.train.batch(..., enqueue_many=True)` approach outlined above, but I get errors when I start using multiple GPUs.\r\n```\r\n\r\nE tensorflow/core/util/events_writer.cc:62] Could not open events file: /data_full/output/full/checkpoint-events/train/events.out.tfevents.1484785002.server: Resource exhausted: /data_full/output/full/checkpoint-events/train/events.out.tfevents.1484785002.server\r\nE tensorflow/core/util/events_writer.cc:95] Write failed because file could not be opened.\r\nE tensorflow/core/util/events_writer.cc:62] Could not open events file: /data_full/output/full/checkpoint-events/train/events.out.tfevents.1484785004.server: Resource exhausted: /data_full/output/full/checkpoint-events/train/events.out.tfevents.1484785004.server\r\nE tensorflow/core/util/events_writer.cc:95] Write failed because file could not be opened.\r\n\r\n```\r\n\r\nThe process continues then exits before completing any steps with the following error originating from where I called `tf.train.batch(..., enqueue_many=True)`:\r\n\r\n```\r\n  File \"/home/ubuntu/.virtualenvs/test-six3/local/lib/python2.7/site-packages/tensorflow/python/training/input.py\", line 692, in batch\r\n  File \"/home/ubuntu/.virtualenvs/test-six3/local/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py\", line 458, in dequeue_many\r\n    self._queue_ref, n=n, component_types=self._dtypes, name=name)\r\n  File \"/home/ubuntu/.virtualenvs/test-six3/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 1099, in _queue_dequeue_many\r\n  File \"/home/ubuntu/.virtualenvs/test-six3/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\r\n  File \"/home/ubuntu/.virtualenvs/test-six3/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/ubuntu/.virtualenvs/test-six3/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nOutOfRangeError (see above for traceback): FIFOQueue '_182_tower_0/batch/fifo_queue' is closed and has insufficient elements (requested 512, current size 0)\r\n\t [[Node: tower_0/batch = QueueDequeueMany[_class=[\"loc:@tower_0/batch/fifo_queue\"], component_types=[DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](tower_0/batch/fifo_queue, tower_0/batch/n/_507)]]\r\n\t [[Node: tower_1/embedding/end_location_shared_embedding/end_location_shared_embeddingweights/Reshape_1/shape/_378 = _HostSend[T=DT_INT32, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:1\", send_device_incarnation=1, tensor_name=\"edge_519_tower_1/embedding/end_location_shared_embedding/end_location_shared_embeddingweights/Reshape_1/shape\", _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/embedding/end_location_shared_embedding/end_location_shared_embeddingweights/Reshape_1/shape)]]\r\n```\r\nIn different runs it references different tensor names, here `.../embedding/end_location_shared_embedding...`, but in previous runs it has referenced other embedding variables. These are created with `tf.contrib.layers.[shared_]embedding_column[s](...)`. Perhaps that is causing a problem on multiple GPUs.\r\n\r\nI do not get this error if I set `num_gpus=1`. I need multiple GPUs because speed is an issue due to the quantity of data I am feeding through it.\r\n\r\nIs there something I need to do differently to use `enqueue_many=True` with multiple GPUs?", "A quick google search shows \"Resource Exhausted\" on writing event file can\nbe caused by running out of memory\nhttp://stackoverflow.com/questions/38340070/after-500-steps-tensorflow-fail-to-write-summaries\n\nOn Wed, Jan 18, 2017 at 4:38 PM, Erik @ MileIQ <notifications@github.com>\nwrote:\n\n> Sorry to comment on a closed thread. I believe I'm running into a similar\n> issue since I have a relatively simple feed-forward network with lots of\n> examples that I want to feed through it. I've generated a trace file which\n> shows my process dominated by QueueDequeueMany, and the GPUs oscillate\n> between 0% and 30-60% usage. I have tried different time.sleep settings\n> to give the queue time to fill, increased the number of preprocess threads,\n> and increased the batch size, but none have had a noticeable effect.\n>\n> I am trying the tf.train.batch(..., enqueue_many=True) approach outlined\n> above, but I get errors when I start using multiple GPUs.\n>\n>\n> E tensorflow/core/util/events_writer.cc:62] Could not open events file: /data_full/output/full/checkpoint-events/train/events.out.tfevents.1484785002.server: Resource exhausted: /data_full/output/full/checkpoint-events/train/events.out.tfevents.1484785002.server\n> E tensorflow/core/util/events_writer.cc:95] Write failed because file could not be opened.\n> E tensorflow/core/util/events_writer.cc:62] Could not open events file: /data_full/output/full/checkpoint-events/train/events.out.tfevents.1484785004.server: Resource exhausted: /data_full/output/full/checkpoint-events/train/events.out.tfevents.1484785004.server\n> E tensorflow/core/util/events_writer.cc:95] Write failed because file could not be opened.\n>\n>\n> I do not get this error if I set num_gpus=1. I need multiple GPUs because\n> speed is an issue due to the quantity of data I am feeding through it.\n>\n> Is there something I need to do differently to use enqueue_many=True with\n> multiple GPUs?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6845#issuecomment-273647478>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABaHDegEBsiCB9GBEySZEzo5vmC3LdMks5rTrCbgaJpZM4Ljje7>\n> .\n>\n", "Thanks for the quick response @yaroslavvb. \r\n\r\nI just now discovered [tf.ReaderBase.read_up_to](https://www.tensorflow.org/api_docs/python/io_ops/readers#ReaderBase.read_up_to), which seems to solve the problem without the extra code of `queue_batch = [] ... queue_batch.append(serialized) ... tf.train.batch(..., enqueue_many=True)`. Now reading is able to keep up with the training and the queue grows over time instead of quickly depleting.\r\n\r\n```\r\n_, serialized = reader.read_up_to(filename_queue, batch_size)\r\nbatch = tf.train.batch([serialized], batch_size=batch_size, ..., enqueue_many=True)\r\n```\r\n\r\n----\r\nFor posterity:\r\nI believe the Resource Exhausted error was a result of a race condition, since it was resolved by adding a sleep command before creating a tf.summary.FileWriter object.\r\n\r\nStepping back, the reason why I believe I have the same issue as described here: when I increase the batch size and time for the queue to fill up, I get much faster speeds when the queue is full (~35k examples/gpu/sec). Once the queue depletes, it falls back to (~5k examples/gpu/sec), which is about what I get with much smaller batch sizes. I also get smaller queue sizes when I increase the number of threads. This leads me to believe that the input is the bottleneck for training.\r\n\r\nHere is the trace at step 1 when the queue is not depleted. UnsortedSegmentSum on the CPU dominates, I think due to the summing of the variables across GPUs.\r\n![image](https://cloud.githubusercontent.com/assets/16887670/22130086/e4b96b94-de5e-11e6-994b-d202d288b1ab.png)\r\n\r\nBy step 200, the queue has been depleted and now the process is dominated by QueueDequeueMany:\r\n![image](https://cloud.githubusercontent.com/assets/16887670/22130127/21d63084-de5f-11e6-9db8-122a7339ebb4.png)", "Glad to see you were able to find a solution.\r\n\r\nYour resource exhausted error may have been due to opening too many files at once and running out of file descriptors. Supervisor has multiple threads doing things concurrently and can cause such issues due to unlucky timing. This is one problem that MonitoredSession/MonitoredTrainingSession were designed to solve -- everything is done in one thread there.\r\n\r\nLong time spent in Dequeue is caused by Enqueue part being too slow. If you add `time.sleep` this masks the symptom, but doesn't solve underlying problem.", "@yaroslavvb  wow! `enqueue_many=True` works great on my training process.  I got `resource  unavailabe ` sometimes.  I'll try `MonitoredSession/MonitoredTrainingSession ` , but it seems designd for  distribution training? ", "what's the difference between `MonitoredTrainingSession` and `MonitoredSession ` ? can not find the docs about these two .", "I wonder if resource unavailable in your case is caused by too much memory. You can try disabling checkpoint writing and summary writing services to see if that helps. MonitoredTrainingSession and MonitoredSession are documented in comments right now in implementation file, main difference is that MonitoredTrainingSession is for distributed training with chief worker."]}]