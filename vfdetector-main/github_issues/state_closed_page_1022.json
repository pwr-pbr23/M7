[{"number": 22679, "title": "1.12-rc0 cherry-pick request: Add missing `import unittest` to control_flow_ops_py_test.py", "body": "PiperOrigin-RevId: 215485333", "comments": []}, {"number": 22678, "title": "1.12-rc0 cherry-pick request: Disable XLA from raspberry pi builds.", "body": "There is no known conceptual reason we can't use XLA, but in practice\r\nwe have some build issues that will need to be fixed.\r\n\r\nPiperOrigin-RevId: 215484942", "comments": []}, {"number": 22677, "title": "Tensorflow serving does not work for TF 1.11.0 ", "body": "\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:No\r\n- **TensorFlow installed from (source or binary)**:Binary with gpu supported\r\n- **TensorFlow version (use command below)**:1.11.0\r\n- **Python version**:python 27\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:9.0\r\n- **GPU model and memory**: ec2 p2.16xlarge 16 NVIDIA K80 GPUs\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nHi all, I already opened this issue in tensorflow-serving but I think this might be a better place to open this issue. Please let know if I need to close the previous one.\r\nI used the code below to test whether tensor-serving works with tensorflow. Here are some combinations and results:\r\nTF 1.10.0 TF-Serving 1.10.0 Success\r\nTF 1.11.0 TF-Serving 1.10.0 Failed\r\nTF 1.11.0 TF-Serving 1.11.0rc1 Failed\r\n\r\n### Source code / logs\r\n```\r\npython tensorflow_serving/example/mnist_saved_model.py /tmp/mnist_model || exit 1\r\n\r\ntensorflow_model_server --port=9000 --model_name=mnist --model_base_path=/tmp/mnist_model/ &\r\nSERVER_PID=$!\r\n\r\npython mnist_client.py --num_tests=1000 --server=localhost:9000 || exit 1\r\n\r\nkill -9 $SERVER_PID\r\n```\r\n\r\nLog\r\n```\r\nTraining model...\r\nSuccessfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\r\nExtracting /tmp/train-images-idx3-ubyte.gz\r\nSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\r\nExtracting /tmp/train-labels-idx1-ubyte.gz\r\nSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\r\nExtracting /tmp/t10k-images-idx3-ubyte.gz\r\nSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\r\nExtracting /tmp/t10k-labels-idx1-ubyte.gz\r\ntraining accuracy 0.9092\r\nDone training!\r\nExporting trained model to /tmp/mnist_model/1\r\nWARNING:tensorflow:From mnist_saved_model.py:139: calling add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPass your op to the equivalent parameter main_op instead.\r\nDone exporting!\r\nExtracting /tmp/train-images-idx3-ubyte.gz\r\nExtracting /tmp/train-labels-idx1-ubyte.gz\r\nExtracting /tmp/t10k-images-idx3-ubyte.gz\r\nExtracting /tmp/t10k-labels-idx1-ubyte.gz\r\n<_Rendezvous of RPC that terminated with (StatusCode.UNAVAILABLE, Connect Failed)>\r\n<_Rendezvous of RPC that terminated with (StatusCode.UNAVAILABLE, Connect Failed)>\r\n<_Rendezvous of RPC that terminated with (StatusCode.UNAVAILABLE, Connect Failed)>\r\n2018-10-02 21:18:32.604303: E external/org_tensorflow/tensorflow/core/common_runtime/executor.cc:697] Executor failed to create kernel. Invalid argument: NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: index_to_string/ToInt64 = Cast[DstT=DT_INT64, SrcT=DT_INT32, Truncate=false, _output_shapes=[[10]], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](index_to_string/range). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n\t [[Node: index_to_string/ToInt64 = Cast[DstT=DT_INT64, SrcT=DT_INT32, Truncate=false, _output_shapes=[[10]], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](index_to_string/range)]]\r\n2018-10-02 21:18:32.604833: E external/org_tensorflow/tensorflow/core/common_runtime/executor.cc:697] Executor failed to create kernel. Invalid argument: NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: index_to_string/ToInt64 = Cast[DstT=DT_INT64, SrcT=DT_INT32, Truncate=false, _output_shapes=[[10]], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](index_to_string/range). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n\t [[Node: index_to_string/ToInt64 = Cast[DstT=DT_INT64, SrcT=DT_INT32, Truncate=false, _output_shapes=[[10]], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](index_to_string/range)]]\r\n2018-10-02 21:18:32.604980: E tensorflow_serving/util/retrier.cc:37] Loading servable: {name: mnist version: 1} failed: Invalid argument: NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: index_to_string/ToInt64 = Cast[DstT=DT_INT64, SrcT=DT_INT32, Truncate=false, _output_shapes=[[10]], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](index_to_string/range). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n\t [[Node: index_to_string/ToInt64 = Cast[DstT=DT_INT64, SrcT=DT_INT32, Truncate=false, _output_shapes=[[10]], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](index_to_string/range)]]\r\n<_Rendezvous of RPC that terminated with (StatusCode.UNAVAILABLE, Connect Failed)>\r\n<_Rendezvous of RPC that terminated with (StatusCode.UNAVAILABLE, Connect Failed)>\r\n<_Rendezvous of RPC that terminated with (StatusCode.DEADLINE_EXCEEDED, Deadline Exceeded)>\r\n<_Rendezvous of RPC that terminated with (StatusCode.UNAVAILABLE, Connect Failed)>\r\n<_Rendezvous of RPC that terminated with (StatusCode.DEADLINE_EXCEEDED, Deadline Exceeded)>\r\n<_Rendezvous of RPC that terminated with (StatusCode.UNAVAILABLE, Connect Failed)>\r\n<_Rendezvous of RPC that terminated with (StatusCode.DEADLINE_EXCEEDED, Deadline Exceeded)>\r\n<_Rendezvous of RPC that terminated with (StatusCode.DEADLINE_EXCEEDED, Deadline Exceeded)>\r\n<_Rendezvous of RPC that terminated with (StatusCode.DEADLINE_EXCEEDED, Deadline Exceeded)>\r\n<_Rendezvous of RPC that terminated with (StatusCode.UNAVAILABLE, Connect Failed)>\r\n```", "comments": ["Duplicate of [this issue](https://github.com/tensorflow/serving/issues/1117). Since this is an issue related to Tensorflow Serving and an issue has been already created by you in the serving repository, request you to track and post observations there. Thank you!"]}, {"number": 22676, "title": "1.12-rc0 cherry-pick request: Make Keras/TPU more robust to closed TF sessions.", "body": "PiperOrigin-RevId: 215313156", "comments": []}, {"number": 22675, "title": "1.12-rc0 cherry-pick request: Disable fused_conv tests that don't build in open-source.", "body": "PiperOrigin-RevId: 215440356", "comments": []}, {"number": 22674, "title": "tf.parse_single_example parses the record incorrectly", "body": "### System information\r\n- **I have written custom code**:\r\n- **OS  -  Linux Ubuntu 16.04**:\r\n- **TensorFlow installed from binary**:\r\n- **TensorFlow version - 1.10.1 (GPU)**:\r\n- **Python version - 3.6.5**:\r\n- **CUDA/cuDNN version - 9.0/7.0**:\r\n\r\n### Issue:\r\nI trying to create tfrecords for my sematic segmentation dataset (rgb_image_in -> binary_raycast_out). \r\n\r\n#### Below is my code to write the list of images to a train.tfrecord.\r\n\r\n\r\n```\r\n        def _process_image_files(image_names, raycast_names):\r\n        \r\n            writer = tf.python_io.TFRecordWriter('train')\r\n    \r\n            #My implementation of decoding jpeg/png image\r\n            coder = ImageCoder()\r\n    \r\n            for i in range(len(image_names)):\r\n                print('{}\\n{}\\n\\n'.format(image_names[i], raycast_names[i]))\r\n\r\n                image_buffer, im_height, im_width, im_channels = _process_image(image_names[i], coder)\r\n\r\n                raycast_buffer, rc_height, rc_width, rc_channels = _process_image(raycast_names[i], coder)\r\n\r\n                example = _convert_to_example(image_names[i], raycast_names[i], image_buffer, raycast_buffer, \\\r\n                                              im_height, im_width, im_channels)\r\n\r\n                writer.write(example.SerializeToString())\r\n            writer.close()\r\n            sys.stdout.flush() \r\n    \r\n    def _process_image(filename, coder):\r\n        with tf.gfile.FastGFile(filename, 'rb') as f:\r\n            image_data = f.read()\r\n\r\n        # Decode the RGB JPEG.\r\n        image = coder.decode_jpeg(image_data)\r\n\r\n        return image_data, height, width, channels\r\n    \r\n    \r\n    def _convert_to_example(image_name, raycast_name, image_buffer, raycast_buffer, sample_height, sample_width, sample_channels):\r\n    \r\n        example = tf.train.Example(features=tf.train.Features(feature={\r\n            'height': _int64_feature(sample_height),\r\n            'width': _int64_feature(sample_width),\r\n            'channels': _int64_feature(sample_channels),\r\n            'image/filename': _bytes_feature(tf.compat.as_bytes(image_name)),\r\n            'image/encoded': _bytes_feature(tf.compat.as_bytes(image_buffer)),\r\n            'raycast/filename': _bytes_feature(tf.compat.as_bytes(raycast_name)),\r\n            'raycast/encoded': _bytes_feature(tf.compat.as_bytes(raycast_buffer))}))\r\n    \r\n        return example\r\n```\r\n\r\n\r\nThe above code works fine in creating the tfrecord file. I put some print statements inside the `_convert_to_example` method to make sure the corresponding filenames (image_file & raycast_file) are getting written in one example.\r\n\r\nHowever, when I read the examples from tfrecord and print the image names, it looks like the image_file & raycast_file names do not correspond. The pair of images read by the tfRecordReader() is wrong.\r\n\r\n#### Below is my code to read the record:\r\n\r\n\r\n```\r\n    def parse_example_proto(example_serialized):\r\n    \r\n        feature_map = {\r\n                        'image/encoded': tf.FixedLenFeature([], dtype=tf.string, default_value=''),\r\n                        'raycast/encoded': tf.FixedLenFeature([], dtype=tf.string, default_value=''),\r\n                        'height': tf.FixedLenFeature([1], dtype=tf.int64, default_value=-1),\r\n                        'width': tf.FixedLenFeature([1], dtype=tf.int64, default_value=-1),\r\n                        'channels': tf.FixedLenFeature([1], dtype=tf.int64, default_value=-1),\r\n                        'image/filename': tf.FixedLenFeature([], dtype=tf.string, default_value=''),\r\n                        'raycast/filename': tf.FixedLenFeature([], dtype=tf.string, default_value='')\r\n                        }\r\n    \r\n        features = tf.parse_single_example(example_serialized, feature_map)\r\n    \r\n        return features['image/encoded'], features['raycast/encoded'], \\\r\n               features['height'], features['width'], features['channels'],\\\r\n               features['image/filename'], features['raycast/filename']\r\n    \r\n                \r\n    \r\n    def retirieve_samples():\r\n        \r\n        with tf.name_scope('batch_processing'):\r\n            data_files = ['train']\r\n    \r\n            filename_queue = tf.train.string_input_producer(data_files, shuffle=False)\r\n    \r\n            reader = tf.TFRecordReader()\r\n    \r\n            _, example_serialized = reader.read(filename_queue)\r\n    \r\n            image_buffer, raycast_buffer, height, width, channels, image_name, raycast_name = parse_example_proto(example_serialized)            \r\n            \r\n            return image_name, raycast_name\r\n```\r\n\r\n#### Below is my code to print a pair of filenames\r\n\r\n\r\n\r\n```\r\n    image_name, raycast_name = retirieve_samples()\r\n    with tf.Session() as sess:    \r\n        for i in range(1):\r\n            coord = tf.train.Coordinator()\r\n            threads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n            print(sess.run(image_name))\r\n            print(sess.run(raycast_name))\r\n            coord.request_stop()\r\n            coord.join(threads)\r\n```\r\nI have spent few days on this. I am not able to identify why I am not able to retrieve the correct pair. An example being retrieved should have the same data as the example being created right ? Why am I seeing different name pairs when I read and write ?\r\n\r\nAny help would be appriciated\r\n", "comments": ["Hi @pennypacker91 , saw you closed this issue. Did you solve the problem? I guess you were having a similar issue to mine #23098"]}, {"number": 22673, "title": "Toco outdated in docker build", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: docker\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a\r\n- **TensorFlow installed from (source or binary)**: docker\r\n- **TensorFlow version (use command below)**: latest-gpu-py3\r\n- **Python version**: python3\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: \r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: toco\r\n\r\n### Describe the problem\r\nAttempting to convert TF graph to TF-lite using the latest stable GPU Python3 docker image. Commands (shown below) differ from those shown in documentation [here](https://www.tensorflow.org/lite/devguide). Additionally, toco conversion of graph throws errors about unsupported operations Fill, Unpack, and MeanSquaredDifference. \r\n\r\nDownloading the latest bazel in the docker container and rebuilding toco from source gives a clearly different version.\r\n\r\n### Source code / logs\r\n\r\n```\r\nusage: toco [-h] --output_file OUTPUT_FILE\r\n            (--graph_def_file GRAPH_DEF_FILE | --saved_model_dir SAVED_MODEL_DIR | --keras_model_file KERAS_MODEL_FILE)\r\n            [--output_format {TFLITE,GRAPHVIZ_DOT}] [--inference_type {FLOAT,QUANTIZED_UINT8}]\r\n            [--inference_input_type {FLOAT,QUANTIZED_UINT8}] [--input_arrays INPUT_ARRAYS] [--input_shapes INPUT_SHAPES]\r\n            [--output_arrays OUTPUT_ARRAYS] [--saved_model_tag_set SAVED_MODEL_TAG_SET]\r\n            [--saved_model_signature_key SAVED_MODEL_SIGNATURE_KEY] [--std_dev_values STD_DEV_VALUES]\r\n            [--mean_values MEAN_VALUES] [--default_ranges_min DEFAULT_RANGES_MIN]\r\n            [--default_ranges_max DEFAULT_RANGES_MAX] [--quantize_weights QUANTIZE_WEIGHTS] [--drop_control_dependency]\r\n            [--reorder_across_fake_quant] [--change_concat_input_ranges] [--allow_custom_ops]\r\n            [--dump_graphviz_dir DUMP_GRAPHVIZ_DIR] [--dump_graphviz_video]\r\n```", "comments": ["@redbeard-the-coder Thanks for catching and reporting this. ", "The dock seems to contain the newer version.\r\n\r\nWhen I check the toco help in the various docker containers I get the new help on all of them.\r\n\r\nI think maybe you just forgot to do a docker pull. Try that and re-open if it doesn't work."]}, {"number": 22672, "title": "Make Keras/TPU more robust to closed TF sessions.", "body": "PiperOrigin-RevId: 215313156", "comments": ["FYI I have a largely-identical PR #22676, where the main difference is the title of the PR.  Let's drop this one, and I'll work with @goldiegadde to get the other one merged in."]}, {"number": 22671, "title": "Apache Ignite Dataset: Fixes merge artifacts", "body": "I've checked [Ignite Dataset](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/ignite) after merge of #22210 and found out several issues:\r\n\r\n1. All datasets are inherited from `DatasetSource`, not from `Dataset` since last week.\r\n2. SSL test environment has been removed, but not SSL tests.\r\n\r\nI fixed these issues here, @mrry, could you please have a look? \r\n\r\nBy the way, is it strictly required to exclude SSL tests as you [mentioned](https://github.com/tensorflow/tensorflow/pull/22210#issuecomment-425570635) or the key point is not to commit private key?", "comments": ["Thank you, @mrry. I've been thinking about generating certificates in tests, but it looks like too tricky in terms of maintaining. Lets keep current approach, at least for now."]}, {"number": 22670, "title": "C API: TF_SessionRun with multiple inputs gives Segmentation Fault ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.10\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.16.1\r\n- **GCC/Compiler version (if compiling from source)**: 7.3.0\r\n- **CUDA/cuDNN version**: 9.2 / 7.3\r\n- **GPU model and memory**: NVidia GTX 1070ti\r\n- **Exact command to reproduce**: compile and execute my program\r\n\r\n### Describe the problem\r\nIn C API, I'm using TF_SessionRun to execute inference. I don't understand why I get Segmentation Fault when I pass to TF_SessionRun multiple input tensor, but when I pass only 1 input tensor I get no errors. I see that the signature of the function is:\r\n```\r\nTF_CAPI_EXPORT extern void TF_SessionRun(\r\n    TF_Session* session,\r\n    // RunOptions\r\n    const TF_Buffer* run_options,\r\n    // Input tensors\r\n    const TF_Output* inputs, TF_Tensor* const* input_values, int ninputs,\r\n    // Output tensors\r\n    const TF_Output* outputs, TF_Tensor** output_values, int noutputs,\r\n    // Target operations\r\n    const TF_Operation* const* target_opers, int ntargets,\r\n    // RunMetadata\r\n    TF_Buffer* run_metadata,\r\n    // Output status\r\n    TF_Status*);\r\n```\r\nIt seems that, with TF_Tensor* const* input_values, I can send more that once input tensor. For passing more that once input tensor, I've created a std::vector of Tensor *, in fact I haven't any errors in compilation. My code below. My graph have this shape: ? x 50 x 50 x 3 for input, ? x 2 for output (like VGG16, you can use VGG16 for tests) \r\n\r\n### Source code / logs\r\n```\r\n#include \"tensorflow/c/c_api.h\"\r\n\r\n#include <stdio.h>\r\n#include <stdlib.h>\r\n#include <memory.h>\r\n#include <string.h>\r\n#include <assert.h>\r\n#include <vector>\r\n#include <algorithm>\r\n#include <iterator>\r\n#include <iostream>\r\n\r\n\r\nTF_Buffer* read_file(const char* file);\r\n\r\nvoid free_buffer(void* data, size_t length) {\r\n}\r\n\r\nstatic void Deallocator(void* data, size_t length, void* arg) {\r\n}\r\n\r\nint main() {\r\n  TF_Buffer* graph_def = read_file(\"data/graph.pb\");\r\n  TF_Graph* graph = TF_NewGraph();\r\n\r\n  TF_Status* status = TF_NewStatus();\r\n  TF_ImportGraphDefOptions* graph_opts = TF_NewImportGraphDefOptions();\r\n  TF_GraphImportGraphDef(graph, graph_def, graph_opts, status);\r\n  if (TF_GetCode(status) != TF_OK) {\r\n          fprintf(stderr, \"ERROR: Unable to import graph %s\", TF_Message(status));\r\n          return 1;\r\n  }\r\n  else {\r\n          fprintf(stdout, \"Successfully imported graph\\n\");\r\n  }\r\n  const int num_bytes_in = 1 * 50 * 50 * 3 * sizeof(float);\r\n  const int num_bytes_out = 1 * 2 * sizeof(float);\r\n\r\n  int64_t in_dims[] = {1, 50, 50, 3};\r\n  int64_t out_dims[] = {1, 2};\r\n\r\n  float values[1 * 50 * 50 * 3] = {0xff};\r\n\r\n  std::vector<TF_Output> inputs;\r\n  std::vector<TF_Tensor*> input_values;\r\n\r\n  inputs.push_back({TF_GraphOperationByName(graph, \"input_1\"), 0});\r\n  input_values.push_back(TF_NewTensor(TF_FLOAT, in_dims, 4, values, num_bytes_in, &Deallocator, 0));\r\n  input_values.push_back(TF_NewTensor(TF_FLOAT, in_dims, 4, values, num_bytes_in, &Deallocator, 0));\r\n\r\n  std::vector<TF_Output> outputs;\r\n  outputs.push_back({TF_GraphOperationByName(graph, \"dense_3/Softmax\"), 0});\r\n\r\n  std::vector<TF_Tensor*> output_values;\r\n\r\n  output_values.push_back(TF_AllocateTensor(TF_FLOAT, out_dims, 2, num_bytes_out));\r\n  output_values.push_back(TF_AllocateTensor(TF_FLOAT, out_dims, 2, num_bytes_out));\r\n\r\n  fprintf(stdout, \"Running session...\\n\");\r\n  TF_SessionOptions* sess_opts = TF_NewSessionOptions();\r\n  TF_Session* session = TF_NewSession(graph, sess_opts, status);\r\n  assert(TF_GetCode(status) == TF_OK);\r\n\r\n  TF_SessionRun(session, nullptr,\r\n                &inputs[0], &input_values[0], input_values.size(),\r\n                &outputs[0], &output_values[0], output_values.size(),\r\n                nullptr, 0, nullptr, status);\r\n\r\n  TF_Code c = TF_GetCode(status);\r\n\r\n  std::cout << c << std::endl;\r\n\r\n  for(size_t i = 0; i < output_values.size(); ++i)\r\n  {\r\n      if (output_values.at(i) == nullptr)\r\n      {\r\n          std::cout << \"bad parameters\" << std::endl;\r\n      }\r\n      else\r\n      {\r\n          const auto data = static_cast<float*>(TF_TensorData(output_values.at(i)));\r\n          std::cout << ((data[1] > 0.5f) ? true : false) << std::endl;\r\n      }\r\n  }\r\n\r\n  fprintf(stdout, \"Successfully run session\\n\");\r\n\r\n  TF_CloseSession(session, status);\r\n  TF_DeleteSession(session, status);\r\n  TF_DeleteSessionOptions(sess_opts);\r\n  TF_DeleteImportGraphDefOptions(graph_opts);\r\n  TF_DeleteGraph(graph);\r\n  TF_DeleteStatus(status);\r\n  return 0;\r\n}\r\n\r\nTF_Buffer* read_file(const char* file) {\r\n  FILE *f = fopen(file, \"rb\");\r\n  fseek(f, 0, SEEK_END);\r\n  long fsize = ftell(f);\r\n  fseek(f, 0, SEEK_SET);  //same as rewind(f);\r\n\r\n  void* data = malloc(fsize);\r\n  fread(data, fsize, 1, f);\r\n  fclose(f);\r\n\r\n  TF_Buffer* buf = TF_NewBuffer();\r\n  buf->data = data;\r\n  buf->length = fsize;\r\n  buf->data_deallocator = free_buffer;\r\n  return buf;\r\n}\r\n```\r\n", "comments": ["It appears that the `inputs` vector has a single element while `input_values` has two elements. `TF_SessionRun` requires that `inputs` and `input_values` be the same size (providing one concrete `TF_Tensor` for each input node in the graph).\r\n\r\nSo I suspect the segmentation fault you're getting is when `TF_SessionRun` tries to access `inputs[1]`.\r\n\r\nHope that helps.\r\nFeel free to reopen if I'm mistaken.\r\nThanks.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)", "Hi @asimshankar and @EnricoGiordano1992,\r\n\r\njust found this thread, I am having the identical problem. I already recognized that the inputs and input_values must have the identical length, however I still get a segmentation fault. I guess that my inputs vector is not correct. I added my the relevant parts of my code, I would be very grateful if you could check whether you recognize my mistake.\r\n\r\nGreets!\r\n\r\n\r\n\r\n    std::vector<TF_Output> inputs;\r\n    std::vector<TF_Tensor*> input_values;\r\n\r\n    TF_Operation* input_op = TF_GraphOperationByName(graph,\"input_1\");\r\n    TF_Operation* input_op2 = TF_GraphOperationByName(graph,\"input_1\");\r\n    \r\n    TF_Output input_output = {input_op,0};\r\n    TF_Output input_output2 = {input_op2,0};\r\n    inputs.push_back(input_output);\r\n    inputs.push_back(input_output2);\r\n\r\n    TF_Tensor* input = TF_NewTensor(TF_FLOAT, in_dims, 2, values, num_bytes_in, &Deallocator, 0);\r\n    TF_Tensor* input2 = TF_NewTensor(TF_FLOAT, in_dims, 2, values, num_bytes_in, &Deallocator, 0);\r\n    \r\n    input_values.push_back(input);  \r\n    input_values.push_back(input2);\r\n\r\n    std::vector<TF_Output> outputs;\r\n    \r\n    TF_Operation* output_op = TF_GraphOperationByName(graph,\"output_1/BiasAdd\");\r\n    TF_Operation* output_op2 = TF_GraphOperationByName(graph,\"output_1/BiasAdd\");\r\n    \r\n    TF_Output output_opout = {output_op,0};\r\n    TF_Output output_opout2 = {output_op2,0};\r\n    outputs.push_back(output_opout);\r\n    outputs.push_back(output_opout2);\r\n\r\n    std::vector<TF_Tensor*> output_values;\r\n    \r\n    TF_Tensor* output_value = TF_AllocateTensor(TF_FLOAT, out_dims, 2, num_bytes_out);\r\n    TF_Tensor* output_value2 = TF_AllocateTensor(TF_FLOAT, out_dims, 2, num_bytes_out);\r\n    output_values.push_back(output_value);\r\n    output_values.push_back(output_value2);\r\n   \r\n    fprintf(stdout,\"Running session...\\n\");\r\n    TF_SessionOptions* sess_opts = TF_NewSessionOptions();\r\n    \r\n    TF_Session* session = TF_NewSession(graph, sess_opts, status);\r\n\r\n    assert(TF_GetCode(status)==TF_OK);\r\n    // Call TF_SessionRun\r\n    TF_SessionRun(session,nullptr,&inputs[0],&input_values[0],inputs.size(),&outputs[0],&output_values[0],outputs.size(),nullptr,0,nullptr,status);\r\n\r\n\r\n", "I am having the exact same issue. ", "Anyone has solved this problem? I am facing this problem. Can't get a solution yet.", "Hi @JinmingYang5,\r\n\r\nI solved the problem by using code form another user. This also helped me to produce my own code. But I recommend to check this:\r\n\r\nhttps://github.com/Neargye/hello_tf_c_api\r\n\r\nGo to the /src and check his work, you should find the desired functionality there.", "Can anyone share a minimal working example with multi-output models?"]}, {"number": 22669, "title": "[ROCm] StreamExecutor logic for ROCm platform (PR 20709 continued)", "body": "This is really the continuation of PR 20709. \r\nFiling this new PR as that one was closed out by the `tensorflowbutler`\r\n\r\nNow that PR #20277 has been merged, this PR is ready to be reviewed and merged as well.\r\n\r\nNote that this PR includes the change being discussed in PR #22658 to fix the broken. Any further changes made as part of that PR will need to be applied to this one too.\r\n\r\nThe other PRs that build upon this one (i.e. have this PR getting merged as pre-req) \r\n1. #20712 \r\n2. #20713\r\n3. #20715\r\nhave also been closed out by `tensorflowbutler`, and I will be filing new ones shortly for each of them.\r\n\r\n\r\n------- Original Description copied from PR #20709 ---------------\r\n\r\nThe commit contains StreamExecutor logic for ROCm platform. It includes\r\nintegration logic with major components on ROCm platform:\r\n\r\n- HIP runtime APIs\r\n- rocRAND for RNG\r\n\r\nAlso included are relevant changes to:\r\n\r\n- bazel script to build ROCm StreamExecutor on ROCm platform\r\n\r\nAuthors:\r\n\r\n- Jack Chung: jack.chung@amd.com\r\n- Deven Desai: deven.desai@amd.com\r\n- Johannes M Dieterich: Johannes.Dieterich@amd.com\r\n- Peng Sun: Peng.Sun@amd.com\r\n- Jeffrey Poznanovic: Jeffrey.Poznanovic@amd.com", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n<!-- ok -->", "@deven-amd Thanks for the renewing the PR. We are still working on getting our internal version of Eigen in sync with upstream to allow us to move ahead with this. ", "@rmlarsen any update on the status on this PR?\r\n\r\nWe have atleast 3 more PRs which we need to renew and for which this PR is a pre-req.  Worried that we will not be able to merge in all the PRs needed to enable ROCm support, before the cutoff for TF 2.0. Wondering if there is anything we can do on our end to speed things up.\r\n\r\nThanks", "@deven-amd I'm as frustrated as you with the slow pace of this, but we are paying down 4 years of technical dept. I am hopeful that we will be able to submit our unforking of Eigen @ Google in about 1 month for now. We mostly need to qualify performance, but have a few outstanding test failures to fix. Once that is done we will roll the Eigen commit used by TensorFlow forward and will be able to merge your PRs.", "/cc @jlebar ", "I'm happy to review at least one of the patches. However, I'm out until next Monday, so I'll figure out which one to review when I start.", "@deven-amd we are finally ready to move forward on this. Google's internal fork of Eigen was \"unforked\" on November 7, and we have since done weekly incremental updates. Please resolve the conflicts.", "@deven-amd gentle ping to resolve branch conflicts", "this PR is no longer needed as #25011 was merged"]}, {"number": 22668, "title": "Android Tensorflow Lite: Efficient YUV -> RGB conversion are handled by libtensorflow_demo.so.", "body": "Solved: yes seems RenderScript faster than YUV -> RGB methods from libtensorflow_demo.so", "comments": []}, {"number": 22667, "title": "Build Fail : Build using MSbuild to create whl file (pip package)", "body": "I am trying to build Tenserflow following these steps:\r\n\r\nhttps://www.python36.com/install-tensorflow-gpu-windows/\r\n\r\nI am on Step 10\r\n\r\nBuild using MSbuild to create whl file (pip package) using the following command:\r\n\r\n**MSBuild /p:Configuration=Release /verbosity:detailed tf_python_build_pip_package.vcxproj**\r\n\r\nI am getting following error and build is failing can anyone help?\r\n\r\n----------------------------------------------------------------------------------\r\nDone executing task \"Touch\".\r\nDone building target \"FinalizeBuildStatus\" in project \"tf_python_copy_scripts_to_destination.vcxproj\".\r\nTarget \"Build\" in file \"C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\v140\\Microsoft.BuildSteps.Targets\" from project \"C:\\tensorflow\\tensorflow\\c\r\nontrib\\cmake\\build\\tf_python_copy_scripts_to_destination.vcxproj\" (entry point):\r\nDone building target \"Build\" in project \"tf_python_copy_scripts_to_destination.vcxproj\".\r\nDone Building Project \"C:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_copy_scripts_to_destination.vcxproj\" (default targets).\r\n\r\nDone executing task \"MSBuild\" -- FAILED.\r\nDone building target \"ResolveProjectReferences\" in project \"tf_python_build_pip_package.vcxproj\" -- FAILED.\r\nDone Building Project \"c:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_build_pip_package.vcxproj\" (default targets) -- FAILED.\r\n\r\n\r\nBuild FAILED.\r\n\r\n\"c:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_build_pip_package.vcxproj\" (default target) (1) ->\r\n\"C:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal.vcxproj\" (default target) (3) ->\r\n\"C:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal_static.vcxproj\" (default target) (4) ->\r\n\"C:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_c.vcxproj\" (default target) (5) ->\r\n\"C:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_cc_framework.vcxproj\" (default target) (6) ->\r\n\"C:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_framework.vcxproj\" (default target) (7) ->\r\n\"C:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\proto_text.vcxproj\" (default target) (8) ->\r\n\"C:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\grpc.vcxproj\" (default target) (9) ->\r\n(CustomBuild target) ->\r\n  C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\v140\\Microsoft.CppCommon.targets(171,5): error MSB6006: \"cmd.exe\" exited with code 1. [C:\\tensorf\r\nlow\\tensorflow\\contrib\\cmake\\build\\grpc.vcxproj]\r\n\r\n    0 Warning(s)\r\n    1 Error(s)\r\n----------------------------------------------------------------------------------\r\n\r\n### System information\r\n- **OS Platform and Distribution**: **Windows 8**\r\n- **TensorFlow installed from (source or binary)**: **Source**\r\n- **TensorFlow version (use command below)**: **1.5**\r\n- **Python version**: **3.6**\r\n- **Bazel version (if compiling from source)**: **N/A**\r\n- **GCC/Compiler version (if compiling from source)**:**N/A**\r\n- **CUDA/cuDNN version**: **9.1/7**\r\n- **GPU model and memory**:**N/A**\r\n- **Mobile Device**:**N/A**\r\n- **Exact command to reproduce**: **MSBuild /p:Configuration=Release /verbosity:detailed tf_python_build_pip_package.vcxproj**\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nMobile device", "Hello @tensorflowbutler ,\r\n\r\nFollwing are the details:\r\n\r\nSystem information\r\nOS Platform and Distribution: **Windows 8**\r\nTensorFlow installed from (source or binary): **Source**\r\nTensorFlow version (use command below): **1.5**\r\nPython version: **3.6**\r\nBazel version (if compiling from source): **N/A**\r\nGCC/Compiler version (if compiling from source): **N/A**\r\nCUDA/cuDNN version: **9.1/7**\r\nGPU model and memory: **N/A**\r\nMobile device: **N/A**\r\nExact command to reproduce: **MSBuild /p:Configuration=Release /verbosity:detailed tf_python_build_pip_package.vcxproj**\r\n\r\nThanks", "@imrsayyed29 Hi, can you provide information for the below field.\r\n\r\nGPU model and memory:", "CMake support is dropped, please see the bazel instructions here:\r\nhttps://www.tensorflow.org/install/source_windows\r\n\r\nMoreover, for support with such 3rd party resources, please contact their authors."]}, {"number": 22666, "title": "Update README.md", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "@sahuyash you are entitled to your opinion, but that was not a very productive contribution. "]}, {"number": 22665, "title": "no such package 'util/hash': BUILD file not found", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: FreeBSD 12.0-ALPHA3\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.11.0\r\n- **Python version**: 2.7.15\r\n- **Bazel version (if compiling from source)**: 0.17.1\r\n- **GCC/Compiler version (if compiling from source)**: clang version 6.0.1\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: `bazel fetch --repository_cache=/path/to/empty/bazel_cache //tensorflow:libtensorflow.so`\r\n\r\n### Describe the problem\r\nI'm doing the port for tensorflow and trying to fetch everything that's required for build first. After extracting tensorflow tarball I run `bazel fetch --repository_cache=/path/to/empty/bazel_cache //tensorflow:libtensorflow.so` and get the following error:\r\n\r\n```\r\nERROR: /wrkdirs/usr/ports/science/tensorflow-core/work/tensorflow-1.11.0/tensorflow/core/common_runtime/eager/BUILD:215:1: no such package 'util/hash': BUILD file not found on package path and referenced by '//tensorflow/core/common_runtime/eager:attr_builder'\r\nERROR: Evaluation of query \"deps(//tensorflow:libtensorflow.so)\" failed: errors were encountered while computing transitive closure\r\n```", "comments": ["@arrowd Looks like some files are missing, try to install TensorFlow from the binary.", "I doubt there are binaries for FreeBSD. This is why I'm doing the port, after all. Can you point me to FreeBSD binaries?", "@arrowd You are correct for FreeBSD it is better to install from port. Try this [link](https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=204512) to download the port for bazel 0.2.2b. You may want to use clang 3.8.", "I already have the latest bazel. The problem is not with bazel, but TensorFlow.", "TensorFlow works on a score of platforms as shown in this [link](https://www.tensorflow.org/install/source). You can also try to install TensorFlow on FreeBSD following [this](https://ecc-comp.blogspot.com/2016/06/tensorflow-on-freebsd.html). ", "I don't quite get what you are saying. I'm simply trying to compile latest TensorFlow release with latest Bazel release and https://www.tensorflow.org/install/source is exactly steps I'm following. Is this supported?\r\n\r\nThe manual you provided uses ancient Bazel to build old version of TF. I don't need it, I want to create a package of recent TF for FreeBSD users.", "Ping? Maybe I'm just doing something wrong? Is\r\n\r\n    bazel fetch --repository_cache=/path/to/empty/bazel_cache //tensorflow:libtensorflow.so\r\n\r\ncommand supposed to work at all?", "I'd like to add a confirmation that I have this issue, too. In NixOS we try to use bazel by fetching, patching, then building. I am using a built-from-source version of AMD's ROCm patches to tensorflow, but had to ultimately create a new bazel wrapper to apply the needed patches during the build since the fetch command fails with this problem.", "@arrowd The instructions can be adapted to the newer TensorFlow version but it won't be trivial. \r\nIn the meantime, you may need to check whether all protobuf patches are applied from ports before fetching and building, after which you can build again. `bazel fetch` does work on most Linux systems.\r\n\r\n@acowley glad you found a solution. Applying all needed patches is essential to get around this problem. Installing TensorFlow from source is usually challenging which is why we recommend binary. \r\n\r\n", "> In the meantime, you may need to check whether all protobuf patches are applied from ports before fetching and building\r\n\r\nCan you please elaborate on this? What protobuf patches?", "Was just hit with this as well.\r\n\r\n@acowley I'm using Nix to build TF as well, how did you end up working around this? Did you override `buildBazelPackage`?", "FWIW versions 1.11, 1.10.1, and 1.10.0 are affected. Version 1.9.0 isn't, it seems.", "@TravisWhitaker Yes, exactly, I used a custom `buildBazelPackage`. I abandoned `bazel fetch` due to this Issue, and baked the patches to the hard coded `/usr/bin/ar` paths used by bazel into `src/main/tools/process-wrapper-legacy.cc` (part of the bazel source) itself. This is horribly gross, but I also needed to patch dozens of other paths in `tensorflow`, so I could stomach it as a temporary fix.\r\n\r\n```nix\r\nrocm-bazel = (pkgs.bazel.override {enableNixHacks = true;}).overrideAttrs (old: {\r\n    # Packages that bazel fetches often include hard-coded paths to\r\n    # /usr/bin/ar. We in turn hard code a fix for that here. If\r\n    # bazel's fetch command works for your build, then this is not\r\n    # needed as these paths can be patched after download but before\r\n    # build. If fetching does not work, and you need to rely upon\r\n    # downloads during the build phase, this can help.\r\n    postPatch = old.postPatch + ''\r\n      find -type f -name CROSSTOOL\\* -exec sed -i -e 's,/usr/bin/ar,${pkgs.binutils.bintools}/bin/ar,g' {} \\;\r\n      sed -e 's|if (execvp(opt.args\\[0\\]|if (execvp(strcmp(opt.args[0], \"/usr/bin/ar\") == 0 ? \"${pkgs.binutils.bintools}/bin/ar\" : opt.args[0]|' \\\r\n          -e 's|\\(#include <vector>\\)|\\1\\n#include <cstring>|' \\\r\n          -i src/main/tools/process-wrapper-legacy.cc\r\n    '';\r\n  });\r\n```", "@acowley Ah nice, I'll give that a go. Thanks!", "Thanks for sharing your fix for NixOS @acowley.\r\n\r\nThe patches that may be helpful for FreeBSD case can be found in [https://svnweb.freebsd.org/ports/head/devel/protobuf/files/](https://svnweb.freebsd.org/ports/head/devel/protobuf/files/), locate the correct files that match with the FreeBSD version installed.", "Closing this out since I understand it to be resolved, but please let me know if I'm mistaken. If the issue is persists again, please open a new ticket. Thanks!"]}, {"number": 22664, "title": "Cannot compile tensorflow with CUDA 10.0 and cuDNN 7.2", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10\r\n- **TensorFlow installed from (source or binary)**:\r\nTrying to build from source.\r\n- **TensorFlow version (use command below)**:\r\n1.11.0\r\n- **Python version**:\r\n3.6.7rc1\r\n- **Bazel version (if compiling from source)**:\r\n0.16.1 (I tried with latest 1.17.2 but that also didn't build).\r\n- **GCC/Compiler version (if compiling from source)**:\r\nI don't know\r\n- **CUDA/cuDNN version**:\r\nCUDA 9.0 and cuDNN 7.3.1\r\n- **GPU model and memory**:\r\nGTX 1080 8gb\r\n- **Exact command to reproduce**:\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Describe the problem\r\nI'm trying to build tensorflow from source. This is the error:\r\n```\r\nERROR: D:/temp/tensorflow/tensorflow/contrib/rnn/BUILD:217:1: C++ compilation of rule '//tensorflow/contrib/rnn:python/ops/_lstm_ops_gpu' failed (Exit 1): msvc_wrapper_for_nvcc.bat failed: error executing command\r\n  cd C:/users/tim/_bazel_tim/lj4hacgi/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17134.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17134.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\Tools;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x86;;C:\\Windows\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/Tim/AppData/Local/Programs/Python/Python37/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/Tim/AppData/Local/Programs/Python/Python37/lib/site-packages\r\n    SET TEMP=C:\\Users\\Tim\\AppData\\Local\\Temp\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_CUDA_VERSION=9.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TMP=C:\\Users\\Tim\\AppData\\Local\\Temp\r\n```", "comments": ["v1.10 also doesn't build for me.", "TensorFlow requires CUDA 9.0 according to: https://www.tensorflow.org/install/gpu", "I've talked to and seen people having it built with CUDA 10.0 :\\ I guess I'll try\r\n\r\nEdit: I used CUDA 10.0 to try to build it", "I'm trying with CUDA 9.0 and cuDNN 7.3.1 (for CUDA 9.0) and it still fails.\r\n\r\n```\r\nRROR: D:/temp/tensorflow/tensorflow/contrib/image/BUILD:22:1: C++ compilation of rule '//tensorflow/contrib/image:python/ops/_image_ops_gpu' failed (Exit 1): msvc_wrapper_for_nvcc.bat failed: error executing command\r\n```", "I've sucessfully  built tensorflow with CUDA 10 and cuDNN 7.3.1 on windows 10. Maybe the problem is caused by the compiler? I used vs 2017. ", "I used vs 2017 professional, fresh install from yesterday.", "there is no cudnn 7.2 for cuda 10\r\n\r\nonly version for cuda 10 is cudnn 7.3", "I get this same issue with cudnn 7.3, Cuda 10, Windows 10, Python 3.7, Anaconda 5.3 (after solving two other unrelated issues involving bazel's `repository_ctx.execute` not being able to find grep or patch).\r\n\r\n@blaueck @Tvde1  How do you specify to use VS2017?\r\n\r\n```\r\nERROR: C:/users/joey/_bazel_joey/juz2ghmw/external/protobuf_archive/BUILD:659:1: C++ compilation of rule '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed (Exit 2): msvc_wrapper_for_nvcc.bat failed: error executing command\r\n  cd C:/users/joey/_bazel_joey/juz2ghmw/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17134.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17134.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\Tools;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;;C:\\WINDOWS\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/Joey/Anaconda3/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/Joey/Anaconda3/lib/site-packages\r\n    SET TEMP=C:\\Users\\Joey\\AppData\\Local\\Temp\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=7.5\r\n    SET TF_CUDA_VERSION=10.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TMP=C:\\Users\\Joey\\AppData\\Local\\Temp\r\n  external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.bat /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/genfiles/external/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Iexternal/local_config_python /Ibazel-out/x64_windows-opt/genfiles/external/local_config_python /Ibazel-out/x64_windows-opt/bin/external/local_config_python /Iexternal/protobuf_archive/python /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/python /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/python /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/local_config_python/python_include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_python/python_include /Ibazel-out/x64_windows-opt/bin/external/local_config_python/python_include /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX /DHAVE_PTHREAD /wd4018 /wd4514 -DGOOGLE_PROTOBUF_HAS_ONEOF=1 -DPROTOBUF_PYTHON_ALLOW_OVERSIZE_PROTOS=1 /Fobazel-out/x64_windows-opt/bin/external/protobuf_archive/_objs/python/google/protobuf/pyext/_message.so/descriptor_containers.o /c external/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc(172): error C2440: '=': cannot convert from 'const char *' to 'char *'\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc(172): note: Conversion loses qualifiers\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc(189): error C2440: '=': cannot convert from 'const char *' to 'char *'\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc(189): note: Conversion loses qualifiers\r\n[2,999 / 6,086] Compiling external/protobuf_archive/src/google/protobuf/generated_message_util.cc; 1s local ... (39 actions running)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: C:/users/joey/downloads/tensorflow/tensorflow/contrib/lite/python/BUILD:42:1 C++ compilation of rule '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed (Exit 2): msvc_wrapper_for_nvcc.bat failed: error executing command\r\n  cd C:/users/joey/_bazel_joey/juz2ghmw/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17134.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17134.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\Tools;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;;C:\\WINDOWS\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/Joey/Anaconda3/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/Joey/Anaconda3/lib/site-packages\r\n    SET TEMP=C:\\Users\\Joey\\AppData\\Local\\Temp\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=7.5\r\n    SET TF_CUDA_VERSION=10.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TMP=C:\\Users\\Joey\\AppData\\Local\\Temp\r\n  external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.bat /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/genfiles/external/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Iexternal/local_config_python /Ibazel-out/x64_windows-opt/genfiles/external/local_config_python /Ibazel-out/x64_windows-opt/bin/external/local_config_python /Iexternal/protobuf_archive/python /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/python /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/python /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/local_config_python/python_include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_python/python_include /Ibazel-out/x64_windows-opt/bin/external/local_config_python/python_include /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX /DHAVE_PTHREAD /wd4018 /wd4514 -DGOOGLE_PROTOBUF_HAS_ONEOF=1 -DPROTOBUF_PYTHON_ALLOW_OVERSIZE_PROTOS=1 /Fobazel-out/x64_windows-opt/bin/external/protobuf_archive/_objs/python/google/protobuf/pyext/_message.so/descriptor_containers.o /c external/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc\r\n```\r\n", "@joseortiz3 @Tvde1 , I noticed that you use the v140 toolset with vs 2017 maybe by setting BAZEL_VC=\"C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\"? I use the v141 toolset with CUDA 10 by setting BAZEL_VS=\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\"(I use community edition).", "@blaueck @Tvde1  , Yeah, I was using msvc 14.0 but then I set my bazel environment variables like you said: `BAZEL_VS` and `BAZEL_VC` (then restarted some things in a random way until the right compiler showed up during compilation). Now I'm almost certain I'm using the newest compiler 14.15.26726.\r\n\r\nHowever I'm still getting similar \" 'const char *' to 'char *'\" errors under the newer compiler.\r\n```\r\nERROR: C:/users/joey/_bazel_joey/juz2ghmw/external/protobuf_archive/BUILD:659:1: C++ compilation of rule '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed (Exit 2): msvc_wrapper_for_nvcc.bat failed: error executing command\r\n  cd C:/users/joey/_bazel_joey/juz2ghmw/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.15.26726\\ATLMFC\\include;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.15.26726\\include;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\cppwinrt\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.15.26726\\ATLMFC\\lib\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.15.26726\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17134.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17134.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.15.26726\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\MSBuild\\15.0\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17134.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\Tools\\;;C:\\WINDOWS\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/Joey/Anaconda3/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/Joey/Anaconda3/lib/site-packages\r\n    SET TEMP=C:\\Users\\Joey\\AppData\\Local\\Temp\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=7.5\r\n    SET TF_CUDA_VERSION=10.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TMP=C:\\Users\\Joey\\AppData\\Local\\Temp\r\n  external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.bat /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/genfiles/external/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Iexternal/local_config_python /Ibazel-out/x64_windows-opt/genfiles/external/local_config_python /Ibazel-out/x64_windows-opt/bin/external/local_config_python /Iexternal/protobuf_archive/python /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/python /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/python /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/local_config_python/python_include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_python/python_include /Ibazel-out/x64_windows-opt/bin/external/local_config_python/python_include /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX /DHAVE_PTHREAD /wd4018 /wd4514 -DGOOGLE_PROTOBUF_HAS_ONEOF=1 -DPROTOBUF_PYTHON_ALLOW_OVERSIZE_PROTOS=1 /Fobazel-out/x64_windows-opt/bin/external/protobuf_archive/_objs/python/google/protobuf/pyext/_message.so/descriptor_containers.o /c external/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc(172): error C2440: '=': cannot convert from 'const char *' to 'char *'\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc(172): note: Conversion loses qualifiers\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc(189): error C2440: '=': cannot convert from 'const char *' to 'char *'\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc(189): note: Conversion loses qualifiers\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 14.784s, Critical Path: 8.98s\r\nINFO: 43 processes: 43 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n@blaueck Could you (pretty-please) post your build configuration?? Python version, MSYS2 version, bazel version, compiler version, cudnn, cuda, .... I think that's all I would need. I kinda give up on trying to get my setup working :( \r\n\r\nSeems like this [`descriptor_containers.cc` ](https://github.com/protocolbuffers/protobuf/blob/master/python/google/protobuf/pyext/descriptor_containers.cc) lines 172 and 189 keep giving me that C2440 error...\r\n", "@joseortiz3 , Have you try to run \"bazel clean\" after switch to compiler 14.15.26726?\r\npython: 3.6.6\r\nMSYS2: don't known how to check the version\r\nbazel:  0.15.0\r\ncuda: 10\r\ncudnn: 7.3\r\ncompiler: 14.15.26726\r\n", "@blaueck Yeah tried that, `bazel clean` and `bazel clean --expunge`, closing shell, etc. Needed to do that when I changed my environment variables.\r\n\r\nNo matter what I do I always get `protobuf` compile errors. I think its a problem with Python 3.7 somehow being incompatible with protobuf; that seems to pop up a lot googling this error.\r\n\r\nAlways get 2-10 C2440 errors in ` descriptor_containers.cc`... going to try python 3.6\r\n\r\n", "@blaueck Tried Python 3.6.6, but now I get the error below. Changing bazel from 17 to 15 doesn't help.\r\n\r\n```\r\nc:\\users\\joey\\_bazel_joey\\juz2ghmw\\execroot\\org_tensorflow\\external\\eigen_archive\\eigen\\src/Core/arch/CUDA/Half.h(212): error: more than one instance of overloaded function \"__hadd\" matches the argument list:\r\n            function \"__hadd(int, int)\"\r\n            function \"__hadd(__half, __half)\"\r\n            argument types are: (const Eigen::half, const Eigen::half)\r\n```\r\n\r\nRelated to https://github.com/tensorflow/tensorflow/issues/19198\r\n\r\nWhat compute capability are you specifying I wonder? I'm specifying 7.5 which is the RTX's compute level.\r\n\r\nI tried both yes and no to \"override strong inline eigen\" stuff, no change.", "I specify 5.0. My card can't support compute capability greater than 5.0.", "Ok, I figured it out. See https://github.com/tensorflow/tensorflow/issues/22715 if interested. Compute >= 5.3 requires patching right now. Followed [this guide to patch it](https://medium.com/@amsokol.com/update-1-how-to-build-and-install-tensorflow-gpu-cpu-for-windows-from-source-code-using-bazel-and-c2e86fec9ef2). I also did `checkout` v1.11 as per that guide, not sure if that's required.", "@Tvde1  Is this still an issue? Can you please post your observations after trying installation guide given in the comment below.\r\n\r\n> Ok, I figured it out. See #22715 if interested. Compute >= 5.3 requires patching right now. Followed [this guide to patch it](https://medium.com/@amsokol.com/update-1-how-to-build-and-install-tensorflow-gpu-cpu-for-windows-from-source-code-using-bazel-and-c2e86fec9ef2). I also did `checkout` v1.11 as per that guide, not sure if that's required.\r\n\r\n", "@joseortiz3 I have the same problem with you. I refer https://medium.com/@amsokol.com/update-1-how-to-build-and-install-tensorflow-gpu-cpu-for-windows-from-source-code-using-bazel-and-c2e86fec9ef2, do the same progress work for me. I think the key is \"There is a BUG in eigen third-party library\" as mentioned in the article. My environment: Windows 10, RTX 2080, cuda 10.0, cudnn 7.3, VS2017", "@RainyRen Yep, that's what I found too, said it in my last comment. Hope the problems with the `eigen` compute stuff and `protobuf`-`python3.7` compatibility get fixed soon.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 22663, "title": "Add demo gif to README", "body": "__Disclaimer: This is a bot__\n\n It looks like your repo is trending. The [github_trending_videos](https://www.instagram.com/github_trending_videos/) Instgram account automatically shows the demo gifs of trending repos in Github.\n\nYour README doesn't seem to have any demo gifs. Add one and the next time the parser runs it will pick it up and post it on its Instagram feed. If you don't want to just close this issue we won't bother you again.", "comments": ["Your link is dead mister robot buddy"]}, {"number": 22662, "title": "h", "body": "", "comments": []}, {"number": 22661, "title": "Update math_ops.py", "body": "adding erfniv in reference to [# TODO(phawkins): implement erfinv](https://github.com/tensorflow/tensorflow/blob/32a3642ef448d93706ab22e894637b2dd0c197c7/tensorflow/compiler/tf2xla/python/xla.py#L84)", "comments": ["I don't understand the deprecated_endpoints decorator. Just don't export to main namespace?  Also needs a unit test comparing to scipy", "Sorry, yup doesn't make sense to add deprecated_endpoints decorator, removed it", "@gautam1858 please resolve the conflict and add a unit test for the new function.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 28 days that this pull-request has stalled. Please create a new pull-request with the requested changes.", "Is there any reason that `erfinv` is still not in the tf.math namespace for tf 1.14 or 2.0?  I am currently having to include tfp and work with the inversecdf of normal to get my hands on this.  "]}, {"number": 22660, "title": "Update backend.py", "body": "adding missing import numpy", "comments": ["please merge this "]}, {"number": 22659, "title": "Update backend.py", "body": "Adding missing import in the commented examples. When trying out that particular example in commented section the TensorFlow and bumpy imports are missing", "comments": ["please merge this\r\n"]}, {"number": 22658, "title": "Fix for the broken \"--config=rocm\" build (followup to PR 20277)", "body": "This is more of a followup to PR #20277 \r\n\r\nAs per the following comment by @yifeif \r\nhttps://github.com/tensorflow/tensorflow/pull/20277#issuecomment-425178770\r\n\r\n`if_cuda_is_configured` was changed to `if_cuda` in the merge to make some internal targets pass. Unfortunately it seems that doing so break the `ROCm` build.\r\n\r\nWhen I try to do a `ROCm` build on a clean repo (post PR 20277 `master` branch), I get the following errors\r\n```\r\n  Successfully uninstalled tensorflow-1.10.0\r\nERROR: /root/tensorflow/tensorflow/contrib/tensor_forest/BUILD:110:1: Label '//tensorflow/core:gpu_lib' is duplicated in the 'deps' attribute of rule 'tensor_forest_kernels'\r\nERROR: /root/tensorflow/tensorflow/contrib/tensor_forest/BUILD:220:1: Label '//tensorflow/core:gpu_lib' is duplicated in the 'deps' attribute of rule 'model_ops_kernels'\r\nERROR: /root/tensorflow/tensorflow/contrib/tensor_forest/BUILD:315:1: Label '//tensorflow/core:gpu_lib' is duplicated in the 'deps' attribute of rule 'stats_ops_kernels'\r\nERROR: /root/tensorflow/tensorflow/contrib/boosted_trees/BUILD:324:1: Label '//tensorflow/core:gpu_lib' is duplicated in the 'deps' attribute of rule 'model_ops_kernels'\r\nERROR: /root/tensorflow/tensorflow/contrib/boosted_trees/BUILD:395:1: Label '//tensorflow/core:gpu_lib' is duplicated in the 'deps' attribute of rule 'split_handler_ops_kernels'\r\nERROR: /root/tensorflow/tensorflow/contrib/boosted_trees/BUILD:439:1: Label '//tensorflow/core:gpu_lib' is duplicated in the 'deps' attribute of rule 'training_ops_kernels'\r\nERROR: /root/tensorflow/tensorflow/contrib/boosted_trees/BUILD:486:1: Label '//tensorflow/core:gpu_lib' is duplicated in the 'deps' attribute of rule 'prediction_ops_kernels'\r\nERROR: /root/tensorflow/tensorflow/contrib/boosted_trees/BUILD:534:1: Label '//tensorflow/core:gpu_lib' is duplicated in the 'deps' attribute of rule 'quantile_ops_kernels'\r\nERROR: /root/tensorflow/tensorflow/contrib/boosted_trees/BUILD:578:1: Label '//tensorflow/core:gpu_lib' is duplicated in the 'deps' attribute of rule 'stats_accumulator_ops_kernels'\r\n```\r\n\r\nThe following commands can be used to do the `ROCm` build\r\n```\r\n<run configure.py to enable the ROCm support>\r\nbazel build --config=opt --config=rocm //tensorflow/tools/pip_package:build_pip_package \r\n```\r\n\r\nThis build does complete without any errors with the commit in this PR.\r\nI realize that this commit will most likely cause regressions in the internal targets/builds that @yifeif eluded to in the comment. So we need to come up with a solution that makes everything work, and we will need your help in that matter.", "comments": ["/cc @gunan @whchung ", "@yifeif did you get a chance to review this PR?", "I just submitted ea42e88, which should make the macros behave as what #20277 originally intended. Let me know if the change at head works for you, and we can close this PR and move forward. Thanks for the patience. Sorry it took a bit longer to get this work both internally and externally.", "closing this PR based on @yifeif 's statement"]}, {"number": 22657, "title": "Fix bug in add_n's handling of IndexedSlices", "body": "The fix for #15943 (https://github.com/tensorflow/tensorflow/pull/21494) modified the Python `add_n` op to allow `IndexedSlices` in its arguments. This fix introduced a bug in how `add_n` handles an input consisting of a single `IndexedSlices` object. Specifically, `add_n` returns the `values` field of the `IndexedSlices` object, when it should return the tensor representation of the object.\r\n\r\nThis PR fixes the bug and adds a regression test.", "comments": ["@frreiss Thanks for the fix!", "@frreiss Please fix the linter errors: https://source.cloud.google.com/results/invocations/2ec1249e-6c88-47f6-8ab2-1529b4abdb10/log", "@rmlarsen should be fixed now.", "@rmlarsen would you mind triggering another CI build on this branch? I believe that my last commit fixed all the linter errors.", "Nagging Assignee @rmlarsen: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@frreiss sorry for the delay.  rerunning tests.", "Thanks for rerunning the build.\r\n\r\nI see some fresh build failures that weren't there before.\r\n\r\nThe \"MacOS Python2 and CC\" build failed because of what looks like a transient error:\r\n```\r\n======================================================================\r\nERROR: setUpClass (__main__.DistributeCoordinatorTestBase)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/distribute/distribute_coordinator_test.runfiles/org_tensorflow/tensorflow/python/distribute/distribute_coordinator_test.py\", line 169, in setUpClass\r\n    NUM_WORKERS, num_ps=NUM_PS)\r\n  File \"/Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/distribute/distribute_coordinator_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2441, in create_local_cluster\r\n    start=True) for ix in range(num_workers)\r\n  File \"/Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/distribute/distribute_coordinator_test.runfiles/org_tensorflow/tensorflow/python/training/server_lib.py\", line 146, in __init__\r\n    self._server = c_api.TF_NewServer(self._server_def.SerializeToString())\r\nUnknownError: Could not start gRPC server\r\n```\r\n\r\nOn my Mac with a Python 2.7 Anaconda environment, the build command\r\n```\r\nbazel test  //tensorflow/python/distribute:distribute_coordinator_test \r\n```\r\nruns without errors.\r\n\r\n\r\nThe \"Ubuntu contrib\" build also seems to have hit a transient error:\r\n\r\n```\r\n======================================================================\r\nFAIL: testWhileLoop (__main__.RateTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/contrib/rate/rate_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 971, in decorated\r\n    f(self, *args, **kwargs)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/contrib/rate/rate_test.runfiles/org_tensorflow/tensorflow/contrib/rate/rate_test.py\", line 93, in testWhileLoop\r\n    self.assertEqual([[2]], self.evaluate(loop[3]))\r\nAssertionError: [[2]] != array([4.])\r\n```\r\n\r\nOn my Ubuntu 16.04 test machine, \r\n```\r\nbazel test --define=grpc_no_ares //tensorflow/contrib/rate:rate_test\r\n```\r\nruns without errors.\r\n\r\nThe Windows builds appear to be failing because this PR is on a branch from before there was a Windows Bazel build.  Should I merge the latest changes from master into this branch?", "I think you can ignore the errors."]}, {"number": 22656, "title": "Updating TF release version to 1.12", "body": "Updating TF release version to 1.12", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "Updated the author id", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->"]}, {"number": 22654, "title": "[ubuntu] Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\\Windows\\Temp' as default", "body": "If you open a GitHub issue, here is our policy:\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nno\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nN/A\r\n- **TensorFlow installed from (source or binary)**:\r\nrelease tag v1.110\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n3.6\r\n- **Bazel version (if compiling from source)**:\r\n0.17.2\r\n- **GCC/Compiler version (if compiling from source)**:\r\n4.9\r\n- **CUDA/cuDNN version**:\r\n9.1\r\n- **GPU model and memory**:\r\nGeForce 940M/PCIe/SSE2 2GB\r\n- **Exact command to reproduce**:\r\nCommands below. ``./configure`` then;\r\n```\r\nbazel build --config=opt --config=cuda //tensorflow/tools\r\n```\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/2435301/tf_env.txt)\r\n\r\nYou can obtain the TensorFlow version with\r\nNone, \r\n\r\n### Describe the problem\r\nGetting on compliation error:\r\nAuto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\\Windows\\Temp' as default\r\n\r\n### Source code / logs\r\n\r\n```\r\n~/tmp/tensorflow$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\n    currently loading: tensorflow/tools/pip_package\r\nDEBUG: /home/guy/.cache/bazel/_bazel_guy/0d29558304d115cf217e115548450924/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5: \r\nAuto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\\Windows\\Temp' as default\r\n```\r\n\r\nmy configure:\r\n```\r\n$ ./configure                  \r\nExtracting Bazel installation...\r\nWARNING: An illegal reflective access operation has occurred\r\nWARNING: Illegal reflective access by com.google.protobuf.UnsafeUtil (file:/home/guy/.cache/bazel/_bazel_guy/install/792a28b07894763eaa2bd870f8776b23/_embedded_binaries/A-server.jar) to field java.lang.String.value\r\nWARNING: Please consider reporting this to the maintainers of com.google.protobuf.UnsafeUtil\r\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\r\nWARNING: All illegal access operations will be denied in a future release\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.17.2 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3\r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/lib/python3/dist-packages\r\n  /usr/local/lib/python3.6/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]\r\n\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: \r\njemalloc as malloc support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n\r\nNo Google Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: n\r\nNo Hadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Amazon AWS Platform support? [Y/n]: n\r\nNo Amazon AWS Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: n\r\nNo Apache Kafka Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: y\r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: n\r\nNo GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: \r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with nGraph support? [y/N]: \r\nNo nGraph support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: \r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: 9.1\r\n\r\n\r\nPlease specify the location where CUDA 9.1 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 7.1\r\n\r\n\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr]: /usr\r\n\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: \r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nPlease specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: 1.3\r\n\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]: 5.0\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: \r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/x86_64-linux-gnu-gcc-7]: /usr/bin/x86_64-linux-gnu-gcc-4.9\r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: \r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: \r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See tools/bazel.rc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\nConfiguration finished\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nMobile device", "Fixed, - **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: \r\nN/A\r\n", "i faced the same issue. Any fixes yet?\r\n", "@guysoft You can use CUDA 9.2/cuDNN 7.1 and Bazel 1.15.0 to install TensorFlow. ", "I had the same issue, but it inexplicably went away after rebooting my computer (very sad, that's the type of behaviour I expect from a Windows system, not linux).  Using CUDA 9.1, cuDNN 7.2.1, Bazel 0.17.2 here, with Ubuntu 18.04.", "That happens sometimes, but much less frequently.", "How can locating a temp folder be non-deterministic? I assume its not a CNN :)\r\nIt happens deterministically here.", "I am also running into this.\r\n\r\nWhy is bazel looking for a windows directory on my ubuntu machine?", "In my case (bazel-1.19.1 / tf-1.12) upgrading to bazel-1.19.2 fixed this issue and also #23719.", "> In my case (bazel-1.19.1 / tf-1.12) upgrading to bazel-1.19.2 fixed this issue and also #23719.\r\n\r\nalso fixed my issue (bazel version should be 0.19.2)", "@bzamecnik How did you build Tensorflow with Bazel 1.19.2? If I try to build the current release of Tensorflow with Bazel>=1, the installer errors and tells me I must downgrade to Bazel 0.26.1 or earlier."]}, {"number": 22653, "title": "R0.7", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "This seems to be accidental."]}, {"number": 22652, "title": "tensorflow gpu problem", "body": "hi all,\r\nbelow is my issue, has tackle it for days and not success\r\nC:\\Users\\chanw>python\r\nPython 3.6.6 (v3.6.6:4cf1f54eb7, Jun 27 2018, 03:37:03) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\chanw\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\chanw\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\chanw\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\chanw\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\chanw\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6a21\u7d44\u3002\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\chanw\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\chanw\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\chanw\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\chanw\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\chanw\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\chanw\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\chanw\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\chanw\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6a21\u7d44\u3002\r\n\r\nFailed to load the native TensorFlow runtime.\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nno\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nwin10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\npip install command\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n9.0/7.3\r\n- **GPU model and memory**:\r\n1070ti\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I have the exact same problem, CUDA v9.2 and cuDNN 7.3", "i use cuda 9.0, hope the problem can be solve", "@chanwaiki I've solved my problem, it seems we can not use cuDNN 7.3, try downgrading to 7.2. I think the install instructions should be more clear, it says cuDNN SDK (>= 7.2), but cuDNN 7.3 does not work.", "cuDNN 7.2 should work. We will do our best to make the instructions as clear as possible. Keep in mind though cuDNN is a third party product thus we don't control the versions, please refer to their website for details."]}, {"number": 22651, "title": "doc bug on gpu installation page", "body": "Your install website (https://www.tensorflow.org/install/gpu) has the wrong cuda version number referenced. It says that tensorflow wants cuda 9.0 and cuda 7.2, but cuda 7.2.1 seems to work only with cuda 9.2. If you don't upgrade cuda you get this obscure error: \r\n \r\n\"Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\"\r\n\r\nI have written custom code, but not relevant for this problem.\r\nos: ubuntu 16.04\r\ntensorflow-gpu installed with pip\r\ntensorflow version: v1.11.0-0-gc19e29306c 1.11.0\r\npython 3.5.2\r\n\r\n\r\nSummary: This combination works for me:\r\n\r\ncuda 9.2, (download .deb from nvidia website, installed with dpkg)\r\ncudnn 7.2.1 (download .deb from nvidia website, installed with dpg\r\npip install tensorflow-gpu  (tensorflow 1.11)\r\npython 3.5.2\r\nubuntu 16.04 \r\nuname - a:\r\n`4.15.0-34-generic #37~16.04.1-Ubuntu SMP Tue Aug 28 10:44:06 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux`\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@chahld you can ignore @tensorflowbutler . @tfboyd any ideas on what exactly it should be?", "cuda 9.2 and cudnn 7.2.1 with tensorflow 1.11 worked for me. ", "It all depends on where you install from.  The instructions assume you install from the tensorflow official pip by pypi or our website, e.g.  `pip install tensorflow-gpu`\r\n\r\nThe official pip requires CUDA 9.0 and cuDNN greater than or equal to 7.2.  You can use newer cuDNN's but CUDA does not have the same backward compatibility agreement.  \r\n\r\nI have been told the Anaconda install is compiled with CUDA 9.2 by the Conda team and that would require CUDA 9.2.  \r\n\r\nFinally, there have been changes to the license with NVIDIA that would allow us to include CUDA/cuDNN/TensorRT/NCCL (now open source) in the builds, but that will bring the package size to almost 500MB well above current pypi limits, which are normally maybe 50MB, if my memory is correct, that we have had increased to well over 200MB.", "@chahld   To help other people please update your comment to say where your install came from.  My concern is people will think they should use CUDA 9.2 and that is not going to work with 1.11 from the official pip.  Thank you for helping.", "My original comment does say that: \"tensorflow-gpu installed with pip\", but I added a summary to try to make it clearer. \r\n\r\nIf you choose cudnn 7.2.1 you have to use cuda 9.2. Based on nvidia's website: \r\nhttps://developer.nvidia.com/rdp/cudnn-archive. This is the only 7.2.xxx version that is available and it says that it requires cuda 9.2. \r\n\r\nNote I chose to install cudnn 7.2.1 instead of latest 7.3.1 because of tensorflow docs. 7.3.1 has a version that works with cuda 9.0. So the 9.0+7.3.1 combination might work also (I haven't tried it). \r\n\r\n", "@chahld  NVIDIA has been slow to move the items to the archive.  If you install with apt-get you can still get 7.2.x with CUDA 9.2.  I would suggest using 7.3 and for the small tests I have done 7.2 and 7.3 perform very similarly performance wise and we are qualifying 7.3 right now.  I am surprised CUDA 9.2 is working for you but I am glad you got it working.  Sorry about the request, for some reason I looked at your comment and thought you were a different person from the original poster.  I thought you were just a random person adding a data point.    ", "@chahld \r\n\r\nWent an extra mile here is what I tried:\r\n- `nvidia-docker run -it nvidia/cuda:9.2-devel-ubuntu16.04 bash`\r\n- `apt-get install python3`\r\n- `apt-get install python3-pip`\r\n- `pip3 install tensorflow-gpu`\r\n- `python3`  and then `import tensorflow as tf`\r\n\r\nError is what I expected below, but it is possible that CUDA 9.0 is still in your path.  I have used cuDNN that says it is for a different version of CUDA that I installed a few times.  No idea how often that should work but it likely works more often than not I would guess as I do not think it does any kind of check the API just has to match up. You can run  `ldconfig -p | grep cuda` for fun and look at the files.  I would guess the debian packages would overwrite/upgrade but you can run this (`apt list --installed | grep cud`) to see if what is installed.  I have seen multiple cuDNN's installed before if you run the debian package directly.  \r\n\r\n```bash\r\nFile \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n```"]}, {"number": 22650, "title": "Updated RELEASE.md", "body": "Added new CUDA compute capability", "comments": ["Nagging Assignee @rmlarsen: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Sorry about missing this, but now that 1.12 is out, this is obsolete, so I'm closing the issue."]}, {"number": 22649, "title": "request for more general einsum", "body": "### Describe the problem\r\nTensors have sometimes a lot of dimensions and explicitly writing them helps a lot for writing concise and readable code without bugs. For this, notations like in tf.einsum is invaluable. Unfortunately, the current implementation of tf.einsum is only a subset of what could be done with it : \r\n* As documented, \"Subscripts that are summed across multiple inputs (e.g., ij,ij,jk->ik).\" are not supported.\r\n* Sums are not supported (eg. ij+ik->ijk, or even both products and sums ij*jk+i*k->ik)\r\n* Expanding dimensions could also be useful (eg. ij,jk->ikl). \r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n"]}]