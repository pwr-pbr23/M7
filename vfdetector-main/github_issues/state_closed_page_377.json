[{"number": 42697, "title": "TFLu: Support operator only input tensors.", "body": "Ethos-u relies on differentiating between having operator input tensors\r\nas subgraph inputs or not.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\r\n\r\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\r\nRead the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\r\nCreated a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\r\nLinked to the issue from the PR description\r\n\r\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.", "Hi Mans,\r\n\r\nTrying to understand the context on this change. \r\n\r\n  tflite::testing::NodeConnection node_list[2] = {\r\n      {\r\n        {t0, t1, t2},  // t0: input (actual input part of subgraph inputs as well as\r\n                       // operator inputs)\r\n                       // t1: scratch1 (only in operator inputs)\r\n                       // t2: scratch2 (only in operator inputs)\r\n        {t3}           // output\r\n      },\r\n      {\r\n        {t3},  // input\r\n        {t4}   // output\r\n      },\r\n  };\r\n\r\nCould you help me understand what's the behavior different with/without this change.\r\n\r\nThe online planning could handle this case pretty well.\r\n\r\nThanks,", "@wangtz  Ethos-U is depending on the offline planner and is using one or two additional operator input tensors (similar to scratch tensors, but part of the tflite file). We want these to remain as input tensors to the operator, but not as inputs to the subgraph.\r\nWithout this change the scratch tensors would be treated as read only and get incorrect address in the Ethos-u kernel. So the unit-test tests that and the model described by the node_list, is trying to picture a typical Ethos-U optimized model.\r\n", "@advaitjain Thanks for the comments. We believe this is not specific for Ethos-U, since we think potentially some other platform/product could use this as well."]}, {"number": 42696, "title": "DataHandler intermittent InvalidArgumentError for large input dimension", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.2.0-rc2-77-gaad398b5e9 2.2.0-rc3\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: V100\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nBased on tracing an error for a call to the `fit` method of a model, I determined that an `InvalidArgumentError` was contingent on both the number of inputs N in a dataset batch (in this case the data is of the form BxNxC) as well as whether `class_weights` were being set.\r\n\r\n**Describe the expected behavior**\r\nSetting `class_weights` as shown in the online [docs](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data) would not break calls to `fit` depending on the number of input data. I determined that the DataHandler was the object that threw the exception during the `model.fit` call, so the min example below is based on a DataHandler object.\r\n\r\nThis is related to this [issue](https://github.com/tensorflow/tensorflow/issues/23698), but I don't see how the issue is resolved for arbitrary models. In particular, it has nothing to do with NLP, as is mentioned in a [comment](https://github.com/tensorflow/tensorflow/issues/23698#issuecomment-673667914)\r\n\r\n**Standalone code to reproduce the issue**\r\nColab [link](https://colab.research.google.com/drive/1jAPPKv5yKMNvwzJV4dvJZFBFzfrn6h8W#scrollTo=FinvXcZG3K67)\r\n\r\nSame code posted here:\r\n``` python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.python.keras.engine import data_adapter\r\ndef test_weights(model, n_samples, use_weights=True):\r\n    def test_data():\r\n        def test_data_gen():\r\n            n_classes = 5\r\n            x = np.random.randn(n_samples,3)\r\n            y = np.random.randint(0,n_classes,n_samples)\r\n            yield (x.astype(np.float32),\r\n                   y.astype(np.int32))\r\n        gen_func = test_data_gen\r\n        gen_types = (tf.float32, tf.int32)\r\n        gen_shapes = ([None, 3], [None])\r\n        return gen_func, gen_types, gen_shapes\r\n    gen_fn, gen_tp, gen_sh = test_data()\r\n    ds_tst = tf.data.Dataset.from_generator(gen_fn, gen_tp, gen_sh)\r\n    ds_tst = ds_tst.batch(2)\r\n    ds_tst = ds_tst.prefetch(2)\r\n    cw = {0 : 0.0, 1 : 1.5, 2 : 0.5,\r\n          3 : 0.5, 4 : 0.5}\r\n    data_handler = data_adapter.DataHandler(\r\n        x=ds_tst,\r\n        y=None,\r\n        sample_weight=None,\r\n        batch_size=None,\r\n        steps_per_epoch=10,\r\n        initial_epoch=0,\r\n        epochs=1,\r\n        shuffle=True,\r\n        class_weight=(cw if use_weights else None),\r\n        max_queue_size=10,\r\n        workers=1,\r\n        use_multiprocessing=False,\r\n        model=model)\r\n    print ('NEXT',next(iter(data_handler._dataset)))\r\n\r\nmodel = tf.keras.Model()\r\ntest_weights(model, 5) # Always succeeds\r\ntest_weights(model, 50000) # Sometimes fails\r\ntest_weights(model, 50000, use_weights=False) # Always succeeds\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n``` bash\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py in execution_mode(mode)\r\n   2101       ctx.executor = executor_new\r\n-> 2102       yield\r\n   2103     finally:\r\n\r\n11 frames\r\nInvalidArgumentError: indices[0] = 5 is not in [0, 5)\r\n\t [[{{node GatherV2}}]] [Op:IteratorGetNext]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/executor.py in wait(self)\r\n     65   def wait(self):\r\n     66     \"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\r\n---> 67     pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\r\n     68 \r\n     69   def clear_error(self):\r\n\r\nInvalidArgumentError: indices[0] = 5 is not in [0, 5)\r\n\t [[{{node GatherV2}}]]\r\n```", "comments": ["@phil0stine \r\nI ran the code shared and able to replicate the issue faced, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/d4fe3d6cc9c4bc8f78a3abcbc68e584b/untitled396.ipynb)\r\n#23698  (please modify the input size and let us know) [link](https://stackoverflow.com/questions/51223936/tensorflow-invalidargumenterror-indices-while-training-with-keras) [link1](https://stackoverflow.com/questions/46940073/tensorflow-object-detection-aborts-with-invalidargumenterror-indices0-2-is/49745588)  [link2](https://blog.csdn.net/u011585024/article/details/88938194)", "@Saduf2019 \r\nI have looked at the references you provided, but they all involve NLP and [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layers that have the incorrect input dimension as the `input_dim` argument.\r\n\r\nMy network has no such layers, and is similar to the architectured described in a related issue https://github.com/tensorflow/tensorflow/issues/23698#issuecomment-673667914.\r\n\r\nThis minimum example in fact has no layers, so problem seems to be independent of model architecture.\r\n\r\nAlso, the colab notebook that replicated the issue is running version 2.3", "Assigning to Tom. It seems that we do special handling for class weights based on the shape of y, and do arg_max for it. So the error might be caused by the dataset element shape.", "The issue is fixed in Tf Nightly version, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/26ecbbef63780e37e88654175a021c69/42696.ipynb). Thanks!", "Closing the issue since the issue is fixed in Tf Nightly, feel free to reopen. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42696\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42696\">No</a>\n"]}, {"number": 42695, "title": "C++ convert_numpy_inputs, _as_ndarray, _is_ndarray", "body": "This should precede the merge of a C++ version of `canonicalize_function_inputs`", "comments": ["Benchmarks over 3 batches of 10 trials each\r\n\r\nMinimum\r\ndefun_matmul_2_by_2_CPU: 151 -> 154.5 us\r\ndefun_matmul_2_by_2_CPU_async: 80.5 -> 81 us\r\ndefun_matmul_2_by_2_with_signature_CPU: 206 -> 206 us\r\n\r\nMean\r\ndefun_matmul_2_by_2_CPU: 159.5 -> 158 us\r\ndefun_matmul_2_by_2_CPU_async: 81.5 -> 83 us\r\ndefun_matmul_2_by_2_with_signature_CPU: 216.5 -> 214 us\r\n\r\nMedian\r\ndefun_matmul_2_by_2_CPU: 159.5 -> 158 us\r\ndefun_matmul_2_by_2_CPU_async: 81.5 -> 83 us\r\ndefun_matmul_2_by_2_with_signature_CPU: 213 -> 215 us\r\n\r\nInconclusive", "Offline discussion(for record): migrating the counter itself saves little but additionally migrating `FrequentTracingDetector` saves ~1us so let's also include that.  Then we can also avoid using a lock(https://github.com/tensorflow/tensorflow/blob/1d5a16dee74d2fab18cea44654c194cec4a45161/tensorflow/python/eager/def_function.py#L92) because we can rely on Python GIL.\r\n\r\n```\r\nMicroBenchmarks.benchmark_cpp_counter\r\ntime               : 0.19 us\r\nruns_per_second    : 5274818\r\n\r\nMicroBenchmarks.benchmark_python_counter\r\ntime               : 0.28 us\r\nruns_per_second    : 3619124\r\n\r\nMicroBenchmarks.benchmark_freq_detector\r\ntime               : 1.30 us\r\nruns_per_second    : 770743\r\n```", "Updated Benchmarks over 3 batches of 10 trials each\r\n\r\nMinimum\r\ndefun_matmul_2_by_2_CPU: 157 -> 154.5 us\r\ndefun_matmul_2_by_2_CPU_async: 79 -> 79 us\r\ndefun_matmul_2_by_2_with_signature_CPU: 207 -> 210.5 us\r\n\r\nMean\r\ndefun_matmul_2_by_2_CPU: 167 -> 163 us\r\ndefun_matmul_2_by_2_CPU_async: 81.5 -> 81.5 us\r\ndefun_matmul_2_by_2_with_signature_CPU: 234 -> 229.5 us\r\n\r\nMedian\r\ndefun_matmul_2_by_2_CPU: 166.5 -> 161 us\r\ndefun_matmul_2_by_2_CPU_async: 81.5 -> 81.5 us\r\ndefun_matmul_2_by_2_with_signature_CPU: 221 -> 221 us\r\n\r\nInconclusive", "Yes, the benchmarks in the last comment are updated @sun51 ", "this pr is still not merged yet.", "@jonathanchu33  Can you please resolve conflicts? Thanks!", "@jonathanchu33  Any update on this PR? Please. Thanks!\r\n", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 42693, "title": "proximal policy gradient tensorflow pendulum issue", "body": "import gym  \r\nimport numpy as np  \r\nimport tensorflow as tf\r\n\r\n\r\nclass Memory(object):  \r\n    \r\n    def __init__(self):\r\n        self.ep_obs, self.ep_act, self.ep_rwd, self.ep_neglogp = [], [], [], []\r\n\r\n    def store_transition(self, obs0, act, rwd, neglogp):\r\n        self.ep_obs.append(obs0)\r\n        self.ep_act.append(act)\r\n        self.ep_rwd.append(rwd)\r\n        self.ep_neglogp.append(neglogp)\r\n\r\n    def covert_to_array(self):\r\n        array_obs = np.vstack(self.ep_obs)\r\n        array_act = np.vstack(self.ep_act)\r\n        array_rwd = np.array(self.ep_rwd)\r\n        array_neglogp = np.array(self.ep_neglogp).squeeze(axis=1)\r\n        return array_obs, array_act, array_rwd, array_neglogp\r\n\r\n    def reset(self):\r\n        self.ep_obs, self.ep_act, self.ep_rwd, self.ep_neglogp = [], [], [], []\r\n\r\n\r\nclass ActorNetwork(object):\r\n    \r\n    def __init__(self, act_dim, name):\r\n        self.act_dim = act_dim\r\n        self.name = name\r\n\r\n    def step(self, obs, reuse):\r\n        with tf.variable_scope(self.name, reuse=reuse):\r\n            h1 = tf.layers.dense(obs, 100, activation=tf.nn.relu)\r\n            mu = 2 * tf.layers.dense(h1, self.act_dim, activation=tf.nn.tanh)\r\n            sigma = tf.layers.dense(h1, self.act_dim, activation=tf.nn.softplus)\r\n            pd = tf.distributions.Normal(loc=mu, scale=sigma)\r\n        return pd\r\n\r\n    def choose_action(self, obs, reuse=False):\r\n        pd = self.step(obs, reuse)\r\n        action = tf.squeeze(pd.sample(1), axis=0)\r\n        action = tf.clip_by_value(action, -2, 2)\r\n        return action\r\n\r\n    def get_neglogp(self, obs, act, reuse=False):\r\n        pd = self.step(obs, reuse)\r\n        logp = pd.log_prob(act)\r\n        return logp\r\n\r\n\r\nclass ValueNetwork(object):\r\n    \r\n    def __init__(self, name):\r\n        self.name = name\r\n\r\n    def step(self, obs, reuse):\r\n        with tf.variable_scope(self.name, reuse=reuse):\r\n            h1 = tf.layers.dense(inputs=obs, units=100, activation=tf.nn.relu)\r\n            value = tf.layers.dense(inputs=h1, units=1)\r\n            return value\r\n\r\n    def get_value(self, obs, reuse=False):\r\n        value = self.step(obs, reuse)\r\n        return value\r\n\r\n\r\nclass PPO(object):\r\n    \r\n    def __init__(self, act_dim, obs_dim, lr_actor, lr_value, gamma, clip_range):\r\n        self.act_dim = act_dim\r\n        self.obs_dim = obs_dim\r\n        self.lr_actor = lr_actor\r\n        self.lr_value = lr_value\r\n        self.gamma = gamma\r\n        self.clip_range = clip_range\r\n\r\n        self.OBS = tf.placeholder(tf.float32, [None, self.obs_dim], name=\"observation\")\r\n        self.ACT = tf.placeholder(tf.float32, [None, self.act_dim], name=\"action\")\r\n        self.Q_VAL = tf.placeholder(tf.float32, [None, 1], name=\"q_value\")\r\n        self.ADV = tf.placeholder(tf.float32, [None, 1], name=\"advantage\")\r\n        self.NEGLOGP = tf.placeholder(tf.float32, [None, 1], name=\"old_neglogp\")\r\n\r\n        actor = ActorNetwork(self.act_dim, 'actor')\r\n        value = ValueNetwork('critic')\r\n        self.memory = Memory()\r\n\r\n        self.action = actor.choose_action(self.OBS)\r\n        self.neglogp = actor.get_neglogp(self.OBS, self.ACT, reuse=True)\r\n        ratio = tf.math.exp(self.neglogp - self.NEGLOGP)\r\n        clip_ratio = tf.clip_by_value(ratio, 1. - self.clip_range, 1. + self.clip_range)\r\n        actor_loss = -tf.reduce_mean((tf.minimum(ratio* self.ADV, clip_ratio* self.ADV)) )\r\n        self.actor_train_op = tf.train.AdamOptimizer(self.lr_actor).minimize(actor_loss)\r\n\r\n        self.value = value.get_value(self.OBS)\r\n        self.advantage = self.Q_VAL - self.value\r\n        value_loss = tf.reduce_mean(tf.square(self.advantage))\r\n        self.value_train_op = tf.train.AdamOptimizer(self.lr_value).minimize(value_loss)\r\n\r\n        self.sess = tf.Session()\r\n        self.sess.run(tf.global_variables_initializer())\r\n\r\n    def step(self, obs):\r\n        if obs.ndim < 2: obs = obs[np.newaxis, :]\r\n        action = self.sess.run(self.action, feed_dict={self.OBS: obs})\r\n        action = np.squeeze(action, 1).clip(-2, 2)\r\n\r\n        neglogp = self.sess.run(self.neglogp, feed_dict={self.OBS: obs, self.ACT: action[np.newaxis, :]})\r\n\r\n        value = self.sess.run(self.value, feed_dict={self.OBS: obs})\r\n        value = np.squeeze(value, 1).squeeze(0)\r\n        return action, neglogp, value\r\n\r\n    def learn(self, last_value, done):\r\n        obs, act, rwd, neglogp = self.memory.covert_to_array()\r\n        rwd = (rwd - rwd.mean()) / (rwd.std() + 1e-5)\r\n        q_value = self.compute_q_value(last_value, obs, rwd)\r\n\r\n        adv = self.sess.run(self.advantage, {self.OBS: obs, self.Q_VAL: q_value})\r\n\r\n        [self.sess.run(self.value_train_op,\r\n                       {self.OBS: obs, self.Q_VAL: q_value}) for _ in range(10)]\r\n        [self.sess.run(self.actor_train_op,\r\n                          {self.OBS: obs, self.ACT: act, self.ADV: adv, self.NEGLOGP: neglogp}) for _ in range(10)]\r\n\r\n        self.memory.reset()\r\n\r\n    def compute_q_value(self, last_value, obs, rwd):\r\n        q_value = np.zeros_like(rwd)\r\n        value = self.sess.run(self.value, feed_dict={self.OBS: obs})\r\n        for t in reversed(range(0, len(rwd)-1)):\r\n            q_value[t] = value[t+1] * self.gamma + rwd[t]\r\n        return q_value[:, np.newaxis]\r\n\r\n\r\nenv = gym.make('Pendulum-v0')\r\nenv.seed(1)\r\nenv = env.unwrapped\r\n\r\nagent = PPO(act_dim=env.action_space.shape[0], obs_dim=env.observation_space.shape[0],\r\n            lr_actor=0.0004, lr_value=0.0003, gamma=0.9, clip_range=0.2)\r\n\r\nnepisode = 1000\r\nnstep = 200\r\n\r\nfor i_episode in range(nepisode):\r\n    obs0 = env.reset()\r\n    ep_rwd = 0\r\n\r\n    for t in range(nstep):\r\n        act, neglogp, _ = agent.step(obs0)\r\n        obs1, rwd, done, _ = env.step(act)\r\n\r\n        agent.memory.store_transition(obs0, act, rwd, neglogp)\r\n\r\n        obs0 = obs1\r\n        ep_rwd += rwd\r\n\r\n        if (t + 1) % 32 == 0 or t == nstep - 1:\r\n            _, _, last_value = agent.step(obs1)\r\n            agent.learn(last_value, done)\r\n\r\n    print('Ep: %i' % i_episode, \"|Ep_r: %i\" % ep_rwd)\r\n\r\nI've implemented proximal policy optimization on tensorflow, enviroment pendulum v0\r\n\r\n    if there is something wrong above tell me, why my code its not working, it's days that im trying to fix this, but my pendulum doesnt converge, I dont see what can be wrong ? can someone help? I've implemented proximal policy optimization on tensorflow, enviroment pendulum v0\r\n    Im implementing a reinforcement learning algorithm, Im trying so hard to make this code to work no success;\r\n", "comments": ["@oussama00000,\r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a TensorFlow bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42692, "title": "[XLA] Update related to concat(many slices, ...) and fusion.", "body": "All the changes are related to concat(many slices):\r\n- This fix the algebraic simplification fixed point warning of too much iteration by doing all the works in one pass.\r\n   This should end up with simpler graph.\r\n- Allow more fusion:\r\n    - From 18 to ~64 by the last commit. Mostly the check that the fusion code generator wasn't going in an exponential case was too restrictive for concatenate. Remove that special case.\r\n    - From ~64 to > 128:\r\n    - The variadic_op_splitter split concat to a max of 128 inputs. But the fusion optimizer doesn't fuse if the results have more then 64 inputs + outputs. Now, if the new fusion have less inputs then the original, we do not block the fusion.", "comments": []}, {"number": 42691, "title": "Cannot build network with a dict input_spec", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 (in docker container)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.2, N/A\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nCannot build a model with a `dict` input_spec (`model.build`)\r\n\r\n**Describe the expected behavior**\r\n\r\nThe model should be able to build the graph with a `dict` input_spec:\r\n\r\n1. We can call a model with dictionary inputs, so specifying that spec as dictionary seems natural\r\n2. I believe this is what the developers intended too, given [this line](https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/engine/network.py#L650) in the current master branch. \r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nIn [33]: class TestModel(keras.Model):\r\n    ...:     def __init__(self, *args, **kwargs):\r\n    ...:         super(TestModel, self).__init__(*args, **kwargs)\r\n    ...:         self.layer = keras.layers.Dense(10)\r\n    ...:     def call(self, inputs):\r\n    ...:         return {'b': self.layer(inputs['a'])}\r\n    ...: \r\n\r\nIn [34]: model = TestModel()\r\n\r\nIn [35]: model.build({ 'a': (None, 5) })\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-35-f5e425698cd1> in <module>\r\n----> 1 model.build({ 'a': (None, 5) })\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in build(self, input_shape)\r\n    633                        'Please specify a batch input shape of type tuple or '\r\n    634                        'list of input shapes. User provided '\r\n--> 635                        'input type: {}'.format(type(input_shape)))\r\n    636\r\n    637     if input_shape and not self.inputs:\r\n\r\nValueError: Specified input shape is not one of the valid types. Please specify a batch input shape of type tuple or list of input shapes. User provided input type: <class 'dict'>\r\n\r\nIn [36]: model({'a': np.random.random((3,5)).astype(np.float32)})\r\nOut[36]: \r\n{'b': <tf.Tensor: shape=(3, 10), dtype=float32, numpy=\r\n array([[ 0.57159793, -0.54181933, -0.05867613, -0.13133162,  0.33426586,\r\n          0.88725936,  0.6102787 ,  0.25660542, -0.32508045,  0.18882477],\r\n        [ 0.5146243 , -0.07790124, -0.13295858,  0.33439606,  0.6062204 ,\r\n          0.67318   ,  0.7965131 , -0.09740189, -0.49425927, -0.3331495 ],\r\n        [ 0.828179  , -0.3307852 , -0.49201646, -0.16802055,  0.44089833,\r\n          0.8766354 ,  0.92791915,  0.28562796, -0.8893677 , -0.428018  ]],\r\n       dtype=float32)>}\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@banerjs \r\nI am able to replicate the issue reported, please find [gist here](https://colab.research.google.com/gist/Saduf2019/332a52117a258cdab41d5502b51654c9/untitled389.ipynb).\r\nPlease refer to [this link](https://stackoverflow.com/questions/33684657/issue-feeding-a-list-into-feed-dict-in-tensorflow) and let us know if it helps.", "Yeah, I get that and I do have a work-around for the problem of building with a dictionary (so it's not a critical bug for me).\r\n\r\nHowever, it is strange behaviour because:\r\n\r\n1. The `ValueError` is raised on [line 632](https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/engine/network.py#L632) where the `input_shape` can only be of types `tuple, list, TensorShape`; and yet later in the same function in [line 650](https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/engine/network.py#L650) the function checks to see if the variable is a dictionary and handles that condition.\r\n1. The fact that we can pass inputs as a dictionary to the model seems to suggest that when we want to build the model with placeholder shapes specified as a dictionary, the API should allow that too.", "Added a PR #42969 for the fix.", "@banerjs Looks like this was resolved in recent `tf-nightly` and stable `TF2.4.1`. I don't see any error when I ran your code with `tf-nightly`. Please check a gist [here](https://colab.research.google.com/gist/jvishnuvardhan/720245bfc87ccff286f09b725a447f59/untitled389.ipynb).\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "Sorry, I no longer have access to the codebase or machines that I tested this on initially. I'm happy to have you close this if this does indeed work (running the colab was fine for me)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42691\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42691\">No</a>\n"]}, {"number": 42690, "title": "warning: 404 not found", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution : SMP Debian 4.19.118-2 (2020-04-29)\r\n- TensorFlow installed from : source\r\n- TensorFlow version: 2.3\r\n- Python version: 3.7.3\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): 3.3\r\n- GCC/Compiler version (if compiling from source):8.3\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\nwhile building I get the following warning what does it mean?\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/642cb7865f35ad7dbac78d716fcddaff561e8639.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel build --config=mkl  //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["This might be a network error. Please take a look at this [issue](https://github.com/google/mediapipe/issues/533) and let me know if it helps.", "Nope, It happens repeatedly ", "I am experiencing the same problem.  ", "It is a warning, the compilation process continues from the other mirror.", "The compilation fails with these types of error messages:\nERROR:\n/home/melrobin/.cache/bazel/_bazel_melrobin/f2116cf848eff0a1d187a7773c1fea53/external/com_google_protobuf/BUILD:301:11:\nC++ compilation of rule '@com_google_protobuf//:protoc_lib' failed (Exit\n1): crosstool_wrapper_driver_is_not_gcc failed: error executing command\nexternal/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc\n-MD -MF\nbazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/code_generator.d\n... (remaining 49 argument(s) skipped)\n\nAre they not related?\n\nOn Fri, Oct 23, 2020 at 9:52 PM Mihai Maruseac <notifications@github.com>\nwrote:\n\n> It is a warning, the compilation process continues from the other mirror.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/42690#issuecomment-715661759>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACQ6BLBSI7IVWLPATLMY4KLSMI6NXANCNFSM4QMICD2Q>\n> .\n>\n", "No, the error you pointed is from protobuf, whereas the warning is from llvm.", "Thanks @mihaimaruseac! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42690\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42690\">No</a>\n", "Hi @electricSamarth . Did you find the work-around?", "Does anyone here have any insight about the problem described at the bottom of this stackoverflow post: https://stackoverflow.com/questions/65402617/tensorflow-automl-model-in-react"]}, {"number": 42689, "title": "[ROCm] Switch rocm build to use ROCm 3.7", "body": "This PR has the changes required to switch the `--config=rocm` build to use ROCm 3.7\r\n\r\n--------------------------------------\r\n\r\n/cc @chsigg @cheshire @nvining-work ", "comments": ["@chsigg gentle ping", "@chsigg gentle ping", "@chsigg gentle ping", "@chsigg gentle ping", "gentle ping", "@chsigg gentle ping", "@deven-amd Can you please resolve conflicts? Thanks!", "@gbaned rebased the PR to remove the merge conflict", "@cheshire please re-approve..thanks"]}, {"number": 42687, "title": "[docs] `nbfmt` removes output cells but does not clear \"execution count\"", "body": "Using `python -m tensorflow_docs.tools.nbfmt` on an executed notebook clears the output cells but leaves the \"execution count\" filled in, which can result in unnecessary diff noise and only really makes sense in the context of a filled-out notebook. For reference, the \"all output -> clear\" jupyter menu option sets all the execution counts to `null`.", "comments": ["Thanks for the report, @mpharrigan \r\nHopefully this fixed it: https://github.com/tensorflow/docs/commit/3dca32f59ff76a1fed82f5e799a4dc38e498b7c6\r\n", "Thanks!"]}, {"number": 42686, "title": "Unrecognized command line -msse3 while cross-compiling tensorflow to aarch64 targets", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\n   -> Linux Ubuntu 18.04 (docker image)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n   -> Build setup is x86_64 machine\r\n- TensorFlow installed from (source or binary):\r\n   -> Source (compiling from source)\r\n- TensorFlow version:\r\n   -> v2.3.0\r\n- Python version:\r\n   -> python3.6\r\n- Installed using virtualenv? pip? conda?:\r\n   -> installed using pip3\r\n- Bazel version (if compiling from source):\r\n   -> bazel 3.1.0 (recommended for v2.3.0 inside documents)\r\n- GCC/Compiler version (if compiling from source):\r\n   -> Cross-compiler: aarch64-linux-gnu-gcc-7 (installed by nvidia sdk manager)\r\n- CUDA/cuDNN version:\r\n   -> Cuda: 10.2, cuDNN 7\r\n- GPU model and memory:\r\n   -> NA\r\n\r\n**Describe the problem**\r\naarch64-linux-gnu-gcc-7: error: unrecognized command line option '-msse3'\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nThis is the build command that I am using:\r\nbazel build --cpu=aarch64 --config=opt --config=noaws //tensorflow:libtensorflow_cc.so --sandbox_debug \r\n\r\nPerformed ./configure before running the above build command.\r\nThis is the output of .tf_configure_bazel.rc file:\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/bin/python3.6\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/lib/python3/dist-packages\"\r\nbuild --python_path=\"/usr/bin/python3.6\"\r\nbuild --config=xla\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr/local/cuda-10.2\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"6.1\"\r\nbuild --action_env LD_LIBRARY_PATH=\"/usr/local/cuda-10.2/lib64:/usr/lib/x86_64-linux-gnu:/usr/lib/i386-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/aarch64-linux-gnu-gcc-7\"\r\nbuild --config=cuda\r\nbuild:opt --copt=-march=armv8-a\r\nbuild:opt --define with_default_optimizations=true\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest --test_env=LD_LIBRARY_PATH\r\ntest:v1 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial\r\ntest:v1 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu\r\ntest:v2 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial,-v1only\r\ntest:v2 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu,-v1only\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["[log.txt](https://github.com/tensorflow/tensorflow/files/5132431/log.txt)\r\nThis is the log file corresponding to the run described above\r\n", "The issue is likely coming from here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorflow.bzl#L335\r\n\r\nBrian, you have looked into these files before. Could you take this issue?\r\n", "Hello, I was wondering if someone had a chance to look at it", "Hello, Just pinging to get to know the status: Even if it is currently going to be unsupported.\r\nAlternatively, can you post a link to a guide somewhere where I could try changing the build files and possibly contribute a solution?\r\n", "Is it possible that we could move this to 'Feature Request': Support cross compilation of libtensorflow_cc.so with CUDA and TensorRT?", "@coolchaits,\r\n\r\nWe are checking to see if this is still an issue. Can you try building latest version of Tensorflow for `aarch64`using this [guide](https://www.tensorflow.org/lite/guide/build_arm#cross-compilation_for_arm_with_bazel) and let us know if it helps? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42686\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42686\">No</a>\n"]}, {"number": 42685, "title": "TFLM: added optimized kernel directory for vexriscv", "body": "This PR adds a directory as a placeholder for kernels optimized specifically for VexRISC-V, I also include a README in the directory, as discussed with @advaitjain earlier today.", "comments": []}, {"number": 42684, "title": "Timing Discrepency betweeen Tflite Benchmarking Tool and Android Studio Inference Time", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 1.15\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhile Benchmarking the model using the benchmarking tool the timing reported did not match that as viewed while running inference on the model on an APK. In almost all instances, the timing reported was almost twice as that seen on the Benchmarking tool. The GPU delegate is used to run inference on this model. \r\nTo measure timing on the APK we employed both the methods. \r\ni) Using the getLastNativeInferenceDurationNanoseconds() function provided by Tflite\r\nii) Using the SystemClock.uptimeMillis() function provided in the os.SystemClock library. \r\n   The code for the same is:\r\n            long str_time = SystemClock.uptimeMillis();\r\n            tflite_intep.run(inp,out);\r\n            System.out.println(\"Inference Time :\"+(SystemClock.uptimeMillis()-str_time)); \r\n\r\nIn this instance the output from both the getLastNativeInferenceDurationNanoseconds() as well as the Benchmarking tool is 35.421 ms while that shown in the logcat from the SystemClock function is 70.33 ms. \r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nThe timing shown in all 3 scenarios need to match or at least be within a reasonable amount from each other. \r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n\r\nPlease do let me know if any furthur information is needed. ", "comments": ["Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 42683, "title": "[XLA] Upstream master grid size", "body": "XLA grid size isn't optimal. This PR use a better grid size.\r\nFor the Fused GELU kernel, we speed up the kernel from 78us to 74us.\r\n\r\nHere is the changes in the grid size and block size changes.\r\nOld block_size: min(num_elements, RoundUpToNearest(max_threads_per_block / unroll_factor, 32))\r\nNow block size: min(**128,** num_elements, RoundUpToNearest(max_threads_per_block / unroll_factor, 32))\r\n\r\nOld grid_size: CeilOfRatio(num_elements, threads_per_block)\r\nNew grid_size: **nb_sm * (max_threads_per_sm/threads_per_block);**\r\n\r\nOld number of elements per threads: unroll_factor\r\nNew number of elements per threads: **loop over** unroll_factor (that strides over the whole grid.) ", "comments": ["I ran our internally wrapped Resnet50 v1.5 with v100 + batch=256 + fp16, and it's regressed by 9%. The single step GPU time increased from 194.6 ms to 213.4 ms.\r\n\r\nI think the external version is here: https://github.com/tensorflow/benchmarks/blob/c55b4aac7861b3c519803d207045c045c9440c4d/scripts/tf_cnn_benchmarks/leading_indicators_test.py#L459\r\n\r\n...but I don't know how exactly to run the external version.", "I'll look into this. Can you confirm it is an 8 GPUs case that you ran?", "I ran on a single GPU.", "I was able to reproduce it with this command:\r\n\r\n`python tf_cnn_benchmarks.py --model='resnet50_v1.5' --batch_size=256 --distortions=False --use_fp16=True --optimizer='momentum' --loss_type_to_report='base_loss' --compute_lr_on_cpu=True --single_l2_loss_op=True --xla=True`\r\n\r\nold code: 1160.78 images/sec, new code: 1115.6. The difference is smaller (~4%), but I should be able to investigate with this.", "I updated this PR. Mostly, the new grid/blocks size was used when it shouldn't be used, like selec_and_scatter.\r\nI did more speed tests here and didn't found other speed regression.", "Can you confirm that Inception V3 [1] is not regressed? I saw 4% of regression internally.\r\n\r\nBesides Inception V3, I also saw other non tf_cnn_benchmark regressions. I'll see if they are public / I can surface them.\r\n\r\n[1] https://github.com/tensorflow/benchmarks/blob/c55b4aac7861b3c519803d207045c045c9440c4d/scripts/tf_cnn_benchmarks/leading_indicators_test.py#L772", "I'm also seeing ~13% regression on bert-squad: https://github.com/tensorflow/models/blob/master/official/nlp/bert/run_squad.py\r\n\r\nCan you take a look?", "Quick update.\r\nI was able to reproduce and mostly fix the 2nd regression found. Sadly, it still regress by a very small amount, but it seems consistent. I found a kernel that cause this. But this one will request that I implement more works.\r\nMostly, that kernel have row broadcasting in it. Currently the new grid doesn't save any loads/compuation with row or columns broadcasting . It does it only for scalar broadcasting. So I see 2 choices:\r\n1) Limit this when there is only scalar broadcasting.\r\n2) Make the kernel row/columns broadcasting aware to have some gain.\r\n\r\nAs my original goal was the GELU kernel and it need 2), I'll try to include that in this PR.\r\n\r\nI'll also look at the 3rd model to help make sure nothing extra is missing.", "I was able to limit even more when my change is done. It now still get the speed up I was looking for and doesn't trigger the 2nd regression you gave me (Inception V3).\r\n\r\nI'm not able to reproduce the bert-squad regression. Do you still see it now? If so, can you give me the command line used to reproduce it?", "> I was able to limit even more when my change is done. It now still get the speed up I was looking for and doesn't trigger the 2nd regression you gave me (Inception V3).\r\n> \r\n> I'm not able to reproduce the bert-squad regression. Do you still see it now? If so, can you give me the command line used to reproduce it?\r\n\r\nThe bert benchmark I ran has internal dependencies, so it's not easy to reproduce with the github version. It doesn't have 13% regression, and now it has 1% regression, but very consistent through multiple runs. \r\n\r\nI did some experiment, that limit few_waves to only instructions that satisfies `instr->IsElementWise()`. This might turn off the GELU case you are interested, but at least I see all regressions gone. I left a comment in the related code to see if we can approach this from the other end (only enable a limited set of instructions).", "I can look at doing a allowlist instead of using a deny list.\r\nBut can you share the HLO/all HLO of the case that have a slow down? I would prefer to enable it for enough cases, even if it use an allowlist. Looking at the HLO could help understand what is going on. If you can't share the HLO, sharing the op inside it would be better then nothing.", "> I can look at doing a allowlist instead of using a deny list.\r\n> But can you share the HLO/all HLO of the case that have a slow down? I would prefer to enable it for enough cases, even if it use an allowlist. Looking at the HLO could help understand what is going on. If you can't share the HLO, sharing the op inside it would be better then nothing.\r\n\r\nIs it easier for you to check in the code first behind a flag (xla::DebugOptions), and we worry about the regressions later? The code itself overall looks good to me.", "I do not like adding a new option not enabled by default. It make it hard to get the benefit from it.\r\nBut we already spend enough time on this. So I want with an allowlist instead of a denylist.\r\nThis way, I still have my speed up. I enable just scalar broadcast, element-wise and parameter operations.\r\n\r\nIf that still show slow down, it will be hard to limit it. Maybe limit it to unary and binary element-wise? I need both of them. Element-wise with 3 inputs isn't needed, but I didn't block it.", "> I do not like adding a new option not enabled by default. It make it hard to get the benefit from it.\r\n> But we already spend enough time on this. So I want with an allowlist instead of a denylist.\r\n> This way, I still have my speed up. I enable just scalar broadcast, element-wise and parameter operations.\r\n> \r\n> If that still show slow down, it will be hard to limit it. Maybe limit it to unary and binary element-wise? I need both of them. Element-wise with 3 inputs isn't needed, but I didn't block it.\r\n\r\nI re-ran all tests and they don't regress now. I left some comments for the style, and the rest looks good!", "I updated the code style per your comment."]}, {"number": 42682, "title": "MobilenetV3 Top 5 Accuracy issue", "body": "I use pre-trained model from this site: https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet \r\nI download v3-large_224_1.0_float.pb model link is (Large dm=1 (float) ). After I run it, I often get this result,\r\n===== TENSORFLOW RESULTS =======\r\ncello, violoncello (score = 1.00000)\r\nhip, rose hip, rosehip (score = 0.00000)\r\nbanjo (score = 0.00000)\r\nsteel drum (score = 0.00000)\r\nmarimba, xylophone (score = 0.00000)\r\nThen, I change input image, but the result is the same. I do know how to solve this problem.Thank you.\r\nMy code is as below:\r\n[mobilenet_v3.txt](https://github.com/tensorflow/tensorflow/files/5131057/mobilenet_v3.txt)\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@jiaqinghao2009 \r\nPlease fill in the issue template for us to replicate the issue faced, we would need the tf version, simple stand alone indented code, system information or if possible share a colab gist with the issue reported and error logs.\r\n\r\n[You could use this [link](https://colab.sandbox.google.com/#create=true&language=python3) to run the code and share the Gist (File -> Save a copy as Github gist) with us. Thanks!]\r\n\r\n\r\n", "Sorry, I can not open your link. My tensorflow version is 1.14.0., Ubuntu 18.04, llvm 10.0. I post the same question at [https://stackoverflow.com/questions/63607979/mobilenetv3-top-5-accuracy-issue]. \r\nWhen I run mobilenetv2, I only change two line code. \r\n1. model_path = os.path.join(pwd, 'model/mobilenet_v2_1.4_224_frozen.pb') \r\n2. softmax_tensor = sess.graph.get_tensor_by_name('MobilenetV2/Predictions/Softmax:0')\r\n Result is as below: \r\n===== TENSORFLOW RESULTS ======= \r\nTibetan mastiff (score = 0.14538)\r\nice cream, icecream (score = 0.09276) \r\nalligator lizard (score = 0.02763) \r\nwater tower (score = 0.02139) \r\nslot, one-armed bandit (score = 0.01965)\r\nI wonder why mobilenetv3 get a 1.00000 score four 0.00000 score.\r\np.s. Code related with tvm can be deleted(I forget delete this code.)", "@ConcentrativeMan \r\n\r\nWe will need the code used to replicate the issue, or a colab gist for us to analyse.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42679, "title": "Bug using dictionary in tensorflow2.3", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: /\r\n- GPU model and memory: /\r\n\r\nThe code below gives an incorrect summary.\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nlayers = tf.keras.layers\r\n\r\n\r\nclass LayerTest(layers.Layer):\r\n    def __init__(self):\r\n        super(LayerTest, self).__init__()\r\n\r\n    def call(self, inputs) -> tf.Tensor:\r\n        predictions = inputs\r\n        \r\n        for k in predictions.keys():\r\n            predictions[k] = tf.math.l2_normalize(predictions[k], axis=-1)\r\n\r\n        for step_name in predictions.keys():\r\n            loss = tf.reduce_mean(predictions[k])\r\n            \r\n        return loss\r\n    \r\n    \r\ndef Model(target_dim: int = 64):\r\n   \r\n    input_tensor = layers.Input(\r\n        shape=[target_dim], name=\"input_tensor\"\r\n    )\r\n \r\n    predictions = {'step_0': layers.Lambda(lambda _x: _x)(input_tensor), 'step_1': layers.Lambda(lambda _x: _x)(input_tensor)}\r\n    \r\n    logits = LayerTest()(predictions)\r\n\r\n    return keras.Model(inputs=input_tensor, outputs=logits)\r\n\r\n               \r\nmodel = Model()\r\n        \r\nmodel.compile()\r\n                       \r\nmodel.summary()\r\n```\r\n\r\nThe output using tensorflow2.3 is the following:\r\n\r\n```\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_tensor (InputLayer)       [(None, 64)]         0                                            \r\n__________________________________________________________________________________________________\r\nlambda_1 (Lambda)               (None, 64)           0           input_tensor[0][0]               \r\n__________________________________________________________________________________________________\r\ntf_op_layer_layer_test/l2_norma [(None, 64)]         0           lambda_1[0][0]                   \r\n__________________________________________________________________________________________________\r\ntf_op_layer_layer_test/l2_norma [(None, 1)]          0           tf_op_layer_layer_test/l2_normali\r\n__________________________________________________________________________________________________\r\ntf_op_layer_layer_test/l2_norma [(None, 1)]          0           tf_op_layer_layer_test/l2_normali\r\n__________________________________________________________________________________________________\r\ntf_op_layer_layer_test/l2_norma [(None, 1)]          0           tf_op_layer_layer_test/l2_normali\r\n__________________________________________________________________________________________________\r\ntf_op_layer_layer_test/l2_norma [(None, 64)]         0           lambda_1[0][0]                   \r\n                                                                 tf_op_layer_layer_test/l2_normali\r\n__________________________________________________________________________________________________\r\n```\r\n\r\nThe correct output using tensorflow2.2 is the following:\r\n\r\n```__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_tensor (InputLayer)       [(None, 64)]         0                                            \r\n__________________________________________________________________________________________________\r\nlambda (Lambda)                 (None, 64)           0           input_tensor[0][0]               \r\n__________________________________________________________________________________________________\r\nlambda_1 (Lambda)               (None, 64)           0           input_tensor[0][0]               \r\n__________________________________________________________________________________________________\r\nlayer_test (LayerTest)          ()                   0           lambda[0][0]                     \r\n                                                                 lambda_1[0][0]                   \r\n=================================================================================================\r\n```\r\n\r\nCreating a new dictionary to make the updates gives the correct results:\r\n\r\n```python\r\nclass LayerTest(layers.Layer):\r\n    def __init__(self):\r\n        super(LayerTest, self).__init__()\r\n\r\n    def call(self, inputs) -> tf.Tensor:\r\n        predictions = inputs\r\n\r\n        predictions2 = {}\r\n        \r\n        for k in predictions.keys():\r\n            predictions2[k] = tf.math.l2_normalize(predictions[k], axis=-1)\r\n\r\n        for step_name in predictions2.keys():\r\n            loss = tf.reduce_mean(predictions2[k])\r\n            \r\n        return loss\r\n```", "comments": ["I am able to replicate the issue reported, please find the gist here for [tf nightly](https://colab.research.google.com/gist/Saduf2019/08cc5eb832c763d1ba449a12e227d335/untitled393.ipynb) and[ tf 2.2](https://colab.research.google.com/gist/Saduf2019/c602120b86439137b22648cac72a0a4d/untitled394.ipynb)", "This is fixed with latest tf-nightly version 2.4.0-dev20200911.\r\nSee the [gist](https://colab.research.google.com/gist/ymodak/a03025d5176b75c7a6a2f189f3fc5b8d/untitled393.ipynb) Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42679\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42679\">No</a>\n"]}, {"number": 42678, "title": "update Readme.md", "body": "Changed 'my' to 'your' preference in guidelines readme.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42678) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42678) for more info**.\n\n<!-- ok -->", "We will not be encouraging one liner grammatical changes as this is expensive process, if possible please include more such changes in a single PR.Thank you\r\nCC @mihaimaruseac "]}, {"number": 42677, "title": "ValueError in tf.data.Dataset.from_tensor_slices when array contains a mixture of integers and strings", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes (variation of Load a pandas.DataFrame tutorial)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS (Bionic Beaver)\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.6.9\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nWhen a DataFrame ```df``` contains a mixture of integers and strings, calling ```tf.data.Dataset.from_tensor_slices(df.values)``` results in the error ```ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type int).```.\r\n\r\n**Describe the expected behavior**\r\n\r\nI don't think this should crash. I think it should produce a TensorSliceDataset object.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nReproduced the issue here: https://colab.research.google.com/drive/15qben_czJvRS80pVZyiltDO4FDzmHryg?usp=sharing\r\n\r\nThe code is loosely based on the \"Load a pandas.DataFrame\" tutorial, except I don't one-hot encode the \"thal\" column, as I have a similar use-case where I have a combination of sentences and numbers as features.", "comments": ["I did discover that using ```tf.data.Dataset.from_tensor_slices(df.to_dict('list'))``` seems to have the desired effect, but then surely I can't use feature columns?", "I have tried in colab with TF version 2.3, nightly version (`2.4.0-dev20200826`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/93aa47b5f037d0bfcbe20a64078cb3df/untitled271.ipynb#scrollTo=7D9CJeWslVpz).Thanks!", "@KieranLitschel, the inputs to `from_tensor_slices` need to be tensors or something convertible to tensors. A tensor has a single dtype (e.g. float64) -- tensors containing mixed types (e.g. strings and floats) are not allowed.\r\n\r\nThe guide at https://www.tensorflow.org/tutorials/load_data/pandas_dataframe converts the `thal` column to be numerical instead of a string:\r\n\r\n>df['thal'] = pd.Categorical(df['thal'])\r\ndf['thal'] = df.thal.cat.codes\r\n", "@KieranLitschel one additional resource you might find helpful -- You can see a similar example in the [guides here](https://www.tensorflow.org/tutorials/structured_data/feature_columns) that creates a dataset from a pandas df with `tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))` and then uses feature columns.", "Thank you for getting back to me, I've now realized `tf.data.Dataset.from_tensor_slices(df.to_dict('list'))` does do exactly what I wanted. But also I realized if I'm having a mixture of numeric and string input, using the Functional API would make more sense, which also means I can separate features of different types into different datasets, so that also solves the problem.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42677\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42677\">No</a>\n"]}, {"number": 42676, "title": "make debug build on Windows MSVC compile", "body": "- fix missing-return-statement errors\r\n- add partial dummy template specializations (which are not needed but without optimization the compiler doesn't find out about it)\r\n\r\nThe second part is a supplement to PR #42307.", "comments": ["> `LOG(FATAL)` calls `std::abort`. So `return` after it should not be needed. Is there a compile failure causing these returns to be needed? If yes, can we actually try to use `std::abort`?\r\n\r\nyes, on Windows debug MSVC (2019) build I get errors like these:\r\n`C:\\...\\execroot\\org_tensorflow\\tensorflow\\core\\framework\\device_base.cc(72) : error C4716: 'tensorflow::DeviceBase::name': must return a value`\r\n`C:\\...\\execroot\\org_tensorflow\\tensorflow\\core\\framework\\device_base.cc(68) : error C4716: 'tensorflow::DeviceBase::attributes': must return a value`\r\n\r\nApparently in debug mode MSVC compiler doesn't get that `LOG(FATAL, ...)` will finally call `abort()`...\r\n\r\nI agree that using `std::abort()` in all cases, instead of returns, is nicer. I'll push an update to this PR. ", "Windows Bazel CPU CI failed on `cwise_op_gpu.lo.lib(cwise_op_gpu_tanh.cu.obj) : error LNK2005: \"unsigned __int64 __cdecl Eigen::internal::_GLOBAL__N__39_cwise_op_gpu_tanh_cu_compute_70_cpp1_ii_ef4b1079::get_random_seed(void)\" (?get_random_seed@_GLOBAL__N__39_cwise_op_gpu_tanh_cu_compute_70_cpp1_ii_ef4b1079@internal@Eigen@@YA_KXZ) already defined in cwise_op_gpu.lo.lib(cwise_op_gpu_tanh.cu.obj)`. I don't see a link to this PR's code changes + see the very same error on other PRs. Maybe some other fixes need to be merged into this branch?", "Let's try rerunning", "Things passed, but internally there are a few breakages. Will look into those"]}, {"number": 42675, "title": "\u201cdefault Round and Robin\u201d, \u201ctf.fixed_ size_ Partitioner\u201d and \u201ctf.min_max_variable_partitioner\u201d ", "body": "Suppose there are three PS servers and three tf.Variable (shape=[100000,256])\u3002\r\nRound and Robin policy: each PS server puts a tensor;\r\ntf.fixed_ size_ Partitioner (3,0): assign 1 / 3 of each tensor to each PS server.\r\n\r\n**Is the computing efficiency of the two strategies the same?**\r\n\r\n**If the shape of tensor is quite different, is the latter better?**\r\n**Would it be better to have some small tensors on the same PS?**\r\n**tf.min_max_variable_partitioner\uff1amin_ slice_ size can't be used up. Will it be recycled?**\r\n\r\nThanks!\r\n", "comments": ["Hi @AI-Friend, Sounds like you are using TF 1.x, since the methods you've mentioned do not exist in TF2. Are you trying to implement your own strategy, or utilize the tf.distribute.Strategy `ParameterServerStrategy`? Let me know if I have misunderstood your question.", "Hi, @nikitamaia,  I use TF1. X, because several embedding variables are relatively large. If we use \"default Round and Robin\" strategy, the load of PS will be unbalanced. Therefore, from the perspective of load balancing, I would like to ask whether the efficiency of placing one embedding variable in multiple PS and one PS is the same?\r\n\r\nIn addition, I would like to ask if I use \"tf.estimator.Estimator\", Is the code of distributed and stand-alone identical? \r\nDistributed information is configured in \"tf.estimator.RunConfig\", Is that ok?\r\n\r\nThanks!", "Closing this issue because Github is mainly for filing bugs. For support, Stack Overflow is a better place to go since there is a larger community of people answering questions. \r\nBut to answer your question about using the estimator yes, the code should remain the same in the distributed and non-distributed case other than what you configure in the `RunConfig`. You might also need to adjust the batch size and input data function to ensure convergence. You can check out [this example here](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_estimator) from the docs."]}, {"number": 42674, "title": "Add more alternative path to find cuda library", "body": "Using docker image `nvidia/cuda:10.1-cudnn7-devel-ubuntu18.04` to compile tf and I found those missing paths show in the code.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42674) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42674) for more info**.\n\n<!-- ok -->"]}, {"number": 42673, "title": "Cannot convert a Tensor of dtype resource to a NumPy array", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- TensorFlow installed from (source or binary): Google Colab\r\n- TensorFlow version (or github SHA if from source): 2.3.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nColab is here: https://colab.research.google.com/drive/1X0MhpbPbguscxwY-UDyp9d0OsL5hANR2?usp=sharing\r\n```\r\ntfliteModel = tf.lite.TFLiteConverter.from_keras_model(model).convert()\r\n```\r\n\r\n**The output from the converter invocation**\r\n```\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-11-ef48ab45d342> in <module>()\r\n----> 1 tfliteModel = tf.lite.TFLiteConverter.from_keras_model(model).convert()\r\n      2 \r\n      3 with tf.io.gfile.GFile('my_model.tflite', 'wb') as f:\r\n      4     f.write(tfliteModel)\r\n\r\n6 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in convert(self)\r\n    807     frozen_func, graph_def = (\r\n    808         _convert_to_constants.convert_variables_to_constants_v2_as_graph(\r\n--> 809             self._funcs[0], lower_control_flow=False))\r\n    810 \r\n    811     input_tensors = [\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/convert_to_constants.py in convert_variables_to_constants_v2_as_graph(func, lower_control_flow, aggressive_inlining)\r\n   1101       func=func,\r\n   1102       lower_control_flow=lower_control_flow,\r\n-> 1103       aggressive_inlining=aggressive_inlining)\r\n   1104 \r\n   1105   output_graph_def, converted_input_indices = _replace_variables_by_constants(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/convert_to_constants.py in __init__(self, func, lower_control_flow, aggressive_inlining, variable_names_whitelist, variable_names_blacklist)\r\n    802         variable_names_whitelist=variable_names_whitelist,\r\n    803         variable_names_blacklist=variable_names_blacklist)\r\n--> 804     self._build_tensor_data()\r\n    805 \r\n    806   def _build_tensor_data(self):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/convert_to_constants.py in _build_tensor_data(self)\r\n    821         data = map_index_to_variable[idx].numpy()\r\n    822       else:\r\n--> 823         data = val_tensor.numpy()\r\n    824       self._tensor_data[tensor_name] = _TensorData(\r\n    825           numpy=data,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in numpy(self)\r\n   1061     \"\"\"\r\n   1062     # TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\r\n-> 1063     maybe_arr = self._numpy()  # pylint: disable=protected-access\r\n   1064     return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr\r\n   1065 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _numpy(self)\r\n   1029       return self._numpy_internal()\r\n   1030     except core._NotOkStatusException as e:  # pylint: disable=protected-access\r\n-> 1031       six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access\r\n   1032 \r\n   1033   @property\r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: Cannot convert a Tensor of dtype resource to a NumPy array.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n```\r\nhttps://drive.google.com/file/d/1iXmNGjMTPoqzBRpeNmTjarGGEIBK8Jdr/view?usp=sharing\r\n```\r\n\r\n**Any other info / logs**\r\nI can't convert a model which contains tf.keras.layers.experimental.preprocessing.TextVectorization to tf.lite.\r\nI've got this error:\r\n```InvalidArgumentError: Cannot convert a Tensor of dtype resource to a NumPy array.```\r\n\r\n- I saw this issue ( https://github.com/tensorflow/tensorflow/issues/32693 ) and tried to use `converter.experimental_new_converter = True` -- it did not help\r\n- I saw this answer ( https://stackoverflow.com/questions/59962509/valueerror-cannot-convert-a-tensor-of-dtype-resource-to-a-numpy-array ) and tried to use a custom layer -- it did not help\r\n\r\nPlease see this colab:\r\nhttps://colab.research.google.com/drive/1X0MhpbPbguscxwY-UDyp9d0OsL5hANR2?usp=sharing\r\n\r\nCan you please help me", "comments": ["@evyasonov \r\nPlease update the issue template with the tf version.\r\nWith respect to the error shared, please refer to:\r\n[link](https://stackoverflow.com/questions/59962509/valueerror-cannot-convert-a-tensor-of-dtype-resource-to-a-numpy-array) #39918 #32693 #42113 #38305 #37441 ", "> @evyasonov\r\n> Please update the issue template with the tf version.\r\n> With respect to the error shared, please refer to:\r\n> [link](https://stackoverflow.com/questions/59962509/valueerror-cannot-convert-a-tensor-of-dtype-resource-to-a-numpy-array) #39918 #32693 #42113 #38305 #37441\r\n\r\nI updated the issue", "I am able to replicate this issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/133680d5d43aa31cf2f1f81d49fa2191/untitled398.ipynb)", "@evyasonov Can you please check [this issue](https://github.com/tensorflow/tensorflow/issues/37441) that is very similar to you and let us know whether it resolved your issue or not. Thanks!", "@jvishnuvardhan \r\nAs you can see, first I added my comment to the issue you mentioned and only then I created this one. I tried to create a custom layed, but had no success. Maybe I did it wrong, but the resolution that woked in the issue you mention did not work for me.", "Could you try the conversion with from_saved_model within tf-nightly version? You can avoid the above error through the from_saved_model with tf-nightly version.", "@abattery \r\nI've installed tf-nightly, but now I can not import tensorflow_text because of it:\r\n```\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-2-cefb365966a1> in <module>()\r\n      1 import tensorflow          as tf\r\n----> 2 import tensorflow_text     as tft\r\n      3 import sys\r\n\r\n3 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/load_library.py in load_op_library(library_filename)\r\n     56     RuntimeError: when unable to load the library or get the python wrappers.\r\n     57   \"\"\"\r\n---> 58   lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\n     59   try:\r\n     60     wrappers = _pywrap_python_op_gen.GetPythonWrappers(\r\n\r\nNotFoundError: /usr/local/lib/python3.6/dist-packages/tensorflow_text/python/metrics/_text_similarity_metric_ops.so: undefined symbol: _ZN10tensorflow8OpKernel11TraceStringEPNS_15OpKernelContextEb\r\n```\r\n\r\nSee this colab:\r\nhttps://colab.research.google.com/drive/1X0MhpbPbguscxwY-UDyp9d0OsL5hANR2?usp=sharing  ", "@evyasonov For your problem, I think you can refer to the following issue:\r\n\r\nhttps://github.com/tensorflow/text/issues/160", "@abattery \r\nI added a comment into the issue you refered. But I found no solution of the issue. What should I do?", "@evyasonov please check out the comment in the above thread. :-) https://github.com/tensorflow/text/issues/160#issuecomment-685967575", "Hi. I'm facing the same issue.\r\n\r\nI tried using the from_saved_model with tf-nightly as @abattery suggested but I got error:\r\n`'tf.MutableHashTableV2' op is neither a custom op nor a flex op`.\r\n\r\nHere is the link to the Colab: https://colab.research.google.com/drive/1IU6dLDkxxOzMHKKqi5_WQgShDL3K-_69", "> Could you try the conversion with from_saved_model within tf-nightly version? You can avoid the above error through the from_saved_model with tf-nightly version.\r\n\r\nThis worked for me. Thanks.\r\nHope the new official version can be released soon.", "> Could you try the conversion with from_saved_model within tf-nightly version? You can avoid the above error through the from_saved_model with tf-nightly version.\r\n\r\n@abattery \r\nI've finally built and installed tf-nightly and tf-text on my laptop and I've succeed to import both nightly and text.\r\nAnd I've saved model and loaded it via `tf.lite.TFLiteConverter.from_saved_model(...)`.\r\nBUT when I called `.convert()`, I've got this error:\r\n```\r\nWARNING:tensorflow:8 out of the last 8 calls to <function recreate_function.<locals>.restored_function_body at 0x7f04a85a3ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\r\nWARNING:tensorflow:9 out of the last 9 calls to <function recreate_function.<locals>.restored_function_body at 0x7f04a85571e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n~/.virtualenvs/python3Tf/lib/python3.6/site-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    211                                                  debug_info_str,\r\n--> 212                                                  enable_mlir_converter)\r\n    213       return model_str\r\n\r\n~/.virtualenvs/python3Tf/lib/python3.6/site-packages/tensorflow/lite/python/wrap_toco.py in wrapped_toco_convert(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n     37       debug_info_str,\r\n---> 38       enable_mlir_converter)\r\n     39 \r\n\r\nException: <unknown>:0: error: loc(\"string_lookup_index_table\"): 'tf.MutableHashTableV2' op is neither a custom op nor a flex op\r\n<unknown>:0: error: loc(callsite(callsite(\"model/text_vectorization/StringLower@__inference__wrapped_model_6417\" at \"StatefulPartitionedCall@__inference_signature_wrapper_7583\") at \"StatefulPartitionedCall\")): 'tf.StringLower' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/text_vectorization/StaticRegexReplace@__inference__wrapped_model_6417\" at \"StatefulPartitionedCall@__inference_signature_wrapper_7583\") at \"StatefulPartitionedCall\")): 'tf.StaticRegexReplace' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/text_vectorization/StringSplit/StringSplitV2@__inference__wrapped_model_6417\" at \"StatefulPartitionedCall@__inference_signature_wrapper_7583\") at \"StatefulPartitionedCall\")): 'tf.StringSplitV2' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/text_vectorization/StringSplit/RaggedFromValueRowIds/RowPartitionFromValueRowIds/bincount/Bincount@__inference__wrapped_model_6417\" at \"StatefulPartitionedCall@__inference_signature_wrapper_7583\") at \"StatefulPartitionedCall\")): 'tf.Bincount' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/text_vectorization/StringSplit/RaggedFromValueRowIds/RowPartitionFromValueRowIds/Cumsum@__inference__wrapped_model_6417\" at \"StatefulPartitionedCall@__inference_signature_wrapper_7583\") at \"StatefulPartitionedCall\")): 'tf.Cumsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/text_vectorization/StringNGrams/StringNGrams@__inference__wrapped_model_6417\" at \"StatefulPartitionedCall@__inference_signature_wrapper_7583\") at \"StatefulPartitionedCall\")): 'tf.StringNGrams' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/text_vectorization/category_encoding/RaggedReduceSum/RaggedSplitsToSegmentIds/range@__inference__wrapped_model_6417\" at \"StatefulPartitionedCall@__inference_signature_wrapper_7583\") at \"StatefulPartitionedCall\")): 'tf.Range' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/text_vectorization/string_lookup/string_lookup_index_table_lookup_table_find/LookupTableFindV2@__inference__wrapped_model_6417\" at \"StatefulPartitionedCall@__inference_signature_wrapper_7583\") at \"StatefulPartitionedCall\")): 'tf.LookupTableFindV2' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/text_vectorization/category_encoding/RaggedReduceSum/UnsortedSegmentSum@__inference__wrapped_model_6417\" at \"StatefulPartitionedCall@__inference_signature_wrapper_7583\") at \"StatefulPartitionedCall\")): 'tf.UnsortedSegmentSum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n\ttf.Bincount {T = i64, device = \"\"}\r\n\ttf.Cumsum {device = \"\", exclusive = false, reverse = false}\r\n\ttf.Range {device = \"\"}\r\n\ttf.StaticRegexReplace {device = \"\", pattern = \"[!\\22#$%&()\\\\*\\\\+,-\\\\./:;<=>?@\\\\[\\\\\\\\\\\\]^_`{|}~\\\\']\", replace_global = true, rewrite = \"\"}\r\n\ttf.StringLower {device = \"\", encoding = \"\"}\r\n\ttf.StringSplitV2 {device = \"\", maxsplit = -1 : i64}\r\n\ttf.UnsortedSegmentSum {device = \"\"}Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):\r\n\ttf.LookupTableFindV2 {device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}\r\n\ttf.MutableHashTableV2 {container = \"\", device = \"\", key_dtype = !tf.string, shared_name = \"table_72\", use_node_name_sharing = false, value_dtype = i64}\r\n\ttf.StringNGrams {Tsplits = i64, device = \"\", left_pad = \"\", ngram_widths = [1, 2], pad_width = 0 : i64, preserve_short_sequences = false, right_pad = \"\", separator = \" \"}\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-12-be72feb906f0> in <module>\r\n      1 converter   = tf.lite.TFLiteConverter.from_saved_model('cannot_convert_model')\r\n      2 \r\n----> 3 tfliteModel = converter.convert()\r\n\r\n~/.virtualenvs/python3Tf/lib/python3.6/site-packages/tensorflow/lite/python/lite.py in convert(self)\r\n    756     converter_kwargs.update(quant_mode.converter_flags())\r\n    757 \r\n--> 758     result = _convert_saved_model(**converter_kwargs)\r\n    759     calibrate_and_quantize, flags = quant_mode.quantizer_flags(\r\n    760         self.inference_input_type, self.inference_output_type)\r\n\r\n~/.virtualenvs/python3Tf/lib/python3.6/site-packages/tensorflow/lite/python/convert.py in convert_saved_model(saved_model_dir, saved_model_version, saved_model_tags, saved_model_exported_names, **kwargs)\r\n    634       None,  # input_data, unused\r\n    635       None,  # debug_info_str, unused\r\n--> 636       enable_mlir_converter=True)\r\n    637   return data\r\n    638 \r\n\r\n~/.virtualenvs/python3Tf/lib/python3.6/site-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    213       return model_str\r\n    214     except Exception as e:\r\n--> 215       raise ConverterError(str(e))\r\n    216 \r\n    217   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:\r\n\r\nConverterError: <unknown>:0: error: loc(\"string_lookup_index_table\"): 'tf.MutableHashTableV2' op is neither a custom op nor a flex op\r\n<unknown>:0: error: loc(callsite(callsite(\"model/text_vectorization/StringLower@__inference__wrapped_model_6417\" at \"StatefulPartitionedCall@__inference_signature_wrapper_7583\") at \"StatefulPartitionedCall\")): 'tf.StringLower' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/text_vectorization/StaticRegexReplace@__inference__wrapped_model_6417\" at \"StatefulPartitionedCall@__inference_signature_wrapper_7583\") at \"StatefulPartitionedCall\")): 'tf.StaticRegexReplace' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/text_vectorization/StringSplit/StringSplitV2@__inference__wrapped_model_6417\" at \"StatefulPartitionedCall@__inference_signature_wrapper_7583\") at \"StatefulPartitionedCall\")): 'tf.StringSplitV2' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/text_vectorization/StringSplit/RaggedFromValueRowIds/RowPartitionFromValueRowIds/bincount/Bincount@__inference__wrapped_model_6417\" at \"StatefulPartitionedCall@__inference_signature_wrapper_7583\") at \"StatefulPartitionedCall\")): 'tf.Bincount' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/text_vectorization/StringSplit/RaggedFromValueRowIds/RowPartitionFromValueRowIds/Cumsum@__inference__wrapped_model_6417\" at \"StatefulPartitionedCall@__inference_signature_wrapper_7583\") at \"StatefulPartitionedCall\")): 'tf.Cumsum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/text_vectorization/StringNGrams/StringNGrams@__inference__wrapped_model_6417\" at \"StatefulPartitionedCall@__inference_signature_wrapper_7583\") at \"StatefulPartitionedCall\")): 'tf.StringNGrams' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/text_vectorization/category_encoding/RaggedReduceSum/RaggedSplitsToSegmentIds/range@__inference__wrapped_model_6417\" at \"StatefulPartitionedCall@__inference_signature_wrapper_7583\") at \"StatefulPartitionedCall\")): 'tf.Range' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/text_vectorization/string_lookup/string_lookup_index_table_lookup_table_find/LookupTableFindV2@__inference__wrapped_model_6417\" at \"StatefulPartitionedCall@__inference_signature_wrapper_7583\") at \"StatefulPartitionedCall\")): 'tf.LookupTableFindV2' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/text_vectorization/category_encoding/RaggedReduceSum/UnsortedSegmentSum@__inference__wrapped_model_6417\" at \"StatefulPartitionedCall@__inference_signature_wrapper_7583\") at \"StatefulPartitionedCall\")): 'tf.UnsortedSegmentSum' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n\ttf.Bincount {T = i64, device = \"\"}\r\n\ttf.Cumsum {device = \"\", exclusive = false, reverse = false}\r\n\ttf.Range {device = \"\"}\r\n\ttf.StaticRegexReplace {device = \"\", pattern = \"[!\\22#$%&()\\\\*\\\\+,-\\\\./:;<=>?@\\\\[\\\\\\\\\\\\]^_`{|}~\\\\']\", replace_global = true, rewrite = \"\"}\r\n\ttf.StringLower {device = \"\", encoding = \"\"}\r\n\ttf.StringSplitV2 {device = \"\", maxsplit = -1 : i64}\r\n\ttf.UnsortedSegmentSum {device = \"\"}Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):\r\n\ttf.LookupTableFindV2 {device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}\r\n\ttf.MutableHashTableV2 {container = \"\", device = \"\", key_dtype = !tf.string, shared_name = \"table_72\", use_node_name_sharing = false, value_dtype = i64}\r\n\ttf.StringNGrams {Tsplits = i64, device = \"\", left_pad = \"\", ngram_widths = [1, 2], pad_width = 0 : i64, preserve_short_sequences = false, right_pad = \"\", separator = \" \"}\r\n```\r\n\r\nSo `from_saved_model` don't work for this case.\r\nI uploaded my notebook to this colab: https://colab.research.google.com/drive/16650qRDj2_I1RRAd5nCHfEcWjuzNPOYs?usp=sharing\r\n\r\nCan you please help me to save my model to tflite format\r\nWhat should I do?", "@abattery @jvishnuvardhan \r\nAny update?", "@evyasonov The last failure is because the model have operations that TFLite doesn't support natively.\r\nFor Hashtable support, we are working on supporting this. Generally for other operations you can use SELECT mode which fallbacks to TF during the runtime with some cost.\r\nSee https://www.tensorflow.org/lite/guide/ops_select", "Sorry for experiencing this. TFLite currently can not support the TextVectorization keras layer right now. We are working with TensorFlow keras team to fix this.", "@karimnosseir @abattery \r\nGot it! Thank you for the answers!\r\nDo you have any estimation of when TextVectorization will be supported by TFLite?", "@evyasonov TextVectorization still doesn't seem to be working yet with TFLite. Have you received any updates on when it will be supported?", "> Sorry for experiencing this. TFLite currently can not support the TextVectorization keras layer right now. We are working with TensorFlow keras team to fix this.\r\n\r\nSo TFLite can not support tensorflow_text.BertTokenizer either? @abattery ", "tensorflow_text.BertTokenizer is not using a mutable hash table but a combination of static hash table and wordpiece tokenizer. This layer can be supported in TFLite soon with hash table support feature and Select TF ops with the tensorflow_text op.\r\n\r\n(1) You can add hash table custom ops (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/kernels/hashtable) and\r\n(2) you can link tensorflow_text project to your TFLite interpreter and converter in order to add wordpiece tokenizer support in your model.\r\n\r\n@thaink can you help with using wordpiece tokenizer?", "To use WordPiece tokenizer, you can do one of following things:\r\n1. Add the //tensorflow/lite/delegate/flex:delegate dependency to your target and then add @org_tensorflow_text//tensorflow_text:ops_lib dependency.\r\n2. Build a custom flex delegate with @org_tensorflow_text//tensorflow_text:ops_lib in additional_deps field. Following the rule here:\r\nhttps://github.com/tensorflow/tflite-support/blob/master/tensorflow_lite_support/custom_ops/BUILD#L10\r\nI would recommend to take a look at above tflite-support example. It is a good start.", "@abattery now I have the same problem, I tested my code with TF 2.2 and TF-nightly 2.5, but encountered the same problem.\r\n\r\n ```python\r\n frozen_func = convert_variables_to_constants_v2(full_model)  \r\n frozen_func.graph.as_graph_def()\r\n```\r\n\r\noutput\uff1a\r\n\r\n```python\r\n  File \"frozen.py\", line 201, in h5_to_pb\r\n    frozen_func = convert_variables_to_constants_v2(full_model)\r\n  File \"/home/.local/lib/python3.6/site-packages/tensorflow/python/framework/convert_to_constants.py\", line 1078, in convert_variables_to_constants_v2\r\n    aggressive_inlining=aggressive_inlining)\r\n  File \"/home/.local/lib/python3.6/site-packages/tensorflow/python/framework/convert_to_constants.py\", line 807, in __init__\r\n    self._build_tensor_data()\r\n  File \"/home/.local/lib/python3.6/site-packages/tensorflow/python/framework/convert_to_constants.py\", line 826, in _build_tensor_data\r\n    data = val_tensor.numpy()\r\n  File \"/home/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1072, in numpy\r\n    maybe_arr = self._numpy()  # pylint: disable=protected-access\r\n  File \"/home/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1040, in _numpy\r\n    six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot convert a Tensor of dtype resource to a NumPy array\r\n```\r\n\r\nCan you give me some advice\uff0c thank you very much\uff01\r\n", "We do not have a plan to support resource type freezing through the `convert_variables_to_constants_v2` method. If you are converting the given model to TFLite, please use `TFLiteConverter.from_saved_model` in TF v2.", "But if I want to use resource type freezing , I want to implement it myself, can I have some suggestions?", "I have the same Problem, when using a simple custom layer in keras, using convert_variables_to_constants_v2 in combination with tf.lookup.StaticHashTable. tf2.4 (worked in 2.2)....\r\nAny ideas?", "@digital-thinking could you file a new issue with the reproducible steps? It is hard to track multiple stuffs at a single thread.", "Closing this issue since we have an alternative solution in the TF v2 saved model converter API.", "FYI, TextVectorization keras layer does work with TFLite converter if you can export it to the TF v1 saved model with the hash table initializer.\r\n\r\n```\r\n  # Disable eager execution to export v1 saved model.\r\n  tf.compat.v1.disable_eager_execution()\r\n\r\n  text_dataset = tf.data.Dataset.from_tensor_slices([\"foo\", \"bar\", \"baz\"])\r\n  max_features = 5000  # Maximum vocab size.\r\n  max_len = 4  # Sequence length to pad the outputs to.\r\n  embedding_dims = 2\r\n\r\n  # Create the layer.\r\n  vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\r\n      max_tokens=max_features,\r\n      output_mode=\"int\",\r\n      output_sequence_length=max_len,\r\n      vocabulary=[\"foo\", \"bar\", \"baz\"])\r\n\r\n  # Create the model that uses the vectorize text layer\r\n  model = tf.keras.models.Sequential()\r\n\r\n  # Start by creating an explicit input layer. It needs to have a shape of\r\n  # (1,) (because we need to guarantee that there is exactly one string\r\n  # input per batch), and the dtype needs to be 'string'.\r\n  model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\r\n\r\n  # The first layer in our model is the vectorization layer. After this\r\n  # layer, we have a tensor of shape (batch_size, max_len) containing vocab\r\n  # indices.\r\n  model.add(vectorize_layer)\r\n\r\n  # Now, the model can map strings to integers, and you can add an embedding\r\n  # layer to map these integers to learned embeddings.\r\n  input_data = [[\"foo qux bar\"], [\"qux baz\"]]\r\n  model.predict(input_data)\r\n\r\n  x = tf.compat.v1.placeholder(tf.string, shape=([None, None]), name=\"input\")\r\n  y = model.call(x)\r\n\r\n  tensor_info_x = tf.compat.v1.saved_model.utils.build_tensor_info(x)\r\n  tensor_info_y = tf.compat.v1.saved_model.utils.build_tensor_info(y)\r\n\r\n  signature_def_map, init_op, assets_collection = {\r\n      \"serving_default\":\r\n          (tf.compat.v1.saved_model.signature_def_utils.build_signature_def(\r\n              inputs={\"x\": tensor_info_x},\r\n              outputs={\"y\": tensor_info_y},\r\n              method_name=\"some_function\"))\r\n  }, tf.compat.v1.tables_initializer(), None\r\n\r\n  lookup = vectorize_layer._index_lookup_layer._table_handler.table._resource_handle\r\n\r\n  import tensorflow.pythons.ops import lookup_ops\r\n  init_op = tf.group(\r\n      lookup_ops.lookup_table_insert_v2(lookup, [\"foo\", \"bar\", \"baz\"],\r\n                                        tf.constant([0, 1, 2], dtype=tf.int64)))\r\n \r\n  saved_model_dir = \"/tmp/textvec\"\r\n\r\n  sess = tf.compat.v1.Session()\r\n  sess.run(tf.compat.v1.initializers.global_variables())\r\n  builder = tf.compat.v1.saved_model.builder.SavedModelBuilder(saved_model_dir)\r\n  builder.add_meta_graph_and_variables(\r\n      sess, [tf.compat.v1.saved_model.tag_constants.SERVING],\r\n      signature_def_map,\r\n      main_op=init_op,\r\n      legacy_init_op=init_op,\r\n      assets_collection=assets_collection,\r\n      strip_default_attrs=True)\r\n  builder.save()\r\n```\r\n\r\nAnd then, it needs to re-enable the eager execution (simply, you can just have another Python script for conversion only) and converts the generated saved model with the TF v2 saved model converter:\r\n\r\n```\r\n  saved_model_dir = \"/tmp/textvec\"\r\n  converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\n  converter.target_spec.supported_ops.add(tf.lite.OpsSet.SELECT_TF_OPS)\r\n  tflite_model = converter.convert()\r\n```", "Hi,\r\nI am getting same issue in AutoKeras convertion to onnx, Is there any help \r\nhttps://github.com/onnx/keras-onnx/issues/647\r\nhttps://stackoverflow.com/questions/65135393/converting-keras-functional-model-to-tf-lite-model-onnx-format\r\nThanks", "@hanzigs please upload a new post for your issue. Fyi, resource types are supported through only saved model converter since TF 2.5.", "@abattery Sure Thanks"]}, {"number": 42672, "title": "tf.contrib.crf.crf_decode ValueError: Input size (depth of inputs) must be accessible via shape inference, but saw value None.", "body": "When I use the tf.contrib.crf.crf_decode function to find the most probably path in CTC. **The Code is:**\r\n\r\n`import numpy as np\r\nimport tensorflow as tf\r\n\r\nxs=tf.constant(np.random.rand(3,10,5),dtype=tf.float32)\r\nxlen=tf.constant([8,10,9],dtype=tf.int32)\r\nys=tf.constant([[4,1,0,0],[2,4,3,3],[3,1,2,0]],dtype=tf.int32)\r\nylen=tf.constant([2,4,3],dtype=tf.int32)\r\n\r\n> def viterbi_search(yt,xlen,ys,ylen):\r\n>     \r\n>     pad=tf.zeros_like(ys)\r\n>     yb=tf.reshape(tf.stack([ys,pad],axis=-1),[tf.shape(ys)[0],-1])\r\n>     yb=tf.concat([tf.zeros([tf.shape(ys)[0],1],dtype=tf.int32),yb],axis=-1) #\u63d2\u5165blank\u4e4b\u540e\u7684\u6807\u7b7e\r\n> \r\n>     def search_best_path(yt,xlen,ys,ylen,yb):\r\n>         # \u6784\u5efa\u6982\u7387\u53d1\u5c04\u77e9\u9635 potentials\r\n>         yt=tf.gather(yt,yb,axis=-1)\r\n>         blen=tf.shape(yb)[0]\r\n>         m=tf.reshape(tf.concat([yt[0,:2],tf.zeros(blen-2)],axis=0),(1,-1))\r\n>         n=tf.reshape(tf.concat([tf.zeros(blen-2),yt[-1,-2:]],axis=0),(1,-1))\r\n>         yt=tf.concat([m,yt[1:-1,:],n],axis=0)\r\n> \r\n>         # \u6784\u5efa\u72b6\u6001\u8f6c\u79fb\u77e9\u9635 transition_params\r\n>         const=tf.constant([[2,3],[2,2]],dtype=tf.int32)\r\n>         k=tf.cast(tf.math.equal(ys[:-1]-ys[1:],0),tf.int32)\r\n>         tlen=tf.reshape(tf.gather(const,k),(-1,))\r\n>         tlen=tf.concat([tlen,[2,2,1]],axis=0)+tf.range(blen)\r\n> \r\n>         trans=tf.map_fn(lambda k:tf.concat([tf.ones(k,dtype=tf.int32),tf.zeros(blen-k,dtype=tf.int32)],axis=0),tlen)\r\n>         trans=tf.math.log(tf.linalg.band_part(tf.cast(trans,dtype=tf.float32),0,-1))\r\n> \r\n>         yt=tf.math.log(tf.expand_dims(yt,axis=0))\r\n>         xlen=tf.reshape(xlen,shape=[-1])\r\n>         yl,_=tf.contrib.crf.crf_decode(potentials=yt,transition_params=trans,sequence_length=xlen)\r\n>         yf=tf.gather(yb,yl[0]) #\u6bcf\u4e00\u5e27\u7684\u5b9e\u9645\u5bf9\u9f50\u7ed3\u679c CTC sequence\r\n>         ref=tf.concat([[0],yf[:-1]],axis=0)\r\n>         yr=tf.where(tf.equal(yf,ref),tf.subtract(yf,ref),yf) #\u53bb\u91cd\u4ee5\u540e\u7684\u5e27\u7ea7\u522b\u5bf9\u9f50\u7ed3\u679c trigger sequence\r\n>         return tf.squeeze(tf.where(yr>0))\r\n>     pad_len=tf.reduce_max(ylen)-ylen\r\n>     def loop_align(i,zs):\r\n>         k=search_best_path(yt[i][:xlen[i]],xlen[i],ys[i][:ylen[i]],ylen[i],yb[i][:ylen[i]*2+1])\r\n>         zi=tf.concat([k,tf.zeros(pad_len[i],dtype=tf.int64)],axis=0)\r\n>         zs=zs.write(i,zi) \r\n>         return i+1,zs\r\n>     zs=tf.TensorArray(dtype=tf.int64,size=0,dynamic_size=True)\r\n>     _,zs=tf.while_loop(lambda i,zs:i<tf.shape(yt)[0],loop_align,[0,zs])\r\n>     zs=tf.cast(zs.stack(),dtype=tf.int32) #triggered sequence\r\n>     return zs\r\n\r\nzs=viterbi_search(xs,xlen,ys,ylen)`\r\n\r\n**It gives the following error:**\r\n  File \"/media/huaxin/tcl1/asr/yujiangling/Stream_Transformer/model.py\", line 135, in search_best_path\r\n    yl,_=tf.contrib.crf.crf_decode(potentials=yt,transition_params=trans,sequence_length=xlen)\r\n  File \"/media/huaxin/tcl1/asr/yujiangling/lib/python3.6/site-packages/tensorflow/contrib/crf/python/ops/crf.py\", line 589, in crf_decode\r\n    false_fn=_multi_seq_fn)\r\n  File \"/media/huaxin/tcl1/asr/yujiangling/lib/python3.6/site-packages/tensorflow/python/layers/utils.py\", line 202, in smart_cond\r\n    pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n  File \"/media/huaxin/tcl1/asr/yujiangling/lib/python3.6/site-packages/tensorflow/python/framework/smart_cond.py\", line 59, in smart_cond\r\n    name=name)\r\n  File \"/media/huaxin/tcl1/asr/yujiangling/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/media/huaxin/tcl1/asr/yujiangling/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2097, in cond\r\n    orig_res_f, res_f = context_f.BuildCondBranch(false_fn)\r\n  File \"/media/huaxin/tcl1/asr/yujiangling/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1930, in BuildCondBranch\r\n    original_result = fn()\r\n  File \"/media/huaxin/tcl1/asr/yujiangling/lib/python3.6/site-packages/tensorflow/contrib/crf/python/ops/crf.py\", line 560, in _multi_seq_fn\r\n    dtype=dtypes.int32)\r\n  File \"/media/huaxin/tcl1/asr/yujiangling/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 664, in dynamic_rnn\r\n    dtype=dtype)\r\n  File \"/media/huaxin/tcl1/asr/yujiangling/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 738, in _dynamic_rnn_loop\r\n    \"Input size (depth of inputs) must be accessible via shape inference,\"\r\nValueError: Input size (depth of inputs) must be accessible via shape inference, but saw value None.\r\n\r\nIt seems that when the tensor was send to the TensorFlow loop (tf.while_loop), the dimension of the tensor become None? Then the function tf.contrib.crf.crf_decode can not work? So, how can I solve it?", "comments": ["@yjiangling \r\n\r\nWhich tensorflow version you are using?\r\n\r\ntensorflow.contrib is being removed in version TF 2.x and it works in only TF 1.x. \r\nFor more info see this https://www.tensorflow.org/guide/migrate#a_note_on_slim_contriblayers.\r\nFor TF 2.X you may try following:\r\nA subset of contrib functions are not part of TF Addons(https://github.com/tensorflow/addons).\r\nYou have to install addons package separately.\r\nYou can also raise a PR on the addons to repo to add feature not already available.\r\n\r\nAlso, request you to share colab link or simple standalone code with proper indentation to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "> @yjiangling\r\n> \r\n> Which tensorflow version you are using?\r\n> \r\n> tensorflow.contrib is being removed in version TF 2.x and it works in only TF 1.x.\r\n> For more info see this https://www.tensorflow.org/guide/migrate#a_note_on_slim_contriblayers.\r\n> For TF 2.X you may try following:\r\n> A subset of contrib functions are not part of TF Addons(https://github.com/tensorflow/addons).\r\n> You have to install addons package separately.\r\n> You can also raise a PR on the addons to repo to add feature not already available.\r\n> \r\n> Also, request you to share colab link or simple standalone code with proper indentation to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!\r\n\r\nThanks a lot for the reply. I use the TensorFlow 1.12. Ok, I will try it. BTW, the code give before is the whole codes, it can be run in any environment as long as the package of numpy and tensorflow are installed. You may try it. Thanks again for your patience.", "I have tried in colab with TF version 1.12, 1.15 and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/55d14d41d001a31960118cc183db47ee/untitled291.ipynb).Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "> I have tried in colab with TF version 1.12, 1.15 and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/55d14d41d001a31960118cc183db47ee/untitled291.ipynb).Thanks!\r\n\r\nThank you for the reply. I finally find out the reasons. It turns out that the function of tf.contrib.crf.crf_decode in TensorFlow implemented with dynamic_rnn, in source code of dynamic_rnn function in TensorFlow (tf.nn.dynamic_rnn function is assigned in tensorflow/python/ops/rnn.py), it assume that the output shape of the RNN net could not be None, so I rewrite a new function \"crf_decode\" refer to the source code and finally solved it.", "Since the problem has been solved, I will close this issue. Thanks a agin for everyone.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42672\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42672\">No</a>\n"]}, {"number": 42671, "title": "[TFLite 16x8] Fixes for TANH and LOGISTIC", "body": "This PR provides fixes for Tanh/Logistic in case of int16. \r\nThe previous implementation of Tanh/Logistic allows only integer multiplies of the 1/4096 scaling factor. It has been changed to handle more general case. Another fixed issue is that Logistic code has no overflow check on the table lookup.\r\nTests are improved as well.", "comments": ["@wwwind  Can you please check @renjie-liu's comments and keep us posted ? Thanks!", "@wwwind  Any update on this PR? Please. Thanks!", "HI @gbaned Thanks for the reminder! I have replied. ", "@wwwind  Can you please check @renjie-liu's comments and keep us posted ? Thanks!", "@wwwind Can you please check @renjie-liu's comments and resolve conflicts?. Thanks!", "@wwwind  Can you please check @fredrec's comments and keep us posted ? Thanks!", "@wwwind  Any update on this PR? Please. Thanks!", "Hi @gbaned I left my comment above that the suggested solution does not work in this case. Could you please re-assign labels that this PR needs response for the review now ? Thanks"]}, {"number": 42670, "title": "Could not find TensorFlow Java 2.3.0 release in maven central", "body": "\r\n**System information**\r\nNot relevant\r\n\r\n\r\n**Describe the problem**\r\n```xml\r\n<dependency>\r\n  <groupId>org.tensorflow</groupId>\r\n  <artifactId>tensorflow</artifactId>\r\n  <version>2.3.0</version>\r\n</dependency>\r\n```\r\nThis dependency cannot resolve.\r\n", "comments": ["Just curious to know what has happened to the Java releases in general. It looks like 1.15 is the latest release on Maven. Are there any plans to continue these releases?", "@goldiegadde I do not know who owns the java releases, do you know?", "I wonder if there is any update on the issue?", "@jason-dai as far as I know they have split (and redone) the java bindings, you can find it at https://github.com/tensorflow/java", "@edwinRNDR Thanks, we'll take a look.", "@yangw1234,\r\n\r\nAs the version numbers used for TF and TF Java are different. Your should update the version as `0.2.0` if you want to install `TF 2.3.0` . Here's a [documentation](https://github.com/tensorflow/java/#tensorflow-version-support) that provides the mapping between `TF` and `TF Java`\r\n\r\n```js\r\n<dependency>\r\n  <groupId>org.tensorflow</groupId>\r\n  <artifactId>tensorflow</artifactId>\r\n  <version>0.2.0</version>\r\n</dependency>\r\n```\r\nLet me know if it answes your question. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42670\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42670\">No</a>\n"]}, {"number": 42669, "title": "test", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["@PannuMuthu \r\nPlease fill in the issue template, share stand alone code/ steps before issue was faced.", "Spam issue"]}, {"number": 42668, "title": "Resnet gradients cpp", "body": "@saxenasaurabh \r\n@alextp \r\n\r\nRemainder of Grads needed for Resnet", "comments": ["@saxenasaurabh \r\nI will take over this pull request", "> @saxenasaurabh\r\n> I will take over this pull request\r\n\r\nGreat, thanks!"]}, {"number": 42667, "title": "Extend documentation for the minimal tflite example. ", "body": "* Add `tflite::` prefix for all tflite types for consistency.\r\n* Extend documentation of how to read the input/output buffers of the interpreter.\r\n\r\nBonus:\r\n* Adds missing entry for `android_arm64` in `.baselrc`. ", "comments": ["Are there any more required changes or what is the status for this PR?", "> Are there any more required changes or what is the status for this PR?\r\n\r\n@nordstroem  Nothing to do at this moment. This is waiting for reviewer approval.  Thank you!"]}, {"number": 42666, "title": "Extend documentation for the minimal tflite example. ", "body": "* Add `tflite::` prefix for all tflite types for consistency.\r\n* Extend documentation of how to actually read the input/output buffers of the interpreter.\r\n\r\nBonus:\r\n* Adds missing entry for `android_arm64` in `.baselrc`. ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42666) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42666) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 42665, "title": "Update bot_config.yml", "body": "New PRs in TFLite Micro will be commented with new welcome message for better  guidelines for user.\r\n", "comments": []}, {"number": 42664, "title": "Slow training speed of  tensorflow.keras.layers.SeparableConv2D and tensorflow.keras.layers.DepthwiseConv2D", "body": "I am trying to train a Fully Convolutional Model with fast inference\r\nSo instead of **tensorflow.keras.layers.Conv2d**, I tried using **tensorflow.keras.layers.SeparableConv2D**.\r\n\r\nThe SeparableConv2D model, when benchmarked using [TensorFlow.js Model Benchmark](https://tensorflow.github.io/tfjs/e2e/benchmarks/local-benchmark/index.html), performs better than Conv2D model.\r\n\r\nBut while training on GPU, the SeparableConv2D is extremely slow when compared to the Conv2D model.\r\n\r\n**System information**\r\n- Google Colab\r\n- TensorFlow version 2.3.0 :\r\n- Python version: 3.6.9\r\n\r\nI have created a simple experiment to illustrate the issue. [Colab Notebook](https://colab.research.google.com/drive/1gZTZcj6VJfMEJS_5e2Lc3kGJKdbgnER5?usp=sharing)\r\n\r\nThe difference in the training time is very large. I also tried DepthwiseConv2D followed by pointwise Conv2d (kernel size 1)\r\n\r\nIt has come to my knowledge that slow speed of DepthwiseConv2D is a known issue. I was wondering if there is any fix or work around.", "comments": ["### Describe the problem\r\n\r\nI'm experiencing the very same issue, standard Conv2d trains >2x **faster** than Depthwise + Pointwise Conv (wich are the building blocks of SeparableConv2D).\r\n\r\n### Source code / logs\r\n[HERE](https://gist.github.com/cscribano/d4c8480167cb53a25245ed6197b8ad7b) a minimal reproducible example (requires [profiler](https://blog.tensorflow.org/2020/04/introducing-new-tensorflow-profiler.html)).\r\n\r\nHow to reproduce:\r\n```\r\npython minimal.py\r\ntensorboard --logdir=./log/\r\n```\r\n\r\nResults obtained by running the above example:\r\nDevice Computing Time: 18.1ms (**Conv2D**), 43.9ms (**Depthwise + Pointwise**)\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: YES\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Red Hat Enterprise 7.6 (Server)\r\n-   **TensorFlow installed from (source or binary)**: both tested with no changes\r\n-   **TensorFlow version**: 2.2.1\r\n-   **Python version**: 3.7.9\r\n-   **CUDA/cuDNN version**: CUDA 10.1, CuDNN 7.6.5\r\n-   **GPU model and memory**: Nvidia Tesla V100 (16GB)", "The Colab link which you have provided is expired, could you please share the code again to reproduce the reported behavior and there is a possibility of improvement in performance in the latest Tensorflow version(2.7 as of now) and you can try running in the latest version. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}]