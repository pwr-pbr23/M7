[{"number": 32130, "title": "Can not store plugin name  in logs ( which is then used by  tensorboard to read data for relavant plugin)", "body": "**System information**\r\n- OS Platform and Distribution -  Linux Ubuntu 18.04:\r\n- TensorFlow version (or github SHA if from source):1.14.0\r\n\r\nIn my existing model, i am using CNN model with tf.layers \r\n\r\n\r\ndef mnist_cnn(inputs):\r\n\r\n    \r\n\r\n\r\n`\r\ndef mnist_cnn(inputs):\r\n\r\n    input_layer = tf.reshape(inputs, [-1, 28, 28, 3])\r\n\r\n    # Convolutional Layer #1\r\n    conv1 = tf.layers.conv2d(\r\n          inputs=input_layer,\r\n          filters=32,\r\n          kernel_size=[5, 5],\r\n          padding=\"same\",\r\n          activation=tf.nn.relu,\r\n          name=\"conv1\",\r\n    )\r\n\t  \r\n\r\n\r\n    # Pooling Layer #1\r\n    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\r\n\r\n    # Convolutional Layer #2 and Pooling Layer #2\r\n    conv2 = tf.layers.conv2d(\r\n          inputs=pool1,\r\n          filters=64,\r\n          kernel_size=[5, 5],\r\n          padding=\"same\",\r\n          activation=tf.nn.relu)\r\n\r\n    # weights_1_array = np.zeros((786, 28, 28, 32))\r\n    # conv_kernel_1 = tf.nn.conv2d(conv1, weights_1_array, [1, 4, 4, 1], padding='SAME')\r\n\r\n\r\n    # kernel = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv1/kernel')[0]\r\n\r\n\r\n\r\n    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\r\n\r\n    # Dense Layer\r\n    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\r\n    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\r\n    dropout = tf.layers.dropout(inputs=dense, rate=0.4)\r\n\r\n\r\n    # Logits Layer\r\n    outputs = tf.layers.dense(inputs=dropout, units=10)\r\n`\r\nAnd i used to write logs using summary \r\n\r\n` tensor_summaries = list(map(lambda graph_node:\r\n                                tf.summary.tensor_summary(graph_node.name,\r\n                                                          graph.get_tensor_by_name(graph_node.name + \":0\"),\r\n                                                          summary_metadata=tf.SummaryMetadata(\r\n                                                              display_name=graph_node.name,\r\n                                                              summary_description=description,\r\n                                                              plugin_data=tf.SummaryMetadata.PluginData(\r\n                                                                  plugin_name=PLUGIN_NAME)),\r\n                                                          collections=collections),\r\n                                tensor_ops))`\r\n\r\nWithin this summary, there was a feature to write plugin name , this plugin name is then used by my custom tensor board to identify data from my plugin.\r\n\r\nWith tf keras , we have callbacks like save.model to store weight but that don't give an option to write plugin name this makes it impossible for tensorboard to identify  data for my plugin , \r\n\r\nIs there any way to write logs with plugin name using call back ?\r\n", "comments": ["So the new way to write arbitrary tensor summaries is using `tf.compat.v2.summary.write()`, which can accept a serialized summary metadata: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/summary/write\r\n\r\ncc @omalleyt12 - the TensorBoard keras callback works with custom summary ops now, right?  So I believe that `write()` would also work within the callback.", "@nfelt , @omalleyt12  Thanks for your suggestions although we are looking in same direction but our visions are not converging. Let me try again.\r\nOf course, there are ways to write logs from TensorFlow using write and tf.compat.v2.summary.write() but my problem is \r\nI was earlier using TF.layers\r\nwhile run \r\nfor every epoch --> summary_op = summary_function(default_graph)\r\nsees.run(summary_op)\r\n\r\nIn summary function --> I was fetching the weights tensors and adding summary data using \r\nplugin_data=tf.SummaryMetadata.PluginData( plugin_name=PLUGIN_NAME)), \r\nand rrturn this to main function which then calls \r\nsess..run(summary_op)\r\nwriter.add_summary(summary, i)\r\n\r\n\r\nNOW WITH KERAS\r\nI have no way to add function directly with epochs, neither I can trigger sessions.\r\nwith callbacks, if I call the same summary function, it returns no data as the scope of session is within the test.fit function. \r\n\r\nEven if I trigger session manually, Keras still trigger it's on session my triggered session never fetches any data.\r\n\r\nso callbacks are able to call the function but the summary function is designed to work with sessions and keras has it's on way to handling sessions \r\n\r\n\r\n", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32130\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32130\">No</a>\n"]}, {"number": 32129, "title": "[TF.2.0 API Docs] tf.ones_like", "body": "# URL(s) with the issue:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/ones_like\r\n\r\n# Description of the issue (what needs changing):\r\nchange \"Creates a tensor with all elements set to zero.\" to \"Creates a tensor with all elements set to one.\"", "comments": ["Automatically closing this out since I understand it to be resolved by the https://github.com/tensorflow/docs/pull/987 (merged already), but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32129\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32129\">No</a>\n"]}, {"number": 32128, "title": "Sessions that are closed and reset and all inputs and outputs are out of scope, do not release GPU memory.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):MacOS 10.13\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):source 1.12.0\r\n- TensorFlow version (use command below):\r\n- Python version:NA\r\n- Bazel version (if compiling from source):0.19.2\r\n- GCC/Compiler version (if compiling from source):Apple LLVM version 9.1.0 (clang-902.0.39.2)\r\n- CUDA/cuDNN version:10.0.130\r\n- GPU model and memory:GTX 1060\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nMemory from tensorflow sessions is not freed when a session is closed and no variables are in scope, memory is only freed on program exit\r\n**Describe the expected behavior**\r\nMemory should be freed from the GPU on calling close and reset in C++, or some other way to release GPU resources without forking a process.\r\n**Code to reproduce the issue**\r\n\r\n```\r\n#include <stdlib.h>     /* getenv */\r\n#include <iostream>\t\t\r\n#include <tensorflow/core/platform/init_main.h>\r\n#include <tensorflow/core/public/session.h>\r\n#include <tensorflow/core/framework/tensor_shape.h>\r\n#include <string>\r\n#include \"cuda_runtime_api.h\"\r\n#include <cuda.h>\r\n\r\n\r\nint freeCudaMem(){\r\n\t\tint gpuCount, i;\r\n\t\tCUresult res;\r\n\t\tCUdevice dev;\r\n\t\tCUcontext ctx;\r\n\t\tint result;\r\n\t\tcuInit(0);\r\n\t\tcuDeviceGetCount(&gpuCount);\r\n\t\tsize_t curClockRate = 0;\r\n\t\tsize_t curMaxMem = 0;\r\n\r\n\t\tfor (int i = 0; i < gpuCount; i++) {\r\n\t\t\tcudaSetDevice(i);\r\n\t\t\tcudaDeviceProp curDeviceProp;\r\n\t\t\tcudaError_t err = cudaGetDeviceProperties(&curDeviceProp, i);\r\n\t\t\tif (err != cudaSuccess) {\r\n\t\t\t\tcontinue;\r\n\t\t\t}\r\n\t\t\tsize_t free_mem, total_mem;\r\n\t\t\tcuDeviceGet(&dev, i);\r\n\t\t\tcuCtxCreate(&ctx, 0, dev);\r\n\t\t\tres = cuMemGetInfo(&free_mem, &total_mem);\r\n\t\t\tresult= (int)free_mem;\r\n\t\t}\r\n\t\treturn result;\r\n}\r\n\r\n\r\nint main (int argc, char *argv[]) { \r\n\tsetenv(\"TF_CPP_MIN_VLOG_LEVEL\", \"3\", 1);\r\n\tsetenv(\"TF_CPP_MIN_LOG_LEVEL\", \"3\", 1);\r\n\r\n\t{\r\n\t\tstd::cout << \"Start of program:  \" << freeCudaMem() << std::endl;\r\n\t\ttensorflow::SessionOptions options;\r\n\t\ttensorflow::ConfigProto &config = options.config;\r\n\t\tconfig.mutable_gpu_options()->set_allow_growth(true);\r\n\t\tauto* device_count = options.config.mutable_device_count();\r\n\t\tdevice_count->insert({ \"GPU\", 1 });\r\n\t\tdevice_count->insert({ \"CPU\", 1 });\r\n\r\n\t\tstd::unique_ptr<tensorflow::Session> session = std::unique_ptr<tensorflow::Session>(tensorflow::NewSession(options));\t\r\n\r\n\t\ttensorflow::GraphDef graph_def;\r\n\t\tstd::string graphFile(\"/path/to/large/frozen_model.pb\");\r\n\t\ttensorflow::Status graphLoadedStatus = ReadBinaryProto(tensorflow::Env::Default(),graphFile.c_str(),&graph_def);\r\n\t\tauto inputTensor2 = tensorflow::Tensor(tensorflow::DT_UINT8, { 1, 2048, 2048, 3 });\r\n\t\tstd::vector<tensorflow::Tensor> outputs;\r\n\t\ttensorflow::Status session_create_status = session->Create(graph_def);                                                        \r\n\t\ttensorflow::Status run_status = session->Run({ { \"InputTensor\", inputTensor2 } }, { { \"OutputTensor\" } },{},&outputs);\r\n\t\toutputs.clear();\t\r\n\t\tint number3;\r\n\t\tstd::cout << \"After session:     \" << freeCudaMem() << std::endl;\r\n\t\ttensorflow::Status closeStatus3  = session\t->Close();\r\n\t\tstd::cout << \"Close Status: \" << closeStatus3 << std::endl;  \r\n\t\tstd::cout << \"After close:       \" << freeCudaMem() << std::endl;                                                                                                        \r\n\t\tsession.reset();\r\n\t\tstd::cout << \"After reset:       \" << freeCudaMem() << std::endl;\r\n\t}\r\n\t{\r\n\t\tstd::cout << \"Start of program2: \" << freeCudaMem() << std::endl;\r\n\t\ttensorflow::SessionOptions options2;\r\n\t\ttensorflow::ConfigProto &config2 = options2.config;\r\n\t\tconfig2.mutable_gpu_options()->set_allow_growth(true);\r\n\t\tauto* device_count2 = options2.config.mutable_device_count();\r\n\t\tdevice_count2->insert({ \"GPU\", 1 });\r\n\t\tdevice_count2->insert({ \"CPU\", 1 });\r\n\r\n\t\tstd::unique_ptr<tensorflow::Session> session2 = std::unique_ptr<tensorflow::Session>(tensorflow::NewSession(options2));\t\r\n\r\n\r\n\t\ttensorflow::GraphDef graph_def2;\r\n\t\tstd::string graphFile2(\"/path/to/large/frozen_model.pb\");\r\n\t\ttensorflow::Status graphLoadedStatus2 = ReadBinaryProto(tensorflow::Env::Default(),graphFile2.c_str(),&graph_def2);\r\n\t\tauto inputTensor22 = tensorflow::Tensor(tensorflow::DT_UINT8, { 1, 2048, 2048, 3 });\r\n\t\tstd::vector<tensorflow::Tensor> outputs2;\r\n\t\ttensorflow::Status session_create_status2 = session2->Create(graph_def2);                                                        \r\n\t\ttensorflow::Status run_status2 = session2->Run({ { \"InputTensor\", inputTensor22 } }, { { \"OutputTensor\" } },{},&outputs2);\r\n\t\toutputs2.clear();\t\r\n\t\r\n\t\tint number2;\r\n\t\tstd::cout << \"After session2:    \" << freeCudaMem() << std::endl;\r\n\t\ttensorflow::Status closeStatus2  = session2->Close();\r\n\t\tstd::cout << \"Close Status2: \" << closeStatus2 << std::endl;  \r\n\t\tstd::cout << \"After close2:      \" << freeCudaMem() << std::endl;                                                                                                        \r\n\t\tsession2.reset();\r\n\t\tstd::cout << \"After reset2:      \" << freeCudaMem() << std::endl;\r\n\t}\r\n\t{\r\n\t\tstd::cout << \"Start of program3: \" << freeCudaMem() << std::endl;\r\n\t\ttensorflow::SessionOptions options3;\r\n\t\ttensorflow::ConfigProto &config3 = options3.config;\r\n\t\tconfig3.mutable_gpu_options()->set_allow_growth(true);\r\n\t\tauto* device_count3 = options3.config.mutable_device_count();\r\n\t\tdevice_count3->insert({ \"GPU\", 1 });\r\n\t\tdevice_count3->insert({ \"CPU\", 1 });\r\n\t\tstd::unique_ptr<tensorflow::Session> session3 = std::unique_ptr<tensorflow::Session>(tensorflow::NewSession(options3));\t\r\n\t\ttensorflow::GraphDef graph_def3;\r\n\t\tstd::string graphFile3(\"/path/to/large/frozen_model.pb\");\r\n\t\ttensorflow::Status graphLoadedStatus3 = ReadBinaryProto(tensorflow::Env::Default(),graphFile3.c_str(),&graph_def3);\r\n\t\tauto inputTensor32 = tensorflow::Tensor(tensorflow::DT_UINT8, { 1, 2048, 2048, 3 });\r\n\t\tstd::vector<tensorflow::Tensor> outputs3;\r\n\t\ttensorflow::Status session_create_status3 = session3->Create(graph_def3);                                                        \r\n\t\ttensorflow::Status run_status3 = session3->Run({ { \"InputTensor\", inputTensor32 } }, { { \"OutputTensor\" } },{},&outputs3);\r\n\t\toutputs3.clear();\t\r\n\t\tint number3;\r\n\t\tstd::cout << \"After session3:    \" << freeCudaMem() << std::endl;\r\n\t\ttensorflow::Status closeStatus3  = session3->Close();\r\n\t\tstd::cout << \"Close Status3: \" << closeStatus3 << std::endl;  \r\n\t\tstd::cout << \"After close3:      \" << freeCudaMem() << std::endl;                                                                                                        \r\n\t\tsession3.reset();\r\n\t\tstd::cout << \"After reset3:      \" << freeCudaMem() << std::endl;\r\n\t}\r\n\r\n}\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nHere is the output from the program above:\r\n```\r\nStart of program:  1031380992\r\nAfter session:     700043264\r\nClose Status: OK\r\nAfter close:       635219968\r\nAfter reset:       570400768\r\nStart of program2: 505581568\r\nAfter session2:    440766464\r\nClose Status2: OK\r\nAfter close2:      375943168\r\nAfter reset2:      311111680\r\nStart of program3: 246292480\r\nAfter session3:    181473280\r\nClose Status3: OK\r\nAfter close3:      116654080\r\nAfter reset3:      1013313536\r\n```\r\n\r\nNote the GPU memory available keeps going down even though the session has been closed and reset and all variables are out of scope.", "comments": ["By default TensorFlow allocates GPU memory for the lifetime of the process, not the lifetime of the session object. More details at: https://www.tensorflow.org/programmers_guide/using_gpu#allowing_gpu_memory_growth\r\n\r\nThus, if you want memory to be freed, you'll have to exit the program, not just close the session.\r\n\r\nHope that helps.\r\n\r\n", "That does not help, the tensorflow library is embedded as a plugin to a parent application.\r\n\r\nEither that process will need to fork a subprocess so that application can terminate the entire subprocess.\r\n\r\nBut at that point I will basically have a client server architecture to a plugin in a parent application, which is not the intended design.\r\n\r\nSurely the memory for the session can be managed without shutting down the application.", "The request to clear the memory without shutting down the application has been a request from at least two users in the last six months, I cannot deliver on this user story without changing the architecture of the application dramatically, which will be different on Windows, OSX and Linux, because I would be spawning subprocesses so they could terminate just to release memory.", "Any update on this?", "Any eyes on this?", "A response would be great ", "@samhodge sorry for late reply. As mentioned by @ymodak above, there is currently no way to reset the memory allocator without exiting the program. There is a [test-only method](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_process_state.h#L130) that can help to reset the memory owned by the process but that is not exposed.", "Sorry for the lack of results reply. It seems odd to be that an API with so many features lacks a tear down function apart from at the operating system level.", "Hi @samhodge,\r\n\r\nWe don't have cycles to investigate this in the near term.  Are you open to contributing a patch to implement this hook?", "I would be more inclined to move away from Tensorflow or to have a subprocess architecture to work around the problem but the possibility to fix it is something I might entertain.\r\n\r\nSam", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32128\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32128\">No</a>\n"]}, {"number": 32127, "title": "Tensorflow 2.0 : Combining model.add_loss and keras losses function in training doesn't work", "body": "I read about TensorFlow 2.0 [tutorial](https://www.tensorflow.org/beta/guide/keras/custom_layers_and_models) in VAE section. I follow the tutorial but the model doesn't work as expected despite running the notebook directly from given [Google Colab](https://colab.research.google.com/github/tensorflow/docs/blob/r2.0rc/site/en/r2/guide/keras/custom_layers_and_models.ipynb). The result actually is the same as in the tutorial (i.e. loss value is very similar) but if you look at the output you'll see that the model can't reconstruct the input at all (i.e. output the same image for all inputs). This seems to be a mistake from the tutorial itself when combining `model.add_loss()` and `keras.losses`.\r\n\r\n![Original code](https://imgur.com/3rTAOrZ.png)\r\n\r\nI changed MSE loss to BinaryCrossentropy but the result is still the same.\r\n\r\nLater I tried compute the BinaryCrossentropy loss explicitly in my forward pass then use `model.add_loss()` in addition with the KL-divergence loss\r\n\r\n![Use only model.add_loss() to calculate the loss](https://imgur.com/VVOROCI.png)\r\n\r\nThis way the model can actually learn the data and the output seems good enough.\r\n\r\nSo I have a question about `model.add_loss()` and losses as a function that takes `(y_true, y_pred)` (i.e. `keras.losses`). The updated code works only if it can calculate losses in forward pass (e.g. kl-divergence or reconstruction loss), how can I combine `model.add_loss()` and `keras.losses` correctly in the case where the model need ground truth of the output (e.g. denoise VAE).", "comments": ["Thank you for the issue @51616. I tried the colab and got different results for MSE and BCE losses in custom training loop:\r\n\r\nMSE loss:\r\n```\r\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\r\n11493376/11490434 [==============================] - 0s 0us/step\r\nStart of epoch 0\r\nstep 0: mean loss = tf.Tensor(0.35074496, shape=(), dtype=float32)\r\nstep 100: mean loss = tf.Tensor(0.12501891, shape=(), dtype=float32)\r\nstep 200: mean loss = tf.Tensor(0.09896767, shape=(), dtype=float32)\r\nstep 300: mean loss = tf.Tensor(0.088992655, shape=(), dtype=float32)\r\nstep 400: mean loss = tf.Tensor(0.084115215, shape=(), dtype=float32)\r\nstep 500: mean loss = tf.Tensor(0.08080357, shape=(), dtype=float32)\r\nstep 600: mean loss = tf.Tensor(0.078655444, shape=(), dtype=float32)\r\nstep 700: mean loss = tf.Tensor(0.07708102, shape=(), dtype=float32)\r\nstep 800: mean loss = tf.Tensor(0.07592539, shape=(), dtype=float32)\r\nstep 900: mean loss = tf.Tensor(0.074898444, shape=(), dtype=float32)\r\nStart of epoch 1\r\nstep 0: mean loss = tf.Tensor(0.07461691, shape=(), dtype=float32)\r\nstep 100: mean loss = tf.Tensor(0.073966034, shape=(), dtype=float32)\r\nstep 200: mean loss = tf.Tensor(0.073460035, shape=(), dtype=float32)\r\nstep 300: mean loss = tf.Tensor(0.07299255, shape=(), dtype=float32)\r\nstep 400: mean loss = tf.Tensor(0.07266195, shape=(), dtype=float32)\r\nstep 500: mean loss = tf.Tensor(0.07227474, shape=(), dtype=float32)\r\nstep 600: mean loss = tf.Tensor(0.07198363, shape=(), dtype=float32)\r\nstep 700: mean loss = tf.Tensor(0.071675226, shape=(), dtype=float32)\r\nstep 800: mean loss = tf.Tensor(0.07145154, shape=(), dtype=float32)\r\nstep 900: mean loss = tf.Tensor(0.07118706, shape=(), dtype=float32)\r\nStart of epoch 2\r\nstep 0: mean loss = tf.Tensor(0.07111354, shape=(), dtype=float32)\r\nstep 100: mean loss = tf.Tensor(0.07093662, shape=(), dtype=float32)\r\nstep 200: mean loss = tf.Tensor(0.07080722, shape=(), dtype=float32)\r\nstep 300: mean loss = tf.Tensor(0.07065797, shape=(), dtype=float32)\r\nstep 400: mean loss = tf.Tensor(0.07056295, shape=(), dtype=float32)\r\nstep 500: mean loss = tf.Tensor(0.07040719, shape=(), dtype=float32)\r\nstep 600: mean loss = tf.Tensor(0.07030004, shape=(), dtype=float32)\r\nstep 700: mean loss = tf.Tensor(0.07016859, shape=(), dtype=float32)\r\nstep 800: mean loss = tf.Tensor(0.07007885, shape=(), dtype=float32)\r\nstep 900: mean loss = tf.Tensor(0.06994936, shape=(), dtype=float32)\r\n```\r\n\r\nBCE loss:\r\n\r\n```\r\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\r\n11493376/11490434 [==============================] - 0s 0us/step\r\nStart of epoch 0\r\nWARNING:tensorflow:From /tensorflow-2.0.0-rc0/python3.6/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nstep 0: mean loss = tf.Tensor(0.7803986, shape=(), dtype=float32)\r\nstep 100: mean loss = tf.Tensor(0.4406416, shape=(), dtype=float32)\r\nstep 200: mean loss = tf.Tensor(0.3644295, shape=(), dtype=float32)\r\nstep 300: mean loss = tf.Tensor(0.33377928, shape=(), dtype=float32)\r\nstep 400: mean loss = tf.Tensor(0.31776747, shape=(), dtype=float32)\r\nstep 500: mean loss = tf.Tensor(0.30728954, shape=(), dtype=float32)\r\nstep 600: mean loss = tf.Tensor(0.30034995, shape=(), dtype=float32)\r\nstep 700: mean loss = tf.Tensor(0.2952541, shape=(), dtype=float32)\r\nstep 800: mean loss = tf.Tensor(0.29153773, shape=(), dtype=float32)\r\nstep 900: mean loss = tf.Tensor(0.28829667, shape=(), dtype=float32)\r\nStart of epoch 1\r\nstep 0: mean loss = tf.Tensor(0.28734177, shape=(), dtype=float32)\r\nstep 100: mean loss = tf.Tensor(0.28517666, shape=(), dtype=float32)\r\nstep 200: mean loss = tf.Tensor(0.28343785, shape=(), dtype=float32)\r\nstep 300: mean loss = tf.Tensor(0.2818749, shape=(), dtype=float32)\r\nstep 400: mean loss = tf.Tensor(0.28066754, shape=(), dtype=float32)\r\nstep 500: mean loss = tf.Tensor(0.27942434, shape=(), dtype=float32)\r\nstep 600: mean loss = tf.Tensor(0.27840894, shape=(), dtype=float32)\r\nstep 700: mean loss = tf.Tensor(0.2775054, shape=(), dtype=float32)\r\nstep 800: mean loss = tf.Tensor(0.27676252, shape=(), dtype=float32)\r\nstep 900: mean loss = tf.Tensor(0.27591947, shape=(), dtype=float32)\r\nStart of epoch 2\r\nstep 0: mean loss = tf.Tensor(0.27567974, shape=(), dtype=float32)\r\nstep 100: mean loss = tf.Tensor(0.27511278, shape=(), dtype=float32)\r\nstep 200: mean loss = tf.Tensor(0.2745993, shape=(), dtype=float32)\r\nstep 300: mean loss = tf.Tensor(0.27408925, shape=(), dtype=float32)\r\nstep 400: mean loss = tf.Tensor(0.2737239, shape=(), dtype=float32)\r\nstep 500: mean loss = tf.Tensor(0.2732339, shape=(), dtype=float32)\r\nstep 600: mean loss = tf.Tensor(0.2728653, shape=(), dtype=float32)\r\nstep 700: mean loss = tf.Tensor(0.27246934, shape=(), dtype=float32)\r\nstep 800: mean loss = tf.Tensor(0.27214858, shape=(), dtype=float32)\r\nstep 900: mean loss = tf.Tensor(0.27174425, shape=(), dtype=float32)\r\n```\r\n\r\nAre you observing something else?", "Closing due to lack of activity, please re-open if you see this issue again.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32127\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32127\">No</a>\n", "Dear pavithrasv,\r\n\r\nYes, the loss changes, but still the model does not learn anything. The KL-Divergence should give much higher errors than those presented.\r\n\r\nBest Regards,\r\n\r\nAnt\u00f3nio", "Is this possibly related to : https://github.com/tensorflow/tensorflow/issues/32058\r\n"]}, {"number": 32126, "title": "Rewrite some code segment to be more elegant and Pythonic", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32126) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32126) for more info**.\n\n<!-- ok -->"]}, {"number": 32125, "title": "LARSOptimizer - Layer-wise decomposition", "body": "LARSOptimizer provides the learning rate control used in LARS, but we need layer-wise decomposed parameters of a neural network. how do we do the layer-wise decomposition for a pre-defined architecture?\r\n\r\nLARS: https://arxiv.org/abs/1708.03888 | [Release 1.14 code](https://github.com/tensorflow/tensorflow/blob/release_1.14.0/tensorflow/contrib/opt/python/training/lars_optimizer.py)\r\nLAMB: https://arxiv.org/abs/1904.00962 | [Unofficial code](https://github.com/ymcui/LAMB_Optimizer_TF)\r\nNovoGrad: https://arxiv.org/abs/1905.11286 | [OpenSeq2Seq code](https://github.com/NVIDIA/OpenSeq2Seq/blob/master/open_seq2seq/optimizers/novograd.py)\r\n\r\ne.g. [Caffe's LARC](https://docs.nvidia.com/deeplearning/frameworks/caffe-user-guide/index.html#larc)\r\n\r\nA possible TensorFlow implementation for conv2d:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import ops\r\n\r\ndef lars_conv2d():\r\n    input = tf.placeholder(tf.float32, shape=[16,224,224,3])\r\n    with tf.name_scope('conv1_1') as scope:\r\n        kernel_size = 7*7*3*32\r\n        bias_size = 32\r\n        total_size = kernel_size + bias_size\r\n        params = tf.Variable(tf.constant(0.0, shape=[total_size], dtype=tf.float32),\r\n                             trainable=True, name='weights_and_biases')\r\n        kernel = tf.reshape(\r\n                    tf.slice(params, [0], [kernel_size], name='weights'),\r\n                    [7, 7, 3, 32])\r\n        conv = tf.nn.conv2d(input, kernel, [1, 1, 1, 1], padding='SAME')\r\n        biases = tf.slice(params, [kernel_size], [bias_size], name='biases')\r\n        bias = tf.nn.bias_add(conv, biases)\r\n        conv1 = tf.nn.relu(bias, name=scope)\r\n\r\n    init_op = tf.variables_initializer(var_list=[params])\r\n    with tf.Session() as sess:\r\n        sess.run(init_op)\r\n        print(np.sum(sess.run(conv1, {input: np.zeros([16,224,224,3])})))\r\n\r\nlars_conv2d()\r\n```", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32125\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32125\">No</a>\n", "https://stackoverflow.com/questions/57781188/larsoptimizer-layer-wise-decomposition\r\n\r\nI am waiting answers from the TensorFlow community.", "https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/contrib/opt/python/training/lars_optimizer.py#L102-L128\r\nIn tensorflow 1.14, lars optimizer updates momentum without weight decay.\r\nIt does not agree with the paper, as shown in the following figure.\r\n\r\n![image](https://user-images.githubusercontent.com/33881637/87132980-3f092380-c2c9-11ea-9d75-2c38ffa70a31.png)\r\n\r\n\r\nmaybe we can fix this bug according to reference: https://people.eecs.berkeley.edu/~youyang/lars_optimizer.py\r\njust as the following code:\r\n    # for lars optimizer\r\n    def _apply_dense(self, grad, var):\r\n      scaled_lr = self.compute_lr(grad, var)\r\n      # adding weight decay before updating the momentum, align with paper.\r\n      if self._skip_list is None or not any(v in var.name\r\n                                            for v in self._skip_list):\r\n          grad = grad  + self._weight_decay * var\r\n\r\n      # scale grad before updating the momentum, align with paper.\r\n      grad = grad * scaled_lr\r\n      mom = self.get_slot(var, \"momentum\")\r\n      return training_ops.apply_momentum(\r\n          var,\r\n          mom,\r\n          1.0,# do not scale grad again while updating momentum, becuase we have scaled it before.\r\n          grad,\r\n          self._momentum,\r\n          use_locking=False,\r\n          use_nesterov=self._use_nesterov)"]}, {"number": 32124, "title": "[INTEL MKL] Add support for Addv2", "body": "Added support for MKL Addv2.  PR also includes some unit tests that were missing.", "comments": []}, {"number": 32123, "title": "Track trackables in graph networks", "body": "This change also removes automatic tracking of Keras-internal attributes. (resolving loading bug from #31893).\r\n\r\nPiperOrigin-RevId: 266440936", "comments": []}, {"number": 32121, "title": "Override map on AutoGraph", "body": "This pull request address #31823 issue. The current implementation override map when all of the input was dataset.", "comments": []}, {"number": 32120, "title": "integer overflow possibility fix v.2", "body": "The current code for static int64 GetDirectConvCost f-n can lead to integer overflow and weird results\r\nCreated issue: #32045\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32120) for more info**.\n\n<!-- need_sender_cla -->", "Thanks for your reply. I will make the corrections just after weekend.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32120) for more info**.\n\n<!-- need_author_cla -->", "@mihaimaruseac,\r\nI have to comply corporate rules and it turns out that I can not use my personal accounts/mails for contributing, especially from resources others than my company\u2019s resources.\r\nMy company even ready to provide me with legal corporate email, but it takes additional money, I am not sure it worths it.\r\nI think the best way here is to close the PR.\r\nThe only thing that I am not sure that PR from shubham769 (https://github.com/tensorflow/tensorflow/pull/32083) is worthy to be accepted.\r\nHe copy-pasted all my commits even with my bugs. He could mention my PR at least. I think that such things should not be allowed.", "Thank you @iurii-kvasniuk for your contribution, closing this PR based on latest comments."]}, {"number": 32119, "title": "Cannot use \"sample_weight\" in tf.data.Dataset along with a tf.keras model with customized loss", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **unknown 1.14.0**\r\n- Python version: **3.7.3**\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nIf a tf.keras.models.Model uses customized loss added to the model by model.add_loss(loss), and  a tf.data.Dataset dataset which yields tuples (x, y, sample_weight) is used in model.fit(dataset), a TypeError will be raised.\r\n\r\n**Describe the expected behavior**\r\nThe model should be trained normally.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.layers import Input, Dense\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\ninputs = Input(shape = [1], dtype = tf.float32)\r\nlabels = Input(shape = [1], dtype = tf.float32)\r\noutputs = Dense(1, use_bias = True, activation = None)(inputs)\r\n\r\nmodel = Model([inputs, labels], outputs)\r\nloss = tf.square(labels - outputs)\r\nmodel.add_loss(loss)\r\n\r\nmodel.compile(Adam(0.1))\r\n\r\nrg = tf.data.Dataset.range(128)\r\nrg = rg.map(lambda x: tf.cast(x, tf.float32))\r\n\r\n'''\r\n# This snippet works fine\r\nds2 = rg.map(lambda x: ((x, 2*x+1), 0))\r\nds2 = ds2.batch(16)\r\nmodel.fit(ds2, epochs = 50)\r\nprint(model.predict(([-32], [0])))\r\n'''\r\n\r\n# This snippet causes exception\r\nds3 = rg.map(lambda x: ((x, 2*x+1), 0, 1))\r\nds3 = ds3.batch(16)\r\nmodel.fit(ds3, epochs = 50)\r\n```\r\n\r\n**Other info / logs**\r\nI encounter the problem in a real-world model. I omit the original model and give a simple model here. It is awkward; but it reproduces the bug in a few lines.\r\n\r\nHere is the traceback:\r\n```\r\nC:\\fakepath\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) o\r\nr '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,\r\n)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\nC:\\fakepath\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) o\r\nr '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,\r\n)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\nC:\\fakepath\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) o\r\nr '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,\r\n)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\nC:\\fakepath\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) o\r\nr '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,\r\n)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\nC:\\fakepath\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) o\r\nr '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,\r\n)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\nC:\\fakepath\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) o\r\nr '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,\r\n)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\nC:\\fakepath\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (typ\r\ne, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,))\r\n / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\nC:\\fakepath\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (typ\r\ne, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,))\r\n / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\nC:\\fakepath\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (typ\r\ne, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,))\r\n / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\nC:\\fakepath\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (typ\r\ne, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,))\r\n / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\nC:\\fakepath\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (typ\r\ne, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,))\r\n / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\nC:\\fakepath\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (typ\r\ne, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,))\r\n / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\nWARNING:tensorflow:From C:\\fakepath\\Python37\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling Varia\r\nnceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future versi\r\non.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nWARNING:tensorflow:Output dense missing from loss dictionary. We assume this was done on purpose. The fit and evaluate A\r\nPIs will not be expecting any data to be passed to dense.\r\nWARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input\r\n dataset.\r\n2019-08-30 23:01:30.110552: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that thi\r\ns TensorFlow binary was not compiled to use: AVX2\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 31, in <module>\r\n    model.fit(ds3, epochs = 50)\r\n  File \"C:\\fakepath\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 780, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"C:\\fakepath\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\", line 174, in mode\r\nl_iteration\r\n    ins = _prepare_feed_values(model, inputs, targets, sample_weights, mode)\r\n  File \"C:\\fakepath\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\", line 501, in _pre\r\npare_feed_values\r\n    extract_tensors_from_dataset=True)\r\n  File \"C:\\fakepath\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 2678, in _standardi\r\nze_user_data\r\n    sample_weight, feed_output_names)\r\n  File \"C:\\fakepath\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\", line 441, in stand\r\nardize_sample_weights\r\n    'sample_weight')\r\n  File \"C:\\fakepath\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\", line 431, in stand\r\nardize_sample_or_class_weights\r\n    str(x_weight))\r\nTypeError: The model has multiple outputs, so `sample_weight` should be either a list or a dict. Provided `sample_weight\r\n` type not understood: Tensor(\"IteratorGetNext:3\", shape=(?,), dtype=int32)\r\n```\r\n\r\n", "comments": ["@tiandeyu-github The model doesn't accept multiple outputs. So, you have to use a list or dictionary. Make the small change as below and it works..!!\r\n\r\n# This snippet causes exception\r\nds3 = rg.map(lambda x: ((x, 2*x+1), [0, 1]))\r\nds3 = ds3.batch(16)\r\nmodel.fit(ds3, epochs = 50)", "@gowthamkpr Thanks for your reply.\r\n\r\nIn my opinion, the dataset used in model.fit(dataset) should generate tuples like (input, target, sample_weight), so in my code, the dataset generates tuples like ((x, 2\\*x+1), 0, 1), where (x, 2\\*x+1) is the input (i.e. \"x\" is the input for \"inputs\", \"2\\*x+1\" is the input for \"labels\"), and 0 is the target (it is a placeholder here, because the model uses a cuotomized loss and do not need the target output), and 1 is the sample_weight. In your code, the dataset generate tuples like ((x, 2\\*x+1), [0, 1]), which does not have a \"sample_weight\" field, and contains it in the \"target\" field. Now I think model.fit() will interpret the whole [0, 1] as the target and ignore it, which are not the behavior desired by us.\r\n\r\nHere is some code to show the problem:\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.layers import Input, Dense\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\ninputs = Input(shape = [1], dtype = tf.float32)\r\nlabels = Input(shape = [1], dtype = tf.float32)\r\noutputs = Dense(1, use_bias = True, activation = None)(inputs)\r\n\r\nmodel = Model([inputs, labels], outputs)\r\nloss = tf.square(labels - outputs)\r\nmodel.add_loss(loss)\r\n\r\nmodel.compile(Adam(0.1))\r\n\r\nrise = tf.data.Dataset.range(60)\r\nrise = rise.map(lambda x: tf.cast(x, tf.float32))\r\nrise = rise.map(lambda x: ((x, x), [0, 1]))\r\n\r\nfall = tf.data.Dataset.range(4)\r\nfall = fall.map(lambda x: tf.cast(x, tf.float32))\r\nfall = fall.map(lambda x: ((x, -x), [0, 9999999]))\r\n\r\nds = rise.concatenate(fall)\r\nds = ds.batch(16)\r\n\r\nmodel.fit(ds, epochs = 50)\r\np = model.predict(([-100], [0]))\r\nprint(p) # p is around [[-100]], which means that y=x is acturally learned.\r\n```\r\n\r\nBy the way, in the above code, if you add ```tf.enable_eager_execution()``` at first, the model.fit() will raise an error:\r\n```\r\nC:\\fakepath\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\nC:\\fakepath\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\nC:\\fakepath\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\nC:\\fakepath\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\nC:\\fakepath\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\nC:\\fakepath\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\nC:\\fakepath\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\nC:\\fakepath\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\nC:\\fakepath\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\nC:\\fakepath\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\nC:\\fakepath\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\nC:\\fakepath\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\nWARNING:tensorflow:From tf_sample_weight_custom_loss.py:43: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.\r\n\r\n2019-08-31 10:02:55.084110: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\nWARNING:tensorflow:Output dense missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense.\r\nWARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\r\nEpoch 1/50\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 65, in <module>\r\n    model.fit(ds, epochs = 50)\r\n  File \"C:\\fakepath\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 694, in fit\r\n    initial_epoch=initial_epoch)\r\n  File \"C:\\fakepath\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1433, in fit_generator\r\n    steps_name='steps_per_epoch')\r\n  File \"C:\\fakepath\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\", line 264, in model_iteration\r\n    batch_outs = batch_function(*batch_data)\r\n  File \"C:\\fakepath\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1153, in train_on_batch\r\n    extract_tensors_from_dataset=True)\r\n  File \"C:\\fakepath\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 2673, in _standardize_user_data\r\n    exception_prefix='target')\r\n  File \"C:\\fakepath\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\", line 305, in standardize_input_data\r\n    'expected no data, but got:', data)\r\nValueError: ('Error when checking model target: expected no data, but got:', <tf.Tensor: id=95, shape=(16, 2), dtype=int32, numpy=\r\narray([[0, 1],\r\n       [0, 1],\r\n       [0, 1],\r\n       [0, 1],\r\n       [0, 1],\r\n       [0, 1],\r\n       [0, 1],\r\n       [0, 1],\r\n       [0, 1],\r\n       [0, 1],\r\n       [0, 1],\r\n       [0, 1],\r\n       [0, 1],\r\n       [0, 1],\r\n       [0, 1],\r\n       [0, 1]])>)\r\n```\r\n", "@tiandeyu-github Are you still facing with the same issue or did you solve it?", "@gowthamkpr Thanks for your reply.\r\n\r\nI finally find a workaround, which is that you can make the \"sample_weight\" field as a new input to the model. For example, the following code works well:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.layers import Input, Dense\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\ninputs = Input(shape = [1], dtype = tf.float32)\r\nlabels = Input(shape = [1], dtype = tf.float32)\r\nsample_weights = Input(shape = [1], dtype = tf.float32)\r\noutputs = Dense(1, use_bias = True, activation = None)(inputs)\r\n\r\nmodel = Model([inputs, labels, sample_weights], outputs)\r\nloss = tf.reduce_mean(tf.square(labels - outputs) / 2 * sample_weights)\r\nmodel.add_loss(loss)\r\n\r\nmodel.compile(Adam(0.1))\r\n\r\nrise = tf.data.Dataset.range(60)\r\nrise = rise.map(lambda x: tf.cast(x, tf.float32))\r\nrise = rise.map(lambda x: ((x, x, 1), 0))\r\n\r\nfall = tf.data.Dataset.range(4)\r\nfall = fall.map(lambda x: tf.cast(x, tf.float32))\r\nfall = fall.map(lambda x: ((x, -x, 9999999), 0))\r\n\r\nds = rise.concatenate(fall)\r\nds = ds.batch(16)\r\n\r\nmodel.fit(ds, epochs = 50)\r\np = model.predict(([-100], [0], [1]))\r\nprint(p) # p is around [[100]]\r\n```\r\n\r\nIf it is recommended to solve the issue in this way, then maybe we can add some hints in TensorFlow's documents?", "@gowthamkpr \r\n\r\nHowever, the issue about ```tf.enable_eager_execution()``` still exists. But it does not bother me, because I can simply disable the \"eager execution\" mode to get rid of it.\r\n", "sure @tiandeyu-github Glad you were able to figure it out. I am closing this issue as it has been resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32119\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32119\">No</a>\n"]}, {"number": 32118, "title": "[TF2] Parallel optimizers in eager mode", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.10\r\n- TensorFlow installed from (source or binary): pip install tensorflow==2.0.0beta1\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\n\r\nI would like to optimize a large number of subproblems in parallel using TF 2.0. (For example, to compute the bootstrapped s.e. of the MLE.) It seems like this is not currently possible.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nclass opt:\r\n    def __init__(self):\r\n        self.x = None\r\n    @tf.function\r\n    def __call__(self, data):\r\n        if self.x is None:\r\n            self.x = tf.Variable(0., dtype=tf.float64)\r\n            self.opt = tf.keras.optimizers.SGD(1.)\r\n        x = self.x\r\n        opt = self.opt\r\n        for _ in range(10):\r\n            with tf.GradientTape() as tape:\r\n                obj = tf.reduce_mean((data - x) ** 2)\r\n            g = tape.gradient(obj, x)\r\n            opt.apply_gradients([(g, x)])\r\n        return x\r\n\r\ndata = np.random.normal(size=10000)\r\nK = 10\r\nreplicates = np.random.choice(data, size=(K, 10000), replace=True)\r\n\r\n@tf.function\r\ndef g():\r\n    return tf.map_fn(lambda r: opt()(r), replicates, dtype=tf.float64)\r\n\r\ng()\r\n```\r\n\r\n**Other info / logs**\r\nThe above code throws the error:\r\n```\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError:  Error while reading resource variable SGD/learning_rate_130 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/SGD/learning_rate_130/N10tensorflow3VarE does not exist.\r\n         [[{{node map/while/body/_1/StatefulPartitionedCall/SGD/SGD/update/Cast/ReadVariableOp}}]] [Op:__inference_g_1191]\r\n\r\nFunction call stack:\r\ng\r\n```\r\nI believe this is because the optimizer creates variables as part of its initialization. These variables are not preserved between calls to `opt()`. \r\n\r\nIf I try to define a new optimizer at each call to `opt()`, then `tf.function` complains that variables are being created on the non-first call.\r\n\r\nThe code works if I do not decorate `g()` with `@tf.function`.\r\n\r\nThus far the only workaround I have found is to manually implement the gradient update rule, forgoing the use of `tf.keras.optimizers` completely. But I'm hoping there's a better way.", "comments": ["I was able to get this to work by manually initializing the optimizer variables:\r\n```\r\nclass opt:\r\n    def __init__(self):\r\n        self.x = None\r\n    @tf.function\r\n    def __call__(self, data):\r\n        if self.x is None:\r\n            self.x = tf.Variable(0., dtype=tf.float64, name=\"x\")\r\n            self.opt = tf.keras.optimizers.SGD()\r\n        x = self.x\r\n        opt = self.opt\r\n        x.assign(0.)\r\n        for v in opt.variables():\r\n            v.assign(0)  # initialize itercount to 0\r\n        for _ in range(100):\r\n            with tf.GradientTape() as tape:\r\n                obj = tf.reduce_mean((data - x) ** 2)\r\n            g = tape.gradient(obj, x)\r\n            opt.apply_gradients([(g, x)])\r\n        return x\r\n```\r\n\r\nThe code runs, and indeed runs faster when I decorate `g()` with `@tf.function` than when I don't. But CPU usage stays around 100%, even when I make the optimization problem less trivial, so I'm not sure if it's actually running in parallel.", "Issue replicating for the given code for TF 2.0beta, please find the [gist](https://colab.research.google.com/gist/oanush/9e8621ef80159028163e646710109ad8/32118.ipynb).\r\nThanks!", "Hi @terhorst ,  unfortunately, tf.function can't trace dynamic Variable creations wrapped in map_fn properly, and probably we won't prioritize it in the near future.\r\n\r\nThough you can simply take out the initialization out of `g` and use Python list comprehension like below, and it will run in parallel.\r\n\r\n```python\r\nclass opt:\r\n    def __init__(self):\r\n        self.x = None\r\n    @tf.function\r\n    def __call__(self, data):\r\n        if self.x is None:\r\n            self.x = tf.Variable(0., dtype=tf.float64)\r\n            self.opt = tf.keras.optimizers.SGD(1.)\r\n        x = self.x\r\n        opt = self.opt\r\n        for _ in tf.range(10):\r\n            with tf.GradientTape() as tape:\r\n                obj = tf.reduce_mean((data - x) ** 2)\r\n            g = tape.gradient(obj, x)\r\n            opt.apply_gradients([(g, x)])\r\n        return x\r\n\r\ndata = np.random.normal(size=10000)\r\nK = 10\r\nreplicates = np.random.choice(data, size=(K, 10000), replace=True)\r\nopts = [opt() for _ in range(len(replicates))]\r\n\r\n@tf.function\r\ndef g(replicates):\r\n    return [opts[i](replicates[i]) for i in range(len(opts))]\r\n\r\ng(replicates)\r\n```", "@terhorst please let us know  if the suggestion by @kkimdev is helpful and we can close the issue. \r\n\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32118\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32118\">No</a>\n"]}, {"number": 32117, "title": "tf.Keras.fit() runs forever with 0 samples", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): `v2.0.0-beta1-5101-gc75bb66a99 2.0.0-rc0`\r\n- Python version: 3.7.4\r\n- CUDA/cuDNN version: n/a (CPU)\r\n\r\n**Describe the current behavior**\r\nCode runs forever although nothing is to be trained (no parameters, no data).\r\n```\r\n2019-08-30 15:50:26.413745: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\nEpoch 1/2\r\nWARNING:tensorflow:The list of trainable weights is empty. Make sure that you are not setting model.trainable to False before compiling the model.\r\n```\r\n\r\n**Describe the expected behavior**\r\nCode stops pretty soon.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nfrom tensorflow.keras import layers, models\r\n\r\nshape = (10, 10, 1)\r\nlayer = layers.Input(shape)\r\nmodel = models.Model(layer, layer)\r\nmodel.compile(loss='mse')\r\ndata = np.zeros((0, *shape))\r\nmodel.fit(x=data, y=data, epochs=2, verbose=2)\r\n```\r\n\r\n**Other info / logs**\r\nIt's somewhat annoying if you set the number of training samples to 0 by mistake before starting to train - you might not notice it for a while.", "comments": ["Was able to reproduce the issue [here](https://colab.sandbox.google.com/gist/gowthamkpr/9c6f9b830da370ac1bf8774b3516b32e/untitled124.ipynb) Thanks for reporting @bersbersbers ", "I want to work on this issue!  Please assign me! ", "@bersbersbers I agree with you that there is an issue with `TF2.0rc0`. However, it was resolved in the `tf-nightly-2.0-preview`. Please take a look at the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/4ee3cac706b982b4a5b80328a764e224/tf_32117.ipynb). It clearly throws `ValueError: Empty training data.` within 2 seconds. \r\n\r\nI am closing the issue here. Please feel free to reopen if the issue persists again in the `tf-nightly-2.0-preview`. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32117\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32117\">No</a>\n", "This problem is not fixed in 2.0.0rc1. This may be expected, but I wanted to point it out.", "@bersbersbers Thanks for finding this. I am reopening this as I could reproduce the issue with `TF2.0rc1`. I closed earlier as I didn't see any issue with `tf-nightly-2.0-preview`.\r\n\r\nCurrently with `TF2.0rc1`, it throws a warning but runs forever as mentioned by @bersbersbers \r\n```\r\nEpoch 1/2\r\nWARNING:tensorflow:The list of trainable weights is empty. Make sure that you are not setting model.trainable to False before compiling the model.\r\n```\r\n\r\nThanks!", "@imskr Please feel free to submit a PR for this issue, thanks!", "@omalleyt12 Thank you, I'm working on it", "@bersbersbers Could you please confirm if you are still facing any issues.\r\nAs i have tried running the same code on tf-nightly and TF 2.1-rc1 and it works fine, please find the [gist](https://colab.sandbox.google.com/drive/1pQn6GLmHo6eyOngJKJUcGkXtX8yRRGkn#scrollTo=YsqkNpZ5TEdW).", "With `tensorflow==2.1.0`, I get an error, as expected:\r\n\r\n```\r\n...\r\n  File \"training_utils.py\", line 144, in finalize\r\n    raise ValueError('Empty training data.')\r\nValueError: Empty training data.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n...\r\n  File \"keras/engine/training_v2.py\", line 772, in on_epoch\r\n    self.progbar.on_epoch_end(epoch, epoch_logs)\r\n  File \"keras/callbacks.py\", line 789, in on_epoch_end\r\n    self.progbar.update(self.seen, self.log_values)\r\n  File \"keras/utils/generic_utils.py\", line 579, in update\r\n    numdigits = int(np.log10(self.target)) + 1\r\nOverflowError: cannot convert float infinity to integer\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32117\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32117\">No</a>\n", "With TF 2.3, I don't get \"Empty training data.\" any longer - I now get \"UnboundLocalError: local variable 'logs' referenced before assignment\" (see #38064)."]}, {"number": 32116, "title": "Backport https://github.com/tensorflow/addons/pull/340 into master", "body": "This is fixing the failure of the `tf.contrib.seq2seq.sequence_loss` to warn if incompatible modes are selected, as reported in https://github.com/tensorflow/addons/issues/329 .", "comments": ["@lmthang can you review?", "closing this PR as contrib folder will be depricated in 2.0, thank you.\r\nCC @mihaimaruseac", "I think this should be picked up in tf.addons", "This comes from tf_addons, where I submitted it initially and where it has been merged. I figured that despite the depecation of TF1.x and contrib in particular, bugfixes were still very much a thing to care about and merge for TF1.x, as it's not gonna go away anytime soon?", "It can only go into TF1.15 as we're not doing patch releases except for vulnerabilities.\r\n\r\nHowever, 1.15 is already in the \"critical cherry-picks only\" phase, so probably it wouldn't be picked there either."]}, {"number": 32115, "title": "[TF2.0.0rc0] TPU update", "body": "When will be updated the nightly build for TPUs in TF2.0.0rc0 ?\r\n\r\nThank you", "comments": ["@nsantavas ,\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32115\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32115\">No</a>\n"]}, {"number": 32114, "title": "Why is the inference for the first time longer than later ones?", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution : Windows 10 and Android 9.0\r\n- Mobile device : Xiaomi 8\r\n- TensorFlow version : org.tensorflow:tensorflow-lite:0.0.0-nightly (using 1.11.0 has the same problem)\r\n- Model: ResNet152, ResNet18  convert from Pytorch and MobileNet-V1 from [tensorflow-lite demo](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo)\r\n\r\nif you wanna know how I convert  ResNet152 and ResNet18, you can get the information from  [#issue 27807](https://github.com/tensorflow/tensorflow/issues/27807)\r\n\r\n**logs from Android 9.0**\r\nLoad ResNet-152 (222 MB)\r\nD/TimeCost: Timecost to load model file: **0 ms**\r\nD/TimeCost: Created a Tensorflow Lite Image Classifier.\r\nLoad MobileNet-V1 (16.1 MB)\r\nD/TimeCost: Timecost to load model file: **0 ms** \r\nD/TimeCost: Created a Tensorflow Lite Image Classifier.\r\nLoad ResNet-18 (42.8MB)\r\nD/TimeCost: Timecost to load model file: **0 ms** \r\nD/TimeCost: Created a Tensorflow Lite Image Classifier.\r\n\r\nInference ResNet-152 for the first time \r\nD/TimeCost: Timecost to put values into ByteBuffer: **29 ms**\r\nD/TimeCost: Timecost to run model inference: **1681 ms** \r\nInference ResNet-152 for the second time \r\nD/TimeCost: Timecost to put values into ByteBuffer: **3 ms** \r\nD/TimeCost: Timecost to run model inference: **589 ms** \r\nInference ResNet-152 for the third time \r\nD/TimeCost: Timecost to put values into ByteBuffer: **3 ms** \r\nD/TimeCost: Timecost to run model inference: **591 ms** \r\n\r\nInference MobileNet-V1 for the first time\r\nD/TimeCost: Timecost to put values into ByteBuffer: **34 ms**\r\nD/TimeCost: Timecost to run model inference: **134 ms** \r\nInference MobileNet-V1 for the second time\r\nD/TimeCost: Timecost to put values into ByteBuffer: **3 ms** \r\nD/TimeCost: Timecost to run model inference: **56 ms** \r\nInference MobileNet-V1 for the third time \r\nD/TimeCost: Timecost to put values into ByteBuffer: **4 ms**\r\nD/TimeCost: Timecost to run model inference: **58 ms** \r\n\r\nInference ResNet-18 for the first time \r\nD/TimeCost: Timecost to put values into ByteBuffer: **10 ms** \r\nD/TimeCost: Timecost to run model inference: **396 ms**\r\nInference ResNet-18 for the second time\r\nD/TimeCost: Timecost to put values into ByteBuffer: **2 ms**\r\nD/TimeCost: Timecost to run model inference: **108 ms**\r\nInference ResNet-18 for the third time\r\nD/TimeCost: Timecost to put values into ByteBuffer: **2 ms**\r\nD/TimeCost: Timecost to run model inference: **85 ms**\r\n\r\n", "comments": ["Below is the code of loading model and inference \r\n\r\n**Load Model**\r\n```java\r\nprivate MappedByteBuffer loadModelFile(Activity activity) throws IOException {\r\n        AssetFileDescriptor fileDescriptor = activity.getAssets().openFd(getModelPath());\r\n        AssetFileDescriptor fileDescriptor = activity.getAssets().openFd(getLabelPath());\r\n        FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());\r\n        FileChannel fileChannel = inputStream.getChannel();\r\n        long startOffset = fileDescriptor.getStartOffset();\r\n        long declaredLength = fileDescriptor.getDeclaredLength();\r\n        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);\r\n    }\r\nlong startTime = SystemClock.uptimeMillis();\r\ntfliteModel = loadModelFile(activity);\r\nlong endTime = SystemClock.uptimeMillis();\r\nLog.d(TAG, \"Timecost to load model file: \" + Long.toString(endTime - startTime));\r\n```\r\n\r\n**inference**\r\n```java\r\nprotected float[] runInference() {\r\n   tflite.run(Data, labelProbArray);\r\n   return  labelProbArray[0];\r\n}\r\npublic float[] classifyFrame(float input[]) {\r\n   if (tflite == null) {\r\n       Log.e(TAG, \"Image classifier has not been initialized; Skipped.\");\r\n    }\r\n   convertArrayToByteBuffer(input);\r\n   // Here's where the magic happens!!!\r\n   long startTime = SystemClock.uptimeMillis();\r\n   float[] result = runInference();\r\n   long endTime = SystemClock.uptimeMillis();\r\n   Log.d(TAG, \"Timecost to run model inference: \" + Long.toString(endTime - startTime));\r\n   return result;\r\n}\r\n```\r\nSince the code that loads the model takes very little time, I think the first inference actually includes the time to copy the model into memory, but this is just a guess. Does anyone know why in details and what happened in the codes?\r\n", "Think about how a computer works. This `fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);` means `mmap()` or something similar is used. Its default behavior could be on-demand loading. So your measured time to load a model probably is not what you intended to measure. And there are TLBs and caches between DRAM and CPUs, for the very first time, cache hit ratio should be relative low.", "> Think about how a computer works. This `fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);` means `mmap()` or something similar is used. Its default behavior could be on-demand loading. So your measured time to load a model probably is not what you intended to measure. And there are TLBs and caches between DRAM and CPUs, for the very first time, cache hit ratio should be relative low.\r\n\r\nThank you. I did some research on [how MappedByteBuffer works](https://docs.oracle.com/en/java/javase/12/docs/api/java.base/java/nio/MappedByteBuffer.html) and some experiments on how the memory changes during the whole process according to your suggestions and source code. I may find some clues. Thanks again.", "@RoyIronGrey \r\nCan you please confirm if @freedomtan's workaround is working for you.Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32114\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32114\">No</a>\n"]}, {"number": 32113, "title": "model.summary() isn't works when dropout layer is in the model.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): custom code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.6\r\n- TensorFlow installed from (source or binary): binary (`pip install tensorflow==2.0.0-rc0`)\r\n- TensorFlow version (use command below): 2.0.0rc0\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I call the model.summary() after define and the build model that is defined by `tensorflow.keras.Model`, the error occurs.\r\nAfter some trial, I found the reason would be the dropout layer.\r\n\r\nThe output is:\r\n```\r\n2019-08-30 21:50:30.618187: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-08-30 21:50:30.630137: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f8a50c20bc0 executing computations on platform Host. Devices:\r\n2019-08-30 21:50:30.630161: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\nModel: \"my_model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\ndense (Dense)                multiple                  44\r\n_________________________________________________________________\r\ndense_1 (Dense)              multiple                  25\r\n_________________________________________________________________\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-1-c4b356e43733> in <module>\r\n     16 model = MyModel()\r\n     17 model.build((None, 10))\r\n---> 18 model.summary()\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py in summary(self, line_length, positions, print_fn)\r\n   1457                               line_length=line_length,\r\n   1458                               positions=positions,\r\n-> 1459                               print_fn=print_fn)\r\n   1460\r\n   1461   def _validate_graph_inputs_and_outputs(self):\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/layer_utils.py in print_summary(model, line_length, positions, print_fn)\r\n    224   for i in range(len(layers)):\r\n    225     if sequential_like:\r\n--> 226       print_layer_summary(layers[i])\r\n    227     else:\r\n    228       print_layer_summary_with_connections(layers[i])\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/layer_utils.py in print_layer_summary(layer)\r\n    182     name = layer.name\r\n    183     cls_name = layer.__class__.__name__\r\n--> 184     fields = [name + ' (' + cls_name + ')', output_shape, layer.count_params()]\r\n    185     print_row(fields, positions)\r\n    186\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in count_params(self)\r\n   1585                          ', but the layer isn\\'t built. '\r\n   1586                          'You can build it manually via: `' + self.name +\r\n-> 1587                          '.build(batch_input_shape)`.')\r\n   1588     return int(sum(np.prod(w.shape.as_list()) for w in self.weights))\r\n   1589\r\n\r\nValueError: You tried to call `count_params` on dropout, but the layer isn't built. You can build it manually via: `dropout.build(batch_input_shape)`.\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe model information should be shown.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self, training=False):\r\n        super(MyModel, self).__init__()\r\n        self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\r\n        self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\r\n        self.dropout = tf.keras.layers.Dropout(0.5)\r\n        self.training = training\r\n    def call(self, inputs):\r\n        x = self.dense1(inputs)\r\n        if self.training:\r\n            x = self.dropout(x, training=training)\r\n        return self.dense2(x)\r\n\r\nmodel = MyModel()\r\nmodel.build((None, 10))\r\nmodel.summary()\r\n```\r\n\r\n**Other info / logs**\r\n\r\nIf I remove the dropout layer, it works.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(MyModel, self).__init__()\r\n        self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\r\n        self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\r\n    def call(self, inputs):\r\n        x = self.dense1(inputs)\r\n        return self.dense2(x)\r\n\r\nmodel = MyModel()\r\nmodel.build((None, 10))\r\nmodel.summary()\r\n```\r\nOutput:\r\n```\r\nModel: \"my_model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\ndense_6 (Dense)              multiple                  44\r\n_________________________________________________________________\r\ndense_7 (Dense)              multiple                  25\r\n=================================================================\r\nTotal params: 69\r\nTrainable params: 69\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```\r\n", "comments": ["It would be weird that you pass `training` flag when constructing model. In your first example, the dropout layer is instantiated but is not built. `model.build()` can not build the dropout layer because `training` is `False` in `call()` method. That's why the error raises. The proper way to make the code work is in the following. I think it makes much sense to pass `training` to `call` instead of `__init__`. You could also find the same usage in [official doc](https://www.tensorflow.org/api_docs/python/tf/keras/Model).\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(MyModel, self).__init__()\r\n        self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\r\n        self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\r\n        self.dropout = tf.keras.layers.Dropout(0.5)\r\n\r\n    def call(self, inputs, training=None):\r\n        x = self.dense1(inputs)\r\n        x = self.dropout(x, training=training)\r\n        return self.dense2(x)\r\n\r\n\r\nmodel = MyModel()\r\nmodel.build((None, 10))\r\nmodel.summary()\r\n```", "Oh, it is my bug\ud83d\ude2d\r\n\r\n@WindQAQ , thank you for telling me about that !!!", "@doiken23, \r\nClosing the issue since its resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32113\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32113\">No</a>\n"]}, {"number": 32112, "title": "can't use tf.nn.moments in tflite", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (or github SHA if from source):Tensorflow nightly\r\n\r\nActually, error doesn't happened when I use the function of tf.nn.moments in code:\r\n`mean, var = tf.nn.moments(x, [2, 3, 4], keep_dims=True)`\r\nbut when i use the tflite model, `aborted (core dumped)` happened, then I realize tflite doesn't support it, but it really important for me. So could you help me to add the function into tflite or just recommend a similar function to me? Thanks a lot", "comments": ["@mmmmayi,\r\nCan you share a simple and standalone code to reproduce the issue?\r\nAlso mentioned TF version being used.Thanks!", "> @mmmmayi,\r\n> Can you share a simple and standalone code to reproduce the issue?\r\n> Also mentioned TF version being used.Thanks!\r\n\r\nI use it in the function to calculate group normalization, same as nn.GroupNorm() in PyTorch:\r\n```\r\ndef GroupNorm(x, G, eps=1e-05):\r\n    # x: input features with shape [N,H,W,C]\r\n    # G: number of groups for GN\r\n  N, H, W, C = x.shape\r\n  gamma = tf.ones([1, 1, 1, C])\r\n  beta = tf.zeros([1, 1, 1, C])\r\n  x = tf.reshape(x, [N, G, H, W, C // G])\r\n  mean, var = tf.nn.moments(x, [2, 3, 4], keep_dims=True)\r\n  x = (x-0) / tf.sqrt(1 + eps)\r\n  x = tf.reshape(x, [N, H, W, C])\r\n  return x * gamma + beta\r\n```\r\nTF version being used is: 1.15.0-dev20190821", "The recommended approach is to [convert the model](https://www.tensorflow.org/lite/guide/ops_select#converting_the_model) with TFLITE_BUILTINS, then with both TFLITE_BUILTINS,SELECT_TF_OPS, and finally with only SELECT_TF_OPS. Using both options (i.e. TFLITE_BUILTINS,SELECT_TF_OPS) creates models with TensorFlow Lite ops where possible.\r\nFurther if problem still persists, you have to add custom implementation to add this op.\r\nSee https://www.tensorflow.org/lite/guide/ops_custom", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32112\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32112\">No</a>\n"]}, {"number": 32111, "title": "ImportError: cannot import name 'dense_features' from 'tensorflow.python.feature_column'", "body": "```\r\nfrom tensorflow.python.feature_column import dense_features\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nImportError: cannot import name 'dense_features' from 'tensorflow.python.feature_column'\r\n```\r\n\r\ntensorflow==1.14.0\r\nPython==3.7.3\r\n\r\nWhat is the issue in this case?", "comments": ["I have the same issue with tensorflow 1.14.0, and trying to run a returnn config file", "I am able to replicate the issue in [TF-1.14](https://colab.research.google.com/gist/oanush/959d6b296f0966051f69cb29c37b1769/32111.ipynb) also issue not replicating in [TF-Nightly](https://colab.research.google.com/gist/oanush/959d6b296f0966051f69cb29c37b1769/32111.ipynb).Thanks!", "@oanush could you pls link the TF-Nightly colab  again correctly? :)", "Just to check, what versions of tensorflow and tensorflow_estimator are being used?", "@aiwalter `Dense_feature` is not there in the TF1.14 repository.  You can check the `feature_column` folder [here](https://github.com/tensorflow/tensorflow/tree/release_1.14.0/tensorflow/python/feature_column).\r\n\r\nI could import the `dense_features` without any issues using `tf-nightly` and `TF2.0rc0`. Here is the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/a245978bea0a4eb8b36bf13d99a8f735/tfnightly_32111.ipynb) with `tf-nightly`. [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/facb1ceb19dee592691a98060fba65de/tf20_32111.ipynb) is the gist with TF2.0. \r\n\r\nDid you see this with any TensorFlow tutorials on TF website? Thanks!", "@aiwalter \r\ni have met the same question. Do you resovle it?", "@mihaimaruseac \r\npython 3.6\r\ntensorflow is 1.14.0\r\ntensorflow.estimator is 1.14.0\r\n\r\nAnd I have met the same problem.In morning ,my program can run.But now it not. I want to know reason", "@aiwalter \r\nI resovle the problem.\r\nReinstall tensorflow  and tensorflow estimator", "@NancyLele thx, it works now for me as well, however Ia am wondering because its the same tf and tf estimator version still\r\n@jvishnuvardhan no, everything fine. I got this issue via `deeppavlov`", "Do you use nightly?", "Fixed this issue by reinstall tensorflow_estimator:\r\nsudo -H pip3 uninstall  tensorflow_estimator\r\nsudo -H pip3 install  tensorflow_estimator\r\n", "> Fixed this issue by reinstall tensorflow_estimator:\r\n> sudo -H pip3 uninstall tensorflow_estimator\r\n> sudo -H pip3 install tensorflow_estimator\r\n\r\nthx!", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32111\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32111\">No</a>\n", "I hit this same issue with tf-nightly-gpu (tb-nightly-1.15.0a20190911, tf-nightly-gpu-1.15.0.dev20190730,  tf-estimator-nightly-1.14.0.dev2019092301) Also tried with tensorflow_estimator 2.0, same error.\r\n\r\n```\r\n File \"/vol0/dan/tfn/lib/python3.6/site-packages/tensorflow_core/contrib/__init__.py\", line 39, in <module>\r\n    from tensorflow.contrib import compiler\r\n  File \"/vol0/dan/tfn/lib/python3.6/site-packages/tensorflow_core/contrib/compiler/__init__.py\", line 21, in <module>\r\n    from tensorflow.contrib.compiler import jit\r\n  File \"/vol0/dan/tfn/lib/python3.6/site-packages/tensorflow_core/contrib/compiler/__init__.py\", line 22, in <module>\r\n    from tensorflow.contrib.compiler import xla\r\n  File \"/vol0/dan/tfn/lib/python3.6/site-packages/tensorflow_core/contrib/compiler/xla.py\", line 22, in <module>\r\n    from tensorflow.python.estimator import model_fn as model_fn_lib\r\n  File \"/vol0/dan/tfn/lib/python3.6/site-packages/tensorflow_core/python/estimator/model_fn.py\", line 26, in <module>\r\n    from tensorflow_estimator.python.estimator import model_fn\r\n  File \"/vol0/dan/tfn/lib/python3.6/site-packages/tensorflow_estimator/__init__.py\", line 10, in <module>\r\n    from tensorflow_estimator._api.v1 import estimator\r\n  File \"/vol0/dan/tfn/lib/python3.6/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py\", line 10, in <module>\r\n    from tensorflow_estimator._api.v1.estimator import experimental\r\n  File \"/vol0/dan/tfn/lib/python3.6/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py\", line 10, in <module>\r\n    from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder\r\n  File \"/vol0/dan/tfn/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py\", line 23, in <module>\r\n    from tensorflow.python.feature_column import dense_features\r\nImportError: cannot import name 'dense_features'\r\n```\r\n\r\nUsing tensorflow-gpu==1.14.0 after uninstalling everything works. ", "That is because tf-nightly has been frozen at 9/11 whereas estimator nightly hasn't.\r\n\r\nYou can also use the 2.0 nightly preview instead, the 2.0 release candidate, or the 1.15 candidate.\r\n\r\nOnce 2.0 is fully released, nightlies will be unfrozen again.", "\r\nI hit this issue with tf_1.14 and 1.15. And also tried this with tf_estimater 2.0. Tried reinstalling tensorflow too. But still could not resolve this issue.\r\n\r\nERROR:root:Internal Python error in the inspect module.\r\nBelow is the traceback from this internal error.\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/vipin/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3319, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-55-3e782cef657a>\", line 4, in <module>\r\n    from alexnet import AlexNet\r\n  File \"/home/vipin/Downloads/MT2019005/alexnet.py\", line 2, in <module>\r\n    import tensorflow as tf\r\n  File \"/home/vipin/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 99, in <module>\r\n    from tensorflow_core import *\r\n  File \"/home/vipin/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py\", line 106, in <module>\r\n    from tensorflow.python.data.ops.optional_ops import OptionalSpec\r\nImportError: cannot import name 'OptionalSpec'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/vipin/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2034, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'ImportError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/vipin/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1151, in get_records\r\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\r\n  File \"/home/vipin/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\r\n    return f(*args, **kwargs)\r\n  File \"/home/vipin/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\r\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\r\n  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\r\n    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\r\n  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\r\n    filename = getsourcefile(frame) or getfile(frame)\r\n  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\r\n    if getattr(getmodule(object, filename), '__loader__', None) is not None:\r\n  File \"/usr/lib/python3.6/inspect.py\", line 733, in getmodule\r\n    if ismodule(module) and hasattr(module, '__file__'):\r\n  File \"/home/vipin/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"/home/vipin/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 941, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/home/vipin/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py\", line 106, in <module>\r\n    from tensorflow.python.data.ops.optional_ops import OptionalSpec\r\nImportError: cannot import name 'OptionalSpec'\r\n", "@kvtheckrock `pip list` should display all packages from the ecosystem at the same version and not mixing nightly and releases. Please open a new issue if that is the case and you still have issues, and please fill in the issue template.", "```\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-27-ff7d7ce981e2> in <module>\r\n----> 1 from keras.models import load_model\r\n      2 \r\n      3 \r\n      4 \r\n      5 lbl=['Close','Open']\r\n\r\nc:\\users\\hp\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\__init__.py in <module>\r\n      1 from __future__ import absolute_import\r\n      2 \r\n----> 3 from . import utils\r\n      4 from . import activations\r\n      5 from . import applications\r\n\r\nc:\\users\\hp\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\utils\\__init__.py in <module>\r\n      4 from . import data_utils\r\n      5 from . import io_utils\r\n----> 6 from . import conv_utils\r\n      7 \r\n      8 # Globally-importable utils.\r\n\r\nc:\\users\\hp\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\utils\\conv_utils.py in <module>\r\n      7 from six.moves import range\r\n      8 import numpy as np\r\n----> 9 from .. import backend as K\r\n     10 \r\n     11 \r\n\r\nc:\\users\\hp\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\__init__.py in <module>\r\n     85 elif _BACKEND == 'tensorflow':\r\n     86     sys.stderr.write('Using TensorFlow backend.\\n')\r\n---> 87     from .tensorflow_backend import *\r\n     88 else:\r\n     89     # Try and load external backend.\r\n\r\nc:\\users\\hp\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py in <module>\r\n      4 \r\n      5 import tensorflow as tf\r\n----> 6 from tensorflow.python.framework import ops as tf_ops\r\n      7 from tensorflow.python.training import moving_averages\r\n      8 from tensorflow.python.ops import tensor_array_ops\r\n\r\nImportError: cannot import name 'ops' from 'tensorflow.python.framework' (unknown location)\r\n```\r\n\r\n\r\ncan any one help me out?\r\n", "Please open new issue and fill in issue template"]}, {"number": 32110, "title": "Unexpected failure when preparing tensor allocations", "body": " Caused by: java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/conv.cc:235 input->dims->size != 4 (1 != 4)Node number 0 (CONV_2D) failed to prepare.", "comments": [" byte[] i=imgData.array();\r\n\r\n\r\n\r\n\r\n\r\n\r\n            ByteBuffer buffer = ByteBuffer.wrap(i);\r\n            FloatBuffer fb = buffer.asFloatBuffer();\r\n\r\n            float[] floatArray = new float[fb.limit()];\r\n            fb.get(floatArray);\r\n\r\n\r\n            tflite.run(floatArray, labelProbArray);", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here.\r\n\r\nIf you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32110\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32110\">No</a>\n"]}, {"number": 32109, "title": "Failed to import tensorflow ImportError: DLL load failed with error code 3221225501", "body": "Traceback (most recent call last):\r\n  File \"C:\\Users\\IEMCSE\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\IEMCSE\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\IEMCSE\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\IEMCSE\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\IEMCSE\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code 3221225501\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/IEMCSE/Documents/Life_In_IEM/Projects/Code/Data_Management/Segment_data.py\", line 2, in <module>\r\n    import tensorflow\r\n  File \"C:\\Users\\IEMCSE\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\IEMCSE\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\IEMCSE\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\IEMCSE\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\IEMCSE\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\IEMCSE\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\IEMCSE\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\IEMCSE\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code 3221225501\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nProcess finished with exit code 1\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32109\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32109\">No</a>\n", "can we uninstalled the gpu version and install the cpu version again?"]}, {"number": 32108, "title": "How to break tf.while_loop inside body function?", "body": "The issues like this, i write py code for segment chinese sentence, it's easy. But when i use tf do this thing, it' s unlucky ! Specific issues are as follows: \r\n\r\n- py code\r\n \r\n``` python\r\nx = list(\"\u6211\u662f\u7b97\u6cd5\u5de5\u7a0b\u5e08\")\r\ny = [3, 3, 0, 2, 0, 1, 2]\r\ndef func(x, y):\r\n    res = []\r\n    for i in range(len(x)):\r\n        if y[i] == 3:\r\n            res.append(x[i])\r\n        elif y[i] == 0:\r\n            str_ = \"\"\r\n            while i < len(x):\r\n                str_ += x[i]\r\n                if y[i] == 2:\r\n                    break\r\n                i += 1\r\n            res.append(str_)\r\n    return res\r\n```\r\n\r\n##### call func\r\nfunc(x, y)\r\n['\u6211', '\u662f', '\u7b97\u6cd5', '\u5de5\u7a0b\u5e08']\r\n\r\n- tf code\r\n\r\n``` python \r\nx = tf.constant(list(\"\u6211\u662f\u7b97\u6cd5\u5de5\u7a0b\u5e08\"))\r\ny = tf.constant([3, 3, 0, 2, 0, 1, 2, 3, 3])\r\n\r\ni = tf.constant(0)\r\nlabel = tf.constant(0)\r\nout = tf.constant([], dtype=tf.string)\r\n\r\ndef cond(i, label, out):\r\n    return tf.not_equal(i, x.shape[0])\r\n\r\ndef body(i, label, out):\r\n    \r\n    str_ = tf.constant(\"\")\r\n    \r\n    def cond(i, label, str_):\r\n        return tf.not_equal(i, x.shape[0])\r\n    \r\n    def body(i, label, str_):\r\n        \r\n        str_ = str_ + x[i]\r\n            \r\n        def _continue(str_):\r\n            def func_1():\r\n                return str_\r\n            return func_1\r\n        \r\n        def _break():\r\n            def func_2():\r\n                return str_\r\n            return func_2\r\n        \r\n        str_ = tf.cond(\r\n            tf.equal(y[i], 2),\r\n            _break(),\r\n            _continue(str_)\r\n        )\r\n        return [tf.add(i, 1), y[i], str_]\r\n    \r\n    _, _, str_ = tf.while_loop(cond, body, [i, label, str_], shape_invariants=[i.get_shape(), label.get_shape(), tf.TensorShape(None)])\r\n        \r\n    return [tf.add(i, 1), y[i], tf.cond(tf.equal(y[i], 3), lambda: tf.concat((out, x[i:i+1]), axis=0), lambda: tf.cond(tf.equal(y[i], 0), lambda: tf.concat((out, [str_]), axis=0), lambda: out))]\r\n\r\ni, label, out = tf.while_loop(cond, body, [i, label, out], shape_invariants=[i.get_shape(), label.get_shape(), tf.TensorShape([None])])\r\n```\r\n\r\n##### session run\r\n``` python\r\nwith tf.Session() as sess:\r\n    o_i, o_label, o_out = sess.run([i, label, out])\r\n```\r\n\r\n\r\ni want to same result from py to tf,  and i know what 's problem cause this result . But i can't break tf.while_loop inside body function, i have no idea for this. Could you give me some advice\uff1f\r\n\r\n- My env\r\n  - tensorflow: 1.13.1\r\n  - os:  win10\r\n  - python: 3.5\r\n", "comments": ["I have tried on colab with TF version 1.13.1 and was able to reproduce the issue.Please, find the [gist ](https://colab.sandbox.google.com/gist/ravikyram/cf54250e8963f686dcc73ad1a0b3d16f/untitled151.ipynb) here.Thanks!", "This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "> I have tried on colab with TF version 1.13.1 and was able to reproduce the issue.Please, find the [gist ](https://colab.sandbox.google.com/gist/ravikyram/cf54250e8963f686dcc73ad1a0b3d16f/untitled151.ipynb) here.Thanks!\r\n\r\nAs you can see,  we can't break inner loop on tf version code. we expect \u2018ai\u2019\uff0cbut return \u2018b'airam\u2018.  The problem is inner loop can't break even if we already get result. ", "@zepen Please check with TF1.15.0rc1 and post the issue in Stackoverflow. You might get quick response from the community as it is more of a support question.\r\n\r\nI am closing the issue here. Please feel free to open a new issue when you notice a bug in TF. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32108\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32108\">No</a>\n"]}, {"number": 32107, "title": "fix lite quantization_spec document error", "body": "", "comments": []}, {"number": 32106, "title": "\"Retval[0] does not have value\" error when using conditions and control dependencies", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):   Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 9.0.252\r\n- GPU model and memory: GeForce GTX 1050 4031MB\r\n\r\n**Describe the current behavior**\r\n Error that gives us no information what is actually wrong:\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value\r\n**Describe the expected behavior**\r\n no error and the result should be 4\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nNote: you might have to run the code multiple times. The error appeared in 3 out of 5 runs.\r\n```python\r\nimport tensorflow as tf\r\n\r\nx= tf.constant(2.0)\r\nbool_var = tf.get_variable(\"bool_var\", initializer=tf.constant(True), trainable=False,\r\n                                             dtype=tf.bool)\r\nx_copy_var = tf.get_variable(\"x_copy_var\", x.shape, initializer=tf.zeros_initializer(),trainable=False)\r\n\r\ndef true_func():\r\n    assign_op = x_copy_var.assign(x)\r\n    set_bool_var_false = bool_var.assign(False)\r\n    with tf.control_dependencies([assign_op,set_bool_var_false]):\r\n        return x*x_copy_var\r\n\r\ncond = tf.cond(bool_var,true_func,lambda:x)\r\ncond= tf.Print(cond,[cond])\r\n\r\nsess= tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nsess.run([cond])\r\n```\r\n\r\n**Other info / logs**\r\n\r\nFull error log:\r\n\r\n/***/***/***/py36Env/bin/python3.6 /snap/pycharm-community/147/helpers/pydev/pydevd.py --multiproc --qt-support=auto --client 127.0.0.1 --port 40811 --file /home/***/PycharmProjects/***/test/cond_test.py\r\npydev debugger: process 7200 is connecting\r\n\r\nConnected to pydev debugger (build 192.6262.63)\r\nWARNING:tensorflow:From /home/***/***/***/test/cond_test.py:16: Print (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2018-08-20.\r\nInstructions for updating:\r\nUse tf.print instead of tf.Print. Note that tf.print returns a no-output operator that directly prints the output. Outside of defuns or eager mode, this operator will not be executed unless it is directly specified in session.run or used as a control dependency for other operators. This is only a concern in graph mode. Below is an example of how to ensure tf.print executes in graph mode:\r\n```python\r\n    sess = tf.Session()\r\n    with sess.as_default():\r\n        tensor = tf.range(10)\r\n        print_op = tf.print(tensor)\r\n        with tf.control_dependencies([print_op]):\r\n          out = tf.add(tensor, tensor)\r\n        sess.run(out)\r\n    ```\r\nAdditionally, to use tf.print in python 2.7, users must make sure to import\r\nthe following:\r\n\r\n  `from __future__ import print_function`\r\n\r\n2019-08-30 10:08:02.921500: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-08-30 10:08:02.928495: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\r\n2019-08-30 10:08:02.928527: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] retrieving CUDA diagnostic information for host: u-172-c053\r\n2019-08-30 10:08:02.928533: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:170] hostname: u-172-c053\r\n2019-08-30 10:08:02.928562: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:194] libcuda reported version is: 390.87.0\r\n2019-08-30 10:08:02.928584: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:198] kernel reported version is: 390.87.0\r\n2019-08-30 10:08:02.928589: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:305] kernel version seems to match DSO: 390.87.0\r\nTraceback (most recent call last):\r\n  File \"/localhome/***/virtualEnvironments/py36Env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"/localhome/***/virtualEnvironments/py36Env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/localhome/***/virtualEnvironments/py36Env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/snap/pycharm-community/147/helpers/pydev/pydevd.py\", line 2060, in <module>\r\n    main()\r\n  File \"/snap/pycharm-community/147/helpers/pydev/pydevd.py\", line 2054, in main\r\n    globals = debugger.run(setup['file'], None, None, is_module)\r\n  File \"/snap/pycharm-community/147/helpers/pydev/pydevd.py\", line 1405, in run\r\n    return self._exec(is_module, entry_point_fn, module_name, file, globals, locals)\r\n  File \"/snap/pycharm-community/147/helpers/pydev/pydevd.py\", line 1412, in _exec\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"/snap/pycharm-community/147/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"/home/***/***/***/test/cond_test.py\", line 21, in <module>\r\n    sess.run([cond])\r\n  File \"/localhome/***/virtualEnvironments/py36Env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/localhome/***/virtualEnvironments/py36Env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/localhome/***/virtualEnvironments/py36Env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/localhome/***/virtualEnvironments/py36Env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value\r\n", "comments": ["A few of such issues were fixed in COND_V2. Could you try with `export TF_ENABLE_COND_V2=1`?", "@MaximusMutschler,\r\nCan you try solution suggested by @ppwwyyxx. Thanks!", "Yes the solution works. Thanks a lot!", "Closing since the issue is resolved.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32106\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32106\">No</a>\n"]}, {"number": 32105, "title": "FATAL error when using TF_CUDNN_USE_AUTOTUNE=0", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v1.12.1-10080-g6e0893c79c 1.14.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): 7.4\r\n- CUDA/cuDNN version: 10.0 / 7.6.2\r\n- GPU model and memory: V100 16G\r\n\r\n**Describe the current behavior**\r\nI have a model which runs fine. However, after I use\r\n```\r\nexport TF_CUDNN_USE_AUTOTUNE=0\r\n```\r\nin order to save some autotuning time, it starts to crash with this error in the first iteration:\r\n```\r\n019-08-30 00:05:05.705707: F tensorflow/stream_executor/cuda/cuda_dnn.cc:262] Unsupported Cudnn convolution backward algorithm for filter: 6\r\nzsh: abort (core dumped)  python3 xx.py\r\n```\r\n\r\n**Code to reproduce the issue**\r\nCan try to provide one but I think the information is probably already enough to pin down the problem.\r\n\r\n**Other info / logs**\r\nThe algorithm 6, mentioned in the error message, corresponds to `CUDNN_CONVOLUTION_BWD_FILTER_ALGO_FFT_TILING`, which is explictly disabled in tensorflow: \r\nhttps://github.com/tensorflow/tensorflow/blob/a4bfabe12869a00138173afe739733065b39f4a0/tensorflow/stream_executor/cuda/cuda_dnn.cc#L258-L265\r\n\r\nWhen I set AUTOTUNE=0, cudnn will choose the best algorithm with its own heuristics, and I suspect that TensorFlow crashes because cudnn chooses this algorithm that TF has disabled.\r\n\r\ncc @chsigg ", "comments": ["In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "I understand you have a templated response to ask for repro, which is good for filtering out a large portion of issues which are more likely due to user error than a tensorflow bug. But I think I already pointed out the root cause of the issue, and a repro isn't quite necessary to investigate the issue.\r\n\r\nIn fact, the issue probably requires a very specific version of cudnn to reproduce anyway, so it's unlikely that whatever I provided would be useful.", "I have the same issue. Did you find any way how to bypass this?", "@ppwwyyxx, would you be willing to send a fix for this?  Probably the easiest thing to do is allow the user to pick something other than `CUDNN_CONVOLUTION_BWD_FILTER_ALGO_FFT_TILING` (if there is a safe choice) when `TF_CUDNN_USE_AUTOTUNE` is `0`.", "@ppwwyyxx \r\nPlease update as per above comment, if this is not an issue now, please move this to closed status.", "The issue should still exist since `CUDNN_CONVOLUTION_BWD_FILTER_ALGO_FFT_TILING` is still disabled. Unfortunately I'm not able to send a fix.", "I have the same issue. Did you find any way how to bypass this?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32105\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32105\">No</a>\n"]}, {"number": 32104, "title": "Training with GPU on TF 2.0 is much slower than on TF 1.14 if set a large number to `input_dim` of `tf.keras.layers.Embedding`", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Linux`-3.10.0-957.21.3.el7.x86_64 `CentOS-7.3.1611`-Core\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): binary, pip install tensorflow-gpu\r\n- TensorFlow version (use command below): `2.0.0-rc0`(v2.0.0-beta1-5101-gc75bb66), `1.14.0`(v1.14.0-rc1-22-gaf24dc91b5)\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): None\r\n- CUDA/cuDNN version: CUDA 10.0.130, cuDNN 7.6.3.30\r\n- GPU model and memory: RTX 2070 Super, 8GB\r\n\r\n**Describe the current behavior**  \r\nI converted the `Keras` implementation of [`Neural Matrix Factorization (NeuMF)`](https://github.com/hexiangnan/neural_collaborative_filtering/blob/master/NeuMF.py) to `tf.keras` and it works well on TF 1.14.  \r\nBut when I run it on TF 2.0.0-rc0, the training is much slower than on TF 1.14.  \r\nI use the profiling tools to check the time, and I found `ReadVariableOp` takes too much time if I set a large number to the `input_dim` of `tf.keras.layers.Embedding`.  \r\n\r\n``` bash\r\nTensorflow version:  2.0.0-rc0\r\nEpoch 1/3\r\n10000/10000 [==============================] - 5s 532us/sample - loss: 0.6935\r\nEpoch 2/3\r\n10000/10000 [==============================] - 4s 436us/sample - loss: 0.6903\r\nEpoch 3/3\r\n10000/10000 [==============================] - 4s 431us/sample - loss: 0.6851\r\n```\r\n\r\n```bash\r\nTensorflow version:  1.14.0\r\nEpoch 1/3\r\n10000/10000 [==============================] - 2s 212us/sample - loss: 0.7035\r\nEpoch 2/3\r\n10000/10000 [==============================] - 0s 28us/sample - loss: 0.6981\r\nEpoch 3/3\r\n10000/10000 [==============================] - 0s 29us/sample - loss: 0.6909\r\n```\r\n\r\n**Describe the expected behavior**  \r\nThe speed of training on TF 2.0 with large `input_dim` of `Embedding` should be the same as TF 1.14 or faster.  \r\n\r\n**Code to reproduce the issue**  \r\nI have shared the codes on [Colab](https://drive.google.com/open?id=1wu3fhjtEYtFVWsby5OO4IQ7tRHf5xHWt)\r\nor check the codes below.  \r\n``` Python\r\n# -*- coding:utf-8 -*-\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.regularizers import l1, l2\r\nfrom tensorflow.keras.layers import Embedding, Input, Dense, Lambda, Flatten\r\n\r\ndef get_model(num_users, num_items, mf_dim=10, layers=[10], reg_layers=[0], reg_mf=0, alpha=0.5):\r\n  assert len(layers) == len(reg_layers)\r\n  num_layer = len(layers) #Number of layers in the MLP\r\n  \r\n  # Input variables\r\n  user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\r\n  item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\r\n  \r\n  # Embedding layer\r\n  MF_Embedding_User = Embedding(input_dim = num_users, output_dim = mf_dim, name = 'mf_embedding_user', \r\n                                embeddings_initializer = keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None), embeddings_regularizer = l2(reg_mf), \r\n                                input_length=1)\r\n  MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = mf_dim, name = 'mf_embedding_item', \r\n                                embeddings_initializer = keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None), embeddings_regularizer = l2(reg_mf), \r\n                                input_length=1)\r\n\r\n  MLP_Embedding_User = Embedding(input_dim = num_users, output_dim = int(layers[0]/2), name = \"mlp_embedding_user\", \r\n                                  embeddings_initializer = keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None), embeddings_regularizer = l2(reg_layers[0]), \r\n                                  input_length=1)\r\n  MLP_Embedding_Item = Embedding(input_dim = num_items, output_dim = int(layers[0]/2), name = 'mlp_embedding_item', \r\n                                  embeddings_initializer = keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None), embeddings_regularizer = l2(reg_layers[0]), \r\n                                  input_length=1)\r\n\r\n  # MF part\r\n  mf_user_latent = Flatten()(MF_Embedding_User(user_input))\r\n  mf_item_latent = Flatten()(MF_Embedding_Item(item_input))\r\n  mf_vector = keras.layers.Multiply()([mf_user_latent, mf_item_latent])\r\n\r\n  # MLP part\r\n  mlp_user_latent = Flatten()(MLP_Embedding_User(user_input))\r\n  mlp_item_latent = Flatten()(MLP_Embedding_Item(item_input))\r\n  mlp_vector = keras.layers.Concatenate(axis=-1)([mlp_user_latent, mlp_item_latent])\r\n\r\n  for idx in range(1, num_layer):\r\n    mlp_vector = Dense(layers[idx], \r\n                      activation='relu', \r\n                      kernel_regularizer = l2(reg_layers[idx]), \r\n                      bias_regularizer = l2(reg_layers[idx]), \r\n                      name=\"layer%d\" %idx)(mlp_vector)\r\n\r\n  # Concatenate MF and MLP parts\r\n  mf_vector = Lambda(lambda x: x * alpha)(mf_vector)\r\n  mlp_vector = Lambda(lambda x : x * (1 - alpha))(mlp_vector)\r\n  predict_vector = keras.layers.Concatenate(axis=-1)([mf_vector, mlp_vector])\r\n\r\n  # Final prediction layer\r\n  prediction = Dense(1, \r\n                    activation='sigmoid', \r\n                    kernel_initializer='lecun_uniform', \r\n                    bias_initializer ='lecun_uniform', \r\n                    name = \"prediction\")(predict_vector)\r\n\r\n  model = keras.Model(inputs=[user_input, item_input], outputs=[prediction])\r\n  return model\r\n\r\ndef generate_data(num_user, num_item, count=100):\r\n    user_input = []\r\n    item_input = []\r\n    labels = []\r\n    for _ in range(count):\r\n        user = np.random.randint(0,num_user)\r\n        item = np.random.randint(0,num_item)\r\n        label = np.random.randint(0,2)\r\n        user_input.append(user)\r\n        item_input.append(item)\r\n        labels.append(label)\r\n    return np.asarray(user_input), np.asarray(item_input), np.asarray(labels)\r\n\r\ndef test_model():\r\n    num_user = 1000000\r\n    num_item = 100000\r\n    count = 10000\r\n    user_input, item_input, labels = generate_data(num_user, num_item, count)\r\n\r\n    model = get_model(num_user, num_item)\r\n    model.compile(\r\n        optimizer=tf.keras.optimizers.Adam(),\r\n        loss=tf.keras.losses.BinaryCrossentropy()\r\n    )\r\n\r\n    # Callbacks\r\n    callbacks = [ tf.keras.callbacks.TensorBoard(log_dir='tb-logs') ]\r\n    model.fit([user_input, item_input], labels, batch_size=256, epochs=3, callbacks=callbacks)\r\n\r\nif __name__ == \"__main__\":\r\n    print(\"Tensorflow version: \", tf.__version__)\r\n    test_model()\r\n```\r\n\r\n**Other info / logs**  \r\nThe attachment '[tb-logs.zip](https://github.com/tensorflow/tensorflow/files/3558500/tb-logs.zip)' is the tensorboard logs.   \r\n\r\nThe profiling screenshot of the training on TF 2.0.0-rc0.  \r\n![tf2-profile](https://user-images.githubusercontent.com/4043644/63997461-2b315480-cb31-11e9-9983-43baaeb3b002.png)  \r\n\r\nThe profiling screenshot of the training on TF 1.14.  \r\n![tf114-profile](https://user-images.githubusercontent.com/4043644/63997474-33898f80-cb31-11e9-8f7d-e02cc6496e69.png)\r\n", "comments": ["@DeviLeo, I tried executing the code on both the versions, looks like both versions performed same. Please take a look at gist of [Tf-2.0.0-rc0](https://colab.sandbox.google.com/gist/gadagashwini/006b6150033cfbeb5ee5bff92f872747/untitled118.ipynb) and [Tf-1.14.0](https://colab.sandbox.google.com/gist/gadagashwini/f66bf06522a8db74e0e7f32657e7aa72/untitled119.ipynb). Please let me if my understanding is bad. Thanks!", "@gadagashwini Could you try again with GPU? If training with CPU and TPU, both versions are almost the same.", "@DeviLeo, I could reproduce the issue with GPU,please take a look at gist [here](https://colab.sandbox.google.com/gist/gadagashwini/bf42d7075db46452aeecfe46630bf455/untitled118.ipynb). Thanks!", "Please take a look at colab [gist](https://colab.sandbox.google.com/gist/gadagashwini/d17393fc030430155a4948491fba1e68/untitled119.ipynb) with Tensorflow 1.14.0.", "@DeviLeo -- are you sure you're running on GPU? It's not clear how you are intending to place ops on GPU here. Have you tried with [tf.distribute.MirroredStrategy](https://www.tensorflow.org/beta/guide/distribute_strategy)?", "@karmel Yes, I'm sure it is running on GPU. At least `nvidia-smi` shows the python process.  \r\n\r\nI have just tried with [tf.distribute.MirroredStrategy](https://www.tensorflow.org/beta/guide/distribute_strategy), and it worked with `model.fit`.   \r\nBut in practical applications, I use `model.fit_generator` and ``NotImplementedError: `fit_generator` is not supported for models compiled with tf.distribute.Strategy.`` is raised.", "Can you use .fit instead? In the code snippet above, you appear to be using .fit; what requires the generator?", "Sorry, I'm afraid I cannot use `` `fit` `` instead.  \r\nThe dataset for training and validation is about 50 million pairs of user-item and I have to use `` `fit_generator` `` to avoid OOM.", "There seems to be a significant slowdown **generally** when using TF2 fit_generator.  It seems to be around a 3x slowdown in my own code between TF1 and TF2.  It is easy to reproduce using the \"Transfer Learning with TFHub\" example from the TF2 official tutorials on Collab:  https://www.tensorflow.org/beta/tutorials/images/hub_with_keras\r\n\r\nTo reproduce it, all I did was change model.fit to model.fit_generator.  I ran these cases for both TF2 and TF1.  TF1 is via this change to the first code cell:\r\n`%tensorflow_version 1.x`\r\n\r\nHere's the training runs for the four cases:\r\n```\r\nTF2 fit\r\nEpoch 1/2\r\n115/115 [======] - 24s 212ms/step - loss: 0.6619 - acc: 0.9062\r\nEpoch 2/2\r\n115/115 [======] - 20s 178ms/step - loss: 0.3309 - acc: 0.8125\r\n\r\nTF2 fit_generator\r\n115/115 [======] - 56s 485ms/step - loss: 0.6666 - acc: 0.9375\r\nEpoch 2/2\r\n115/115 [======] - 49s 424ms/step - loss: 0.3345 - acc: 0.9688\r\n\r\nTF1 fit\r\nEpoch 1/2\r\n115/115 [======] - 16s 136ms/step - loss: 0.6406 - acc: 0.8750\r\nEpoch 2/2\r\n115/115 [======] - 15s 129ms/step - loss: 0.3279 - acc: 0.8750\r\n\r\nTF1 fit_generator\r\nEpoch 1/2\r\n115/115 [======] - 16s 139ms/step - loss: 0.7300 - acc: 0.8125\r\nEpoch 2/2\r\n115/115 [======] - 15s 132ms/step - loss: 0.3492 - acc: 0.9062\r\n```\r\nWith TF1, there is no difference between fit and fit_geneator as you might hope.  TF2 seems slower in general and fit_generator in particular is 3x slower than TF1--at least for this tutorial and my own code.\r\n\r\nBTW, Collab is using TF2 RC1 at the moment:\r\n```\r\ntf.__version__\r\n'2.0.0-rc1'\r\n```", "Can confirm with @cupdike . I had a similar issue with my own project when switching to TF2 (stable. I waited for the official release a couple days ago), with a 2x to 3x decrease in training time for the same data and code, as compared to TF1. After some Google searching and reading, I then proceeded to implement the code using tf.data.Dataset.from_generator(), instead, which allows me to use model.fit().\r\n\r\n**Unfortunately there was 0 performance benefit either way.**\r\n\r\nAs for some pseudocode (posting here just in case someone can point out something fundamentally wrong with my setup), my fit_generator version of my code went something like this below. All my code uses the internal tf.keras instead of the external one:\r\n\r\n    def datagen(args):\r\n        while True:\r\n            #some code here to load and manipulate data into x and y. Mostly numpy functions\r\n            yield x,y\r\n\r\n    #some here code to create and compile model \r\n\r\n    model.fit_generator(datagen(args), . . . )\r\n\r\n\r\n\r\nFor the pseudocode using tf.data.Dataset.from_generator():\r\n\r\n    from tensorflow.compat.v2.data import Dataset\r\n\r\n    def datagen(args):\r\n        while True:\r\n            #some code here to load and manipulate data into x and y. Mostly numpy functions\r\n            yield x,y\r\n\r\n    #some code here to create and compile model \r\n\r\n    train_data = Dataset.from_generator(generator=lambda: datagen(args), . . . )\r\n    model.fit(train_data , . . . )", "FYI this has been diagnosed in https://github.com/tensorflow/tensorflow/issues/33024. The issue is that `Model.fit_generator` is incorrectly running eagerly. (Consistent with @cupdike's observations)", "I am not sure `fit_generator` explains all of it: I ported code that uses embedding models from TF1 to TF2 and there was a significant decrease in performance, it was 2-3x slower.", "@tabacof Hi, I created the other issue linked. \r\n\r\n\r\nThere are two current resolutions to the issues people were having there;\r\nAdding the line `tf.compat.v1.disable_eager_execution()` right after `import tf` or switching to `model.fit(` when using TF 2.0\r\n\r\n\r\nYou say that you don't believe the issue is because of eager execution, the easiest way to prove that is to add the first fix `tf.compat.v1.disable_eager_execution()` right after importing TF. If this does not improved performance, then that should put to rest the eager execution argument.\r\n\r\n\r\nThe issue I encountered is that `fit_generator` is kicking it into eager execution no matter what as robeita says. And in my experience, eager execution is much slower in every case.\r\n\r\n\r\nI hope that this can help you isolate the issue so that the cause can be identified.", "Thanks to all. \r\n\r\nI've tried `tf.compat.v1.disable_eager_execution()` and `model.fit(x=generator, ...)` with and without [tf.distribute.MirroredStrategy](https://www.tensorflow.org/guide/distributed_training) but no help.  \r\nI think the key problem is the large `input_dim` of `tf.keras.layers.Embedding` and training with the generator.\r\n\r\nThe following cases are all tested with GPU on TF 2.0.0rc2 compared with TF 1.14.  \r\n\r\n0. Small `input_dim`, `model.fit` without generator, without [tf.distribute.MirroredStrategy](https://www.tensorflow.org/guide/distributed_training).  [Fast]\r\n1. Large `input_dim`, `model.fit` without generator, without [tf.distribute.MirroredStrategy](https://www.tensorflow.org/guide/distributed_training).  [Slow]\r\n2. Large `input_dim`, `model.fit` without generator, with [tf.distribute.MirroredStrategy](https://www.tensorflow.org/guide/distributed_training).  [Fast]\r\n3. Large `input_dim`, `model.fit` with generator, without [tf.distribute.MirroredStrategy](https://www.tensorflow.org/guide/distributed_training).  [Slow]\r\n4. Large `input_dim`, `model.fit` with generator, with [tf.distribute.MirroredStrategy](https://www.tensorflow.org/guide/distributed_training).  [Slow]\r\n\r\nHere the pseudo code I have tried.\r\n``` python\r\nimport tensorflow as tf\r\n\r\n# `disable_eager_execution` conflicts with `tf.distribute.MirroredStrategy`. \r\n# \"AssertError: assert isinstance(x, dataset_ops.DatasetV2)\" will be raised.\r\n# tf.compat.v1.disable_eager_execution() \r\n\r\nfrom tensorflow.keras.utils import Sequence\r\n\r\nclass MyGenerator(Sequence):\r\n    def __init__(self, ...):\r\n        # do something\r\n\r\n    def __iter__(self):\r\n        return self\r\n\r\n    def __len__(self):\r\n        return batches\r\n\r\n    def __getitem__(self, index):\r\n        # do something\r\n        return tuple([x1, x2, ...]), y\r\n\r\ndef train():\r\n    strategy = tf.distribute.MirroredStrategy()\r\n    print ('Number of devices: {}'.format(strategy.num_replicas_in_sync))\r\n    with strategy.scope():\r\n        model = ... # Create model\r\n        model.compile(...)\r\n        \r\n        my_gen = MyGenerator(...) # Create generator\r\n\r\n        model.fit(\r\n            x=my_gen,\r\n            # Do not specify `y` and `batch_size` if x is a dataset, generator, or keras.utils.Sequence instance\r\n            # Other arguments are the same as `fit_generator`'s\r\n            ...\r\n        )\r\n\r\nif __name__ == '__main__':\r\n    train()\r\n```", "Regarding your gist on colab, I suspect it is normal to see slower training with 2.0. When you do `print(tf.test.is_gpu_available())` it returns False currently, so that means the gpu is not used right ?\r\nIs it the case in your computer too?", "@Shiro-LK No, it returns True both on the colab and my computer.\r\n\r\n``` bash\r\n# python3\r\nPython 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34)\r\n[GCC 7.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> print(\"Tensorflow version: \", tf.__version__)\r\nTensorflow version:  2.0.0\r\n>>> print(\"is_gpu_available: \", tf.test.is_gpu_available())\r\n2019-11-21 17:15:31.360917: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-11-21 17:15:31.366375: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3593275000 Hz\r\n2019-11-21 17:15:31.367054: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56555a7aa340 executing computations on platform Host. Devices:\r\n2019-11-21 17:15:31.367080: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-11-21 17:15:31.368710: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-11-21 17:15:31.977171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-21 17:15:31.977604: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56555a8622c0 executing computations on platform CUDA. Devices:\r\n2019-11-21 17:15:31.977636: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2070 SUPER, Compute Capability 7.5\r\n2019-11-21 17:15:31.977803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-21 17:15:31.978367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce RTX 2070 SUPER major: 7 minor: 5 memoryClockRate(GHz): 1.905\r\npciBusID: 0000:08:00.0\r\n2019-11-21 17:15:31.978611: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-11-21 17:15:31.979953: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-11-21 17:15:31.981247: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-11-21 17:15:31.981508: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-11-21 17:15:31.983163: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-11-21 17:15:31.984398: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-11-21 17:15:31.988258: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-11-21 17:15:31.988370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-21 17:15:31.988985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-21 17:15:31.989527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-11-21 17:15:31.989563: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-11-21 17:15:31.990447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-11-21 17:15:31.990469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0\r\n2019-11-21 17:15:31.990482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N\r\n2019-11-21 17:15:31.990590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-21 17:15:31.991174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-21 17:15:31.991771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:0 with 7478 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:08:00.0, compute capability: 7.5)\r\nis_gpu_available:  True\r\n```", "Hi guys, any updates?", "@DeviLeo and @maximveksler can you please try with 2.2.0-rc2, there have been multiple fixes in improving 2.x performance. \r\n\r\nhere is a [colab gist](https://colab.research.google.com/gist/goldiegadde/6792061490ab598b5daa8eda18660160/github-issue-32104.ipynb) of the original issue with 2.2.0-rc2.", "@DeviLeo Thanks for the report! Would you mind trying this on tf==2.2, since it contains many performance improvements on eager", "@goldiegadde and @tanzhenyu I have tried on `tf 2.2.0-rc2` with eager mode **disabled** and the issue has gone. But with eager mode **enabled**, both on `tf 2.2.0-rc2` and `tf 1.15.2` are slower.\r\n\r\n**Eager mode: Disabled**\r\n``` bash\r\nTensorflow version:  2.2.0-rc2\r\nTensorflow eager mode:  False\r\nis_gpu_available:  True\r\nTrain on 10000 samples\r\nEpoch 1/3\r\n10000/10000 [==============================] - 1s 87us/sample - loss: 0.8003\r\nEpoch 2/3\r\n10000/10000 [==============================] - 1s 90us/sample - loss: 0.7889\r\nEpoch 3/3\r\n10000/10000 [==============================] - 1s 90us/sample - loss: 0.7758\r\n```\r\n\r\n``` bash\r\nTensorflow version:  1.15.2\r\nTensorflow eager mode:  False\r\nis_gpu_available:  True\r\nTrain on 10000 samples\r\nEpoch 1/3\r\n10000/10000 [==============================] - 1s 95us/sample - loss: 0.7089\r\nEpoch 2/3\r\n10000/10000 [==============================] - 0s 31us/sample - loss: 0.7039\r\nEpoch 3/3\r\n10000/10000 [==============================] - 0s 32us/sample - loss: 0.6972\r\n```\r\n\r\n**Eager mode: Enabled**\r\n``` bash\r\nTensorflow version:  2.2.0-rc2\r\nTensorflow eager mode:  True\r\nis_gpu_available:  True\r\nEpoch 1/3\r\n40/40 [==============================] - 7s 181ms/step - loss: 0.7136\r\nEpoch 2/3\r\n40/40 [==============================] - 7s 178ms/step - loss: 0.7085\r\nEpoch 3/3\r\n40/40 [==============================] - 7s 177ms/step - loss: 0.7034\r\n```\r\n\r\n``` bash\r\nTensorflow version:  1.15.2\r\nTensorflow eager mode:  True\r\nis_gpu_available:  True\r\nTrain on 10000 samples\r\nEpoch 1/3\r\n10000/10000 [==============================] - 9s 947us/sample - loss: 0.9751\r\nEpoch 2/3\r\n10000/10000 [==============================] - 8s 845us/sample - loss: 0.9588\r\nEpoch 3/3\r\n10000/10000 [==============================] - 8s 848us/sample - loss: 0.9409\r\n```\r\nSo, I think the issue is solved. \r\n\r\nThank you all.\r\n", "> @goldiegadde and @tanzhenyu I have tried on `tf 2.2.0-rc2` with eager mode **disabled** and the issue has gone. But with eager mode **enabled**, both on `tf 2.2.0-rc2` and `tf 1.15.2` are slower.\r\n> \r\n> **Eager mode: Disabled**\r\n> \r\n> ```shell\r\n> Tensorflow version:  2.2.0-rc2\r\n> Tensorflow eager mode:  False\r\n> is_gpu_available:  True\r\n> Train on 10000 samples\r\n> Epoch 1/3\r\n> 10000/10000 [==============================] - 1s 87us/sample - loss: 0.8003\r\n> Epoch 2/3\r\n> 10000/10000 [==============================] - 1s 90us/sample - loss: 0.7889\r\n> Epoch 3/3\r\n> 10000/10000 [==============================] - 1s 90us/sample - loss: 0.7758\r\n> ```\r\n> \r\n> ```shell\r\n> Tensorflow version:  1.15.2\r\n> Tensorflow eager mode:  False\r\n> is_gpu_available:  True\r\n> Train on 10000 samples\r\n> Epoch 1/3\r\n> 10000/10000 [==============================] - 1s 95us/sample - loss: 0.7089\r\n> Epoch 2/3\r\n> 10000/10000 [==============================] - 0s 31us/sample - loss: 0.7039\r\n> Epoch 3/3\r\n> 10000/10000 [==============================] - 0s 32us/sample - loss: 0.6972\r\n> ```\r\n> \r\n> **Eager mode: Enabled**\r\n> \r\n> ```shell\r\n> Tensorflow version:  2.2.0-rc2\r\n> Tensorflow eager mode:  True\r\n> is_gpu_available:  True\r\n> Epoch 1/3\r\n> 40/40 [==============================] - 7s 181ms/step - loss: 0.7136\r\n> Epoch 2/3\r\n> 40/40 [==============================] - 7s 178ms/step - loss: 0.7085\r\n> Epoch 3/3\r\n> 40/40 [==============================] - 7s 177ms/step - loss: 0.7034\r\n> ```\r\n> \r\n> ```shell\r\n> Tensorflow version:  1.15.2\r\n> Tensorflow eager mode:  True\r\n> is_gpu_available:  True\r\n> Train on 10000 samples\r\n> Epoch 1/3\r\n> 10000/10000 [==============================] - 9s 947us/sample - loss: 0.9751\r\n> Epoch 2/3\r\n> 10000/10000 [==============================] - 8s 845us/sample - loss: 0.9588\r\n> Epoch 3/3\r\n> 10000/10000 [==============================] - 8s 848us/sample - loss: 0.9409\r\n> ```\r\n> \r\n> So, I think the issue is solved.\r\n> \r\n> Thank you all.\r\n\r\nAwesome!", "Just upgraded to TF 2.2 . Having huge performance issues with keras models.\r\n\r\n**Update** : \r\nDisabled eager execution and changed all my inputs to the `model.predict()` function from tensors to numpy arrays. Looks like its much faster now.\r\n\r\n**Update 2** : \r\nNevermind. Im switching back to 1.14. My summary writer dosent work if i use `tf.compat.v1.disable_eager_execution()`. Have no time to deal with all these bugs.", "Having similar problems to DollarAkshay. For me, I can't get lookup table initializers to work with eager execution disabled. Strongly considering PyTorch at this point.", "It seems contradictory to what @DeviLeo verified -- do you have a concreate code snippet to reproduce?", "Having the same problem. I have upgraded to TF2.2. Moreover, I use the CPU to run the code. Through analyzing the timeprofile, I also find the _embedding_lookup ReadVariableOp_ takes the too much time.", "> Having the same problem. I have upgraded to TF2.2. Moreover, I use the CPU to run the code. Through analyzing the timeprofile, I also find the _embedding_lookup ReadVariableOp_ takes the too much time.\r\n\r\nI have the same problem\uff0cTensorflow version is 2.5.0. `ReadVariableOp` took a long time\uff0cand unstable memory usage.", "> I have the same problem\uff0cTensorflow version is 2.5.0. `ReadVariableOp` took a long time\uff0cand unstable memory usage.\r\n\r\n@PWZER I met the same issue and I fixed it by upgrading to TF2.6"]}, {"number": 32103, "title": "TF-TRT slower than optimized saved model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, I have a network that does 2D convultions + batch normalization on an image.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): TF 1.14\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): 7.4\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: T4, 12GB\r\n\r\n**Describe the current behavior**\r\n\r\nI'm trying to optimize a custom model comprised of 2D convolutions and batch normalizations done on an image.  The entire network has fixed dimensions.  I'm using the nightly docker TF image to perform TF-TRT.\r\n\r\nI've tried to create a TRT model using both of the following functions:\r\n\r\n```\r\ndef create_trt_saved_model(saved_model_dir, output_saved_model_dir, precision, batch_size=1):\r\n\t''' convert saved model to TRT saved model'''\r\n\r\n\tconverter = trt.TrtGraphConverter(\r\n\t\tinput_saved_model_dir = str(saved_model_dir),\r\n\t\tmax_batch_size = batch_size,\r\n\t\tprecision_mode = precision )\r\n\tconverter.convert()\r\n\tconverter.save(output_saved_model_dir = str(output_saved_model_dir))\r\n```\r\n\r\nand\r\n\r\n```\r\ndef create_trt_frozen_graph(graph_def, output_nodes, precision, \r\n\toutput_graph_path = None, workspace_size=2<<10, batch_size=1):\r\n\t''' convert frozen_graph to a TRT frozen graph'''\r\n\t\r\n\tconverter = trt.TrtGraphConverter(\r\n\t\tinput_graph_def = graph_def,\r\n\t\tnodes_blacklist = output_nodes,\r\n\t\tmax_batch_size = batch_size,\r\n\t\tmax_workspace_size_bytes = workspace_size<<20,\r\n\t\tprecision_mode = precision)\r\n\r\n\ttrt_graph_def = converter.convert()\r\n\r\n\tif not (output_graph_path is None):\r\n\t\twrite_graph_to_file(trt_graph_def, output_graph_path)\r\n\r\n\treturn trt_graph_def\r\n```\r\n\r\nIn both cases, the TF-TRT model is about 35X slower (20ms vs 700ms inference).  The results are the same regardless if I use the graph_def from memory, or load the TF-TRT saved model.\r\n\r\nHere is the respective TRT output:\r\n\r\n```\r\n2019-08-30 04:48:35.865582: I tensorflow/compiler/tf2tensorrt/segment/segment.cc:460] There are 4 ops of 3 different types in the graph that are not converted to TensorRT: Identity, NoOp, Placeholder, (For more information see https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#supported-ops).\r\n2019-08-30 04:48:35.953878: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:633] Number of TensorRT candidate segments: 1\r\n2019-08-30 04:48:35.969432: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.5\r\n2019-08-30 04:48:35.969838: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.5\r\n2019-08-30 04:55:04.225714: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:734] TensorRT node TRTEngineOp_0 added for segment 0 consisting of 944 nodes succeeded.\r\n2019-08-30 04:55:04.352053: W tensorflow/compiler/tf2tensorrt/convert/trt_optimization_pass.cc:183] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.\r\n2019-08-30 04:55:04.402986: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: tf_graph\r\n2019-08-30 04:55:04.403043: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 818 nodes (-817), 914 edges (-949), time = 62.326ms.\r\n2019-08-30 04:55:04.403049: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   layout: Graph size after: 950 nodes (132), 1046 edges (132), time = 50.486ms.\r\n2019-08-30 04:55:04.403054: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 946 nodes (-4), 1042 edges (-4), time = 45.175ms.\r\n2019-08-30 04:55:04.403059: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   TensorRTOptimizer: Graph size after: 3 nodes (-943), 2 edges (-1040), time = 388396.844ms.\r\n2019-08-30 04:55:04.403063: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 3 nodes (0), 2 edges (0), time = 2.443ms.\r\n2019-08-30 04:55:04.403067: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: TRTEngineOp_0_native_segment\r\n2019-08-30 04:55:04.403072: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 946 nodes (0), 1042 edges (0), time = 26.303ms.\r\n2019-08-30 04:55:04.403076: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   layout: Graph size after: 946 nodes (0), 1042 edges (0), time = 30.005ms.\r\n2019-08-30 04:55:04.403092: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 946 nodes (0), 1042 edges (0), time = 26.387ms.\r\n2019-08-30 04:55:04.403097: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   TensorRTOptimizer: Graph size after: 946 nodes (0), 1042 edges (0), time = 3.387ms.\r\n2019-08-30 04:55:04.403103: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 946 nodes (0), 1042 edges (0), time = 26.708ms.\r\n2019-08-30 04:55:04.727419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at \r\n```\r\n\r\nSince the TRT model only has 3 nodes, one of which is the TRT Engine node, does it make sense to convert this via UFF?  Would that get me a speed improvement?\r\n\r\nOr, is there a bug in the latest version of TF docker?\r\n\r\nThanks!\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I wonder why you are seeing `Optimization results for grappler item: TRTEngineOp_0_native_segment`. Typically only `Optimization results for grappler item: tf_graph` is reported. Looks like you are running the conversion twice?", "Are you using INT8 by any chance? If yes, you might be in the calibration phase which is much slower than inference. You would need to end the calibration. \r\n\r\nHere is the docs: https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#post-train", "Note that TF-TRT has made one TRT segment (TRTEngineOp_0) that has 944 nodes in it. This is a large subgraph which is the ideal case.", "@pooyadavoodi thanks for the replies.\r\n\r\nI'm getting similar results using either FP16 and FP32. I first wanted to test these before trying calibration.\r\n\r\nI'm going straight from the saved model (created with following code) to the convert_trt code. I did notice using saved_model_cli actually converts my saved model to TRT faster.\r\n\r\nbuilder = tf.saved_model.builder.SavedModelBuilder(str(graph_path))\r\n\r\n#define signatures\r\nprediction_signature = (\r\n\ttf.saved_model.signature_def_utils.build_signature_def(\r\n\t\tinputs=inputs,\r\n\t\toutputs=outputs,\r\n\t\tmethod_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))\r\n\r\nbuilder.add_meta_graph_and_variables(\r\n\tsess, [tf.saved_model.tag_constants.SERVING],\r\n\tsignature_def_map = \r\n\t\t{tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY : prediction_signature},\r\n\tclear_devices=False)\r\n\r\nbuilder.save(as_text=False)\r\n\r\nwriter = tf.summary.FileWriter(\"output\", sess.graph)\r\nwriter.close()\r\nI'll take a look to see how I could be running the conversion twice. From what I remember, I'm simply specifying the saved model folder, and then running create_trt_saved_model.\r\n\r\nAlso, is this the correct link to start profiling?\r\n\r\nhttps://docs.nvidia.com/deeplearning/sdk/tensorrt-best-practices/index.html#profiling", "@jtressle the profiling link is correct.", "@jtressle TF 1.14 has TF-TRT disabled accidentally. Could you try again with TF 1.15.0rc1? Also could you provide the model data so we can reproduce the slowness?", "maybe i can answer the question, when the start a sess and load the model which has been optimiszed by TF-TRT, it will load the TRT-engine op , and it will use a lot of time, maybe some seconds,but you will get a smaller latency in the rest test", "@jtressle I think what @leo-XUKANG could be possible. I'm closing this, please feel free to reopen and provide the model and script to reproduce the problem if it still exists."]}, {"number": 32102, "title": "Tested Hello World example on Arduino MKR1000 WiFi", "body": "Successfully tested  Hello World example on Arduino MKR1000 WiFi with fading LED effect.", "comments": ["@metanav Can you please resolve conflicts? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I am working on this.\r\n"]}, {"number": 32101, "title": "Unsupported Operation (MEAN) while trying to apply GpuDelegate to tflite", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code: Nop, just some assembled statements\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung S9\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tf-nightly 1.15.0.dev20190812\r\n- Python version: 3.6.5\r\n\r\n**Describe the current behavior**\r\nI'm trying to convert the Keras's MobileNet model with float16 precision for Gpu Inference. But when running tasks, I encountered the following Error:\r\n\r\n\r\n Caused by: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Next operations are not supported by GPU delegate:\r\n    MEAN: Operation is not supported.\r\n    First 88 operations will run on the GPU, and the remaining 5 on the CPU.tensorflow/lite/kernels/conv.cc:259 bias->type != input_type (10 != 1)Node number 90 (CONV_2D) failed to prepare.\r\n    tensorflow/lite/kernels/conv.cc:259 bias->type != input_type (10 != 1)Node number 3 (CONV_2D) failed to prepare.\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nthe Python script for conversion is here:\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\n\r\nmodel = keras.applications.mobilenet.MobileNet(input_shape=None, alpha=1.0, depth_multiplier=1, dropout=1e-3, include_top=True, weights='imagenet', input_tensor=None, pooling=None, classes=1000)\r\nconverter = tf.lite.TFLiteConverter.from_keras_model_file(\"mobilenet.h5\")\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_types = [tf.lite.constants.FLOAT16]\r\ntflite_model = converter.convert()\r\nopen(\"mobilenet.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\nand in Android code, I called `tfliteOptions.setAllowFp16PrecisionForFp32(true);`\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@David-Xiang \r\n\r\nYeah, unfortunately, MEAN is not implemented.  We have gotten enough requests re: MEAN, so that we decided to implement it, but we don't have an ETA yet :(  As your graph has 93 operations, and the first 88 is running on GPU, and only the last 5 on the CPU, it should be ok.\r\n\r\nThe real failure why your graph is not working is because of:\r\n> CPU.tensorflow/lite/kernels/conv.cc:259 bias->type != input_type (10 != 1)Node number 90 (CONV_2D) failed to prepare.\r\n> tensorflow/lite/kernels/conv.cc:259 bias->type != input_type (10 != 1)Node number 3 (CONV_2D) failed to prepare.\r\n\r\nand that's a failure on the TFLite CPU, and not GPU.  @gadagashwini can you re-assign?", "Hi. Is there any update on this issue? \r\n\r\nI am also facing failures on TFLite CPU when trying to infer from a F16 MobileNetV2 model using GPU delegate.\r\n\r\n```\r\njava.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Next operations are not supported by GPU delegate:\r\n    MEAN: Operation is not supported.\r\n    First 173 operations will run on the GPU, and the remaining 3 on the CPU.\r\n    tensorflow/lite/kernels/fully_connected.cc:109 filter->type != kTfLiteFloat32 (10 != 1)\r\n    Node number 174 (FULLY_CONNECTED) failed to prepare.\r\n    \r\n    tensorflow/lite/kernels/conv.cc:272 bias->type != input_type (10 != 1)\r\n    Node number 3 (CONV_2D) failed to prepare.\r\n```", "@bilalsoomro Unsupported ops should not result in failure.\r\nNot sure about Java bindings, but when using C++ bindings it successfully executes model, despite unsupported ops (which should run without troubles on CPU as log states)\r\n\r\nOn side note, I noticed that 1.15 doesn't yield this error for me, last time I checked.", "The actual failure is coming from tensor dimensions not matching, and not\nfrom unsupported ops.\n\nOn Fri, Nov 8, 2019 at 02:31 Douman <notifications@github.com> wrote:\n\n> @bilalsoomro <https://github.com/bilalsoomro> Unsupported ops should not\n> result in failure.\n> Not sure about Java bindings, but when using C++ bindings it successfully\n> executes model, despite unsupported ops (which should run without troubles\n> on CPU as log states)\n>\n> On side note, I noticed that 1.15 doesn't yield this error for me, last\n> time I checked.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/32101?email_source=notifications&email_token=ACKKUT5Q2PYDSQXRIJEKFE3QSU5WTA5CNFSM4ISIYWY2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEDP25AQ#issuecomment-551530114>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACKKUTZ52VCENDYZAWYGUCDQSU5WTANCNFSM4ISIYWYQ>\n> .\n>\n", "I meant\n\nfilter->type != kTfLiteFloat32\n\n\nOn Fri, Nov 8, 2019 at 08:45 Juhyun Lee <impjdi@google.com> wrote:\n\n> The actual failure is coming from tensor dimensions not matching, and not\n> from unsupported ops.\n>\n> On Fri, Nov 8, 2019 at 02:31 Douman <notifications@github.com> wrote:\n>\n>> @bilalsoomro <https://github.com/bilalsoomro> Unsupported ops should not\n>> result in failure.\n>> Not sure about Java bindings, but when using C++ bindings it successfully\n>> executes model, despite unsupported ops (which should run without troubles\n>> on CPU as log states)\n>>\n>> On side note, I noticed that 1.15 doesn't yield this error for me, last\n>> time I checked.\n>>\n>> \u2014\n>> You are receiving this because you were assigned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/32101?email_source=notifications&email_token=ACKKUT5Q2PYDSQXRIJEKFE3QSU5WTA5CNFSM4ISIYWY2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEDP25AQ#issuecomment-551530114>,\n>> or unsubscribe\n>> <https://github.com/notifications/unsubscribe-auth/ACKKUTZ52VCENDYZAWYGUCDQSU5WTANCNFSM4ISIYWYQ>\n>> .\n>>\n>\n", "@impjdi The error occurs when I initialize the interpreter with a GPU delegate and my converted F16 MobileNet V2 model. It crashes before I run the model so where is it that the dimensions are not matching?\r\n\r\n```\r\nval tfliteOptions: Interpreter.Options = Interpreter.Options()\r\nval gpuDelegate = GpuDelegate()\r\ntfliteOptions.addDelegate(gpuDelegate)\r\nval tfliteModel = Utils.loadModelFile(activity, getModelPath())\r\nval tfliteInterpreter = Interpreter(tfliteModel, tfliteOptions)   <-------- Crashes the android app\r\n```", "> @bilalsoomro Unsupported ops should not result in failure.\r\n> Not sure about Java bindings, but when using C++ bindings it successfully executes model, despite unsupported ops (which should run without troubles on CPU as log states)\r\n> \r\n> On side note, I noticed that 1.15 doesn't yield this error for me, last time I checked.\r\n\r\n@DoumanAsh I am currently importing the nightly tensorflow lite build into the app as shown below. Do you set the version for these to 1.15?\r\n```\r\nimplementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\nimplementation 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly'\r\n```", "@bilalsoomro I build from sources, but use `r1.15` branch\r\n\r\nBut unsupported MEAN is not going to cause this failure.\r\nAs impjdi mentioned it seems your model is not entirely correct.\r\nI guess it is something to do with float size", "@DoumanAsh Thanks for clarifying that. I'll try building r1.15 from source\r\n\r\nI am converting the model using the same method that is described in the original post so I guess I'll have to wait for the answer/solution to the original post. I am using tensorflow version 2.0.0.\r\n\r\n```\r\nmodel = tf.keras.applications.MobileNetV2(weights=\"imagenet\", input_shape=(224, 224, 3))\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_types = [tf.float16]\r\ntflite_model = converter.convert()\r\n```", "I would advice to use the same tensorflow version for both conversion and execution.\r\nSo maybe it would be better to use 2.0.0 which is now released as stable version?\r\nNot 1.15\r\n\r\nAs a side note I suspect the problem is with `converter.target_spec.supported_types = [tf.float16]` so someone who works on GPU delegate might say more about it", "I am using `tf.float16` instead of `tf.lite.constants.FLOAT16` because I read here (https://www.tensorflow.org/lite/convert/python_api#liteconstants) that `lite.constants` API has been deprecated in tensorflow 2.0 and we should switch to `tf.DTYPE`.", "I have managed to successfully convert the MobileNet V2 model to float16. I had to use the frozen graph version of the model and the V1 of the converter. I've shared the code below for anyone who is facing similar errors with models converted from keras models.\r\n```\r\ninput_arrays = [\"input\"]\r\noutput_arrays = [\"MobilenetV2/Predictions/Reshape_1\"]\r\ninput_shapes = {'input': [1, 224, 224, 3]}\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph('mobilenet_v2_1.0_224_frozen.pb', input_arrays, output_arrays, input_shapes)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_types = [tf.float16]\r\ntflite_model = converter.convert()\r\nopen('mobilenet_v2_f16.tflite', \"wb\").write(tflite_model)\r\n```", "I had the same problem\uff1a\r\n```\r\n2020-01-10 12:15:08.349 13545-13545/? I/tflite: Created TensorFlow Lite delegate for GPU.\r\n2020-01-10 12:15:08.352 13545-13545/? I/tflite: Initialized TensorFlow Lite runtime.\r\n2020-01-10 12:15:08.353 13545-13545/? W/System.err: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Next operations are not supported by GPU delegate:\r\n2020-01-10 12:15:08.353 13545-13545/? W/System.err: MEAN: Operation is not supported.\r\n2020-01-10 12:15:08.353 13545-13545/? W/System.err: MIRROR_PAD: Operation is not supported.\r\n2020-01-10 12:15:08.353 13545-13545/? W/System.err: First 20 operations will run on the GPU, and the remaining 66 on the CPU.\r\n2020-01-10 12:15:08.353 13545-13545/? W/System.err: tensorflow/lite/kernels/conv.cc:319 bias->type != input_type (10 != 1)\r\n2020-01-10 12:15:08.353 13545-13545/? W/System.err: Node number 4 (CONV_2D) failed to prepare.\r\n2020-01-10 12:15:08.353 13545-13545/? W/System.err: tensorflow/lite/kernels/conv.cc:319 bias->type != input_type (10 != 1)\r\n2020-01-10 12:15:08.353 13545-13545/? W/System.err: Node number 4 (CONV_2D) failed to prepare.\r\n```", "I found when I removed `tf.pad`,`tf.contrib.layers.instance_norm`,`tf.image.ResizeMethod.NEAREST_NEIGHBOR`, my model can work normally in GPU mode", "> I meant filter->type != kTfLiteFloat32\r\n\r\n@impjdi : Can you explain this, I thought it was due to  `bias->type != input_type (10 != 1)` i.e something related to `10 !=1`, or is it just because of _type mismatch_, also what is **kTfLiteFloat32**?", "@sirius0503 Did you figure this out?  ", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32101\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32101\">No</a>\n"]}, {"number": 32100, "title": "TF2.0-rc0 ValueError on tensor comparison with None using ==", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.0-rc0\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nPreviously when running the code from the CVAE tutorial https://www.tensorflow.org/beta/tutorials/generative/cvae, I used in the sample function\r\n`if eps == None:`\r\nWhich worked fine, however with rc0 it now throws an error from comparing a tensor to None and only works if the following is used\r\n`if eps is None`\r\n\r\n**Describe the expected behavior**\r\nComparing if something == None and is None should have the same behavior\r\n\r\n**Code to reproduce the issue**\r\nSee https://www.tensorflow.org/beta/tutorials/generative/cvae and change the sample function as described.\r\n", "comments": ["@Ryandry1st, I tried both the case in colab with Tensorflow 2.0.0.rc0. It works, Please take a look at colab gist [here](https://colab.sandbox.google.com/gist/gadagashwini/029f514c526c246f41f0c177cb2cb906/untitled116.ipynb). Let us know this is the expected behavior. Thanks!", "Did you make sure to update to TF 2.0-rc0 and replace the line specified? I am still able to get the error on a colab on my end.", "In your colab you did not use the tensorflow comparison because you were not using any tensors. Converting your code to make eps a tensor, especially a tensor with some values, will throw the error", "here is a minimum repro [colab](https://colab.sandbox.google.com/drive/187REdqdMP1N94_sToSnvwABdDdY2LlhH#scrollTo=NyBS6UeSe0C3) for the issue reported. \r\n", "> here is a minimum repro [colab](https://colab.sandbox.google.com/drive/187REdqdMP1N94_sToSnvwABdDdY2LlhH#scrollTo=NyBS6UeSe0C3) for the issue reported.\r\n\r\nYou will need to change the sharing settings as each person will have to request access to see that btw.", "here is the [colab gist](https://colab.research.google.com/drive/187REdqdMP1N94_sToSnvwABdDdY2LlhH)", "Please see the following section in PEP8: https://www.python.org/dev/peps/pep-0008/#programming-recommendations\r\n\r\nIn Python comparing with `None` using `==` is considered an anti-pattern. `==` implies the values are equal vs `is` checks if the two objects are the same object. Often you can get away with using `==` with `None` since `None is None` as they are the same object. However, this is not recommended and or supported for TensorFlow.\r\n\r\nNote, the equality operator has been changed in 2.0 to do element-wise comparison. So `==` triggers a `tf.math.equal` function which is what causes the code to try (and fail) to convert `None` to a Tensor.", "I agree with the premise, as it is the recommended/agreed-upon method. The main issue is that this seems to raise some compatibility issues, considering the operation has been changed in 2.0 and is an overloaded operator so it does not have documentation to reference. A suggestion might be to include a simple check with a warning/error handling. That would make more sense than quietly changing the functionality.", "@Ryandry1st: Thanks for the feedback. The compatibility issue is known, but from what has been seen, it has actually uncovered more bugs in the application code where developers expected a different behavior than what they wrote.\r\n\r\nI agree the change has had limited documentation. We'll see what we can do to improve that for the release."]}]