[{"number": 13289, "title": "Bug: SDCAModel missbehave/bug when mixing sparse and dense features", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUp-to-date Arch Linux\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\n1.3.0\r\n- **Python version**: \r\n3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\nAny toy problem using [https://www.tensorflow.org/api_docs/python/tf/contrib/linear_optimizer/SdcaModel](SDCAModel) that mixes sparse and dense features.\r\n\r\n### Describe the problem\r\nIt's a bug. \r\nWhen using both sparse and dense features in [SDCAModel](https://www.tensorflow.org/api_docs/python/tf/contrib/linear_optimizer/SdcaModel) the _linear_predictions method output a tensor of wrong shape. This happens because the sparse results and dense results ranks are inconsistent in the current version and summing them trigger an unexpected broadcast. \r\nhttps://github.com/tensorflow/tensorflow/pull/13279 fixes the bug.\r\n", "comments": ["It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Fixed by https://github.com/tensorflow/tensorflow/pull/13279"]}, {"number": 13288, "title": "Cannot put a Tensor into a StagingArea", "body": "TensorFlow version: 1.3.0\r\n\r\nAccording to the document, I should be able to put a tensor to a StagingArea<sup>[[1]](https://www.tensorflow.org/api_docs/python/tf/contrib/staging/StagingArea#put)</sup>. But the following code does not work:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import staging\r\n\r\nstaging.StagingArea(dtypes=[tf.int32]).put(tf.constant(1))\r\n```\r\n\r\nThe output says:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/\u2588\u2588\u2588\u2588\u2588\u2588\u2588/dataset.py\", line 4, in <module>\r\n    staging.StagingArea(dtypes=[tf.int32]).put(tf.constant(1))\r\n  File \"/\u2588\u2588\u2588\u2588\u2588\u2588\u2588/site-packages/tensorflow/python/ops/data_flow_ops.py\", line 1671, in put\r\n    vals, _ = self._check_put_dtypes(values, indices)\r\n  File \"/\u2588\u2588\u2588\u2588\u2588\u2588\u2588/site-packages/tensorflow/python/ops/data_flow_ops.py\", line 1480, in _check_put_dtypes\r\n    raise ValueError(\"Indices must be supplied when inserting a list \"\r\nValueError: Indices must be supplied when inserting a list of tensors\r\n```", "comments": ["This question seems suitable for StackOverFlow. ", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@asimshankar Maybe my description is not very clear, but I was not asking a question. I am fairly sure this is a bug, because the code\u2019s behavior is different than the documented behavior. I think either the implementation is wrong, or the documentation is wrong.\r\n\r\nConsider the following two pieces of codes:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import staging\r\n\r\nstaging.StagingArea(dtypes=[tf.int32]).put(tf.constant(1))\r\n```\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import staging\r\n\r\nstaging.StagingArea(dtypes=[tf.int32]).put((tf.constant(1),))\r\n```\r\n\r\nThe first one won\u2019t work as I described, the second one works. If the documentation is correct, both ones should work, so I think this is a bug that need to be fixed.", "I see. Indeed it seems the documentation is incorrect.\r\n\r\nPossible fixes include:\r\n\r\n- Updating the documentation in [this line](https://github.com/tensorflow/tensorflow/blob/3dbe216/tensorflow/python/ops/data_flow_ops.py#L1676) to something like `A tuple or list of Tensors. The number of elements must match the length of the list provided to the dtypes argument when creating the StagingArea`.\r\n- Updating the code to allow for a single Tensor by changing [this line](https://github.com/tensorflow/tensorflow/blob/3dbe216/tensorflow/python/ops/data_flow_ops.py#L1690) so that `indices` ends up as `[0]` instead of `None` when the argument is an instance of `Tensor`\r\n\r\nPRs to fix these are welcome!\r\n\r\nFYI @ekelsen ", "Thank you for outlining how to address this, @asimshankar, and for making it accessible for my first contribution. I've created a PR here: #13502 and tested it locally."]}, {"number": 13287, "title": "Make a tools CLI for easy access to freeze_graph, saved_model_cli, etc.", "body": "Related to #6134. Right now, the users of the `tensorflow/python/tools` scripts have these options:\r\n\r\n* Download and save a copy of the individual `.py` file with whatever model/project they are working on, so that it is bundled with their code. This causes tons of code duplication across projects.\r\n* Import the tools programmatically and use them there. Being able to do this is extremely handy, but often the most convenient way to use the tool (and the way they are generally designed to be used) are as ad-hoc command-line calls.\r\n* Build a binary using Bazel. This is extremely inconvenient, and doesn't really solve the problem of code-reuse from above. I'd also argue that building a binary for what amounts to a Python script is overkill.\r\n* Create a custom CLI/module/alias that bundles these tools together (or allows for a single tool to be called from anywhere). This is convenient, but complicates setting up a development environment, and leads to divergent patterns across the user base.\r\n\r\nI think having some sort of unified tool that is able to provide uniform access to the CLI of frequently used tools, such as `freeze_graph.py` and `saved_model_cli.py` would greatly improve UX, as well as make it much easier to explain how to use these tools. Additionally, it would help prevent version mismatches of tooling, as the correct version of the tool would already be at their fingertips (instead of fumbling through GitHub branches).\r\n\r\nI think having some sort of simple umbrella module, let's call it `tftools`, which provides access to the underlying python files would suffice. E.g.\r\n\r\n```\r\n$ tftools freeze_graph --input_graph=... --output_graph=...\r\n\r\n$ tftools saved_model_cli show --dir /tmp/saved_model_dir\r\n```\r\n\r\nThis would be a little bit of legwork, but I think the benefit-out to effort-in ratio is pretty large, at least from a user perspective. Not sure if the TensorFlow team would want to handle this or leave it to the community (if I can get company approval, I'd be happy to take a crack at it).", "comments": ["There's the [Graph Transform Tool](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms).", "@carlthome that's kind of the point: if you read the README for the GTT, you'll see this:\r\n\r\n>If you want to work with the values of your trained parameters, for example to quantize weights, you'll need to run tensorflow/python/tools/freeze_graph.py to convert the checkpoint values into embedded constants within the graph file itself.\r\n\r\nIt's basically saying that in order to make full use of the tool, you need to go through one of the above-listed options to make use of `freeze_graph.py`.\r\n\r\nUnless you were saying that the Graph Transform Tool would also be an excellent addition to a `tftools` CLI, in which case I agree wholeheartedly :D", "saved_model_cli is already available directly from command line if you pip install tensorflow. We can probably set up the same thing for freeze_graph. Is that something you are looking for @samjabrahams ?", "@yifeif that would be awesome! I didn't realize `saved_model_cli` was already installed directly. Maybe a `tftools ...` interface is a bit overkill, but at a certain point having a unified way to call these utilities might be worth looking into. I think `freeze_graph` is used enough to be worth the extra effort.", "cc @petewarden for `freeze_graph`. If there is no objection from Pete, we can set it up.", "I like this idea a lot, for both freeze_graph and the graph transform tool!", "Having the graph transform tool be included as part of the binary would be an even bigger quality of life improvement than just adding `freeze_graph` (if a bit more work on your guys' end).", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Looks like the graph transform tool binary is quite large and has significantly increased our pip package size(+ 80MB). Unfortunately we will need to revert the change for graph transform tool. `freeze_graph` will still be available through command line.", "How come it is so big? "]}, {"number": 13286, "title": "[bug/docs] tf.Estimator: 'train' method is missing", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Used doc\r\n\r\n- **OS Platform and Distribution, versions etc**:\r\n<details>\r\n== cat /etc/issue ===============================================\r\nDarwin PEDS-0MAHH2C-LT 16.7.0 Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64 x86_64\r\nMac OS X 10.12.6\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 8.1.0 (clang-802.0.42)\r\nTarget: x86_64-apple-darwin16.7.0\r\nThread model: posix\r\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n\r\n== uname -a =====================================================\r\nDarwin PEDS-0MAHH2C-LT 16.7.0 Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64 x86_64\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.1)\r\nprotobuf (3.3.0)\r\ntensorflow (1.2.1)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.2.1\r\ntf.GIT_VERSION = v1.2.0-5-g435cdfc\r\ntf.COMPILER_VERSION = v1.2.0-5-g435cdfc\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\ntf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\nNo GPU\r\n</details>\r\n\r\n- **Exact command to reproduce**:\r\nI am modifying this [script that works](http://localhost:8888/notebooks/keras_tfest/keras-estimator/Integration%20of%20Keras%20with%20Tensorflow.ipynb) + documentation as referenced below. The resulting code is following:\r\n\r\n<details>\r\n\r\n    # coding: utf-8\r\n    import sys\r\n    sys.path.append(\"/data/dlituiev/target2/\")\r\n    from inception import get_model\r\n    from keras.models import Sequential, Model\r\n    from keras.layers import Dense, Dropout, Activation, Flatten, InputLayer, Reshape, Input\r\n    import tensorflow as tf\r\n    from tensorflow.python.framework.ops import _get_graph_from_inputs\r\n    from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib\r\n    import numpy as np\r\n    from keras import backend as K\r\n    K.set_image_dim_ordering('tf')\r\n\r\n    BATCH_SIZE = 4\r\n    n_classes=10\r\n    final_activation='linear'\r\n    lr=1e-4\r\n    opt_name = \"Adadelta\"\r\n    epochs = 20\r\n    HEIGHT, WIDTH = 512, 512\r\n\r\n    def get_model():\r\n        model = Sequential()\r\n        model.add(InputLayer(input_shape=[512 , 512, 1], batch_size=BATCH_SIZE))\r\n        #     model.add(Reshape([512*512,]))\r\n        model.add(Flatten())\r\n        model.add(Dense(n_classes, activation='relu', ))\r\n        return model\r\n\r\n\r\n    g = tf.Graph()\r\n    with g.as_default():\r\n        model = get_model()\r\n        model.build()\r\n        funcmodel = model.model\r\n\r\n        def model_fn(features, labels, params):\r\n            optimizer = params[\"optimizer\"]\r\n            opt_params= params.get(\"opt_params\", {})\r\n        #     x = tf.placeholder(tf.float32, shape=model.layers[0].input.shape)\r\n            print(\"features\", features.shape)\r\n            logyhat = funcmodel(features)\r\n\r\n            if (mode == tf.estimator.ModeKeys.TRAIN or\r\n                mode == tf.estimator.ModeKeys.EVAL):\r\n        #         loss = tf.contrib.keras.backend.categorical_crossentropy()\r\n                loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logyhat)\r\n            else:\r\n                loss = None\r\n            if mode == tf.estimator.ModeKeys.TRAIN:\r\n                optimizer = getattr(tf.train, optimizer)\r\n                train_op = optimizer(opt_params).minimize(loss)\r\n            else:\r\n                train_op = None\r\n            if mode == tf.estimator.ModeKeys.PREDICT:\r\n                predictions = tf.nn.softmax(logyhat)\r\n            else:\r\n                predictions = None\r\n\r\n            \"\"\"\r\n            return tf.estimator.EstimatorSpec(\r\n                  mode=mode,\r\n                  predictions=predictions,\r\n                  loss=loss,\r\n                  train_op=train_op)\r\n                  \"\"\"\r\n\r\n            return model_fn_lib.ModelFnOps(\r\n                  mode=mode,\r\n                  predictions=predictions,\r\n                  loss=loss,\r\n                  train_op=train_op,\r\n                  #eval_metric_ops=eval_metric_ops\r\n                  )\r\n        ##############################################\r\n\r\n        def parser(record):\r\n            keys_to_features = {\r\n                'height': tf.FixedLenFeature([], tf.int64),\r\n                'width': tf.FixedLenFeature([], tf.int64),\r\n                'image_raw': tf.FixedLenFeature([], tf.string),\r\n                'label': tf.FixedLenFeature([], tf.int64)\r\n                }\r\n            features = tf.parse_single_example(\r\n              record,\r\n              features=keys_to_features)\r\n\r\n            # Convert from a scalar string tensor (whose single string has\r\n            # length mnist.IMAGE_PIXELS) to a uint8 tensor with shape\r\n            # [mnist.IMAGE_PIXELS].\r\n            image = tf.decode_raw(features['image_raw'], tf.float32)\r\n            #annotation = tf.decode_raw(features['mask_raw'], tf.uint8)\r\n\r\n            height = tf.cast(features['height'], tf.int32)\r\n            width = tf.cast(features['width'], tf.int32)\r\n\r\n    #         image_shape = tf.stack([height, width, 1])\r\n            image_shape = tf.stack([HEIGHT, WIDTH,1])\r\n            #annotation_shape = tf.pack([height, width, 1])\r\n\r\n            image = tf.reshape(image, image_shape)\r\n            label = tf.cast(features[\"label\"], tf.int32)\r\n            return image, label\r\n\r\n        def get_dataset_inp_fn(filenames, epochs=20, batch_size=2,\r\n                               buffer_size=100):\r\n            def dataset_input_fn():\r\n                dataset = tf.contrib.data.TFRecordDataset(filenames)\r\n\r\n                # Use `tf.parse_single_example()` to extract data from a `tf.Example`\r\n                # protocol buffer, and perform any additional per-record preprocessing.\r\n\r\n                # Use `Dataset.map()` to build a pair of a feature dictionary and a label\r\n                # tensor for each example.\r\n                dataset = dataset.map(parser)\r\n                dataset = dataset.shuffle(buffer_size=buffer_size)\r\n                dataset = dataset.batch(batch_size)\r\n                dataset = dataset.repeat(epochs)\r\n                iterator = dataset.make_one_shot_iterator()\r\n\r\n                # `features` is a dictionary in which each value is a batch of values for\r\n                # that feature; `labels` is a batch of labels.\r\n                features, labels = iterator.get_next()\r\n                return features, labels\r\n            return dataset_input_fn\r\n\r\n    ##############################################\r\n    def get_optimizer(opt_name = \"Adadelta\", **kwargs):\r\n        #lr=kwargs.pop(\"lr\", 1e-4)\r\n        opt_name = opt_name if opt_name.endswith(\"Optimizer\") else opt_name.title()+\"Optimizer\"\r\n        optimizer = getattr(tf.train, opt_name)\r\n        return optimizer#(lr, **kwargs)\r\n    ##############################################\r\n\r\n    from tensorflow.contrib.learn.python.learn.estimators import estimator\r\n    LEARNING_RATE = 0.001\r\n    # Set model params\r\n    model_params = {\"opt_params\":{\"learning_rate\": LEARNING_RATE}, 'optimizer' : get_optimizer()}\r\n\r\n    inpfun = get_dataset_inp_fn([\"example.tfrecords\"],\r\n                                    epochs=epochs,\r\n                                    batch_size=BATCH_SIZE)\r\n    # Instantiate Estimator\r\n    est = estimator.Estimator(model_fn=model_fn, params=model_params)\r\n\r\n    print(help(est))\r\n\r\n    ### Option 1 ###\r\n    est.train(input_fn=inpfun, steps=5000)\r\n\r\n    ### Option 2 ###\r\n    est.fit(input_fn=inpfun, steps=5000)\r\n\r\n\r\n</details>\r\n\r\n### Describe the problem\r\nIn this manual it implies that `tf.Estimator` has a method `train()`:\r\nhttps://www.tensorflow.org/extend/estimators\r\n\r\nWhen I instantiate an estimator and run `est.train(input_fn=inpfun, steps=5000)` I am getting:\r\n\r\n     AttributeError: 'Estimator' object has no attribute 'train'\r\n", "comments": ["Maybe try upgrading to 1.3? This is what I get\r\n```\r\n>>> foo =  tf.estimator.Estimator(model_fn=lambda features: x)\r\nINFO:tensorflow:Using default config.\r\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmptdlpr60z\r\nINFO:tensorflow:Using config: {'_session_config': None, '_keep_checkpoint_max': 5, '_log_step_count_steps': 100, '_save_checkpoints_secs': 600, '_save_summary_steps': 100, '_model_dir': '/tmp/tmptdlpr60z', '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_steps': None, '_tf_random_seed': 1}\r\n>>> foo.train\r\n<bound method Estimator.train of <tensorflow.python.estimator.estimator.Estimator object at 0x7fecf54fbbe0>>\r\n```\r\nDoes that work for you? Your reproduction script is linked off of your localhost:8888 so I can't see it. Good luck. Thanks!\r\n", "I found that I am calling `from tensorflow.contrib.learn.python.learn.estimators import estimator` and `estimator.Estimator`. This one does not have `train()` method", "I see, does calling the tf.estimator one solve your problem?", "solves. Close?\r\n\r\nOn Tue, Sep 26, 2017 at 1:13 PM, Andrew Selle <notifications@github.com>\r\nwrote:\r\n\r\n> I see, does calling the tf.estimator one solve your problem?\r\n>\r\n> \u2014\r\n> You are receiving this because you authored the thread.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/13286#issuecomment-332320477>,\r\n> or mute the thread\r\n> <https://github.com/notifications/unsubscribe-auth/AICTMkD2tqR-kSTcdmcRphk2E1I3Vmiwks5smVrzgaJpZM4PiPs->\r\n> .\r\n>\r\n", "They are two different modules. `train` in [`tf.estimator.Estimator`](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator) is equivalent to `fit` in [` tf.contrib.learn.Estimator `](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Estimator)."]}, {"number": 13285, "title": "After installing tenserflow the following error is generated  Python 3.6.2 (v3.6.2:5fd33b5, Jul  8 2017, 04:57:36) [MSC v.1900 64 bit (AMD64)] on win32 Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import tenserflow as tf Traceback (most recent call last):   File \"<stdin>\", line 1, in <module> ModuleNotFoundError: No module named 'tenserflow'", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **No Custom code written. Only codes that have been used are from Tensorflow official documentation  (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **Windows 10 Enterprise x64 (e.g., Linux Ubuntu 16.04)**:\r\n- **Tensorflow installed from cmd using pip3 install command (source or binary)**:\r\n- **TensorFlow with CPU support only :\r\n- **Python version 3.6.2**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU Intel HD Graphics 5500 RAM 8GB, Total Graphics Memory 4161MB**:\r\n- pip3 install --upgrade tensorflow\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nAfter I install Tensorflow using the command pip3 install --upgrade tensorflow the installation completes without any issues. When I put in the line \"import tensorflow as tf\", in the pyton interactive shell I get the following error File \"<stdin>\", line 1, in <module> ModuleNotFoundError: No module named 'tensorflow'\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Probably because it's `import tensorflow` and not `import tenserflow`?", "Also: http://mathworld.wolfram.com/Tensor.html", "Corrected the spelling mistake. Still same issue\r\nNote: I'm using the same lines as directed per tensorflow documentation", "Your error message suggests that perhaps you did not install or invoke tensorflow correctly.  This forum is for bug reports, not for usage help.   Stackoverflow is a much better venue for those questions.\r\n\r\nPlease make sure you are following the most recent directions to install the correct version for your system.\r\n\r\nIf you believe you are experiencing an actual bug, please fill in the submission template completely and accurately, striving to clearly communicate a minimal reproducible example of the failure. \r\n\r\nWe ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "Thanks. I have updated the submission template with all the details.", "Well, you'll want to make sure that `pip3` is installed to the `python` interpreter you're then trying to `import tensorflow` in. In other words, try starting `python3` instead.\r\n\r\nAs you're on Windows, you might have fewer headaches by relying on prebuilt packages via conda instead. https://www.anaconda.com/downloads", "I was able to import Tensorflow after a couple of uninstalls and installs. Now its working. I'm not sure what was the issue because I was following the same steps mentioned in tensorflow documentation. This issue can be closed now but I'm still eager to know what was the issue due to which tensorflow was not getting properly installed/import was not successfull\r\n\r\n"]}, {"number": 13284, "title": "MonitoredTrainingSession supports partial restore from checkpoint", "body": "As mentioned in #6604 , `MonitoredTrainingSession` is preferred to `Supervisor`, I think it is useful to support partial restoration from checkpoint files in `MonitoredTrainingSession`.\r\n\r\nThe main idea is bypassing checkpoint restoration of `SessionManager`, and adding a new `CheckpointRestorerHook` to restore data from checkpoint, after `tf.Session` is created and `init_op` is run.\r\n\r\nA new ctor parameter `restore_var_list=None` added to explicitly specify which part of variables to be restored. If not set, all variables are restored by default behavior of `Saver`.\r\n\r\nThis way the user can train a model, save checkpoint file, change the model a bit and fine-tune the new model by specifying `restore_var_list` when he/she continues training.\r\n", "comments": ["Can one of the admins verify this patch?", "@immars, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ispirmustafa, @tensorflower-gardener and @adarob to be potential reviewers.", "Thanks for this PR, can you add unit-tests for the new functionality. ", "@ispirmustafa @sb2nov ok, I'll push when ready. Thanks.", "@ispirmustafa so what I need to do is to move `CheckpointRestorerHook` into contrib, revert my changes to `MonitoredTrainingSession`, and provide another version of `MonitoredTrainingSession` in contrib, is that correct?\r\n\r\nBecause if I move `CheckpointRestorerHook` into contrib, I guess what's in `tensorflow.training` should not depend on anything in `contrib`, right?\r\n", "@immars \r\nyes, please add `CheckpointRestorerHook` to tf.contrib.training.\r\nI think you don't need to modify `MonitoredTrainingSession` or provide similar in contrib. Users can create your hook and send it to the  `MonitoredTrainingSession`, right?", "@ispirmustafa user also need to bypass checkpoint restoration of `SessionManager`, while he/she cannot set parameter `checkpoint_dir` to None because it's also for `CheckpointSaverHook` to save newer checkpoints. Otherwise, `SessionManager` would still complain about missing variables because it happens right after `tf.Session` is created, before hooks can get access to the `sess`.\r\n\r\nIt seems to me that it's not possible to achieve this without modifying `MonitoredTrainingSession`, because currently the same `Saver` instance is used to restore and save checkpoints.\r\n\r\nplease see https://github.com/tensorflow/tensorflow/pull/13284/files#diff-078d8aa8dc9179be5e99435311ade88fR352 , about `restore_checkpoint_dir`. \r\n\r\n\r\n", "Hi @immars\r\nUser can use `MonitoredSession` directly instead of `MonitoredTrainingSession`. Please document in your hook, how should they use it and give an example of usage with `MonitoredSession`.\r\nHaving another `MonitoredTrainingSession` inside contrib will confuse users. ", "@ispirmustafa ok, I'll move changes into contrib, as well as a separate helper method as example.\r\n\r\nI think A good example of usage with `MonitoredSession` would be similar to `MonitoredTrainingSession`, because the interface of `MonitoredTrainingSession` is more user friendly than `MonitoredSession`. I would probably wrap `MonitoredSession` and hooks up in something like, if not use directly, `MonitoredTrainingSession` in my code.\r\n\r\nplease see `PartialRestoreSession` in `partial_restore_session.py`.\r\n\r\n", "Jenkins, test this please.", "Jenkins, test this please.", "@ispirmustafa is there anything else I should do?\r\n", "@immars can you remove the new session class as Mustafa requested? \r\n\r\nAll MonitoredTrainingSession does is add a few hooks for MonitoredSession. No need to make every combination of hooks a new class.", "Closing as stalled. Please open a new PR if there is progress."]}, {"number": 13283, "title": "'Cannot interpret feed_dict key as Tensor: Can not convert a int into a Tensor' when using probabilities.eval?", "body": "I want to use tensor-flow to print all the probabilities for my own image. The following code works well for any mnist image that i get from online but does not work with my own images. Here is the whole code.\r\n\r\n    from PIL import Image\r\n    import random\r\n    from tensorflow.examples.tutorials.mnist import input_data\r\n    import tensorflow as tf\r\n    import numpy as np\r\n    import scipy.ndimage\r\n    from PIL import Image\r\n    \r\n    \r\n    np.set_printoptions(threshold='nan')\r\n    \r\n    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\r\n    \r\n    \r\n    x = tf.placeholder(tf.float32, [None, 784])\r\n    W = tf.Variable(tf.zeros([784, 10]))\r\n    b = tf.Variable(tf.zeros([10]))\r\n    \r\n    y = tf.nn.softmax(tf.matmul(x, W) + b)\r\n    y_ = tf.placeholder(tf.float32, [None, 10])\r\n    \r\n    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\r\n    \r\n    train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\r\n    \r\n    init = tf.initialize_all_variables()\r\n    \r\n    sess = tf.Session()\r\n    sess.run(init)\r\n    \r\n    for i in range(1000):\r\n        batch_xs, batch_ys = mnist.train.next_batch(1000)\r\n        sess.run(train_step, feed_dict= {x: batch_xs, y_: batch_ys})\r\n    \r\n    print (\"done with training\")\r\n    print (\"\\n\\n\\n\")\r\n    \r\n    #Weights = (W.eval())\r\n    ##print Weights\r\n    #\r\n    #Bias = (b.eval())\r\n    ##print Bias\r\n    \r\n    probabilities  = y\r\n     \r\n    #digit = 80\r\n    #batch_xs = np.reshape(mnist.test.images[digit], (1, 784))\r\n    ##print isinstance(mnist.test.images[digit], tuple)\r\n    #print batch_xs\r\n    \r\n    img = Image.open('mnist_7_367.jpg').convert('L')  # convert image to 8-bit grayscale\r\n    data = list(img.getdata()) # convert image data to a list of integers\r\n    newList = [float(x) / 255.0 for x in data]\r\n    newList1 = np.asarray(newList)\r\n    batch_xs = np.reshape(newList1, (1, 784))\r\n    batch_xs[batch_xs < 0.1] = 0\r\n    #print isinstance(newList1, list)\r\n    print batch_xs\r\n    \r\n    \r\n    counter = 0\r\n    for x in probabilities.eval(feed_dict={x: batch_xs}, session = sess):\r\n        print ('[')\r\n        for y in x:\r\n            print (counter, '{:f}'.format(float(y)))\r\n            counter += 1\r\n        print (']')\r\n    \r\n    hello = np.reshape(batch_xs, (28, 28))  \r\n    \r\n    \r\n    Matrix = np.array([])\r\n    \r\n    for x in hello:\r\n        for y in x:\r\n            Matrix = np.append(Matrix, int(255 * y))    \r\n    \r\n    \r\n    imageReal = np.reshape(Matrix, (28, 28))\r\n    \r\n    img = Image.fromarray(imageReal)\r\n    img.show()\r\n    \r\n    sess.close()\r\n\r\nThe problem I am having is that\r\n\r\n    for x in probabilities.eval(feed_dict={x: batch_xs}, session = sess): \r\n\r\nis producing the following error:\r\n\r\n    Cannot interpret feed_dict key as Tensor: Can not convert a int into a Tensor.", "comments": ["I suspect the problem stems from this line:\r\n\r\n```python\r\nnewList = [float(x) / 255.0 for x in data]\r\n```\r\n\r\n...which has the side effect of assigning each element of `data` to the variable called `x` (and overwriting the `tf.placeholder()` that you assigned to it earlier on).\r\n\r\n(In future, questions like this are more suited to Stack Overflow. GitHub issues are for bugs or feature requests only.)"]}, {"number": 13282, "title": "Relex restrictions on `tf.foldr` and `tf.foldl`", "body": "This fix tries to address the issue raised in #12019 where `tf.foldr` and `tf.foldl` only accpet first shape dimensions be identical for all list elements.\r\n\r\nThe issue comes from the implementation of `tf.foldr` and `tf.foldl` where the input elems is converted to tensor, then converted to tensor array:\r\nhttps://github.com/tensorflow/tensorflow/blob/1e1b3d90295f9396a25b9cfd4c571f7949716182/tensorflow/python/ops/functional_ops.py#L100-L106\r\n\r\nIt is possible to bypass the conversion to tensor, and go directly to tensor array, as long as the input elems is Iterable (and pass `infer_shape=False`)\r\n\r\nThis fix is an proposal to relex the restriction on `tf.foldr` and `tf.foldl` so that a list of elements with different shapes could be passed.\r\n\r\nThis fix fixes #12019.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@yongtang, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ebrevdo, @lukaszkaiser and @yuanbyu to be potential reviewers.", "Not sure about the input value of `tf.foldr` and `tf.foldl` so this PR is more about a \"proposal\". ", "@ebrevdo: Can you take a look at the `TensorArray` manipulation here and see if it looks alright?\r\n\r\nI'm a little dubious about these functions, because they don't appear to accept `nest`-ed components, and that seems like an equally likely feature request that might conflict with this one in annoying ways....", "It would indeed be better to extent these functions to support nested inputs and outputs in the same way that map_fn does.", "I found that most of ops use `ops.convert_to_tensor` to convert its inputs, however the method is really strict (same dimension, same dtype etc), and brings some inconvenience, for example #12486. Is it possible to find a more general and solid solution? say, create some more loose helper method?", "@mrry @ebrevdo Thanks. I will update the PR with nested input.", "@facaiy I guess some of the restrictions might be related to proto definition. For example, the issue #12486 you referred is the op `ShapeN` which requires the same type `T` for input:\r\nhttps://github.com/tensorflow/tensorflow/blob/de46da6f748d2ea8472154bb067a361e8238568d/tensorflow/core/ops/array_ops.cc#L2096-L2102\r\n\r\nIt might be possible to pass the proto such that same type is not required. Though that would require proto update I guess.", "@mrry @ebrevdo I looked into the way `map_fn` handles nested structures. However, it seems that `map_fn`-like method (and `scan`) expects the same (nested) structure for all elems along dimension 0. But this is not the case raised in #12019.\r\n\r\nIn #12019, the input was\r\n```\r\ndef concat2(A, B):\r\n    return tf.concat([A, B], axis=0)\r\n\r\nA = tf.constant([[10,10]])             # A.shape => (1,2)\r\nB = tf.constant([[20, 20], [30, 30]])  # B.shape => (2,2)\r\n\r\nprint(concat2(A, B).eval())              # => [[10, 10], [20, 20], [30, 30]]\r\nprint(tf.foldr(concat2, [A, B]).eval())  # => ERROR!\r\n```\r\n\r\nMy understanding is that the above scenario will not be compatible with nested structure as `map_fn`.\r\n\r\nFor that I am thinking we might consider:\r\n- Update the `tf.foldl` and `tf.foldr` to support `map_fn`-like operations that allows nested structure.\r\n- Add method `foldl` and `foldr` to `TensorArray` so that tensors with different shapes could be supported.\r\n\r\nLet me know if the above makes sense or not. If it does, then I can update this PR for nested structure support of `tf.foldl` and `tf.foldr`, and create another PR to address #12019 with `foldl` and `foldr` to `TensorArray`.\r\n\r\n/cc @edgimar\r\n", "I'd prefer not making any more changes to the TensorArray API.  Can all the\nlogic be done in tf.foldl and tf.foldr?\n\nOn Thu, Oct 12, 2017 at 1:34 PM, Yong Tang <notifications@github.com> wrote:\n\n> @mrry <https://github.com/mrry> @ebrevdo <https://github.com/ebrevdo> I\n> looked into the way map_fn handles nested structures. However, it seems\n> that map_fn-like method (and scan) expects the same (nested) structure\n> for all elems along dimension 0. But this is not the case raised in #12019\n> <https://github.com/tensorflow/tensorflow/issues/12019>.\n>\n> In #12019 <https://github.com/tensorflow/tensorflow/issues/12019>, the\n> input was\n>\n> def concat2(A, B):\n>     return tf.concat([A, B], axis=0)\n>\n> A = tf.constant([[10,10]])             # A.shape => (1,2)\n> B = tf.constant([[20, 20], [30, 30]])  # B.shape => (2,2)\n>\n> print(concat2(A, B).eval())              # => [[10, 10], [20, 20], [30, 30]]\n> print(tf.foldr(concat2, [A, B]).eval())  # => ERROR!\n>\n> My understanding is that the above scenario will not be compatible with\n> nested structure as map_fn.\n>\n> For that I am thinking we might consider:\n>\n>    - Update the tf.foldl and tf.foldr to support map_fn-like operations\n>    that allows nested structure.\n>    - Add method foldl and foldr to TensorArray so that tensors with\n>    different shapes could be supported.\n>\n> Let me know if the above makes sense or not. If it does, then I can update\n> this PR for nested structure support of tf.foldl and tf.foldr, and create\n> another PR to address #12019\n> <https://github.com/tensorflow/tensorflow/issues/12019> with foldl and\n> foldr to TensorArray.\n>\n> /cc @edgimar <https://github.com/edgimar>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/13282#issuecomment-336257619>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimx3PhXGCS-a1f8OnFSDDcYsBIIIOks5srnfXgaJpZM4PiDxC>\n> .\n>\n", "@yongtang any update?", "@yongtang and update on this? Or did you make another PR?", "Sorry for the late response. I have been thinking about addressing the issue of  #12019  while at the same time limit the changes to `foldr/foldl` (vs. TensorArray) but couldn't find an easy way to get around that. For now I think it probably make sense to close this PR. I will keep the issue #12019 in mind and if I could find some way to make it work then will reopen.\r\n\r\nThanks all for the help and sorry for the inconvenience."]}, {"number": 13281, "title": "Non-working example in documentation, with recommendations for how to fix", "body": "== cat /etc/issue ===============================================\r\nLinux EricDesktop 4.4.0-96-generic #119-Ubuntu SMP Tue Sep 12 14:59:54 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.2 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux EricDesktop 4.4.0-96-generic #119-Ubuntu SMP Tue Sep 12 14:59:54 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.1)\r\nprotobuf (3.4.0)\r\ntensorflow (1.3.0)\r\ntensorflow-tensorboard (0.1.6)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.3.0\r\ntf.GIT_VERSION = v1.3.0-rc1-2409-g5ee3804\r\ntf.COMPILER_VERSION = v1.3.0-rc1-2409-g5ee3804\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda-8.0/lib64\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nSun Sep 24 14:08:03 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.82                 Driver Version: 375.82                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 670     Off  | 0000:03:00.0     N/A |                  N/A |\r\n| 33%   50C    P0    N/A /  N/A |    254MiB /  4031MiB |     N/A      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 670     Off  | 0000:04:00.0     N/A |                  N/A |\r\n| 32%   48C    P0    N/A /  N/A |    253MiB /  4036MiB |     N/A      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0                  Not Supported                                         |\r\n|    1                  Not Supported                                         |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61\r\n\r\n### Describe the problem\r\n\r\n    I have been trying to get a custom op working for many hours, and have finally collected enough information to know that it isn't entirely my fault, and I can help you improve the custom op documentation to save others such effort.  \r\nWalkthrough to see what the problem is:\r\n1) Following the instructions on the documentation page (https://www.tensorflow.org/extend/adding_an_op) I copy the zero_out.cc code\r\n2) Compile with Bazel\r\n3) Test with python.  Everything works, this example behaves exactly as it should.\r\n4) But I want a GPU kernel, so now I try the \"example\" example.\r\nCopy the example.h, example.cc and example.cu.cc files on the documentation page.  Didn't make any changes.\r\n5) Now I hit some problems.  The documentation isn't very clear on how to compile this more complicated op with bazel.  I managed to figure it out, but I would strongly recommend notifying users about the \"gpu_srcs\" argument that can be used inside the BUILD file, since this took me quite a while to discover.\r\n\r\nWorking BUILD file:\r\nload(\"//tensorflow:tensorflow.bzl\", \"tf_custom_op_library\")\r\n\r\ntf_custom_op_library(\r\n    name = \"example.so\",\r\n    srcs = [\"example.cc\", \"example.h\"],\r\n    gpu_srcs = [\"example.cu.cc\", \"example.h\"],\r\n)\r\nWorking Bazel command to invoke the BUILD file:\r\nbazel build --config opt --config=cuda --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/core/user_ops:example.so\r\n\r\n6) Now it compiles.  But the op does not load properly in python; the module created by tf.load_op_library does not contain the actual op, as you can see by my tests under the source code section.\r\n7) After a long time, I discover the extremely obvious problem: the example code in the documentation does not have any REGISTER_OP macro call.  This is clearly an omission in the documentation.\r\n8) I try copying the REGISTER_OP macro call from the zero_out example, and it doesn't really work since the \"example\" example is more complicated, but python now at least recognizes the op.  I am sure if I were to spend the time to figure out how to correctly call the REGISTER_OP macro in example.cc, the example would work correctly for me.\r\n\r\n    I could make some other recommendations about changing the \"Adding a New Op\" documentation page, though I will refrain for now since I am not sure this is the appropriate place to do so.  I would be happy to take a stab at editing the doc page myself, though since I am very new to github I am concerned that I would end up creating more problems than I would fix, so it might be better to have someone with more experience do it.  I would be happy to help though.\r\n\r\n    One thing I will say though: I would really love it if some TF expert would add a fully fleshed out template op to the user op documentation page with all the bells and whistles, and with clear instructions on exactly how to compile and run it.  It should have both GPU support and a gradient implementation.  And ideally it would be multi-threaded (if the \"example\" example isn't already - its not obvious to me either way).  The template would be provided with everything you need, and would have a clearly labeled code block where the user can add their own code:  \"here is a for loop that iterates over every element in the tensor.  write whatever you want here.\"  If you can make it really easy for users to add their own, high-quality operators, you might be able to cut down on your workload responding to problems with custom ops and with users requesting new ops.  And I think that a template that we can fill in would be enough to do that for many people.\r\n\r\n### Source code / logs\r\n\r\n=========================================== start test.py:\r\nimport tensorflow as tf\r\nzero_out_module = tf.load_op_library('./zero_out.so')\r\nwith tf.Session(''):\r\n  print \"zero_out:\"\r\n  print(zero_out_module.zero_out([[1, 2], [3, 4]]).eval())\r\n\r\nexample_module = tf.load_op_library('./example.so')\r\nwith tf.Session(''):\r\n  print \"example:\"\r\n  try:\r\n    print(example_module.example([[1, 2], [3, 4]]).eval())\r\n  except AttributeError as err:\r\n    print \"Error: \", err\r\n    \r\n  \r\nprint \"Analysis of zero_out_module:\"  \r\nprint zero_out_module.__dict__.keys()\r\nprint zero_out_module.OP_LIST\r\n\r\nprint \"Analysis of example_module:\"\r\nprint example_module.__dict__.keys()\r\nprint example_module.OP_LIST\r\n\r\n====================================== end test.py\r\n\r\nOutput of running test.py\r\neric@EricDesktop:~/tensorflow/tensorflow/core/user_ops$ python test.py2017-09-24 14:14:08.183509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-09-24 14:14:08.183794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 0 with properties: \r\nname: GeForce GTX 670 major: 3 minor: 0 memoryClockRate(GHz): 1.0455\r\npciBusID: 0000:04:00.0\r\ntotalMemory: 3.94GiB freeMemory: 3.66GiB\r\n2017-09-24 14:14:08.208238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-09-24 14:14:08.208514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 1 with properties: \r\nname: GeForce GTX 670 major: 3 minor: 0 memoryClockRate(GHz): 1.0455\r\npciBusID: 0000:03:00.0\r\ntotalMemory: 3.94GiB freeMemory: 3.66GiB\r\n2017-09-24 14:14:08.208964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:980] Device peer to peer matrix\r\n2017-09-24 14:14:08.208988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] DMA: 0 1 \r\n2017-09-24 14:14:08.208994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] 0:   Y Y \r\n2017-09-24 14:14:08.209000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] 1:   Y Y \r\n2017-09-24 14:14:08.209012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 670, pci bus id: 0000:04:00.0, compute capability: 3.0)\r\n2017-09-24 14:14:08.209020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: GeForce GTX 670, pci bus id: 0000:03:00.0, compute capability: 3.0)\r\nzero_out:\r\n[[1 0]\r\n [0 0]]\r\n2017-09-24 14:14:08.239952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 670, pci bus id: 0000:04:00.0, compute capability: 3.0)\r\n2017-09-24 14:14:08.239970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: GeForce GTX 670, pci bus id: 0000:03:00.0, compute capability: 3.0)\r\nexample:\r\nError:  'module' object has no attribute 'example'\r\nAnalysis of zero_out_module:\r\n['_op_def_pb2', '_op_def_lib', '_op_def_registry', '_ops', '_collections', '_common_shapes', '__builtins__', 'zero_out', '__package__', '_op_def_library', 'OP_LIST', 'LIB_HANDLE', '__name__', '_InitOpDefLibrary', '__doc__']\r\nop {\r\n  name: \"ZeroOut\"\r\n  input_arg {\r\n    name: \"to_zero\"\r\n    type: DT_INT32\r\n  }\r\n  output_arg {\r\n    name: \"zeroed\"\r\n    type: DT_INT32\r\n  }\r\n}\r\n\r\nAnalysis of example_module:\r\n['_op_def_pb2', '_op_def_lib', '_op_def_registry', '_ops', '_collections', '_common_shapes', '__builtins__', '__package__', '_op_def_library', 'OP_LIST', 'LIB_HANDLE', '__name__', '_InitOpDefLibrary', '__doc__']\r\n\r\n\r\n", "comments": ["Thanks for your detailed description and suggestions.", "Would you be able to make these documentation changes and send us a pull request?", "Ok, I will give it a shot in the next few days.\n\nOn Wed, Oct 18, 2017 at 3:35 PM, Deanna Rubin <notifications@github.com>\nwrote:\n\n> Would you be able to make these documentation changes and send us a pull\n> request?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13281#issuecomment-337747080>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AeY6kvtaBWwNpWkevFQhldwHu_VvAlw6ks5stn0XgaJpZM4PiC0H>\n> .\n>\n", "Ok, I have made the pull request.  I made four changes, the first two of which should be straightforward, and the second two of which you might want to think about before merging\r\n\r\nI am new to GitHub, so let me know if I am there is something I can be doing better.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "(their PR is here: https://github.com/tensorflow/tensorflow/pull/13985 but I think it's still in flight)", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'm reassigning this to Mark because the PR in question is also assigned to him.\r\n\r\n(It's also possible we could close out this bug, but I'll leave that up to others.)", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @MarkDaoust: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @MarkDaoust: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @MarkDaoust: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Reviewer @MarkDaoust: It has been 15 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @MarkDaoust: It has been 15 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @MarkDaoust: It has been 121 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @MarkDaoust: It has been 136 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @MarkDaoust: It has been 151 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @MarkDaoust: It has been 166 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Closing this issue since https://github.com/tensorflow/tensorflow/pull/13985 is closed."]}, {"number": 13280, "title": "Run as the host user in docker when running docker in CI scripts.", "body": "", "comments": ["@gunan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @tensorflower-gardener and @caisq to be potential reviewers.", "NVM, It looks like I was looking at some old builds. "]}, {"number": 13279, "title": "Fix inconsistent tensor ranks in _linear_predictions", "body": "When mixing sparse feature columns and dense tensors of features _linear_predictions behave abnormaly without this fix (and is unusable).", "comments": ["Can one of the admins verify this patch?", "@fcharras, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @jart and @ebrevdo to be potential reviewers.", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please", "Thank you for the patience. This change looks OK but I'm not sure I fully understand. I'm going to reassign to contrib/linear_optimizer owner @petrosmol to play it safe. CC: @andreasst", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Jenkins, test this please!", "Hello, thank you very much for looking again at this PR. I'm of course OK with this additional commit, sorry for the formatting mistakes. Aside from this I don't think this PR is responsible for the failed test ? ", "Jenkins: test this please.\r\n\r\n@fcharras It appears unrelated, although I'm not entirely certain. Let's give it another run. Please also rebase onto HEAD in case we need to try again.", "Thank you !!"]}, {"number": 13278, "title": "tf_cc_binary() makes opencv unable to load an image", "body": "I try to load an image with opencv and work further on it with the tensorflow framework. Unfortunately I get a really weird behaviour:\r\n\r\nThe image is loaded without problems using `cc_binary(...)` in Bazel. Changing it to `tf_cc_binary(...)` doesn't stop the code from compilation or running, but opencv can't load any images any more.\r\n\r\n### Source code / logs\r\nThis is my BUILD file:\r\n\r\n    load(\"//tensorflow:tensorflow.bzl\", \"tf_cc_binary\")\r\n\r\n    #tf_cc_binary( <-- using this, no image could be loaded anymore\r\n    cc_binary(\r\n        name = \"main\",\r\n        srcs = [\"main.cpp\"],\r\n        linkopts = [\r\n            \"-lopencv_core\",\r\n            \"-lopencv_highgui\",\r\n            \"-lopencv_imgcodecs\",\r\n            \"-lopencv_imgproc\",\r\n        ],\r\n        visibility=[\"//visibility:public\"]\r\n    )\r\n\r\nI use the standard example code from the opencv website. Again, it is working and the image gets loaded using `cc_binary(`:\r\n\r\n\r\n    #include <opencv2/core/core.hpp>\r\n    #include <opencv2/highgui/highgui.hpp>\r\n    #include <iostream>\r\n    \r\n    using namespace cv;\r\n    using namespace std;\r\n    \r\n    int main( int argc, char** argv )\r\n    {\r\n        Mat image;\r\n        image = imread(\"tensorflow/test/imageHolder/data/example.jpg\", CV_LOAD_IMAGE_COLOR);   // Read the file\r\n    \r\n        if(! image.data )                              // Check for invalid input\r\n        {\r\n            cout <<  \"Could not open or find the image\" << std::endl ;\r\n            return -1;\r\n        }\r\n    \r\n        namedWindow( \"Display window\", WINDOW_AUTOSIZE );// Create a window for display.\r\n        imshow( \"Display window\", image );                   // Show our image inside it.\r\n    \r\n        waitKey(0);                                          // Wait for a keystroke in the window\r\n        return 0;\r\n    }\r\n\r\nThis is my file structure in case it matters:\r\n\r\n    \u251c\u2500\u2500 data\r\n     \u00a0\u00a0 \u251c\u2500\u2500 example.jpg\r\n    \u2514\u2500\u2500 src\r\n        \u251c\u2500\u2500 BUILD\r\n        \u251c\u2500\u2500 main.cpp\r\n", "comments": ["I think this may be a duplicate of #1924. There has been a history of conflicts between opencv and tensorflow see  #7378. This is usually caused by conflicting libraries that both depend on. Try following the advice in #1924, and let us know how it goes. @allenlavoie , will your library isolation stuff help here?", "I try to follow the advice in [#1924](https://github.com/tensorflow/tensorflow/issues/1924) to compile a shared library. That worked fine. But there is hardly any documentation about how to work in an external project with the tensorflow_cc.so library.\r\nIs there any example project, that shows how to compile projects with it? I'm mainly confused about which headers to include and where I can find them.", "We may want to shift some symbols to a private shared object (or the language bindings) so we don't conflict with symbols in extensions, but it doesn't sound like that's an issue here. If you want to use TensorFlow rather than adding ops/kernels, use the C++ or C API, not tf_cc_binary rules.\r\n\r\n@skye is working on a distribution for the C++ API AFAIK. The C API [has one](https://www.tensorflow.org/install/install_c).", "Unfortunately no one is working on a distribution for the C++ API right now, so if you're trying to use the C++ API, building against TF with bazel is probably the best way. If you can use the C API and follow the instructions Allen linked to, that should avoid this issue.", "I struggle quite a bit using the C API. Except for an introduction of how to install the library I can't find any working examples nor any documentation except for some helpful comments in the source code. Why is there a C++ API documentation, but no C API documentation?\r\nCan you give me some help to start with the C API? That would be great! ", "I would look at the C API tests, e.g. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api_test.cc#L686 (this goes over building and running a graph). Note that it uses some helper functions defined in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_test_util.cc."]}, {"number": 13277, "title": "Refactoring of canned estimators", "body": "Class specific `model_fn()` used by each canned estimator are replaced by `common_model_fn()` and `common_combined_model_fn()` functions and moved to `utils.py`, together with common `classifier_head()` and `regression_head()` functions. Canned estimators are then refactored to use these common functions which simplified their code and increased readability.", "comments": ["Can one of the admins verify this patch?", "@davorrunje, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @ispirmustafa and @xiejw to be potential reviewers.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "@roumposg Thank you for the explanation of your design decision.\r\n\r\nMy primary motive for the refactorization is my own work which requires implementing my own Estimators. E.g. I would need a CNN estimator and with this refactorization I could easily implement one by simply implementing its own logit_fn and reusing common head and model_fn. In case I need to extend DNN estimator with batch normalization the same tactic would sufice. Most use cases I can think of use the same model_fn and only differ in logit_fn and head. Reimplementing the same model function each time would make code more difficult to maintain because it would require every change to be propagated through each Estimator.\r\n\r\nOf course, I can always use this refactorization for my own estimators regardless of merging it with the master, but I feel it would be useful to others as well.", "davorrunje@\r\nif you have head and logit_fn, will your model_fn look like as follows, rigth?\r\ndef model_fn(features, labels, mode):\r\n  logits = logit_fn(features)\r\n  return head.create_estimator_spec(...)\r\n", "@ispirmustafa yes, the `model_fn()`  is exactly the same as before externally, and internally it is really just combining of the existing ones. First, I noticed that `model_fn()` of DNN and linear estimators is the same and there is no need for code duplication. Then I noticed that combined `model_fn()` is just unrolling of that same function twice and can be written as a loop. This way one function can replace all three written so far.\r\n\r\nThe reason why I think it is a good idea to have one` model_fn()` is when you start writing your own estimators based on existing ones. I am teaching a class on deep learning using Tensorflow and I teach students different techniques and architectures. We start with the existing canned Estimators and then we learn how to extend them with new functionality. With the existing code base, we would need to make numerous copies of the same `model_fn()` because a typical use case requires new `logit_fn()` and the rest of the code remains the same. This code duplication becomes obvious after writing more than five different estimators. E.g. adding batch norm layers is one example and using convolutional layers is another, they all require changes to `logits_fn()` only. ", "CLAs look good, thanks!\n\n<!-- ok -->", "@ispirmustafa please let me know if you have more questions.", "@ispirmustafa any update on this?", "Hi @davorrunje \r\nFirst of all thank you for your focus on making expansions easy. I see how common_model_fn fits well to your use cases and training. We don't wanna add one more layer in to existing model_fns. \r\nThanks again"]}, {"number": 13276, "title": "tensor flow optimizer", "body": "![desto](https://user-images.githubusercontent.com/29457825/30785133-38286aee-a17f-11e7-9130-2e271e4543a3.PNG)\r\n", "comments": ["https://stackoverflow.com/questions/tagged/tensorflow may be more suited for this sort of questions. \r\n\r\nBut in your case, just change your line 5 to:\r\n```python\r\nsess = tf.Session()\r\nsess.run(init)\r\n```"]}, {"number": 13275, "title": "Extracted time_series_regression_head", "body": "Extracted the head for time series estimators. This is the baby step to make sure the implementation is modeled after the ones in `tf.estimator.canned`. \r\ncc: @allenlavoie @martinwicke", "comments": ["@terrytangyuan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @allenlavoie to be a potential reviewer.", "@allenlavoie Thanks! I think I've addressed your comments and hopefully the build has been fixed. ", "Jenkins, test this please.", "I think the BUILD file needs to be formatted with [buildifier](https://github.com/bazelbuild/buildtools#buildifier) (sorry that CI test error is awful; there's already a py_test import). Then hopefully the tests will pass and I will take another look.", "Thanks! That's useful tip to know (so that I don't have to order those deps myself)! I think the sanity check is passing now. Let's see if other tests are passing as well. ", "Looks like you may need a dependency on the head py_library from the estimators py_library. Not that you have to, but you can also run these tests locally with \"bazel run //tensorflow/contrib/timeseries/python/timeseries:head_test\" (or go to the directory and just specify :head_test). If that's passing you can run all of the timeseries tests locally with \"bazel test //tensorflow/contrib/timeseries/...\".", "Just added that! I will run bazel test if this still fails (away from my machine currently) and make sure to do it next time before PR as well. Thank you for your patience (haven't done any major dev here for a while)!", "@allenlavoie I think I fixed everything now (at least locally). I left a couple of TODOs in the code. I'll try work on those in separate PRs when I have more time!", "Jenkins, test this please.\r\n\r\nShould be fine; was probably just a one-off tool failure.", "Jenkins, test this please. Probably a flake, retesting", "@gunan what is Ubuntu CC, restarting didn't seem to kick that off again.", "That is a replacement we are building for jenkins.\r\nIt will be triggered using labels.\r\n", "Triggering run via labels is super neat! Thanks it's finally passing. "]}, {"number": 13274, "title": "Add multitask optimizer wrapper and global norm gradient clipper", "body": "#12453 ", "comments": ["Can one of the admins verify this patch?", "@ilya-edrenkin, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @jart and @strategist333 to be potential reviewers.", "Jenkins, test this please.", "@ilya-edrenkin can you add some unittests for this.", "@sb2nov: OK, I will add tests next weekend.\r\n\r\n@alextp:\r\nI can do it both ways. However, have chosen this particular way to keep the code minimal. \r\nIf I use composition, I will have to add a considerable amount of boilerplate code to proxy requests like `get_slot` etc to the wrapped optimizer instance. This can be easily avoided by the proposed solution.\r\n\r\nMoreover, if the wrapped optimizer instance does provide some arbitrary non-standard interface (actually anything not defined in the abstract tf.train.Optimizer superclass), I don't see an easy way to proxy it at all (at least without magic that would override method/field lookups).\r\nSo the client will have to use `wrapper._opt.custom_call()` instead of `wrapper.custom_call()`. Therefore, the wrapper will no longer be a drop-in replacement to the original optimizer, as the client code will have to be modified in this manner.\r\n\r\nAm I missing something?", "You can override __getattr__ on the wrapped object to handle these extra\nmethods.\n\nOn Mon, Oct 2, 2017 at 8:05 AM, Ilya Edrenkin <notifications@github.com>\nwrote:\n\n> @sb2nov <https://github.com/sb2nov>: OK, I will add tests next weekend.\n>\n> @alextp <https://github.com/alextp>:\n> I can do it both ways. However, have chosen this particular way to keep\n> the code minimal.\n> If I use composition, I will have to add a considerable amount of\n> boilerplate code to proxy requests like get_slot etc to the wrapped\n> optimizer instance. This can be easily avoided by the proposed solution.\n>\n> Moreover, if the wrapped optimizer instance does provide some arbitrary\n> non-standard interface (actually anything not defined in the abstract\n> tf.train.Optimizer superclass), I don't see an easy way to proxy it at all\n> (at least without magic that would override method/field lookups).\n> So the client will have to use wrapper._opt.custom_call() instead of\n> wrapper.custom_call(). Therefore, the wrapper will no longer be a drop-in\n> replacement to the original optimizer, as the client code will have to be\n> modified in this manner.\n>\n> Am I missing something?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/13274#issuecomment-333561402>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxVYf1H_pageT2O2oHwTfG_JEIq9Mks5soPvMgaJpZM4Ph9bf>\n> .\n>\n\n\n\n-- \n - Alex\n", "Yes, sure. In my opinion, this is slightly more obscure, but I can rewrite it in this way if you wish.", "I'd prefer exposing the internal optimizer and wrapping instances, so users\ncan do wrapped.opt.method when accessing a private method, and some\nboilerplate to forward the public Optimizer API methods.\n\nOn Mon, Oct 2, 2017 at 8:24 AM, Ilya Edrenkin <notifications@github.com>\nwrote:\n\n> Yes, sure. In my opinion, this is slightly more obscure, but I can rewrite\n> it in this way if you wish.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/13274#issuecomment-333567876>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxYtq1EgWNYPHwnLO6hLfIHaaUmZtks5soQBHgaJpZM4Ph9bf>\n> .\n>\n\n\n\n-- \n - Alex\n", "Hmm, for me that feels a little bit not right. In this case we'd lose the \"drop-in-replacement\" feature: users will have to rewrite their client code to access private methods, and, given the nature of Python, probably get disturbing runtime errors if they forget to do that.\r\nIf I were doing composition in this case I'd rather override `__getattr__` for the wrapper, although inheritance would be in my opinion somewhat more idiomatic.\r\n\r\nDoes composition with overriden `__getattr__` sound good for you?", "Yes, I'm ok with that.\n\nOn Mon, Oct 2, 2017 at 9:05 AM, Ilya Edrenkin <notifications@github.com>\nwrote:\n\n> Hmm, for me that feels a little bit not right. In this case we'd lose the\n> \"drop-in-replacement\" feature: users will have to rewrite their client code\n> to access private methods, and, given the nature of Python, probably get\n> disturbing runtime errors if they forget to do that.\n> If I were doing composition in this case I'd rather override __getattr__\n> for the wrapper, although inheritance would be in my opinion somewhat more\n> idiomatic.\n>\n> Does composition with overriden __getattr__ sound good for you?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/13274#issuecomment-333580359>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxZD2QogN_eBbAd_hMH5TKbOKpuyXks5soQnlgaJpZM4Ph9bf>\n> .\n>\n\n\n\n-- \n - Alex\n", "@alextp Changed to composition wrt to your comments. Also added a few tests.", "Jenkins, test this pleace.", "Jenkins, test this please.", "Jenkins, test this please.", "Looks like the following failure is caused by this change:\r\n```\r\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\r\n-----------------------------------------------------------------------------\r\n2017-11-02 19:20:23.426196: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\nE/tmp/botexec/bazel-out/k8-py3-opt/bin/tensorflow/contrib/opt/multitask_optimizer_wrapper_test.runfiles/org_tensorflow/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\r\n  if d.decorator_argspec is not None), _inspect.getargspec(target))\r\n/tmp/botexec/bazel-out/k8-py3-opt/bin/tensorflow/contrib/opt/multitask_optimizer_wrapper_test.runfiles/org_tensorflow/tensorflow/contrib/opt/python/training/multitask_optimizer_wrapper_test.py:58: DeprecationWarning: Please use assertEqual instead.\r\n  self.assertEquals(slot0.get_shape(), var0.get_shape())\r\n..\r\n======================================================================\r\nERROR: testGradientClipping (__main__.MultitaskOptimizerWrapperTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/tmp/botexec/bazel-out/k8-py3-opt/bin/tensorflow/contrib/opt/multitask_optimizer_wrapper_test.runfiles/org_tensorflow/tensorflow/contrib/opt/python/training/multitask_optimizer_wrapper_test.py\", line 108, in testGradientClipping\r\n    zip(gradients, varlist), clip_norm=1.0)\r\n  File \"/tmp/botexec/bazel-out/k8-py3-opt/bin/tensorflow/contrib/opt/multitask_optimizer_wrapper_test.runfiles/org_tensorflow/tensorflow/contrib/opt/python/training/multitask_optimizer_wrapper.py\", line 135, in clip_gradients_by_global_norm\r\n    fixed_global_norm = clip_ops.global_norm(nonzero_gradients)\r\n  File \"/tmp/botexec/bazel-out/k8-py3-opt/bin/tensorflow/contrib/opt/multitask_optimizer_wrapper_test.runfiles/org_tensorflow/tensorflow/python/ops/clip_ops.py\", line 143, in global_norm\r\n    raise TypeError(\"t_list should be a sequence\")\r\nTypeError: t_list should be a sequence\r\n\r\n----------------------------------------------------------------------\r\nRan 3 tests in 0.172s\r\n\r\nFAILED (errors=1)\r\n````", "@gunan thanks! The reason seems to be that `map` returns an iterator rather than a list in Python3. Will fix now.\r\nBy the way, how do I reproduce this kind of failures locally? I was running `tensorflow/tools/ci_build/ci_build.sh CPU tensorflow/tools/ci_build/ci_sanity.sh` and `tensorflow/tools/ci_build/ci_build.sh CPU bazel test //tensorflow/contrib/opt:multitask_optimizer_wrapper_test` which did not report any errors. Probably there is a separate tool to check py2to3 compatibility in the CI suite that I am not aware of?", "@caisq What would be the way to run python3 tests with the above script?\r\n\r\nWe are working on easier to run scripts for these, but in the meantime I think there are some environment variables you can set to run python3 tests with ci_build script.", "@gunan @ilya-edrenkin  The [ci_sanity.sh script](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/ci_sanity.sh) runs both python2 and python3 pyint by default.\r\n\r\nSee: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/ci_sanity.sh#L508\r\n\r\nI'm not sure why it didn't catch that error. Maybe that's an error that pylint isn't supposed to be able to catch? \r\n\r\nAs for `tensorflow/tools/ci_build/ci_build.sh CPU bazel test //tensorflow/contrib/opt:multitask_optimizer_wrapper_test`, the `ci_build.sh` script doesn't know anything about python version, it simply uses the python version configured during `./configure`. So if you configure to python3 and rerun the test, it should be able to catch the rror.", "Correction: ci_build.sh calls out to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/builds/configured, which in turn answers the default answer to all questions, so it uses python2...\r\n\r\nYou could manually run `./configure` and point to python3. Then run\r\n`bazel test //tensorflow/contrib/opt:multitask_optimizer_wrapper_test`", "Thanks @caisq !\r\nPushed the fix for python3. Everything should be fine now (at least it is locally); unfortunately I don't have access to the logs of the two pending checks to ensure it.", "@tensorflow-jenkins test this please", "@martinwicke looks like this PR is ready to merge.", "@ilya-edrenkin I've run into the problem that this looks to solve and wanted to check a few things to make sure I'm understanding this right. I'm switching over from alternating to joint learning so help and comments would be very much appreciated. My initial thought was to pass a boolean (well, 1/0 int) vector into the loss function as the \"weights\" term, but I'm not sure if the optimizer would understand to zero out those gradients as we want it to.\r\n\r\nIn the case that you have a multi-head/task learner and a minibatch of size n, where k of the of the n samples should have their losses masked out for a single task, you would apply a boolean mask of [n, 1] shape to the loss, right? As below:\r\n\r\n```\r\n## Batch size 2, 3 logits\r\ntargets = tf.constant([[1.0,1.0,0.0], [0.0,0.0,0.0]], tf.float32)\r\nlogits = tf.constant([[0.8,0.9,0.2], [0.3,0.3,0.2]], tf.float32)\r\n\r\n# Compute the mask\r\nmask_targets = tf.count_nonzero(targets, 1)\r\nmask_targets = tf.cast(mask_targets, tf.bool)\r\n\r\n#Compute loss components\r\nlosses = tf.losses.sigmoid_cross_entropy(\r\n    targets, logits, reduction=tf.losses.Reduction.NONE)\r\n\r\n# Apply the mask\r\nmasked_losses = tf.boolean_mask(losses, mask_targets)\r\n\r\n# Reduce\r\nreduced_loss = tf.reduce_sum(masked_losses)\r\n\r\nsess = tf.Session()\r\nprint(sess.run(reduced_loss))\r\n```\r\n\r\nIs this the order you apply the losses, masks, and reductions in? Also, I assume the `reduction=tf.losses.Reduction.NONE`kwarg is necessary to be able to apply the masks in the batch dimension? I think that in practice if you tried to take the `tf.reduce_mean` of a fully masked label vector you would get a nan, is it appropriate here to `tf.reduce_sum` instead (/ is that what you do in practice to mitigate)?  Thank you!", "Dear @tfnub: this issue is probably not the best place for this kind of questions, I guess it would be better to use stackoverflow for that. (Otherwise everyone subscribed to this issue will be spammed by this conversation.)\r\n\r\nRe the masking: if you have a special value for the missing elements (like 0), you can get get the mask by using `tf.not_equal(targets, SPECIAL_MISSING_VALUE)`. Afterwards you can either use `tf.boolean_mask` or multiply the loss tensor by the float-casted mask elementwise. If you want the mean batch loss, you can divide the loss sum by `tf.maximum(1, number_valid_elements)` to avoid getting nans."]}, {"number": 13273, "title": "Fix several issues with `go fmt` and `go lint`", "body": "This fix fixes several issues related to `gofmt` and `golint` based on https://goreportcard.com/report/github.com/tensorflow/tensorflow\r\n\r\nThere are several changes:\r\n- `gofmt -s tensorflow/go/tensor.go`\r\n- `gofmt -s tensorflow/go/example_inception_inference_test.go`\r\n- `golint tensorflow/go/genop/internal/lib.go`\r\n\r\nAt the moment there are still quite a few golint and ineffassign warnings in the current go code base. However, all of them are from `tensorflow/go/op/wrappers.go` which is machine generated code.\r\n\r\nThis fix does not cover `tensorflow/go/op/wrappers.go`.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@yongtang, thanks for your PR! By analyzing the history of the files in this pull request, we identified @asimshankar, @jhseu and @tensorflower-gardener to be potential reviewers.", "Jenkins, test this please."]}, {"number": 13272, "title": "Why will tf.PaddingFIFOQueue print some information? ", "body": "I have asked this question on [stackoverflow](https://stackoverflow.com/questions/46390289/why-is-padding-fifo-queue-closed), but never receives a response.  I think it is not error, but  log information?", "comments": ["Closing this issue, since Stack Overflow is a more appropriate forum for the discussion."]}, {"number": 13271, "title": "Fix polynomial decay with cycle for global step=0", "body": "For polynomial decay with cycle=True the learning rate at\r\nstep 0 becomes NaN, because in the process of calculating it we\r\ndevide by 0. This change should fix it, by setting the multiplier\r\nfor the decay steps to one for global_step=0.", "comments": ["Can one of the admins verify this patch?", "@jemaw, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @martinwicke and @vrv to be potential reviewers.", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please\r\n\r\nChecking a potential flake in timeseries:input_pipeline_test"]}, {"number": 13270, "title": "Fix docs for tf.tanh, tf.sigmoid and tf.tan.", "body": "The API docs incorrectly state that tf.tanh accepts int32\r\nor int64 inputs. This was referenced in issue #10376, however,\r\nthe fix to #10376 only removed the reference to qint32. A look\r\nat the implementation in cwise_op_tanh.cc shows that int32 and\r\nint64 are indeed not supported. This fix removes them from\r\nthe API docs.\r\n\r\nSimilarly, we remove int32, int64 and qint32 from tf.sigmoid\r\nAPI docs.\r\n\r\nFinally, tf.tan is defined only for float and double, so I\r\nchanged in math_ops.cc to use UNARY_REAL.", "comments": ["Can one of the admins verify this patch?", "@codrut3, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @keveman and @tensorflower-gardener to be potential reviewers.", "@tensorflow-jenkins test this please", "Hi,\r\n\r\nIt seems changing UNARY() to UNARY_REAL() broke the backwards compatibility test, so I reverted that change. I hope all tests will pass now.", "@tensorflow-jenkins test this please", "Jenkins, test this please."]}, {"number": 13269, "title": "TensorFlow 1.0 and 1.2 behave differently on MultiRNNCell.", "body": "The following code works well on TF 1.0.1, but doesn't work on TF 1.2 and above, leaving error massage, \r\n\r\nFile \"pb_OE_column_on_the_spot.py\", line 318, in <module>\r\n    outputs1, _states = tf.nn.dynamic_rnn(_multi_cells, tf.one_hot(_X, data_dim), dtype=tf.float32)\r\nValueError: Trying to share variable rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel, but specified shape (256, 512) and found shape (132, 512).\r\n\r\nI followed the change after version 1.2: [https://github.com/tensorflow/tensorflow/releases](url),\r\nwhere states: To get 5 layers each with their own parameters, write: MultiRNNCell([LSTMCell(...) for _ in range(5)])\r\n\r\nand I also reviewed similar posts regarding same issue: [https://github.com/udacity/deep-learning/issues/132](url)\r\n\r\n    seq_length = 6\r\n    data_dim =4\r\n    hidden_dim = 12\r\n    X = tf.placeholder(tf.int32, [None, seq_length])\r\n    Y = tf.placeholder(tf.int32, [None]) \r\n    keep_prob = tf.placeholder(tf.float32) \r\n\r\n    X_one_hot = tf.one_hot(X, data_dim)\r\n    Y_one_hot = tf.one_hot(Y, 2) \r\n\r\n    def lstm_cell():\r\n        lstm = tf.contrib.rnn.BasicLSTMCell(hidden_dim, state_is_tuple=True)\r\n        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\r\n        return drop\r\n\r\n    multi_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(3)], state_is_tuple=True)\r\n    outputs1, _states = tf.nn.dynamic_rnn(multi_cells, X_one_hot, dtype=tf.float32)\r\n\r\nWhy does this error happen?\r\n.\r\n.\r\n.\r\nby the way, would 12 for the number of units in the LSTM cell be too small?", "comments": ["> Why does this error happen?\r\n\r\nI had no problems running your code.\r\n\r\n```sh\r\nC:\\Users\\carlt>python\r\nPython 3.6.0 |Anaconda custom (64-bit)| (default, Dec 23 2016, 11:57:41) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> seq_length = 6\r\n>>> data_dim =4\r\n>>> hidden_dim = 12\r\n>>> X = tf.placeholder(tf.int32, [None, seq_length])\r\n>>> Y = tf.placeholder(tf.int32, [None])\r\n>>> keep_prob = tf.placeholder(tf.float32)\r\n>>>\r\n>>> X_one_hot = tf.one_hot(X, data_dim)\r\n>>> Y_one_hot = tf.one_hot(Y, 2)\r\n>>>\r\n>>> def lstm_cell():\r\n...     lstm = tf.contrib.rnn.BasicLSTMCell(hidden_dim, state_is_tuple=True)\r\n...     drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\r\n...     return drop\r\n...\r\n>>> multi_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(3)], state_is_tuple=True)\r\n>>> outputs1, _states = tf.nn.dynamic_rnn(multi_cells, X_one_hot, dtype=tf.float32)\r\n>>>\r\n>>> outputs1.shape\r\nTensorShape([Dimension(None), Dimension(6), Dimension(12)])\r\n```\r\n```sh\r\nC:\\Users\\carlt>pip show tensorflow\r\nName: tensorflow\r\nVersion: 1.3.0\r\nSummary: TensorFlow helps the tensors flow\r\nHome-page: http://tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: opensource@google.com\r\nLicense: Apache 2.0\r\nLocation: c:\\users\\carlt\\anaconda3\\lib\\site-packages\r\nRequires: six, wheel, numpy, tensorflow-tensorboard, protobuf\r\n```\r\n\r\n> would 12 for the number of units in the LSTM cell be too small?\r\n\r\nDepends on your problem. The number of hidden units needs tuning, like most hyperparameters. I think common values values are around 32-512.", "This issue has been resolved in latest versions , closing this request now. Thank you "]}, {"number": 13268, "title": "ENH: add Relu6GradGrad", "body": "Fix #13144.", "comments": ["@facaiy, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @keveman and @zhangyaobit to be potential reviewers.", "Can one of the admins verify this patch?", "Can you add a gradient_checker test (similar to other grad_grad functions) to verify that this works now and continues to work in the future?", "@alextp Sure! Since I'm on holiday, I'll add the test later.", "Hi, @alextp .  I am a little confusing with xxxGradGrad functions.\r\n\r\nSay, a simple network as below:\r\nL = h(y), \r\ny = func(x), \r\n\r\n### first-order gradient\r\n\r\nAs chain ruler, It is easy to get first-order gradient:\r\ndL / dx = (dy / dx) * (dL / dy) \r\n\r\nAnd the implementation is straight as well:\r\n```python\r\ndef xxxGrad(op, grad):\r\n      x = op.inputs[0]\r\n      y = op.outputs[0]\r\n      dy_dx =  # math calculation\r\n      return dy_dx * grad\r\n```\r\nHere `grad` is `dL / dy`.\r\n\r\n### second-order gradient\r\n\r\nHowever, for second-order gradient,  it seems that `xxxGradGrad` has two inputs.\r\n\r\nTake `_SoftplusGradGrad` for example:\r\n\r\n```python\r\ndef _SoftplusGradGrad(op, grad):\r\n  # Let:\r\n  #   y = tf.nn.softplus(x)\r\n  #   dx = gen_nn_ops._softplus_grad(dy, x) = dy / (1 + exp(-x))\r\n  # This op computes (ddy, d2x) from op.inputs == [dy, x] and grad == ddx.\r\n  dy, x = op.inputs\r\n  with ops.control_dependencies([grad.op]):\r\n    ddy = gen_nn_ops._softplus_grad(grad, x)  # pylint: disable=protected-access\r\n    d2x = grad * dy / (math_ops.exp(-x) + 2.0 + math_ops.exp(x))\r\nreturn (ddy, d2x)\r\n```\r\nhttps://github.com/facaiy/tensorflow/blob/ed66e5a9dee770492bfdfe37d1a01c87d3bca3de/tensorflow/python/ops/nn_grad.py#L377\r\n\r\nI don't know how to calculate 2nd gradient with backpropagation, and could you explain what are `dy`, `grad`, `ddy` and `d2x` here? Thank you very much.", "Oh, perhaps I understand it.\r\n\r\nFor `_SoftplusGradGrad`,\r\nLet: \r\ny = tf.nn.softplus(x)\r\ndx = gen_nn_ops._softplus_grad(dy, x)\r\n\r\nhere `dy` is short for `dL/dy`, `dx` is short for `dL/dx`. Since `SoftplusGrad` is a function with `dy` and `x`, so `SoftplusGradGrad` need to calculate the gradient for `dy` and `x` respectively:\r\n`ddy` is short for `d(dx) / dy`,\r\n`d2x` is for `d(dx) / dx`.\r\n\r\nright?\r\n", "Hi, test case has been added, however, something seems missing:\r\n\r\n```python\r\n======================================================================\r\nERROR: testRelu6GradGrad (__main__.Relu6OpTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/data1/users/facai/.cache/bazel/_bazel_facai/3338df3cdc4fd0e5fdd3f3ae6490e0be/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/nn_grad_test.runfiles/org_tensorflow/tensorflow/python/ops/nn_grad_test.py\", line 34, in testRelu6GradGrad\r\n    r_g = gradients_impl.gradients(r, inputs)\r\n  File \"/data1/users/facai/.cache/bazel/_bazel_facai/3338df3cdc4fd0e5fdd3f3ae6490e0be/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/nn_grad_test.runfiles/org_tensorflow/tensorflow/python/ops/gradients_impl.py\", line 516, in gradients\r\n    (op.name, op.type))\r\nLookupError: No gradient defined for operation 'Relu6' (op type: Relu6)\r\n```", "Jenkins, test this please.", "It seems unrelated failure ", "Jenkins, test this please.", "Thank you, @alextp . The latest master is merged, could we retest it?", "Jenkins, test this please.\n\nOn Oct 13, 2017 18:50, \"Yan Facai (\u989c\u53d1\u624d)\" <notifications@github.com> wrote:\n\n> Thank you, @alextp <https://github.com/alextp> . Latest master is merged,\n> could we retest it?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/13268#issuecomment-336584908>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxdAQa_B5dFyiYuobMp7_U0CQZ0OSks5sr-kzgaJpZM4Phwo->\n> .\n>\n", "@tensorflow-jenkins test this please", "Hi, does anyone know what is wrong with Ubuntu CC? Thanks.", "We have to kick it off manually using a separate label, just did that.", "@vrv thank you. Two tests time out, and it seems unrelated.", "Yeah, unrelated, thanks!"]}, {"number": 13267, "title": "Ubuntu 16.04 + CUDA8.0 +CUDNN5.1, C++ compilation fails", "body": "Operating System: Ubuntu 16.04 LTS\r\n\r\nInstalled version of CUDA and cuDNN: 8.0.61 + 5.1.10\r\n```\r\n-rw-r--r-- 1 root root   556000  9\u6708 23 23:17 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16  9\u6708 23 23:17 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19  9\u6708 23 23:17 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61\r\n-rwxr-xr-x 1 root root   415432  9\u6708 23 23:17 /usr/local/cuda/lib64/libcudart.so.8.0.61\r\n-rw-r--r-- 1 root root   775162  9\u6708 23 23:17 /usr/local/cuda/lib64/libcudart_static.a\r\n-rwxr-xr-x 1 root root 84163560  9\u6708 23 23:35 /usr/local/cuda/lib64/libcudnn.so\r\n-rwxr-xr-x 1 root root 84163560  9\u6708 23 23:35 /usr/local/cuda/lib64/libcudnn.so.5\r\n-rwxr-xr-x 1 root root 84163560  9\u6708 23 23:35 /usr/local/cuda/lib64/libcudnn.so.5.1.10\r\n-rw-r--r-- 1 root root 70364814  9\u6708 23 23:35 /usr/local/cuda/lib64/libcudnn_static.a\r\n\r\n```\r\n\r\n```\r\nBuild label: 0.5.4\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Aug 25 10:00:00 2017 (1503655200)\r\nBuild timestamp: 1503655200\r\nBuild timestamp as int: 1503655200\r\n```\r\nsource:https://github.com/ymgaq/AQ\r\n\r\nI use command: \"bazel build :AQ\" compile it\r\nerror message have 52mb ,cant upload\r\nThe error message is similar\r\n```\r\nLoading: \r\nLoading: 0 packages loaded\r\nWARNING: /home/jack/tensorflow/tensorflow/core/BUILD:1773:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/jack/tensorflow/tensorflow/tensorflow.bzl:1029:30\r\nINFO: Analysed target //AQ/src:AQ (0 packages loaded).\r\nINFO: Found 1 target...\r\n[0 / 2] BazelWorkspaceStatusAction stable-status.txt\r\n[1 / 2] Linking AQ/src/AQ; 1s local\r\n[1 / 2] Linking AQ/src/AQ; 11s local\r\nERROR: /home/jack/tensorflow/AQ/src/BUILD:1:1: Linking of rule '//AQ/src:AQ' failed (Exit 1)\r\nbazel-out/local_linux-opt/bin/AQ/src/_objs/AQ/AQ/src/nueral_net.o: In function `std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>::~pair()':\r\nnueral_net.cpp:(.text._ZNSt4pairINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEN10tensorflow6TensorEED2Ev[_ZNSt4pairINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEN10tensorflow6TensorEED5Ev]+0x15): undefined reference to `tensorflow::Tensor::~Tensor()'\r\nbazel-out/local_linux-opt/bin/AQ/src/_objs/AQ/AQ/src/nueral_net.o: In function `tensorflow::TTypes<float, 2ul, long>::Tensor tensorflow::Tensor::tensor<float, 2ul>()':\r\nnueral_net.cpp:(.text._ZN10tensorflow6Tensor6tensorIfLm2EEENS_6TTypesIT_XT0_ElE6TensorEv[_ZN10tensorflow6Tensor6tensorIfLm2EEENS_6TTypesIT_XT0_ElE6TensorEv]+0x20): undefined reference to `tensorflow::Tensor::CheckTypeAndIsAligned(tensorflow::DataType) const'\r\nnueral_net.cpp:(.text._ZN10tensorflow6Tensor6tensorIfLm2EEENS_6TTypesIT_XT0_ElE6TensorEv[_ZN10tensorflow6Tensor6tensorIfLm2EEENS_6TTypesIT_XT0_ElE6TensorEv]+0x2d): undefined reference to `tensorflow::TensorShape::CheckDimsEqual(int) const'\r\nnueral_net.cpp:(.text._ZN10tensorflow6Tensor6tensorIfLm2EEENS_6TTypesIT_XT0_ElE6TensorEv[_ZN10tensorflow6Tensor6tensorIfLm2EEENS_6TTypesIT_XT0_ElE6TensorEv]+0x3a): undefined reference to `tensorflow::TensorShape::CheckDimsAtLeast(int) const'\r\nnueral_net.cpp:(.text._ZN10tensorflow6Tensor6tensorIfLm2EEENS_6TTypesIT_XT0_ElE6TensorEv[_ZN10tensorflow6Tensor6tensorIfLm2EEENS_6TTypesIT_XT0_ElE6TensorEv]+0x64): undefined reference to `tensorflow::TensorShapeBase<tensorflow::TensorShape>::dim_size(int) const'\r\nbazel-out/local_linux-opt/bin/AQ/src/_objs/AQ/AQ/src/nueral_net.o: In function `void std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > >::_M_assign_aux<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> const*>(std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> const*, std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> const*, std::forward_iterator_tag)':\r\nnueral_net.cpp:(.text._ZNSt6vectorISt4pairINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEN10tensorflow6TensorEESaIS9_EE13_M_assign_auxIPKS9_EEvT_SF_St20forward_iterator_tag[_ZNSt6vectorISt4pairINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEN10tensorflow6TensorEESaIS9_EE13_M_assign_auxIPKS9_EEvT_SF_St20forward_iterator_tag]+0x90): undefined reference to `tensorflow::Tensor::CopyFromInternal(tensorflow::Tensor const&, tensorflow::TensorShape const&)'\r\nnueral_net.cpp:(.text._ZNSt6vectorISt4pairINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEN10tensorflow6TensorEESaIS9_EE13_M_assign_auxIPKS9_EEvT_SF_St20forward_iterator_tag[_ZNSt6vectorISt4pairINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEN10tensorflow6TensorEESaIS9_EE13_M_assign_auxIPKS9_EEvT_SF_St20forward_iterator_tag]+0xb5): undefined reference to `tensorflow::Tensor::~Tensor()'\r\nnueral_net.cpp:(.text._ZNSt6vectorISt4pairINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEN10tensorflow6TensorEESaIS9_EE13_M_assign_auxIPKS9_EEvT_SF_St20forward_iterator_tag[_ZNSt6vectorISt4pairINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEN10tensorflow6TensorEESaIS9_EE13_M_assign_auxIPKS9_EEvT_SF_St20forward_iterator_tag]+0x11f): undefined reference to `tensorflow::Tensor::CopyFromInternal(tensorflow::Tensor const&, tensorflow::TensorShape const&)'\r\nnueral_net.cpp:(.text._ZNSt6vectorISt4pairINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEN10tensorflow6TensorEESaIS9_EE13_M_assign_auxIPKS9_EEvT_SF_St20forward_iterator_tag[_ZNSt6vectorISt4pairINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEN10tensorflow6TensorEESaIS9_EE13_M_assign_auxIPKS9_EEvT_SF_St20forward_iterator_tag]+0x226): undefined reference to `tensorflow::TensorShapeRep::SlowCopyFrom(tensorflow::TensorShapeRep const&)'\r\nnueral_net.cpp:(.text._ZNSt6vectorISt4pairINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEN10tensorflow6TensorEESaIS9_EE13_M_assign_auxIPKS9_EEvT_SF_St20forward_iterator_tag[_ZNSt6vectorISt4pairINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEN10tensorflow6TensorEESaIS9_EE13_M_assign_auxIPKS9_EEvT_SF_St20forward_iterator_tag]+0x38d): undefined reference to `tensorflow::Tensor::~Tensor()'\r\nnueral_net.cpp:(.text._ZNSt6vectorISt4pairINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEN10tensorflow6TensorEESaIS9_EE13_M_assign_auxIPKS9_EEvT_SF_St20forward_iterator_tag[_ZNSt6vectorISt4pairINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEN10tensorflow6TensorEESaIS9_EE13_M_assign_auxIPKS9_EEvT_SF_St20forward_iterator_tag]+0x400): undefined reference to `tensorflow::TensorShapeRep::SlowCopyFrom(tensorflow::TensorShapeRep const&)'\r\nbazel-out/local_linux-opt/bin/AQ/src/_objs/AQ/AQ/src/nueral_net.o: In function `PolicyNet(tensorflow::Session*, std::vector<FeedTensor, std::allocator<FeedTensor> >&, std::vector<std::array<double, 441ul>, std::allocator<std::array<double, 441ul> > >&, double, int)':\r\nnueral_net.cpp:(.text._Z9PolicyNetPN10tensorflow7SessionERSt6vectorI10FeedTensorSaIS3_EERS2_ISt5arrayIdLm441EESaIS8_EEdi+0xb6): undefined reference to `tensorflow::TensorShapeBase<tensorflow::TensorShape>::TensorShapeBase(tensorflow::gtl::ArraySlice<long long>)'\r\nnueral_net.cpp:(.text._Z9PolicyNetPN10tensorflow7SessionERSt6vectorI10FeedTensorSaIS3_EERS2_ISt5arrayIdLm441EESaIS8_EEdi+0xd1): undefined reference to `tensorflow::Tensor::Tensor(tensorflow::DataType, tensorflow::TensorShape const&)'\r\nnueral_net.cpp:(.text._Z9PolicyNetPN10tensorflow7SessionERSt6vectorI10FeedTensorSaIS3_EERS2_ISt5arrayIdLm441EESaIS8_EEdi+0xe3): undefined reference to `tensorflow::TensorShapeRep::DestructorOutOfLine()'\r\nnueral_net.cpp:(.text._Z9PolicyNetPN10tensorflow7SessionERSt6vectorI10FeedTensorSaIS3_EERS2_ISt5arrayIdLm441EESaIS8_EEdi+0xf7): undefined reference to `tensorflow::Tensor::CheckTypeAndIsAligned(tensorflow::DataType) const'\r\nnueral_net.cpp:(.text._Z9PolicyNetPN10tensorflow7SessionERSt6vectorI10FeedTensorSaIS3_EERS2_ISt5arrayIdLm441EESaIS8_EEdi+0x104): undefined reference to `tensorflow::TensorShape::CheckDimsEqual(int) const'\r\nnueral_net.cpp:(.text._Z9PolicyNetPN10tensorflow7SessionERSt6vectorI10FeedTensorSaIS3_EERS2_ISt5arrayIdLm441EESaIS8_EEdi+0x111): undefined reference to `tensorflow::TensorShape::CheckDimsAtLeast(int) const'\r\nnueral_net.cpp:(.text._Z9PolicyNetPN10tensorflow7SessionERSt6vectorI10FeedTensorSaIS3_EERS2_ISt5arrayIdLm441EESaIS8_EEdi+0x146): undefined reference to `tensorflow::TensorShapeBase<tensorflow::TensorShape>::dim_size(int) const'\r\nnueral_net.cpp:(.text._Z9PolicyNetPN10tensorflow7SessionERSt6vectorI10FeedTensorSaIS3_EERS2_ISt5arrayIdLm441EESaIS8_EEdi+0x1ee): undefined reference to `tensorflow::TensorShapeBase<tensorflow::TensorShape>::TensorShapeBase()'\r\nnueral_net.cpp:(.text._Z9PolicyNetPN10tensorflow7SessionERSt6vectorI10FeedTensorSaIS3_EERS2_ISt5arrayIdLm441EESaIS8_EEdi+0x202): undefined reference to `tensorflow::Tensor::Tensor(tensorflow::DataType, tensorflow::TensorShape const&)'\r\n```\r\n```\r\nfunctional_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.17+0x8f): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'\r\nfunctional_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.17+0xa3): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'\r\nfunctional_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.17+0xb7): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'\r\nfunctional_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.17+0xc6): undefined reference to `tensorflow::OpDefBuilder::SetShapeFn(tensorflow::Status (*)(tensorflow::shape_inference::InferenceContext*))'\r\nfunctional_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.17+0xda): undefined reference to `tensorflow::OpDefBuilder::Doc(tensorflow::StringPiece)'\r\nfunctional_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.17+0xe9): undefined reference to `tensorflow::register_op::OpDefBuilderReceiver::OpDefBuilderReceiver(tensorflow::register_op::OpDefBuilderWrapper<true> const&)'\r\nfunctional_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.17+0x114): undefined reference to `tensorflow::OpDefBuilder::Input(tensorflow::StringPiece)'\r\nfunctional_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.17+0x128): undefined reference to `tensorflow::OpDefBuilder::Input(tensorflow::StringPiece)'\r\nfunctional_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.17+0x13c): undefined reference to `tensorflow::OpDefBuilder::Output(tensorflow::StringPiece)'\r\nfunctional_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.17+0x150): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'\r\nfunctional_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.17+0x164): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'\r\nfunctional_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.17+0x178): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'\r\nfunctional_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.17+0x17f): undefined reference to `tensorflow::shape_inference::UnknownShape(tensorflow::shape_inference::InferenceContext*)'\r\nfunctional_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.17+0x187): undefined reference to `tensorflow::OpDefBuilder::SetShapeFn(tensorflow::Status (*)(tensorflow::shape_inference::InferenceContext*))'\r\nfunctional_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.17+0x19b): undefined reference to `tensorflow::OpDefBuilder::Doc(tensorflow::StringPiece)'\r\nfunctional_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.17+0x1aa): undefined reference to `tensorflow::register_op::OpDefBuilderReceiver::OpDefBuilderReceiver(tensorflow::register_op::OpDefBuilderWrapper<true> const&)'\r\nbazel-out/local_linux-opt/bin/tensorflow/core/libno_op_op_lib.lo(no_op.o): In function `__static_initialization_and_destruction_0(int, int) [clone .constprop.11]':\r\nno_op.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.11+0x5c): undefined reference to `tensorflow::OpDefBuilder::OpDefBuilder(tensorflow::StringPiece)'\r\nno_op.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.11+0x6b): undefined reference to `tensorflow::OpDefBuilder::SetShapeFn(tensorflow::Status (*)(tensorflow::shape_inference::InferenceContext*))'\r\nno_op.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.11+0x7f): undefined reference to `tensorflow::OpDefBuilder::Doc(tensorflow::StringPiece)'\r\nno_op.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.11+0x8e): undefined reference to `tensorflow::register_op::OpDefBuilderReceiver::OpDefBuilderReceiver(tensorflow::register_op::OpDefBuilderWrapper<true> const&)'\r\nno_op.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.11+0x1bd): undefined reference to `tensorflow::OpDef::~OpDef()'\r\nbazel-out/local_linux-opt/bin/tensorflow/core/libsendrecv_ops_op_lib.lo(sendrecv_ops.o): In function `__static_initialization_and_destruction_0(int, int) [clone .constprop.10]':\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x69): undefined reference to `tensorflow::OpDefBuilder::Input(tensorflow::StringPiece)'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x7d): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x91): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0xa5): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0xb9): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0xcd): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'\r\nbazel-out/local_linux-opt/bin/tensorflow/core/libsendrecv_ops_op_lib.lo(sendrecv_ops.o):sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0xe1): more undefined references to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)' follow\r\nbazel-out/local_linux-opt/bin/tensorflow/core/libsendrecv_ops_op_lib.lo(sendrecv_ops.o): In function `__static_initialization_and_destruction_0(int, int) [clone .constprop.10]':\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0xe9): undefined reference to `tensorflow::OpDefBuilder::SetIsStateful()'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0xf0): undefined reference to `tensorflow::shape_inference::UnknownShape(tensorflow::shape_inference::InferenceContext*)'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0xfb): undefined reference to `tensorflow::OpDefBuilder::SetShapeFn(tensorflow::Status (*)(tensorflow::shape_inference::InferenceContext*))'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x10f): undefined reference to `tensorflow::OpDefBuilder::Doc(tensorflow::StringPiece)'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x11e): undefined reference to `tensorflow::register_op::OpDefBuilderReceiver::OpDefBuilderReceiver(tensorflow::register_op::OpDefBuilderWrapper<true> const&)'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x149): undefined reference to `tensorflow::OpDefBuilder::Output(tensorflow::StringPiece)'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x15d): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x171): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x185): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x199): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x1ad): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'\r\nbazel-out/local_linux-opt/bin/tensorflow/core/libsendrecv_ops_op_lib.lo(sendrecv_ops.o):sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x1c1): more undefined references to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)' follow\r\nbazel-out/local_linux-opt/bin/tensorflow/core/libsendrecv_ops_op_lib.lo(sendrecv_ops.o): In function `__static_initialization_and_destruction_0(int, int) [clone .constprop.10]':\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x1c9): undefined reference to `tensorflow::OpDefBuilder::SetIsStateful()'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x1d4): undefined reference to `tensorflow::OpDefBuilder::SetShapeFn(tensorflow::Status (*)(tensorflow::shape_inference::InferenceContext*))'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x1e8): undefined reference to `tensorflow::OpDefBuilder::Doc(tensorflow::StringPiece)'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x1f7): undefined reference to `tensorflow::register_op::OpDefBuilderReceiver::OpDefBuilderReceiver(tensorflow::register_op::OpDefBuilderWrapper<true> const&)'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x222): undefined reference to `tensorflow::OpDefBuilder::Input(tensorflow::StringPiece)'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x236): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x24a): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x25e): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x272): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x286): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'\r\nbazel-out/local_linux-opt/bin/tensorflow/core/libsendrecv_ops_op_lib.lo(sendrecv_ops.o):sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x29a): more undefined references to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)' follow\r\nbazel-out/local_linux-opt/bin/tensorflow/core/libsendrecv_ops_op_lib.lo(sendrecv_ops.o): In function `__static_initialization_and_destruction_0(int, int) [clone .constprop.10]':\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x2a2): undefined reference to `tensorflow::OpDefBuilder::SetIsStateful()'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x2ad): undefined reference to `tensorflow::OpDefBuilder::SetShapeFn(tensorflow::Status (*)(tensorflow::shape_inference::InferenceContext*))'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x2c1): undefined reference to `tensorflow::OpDefBuilder::Doc(tensorflow::StringPiece)'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x2d0): undefined reference to `tensorflow::register_op::OpDefBuilderReceiver::OpDefBuilderReceiver(tensorflow::register_op::OpDefBuilderWrapper<true> const&)'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x2fb): undefined reference to `tensorflow::OpDefBuilder::Output(tensorflow::StringPiece)'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x30f): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x323): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x337): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x34b): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x35f): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'\r\nbazel-out/local_linux-opt/bin/tensorflow/core/libsendrecv_ops_op_lib.lo(sendrecv_ops.o):sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x373): more undefined references to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)' follow\r\nbazel-out/local_linux-opt/bin/tensorflow/core/libsendrecv_ops_op_lib.lo(sendrecv_ops.o): In function `__static_initialization_and_destruction_0(int, int) [clone .constprop.10]':\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x37b): undefined reference to `tensorflow::OpDefBuilder::SetIsStateful()'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x386): undefined reference to `tensorflow::OpDefBuilder::SetShapeFn(tensorflow::Status (*)(tensorflow::shape_inference::InferenceContext*))'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x39a): undefined reference to `tensorflow::OpDefBuilder::Doc(tensorflow::StringPiece)'\r\nsendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x3a9): undefined reference to `tensorflow::register_op::OpDefBuilderReceiver::OpDefBuilderReceiver(tensorflow::register_op::OpDefBuilderWrapper<true> const&)'\r\nbazel-out/local_linux-opt/bin/tensorflow/core/libcore_cpu_base.lo(algorithm.o): In function `tensorflow::DFS(tensorflow::Graph const&, std::function<void (tensorflow::Node*)> const&, std::function<void (tensorflow::Node*)> const&)':\r\nalgorithm.cc:(.text._ZN10tensorflow3DFSERKNS_5GraphERKSt8functionIFvPNS_4NodeEEES9_+0x223): undefined reference to `tensorflow::Node::out_nodes() const'\r\nbazel-out/local_linux-opt/bin/tensorflow/core/libcore_cpu_base.lo(algorithm.o): In function `tensorflow::FixupSourceAndSinkEdges(tensorflow::Graph*)':\r\nalgorithm.cc:(.text._ZN10tensorflow23FixupSourceAndSinkEdgesEPNS_5GraphE+0x29): undefined reference to `tensorflow::Graph::kControlSlot'\r\nalgorithm.cc:(.text._ZN10tensorflow23FixupSourceAndSinkEdgesEPNS_5GraphE+0x102): undefined reference to `tensorflow::Graph::AddEdge(tensorflow::Node*, int, tensorflow::Node*, int)'\r\nalgorithm.cc:(.text._ZN10tensorflow23FixupSourceAndSinkEdgesEPNS_5GraphE+0x135): undefined reference to `tensorflow::Graph::kControlSlot'\r\nalgorithm.cc:(.text._ZN10tensorflow23FixupSourceAndSinkEdgesEPNS_5GraphE+0x146): undefined reference to `tensorflow::Graph::AddEdge(tensorflow::Node*, int, tensorflow::Node*, int)'\r\nbazel-out/local_linux-opt/bin/tensorflow/core/libcore_cpu_base.lo(algorithm.o): In function `tensorflow::PruneForReverseReachability(tensorflow::Graph*, std::unordered_set<tensorflow::Node const*, std::hash<tensorflow::Node const*>, std::equal_to<tensorflow::Node const*>, std::allocator<tensorflow::Node const*> >)':\r\nalgorithm.cc:(.text._ZN10tensorflow27PruneForReverseReachabilityEPNS_5GraphESt13unordered_setIPKNS_4NodeESt4hashIS5_ESt8equal_toIS5_ESaIS5_EE+0x157): undefined reference to `tensorflow::Node::name[abi:cxx11]() const'\r\nalgorithm.cc:(.text._ZN10tensorflow27PruneForReverseReachabilityEPNS_5GraphESt13unordered_setIPKNS_4NodeESt4hashIS5_ESt8equal_toIS5_ESaIS5_EE+0x27b): undefined reference to `tensorflow::Node::in_nodes() const'\r\nalgorithm.cc:(.text._ZN10tensorflow27PruneForReverseReachabilityEPNS_5GraphESt13unordered_setIPKNS_4NodeESt4hashIS5_ESt8equal_toIS5_ESaIS5_EE+0x42d): undefined reference to `tensorflow::Node::name[abi:cxx11]() const'\r\nalgorithm.cc:(.text._ZN10tensorflow27PruneForReverseReachabilityEPNS_5GraphESt13unordered_setIPKNS_4NodeESt4hashIS5_ESt8equal_toIS5_ESaIS5_EE+0x440): undefined reference to `tensorflow::Node::name[abi:cxx11]() const'\r\nalgorithm.cc:(.text._ZN10tensorflow27PruneForReverseReachabilityEPNS_5GraphESt13unordered_setIPKNS_4NodeESt4hashIS5_ESt8equal_toIS5_ESaIS5_EE+0x6d5): undefined reference to `tensorflow::Graph::RemoveNode(tensorflow::Node*)'\r\nbazel-out/local_linux-opt/bin/tensorflow/core/libcore_cpu_base.lo(tensor_id.o): In function `tensorflow::ParseTensorName(tensorflow::StringPiece)':\r\ntensor_id.cc:(.text._ZN10tensorflow15ParseTensorNameENS_11StringPieceE+0x89): undefined reference to `tensorflow::Graph::kControlSlot'\r\nbazel-out/local_linux-opt/bin/tensorflow/core/libcore_cpu_base.lo(validate.o): In function `tensorflow::graph::ValidateGraphDef(tensorflow::GraphDef const&, tensorflow::OpRegistryInterface const&)':\r\nvalidate.cc:(.text._ZN10tensorflow5graph16ValidateGraphDefERKNS_8GraphDefERKNS_19OpRegistryInterfaceE+0x13): undefined reference to `tensorflow::_VersionDef_default_instance_'\r\nvalidate.cc:(.text._ZN10tensorflow5graph16ValidateGraphDefERKNS_8GraphDefERKNS_19OpRegistryInterfaceE+0x7a): undefined reference to `tensorflow::OpRegistryInterface::LookUpOpDef(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tensorflow::OpDef const**) const'\r\nvalidate.cc:(.text._ZN10tensorflow5graph16ValidateGraphDefERKNS_8GraphDefERKNS_19OpRegistryInterfaceE+0x92): undefined reference to `tensorflow::ValidateNodeDef(tensorflow::NodeDef const&, tensorflow::OpDef const&)'\r\nvalidate.cc:(.text._ZN10tensorflow5graph16ValidateGraphDefERKNS_8GraphDefERKNS_19OpRegistryInterfaceE+0xaa): undefined reference to `tensorflow::CheckOpDeprecation(tensorflow::OpDef const&, int)'\r\nbazel-out/local_linux-opt/bin/tensorflow/core/util/tensor_bundle/libnaming.a(naming.o): In function `tensorflow::MetaFilename[abi:cxx11](tensorflow::StringPiece)':\r\nnaming.cc:(.text._ZN10tensorflow12MetaFilenameB5cxx11ENS_11StringPieceE+0x19): undefined reference to `tensorflow::strings::Printf[abi:cxx11](char const*, ...)'\r\nbazel-out/local_linux-opt/bin/tensorflow/core/util/tensor_bundle/libnaming.a(naming.o): In function `tensorflow::DataFilename[abi:cxx11](tensorflow::StringPiece, int, int)':\r\nnaming.cc:(.text._ZN10tensorflow12DataFilenameB5cxx11ENS_11StringPieceEii+0x1f): undefined reference to `tensorflow::strings::Printf[abi:cxx11](char const*, ...)'\r\ncollect2: error: ld returned 1 exit status\r\nTarget //AQ/src:AQ failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 18.153s, Critical Path: 17.81s\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n", "comments": ["You'll need to build with --config=monolithic, or get the maintainer to start using TensorFlow's tf_cc_test/tf_cc_binary rules. We've started building with an explicit shared object, and Bazel will not (yet) link those in transitively."]}, {"number": 13266, "title": "ENH: row_shape supports unknown dim in Dataset.dense_to_sparse_batch", "body": "see issue: #13216 \r\n\r\n### What changes were proposed in this pull request?\r\n\r\neg: `row_shape = [20, -1, 10]`, `row_shape = [20, None, 10]`\r\n\r\n### How to test\r\n\r\n+ [x] add test cases.\r\n+ [ ] pass all tests.", "comments": ["@facaiy, thanks for your PR! By analyzing the history of the files in this pull request, we identified @mrry, @jhseu and @ebrevdo to be potential reviewers.", "Can one of the admins verify this patch?", "Jenkins, test this please.", "Thanks, @sb2nov . Fetch failed. Might we retest the PR?\r\n\r\n```\r\n> git fetch --tags --progress https://github.com/tensorflow/tensorflow.git +refs/pull/*:refs/remotes/origin/pr/*\r\nERROR: Error fetching remote repo 'origin'\r\nhudson.plugins.git.GitException: Failed to fetch from https://github.com/tensorflow/tensorflow.git\r\n```", "@tensorflow-jenkins test this please.\r\n\r\nThanks for this contribution! Should be good to merge once the tests pass."]}, {"number": 13265, "title": "CheckpointSaverHook does not check Graph of Saver", "body": "The following code will run just fine, but will not save any Variables to checkpoints:\r\n```python\r\nimport tensorflow as tf\r\n\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n    v = tf.get_variable(\"test\", shape=(100, 100), dtype=tf.float32)\r\n\r\nsave = tf.train.CheckpointSaverHook(\"test_dir\", 10)\r\nwith graph.as_default():\r\n    tf.train.create_global_step()\r\n    a = tf.constant(1)\r\n    with tf.train.MonitoredSession(hooks=[save]) as sess:\r\n        sess.run(a)\r\n```\r\n\r\nThis is because the `CheckpointSaverHook` does not find an existing Saver, so it creates a new one in its constructor, which is executed with a different default Graph. \r\n", "comments": ["Is it possible to set the graph manually? Got the same problem ...\r\nhttps://stackoverflow.com/questions/46299112/tensorflow-how-to-setup-a-checkpointsaverhook", "You need to initialize the `CheckpointSaverHook` using the same graph as your variables.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n    v = tf.get_variable(\"test\", shape=(100, 100), dtype=tf.float32)\r\n    save = tf.train.CheckpointSaverHook(\"test_dir\", 10)\r\n\r\nwith graph.as_default():\r\n    tf.train.create_global_step()\r\n    a = tf.constant(1)\r\n    with tf.train.MonitoredSession(hooks=[save]) as sess:\r\n        sess.run(a)\r\n\r\nprint(tf.contrib.framework.list_variables(\"test_dir\"))\r\n```\r\n\r\nprints `[('test', [100, 100])]`.", "Maybe I should have been a bit clearer in my description, but my original issue was not meant as a question on how to fix the presented code (on the user side), but as code that I would have expected to work, but which did not. \r\nAs the Hook has a dedicated `begin` which is called in the correct graph, and supposed to do any  modifications of that graph. So I think it is at least surprising that in this case the Saver is added during the `__init__` and not the `begin` call."]}, {"number": 13264, "title": "Branch 169770126", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 13263, "title": "`params` is Not Used for Canned Estimators Implementations", "body": "The canned estimators implementations do not seem to pass the `params` to their parent class `Estimator`. For example, `DNNClassifier` puts all the parameters, e.g. `hidden_units`, `optimizer`, etc. inside the `model_fn` and then pass the `model_fn` to the parent class [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/canned/dnn.py#L314). This makes it difficult for reporting or parameter tuning purposes. \r\n\r\nI saw [`params` is an exposed property for `Estimator`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L188), however, it's mostly empty for canned estimators since it's not used. Is this on purpose?\r\n\r\nThanks!\r\n\r\n", "comments": ["Yes this is on purpose. There is no semantics on params. for example we cannot assume that each estimators params has a field 'learning_rate' or 'feature_column'. Framework developers depending on estimator should not rely on params.\r\nUser needs to control the construction of Estimator to play with hyper parameters.", "Sounds good. Thanks for the clarification!"]}, {"number": 13262, "title": "Training Mini-batches of data (without labels) for unsupervised learning problem", "body": "Hi Everyone,\r\n\r\nHas anyone trained mini-batches of data for an unsupervised learning problem? \r\nThe feed_dict uses the label and in an unsupervised setting. How do you overcome that? Could we use fake labels that never contribute to the loss function?\r\n\r\nBasically, I want to iterate over my huge dataset and then optimize a custom loss function. However, I couldn't figure out how to retain my training parameters (weights) when using a new mini-batch from the data explicitly.\r\n\r\nFor example, the whole dataset is 6000 points and the mini-batch size is 600.\r\nCurrently, for every mini-batch I could only use new independent weight parameters because the weights are initialized based on the data points from this mini-batch. When we optimize the loss over the first mini-batch of 600 data points, we get some optimized weights. How does one use these weights to optimize the next mini-batch of 600 data points and so on. The problem is we cannot use a shared global variable.\r\n\r\nAny help or pointers in this regard would be really appreciated. Thanks in advance!", "comments": ["Variables should maintain state between calls to `run` so unless you're reinitializing the weights between every minibatch, you shouldn't have an issue. An example demonstrating your problem would be helpful.\r\n\r\nAlso, this question would be better suited for Stack Overflow than a GH issue", "Hi Adam,\r\n\r\nI researched on stackoverflow forums but couldn't find anything relevant for mini-batches over unsupervised data. Hence, I posted here after a friend's suggestion. My apologies for the same.\r\n\r\n'f' is my whole dataset say text data of N points with dimension D\r\nU is cluster centroid with K clusters again of dimension D\r\n\r\nI define my variables as below:\r\n```\r\nF = tf.Variable(f.astype(np.float32), name='F') \r\nU = tf.Variable(u.astype(np.float32), name='U')\r\nFMod = tf.reshape(F, [N/K, K, D], name='FMod')\r\nUMod = tf.reshape(U, [1, K, D], name='UMod')\r\n```\r\n\r\nThen I define a custom loss or objective function as 'objective'\r\n\r\nNext I use an optimizer\r\n```\r\noptimizer = tf.train.AdamOptimizer(learning_rate)\r\ntrain_W = optimizer.minimize(objective, var_list=[F, U])\r\n```\r\n\r\nFinally, I evaluate the variable as\r\n```\r\nwith tf.Session() as sess:\r\n                \r\n    # Initialize all the variables\r\n    sess.run(init_op)\r\n\r\n    for n in range(noEpochs):\r\n\r\n        objval1 = sess.run([train_W, objective])\r\n\t\t\r\n```\r\n\r\nThe thing I am stuck at - is to iterate over batches of my data 'f' which is ultimately used in the optimizer train_W. If I have a for loop over these mini-batches, I will assign a new variable train_W for each of these iterations. How can I pass this value so that it can be used in the next mini-batch?\r\n \r\nThe full code can be found at this link: [Semi-supervised Clustering](https://github.com/vishalbhalla/semi-supervised-clustering/blob/master/semi-supervised-clustering/code/Semi-supervised%20Clustering.ipynb)\r\n", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 13261, "title": "Updating install_golang.sh - bumping to 1.9", "body": "", "comments": ["@ctava, thanks for your PR! By analyzing the history of the files in this pull request, we identified @asimshankar to be a potential reviewer.", "Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 13260, "title": "ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients,", "body": " I am green in a tensorflow\r\n\r\nI do not know where it is wrong\r\n\r\nhere is a warning\r\n\r\nValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"\", \"\r\n\r\nhere is my code\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport pandas as pd\r\nimport math\r\nfrom sklearn import tree\r\nfrom sklearn.cross_validation import train_test_split\r\n\r\n\r\nCOLUMNS = [\"CLD_A\",\"DDI\",\"DFS_A\",\"DFS_B\",\"deg1\",\"deg2\",\"y\"]\r\n\r\nfilename=\"D:/outputs/train.csv\"\r\nfile = pd.read_csv(filename, skipinitialspace=True,\r\n                       skiprows=0, names=COLUMNS)\r\n\r\n\r\nn_samples = file[['CLD_A','DDI','DFS_A','DFS_B','deg1','deg2']]\r\nn_samples=n_samples.as_matrix()\r\n\r\nn_features= file[['y']]\r\nn_features=n_features.as_matrix()\r\n\r\ntrain_X, test_X, train_y, test_y = train_test_split(n_samples, n_features, \r\ntest_size = 0.8,random_state=0)\r\n\r\n\r\nx=tf.placeholder(tf.int32,shape=[None, 6]) \r\ny=tf.placeholder(tf.int32,shape=[None, 1])\r\n\r\n\r\nl1 = tf.layers.dense(x, 10, tf.nn.relu)          # hidden layer\r\noutput = tf.layers.dense(l1, 1) \r\n\r\n\r\nloss = tf.losses.mean_squared_error(y, output)\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)\r\ntrain_op = optimizer.minimize(loss)\r\nwith tf.Session() as sess:\r\n  init=tf.global_variables_initializer()\r\n  sess = tf.Session()\r\n  sess.run(init)\r\nfor i in range(1000):\r\n    total_loss = 0\r\n    for i in range(len(train_X)):\r\n        feed={x:train_X,y:train_y}\r\n        sess.run(train_op,feed_dict={x:train_X,y:train_y})\r\n        if i%100==0:\r\n            print(sess.run(loss, feed ={x:train_X,y:train_y}))\r\n\r\n", "comments": ["Try asking here instead: https://stackoverflow.com/questions/tagged/tensorflow", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}]