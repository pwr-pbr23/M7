[{"number": 16201, "title": "Branch 182280342", "body": "", "comments": []}, {"number": 16200, "title": "Accepts `PathLike` objects for `model_dir`", "body": "* Retrieves the file system path representation if `PathLike` object is passed to `Estimator` or `RunConfig` for `model_dir`, instead of `str`.\r\n* Closes #15784", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please.", "Woops, force of habit."]}, {"number": 16199, "title": "Branch 182265266", "body": "", "comments": []}, {"number": 16198, "title": "Unable to build Tensorflow Benchmark model for Android", "body": "I've been trying to benchmark the model on mobile but I'm not able to build the model for Android. For desktop, I have been able to build and run the benchmark model.\r\n\r\nThe machine I'm using is a MacBook pro 15 inch with High Sierra and tensorflow v.1.4\r\n\r\nI've been following the directions given at the following links:\r\n\r\n(https://www.tensorflow.org/mobile/optimizing#how_to_profile_your_model\r\nhttps://github.com/tensorflow/tensorflow/tree/r1.4/tensorflow/tools/benchmark)\r\n\r\nEdit: Updated answer to the issue template\r\n\r\nHave I written custom code\r\n\r\n- No custom code was written\r\n\r\nOS Platform and Distribution\r\n\r\n- Mac OS High Sierra\r\n\r\nTensorFlow installed from\r\n\r\n- Tensorflow installed from source\r\n\r\nTensorFlow version\r\n\r\n- 1.4\r\n\r\nBazel version\r\n\r\n- Bazel version 0.9.0\r\n\r\nCUDA/cuDNN version\r\n\r\n- N/A\r\n\r\nGPU model and memory\r\n\r\n- N/A\r\n\r\nExact command to reproduce\r\n\r\n```\r\nbazel build -c opt --cpu=armeabi-v7a \\\r\n  --crosstool_top=//external:android/crosstool \\\r\n  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n  tensorflow/tools/benchmark:benchmark_model\r\n```\r\n\r\nError message received\r\n\r\n```\r\nERROR: /private/var/tmp/_bazel_abhisheksehgal/486c675b3107dc770b2281b905f670fe/external/highwayhash/BUILD:8:1: C++ compilation of rule '@highwayhash//:sip_hash' failed (Exit 1)\r\nIn file included from external/highwayhash/highwayhash/sip_hash.cc:15:\r\nIn file included from external/highwayhash/highwayhash/sip_hash.h:25:\r\nexternal/highwayhash/highwayhash/state_helpers.h:76:3: error: use of undeclared identifier 'static_assert'; did you mean 'static_cast'?\r\n  static_assert((kPacketSize & (kPacketSize - 1)) == 0, \"Size must be 2^i.\");\r\n  ^\r\nIn file included from external/highwayhash/highwayhash/sip_hash.cc:15:\r\nexternal/highwayhash/highwayhash/sip_hash.h:33:15: warning: alias declarations are a C++11 extension [-Wc++11-extensions]\r\n  using Key = HH_U64[2];\r\n              ^\r\nexternal/highwayhash/highwayhash/sip_hash.h:104:22: warning: alias declarations are a C++11 extension [-Wc++11-extensions]\r\nusing SipHashState = SipHashStateT<2, 4>;\r\n                     ^\r\nexternal/highwayhash/highwayhash/sip_hash.h:105:24: warning: alias declarations are a C++11 extension [-Wc++11-extensions]\r\nusing SipHash13State = SipHashStateT<1, 3>;\r\n                       ^\r\nexternal/highwayhash/highwayhash/sip_hash.cc:20:13: warning: alias declarations are a C++11 extension [-Wc++11-extensions]\r\nusing Key = highwayhash::SipHashState::Key;\r\n            ^\r\nexternal/highwayhash/highwayhash/sip_hash.cc:21:15: warning: alias declarations are a C++11 extension [-Wc++11-extensions]\r\nusing Key13 = highwayhash::SipHash13State::Key;\r\n              ^\r\n5 warnings and 1 error generated.\r\nTarget //tensorflow/tools/benchmark:benchmark_model failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 2.060s, Critical Path: 1.60s\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I have updated the original post with the issue template", "Hi, that code requires C++11 in order to build. Apparently TensorFlow also requires C++11 (at least on Windows: https://github.com/tensorflow/tensorflow/issues/10285).\r\nIs there some way you can add -std=c++11 to the compiler flags?", "Hi @jan-wassenberg, we can do that using cxxopts version but even that wasn't working.\r\n\r\nSo I built tensorflow from source again using Python 2.7 instead of 3.5 and changed my bazel version to 0.5.4 (the one they had mentioned in their installation guide) and then it started working. Also the android ndk I used was r14b and additionally in the build command I used a --config=monolithic.", "The original poster has replied to this issue after the stat:awaiting response label was applied."]}, {"number": 16197, "title": "add broadcasting to `softmax_cross_entropy_with_logits`", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yeah\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: pip3 binary\r\n- **TensorFlow version (use command below)**:\r\n```\r\n>>> tf.__git_version__\r\n'v1.4.0-rc1-11-g130a514'\r\n>>> tf.__version__\r\n'1.4.0'\r\n```\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: tf.nn.softmax_cross_entropy_with_logits(labels=tf.constant(1., shape=(2,)), logits=tf.constant(1., shape=(50,2)))\r\n\r\n\r\n### Describe the problem\r\nFEATURE REQUEST: `softmax_cross_entropy_with_logits` should broadcast, maybe?  I'm reading groups of data that all have the same label.  Seems a waste to have to replicate the label a gazillion times.\r\n\r\n### Source code / logs\r\n```\r\n>>> a = tf.nn.softmax_cross_entropy_with_logits(labels=tf.constant(1., shape=(2,)), logits=tf.constant(1., shape=(50,2)))\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/common_shapes.py\", line 686, in _call_cpp_shape_fn_impl\r\n    input_tensors_as_shapes, status)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Dimension 0 in both shapes must be equal, but are 50 and 1 for 'SoftmaxCrossEntropyWithLogits_3' (op: 'SoftmaxCrossEntropyWithLogits') with input shapes: [50,2], [1,2].\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py\", line 1783, in softmax_cross_entropy_with_logits\r\n    precise_logits, labels, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 4364, in _softmax_cross_entropy_with_logits\r\n    name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2958, in create_op\r\n    set_shapes_for_outputs(ret)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2209, in set_shapes_for_outputs\r\n    shapes = shape_func(op)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2159, in call_with_requiring\r\n    return call_cpp_shape_fn(op, require_shape_fn=True)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/common_shapes.py\", line 627, in call_cpp_shape_fn\r\n    require_shape_fn)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/common_shapes.py\", line 691, in _call_cpp_shape_fn_impl\r\n    raise ValueError(err.message)\r\nValueError: Dimension 0 in both shapes must be equal, but are 50 and 1 for 'SoftmaxCrossEntropyWithLogits_3' (op: 'SoftmaxCrossEntropyWithLogits') with input shapes: [50,2], [1,2].\r\n```\r\n", "comments": ["I'm not sure how likely the label is to be the same on multiple entries of the batch (which I assume that is the broadcast dimension you are thinking about).", "Perhaps it's safer to broadcast your labels by yourself explicitly. I'm afraid that the feature would be unexpected (or even wrong) for user in most cases. \r\n\r\n```python\r\nlabels=tf.constant(1., shape=(2,))\r\nlogits=tf.constant(1., shape=(50,2))\r\nlabels = labels + tf.zeros_like(logits)\r\ntf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\r\n```", "That's cool.  Seemed reasonable that broadcasting might generalize as a\nuseful feature for many binary operators, just like it is for basic math...\nI have a use case for broadcasting in this particular binary operation, but\nI'm just `.tile`ing my labels now, and it works just fine, so it's a minor\nefficiency point.\n\nJust in case solving a more general problem would solve this one:\n\nCan I hack broadcasting together with what currently exists?  Maybe a\n`broadcast` node would be useful?\n\nOn Jan 17, 2018 8:28 PM, \"Yan Facai (\u989c\u53d1\u624d)\" <notifications@github.com> wrote:\n\n> Perhaps it's safer to broadcast your labels explicitly. I'm afraid that\n> the feature would be unexpected (or even wrong) for user in most cases.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/16197#issuecomment-358517314>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABZE9O_nkt-E1E9gooCYvz9dG6YCAkEEks5tLqxCgaJpZM4Rh8bo>\n> .\n>\n", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "I think using `tf.tile` is the best solution for now, and adding broadcasting to `softmax_cross_entropy_with_logits` would make it easy to accidentally introduce bugs, as no error would be raised if the second dimension of `labels` was accidentally omitted. I don't see what a \"boradcast\" node would do that a `tf.tile` would not.\r\n\r\n/CC @ebrevdo, thoughts?", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "I agree that for now, tiling manually is the best approach. A recent change to broadcasting in a similar softmax op caused invisible/subtle errors.", "We answered this problem which is also related to architecture Understand this in following link\r\nhttps://stackoverflow.com/questions/46465925/input-to-reshape-is-a-tensor-with-37632-values-but-the-requested-shape-has-1505/52240509#52240509\r\nLet us know if you face any issue"]}, {"number": 16196, "title": "Fix issue of branch switching not working with bazel", "body": "This fix tries to address the issue raised in #15957 where bazel stops working after switching git branch, and reconfigure with `./configure` will not work as well.\r\n\r\nThis fix adds a quick fix as was suggested, by having `export TF_CONFIG_TIME=\"$(date)\" `in configure.py and add it to the environ list in git_configure.bzl.\r\n\r\nThis fix fixes #15957.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Another fix, which I pretty sure will work, is tell Bazel within the repo rule that it depends on .git/HEAD. Then whenever that file changes Im pretty sure the repo rule should be rerun. I think you can add something like...\r\n\r\nr = repository_ctx.execute([\"test\", \"-f\", \"%s/.git/logs/HEAD\" % tensorflow_root_path])\r\nif r.return_code == 0:\r\n    unused_var = repository_ctx.path(Label(\"//:.git/HEAD\"))  # pylint: disable=unused-variable\r\n\r\n...to the git_configure repo rule.", "@case540 Thanks for the help. I tested locally and it works as expected. The PR has been updated. Please take a look."]}, {"number": 16195, "title": "Fixing deprecated URL link", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 16194, "title": "Cannot interpret feed_dict key as Tensor: The name 'DecodeJpeg/contents:0' refers to a Tensor which does not exist. The operation, 'DecodeJpeg/contents', does not exist in the graph.", "body": "Hello,\r\n\r\nI try to get the output of each layer of my CNN. Here is the full example:\r\n```\r\n`from __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport argparse\r\nimport sys\r\nimport tempfile\r\nimport os\r\nimport DatasetReader as dr\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\n\r\nimport tensorflow as tf\r\nimport utils\r\nFLAGS = None\r\nPLOT_DIR = './output/plots'\r\n\r\n\r\ndef deepnn(x):\r\n\r\n  with tf.name_scope('reshape'):\r\n\r\n    x_image = tf.reshape(x, [-1, 100, 100, 1])#(x, [-1, 28, 28, 1])\r\n\r\n  # First convolutional layer - maps one grayscale image to 32 feature maps.\r\n  with tf.name_scope('conv1'):\r\n    W_conv1 = weight_variable([5, 5, 1, 32])#([5, 5, 1, 32])\r\n    b_conv1 = bias_variable([32])\r\n    # conv1dis = conv2d(x_image, W_conv1) + b_conv1\r\n    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\r\n    tf.add_to_collection('conv_weights', conv2d(x_image, W_conv1))\r\n\r\n  # Pooling layer - downsamples by 2X.\r\n  with tf.name_scope('pool1'):\r\n    h_pool1 = max_pool_2x2(h_conv1)\r\n\r\n  # Second convolutional layer -- maps 32 feature maps to 64.\r\n  with tf.name_scope('conv2'):\r\n    W_conv2 = weight_variable([5, 5, 32, 64])\r\n    b_conv2 = bias_variable([64])\r\n    # conv2dis = conv2d(h_pool1, W_conv2) + b_conv2\r\n    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\r\n    # tf.add_to_collection('conv_weights', h_conv2)\r\n\r\n  # Second pooling layer.\r\n  with tf.name_scope('pool2'):\r\n    h_pool2 = max_pool_2x2(h_conv2)\r\n\r\n  # Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image\r\n  # is down to 7x7x64 feature maps -- maps this to 1024 features.\r\n  with tf.name_scope('fc1'):\r\n    W_fc1 = weight_variable([25 * 25 * 64, 1024])\r\n    b_fc1 = bias_variable([1024])\r\n    h_pool2_flat = tf.reshape(h_pool2, [-1, 25*25*64])\r\n    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\r\n    # tf.add_to_collection('conv_weights', W_fc1)\r\n\r\n  # Dropout - controls the complexity of the model, prevents co-adaptation of\r\n  # features.\r\n  with tf.name_scope('dropout'):\r\n    keep_prob = tf.placeholder(tf.float32)\r\n    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\r\n\r\n  # Map the 1024 features to 10 classes, one for each digit\r\n  with tf.name_scope('fc2'):\r\n    W_fc2 = weight_variable([1024, 2])\r\n    b_fc2 = bias_variable([2])\r\n\r\n  y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\r\n\r\n  return y_conv, keep_prob\r\n\r\n\r\ndef conv2d(x, W):\r\n  \"\"\"conv2d returns a 2d convolution layer with full stride.\"\"\"\r\n  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\r\n\r\n\r\ndef max_pool_2x2(x):\r\n  \"\"\"max_pool_2x2 downsamples a feature map by 2X.\"\"\"\r\n  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\r\n                        strides=[1, 2, 2, 1], padding='SAME')\r\n\r\n\r\ndef weight_variable(shape):\r\n  \"\"\"weight_variable generates a weight variable of a given shape.\"\"\"\r\n  initial = tf.truncated_normal(shape, stddev=0.1)\r\n  return tf.Variable(initial)\r\n\r\n\r\ndef bias_variable(shape):\r\n  \"\"\"bias_variable generates a bias variable of a given shape.\"\"\"\r\n  initial = tf.constant(0.1, shape=shape)\r\n  return tf.Variable(initial)\r\n\r\n\r\ndef main(_):\r\n  # Import data\r\n  V0Dataset = dr.read_data_sets(FLAGS.data_dir, one_hot=True)\r\n\r\n  datasize = 10000\r\n  # Create the model\r\n  x = tf.placeholder(tf.float32, [None, datasize])#224*172])\r\n\r\n  # Define loss and optimizer\r\n  y_ = tf.placeholder(tf.float32, [None, 2])\r\n  print(\"logits shape {}\".format(y_))\r\n\r\n\r\n  # # Build the graph for the deep net\r\n  y_conv, keep_prob = deepnn(x)\r\n\r\n  with tf.name_scope('loss'):\r\n    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_,\r\n                                                            logits=y_conv)\r\n  cross_entropy = tf.reduce_mean(cross_entropy)\r\n\r\n  with tf.name_scope('adam_optimizer'):\r\n    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\r\n\r\n  with tf.name_scope('accuracy'):\r\n    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\r\n    correct_prediction = tf.cast(correct_prediction, tf.float32)\r\n\r\n  accuracy = tf.reduce_mean(correct_prediction)\r\n\r\n  print('cross_entropy {}'.format(cross_entropy))\r\n  print('accuracy {}'.format(accuracy))\r\n\r\n\r\n\r\n  with tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    for i in range(2):#500):\r\n      batch = V0Dataset.train.next_batch(10)\r\n      a = batch[1];\r\n      a = a.reshape(10,2)\r\n      train_step.run(feed_dict={x: batch[0], y_: a, keep_prob: 0.5})\r\n\r\n\r\n\r\n    graph_location = tempfile.mkdtemp()\r\n    print('Saving graph to: %s' % graph_location)\r\n    train_writer = tf.summary.FileWriter(\"/tmp/tensorflow/\")\r\n    train_writer.add_graph(tf.get_default_graph())\r\n\r\n    conv0 = sess.graph.get_tensor_by_name('conv1/Conv2D:0')\r\n    print(\"conv0 {}\".format(conv0))\r\n\r\n    predictions0 = sess.run(conv0,\r\n                           {'DecodeJpeg/contents:0': batch[0]}) # Error!!!!\r\n    print(\"predictions0 {}\".format(predictions0))\r\n    print(\"predictions0 {}\".format(predictions0.size))\r\n\r\n```\r\n\r\nHere are the errors I get:\r\n```\r\n`Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1064, in _run\r\n    allow_operation=False)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3035, in as_graph_element\r\n    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3077, in _as_graph_element_locked\r\n    \"graph.\" % (repr(name), repr(op_name)))\r\nKeyError: \"The name 'DecodeJpeg/contents:0' refers to a Tensor which does not exist. The operation, 'DecodeJpeg/contents', does not exist in the graph.\"\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"./deep_charging_station_train.py\", line 309, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"./deep_charging_station_train.py\", line 297, in main\r\n    {'DecodeJpeg/contents:0': batch[0]})\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1067, in _run\r\n    + e.args[0])\r\nTypeError: Cannot interpret feed_dict key as Tensor: The name 'DecodeJpeg/contents:0' refers to a Tensor which does not exist. The operation, 'DecodeJpeg/contents', does not exist in the graph.\r\n`\r\n\r\n\r\n\r\n```\r\n\r\nI don't understand why this appends. I looked with Tensorboard I don't know where should I get the DecodeJpeg informations of the layer\r\n\r\nEdit:\r\nHave I written custom code : I use deep mnist tutorial example and I modify the size of the input image\r\nOS Platform and Distribution : Ubuntu 16.04\r\nTensorFlow installed from\r\nTensorFlow version 1.4.0\r\nBazel version N/A\r\nCUDA/cuDNN version N/A\r\nGPU model and memory N/A\r\nExact command to reproduce\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "In the line where the error happens the data I use are raw grayscale image ( array of 100x100 float) : \r\n`predictions0 = sess.run(conv0,\r\n                           {'DecodeJpeg/contents:0': batch[0]}) # Error!!!!`\r\nI think this is the reason why the DecodeJpeg/contents:0 doesn't works.\r\n\r\nHow could I run the session with raw  array of 100x100 float ?\r\n", "Issue already had a thread in #12250", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "@xav12358 does the resolution in that issue solve your problem?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "I forget to but an d\u00e9coder on input. Now it works. Thanks you", "can any one who have sloved this problem help me ?", "my solution is to use this loop to find the tensor name of the graph, to see if \"DecodeJpeg/contents\" is the first tensor in the graph, my first tensor is \"Placeholder\". use {'Placeholder:0': image_data}  to replace {'DecodeJpeg/contents:0': image_data})\r\n\r\ntensor_name_list = [tensor.name for tensor in tf.get_default_graph().as_graph_def().node]\r\nfor tensor_name in tensor_name_list:\r\nprint(tensor_name, '\\n')", "@yufengjinsong \r\nI tried your solution but it says error again as:\r\n\"ValueError: could not convert string to float: b'\\xff\\\"\r\nat line:\r\n    predictions = sess.run(softmax_tensor, {'Placeholder:0': image_data})", "@bipika Do your problem have been solved?I meet the same."]}, {"number": 16193, "title": "Performance issues with TF1.5 on CPU", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.5.0-rc1\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.5.4\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**:\r\n\r\nHello,\r\nI'm facing performance issues with the last releases of TF using a CPU.\r\nI'm using the [benchmark tool](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/benchmark/benchmark_model.cc) to calculate mean inference time of a model.\r\n\r\nFor example, in order to evaluate mobilenet (trained on a custom dataset), I'm using this command :\r\n`bazel-bin/tensorflow/tools/benchmark/benchmark_model --graph=\"path to mobilenet graph\" --input_layer=\"input\" --input_layer_shape=\"1,224,224,3\" --input_layer_type=\"float\" --output_layer=\"MobilenetV1/Predictions/Reshape_1\"`\r\nAfter setting CUDA_VISIBLE_DEVICES to \"\" in order to run on CPU.\r\n\r\nWith TF 1.4.1, I obtain a mean inference time equals to 26ms (13ms if I compile with optimization flags).\r\nUsing tf 1.5.*, I obtain a mean inference time equals to 51ms (45ms if I compile with optimization flags).\r\n\r\nThe loss is very important, so I'm wondering if it's a known issue and how I can improve this.\r\n\r\nI tried with tags/v1.5.0-rc0, tags/v1.5.0-rc1 and master, and the problem is the same.\r\n\r\nThank you", "comments": ["@tfboyd, could you or somebody you know take a look? This seems somewhat worrying.", "Taking a look, I do not doubt the results but I am reproducing them this morning.  ", "@Pelups :  Could you either share the exact graph file you used and/or test with the graph I used below?  I am getting contradictory results and while I try different setups it would help if I was testing the exact same setup.  Also what CPU setup are you using, the CPU model code is the easiest way to share that.\r\n\r\nI am still investigating and it is possible I am running the test incorrectly, there are a lot of results from the script and I believe I am interpreting them correctly but maybe not.  On an AWS m4.4xlarge 16 vCPUs at 2.3 Ghz (broadwell) I am seeing different results and I want to keep looking as I find it odd my results are so different.  If you have the exact graph you were using that would help.   Here is the test that I ran.\r\n\r\nI used the following graphs:\r\n- [mobilenetv1](http://download.tensorflow.org/models/mobilenet_v1_1.0_224_frozen.tgz) \r\n- [inception](https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip)\r\n\r\nI ran mobilenet with:  \r\n```bash\r\n./benchmark_model --graph=/data/mobilenet_v1_1_224/mobilenet_v1_1.0_224/frozen_graph.pb --input_layer=\"input\" --output_layer=\"MobilenetV1/Predictions/Reshape_1\"\r\n```\r\n**For Inception** there was no difference between 1.4.1 and 1.5RC1\r\n\r\n- 1.4.1: **32.7ms**  2018-01-18 22:30:25.591497: I tensorflow/core/util/stat_summarizer.cc:468] Timings (microseconds): count=450 first=32455 curr=32522 min=30880 max=36377 avg=32680.5 std=703\r\n- 1.5RC1: **32.5ms**   2018-01-18 22:43:14.819298: I tensorflow/core/util/stat_summarizer.cc:468] Timings (microseconds): count=451 first=33216 curr=33187 min=31072 max=37290 avg=32510 std=768\r\n\r\n\r\n**For mobilenetv1** the results were perplexing and I am digging in.  1.5RC1 was faster.  I double checked that I did not compile in AVX by looking at the logs.\r\n\r\n**All Threads (16)**\r\n\r\n- 1.4.1: **32.4ms** 2018-01-19 00:01:13.230477: I tensorflow/core/util/stat_summarizer.cc:468] Timings (microseconds): count=308 first=34539 curr=32567 min=30796 max=34958 avg=32374.9 std=650\r\n- 1.5RC1: **24.9ms**  2018-01-18 23:57:49.456252: I tensorflow/core/util/stat_summarizer.cc:468] Timings (microseconds): count=400 first=24113 curr=24401 min=23805 max=26506 avg=24910.6 std=524\r\n- 1.5.RC1  **17.8ms** AVX2 compiled with broadwell running on the Docker.  Weird and repeated as outside Docker AVX2 is consistent 21ms.\r\n\r\n**1 Thread**\r\n\r\n- 1.4.1: **179ms**  2018-01-19 00:42:10.455322: I tensorflow/core/util/stat_summarizer.cc:468] Timings (microseconds): count=56 first=179066 curr=178621 min=177166 max=180046 avg=178665 std=574\r\n- 1.5RC1: **106ms**  2018-01-19 00:12:49.710419: I tensorflow/core/util/stat_summarizer.cc:468] Timings (microseconds): count=94 first=106362 curr=106541 min=105246 max=108088 avg=106450 std=356\r\n- 1.5.RC1  **52ms** AVX2 compiled with broadwell running on the Docker. \r\n\r\nTesting outside docker (still testing)\r\nmobilenetv1:\r\nAll Threads (16):\r\n- 1.5.RC1  **21ms** AVX2 compiled with broadwell (consistent)\r\n- 1.4.1 **31ms** SSE3\r\n\r\nOne Thread:\r\n- 1.5.RC1  **56.3ms** AVX2 compiled with broadwell\r\n- 1.4.1 **178ms** SSE3\r\n\r\n\r\n**Extra info for tracking:**\r\n- I used Bazel 0.5.4 for 1.4.1 and Bazel 0.9 for 1.5RC1.  I doubt the version of bazel matters\r\n- I built for CPU only in a Docker.  bazel --config=opt\r\n- I have the log files so I have everything that was output\r\n- I ran the tests a few times and did not average them I just confirmed the results were not jumping around and took one.  Not 100% scientific.  \r\n\r\nWe are currently holding the 1.5 build as we investigate further.  \r\n", "Hello! May I know what CPU are you currently using??", "@asp143  Just in case you were referencing me.  I am using AWS m4.4xlarge 16 vCPUs which is a  2.3 Ghz (broadwell) is CPU model  Intel Xeon E5-2686 v4 (Broadwell). ", "@tfboyd Did you recreate the problem? I think this has something with the intel kernel issue", "Hello !\r\nThank you @tfboyd  for your answer.\r\nI really don't understand what happened when I ran my evaluation, because after runing it again, I obtain quite the same results than yours.\r\nI did exactly the same installation/compilation steps than previously ...\r\nMaybe a process was runing on the computer's cpu, lowering the performances of the evaluation.\r\n\r\nSo, thank you for your answer, sorry for your time.", "@Pelups   Not a problem at all and I mean that completely.  Your work showed you tried a few things or I would not have taken you seriously.  We also should have a nightly/hourly tests with this script on desktop CPUs, we have a bunch on android devices.  If it was python I would setup the test myself in my \"rogue\" test suite.  Thank you for retesting so quickly.  Please do not hesitate to report issue.    "]}, {"number": 16192, "title": "how to set ignore_label in tensorflow?", "body": "when i set the label=-1, there is an error: Received a label value of -1 which is outside the valid range of [0, 8)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code       yes\r\nOS Platform and Distribution     centos6\r\nTensorFlow installed from         github\r\nTensorFlow version                   1.4\r\nBazel version    n/a\r\nCUDA/cuDNN version 8.0\r\nGPU model and memory n/a\r\nExact command to reproduce n/a", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "# read images, labels\r\ndef read_img(path):\r\n    imgs = []\r\n    labels = []\r\n    #label = []\r\n for line in open(path):\r\n        label = []\r\n        print(line)\r\n        line = line.replace(\"\\n\",\"\")\r\n        strs = line.split('\\t')\r\n        imgs.append(strs[0])\r\n        label.append(strs[1])\r\n        label.append(strs[2])\r\n                 if str(temp_str[1]) == \"-1\":\r\n                    label.append(\"0\")\r\n                else:\r\n                    label.append(\"1\")\r\n                if str(temp_str[2]) == \"-1\":\r\n                    label.append(\"0\")\r\n                else:\r\n                    label.append(\"1\")\r\n        labels.append(np.asarray(label, np.int32))\r\n    return np.asarray(imgs, np.string_), np.asarray(labels, np.int32)\r\n \r\n \r\n\uff083\uff09split labels and ignore_weight\r\n  y_ = tf.squeeze(tf.slice(y_all, [0, 0], [-1, 1]))\r\n  y_gb_ = tf.squeeze(tf.slice(y_all, [0, 1], [-1, 1]))\r\n    w_y = tf.squeeze(tf.slice(y_all, [0, 2], [-1, 1]))      #label1 weight\r\n    w_y_gb = tf.squeeze(tf.slice(y_all, [0, 3], [-1, 1]))  #label2 weight\r\n\uff084\uff09use tf.losses.sparse_softmax_cross_entropy\r\n        y_s_ = y_ * w_y\r\n        y_gb_s_ = y_gb_ * w_y_gb\r\n        y, y_gb = net.squeezenet(x, True, 0.999, CLASSES, CLASSES_FOOD)\r\n        \r\n        cross_entropy_gb = tf.losses.sparse_softmax_cross_entropy(labels=y_gb_s_, logits=y_gb, weights=w_y_gb)\r\n         \r\n        cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=y_s_, logits=y, weights = w_y)\r\n       \r\n        self.cost = tf.reduce_mean(cross_entropy, name='cross_entropy_loss') + tf.reduce_mean(cross_entropy_gb, name='cross_entropy_loss_gb')"]}, {"number": 16191, "title": "Fix docstring typo of losses_impl.py", "body": "Add missing \"`\" to the docstring.", "comments": ["Can one of the admins verify this patch?", "This kind of typos seems to be common in the code base, maybe we could consider using some tools to find them.", "@qmick Good point. I agree. cc @av8ramit @gunan @yifeif"]}, {"number": 16190, "title": "ValueError: Inputs to `Dense` should have rank >= 2.", "body": "##error\r\n\r\nTraceback (most recent call last):\r\n  File \"firstGANtf.py\", line 90, in <module>\r\n    G = Generator(input_size=g_input_size, hidden_size=g_hidden_size, output_size=g_output_size)\r\n  File \"firstGANtf.py\", line 55, in __init__\r\n    self.map1 = tf.contrib.layers.linear(inputs=input_size , num_outputs=hidden_size)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 177, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 1409, in fully_connected\r\n    outputs = layer.apply(inputs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/layers/base.py\", line 303, in apply\r\n    return self.__call__(inputs, **kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/layers/base.py\", line 269, in __call__\r\n    self.build(input_shapes[0])\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/layers/core.py\", line 110, in build\r\n    raise ValueError('Inputs to `Dense` should have rank >= 2.')\r\nValueError: Inputs to `Dense` should have rank >= 2.\r\n\r\n\r\n##code\r\n\r\n       self.map1 = tf.contrib.layers.linear(inputs=input_size , num_outputs=hidden_size)\r\n        self.map2 = tf.contrib.layers.linear(inputs=hidden_size,  num_outputs=hidden_size)\r\n        self.map3 = tf.contrib.layers.linear(inputs=hidden_size, num_outputs=output_size)", "comments": ["Running into similar issues for sometime now. Any help would be appericiated", "I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.", "I think `tf.contrib.layers.linear` has been removed in the latest version. And for functional interface, you should use `inputs`, rather than `inputs_size`.  \r\n\r\nTake dense layer for example, `tf.layers.Dense` is the layer class, and its functional interface named ` tf.layers.dense`.  The layer class uses `input_shape` to build it, while functional interface requires `inputs`.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Closing due to inactivity."]}, {"number": 16189, "title": "Will tf-Lite have GPU support , if the answer is yes ,the compute API will be which one ,OpenCL or gles ?", "body": "", "comments": ["Yes, we plan to have some method for  targeting the GPU. On Android OpenCL is not well supported, so it would likely be through GLES. Also, the NN API will be providing abstraction to various hardware acceleration backends (including possibly GPUs). Stay posted for more information.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing this issue, as the question was answered, and we are working on this independently of this issue. Thanks!", "Just wonder what's the ETA for the support of GPU in TF lite?", "Yep, still wondering."]}, {"number": 16188, "title": "benchmark_model tool not build successfully for android version", "body": "Hello,\r\n\r\nI try to build the benchmark_model for the android, but I encounter some errors.\r\nPlease help, is any setting not correct?\r\n\r\nThe configuration of the SDK and NDK in the WORKSPACE is\r\nandroid_sdk_repository(\r\n    name = \"androidsdk\",\r\n    api_level = 23,\r\n    build_tools_version = \"26.0.1\",\r\n    path = \"/home/kk/android_sdk/android-sdk-linux\",\r\n)\r\n\r\nandroid_ndk_repository(\r\n    name=\"androidndk\",\r\n    path=\"/home/kk/android_sdk/ndk/android-ndk-r14\",\r\n    api_level=14)\r\n\r\nUse the command to build:\r\nbazel build --cxxopt='--std=c++11' -c opt \r\n--crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain tensorflow/tools/benchmark:benchmark_model\r\n\r\nThere are three errors\r\n1. external/gif_archive/lib/openbsd-reallocarray.c:33:19: error: use of undeclared identifier 'SIZE_MAX'\r\n2. tensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc:172:53: error: no member named 'nanf' in namespace 'std'; did you mean simply 'nanf'?\r\n3. external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: cannot find -lpthread\r\n\r\nThanks\r\n\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04.4 LTS\r\n- **TensorFlow installed from (source or binary)**:use the pip install\r\n- **TensorFlow version (use command below)**:1.4.0\r\n- **Python version**: Python 2.7.6\r\n- **Bazel version (if compiling from source)**:0.9.0\r\n- **GCC/Compiler version (if compiling from source)**:(Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4\r\n- **CUDA/cuDNN version**:NA\r\n- **GPU model and memory**:NA\r\n- **Exact command to reproduce**:NA\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Hi All,\r\nI meet the same issue.\r\nNeed your help.\r\nThanks.", "Increase NDK api level and add '--config monolithic' may help", "HI freedomtan,\r\nIt's work.\r\nThanks your kindly help.", "Thanks for freedomtan.\r\n\r\nYour comment is work, and I can build pass.\r\n\r\nI don't increase the NDK api level, only add '--config monolithic'."]}, {"number": 16187, "title": "Faster R-CNN: too many resources requested for launch", "body": "I am trying to deploy the pretrained Faster-RCNN Inception V2 from the object detection API on a Jetson TX2. \r\nI am running CUDA 8, cuDNN 6 and have tested with both TF 1.3 and 1.5 in a Jupyter Notebook environment. \r\nWhen I monitor the GPU memory it starts out by having 4.8 GB free and when launching these fills up immediately. When I run on my GTX1060 6 GB GPU I have effectively the same amount of memory free but are having no issues running.\r\nSmaller models as SSD MobileNet runs without problems.\r\n\r\nFrom tests performed today, I can supply the following dumps.\r\n\r\nJupyter Notebook terminal output:\r\n\r\n```\r\n2018-01-17 16:16:19.584106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:859] ARM64 does not support NUMA - returning NUMA node zero\r\n2018-01-17 16:16:19.584261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1206] Found device 0 with properties: \r\nname: NVIDIA Tegra X2 major: 6 minor: 2 memoryClockRate(GHz): 1.3005\r\npciBusID: 0000:00:00.0\r\ntotalMemory: 7.67GiB freeMemory: 4.97GiB\r\n2018-01-17 16:16:19.584312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Adding visible gpu device 0\r\n2018-01-17 16:16:20.824479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:987] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4437 MB memory) -> physical GPU (device: 0, name: NVIDIA Tegra X2, pci bus id: 0000:00:00.0, compute capability: 6.2)\r\n2018-01-17 16:17:09.816477: E tensorflow/stream_executor/cuda/cuda_driver.cc:1080] failed to synchronize the stop event: CUDA_ERROR_LAUNCH_FAILED\r\n2018-01-17 16:17:09.816703: E tensorflow/stream_executor/cuda/cuda_timer.cc:54] Internal: error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED\r\n2018-01-17 16:17:09.816771: E tensorflow/stream_executor/cuda/cuda_timer.cc:59] Internal: error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED\r\n2018-01-17 16:17:09.816912: E tensorflow/stream_executor/cuda/cuda_dnn.cc:2456] failed to enqueue convolution on stream: CUDNN_STATUS_EXECUTION_FAILED\r\n2018-01-17 16:17:10.174651: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED\r\n2018-01-17 16:17:10.174772: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED\r\n2018-01-17 16:17:10.174806: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED\r\n2018-01-17 16:17:10.174836: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED\r\n2018-01-17 16:17:10.174865: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED\r\n```\r\n\r\nError dump from printout inside the notebook:\r\n\r\n```\r\nException in thread Thread-4:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib/python2.7/threading.py\", line 754, in run\r\n    self.__target(*self.__args, **self.__kwargs)\r\n  File \"<ipython-input-5-a51933cd03d8>\", line 19, in worker\r\n    im, t_elapsed = detect_objects(frame_rgb, sess, detection_graph)\r\n  File \"<ipython-input-4-6c8da66803e2>\", line 19, in detect_objects\r\n    feed_dict={image_tensor: image_np_expanded})\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1128, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1344, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1363, in _do_call\r\n    raise type(e)(node_def, op, message)\r\nInternalError: cuDNN launch failure : input shape([1,64,138,256]) filter shape([3,3,64,192])\r\n\t [[Node: FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_2c_3x3/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_2b_1x1/Relu, FirstStageFeatureExtractor/InceptionV2/Conv2d_2c_3x3/weights/read/_47__cf__53)]]\r\n\t [[Node: BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal/_883 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_10508...ield/Equal\", tensor_type=DT_BOOL, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopBatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/non_max_suppression/iou_threshold/_1)]]\r\n\r\nCaused by op u'FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_2c_3x3/Conv2D', defined at:\r\n  File \"/usr/lib/python2.7/threading.py\", line 774, in __bootstrap\r\n    self.__bootstrap_inner()\r\n  File \"/usr/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib/python2.7/threading.py\", line 754, in run\r\n    self.__target(*self.__args, **self.__kwargs)\r\n  File \"<ipython-input-5-a51933cd03d8>\", line 10, in worker\r\n    tf.import_graph_def(od_graph_def, name='')\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 316, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 548, in import_graph_def\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3176, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1617, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInternalError (see above for traceback): cuDNN launch failure : input shape([1,64,138,256]) filter shape([3,3,64,192])\r\n\t [[Node: FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_2c_3x3/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_2b_1x1/Relu, FirstStageFeatureExtractor/InceptionV2/Conv2d_2c_3x3/weights/read/_47__cf__53)]]\r\n\t [[Node: BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal/_883 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_10508...ield/Equal\", tensor_type=DT_BOOL, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopBatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/non_max_suppression/iou_threshold/_1)]]\r\n```\r\n\r\nOutput of tegrastats at the point of error:\r\n```\r\nRAM 3151/7851MB (lfb 915x4MB) cpu [2%@345,100%@2034,99%@2034,1%@348,3%@348,6%@349] EMC 5%@1866 APE 150 GR3D 0%@114\r\nRAM 3151/7851MB (lfb 915x4MB) cpu [0%@345,100%@1981,100%@1988,3%@348,5%@348,4%@349] EMC 5%@1866 APE 150 GR3D 0%@114\r\nRAM 3152/7851MB (lfb 915x4MB) cpu [2%@345,100%@2021,100%@2021,4%@348,5%@348,2%@349] EMC 5%@1866 APE 150 GR3D 0%@114\r\nRAM 3152/7851MB (lfb 915x4MB) cpu [2%@345,100%@2035,100%@2034,3%@349,4%@348,2%@348] EMC 5%@1866 APE 150 GR3D 0%@114\r\nRAM 3152/7851MB (lfb 915x4MB) cpu [1%@345,100%@2016,100%@2019,2%@345,1%@349,3%@348] EMC 5%@1866 APE 150 GR3D 0%@114\r\nRAM 3181/7851MB (lfb 898x4MB) cpu [21%@806,100%@2021,56%@2024,8%@499,10%@500,3%@500] EMC 5%@1866 APE 150 GR3D 24%@114\r\nRAM 3210/7851MB (lfb 887x4MB) cpu [8%@345,100%@2018,32%@2026,7%@345,24%@345,13%@349] EMC 5%@1866 APE 150 GR3D 99%@114\r\nRAM 3327/7851MB (lfb 838x4MB) cpu [2%@1573,100%@1987,31%@1992,35%@1574,13%@1575,5%@1573] EMC 5%@1866 APE 150 GR3D 8%@114\r\nRAM 3578/7851MB (lfb 758x4MB) cpu [19%@1806,100%@2080,0%@2035,7%@2035,2%@2035,56%@1727] EMC 5%@1866 APE 150 GR3D 10%@114\r\nRAM 3732/7851MB (lfb 715x4MB) cpu [2%@345,100%@2034,83%@2035,5%@348,21%@345,2%@346] EMC 7%@1866 APE 150 GR3D 99%@624\r\nRAM 3732/7851MB (lfb 715x4MB) cpu [94%@2036,100%@2035,97%@2034,87%@1987,13%@2035,1%@2035] EMC 4%@1866 APE 150 GR3D 43%@1032\r\nRAM 3659/7851MB (lfb 727x4MB) cpu [2%@653,81%@2022,20%@2027,28%@652,2%@655,4%@655] EMC 3%@1866 APE 150 GR3D 0%@114\r\nRAM 3661/7851MB (lfb 727x4MB) cpu [1%@345,100%@2033,0%@2035,1%@346,2%@348,3%@349] EMC 3%@1866 APE 150 GR3D 0%@114\r\nRAM 3661/7851MB (lfb 727x4MB) cpu [2%@345,100%@2035,0%@2034,0%@348,3%@348,0%@348] EMC 2%@1866 APE 150 GR3D 0%@114\r\nRAM 3661/7851MB (lfb 727x4MB) cpu [3%@345,100%@2034,0%@2035,1%@348,1%@348,3%@348] EMC 2%@1866 APE 150 GR3D 0%@114\r\nRAM 3661/7851MB (lfb 727x4MB) cpu [2%@345,100%@2034,0%@2034,2%@348,4%@348,1%@348] EMC 2%@1866 APE 150 GR3D 0%@114\r\nRAM 3661/7851MB (lfb 727x4MB) cpu [4%@345,100%@1988,0%@1987,2%@346,2%@345,1%@345] EMC 2%@1866 APE 150 GR3D 9%@114\r\nRAM 3661/7851MB (lfb 727x4MB) cpu [4%@345,100%@2026,0%@2026,1%@347,0%@348,3%@348] EMC 2%@1866 APE 150 GR3D 0%@114\r\nRAM 3661/7851MB (lfb 727x4MB) cpu [8%@345,100%@2024,0%@2028,5%@345,8%@345,1%@345] EMC 2%@1866 APE 150 GR3D 0%@114\r\n```\r\nAs you can see the RAM are nowhere near full at the moment of the error.\r\n\r\nCan anybody suggest a solution to this?", "comments": ["@zheng-xq, could you comment?\r\n@erwincoumans, I know you have been playing with these boards. DO you have anythoughts?", "The problem is that a Cuda kernel has failed for some reason. The error indicates and cudnn call for conv has failed. Adding @benbarsdell from NVIDIA in case he notices something. ", "I have tested both with CUDA 8.0 and 9.0 along with cuDNN 6.0 and 7.0 in TF 1.3 and 1.5.\r\nI have no problems running any of the SSD nets from the model zoo. But I kind of need the accuracy of a Faster R-CNN net.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "@benbarsdell any updates here? Thanks.", "Marking as contributions welcome for now, since no one on TensorFlow is actively working on Jetson TX2 support (@benbarsdell works for NVidia).\r\n\r\nHaving a small code fragment to reproduce the issue would be helpful, but I understand its frustrating considering object detection is in the official tensorflow/models repo.", "@reedwm to reproduce the issue, you need nothing more than trying to load a Faster R-CNN model in TF on a Tegra (Jetson) device. ", "@JesperChristensen89 I also got this problem.  Have you solved it? any progress? ", "@samuel1208 No, unfortunately not. Let me know if you have any progress on the issue.", "Same problem happened to me. If this cannot be solved, MaskRCNN cannot be used on Jetson TX2 either. Any progress?", "also hanging on this error for a while.\r\nMy mask rcnn consumes 1.8GB in total, but is not able to run on the Jetson TX2 gpu, only on cpu.", "I finally make it work on tx2 but using mxnet....said story\r\n", "Any updates on the same? ", "Hi @JesperChristensen89! \r\nWe are checking to see if you still need help in this issue .Have you tried latest stable version TF 2.6  yet? Please create a new issue if the issue is replicating in the latest version. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/16187\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/16187\">No</a>\n"]}, {"number": 16186, "title": "A bug when applying MultiRNNCell?", "body": "\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: This code is very similar to an official example\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: pip install tensorflow\r\n- **TensorFlow version (use command below)**: b'unknown' 1.4.0\r\n- **Python version**: Python 3.5.2 :: Anaconda 4.2.0 (64-bit)\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\ntf.nn.MultiRNNCell sometimes doesn't work.\r\n\r\nIt raises an issue like this:\r\nValueError: Dimensions must be equal, but are 64 and 96 for 'lstm/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [128,64], [96,128].\r\n\r\n### Source code / logs\r\n  import tensorflow as tf\r\n  import numpy as np\r\n\r\n  hidden_layer_size = 32\r\n  embed = tf.zeros((128, 6, 64), dtype=tf.float32)\r\n\r\n  num_LSTM_layers = 2\r\n  with tf.variable_scope(\"lstm\"):\r\n    \r\n    lstm_cell = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size, forget_bias=1.0)\r\n    cell = tf.contrib.rnn.MultiRNNCell(cells=[lstm_cell]*num_LSTM_layers, state_is_tuple=True)\r\n    outputs, states = tf.nn.dynamic_rnn(cell, embed, dtype=tf.float32)\r\n   \r\nError:\r\nValueError: Dimensions must be equal, but are 64 and 96 for 'lstm/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [128,64], [96,128].\r\n\r\n", "comments": ["I had the same problem and found how to solve it.\r\nYou may change the highlighted code as below.\r\n```\r\ndef lstm_cell():\r\n   lstm = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size, forget_bias=1.0)\r\n   return lstm\r\ncell = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(num_LSTM_layers)])\r\noutputs, states = tf.nn.dynamic_rnn(cell, embed, dtype=tf.float32)\r\n```", "As said by @peytonhong ,`[lstm_cell] * num_LSTM_layers` all refer to the same object `lstm_cell`. The multiplication operator on list doesn't copy its elements. So it's not a bug, right?", "No, I don't think it is a bug. The problem was came from a misunderstanding about the structure of TensorFlow.", "Hi, I encountered the same problem. After searching issues, I found many users have the same difficulties to construct MultiCellRNN correctly. So I did some code trace and explained what I understood from code in this Stackflow post https://stackoverflow.com/questions/48865554/using-dynamic-rnn-with-multirnn-gives-error/49066981#49066981  \r\nI hope this might help future users have the same problem with more clear understanding and also would like invite developers  to correct me if there are something wrong in my explanation. Thanks!"]}, {"number": 16185, "title": "add LU operation and return factorized matrices, related to #6992", "body": "This should be related to #6992 for mccajm request \"It would be nice to expose the decomposition, like tf.cholesky_solve, so that it can be reused on the next solve if A doesn't change.\" \r\n\r\ntf.lu op is added. L, U, P = tf.lu(A), where A = P^-1 LU\r\nThe test case script is added to python/kernel_test \r\n\r\n", "comments": ["Can one of the admins verify this patch?", "The related issue seems to be #6992 not #6692.", "@qmick thanks. Typo in the description and fixed.", "@zhuangh please rebase to resolve the conflicts.", "@rmlarsen sure. I will do it after finishing the PartialPivotLU. I am still working on it for the permutation matrix (vector) and testing. \r\n\r\nBTW, did you get the chance to get the information regarding our last discussion \"... that it might be convenient to return a status instead of having the user check the diagonal again (which is not a trivial amount of time compared to the op itself if the matrix is small). Let me discuss with my colleagues how to go about it.\"\r\n\r\nThanks", "@zhuangh I think it would be helpful to return a scalar integer containing the index of the first zero diagonal  in U, similar to the INFO argument in LAPACK function xGETRF.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@tensorflowbutler @rmlarsen sorry for the late, I am still working on the requirements from the review and merging the code conflicts. Please give me more time. Thanks.", "@rmlarsen just FYI, the new commit only fixed the conflicts and test bench errors for the new partialPivLU during merging the master branch yesterday. You do not need to review it at this moment if you do not have time since several features are still on-going.\r\n\r\nI will push new changes for (1) change permutation matrix to indices. (2) add test cases for the singular matrix, etc. (3) check the instability inside LU_op.\r\n", "@zhuangh Thanks for the update. Looking forward to your changes.", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@zhuangh Just checking in. Are you still planning to finish this?", "Hi @rmlarsen Yes. But I have been quite busy with my work since the last commit I pushed, and have not allocated enough time to finish your review comments. Hope this can be finished in the late May. Or let me know if you have any suggestion or concern. Thank you.", "It has been 28 days that this pull-request has stalled. Please create a new pull-request with the requested changes.", "Hi @rmlarsen \r\n\r\nNot sure where I should post the following question (let me know if this is not the good place).\r\n\r\nI made some code change to translate the permutation matrix to indices. Code got complied but failed at the test part.\r\n\r\nThe error \"tensorflow/core/framework/tensor.cc:617] Check failed: dtype() == expected_dtype (3 vs. 1)\"\r\n\r\nDetail log https://docs.google.com/document/d/174PI43b40Lnca6goHnp-yhghj1N7QJlSG9NqY7vHm1g/edit\r\n\r\nI suspect ```outputs->at(2) = perm.cast<Scalar>()``` in  \r\nhttps://github.com/zhuangh/tensorflow/blob/8b304e22bba7f8ccab137d2985e25fcfef6ba3bd/tensorflow/core/kernels/lu_op.cc#L75-L82\r\n \r\nhas a conflict with \"Tperm\" in\r\n\r\n```\r\nREGISTER_OP(\"Lu\")\r\n    .Input(\"input: T\")\r\n    .Output(\"l: T\")\r\n    .Output(\"u: T\")   \r\n    .Output(\"p: Tperm\")\r\n    .Attr(\"T: {double, float, complex64, complex128}\")\r\n    .Attr(\"Tperm: {int32, int64} = DT_INT32\")\r\n    .SetShapeFn(LuShapeFn);\r\n```\r\nin\r\nhttps://github.com/zhuangh/tensorflow/blob/8b304e22bba7f8ccab137d2985e25fcfef6ba3bd/tensorflow/core/ops/linalg_ops.cc#L363\r\n\r\nIf I turned ```outputs->at(2) = perm.cast<Scalar>()``` into ```outputs->at(2) = perm.cast<int64>()```, the complication failed due to \r\n\"external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/AssignEvaluator.h:834:3: error: static assertion failed: YOU_MIXED_DIFFERENT_NUMERIC_TYPES__YOU_NEED_TO_USE_THE_CAST_METHOD_OF_MATRIXBASE_TO_CAST_NUMERIC_TYPES_EXPLICITLY\r\n   EIGEN_CHECK_BINARY_COMPATIBILIY(Func,typename ActualDstTypeCleaned::Scalar,typename Src::Scalar);\r\n\"\"\r\n\r\nThe detail log: https://docs.google.com/document/d/1GtozEITwvVNYGJsK6yqS0ybuh6BuS53pmjklgVVfq8U/edit\r\n\r\n**So my question, what is a good way to cast the type for the Op with mixed-type output tensors?**\r\n\r\nThanks\r\n"]}, {"number": 16184, "title": "Tensorflow and libcuda.so.1", "body": "Tensorflow 1.4.1\r\nOS:  CentOS 6/7  (we use customized gcc 4.9.2 build for CentOS 6)\r\n\r\nWe have a number of computational servers.  Some are with GPU but some are not.  For the ease of maintenance, we build tensorflow from source code (bazel build) and install the modules under\r\n/usr/local/... that all computational servers mount to the same /usr/local by means of NFS.\r\n\r\nIn the past (Tensorflow 1.0.0), the module could be built without linking with libcuda.so.1.  When a computational server without GPU runs tensorflow, it could run as CPU mode without problems.  When the computational server with GPU runs tensorflow, it could detect the GPU and load up libcuda.so.1 (and libcudart.so and libcudnn.so) by using dso_loader.  This works great for supporting both GPU and non-GPU servers while sharing the same module.\r\n\r\nBut I think that since 1.2.1 (at least, still 1.4.1), it seems that linking libcuda.so.1 is mandatory.  This is bad when non-GPU server would fail loading the module (missing libcuda.so.1), unless we explicitly putting libcuda.so.1 under /usr/lib64 (but this is a non-GPU server...!).  \r\n\r\nWonder if it is possible to make use of the old method of dso_loader instead of linking libcuda.so.1 for bazel building.  Thanks.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "OS:  CentOS 6 x86_64\r\n(we have our compiled gcc 4.9.2 installed under /usr/local, and making use of this\r\ncustomized toolchain for building bazel and tensorflow)\r\nTensorflow version:  1.4.1, grabbed from github.\r\nBazel: 0.9.0\r\nCUDA: 9.0\r\nCUDNN: 7.0.3\r\nGPU:  K20m, 4GB\r\ncommand to build:\r\nbazel build --config=opt --config=cuda \\\r\n        --incompatible_load_argument_is_label=false \\\r\n         //tensorflow/tools/pip_package:build_pip_package\r\n\r\nIn fact, if we compile the package under the host with CUDA installed, we could have a workable\r\ntensorflow package.  However, this package has a dynamic link to /usr/lib64/libcuda.so.1.\r\n(eg.  libtensorflow_framework.so).  This arrangement works under computational server with\r\nGPU installed.  But it fails when running under a server without GPU (ie. no CUDA library) installed.\r\n\r\nIn the past (tensorflow 1.0.0), tensorflow loads up cuda libraries by means of dso_loader \r\n(./tensorflow/stream_executor/dso_loader.cc), but it is no longer the case under \r\ntensorflow 1.4.1.  As a result, in the past (1.0.0), the same binary works for servers with or without CUDA library installed (ie. both GPU and non-GPU servers).  But under 1.4.1, due to the dynamic link to libcuda.so.1, the binary does not work with non-GPU servers that are without CUDA installed.  Of course, we can install cuda under those non-GPU servers to make it run.  But hey, what is the point when installing the CUDA toolkit to a non-GPU server??!!!\r\n", "@allenlavoie do you understand the implications of this?", "There are hopes of going back to dynamic loading of libcuda, where not having it would just not register the device. @gunan is leading the effort.", "Created a fast hack to load libcuda at runtime:\r\nhttps://gist.github.com/pkit/e06c2d23046a265a4f0fea302a5ce539\r\nP.S. the \"fast cuda device memory allocator\" probably needs more love. Because currently it looks ugly.\r\nCan probably pull all the pretty interfaces from StreamExecutor there, but needs much more work. Because of `Deallocate(DeviceMemoryBase *mem)` decision.", "Assuming you don't want to run the cuda binaries on the non-GPU server, would it be ok for you if the build didn't require libcuda.so.1? I think this is currently the case because we don't pass -lcuda to builds that depend on .so libraries that have been built with -lcuda, in which case the linker falls back to trying to find the library via the SONAME in the original linked libcuda.so, which is libcuda.so.1.", "I'm not sure I understand the comment above.\r\nCurrently `tensorflow-gpu` links dynamically against proprietary Nvidia driver with a very restrictive EULA.\r\nSo, my patch solves that problem, by not requiring any Nvidia proprietary software at the load time.", "Thanks for your contribution @pkit your patch looks really useful. Would you like to upstream this into TF itself?\r\nI think the comment above is more about the dependency of TF at build time on CUDA. @r4nt it is possible that the patch by @pkit will tackle that problem. If cuda is only loaded when needed, our build system may avoid depending on it.", "Yes, that would help - I think we still should figure out why we have that build dependency at all for build time generators. I've found that internally we also have some code to load libcuda dynamically (currently ooo so can't check details), not sure how this relates to the patch here / how hard integrating it will be :)", "I've found a commit undoing the dynamic loading of CUDA libraries in TensorFlow: 191658d54f90ac03c15b339326129cd52d1f56a3. The commit was part of v1.1.0 release, but unfortunately, neither the commit message nor the [changelog](https://github.com/tensorflow/tensorflow/releases/tag/v1.1.0) for that release provide any context. @gunan could you comment?", "Looking at the change, both its author and its reviewer has left our team.\r\n@hawkinsp do you have any memory of the change?\r\nWho would be the best to comment about stream executor?", "Again, I would be happy to review and accept a change to resolve this.", "Quick update, currently the tf-nightly-gpu package no longer dynamically links to libcuda.so. Both core and kernel shared libraries do still link to libcudart and kernel still link to cublas etc, which we are working on clean up as much as possible.", "@yifeif Could you point to the commit/pull request where this was changed? Does it happen by default or do we have to alter the build process somehow? I guess this will not be in 1.13?", "@mika-fischer https://github.com/tensorflow/tensorflow/commit/e0d156b3bfef7388b31b35793a2add7b0a5fab6d", "This won't be in 1.13.", "Thanks @pkit. As @gunan mentioned, this won't be in 1.13, but it is in the latest nightly, as well as the default if you follow the official guide building from source.", "Closing this as the issue with TF explicitly linking to driver should be fixed at head. "]}, {"number": 16183, "title": "`AssignVariableOp` supports `DT_BFLOAT16`", "body": "Fix #16103", "comments": ["Can one of the admins verify this patch?", "Has been fixed by ccbd14b"]}, {"number": 16182, "title": "What are the relation ship between TF.Slim, TF high level API and Keras", "body": "I am very confused. What are the relationships between TF.Slim, TF high level API and Keras. I just want to know which one has the long term evolution. Fragmentation, like Android OS, is a very bad and dangerous thing. At least for me, I am not comfortable with TF.Slim at all. Why TF cannot have a unified and Standardized API? The benefits are so obvious. It should not become different political parties fight each other.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "This is a somewhat related twitter thread with responses from main TF contributors:\r\n\r\nhttps://twitter.com/chrisdonahuey/status/925435123767558144", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Totally agree on the high level api unification. The problem is also that Keras API needs to mediate with multiple backends so its API evolution process could not be totally TF team driven. See also https://github.com/tensorflow/models/issues/1357#issuecomment-347722179", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "/cc @tfboyd", "No idea why anyone thinks I could answer this but I can try to be helpful.  thanks @bhack \r\n\r\ntf.slim is deprecated and other than a few people using it the core TensorFlow team does not support it.  I realize this is frustrating because there are a lot of good examples in SLIM.  If you look at SLIM it extends what is tf.layers which is core and supported.  The multi-GPU support in SLIM is not very good, it works and is computationally correct but all the parameters are put on GPU:0 which is not efficient and in some setups without GPUDirect peer-to-peer it is awful for scaling.  \r\n\r\nGoing forward there are two options for high performance code for multi-GPU and otherwise:\r\n1)  This is the path forward:  use tf.layers to create the models and run with with tf.estimator or as part of tf.eager.  Keep in mind if using tf.eager the imperative execution can be made into a graph with a single call I believe.  Multi-GPU for Estimator is in alpha and a unified method for taking a single-GPU model going multi-GPU and then distributed is in development now.  \r\n\r\n2) Use the bleeding edge code in tf_cnn_benchmarks to roll your own solution.  This means make your model in tf.layers and then use tf_cnn_benchmarks to create a multi-GPU or multi-node solution.  I would not do this as it is time consuming and prone to error.  The go forward solution is very close and I am monitoring the progress personally as I am also excited to bring multi-GPU to everyone.  I also own the performance section of tensorflow.org and have advocated for a high level API over the past year.  \r\n\r\nI apologize for grammar errors, I just wrote this down between meetings as I doubted I would come back to it.  I hope this is helpful.  I am marking this closed but I will try to answer other questions if you have them in the thread.  I also have insider information there might be some clarity on this as the TensorFlow Summit in a few weeks at the end of March.  \r\n\r\n\r\n", "I am happy to see TF.Slim is dropped. It is sucks!!!", "@tfboyd Nice to know but the problem is that [super blending edge/fresh solutions](https://github.com/tensorflow/models/tree/master/research/deeplab) are still contributed with SLIM from the \"internal\" flow pipeline.\r\nIt is hard to know when this projects was started so I don't know what is your internal policy now.\r\nI mentioned this basically related to my previous comment on https://github.com/tensorflow/models/issues/1357#issuecomment-347722179\r\n", "My hope is when tf.estimator has clean multi-gpu performance we can migrate\nthe group that uses tf.slim to that, which will be soon.  The performance\nalone will be worth it and it would be silly for them to not use a\nframework with much higher performance.  I cannot push them until\nTF.estimator is ready.  I also cannot force them but I will strongly\nencourage them at that point.  We all know each other so this is not a\nhostile situation just change management.\n\nOn Mon, Mar 12, 2018, 2:19 PM bhack <notifications@github.com> wrote:\n\n> @tfboyd <https://github.com/tfboyd> Nice to know but the problem is that super\n> blending edge/fresh solutions\n> <https://github.com/tensorflow/models/tree/master/research/deeplab> are\n> still contributed with SLIM from the \"internal\" flow pipeline.\n> It is hard to know when this projects was started so I don't know what is\n> your internal policy now.\n> I mentioned this basically related to my previous comment on tensorflow/models#1357\n> (comment)\n> <https://github.com/tensorflow/models/issues/1357#issuecomment-347722179>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/16182#issuecomment-372465315>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AWZesiKrXKBRHgb_CYyjgqdgIKRojyIYks5tduZtgaJpZM4Rg7x2>\n> .\n>\n", "@tfboyd API and UX create habits so It is always a change management :smile_cat: Also many new users land with TF.estimator and like to grow experience over this paradigm. Probably the new performances could unify internals and new users needs and avoid high levelfragmentation.", "Just in the meantime the dev ml will be opened /cc @ewilderj", "I've noticed that : \r\n\r\n* layers in `slim` are present in `tf.contrib.layers` (Is this due to be merged with `tf.layers `sometime?)\r\n\r\n* `losses` in `slim` are equivalent to `tf.losses`.\r\n\r\n* `datasets` in `slim` are now more or less (with equivalent forms) in tf.data.Datasets etc\r\n\r\n* `metrics` are (and were) originally from `tf.contrib.metrics`\r\n\r\nHow about `learning`?\r\n\r\nI have quite a substantial amount of code built using slim, and while support exists, am attempting to shift to a `tf.layers` + `tf.contrib.framework` + `tf.Estimators` usage.\r\n\r\nAre my observations correct regarding moving code. And how would you suggest structuring code like tf slim's?", "Okay `learning` is there under `tf.contrib.training`", "How about changing slim datasets to `tf.data.Datasets`?", "@varun19299  slim.learning is based on the deprecated Supervisor class. I'm guessing that soon they will have to either update it and make it consistent or abandon it all together. ", "Does tf.layers have any alternative way of sharing function parameters like slim? With slim.arg_scope, we can define any common parameters once. But I do not see an way of using tf.layers.\r\n\r\nBy the way, if you found this link: https://stackoverflow.com/questions/48173368/alternative-to-arg-scope-when-using-tf-layers. It does not answer my question. In that example, kernel_regularizer still requires two times of definition, and it is weird that the author can use \"initializer\" parameter in variable_scope to pass to \"kernel_initializer\" parameter defined in dense function. I think that answer incorrect.\r\n\r\nDoes anyone have a solution? Thank you.", "You could use:\n`tf.contrib.framework.argscope()`\n\nOn Thu 5 Jul, 2018, 9:45 PM ybsave, <notifications@github.com> wrote:\n\n> Does tf.layers have any alternative way of sharing function parameters\n> like slim? With slim.arg_scope, we can define any common parameters once.\n> But I do not see an way of using tf.layers.\n>\n> By the way, if you found this link:\n> https://stackoverflow.com/questions/48173368/alternative-to-arg-scope-when-using-tf-layers.\n> It does not answer my question. In that example, kernel_regularizer still\n> requires two times of definition, and it is weird that the author can use\n> \"initializer\" parameter in variable_scope to pass to \"kernel_initializer\"\n> parameter defined in dense function. I think that answer incorrect.\n>\n> Does anyone have a solution? Thank you.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/16182#issuecomment-402799285>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AUME5855DaZsQTYthdH0HZ50_t9e9kNlks5uDlCrgaJpZM4Rg7x2>\n> .\n>\n-- \n\nThank you,\nVarun.\n", "https://github.com/tensorflow/tensorflow/issues/14703#issuecomment-376654034 :\r\n> tf.layers will eventually be removed (in TF 2.0) ", "@bhack If tf.layers would be removed later, what should I use in the future? Is there any Tensorflow based middle level package like slim but supported officially by Google in later versions?\r\n\r\nI really do not like keras, because I cannot flexibly define my own tensorboard data, such as personalized middle results, selected losses or only part of the variables' statistics. I just want to use Tensorflow only. Thank you.", "You can use keras layers as drop-in replacement for tensorflow `tf.layers`. No need to change the rest of your code. There is a common misconception, which is that as soon as you start using a keras layer, you should use keras functions from start to end, which is simply not true. See this dummy example:\r\n\r\n```python\r\nfrom keras.layers import Dropout, Dense\r\nfrom keras import backend as K\r\n\r\nimg = tf.placeholder(tf.float32, shape=(None, 784))\r\nlabels = tf.placeholder(tf.float32, shape=(None, 10))\r\n\r\nx = Dense(128, activation='relu')(img)\r\nx *= 4\r\nx = tf.clip_by_value(x, 4, 7)\r\nx = Dense(128, activation='relu')(x)\r\nx = Dropout(0.5)(x)\r\npreds = Dense(10, activation='softmax')(x)\r\n\r\nloss = tf.reduce_mean(categorical_crossentropy(labels, preds))\r\n\r\ntrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\r\nwith sess.as_default():\r\n    for i in range(100):\r\n        batch = mnist_data.train.next_batch(50)\r\n        train_step.run(feed_dict={img: batch[0],\r\n                                  labels: batch[1],\r\n                                  K.learning_phase(): 1})\r\n\r\nacc_value = accuracy(labels, preds)\r\nwith sess.as_default():\r\n    print acc_value.eval(feed_dict={img: mnist_data.test.images,\r\n                                    labels: mnist_data.test.labels,\r\n                                    K.learning_phase(): 0})\r\n```\r\n\r\nSee https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html\r\n", "Thank you, @gabrieldemarmiesse. This approach sounds promising.", "@gabrieldemarmiesse: this is not exactly true, as tf.keras.layers and tf.layers occasionally behave differently. ~~For example the default activation function tf.keras.layers.Dense is \"relu\", but for tf.layers.Dense it is \"linear\"~~. Caveat emptor!\r\n\r\nEDIT: Nevermind, I must have hallucinated this difference...", "I see from the slide [number 5](https://web.stanford.edu/class/cs20si/lectures/march9guestlecture.pdf) that:\r\n\r\n> Keras is the official high-level API of \r\nTensorFlow\r\n\r\nSo is this means that:\r\n- we have multiple \"official high-level API\" or\r\n- that tf.slim and tf.layers are low level/middle level API or\r\n- the tf.slim and tf.layers are high-level but unofficial\r\n\r\nHow we need to interpret this? ", "@cweill. Sure, some code will have to change. Though what is your source for the default relu activation for dense layers in keras?\r\n\r\nThe doc https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense says that the default activation is the identity function. ", "From what I understood by reading this thread, tf.layers and tf.slim will be removed in TF 2.0 (or depreciated). ", "@gabrieldemarmiesse  Nobody from the TF confirmed but if true could be nice. ", "For tf.slim: tf.slim is a contrib package. All contrib packages will be removed in tf 2.0. CF https://groups.google.com/a/tensorflow.org/forum/m/#!msg/announce/qXfsxr2sF-0/jHQ77dr3DAAJ\r\n\r\nAlso, tf.slim is depreciated. Cf the msg earlier in this issue by @tfboyd.\r\n\r\nFor tf.layers: Fran\u00e7ois Chollet stated that in TF 2.0, tf.layers will be removed. Since he is working at google closely with the TF team, I think he knows what he is talking about. Cf: #14703. ", "@gabrieldemarmiesse \r\n\r\n> For each of the contrib modules we will either a) integrate the project into TensorFlow; b) move it to a separate repository or c) remove it entirely", "I see, I was maybe too quick too judge, thanks for the quote. ", "We'll be publishing an RFC soon with more details of contrib migration for comment and feedback. It'll be announced to the [developers@](https://groups.google.com/a/tensorflow.org/forum/#!forum/developers) mailing list.\r\n\r\n", "So if tf-slim will be deprecated, then what about tensorflow object detection API framework? odapi is highly dependent on tf-slim. ", "So would you say that nowadays, using tf.slim is shady? ", "Check  https://github.com/tensorflow/community/pull/18#issuecomment-422074773", "tf slim is beautiful but I am really glad that it is going to be gone for next update. We can just use Keras. "]}, {"number": 16181, "title": "How to control the number of threads on Tensorflow and TF-Slim?", "body": "Do we have any options to control the number of threads in TF-Slim both in training and evaluation processes?\r\n\r\nSpecifically, I use [this network](https://github.com/pudae/tensorflow-densenet) for my classification problem. I changed the evaluation part in a way that runs train and evaluation in parallel like [this code](https://github.com/mnuke/tf-slim-mnist). I can run it on my own CPU without any problem. But I can't execute them on a supercomputer. It seems that it is related to the very large number of threads which are being created by Tensorflow. If the number of threads exceeds the maximum number of threads pre-set in SLURM (= 28) then the job will fail. Since it's unable to create new threads it will end up with error \"resource temporarily unavailable\".\r\n\r\nThis error provided when the code tries to restore parameters from checkpoints. If there is no limitation on the number of threads (like on my pc) it works fine:\r\n\r\n    INFO:tensorflow:Restoring parameters from ./model.ckpt-0\r\n    INFO:tensorflow:Starting evaluation at\r\n    I tensorflow/core/kernels/logging_ops.cc:79] eval/Accuracy[0]\r\n    I tensorflow/core/kernels/logging_ops.cc:79] eval/Recall_5[0]\r\n    INFO:tensorflow:Evaluation [1/60]\r\n\r\nHowever, when there is a limitation on the number of threads (like SLURM job submission on supercomputers) we get:\r\n\r\n    INFO:tensorflow:Restoring parameters from ./model.ckpt-0\r\n    terminate called after throwing an instance of 'std::system_error'\r\n    what():  Resource temporarily unavailable\r\n\r\nI tried to limit the number of CPU threads used by Tensorflow to 1 by creating config like:\r\n\r\n      FLAGS.num_preprocessing_threads=1\r\n\r\n      config = tf.ConfigProto()\r\n      config.intra_op_parallelism_threads = FLAGS.num_preprocessing_threads\r\n      config.inter_op_parallelism_threads = FLAGS.num_preprocessing_threads\r\n    \r\n        slim.evaluation.evaluation_loop(\r\n            master=FLAGS.master,\r\n            checkpoint_path=each_ckpt,\r\n            logdir=FLAGS.eval_dir,\r\n            num_evals=num_batches,\r\n            eval_op=list(names_to_updates.values()) + print_ops,\r\n            variables_to_restore=variables_to_restore,\r\n            session_config=config)\r\nBut unfortunately, that didn't help. In my opinion, the main problem we are having here is the fact that we are not able to control the number of threads here. Although we set it to 1 with various TF options you can actually see that this job is creating many more threads on the node:\r\n\r\n\r\n    slurm_script\u2500\u252c\u2500python\u2500\u2500\u2500128*[{python}]\r\n                 \u2514\u2500python\u2500\u2500\u25008*[{python}]\r\n\r\nTraining script is creating 128 threads and evaluation script is creating 8 (both numbers vary over time).  \r\n\r\nP.S. I'm using Python 2.7.13 and Tensorflow 1.3.0.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Duplicate of issue #14900?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 77 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 16180, "title": "Tensorboard is down after upgrading the tensorflow?", "body": "Hello everyone:\r\n\r\nI meet a issue about tensorboard after upgrading the tensorflow. It runs nicely before, but I want maintain some Python2.7 codes in Python3.4. That is why I install tensorflow .whl file of Python 3.4 and modify some grammer from Python2.7 to Python3.4. Then codes still run fine, but tensorboard is donw. The error message as following:\r\n\r\n![image](https://user-images.githubusercontent.com/12611573/35029640-d1981942-fb96-11e7-9b89-c3c14ffaa54b.png)\r\n\r\nOS Platform: Ubuntu 14.04\r\nTensorFlow installed from: pip instll .whl file\r\nTensorFlow version: tensorflow 1.2.1 for Python2, but can not check the version for Python 3\r\n\r\nWhat should I do for this issue? degrade tensorflow or upgrade CUDA?\r\nCan anybody give me any help? Thank you!\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I noticed that when installing tensorflow-1.5.0rc1 it installs an old version of tensorboard `0.1.8` when install tensorflow with pip.", "@sleighsoft I think the new version tensorboard needs CUDA 9. Is here any ways to degrade tensorboard for Python2?", "You can do `pip uninstall tensorflow-tensorboard` and then `pip install tensorflow-tensorboard==<your_version>`. At least this is what I do as long as pip installs the wrong tensorflow-tensorboard version.", "@sleighsoft Thanks! I will give a try. But there is one more question? How can I check the tensorboard version?", "@sleighsoft Here is problem. I can not uninstall tensorboard?\r\nDo you have any methods for this?\r\n![image](https://user-images.githubusercontent.com/12611573/35179615-e6831b88-fdd8-11e7-9634-86e58924e696.png)\r\n", "run `pip freeze` to see what you have installed first maybe.", "@sleighsoft I have tried to run pip freeze, the details as following. There is only one about tf, nothing about tensorboard.\r\n\r\nabsl-py==0.1.8\r\nalabaster==0.7.10\r\nastor==0.6.2\r\nBabel==2.5.1\r\nbleach==1.5.0\r\ncertifi==2017.11.5\r\nchardet==3.0.4\r\ndecorator==4.2.1\r\ndocutils==0.14\r\nentrypoints==0.2.3\r\ngast==0.2.0\r\nhtml5lib==0.9999999\r\nidna==2.6\r\nimagesize==0.7.1\r\nipykernel==4.7.0\r\nipyparallel==6.0.2\r\nipython==6.2.1\r\nipython-genutils==0.2.0\r\nipywidgets==7.1.0\r\njedi==0.11.1\r\nJinja2==2.10\r\njsonschema==2.6.0\r\njupyter-client==5.2.1\r\njupyter-core==4.4.0\r\nMarkdown==2.6.11\r\nMarkupSafe==1.0\r\nmistune==0.8.3\r\nnbconvert==5.3.1\r\nnbformat==4.4.0\r\nnose==1.3.7\r\nnotebook==5.2.2\r\nnumpy==1.14.0\r\npandocfilters==1.4.2\r\nparso==0.1.1\r\npexpect==4.3.1\r\npickleshare==0.7.4\r\nprompt-toolkit==1.0.15\r\nprotobuf==3.5.1\r\nptyprocess==0.5.2\r\nPygments==2.2.0\r\npython-dateutil==2.6.1\r\npytz==2017.3\r\npyzmq==16.0.3\r\nqtconsole==4.3.1\r\nrequests==2.18.4\r\nsimplegeneric==0.8.1\r\nsix==1.11.0\r\nsnowballstemmer==1.2.1\r\nSphinx==1.6.6\r\nsphinxcontrib-websupport==1.0.1\r\ntb-nightly==1.5.0a20180114\r\ntermcolor==1.1.0\r\nterminado==0.8.1\r\ntestpath==0.3.1\r\ntf-nightly-gpu==1.6.0.dev20180114\r\ntornado==4.5.3\r\ntraitlets==4.3.2\r\nurllib3==1.22\r\nwcwidth==0.1.7\r\nwebencodings==0.5.1\r\nWerkzeug==0.14.1\r\nwidgetsnbextension==3.1.0\r\n", "So it is probably not installed or you already uninstalled it. Have you tried running `pip install tensorflow-tensorboard==<your_version>`. I think I cannot help you more from that point on.", "@sleighsoft Truly thank you!\r\nI have tried other solutions. But still did not work.\r\nI think the point is my tensorboard always depends on python3.5. I do not know why.\r\nMy python2.7 still work on tensorflow.\r\n![image](https://user-images.githubusercontent.com/12611573/35196071-35c5b64e-ff08-11e7-9419-ff534647b267.png)\r\nI will keep checking other methods. Thanks again!", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "Is that still a problem?", "It seems that when installing tf15rc1 now it installs tensorflow-tensorboard1.5.0\r\nThis seems solved", "@drpngx  Thanks, I reinstall the tensorflow. It seems solved.", "@sleighsoft Thanks. I just reinstall tensorflow, because it is not convenient for other people to use it."]}, {"number": 16179, "title": "ProfilerHook and loading libcupti.so cause Ubuntu to completely freeze", "body": "1. OS Platform and Distribution: Ubuntu 14.04 LTE\r\n2. TensorFlow version: 1.14\r\n3. Bazel version: 0.9.0\r\n4. CUDA/cuDNN version: 8.0/7.0.5\r\n5. GPU model and memory: GeForce GTX1060 - 6070MB\r\n6. Exact command to reproduce: python3.4 -m music_modeling \r\n--\r\n\r\nI added \"ProfilerHook\" to Estimator for recording GPU memory consumption; but it always causes my Ubuntu to freeze indefinitely and Ubuntu never makes away with it.\r\n\r\nHere is the source code:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.python.layers.core import dense\r\nfrom tensorflow.python import debug as tf_debug\r\n\r\nfrom data.music_data_reader import MusicDataReader\r\nfrom model.tf_msa_rnn import dynamic_msa_rnn\r\n\r\n\r\nFLAGS = tf.app.flags.FLAGS\r\ntf.app.flags.DEFINE_string(\"mode\", tf.estimator.ModeKeys.TRAIN,\r\n                           \"\"\"\"Is training or testing mode\"\"\")\r\ntf.app.flags.DEFINE_string(\"model_dir\", './msa_model',\r\n                           \"\"\"\"Directory in where checkpoints are stored\"\"\")\r\ntf.app.flags.DEFINE_integer(\"batch_size\", 20,\r\n                            \"\"\"Number of samples in a batch\"\"\")\r\ntf.app.flags.DEFINE_integer(\"num_epochs\", 100,\r\n                            \"\"\"\"How many times the whole training set has to be fed into network\"\"\")\r\ntf.app.flags.DEFINE_string(\"log_directory\", './log_dir',\r\n                           \"\"\"\"Directory in where logs and checkpoints are stored\"\"\")\r\n# *************************************\r\n# *************************************Configuration options for the network\r\n# *************************************\r\ntf.app.flags.DEFINE_integer(\"num_units\", 30,\r\n                            \"\"\"\"# of hidden units in an LSTM cell\"\"\")\r\ntf.app.flags.DEFINE_integer(\"num_msa_feats\", 10,\r\n                            \"\"\"\"# of MS features to be learned\"\"\")\r\ntf.app.flags.DEFINE_integer(\"signal_len\", 100,\r\n                            \"\"\"\"Length of the signal at a time to be processed by \r\n                                multi-scale analyzer\r\n                            \"\"\")\r\ntf.app.flags.DEFINE_integer(\"dim_pitch\", 88,\r\n                            \"\"\"\"# of hidden units in an LSTM cell\"\"\")\r\n# *************************************\r\n# *************************************Configuration options for dataset\r\n# *************************************\r\ntf.app.flags.DEFINE_string(\"dir_path\",\r\n                           './music_samples/MuseData',\r\n                           \"\"\"\"Absolute path to the music files for reading training/testing samples\"\"\")\r\ntf.app.flags.DEFINE_integer(\"pitch_low\",\r\n                            21,\r\n                            \"\"\"\"Low pitch value\"\"\")\r\ntf.app.flags.DEFINE_integer(\"pitch_high\",\r\n                            109,\r\n                            \"\"\"\"High pitch value\"\"\")\r\ntf.app.flags.DEFINE_float(\"dt\",\r\n                          0.3,\r\n                          \"\"\"\"Not sure yet...\"\"\")\r\n\r\n\r\ndef loss_fn(y_pred, y_true):\r\n    '''\r\n\r\n    :param y_pred: Logits predicted by the model\r\n    :param y_true: Correct values corresponding each prediction\r\n    :return:\r\n    '''\r\n\r\n    y_pred = tf.log(tf.nn.softmax(y_pred, name=\"probs_tensor\"))  # [BSxMTxOS] -- Probabilities\r\n    p_trun = y_pred[:, 0:-1, ...]  # x'[1], x'[2], ..., x'[N-1]\r\n    t_trun = y_true[:, 1:, ...]  # x[1], x[2], ..., x[N-1]\r\n    loss = tf.reduce_sum(p_trun*t_trun, axis=2)  # [BSxMT] -- Dot product between 3rd dimensions\r\n    loss = tf.reduce_mean(loss, name=\"piano_roll_loss\")  # loss function -- returns a scalar\r\n    tf.summary.scalar(\"loss_fn\", loss)  # Add summary for the loss\r\n    loss = tf.Print(loss, [loss], \"Loss: \")\r\n    return loss\r\n\r\n\r\ndef model_fn(features,\r\n             mode=tf.estimator.ModeKeys.TRAIN,\r\n             params=None):\r\n\r\n    print(\"Creating Model...\")\r\n    input_ph = tf.reshape(features['input_ph'], [FLAGS.batch_size, params['max_seq'], FLAGS.dim_pitch])\r\n    seq_len_ph = tf.reshape(features['seq_len_ph'], [FLAGS.batch_size])\r\n    outputs, state = dynamic_msa_rnn(FLAGS.batch_size,\r\n                                     input_ph,\r\n                                     seq_len_ph,\r\n                                     params['max_seq'],\r\n                                     FLAGS.signal_len,\r\n                                     [20, 10],  # Number of filters per layer\r\n                                     [11, 13],  # Kernel size for each layer\r\n                                     [3],  # Pooling size for each layer\r\n                                     FLAGS.num_msa_feats,\r\n                                     FLAGS.num_units,\r\n                                     activation=tf.nn.tanh,\r\n                                     initializer=tf.glorot_normal_initializer())  # [BSxMTxOS], [BSxSS]\r\n    # Create fully connected layer to generate output for piano keys\r\n    outs = dense(outputs,\r\n                 FLAGS.dim_pitch,\r\n                 kernel_initializer=tf.glorot_normal_initializer())  # BSxMTxID\r\n    loss = loss_fn(outs, features['input_ph'])\r\n    print(\"Creating Estimator Spec for %s ...\" % mode)\r\n    # For training\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        optimizer = tf.train.AdamOptimizer()\r\n        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\r\n        return tf.estimator.EstimatorSpec(mode=mode,\r\n                                          loss=loss,\r\n                                          train_op=train_op)\r\n    # For evaluation\r\n    eval_metric_ops = {\r\n        \"accuracy\": tf.metrics.accuracy(\r\n            labels=tf.argmax(features['input_ph'][:, 1:, ...], axis=-1),\r\n            predictions=tf.argmax(outs[:, 0:-1, ...], axis=-1)\r\n        )\r\n    }\r\n    return tf.estimator.EstimatorSpec(mode=mode,\r\n                                      loss=loss,\r\n                                      eval_metric_ops=eval_metric_ops)\r\n\r\n\r\ndef do_train(tr_data, vl_data):\r\n\r\n    # Create Estimator\r\n    sess_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\r\n    sess_conf.gpu_options.allow_growth = True\r\n    config = tf.estimator.RunConfig(model_dir=FLAGS.model_dir,  # CheckpointSaverHook\r\n                                    save_checkpoints_steps=100,  # CheckpointSaverHook\r\n                                    log_step_count_steps=10,  # SummarySaverHook\r\n                                    session_config=sess_conf)\r\n    music_classifier = tf.estimator.Estimator(model_fn,\r\n                                              config=config,\r\n                                              params={'max_seq': tr_data.max_seq})\r\n    # Prepare input data\r\n    tr_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        {'input_ph': tr_data.data, 'seq_len_ph': tr_data.seq_len},\r\n        batch_size=FLAGS.batch_size,\r\n        num_epochs=FLAGS.num_epochs,\r\n        num_threads=1,\r\n        shuffle=True\r\n    )\r\n    # Extra Hooks\r\n    logging_hook = tf.train.LoggingTensorHook(\r\n        tensors={'probabilities': 'probs_tensor'},\r\n        every_n_secs=60\r\n    )\r\n    # debugging_hook = tf_debug.LocalCLIDebugHook(thread_name_filter=\"MainThread$\", dump_root=\"./dump\")\r\n    profiler_hook = tf.train.ProfilerHook(save_steps=1,\r\n                                          output_dir=\"./profile\",\r\n                                          show_dataflow=False,\r\n                                          show_memory=True)\r\n    # Train\r\n    music_classifier.train(tr_input_fn, hooks=[profiler_hook])\r\n    print(\"Training is over...\")\r\n\r\n\r\ndef do_test(te_data):\r\n\r\n    print(\"Start testing...\")\r\n\r\n\r\ndef main(_):\r\n\r\n    if FLAGS.mode == tf.estimator.ModeKeys.TRAIN:\r\n        tr_data = MusicDataReader(FLAGS.dir_path,\r\n                                  'train',\r\n                                  (FLAGS.pitch_low, FLAGS.pitch_high),\r\n                                  FLAGS.dt)\r\n        vl_data = MusicDataReader(FLAGS.dir_path,\r\n                                  'valid',\r\n                                  (FLAGS.pitch_low, FLAGS.pitch_high),\r\n                                  FLAGS.dt)\r\n        print(\"Number of training samples: %d\" % tr_data.data_num)\r\n        print(\"Number of validation samples: %d\" % vl_data.data_num)\r\n        do_train(tr_data, vl_data)\r\n    else:\r\n        te_data = MusicDataReader(FLAGS.dir_path,\r\n                                  'test',\r\n                                  (FLAGS.pitch_low, FLAGS.pitch_high),\r\n                                  FLAGS.dt,)\r\n        print(\"Number of testing samples: %d\" % te_data.data_num)\r\n        do_test(te_data)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    tf.app.run(main=main)\r\n```\r\nI cannot attach the output of my console for it is impossible due to the indefinite freezing of my Ubuntu. All I can say is that it tells me that it loads **libcupti.so** and computes a step of training process. Then, everything totally messes up.\r\n\r\nThank you for your support in advance.", "comments": ["\r\n@ispirmustafa, @martinwicke, it seems like there is a deadlock when using the ProfilerHook. On the other hand the documentation of ProfilerHook implies it was designed for monitored session, so perhaps it is not supported. Please take a look.\r\n\r\n", "@aselle I've checked out the source code and it creates a **MonitoredSession** wrapped with 3-4 more different session wrappers in a nested fashion. \r\n\r\nOh Btw. Upon request, I can use my phone in order to capture a screenshot for you to see the output of execution.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Hi,\r\nI have the same problem in Tensorflow 1.5, when I try to use ProfilerHook, and I launch:\r\n`my_model.train(...)`\r\nmy jupyter notebook kernel dies instantaneously. Is there a workaround I can use ? \r\nThanks", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Assigning to @ispirmustafa who worked on ProfilerHook.\r\n\r\nAlso @iliTheFallen, can you provide a short example that reproduces the problem? It's very difficult to debug long examples, so try making the example as short as possible.", "@iliTheFallen, it sounds like the machine might be running out of RAM. Do you know if that is happening?\r\n\r\nUsing profiling [turns on `FULL_TRACE`](https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/python/training/basic_session_run_hooks.py#L878), which will take some extra memory. You could try turning the trace level down (by [editing the code](https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/python/training/basic_session_run_hooks.py#L878)) to one of the lower levels, like [`SOFTWARE_TRACE`](https://www.tensorflow.org/api_docs/python/tf/RunOptions).", "Nagging Assignee @ispirmustafa: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 36 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 16178, "title": "Crash in TF lite demo android app when using preprocessing layer", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** Linux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: v1.4.0-rc0-21-g1e25994 1.4.0-rc1\r\n- **Python version**: Python 3.6.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8/6\r\n- **GPU model and memory**: Titan X (Pascal), 12 GB\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nI have a problem adding preprocessing layers to MobileNetV1 model that is quantized afterward. As preprocessing method I would like to use inception preprocessing, but TF lite does not support several operations (sub, div, broadcasting, ...), so I modified following preprocessing\r\n\r\n```python\r\nimages = tf.divide(images, tf.constant(255.0))\r\nimages = tf.subtract(images, tf.constant(0.5))\r\nimages = tf.multiply(images, tf.constant(2.0))\r\n```\r\n\r\nto\r\n\r\n```python\r\nshape = images.get_shape()\r\nc1 = tf.constant(1.0/255.0, shape=shape)\r\nc1 = tf.fake_quant_with_min_max_args(c1, min=-1, max=1)\r\nc2 = tf.constant(-0.5, shape=shape)\r\nc2 = tf.fake_quant_with_min_max_args(c1, min=-1, max=1)\r\nc3 = tf.constant(2.0, shape=shape)\r\nc3 = tf.fake_quant_with_min_max_args(c1, min=-1, max=1)\r\n\r\nimages = tf.multiply(images, c1)\r\nimages = tf.fake_quant_with_min_max_args(images, min=0, max=1)\r\nimages = tf.add(images, c2)\r\nimages = tf.fake_quant_with_min_max_args(images, min=-0.5, max=0.5)\r\nimages = tf.multiply(images, c3)\r\nimages = tf.fake_quant_with_min_max_args(images, min=-1.0, max=1.0)\r\n```\r\n\r\nQuantization is performed with\r\n```python\r\nfold_batch_norms.FoldBatchNorms(graph)\r\nquantize.Quantize(graph, is_training=is_training)\r\n```\r\n and can be trained and evaluated.\r\n\r\nFurther, graph is frozen.\r\n```bash\r\nbazel-bin/tensorflow/python/tools/freeze_graph \\\r\n  --input_graph=MobileNetV1-4.pbtxt \\\r\n  --input_checkpoint=MobileNetV1-4.ckpt \\\r\n  --output_node_names=output/softmax \\\r\n  --output_graph=MobileNetV1-4-frozen.pb\r\n```\r\n\r\nFinally, frozen graph is converted to TF lite model using command.\r\n```bash\r\nbazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n --input_file=MobileNetV1-4-frozen.pb \\\r\n --input_format=TENSORFLOW_GRAPHDEF \\\r\n --output_format=TFLITE \\\r\n --output_file=model.tflite \\\r\n --inference_type=QUANTIZED_UINT8 \\\r\n --inference_input_type=QUANTIZED_UINT8 \\\r\n --input_array=input/image \\\r\n --output_array=output/softmax \\\r\n --input_shape=1,224,224,3\r\n```\r\n\r\nDuring conversion no error occurs.\r\n```\r\n2018-01-17 11:25:52.905034: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 604 operators, 896 arrays (0 quantized)\r\n2018-01-17 11:25:53.301108: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 66 operators, 127 arrays (1 quantized)\r\n2018-01-17 11:25:53.302502: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 66 operators, 127 arrays (1 quantized)\r\n2018-01-17 11:25:53.303020: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 35 operators, 96 arrays (1 quantized)\r\n2018-01-17 11:25:53.303601: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 35 operators, 96 arrays (1 quantized)\r\n2018-01-17 11:25:53.326761: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After quantization graph transformations pass 1: 34 operators, 95 arrays (94 quantized)\r\n2018-01-17 11:25:53.327269: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After quantization graph transformations pass 2: 34 operators, 95 arrays (94 quantized)\r\n2018-01-17 11:25:53.327854: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:313] Total transient array allocated size: 1756160 bytes, theoretical optimal value: 1204224 bytes.\r\n2018-01-17 11:25:53.328080: I tensorflow/contrib/lite/toco/toco_tooling.cc:269] Estimated count of arithmetic ops: 1.14175 billion (note that a multiply-add is counted as 2 ops).\r\n```\r\n\r\nWhen I upload generated model to TF lite demo application, app crashes logcat prints this error.\r\n```\r\n01-17 11:56:52.190 10923-10923/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\n01-17 11:56:52.190 10923-10923/? A/DEBUG: Build fingerprint: 'samsung/dreamlteks/dreamlteks:7.0/NRD90M/G950NKSU1AQL3:user/release-keys'\r\n01-17 11:56:52.191 10923-10923/? A/DEBUG: Revision: '11'\r\n01-17 11:56:52.191 10923-10923/? A/DEBUG: ABI: 'arm64'\r\n01-17 11:56:52.191 10923-10923/? A/DEBUG: pid: 10865, tid: 10881, name: CameraBackgroun  >>> android.example.com.tflitecamerademo <<<\r\n01-17 11:56:52.191 10923-10923/? A/DEBUG: signal 6 (SIGABRT), code -6 (SI_TKILL), fault addr --------\r\n01-17 11:56:52.191 10923-10923/? A/DEBUG:     x0   0000000000000000  x1   0000000000002a81  x2   0000000000000006  x3   0000000000000008\r\n01-17 11:56:52.191 10923-10923/? A/DEBUG:     x4   0000007e81515040  x5   0000007e815166c0  x6   0000ffffffffffff  x7   ffffffffffffffff\r\n01-17 11:56:52.191 10923-10923/? A/DEBUG:     x8   0000000000000083  x9   ffffffffffffffdf  x10  0000000000000000  x11  ffffffffffffffff\r\n01-17 11:56:52.191 10923-10923/? A/DEBUG:     x12  0000000000000000  x13  ffffffffffff0000  x14  00000000000002e0  x15  000000000000044d\r\n01-17 11:56:52.191 10923-10923/? A/DEBUG:     x16  0000007e9533aed0  x17  0000007e952e29f4  x18  0000000000000001  x19  0000007e817524f8\r\n01-17 11:56:52.191 10923-10923/? A/DEBUG:     x20  0000000000000006  x21  0000007e81752450  x22  000000000000000b  x23  0000007e858340f0\r\n01-17 11:56:52.191 10923-10923/? A/DEBUG:     x24  0000007e817524e8  x25  0000000000000000  x26  0000000000000080  x27  0000007e81516740\r\n01-17 11:56:52.191 10923-10923/? A/DEBUG:     x28  0000000000000001  x29  0000007e81750730  x30  0000007e952dfd14\r\n01-17 11:56:52.191 10923-10923/? A/DEBUG:     sp   0000007e81750710  pc   0000007e952e29fc  pstate 0000000060000000\r\n01-17 11:56:52.198 10923-10923/? A/DEBUG: backtrace:\r\n01-17 11:56:52.198 10923-10923/? A/DEBUG:     #00 pc 000000000006f9fc  /system/lib64/libc.so (tgkill+8)\r\n01-17 11:56:52.198 10923-10923/? A/DEBUG:     #01 pc 000000000006cd10  /system/lib64/libc.so (pthread_kill+64)\r\n01-17 11:56:52.198 10923-10923/? A/DEBUG:     #02 pc 0000000000025078  /system/lib64/libc.so (raise+24)\r\n01-17 11:56:52.198 10923-10923/? A/DEBUG:     #03 pc 000000000001cc04  /system/lib64/libc.so (abort+52)\r\n01-17 11:56:52.198 10923-10923/? A/DEBUG:     #04 pc 00000000000881a0  /data/app/android.example.com.tflitecamerademo-1/lib/arm64/libtensorflowlite_jni.so\r\n01-17 11:56:52.199 10923-10923/? A/DEBUG:     #05 pc 0000000000071ce4  /data/app/android.example.com.tflitecamerademo-1/lib/arm64/libtensorflowlite_jni.so\r\n01-17 11:56:52.199 10923-10923/? A/DEBUG:     #06 pc 00000000000707fc  /data/app/android.example.com.tflitecamerademo-1/lib/arm64/libtensorflowlite_jni.so\r\n01-17 11:56:52.199 10923-10923/? A/DEBUG:     #07 pc 000000000007f99c  /data/app/android.example.com.tflitecamerademo-1/lib/arm64/libtensorflowlite_jni.so\r\n01-17 11:56:52.199 10923-10923/? A/DEBUG:     #08 pc 0000000000011c5c  /data/app/android.example.com.tflitecamerademo-1/lib/arm64/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+1628)\r\n01-17 11:56:52.199 10923-10923/? A/DEBUG:     #09 pc 0000000000384abc  /data/app/android.example.com.tflitecamerademo-1/oat/arm64/base.odex (offset 0x329000)\r\n```\r\n\r\nThis error doesn't seem to be related to added preprocessing layer, but without adding preprocessing layer, no error occurs and app can run.\r\n", "comments": ["How about preprocessing like this?\r\n\r\n```python\r\nw1 = tf.Variable([[[\r\n    [1/255, 0, 0],\r\n    [0, 1/255, 0],\r\n    [0, 0, 1/255]\r\n]]])  # shape = (1, 1, 3, 3) = [filter_height, filter_width, in_channels, out_channels]\r\nw2 = tf.Variable([[[\r\n    [2., 0, 0],\r\n    [0, 2., 0],\r\n    [0, 0, 2.]\r\n]]])\r\nb = tf.Variable([-0.5, -0.5, -0.5])\r\np4 = tf.nn.conv2d(images, w1, (1, 1, 1, 1), padding=\"VALID\")\r\np4 = tf.nn.bias_add(p4, b)\r\np4 = tf.nn.conv2d(p4, w2, (1, 1, 1, 1), padding=\"VALID\")\r\n```", "Thanks @shastakr!  With small modifications it works perfectly even in Android TF lite demo app.\r\n```python\r\nw0 = tf.constant([[[\r\n  [1/255, 0, 0],\r\n  [0, 1/255, 0],\r\n  [0, 0, 1/255]\r\n]]], name=\"preprocess/weights/0\")\r\n\r\nw1 = tf.constant([[[\r\n  [2., 0, 0],\r\n  [0, 2., 0],\r\n  [0, 0, 2.]\r\n]]], name=\"preprocess/weights/1\")\r\n\r\nb0 = tf.constant([-0.5, -0.5, -0.5], name=\"preprocess/biases/0\")\r\nb1 = tf.constant([0.0, 0.0, 0.0], name=\"preprocess/biases/1\")\r\n\r\nimages = tf.nn.conv2d(images, w0, (1, 1, 1, 1), padding=\"VALID\", name=\"preprocess/conv2d0/0\")\r\nimages = tf.nn.bias_add(images, b0, name=\"preprocess/bias_add/0\")\r\nimages = tf.nn.conv2d(images, w1, (1, 1, 1, 1), padding=\"VALID\", name=\"preprocess/conv2d1/1\")\r\nimages = tf.nn.bias_add(images, b1, name=\"preprocess/bias_add/1\")\r\n```"]}, {"number": 16177, "title": "RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6   return f(*args, **kwds)", "body": "### System information\r\n- **OS Platform and Distribution :**  Linux Ubuntu 17.10\r\n- **TensorFlow installed from :**  Anaconda [followed this tutorial](https://www.tensorflow.org/install/install_linux#InstallingAnaconda)\r\n- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1\r\n- **Python version**: Python 3.6.4 :: Anaconda, Inc.\r\n- **CUDA/cuDNN version**: not using GPU version\r\n- **GPU model and memory**: 2GB GT720\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**result :** \r\n/home/pankaja/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)\r\nv1.4.0-19-ga52c8d9 1.4.1\r\n\r\n### Describe the problem\r\nFollowed Official tensorflow documentation to install tensorflow on Ubuntu 17.10, python3 (python 3.6) and with CPU support. Used conda environment. Followed [this](https://www.tensorflow.org/install/install_linux#InstallingAnaconda) and in the 4th step this is the command I used `pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.4.1-cp36-cp36m-linux_x86_64.whl`\r\nit installed successfully. But when I try to import tensorflow in python I'm getting this error.\r\n\r\n```\r\n/home/pankaja/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)\r\n\r\n```\r\n\r\n### Source code / logs\r\nActivate Conda environment\r\n`source activate tensorflow`\r\n\r\n**command :** `python`\r\n**log :** \r\n```\r\nPython 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19) \r\n[GCC 7.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n```\r\n\r\n**command :** `import tensorflow`\r\n**log :**\r\n```\r\n/home/pankaja/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)\r\n```\r\n\r\n\r\nWhy can't I use the tensorflow for python 3.6 in python 3.6 . How to fix this ?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nBazel version\nExact command to reproduce", "@tensorflowbutler I removed the conda environment and reinstalled all of them for python 3.5 . Now it's working fine . (But for python 3.6 still gives this error)"]}, {"number": 16176, "title": "4d55397500 patch 1", "body": "Provide a practical meaning for the `pos_weights` parameter.\r\n\r\nThe current  weighted_cross_entropy_with_logits docs don't explain practically the relationship of\r\n`pos_weights > 1`,  `pos_weights < 1` to precision, recall, and class imbalance.\r\n", "comments": ["Can one of the admins verify this patch?", "@4d55397500 Thanks for the contribution."]}, {"number": 16175, "title": "make applications using TFLite work on ARM64 Linux (non-Android) platforms", "body": "When build TF Lite related stuff such as lable_image for tflite on ARMv64 non-Android environment (I am running Debian on an internal development board). I saw something like:\r\n\r\n ```\r\n /home/freedom/work/tensorflow/tensorflow/contrib/lite/kernels/internal/BUILD:264:1: C++ compilation of rule '//tensorflow/contrib/lite/kernels/internal:neon_tensor_utils' failed (Exit 1)\r\n  gcc: error: unrecognized command line option '-mfpu=neon'\r\n  gcc: error: unrecognized command line option '-mfloat-abi=softfp'\r\n```\r\n\r\nIt seems ARM64 falls into the \":arm\" category. After removing it, I can build my tflite-based command line programs without problems.", "comments": ["Can one of the admins verify this patch?", "rebased to resolve a conflict", "I had this issue with the jetson tx2. Thanks for the PR", "Hi. This is my understanding of the issue:\r\n\r\nNEON is mandatory (always present) for aarch64 (arm64), so -mfpu and -mfloat are not only not needed but invalid as the aarch64 and arm backends are completely separate in gcc.\r\nThey're needed for the arm 32 archs like armeabi-v7a, and armv7a though.\r\nHaving said that, the -O3 option is still needed on both, so it's good it's the default here.\r\n\r\nI also can see how if you're compiling, the arm64-v8a would map to the arm target. But I also see how on a 32 bit arm it would also do the same (I haven't tried myself).\r\n\r\nSo, can you try adding a more specific target:\r\n\r\n    \":arm64-v8a\": [\r\n        \"-O3\",\r\n    ],\r\n\r\nOtherwise, by removing the arm target, there could be 32 bit cases that would not get the flags needed.", "@raziel: I tried that before. Simply adding\r\n```\r\n\":arm64-v8a\": [\r\n    \"-O3\",\r\n],\r\n```\r\nwon't work. As far as I can remember, \":arm\" will be matched. I really don't why :-). \r\n\r\nFor other ARM platforms with NEON, I think adding `--cxxopt=..` or `--copt=..` will work. And, as you might know, there are some NEON and VFP variants on ARMv6 and ARMv7-A platforms, so `-mfpu=neon` may not be what you want. E.g., on RPI 3, which is an ARMv8 platform, but running ARMv7-a binaries, I usually add something like \r\n```\r\n--cxxopt=-mfpu=neon-vfpv4 --cxxopt=-ftree-vectorize --cxxopt=-funsafe-math-optimizations --cxxopt=-ftree-loop-vectorize --cxxopt=-fomit-frame-pointer\r\n```\r\nwhen using bazel", "@freedomtan Unfortunately, building with your patch on the TX1 failed as follows (see platform details in https://github.com/tensorflow/tensorflow/issues/21332):\r\n```\r\nERROR: /home/anton/CK_TOOLS/lib-tensorflow-src-cpu-1.9-compiler.python-2.7.12-linux-64/src/tensorflow/contrib/lite/kernels/BUILD:128:1: C++ compilation of rule '//tensorflow/contrib/lite/kernels:builtin_ops' failed (Exit 1)\r\nIn file included from ./tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8.h:21:0,\r\n                 from tensorflow/contrib/lite/kernels/depthwise_conv.cc:26:\r\n./tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:80:43: error: expected primary-expression before ',' token\r\n static_assert(offsetof(DepthwiseConvParams, input_depth) ==\r\n                                           ^\r\n./tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:80:45: error: 'input_depth' was not declared in this scope\r\n static_assert(offsetof(DepthwiseConvParams, input_depth) ==\r\n                                             ^\r\n./tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:80:56: error: 'offsetof' was not declared in this scope\r\n static_assert(offsetof(DepthwiseConvParams, input_depth) ==\r\n                                                        ^\r\n./tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:82:43: error: expected primary-expression before ',' token\r\n static_assert(offsetof(DepthwiseConvParams, input_row_size) ==\r\n                                           ^\r\n./tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:82:45: error: 'input_row_size' was not declared in this scope\r\n static_assert(offsetof(DepthwiseConvParams, input_row_size) ==\r\n                                             ^\r\n./tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:82:59: error: 'offsetof' was not declared in this scope\r\n static_assert(offsetof(DepthwiseConvParams, input_row_size) ==\r\n                                                           ^\r\n./tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:84:43: error: expected primary-expression before ',' token\r\n static_assert(offsetof(DepthwiseConvParams, output_depth) ==\r\n                                           ^\r\n./tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:84:45: error: 'output_depth' was not declared in this scope\r\n static_assert(offsetof(DepthwiseConvParams, output_depth) ==\r\n                                             ^\r\n./tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:84:57: error: 'offsetof' was not declared in this scope\r\n static_assert(offsetof(DepthwiseConvParams, output_depth) ==\r\n                                                         ^\r\n./tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:86:43: error: expected primary-expression before ',' token\r\n static_assert(offsetof(DepthwiseConvParams, output_row_size) ==\r\n                                           ^\r\n./tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:86:45: error: 'output_row_size' was not declared in this scope\r\n static_assert(offsetof(DepthwiseConvParams, output_row_size) ==\r\n                                             ^\r\n./tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:86:60: error: 'offsetof' was not declared in this scope\r\n static_assert(offsetof(DepthwiseConvParams, output_row_size) ==\r\n                                                            ^\r\n./tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:88:43: error: expected primary-expression before ',' token\r\n static_assert(offsetof(DepthwiseConvParams, input_offset) ==\r\n                                           ^\r\n./tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:88:45: error: 'input_offset' was not declared in this scope\r\n static_assert(offsetof(DepthwiseConvParams, input_offset) ==\r\n                                             ^\r\n./tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:88:57: error: 'offsetof' was not declared in this scope\r\n static_assert(offsetof(DepthwiseConvParams, input_offset) ==\r\n                                                         ^\r\n./tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:90:43: error: expected primary-expression before ',' token\r\n static_assert(offsetof(DepthwiseConvParams, output_offset) ==\r\n                                           ^\r\n./tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:90:45: error: 'output_offset' was not declared in this scope\r\n static_assert(offsetof(DepthwiseConvParams, output_offset) ==\r\n                                             ^\r\n./tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:90:58: error: 'offsetof' was not declared in this scope\r\n static_assert(offsetof(DepthwiseConvParams, output_offset) ==\r\n                                                          ^\r\n./tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:92:43: error: expected primary-expression before ',' token\r\n static_assert(offsetof(DepthwiseConvParams, filter_offset) ==\r\n                                           ^\r\n./tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:92:45: error: 'filter_offset' was not declared in this scope\r\n static_assert(offsetof(DepthwiseConvParams, filter_offset) ==\r\n                                             ^\r\n./tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:92:58: error: 'offsetof' was not declared in this scope\r\n static_assert(offsetof(DepthwiseConvParams, filter_offset) ==\r\n                                                          ^\r\n...\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 21412.854s, Critical Path: 162.92s\r\nINFO: 3111 processes: 3111 local.\r\n```", "@psyhtest Don't know why that happened. I ran Debian on an internal board and also chroot() debian rootfs on various rooted Android devices. Tested with both gcc-6 and gcc-7.", "@freedomtan I will try on a couple of other boards. This failure might be due to gcc-5 not being able to reach some headers. But this suggests a proper fix is needed (although it's useful to know about the workaround).", "Same result on HiKey960 with gcc 7.2 and Bazel 0.14.1:\r\n```\r\nERROR: /home/anton/CK_TOOLS/lib-tensorflow-src-cpu-1.9-compiler.python-2.7.13-linux-64/src/tensorflow/contrib/lite/kernels/BUILD:128:1: C++ compilation of rule '//tensorflow/contrib/lite/kernels:builtin_ops' failed (Exit 1)\r\nIn file included from ./tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8.h:21:0,                                                                          \r\n                 from tensorflow/contrib/lite/kernels/depthwise_conv.cc:26:                                                                                          \r\n./tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:80:43: error: expected primary-expression before ',' token      \r\n static_assert(offsetof(DepthwiseConvParams, input_depth) ==      \r\n                                           ^                     \r\n./tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:80:45: error: 'input_depth' was not declared in this scope \r\n static_assert(offsetof(DepthwiseConvParams, input_depth) ==                                                                    \r\n                                             ^~~~~~~~~~~                                                                                                                                                                          \r\n./tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:80:56: error: 'offsetof' was not declared in this scope                                                                 \r\n static_assert(offsetof(DepthwiseConvParams, input_depth) ==          \r\n                                                        ^   \r\n...\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 16652.070s, Critical Path: 857.48s\r\nINFO: 4664 processes, local.\r\nFAILED: Build did NOT complete successfully\r\n```", "@freedomtan Apologies, I've only just noticed that you opened this PR on 17 January 2018, i.e. between TF 1.4 (2 November 2017) and TF 1.5 (26 January 2018), while I didn't have any problem building on aarch64 platforms until 1.8 (19 April 2018). So your fix has helped me to make progress on building TF 1.9, but the actual failure is different. Thanks.", "@psyhtest I usually build master branch. Yes, I sent this PR in Jan, 2018 when I was trying to build TF Lite at that time. I use TensorFlow master on the board and build pip package from time to time since then.\r\n\r\nRegarding the offsetof(), which is supposed to be defined in `stddef.h`, problem, I don't know why NVIDIA's Linux doesn't define it. Maybe Denver 2 of TX2 is weird. However, I think something like\r\n```\r\n#define offsetof(type, member) ((size_t)* ((type *) 0)->member)\r\n```\r\nshould work most of the time. And for HiKey 960, which Linux distro your use?", "@freedomtan This problem might have appeared between the 1.4 and 1.5 releases. Unless you know the exact revision, we might never find out. (Not that I want to become a TF archaeologist. :)\r\n\r\nAnyway, it is back with a vengeance. Still, I managed to build it on HiKey960 (Debian 9) by commenting out all asserts in `depthwiseconv_uint8_3x3_filter.h` as follows:\r\n```\r\nanton@hikey962:~/CK_REPOS/ck-tensorflow$ cat package/lib-tensorflow-1.9.0-src-cpu/patch.linux/offsetof-off.patch\r\ndiff --git a/tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h b/tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h\r\nindex 8cd72239e9..a7c4cf8f76 100644\r\n--- a/tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h\r\n+++ b/tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h\r\n@@ -76,7 +76,7 @@ struct DepthwiseConvParams {\r\n #define OFFSET_INPUT_HEIGHT 64\r\n #define OFFSET_OUTPUT_WIDTH 68\r\n #define OFFSET_OUTPUT_HEIGHT 72\r\n-\r\n+#if 0\r\n static_assert(offsetof(DepthwiseConvParams, input_depth) ==\r\n                   OFFSET_INPUT_DEPTH, \"\");\r\n static_assert(offsetof(DepthwiseConvParams, input_row_size) ==\r\n@@ -107,7 +107,7 @@ static_assert(offsetof(DepthwiseConvParams, output_width) ==\r\n                   OFFSET_OUTPUT_WIDTH, \"\");\r\n static_assert(offsetof(DepthwiseConvParams, output_height) ==\r\n                   OFFSET_OUTPUT_HEIGHT, \"\");\r\n-\r\n+#endif\r\n```", "@psyhtest I've been using Debian unstable (which is supposed to be Debian 10 under development at this time) to build TensorFlow master branch on my board and chroot() Android devices more than one year, which includes 1.4 to 1.9, without running into `offsetof()` problem. \r\n\r\nSo, if your environment has `offsetoff()` problem, I think probably the \"right\" solution is to add something like, \r\n```\r\n#ifdef MY_DEBIAN_WHATEVER_WITHOUT_OFFSETOF\r\n#define offsetof(type, member) ......\r\n#endif\r\n````", "it turns out why aarch64/arm64 falls into `arm:` is bazel's problem, after https://github.com/bazelbuild/bazel/commit/886d01c89fca32e46f5841081eb3288e5b4f313b, in bazel 0.16.0 or later, arm64 will be recognized as `aarch64`. So I'll revert this and add `aarch64` related settings.", "Additional notes.\r\n\r\nWithout this patch, I can build pip wheel successfully, but when running something use TF Lite Python binding, e.g., the [label_image in TF Lite Python binding](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/examples/python/label_image.md), I got error messages the same as what reported in https://github.com/tensorflow/tensorflow/issues/21574\r\n\r\n```\r\nImportError: /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so: undefined symbol: _ZN6tflite12tensor_utils39NeonMatrixBatchVectorMultiplyAccumulateEPKfiiS2_iPfi\r\n```\r\n\r\nAnd when building something like TF Lite benchmark_model or label_image for TF Lite, I saw \r\n\r\n```\r\nERROR: /home/freedom/work/tensorflow/tensorflow/contrib/lite/tools/benchmark/BUILD:19:1: Linking of rule '//tensorflow/contrib/lite/tools/benchmark:benchmark_model' failed (Exit 1)\r\nbazel-out/aarch64-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensor_utils.o:tensor_utils.cc:function tflite::tensor_utils::MatrixBatchVectorMultiplyAccumulate(float const*, int, int, float const*, int, float*, int): error: undefined reference to 'tflite::tensor_utils::NeonMatrixBatchVectorMultiplyAccumulate(float const*, int, int, float const*, int, float*, int)'\r\nbazel-out/aarch64-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensor_utils.o:tensor_utils.cc:function tflite::tensor_utils::MatrixBatchVectorMultiplyAccumulate(signed char const*, int, int, signed char const*, float const*, int, float*, int): error: undefined reference to 'tflite::tensor_utils::NeonMatrixBatchVectorMultiplyAccumulate(signed char const*, int, int, signed char const*, float const*, int, float*, int)'\r\nbazel-out/aarch64-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensor_utils.o:tensor_utils.cc:function tflite::tensor_utils::VectorVectorCwiseProduct(float const*, float const*, int, float*): error: undefined reference to 'tflite::tensor_utils::NeonVectorVectorCwiseProduct(float const*, float const*, int, float*)'\r\nbazel-out/aarch64-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensor_utils.o:tensor_utils.cc:function tflite::tensor_utils::VectorVectorCwiseProductAccumulate(float const*, float const*, int, float*): error: undefined reference to 'tflite::tensor_utils::NeonVectorVectorCwiseProductAccumulate(float const*, float const*, int, float*)'\r\nbazel-out/aarch64-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensor_utils.o:tensor_utils.cc:function tflite::tensor_utils::VectorBatchVectorCwiseProduct(float const*, int, float const*, int, float*): error: undefined reference to 'tflite::tensor_utils::NeonVectorBatchVectorCwiseProduct(float const*, int, float const*, int, float*)'\r\nbazel-out/aarch64-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensor_utils.o:tensor_utils.cc:function tflite::tensor_utils::VectorBatchVectorCwiseProductAccumulate(float const*, int, float const*, int, float*): error: undefined reference to 'tflite::tensor_utils::NeonVectorBatchVectorCwiseProductAccumulate(float const*, int, float const*, int, float*)'\r\nbazel-out/aarch64-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensor_utils.o:tensor_utils.cc:function tflite::tensor_utils::BatchVectorBatchVectorDotProduct(float const*, float const*, int, int, float*, int): error: undefined reference to 'tflite::tensor_utils::NeonBatchVectorBatchVectorDotProduct(float const*, float const*, int, int, float*, int)'\r\nbazel-out/aarch64-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensor_utils.o:tensor_utils.cc:function tflite::tensor_utils::Sub1Vector(float const*, int, float*): error: undefined reference to 'tflite::tensor_utils::NeonSub1Vector(float const*, int, float*)'\r\nbazel-out/aarch64-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensor_utils.o:tensor_utils.cc:function tflite::tensor_utils::IsZeroVector(float const*, int): error: undefined reference to 'tflite::tensor_utils::NeonIsZeroVector(float const*, int)'\r\nbazel-out/aarch64-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensor_utils.o:tensor_utils.cc:function tflite::tensor_utils::VectorScalarMultiply(signed char const*, int, float, float*): error: undefined reference to 'tflite::tensor_utils::NeonVectorScalarMultiply(signed char const*, int, float, float*)'\r\nbazel-out/aarch64-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensor_utils.o:tensor_utils.cc:function tflite::tensor_utils::ClipVector(float const*, int, float, float*): error: undefined reference to 'tflite::tensor_utils::NeonClipVector(float const*, int, float, float*)'\r\nbazel-out/aarch64-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensor_utils.o:tensor_utils.cc:function tflite::tensor_utils::SymmetricQuantizeFloats(float const*, int, signed char*, float*, float*, float*): error: undefined reference to 'tflite::tensor_utils::NeonSymmetricQuantizeFloats(float const*, int, signed char*, float*, float*, float*)'\r\nbazel-out/aarch64-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensor_utils.o:tensor_utils.cc:function tflite::tensor_utils::VectorShiftLeft(float*, int, float): error: undefined reference to 'tflite::tensor_utils::NeonVectorShiftLeft(float*, int, float)'\r\nbazel-out/aarch64-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensor_utils.o:tensor_utils.cc:function tflite::tensor_utils::ReductionSumVector(float const*, float*, int, int): error: undefined reference to 'tflite::tensor_utils::NeonReductionSumVector(float const*, float*, int, int)'\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/contrib/lite/tools/benchmark:benchmark_model failed to build\r\n```", "The one I used to build and test this patch last time was 0.16.1", "rebased because TFLite moved from tensorflow/contrib to tensorflow/", "Vaguely related PR: #24876"]}, {"number": 16174, "title": "Failed to Create Session: CUDA_ERROR_UNKNOWN", "body": "Failed to create session:\r\n```\r\nimport tensorflow as tf\r\ntf.Session()\r\n```\r\nError info:\r\nE tensorflow/core/common_runtime/direct_session.cc:170] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_UNKNOWN\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/ubuntu/envs/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1482, in __init__\r\n    super(Session, self).__init__(target, graph, config=config)\r\n  File \"/home/ubuntu/envs/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 622, in __init__\r\n    self._session = tf_session.TF_NewDeprecatedSession(opts, status)\r\n  File \"/home/ubuntu/envs/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InternalError: Failed to create session.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: tensorflow-gpu ('v1.4.0-19-ga52c8d9', '1.4.1')\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA 8.0, cuDNN 6.0\r\n- **GPU model and memory**: Tesla M40 24GB\r\n- **Exact command to reproduce**: N/A\r\n", "comments": ["Seems like the issue https://github.com/NVIDIA/nvidia-docker/issues/262. Take a look if that can help.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Turns out to be the driver's issue. I have sent it to NVIDIA and they are solving it.", "Hi \uff0c@b1ueshad0w , has NVIDIA solved this issue\uff1f"]}, {"number": 16173, "title": "Add C++ toolchain for portable Linux builds", "body": "See #15777", "comments": ["gunan@ asked me to test if TensorFlow pip package built with this clang crosstool on Ubuntu 16 will run correctly on Ubuntu 14. Working on testing that now.", "Oh by the way @case540, here are some notes I took on building Clang from source:\r\n\r\n```sh\r\n# Install CMake (to build Clang snapshot)\r\nsudo apt-get purge 'cmake*'\r\nwget https://cmake.org/files/v3.10/cmake-3.10.1.tar.gz\r\ntar xvzf cmake-3.10.1.tar.gz\r\npushd cmake-3.10.1\r\nCFLAGS='-O2 -march=native' CXXFLAGS='-O2 -march=native' ./bootstrap --prefix=/usr --parallel=96\r\nmake CFLAGS='-O2 -march=native' CXXFLAGS='-O2 -march=native' -j96\r\nsudo make install\r\npopd\r\n\r\n# Install Ninja\r\ngit clone git://github.com/ninja-build/ninja.git\r\npushd ninja\r\ngit checkout release\r\nCC=clang-6.0 CXX=clang++-6.0 CFLAGS='-O2 -march=native' CXXFLAGS='-O2 -march=native' ./configure.py --bootstrap\r\nsudo cp ninja /usr/bin\r\npopd\r\n\r\n# Install Clang 7.0\r\nsvn co http://llvm.org/svn/llvm-project/llvm/trunk llvm\r\ncd llvm/tools\r\nsvn co http://llvm.org/svn/llvm-project/cfe/trunk clang\r\ncd clang/tools  # llvm/tools/clang/tools\r\nsvn co http://llvm.org/svn/llvm-project/clang-tools-extra/trunk extra\r\ncd ../..  # llvm/tools\r\nsvn co http://llvm.org/svn/llvm-project/lld/trunk lld\r\nsvn co http://llvm.org/svn/llvm-project/polly/trunk polly\r\ncd ../projects  # llvm/projects\r\nsvn co http://llvm.org/svn/llvm-project/compiler-rt/trunk compiler-rt\r\nsvn co http://llvm.org/svn/llvm-project/openmp/trunk openmp\r\nsvn co http://llvm.org/svn/llvm-project/libcxx/trunk libcxx\r\nsvn co http://llvm.org/svn/llvm-project/libcxxabi/trunk libcxxabi\r\ncd ..  # llvm\r\n\r\n# Note: Build Clang using GCC 4.8.2+ (rather than Clang)\r\nllvm_build_opts=(\r\n  -DBUILD_SHARED_LIBS=On\r\n  -DCMAKE_BUILD_TYPE=Release\r\n  -DCMAKE_CXX_FLAGS=-march=native\r\n  -DCMAKE_C_FLAGS=-march=native\r\n  -DCMAKE_EXE_LINKER_FLAGS=-fuse-ld=gold\r\n  -DCMAKE_EXPORT_COMPILE_COMMANDS=ON\r\n  -DLLVM_EXPERIMENTAL_TARGETS_TO_BUILD=WebAssembly\r\n  -DLLVM_OPTIMIZED_TABLEGEN=On\r\n  -DLLVM_PARALLEL_COMPILE_JOBS=100\r\n  -DLLVM_PARALLEL_LINK_JOBS=50\r\n)\r\nmkdir _build\r\ncd _build\r\ncmake -G Ninja .. \"${llvm_build_opts[@]}\"\r\nninja\r\nsudo cmake -DCMAKE_INSTALL_PREFIX=/usr/lib/llvm-7.0 -P cmake_install.cmake\r\n```\r\n\r\nI was experimenting to see if Clang 7.0 unstable would be a better choice, but it seemed a little\u2026 unstable, and didn't have some of the bleeding edge features out of the box I was hoping it'd have.\r\n\r\nSo if you end up needing to build from source, hopefully those notes will save you time.", "I tried building using this crosstool on Ubuntu 16 and ran into some issues. I made the following modifications to the crosstool file...\r\n- Changed glibc and libstdc++ to the versions that were installed by default\r\n- Changed -fuse-ld=gold to -fuse-ld=lld since I kept getting an error that \"gold\" was not a valid choice.\r\n\r\nThe build makes it a good bit of the way through but then I ran into this error. Any idea what the issue could be? Have you seen this before?\r\n\r\nERROR: /home/mikecase/.cache/bazel/_bazel_mikecase/c22aed87a372cfab86407106f2bbe57d/external/protobuf_archive/BUILD:271:1: Executing genrule @protobuf_archive//:generate_js_well_known_types_embed failed (Segmentation fault): bash failed: error executing command ", "@jart please resolve conflicts so we can merge this.", "@rmlarsen This is now sync'd and ready to merge.\r\n\r\n@case540 and I spoke offline about troubleshooting segfaults with Bazel.\r\n\r\nGentlemen, here's to looking forward to a smooth 1.5 that exceeds our users' expectations by being boring and maintaining compatibility commitments.", "I built using the clang6 toolchain on ubuntu 16.\r\n\r\nI modified the crosstool with the following versions (to what was installed by default on the machine)...\r\nglibc to from 2.19 -> 2.23\r\nlibstdc++ from 4.8 -> 5.4.0\r\n\r\nUnfortunately, I still hit the same error...\r\n\r\nImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.20' not found\r\n\r\n...when running this pip package on Ubuntu 14. Anything else I should try?\r\n\r\n", "@case540 I can't remember, did we use `-nostdlib` and `-nostdlib++` flags so we could use the force in manually defining these `-L` and `-l` things ourselves?", "I did not use those flags when building. Is that something I should try out?", "@case540 Yes absolutely give them a try.", "I wouldn't be surprised if you could manually extract old versions of libc, libstdc++, libgcc, etc. into hard-coded folders and Clang and then finagle Clang 6.0 on Ubuntu 16 to just ignore what's on the system and use *only* those highly specific paths you've specified.", "Talked to someone from one of the compiler teams. Sounds like it might be possible to compile against glibc libraries extracted from Ubuntu 14 on Ubuntu 16. But he did not recommend doing so. Will give it a try when I have some spare time and see how bad it is.", "That's a sane default recommendation. It's just that, based on what I've been told, not making this heroic hack work means your team might have no choice but to own a monolithic megacore Debian 8 buildbox, which honestly, IMHO ain't so bad."]}, {"number": 16172, "title": "Merge changes from r1.5 into master", "body": "This change picks up the commits exclusive to the r1.5 branch and puts them back into master.\r\n\r\nThere were a bunch of merge conflicts here. I favored master in most cases except those to do with obvious versioning differences.\r\n\r\nI'm not sure if I did the merge correctly, considering there are a great many CLs presented here.", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "I addressed @av8ramit's changes after doing some hearty investigation as to why they happened.", "Re-un-disabled some tests that were removed by a merge."]}]