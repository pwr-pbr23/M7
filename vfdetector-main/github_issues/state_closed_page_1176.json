[{"number": 17911, "title": "spelling fix", "body": "", "comments": []}, {"number": 17910, "title": "Timeline Logging Duplicates of Operations", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.6.0-rc1\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.11.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4\r\n- **CUDA/cuDNN version**: 9.1/7.0\r\n- **GPU model and memory**: Tesla k80 (11441MiB)\r\n- **Exact command to reproduce**: python cifar10_train.py\r\n\r\n\r\n### Describe the problem\r\nI used timeline to profile the time taken by each operation of the standard cifar10 model available in tensorflow/models repo. After looking at the logfile, it looks like logs of some of the operations are duplicated i.e. it looks like some of the operations in the graph are ran multiple times over the single run of the complete graph. For example, Operation \"gradients/conv2/Conv2D_grad/Conv2DBackpropFilter\" (link to logfile : https://gist.github.com/xilenteyex/d54305e0448e1aa3d878872c45b8ed3a#file-timeline-1-json-L2270) is logged multiple times. Is this some sort of bug or am I missing something? \r\n\r\nThanks a lot for looking into this issue!  \r\n\r\n\r\n### Source code / logs\r\ncifar10 example : https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10\r\nhere is a the link to my modified version of cifar10_train.py in which I added logging : \r\nhttps://gist.github.com/xilenteyex/b6fab3a5abdb65bf674aa7d0a4ec4b5c\r\none of the sample log files is : https://gist.github.com/xilenteyex/d54305e0448e1aa3d878872c45b8ed3a\r\n\r\n", "comments": ["@prb12, PTAL.\r\n", "Any sort of Input is much appreciated. \r\nThanks!\r\n", "Well, it's very hard to see what the problem might be from the \"modified code\" link you sent because the timeline capturing code is commented out in a way that obfuscates the original control-flow:\r\n```\r\n      #run_metadata = tf.RunMetadata()\r\n      #count = 0\r\n      while not mon_sess.should_stop():\r\n#        mon_sess.run(train_op, options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE), run_metadata=run_metadata)\r\n        mon_sess.run(train_op)\r\n#        from tensorflow.python.client import timeline\r\n#        trace = timeline.Timeline(step_stats=run_metadata.step_stats)\r\n#        trace_file = open('timeline_%d_cifar10_cpu.ctf.json' % (count), 'w')\r\n#        trace_file.write(trace.generate_chrome_trace_format())\r\n#        count += 1\r\n```\r\n\r\nBut... the fact that you create the `tf.RunMetadata` protobuf outside the loop and potentially reuse it many times means that you quite probably have merged a bunch of `StepStats` into the same protobuf.  The `Timeline` code would be very confused by this.  \r\n\r\nTo debug, I would suggest writing `step_stats` to a file, or printing it out.  If there are numerous copies of the same step/ops in there then its not a timeline problem.\r\n\r\nBut first I would try making a new `tf.RunMetadata` object for each `sess.run` call, make sure you just trace a *single* step and only write the trace file once.\r\n\r\nFailing any of those, I would suspect maybe something funny with the CUPTI library version and/or GPU driver-  but that seems very unlikely.\r\n\r\n", "Hi @prb12 ,\r\nThanks for replying.\r\n\r\nI am trying to understand logging in tensorflow in depth. I think cifar is slightly a bigger model to start with. Now, I am using a MNIST example. Script that I ran can be found [(here)](https://gist.github.com/xilenteyex/f652be2306573020eec9152a87915324). Following is the list of issues:\r\n\r\n- I made sure that run_metadata is called separately for every sess.run call. I am still seeing multiple entries for the same operation in the same thread . For example, op named \"train/Adam/update_layer2/weights/Variable/ApplyAdam\" is logged three times in pid = '11'. Does this really mean that my model is designed in such a way that it requires \"train/Adam/update_layer2/weights/Variable/ApplyAdam\" op to be executed three times for each sess.run call ? (step_stats can be found [here](https://gist.github.com/xilenteyex/aef5dd189fe210acd4d8720c804b43a8) and timline can be found [here](https://gist.github.com/xilenteyex/6ac4f81d767bc6a2cab89822a6d38584))\r\n\r\n- According to my understanding (please correct me, if I am wrong). There are 4 threads running on GPU, one is responsible for MEMCPYHtoD, one for MEMCPYDtoH, one for MEMCPYPtoP and one thread for execution of compute nodes. But, I see a lot of GPU streams in the timeline, is this just an artifact of timeline to enable it to be visualized in the timeline or it has some other significance ?\r\n\r\n- Also, reading ur comments, on [other issues](https://github.com/tensorflow/tensorflow/issues/1824#issuecomment-244251867), I understand that the pid named : \"/job:localhost/replica:0/task:0/device:GPU:0 Compute\" just logs the time it takes to launch the kernel on the GPU. Actual execution time is logged in the streams named \"/device:GPU:0/stream:* Compute\". But after looking at the timestamps and the duration, it looks like \"/job:localhost/replica:0/task:0/device:GPU:0 Compute\" is actually logging the total time (kernel launch + actual compute node execution time on GPU) e.g. \r\nfor the op \"train/Adam/update_layer2/weights/Variable/ApplyAdam\", there is one entry in \"/job:localhost/replica:0/task:0/device:GPU:0 Compute\" (pid : 9) which starts at timestamp : 1536530947160930 and ends at 1536530947161023, but there are three entries in the pid : 11, named : \"/device:GPU:0/stream:22 Compute\". These three entries start and end with timestamps as follows:\r\n(start, end)\r\n(1536530947160982, 1536530947160984)\r\n(1536530947160999, 1536530947161001)\r\n(1536530947161016, 1536530947161021)\r\nAll three of these have starting time after the start timestamp of \"/job:localhost/replica:0/task:0/device:GPU:0 Compute\" and ends before the end timestamp of \"/job:localhost/replica:0/task:0/device:GPU:0 Compute\". What does this really mean ? What does these gaps in these three timeslots show? Also, I expect actual execution to start after the kernel has been queued. These timestamps show otherwise. Am I missing something ? Also, if they are executed three times, I expect three kernels to be queued, but \"/job:localhost/replica:0/task:0/device:GPU:0 Compute\" process has only one entry.\r\nAlso, what is the difference between two GPU streams \"/device:GPU:0/stream:22 Compute\" (pid: 11) and \"/device:GPU:0/stream:all Compute\" (pid: 7)\r\n\r\n- In pid: \"5\" named \"/device:GPU:0/memcpy Compute\", I assumed that it will only contain memcpy nvidia cuda operations. But, there is an op of type \"Assign\" in this stream. Is this the expected behavior ?\r\n\r\n- In order to log the time it takes to send a tensor from one one device to another, is it enough to look at the duration and timestamps of memcpy ops ? or do i need to enable logging of send/recv op nodes using the hack as discussed in this [github issue](https://github.com/tensorflow/tensorflow/issues/4809) ? If those memcpy ops report the correct times and we don't really need the hack to enable the send/recv ops. Is there a way to map MEMCPY ops to tensors ?\r\n\r\n- Also, whats the purpose of \"/device:GPU:0/stream:* Tensors\" and \"/job:localhost/replica:0/task:0/device:GPU:0 Tensors\" processes ?\r\n\r\n- Overall, for a given tensorflow graph, I am interested in creating a timeline programmatically (not UI, raw values), using which I can find the time at which an op (compute as well as communication op) started and the time it took to complete. Do you think tensorflow.timeline is the right way to go about doing this ?\r\n\r\nP.S. : I am sorry, I know this is not a discussion forum and lack of documentation about usage and what each field represents about logging, I am unable to figure out how to use this exactly. I tried asking the same thing over stackoverflow a few times, but my questions remained unanswered. I am stuck at this for almost 3 months now. If you can clarify all these things, that wil be awesome! \r\n\r\nThanks a lot for looking into this.", "@prb12 ,  it will be great if you can look into this.\r\n\r\nThanks!", "We see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 17909, "title": "Test cases fail on Nvidia Jetson TX2 for Tensorflow v1.4", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 LTS\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.4.0-0-gd752244\r\n- **Python version**: Python 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: V8.0.72/6.0.21\r\n- **GPU model and memory**: NVIDIA Pascal GPU/8GB 128-bit LPDDR4\r\n- **Exact command to reproduce**: \r\nbazel test --build_tests_only -c opt --local_resources 2048,1.0,2.0 --verbose_failures --config=cuda --test_verbose_timeout_warnings //tensorflow/python/...\r\n\r\nconfiguration output (\".tf_configure.bazelrc\" file):\r\n\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/local/lib/python2.7/dist-packages\"\r\nbuild --define PYTHON_BIN_PATH=\"/usr/bin/python\"\r\nbuild --define PYTHON_LIB_PATH=\"/usr/local/lib/python2.7/dist-packages\"\r\nbuild --force_python=py2\r\nbuild --host_force_python=py2\r\nbuild --python2_path=\"/usr/bin/python\"\r\ntest --force_python=py2\r\ntest --host_force_python=py2\r\ntest --define PYTHON_BIN_PATH=\"/usr/bin/python\"\r\ntest --define PYTHON_LIB_PATH=\"/usr/local/lib/python2.7/dist-packages\"\r\nrun --define PYTHON_BIN_PATH=\"/usr/bin/python\"\r\nrun --define PYTHON_LIB_PATH=\"/usr/local/lib/python2.7/dist-packages\"\r\nbuild --define with_jemalloc=true\r\nbuild:opt --cxxopt=-march=native --copt=-march=native\r\nbuild --action_env TF_NEED_CUDA=\"1\"\r\nbuild --action_env TF_NEED_OPENCL=\"0\"\r\nbuild --action_env TF_CUDA_CLANG=\"0\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr/local/cuda\"\r\nbuild --action_env TF_CUDA_VERSION=\"8.0\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc\"\r\nbuild --action_env TF_CUDNN_VERSION=\"\"\r\nbuild --action_env CUDNN_INSTALL_PATH=\"/usr/lib/aarch64-linux-gnu\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"6.2\"\r\n\r\n### Describe the problem\r\nWe build Tensorflow v1.4 and install it in a Nvidia Jetson TX2. The device is equipped with ARM Cortex-A57 quad-core, Nvidia Denver 2 dual-core and 256-core Pascal GPU. Then we run the unit test cases using the above command. 26 out of 456 tensorflow test cases fail on this platform.\r\n\r\nThe failing tests are shown below. Detailed logs can be found in the attachment.\r\n\r\nkernel_tests:string_to_hash_bucket_op_test\r\nkernel_tests:sparse_matmul_op_test\r\nkernel_tests:spacetobatch_op_test\r\nkernel_tests:self_adjoint_eig_op_test: timeout\r\nkernel_tests:lookup_ops_test\r\nkernel_tests:large_concat_op_test\r\nkernel_tests:large_concat_op_test\r\nkernel_tests:depthtospace_op_test: timeout\r\nkernel_tests:denormal_test\r\nkernel_tests:conv_ops_test\r\nkernel_tests:cast_op_test\r\neager:tensor_test\r\ndebug:tensor_format_test\r\ndebug:session_debug_grpc_test\r\ndebug:dist_session_debug_grpc_test\r\ndebug:curses_ui_test\r\ndebug:analyzer_cli_test\r\npython:special_math_ops_test\r\npython:session_test\r\npython:session_list_devices_test\r\npython:session_clusterspec_prop_test\r\npython:saver_large_partitioned_variable_test\r\npython:nn_fused_batchnorm_test\r\npython:item_test\r\npython:image_grad_test\r\npython:framework_importer_test\r\n\r\n\r\n\r\n### Source code / logs\r\nSee attached:\r\n\r\n[fail.zip](https://github.com/tensorflow/tensorflow/files/1835427/fail.zip)\r\n", "comments": ["Nagging Assignee @shivaniag: It has been 185 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 17908, "title": "contrib.nccl.broadcast raises UnimplementedError", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Docker image tensorflow/tensorflow:nightly-devel-gpu\r\n- **TensorFlow version (use command below)**: nightly (also 1.7rc1)\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: 7.0.5\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**:\r\n\r\nCopying repro from https://github.com/tensorflow/tensorflow/issues/15425#issuecomment-361835192:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.nccl as nccl\r\n\r\nif __name__ == '__main__':\r\n    devices = ['/gpu:0', '/gpu:0']\r\n    with tf.device(devices[0]):\r\n        data0 = tf.constant([1.1, 2.2, 3.3, 4.4])\r\n        received_tensor = nccl.broadcast(data0)\r\n    received_tensors = []\r\n    for device in devices[1:]:\r\n        with tf.device(device):\r\n            received_tensors.append(tf.identity(received_tensor))\r\n    sess = tf.Session()\r\n    sess.run(received_tensors)\r\n```\r\n\r\n### Describe the problem\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 11, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 906, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1141, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1322, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1341, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.UnimplementedError: This op should be replaced during graph optimization.\r\n\t [[Node: NcclBroadcast = NcclBroadcast[T=DT_FLOAT, shape=[4], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Const)]]\r\n\r\nCaused by op u'NcclBroadcast', defined at:\r\n  File \"<stdin>\", line 5, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/nccl/python/ops/nccl_ops.py\", line 187, in broadcast\r\n    return gen_nccl_ops.nccl_broadcast(input=tensor, shape=tensor.shape)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/nccl/ops/gen_nccl_ops.py\", line 132, in nccl_broadcast\r\n    \"NcclBroadcast\", input=input, shape=shape, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3305, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1669, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nUnimplementedError (see above for traceback): This op should be replaced during graph optimization.\r\n\t [[Node: NcclBroadcast = NcclBroadcast[T=DT_FLOAT, shape=[4], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Const)]]\r\n```", "comments": ["I can reproduce the bug. Assigning.", "I get same issue on install from source tf-1.7.0", "Get the same issue on TF-1.7 installing from source.", "Analysis: graph optimization is implemented in [`tensorflow/contrib/nccl/kernels/nccl_rewrite.cc`](https://github.com/tensorflow/tensorflow/blob/v1.9.0/tensorflow/contrib/nccl/kernels/nccl_rewrite.cc). The only bazel target that compiles that file is [`//tensorflow/contrib/nccl:nccl_kernels`](https://github.com/tensorflow/tensorflow/blob/v1.9.0/tensorflow/contrib/nccl/BUILD#L74). I found two other targets that seem to depend on `//tensorflow/contrib/nccl:nccl_kernels`: [`//tensorflow/contrib:contrib_kernels`](https://github.com/tensorflow/tensorflow/blob/v1.9.0/tensorflow/contrib/BUILD#L149) and [`//tensorflow/contrib/nccl:nccl_py`](https://github.com/tensorflow/tensorflow/blob/v1.9.0/tensorflow/contrib/nccl/BUILD#L107).\r\n\r\nFor the first target `//tensorflow/contrib:contrib_kernels`, I could not find any targets that depend on this one. And for the target `//tensorflow/contrib/nccl:nccl_py`, it uses `tf_custom_op_py_library` to define a target, where the dependency is passed as `kernels` argument. But in the definition of `tf_custom_op_py_library`, the `kernels` argument [is ignored](https://github.com/tensorflow/tensorflow/blob/v1.9.0/tensorflow/tensorflow.bzl#L1342). So it seems that in no way the final python package would include the `nccl_rewrite.cc` file. So it is impossible for TensorFlow to run NCCL graph optimization pass.", "Hi @chsigg, is there any news here? I use tf 1.9.0 and have exactly the same problem. Thanks.", "Still got this problem with tf1.10", "Any news here? @chsigg ", "The file `nccl_rewrite.cc` was added by ca8af1d0dbb605087a4f8ae076188f2b9a26b1ba on Oct 15 2017.  Was it working back then?\r\n", "Nagging Assignee @chsigg: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "nccl ops are no longer in contrib and can't be run directly."]}, {"number": 17907, "title": "Error with GDR", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nRed hat\r\n- **TensorFlow installed from (source or binary)**:\r\nsource (1.6)\r\n- **TensorFlow version (use command below)**:\r\n1.6\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n0.11\r\n- **GCC/Compiler version (if compiling from source)**:\r\n4.85\r\n- **CUDA/cuDNN version**:\r\n9/7\r\n- **GPU model and memory**:\r\nP100\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nWhile building from source, I got the following error:\r\n                                                                             ^\r\nERROR: /gpfshome01/u/amalik/Tensorflow/tensorflow/tensorflow/contrib/gdr/BUILD:52:1: C++ compilation of rule '//tensorflow/contrib/gdr:gdr_memory_manager' failed (Exit 1)\r\ntensorflow/contrib/gdr/gdr_memory_manager.cc:28:27: fatal error: rdma/rdma_cma.h: No such file or directory\r\n #include <rdma/rdma_cma.h>\r\n                           ^\r\ncompilation terminated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 556.299s, Critical Path: 183.28s\r\nFAILED: Build did NOT complete successfully\r\n\r\n-----\r\nI check on google but could not find any information. \r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["You need to have Linux RDMA user space libraries installed. These libraries should be available in forms of `libibverbs-dev librdmacm-dev` (Debian) or `libibverbs-devel librdmacm-devel` (RHEL), or `rdma-core` if you are using the latest Debian unstable. You could go further to consult the [Linux RDMA](https://github.com/linux-rdma/rdma-core) community or your RDMA network vendor for setting up a proper development environment.", "Did byronyi's comment address your issue?\r\n", "yes", "@byronyi , I want to consult some problems about RDMA, could you help me?"]}, {"number": 17906, "title": "Branch 189962437", "body": "", "comments": []}, {"number": 17905, "title": "GPU sits idle for increasing time between consecutive training runs", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Version 10.0.16299 \r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.5.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: not sure (is this important here?)\r\n- **GCC/Compiler version (if compiling from source)**: not sure (is this important here?)\r\n- **CUDA/cuDNN version**: not sure (is this important here?)\r\n- **GPU model and memory**: NVIDIA Quadro P5000 16GB\r\n- **Exact command to reproduce**: Not exactly an \"exact command\", but what I'm trying to do is perform CNN training runs consecutively without the GPU stalling between subsequent runs. \r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI'm using an evolutionary algorithm to optimize the hyperparameters of a CNN. However, over time, the GPU \"waits\" for greater and greater times between training cycles (afterburner visualization):\r\n\r\n![image](https://user-images.githubusercontent.com/37384864/37736912-93ab0f72-2d28-11e8-837b-19c4da783251.png)\r\n\r\nI assume this is a memory issue in a similar vein to https://github.com/tensorflow/tensorflow/issues/17048 and http://forums.fast.ai/t/tip-clear-tensorflow-gpu-memory/1979, however, the fixes mentioned there are not working. \r\n\r\nAny ideas of a workaround? The second link above uses Keras to reset GPU memory after each run, and have tried this, but it has not solved the problem (it may have somewhat shortened the lag between training runs, but not enough to constitute a fix). \r\n\r\nWhat do you suppose is going on here? I.e. what may be the source of the problem, and resources that could help me address it?\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n(I'm not sure source code will be helpful in this general issue; let me know if you'd like to see my code).", "comments": ["@asimshankar and @vrv, you seem to have expertise in this area- any thoughts?", "I don't think it is related to GPU memory usage, as you show it gets reset properly.  My only guess is that the size of the graph being built is increasing each time, rather than creating a new graph from scratch for every new hyperparameter setting.  Theoretically these should be pruned effectively, but I wonder if this is a problem.  I don't know much about Keras details, but there may be a way to reset the \"graph\" or \"session\" between model training runs.\r\n\r\nBeyond that, I don't think we have enough information here to really know why this is happening (e.g., where is the time being idle; is it in your application program, in python, at the C++ level, etc.)", "Hi @vrv- thanks for your reply, and good point re. it maybe not being a memory issue. I will investigate Keras graph/session resets and report back, though I will note that this issue was occurring prior to using Keras in this script. If it is helpful, here is a snippet of the error report (goes on for pages):\r\n\r\nf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:662] Chunk at 00000015F2859800 of size 1280\r\n2018-03-24 05:25:21.775992: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:662] Chunk at 00000015F2859D00 of size 1280\r\n2018-03-24 05:25:21.776691: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:662] Chunk at 00000015F285A200 of size 159488\r\n2018-03-24 05:25:21.777238: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:662] Chunk at 00000015F2881100 of size 1280\r\n2018-03-24 05:25:21.777484: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:662] Chunk at 00000015F2881600 of size 1280\r\n2018-03-24 05:25:21.777704: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:662] Chunk at 00000015F2881B00 of size 95744\r\n2018-03-24 05:25:21.777897: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:662] Chunk at 00000015F2899100 of size 95744\r\n.\r\n.\r\n.\r\nLimit:                 13618452890\r\nInUse:                 13618441472\r\nMaxInUse:              13618452736\r\nNumAllocs:                48687782\r\nMaxAllocSize:           3805937664\r\n\r\n2018-03-24 05:25:22.292220: W C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:277] ****************************************************************************************************\r\n2018-03-24 05:25:22.292545: W C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1198] Resource exhausted: OOM when allocating tensor with shape[5,7,72,16] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n2018-03-24 05:25:32.294665: W C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:273] Allocator (GPU_0_bfc) ran out of memory trying to allocate 34.00MiB.  Current allocation summary follows.\r\n2018-03-24 05:25:32.295801: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:628] Bin (256): \tTotal Chunks: 1118, Chunks in use: 1118. 279.5KiB allocated for chunks. 279.5KiB in use in bin. 32.0KiB client-requested in use in bin.\r\n2018-03-24 05:25:32.297028: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:628] Bin (512): \tTotal Chunks: 746, Chunks in use: 746. 466.3KiB allocated for chunks. 466.3KiB in use in bin. 303.1KiB client-requested in use in bin.\r\n.\r\n.\r\n.\r\nTraceback (most recent call last):\r\n  File \"C:/Users/ashort7/PycharmProjects/CNN/venv1/SSVEP_TimeDomain_DeepCNN/layer_num_evo.py\", line 1040, in <module>\r\n    sess.run(init_op)\r\n  File \"C:\\Users\\ashort7\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\ashort7\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\", line 1128, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Users\\ashort7\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\", line 1344, in _do_run\r\n    options, run_metadata)\r\n  File \"C:\\Users\\ashort7\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\", line 1363, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[3,7,1,72] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[Node: Variable_910/Adam/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable_910\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Variable_910/Adam, Variable_910/Adam/Initializer/zeros)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\r\nCaused by op 'Variable_910/Adam/Assign', defined at:\r\n  File \"C:/Users/ashort7/PycharmProjects/CNN/venv1/SSVEP_TimeDomain_DeepCNN/layer_num_evo.py\", line 1017, in <module>\r\n    train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cross_entropy)\r\n  File \"C:\\Users\\ashort7\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 365, in minimize\r\n    name=name)\r\n  File \"C:\\Users\\ashort7\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 516, in apply_gradients\r\n    self._create_slots([_get_variable_for(v) for v in var_list])\r\n  File \"C:\\Users\\ashort7\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\training\\adam.py\", line 139, in _create_slots\r\n    self._zeros_slot(v, \"m\", self._name)\r\n  File \"C:\\Users\\ashort7\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 883, in _zeros_slot\r\n    named_slots[_var_key(var)] = slot_creator.create_zeros_slot(var, op_name)\r\n  File \"C:\\Users\\ashort7\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\training\\slot_creator.py\", line 174, in create_zeros_slot\r\n    colocate_with_primary=colocate_with_primary)\r\n  File \"C:\\Users\\ashort7\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\training\\slot_creator.py\", line 148, in create_slot_with_initializer\r\n    dtype)\r\n  File \"C:\\Users\\ashort7\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\training\\slot_creator.py\", line 67, in _create_slot_var\r\n    validate_shape=validate_shape)\r\n  File \"C:\\Users\\ashort7\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1262, in get_variable\r\n    constraint=constraint)\r\n  File \"C:\\Users\\ashort7\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1097, in get_variable\r\n    constraint=constraint)\r\n  File \"C:\\Users\\ashort7\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 435, in get_variable\r\n    constraint=constraint)\r\n  File \"C:\\Users\\ashort7\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 404, in _true_getter\r\n    use_resource=use_resource, constraint=constraint)\r\n  File \"C:\\Users\\ashort7\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 806, in _get_single_variable\r\n    constraint=constraint)\r\n  File \"C:\\Users\\ashort7\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 229, in __init__\r\n    constraint=constraint)\r\n  File \"C:\\Users\\ashort7\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 366, in _init_from_args\r\n    validate_shape=validate_shape).op\r\n  File \"C:\\Users\\ashort7\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\state_ops.py\", line 276, in assign\r\n    validate_shape=validate_shape)\r\n  File \"C:\\Users\\ashort7\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 62, in assign\r\n    use_locking=use_locking, name=name)\r\n  File \"C:\\Users\\ashort7\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\ashort7\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3160, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\ashort7\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1625, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[3,7,1,72] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[Node: Variable_910/Adam/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable_910\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Variable_910/Adam, Variable_910/Adam/Initializer/zeros)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n", "Is this still a problem?  It seems to me the interesting question is what's going on during the time the GPU is paused.  I'm guessing the CPU is busy doing something that gets harder over time.", "Hi poxvoculi,\r\n\r\nThanks for your response. It is still a problem, and I will examine what's going on with the CPU during the GPU pause. I'll post a screenshot next chance I get. ", "Here's a word doc with images of the afterburner output during a period when the GPU stalling is visible:\r\n[afterburner logs.docx](https://github.com/tensorflow/tensorflow/files/1876339/afterburner.logs.docx)\r\n\r\nIt seems you're correct about the CPU idea; CPUs 1 and 2 (and some of the others) tend to exhibit high activity during the \"GPU stalls\". \r\n\r\nI'll upload a .py file and some test data with a version of my CNN that should hopefully replicate the issue later today. ", "Rather than uploading your program can you capture a timeline that shows both CPU and GPU activity?", "The word doc I uploaded displays both CPU and GPU activity (though in separate images); is that what you're looking for? Note that the vertical line on the images is placed at the same time in each, so it should be easy to compare. Let me know if that's not what you're looking for. ", "I think that maybe most importantly, the GPU memory does not decrease between loading models, as you can see here: \r\n![image](https://user-images.githubusercontent.com/37384864/38323304-e1f5edd6-380a-11e8-91bd-dc7c8ed04996.png)\r\n", "I do have something else in mind, I wonder what the CPU is doing between periods when the GPU is active.  Just knowing that it's consuming power is much less interesting that knowing what task it's doing and whether running the next step (or sequence of steps?) on the GPU requires waiting for that task.\r\n\r\nYou said:\r\n\"I'm using an evolutionary algorithm to optimize the hyperparameters of a CNN. However, over time, the GPU \"waits\" for greater and greater times between training cycles (afterburner visualization):\"\r\n\r\nDoes your algorithm execute on the CPU between GPU runs?  Is it possible that your algorithm is more expensive than you anticipated?  Do you have a way of tracing the CPU activity during that time and identifying which subroutines it's executing in?\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "hello, i have got the same error, once the gpu is used for training and then i tune the hyperparameters in the same script it shows out of memeory, how to resolve it?\r\n"]}, {"number": 17904, "title": "SetUsrMemDataHandle should return void", "body": "It looks like there are two methods called `SetUsrMemDataHandle`, but this causes compilation errors because because of incorrect return signature.", "comments": ["Here is the traceback from the failed compilation.\r\n\r\n```\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/darwin-opt/bin/external/grpc/libgrpc_base_c.a(wakeup_fd_eventfd.o) has no symbols\r\nERROR: /private/tmp/c01f7f89aeea212910f96158a0abb0c2/tensorflow-1.7.0-rc1/tensorflow/core/kernels/BUILD:5996:1: C++ compilation of rule '//tensorflow/core/kernels:mkl_lrn_op' failed (Exit 1): wrapped_clang failed: error executing command\r\n  (cd /private/var/tmp/_bazel_sanghan/ff1682f621d6f58b714fa4776abe5fb4/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    APPLE_SDK_PLATFORM='' \\\r\n    APPLE_SDK_VERSION_OVERRIDE='' \\\r\n    PATH='/Users/sanghan/.opam/system/bin:/usr/local/bin:/usr/local/sbin:/usr/local/opt/python/bin:/usr/local/Caskroom/google-cloud-sdk/latest/google-cloud-sdk/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Applications/VMware Fusion.app/Contents/Public:/Library/TeX/texbin:/usr/local/MacGPG2/bin:/opt/X11/bin:/usr/local/opt/qt/bin:/usr/local/opt/go/bin:/usr/local/opt/go/libexec/bin:/usr/local/opt/coreutils/bin:/usr/local/opt/coreutils/libexec/gnubin:/Users/sanghan/bin:/Users/sanghan/Go/bin:/Users/sanghan/.iterm2:/System/Library/PrivateFrameworks/Apple80211.framework/Versions/Current/Resources:/System/Library/CoreServices/RemoteManagement/ARDAgent.app/Contents/Resources' \\\r\n    XCODE_VERSION_OVERRIDE=9.2.0 \\\r\n  external/local_config_cc/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DEIGEN_USE_VML -mavx -mavx2 -mfma -msse4.1 -msse4.2 '-std=c++11' -iquote . -iquote bazel-out/darwin-opt/genfiles -iquote external/nsync -iquote bazel-out/darwin-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/darwin-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/darwin-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/darwin-opt/genfiles/external/local_config_sycl -iquote external/com_google_absl -iquote bazel-out/darwin-opt/genfiles/external/com_google_absl -iquote external/gif_archive -iquote bazel-out/darwin-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/darwin-opt/genfiles/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/darwin-opt/genfiles/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/darwin-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/darwin-opt/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/darwin-opt/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/darwin-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/darwin-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/darwin-opt/genfiles/external/zlib_archive -iquote external/mkl -iquote bazel-out/darwin-opt/genfiles/external/mkl -iquote external/mkl_dnn -iquote bazel-out/darwin-opt/genfiles/external/mkl_dnn -isystem external/nsync/public -isystem bazel-out/darwin-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -isystem third_party/eigen3/mkl_include -isystem bazel-out/darwin-opt/genfiles/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/darwin-opt/genfiles/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/darwin-opt/genfiles/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/darwin-opt/genfiles/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/darwin-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/darwin-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/darwin-opt/genfiles/external/zlib_archive -isystem external/mkl/include -isystem bazel-out/darwin-opt/genfiles/external/mkl/include -isystem external/mkl_dnn/include -isystem bazel-out/darwin-opt/genfiles/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/darwin-opt/genfiles/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/darwin-opt/genfiles/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/darwin-opt/genfiles/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/darwin-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -MD -MF bazel-out/darwin-opt/bin/tensorflow/core/kernels/_objs/mkl_lrn_op/tensorflow/core/kernels/mkl_lrn_op.d '-frandom-seed=bazel-out/darwin-opt/bin/tensorflow/core/kernels/_objs/mkl_lrn_op/tensorflow/core/kernels/mkl_lrn_op.o' -DEIGEN_MPL2_ONLY -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DTENSORFLOW_USE_ABSL -DTF_USE_SNAPPY '-isysroot __BAZEL_XCODE_SDKROOT__' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' '-DINTEL_MKL=1' -DEIGEN_USE_VML -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/core/kernels/mkl_lrn_op.cc -o bazel-out/darwin-opt/bin/tensorflow/core/kernels/_objs/mkl_lrn_op/tensorflow/core/kernels/mkl_lrn_op.o)\r\nIn file included from tensorflow/core/kernels/mkl_lrn_op.cc:34:\r\n./tensorflow/core/util/mkl_util.h:749:3: warning: ignoring return value of function declared with 'warn_unused_result' attribute [-Wunused-result]\r\n  ctext->input_list(name, input_tensors);\r\n  ^~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~\r\n./tensorflow/core/util/mkl_util.h:1585:12: error: cannot initialize return object of type 'void *' with an rvalue of type 'void'\r\n    return user_memory_->set_data_handle(data_buffer);\r\n           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/kernels/mkl_lrn_op.cc:1046:21: warning: unused variable 'orig_output_tensor' [-Wunused-variable]\r\n      const Tensor& orig_output_tensor = MklGetInput(context, kIdxOrigOutput);\r\n                    ^\r\ntensorflow/core/kernels/mkl_lrn_op.cc:755:5: warning: ignoring return value of function declared with 'warn_unused_result' attribute [-Wunused-result]\r\n    context->GetAttr(\"workspace_enabled\", &workspace_enabled_);\r\n    ^~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/kernels/mkl_lrn_op.cc:1359:15: note: in instantiation of member function 'tensorflow::MklLRNOp<float>::MklLRNOp' requested here\r\nTF_CALL_float(REGISTER_MKL_LRN_CPU);\r\n              ^\r\ntensorflow/core/kernels/mkl_lrn_op.cc:1004:5: warning: ignoring return value of function declared with 'warn_unused_result' attribute [-Wunused-result]\r\n    context->GetAttr(\"workspace_enabled\", &workspace_enabled_);\r\n    ^~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/kernels/mkl_lrn_op.cc:1359:15: note: in instantiation of member function 'tensorflow::MklLRNGradOp<float>::MklLRNGradOp' requested here\r\nTF_CALL_float(REGISTER_MKL_LRN_CPU);\r\n              ^\r\n4 warnings and 1 error generated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 941.600s, Critical Path: 100.70s\r\nFAILED: Build did NOT complete successfully\r\n```", "As a part of Intel's TensorFlow team (owner of this code), I can confirm that the change looks fine."]}, {"number": 17903, "title": "Add Scaled Exponential Linear Unit activation (Klambauer et al, 2017)", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to `go/cla#troubleshoot`.\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "(Closing in anticipation of this being true, but reopen if I'm misunderstood something, thanks!)."]}, {"number": 17902, "title": "Allocating reusable memory in Tensorflow custom operator", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: \r\nMaxOS 10.13 and Linux version 2.6.32-696.3.2.el6.x86_64\r\n- **TensorFlow installed from (source or binary)**: \r\nsource\r\n- **TensorFlow version (use command below)**: \r\n1.5\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:  \r\n0.11.0\r\n- **GCC/Compiler version (if compiling from source)**: \r\ngcc 4.4.7\r\n- **CUDA/cuDNN version**: \r\n8.0/5.1\r\n- **GPU model and memory**: \r\nTesla K40M\r\n\r\n\r\n### Describe the problem\r\nHi, I'm having a memory related question when defining a Tensorflow custom operator with cuBLAS/C++.\r\n\r\nI defined a custom operator which needs to be called multiple times during training, computed with a BLAS level 3 algorithm.\r\nMy algorithm (as well as many other BLAS3 algorithms) requires a 'workspace' memory to exploit the hardware efficiently. However, I cannot seem to find a way to allocate such memory and keep it throughout the training process. Instead, I have to allocate a new chunk of workspace memory and free it during each call to my operator. \r\n\r\nI've also tried to define a workspace variable in Tensorflow and feed it to the operator as an input. But I found that TF treats it as a const pointer and does not allow me to write to it.\r\n\r\nSo I am wondering if there is an efficient way to allocate reusable GPU memory for the operator at the beginning, and keep it throughout the training process. If there is not, would it be better to include such features in the future versions so that many well-developed linear algebra algorithms could avoid such memory allocating issue.", "comments": ["Note that unless you set `allow_growth=True` TensorFlow pre-allocates all the GPU memory and uses its own allocator which may be already efficient. If you search for TF_CUDNN_WORKSPACE_LIMIT_IN_MB, you'll see that ops like Conv2D are allocating scratch memory during launch call rather than initialization.", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 17901, "title": "TypeError: build() got an unexpected keyword argument 'num_workers'", "body": "I am getting this error while training \r\n```\r\npython3 train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_coco.config\r\n/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 169, in <module>\r\n    tf.app.run()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"train.py\", line 165, in main\r\n    worker_job_name, is_chief, FLAGS.train_dir)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/object_detection-0.1-py3.6.egg/object_detection/trainer.py\", line 235, in train\r\n    train_config.prefetch_queue_capacity, data_augmentation_options)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/object_detection-0.1-py3.6.egg/object_detection/trainer.py\", line 59, in create_input_queue\r\n    tensor_dict = create_tensor_dict_fn()\r\n  File \"train.py\", line 122, in get_next\r\n    worker_index=FLAGS.task)).get_next()\r\nTypeError: build() got an unexpected keyword argument 'num_workers'\r\n\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "No, it's not an issue anymore.\n\nOn Mon, Apr 23, 2018 at 12:03 AM, Alfred Sorten Wolf <\nnotifications@github.com> wrote:\n\n> It has been 29 days with no activity and the awaiting response label was\n> assigned. Is this still an issue?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17901#issuecomment-383402218>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AVqJlWYbnieYJECQEJanrBr1qXVqhmxqks5trMzlgaJpZM4S01Z1>\n> .\n>\n\n\n\n-- \nBest Regards\nAmit Kumar ( CSE@IIITKalyani )\n", "Facing the same error. Can you tell how you solved it?"]}, {"number": 17900, "title": "Fix a variable name typo in the python api example", "body": "This PR is to fix a small typo in the python api example.", "comments": []}, {"number": 17899, "title": "Branch 189913309", "body": "", "comments": []}, {"number": 17898, "title": "Fix the quantized table order by float value for easy reading", "body": "This PR is to fix the quantized table order for better reading and understanding in [How to Quantize Neural Networks with TensorFlow](https://www.tensorflow.org/performance/quantization#what_representation_is_used_for_quantized_tensors).\r\nBefore fix:\r\n<figure>\r\n  <table>\r\n    <tr><th>Quantized</th><th>Float</th></tr>\r\n    <tr><td>0</td><td>-10.0</td></tr>\r\n    <tr><td>255</td><td>30.0</td></tr>\r\n    <tr><td>128</td><td>10.0</td></tr>\r\n  </table>\r\n</figure>\r\nAfter fix:\r\n<figure>\r\n  <table>\r\n    <tr><th>Quantized</th><th>Float</th></tr>\r\n    <tr><td>0</td><td>-10.0</td></tr>\r\n    <tr><td>128</td><td>10.0</td></tr>\r\n    <tr><td>255</td><td>30.0</td></tr>\r\n  </table>\r\n</figure>", "comments": ["Nagging Assignee @benoitsteiner: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@suharshs Could you pls take a look?", "Nagging Reviewer @suharshs: It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @suharshs: It has been 29 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @suharshs: It has been 45 days with no activity and the `awaiting review` label was assigned. Can you please take a look?"]}, {"number": 17897, "title": "importing and running frozen graph from old version TF produces empty output.", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.06\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.6\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4\r\n- **CUDA/cuDNN version**: 9.0 / 7\r\n- **GPU model and memory**: GTX 1080 Ti\r\n- **Exact command to reproduce**: \r\n\r\nI have a exported graph file in tensorflow 1.3 version with ubuntu 14.04 (CUDA 7.5 / cudnn 6), and it is working well under the same system so far. \r\nThe graph was frozen and I cleared device names when exported it, so it can run both on cpu/gpu mode in my previous machine ubuntu 14.04.\r\n\r\nThe problem is when I try to run the same graph file in new machine (ubuntu 16.04, tensorflow 1.6, CUDA 9, cudnn 7), the output is just empty.\r\nThere were no problem of importing the graph file and no error messages when running it. \r\nAlso the output was produced and shape is what expected except the content is empty.\r\nMy graph takes an image as input and produce an image as output. And when imported and run the graph in 16.04, the result is just black image.\r\n\r\nI also opened the graph using tensorflow/python/tools/import_pb_to_tensorboard.py, and tensorboard visualized the right network I designed. \r\n\r\nAlso tested with tensorflow version 1.5, 1.4 but the result was same.\r\n\r\nAny solution for this problem?\r\n", "comments": ["After trying installing and uninstalling, it looks like it only happens when installing from source.\r\npip install tensorflow or tensorflow-gpu works well. But once installed from source with bazel, this problem happens. ", "So, if I understand correctly - things work well if you install TensorFlow binaries for any of the new recent versions (1.5, 1.6, 1.7 etc.)? But the output tensor is empty only if building from source?\r\n\r\nHard to say what's happening without more details, or ideally an ability to reproduce the problem. Are you building from unedited source? When you say the \"output is empty\", is there no output tensor returned, or is the output tensor of the wrong shape, or is it full of zeroes, or something else?\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 17896, "title": "Fix dataset resampling bug introduced by a bug in datasets itself. fixes #16606", "body": "Fixes github issue #16606.\r\n\r\nThis is a correctly-formatted version of PR #17858.\r\n\r\nThe core issue is that in the case of certain random Tensors, the\r\nfollowing two lines aren't the same:\r\n\r\n```\r\nrand_0s_and_1s_ds = ...\r\ngather_ds = rand_0s_and_1s_ds.map(lambda i: tf.gather([0, 1], i))\r\ntup_ds = tf.data.Dataset.zip(gather_ds, rand_0s_and_1s_ds)\r\n```\r\n\r\n```\r\nrand_0s_and_1s_ds = ...\r\ntup_ds = rand_0s_and_1s_ds.map(lambda i: (tf.gather([0, 1], i), i))\r\nNote that this does NOT fix the underlying issue of drawing multiple\r\nsampes from the underlying distribution.\r\n```\r\n\r\nTested:\r\nWith the new test, bazel test :resample_test fails before and succeeds\r\nafter.", "comments": []}, {"number": 17895, "title": "Failed compilation of build_all_android.sh on Ubuntu 17", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 17\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\nMaster\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n0.10\r\n- **GCC/Compiler version (if compiling from source)**:\r\n4.9\r\n- **CUDA/cuDNN version**:\r\nNo\r\n- **GPU model and memory**:\r\nNo\r\n- **Exact command to reproduce**:\r\n./tensorflow/contrib/makefile/samples/build_and_run_inception_hexagon.sh\r\n\r\n### Describe the problem\r\n1. NDK version 16\r\n2.after build_all_android.sh download successfully  dependencies it start  building protocol buffer from sources.\r\n3.Compilation error:\r\n### Source code / logs\r\n```\r\nlibtool: compile:  aarch64-linux-android-g++ --sysroot /home//Android/Sdk/ndk-bundle/platforms/android-27/arch-arm64 -std=c++11 -DHAVE_CONFIG_H -I. -I.. -Wall -Wno-sign-compare -frtti -fexceptions -I/home/dnozik/Android/Sdk/ndk-bundle/sources/android/support/include -I/home/dnozik/Android/Sdk/ndk-bundle/sources/cxx-stl/gnu-libstdc++/4.9/include -I/home/dnozik/Android/Sdk/ndk-bundle/sources/cxx-stl/gnu-libstdc++/4.9/libs/arm64-v8a/include -MT google/protobuf/stubs/atomicops_internals_x86_gcc.lo -MD -MP -MF google/protobuf/stubs/.deps/atomicops_internals_x86_gcc.Tpo -c google/protobuf/stubs/atomicops_internals_x86_gcc.cc -o google/protobuf/stubs/atomicops_internals_x86_gcc.o\r\nIn file included from google/protobuf/stubs/atomicops_internals_x86_gcc.cc:34:0:\r\n/home//Android/Sdk/ndk-bundle/sources/cxx-stl/gnu-libstdc++/4.9/include/cstring:42:20: fatal error: string.h: No such file or directory\r\n #include <string.h>\r\n                    ^\r\ncompilation terminated.\r\nMakefile:4123: recipe for target 'google/protobuf/stubs/atomicops_internals_x86_gcc.lo' failed\r\nmake[3]: *** [google/protobuf/stubs/atomicops_internals_x86_gcc.lo] Error 1\r\nmake[3]: Leaving directory '/home//Code/Tensorflow_Hexagon/tensorflow-master/tensorflow/contrib/makefile/downloads/protobuf/src'\r\nMakefile:2109: recipe for target 'all' failed\r\n\r\n```\r\n\r\nThanks for help", "comments": ["@dimitryn Did you find a solution? I am hitting the same problem.", "install latest bazel version 14.0 , it support NDK 16.", "I'm trying to build via makefile because I hit this issue too: https://github.com/tensorflow/tensorflow/issues/11777 and wanted to try the solution"]}, {"number": 17894, "title": "onv1d", "body": "", "comments": []}, {"number": 17893, "title": "Fix the inconsistency in the accepted shape/data_format in layers doc", "body": "This PR is to fix #17892 on the the inconsistency in the accepted shape/data_format of layers doc.\r\n\r\nAs described in the above issue, the [Tensorflow Layers Guide](https://www.tensorflow.org/tutorials/layers) at specifies:\r\n> The methods in the layers module for creating convolutional and pooling layers for two-dimensional image data expect input tensors to have a shape of [batch_size, image_width, image_height, channels]\r\n\r\nWhile, the inline documentation specified for [conv2D](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/convolutional.py#L444) mentions the following valid data_formats:\r\n> data_format: A string, one of channels_last (default) or channels_first.\r\n> The ordering of the dimensions in the inputs.\r\n> channels_last corresponds to inputs with shape\r\n> (batch, height, width, channels) while channels_first corresponds to\r\n> inputs with shape (batch, channels, height, width)\r\n\r\nMoreover, [nn.bias_add, avg_pool2d etc. ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L99) also accept [...,height, width...] but using different strings/methodology.\r\n> inputs: A 4-D tensor of shape [batch_size, height, width, channels] if\r\n> data_format is NHWC, and [batch_size, channels, height, width] if\r\n> data_format is NCHW.\r\n\r\nThis PR is to fix #17892.", "comments": ["Hi,\r\n\r\nThere are several other instance where the inconsistency needs to be fixed. Moreover, for data_format=channels_first the shape should be NCHW while it is specified as CNWH. I have created #17920 to cover all the instances where the shape should be changed.\r\n\r\nThanks!"]}, {"number": 17892, "title": "Inconsistency in the accepted shape/data_format of Input Tensor to Conv2D in documentation.", "body": "### Problem\r\nThe Tensorflow Layers Guide at https://www.tensorflow.org/tutorials/layers specifies:\r\n\r\n> The methods in the layers module for creating convolutional and pooling layers for two-dimensional image data expect input tensors to have a shape of [batch_size, image_width, image_height, channels]\r\n\r\nWhile, the inline documentation specified for conv2D (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/convolutional.py#L444) mentions the following valid data_formats:\r\n\r\n> data_format: A string, one of `channels_last` (default) or `channels_first`.\r\n      The ordering of the dimensions in the inputs.\r\n      `channels_last` corresponds to inputs with shape\r\n      `(batch, height, width, channels)` while `channels_first` corresponds to\r\n      inputs with shape `(batch, channels, height, width)`\r\n\r\nMoreover, nn.bias_add, avg_pool2d (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L99) etc. also accept [...,height, width...] but using different strings/methodology.\r\n\r\n> inputs: A 4-D tensor of shape `[batch_size, height, width, channels]` if\r\n      `data_format` is `NHWC`, and `[batch_size, channels, height, width]` if\r\n      `data_format` is `NCHW`.\r\n\r\nThus; shouldn't https://www.tensorflow.org/tutorials/layers be updated and the valid data_format values for Convolutional Layers be changed?", "comments": ["Created #17893 to fix the inconsistency mentioned above."]}, {"number": 17891, "title": "Update TF Lite android demo", "body": "/del\r\nDuplication", "comments": []}, {"number": 17890, "title": "Does tf.slim out of maintenance ?", "body": "I didn't find any tf-slim API document at www.tensorflow.org. In the meantime, TF-slim seems not user-friendly. \r\n\r\nI just wondering does tf.slim is of out of maintenance? ", "comments": ["see #16182 ", "Nagging Assignee @tatatodd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatatodd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Docs of TF-Slim are here https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim\r\nTF-Slim Models are here https://github.com/tensorflow/models/tree/master/research/slim\r\n", "Nagging Assignee @sguada: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @sguada: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @sguada: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "TF-Slim is on maintenance mode, not adding new functionality.", "Can the tf.data.dataset API be used with TF-Slim?\r\nSimilarly can models from TF-SLIM be used with the Estimator API?"]}, {"number": 17889, "title": "Clean up image_retraining example", "body": "The data files are remnants from when label_image.py used to be located\r\nin the image_retraining directory and are no longer used.\r\n\r\nAlso made image_dir a required argument in retrain.py to make it clearer\r\nthat the user has to pass in a path.", "comments": ["Nagging Assignee @benoitsteiner: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Image retraining example moved out of this repository in favor of tensorflow/hub"]}, {"number": 17888, "title": "tf.nn.softmax_cross_entropy_with_logits_v2 loss function so big", "body": "### Describe the problem\r\nI am trying to build up a binary image classifcation project (2 classes are balanced) . There are 2 nodes in output layer. However, my loss function is starting from a high point(3000). Is my loss function okay? Should I change my loss function ? I want to use a metric similar to Sklearn's log-loss. At the end of 1st epoch, it reaches to 80% accuracy and after 10 epochs and its accuracy becomes 82 %. During 10 epochs, it decreases from 3000 to 20. The graph of _my loss function_ is as following(1 epoch = 175 steps) .\r\n![loss](https://user-images.githubusercontent.com/33747602/37697942-0eb62702-2cf1-11e8-8ebe-cfdac4d34750.png)\r\n\r\nThe graph of _accuracy_ for 10 epochs is below:\r\n![acc](https://user-images.githubusercontent.com/33747602/37697954-28500084-2cf1-11e8-93c6-3f514531a8f8.png)\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows 10 Home\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**:1.6.0\r\n- **Python version**: 3.6.0(no anaconda) \r\n- **CUDA/cuDNN version**:9.0 - 7.0\r\n- **GPU model and memory**:GTX 1050\r\n- **Bazel Version**:N/A\r\n\r\n### Source code / logs\r\ncross_entropy=tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_true,logits=y_pred ))\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nExact command to reproduce", "I updated bazel information.", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "bro have you found solution of this ?\r\n"]}, {"number": 17887, "title": "How does the line of code ensure that total_loss are computed after finishing update_ops?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: ('v1.7.0-rc0-11-g8fded78', '1.7.0-rc1') \r\n- **Python version**: Python 2.7.12\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\nHi, there may be a bug in create_train_op().\r\n\r\nIn the newest tensorflow code, the line (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/training/python/training/training.py#L428) intends to make sure that total_loss is computed after finishing update_ops.\r\n\r\nI think it wants to make sure that the total_loss is computed with up-to-date values of variables after update_ops.\r\n\r\nHowever, my question is that the total_loss is defined elsewhere. There is just a reference to total_loss, which should not make the control_dependencies effective. So how does it work? **Is it possible the total_loss is computed with stale values of variables before update_ops?**\r\n\r\nThank you very much:-).\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler The information has been added. Thank you very much. By the way, these fields are irrelevant to my issue.", "This seems to be a misunderstanding of how TensorFlow graphs work. The reference to total_loss is a reference to the operation that has not yet been executed. The python calls are not executing things, they are building a graph which is later executed. A control dependency alters the graph to represent that one node has to run before another.\r\n", "@aselle Thank you for your reply. I know the python code just defines the operations. However, a control dependency does not work for an operation defined elsewhere. So this control dependency does not make sure the total_loss is computed with up-to-date variables. I mean there is a race condition.\r\n\r\nThe question has been answered on StackOverflow (https://stackoverflow.com/questions/49421529/how-does-tf-control-dependencies-work-for-operations-defined-elsewhere/49455352?noredirect=1#comment85995693_49455352). Thank you very much."]}, {"number": 17886, "title": "Data on GPU not garbage collected", "body": "### System information\r\n- No custom code written\r\n- Linux Ubuntu 16.04)\r\n- from source\r\n- TF 1.6\r\n- Python 2.7\r\n\r\nI've also posted the question on stack overflow (https://stackoverflow.com/questions/49399495/data-on-gpu-not-garbage-collected). However this could potentially be a bug so I decided to post an issue anyway.\r\n\r\nFor our current implementation, the return value of a sess.run() will be stored in a dictionary. We do that to implement statefulness of RNN.\r\n\r\nHowever, it turns out that storing data that way prevents tensorflow from garbage-collecting the data allocated on GPU. GPU memory usage simply keeps growing and it eventually blows up.\r\n\r\nThe return value from a sess.run() should be an numpy.ndarray as far as I'm concerned. And the data of an numpy.ndarray should live on good old CPU, shouldn't it?\r\n\r\nThen why would storing the return value prevent GC? I don't think this is a reference counting problem.\r\n\r\nOur current theory is that the returned numpy.ndarray actually has some reference to the GPU memory. Hope someone could verify/debunk this.\r\n\r\nWe actually tried commenting out the data storing part. Then the GPU memory becomes constant.\r\n\r\nThanks!!!", "comments": ["That sounds like a bug, saving numpy arrays should never lead to OOM on GPU, can you provide a reproducible case?\r\n\r\nReturned numpy arrays are in fact linked with underlying TensorFlow tensors, see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.h#L53 . However, this linking doesn't make sense to do for GPU tensors, only for CPU tensors cc @alextp ", "The research project is ongoing so it's kinda tough to come up with a new reproducible case.\r\nBut the code looks kinda like the following:\r\n```python\r\n# lookup index\r\ndefault = np.zeros((2, sum, HIDDEN_SIZE * 2)) \\\r\n    if CELL_TYPE == \"LSTM\" \\\r\n    else np.zeros((sum, HIDDEN_SIZE * 2))\r\nall_hiddens = []\r\nfor gpu_idx in range(NUM_GPUS):\r\n    actual_index_of_gpu = actual_index[gpu_idx]\r\n    all_hiddens_of_gpu = []\r\n    for b in xrange(BATCH_SIZE):\r\n        idx_long = actual_index_of_gpu[b][0]\r\n        idx_short = actual_index_of_gpu[b][1] - 1  # minus one is important!...\r\n        hidden_of_index = hidden_dict.get((idx_long, idx_short), default)  # THIS IS THE ACTUAL LOOKUP\r\n        # hidden_of_index: (2, sum, hidden_size) if lstm\r\n        all_hiddens_of_gpu.append(hidden_of_index)\r\n    all_hiddens_of_gpu = np.stack(all_hiddens_of_gpu, axis=-2)  # (2, sum, batch_size, hidden_size)\r\n    all_hiddens.append(all_hiddens_of_gpu)\r\nall_hiddens = np.array(all_hiddens)  # (NUM_GPUS, 2, sum, batch_size, hidden_size)\r\n\r\nfeed_dict = {\r\n    hidden_entries: all_hiddens,\r\n    data_entries: actual_data,\r\n    index_entries: actual_index # IDEA: feed data_index in as well\r\n}  # BIG BATCH fed here\r\n_, loss_value, run_states, run_indices = sess.run([train_op, loss, tower_states, tower_indices],\r\n                                                  feed_dict=feed_dict)\r\n# run_states: ([2], sum, batch_size, dim)\r\n# write hidden state back to dict\r\nfor tower_state, tower_index in zip(run_states, run_indices):  # a NUM_GPUS loop\r\n    tower_state = np.copy(tower_state) # A deep copy that may or may not fix the problem???\r\n    for batch_idx in range(BATCH_SIZE):\r\n        idx_long = tower_index[batch_idx][0]\r\n        idx_short = tower_index[batch_idx][1]\r\n        curr_hidden = tower_state[\r\n                      :, # LSTM-specific\r\n                      :,\r\n                      batch_idx, # current token\r\n                      :] \\\r\n                    if CELL_TYPE == \"LSTM\" \\\r\n                    else tower_state[\r\n                         :,\r\n                         batch_idx,\r\n                         :]\r\n        hidden_dict[\r\n            (idx_long, idx_short)\r\n        ] = curr_hidden  # curr_hidden: (2, sum, input_dim) if LSTM\r\n```\r\nBasically we look up hidden state from the hash table, sess.run, and write the retrieved hidden state back into the hash table. \r\nAfter commenting out the write-back part of the code, the memory issue disappears. ", "The way to verify it's a client-caused GPU memory leak would be to do the following:\r\n```\r\nfrom tensorflow.contrib.memory_stats.python.ops import memory_stats_ops\r\nwith tf.device(\"/gpu:0\"):\r\n  bytes_in_use_op = memory_stats_ops.BytesInUse()\r\nprint(\"memory before \", sess.run(bytes_in_use_op))\r\ndel numpy_result\r\nprint(\"memory after \", sess.run(bytes_in_use_op))\r\n```\r\n\r\nIf memory changes, then it's a GPU leak", "It's far more likely that you're leaking GPU memory through tf.constant in the same way @yaroslavvb identified.\r\n\r\nAre you sure you're not adding nodes to your graph every time you call session.run? Can you try finalizing your graph with tf.get_default_graph().finalize() after the first call to session.run?", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "> It's far more likely that you're leaking GPU memory through tf.constant in the same way @yaroslavvb identified.\r\n> \r\n> Are you sure you're not adding nodes to your graph every time you call session.run? Can you try finalizing your graph with tf.get_default_graph().finalize() after the first call to session.run?\r\n\r\ncan not agree more"]}, {"number": 17885, "title": "make toco build for android", "body": "for ARMv8\r\n`\r\nbazel build --config android_arm64 --cxxopt=-std=c++11 --linkopt=\"-llog\" --linkopt=-pie //tensorflow/contrib/lite/toco:toco   --config monolithic\r\n`\r\nfor ARMv7a\r\n`\r\nbazel build --config android_arm --cxxopt=-std=c++11 --linkopt=\"-llog\" --linkopt=-pie //tensorflow/contrib/lite/toco:toco   --config monolithic\r\n`", "comments": ["Nagging Assignee @benoitsteiner: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Reviewer @aselle: It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Mostly out of curiosity: why build toco for android?\r\n\r\n(also, could you move the non-tf-lite changes to a different pull request? It is easier for us to review it that way)", "@andrehentz To provide on-device model conversion, so that it would be possible to download non-tflite models and do some device-specific conversion in the future :-)", "Nagging Reviewer @aselle: It has been 15 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @aselle: It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "This breaks the Raspberry Pi build, since `#ifdef __ARM_ARCH_7A__` isn't sufficient to identify a platform that does or doesn't have double() already defined. Requesting a rollback, this could be re-done with an extra guard for Android-only."]}, {"number": 17884, "title": "\"dangling\" iterator after freeze model", "body": "I've using tf.data.Iterator to feed in data (as suggested?)\r\nHowever, when I freeze my model using tf.graph_util.convert_variables_to_constants, all the prerequisite nodes which constructs the iterator are lost.\r\nIs this a feature or a bug?\r\nIf it's a feature, how to utilize the iterator again or what's the point?\r\n\r\nThe following code snippet shows the \"problem\"\r\n\r\nPS: I am using tf1.4-gpu installed installed on win7 using PIP \r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n# build graph\r\ng = tf.Graph()\r\nwith g.as_default():\r\n    with tf.variable_scope(\"data_set\"):\r\n        # one x-y pair for single evaluation\r\n        x = tf.placeholder(\r\n            dtype=tf.float32, shape=(None, None, 1), name='inputs')\r\n        y = tf.placeholder(\r\n            dtype=tf.float32, shape=(None, None, 1), name='target')\r\n        eval_set = tf.data.Dataset()\\\r\n            .from_tensors((x, y))\r\n        iterator = tf.data.Iterator.from_structure(eval_set.output_types,\r\n                                                   output_shapes=eval_set.output_shapes)\r\n        eval_init_op = iterator.make_initializer(eval_set)\r\n        # some other dataset for training\r\n        # train_set = tf.data.Dataset()......\r\n        ##training_init_op = iterator.make_initializer(train_set)\r\n        x_in, y_in = iterator.get_next()\r\n    with tf.variable_scope(\"network\"):\r\n        var = tf.Variable(initial_value=5.0, dtype=tf.float32)\r\n        l0 = var * tf.reduce_sum(x_in)\r\n        l1 = tf.reduce_sum(y_in)\r\n        with tf.variable_scope(\"output\"):\r\n            out = l1 - l0\r\n        variable_init_op = tf.initialize_all_variables()\r\n\r\n# some eval step\r\n'''\r\nwith tf.Session(graph=g) as sess:\r\n    sess.run(variable_init_op)\r\n    sess.run(eval_init_op, feed_dict={x: np.ones(\r\n        (2, 2, 1), dtype=np.float32), y: np.ones((3, 3, 1), dtype=np.float32)})\r\n    result = sess.run(out)\r\n    print(\"result: {}\".format(result))\r\n'''\r\n\r\n# freeze the graph\r\nwith tf.Session(graph=g) as sess:\r\n    sess.run(variable_init_op)\r\n    names = [n.name for n in g.as_graph_def().node]\r\n    print(\"oroiginal graph has {} nodes\".format(len(g.as_graph_def().node)))\r\n    print([n.name for n in g.as_graph_def().node])\r\n    '''\r\n    ['data_set/inputs', 'data_set/target', 'data_set/Iterator', 'data_set/TensorDataset', 'data_set/make_initializer', 'data_set/IteratorGetNext', 'network/Variable/initial_value', 'network/Variable', 'network/Variable/Assign', 'network/Variable/read', 'network/Const', 'network/Sum', 'network/mul', 'network/Const_1', 'network/Sum_1', 'network/output/sub', 'network/init']\r\n    '''\r\n    frozen_graph = tf.graph_util.convert_variables_to_constants(\r\n        sess, g.as_graph_def(), ['network/output/sub'])\r\n    print(\"new graph nodes: {}\".format(len(frozen_graph.node)))\r\n    print([n.name for n in frozen_graph.node])\r\n    '''\r\n    ['data_set/Iterator', 'data_set/IteratorGetNext', 'network/Variable', 'network/Variable/read', 'network/Const', 'network/Sum', 'network/mul', 'network/Const_1', 'network/Sum_1', 'network/output/sub']\r\n    '''\r\n    ```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@shivaniag do you know anything about graph freezing with iterators?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 37 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "Hi there, facing the same issue...", "Also facing the same issue...", "still facing the same problem.", "get fixed by replace the iterator node with place holder nodes, although this fix will cause the performance degrade. "]}, {"number": 17883, "title": "Upgrade gRPC commit in TensorFlow", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: s390x Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.4.1\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.7.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: -\r\n\r\n### Describe the problem\r\nThere was a recent [commit ](https://github.com/grpc/grpc/commit/bce37d2785d508983f9d0501f6d711ef55ffc124) in gRPC added via [PR](https://github.com/grpc/grpc/pull/14464) to fix issues related to wrong address length calculation on big endian systems. This helps in resolving multiple test failures on big endian systems.\r\n\r\nWill it be possible to pick this or higher commit of gRPC in TensorFlow?\r\n\r\n", "comments": ["@YashalShakti can you please take a look at this issue?\r\n", "Hi @bignamehyp , \r\n\r\nProbably @yashykt could be of more help here. ", "Hi! We do try to keep the gRPC in TensorFlow in sync with the original repository, so it should get picked up soon.", "gRPC version was updated in [this commit](https://github.com/tensorflow/tensorflow/commit/f80486324807181614ac71367dbb9cf588aa2804#diff-455a4c7f8e22d7c514e8c2caa27506c5). I think this issue can be closed", "Nagging Assignee @bignamehyp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @bignamehyp: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @bignamehyp: It has been 46 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @bignamehyp: It has been 61 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 17882, "title": "pip install TensorFlow wheel fails due to BoringSSL", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: s390x Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.6.0\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.7.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: pip install tensorflow-1.6.0-cp27-cp27mu-linux_s390x.whl\r\n\r\n### Describe the problem\r\nTensorFlow v1.6.0 wheel installation fails on s390x since it installs `grpcio` which fails due to BoringSSL. \r\n@gunan, this is similar to earlier issues related to BoringSSL on s390x. \r\n\r\n### Source code / logs\r\n``` \r\nsudo pip install /tmp/tensorflow_wheel/tensorflow-1.6.0-cp27-cp27mu-linux_s390x.whl\r\n.\r\n.\r\n.\r\n  Running setup.py install for gast ... done\r\n  Running setup.py install for termcolor ... done\r\n  Running setup.py install for absl-py ... done\r\n  Running setup.py install for grpcio ... error\r\n.\r\n.\r\n.\r\nWerror=format-security -fPIC -DOPENSSL_NO_ASM=1 -D_WIN32_WINNT=1536 -DGPR_BACKWARDS_COMPATIBILITY_MODE=1 -DHAVE_CONFIG_H=1 -DGRPC_ENABLE_FORK_SUPPORT=1 -DPyMODINIT_FUNC=__attribute__((visibility (\"default\"))) void -DGRPC_POSIX_FORK_ALLOW_PTHREAD_ATFORK=1 -Isrc/python/grpcio -Iinclude -I. -Ithird_party/boringssl/include -Ithird_party/zlib -Ithird_party/cares -Ithird_party/cares/cares -Ithird_party/cares/config_linux -I/usr/include/python2.7 -c src/core/lib/security/credentials/google_default/google_default_credentials.cc -o python_build/temp.linux-s390x-2.7/src/core/lib/security/credentials/google_default/google_default_credentials.o -std=c++11 -std=gnu99 -fvisibility=hidden -fno-wrapv -fno-exceptions -pthread\r\n    cc1plus: warning: command line option '-Wstrict-prototypes' is valid for C/ObjC but not for C++\r\n    cc1plus: warning: command line option '-std=gnu99' is valid for C/ObjC but not for C++\r\n    In file included from third_party/boringssl/include/openssl/rsa.h:60:0,\r\n                     from ./src/core/lib/security/credentials/jwt/json_token.h:23,\r\n                     from ./src/core/lib/security/credentials/jwt/jwt_credentials.h:23,\r\n                     from src/core/lib/security/credentials/google_default/google_default_credentials.cc:34:\r\n    third_party/boringssl/include/openssl/base.h:114:2: error: #error \"Unknown target CPU\"\r\n     #error \"Unknown target CPU\"\r\n      ^\r\n```", "comments": ["@caisq @jart @chihuahua \r\nLooks like this was added with https://github.com/tensorflow/tensorflow/commit/549581f635a439ddcc6fdabf05f972c8ced1738d\r\n\r\nDo you remember why we made this change?\r\nCan we remove the dependency on grpcio?", "Is the IBM Z mainframe \"zEC12\" CPU in our support matrix?\r\n\r\n```\r\n// Note BoringSSL only supports standard 32-bit and 64-bit two's-complement,\r\n// little-endian architectures. Functions will not produce the correct answer\r\n// on other systems. Run the crypto_test binary, notably\r\n// crypto/compiler_test.cc, before adding a new architecture.\r\n#error \"Unknown target CPU\"\r\n```\r\n\r\nMaybe this issue should be sent upstream as contributions welcome?", "It is not in our support matrix, but grpc makes even contributed support impossible.\r\nIBM has been maintaining support for us.\r\n\r\nOn the upstream feature request side, boringSSL owners reported that they will not support big endian systems. Therefore our option now is to enable building TF without SSL.", "It seems this started in https://github.com/tensorflow/tensorflow/pull/6681. That's a big commitment on our part. I'm not sure TensorFlow stored data would be portable between such systems. Can't these CPUs simulate little endian?\r\n\r\nWould it be possible for grpcio to update their setup.py so, if the system is big endian, it favors linking OpenSSL from the system, rather than compiling BoringSSL from scratch?", "@martinwicke to loop him in.\r\nI guess that is one bug we can file to grpc team, but until that is done, our dependence on grpcio will completely block the big endian systems.", "Since TF has to be build from source on big endian anyway, can we do this: If building build_pip_package for a BE system, remove the grpcio dependency from the generated setup.py. \r\n\r\nThis will break things that actually depend on grpcio, so we'd have to make those failures soft(er), for instance, the debugger may just error out on big endian systems, but TF import shouldn't be affected.\r\n\r\n@namrata-ibm do you think that would be workable? ", "+1 what @martinwicke said. If that plan sounds good to everyone, I'll be happy to modify setup.py to make the grpcio pip dependency conditional on little endian.", "This plan sounds good to me.\r\nPlease add @yifeif as a reviewer as she has experience with conditional pip dependencies.", "+1 @martinwicke again. We can make grpcio an optional but important dependency for folks who need it. Please note I also support the BoringSSL decision to not support big endian, which was done for reasons that likely don't apply here.", "Sounds good. Grpcio can be installed separately with system OpenSSL till it is fixed in grpc."]}]