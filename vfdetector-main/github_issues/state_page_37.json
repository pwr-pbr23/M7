[{"number": 47089, "title": "tf.keras.metric.Metric.reset_states() fails if state contains variables with rank greater than zero", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colaboratory\r\n- TensorFlow installed from (source or binary): Google Colaboratory\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.6.9\r\n\r\n**Describe the current behavior**\r\nWhen I implement my own custom keras metric that has an internal weight of rank greater than zero (meaning, its shape has length greater than zero), calling the ``reset_states()`` method on an instance of this metric fails.\r\n\r\nSee example below. This is reproducible in colab. \r\n\r\n**Describe the expected behavior**\r\nI would expect ``reset_metrics()`` not to fail.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\nclass MyMetric(tf.keras.metrics.Metric):\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.w = self.add_weight(name='w', shape=(1,))\r\n\r\n    def update_state(self, w):\r\n        self.w.assign(w)\r\n\r\n    def result(self):\r\n        return self.w\r\n\r\nm = MyMetric()\r\nm.reset_states()\r\n```\r\n\r\n**Other info / logs** \r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-86-2dd397d265f4> in <module>()\r\n     14 \r\n     15 m = MyMetric()\r\n---> 16 m.reset_states()\r\n\r\n3 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/metrics.py in reset_states(self)\r\n    251     when a metric is evaluated during training.\r\n    252     \"\"\"\r\n--> 253     K.batch_set_value([(v, 0) for v in self.variables])\r\n    254 \r\n    255   @abc.abstractmethod\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    199     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    200     try:\r\n--> 201       return target(*args, **kwargs)\r\n    202     except (TypeError, ValueError):\r\n    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in batch_set_value(tuples)\r\n   3704   if ops.executing_eagerly_outside_functions():\r\n   3705     for x, value in tuples:\r\n-> 3706       x.assign(np.asarray(value, dtype=dtype(x)))\r\n   3707   else:\r\n   3708     with get_graph().as_default():\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in assign(self, value, use_locking, name, read_value)\r\n    889             (\"Cannot assign to variable%s due to variable shape %s and value \"\r\n    890              \"shape %s are incompatible\") %\r\n--> 891             (tensor_name, self._shape, value_tensor.shape))\r\n    892       assign_op = gen_resource_variable_ops.assign_variable_op(\r\n    893           self.handle, value_tensor, name=name)\r\n\r\nValueError: Cannot assign to variable w:0 due to variable shape (1,) and value shape () are incompatible\r\n```", "comments": ["Was able to reproduce the issue with  TF v2.4 and TF-nightly(`2.5.0-dev20210211`). Please find the gist of it [here](https://colab.research.google.com/gist/ravikyram/3f9f178cb17c4890bcaf1b6ecae9dc40/untitled670.ipynb). Thanks!", "Was able to reproduce the issue using TF Nightly 2.6 version.Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/bc5f710e75b6d271cd09396ea147b24d/untitled96.ipynb).Thanks!", "Is there any workaround for this issue?"]}, {"number": 47081, "title": "Multiple GPU tests run redundantly", "body": "Using TensorFlow 2.4.1\r\n\r\n**Describe the problem**\r\n\r\nI noticed that some GPU tests are run multiple times redundantly with apparently no need or benefit causing test times to increase.\r\nExample: //tensorflow/core/kernels/mlir_generated:gpu_tanh_test, //tensorflow/core/kernels/mlir_generated:gpu_tanh_test_gpu\r\n\r\nAs you can see there is a gpu_* test and a gpu_*_gpu test\r\n\r\nThis happens because `tf_cuda_cc_test` rule is used which creates a CPU and a GPU test already. However as the test is meant for GPU already and has `tags = tf_cuda_tests_tags()` set, it will create 2 tests that are both run (only) on GPU\r\n\r\nAs current master has changed considerably I just did a quick check and found e.g.:\r\nhttps://github.com/tensorflow/tensorflow/blob/c820e5278288773ef2298b1796164a9826d847cf/tensorflow/core/kernels/mlir_generated/BUILD#L332-L345\r\n\r\nThis follows the same pattern and should likely be revised. I quick fix would likely be: If the passed tags already contain `gpu` then the CPU test will not be added.\r\n", "comments": []}, {"number": 47068, "title": "QAT model to TFLite strict int8 quantisation - big performance gap", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installation (pip package or built from source): pip package\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.5.0-dev20201210\r\n\r\n### 2. Code\r\n\r\n```python\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\nprint(\"TENSORFLOW VERSION:\", tf.__version__)\r\nfrom tensorflow.python.keras.engine.input_layer import Input\r\nimport tensorflow_model_optimization as tfmo\r\n\r\nfrom tensorflow.keras.layers import Input, Conv2D\r\nfrom tensorflow.keras.layers import ReLU\r\nfrom tensorflow.keras import Model\r\n\r\n\r\ndef calc_mse(target, pred, dtype=\"float\"):\r\n\r\n    if dtype==\"float\":\r\n        scale = 255 ** 2\r\n    elif dtype==\"int\":\r\n        scale = 1\r\n    else:\r\n        raise NotImplementedError(\"dtype must be float or int.\")\r\n\r\n    mse = tf.reduce_mean(tf.pow(target - pred, 2)) * scale\r\n    return mse\r\n\r\n\r\ndef validate_tflite(interpreter, dataset):\r\n    mse = 0\r\n\r\n    for img, img in dataset:\r\n\r\n        # Preprocess image\r\n        input_details = interpreter.get_input_details()[0]\r\n        scale, zero_point = input_details['quantization']\r\n        tflite_integer_input = img / scale + zero_point\r\n        tflite_integer_input = tf.cast(tflite_integer_input, input_details['dtype'])\r\n        interpreter.set_tensor(input_details['index'], tflite_integer_input)\r\n\r\n        interpreter.invoke()\r\n\r\n        output_details = interpreter.get_output_details()[0]\r\n        tflite_integer_output = interpreter.get_tensor(output_details['index'])\r\n\r\n        tflite_integer_input = tf.cast(tflite_integer_input, \"float32\")\r\n        tflite_integer_output = tf.cast(tflite_integer_output, \"float32\")\r\n\r\n        mse += calc_mse(tflite_integer_input, tflite_integer_output, dtype=\"int\")\r\n\r\n    return mse / len(dataset)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    tfmodel_old = tf.keras.models.load_model(\"simpleconv\")\r\n\r\n    tfmodel = tf.keras.Sequential([\r\n        Input((32, 32, 3)),\r\n        Conv2D(128, 3, padding=\"same\", name=\"Conv1\"),\r\n        ReLU(name=\"Act1\"),\r\n        Conv2D(3, 3, padding=\"same\", name=\"Conv2\")])\r\n    \r\n    tfmodel.set_weights(tfmodel_old.get_weights())\r\n\r\n\r\n    cifar = tf.keras.datasets.cifar10\r\n    (train_images, _), (test_images, _) = cifar.load_data()\r\n    train_images = train_images[:1000].astype(np.float32) / 255.0\r\n    test_images = test_images[:1000].astype(np.float32) / 255.0\r\n\r\n    cifar_train = tf.data.Dataset.from_tensor_slices((train_images, train_images)).map(\r\n        lambda x, y: (tf.expand_dims(x, axis=0), tf.expand_dims(y, axis=0)))\r\n\r\n    tfmodel.compile(optimizer=\"adam\", loss=calc_mse)\r\n    mse = tfmodel.evaluate(cifar_train)\r\n\r\n    print(f\"tf savedmodel mse: {mse}\")\r\n\r\n    # Finetune tensorflow model with QAT\r\n    quantise_model = tfmo.quantization.keras.quantize_model\r\n    qa_model = quantise_model(tfmodel)\r\n    adam = tf.keras.optimizers.Adam(learning_rate=1e-6)\r\n    qa_model.compile(optimizer=adam, loss=calc_mse)\r\n\r\n    # NOTE: check mse here\r\n    qa_model.fit(cifar_train)\r\n\r\n    # Convert to TFLite and quantise\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(tfmodel)\r\n\r\n    # Quantise to int8 without float fallback\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\n    def create_repr_data_gen(data):\r\n            \r\n        def representative_data_gen():\r\n            for input_arr, target in data:\r\n                yield [input_arr]\r\n        return representative_data_gen\r\n\r\n    converter.representative_dataset = create_repr_data_gen(cifar_train)\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n    converter.inference_input_type = tf.uint8\r\n    converter.inference_output_type = tf.uint8\r\n\r\n    quantised_qa_model = converter.convert()\r\n    print(\"TFLite conversion done!\")\r\n\r\n    # Validate TFLite model and check mse again\r\n    interpreter = tf.lite.Interpreter(model_content=quantised_qa_model)\r\n    interpreter.allocate_tensors()\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n\r\n    print(input_details, \"\\n\", output_details)\r\n    mse_tflite = validate_tflite(interpreter, cifar_train)\r\n    print(f\"TFLite mse: {mse_tflite}\")\r\n```\r\n\r\n### 3. Failure after conversion\r\n\r\nI have a pretrained tensorflow model (just a simple Conv2D + ReLU + Conv2D trained with MSE) that I load, do quantisation aware training on (with the CIFAR dataset), then quantise to int8 in tensorflow lite.\r\n\r\nMSE of the original tensorflow model is 139.7, and MSE of the QAT model is 193.9, which is reasonable given that I'm doing QAT on a 1000 images only.\r\nHowever, MSE of the int8 quantised tensorflow lite model is much higher at 1620.5. The representative dataset I provide for quantisation calibration is the same, so I don't understand how performance could degrade this much.\r\n\r\n### 5. (optional) Any other info / logs\r\n\r\nBelow is the console output of the program:\r\n```\r\n2021-02-10 14:17:33.598712: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib:/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib\r\n2021-02-10 14:17:33.598730: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nTENSORFLOW VERSION: 2.5.0-dev20201210\r\n2021-02-10 14:17:34.496241: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-02-10 14:17:34.520408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-10 14:17:34.520885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: TITAN RTX computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.64GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2021-02-10 14:17:34.520962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-10 14:17:34.521347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 1 with properties: \r\npciBusID: 0000:02:00.0 name: TITAN RTX computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2021-02-10 14:17:34.521474: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib:/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib\r\n2021-02-10 14:17:34.521603: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib:/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib\r\n2021-02-10 14:17:34.521709: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib:/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib\r\n2021-02-10 14:17:34.524210: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-02-10 14:17:34.524500: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-02-10 14:17:34.527516: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2021-02-10 14:17:34.527711: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib:/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib\r\n2021-02-10 14:17:34.527835: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib:/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib\r\n2021-02-10 14:17:34.527845: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1764] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2021-02-10 14:17:34.528075: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-02-10 14:17:34.528483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-02-10 14:17:34.528492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1273]      \r\nWARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\r\n2021-02-10 14:17:35.448465: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:127] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-02-10 14:17:35.449007: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3600000000 Hz\r\n1000/1000 [==============================] - 1s 588us/step - loss: 140.2766\r\ntf savedmodel mse: 139.69656372070312\r\n1000/1000 [==============================] - 3s 3ms/step - loss: 193.8917\r\n2021-02-10 14:17:40.852032: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\n2021-02-10 14:17:41.087654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-10 14:17:41.087979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-10 14:17:41.088209: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 2\r\n2021-02-10 14:17:41.088302: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2021-02-10 14:17:41.321745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-10 14:17:41.322082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: TITAN RTX computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.64GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2021-02-10 14:17:41.322177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-10 14:17:41.322633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 1 with properties: \r\npciBusID: 0000:02:00.0 name: TITAN RTX computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2021-02-10 14:17:41.322751: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib:/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib\r\n2021-02-10 14:17:41.322817: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib:/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib\r\n2021-02-10 14:17:41.322874: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib:/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib\r\n2021-02-10 14:17:41.322891: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-02-10 14:17:41.322903: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-02-10 14:17:41.322915: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2021-02-10 14:17:41.322968: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib:/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib\r\n2021-02-10 14:17:41.323021: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib:/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib\r\n2021-02-10 14:17:41.323032: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1764] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2021-02-10 14:17:41.323050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-02-10 14:17:41.323058: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1273]      0 1 \r\n2021-02-10 14:17:41.323065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 0:   N N \r\n2021-02-10 14:17:41.323070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 1:   N N \r\n2021-02-10 14:17:41.324051: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:933] Optimization results for grappler item: graph_to_optimize\r\n  function_optimizer: function_optimizer did nothing. time = 0.005ms.\r\n  function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n\r\n2021-02-10 14:17:41.341282: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:330] Ignored output_format.\r\n2021-02-10 14:17:41.341306: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:333] Ignored drop_control_dependency.\r\n2021-02-10 14:17:41.345935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-10 14:17:41.346306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: TITAN RTX computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.64GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2021-02-10 14:17:41.346397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-10 14:17:41.346725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 1 with properties: \r\npciBusID: 0000:02:00.0 name: TITAN RTX computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2021-02-10 14:17:41.346839: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib:/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib\r\n2021-02-10 14:17:41.346902: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib:/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib\r\n2021-02-10 14:17:41.346958: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib:/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib\r\n2021-02-10 14:17:41.346974: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-02-10 14:17:41.346987: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-02-10 14:17:41.346999: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2021-02-10 14:17:41.347050: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib:/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib\r\n2021-02-10 14:17:41.347105: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib:/usr/local/QT_5_13_2/lib:/usr/local/OpenCV_4_1_1/lib:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64:/usr/local/magma/lib\r\n2021-02-10 14:17:41.347115: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1764] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2021-02-10 14:17:41.347135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-02-10 14:17:41.347141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1273]      0 1 \r\n2021-02-10 14:17:41.347147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 0:   N N \r\n2021-02-10 14:17:41.347152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 1:   N N \r\nTFLite conversion done!\r\n[{'name': 'input_1', 'index': 7, 'shape': array([ 1, 32, 32,  3], dtype=int32), 'shape_signature': array([-1, 32, 32,  3], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.003921568859368563, 0), 'quantization_parameters': {'scales': array([0.00392157], dtype=float32), 'zero_points': array([0], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}] \r\n [{'name': 'Identity', 'index': 8, 'shape': array([ 1, 32, 32,  3], dtype=int32), 'shape_signature': array([-1, 32, 32,  3], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.007350581232458353, 30), 'quantization_parameters': {'scales': array([0.00735058], dtype=float32), 'zero_points': array([30], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\nTFLite mse: 1620.45458984375\r\n```\r\n\r\n", "comments": ["Forgot to add, here's the pretrained model as a SavedModel directory (zipped).\r\n\r\n[simpleconv.zip](https://github.com/tensorflow/tensorflow/files/5959176/simpleconv.zip)\r\n", "hello, any updates?", "`tfmodel` does not seems to be able to correctly classify images into 10 classes. also, note that you're not using class label during training. Please refer to this page for CNN basics: https://www.tensorflow.org/tutorials/images/cnn", "Apologies, I didn't explain myself clearly in the original post. \r\n\r\nThis model is not trained as a classifier, it's trained to reconstruct the original image and it's trained using MSE. So, it takes an image of shape (1,H,W,3) and outputs (1,H,W,3). Hence why I'm discarding the CIFAR labels.", "Oh, I see. Will take a look. Meanwhile, this could happen if the original range is too broad so that the quantization error goes large. For example, you can try applying ReLU6 to narrow the quantization range. For now it doesn't seem like a bug, but some technical issue in the model structure itself.", "QA-trained a simple classifier with the same code (doing the necessary modifications) and it doesn't incur any accuracy loss. \r\nAlso tested the autoencoder swapping ReLU with ReLU6, and the mse of the quantised model does decrease compared to ReLU, although it's still nowhere close to the mse of the non-quantised model.", "I tried training the autoencoder without normalising the images to the 0-1 range. I kept them in the 0-255 range (converting to float of course), since I noticed that some pre-trained models used in the AI-Benchmark app have been trained in this way.\r\n\r\nHowever, no dice. Once I quantise and convert to tensorflow lite, mse is terrible. Any updates on your end?"]}, {"number": 47058, "title": "CUDA_ERROR_NO_BINARY_FOR_GPU on A100 when compiled with CC 7.0", "body": "**System information**\r\n- Have I written custom code: No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): 3.7.1\r\n- GCC/Compiler version (if compiling from source): 8.3.0\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: A100\r\n\r\n**Describe the current behavior**\r\n\r\nI'm compiling TF for our cluster with a CUDA 11 driver but 10.1 toolkit (there are reasons not to upgrade nvcc yet). Hence the maximum supported CUDA compute capability by nvcc is 7.0, but the A100 GPUs used have 8.0. This shouldn't be a problem as the CUDA driver can JIT compile the 7.0 PTX code for the A100.\r\n\r\nHowever running the tests from the TensorFlow repo yields errors like: `tensorflow.python.framework.errors_impl.InternalError: Failed to load in-memory CUBIN: CUDA_ERROR_NO_BINARY_FOR_GPU: no kernel image is available for execution on the device [Op:Abs]`\r\n\r\nFollowing the error message I come to `GpuExecutor::GetKernel` which has some logic whether to load a cubin or ptx binary, so my feeling is that that is faulty.\r\n\r\n**Describe the expected behavior**\r\n\r\nPTX code is loaded and kernel executed successfully\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n- `export TF_CUDA_COMPUTE_CAPABILITIES=3.7,6.1,7.0`\r\n- `bazel test --config=noaws --config=nogcp --config=nohdfs --compilation_mode=opt --config=opt --subcommands --verbose_failures --jobs=64 --copt=\"-fPIC\" --distinct_host_configuration=false --config=mkl --test_output=errors --build_tests_only --local_test_jobs=1 --test_timeout=3600 --test_size_filters=small -- //tensorflow/core/kernels/mlir_generated:gpu_abs_test_gpu`\r\n\r\n**Other info / logs**\r\nExample error:\r\n\r\n```\r\nERROR: testBroadcastWithBatchParamsAndBiggerEvent (__main__.CategoricalTest)\r\nCategoricalTest.testBroadcastWithBatchParamsAndBiggerEvent\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/dev/shm/s3248973-EasyBuild/TensorFlow/2.4.1/fosscuda-2019b-Python-3.7.4/tmp9lUSI1-bazel-tf/fdff6046a749a079864ed2bee7e018bf/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/distributions/categorical_test_gpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/distributions/categorical_test.py\", line 296, in testBroadcastWithBatchParamsAndBiggerEvent\r\n    \"norm_cdf\": norm.cdf(real_event_tf),\r\n  File \"/dev/shm/s3248973-EasyBuild/TensorFlow/2.4.1/fosscuda-2019b-Python-3.7.4/tmp9lUSI1-bazel-tf/fdff6046a749a079864ed2bee7e018bf/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/distributions/categorical_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/distributions/distribution.py\", line 898, in cdf\r\n    return self._call_cdf(value, name)\r\n  File \"/dev/shm/s3248973-EasyBuild/TensorFlow/2.4.1/fosscuda-2019b-Python-3.7.4/tmp9lUSI1-bazel-tf/fdff6046a749a079864ed2bee7e018bf/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/distributions/categorical_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/distributions/distribution.py\", line 874, in _call_cdf\r\n    return self._cdf(value, **kwargs)\r\n  File \"/dev/shm/s3248973-EasyBuild/TensorFlow/2.4.1/fosscuda-2019b-Python-3.7.4/tmp9lUSI1-bazel-tf/fdff6046a749a079864ed2bee7e018bf/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/distributions/categorical_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/distributions/normal.py\", line 207, in _cdf\r\n    return special_math.ndtr(self._z(x))\r\n  File \"/dev/shm/s3248973-EasyBuild/TensorFlow/2.4.1/fosscuda-2019b-Python-3.7.4/tmp9lUSI1-bazel-tf/fdff6046a749a079864ed2bee7e018bf/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/distributions/categorical_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/distributions/special_math.py\", line 143, in ndtr\r\n    return _ndtr(x)\r\n  File \"/dev/shm/s3248973-EasyBuild/TensorFlow/2.4.1/fosscuda-2019b-Python-3.7.4/tmp9lUSI1-bazel-tf/fdff6046a749a079864ed2bee7e018bf/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/distributions/categorical_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/distributions/special_math.py\", line 151, in _ndtr\r\n    z = math_ops.abs(w)\r\n  File \"/dev/shm/s3248973-EasyBuild/TensorFlow/2.4.1/fosscuda-2019b-Python-3.7.4/tmp9lUSI1-bazel-tf/fdff6046a749a079864ed2bee7e018bf/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/distributions/categorical_test_gpu.runfiles/org_tensorflow/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/dev/shm/s3248973-EasyBuild/TensorFlow/2.4.1/fosscuda-2019b-Python-3.7.4/tmp9lUSI1-bazel-tf/fdff6046a749a079864ed2bee7e018bf/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/distributions/categorical_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/math_ops.py\", line 401, in abs\r\n    return gen_math_ops._abs(x, name=name)\r\n  File \"/dev/shm/s3248973-EasyBuild/TensorFlow/2.4.1/fosscuda-2019b-Python-3.7.4/tmp9lUSI1-bazel-tf/fdff6046a749a079864ed2bee7e018bf/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/distributions/categorical_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/gen_math_ops.py\", line 46, in _abs\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/dev/shm/s3248973-EasyBuild/TensorFlow/2.4.1/fosscuda-2019b-Python-3.7.4/tmp9lUSI1-bazel-tf/fdff6046a749a079864ed2bee7e018bf/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/distributions/categorical_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/ops.py\", line 6862, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: Failed to load in-memory CUBIN: CUDA_ERROR_NO_BINARY_FOR_GPU: no kernel image is available for execution on the device [Op:Abs]\r\n```\r\n", "comments": ["I think I was able to trace this to ultimately `tf_to_gpu_binary` which is used to compile MLIR code to SASS (CUDA binary format) which is then put as binary inside the executable and loaded via `LoadModuleFromCuBin`\r\nHowever it looks like PTX code is not included when creating that (fat) binary which makes TensorFlow basically unusable on GPUs it has not been exactly compiled for.\r\nAny ideas how to solve that?", "You want to specify something like\r\n\r\n`TF_CUDA_COMPUTE_CAPABILITIES=\"sm_35,sm_50,sm_60,sm_70,sm_75,compute_80\"`\r\n\r\nwhere the `sm_xx` are the compute capabilities you want to ship fully compiled cubins for and the `compute_xx` are the ones that also include a PTX source variant. Can you try and report whether that fixes your issue?", "No this does not help. The current values used (7.5 and so on) are already converted to `compute_75`: https://github.com/tensorflow/tensorflow/blob/v2.4.1/third_party/gpus/cuda_configure.bzl#L425-L430\r\n\r\nThe problem is that this is ignored by the kernel builder: https://github.com/tensorflow/tensorflow/blob/v2.4.1/third_party/gpus/cuda_configure.bzl#L425-L430\r\n\r\nSee also https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_gpu_binary.cc#L52 for that param", "Looking at this, I believe we are generating a ptx variant. Could you try remove the `\"--cmdline=--compile-only\",` in this file https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/stream_executor/gpu/asm_compiler.cc#L300 and see whether that helps?\r\n\r\nI will prepare a patch to clean this up, just wanting to check whether this fixes your issue.", "No this does not fix the issue. The reason is that the tf_to_gpu_binary did not include the PTX code at all\r\n\r\nHowever the mentioned line can likely be removed. I had problems after I patched the generator to return either PTX or SM code depending on whether it was passed sm_* or compute_*. Those problems were resolved by removing the `--compile-only` and I asked on SO for clarification without any response so far: https://stackoverflow.com/questions/66171048/significance-of-compile-only-for-cuda-fatbinary-creation\r\n\r\nBut again: This alone does not work, because no PTX code is returned in 2.4.1, see https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc#L170-L185 where the `!generate_fatbin_` branch is taken and the ptx code is ignored. My patch consisted of mainly:\r\n```\r\n--- a/tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc\r\n+++ b/tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc\r\n@@ -171,6 +171,11 @@ class GpuKernelToBlobPass\r\n         // Skip fatbin generation and return the first and only GPU machine\r\n         // code. This is currently only used for `tf_to_gpu_binary` and will\r\n         // eventually disappear.\r\n+        if (is_compute_profile) {\r\n+          std::vector<uint8_t> ptx_bytes;\r\n+          std::copy(ptx.begin(), ptx.end(), std::back_inserter(ptx_bytes)); // can probably be done in the ctor directly\r\n+          return ptx_bytes;\r\n+        }\r\n         return gpu_asm;\r\n       }\r\n\r\n```\r\n\r\nDuring my tests on a cluster with A100s this did seemingly work, but the CUDA JIT compilation from PTX turned out to be REALLY slow and produced GigaBytes of compiled code in the compute cache folder. To make that worse, the default cache folder is inside the users HOME which on clusters is usually a NFS mounted folder making it even slower which is why I ultimately gave up on that.", "Thank you for looking into this further. I had missed that this was using the `tf_to_gpu_binary` path. You are correct, on that path we only include ptx code and we really should ship a fatbin there, as well.\r\n\r\nWith TensorFlow 2.5 this path will go away. I believe another valid approach would be to remove the entire block for `!generate_fatbin` and use the fatbin path always. With the additional fix of removing the `--cmdline=--compile-only` string.\r\n\r\nIt is unlikely, though, that this will address your performance issue. The driver will still need to jit the ptx code for every involved operation, which is inherently slow."]}, {"number": 47047, "title": "c_api_distributed_test creates huge amount of threads and segfaults", "body": "**System information**\r\n- Have I written custom code: no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): 3.7.1\r\n- GCC/Compiler version (if compiling from source): 8.3.0\r\n- CUDA/cuDNN version: 10.1\r\n\r\n**Describe the current behavior**\r\n\r\nWhen running `bazel test` on a system with a large physical core count the test `//tensorflow/c/eager:c_api_distributed_test` finishes and then segfaults on exit.\r\n\r\nWhen I manually set `OMP_NUM_THREADS=80` the test succeeds without a segfault but at around 85 it again crashes.\r\n\r\nI'm unable to get a stacktrace neither through TensorFlow nor through gdb and even valgrind gives up with\r\n\r\n> valgrind: the 'impossible' happened:\r\n   Max number of threads is too low\r\n\r\nIt then prints the stacks of 500(!) threads. In GDB I was sometimes able to catch a part of the stack pointing to libiomp from the included llvm-OpenMP, but that was difficult and hard to reproduce. Usually the process would just be terminated even when in GDB.\r\n\r\nSomething I noticed: The ThreadPool(Device) creates a large amount of threads which don't terminate until program exit. I don't think this is intended and expect this to be the cause which triggers some limitation in the OpenMP runtime.\r\n\r\nAlso the crash does not happen when not all subtests are run (via the GTest filter), excluding any of the 5 (or 6?) makes the crash disappear\r\n\r\n**Describe the expected behavior**\r\n\r\nThreads exit when ThreadPool is destroyed and no crash happens.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n- build with bazel\r\n- `CUDA_VISIBLE_DEVICES=-1 gdb /dev/shm//tmpzWGWuq-bazel-tf/fdff6046a749a079864ed2bee7e018bf/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/c/eager/c_api_distributed_test`\r\n\r\n**Other info / logs**\r\n```\r\nExecuting tests from //tensorflow/c/eager:c_api_distributed_test\r\n-----------------------------------------------------------------------------\r\n2021-02-08 19:58:39.296267: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\r\nRunning main() from test_main.cc\r\n[==========] Running 6 tests from 1 test suite.\r\n[----------] Global test environment set-up.\r\n[----------] 6 tests from CAPI\r\n[ RUN      ] CAPI.TestLocalFunctionWithPackedInput\r\n2021-02-08 19:58:39.510017: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-02-08 19:58:39.748990: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2021-02-08 19:58:39.749037: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: taurusi8028\r\n2021-02-08 19:58:39.749045: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: taurusi8028\r\n2021-02-08 19:58:39.749373: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 460.32.3\r\n2021-02-08 19:58:39.749416: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.32.3\r\n2021-02-08 19:58:39.749430: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 460.32.3\r\n2021-02-08 19:58:39.749508: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 19:58:39.796167: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59685, 1 -> localhost:50055, 2 -> localhost:31398}\r\n2021-02-08 19:58:39.841581: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:50055\r\n2021-02-08 19:58:39.841721: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 19:58:40.095546: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59685, 1 -> localhost:50055, 2 -> localhost:31398}\r\n2021-02-08 19:58:40.095796: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:31398\r\n2021-02-08 19:58:40.095865: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 19:58:40.095922: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-02-08 19:58:40.258067: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59685, 1 -> localhost:50055, 2 -> localhost:31398}\r\n2021-02-08 19:58:40.406436: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59685, 1 -> localhost:50055, 2 -> localhost:31398}\r\n2021-02-08 19:58:40.406478: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59685, 1 -> localhost:50055, 2 -> localhost:31398}\r\n2021-02-08 19:58:40.406627: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:270] Creating async eager service context with rendezvous_id on host taurusi8028 /job:localhost/replica:0/task:1\r\n2021-02-08 19:58:40.406634: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:270] Creating async eager service context with rendezvous_id on host taurusi8028 /job:localhost/replica:0/task:2\r\n2021-02-08 19:58:40.406664: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-02-08 19:58:40.406670: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-02-08 19:58:40.408781: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59685, 1 -> localhost:50055, 2 -> localhost:31398}\r\n2021-02-08 19:58:40.409412: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:59685\r\n2021-02-08 19:58:40.647385: I tensorflow/core/common_runtime/eager/kernel_and_device.cc:92] Ignoring error status when releasing multi-device function handle Unimplemented: Releasing a multi-device component \r\nhandle on a remote device is not yet implemented.\r\n[       OK ] CAPI.TestLocalFunctionWithPackedInput (1209 ms)\r\n[ RUN      ] CAPI.TestRemoteFunctionWithPackedInput\r\n2021-02-08 19:58:40.647938: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 19:58:40.673207: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:62634, 1 -> localhost:31692, 2 -> localhost:39353}\r\n2021-02-08 19:58:40.673481: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:31692\r\n2021-02-08 19:58:40.673544: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 19:58:40.775266: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:62634, 1 -> localhost:31692, 2 -> localhost:39353}\r\n2021-02-08 19:58:40.778366: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:39353\r\n2021-02-08 19:58:40.778517: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 19:58:40.778614: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-02-08 19:58:40.850707: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:62634, 1 -> localhost:31692, 2 -> localhost:39353}\r\n2021-02-08 19:58:40.857370: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:62634, 1 -> localhost:31692, 2 -> localhost:39353}\r\n2021-02-08 19:58:40.857557: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:62634, 1 -> localhost:31692, 2 -> localhost:39353}\r\n2021-02-08 19:58:40.857947: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:270] Creating async eager service context with rendezvous_id on host taurusi8028 /job:localhost/replica:0/task:2\r\n2021-02-08 19:58:40.857974: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-02-08 19:58:40.858061: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:270] Creating async eager service context with rendezvous_id on host taurusi8028 /job:localhost/replica:0/task:1\r\n2021-02-08 19:58:40.858100: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-02-08 19:58:40.865056: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:62634, 1 -> localhost:31692, 2 -> localhost:39353}\r\n2021-02-08 19:58:40.866478: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:62634\r\n[       OK ] CAPI.TestRemoteFunctionWithPackedInput (367 ms)\r\n[ RUN      ] CAPI.DistributedFunctionGraphPassOnlyOnce\r\n2021-02-08 19:58:41.014661: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 19:58:41.024886: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:45659, 1 -> localhost:44179, 2 -> localhost:57750}\r\n2021-02-08 19:58:41.025155: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:44179\r\n2021-02-08 19:58:41.025233: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 19:58:41.101530: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:45659, 1 -> localhost:44179, 2 -> localhost:57750}\r\n2021-02-08 19:58:41.103305: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:57750\r\n2021-02-08 19:58:41.103445: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 19:58:41.103492: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-02-08 19:58:41.265757: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:45659, 1 -> localhost:44179, 2 -> localhost:57750}\r\n2021-02-08 19:58:41.272939: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:45659, 1 -> localhost:44179, 2 -> localhost:57750}\r\n2021-02-08 19:58:41.273025: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:45659, 1 -> localhost:44179, 2 -> localhost:57750}\r\n2021-02-08 19:58:41.273080: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:270] Creating sync eager service context with rendezvous_id on host taurusi8028 /job:localhost/replica:0/task:1\r\n2021-02-08 19:58:41.273111: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-02-08 19:58:41.273240: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:270] Creating sync eager service context with rendezvous_id on host taurusi8028 /job:localhost/replica:0/task:2\r\n2021-02-08 19:58:41.273276: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-02-08 19:58:41.275488: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:45659, 1 -> localhost:44179, 2 -> localhost:57750}\r\n2021-02-08 19:58:41.275920: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:45659\r\n[       OK ] CAPI.DistributedFunctionGraphPassOnlyOnce (316 ms)\r\n[ RUN      ] CAPI.DistributedFunctionNoError\r\n2021-02-08 19:58:41.331075: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 19:58:41.405806: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59564, 1 -> localhost:34434, 2 -> localhost:37620}\r\n2021-02-08 19:58:41.406048: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:34434\r\n2021-02-08 19:58:41.406115: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 19:58:41.445328: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59564, 1 -> localhost:34434, 2 -> localhost:37620}\r\n2021-02-08 19:58:41.446777: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:37620\r\n2021-02-08 19:58:41.446932: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 19:58:41.447020: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-02-08 19:58:41.709247: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59564, 1 -> localhost:34434, 2 -> localhost:37620}\r\n2021-02-08 19:58:41.713390: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59564, 1 -> localhost:34434, 2 -> localhost:37620}\r\n2021-02-08 19:58:41.713392: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59564, 1 -> localhost:34434, 2 -> localhost:37620}\r\n2021-02-08 19:58:41.713504: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:270] Creating sync eager service context with rendezvous_id on host taurusi8028 /job:localhost/replica:0/task:2\r\n2021-02-08 19:58:41.713531: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-02-08 19:58:41.713555: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:270] Creating sync eager service context with rendezvous_id on host taurusi8028 /job:localhost/replica:0/task:1\r\n2021-02-08 19:58:41.713588: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-02-08 19:58:41.714723: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59564, 1 -> localhost:34434, 2 -> localhost:37620}\r\n2021-02-08 19:58:41.715081: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:59564\r\n[       OK ] CAPI.DistributedFunctionNoError (448 ms)\r\n[ RUN      ] CAPI.RemoteExecuteDeleteContextWithOutstandingRPC\r\n2021-02-08 19:58:41.778430: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 19:58:41.843268: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:60792, 1 -> localhost:32251}\r\n2021-02-08 19:58:41.843574: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:32251\r\n2021-02-08 19:58:41.843637: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 19:58:41.843678: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-02-08 19:58:41.896621: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:60792, 1 -> localhost:32251}\r\n2021-02-08 19:58:41.952439: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:60792, 1 -> localhost:32251}\r\n2021-02-08 19:58:41.952676: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:270] Creating sync eager service context with rendezvous_id on host taurusi8028 /job:localhost/replica:0/task:1\r\n2021-02-08 19:58:41.952707: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-02-08 19:58:41.953443: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:60792, 1 -> localhost:32251}\r\n2021-02-08 19:58:41.953942: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:60792\r\n[       OK ] CAPI.RemoteExecuteDeleteContextWithOutstandingRPC (178 ms)\r\n[ RUN      ] CAPI.RemoteExecuteDeleteContextWithOutstandingRPCAsync\r\n2021-02-08 19:58:41.956466: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 19:58:41.980731: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:63496, 1 -> localhost:32519}\r\n2021-02-08 19:58:41.980969: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:32519\r\n2021-02-08 19:58:41.981026: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 19:58:41.981060: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-02-08 19:58:42.347899: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:63496, 1 -> localhost:32519}\r\n2021-02-08 19:58:42.350811: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:63496, 1 -> localhost:32519}\r\n2021-02-08 19:58:42.350917: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:270] Creating async eager service context with rendezvous_id on host taurusi8028 /job:localhost/replica:0/task:1\r\n2021-02-08 19:58:42.350946: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-02-08 19:58:42.358578: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:63496, 1 -> localhost:32519}\r\n2021-02-08 19:58:42.359091: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:63496\r\n[       OK ] CAPI.RemoteExecuteDeleteContextWithOutstandingRPCAsync (403 ms)\r\n[----------] 6 tests from CAPI (2921 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 6 tests from 1 test suite ran. (2921 ms total)\r\n[  PASSED  ] 6 tests.\r\n\r\n  YOU HAVE 1 DISABLED TEST\r\n\r\n*** Received signal 11 ***\r\n*** BEGIN MANGLED STACK TRACE ***\r\n\r\n```\r\n\r\n(yes the log ends here, no stack trace!)", "comments": []}, {"number": 47044, "title": "Upload sdists to PyPI", "body": "\r\nI wanted to check if sdists can be uploaded on PyPi and also understand if there are any issues?\r\n  \r\n   \r\n2017 : https://github.com/tensorflow/tensorflow/issues/7286 (not possible)\r\n```\r\nbazel build //tensorflow/tools/pip_package:build_pip_package     \r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package --src \"${srcdir}\"    \r\ncd \"${srcdir}\" && python3 ./setup.py sdist    \r\n```\r\n@perfinion  also mentioned that the above commands cannot create a true sdist since sdist has the compiled *.so files from bazel.\r\nAnd we need to translate the bazel rules into python build rules so it can compile the libs.\r\n", "comments": ["I think the status of #7286 is still current. Unless we get a serious investment in converting bazel rules for building the pip package into python commands we won't be able to do this."]}, {"number": 47041, "title": "TF 2.4.1 with python 3.8.7 (apple silicon with rosetta) terminated by SIGILL on import", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur 11.2\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.8.7 (x86_64 through Rosetta)\r\n\r\n**Describe the current behavior**\r\nI am using a fish shell via Rosetta (`uname -m` gives me x86_64) and I could import TF without any problems.\r\nAfter the 11.1 -> 11.2 Big Sur update, I can no longer import TensorFlow. I have no idea why.\r\n\r\n**Describe the expected behavior**\r\nno crash\r\n\r\n**Standalone code to reproduce the issue**\r\njust import tensorflow:\r\n\r\n```bash\r\nPython 3.8.7 (default, Feb  3 2021, 07:09:08)\r\n[Clang 12.0.0 (clang-1200.0.32.29)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nfish: 'python' terminated by signal SIGILL (Illegal instruction)\r\n```\r\n\r\n\r\n", "comments": ["@ziofil,\r\n\r\nCan you try to install the latest stable version of tensorflow i.e `2.6.0` and lets us know if the issue still persists. You can follow this [guide](https://www.tensorflow.org/install/source) to build from source. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "> @ziofil,\r\n> \r\n> Can you try to install the latest stable version of tensorflow i.e `2.6.0` and lets us know if the issue still persists. You can follow this [guide](https://www.tensorflow.org/install/source) to build from source. Thanks!\r\n\r\nthe issue is still relevant", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47041\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47041\">No</a>\n", "@ziofil please reopen", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47041\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47041\">No</a>\n", "Hi @ziofil ! Could you uninstall previous Tensorflow versions and try with instructions in this links [1](https://developer.apple.com/metal/tensorflow-plugin/),[ 2](https://github.com/tensorflow/tensorflow/issues/51506#issuecomment-901460541) . Attaching relevant [thread](https://github.com/tensorflow/tensorflow/issues/8976#issuecomment-299663626) for reference. Thank you!"]}, {"number": 47037, "title": "shared_embeddings columns BUG: not updated when BP in Estimator which will cause lower metrics in model", "body": "When using [share embedding column](https://www.tensorflow.org/api_docs/python/tf/feature_column/shared_embeddings) in premade Estimators, embedding table variables created in it will not be updated in training process.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Local run or distributed training have the same bug.\r\n- TensorFlow installed from (source or binary):2.4.0\r\n- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:No cuda. \r\n- GPU model and memory:No GPU. CPU training.\r\n\r\n**Describe the current behavior**\r\n\r\nWhen using tf.feature_column.shared_embeddings in premade Estimator, embedding table variables create in it will not be included in traininng_variables. When I list all variables in checkpoint, Adagrad optimizer will not create any accumulator for shared embedding table variable which means  shared embedding table variable will not  be updated when BP. This bug will cause lower metrics such as AUC compared with old share embedding column in TF1.x with almost the same code.\r\n\r\n**Describe the expected behavior**\r\n\r\n shared embedding table variable  should create accumulator in Adagrad or other optmizer.\r\n\r\nThe reason why this hadppen is that the way shared_embeddings column creates state is not like other feature column such as EmbeddingColumn which using tf.keras.layers.DenseFeatures._state_manager.create_variable, in create_variable function, it will use self._layer.add_weight so that variables can be included in keras.Model.trainable_variables. Finally I have to rewrite shared embedding column code using _state_manager and every thing will be OK.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\n\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom tensorflow import feature_column\r\n\r\ndataset_url = 'http://storage.googleapis.com/download.tensorflow.org/data/petfinder-mini.zip'\r\ncsv_file = 'datasets/petfinder-mini/petfinder-mini.csv'\r\n\r\ntf.keras.utils.get_file('petfinder_mini.zip', dataset_url,\r\n                        extract=True, cache_dir='.')\r\ndataframe = pd.read_csv(csv_file)\r\ndataframe['target'] = np.where(dataframe['AdoptionSpeed']==4, 0, 1)\r\n\r\n# Drop un-used columns.\r\ndataframe = dataframe.drop(columns=['AdoptionSpeed', 'Description'])\r\n\r\ntrain = dataframe\r\ntrain_y = dataframe.pop('target')\r\n\r\ndef input_fn(features, labels, training=True, batch_size=256):\r\n    \"\"\"An input function for training or evaluating\"\"\"\r\n    # Convert the inputs to a Dataset.\r\n    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\r\n\r\n    # Shuffle and repeat if you are in training mode.\r\n    if training:\r\n        dataset = dataset.shuffle(1000).repeat()\r\n\r\n    return dataset.batch(batch_size)\r\n\r\nwide_columns = []\r\ndeep_columns = []\r\n# numeric cols\r\nfor header in ['PhotoAmt', 'Fee', 'Age']:\r\n  wide_columns.append(feature_column.numeric_column(header))\r\n\r\nshare_embedding_list = feature_column.shared_embeddings([feature_column.categorical_column_with_hash_bucket('Color1', 10), feature_column.categorical_column_with_hash_bucket('Color2', 10)], dimension=8)\r\n\r\ndeep_columns.extend(share_embedding_list)\r\n\r\nclassifier = tf.estimator.DNNLinearCombinedClassifier(\r\n    model_dir='/tmp/model_test',\r\n    linear_feature_columns=wide_columns,\r\n    dnn_feature_columns=deep_columns,\r\n    dnn_hidden_units=[30, 10])\r\n# Train the Model.\r\nclassifier.train(\r\n    input_fn=lambda: input_fn(train, train_y, training=True),\r\n    steps=5000)\r\n# Print all variables\r\nsave_path ='/tmp/model_test/model.ckpt-5000'\r\nfor var in tf.train.list_variables(save_path):\r\n    print(var)\r\n\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n\r\nOutput: \r\n\r\n**('Color1_Color2_shared_embedding', [10, 8])**\r\n('dnn/hiddenlayer_0/bias', [30])\r\n('dnn/hiddenlayer_0/kernel', [16, 30])\r\n('dnn/hiddenlayer_1/bias', [10])\r\n('dnn/hiddenlayer_1/kernel', [30, 10])\r\n('dnn/logits/bias', [1])\r\n('dnn/logits/kernel', [10, 1])\r\n('global_step', [])\r\n('linear/linear_model/Age/weights', [1, 1])\r\n('linear/linear_model/Fee/weights', [1, 1])\r\n('linear/linear_model/PhotoAmt/weights', [1, 1])\r\n('linear/linear_model/bias_weights', [1])\r\n('training/Adagrad/decay', [])\r\n('training/Adagrad/dnn/hiddenlayer_0/bias/accumulator', [30])\r\n('training/Adagrad/dnn/hiddenlayer_0/kernel/accumulator', [16, 30])\r\n('training/Adagrad/dnn/hiddenlayer_1/bias/accumulator', [10])\r\n('training/Adagrad/dnn/hiddenlayer_1/kernel/accumulator', [30, 10])\r\n('training/Adagrad/dnn/logits/bias/accumulator', [1])\r\n('training/Adagrad/dnn/logits/kernel/accumulator', [10, 1])\r\n('training/Adagrad/learning_rate', [])\r\n('training/Ftrl/beta', [])\r\n('training/Ftrl/decay', [])\r\n('training/Ftrl/iter', [])\r\n('training/Ftrl/l1_regularization_strength', [])\r\n('training/Ftrl/l2_regularization_strength', [])\r\n('training/Ftrl/learning_rate', [])\r\n('training/Ftrl/learning_rate_power', [])\r\n('training/Ftrl/linear/linear_model/Age/weights/accumulator', [1, 1])\r\n('training/Ftrl/linear/linear_model/Age/weights/linear', [1, 1])\r\n('training/Ftrl/linear/linear_model/Fee/weights/accumulator', [1, 1])\r\n('training/Ftrl/linear/linear_model/Fee/weights/linear', [1, 1])\r\n('training/Ftrl/linear/linear_model/PhotoAmt/weights/accumulator', [1, 1])\r\n('training/Ftrl/linear/linear_model/PhotoAmt/weights/linear', [1, 1])\r\n('training/Ftrl/linear/linear_model/bias_weights/accumulator', [1])\r\n('training/Ftrl/linear/linear_model/bias_weights/linear', [1])\r\n", "comments": ["@wuxianxingkong \r\nI ran your code on tf 2.4 and nightly, please let us now if it confirms [your issue](https://colab.research.google.com/gist/Saduf2019/f7dfe59a31eab21655e311e6779112fb/untitled527.ipynb).", "@Saduf2019 Thanks for your reply, yes, it's the same with the output of mine. And I rewrite the code in [notebook](https://colab.research.google.com/gist/wuxianxingkong/0ad5e8867df4738c3734d86707cf3f16/untitled527.ipynb) to compare different behavior between feature_column.embedding_column and feature_column.shared_embeddings. \r\n``` Python\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom tensorflow import feature_column\r\n\r\ndataset_url = 'http://storage.googleapis.com/download.tensorflow.org/data/petfinder-mini.zip'\r\ncsv_file = 'datasets/petfinder-mini/petfinder-mini.csv'\r\n\r\ntf.keras.utils.get_file('petfinder_mini.zip', dataset_url,\r\n                        extract=True, cache_dir='.')\r\ndataframe = pd.read_csv(csv_file)\r\ndataframe['target'] = np.where(dataframe['AdoptionSpeed']==4, 0, 1)\r\n\r\n# Drop un-used columns.\r\ndataframe = dataframe.drop(columns=['AdoptionSpeed', 'Description'])\r\n\r\ntrain = dataframe\r\ntrain_y = dataframe.pop('target')\r\n\r\ndef input_fn(features, labels, training=True, batch_size=256):\r\n    \"\"\"An input function for training or evaluating\"\"\"\r\n    # Convert the inputs to a Dataset.\r\n    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\r\n\r\n    # Shuffle and repeat if you are in training mode.\r\n    if training:\r\n        dataset = dataset.shuffle(1000).repeat()\r\n\r\n    return dataset.batch(batch_size)\r\n\r\nwide_columns = []\r\ndeep_columns = []\r\n# numeric cols\r\nfor header in ['PhotoAmt', 'Fee', 'Age']:\r\n  wide_columns.append(feature_column.numeric_column(header))\r\nc1 = feature_column.categorical_column_with_hash_bucket('Color1', 10)\r\nc2 = feature_column.categorical_column_with_hash_bucket('Color2', 10)\r\nc1_embdding = feature_column.embedding_column(c1, 4)\r\nshare_embedding_list = feature_column.shared_embeddings([c1, c2], dimension=8)\r\n\r\ndeep_columns.extend(share_embedding_list)\r\ndeep_columns.append(c1_embdding)\r\n\r\nclassifier = tf.estimator.DNNLinearCombinedClassifier(\r\n    model_dir='/tmp/model_test_tf24',\r\n    linear_feature_columns=wide_columns,\r\n    dnn_feature_columns=deep_columns,\r\n    dnn_hidden_units=[30, 10])\r\n# Train the Model.\r\nclassifier.train(\r\n    input_fn=lambda: input_fn(train, train_y, training=True),\r\n    steps=5000)\r\n# Print all variables\r\nsave_path ='/tmp/model_test_tf24/model.ckpt-5000'\r\nfor var in tf.train.list_variables(save_path):\r\n    print(var)\r\n\r\n\r\n```\r\n As you can see from the output of [notebook](https://colab.research.google.com/gist/wuxianxingkong/0ad5e8867df4738c3734d86707cf3f16/untitled527.ipynb), I add a c1_embdding column to deep_columns array,  c1_embedding column will create **('dnn/input_from_feature_columns/input_layer/Color1_embedding/embedding_weights', [10, 4])** and **('training/Adagrad/dnn/input_from_feature_columns/input_layer/Color1_embedding/embedding_weights/accumulator', [10, 4])** , but shared_embeddings([c1, c2]) only creates **('Color1_Color2_shared_embedding', [10, 8])**.\r\n\r\nYou can also print all trainable_variables in the line [505](https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/canned/dnn.py#L505)\uff0c the shared embedding variables will not appear in it.", "@ymodak Any update on this issue?"]}, {"number": 47032, "title": "Setting class_weight in model.fit() with tf.data.Dataset causes error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04  / Windows 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0 / 2.4.1\r\n- Python version: 3.6 / 3.7\r\n- CUDA/cuDNN version: 10.1 / none\r\n- GPU model and memory: RTX2080 / none\r\n\r\n**Describe the current behavior**\r\nWhen a tf.data.Dataset is used in model.fit(), setting class_weight causes an error.\r\n\r\n**Describe the expected behavior**\r\nNo error occurs.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nfrom tensorflow import keras\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\ndef get_model():\r\n    inputs = keras.layers.Input(shape=(10, 10, 3))\r\n    x = keras.layers.Flatten()(inputs)\r\n    outputs = keras.layers.Dense(5)(x)\r\n    model = keras.Model(inputs, outputs)\r\n    model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\r\n    return model\r\n\r\n\r\ndef map_fun(_):\r\n    dummy_image = np.zeros((10, 10, 3))  \r\n    dummy_label = np.array([0, 0, 1, 0, 0]) \r\n    return dummy_image, dummy_label\r\n\r\n\r\nif __name__ == '__main__':\r\n    # dummy dataset\r\n    dataset = tf.data.Dataset.from_tensor_slices([1, 2])  # values are ignored, dummy data generated in map()\r\n    dataset = dataset.map(map_func=lambda x: tf.py_function(map_fun, [x], [tf.uint8, tf.uint8])).batch(2)\r\n\r\n    # dummy model\r\n    model = get_model()\r\n\r\n    # call fit() without class weights - ok\r\n    model.fit(dataset, epochs=1)\r\n\r\n    # define class weights\r\n    class_weight = {idx: weight for (idx, weight) in enumerate([1., 1., 1., 1., 1.])}\r\n\r\n    # transform dataset to iterator, call fit() with class weights - ok\r\n    model.fit(dataset.as_numpy_iterator(), class_weight=class_weight, epochs=1)\r\n\r\n    # call fit() with class weights on tf.data.Dataset - error\r\n    model.fit(dataset, class_weight=class_weight, epochs=1)\r\n```\r\n\r\n**Error message**\r\n```\r\nTraceback (most recent call last):\r\n  File \"/data/sandbox/reproduce.py\", line 39, in <module>\r\n    model.fit(dataset, class_weight=class_weight, epochs=1)\r\n  File \"/data/sandbox/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 108, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/data/sandbox/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1063, in fit\r\n    steps_per_execution=self._steps_per_execution)\r\n  File \"/data/sandbox/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 1122, in __init__\r\n    dataset = dataset.map(_make_class_weight_map_fn(class_weight))\r\n  File \"/data/sandbox/venv/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1695, in map\r\n    return MapDataset(self, map_func, preserve_cardinality=True)\r\n  File \"/data/sandbox/venv/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 4045, in __init__\r\n    use_legacy_function=use_legacy_function)\r\n  File \"/data/sandbox/venv/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 3371, in __init__\r\n    self._function = wrapper_fn.get_concrete_function()\r\n  File \"/data/sandbox/venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 2939, in get_concrete_function\r\n    *args, **kwargs)\r\n  File \"/data/sandbox/venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 2906, in _get_concrete_function_garbage_collected\r\n    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n  File \"/data/sandbox/venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 3213, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/data/sandbox/venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 3075, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/data/sandbox/venv/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\", line 986, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/data/sandbox/venv/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 3364, in wrapper_fn\r\n    ret = _wrapper_helper(*args)\r\n  File \"/data/sandbox/venv/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 3299, in _wrapper_helper\r\n    ret = autograph.tf_convert(func, ag_ctx)(*nested_args)\r\n  File \"/data/sandbox/venv/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 255, in wrapper\r\n    return converted_call(f, args, kwargs, options=options)\r\n  File \"/data/sandbox/venv/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 532, in converted_call\r\n    return _call_unconverted(f, args, kwargs, options)\r\n  File \"/data/sandbox/venv/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 339, in _call_unconverted\r\n    return f(*args, **kwargs)\r\n  File \"/data/sandbox/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 1314, in _class_weights_map_fn\r\n    if y.shape.rank > 2:\r\nTypeError: '>' not supported between instances of 'NoneType' and 'int'\r\n\r\nProcess finished with exit code 1\r\n```", "comments": ["@tensortorch,\r\nPlease take a look at [this comment](https://github.com/keras-team/keras/issues/3653#issuecomment-243939748) from similar issue and check if it helps. Thanks!", "@amahendrakar,\r\nthank you for your suggestion!\r\n\r\nIt does help since I can make my dataset return a sample weight as a third value, which is what model.fit() does anyway under the hood when class_weight is provided. So one can work around this issue using sample weights instead.\r\n\r\nHowever, I believe providing class_weight in model.fit() should still work. The linked comment explains that it is not expected to work for 3+ dimensional targets, but this is not the case here. In fact, the error occurs within the check for target dimensionality, and is caused by the target rank being None for some reason:\r\n```\r\n    if y.shape.rank > 2:   # <== this is where the error occurs, because y.shape.rank is None\r\n      raise ValueError(\"`class_weight` not supported for \"\r\n                       \"3+ dimensional targets.\")\r\n```\r\nAlso, it does work for for the same inputs when the dataset is converted to an iterator, as my minimal example shows. The error only occurs for a tf.data.Dataset as input value. Here, the DataHandler attempts to add the third output to the dataset by calling map() to convert class_weight to sample_weight:\r\n```\r\n    if class_weight:\r\n      dataset = dataset.map(_make_class_weight_map_fn(class_weight))\r\n```\r\nwhich fails due to the aforementioned error.\r\n\r\n", "The [workaround](https://github.com/tensorflow/tensorflow/issues/32912#issuecomment-550363802) for a somewhat related problem also works in this case: an additional call to map() to manually set the tensor shape makes it work.\r\n\r\nI previously tried manually converting the outputs of the py_function to tensors and also manually setting their shapes, but it did not work, so the key here is the second call to map() to set the shapes after batch().\r\n", "@tensortorch,\r\nThank you for the update. Is this still an issue?", "@amahendrakar \r\nI still think this is an issue. I believe it should not be necessary to call map() a second time - I would expect this to work without the extra steps. The issue might not be with the class weights functionality itself though - maybe rather with the combination of map/py_function.", "@jvishnuvardhan,\r\nI was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/05d275a4b5719c6e85b89425ab8e639a/47032.ipynb). Thanks!", "Same error, checked on python 3.8 and TF: v2.2.0, 2.3.0, 2.4.0, 2.4.1\r\nFound a quick solution with custom loss function:\r\n```\r\ndef weighted_categorical_crossentropy( weights ):\r\n    # weights = [ 0.9, 0.05, 0.04, 0.01 ]\r\n    def wcce( y_true, y_pred ):\r\n        tf_weights = tf.constant( weights )\r\n        if not tf.is_tensor( y_pred ):\r\n            y_pred = tf.constant( y_pred )\r\n\r\n        y_true = tf.cast( y_true, y_pred.dtype )\r\n        return tf.keras.losses.categorical_crossentropy( y_true, y_pred ) * tf.experimental.numpy.sum( y_true * tf_weights, axis = -1 )\r\n    return wcce\r\n\r\n...\r\nconfig['loss'] = weighted_categorical_crossentropy( config['classWeight'] )\r\nmodel.compile(\r\n    loss = config['loss'],\r\n    optimizer = config['optimizer'],\r\n    metrics = ['accuracy'],\r\n    run_eagerly = True\r\n)\r\n```", "This still fails in tf 2.5.0. On the go I've created another reproduction script: https://gist.github.com/kretes/ca911085b2eb0fa3985894245ce3fd0c\r\nSetting shape works but introduce burden on user's code.\r\n\r\nI suggest changing the name of the issue to 'Setting class_weight in model.fit() with tf.data.Dataset using py_function causes error'. as this is the step that makes for unknown shape\r\n\r\n", "> Same error, checked on python 3.8 and TF: v2.2.0, 2.3.0, 2.4.0, 2.4.1\r\n> Found a quick solution with custom loss function:\r\n> \r\n> ```\r\n> def weighted_categorical_crossentropy( weights ):\r\n>     # weights = [ 0.9, 0.05, 0.04, 0.01 ]\r\n>     def wcce( y_true, y_pred ):\r\n>         tf_weights = tf.constant( weights )\r\n>         if not tf.is_tensor( y_pred ):\r\n>             y_pred = tf.constant( y_pred )\r\n> \r\n>         y_true = tf.cast( y_true, y_pred.dtype )\r\n>         return tf.keras.losses.categorical_crossentropy( y_true, y_pred ) * tf.experimental.numpy.sum( y_true * tf_weights, axis = -1 )\r\n>     return wcce\r\n> \r\n> ...\r\n> config['loss'] = weighted_categorical_crossentropy( config['classWeight'] )\r\n> model.compile(\r\n>     loss = config['loss'],\r\n>     optimizer = config['optimizer'],\r\n>     metrics = ['accuracy'],\r\n>     run_eagerly = True\r\n> )\r\n> ```\r\n\r\nthis still shows error , please help \r\ntf.keras.losses.categorical_crossentropy( y_true, y_pred ) expects 0 arguments got 2"]}, {"number": 47022, "title": "tf.strings.split cannot handle some characters", "body": "**System information**\r\n- OS Platform: Windows 10\r\n- TensorFlow version: 2.3.2\r\n- Python version: 3.8.5\r\n\r\n**Issue**\r\nThis code:\r\n````\r\nprint(tf.strings.split(tf.constant([\"abc \u00e0 cc\"])))\r\n````\r\nproduces this:\r\n````\r\n<tf.RaggedTensor [[b'abc', b'\\xc3']]>\r\n````\r\nPython split works just fine:\r\n````\r\nprint(\"abc \u00e0 cc\".split(\" \"))\r\n````\r\nand prints:\r\n````\r\n['abc', '\u00e0', 'cc']\r\n````\r\nThis is what I suspect is the problem: this code `\"\u00e0\".encode() ` prints `b'\\xc3\\xa0'`. This byte literal `\\xa0` is the nonbreaking space character (codepoint 160 in UTF-16 and UTF-32).", "comments": ["I am able to replicate the issue reported on tf 2.4 and nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/3c84e607809e252bd2553092599c11e8/untitled524.ipynb).", "@Saduf2019 \r\nThe gist you linked above shows that it's working fine on colab in 2.4 and nightly. I think you got confused by the byte literals (string tensors hold strings as bytes instead of unicode strings), but that's not a bug to the best of my knowledge. This is likely a problem that occurs with tensorflow only on Windows, because this issue is related to [this other issue](https://github.com/tensorflow/tensorflow/issues/43559). \r\n\r\nIn fact, I was running into that issue again (the issue linked above), so I tried to get to the root cause and discovered that this orphan byte literal `b'\\xc3'` (sometimes `b'\\xc2'` too) is created during splitting (you can find [my comment](https://github.com/tensorflow/tensorflow/issues/43559#issuecomment-762510624) on that issue as well). I thought it's better to report the isolated root problem.\r\n\r\nBut note that this can't be simply dismissed as a Windows or python problem since the regular python split works without any issues.", "@princyok  Yeah the code is working fine on colab and  will let you know the update once we fix it. Thanks!", "I can reproduce this issue on Windows 10, Python 3.8.7, Tensorflow 2.6.0:\r\n```\r\nPython 3.8.7 (tags/v3.8.7:6503f05, Dec 21 2020, 17:59:51) [MSC v.1928 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.strings.split(\"verit\u00e0 truth\").numpy()\r\narray([b'verit\\xc3'], dtype=object)\r\n>>> tf.strings.split(\"verita truth\").numpy()\r\narray([b'verita', b'truth'], dtype=object)\r\n>>> tf.__version__\r\n'2.6.0'\r\n>>>\r\n```", "Tracing back how `tf.strings.split` is actually implemented, it would seem the error is in https://github.com/abseil/abseil-cpp/blob/master/absl/strings/str_split.h possibly? At least that's what I can make out from reading https://github.com/tensorflow/tensorflow/blob/556c3a9c41ce0590c12390a0d2fb90af7e035f0a/tensorflow/core/ops/BUILD#L122-L132 and https://github.com/tensorflow/tensorflow/blob/556c3a9c41ce0590c12390a0d2fb90af7e035f0a/tensorflow/core/ops/string_ops.cc#L19", "Nevermind, I am new to tensorflow and didn't know where the kernels are actually implemented. This op is implemented at https://github.com/tensorflow/tensorflow/blob/0b6b491d21d6a4eb5fbab1cca565bc1e94ca9543/tensorflow/core/kernels/string_split_op.cc#L102-L158, and notably it's just the implicit delimiter version that's broken:\r\n```\r\nPython 3.8.7 (tags/v3.8.7:6503f05, Dec 21 2020, 17:59:51) [MSC v.1928 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.strings.split([\"verit\u00e0 truth\"], ' ')\r\n<tf.RaggedTensor [[b'verit\\xc3\\xa0', b'truth']]>\r\n>>> tf.strings.split([\"verit\u00e0 truth\"])\r\n<tf.RaggedTensor [[b'verit\\xc3']]>\r\n>>>\r\n```\r\nThe correctness of that algorithm seems to depend on https://github.com/tensorflow/tensorflow/blob/0b6b491d21d6a4eb5fbab1cca565bc1e94ca9543/tensorflow/core/platform/str_util.cc#L88-L105, which in turn depends on a valid `isspace` function. Despite `\\xa0` being a non-breaking space, both absl and std ignore that and just split on ascii spaces. I'm not entirely sure where this `isspace` function is coming from, however, so if it is OS-defined then everything makes sense.\r\n\r\nOnce I figure out how to build Tensorflow from source, I'll post an update on whether switching to `absl::ascii_isspace` makes a difference."]}, {"number": 46996, "title": "tf.nn.local_response_normalization outputs NaN", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n**Describe the current behavior**\r\n`tf.nn.local_response_normalization` outputs NaN when `input` contains large value\r\n\r\n**Describe the expected behavior**\r\nExpect a grace exception message if the input is invalid instead of Nan as output\r\n\r\n**Standalone code to reproduce the issue**\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ninput=np.array([[[[-2e+38, 1, 1e+38,1, 1, 1, 1]]]])\r\ntf.nn.local_response_normalization(input=input, alpha=1, beta=1)\r\n~~~\r\n\r\n\r\nOutput:\r\n~~~python\r\n<tf.Tensor: shape=(1, 1, 1, 7), dtype=float32, numpy=array([[[[-0.,  0.,  0.,  0.,  0.,  0., nan]]]], dtype=float32)>\r\n~~~\r\n\r\n", "comments": ["I am able to replicate the issue reported on tf 2.4 and tf-nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/ec7955763c3e037dc713b13466358e86/untitled522.ipynb).\r\nThanks!", "Issue still exists in TF 2.6 as well and please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/c22526fed213e1acf66e96b4f2f7d5a4/untitled92.ipynb).Thanks!", "Thanks for reporting this. This possibly requires proper handling of large numbers in the [LRN op](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/kernels/lrn_op.cc;l=114-123;drc=3c2220abe73e887d17db83653afb49c2d0beef2e). We would be happy to review a fix.", "Thank you for filing this issue. \r\n\r\n1. Regarding having a graceful exception message if the input is invalid instead of Nan as output: Generating NaNs on rare occasions may not significantly impact training or inference, if the model is robust to such bad values.  However, generating an exception would stop the process, causing the training or inference to fail. Outputting NaN is therefore preferable for this case. For cases where generating an exception is desired, a test for NaN after (external to) tf.nn.local_response_normalization could be used. Therefore, we decided the output is as intended.\r\n\r\n2. As part of the investigation of this issue, we considered a more robust computation that that reduces the occurrence of NaNs caused by over/underflows in intermediate calculations.  However, this comes with a significant performance penalty. We did not identify any solution that applied to all backends and execution paths that did not have a significant negative impact on performance. Therefore, we decided against making any changes for this.\r\n\r\nBackground information: As an example, for float32 type, 3.4e+38 is near the maximum, so values above approximately 1.844e+19 will cause an internal overflow with computational procedures that square inputs (even if the final output could be represented as a float32). The cases where NaNs are generated can vary due to backend and execution path, so there is no guarantee either way about whether inputs will be squared. See Blue's algorithm, \u201cA Portable Fortran Program to Find the Euclidean Norm of a Vector, ACM TOMS, Vol 4, Issue 1, 1978\u201d for discussion of stable computation for the beta=0.5 case."]}, {"number": 46995, "title": "`tf.nn.weighted_moments` outputs NaN when `axes=[]` and `x` is complex", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n-\r\n\r\n**Describe the current behavior**\r\n`tf.nn.weighted_moments` outputs NaN when `axes=[]` and `x` is complex\r\n**Describe the expected behavior**\r\nexpect no Nan in output\r\n\r\n**Standalone code to reproduce the issue**\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.nn.weighted_moments(x=np.array([1e+38+0.j, 2e+38+0.j], dtype=np.complex64), frequency_weights=10, axes=[])\r\n~~~\r\n\r\nOutput:\r\n~~~python\r\n(<tf.Tensor: shape=(2,), dtype=complex64, numpy=array([inf+nanj, inf+nanj], dtype=complex64)>, <tf.Tensor: shape=(2,), dtype=complex64, numpy=array([inf+nanj, inf+nanj], dtype=complex64)>)\r\n~~~", "comments": ["Was able to reproduce the issue with TF v2.1, TF v2.4 and TF-nightly(`2.5.0-dev20210204`). Please find the gist of it [here](https://colab.research.google.com/gist/ravikyram/178106455baefc09e20739cdaded9c97/untitled663.ipynb). Thanks!", "Issue still exists in TF 2.6 and please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/8b60e3cd2d1c73fb3c106b1b598572e5/untitled92.ipynb).Thanks!", "Thank you for filing this issue.\r\nLarge _x_ values multiplied by large _frequency_weights_ with a result greater than about 3.4e+38 causes overflow in float32 representation (which is used by np.complex64). As a work around for cases where avoiding this kind of overflow is appropriate, the caller can normalize frequency_weights to have small values (e.g. by using a floating point type and dividing by the maximum value so that the maximum value of the normalized result is 1.0). However, there is no generic way to rescale weights that would not conflict with a valid use case (e.g. that would not underflow for small values). Therefore, we decided to not change the behavior.\r\n"]}, {"number": 46985, "title": "Stateful global metrics with multi-device distribution", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.4.1\r\n- Are you willing to contribute it (Yes/No): Unsure\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nAssume you want to create a metric which is calculated over the full dataset (i.e. evaluating how good a model is at sorting via some regression problem). On a single GPU you could do this:\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras.backend import track_variable\r\n\r\nclass MyCustomGlobalMetric(tf.keras.metrics.Metric):\r\n    def __init__(self, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.y_true = tf.Variable(name='y_true', shape=tf.TensorShape(None), dtype=tf.dtypes.float32,\r\n                                  initial_value=[0.],\r\n                                  validate_shape=False, synchronization=tf.VariableSynchronization.ON_READ,\r\n                                  aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA)\r\n        self.y_pred = tf.Variable(name='y_pred', shape=tf.TensorShape(None), dtype=tf.dtypes.float32,\r\n                                  initial_value=[0.],\r\n                                  validate_shape=False, synchronization=tf.VariableSynchronization.ON_READ,\r\n                                  aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA)\r\n        self._non_trainable_weights.append(self.y_true)\r\n        self._non_trainable_weights.append(self.y_pred)\r\n        track_variable(self.y_true)\r\n        track_variable(self.y_pred)\r\n        self.was_called = self.add_weight('was_called', initializer=tf.keras.initializers.zeros, dtype=tf.dtypes.uint8,\r\n                                          synchronization=tf.VariableSynchronization.ON_READ,\r\n                                          aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA)\r\n\r\n    def update_state(self, y_true, y_pred, **kwargs):\r\n        y_true = K.flatten(K.cast(y_true, self.y_true.dtype))\r\n        y_pred = K.flatten(K.cast(y_pred, self.y_true.dtype))\r\n        a = self.y_true.assign(tf.concat([self.y_true, y_true], axis=0))\r\n        b = self.y_pred.assign(tf.concat([self.y_pred, y_pred], axis=0))\r\n        c = self.was_called.assign(1)\r\n        return tf.group(a, b, c)\r\n\r\n    def result(self):\r\n        return tf.cond(self.was_called >= 1, lambda: some_custom_metric(self.y_true[1:], self.y_pred[1:]), lambda: 0.)\r\n\r\n    def reset_states(self):\r\n        super().reset_states()\r\n        self.y_true.assign(tf.constant([0.]))\r\n        self.y_pred.assign(tf.constant([0.]))\r\n        self.was_called.assign(0)\r\n```\r\nThis will store all predictions and labels and calculates `some_custom_metric` for them. But this fails to calculate the correct metric when instantiated and used in a MirroredStrategy scope. Growing the variable shape is a hacky workaround to get this running, but there is no workaround in the first way. None of the `VariableAggregation` options actually make sense when you want to calculate the metric in a global sense on multiple devices.\r\n\r\nI'd like to have the possibility to implement a stateful metric which allows me to store all predictions and labels, such that I can calculate some arbitrary metric over them, which should be consistent and correct even if I'm using a MirroredStrategy.\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n", "comments": []}, {"number": 46977, "title": "Is there a error \"DCHECK((ref_.store(0), true));\"  of RefCounted::Unref() function.", "body": "https://github.com/tensorflow/tensorflow/blob/3db52be7be81a87c623cdeb7f03d3767521c5246/tensorflow/core/lib/core/refcount.h#L103\r\n\r\n`DCHECK((ref_.store(0), true));`  shoud convert the ref_'value  to 0. but after run `DCHECK((ref_.store(0), true));`, the ref_'value is still 1.\r\n\r\nthen, there is a test code\uff1a\r\n`#include<iostream>\r\n\r\n#include \"tensorflow/core/lib/core/refcount.h\"\r\n\r\n#include \"tensorflow/core/platform/test.h\"\r\n\r\nnamespace tensorflow {\r\nnamespace core {\r\nnamespace {\r\nTEST(RefCountTest, UnRefTest) {\r\n  std::atomic_int_fast32_t ref_(1);\r\n  DCHECK((ref_.store(0), true));\r\n  // CHECK(ref_.load() == 0);   //here's check could not passed.\r\n  std::cout << \"ref_value: \" <<  ref_.load(std::memory_order_acquire);\r\n}\r\n}\r\n}\r\n}`\r\n\r\nResult:\r\n`Running main() from test_main.cc\r\n   [==========] Running 1 test from 1 test suite.\r\n  [----------] Global test environment set-up.\r\n  [----------] 1 test from RefCountTest\r\n  [ RUN      ] RefCountTest.UnRefTest\r\n  ref_value: 1[       OK ] RefCountTest.UnRefTest (0 ms)\r\n  [----------] 1 test from RefCountTest (0 ms total)\r\n \r\n  [----------] Global test environment tear-down\r\n  [==========] 1 test from 1 test suite ran. (0 ms total)\r\n[  PASSED  ] 1 test.`\r\n", "comments": ["cc @jvishnuvardhan ", "The tuple in DCHECK is only evaluated under debug mode. (`#undef NDEBUG`). Could this be the cause of this anomaly?"]}, {"number": 46975, "title": "tensorflow not working with RTX 3070 cuda 11.1+cudnn 8.0 and 11.2+cudnn 8.1", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\nI have first installed cuda 11.1 and cudnn 8.0 but i was getting error could not find file libcusolver.so.10 \r\nto solve this i run command \r\nsudo ln -s /usr/local/cuda-11.1/lib64/libcusolver.so.11 /usr/local/cuda-11.1/lib64/libcusolver.so.10\r\nerror was resolved and showed message \r\nSuccessfully opened dynamic library libcusolver.so.10\r\nbut now i got another error shown below\r\nI also tried cuda 11.2 and cudnn 8.1 there with the same error could not find file libcusolver.so.10  i am getting error could not find libcudnn.so.8\r\n\r\nPlease help me i just re installed multiple cuda versions cudnn versions and nvidia drivers but could not resolve the issue\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip install tensorflow-gpu \r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version:3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda 11.1 cudnn 8.0\r\n- GPU model and memory: RTX 3070\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n2021-02-07 02:05:18.468688: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\nx_train shape: (60000, 28, 28, 1)\r\n60000 train samples\r\n10000 test samples\r\n2021-02-07 02:05:19.608268: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-07 02:05:19.608796: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-02-07 02:05:19.636743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-07 02:05:19.637151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: Graphics Device computeCapability: 8.6\r\ncoreClock: 1.725GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-02-07 02:05:19.637166: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-02-07 02:05:19.638801: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-02-07 02:05:19.638838: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-02-07 02:05:19.639414: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-02-07 02:05:19.639618: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-02-07 02:05:19.835524: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2021-02-07 02:05:19.837327: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-02-07 02:05:19.837692: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-02-07 02:05:19.837958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-07 02:05:19.839606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-07 02:05:19.851728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-02-07 02:05:19.852873: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-02-07 02:05:19.854566: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-07 02:05:19.854939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-07 02:05:19.856558: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: Graphics Device computeCapability: 8.6\r\ncoreClock: 1.725GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-02-07 02:05:19.856623: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-02-07 02:05:19.856695: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-02-07 02:05:19.856746: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-02-07 02:05:19.856795: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-02-07 02:05:19.856844: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-02-07 02:05:19.856893: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2021-02-07 02:05:19.856942: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-02-07 02:05:19.856991: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-02-07 02:05:19.857186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-07 02:05:19.858869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-07 02:05:19.860412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-02-07 02:05:19.866190: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-02-07 02:05:22.758155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-02-07 02:05:22.758203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2021-02-07 02:05:22.758221: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2021-02-07 02:05:22.758447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-07 02:05:22.759222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-07 02:05:22.759935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-07 02:05:22.760605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6864 MB memory) -> physical GPU (device: 0, name: Graphics Device, pci bus id: 0000:01:00.0, compute capability: 8.6)\r\n2021-02-07 02:05:22.990187: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-02-07 02:05:23.007092: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2899885000 Hz\r\nEpoch 1/12\r\n2021-02-07 02:05:23.350598: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-02-07 02:05:27.235033: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-02-07 02:05:27.282436: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-02-07 02:05:29.037626: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2021-02-07 02:05:29.061028: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 63, in <module>\r\n    validation_data=(x_test, y_test))\r\n  File \"/home/mohsin/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1100, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n  File \"/home/mohsin/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/mohsin/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 888, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/home/mohsin/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2943, in __call__\r\n    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\r\n  File \"/home/mohsin/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1919, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/home/mohsin/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 560, in call\r\n    ctx=ctx)\r\n  File \"/home/mohsin/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node sequential/conv2d/Conv2D (defined at main.py:63) ]] [Op:__inference_train_function_790]\r\n\r\nFunction call stack:\r\ntrain_function\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@game-sys \r\n\r\nPlease take a look at this [comment ](https://github.com/tensorflow/tensorflow/issues/43947#issuecomment-716102142)from issue #43947 and #45848  with a similar error and let us know if it helps. Thanks!", "https://github.com/tensorflow/tensorflow/issues/45848#issuecomment-748130779 and other mentioned downgrading cuda to 11.0 and cudnn 8.0, but I think cuda 11.0 is not supported in rtx 3070. Please can you confirm  ", "> [#45848 (comment)](https://github.com/tensorflow/tensorflow/issues/45848#issuecomment-748130779) and other mentioned downgrading cuda to 11.0 and cudnn 8.0, but I think cuda 11.0 is not supported in rtx 3070. Please can you confirm\r\n\r\nsee https://docs.nvidia.com/deeplearning/cudnn/support-matrix/index.html and https://en.wikipedia.org/wiki/CUDA#GPUs_supported", "**i tried to install cuda 11.0 using :**\r\n\r\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin\r\nsudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600\r\nwget http://developer.download.nvidia.com/compute/cuda/11.0.2/local_installers/cuda-repo-ubuntu2004-11-0-local_11.0.2-450.51.05-1_amd64.deb\r\nsudo dpkg -i cuda-repo-ubuntu2004-11-0-local_11.0.2-450.51.05-1_amd64.deb\r\nsudo apt-key add /var/cuda-repo-ubuntu2004-11-0-local/7fa2af80.pub\r\nsudo apt-get update\r\nsudo apt-get -y install cuda\r\n\r\n**but i got error :**\r\n\r\nProcessing triggers for initramfs-tools (0.136ubuntu6.3) ...\r\nupdate-initramfs: Generating /boot/initrd.img-5.8.0-41-generic\r\nErrors were encountered while processing:\r\n nvidia-dkms-450\r\n nvidia-driver-450\r\n cuda-drivers-450\r\n cuda-drivers\r\n cuda-runtime-11-0\r\n cuda-11-0\r\n cuda-demo-suite-11-0\r\n cuda\r\nE: Sub-process /usr/bin/dpkg returned an error code (1)\r\n\r\ni think cuda 11.0 or 450 driver do not support rtx 3000 series, although https://docs.nvidia.com/deeplearning/cudnn/support-matrix/index.html state it support 11.0 or i dont if it not support with ubuntu 20 as i installed cuda 11.1 which use 455 driver and 11.2 which use 460 driver using above method and I got no error. \r\n\r\nafter restarting system i check driver using command **nvidia-smi** \r\n\r\n**i got error :**\r\nNVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\r\n", "**I purge cuda 11.0 using command:**\r\n\r\nsudo apt --purge remove \"cublas*\" \"cuda*\"\r\nsudo apt --purge remove \"nvidia*\"\r\nrm -rf /usr/local/cuda*\r\nsudo apt-get autoremove && sudo apt-get autoclean\r\n\r\n**and upgrade to cuda 11.1 using command:**\r\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin\r\nsudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600\r\nwget https://developer.download.nvidia.com/compute/cuda/11.1.0/local_installers/cuda-repo-ubuntu2004-11-1-local_11.1.0-455.23.05-1_amd64.deb\r\nsudo dpkg -i cuda-repo-ubuntu2004-11-1-local_11.1.0-455.23.05-1_amd64.deb\r\nsudo apt-key add /var/cuda-repo-ubuntu2004-11-1-local/7fa2af80.pub\r\nsudo apt-get update\r\nsudo apt-get -y install cuda\r\n\r\n**update path using command :**\r\n nano /home/$USER/.bashrc\r\n\r\n_add below lines_\r\n\r\nexport PATH=\"/usr/local/cuda-8.0/bin:$PATH\"\r\nexport LD_LIBRARY_PATH=\"/usr/local/cuda-8.0/lib64:$LD_LIBRARY_PATH\"\r\n\r\nsource .bashrc\r\n\r\n**finally got nvcc using command:** \r\nnvcc --version\r\n\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2020 NVIDIA Corporation\r\nBuilt on Tue_Sep_15_19:10:02_PDT_2020\r\nCuda compilation tools, release 11.1, V11.1.74\r\nBuild cuda_11.1.TC455_06.29069683_0\r\n\r\n", "**run my code on terminal using command:**\r\npython main.py \r\n\r\n**obviously not installed CUDNN so got a message:**\r\n\r\n2021-02-09 00:39:50.118328: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-02-09 00:39:50.119809: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-02-09 00:39:50.119836: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-02-09 00:39:50.120394: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-02-09 00:39:50.120525: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n**2021-02-09 00:39:50.120603: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.1/lib64:/usr/local/cuda-11.1/lib64:**\r\n2021-02-09 00:39:50.120943: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n**2021-02-09 00:39:50.120991: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.1/lib64:/usr/local/cuda-11.1/lib64:**\r\n2021-02-09 00:39:50.120999: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n\r\nthen i downloaded **CUDNN 8.0** and \r\n**installed it using command:**\r\nsudo cp cuda/include/cudnn*.h /usr/local/cuda/include \r\nsudo cp -P cuda/lib64/libcudnn* /usr/local/cuda/lib64 \r\nsudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn*\r\n\r\n**i re run code using command:**\r\npython main.py \r\n\r\n**and got message** \r\n\r\n2021-02-09 00:42:01.374301: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-02-09 00:42:01.375808: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-02-09 00:42:01.375853: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-02-09 00:42:01.376430: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-02-09 00:42:01.376587: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n**2021-02-09 00:42:01.376701: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.1/lib64:/usr/local/cuda-11.1/lib64:**\r\n2021-02-09 00:42:01.377062: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n**2021-02-09 00:42:01.377170: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8**\r\n2021-02-09 00:42:01.377179: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n\r\nas you can see from above message **libcudnn.so.8** is sucessfully loaded but still **libcusolver.so.10** is not loaded what is the problem, am i not doing something wrong or missing some step.\r\nfor further debugging i go to **/usr/local/cuda/lib64** and noticed there is no file **libcusolver.so.10**  but there is a file named **libcusolver.so.11** ", "Still issue is not solved waiting for your reply :(", "hey @game-sys FWIW I have TensorFlow running with cuda 11.0 + cudnn 8.0.4 installed via `conda` using the `nvidia` channel.\r\n\r\nPerhaps cuda 11.1 would bring performance improvements, but if you're blocked from working at all because of this, there are options. Cheers", "@nathanin how did u install cuda 11.0 did u install it like i showed above or some another way because using above method it also install Nvidia 450 driver which is the problem i think and can u please confirm your driver version. Thanks", "I observe the same issue - I install tensorflow 2.4.1 into a docker container and run jupyter lab from there... nvidia-smi works fine but tensorflow reports zero GPUs available ... adding one line to the docker file solves the problem: (see [here](https://github.com/tensorflow/tensorflow/issues/43947) )\r\n\r\n```\r\n# Copyright (c) Jupyter Development Team.\r\n# Distributed under the terms of the Modified BSD License.\r\nARG BASE_CONTAINER=jupyter/scipy-notebook\r\nFROM $BASE_CONTAINER\r\n\r\nLABEL maintainer=\"Jupyter Project <jupyter@googlegroups.com>\"\r\n\r\n# Install Tensorflow\r\nRUN pip install --quiet --no-cache-dir \\\r\n    'tensorflow==2.4.1' && \\\r\n    fix-permissions \"${CONDA_DIR}\" && \\\r\n    fix-permissions \"/home/${NB_USER}\"\r\n\r\n## BUG FIX:\r\nRUN ln -s /usr/local/cuda-11.2/targets/x86_64-linux/lib/libcusolver.so.11 $(python -c \"import tensorflow.python as x; print(x.__path__[0])\")/libcusolver.so.10\r\n\r\n```", "`nvidia-smi`:\r\n```\r\nMon Feb 15 13:31:01 2021       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 3070    Off  | 00000000:08:00.0 Off |                  N/A |\r\n|  0%   40C    P8    18W / 240W |     18MiB /  7979MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A      1628      G   /usr/lib/xorg/Xorg                  9MiB |\r\n|    0   N/A  N/A      1937      G   /usr/bin/gnome-shell                4MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nI'm not entirely sure where the `CUDA Version: 11.2` comes from, it must have been installed when I installed the driver from the nvidia ppa. The environment I have working was built by installing `cudatoolkit` from conda with: `conda install -c nvidia cudatoolkit=11.0` (the pytorch conda channel will probably also work):\r\n```\r\nconda list | grep cuda\r\ncudatoolkit               11.0.221             h6bb024c_0    nvidia\r\ncudnn                     8.0.4                cuda11.0_0    nvidia\r\n```\r\n\r\nI have found `cudnn=8.0.4` to be necessary with the 3070. With `cudnn=8.0` everything appears to load, but actually calling a 2D convolution will error. ", "@nathanin nvidia-smi return cuda version 11.2 it state that your driver can support cuda 11.2 you can check actual version  of cuda using nvcc --version and i installed all cuda cudnn using Nvidia guidelines on webpage while i will also try conda and let u know.", "Hi,\r\n\r\nTensorFlow 2.4 needs CUDA 11.**0**.  Can you confirm that you've tried running it with CUDA 11.0 and it didn't work?", "In above comment i mentioned i installed cuda 11.0 https://github.com/tensorflow/tensorflow/issues/46975#issuecomment-775356022\nMethod used for installation is also mentioned and error i got is also mentioned please let me know did i follow correct steps to install cuda 11.0.", "one thing to note is that with cuda 11.2 :\r\n- nvidia-smi IS able to detect the GPU\r\n- tensorflow is NOT able to detect the GPU\r\n- pytorch IS able to detect the GPU\r\n", "@miramar-labs same issue here :(", "@game-sys   try running this workaround:\r\n\r\n```ln -s /usr/local/cuda-11.2/targets/x86_64-linux/lib/libcusolver.so.11 $(python -c \"import tensorflow.python as x; print(x.path[0])\")/libcusolver.so.10```", "Looking at the TF [build docs](https://www.tensorflow.org/install/source) I noticed that TF 2.4.0 is tested for compatibility with CUDA 11.0 and cuDNN 8.0 .... so I downgraded to those versions and things went a lot smoother :)   \r\n11.2 != 11.0  ....\r\nWhat I learned: \r\n- stick to the versions of TF/CUDA/cuDNN mentioned in the docs ... go beyond at your own risk :) \r\n- ignore nvidia-smi when it comes to CUDA version .. do this instead:\r\n`nvcc --version | grep cuda`  ", "@miramar-labs which driver u are using is it 460 and how u install cuda because when i install cuda 11.0 it also come with driver and it install driver automatically and rtx 3070 dont support driver 450 can u please run nvidia smi, and nvcc -- version what it is returning ", "@game-sys  I have a [repo on github](https://github.com/miramar-labs/tensorflow-build) for building TF ... I added some pretty detailed notes as to exactly what versions of things I ended up using .. hope it helps :)\r\n\r\n```\r\naaron@blade:~/esxi-dev/k8s/sh$ nvcc --version | grep cuda\r\nBuild cuda_11.0_bu.TC445_37.28845127_0\r\n\r\n\r\naaron@blade:~/esxi-dev/k8s/sh$ nvidia-smi\r\nSat Feb 27 23:20:36 2021       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 460.39       Driver Version: 460.39       CUDA Version: 11.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 207...  On   | 00000000:01:00.0  On |                  N/A |\r\n| N/A   49C    P8    19W /  N/A |   1050MiB /  7982MiB |     20%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A      1654      G   /usr/lib/xorg/Xorg                148MiB |\r\n|    0   N/A  N/A      2183      G   /usr/lib/xorg/Xorg                673MiB |\r\n|    0   N/A  N/A      2360      G   /usr/bin/gnome-shell               63MiB |\r\n|    0   N/A  N/A   2423694      G   ...AAAAAAAA== --shared-files       22MiB |\r\n|    0   N/A  N/A   2423752      G   ...gAAAAAAAAA --shared-files      100MiB |\r\n|    0   N/A  N/A   2457096      G   gnome-control-center                2MiB |\r\n|    0   N/A  N/A   3124619      G   ...m-2019.1.4/jre64/bin/java        6MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n```\r\n\r\nalso, I set up the environment variables in ~/.bashrc like so:\r\n```\r\nexport PATH=/usr/local/cuda/bin:$PATH\r\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\r\n```\r\nTIP: check what the softlink /usr/local/cuda actually points to ... for me:\r\n```\r\naaron@blade:~/esxi-dev/k8s/sh$ ls -al /usr/local/cuda\r\nlrwxrwxrwx 1 root root 21 Feb 22 22:39 /usr/local/cuda -> /usr/local/cuda-11.0/\r\n```", "Ok i will try and let u know. @miramar-labs ", "I solve it by just creating a conda environment \r\n**using command**\r\nconda create -n tensor2 python=3.6 tensorflow-gpu cudatoolkit cudnn\r\nand then I ran **command**\r\n\r\npython\r\n>>> import tensorflow as tf\r\n>>>tf.test.is_gpu_available(\r\n    cuda_only=False, min_cuda_compute_capability=None\r\n)\r\n\r\nIt **returned** \r\n\r\nWARNING:tensorflow:From <stdin>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.config.list_physical_devices('GPU')` instead.\r\n2021-03-06 11:39:38.634479: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2021-03-06 11:39:38.658818: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2899885000 Hz\r\n2021-03-06 11:39:38.659188: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5621530cce70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2021-03-06 11:39:38.659204: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2021-03-06 11:39:38.659809: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2021-03-06 11:39:38.701441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-06 11:39:38.703134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\ncoreClock: 1.725GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-03-06 11:39:38.733974: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2021-03-06 11:39:38.958161: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2021-03-06 11:39:39.074762: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2021-03-06 11:39:39.128519: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2021-03-06 11:39:39.348554: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2021-03-06 11:39:39.399564: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2021-03-06 11:39:39.851696: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2021-03-06 11:39:39.851985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-06 11:39:39.853718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-06 11:39:39.855140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2021-03-06 11:39:39.855246: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2021-03-06 11:39:40.116405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-03-06 11:39:40.116440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \r\n2021-03-06 11:39:40.116448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \r\n2021-03-06 11:39:40.116630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-06 11:39:40.117258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-06 11:39:40.117840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-06 11:39:40.118403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/device:GPU:0 with 6987 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6)\r\n2021-03-06 11:39:40.140623: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5621541ecce0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2021-03-06 11:39:40.140679: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 3070, Compute Capability 8.6\r\nTrue\r\n\r\n\r\nbut i dont know what this mean \r\nsuccessful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-06 11:39:40.117840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n\r\nis it correctly installed ", "I ran a simple python file\r\n>import keras\r\nfrom keras.datasets import mnist\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout, Flatten\r\nfrom keras.layers import Conv2D, MaxPooling2D\r\nfrom keras import backend as K\r\n\r\n\r\nbatch_size = 128\r\nnum_classes = 10\r\nepochs = 12\r\n\r\n# input image dimensions\r\nimg_rows, img_cols = 28, 28\r\n\r\n# the data, split between train and test sets\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n\r\n\r\nif K.image_data_format() == 'channels_first':\r\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\r\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\r\n    input_shape = (1, img_rows, img_cols)\r\nelse:\r\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\n    input_shape = (img_rows, img_cols, 1)\r\n\r\nx_train = x_train.astype('float32')\r\nx_test = x_test.astype('float32')\r\nx_train /= 255\r\nx_test /= 255\r\nprint('x_train shape:', x_train.shape)\r\nprint(x_train.shape[0], 'train samples')\r\nprint(x_test.shape[0], 'test samples')\r\n\r\n\r\n\r\n# convert class vectors to binary class matrices\r\ny_train = keras.utils.to_categorical(y_train, num_classes)\r\ny_test = keras.utils.to_categorical(y_test, num_classes)\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv2D(32, kernel_size=(3, 3),\r\n                 activation='relu',\r\n                 input_shape=input_shape))\r\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Dropout(0.25))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(128, activation='relu'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(num_classes, activation='softmax'))\r\n\r\nmodel.compile(loss=keras.losses.categorical_crossentropy,\r\n              optimizer=keras.optimizers.Adadelta(),\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train,\r\n          batch_size=batch_size,\r\n          epochs=epochs,\r\n          verbose=1,\r\nvalidation_data=(x_test, y_test))\r\n\r\nbut it **hang** on \r\nx_train shape: (60000, 28, 28, 1)\r\n60000 train samples\r\n10000 test samples\r\n2021-03-06 11:47:01.777594: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2021-03-06 11:47:01.812831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-06 11:47:01.813431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\ncoreClock: 1.725GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-03-06 11:47:01.813620: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2021-03-06 11:47:01.815197: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2021-03-06 11:47:01.816635: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2021-03-06 11:47:01.816843: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2021-03-06 11:47:01.818293: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2021-03-06 11:47:01.819109: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2021-03-06 11:47:01.822096: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2021-03-06 11:47:01.822182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-06 11:47:01.822686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-06 11:47:01.823101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2021-03-06 11:47:01.823360: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2021-03-06 11:47:01.828401: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2899885000 Hz\r\n2021-03-06 11:47:01.828730: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d67c663200 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2021-03-06 11:47:01.828744: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2021-03-06 11:47:01.828871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-06 11:47:01.829358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\ncoreClock: 1.725GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-03-06 11:47:01.829393: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2021-03-06 11:47:01.829404: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2021-03-06 11:47:01.829413: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2021-03-06 11:47:01.829422: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2021-03-06 11:47:01.829431: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2021-03-06 11:47:01.829439: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2021-03-06 11:47:01.829448: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2021-03-06 11:47:01.829492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-06 11:47:01.829898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-06 11:47:01.830287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2021-03-06 11:47:01.830310: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2021-03-06 11:47:01.896638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-03-06 11:47:01.896662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \r\n2021-03-06 11:47:01.896666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \r\n2021-03-06 11:47:01.896778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-06 11:47:01.897152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-06 11:47:01.897498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-06 11:47:01.897840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7007 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6)\r\n2021-03-06 11:47:01.899080: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d67d8d3850 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2021-03-06 11:47:01.899090: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 3070, Compute Capability 8.6\r\n", "**After 5 min** \r\nEpoch 1/12\r\n2021-03-06 11:51:58.746746: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n**After 2 min**\r\n2021-03-06 11:53:25.153184: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n", "Hi I want to share my results with you:\r\n\r\nOperative System: Ubuntu 20.04\r\nCPU : AMD Ryzen 7 3800X\r\nTensorFlow version: 2.4\r\nGPU : RTX 3070\r\nPython version: 3.8 (default with Ubuntu)\r\nCuda version :  11.1.0   The installer name is : cuda_11.1.0_455.23.05_linux.run\r\nCudNN version: 8.0.5    The installer package is : libcudnn8_8.0.5.39-1+cuda11.1_amd64.deb\r\n\r\n**The first error a got was:**\r\n\r\nCannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\n\r\nI solved it creating the symbolic link with:\r\n\r\nsudo ln -s ./libcusolver.so.11 ./libcusolver.so.10\r\n\r\nThe current directory was: /usr/local/cuda-11.1/lib64\r\n\r\n**The second error was:**\r\n\r\nCould not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR \r\n\r\nI solved it adding the following two lines of code after the imports in my python code:\r\n\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n\r\n**Finally i want to share my speed results:**\r\n\r\nWith my MacBook Pro 2019 15\" Core i7 + AMD GPU 555X + Apple Custom TensorFlow 2.4 the training of a model takes 45minutes per epoch, using TensorFlow 2.2 and CPU it takes 60 minutes per epoch.\r\n\r\nWith my new Ryzen 7 3080X custom assembled PC, using CPU it takes 24 minutes per epoch.\r\n\r\nWith my new Ryzen 7 3080X custom assembled PC, using RTX 3070 GPU it only takes 5:20 minutes per epoch, this is a super improvement in my training.\r\n\r\n\r\nI hope it can help you, i spent 2 days trying to solve this errors.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "Hy I hope that  you all are doing good. I need to train my mrcnn model on gtx 3070. Model loads onto the gpu but stuck while starting training no error appears but it stuck. When I list tensorflow device it show GPU exists but training not starts.\r\n\r\n![Screenshot from 2021-04-18 12-53-28](https://user-images.githubusercontent.com/29427728/115138389-3bb77b00-a045-11eb-9534-90d55308cfef.png)\r\n\r\n\r\n\r\nVersions I am using:\r\n\r\n1. Tensorflow 2.4\r\n2. cudnn 8\r\n3. cuda 11.0\r\n4. nvidia-drivers 460\r\n\r\n\r\n![Screenshot from 2021-04-18 12-45-13](https://user-images.githubusercontent.com/29427728/115138252-681ec780-a044-11eb-9e7d-c567d1219df0.png)\r\n\r\n![Screenshot from 2021-04-18 12-45-37](https://user-images.githubusercontent.com/29427728/115138260-6ead3f00-a044-11eb-8e35-7fb1f976b51e.png)\r\n\r\n![Screenshot from 2021-04-18 12-47-57](https://user-images.githubusercontent.com/29427728/115138273-7ec51e80-a044-11eb-91df-b8f536cb8952.png)\r\n\r\n\r\nI will really be thankful to you for helping me out. Thank you\r\n\r\n", "Hi, sorry I can't help you, because I only work with local training and never use Jupiter and/or Notebooks.\r\n\r\n", "> Hi, sorry I can't help you, because I only work with local training and never use Jupiter and/or Notebooks.\r\n\r\nI am currently using this repo\r\nGithub Link: https://github.com/SohaibAnwaar/Mask-Rcnn-Tensorflow-2.0\r\n\r\nI tried by using py file But still stuck on the same point\r\n![Screenshot from 2021-04-18 14-55-46](https://user-images.githubusercontent.com/29427728/115141570-94dbda80-a056-11eb-81d5-47cab56f59ae.png)\r\n\r\n\r\n", "I struggled with this as well, having just purchased a 3070 TI. I had to symlink the libcusolver.so as well, but it didn't resolve all of my issues. I got everything working with these additional steps:\r\n- `for a in /sys/bus/pci/devices/*; do echo 0 | sudo tee -a $a/numa_node; done` (not sure if this was necessary, haven't booted the machine yet)\r\n- Upgrade Tensorflow 2.4 -> 2.5\r\n- Upgrade CUDA 11.1 -> 11.2 ([link](https://developer.nvidia.com/cuda-11.2.2-download-archive?target_os=Linux&target_arch=x86_64&target_distro=Ubuntu&target_version=1804&target_type=deblocal))\r\n- Install cudnn-11.2-linux-x64-v8.1.1.33\r\n\r\nI kept the 460.91.03 driver I used with the previous 1080 TI. Actually I intended to update it first but couldn't get it to work via a \".run\" installer or apt :D"]}, {"number": 46972, "title": "How to build tensorflow when making small changes?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.3.1\r\n- Python version: 3.6.9\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\nI know that I can build tensorflow using the following command:\r\n\r\n`bazel build --config=opt -c opt -j 8 //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nBut this command takes a lot of time and I need to make small changes in the code of tensorflow and check them immediately. How do you build tensorflow while contributing? There must be a faster way. \r\n  \r\n\r\n\r\n\r\n", "comments": ["Bazel keeps a cache of compilation results. Each incremental change will reuse most of the cache.\r\n\r\nYou can also compile more granular targets while developing and only at the end build a full pip package and run integration tests.\r\n\r\nYou don't need to always compile with optimizations on. That takes time.", "How could I find and use those granular targets? Do you have any suggestions? \r\n\r\nRegarding the optimizations, you suggest removing the flags `--config=opt -c opt`, right?", "The `BUILD` file specify all possible BUILD targets. `bazel query 'deps(....)'` can also list what Bazel needs to build before the target inside `deps` can be built.", "Thanks for your response!\r\n\r\nAfter building with the specified targets, in order to run TensorFlow with the new code, do I also have to run the following commands:\r\n\r\n`\r\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg`\r\n`pip install /tmp/tensorflow_pkg/tensorflow-2.3.0-cp38-cp38m-linux_x86_64.whl\r\n`\r\n\r\nlike when installing from source? \r\n\r\nOr is it that by just building, the changes should be applied when using TensorFlow?", "You have to fully build the pip package and install it if you want to manually test the code / test the pip package.\r\n\r\nBut during development you usually only want to run a small unit test.", "I am using the following commands (same to those I used when installing for the first time from source):\r\n\r\n`\r\nbazel build -j 16 //tensorflow/tools/pip_package:build_pip_package\r\n`\r\n`\r\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n`\r\n`\r\npip install /tmp/tensorflow_pkg/tensorflow-2.3.0-cp38-cp38m-linux_x86_64.whl\r\n`\r\n\r\nBut when using TensorFlow, I do not see any differences. Do I use the correct commands for building and installing? ", "Yes, these are the correct commands. Make sure you are using the right environment, the right python/pip combination.", "@dimitraka This is a well know issue. You can subscribe or comment about your experience at https://github.com/tensorflow/build/issues/5", "@dimitraka \r\nCan you try updating to the latest stable version 2.6.0 and let us know if the issue still persists? "]}, {"number": 46961, "title": "tf.data function mapping slower when using tf.GradientTape", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): conda binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: - \r\n- GPU model and memory: -\r\n\r\n**Describe the current behavior**\r\n\r\nMapping a function to a `tf.data.Dataset` is slower, than computing the function directly with the tensor.\r\n\r\nThe example below is exaggerated, but it represents the issue I have, with less gradients but a more complex function.\r\nFurther more, the tf.data mapping uses more memory, which is my main concern.\r\n\r\n**Describe the expected behavior**\r\n\r\nI would expect both function are compiled and should result in similiar performance and memory usage.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n\r\n```py\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef func(r):\r\n    with tf.GradientTape(persistent=True, watch_accessed_variables=False) as tape:\r\n        tape.watch(r)\r\n        out = []\r\n        for idx in range(2000):\r\n            out.append(r**idx)\r\n    grad = []\r\n    for layer in out:\r\n        grad.append(tape.gradient(layer, r))\r\n    return tf.stack(grad)\r\n\r\ndata = tf.random.normal((100000, 1000))\r\ndataset = tf.data.Dataset.from_tensor_slices(data)\r\ndataset = dataset.batch(2).map(func)\r\n\r\n# Compile\r\niterator = iter(dataset)\r\n_ = next(iterator)\r\n_ = func(data[:2])\r\n\r\n%timeit next(iterator)  # 57.5 ms \u00b1 265 \u00b5s per loop\r\n%timeit func(data[:2])  # 30 ms \u00b1 34.9 \u00b5s per loop\r\n```\r\n\r\nLink to [Google Colab](https://colab.research.google.com/drive/1-4TrBifOZDEkdeXWwPAkfp7Q6KIRYHfV?usp=sharing)\r\n\r\n", "comments": ["I am able to replicate the issue reported on tf 2.4 and nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/58b3ac239e4bcd9a31024774fcc98b60/untitled524.ipynb)"]}, {"number": 46952, "title": "Bug reported: Tensorflow GPU version(Cuda) may cause thread blocking when calling scripts across languages.  C#.", "body": "Actually I'm not sure whether it's a bug 'cause I've solved my problem, but i'll still trying to make a report.\r\n1. Basically, I wrote a py script to generate a TFLite model.\r\n2. Trying to run the test code, and worked good.\r\n```python\r\nimport threading\r\nfrom tensorflow import keras\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport sys\r\nimport time\r\nimport asyncio\r\n\r\ndef load_interpreter(model_dir:str):\r\n    interpreter = tf.lite.Interpreter(model_path=model_dir)\r\n    interpreter.allocate_tensors()\r\n    return interpreter\r\ndef classify_single_image(img_dir:str, interpreter:tf.lite.Interpreter):\r\n    start_time = time.time()\r\n    img = keras.preprocessing.image.load_img(img_dir, target_size=(160, 160))\r\n    img_array = keras.preprocessing.image.img_to_array(img)\r\n    img_array = np.expand_dims(img_array, 0)\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n    interpreter.set_tensor(input_details[0]['index'], img_array)\r\n    interpreter.invoke()\r\n    output_data = np.squeeze(interpreter.get_tensor(output_details[0]['index']))\r\n    probs = tf.nn.softmax(output_data)\r\n    probs = np.round(probs * 100, 2)\r\n    elapsed_time = time.time()\r\n    result = \"{}+{:.2f}\".format(probs, (elapsed_time - start_time) * 1000)\r\n    print(result, flush=True)\r\n\r\ninterpreter = load_interpreter(\"trash_model_lite.tflite\")\r\nthreading.Thread(classify_single_image(\"1.jpg\", interpreter)).start()\r\nthreading.Thread(classify_single_image(\"2.jpg\", interpreter)).start()\r\nthreading.Thread(classify_single_image(\"3.jpg\", interpreter)).start()\r\nthreading.Thread(classify_single_image(\"4.jpg\", interpreter)).start()\r\nthreading.Thread(classify_single_image(\"5.jpg\", interpreter)).start()\r\n# \u5f88\u663e\u7136\uff0c\u5982\u679c\u7ee7\u7eed\u8fd9\u6837\u505a, interpreter \u4e0d\u662f\u7ebf\u7a0b\u5b89\u5168\u7684. \u9700\u8981\u52a0\u9501. \r\n```\r\n3. i'm tring to call it in my C# project just by openning python and let it to run the script\r\n4. Problem is that C# cannot get any result, and i'll give the code in the end 'cause this is a tf bug report.\r\n5. i'm trying to see where's the problem, just by print(\"n\", flush=True) after every slice of code.\r\n6. The result shows that the tensorflow module will \"BLOCK\"(maybe not, but will stop here) the program such as:\r\n```python\r\n    print(\"0\", flush=True) # will see 0 in the python console and C# console\r\n    img_array = tf.expand_dims(img_array, 0)\r\n    print(\"1\", flush=True) # will see 1 printed in the python console but not in C# console\r\n```\r\n7. trying to avoid tensorflow will exam the hypothesis:\r\n```python\r\n    print(\"0\", flush=True) # 0 in python and C#\r\n    img_array = np.expand_dims(img_array, 0)\r\n    print(\"1\", flush=True) # 1 in python and C#\r\n......\r\n    probs = tf.nn.softmax(output_data) # still wrong\r\n```\r\n8. I suddenly understand that the tensorflow module might be made a mistake, so i'm tring to install cpu version and get the correct answer by only work with cpu.\r\n```python\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\r\nimport tensorflow as tf\r\n......\r\n```\r\n9. After delete the print code, i've got the almost same answer in both C# and python, and which is correct after deleting the empty line.\r\n```python\r\n[  0.   0.   0. 100.]+960.62\r\n[9.997e+01 0.000e+00 0.000e+00 2.000e-02]+31.92\r\n[  0. 100.   0.   0.]+35.90\r\n[ 0.35  0.2  41.75 57.7 ]+32.91\r\n[  0.   0. 100.   0.]+33.91\r\n```\r\n```C#\r\n[  0.   0.   0. 100.]+382.50\r\n[9.997e+01 0.000e+00 0.000e+00 2.000e-02]+31.99\r\n[  0. 100.   0.   0.]+31.91\r\n[ 0.35  0.2  41.75 57.7 ]+31.96\r\n[  0.   0. 100.   0.]+32.90\r\n```\r\n10. If i delete the code which forbide the cuda, the C# program will still wrong. So the right thing is to just work on cpu, and i think this maybe a bug( or config problem maybe)\r\n\r\n**THESE ARE MY SYSTEM CONFIG:**\r\nOS: Windows 10 from Microsoft\r\nPython: 3.8, download in python.org\r\nTensorflow: GPU==2.4.1, CPU==2.4.1( installed after the bug became) with tflite, download with pip\r\nKeras: download with TF\r\nIDE or Text Editor: VSCode, Visual Studio\r\nCuda: 11.0\r\nCuddn: the right version with cuda 11.0\r\nGPU: Laptop gpu -- MX250\r\nBazel: No bazel.\r\n\r\nConfig with C#:\r\n.Net: .Net 5 or .Net Core 3.1\r\nProject: dotnet new Console\r\nNuget packages: None\r\n\r\nC# code :\r\n```C#\r\nusing System;\r\nusing System.Diagnostics;\r\n\r\nnamespace ConsoleApp1\r\n{\r\n    class Program\r\n    {\r\n        static void Main(string[] args)\r\n        {\r\n            string[] args_array = new string[2];\r\n            string pyDir = \"test.py\";\r\n            args_array[0] = \"\";\r\n            args_array[1] = \"\";\r\n            RunPythonScript(pyDir,\"\", args_array);\r\n        }\r\n        public static void RunPythonScript(string sArgName, string args = \"\", params string[] teps)\r\n        {\r\n            Process p = new Process();\r\n            p.StartInfo.FileName = @\"C:/Program Files/Python38/python.exe\";\r\n            string sArguments = sArgName;\r\n            foreach (string sigstr in teps)\r\n            {\r\n                sArguments += \" \" + sigstr;\r\n            }\r\n            sArguments += \" \" + args;\r\n            p.StartInfo.Arguments = sArguments;\r\n            p.StartInfo.UseShellExecute = false;\r\n            p.StartInfo.RedirectStandardOutput = true;\r\n            p.StartInfo.RedirectStandardInput = true;\r\n            p.StartInfo.RedirectStandardError = true;\r\n            p.StartInfo.CreateNoWindow = true;\r\n            p.OutputDataReceived += new DataReceivedEventHandler(p_OutputDataReceived);\r\n            p.Start();\r\n            p.BeginOutputReadLine();\r\n            p.WaitForExit();\r\n            Console.ReadLine();\r\n        }\r\n        static void p_OutputDataReceived(object sender, DataReceivedEventArgs e)\r\n        {\r\n            if (!string.IsNullOrEmpty(e.Data))\r\n            {\r\n                AppendText(e.Data + Environment.NewLine);\r\n            }\r\n        }\r\n        public delegate void AppendTextCallback(string text);\r\n        public static void AppendText(string text)\r\n        {\r\n            Console.WriteLine(text);\r\n        }\r\n    }\r\n}\r\n\r\n```\r\nAnd i hope you could solve your problem like this. That's why i made this report. Have a good time! :) ", "comments": []}, {"number": 46950, "title": "TensorFlow with DLPack invokes unexpected data transfers", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu-18.04\r\n- TensorFlow installed from (source or binary): binary, TensorFlow official docker image \r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.6.9\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version: CUDA 11.0\r\n- GPU model and memory: Tesla V100\r\n\r\n**Describe the current behavior**\r\nTensorFlow with DLPack invokes unexpected data transfers. For instance, before prediction, TensorFlow transfers the data, allocated by CuPy and wrapped with DLPack to CPU, then transfers data back to GPU, and then starts prediction. This behavior makes DLPack meaningless.\r\n\r\n**Describe the expected behavior**\r\nTensorFlow shouldn't transfer the data, allocated by CuPy and wrapped with DLPack to CPU. Instead, it should take the data on GPU directly.\r\n\r\n**Standalone code to reproduce the issue**\r\n1. sample code (foo.py):\r\n```python\r\nimport cupy as cp\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Input, Lambda\r\nfrom tensorflow.keras.models import Model\r\n\r\nn = 10000000\r\n\r\na = Input(shape=(n))\r\noutput = Lambda(lambda x: x ** 2)(a)\r\nmodel = Model(a, output)\r\n\r\nx = cp.arange(n, dtype=cp.float32).reshape(1, n)\r\ndltensor = x.toDlpack()\r\nx2 = tf.experimental.dlpack.from_dlpack(dltensor)\r\n\r\ncp.cuda.nvtx.RangePush('model.predict')\r\ny = model.predict(x2)\r\ncp.cuda.nvtx.RangePop()\r\nprint('done')\r\n```\r\n\r\n2. pull TensorFlow image\r\n```bash\r\n$ docker pull tensorflow/tensorflow:latest-gpu\r\n```\r\n\r\n3. install CuPy\r\n```bash\r\n# after login the container\r\n$ pip install cupy-cuda110\r\n```\r\n\r\n4. run:\r\n```\r\n$ nvprof -o output.nvprof python ./foo.py\r\n# or use Nsight-Systems to record behavior\r\n$ nsys profile -t cuda,nvtx python ./foo.py\r\n```\r\n**Other info / logs**\r\n![tf2_with_dlpack](https://user-images.githubusercontent.com/5894445/107031704-96368e80-67ed-11eb-9092-11e669b3853b.png)\r\n\r\n", "comments": ["++ @VoVAllen @jermainewang for awareness.\r\nAny thoughts about what could be going on here?\r\n", "It's weird, did you see D2H copy without the forward process? \r\nAlso by setting ` tf.config.experimental.set_device_policy(\"explicit\")`, got error \r\n```\r\nInvalidArgumentError: Tensors on conflicting devices: cannot compute LogicalNot as input #0 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0) Tensors can be copied explicitly using: `with tf.device(device_name): x = tf.identity(x)` or transparently copied by using tf.config.experimental.set_device_policy('silent'). Copying tensors between devices may slow down your model [Op:LogicalNot]\r\n```\r\nSeems the there's problem on the real device. Also why would it be D2H instead of cudamemcpy?", "@VoVAllen ,\r\n\r\n> It's weird, did you see D2H copy without the forward process?\r\n\r\nI saw it did D2H copy, H2D copy, forward process and then D2H copy. Obviously, \"D2H copy, H2D copy\" was redundant.\r\n\r\n> Also by setting tf.config.experimental.set_device_policy(\"explicit\"), got error...\r\n> Seems the there's problem on the real device. Also why would it be D2H instead of cudamemcpy?\r\n\r\nI saw the error message after setting \"explicit\" too. \r\nMy machine is fine. I am not familiar with TensorFlow implementation.\r\nI suppose that it is a bug from `tf.experimental.dlpack`, because `x2` should be set as `device:GPU:0` instead of `device:CPU:0`, isn't?\r\n", "I found the error with `tf.config.experimental.set_device_policy(\"explicit\")` is not the error we are looking for. It's some Keras internal issues.\r\n\r\nActually I cannot reproduce the error with \r\n```python\r\nimport cupy as cp\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Input, Lambda\r\nfrom tensorflow.keras.models import Model\r\nn = 10000000\r\n\r\nu = tf.ones(1)\r\n\r\ntf.config.experimental.set_device_policy(\"explicit\")\r\nx = cp.arange(n, dtype=cp.float32).reshape(1, n)\r\ndltensor = x.toDlpack()\r\nx2 = tf.experimental.dlpack.from_dlpack(dltensor)\r\n\r\nx3 = x2 + u\r\nprint('done')\r\n```\r\nIt seems there's no copy issue for x3 and x2. Therefore I guess this might related to Keras instead of problem with dlpack", "```python\r\nimport cupy as cp\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Input, Lambda\r\nfrom tensorflow.keras.models import Model\r\n\r\nn = 10000000\r\n\r\na = Input(shape=(n))\r\noutput = Lambda(lambda x: x ** 2)(a)\r\nmodel = Model(a, output)\r\nx = cp.arange(n, dtype=cp.float32).reshape(1, n)\r\ndltensor = x.toDlpack()\r\nx2 = tf.experimental.dlpack.from_dlpack(dltensor)\r\n\r\ntf.config.experimental.set_device_policy(\"warn\")\r\ncp.cuda.nvtx.RangePush('model.predict')\r\ny = model.predict(x2)\r\ncp.cuda.nvtx.RangePop()\r\nprint('done')\r\n```\r\n\r\nYou can get \r\n```\r\n2021-02-08 16:20:19.669326: W tensorflow/core/common_runtime/eager/execute.cc:140] before computing TensorDataset input #0 was expected to be on /job:localhost/replica:0/task:0/device:CPU:0 but is actually on /job:localhost/replica:0/task:0/device:GPU:0 (operation running on /job:localhost/replica:0/task:0/device:CPU:0). This triggers a copy which can be a performance bottleneck.\r\n2021-02-08 16:20:19.704433: W tensorflow/core/common_runtime/eager/execute.cc:140] before computing AssignVariableOp input #1 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0). This triggers a copy which can be a performance bottleneck.\r\n```\r\n\r\nThe copy is probably due to the `TensorDataset` op.", "If I use `x2 = tf.reshape(tf.range(n, dtype=tf.float32), (1, n))` as input, the copy still happens. It seems not dlpack problems. Could you check this with nvprof?", "Also the copy behavior won't be considered as bug, because all tensors inside tensorflow are immutable. Therefore the copy will only happen once for each tensor.", "> Could you check this with nvprof?\r\n\r\nYes, even if `x2` is created by `tf.range`, TensorFlow Keras still does the redundant data transfers (D2H, H2D):\r\n\r\n![image](https://user-images.githubusercontent.com/5894445/107303798-b9e22900-6aba-11eb-890a-fa1f9634a65e.png)\r\n", "@VoVAllen ,\r\n\r\n>  the copy behavior won't be considered as bug, because all tensors inside tensorflow are immutable. \r\n\r\nDo you mean that due to the design of TensorFlow and the `x2` is not inside TensorFlow, so it must be copied to the tensor which is inside TensorFlow?\r\n\r\nIf so, the copy of `x2` should be a D2D but it is \"D2H, H2D\". To address it, we need Keras people's help or authors  of TensorDataset's help.\r\n\r\n@EvenOldridge ,\r\nCould you help to loop the right persons?", "@jeng1220 No. The copy is merely due to `TensorDataset` op inside Keras. It's a cpu op and requires all tensors to be on cpu, thus D2H happens. And the latter operation needs gpu tensor, thus H2D happens. However this should only happen at the first iteration, since tensor already had both cpu and gpu memory.\r\n\r\nThis has nothing related with dlpack."]}, {"number": 46944, "title": "Unable to create functional model as slice from internal layer output if it is a Sequential model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Colab default\r\n- TensorFlow version (use command below): v2.4.1-0-g85c8b2a817f 2.4.1\r\n- Python version: 3 (Colab default)\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\n**Describe the current behavior**\r\nI'm trying to create feature extractor for semantic segmentation model like here https://www.tensorflow.org/tutorials/images/segmentation\r\nBut my pretrained backbone built with combination of custom layers and sequential models (used as layer). Particularly it is BiT-S model from here https://github.com/google-research/big_transfer/blob/master/bit_tf2/models.py\r\n\r\nDuring the feature extraction i got a `Graph disconnected` error every time i try to use Sequetial-layer output in new model.\r\n\r\n**Describe the expected behavior**\r\nThere should be no error\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1hg4RJtux1md5zJErzG7RDCQg0ye5cmFi?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```python\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-5-6cce9314b5b2> in <module>()\r\n----> 1 ext3_model = tf.keras.Model(inputs=funct_model.inputs, outputs=funct_model.get_layer(name='block2').output)\r\n\r\n4 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    515     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    516     try:\r\n--> 517       result = method(self, *args, **kwargs)\r\n    518     finally:\r\n    519       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py in __init__(self, inputs, outputs, name, trainable, **kwargs)\r\n    118     generic_utils.validate_kwargs(kwargs, {})\r\n    119     super(Functional, self).__init__(name=name, trainable=trainable)\r\n--> 120     self._init_graph_network(inputs, outputs)\r\n    121 \r\n    122   @trackable.no_automatic_dependency_tracking\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    515     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    516     try:\r\n--> 517       result = method(self, *args, **kwargs)\r\n    518     finally:\r\n    519       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py in _init_graph_network(self, inputs, outputs)\r\n    202     # Keep track of the network's nodes and layers.\r\n    203     nodes, nodes_by_depth, layers, _ = _map_graph_network(\r\n--> 204         self.inputs, self.outputs)\r\n    205     self._network_nodes = nodes\r\n    206     self._nodes_by_depth = nodes_by_depth\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py in _map_graph_network(inputs, outputs)\r\n    988                              'The following previous layers '\r\n    989                              'were accessed without issue: ' +\r\n--> 990                              str(layers_with_complete_input))\r\n    991         for x in nest.flatten(node.outputs):\r\n    992           computable_tensors.add(id(x))\r\n\r\nValueError: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, None, None, 10), dtype=tf.float32, name='conv2d_2_input'), name='conv2d_2_input', description=\"created by layer 'conv2d_2_input'\") at layer \"conv2d_2\". The following previous layers were accessed without issue: []\r\n```", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly(`2.5.0-dev20210204`). Please find the gist of it [here](https://colab.research.google.com/gist/ravikyram/26403b8209d09119635a9f6bbe012864/untitled661.ipynb). Thanks!", "Just a side note.\r\n`ext3_model = tf.keras.Model(inputs=funct_model.inputs, outputs=funct_model.get_layer(name='proj').input)`\r\n\r\nThis works as expected ([gist](https://colab.research.google.com/gist/AdityaKane2001/19c405c886b83133585f35003be3c4ef/untitled661.ipynb#scrollTo=E7iFFMIAZKzN)).\r\n\r\nOutput is:\r\n\r\n```\r\nModel: \"model_5\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nimage (InputLayer)           [(None, None, None, 3)]   0         \r\n_________________________________________________________________\r\nblock0 (Sequential)          (None, None, None, 10)    320       \r\n_________________________________________________________________\r\nblock1 (Sequential)          (None, None, None, 10)    950       \r\n_________________________________________________________________\r\nblock2 (Sequential)          (None, None, None, 10)    950       \r\n=================================================================\r\nTotal params: 2,220\r\nTrainable params: 2,160\r\nNon-trainable params: 60\r\n_________________________________________________________________\r\n```", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "The issue is still here, \"stalled\" label please ", "@shkarupa-alex Seems the code is working as expected  with the changes mentioned above by @AdityaKane2001. Please let us know if it is stil an issue?.Thanks!", "@saikumarchalla , changes mentioned above are not a solution but a hack. That hack can't be used always.\r\nE.g. sequential models can be enclosed in another sequential many times. \r\n\r\nSo, i think this issue is still actual.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 46938, "title": "`tf.random.learned_unigram_candidate_sampler` crashes(abort) when `range_max` is large", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n**Describe the current behavior**\r\n`tf.random.learned_unigram_candidate_sampler` crashes(abort) when `range_max` is large\r\n\r\n**Describe the expected behavior**\r\nexpect an exception message if the file format is incorrect instead of crash\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.random.learned_unigram_candidate_sampler(true_classes=np.ones((1,1)), num_true=1, num_sampled=1, unique=True, range_max=3118649543)\r\n~~~\r\n\r\nOutput:\r\n~~~python\r\n2021-02-04 23:02:12.013481: F tensorflow/core/lib/random/weighted_picker.cc:27] Check failed: N >= 0 (-1176317753 vs. 0)\r\nAborted (core dumped)\r\n~~~", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/b8e2017e50e2d6bb9ff385b256dec117/46938.ipynb). Thanks!", "Colab crashes in TF 2.6 Nightly as well. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/c0ee174eaa10bbcd578f3f6652fe9a57/untitled92.ipynb).Thanks!"]}, {"number": 46929, "title": "Possible bug: tf2 raise `OverflowError: Python int too large to convert to C long` for `feature_column.crossed_column`", "body": "**System information**\r\nwindow 10 python 3.7.9 tf 2.3.x cpu/gpu, for tf-gpu, cuda 10.1 \r\n\r\n**Describe the current behavior**\r\nThis is my code\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\n# import pandas as pd\r\n\r\ndata = {\r\n    \"feat\": [0,1,2],\r\n    \"gender\": [\"f\", \"f\", \"m\"]\r\n}\r\n\r\n\r\nfeat_c = tf.feature_column.categorical_column_with_identity(\r\n    \"feat\", 3\r\n)\r\n\r\ngender_c = tf.feature_column.categorical_column_with_vocabulary_list(\r\n    \"gender\", ['f', 'm']\r\n)\r\n\r\nfeat = tf.feature_column.indicator_column(feat_c)\r\ngender = tf.feature_column.indicator_column(gender_c)\r\n\r\nfeat_gender_cross_c = tf.feature_column.crossed_column(\r\n    [feat_c, gender_c], 5\r\n)\r\n\r\nfeat_gender_cross = tf.feature_column.indicator_column(feat_gender_cross_c)\r\n\r\nfeature_columns = [feat_gender_cross]\r\n\r\ndense = layers.DenseFeatures(feature_columns)\r\nprint(dense(data))\r\n```  \r\n\r\nError msg from `dense(data)`:    \r\n```\r\nTraceback (most recent call last):\r\n  File \"D:\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\feature_column\\dense_features.py\", line 167, in call\r\n    transformation_cache, self._state_manager, training=training)\r\nTypeError: get_dense_tensor() got an unexpected keyword argument 'training'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\", line 2353, in get\r\n    self, state_manager, training=training)\r\nTypeError: transform_feature() got an unexpected keyword argument 'training'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\", line 2353, in get\r\n    self, state_manager, training=training)\r\nTypeError: transform_feature() got an unexpected keyword argument 'training'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"e:/test.py\", line 32, in <module>\r\n    print(dense(data))\r\n  File \"D:\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 985, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"D:\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\feature_column\\dense_features.py\", line 170, in call\r\n    self._state_manager)\r\n  File \"D:\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\", line 4158, in get_dense_tensor\r\n    return transformation_cache.get(self, state_manager)\r\n  File \"D:\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\", line 2355, in get\r\n    transformed = column.transform_feature(self, state_manager)\r\n  File \"D:\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\", line 4095, in transform_feature\r\n    transformation_cache, state_manager)\r\n  File \"D:\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\", line 3952, in get_sparse_tensors\r\n    transformation_cache.get(self, state_manager), None)\r\n  File \"D:\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\", line 2355, in get\r\n    transformed = column.transform_feature(self, state_manager)\r\n  File \"D:\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\", line 3913, in transform_feature\r\n    hash_key=self.hash_key)\r\n  File \"D:\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\ops\\sparse_ops.py\", line 677, in sparse_cross_hashed\r\n    name=name)\r\n  File \"D:\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\ops\\sparse_ops.py\", line 756, in _sparse_cross_internal\r\n    name=name)\r\n  File \"D:\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\ops\\gen_sparse_ops.py\", line 1068, in sparse_cross\r\n    internal_type=internal_type, name=name, ctx=_ctx)\r\n  File \"D:\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\ops\\gen_sparse_ops.py\", line 1146, in sparse_cross_eager_fallback\r\n    attrs=_attrs, ctx=ctx, name=name)\r\n  File \"D:\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\nOverflowError: Python int too large to convert to C long\r\n```   \r\n\r\n\r\nI test above code  with tf2.3.x -cpu/gpu tf2.4-cpu on windows (and window server), they all give me same error. But on linux, it runs well\r\n", "comments": ["@BruceChen2017,\r\nAs you've mentioned, I was able to run the code without any issues on Linux, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/944fc8b884efbb20f2d49bccae08743e/46929.ipynb). \r\n\r\nCould you please try running the code in a new virtual environment and let us know if you are facing the same error.\r\n\r\nAlso, please take a look at these comments [link #1](https://stackoverflow.com/a/49329034), [link #2](https://stackoverflow.com/a/52796383)  from similar StackOverflow queries and check if it helps. \r\n\r\nThanks!", "@amahendrakar   \r\nAs  I point out in last line `I test above code with tf2.3.x -cpu/gpu tf2.4-cpu on windows (and window server), they all give me same error. But on linux, it runs well`, I have tested it with tf2.3.x with cpu version and gpu version and tf2.4 with cpu on different virtual environment (created by `conda create`), even tf2.3.1 with cpu on another machine, whose system is windows server . I check two links above, changing to `\"feat\": np.array([0,1,2], dtype=np.int64)`  is also no help.   \r\n\r\nFor Linux, I have test it with tf2.3.x with cpu on WSL 2 ubuntu 18 and ubuntu16(the latter is on another machine), it all works."]}, {"number": 46923, "title": "Training is different depending on loss function being decorate with tf.function or not", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nI am training with 4 different versions of a class weighted loss function and am getting very different results. These 4 differ only in from_logits and tf.function decorator. I can compare with the result from running model.fit with the standard categorical cross entropy loss function and explicit weights and see which ones are correct and get\r\n\r\n1. from_logits = True, no tf.function decorator ---> wrong answer\r\n2. from_logits = True, tf.function decorator ---> wrong answer\r\n3. from_logits = False, no tf.function decorator ---> wrong answer\r\n4. from_logits = False, tf.function decorator ---> correct answer\r\n\r\nIncidentally, all the three incorrect answers are equal to each other. This is not related to the numerical stability issue related to use of from_logits as we can see the correct answer actually comes from the one that does not use logits. Furthermore, the standard categorical cross entropy loss function with class weights give the same answer irrespective of the from_logits flag.\r\n\r\n**Describe the expected behavior**\r\n\r\nAll four loss functions should result in the same training.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nhttps://colab.research.google.com/drive/1LrB6krDIowtzDfmstU-oGi6iHOoDYRYm?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/78e246e675a878b75a9b83f07f06b5e9/46923.ipynb). Thanks!", "Was able to replicate the issue in TF 2.6.0-dev20210603,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/8609ad090b93b8a8b33021247cafc6d5/untitled158.ipynb?authuser=1#scrollTo=7mT26aaVjXsf)..Thanks !"]}, {"number": 46915, "title": "tf.nn.atrous_conv2d crashes(aborts) when rate is large value", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n**Describe the current behavior**\r\n`tf.nn.atrous_conv2d` crashes(aborts) when `rate` is large value\r\n\r\n**Describe the expected behavior**\r\nexpect an exception message if the input unexpected instead of crash. \r\n\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.nn.atrous_conv2d(value=np.ones((1,1,1,5)), filters=np.ones((1,1,5,1)), rate=2147483647, padding='SAME')\r\n~~~\r\n\r\n~~~python\r\n2021-02-04 04:47:25.891213: F tensorflow/core/framework/tensor_shape.cc:353] Check failed: 0 <= new_num_elements (0 vs. -1)\r\nAborted (core dumped)\r\n~~~", "comments": ["I have tried in colab with TF versions 2.1, 2.4, nightly versions (`2.5.0-dev20210203`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/1fda44be7360e4382bedf8f9c6c8dfb9/untitled658.ipynb). Thanks!", "BTW, `tf.nn.atrous_conv2d_transpose` has similar crash which is due to large `rate`.\r\n\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.nn.atrous_conv2d_transpose(value=np.ones((10,1,1,1)), filters=np.ones((1,1,1,1)), rate=1356819205, padding='SAME', output_shape=[1,1,1,1])\r\n~~~\r\n\r\nError Message:\r\n~~~python\r\n2021-04-15 00:08:19.741409: F tensorflow/core/framework/tensor_shape.cc:397] Check failed: size >= 0 (-37160523141231366 vs. 0)\r\nAborted (core dumped)\r\n~~~", "Colab crashes in TF Nightly 2.6 as well.Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/b44477be4d8ec2fdc1028f4684ec4cdf/untitled92.ipynb).Thanks!"]}, {"number": 46910, "title": "tf.quantization.fake_quant_with_min_max_vars(_gradient) crash(abort)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n\r\n**Describe the current behavior**\r\n`tf.quantization.fake_quant_with_min_max_vars` and `tf.quantization.fake_quant_with_min_max_vars_gradient` crash(abort)\r\n\r\n\r\n**Describe the expected behavior**\r\nexpect an exception message if the input unexpected instead of crash. \r\n\r\n**Standalone code to reproduce the issue**\r\n~~~python\r\ntf.quantization.fake_quant_with_min_max_vars_gradient(gradients=1, inputs=1, max=[1,1], min=[1,1])\r\ntf.quantization.fake_quant_with_min_max_vars_gradient(gradients=[], inputs=[], max=[1,1], min=[1,1])\r\ntf.quantization.fake_quant_with_min_max_vars(inputs=np.ones((1,1)), max=[1,1], min=[1,1])\r\n~~~\r\n\r\n\r\nOutput\r\n~~~python\r\n2021-02-04 04:05:44.055418: F tensorflow/core/framework/tensor.cc:669] Check failed: 1 == NumElements() (1 vs. 2)Must have a one element tensor\r\nAborted (core dumped)\r\n~~~", "comments": ["@ymodak \r\nI ran the code shared on tf 2.4 and nightly, colab crashes. please find the [gist here](https://colab.research.google.com/gist/Saduf2019/b6ce7dad554d4bc34a08de023d23ffcd/untitled520.ipynb)", "I just found `tf.quantization.fake_quant_with_min_max_vars_per_channel` and `tf.quantization.fake_quant_with_min_max_vars_per_channel_gradient` also aborts\r\n~~~python\r\ntf.quantization.fake_quant_with_min_max_vars_per_channel(inputs=[], max=[], min=np.ones((0,1)))\r\ntf.quantization.fake_quant_with_min_max_vars_per_channel_gradient(inputs=1, gradients=1, max=[], min=-1)\r\n~~~", "I ran the code shared on tf-nightly, & colab crashes. please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/bf4216da6dc1467fbf5227bb63ac5878/untitled520.ipynb#scrollTo=gBvtr8o68x53).Thank you!", "I ran the code shared on tf 2.7 and nightly(2.8.0-dev20211222), colab crashes. please find the gist [here](https://colab.research.google.com/gist/tilakrayal/6cfc9f05bf1302fb9993301693e79e90/untitled150.ipynb)"]}, {"number": 46907, "title": "Tokenizer doesn't work with string tensors (AttributeError)", "body": "**System information**\r\n- OS Platform: Windows 10\r\n- TensorFlow version: 2.3.2\r\n- Python version: 3.8.5\r\n- CUDA version: 10.1.243\r\n\r\n**Issue**\r\nTokenizer doesn't work with tensors (at least string tensors).\r\n\r\n**Standalone code to reproduce**\r\n\r\nFails with a string tensor:\r\n\r\n````\r\na = ['hello world', 'what is your name', 'start with a scene']\r\n\r\nt = tf.constant(a, dtype=tf.string)\r\ntokenizer = tf.keras.preprocessing.text.Tokenizer()\r\ntokenizer.fit_on_texts(t)\r\n````\r\nThrows this error:\r\n\r\n````\r\nAttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'lower'\r\n````\r\n\r\nWorks fine with a regular python list:\r\n\r\n````\r\na = ['hello world', 'what is your name', 'start with a scene']\r\n\r\ntokenizer = tf.keras.preprocessing.text.Tokenizer()\r\ntokenizer.fit_on_texts(a)\r\n````\r\n", "comments": ["@princyok \r\nSorry for the delayed response, can you please refer to this [link](https://keras.io/) which has it explained in detail.\r\nAlso similar resolved issues: [link](https://github.com/tensorflow/tensorflow/issues/26922),[link1](https://github.com/tensorflow/tensorflow/issues/29972)", "@Saduf2019 \r\nThanks. I've gone through the links, although I'm not sure if the first link to keras.io was a mistake. \r\n\r\nNone of those links seem to address the key issue here: Why the `Tokenizer` class has no support for tensors or even numpy arrays. It's strictly list of strings (even the documentation clearly spells this out), but why should this be the case? The `TextVectorization` layer (also keras API) supports tensors, so does the various classes in `text_tensorflow`.\r\n\r\nBut I guess it is simply what it is.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46907\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46907\">No</a>\n", "i am able to replicate the issue reported on f 2.3 and 2.4, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/5304494b257b2262257522966d779a7d/untitled524.ipynb)", "@princyok Input Argument text sequence can be `can be a list of strings, a generator of strings (for memory-efficiency), or a list of list of strings.`  So, after changing one line in your code (`# t = tf.constant(a, dtype=tf.string)`), everything worked as expected.\r\n\r\n [Here](https://colab.research.google.com/gist/jvishnuvardhan/8e6459701bedb03e56a02a6f919453ba/untitled524.ipynb) is a gist for reference. Thanks!\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!\r\n", "@jvishnuvardhan , thats what @princyok tried to point out in the issue. The code works well with strings as you mentioned but throws in the error when using a tensor. I think this needs to be fixed. Please revert back if you would like a fix for this issue.", "@jvishnuvardhan I already addressed your in my other comment above. What you are pointing out is the problem itself.", "Was able to replicate the issue in TF 2.6.0-dev20210528,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/4d65eacbe55981f7ceaa1df965c35fe0/untitled36.ipynb)..Thanks !"]}, {"number": 46898, "title": "`tf.random.fixed_unigram_candidate_sampler` crashes(abort) when `vocab_file` is invalid", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n**Describe the current behavior**\r\n`tf.random.fixed_unigram_candidate_sampler` crashes(abort) when `vocab_file` is invalid\r\n\r\n**Describe the expected behavior**\r\nexpect an exception message if the file format is incorrect instead of crash\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n~~~python\r\ntf.random.fixed_unigram_candidate_sampler(true_classes=np.ones((1,1)), num_true=1, num_sampled=1, unique=True, range_max=1, vocab_file='abc')\r\n~~~\r\nOutput:\r\n~~~python\r\n2021-02-03 22:08:02.817419: F tensorflow/core/kernels/range_sampler.cc:246] Non-OK-status: LoadFromFile(env, vocab_file, distortion) status: Not found: abc; No such file or directory\r\nAborted (core dumped)\r\n~~~\r\n\r\n", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/b506631d184ce832dc87a808c71841d5/46898.ipynb). Thanks!", "Was able to reproduce the issue in TF 2.6.0-dev20210528 ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/6607926a89e0d69ab2a5223df6dd18cf/untitled39.ipynb#scrollTo=8D_c_j0gqQ0U)...Thanks !"]}, {"number": 46857, "title": "LU solve output shape incorrect for wildcard batched inputs", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab version\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): NA\r\n- TensorFlow version (use command below): colab version\r\n- Python version: colab version\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: colab version\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\ntf.linalg.lu_solve does not trace shapes properly:\r\nif M is a tensor of shape (None, a, a) and RHS a tensor of shape (None, a, b), computing M^{-1}RHS returns a tensor of shape (None, a, None).\r\nThis is in contradiction with using tf.linalg.solve directly which is better behaved.\r\n\r\n**Describe the expected behavior**\r\nThe shape tracing should happen properly.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1qkZfPVaLIjKyLwg-N-n4DBkvwZUrBCkW?usp=sharing\r\n\r\n", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/88c9436b2446ea66c34436fa40e0a4a0/46857.ipynb). Thanks!", "Was able to reproduce the issue in TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/441ca7be27bcef72f84bb789e6c4f971/untitled45.ipynb)..Thanks !"]}, {"number": 46853, "title": "Cannot use load_model when using a layer with dynamic = True and saving to hdf5 format", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: NVIDIA Quadro RXT 6000, 24 GB NVRAM\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nWhen building a model (using the Function API), I need to use a layer in dynamic mode as far as I can tell, since the task is going to involve operations along an axis of variable size.\r\nWhile the training works perfectly fine, and the saving of the model also does as far as I can tell (no error, at least), when loading the model back up using load_model fails with the error:\r\nValueError: You are trying to load a weight file containing [n] layers into a model with [n-1] layers.\r\n\r\nUpon inspection of the load_weights_from_hdf5_group function, it appears that layers build with dynamic=True return an empty list with trying to get the weights, which results in such layers not being included in the following operations, ultimately resulting in the function failing. However, the layer itself is found and otherwise normal (e.g. its name gets found properly by the same function).\r\n\r\nThis affects both new models and ones saved before the update.\r\n\r\n**Describe the expected behavior**\r\nThe model should load properly, as it did in previous versions (I have tested up to 2.3.1). \r\n\r\n**Standalone code to reproduce the issue**\r\nI write two versions of the same little script, the only difference is the `dynamic` flag in the first Conv2D layer.\r\nI originally discovered this when using a custom layer (where the need for the dynamic=True flag came in), but this also happens when using the Conv2D layer.\r\n```\r\nfrom tensorflow.keras.models import load_model, Model\r\nfrom tensorflow.keras.layers import Conv2D, Input\r\n\r\n# this works\r\ninp = Input(shape=(64, None, 3))\r\nconv = Conv2D(4, (3, 3), dynamic=False)\r\n# dynamic = True apparently requires manual building, which might or might not be a separate issue\r\nconv.build(inp.shape)\r\nx = conv(inp)\r\nx = Conv2D(4, (3, 3))(x)\r\n\r\nmod1 = Model(inputs=inp, outputs=x)\r\n\r\nmod1.save(\"test.hdf5\")\r\nload_model(\"test.hdf5\")\r\nprint(\"Succesfully loaded this model\")\r\n\r\n# this fails:\r\ninp = Input(shape=(64, None, 3))\r\nconv = Conv2D(4, (3, 3), dynamic=True)\r\n# dynamic = True apparently requires manual building, which might or might not be a separate issue\r\nconv.build(inp.shape)\r\nx = conv(inp)\r\nx = Conv2D(4, (3, 3))(x)\r\n\r\nmod1 = Model(inputs=inp, outputs=x)\r\n\r\nmod1.save(\"test.hdf5\")\r\nload_model(\"test.hdf5\")\r\nprint(\"Succesfully loaded this model\")`\r\n\r\n**Other info / logs** \r\n`Traceback (most recent call last):\r\n  File \"C:/Users/killian/AppData/Roaming/JetBrains/PyCharmCE2020.1/scratches/scratch_7.py\", line 29, in <module>\r\n    load_model(\"test.hdf5\")\r\n  File \"C:\\Users\\killian\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\", line 207, in load_model\r\n    compile)\r\n  File \"C:\\Users\\killian\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\hdf5_format.py\", line 187, in load_model_from_hdf5\r\n    load_weights_from_hdf5_group(f['model_weights'], model.layers)\r\n  File \"C:\\Users\\killian\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\hdf5_format.py\", line 691, in load_weights_from_hdf5_group\r\n    ' layers.')\r\nValueError: You are trying to load a weight file containing 2 layers into a model with 1 layers.\r\n`\r\n", "comments": ["@kimartin,\r\nLooking at issues [#44533](https://github.com/tensorflow/tensorflow/issues/44533#issuecomment-749246090) and [#43498](https://github.com/tensorflow/tensorflow/issues/43498#issuecomment-709533001) with a similar error log, I was able to save and load the model in `tf` format. \r\n\r\nPlease find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/6b76cf7cae52a1de37049395ab682641/46853.ipynb). Thanks!", "@amahendrakar Switching to the tf format as in the gist does provide a workaround for saving new models, thank you. \r\nUnlike the issues mentioned however I had no problem with custom objects (in particular at least), just with the dynamic flag, so this definitely seems like a bug.", "Was able to reproduce the issue in TF 2.6.0-dev20210528,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/85077ec6810f624599b3c2d1643f816f/untitled46.ipynb#scrollTo=HZuVem9A10lx)..Thanks !", "Thanks for opening this issue. Development of keras moved to separate repository https://github.com/keras-team/keras/issues\r\n\r\nPlease post this issue on keras-team/keras repo.\r\nTo know more see;\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999\r\nThank you!"]}, {"number": 46849, "title": "the label_smoothing argument for CE loss in keras", "body": "Recently, I find that only the loss function \r\n\r\ntf.keras.losses.categorical_crossentropy(y_true,y_pred,from_logits=True,label_smoothing=0.1)\r\n\r\nhas argument \"label_smoothing\", \r\n\r\nboth function:  tf.keras.losses.sparse_categorical_crossentropy\r\nand class:  tf.keras.losses.SparseCategoricalCrossentropy\r\n\r\ndo not have this parameter.\r\n\r\nWhy not add \"label_smoothing\" to this CE loss functions?", "comments": ["Hi, @Saduf2019 \r\nI had been using TensorFlow for machine learning for a little while and feel confident upon the good hold over TensorFlow.\r\nI would like to resolve this issue with your guidance. \r\nIf you feel that It could be the best issue for a user like me who is going to contribute to TensorFlow for the first time kindly assign me this issue.\r\nthanks"]}]