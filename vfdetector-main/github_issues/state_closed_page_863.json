[{"number": 27618, "title": "not support MatrixBandPart in tflite", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version : 2.0alpha\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nHere is a list of operators for which you will need custom implementations: MatrixBandPart.\r\n```\r\n\r\n", "comments": ["I replaced tf.linalg.band_part by mask e.g:\r\n\t# mask = tf.linalg.band_part(mask, 0, -1)\r\n\ta = tf.range(tf.shape(mask)[0])\r\n\tmask1 = a[:,None]<=a[None,:]# true in upper_triangular\r\n\tmask = tf.logical_and(mask, mask1)", "@anhtu812 In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "@anhtu812 Could you provide the issue and its context with more details? Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27618\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27618\">No</a>\n"]}, {"number": 27617, "title": "'flatten_atrous_conv'  misses to flatten some atrous_convs", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):  master\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.22.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nI'm trying to optimize TF Model ' [xception65_coco_voc_trainval](http://download.tensorflow.org/models/deeplabv3_pascal_trainval_2018_01_04.tar.gz)' with graph_transforms tools.\r\n\r\nOptimize for inference\r\n`\r\nbazel-bin/tensorflow/python/tools/optimize_for_inference --input=frozen.pb --output=stripped.pb --frozen_graph=True --input_names=\"sub_7\" --output_names=\"ResizeBilinear_2\"\r\n`\r\nGraph Transforms\r\n`bazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=\"stripped.pb\" --out_graph=\"flatten.pb\" --inputs='sub_7' --outputs='ResizeBilinear_2' --transforms='flatten_atrous_conv'`\r\n\r\n**Describe the expected behavior**\r\n'flatten_atrous_conv'  misses to flatten some atrous_convs. \r\n![transform](https://user-images.githubusercontent.com/31765154/55704734-80988800-5a0f-11e9-8985-69f0bcb7b837.jpg)\r\n\r\n", "comments": ["The optimize_for_inference script is deprecated, so we're not likely to be able to fix this soon unfortunately. Adding to contributions welcome.", "@WenguoLi Could you please let us know if you still need help on this ? We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. please check [**`link`**](https://stackoverflow.com/questions/64603919/how-to-do-atrous-convolution-in-tensorflow-2-tf-keras) If it is resolved then please feel free to move this issue to close status ? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27617\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27617\">No</a>\n"]}, {"number": 27616, "title": "Cleanup & TC added l2_norm graph transformation.", "body": "Clean-up: replace FindOperator with FindOp and add missing unit tests\r\nfor the l2_norm graph transformation.", "comments": ["Change the commit description to be more specific? Like \"Clean-up: replace FindOperator with FindOp and add missing unit tests for the l2_norm graph transformation\"?", "@multiverse-tf , thanks for the review i have updated as per your suggestion, kindly check.\r\n\r\nRegards\r\nAmit", "@multiverse-tf , thanks for approving the PR.\r\n@gbaned , @rthadur please change the PR label to approved.\r\n\r\nRegards\r\nAmit", "@miaout17 , can you please review the PR and provide your feedback.\r\n\r\nRegards\r\nAmit"]}, {"number": 27615, "title": " https://www.tensorflow.org/alpha/tutorials/next_steps is 404", "body": "on this page\r\nhttps://www.tensorflow.org/alpha/tutorials/keras\r\n\r\nthe next step link\r\n \r\n \r\n learning resources are listed in next steps.\r\n https://www.tensorflow.org/alpha/tutorials/next_steps\r\n \r\n is 404\r\n ", "comments": ["@pleabargain  yes! the additional resources link is broken, i corrected it an created a pull request for the same\r\n\r\n@lamberta  @MarkDaoust  please review this", "still doesn't work\r\nhttps://www.tensorflow.org/alpha/tutorials/keras\r\nlink to \r\nhttps://www.tensorflow.org/alpha/tutorials/next_steps\r\nstill a 404\r\nI forced a new cache ctrl +f5. \r\n\r\nstill a 404\r\n404\r\nSorry, we couldn't find that page.\r\n\r\n![image](https://user-images.githubusercontent.com/640846/55779960-7d54d900-5aaf-11e9-8f3f-4c5bef16f76e.png)\r\n", "Thanks. The next_steps link and page were removed altogether. The index.md page will be updated on the next site publish.", "Thanks for the heads-up!"]}, {"number": 27614, "title": "how to freeze graph in tensorflow 2.0", "body": "not found any related document", "comments": ["@anhtu812 Request you to please go through this link https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py ", "i tried, but i get error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"convert_h5_to_tflite.py\", line 186, in <module>\r\n    variable_names_blacklist)\t\t\t\t\r\n  File \"/docker_environment/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py\", line 340, in freeze_graph\r\n    input_graph_def = _parse_input_graph_proto(input_graph, input_binary)\r\n  File \"/docker_environment/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py\", line 251, in _parse_input_graph_proto\r\n    input_graph_def.ParseFromString(f.read())\r\ngoogle.protobuf.message.DecodeError: Error parsing message\r\n```\r\ni think freeze_graph.py have not updated to compatible with tensorlow 2.0 (pip install tf-nightly-2.0-preview==2.0.0.dev20190405)", "@anhtu812 Here is the correct link to 2.0 [Freeze_graph.py](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/tools/freeze_graph.py). \r\nThe link provided by @muddham corresponds to \"master\" which is TF1.13.1. Thanks!\r\n\r\nI think it was resolved. I am closing the issue. Thanks!", "what is output_node_names in this case ( saved_model.save(keras_model, 'saved_model') ).\r\nFollowing code out error:\r\n```\r\nfrom tensorflow.python.tools import freeze_graph\r\nimport tensorflow.keras as keras\r\nimport tensorflow as tf\r\n\r\n\r\n\r\n# input_data = keras.layers.Input((300,300,3), dtype=tf.float32)\r\n# y_pred = keras.layers.Lambda(lambda x: tf.identity(x, name=\"out\"), name='fts_output')(input_data)\r\n# model = keras.models.Model([input_data], y_pred)\r\n# tf.saved_model.save(model, 'saved_model')\r\n\r\n\r\n\r\n\r\noutput_graph = 'final_model.pb'\r\noutput_node_names = 'model/fts_output/out'\r\ninput_saver = \"\"\r\ninput_binary = True\r\nrestore_op_name = 'save/restore_all'\r\nfilename_tensor_name = 'save/Const:0'\r\nclear_devices = True\r\ninitializer_nodes = ''\r\nvariable_names_blacklist = ''\r\n\r\n\r\nfreeze_graph.freeze_graph('', input_saver, input_binary,\r\n\t\t\t'', output_node_names,\r\n\t\t\trestore_op_name, filename_tensor_name,\r\n\t\t\toutput_graph, clear_devices, initializer_nodes,\r\n\t\t\tvariable_names_blacklist, input_saved_model_dir='saved_model')\r\n```\r\nlog error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"freeze_graph.py\", line 30, in <module>\r\n    variable_names_blacklist, input_saved_model_dir='saved_model')\r\n  File \"/docker_environment/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py\", line 363, in freeze_graph\r\n    checkpoint_version=checkpoint_version)\r\n  File \"/docker_environment/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py\", line 232, in freeze_graph_with_def_protos\r\n    variable_names_blacklist=variable_names_blacklist)\r\n  File \"/docker_environment/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/docker_environment/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py\", line 270, in convert_variables_to_constants\r\n    inference_graph = extract_sub_graph(input_graph_def, output_node_names)\r\n  File \"/docker_environment/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/docker_environment/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py\", line 182, in extract_sub_graph\r\n    _assert_nodes_are_present(name_to_node, dest_nodes)\r\n  File \"/docker_environment/home/docker/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py\", line 137, in _assert_nodes_are_present\r\n    assert d in name_to_node, \"%s is not in graph\" % d\r\nAssertionError: model/fts_output/out is not in graph\r\n```", "I doubt that TensorFlow 2.0 has officially supported frozen graph.", "@leimao could you give more information, I can't find the API\r\n", "> @leimao could you give more information, I can't find the API\r\n\r\nIt has been confirmed that TensorFlow 2.0 deprecated the usage of frozen graph. To use frozen graph in TensorFlow 1.x, check out my blog post at https://leimao.github.io/blog/Save-Load-Inference-From-TF-Frozen-Graph/", "> > @leimao could you give more information, I can't find the API\r\n> \r\n> It has been confirmed that TensorFlow 2.0 deprecated the usage of frozen graph. To use frozen graph in TensorFlow 1.x, check out my blog post at https://leimao.github.io/blog/Save-Load-Inference-From-TF-Frozen-Graph/\r\nthks", "https://github.com/tensorflow/tensorflow/releases/tag/v2.0.0\r\n\r\n\"Removed the freeze_graph command line tool; SavedModel should be used in place of frozen graphs.\"", "why freeze_graph is deprecated in TF2? freeze_graph can reduce the latency of my model by 2ms or 5% in TF1.14, even with XLA enabled. @leimao  @nmatare ", "> why freeze_graph is deprecated in TF2? freeze_graph can reduce the latency of my model by 2ms in TF1.14, even with XLA enabled. @leimao @nmatare\r\n\r\nSimply because there is no tf.Session(), which is a necessary component to build frozen models in TF 1.x, anymore in TF 2.0, and I guess Google does not want to bother to implement another protocol to freeze models without using the tf.Session() path.", "> > why freeze_graph is deprecated in TF2? freeze_graph can reduce the latency of my model by 2ms in TF1.14, even with XLA enabled. @leimao @nmatare\r\n> \r\n> Simply because there is no tf.Session(), which is a necessary component to build frozen models in TF 1.x, anymore in TF 2.0, and I guess Google does not want to bother to implement another protocol to freeze models without using the tf.Session() path.\r\n\r\nBut when we deploy the model in c++ (or other language?), we still need to use session. The latency reduced by freeze_graph is also important. Does TF2.0 clear the need to use frozen graph to reduce the model latency? In my test, at least for TF1.15 I still need to use freeze_graph to further reduce the model latency.\r\n\r\nAnother problem is model size. For mixed precision training, full-precision weights are stored in SavedModel, which are big. Freeze_graph can help to reduce the model size.", "@x10000year @leimao I think TF2 has this: \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/convert_to_constants.py\r\nThis generates a .pb file that has the graph and weights but I am not to sure if its the same thing as freeze model", "> @x10000year @leimao I think TF2 has this:\r\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/convert_to_constants.py\r\n> This generates a .pb file that has the graph and weights but I am not to sure if its the same thing as freeze model\r\n\r\nDoes it generate a single .pb file without anything else?", "@leimao No, it just generates the pb file\r\n", "https://github.com/Unity-Technologies/barracuda-release/blob/release/0.3.2/Documentation~/Barracuda.md.\r\nThere is also some code right here that simplifies that whole process in that file. It is under Converting TensorFlow models to Barracuda format section.\r\n\r\n\"In TF2.x use convert_to_constants. Example from a Keras Model (more examples):\r\n\r\nfrom tensorflow.python.framework import convert_to_constants\r\n@tf.function(input_signature=[tf.TensorSpec(shape=[<input_shape>], dtype=tf.float32)])\r\ndef to_save(x):\r\n    return model(x)\r\nf = to_save.get_concrete_function()\r\nconstantGraph = convert_to_constants.convert_variables_to_constants_v2(f)\r\ntf.io.write_graph(constantGraph.graph.as_graph_def(), <output_dir>, <output_file>) \"", "> @leimao No, it just generates the pb file\r\n\r\nIf it is a single `pb` file whose size is several MB (depending on the size of the model), it is likely to be a frozen model, although it remains to be verified. If the `pb` file is only several KB, it is a graph file without any weights in it. This is an interesting finding @Robin2091 .", "@leimao I have the file generated for a YOLOv3 tiny object detector. It is only 96,300 kB, so I am not sure. Also when I try to convert the file into an onnx format I get an error saying failed to parse file. I used the code from tf2onnx converter.\r\nhttps://github.com/onnx/tensorflow-onnx/\r\nSo I am not sure if it has the weights in it then. Although my .tf weights file is only 30,000 kB so there is a chance the 96,000kB pb file includes the weights\r\n", "> I have the file generated for a YOLOv3 tiny object detector. It is only 96300 kB, so I am not sure. Also when I try to convert the file into an onnx format I get an error saying failed to parse file. I used the code from tf2onnx converter.\r\n> https://github.com/onnx/tensorflow-onnx/\r\n> So I am not sure if it has the weights in it then. Although my .tf weights file is only 30000 kB so there is a chance the 96000kB file includes the weights\r\n\r\nIt is very likely that you are correct. Note that the function used for freezing model in TF1.x is called [`convert_variables_to_constants`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/graph_util/convert_variables_to_constants) which is also included in the file you provided here.\r\nI will look into this at my convenience. Thanks @Robin2091 .\r\nIn addition, I have not used tf2onnx myself in practice. So I could probably not advise on that. However, if you are interested, you may try NVIDIA TensorRT for inference, it takes frozen model as input. But I am not sure if it can still work for the \"frozen model\" generated from your protocol.", "@leimao Ok, let me know if you find any related information to this. I will look into TensorRT. Also, do you know if a model in TF2 can be processed by TF1.x. Since the converters(tf2onnx and others) use TF1 so that might be why my pb file cannot be read? \r\nThanks for the advice. ", "> @leimao Ok, let me know if you find any related information to this. I will look into TensorRT. Also, do you know if a model in TF2 can be processed by TF1.x. Since the converters(tf2onnx and others) use TF1 so that might be why my pb file cannot be read?\r\n> Thanks for the advice.\r\n\r\nTensorFlow 2.x does support freezing models and these frozen models should be equivalent to the frozen models used for TensorFlow 1.x. \r\nPlease check my [blog post](https://leimao.github.io/blog/Save-Load-Inference-From-TF2-Frozen-Graph/) and [code example](https://github.com/leimao/Frozen_Graph_TensorFlow/tree/master/TensorFlow_v2).", "@leimao Thanks for the confirmation. I also got the conversion working so the .pb generated is equivalent. ", "@leimao thank you so much. you saved my day. i hope TF team can document more for this approach, as frozen graph is still the most common format used by other libraries."]}, {"number": 27613, "title": "No improvement in performance of deeplabv3_257_mv_gpu.tflite on TFLite for GPU", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: MI 5\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: \r\n\r\n**Describe the current behavior**\r\nI'm trying to run TFLite Model '[deeplabv3_257_mv_gpu.tflite](https://storage.googleapis.com/download.tensorflow.org/models/tflite/gpu/deeplabv3_257_mv_gpu.tflite)' with GPU delegate on Android  devices.  According to PR for label_image #27464,  I modified the label_image code to support the gpu delegate and removed the code from [205](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/label_image.cc#L205) to [251](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/label_image.cc#L251) lines .  \r\n\r\n```\r\nvoid RunInference(Settings* s) {\r\n  if (!s->model_name.c_str()) {\r\n    LOG(ERROR) << \"no model file name\\n\";\r\n    exit(-1);\r\n  }\r\n\r\n  std::unique_ptr<tflite::FlatBufferModel> model;\r\n  std::unique_ptr<tflite::Interpreter> interpreter;\r\n  model = tflite::FlatBufferModel::BuildFromFile(s->model_name.c_str());\r\n  if (!model) {\r\n    LOG(FATAL) << \"\\nFailed to mmap model \" << s->model_name << \"\\n\";\r\n    exit(-1);\r\n  }\r\n  LOG(INFO) << \"Loaded model \" << s->model_name << \"\\n\";\r\n  model->error_reporter();\r\n  LOG(INFO) << \"resolved reporter\\n\";\r\n\r\n  tflite::ops::builtin::BuiltinOpResolver resolver;\r\n\r\n  tflite::InterpreterBuilder(*model, resolver)(&interpreter);\r\n  if (!interpreter) {\r\n    LOG(FATAL) << \"Failed to construct interpreter\\n\";\r\n    exit(-1);\r\n  }\r\n\r\n  interpreter->UseNNAPI(s->accel);\r\n  interpreter->SetAllowFp16PrecisionForFp32(s->allow_fp16);\r\n\r\n  if (s->verbose) {\r\n    LOG(INFO) << \"tensors size: \" << interpreter->tensors_size() << \"\\n\";\r\n    LOG(INFO) << \"nodes size: \" << interpreter->nodes_size() << \"\\n\";\r\n    LOG(INFO) << \"inputs: \" << interpreter->inputs().size() << \"\\n\";\r\n    LOG(INFO) << \"input(0) name: \" << interpreter->GetInputName(0) << \"\\n\";\r\n\r\n    int t_size = interpreter->tensors_size();\r\n    for (int i = 0; i < t_size; i++) {\r\n      if (interpreter->tensor(i)->name)\r\n        LOG(INFO) << i << \": \" << interpreter->tensor(i)->name << \", \"\r\n                  << interpreter->tensor(i)->bytes << \", \"\r\n                  << interpreter->tensor(i)->type << \", \"\r\n                  << interpreter->tensor(i)->params.scale << \", \"\r\n                  << interpreter->tensor(i)->params.zero_point << \"\\n\";\r\n    }\r\n  }\r\n\r\n  if (s->number_of_threads != -1) {\r\n    interpreter->SetNumThreads(s->number_of_threads);\r\n  }\r\n\r\n  int image_width = 224;\r\n  int image_height = 224;\r\n  int image_channels = 3;\r\n  std::vector<uint8_t> in = read_bmp(s->input_bmp_name, &image_width,\r\n                                     &image_height, &image_channels, s);\r\n\r\n  int input = interpreter->inputs()[0];\r\n  if (s->verbose) LOG(INFO) << \"input: \" << input << \"\\n\";\r\n\r\n  const std::vector<int> inputs = interpreter->inputs();\r\n  const std::vector<int> outputs = interpreter->outputs();\r\n\r\n  if (s->verbose) {\r\n    LOG(INFO) << \"number of inputs: \" << inputs.size() << \"\\n\";\r\n    LOG(INFO) << \"number of outputs: \" << outputs.size() << \"\\n\";\r\n  }\r\n#if defined(ANDROID) || defined(__ANDROID__)\r\n  TfLiteGpuDelegateOptions kMyOptions = {\r\n      .metadata = nullptr,\r\n      .compile_options =\r\n          {\r\n              .precision_loss_allowed = 0,\r\n              .preferred_gl_object_type = TFLITE_GL_OBJECT_TYPE_FASTEST,\r\n              .dynamic_batch_enabled = 0,\r\n          },\r\n  };\r\n  if (s->allow_fp16) kMyOptions.compile_options.precision_loss_allowed = 1;\r\n\r\n  TfLiteDelegate* delegate;\r\n  if (s->gl_backend) {\r\n    delegate = TfLiteGpuDelegateCreate(&kMyOptions);\r\n    if (interpreter->ModifyGraphWithDelegate(delegate) != kTfLiteOk) return;\r\n  } else {\r\n#endif\r\n    if (interpreter->AllocateTensors() != kTfLiteOk) {\r\n      LOG(FATAL) << \"Failed to allocate tensors!\";\r\n    }\r\n#if defined(ANDROID) || defined(__ANDROID__)\r\n  }\r\n#endif\r\n\r\n  if (s->verbose) PrintInterpreterState(interpreter.get());\r\n\r\n  // get input dimension from the input tensor metadata\r\n  // assuming one input only\r\n  TfLiteIntArray* dims = interpreter->tensor(input)->dims;\r\n  int wanted_height = dims->data[1];\r\n  int wanted_width = dims->data[2];\r\n  int wanted_channels = dims->data[3];\r\n\r\n  switch (interpreter->tensor(input)->type) {\r\n    case kTfLiteFloat32:\r\n      s->input_floating = true;\r\n      resize<float>(interpreter->typed_tensor<float>(input), in.data(),\r\n                    image_height, image_width, image_channels, wanted_height,\r\n                    wanted_width, wanted_channels, s);\r\n      break;\r\n    case kTfLiteUInt8:\r\n      resize<uint8_t>(interpreter->typed_tensor<uint8_t>(input), in.data(),\r\n                      image_height, image_width, image_channels, wanted_height,\r\n                      wanted_width, wanted_channels, s);\r\n      break;\r\n    default:\r\n      LOG(FATAL) << \"cannot handle input type \"\r\n                 << interpreter->tensor(input)->type << \" yet\";\r\n      exit(-1);\r\n  }\r\n\r\n  profiling::Profiler* profiler = new profiling::Profiler();\r\n  interpreter->SetProfiler(profiler);\r\n\r\n  if (s->profiling) profiler->StartProfiling();\r\n  if (s->loop_count > 1)\r\n    for (int i = 0; i < s->number_of_warmup_runs; i++) {\r\n      if (interpreter->Invoke() != kTfLiteOk) {\r\n        LOG(FATAL) << \"Failed to invoke tflite!\\n\";\r\n      }\r\n    }\r\n\r\n  struct timeval start_time, stop_time;\r\n  gettimeofday(&start_time, nullptr);\r\n  for (int i = 0; i < s->loop_count; i++) {\r\n    if (interpreter->Invoke() != kTfLiteOk) {\r\n      LOG(FATAL) << \"Failed to invoke tflite!\\n\";\r\n    }\r\n  }\r\n  gettimeofday(&stop_time, nullptr);\r\n  LOG(INFO) << \"invoked \\n\";\r\n  LOG(INFO) << \"average time: \"\r\n            << (get_us(stop_time) - get_us(start_time)) / (s->loop_count * 1000)\r\n            << \" ms \\n\";\r\n#if 0\r\n  if (s->profiling) {\r\n    profiler->StopProfiling();\r\n    auto profile_events = profiler->GetProfileEvents();\r\n    for (int i = 0; i < profile_events.size(); i++) {\r\n      auto op_index = profile_events[i]->event_metadata;\r\n      const auto node_and_registration =\r\n          interpreter->node_and_registration(op_index);\r\n      const TfLiteRegistration registration = node_and_registration->second;\r\n      PrintProfilingInfo(profile_events[i], op_index, registration);\r\n    }\r\n  }\r\n\r\n  const float threshold = 0.001f;\r\n\r\n  std::vector<std::pair<float, int>> top_results;\r\n\r\n  int output = interpreter->outputs()[0];\r\n  TfLiteIntArray* output_dims = interpreter->tensor(output)->dims;\r\n  // assume output dims to be something like (1, 1, ... ,size)\r\n  auto output_size = output_dims->data[output_dims->size - 1];\r\n  switch (interpreter->tensor(output)->type) {\r\n    case kTfLiteFloat32:\r\n      get_top_n<float>(interpreter->typed_output_tensor<float>(0), output_size,\r\n                       s->number_of_results, threshold, &top_results, true);\r\n      break;\r\n    case kTfLiteUInt8:\r\n      get_top_n<uint8_t>(interpreter->typed_output_tensor<uint8_t>(0),\r\n                         output_size, s->number_of_results, threshold,\r\n                         &top_results, false);\r\n      break;\r\n    default:\r\n      LOG(FATAL) << \"cannot handle output type \"\r\n                 << interpreter->tensor(input)->type << \" yet\";\r\n      exit(-1);\r\n  }\r\n\r\n  std::vector<string> labels;\r\n  size_t label_count;\r\n\r\n  if (ReadLabelsFile(s->labels_file_name, &labels, &label_count) != kTfLiteOk)\r\n    exit(-1);\r\n\r\n  for (const auto& result : top_results) {\r\n    const float confidence = result.first;\r\n    const int index = result.second;\r\n    LOG(INFO) << confidence << \": \" << index << \" \" << labels[index] << \"\\n\";\r\n  }\r\n#endif  \r\n}\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nVersion with GPUDelegate is a lot slower (228.445 ms per run vs 132.303 ms) than CPU.\r\n\r\n**Other info / logs**\r\n\r\nGPU:\r\n```\r\ngemini:/data/local/tmp $ ./label_image -g 1 -m deeplabv3_257_mv_gpu.tflite\r\nnnapi error: requires android sdk version to be at least 27\r\nLoaded model deeplabv3_257_mv_gpu.tflite\r\nresolved reporter\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\ninvoked \r\naverage time: 228.445 ms \r\n```\r\nCPU:\r\n```\r\ngemini:/data/local/tmp $ ./label_image -m deeplabv3_257_mv_gpu.tflite                                   <\r\nnnapi error: requires android sdk version to be at least 27\r\nLoaded model deeplabv3_257_mv_gpu.tflite\r\nresolved reporter\r\nINFO: Initialized TensorFlow Lite runtime.\r\ninvoked \r\naverage time: 132.303 ms\r\n```\r\n\r\n\r\n", "comments": ["Watching this", "Performance regression on Mali is still being investigated.  We spent a full week, but wasn't able to easily identify the root cause yet.  Especially with Mali showing a higher variance on performance than Adreno where numbers are more stable.  I will update as soon as we have progress.", "@impjdi\r\nIs there any good news about Mali now?", "@impjdi \r\nIs there any good news about this?", "@anneyking @holyhao \r\n\r\nSorry, the engineers in charge don't have cycles this week to investigate.  They may start next week.", "any new findings about MALI performance optmization?", "Yeah, we have a candidate for the root cause, but are working on a more systematic approach.  We \"might\" see some improvement by end of year if everything goes well =/", "@impjdi I'm super curious about this suspicious root cause. I found some MALI gpu phones run much faster with delegate v2, which uses opencl delegate as the default backend. I tried to tune the work groups setting of conv2d and depthwise conv2d for opengl compute shader, but just like your team I don't see any obvious improvement. It's completely counterintuitive.\r\n\r\nYou mentioned about \"a more systematic approach\". Is that something about the opencl-opengl interoperability?", "I would say that connection is like rendering system for gui order of\ncomplexity.\n\nOn Fri, Jun 5, 2020, 3:53 AM Bruce Chou <notifications@github.com> wrote:\n\n> @impjdi <https://github.com/impjdi> I'm super curious about this\n> suspicious root cause. I found some MALI gpu phones run much faster with\n> delegate v2, which uses opencl delegate as the default backend. I tried to\n> tune the work groups setting of conv2d and depthwise conv2d for opengl\n> compute shader, but just like your team I don't see any obvious\n> improvement. It's completely counterintuitive.\n>\n> You mentioned about \"a more systematic approach\". Is that something about\n> the opencl-opengl interoperability?\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27613#issuecomment-639209312>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AIT525SC26TRVZ7JF57L5C3RVBF2HANCNFSM4HEE66VQ>\n> .\n>\n", "@brucechou1983 no, it's something else.  for example, loop unrolling & tiling are not handled efficiently in many of the mali drivers, so we may want to do the unrolling manually.  however, doing that makes the shader code quite unreadable and difficult to debug.  so we're making things to make that sort of task easy.", "@WenguoLi  It looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version 2.5 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27613\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27613\">No</a>\n"]}, {"number": 27612, "title": "Incomplete instructions for using images as metadata in https://www.tensorflow.org/guide/embedding", "body": "**System information**\r\n- Doc Link: https://www.tensorflow.org/guide/embedding\r\n\r\n\r\n**Describe the documentation issue**\r\n\r\nThe guide says\r\n\r\n> To use images as metadata, you must produce a single sprite image, consisting of small thumbnails, one for each vector in the embedding. \r\n\r\nIt then goes on to explain what the sprite image should look like (but it doesn't make clear if it needs to be \"square\" where each row contains a number of thumbnails close to the square root of the total number of vectors). Is any standard image format OK? What does one do with the image file? It doesn't work to choose it in the dialog that says \"Step 2 (optional): Load a TSV file of metadata.\". If one creates a config.json file how should one refer to the sprite image URL?\r\n", "comments": ["@ToonTalk \r\n\r\n`sprite.png` : It consist all the sample images in a very large image.\r\n`metadata.tsv` : It store index and label of each sample\r\n\r\nNot any standard image format is ok, till now .png is what it supports to my knowledge, if we do not use these files, then our visualization is nothing but some points on the Visualizer, introducing these files will help the visulaizer to use the particular image of the point and labels too.\r\n\r\nYou have to generate the `sprite.png` and `metadata.tsv` with the help of a function and yes the tutorial lacks some key points upon the usage of these files and there generation that how a user generate these files.\r\n\r\nIf you want any help in how to generate these file please let me know i would love to help.\r\n\r\nI hope your confusion is clear now if any more doubts feel free to ask.", "Thanks. I finally got this working when I noticed that if I click the \"Publish\" button it shows an example of how to specify the image URL and the dimensions. If someone could update the guide this would have been much easier.", "@ToonTalk  Thanks for raising this issue. I am working on it.", "A while back a PR addressing this was merged, and now this link is about word embeddings, so I'm going to close it.  Thanks!"]}, {"number": 27611, "title": "E1102 non-callable raised by pylint when extending from tf.keras.layers.Layer", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): custom-ops container\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): nightly-2.0-preview\r\n- Python version: the one with custom-ops container\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\nAs given in this page : https://www.tensorflow.org/tutorials/eager/custom_layers\r\nI am trying to change the code in seq2seq as follows:\r\nin decoder.py : `class BaseDecoder(tf.keras.layers.Layer):`\r\n\r\nHowever, while running the pylint stage of sanity_checks it produces the following error:\r\n\r\n`tensorflow_addons/seq2seq/decoder_test.py:63: [E1102(not-callable), DecodeRNNTest._testDecodeRNN] my_decoder is not callable\r\n\r\ntensorflow_addons/seq2seq/decoder_test.py:140: [E1102(not-callable), DecodeRNNTest._testDynamicDecodeRNNWithTrainingHelperMatchesDynamicRNN] my_decoder is not callable\r\n`\r\n\r\n**Describe the expected behavior**\r\n\r\nThe E1102 warning should not be raised.\r\n\r\n**Code to reproduce the issue**\r\nTo reproduce, in this pull request: https://github.com/tensorflow/addons/pull/145\r\nGo to decoder.py and change the signature of class in line 132\r\n\r\nThe warning disappear for pylint2 in current pull request, as currently it imports layers from tensorflow.python.keras and then extends layers.Layers\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n", "comments": ["This code shows that there is something that pylint is not able to infer but objects from classes inherited from tf.keras.layers.Layer are callable: \r\n\r\n```\r\nclass a(tf.keras.layers.Layer):\r\n  pass\r\nclass b(a):\r\n  pass\r\nclass c(b):\r\n  pass\r\naobj = a()\r\nbobj = b()\r\ncobj = c()\r\nprint(callable(aobj))\r\nprint(callable(bobj))\r\nprint(callable(cobj))\r\n\r\n```\r\nprints True 3 times.\r\n", "@armando-fandango Could you check whether the issue was resolved or not? I know recently there were some updates in pylint. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27611\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27611\">No</a>\n"]}, {"number": 27610, "title": "PackageNotFoundError: Package missing in current win-64 channels: - python 3.7*", "body": "**System information**\r\n- Windows 10\r\n- TensorFlow installed from GitHub (devtools::install_github(\"rstudio/tensorflow\"))\r\n- TensorFlow version: latest per 2019.04.08 (r1.13)\r\n- Python version: 3.7\r\n- Bazle version: 0.24.1 (bazel-0.24.1-windows-x86_64.exe)\r\n- Installed using 'default' for windows, i.e. 'system'. The same with 'conda'\r\n\r\n**I followed issue [20517](https://github.com/tensorflow/tensorflow/issues/20517), but I keep getting this exception in RStudio with R/3.5.0:**\r\n\r\n```\r\n> install_tensorflow()\r\nCreating r-tensorflow conda environment for TensorFlow installation...\r\nFetching package metadata ...........\r\n\r\nPackageNotFoundError: Package missing in current win-64 channels: \r\n  - python 3.7*\r\n\r\nError: Error 1 occurred creating conda environment r-tensorflow\r\n```\r\n**I get the same error when I try installing Kras:**\r\n\r\n```\r\nlibrary(keras)\r\ninstall_keras()\r\n\r\nCreating r-tensorflow conda environment for TensorFlow installation...\r\nFetching package metadata ...........\r\n\r\nPackageNotFoundError: Package missing in current win-64 channels: \r\n  - python 3.7*\r\n\r\nError: Error 1 occurred creating conda environment r-tensorflow\r\n\r\n```\r\n\r\n**I tried to install version 1.13.1, but after installation, I got this error when running 'sess = tf$Session()':**\r\n\r\n```\r\nError: Installing TensorFlow requires a 64-bit version of Python 3.5 or 3.6\r\n\r\nPlease install 64-bit Python 3.5 or 3.6 to continue, supported versions include:\r\n\r\n - Anaconda Python (Recommended): https://www.anaconda.com/download/#windows\r\n - Python Software Foundation   : https://www.python.org/downloads/\r\n```", "comments": ["After installing TF you need to specify the [```TENSORFLOW_PYTHON```](https://tensorflow.rstudio.com/tensorflow/articles/installation.html#locating-tensorflow) environment variable. Can you please confirm? Thanks!", "I reinstalled everything and apparently there was a problem with a new warning in the new version of conda, which is now fixed. So, my problem has been fixed. Thanks for the response. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27610\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27610\">No</a>\n", "I also encountered the `PackageNotFoundError` while installing TensorFlow. I took the following steps and eventually resolved the issue:\r\n\r\n1. Find the PC path to R: `C:\\Program Files\\R\\R-3.6.0\\bin`\r\n\r\n2. From the Start menu, open the Anaconda Prompt (black window terminal) and change to the directory above. The command is `cd C:\\Program Files\\R\\R-3.6.0\\bin`.\r\n\r\n3. In the Anaconda Prompt, run `R` or `.\\R.exe` to get into the R command line.\r\n\r\n4. In the R command line, include `library(tensorflow)` and run `install_tensorflow()`.\r\n\r\n5. Check `tf_version()` to be `1.13`.\r\n\r\nTo install keras, just change the commands in Step 4 to `library(keras)` and `install_keras()`.\r\n\r\nI would like to thank @javierluraschi for helping me with this. :)"]}, {"number": 27609, "title": "IOS tensorflow-lite , error: 'tensorflow/core/framework/resource_handle.h' file not found", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `MacOS High Sierra v10.13.7`\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: `Simulator iPhone XR`\r\n\r\n**Describe the problem**\r\nI am trying to run the sample IOS app, by following this guideline\r\nhttps://www.tensorflow.org/lite/guide/ios\r\nI did the following steps\r\n- [x] Install `Xcode`\r\n- [x] Install `brew`\r\n- [x] Run\r\n```\r\nbrew install automake\r\nbrew install libtool\r\n```\r\n- [x] git clone https://github.com/tensorflow/tensorflow\r\n- [x] run `sudo gem install cocoapods`\r\n- [x] run `pod install`\r\n- [x] run `build_all_ios.sh`\r\n- [x] run `tensorflow/lite/examples/ios/download_models.sh`\r\n\r\nAll seems ok. But as soon as, I build the app in XCode, it throws below error\r\n<img width=\"1133\" alt=\"Screen Shot 2019-04-08 at 9 42 48 AM\" src=\"https://user-images.githubusercontent.com/241914/55693661-bf641900-59e2-11e9-9421-aa3417053f7b.png\">\r\n\r\nIt looks similar to #9354 but here the missing file is `resource_handle.h` not `resource_handle.pb.h`\r\n\r\nAny suggestion? Thanks\r\n", "comments": ["I rerun `pod install`, restart `Xcode`, then this _'tensorflow/core/framework/resource_handle.h' file not found_ disappeared."]}, {"number": 27608, "title": "Multidimensional Keras recurrent layers", "body": "Tensorflow 2.0.0-alpha\r\n\r\ncurrently Keras recurrent layer expect their input to have exactly 3 dimensions: (batch, sequence_length, element_size)\r\n\r\nInput of higher dimensions will result in the following error:\r\n> ValueError: Input 0 of layer unified_lstm is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [2, 3, 5, 4]\r\n\r\nThe wanted behavior is as follows:\r\nlet's say that I have in input of shape (2, 3, 5, 4) where every element in my batch have 3 documents: (batch, document_number, sequence_length, element_size).\r\nI would like for the LSTM to consume only the last 2 dimensions (2*3 time)  in the same manner that tf.keras.layers.Dense consumes only the last dimension.\r\n\r\n**This will not change the current api**\r\n**I an willing to help (though it will be my first time committing to FT)**\r\n**Anyone that wants to run several RNNs in parallel will benefit from this feature**", "comments": ["I think for LSTM itself, the specification is quite clear that the input has to be 3D, but this is not enforce for RNN layer.\r\n\r\nIf you want to have a RNN cell accept high dimension input, you can certainly create a custom cell for that. In fact we have a similar example in the unit test.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/5581b91ada226f1ec20f55cd6423853072b2813c/tensorflow/python/keras/layers/recurrent_test.py#L1340.\r\n\r\nFurthermore, I think LSTM has clear math definition, which is not like Dense layer. Updating default LSTM is probably not something we can intake. \r\n\r\n", "The other alternative I can think of is that you reshape your data before feeding it to LSTM, eg [2, 3, 5, 4] -> [6, 5 ,4]. Since document_number is not something that you want LSTM to aware, the math result for the data after conversion should be same as before. You can also reshape the output from LSTM to bring back your document_number dim. \r\n\r\nIf you want LSTM to aware your document_number, then you will need a customized cell to do that.", "The example could be like:\r\n\r\n```\r\nbatch, document_number, sequence_length, element_size = 2, 3, 5, 4\r\ninputs = tf.ones([batch, document_number, sequence_length, element_size])\r\nlstm_inputs = tf.reshape(inputs, [None, sequence_length, element_size])\r\nlstm = tf.keras.layers.LSTM(10, return_sequence=True)\r\nlstm_outputs = lstm(lstm_inputs)\r\noutputs = tf.reshape(lstm_outputs, [batch, document_number, sequence_length, 10])\r\n```\r\n", "I am closing this FR since there isn't anything i can address here. Feel free to reopen this if you feel otherwise. Thanks.", "@qlzh727 Thanks. Reshaping to 3D and back is exactly the behavior I would expect so Keras layers will be interchangeable with other TF layers. I can implement it myself for my code but I really believe other people could also enjoy it and the implementation is not hard.\r\n"]}, {"number": 27607, "title": "ImportError: DLL load failed: The specified module could not be found.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (Windows 10):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):anaconda\r\n- TensorFlow version:1.13.1\r\n- Python version:3.5\r\n- Installed using pip\r\n- Bazel version (if compiling from source):none\r\n- GCC/Compiler version (if compiling from source):none\r\n- CUDA/cuDNN version:none\r\n- GPU model and memory:none\r\n\r\nThis is supposed to be the tensorflow cpu version\r\n\r\njust trying a simple import script and im recieving this error\r\n\r\n**This is my small script**\r\n\r\nimport numpy as np\r\nimport os\r\nimport six.moves.urllib as urllib\r\nimport sys\r\nimport tarfile\r\nimport tensorflow as tf\r\nimport zipfile\r\n\r\nfrom collections import defaultdict\r\nfrom io import StringIO\r\nfrom matplotlib import pyplot as plt\r\nfrom PIL import Image\r\n\r\nsys.path.append(\"..\")\r\nfrom object_detection.utils import ops as utils_ops\r\n\r\nfrom utils import label_map_util\r\n\r\nfrom utils import visualization_utils as vis_util\r\n\r\n**Describe the problem**\r\nTraceback (most recent call last):\r\n  File \"D:/tens/test.py\", line 6, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Name\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Name\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Name\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Name\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Name\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Abdelrahman\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nProcess finished with exit code 1\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Did you download and install the Microsoft Visual C++ 2015 Redistributable Update 3? Can you please confirm? Also take a look at https://www.tensorflow.org/install/pip if haven't already. ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27607\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27607\">No</a>\n"]}, {"number": 27606, "title": "CMake Error at tf_core_ops.cmake", "body": "\r\n**System information**\r\n- OS Platform and Distribution : Windows 10 64bit\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v1.12.0\r\n- Python version: python3.6 64bit\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): bazel-0.24.1-windows-x86_64.exe\r\n- GCC/Compiler version (if compiling from source): vs2015\r\n- CUDA/cuDNN version: cpu only\r\n\r\n\r\n\r\nI have been trying to build a tensorflow c++ lib for a long time but never succeed.\r\n\r\nThis time, I tried to build it with cmake(gui), when I click \"generate\" some error occurred.\r\n\r\n> CMake Error at tf_core_ops.cmake:73 (add_library):\r\n>   Cannot find source file:\r\n> \r\n>     D:/tensorflow/tensorflow/contrib/data/ops/dataset_ops.cc\r\n> \r\n>   Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm\r\n>   .hpp .hxx .in .txx\r\n> Call Stack (most recent call first):\r\n>   tf_core_ops.cmake:92 (GENERATE_CONTRIB_OP_LIBRARY)\r\n>   CMakeLists.txt:512 (include)\r\n> \r\n> \r\n> CMake Error at tf_core_kernels.cmake:221 (add_library):\r\n>   Cannot find source file:\r\n> \r\n>     D:/tensorflow/tensorflow/contrib/data/kernels/assert_next_dataset_op.cc\r\n> \r\n>   Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm\r\n>   .hpp .hxx .in .txx\r\n> Call Stack (most recent call first):\r\n>   CMakeLists.txt:514 (include)\r\n> \r\n> \r\n> CMake Error at tf_core_ops.cmake:73 (add_library):\r\n>   No SOURCES given to target: tf_contrib_data_dataset_ops\r\n> Call Stack (most recent call first):\r\n>   tf_core_ops.cmake:92 (GENERATE_CONTRIB_OP_LIBRARY)\r\n>   CMakeLists.txt:512 (include)\r\n> \r\n> \r\n> CMake Error at tf_core_kernels.cmake:221 (add_library):\r\n>   No SOURCES given to target: tf_core_kernels\r\n> Call Stack (most recent call first):\r\n>   CMakeLists.txt:514 (include)\r\n> \r\n> ", "comments": []}, {"number": 27605, "title": "Fixed #27598", "body": "Converting the Ragged Tensor to General Tensor is the fastest way to fix the Bug.", "comments": ["This doesn't seem quite right to me... `Dataset.from_generator()` is meant for wrapping code that generates Python values such as NumPy arrays, and convering a RaggedTensor to a Tensor seems like it might lead to a surprising increase in memory use.\r\n\r\n@jsimsa is reviewing some RaggedTensor support in other parts of `tf.data`, and might have a better idea of how to handle this.", "> This doesn't seem quite right to me... `Dataset.from_generator()` is meant for wrapping code that generates Python values such as NumPy arrays, and convering a RaggedTensor to a Tensor seems like it might lead to a surprising increase in memory use.\r\n> \r\n> @jsimsa is reviewing some RaggedTensor support in other parts of `tf.data`, and might have a better idea of how to handle this.\r\n\r\n@mrry , I know that this might cause memory problem. But this a temporary fix, as long as there's no support for RaggedTensor in `tf.data` this is the only way to deal with this.\r\nApart from that, Can you give me some pointers or Documentation Links, how to get started with building support for Special Tensor Types? @jsimsa is working on Supporting RaggedTensor, so I was thinking if I could add Support to SparseTensors.", "The current PR couldn't be a temporary change though... `Dataset.from_generator()` is in the public API, so applying this PR would prevent us from making the proper fix, because users might come to depend on the temporary behavior in the meantime.", "> The current PR couldn't be a temporary change though... `Dataset.from_generator()` is in the public API, so applying this PR would prevent us from making the proper fix, because users might come to depend on the temporary behavior in the meantime.\r\n\r\nOkay. Thanks @mrry for Clearing out. Can you Point me some resources from which I can teach myself adding C++ Kernels for Ops? I went through the codes already available Ops, but that doesn't really help.\r\nThis Error also occurs for SparseTensors, So, I can start working for supporting Sparse Tensors in `tf.data`.", "Hi @captain-pool, here is a tutorial that demonstrates how to add new ops and kernels to TensorFlow: https://www.tensorflow.org/guide/extend/op.\r\n\r\nAs for `tf.SparseTensor`, tf.data already supports that type for most transformations. `tf.data.Dataset.from_generator` is an exception. Because of its implementation details only numpy values and (dense) tensors are supports and there are no plans to add support for other types.\r\n\r\nCan you describe the high-level use case you have for using `tf.SparseTensor` or `tf.RaggedTensor`?\r\n\r\nLast but not least, I agree with @mrry that this PR should not be submitted.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 27604, "title": "tensorflow-gpu works with this exact config on Windows 10 (python 3.6.2 ONLY)", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10-1703\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.6.2\r\n- Installed using virtualenv? pip? conda?: pip3\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: CUDA 9.0, cuDNN 7.1\r\n- GPU model and memory: GeForce MX150/920M/740M\r\n\r\n\r\n\r\n**Tensorflow-GPU installs perfectly without DLL Load/Runtime Errors on the exact configuration stated above. Reason of incompatibility with other dependencies (protobuf) unknown. To run tf-gpu, use this configuration: python==3.6.2, tensorflow-gpu==1.12.0, protobuf==3.5 or later, cuda==9.0, cudnn==7.1 or later. I think it has more to do with python 3.6.2. It supports.**\r\n\r\n\r\n\r\n", "comments": ["@MasterSkepticista Thanks for the information. I have installed TF-gpu on same configuration as you mentioned with python 3.6.6 without any issue. Steps are mentioned [here](https://github.com/jvishnuvardhan/Installing-TensorFlow-GPU-on-Windows-10-TF1.12). Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!"]}, {"number": 27603, "title": "GPU Installation Issue", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, i5-750 only SSE4.2, without AVX.\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): pip install tf-nightly-gpu-2.0-preview\r\n- TensorFlow version: TensorFlow 2.0\r\n- Python version: conda create --name tensorflow-2.0 python=3.6\r\n- Installed using virtualenv? pip? conda?: pip install tf-nightly-gpu-2.0-preview\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda10, cuDNN 7.3.1,\r\n- GPU model and memory: GTX1060 6GB\r\n\r\nDo you know how to make tensorflow-2.0 work on nvidia gpu? I did the following without any error:\r\nconda create --name tensorflow-2.0 python=3.6\r\nactivate tensorflow-2.0\r\npip install tf-nightly-gpu-2.0-preview\r\nconda install -c anaconda cudatoolkit\r\nconda install -c anaconda cudnn\r\n\r\nBut then:\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Frust\\Anaconda3\\envs\\tensorflow-2.0\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Frust\\Anaconda3\\envs\\tensorflow-2.0\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Frust\\Anaconda3\\envs\\tensorflow-2.0\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Frust\\Anaconda3\\envs\\tensorflow-2.0\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Frust\\Anaconda3\\envs\\tensorflow-2.0\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: \u041f\u0440\u043e\u0438\u0437\u043e\u0448\u0435\u043b \u0441\u0431\u043e\u0439 \u0432 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0435 \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 \u0434\u0438\u043d\u0430\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u043a\u043e\u043c\u043f\u043e\u043d\u043e\u0432\u043a\u0438 (DLL).\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Frust\\Anaconda3\\envs\\tensorflow-2.0\\lib\\site-packages\\tensorflow\\__init__.py\", line 27, in <module>\r\n    from tensorflow._api.v2 import audio\r\n  File \"C:\\Users\\Frust\\Anaconda3\\envs\\tensorflow-2.0\\lib\\site-packages\\tensorflow\\_api\\v2\\audio\\__init__.py\", line 8, in <module>\r\n    from tensorflow.python.ops.gen_audio_ops import decode_wav\r\n  File \"C:\\Users\\Frust\\Anaconda3\\envs\\tensorflow-2.0\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Frust\\Anaconda3\\envs\\tensorflow-2.0\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Frust\\Anaconda3\\envs\\tensorflow-2.0\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Frust\\Anaconda3\\envs\\tensorflow-2.0\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Frust\\Anaconda3\\envs\\tensorflow-2.0\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Frust\\Anaconda3\\envs\\tensorflow-2.0\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Frust\\Anaconda3\\envs\\tensorflow-2.0\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: \u041f\u0440\u043e\u0438\u0437\u043e\u0448\u0435\u043b \u0441\u0431\u043e\u0439 \u0432 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0435 \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 \u0434\u0438\u043d\u0430\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u043a\u043e\u043c\u043f\u043e\u043d\u043e\u0432\u043a\u0438 (DLL).\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\ufeff\r\n", "comments": ["I also have problem when trying to install gpu version of tf2. Only the error is different.\r\n\r\n`THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\r\n    tensorflow-gpu==2.0.0-alpha0 from https://files.pythonhosted.org/packages/2b/39/5447eb09d096fba399614d856ecb0f7f51c78070074592efe81e4cdd52d8/tensorflow_gpu-2.0.0a0-cp36-cp36m-win_amd64.whl#sha256=70b7faae24f6a20394b60f7ae38ad4d37232d8be85bd3289479c74d51366c2b9:\r\n        Expected sha256 70b7faae24f6a20394b60f7ae38ad4d37232d8be85bd3289479c74d51366c2b9\r\n             Got        e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855`", "Any suggestion how to fix that?", " Hi, Did you download and install the Microsoft Visual C++ 2015 Redistributable Update 3? Can you please confirm? ", "It's installed already.", "@qo4on  Thanks for confirming.", "Did you add cuda, cudnn paths to environment? Please take a look at similar issues. #26059, #26364", "> Did you add cuda, cudnn paths to environment?\r\n\r\nWhich paths should I use when install cuda and cudnn through these commands?\r\nconda install -c anaconda cudatoolkit\r\nconda install -c anaconda cudnn", "You need to add sub folders: ```bin``` , ```include``` and ```lib``` to the path.", "> You need to add sub folders: `bin` , `include` and `lib` to the path.\r\n\r\nymodak, thank you for your help.\r\n\r\nWhen I install through these conda commands\r\nconda install -c anaconda cudatoolkit\r\nconda install -c anaconda cudnn\r\nI see in Anaconda directory C:\\Users\\User\\Anaconda3\\pkgs\\:\r\n![image](https://user-images.githubusercontent.com/35609308/56077148-1f4b2d00-5df2-11e9-9366-d69872604fec.png)\r\nWhich of these folders should I put into path? There are 2 different bin folders.\r\nI mean do I have to install cudnn-10.1-windows10-x64-v7.5.0.56.zip and cuda_10.1.105_418.96_win10.exe from nVidia site when cudatoolkit and cudnn are already installed in Anaconda3\\pkgs\\?\r\n\r\nDo you know conda command for TF 2.0, the same as this:\r\nconda create --name tf_gpu tensorflow-gpu \r\nbut for TF 2.0?", "You already have cuda 10.0 toolkit, so need to install separately. You can add bin, include, lib folders consecutively placed in Library directory.", "What should I install separately?", "[NVIDIA\u00ae GPU drivers](https://www.nvidia.com/Download/index.aspx?lang=en-us) \u2014CUDA 10.0 requires 410.x or higher.", "I have NVIDIA Creator Ready Driver 419.67 already installed. Cudatoolkit 10.0.130-0 and cudnn-7.3.1 installed also as I showed on the screenshot above. I don't understand what I should install separately.", "Right now you have bin, include, lib folders in your file system but not added to windows path. You have to add them. Take a look at  [Adding folder paths to the Windows path variable](https://docs.alfresco.com/4.2/tasks/fot-addpath.html)", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 27602, "title": "tf_upgrade_v2 fails on google colab and latest jupyter notebook", "body": "in all fairness tensorflow 2 is in alpha... but the docs indicate the tf1 > tf2 script should work:\r\nhttps://www.tensorflow.org/alpha/guide/upgrade\r\n\r\n**System information**\r\nusing the code from https://www.tensorflow.org/alpha/guide/upgrade\r\nI get syntax errors on google colab and on jupyter notebook\r\n\r\n- TensorFlow installed from (source or binary):\r\n!pip install tensorflow==2.0.0-alpha0 \r\n\r\n- TensorFlow version (use command below):\r\nmesh-tensorflow          0.0.5                \r\ntensorflow               2.0.0a0              \r\ntensorflow-estimator     1.13.0               \r\ntensorflow-hub           0.4.0                \r\ntensorflow-metadata      0.13.0               \r\ntensorflow-probability   0.6.0  \r\n\r\n- Python version:\r\ngoogle-api-python-client 1.6.7                \r\nipython                  5.5.0                \r\nipython-genutils         0.2.0                \r\nipython-sql              0.3.9                \r\nopencv-contrib-python    3.4.3.18             \r\nopencv-python            3.4.5.20             \r\npython-apt               1.6.3+ubuntu1        \r\npython-chess             0.23.11              \r\npython-dateutil          2.5.3                \r\npython-louvain           0.13                 \r\npython-rtmidi            1.2.1                \r\npython-slugify           3.0.2                \r\npython-utils             2.3.0 \r\n\r\n\r\n\r\n**Describe the current behavior**\r\ntested on jupyter notebook \r\njupyter --version\r\n4.4.0\r\nand google colab\r\n\r\n\r\n**Describe the expected behavior**\r\ntf_upgrade_v2  should work in in jupyter and google colab\r\n**Code to reproduce the issue**\r\ntry the code yourself here:\r\nhttps://colab.research.google.com/drive/1pVvIgkjGeWNyGWdy4A4gfEFQRdLJO8_G\r\n\r\n\r\n**Other info / logs**\r\nhere is the source code\r\nhttps://colab.research.google.com/drive/1pVvIgkjGeWNyGWdy4A4gfEFQRdLJO8_G\r\n\r\n\r\nHere is the video showing the results:\r\nhttps://youtu.be/u15oD3c_xHk", "comments": ["@pleabargain in colab/jupyter you need to prefix your bash commands.\r\n\r\ntry to run it like\r\n```\r\n!tf_upgrade_v2 --infile copy_of_got_book_generator.py --outfile TF2_got_book_generator.py\r\n```\r\n\r\nStarting with `!`", "I am SO embarrassed. Thank you for pointing to my bone head mistake!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27602\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27602\">No</a>\n"]}, {"number": 27601, "title": "tensorflow/contrib/android build feild nsync.a: malformed archive header name at 8", "body": "\r\n", "comments": ["later report"]}, {"number": 27600, "title": "Create ABOUT ME", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27600) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 27599, "title": "Doc: Update example for current version", "body": "Update example. Fix new line errors on webpage and update example imports\r\n\r\ncc @dynamicwebpaige ", "comments": ["Hi @tensorflowbutler I shall resume work on this shortly. ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 27598, "title": "RaggedTensor casting bug", "body": "version 2.0.0-alpha.\r\nNested RaggedTensor are cast to int64 without apparent reason:\r\n\r\nwith regular tensors (ok):\r\n```\r\n>>> tf.constant([[1]], dtype=tf.int8)\r\n<tf.Tensor: id=98, shape=(1, 1), dtype=int8, numpy=array([[1]], dtype=int8)>\r\n```\r\nwith nested RaggedTensor (not ok):\r\n```\r\n>>> tf.ragged.constant([[1]], dtype=tf.int8)\r\ntf.RaggedTensor(values=tf.Tensor([1], shape=(1,), dtype=int8), row_splits=tf.Tensor([0 1], shape=(2,), dtype=int64))\r\n```\r\n\r\nAlso they can not be used to create generators even with dtype=int64. The following code leads to:\r\n\r\n> The expected type was int64, but the yielded element was <tf.RaggedTensor [[6]]>.\r\n\r\n\r\n```\r\n\r\nclass LineGenerator(object):\r\n  def get_next_line(self):\r\n    while True:\r\n      out = [[6]]\r\n      yield tf.ragged.constant(out, dtype=tf.int64)\r\n\r\nclass Dataset(object):\r\n  def __init__(self, generator=LineGenerator()):\r\n    self.next_element = self.build_iterator(generator)\r\n\r\n  def build_iterator(self, gen: LineGenerator):\r\n    dataset = tf.data.Dataset.from_generator(gen.get_next_line,output_types = tf.int64)\r\n    #some other code...\r\n```", "comments": ["the data generated from `tf.data.from_generator` is type casted into desired type, by converting it into numpy array\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/20bbe5a17eeda011c4de74aa48d59ff4fc2f2090/tensorflow/python/data/ops/dataset_ops.py#L456-L457\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/20bbe5a17eeda011c4de74aa48d59ff4fc2f2090/tensorflow/python/ops/script_ops.py#L167\r\n\r\nHowever, as for numpy it **doesn't allow** variable length Sequences and Raises a `ValueError` when the cast is being performed.\r\n[https://stackoverflow.com/a/4675383/9947584](https://stackoverflow.com/a/4675383/9947584)\r\nSo, for that reason, the following error is raised.\r\nAn Alternative approach for your issue will be using SparseTensors. Please take a look into it.", "On a Second thought,\r\nThis issue can be solved by modifying the from_generator() function.\r\n@dynamicwebpaige  @alextp can I work on it?", "Thanks, the above solution avoids the conversion bug though it seems there is another issue with building a generator of RuggedTensors\r\n\r\n> tensorflow/core/framework/op_kernel.cc:1419] Unimplemented: Unsupported object type RaggedTensor\r\n\r\nstack trace:\r\n\r\n>   File \"/Users/alonrozental/IdeaProjects/AGENT/create_pretraining_data.py\", line 92, in <module>\r\n> 2019-04-07 16:39:02.397857: W tensorflow/core/framework/op_kernel.cc:1419] Unimplemented: Unsupported object type RaggedTensor\r\n>     tf.compat.v1.app.run()\r\n>   File \"/Users/alonrozental/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n> 2019-04-07 16:39:02.398489: W tensorflow/core/framework/op_kernel.cc:1419] Unimplemented: Unsupported object type RaggedTensor\r\n>     _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n>   File \"/Users/alonrozental/.pyenv/versions/3.6.5/lib/python3.6/site-packages/absl/app.py\", line 300, in run\r\n>     _run_main(main, args)\r\n>   File \"/Users/alonrozental/.pyenv/versions/3.6.5/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main\r\n>     sys.exit(main(argv))\r\n>   File \"/Users/alonrozental/IdeaProjects/AGENT/create_pretraining_data.py\", line 79, in main\r\n>     d = Dataset()\r\n>   File \"/Users/alonrozental/IdeaProjects/AGENT/create_pretraining_data.py\", line 48, in __init__\r\n>     self.next_element = self.build_iterator(generator)\r\n>   File \"/Users/alonrozental/IdeaProjects/AGENT/create_pretraining_data.py\", line 60, in build_iterator\r\n>     element = iter.get_next()\r\n>   File \"/Users/alonrozental/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 644, in get_next\r\n>     return self._next_internal()\r\n>   File \"/Users/alonrozental/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 577, in _next_internal\r\n>     output_shapes=self._flat_output_shapes)\r\n>   File \"/Users/alonrozental/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1954, in iterator_get_next_sync\r\n>     _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n>   File \"<string>\", line 3, in raise_from\r\n> tensorflow.python.framework.errors_impl.UnimplementedError: Unsupported object type RaggedTensor\r\n> \t [[{{node PyFunc}}]] [Op:IteratorGetNextSync]\r\n> 2019-04-07 16:39:02.437561: W tensorflow/core/kernels/data/generator_dataset_op.cc:79] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.\r\n> \t [[{{node PyFunc}}]]\r\n\r\nmy code looks like this:\r\n\r\n```\r\nclass Dataset(object):\r\n  def __init__(self, generator=LineGenerator()):\r\n    self.next_element = self.build_iterator(generator)\r\n\r\n  def build_iterator(self, gen: LineGenerator):\r\n    dataset = tf.data.Dataset.from_generator(gen.get_next_line,output_types = tf.int64) #the right way\r\n    iter = dataset.make_one_shot_iterator()\r\n    element = iter.get_next() #this line gives the error\r\n\r\n    return element\r\nd = Dataset()\r\n```\r\n\r\n\r\n", "I know. I'm not yet complete. I'm testing out various ways.\nWill create a PR as soon as I'm done.\n\nOn Sun, 7 Apr 2019, 7:18 pm ARozental, <notifications@github.com> wrote:\n\n> Thanks, the above solution avoids the conversion bug though it seems there\n> is another issue with building a generator of RuggedTensors\n>\n> tensorflow/core/framework/op_kernel.cc:1419] Unimplemented: Unsupported\n> object type RaggedTensor\n>\n> stack trace:\n>\n> File \"/Users/alonrozental/IdeaProjects/AGENT/create_pretraining_data.py\",\n> line 92, in\n> 2019-04-07 16:39:02.397857: W tensorflow/core/framework/op_kernel.cc:1419]\n> Unimplemented: Unsupported object type RaggedTensor\n> tf.compat.v1.app.run()\n> File\n> \"/Users/alonrozental/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/platform/app.py\",\n> line 40, in run\n> 2019-04-07 16:39:02.398489: W tensorflow/core/framework/op_kernel.cc:1419]\n> Unimplemented: Unsupported object type RaggedTensor\n> _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n> File\n> \"/Users/alonrozental/.pyenv/versions/3.6.5/lib/python3.6/site-packages/absl/app.py\",\n> line 300, in run\n> _run_main(main, args)\n> File\n> \"/Users/alonrozental/.pyenv/versions/3.6.5/lib/python3.6/site-packages/absl/app.py\",\n> line 251, in _run_main\n> sys.exit(main(argv))\n> File \"/Users/alonrozental/IdeaProjects/AGENT/create_pretraining_data.py\",\n> line 79, in main\n> d = Dataset()\n> File \"/Users/alonrozental/IdeaProjects/AGENT/create_pretraining_data.py\",\n> line 48, in *init*\n> self.next_element = self.build_iterator(generator)\n> File \"/Users/alonrozental/IdeaProjects/AGENT/create_pretraining_data.py\",\n> line 60, in build_iterator\n> element = iter.get_next()\n> File\n> \"/Users/alonrozental/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\",\n> line 644, in get_next\n> return self._next_internal()\n> File\n> \"/Users/alonrozental/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\",\n> line 577, in _next_internal\n> output_shapes=self._flat_output_shapes)\n> File\n> \"/Users/alonrozental/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\",\n> line 1954, in iterator_get_next_sync\n> _six.raise_from(_core._status_to_exception(e.code, message), None)\n> File \"\", line 3, in raise_from\n> tensorflow.python.framework.errors_impl.UnimplementedError: Unsupported\n> object type RaggedTensor\n> [[{{node PyFunc}}]] [Op:IteratorGetNextSync]\n> 2019-04-07 16:39:02.437561: W\n> tensorflow/core/kernels/data/generator_dataset_op.cc:79] Error occurred\n> when finalizing GeneratorDataset iterator: Failed precondition: Python\n> interpreter state is not initialized. The process may be terminated.\n> [[{{node PyFunc}}]]\n>\n> my code looks like this:\n>\n> class Dataset(object):\n>   def __init__(self, generator=LineGenerator()):\n>     self.next_element = self.build_iterator(generator)\n>\n>   def build_iterator(self, gen: LineGenerator):\n>     dataset = tf.data.Dataset.from_generator(gen.get_next_line,output_types = tf.int64) #the right way\n>     iter = dataset.make_one_shot_iterator()\n>     element = iter.get_next() #this line gives the error\n>\n>     return element\n> d = Dataset()\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27598#issuecomment-480591888>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ANWI2ZN5kFeG2AkmxkouHtlfPWyZSANeks5vefc-gaJpZM4cgi-E>\n> .\n>\n", "@ARozental You may now take a look. This is the fastest way to fix the bug. `gen_dataset_ops` creates Tensor out of the Data provided, and there's no support for RaggedTensors.", " @captain-pool This is what my second note was about. When I add these lines:\r\n```\r\n            if isinstance(ret, ragged_tensor.RaggedTensor):\r\n              ret_arrays.append(math_ops.cast(ret, dtype))\r\n              continue\r\n```\r\nIt does solve the first bug:\r\n> The expected type was int64, but the yielded element was <tf.RaggedTensor [[6]]>.\r\n\r\n\r\nbut then you get to the second one (which I could not see before):\r\n> tensorflow/core/framework/op_kernel.cc:1419] Unimplemented: Unsupported object type RaggedTensor\r\n\r\nI wasn't able to find out my self how deep this rabbit hole goes.", "The easiest Fix for this is by typecasting the RaggedTensor into normal tensor. Because, I saw, there's no support for RaggedTensor by dataset on a C++ level.", "That is a problem...\r\nmy use case is the one from the TF documentation\r\n\r\nhttps://www.tensorflow.org/guide/ragged_tensors\r\n> Hierarchical inputs, such as text documents that are subdivided into sections, paragraphs, sentences, and words\r\n\r\nBasically, I'm trying to encode entire books, but if every book has \"max_number_of_chapters\", every chapter has \"max_number_of_paragraphs\" and every paragraph has \"max_number_of_sentences\" then I won't be able to fit even one book in my GPU memory :/\r\n\r\nalso: @captain-pool thanks for the quick response ", "@ARozental you can read the tensors. And Remove the Paddings from the tensors as they arrive, to create RaggedTensors out of it. That can be a way to handle this.", "@captain-pool thanks. I'm trying something similar to make it work for now. Though creating padding just so that they can be removed is not very efficient as I need to create the RuggedTensor 1 element at a time from the non-padding tokens before I embed it in to vectors.", "For the dataset & raggedtensor integration you should be able to return a raggedtensor from a dataset because dataset can under the hood destructure ragged tensors into their component tensors", "A clarification for the initial bug description: there's no casting issue here, and the values are not being converted to int64.  The dtype=int64 that you see at the end of the following example applies only to the row_splits tensor, not to the overall ragged tensor:\r\n\r\n```\r\n>>> tf.ragged.constant([[1]], dtype=tf.int8)\r\ntf.RaggedTensor(values=tf.Tensor([1], shape=(1,), dtype=int8), row_splits=tf.Tensor([0 1], shape=(2,), dtype=int64))\r\n```\r\n\r\nTo be more explicit:\r\n\r\n```\r\n>>> rt = tf.ragged.constant([[1]], dtype=tf.int8)\r\n>>> assert rt.dtype == tf.int8\r\n>>> assert rt.values.dtype == tf.int8\r\n>>> assert rt.row_splits.dtype == tf.int64\r\n```\r\n\r\nAs subsequent comments in the bug deduced, the basic problem is that RaggedTensors are not fully supported by datasets yet.  We are actively working on adding this support, and expect it to be ready within the next few weeks.", "Thank you @edloper  ", "tf.data support for RaggedTensors was added by 5fe90dc.", "@edloper how do you use this? especially with parse_sequence_example ", "@edloper, even though the title of this issue wasn't really accurate, I have the feeling that the issue was not resolved when adding ragged tensor support to datasets.\r\n\r\nHere a short snippet that reproduces the issue with `tf 2.2.0-rc3`:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\ndef ragged_tensor_generator():\r\n    while True:\r\n        yield tf.ragged.constant([[1, 2], [1]], dtype=tf.int32)\r\n\r\n\r\nds = tf.data.Dataset.from_generator(\r\n    generator=ragged_tensor_generator,\r\n    output_types=tf.int32,\r\n    output_shapes=(2, None))\r\n\r\niterator = iter(ds)\r\nrecord = next(iterator)\r\n```\r\n\r\nAs mentioned [earlier](https://github.com/tensorflow/tensorflow/issues/27598#issuecomment-480566659), the numpy conversion fails which produces the following error:\r\n\r\n```\r\nTypeError: `generator` yielded an element that could not be converted to the expected type. The expected type was int32, but the yielded element was <tf.RaggedTensor [[1, 2], [1]]>.\r\n```\r\n", "@stekiri This is a bug with Dataset.from_generator.  #37400 is a PR to fix it, though I think it's been stalled for a little while on making sure it doesn't break a test.", "@bionicles If you want to get ragged tensors from sequence examples, you can use [tf.io.RaggedFeature](https://www.tensorflow.org/api_docs/python/tf/io/RaggedFeature).  E.g.:\r\n\r\n```\r\nsequence_features = {'my_feature': tf.io.RaggedFeature(dtype=tf.int64)}\r\nresult = tf.io.parse_sequence_example(\r\n    example_batch, sequence_features=sequence_features) \r\n```", "> @bionicles If you want to get ragged tensors from sequence examples, you can use [tf.io.RaggedFeature](https://www.tensorflow.org/api_docs/python/tf/io/RaggedFeature). E.g.:\r\n> \r\n> ```\r\n> sequence_features = {'my_feature': tf.io.RaggedFeature(dtype=tf.int64)}\r\n> result = tf.io.parse_sequence_example(\r\n>     example_batch, sequence_features=sequence_features) \r\n> ```\r\n\r\nHi @edloper ~\r\nDo you have any ideas about how to write a data generator outputting dict?\r\nhttps://github.com/tensorflow/tensorflow/issues/40425\r\n\r\nThanks!\r\n"]}, {"number": 27597, "title": "RuntimeError: Attempted to use a closed Session. with flask", "body": "I'm trying to run a Tacotron-2 training in flask, get parameters in json format. but the following error arises when I execute the training:\r\n\r\n```\r\nARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\nIf you depend on functionality not listed there, please file an issue.\r\nUsing TensorFlow backend.\r\n * Serving Flask app \"server\" (lazy loading)\r\n * Environment: production\r\n   WARNING: Do not use the development server in a production environment.\r\n   Use a production WSGI server instead.\r\n * Debug mode: on\r\n * Running on http://0.0.0.0:8891/ (Press CTRL+C to quit)\r\n * Restarting with stat\r\n/usr/local/lib/python3.6/dist-packages/numba/errors.py:105: UserWarning: Insufficiently recent colorama version found. Numba requires colorama >= 0.3.9\r\n  warnings.warn(msg)\r\nWARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\nIf you depend on functionality not listed there, please file an issue.\r\nUsing TensorFlow backend.\r\n * Debugger is active!\r\n * Debugger PIN: 303-138-053\r\nCheckpoint path: logs-Tacotron-2/taco_pretrained/tacotron_model.ckpt\r\nLoading training data from: tacotron-files/servex-tacotron/training_data/train.txt\r\nUsing model: Tacotron\r\nHyperparameters:\r\n  GL_on_GPU: True\r\n  NN_init: True\r\n  NN_scaler: 0.3\r\n  allow_clipping_in_normalization: True\r\n  attention_dim: 128\r\n  attention_filters: 32\r\n  attention_kernel: (31,)\r\n  attention_win_size: 7\r\n  batch_norm_position: after\r\n  cbhg_conv_channels: 128\r\n  cbhg_highway_units: 128\r\n  cbhg_highwaynet_layers: 4\r\netc...\r\nTacotron Parameters       29.039 Million.\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.cast instead.\r\ninitialisation done /gpu:0\r\nInitialized Tacotron model. Dimensions (? = dynamic shape): \r\n  Train mode:               False\r\n  Eval mode:                True\r\n  GTA mode:                 False\r\n  Synthesis mode:           False\r\n  Input:                    (?, ?)\r\n  device:                   0\r\n  embedding:                (?, ?, 512)\r\n  enc conv out:             (?, ?, 512)\r\n  encoder out:              (?, ?, 512)\r\n  decoder out:              (?, ?, 80)\r\n  residual out:             (?, ?, 512)\r\n  projected residual out:   (?, ?, 80)\r\n  mel out:                  (?, ?, 80)\r\n  linear out:               (?, ?, 1025)\r\n  <stop_token> out:         (?, ?)\r\n  Tacotron Parameters       29.039 Million.\r\nTacotron training set to a maximum of 200000 steps\r\nholaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\r\nLoading checkpoint logs-Tacotron-2/taco_pretrained/tacotron_model.ckpt-0\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\nExiting due to exception: '<' not supported between instances of 'int' and 'str'\r\nTraceback (most recent call last):\r\n  File \"/home/manuel_garcia02/Tacotron/tacotron/train.py\", line 224, in train\r\n    while not coord.should_stop() and step < args.tacotron_train_steps:\r\nTypeError: '<' not supported between instances of 'int' and 'str'\r\n186.179.100.228 - - [07/Apr/2019 02:52:15] \"POST /tts/train HTTP/1.1\" 200 -\r\nGenerated 15 test batches of size 32 in 0.891 sec\r\nGenerated 64 train batches of size 32 in 2.169 sec\r\nException in thread background:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib/python3.6/threading.py\", line 864, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/manuel_garcia02/Tacotron/tacotron/feeder.py\", line 169, in _enqueue_next_train_group\r\n    self._session.run(self._enqueue_op, feed_dict=feed_dict)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1075, in _run\r\n    raise RuntimeError('Attempted to use a closed Session.')\r\n\r\n\r\n\r\n```\r\nWhy can it be produced?\r\n---\r\n\r\n### Expected Behavior\r\n\r\nI should start training normally\r\nIf I execute the training without flask, this is executed normally\r\n\r\n### Environment\r\n\r\n* Python version: 3.6\r\n* Flask version: 1.0.2\r\n* Tensorflow version: 1.13.1", "comments": ["Looks like you are attempting to use a closed session. The Main Reason may be because you are wrapping the training in a context manager of Session. So when you are trying to access the session outside the Context Manager, the error is being raised.\r\nIn this kind of situations, it would be better not to use context managers, instead define a Session in a class variable, and close the session when flask is exiting.\r\n", "what a pity, I'm new to this tensorflow, the problem lies in the use of flask, when I run the training in the terminal, this is executed normally. but if I use flask, the problem of the closed session occurs.\r\n\r\nI do not know what the code should be attached to\r\n\r\n### train.py\r\n```\r\n\r\n\r\n@router_train.route(\"/tts/train\",methods=['POST'])\r\ndef tts_train():\r\n\ttry:\r\n\t\tglobal args\r\n\r\n\t\targs = easydict.EasyDict()\r\n\r\n\r\n\t\taccepted_models = ['Tacotron', 'WaveNet', 'Tacotron-2']\r\n\t\t\r\n                #here, the whole part of converting the parameters received in json and passing them to hparams goes. This part, I'm sure it works well.\r\n\r\n\t\tif args.model not in accepted_models:\r\n\t\t\traise ValueError('please enter a valid model to train: {}'.format(accepted_models))\r\n\r\n\r\n\t\tstart_train(args)\r\n\t\t\r\n\r\n\t\treturn jsonify(data)\r\n\texcept Exception as e:\r\n\t\tprint('Ocurrio un error: ' + str(e))\r\n\t\treturn jsonify(service='Ocurrio un error: ' + str(e))\r\n\r\n\r\n\r\ndef start_train(args):\r\n\taccepted_models = ['Tacotron', 'WaveNet', 'Tacotron-2']\r\n\tlog_dir, hparams = prepare_run(args)\r\n\r\n\tif args.model == 'Tacotron':\r\n\t\ttacotron_train(args, log_dir, hparams)\r\n\telif args.model == 'WaveNet':\r\n\t\twavenet_train(args, log_dir, hparams, args.wavenet_input)\r\n\telif args.model == 'Tacotron-2':\r\n\t\ttrain(args, log_dir, hparams)\r\n\telse:\r\n\t\traise ValueError('Model provided {} unknown! {}'.format(args.model, accepted_models))\r\n```\r\n\r\n### tacotron/train.py\r\n\r\n```\r\n\r\ndef train(log_dir, args, hparams):\r\n\tsave_dir = os.path.join(log_dir, 'taco_pretrained')\r\n\tplot_dir = os.path.join(log_dir, 'plots')\r\n\twav_dir = os.path.join(log_dir, 'wavs')\r\n\tmel_dir = os.path.join(log_dir, 'mel-spectrograms')\r\n\teval_dir = os.path.join(log_dir, 'eval-dir')\r\n\teval_plot_dir = os.path.join(eval_dir, 'plots')\r\n\teval_wav_dir = os.path.join(eval_dir, 'wavs')\r\n\ttensorboard_dir = os.path.join(log_dir, 'tacotron_events')\r\n\tmeta_folder = os.path.join(log_dir, 'metas')\r\n\tos.makedirs(save_dir, exist_ok=True)\r\n\tos.makedirs(plot_dir, exist_ok=True)\r\n\tos.makedirs(wav_dir, exist_ok=True)\r\n\tos.makedirs(mel_dir, exist_ok=True)\r\n\tos.makedirs(eval_dir, exist_ok=True)\r\n\tos.makedirs(eval_plot_dir, exist_ok=True)\r\n\tos.makedirs(eval_wav_dir, exist_ok=True)\r\n\tos.makedirs(tensorboard_dir, exist_ok=True)\r\n\tos.makedirs(meta_folder, exist_ok=True)\r\n\r\n\tcheckpoint_path = os.path.join(save_dir, 'tacotron_model.ckpt')\r\n\tinput_path = os.path.join(args.base_dir, args.tacotron_input)\r\n\r\n\tif hparams.predict_linear:\r\n\t\tlinear_dir = os.path.join(log_dir, 'linear-spectrograms')\r\n\t\tos.makedirs(linear_dir, exist_ok=True)\r\n\r\n\tlog('Checkpoint path: {}'.format(checkpoint_path))\r\n\tlog('Loading training data from: {}'.format(input_path))\r\n\tlog('Using model: {}'.format(args.model))\r\n\tlog(hparams_debug_string())\r\n\r\n\t#Start by setting a seed for repeatability\r\n\ttf.set_random_seed(hparams.tacotron_random_seed)\r\n\r\n\t#Set up data feeder\r\n\tcoord = tf.train.Coordinator()\r\n\twith tf.variable_scope('datafeeder') as scope:\r\n\t\tfeeder = Feeder(coord, input_path, hparams)\r\n\r\n\t#Set up model:\r\n\tglobal_step = tf.Variable(0, name='global_step', trainable=False)\r\n\tmodel, stats = model_train_mode(args, feeder, hparams, global_step)\r\n\teval_model = model_test_mode(args, feeder, hparams, global_step)\r\n\r\n\t#Embeddings metadata\r\n\tchar_embedding_meta = os.path.join(meta_folder, 'CharacterEmbeddings.tsv')\r\n\tif not os.path.isfile(char_embedding_meta):\r\n\t\twith open(char_embedding_meta, 'w', encoding='utf-8') as f:\r\n\t\t\tfor symbol in symbols:\r\n\t\t\t\tif symbol == ' ':\r\n\t\t\t\t\tsymbol = '\\\\s' #For visual purposes, swap space with \\s\r\n\r\n\t\t\t\tf.write('{}\\n'.format(symbol))\r\n\r\n\tchar_embedding_meta = char_embedding_meta.replace(log_dir, '..')\r\n\r\n\t#Potential Griffin-Lim GPU setup\r\n\tif hparams.GL_on_GPU:\r\n\t\tGLGPU_mel_inputs = tf.placeholder(tf.float32, (None, hparams.num_mels), name='GLGPU_mel_inputs')\r\n\t\tGLGPU_lin_inputs = tf.placeholder(tf.float32, (None, hparams.num_freq), name='GLGPU_lin_inputs')\r\n\r\n\t\tGLGPU_mel_outputs = audio.inv_mel_spectrogram_tensorflow(GLGPU_mel_inputs, hparams)\r\n\t\tGLGPU_lin_outputs = audio.inv_linear_spectrogram_tensorflow(GLGPU_lin_inputs, hparams)\r\n\r\n\t#Book keeping\r\n\tstep = 0\r\n\ttime_window = ValueWindow(100)\r\n\tloss_window = ValueWindow(100)\r\n\tsaver = tf.train.Saver(max_to_keep=20)\r\n\r\n\tlog('Tacotron training set to a maximum of {} steps'.format(args.tacotron_train_steps))\r\n\r\n\t#Memory allocation on the GPU as needed\r\n\tconfig = tf.ConfigProto()\r\n\tconfig.gpu_options.allow_growth = True\r\n\tconfig.allow_soft_placement = True\r\n\r\n\t#Train\r\n\twith tf.Session(config=config) as sess:\r\n\t\ttry:\r\n\t\t\tsummary_writer = tf.summary.FileWriter(tensorboard_dir, sess.graph)\r\n\t\t\tsess.run(tf.global_variables_initializer())\r\n\r\n\t\t\t#saved model restoring\r\n\t\t\tif args.restore:\r\n\t\t\t\t# Restore saved model if the user requested it, default = True\r\n\t\t\t\ttry:\r\n\t\t\t\t\tcheckpoint_state = tf.train.get_checkpoint_state(save_dir)\r\n\r\n\t\t\t\t\tif (checkpoint_state and checkpoint_state.model_checkpoint_path):\r\n\t\t\t\t\t\tlog('Loading checkpoint {}'.format(checkpoint_state.model_checkpoint_path), slack=True)\r\n\t\t\t\t\t\tsaver.restore(sess, checkpoint_state.model_checkpoint_path)\r\n\r\n\t\t\t\t\telse:\r\n\t\t\t\t\t\tlog('No model to load at {}'.format(save_dir), slack=True)\r\n\t\t\t\t\t\tsaver.save(sess, checkpoint_path, global_step=global_step)\r\n\r\n\t\t\t\texcept tf.errors.OutOfRangeError as e:\r\n\t\t\t\t\tlog('Cannot restore checkpoint: {}'.format(e), slack=True)\r\n\t\t\telse:\r\n\t\t\t\tlog('Starting new training!', slack=True)\r\n\t\t\t\tsaver.save(sess, checkpoint_path, global_step=global_step)\r\n\r\n\t\t\t#initializing feeder\r\n\t\t\tfeeder.start_threads(sess)\r\n\r\n\t\t\t#Training loop\r\n\t\t\twhile not coord.should_stop() and step < args.tacotron_train_steps:\r\n\t\t\t\tstart_time = time.time()\r\n\t\t\t\tstep, loss, opt = sess.run([global_step, model.loss, model.optimize])\r\n\t\t\t\ttime_window.append(time.time() - start_time)\r\n\t\t\t\tloss_window.append(loss)\r\n\t\t\t\tmessage = 'Step {:7d} [{:.3f} sec/step, loss={:.5f}, avg_loss={:.5f}]'.format(\r\n\t\t\t\t\tstep, time_window.average, loss, loss_window.average)\r\n\t\t\t\tlog(message, end='\\r', slack=(step % args.checkpoint_interval == 0))\r\n\r\n\t\t\t\tif np.isnan(loss) or loss > 100.:\r\n\t\t\t\t\tlog('Loss exploded to {:.5f} at step {}'.format(loss, step))\r\n\t\t\t\t\traise Exception('Loss exploded')\r\n\r\n\t\t\t\tif step % args.summary_interval == 0:\r\n\t\t\t\t\tlog('\\nWriting summary at step {}'.format(step))\r\n\t\t\t\t\tsummary_writer.add_summary(sess.run(stats), step)\r\n\r\n\t\t\t\tif step % args.eval_interval == 0:\r\n\t\t\t\t\t#Run eval and save eval stats\r\n\t\t\t\t\tlog('\\nRunning evaluation at step {}'.format(step))\r\n\r\n\t\t\t\t\teval_losses = []\r\n\t\t\t\t\tbefore_losses = []\r\n\t\t\t\t\tafter_losses = []\r\n\t\t\t\t\tstop_token_losses = []\r\n\t\t\t\t\tlinear_losses = []\r\n\t\t\t\t\tlinear_loss = None\r\n\r\n\t\t\t\t\tif hparams.predict_linear:\r\n\t\t\t\t\t\tfor i in tqdm(range(feeder.test_steps)):\r\n\t\t\t\t\t\t\teloss, before_loss, after_loss, stop_token_loss, linear_loss, mel_p, mel_t, t_len, align, lin_p, lin_t = sess.run([\r\n\t\t\t\t\t\t\t\teval_model.tower_loss[0], eval_model.tower_before_loss[0], eval_model.tower_after_loss[0],\r\n\t\t\t\t\t\t\t\teval_model.tower_stop_token_loss[0], eval_model.tower_linear_loss[0], eval_model.tower_mel_outputs[0][0],\r\n\t\t\t\t\t\t\t\teval_model.tower_mel_targets[0][0], eval_model.tower_targets_lengths[0][0],\r\n\t\t\t\t\t\t\t\teval_model.tower_alignments[0][0], eval_model.tower_linear_outputs[0][0],\r\n\t\t\t\t\t\t\t\teval_model.tower_linear_targets[0][0],\r\n\t\t\t\t\t\t\t\t])\r\n\t\t\t\t\t\t\teval_losses.append(eloss)\r\n\t\t\t\t\t\t\tbefore_losses.append(before_loss)\r\n\t\t\t\t\t\t\tafter_losses.append(after_loss)\r\n\t\t\t\t\t\t\tstop_token_losses.append(stop_token_loss)\r\n\t\t\t\t\t\t\tlinear_losses.append(linear_loss)\r\n\t\t\t\t\t\tlinear_loss = sum(linear_losses) / len(linear_losses)\r\n\r\n\t\t\t\t\t\tif hparams.GL_on_GPU:\r\n\t\t\t\t\t\t\twav = sess.run(GLGPU_lin_outputs, feed_dict={GLGPU_lin_inputs: lin_p})\r\n\t\t\t\t\t\t\twav = audio.inv_preemphasis(wav, hparams.preemphasis, hparams.preemphasize)\r\n\t\t\t\t\t\telse:\r\n\t\t\t\t\t\t\twav = audio.inv_linear_spectrogram(lin_p.T, hparams)\r\n\t\t\t\t\t\taudio.save_wav(wav, os.path.join(eval_wav_dir, 'step-{}-eval-wave-from-linear.wav'.format(step)), sr=hparams.sample_rate)\r\n\r\n\t\t\t\t\telse:\r\n\t\t\t\t\t\tfor i in tqdm(range(feeder.test_steps)):\r\n\t\t\t\t\t\t\teloss, before_loss, after_loss, stop_token_loss, mel_p, mel_t, t_len, align = sess.run([\r\n\t\t\t\t\t\t\t\teval_model.tower_loss[0], eval_model.tower_before_loss[0], eval_model.tower_after_loss[0],\r\n\t\t\t\t\t\t\t\teval_model.tower_stop_token_loss[0], eval_model.tower_mel_outputs[0][0], eval_model.tower_mel_targets[0][0],\r\n\t\t\t\t\t\t\t\teval_model.tower_targets_lengths[0][0], eval_model.tower_alignments[0][0]\r\n\t\t\t\t\t\t\t\t])\r\n\t\t\t\t\t\t\teval_losses.append(eloss)\r\n\t\t\t\t\t\t\tbefore_losses.append(before_loss)\r\n\t\t\t\t\t\t\tafter_losses.append(after_loss)\r\n\t\t\t\t\t\t\tstop_token_losses.append(stop_token_loss)\r\n\r\n\t\t\t\t\teval_loss = sum(eval_losses) / len(eval_losses)\r\n\t\t\t\t\tbefore_loss = sum(before_losses) / len(before_losses)\r\n\t\t\t\t\tafter_loss = sum(after_losses) / len(after_losses)\r\n\t\t\t\t\tstop_token_loss = sum(stop_token_losses) / len(stop_token_losses)\r\n\r\n\t\t\t\t\tlog('Saving eval log to {}..'.format(eval_dir))\r\n\t\t\t\t\t#Save some log to monitor model improvement on same unseen sequence\r\n\t\t\t\t\tif hparams.GL_on_GPU:\r\n\t\t\t\t\t\twav = sess.run(GLGPU_mel_outputs, feed_dict={GLGPU_mel_inputs: mel_p})\r\n\t\t\t\t\t\twav = audio.inv_preemphasis(wav, hparams.preemphasis, hparams.preemphasize)\r\n\t\t\t\t\telse:\r\n\t\t\t\t\t\twav = audio.inv_mel_spectrogram(mel_p.T, hparams)\r\n\t\t\t\t\taudio.save_wav(wav, os.path.join(eval_wav_dir, 'step-{}-eval-wave-from-mel.wav'.format(step)), sr=hparams.sample_rate)\r\n\r\n\t\t\t\t\tplot.plot_alignment(align, os.path.join(eval_plot_dir, 'step-{}-eval-align.png'.format(step)),\r\n\t\t\t\t\t\ttitle='{}, {}, step={}, loss={:.5f}'.format(args.model, time_string(), step, eval_loss),\r\n\t\t\t\t\t\tmax_len=t_len // hparams.outputs_per_step)\r\n\t\t\t\t\tplot.plot_spectrogram(mel_p, os.path.join(eval_plot_dir, 'step-{}-eval-mel-spectrogram.png'.format(step)),\r\n\t\t\t\t\t\ttitle='{}, {}, step={}, loss={:.5f}'.format(args.model, time_string(), step, eval_loss), target_spectrogram=mel_t,\r\n\t\t\t\t\t\tmax_len=t_len)\r\n\r\n\t\t\t\t\tif hparams.predict_linear:\r\n\t\t\t\t\t\tplot.plot_spectrogram(lin_p, os.path.join(eval_plot_dir, 'step-{}-eval-linear-spectrogram.png'.format(step)),\r\n\t\t\t\t\t\t\ttitle='{}, {}, step={}, loss={:.5f}'.format(args.model, time_string(), step, eval_loss), target_spectrogram=lin_t,\r\n\t\t\t\t\t\t\tmax_len=t_len, auto_aspect=True)\r\n\r\n\t\t\t\t\tlog('Eval loss for global step {}: {:.3f}'.format(step, eval_loss))\r\n\t\t\t\t\tlog('Writing eval summary!')\r\n\t\t\t\t\tadd_eval_stats(summary_writer, step, linear_loss, before_loss, after_loss, stop_token_loss, eval_loss)\r\n\r\n\r\n\t\t\t\tif step % args.checkpoint_interval == 0 or step == args.tacotron_train_steps or step == 300:\r\n\t\t\t\t\t#Save model and current global step\r\n\t\t\t\t\tsaver.save(sess, checkpoint_path, global_step=global_step)\r\n\r\n\t\t\t\t\tlog('\\nSaving alignment, Mel-Spectrograms and griffin-lim inverted waveform..')\r\n\t\t\t\t\tif hparams.predict_linear:\r\n\t\t\t\t\t\tinput_seq, mel_prediction, linear_prediction, alignment, target, target_length, linear_target = sess.run([\r\n\t\t\t\t\t\t\tmodel.tower_inputs[0][0],\r\n\t\t\t\t\t\t\tmodel.tower_mel_outputs[0][0],\r\n\t\t\t\t\t\t\tmodel.tower_linear_outputs[0][0],\r\n\t\t\t\t\t\t\tmodel.tower_alignments[0][0],\r\n\t\t\t\t\t\t\tmodel.tower_mel_targets[0][0],\r\n\t\t\t\t\t\t\tmodel.tower_targets_lengths[0][0],\r\n\t\t\t\t\t\t\tmodel.tower_linear_targets[0][0],\r\n\t\t\t\t\t\t\t])\r\n\r\n\t\t\t\t\t\t#save predicted linear spectrogram to disk (debug)\r\n\t\t\t\t\t\tlinear_filename = 'linear-prediction-step-{}.npy'.format(step)\r\n\t\t\t\t\t\tnp.save(os.path.join(linear_dir, linear_filename), linear_prediction.T, allow_pickle=False)\r\n\r\n\t\t\t\t\t\t#save griffin lim inverted wav for debug (linear -> wav)\r\n\t\t\t\t\t\tif hparams.GL_on_GPU:\r\n\t\t\t\t\t\t\twav = sess.run(GLGPU_lin_outputs, feed_dict={GLGPU_lin_inputs: linear_prediction})\r\n\t\t\t\t\t\t\twav = audio.inv_preemphasis(wav, hparams.preemphasis, hparams.preemphasize)\r\n\t\t\t\t\t\telse:\r\n\t\t\t\t\t\t\twav = audio.inv_linear_spectrogram(linear_prediction.T, hparams)\r\n\t\t\t\t\t\taudio.save_wav(wav, os.path.join(wav_dir, 'step-{}-wave-from-linear.wav'.format(step)), sr=hparams.sample_rate)\r\n\r\n\t\t\t\t\t\t#Save real and predicted linear-spectrogram plot to disk (control purposes)\r\n\t\t\t\t\t\tplot.plot_spectrogram(linear_prediction, os.path.join(plot_dir, 'step-{}-linear-spectrogram.png'.format(step)),\r\n\t\t\t\t\t\t\ttitle='{}, {}, step={}, loss={:.5f}'.format(args.model, time_string(), step, loss), target_spectrogram=linear_target,\r\n\t\t\t\t\t\t\tmax_len=target_length, auto_aspect=True)\r\n\r\n\t\t\t\t\telse:\r\n\t\t\t\t\t\tinput_seq, mel_prediction, alignment, target, target_length = sess.run([\r\n\t\t\t\t\t\t\tmodel.tower_inputs[0][0],\r\n\t\t\t\t\t\t\tmodel.tower_mel_outputs[0][0],\r\n\t\t\t\t\t\t\tmodel.tower_alignments[0][0],\r\n\t\t\t\t\t\t\tmodel.tower_mel_targets[0][0],\r\n\t\t\t\t\t\t\tmodel.tower_targets_lengths[0][0],\r\n\t\t\t\t\t\t\t])\r\n\r\n\t\t\t\t\t#save predicted mel spectrogram to disk (debug)\r\n\t\t\t\t\tmel_filename = 'mel-prediction-step-{}.npy'.format(step)\r\n\t\t\t\t\tnp.save(os.path.join(mel_dir, mel_filename), mel_prediction.T, allow_pickle=False)\r\n\r\n\t\t\t\t\t#save griffin lim inverted wav for debug (mel -> wav)\r\n\t\t\t\t\tif hparams.GL_on_GPU:\r\n\t\t\t\t\t\twav = sess.run(GLGPU_mel_outputs, feed_dict={GLGPU_mel_inputs: mel_prediction})\r\n\t\t\t\t\t\twav = audio.inv_preemphasis(wav, hparams.preemphasis, hparams.preemphasize)\r\n\t\t\t\t\telse:\r\n\t\t\t\t\t\twav = audio.inv_mel_spectrogram(mel_prediction.T, hparams)\r\n\t\t\t\t\taudio.save_wav(wav, os.path.join(wav_dir, 'step-{}-wave-from-mel.wav'.format(step)), sr=hparams.sample_rate)\r\n\r\n\t\t\t\t\t#save alignment plot to disk (control purposes)\r\n\t\t\t\t\tplot.plot_alignment(alignment, os.path.join(plot_dir, 'step-{}-align.png'.format(step)),\r\n\t\t\t\t\t\ttitle='{}, {}, step={}, loss={:.5f}'.format(args.model, time_string(), step, loss),\r\n\t\t\t\t\t\tmax_len=target_length // hparams.outputs_per_step)\r\n\t\t\t\t\t#save real and predicted mel-spectrogram plot to disk (control purposes)\r\n\t\t\t\t\tplot.plot_spectrogram(mel_prediction, os.path.join(plot_dir, 'step-{}-mel-spectrogram.png'.format(step)),\r\n\t\t\t\t\t\ttitle='{}, {}, step={}, loss={:.5f}'.format(args.model, time_string(), step, loss), target_spectrogram=target,\r\n\t\t\t\t\t\tmax_len=target_length)\r\n\t\t\t\t\tlog('Input at step {}: {}'.format(step, sequence_to_text(input_seq)))\r\n\r\n\t\t\t\tif step % args.embedding_interval == 0 or step == args.tacotron_train_steps or step == 1:\r\n\t\t\t\t\t#Get current checkpoint state\r\n\t\t\t\t\tcheckpoint_state = tf.train.get_checkpoint_state(save_dir)\r\n\r\n\t\t\t\t\t#Update Projector\r\n\t\t\t\t\tlog('\\nSaving Model Character Embeddings visualization..')\r\n\t\t\t\t\tadd_embedding_stats(summary_writer, [model.embedding_table.name], [char_embedding_meta], checkpoint_state.model_checkpoint_path)\r\n\t\t\t\t\tlog('Tacotron Character embeddings have been updated on tensorboard!')\r\n\r\n\t\t\tlog('Tacotron training complete after {} global steps!'.format(args.tacotron_train_steps), slack=True)\r\n\t\t\treturn save_dir\r\n\r\n\t\texcept Exception as e:\r\n\t\t\tlog('Exiting due to exception: {}'.format(e), slack=True)\r\n\t\t\ttraceback.print_exc()\r\n\t\t\tcoord.request_stop(e)\r\n\r\ndef tacotron_train(args, log_dir, hparams):\r\n\treturn train(log_dir, args, hparams)\r\n\r\n```\r\n\r\n### feeder.py\r\n\r\n```\r\n\r\n\tdef start_threads(self, session):\r\n\t\tself._session = session\r\n\t\tthread = threading.Thread(name='background', target=self._enqueue_next_train_group)\r\n\t\tthread.daemon = True #Thread will close when parent quits\r\n\t\tthread.start()\r\n\r\n\t\tthread = threading.Thread(name='background', target=self._enqueue_next_test_group)\r\n\t\tthread.daemon = True #Thread will close when parent quits\r\n\t\tthread.start()\r\n\r\n\t\r\n\tdef _enqueue_next_train_group(self):\r\n\t\twhile not self._coord.should_stop():\r\n\t\t\tstart = time.time()\r\n\r\n\t\t\t# Read a group of examples\r\n\t\t\tn = self._hparams.tacotron_batch_size\r\n\t\t\tr = self._hparams.outputs_per_step\r\n\t\t\texamples = [self._get_next_example() for i in range(n * _batches_per_group)]\r\n\r\n\t\t\t# Bucket examples based on similar output sequence length for efficiency\r\n\t\t\texamples.sort(key=lambda x: x[-1])\r\n\t\t\tbatches = [examples[i: i+n] for i in range(0, len(examples), n)]\r\n\t\t\tnp.random.shuffle(batches)\r\n\r\n\t\t\tlog('\\nGenerated {} train batches of size {} in {:.3f} sec'.format(len(batches), n, time.time() - start))\r\n\t\t\tfor batch in batches:\r\n\t\t\t\tfeed_dict = dict(zip(self._placeholders, self._prepare_batch(batch, r)))\r\n\t\t\t\tself._session.run(self._enqueue_op, feed_dict=feed_dict)\r\n\r\n\t\r\n```\r\n\r\nThe tour is the following:\r\n\r\ngoes to the function start_train in train.py, there goes to the function tacotron_train (tacotron_train is at the end of tacotron / train.py). Now go to the train function (it's in tacotron / train.py), then train (tacotron / train.py), in the code line: feeder.start_threads (sess), go to the function start_threads (it's in feeder.py) ) in start_threads goes to the function _enqueue_next_train_group (it's in feeder.py), that's when it cours the error. more exactly in:\r\n\r\n```\r\nfor batch in batches:\r\n         feed_dict = dict (zip (self._placeholders, self._prepare_batch (batch, r)))\r\n         self._session.run (self._enqueue_op, feed_dict = feed_dict)\r\n```\r\nwhen doing the session.run\r\n\r\n\r\nI hope you can help me.\r\n", "Please don't paste **complete** codes here", "I'm sorry, I already edited the comment and left the 6 main functions.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n"]}, {"number": 27596, "title": "Fix doc not displayed properly issue (block) in \"predict\" function of keras", "body": "This fix fixes the issue raised in #27583 where the doc\r\nin \"predict\" function of keras is not displayed properly. (full\r\nblock instead of list).\r\nThe reason of the issue seems to be casue by the unaligned\r\nfirst line: `x: Input samples.` should be prefixed by 4 spaces (not 5).\r\n\r\nThis fix fixes #27583 \r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 27595, "title": "Fix typo in python/keras", "body": "Fix some typos in python/keras.", "comments": []}, {"number": 27594, "title": "Completed TODO in file", "body": "Added various scenarios as asked in the TODO", "comments": ["Review pleasee?? @rthadur @angersson ", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 27593, "title": "ModuleNotFoundError: No module named 'tensorflow.python'", "body": "**System information**\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: tried \"conda install tensorflow\", \"pip install tensorflow\", and \"pip install https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.13.1-cp36-cp36m-win_amd64.whl\" \r\n- **TensorFlow version**: latest\r\n- **Python version**: 3.6\r\n- **Installed using virtualenv? pip? conda?**: tried pip and conda\r\n\r\nI am trying to run Tensorflow in an anaconda 3.6 environment. I've tried installing tensorflow using pip, conda, and pip with the link to the wheel. No matter the installation method when I try to import tensorflow to test the installation I receive the error:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\\"USERNAME\"\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\nModuleNotFoundError: No module named 'tensorflow.python'\r\n\r\nI've looked through the C:\\Users\\\"USERNAME\"\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow directory and cannot find any files named \"tensorflow.python\" nor can I find the \"pywrap_tensorflow\" file. \r\n\r\nAfter searching the issues tab I found thread #22300 with a similar problem. The admins suggested the user was missing a dll that contains the tensorflow.python file, but there was never a resolution. The user in thread #22300 is on linux while I am on windows 10. Is there a way to download the missing dlls or any other debugging steps I can take?", "comments": ["Hi Parker, have you tried the steps for Windows/Conda here? https://www.tensorflow.org/install/pip\r\n", "Billy,\r\nI\u2019ve tried all of the options listed on the tensorflow install docs. All produce the same \u201cno tensorflow.python\u201d module error. ", "Did you download and install the Microsoft Visual C++ 2015 Redistributable Update 3? Can you please confirm? ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27593\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27593\">No</a>\n", "It's correct.", "I'm facing same problem please give me solution this is embracing.\r\n"]}, {"number": 27592, "title": "Updated Doc: Change Shape Brackets and Code Formatting", "body": "Formatted:\r\n\r\n- \"shape\" brackets from '(' to '['\r\n- code for return value docs for `tf.nn.ctc_beam_search_decoder`.", "comments": ["@ymodak friendly reminder to have this PR reviewed. Thanks!"]}, {"number": 27591, "title": "Updated Doc: Change Formatting of Source Paper Link", "body": "Changed direct arxiv link to format with paper title and authors that looks like:\r\n\r\nSee Source [Batch Normalization: Accelerating Deep Network Training by Reducing \r\nInternal Covariate Shift, S. Ioffe, C. Szegedy](http://arxiv.org/abs/1502.03167).", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27591) for more info**.\n\n<!-- need_author_consent -->", "@BradHuang1999 please sign CLA", "@rthadur Already have. It seems that since I have integrated @kyscg\u2019s change, we will need his consent as well", "I've signed it. @rthadur Could you set CLA to yes on this.", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27591) for more info**.\n\n<!-- cla_yes -->", "@rthadur @gbaned Gentle ping to pull", "@BradHuang1999 can you please resolve conflicts.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27591) for more info**.\n\n<!-- need_author_consent -->", "Done.", "@rthadur It's done, gentle ping to set CLA's to yes and pull. Thankss", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27591) for more info**.\n\n<!-- cla_yes -->", "@rthadur It's been a while, could you pull this in, all changes have been approved"]}, {"number": 27590, "title": "Spelling fix for KL Divergence", "body": "Kullback-Leibler divergence should be a hyphenated name.", "comments": []}, {"number": 27589, "title": "update href tags", "body": "updating href tags", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27589) for more info**.\n\n<!-- need_sender_cla -->", "@n-vijaykarthik thanks for your contribution, please sign CLA.", "@n-vijaykarthik gentle ping to sign cla"]}]