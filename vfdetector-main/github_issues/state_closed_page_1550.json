[{"number": 6421, "title": "Add sequence_loss and sequence_loss_by_example working with dynamic_rnn without having to change outputs shape", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@lukaszkaiser LMK if you have cycles to handle this.", "Sorry, I do not. I think Thang Living is already working on it. I'll\nreassign when I get back to a laptop, or you do it. Thanks!\n\nLukasz\n\nOn Jan 11, 2017 15:32, \"drpngx\" <notifications@github.com> wrote:\n\n> @lukaszkaiser <https://github.com/lukaszkaiser> LMK if you have cycles to\n> handle this.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/6421#issuecomment-272029537>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AApzZd1QkpnCt3crpujRtOHBL5Ki6bwdks5rRWaMgaJpZM4LRzS5>\n> .\n>\n", "No worries, thanks!", "Hi @drpngx,\r\n\r\nEugene @ebrevdo is revising the dynamic_rnn_decoder and I believe he wants to hold off all developments involving dynamic_rnn_decoder for the moment.\r\n\r\nPS: while I'll try my best to provide as much inputs as I can, it's probably better to assign these requests to a software engineer for timely responses.\r\n\r\n-Thang\r\n\r\n", "OK, let's wait to hear back from @ebrevdo .", "@lmthang please pull rebase & push again.", "Sorry @chenghuige could you pull rebase and push again?", "@lmthang please review this.", "So this calculates the log perplexity?  why is it called sequence_loss and\nsequence_loss_by_example?\n\nOn Tue, Jan 31, 2017 at 1:58 PM, Rasmus Munk Larsen <\nnotifications@github.com> wrote:\n\n> @lmthang <https://github.com/lmthang> please review this.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/6421#issuecomment-276506091>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim728vl6gMBeNd5KD0dGaTXJC02vxks5rX66YgaJpZM4LRzS5>\n> .\n>\n", "It looks like there are conflicts -- @chenghuige can you address the conflicts and also @ebrevdo's most recent question?  Thanks!", "Haven't heard back in almost a month so I'll close for now -- please send a new PR when you get around to addressing the comments, thank you!", "Hi,\r\nI'm trying to use tf.contrib.seq2seq.sequence_loss with dynamic_decode output, but I always get tensor incompatibility error, because, the output of dynamic_decoder is variable, and input to sequence_loss is not. Basically, example code that raises this problem is.:\r\nThe code seems fine when I feed sequence length placeholders with fixed-sized length.\r\n\r\n    `with tf.device(\"/cpu:0\"), tf.name_scope(\"embeddings\"):\r\n            W_target_emb = tf.Variable(tf.random_uniform([target_vocab_size, size], -1.0, 1.0), name=\"W_target_emb\")\r\n            target_embedded_chars = tf.nn.embedding_lookup(W_target_emb, tf.stack(self.decoder_inputs))\r\n        with tf.name_scope(\"decoder\"):\r\n            half = tf.constant(0.5)\r\n            dec_inp = tf.cast(tf.stack(self.decoder_inputs), tf.float32)\r\n            if not forward_only:\r\n                helper = seq2seq.TrainingHelper(inputs = target_embedded_chars, sequence_length = self.decoder_seq_len, time_major=True)\r\n               \r\n\r\n            else:\r\n                helper = seq2seq.GreedyEmbeddingHelper(target_embedded_chars, \r\n                                                       start_tokens=self.decoder_inputs[0],\r\n                                                       end_token=data_utils.EOS_ID)\r\n\r\n        \r\n           \r\n            my_decoder = seq2seq.BasicDecoder(cell=MultiRNNCell([DeviceWrapper(ResidualWrapper(LSTMBlockCell(num_units=size)),\r\n                                                                               device='/gpu:%d' % i) for i in range(self.num_gpus) ]),\r\n                                              helper=helper,\r\n                                              initial_state=encoder_final_state)\r\n        \r\n            decoder_outputs, decoder_state = seq2seq.dynamic_decode(my_decoder, output_time_major=False, parallel_iterations=32,\r\n                       swap_memory = True)\r\n\r\n    with tf.name_scope(\"loss\"):\r\n        self.logits = tf.reshape(decoder_outputs.rnn_output, [self.batch_size, self.decoder_max_size, -1], name=\"logits_\")\r\n        self.sample_ids = decoder_outputs.sample_id\r\n        self.loss = loss.sequence_loss(self.logits, tf.transpose(tf.stack(targets), [1,0], name=\"targets_\"),\r\n                                                     tf.transpose(tf.stack(self.target_weights), [1,0], name=\"weights_\"),\r\n                                                     softmax_loss_function = softmax_loss_function)\r\n`\r\n"]}, {"number": 6420, "title": "\u540c\u6b65\u5b98\u7f51\u6e90\u7801 20161220", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "Can one of the admins verify this patch?"]}, {"number": 6419, "title": "I cannot generate the pip  wheel ", "body": "Hi,\r\nI am trying to compile tensorflow from scratch. I find the following problem. \r\nWhen I execute:\r\n```\r\nbazel   build  -c opt  --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nThe compilation goes ok.\r\nBut when I try to build the wheel from the program  : \r\n```\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\nTue Dec 20 11:47:44 CET 2016 : === Using tmpdir: /tmp/tmp.zISC44bEnF\r\ntensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles tensorflow\r\ntensorflow\r\n/tmp/tmp.zISC44bEnF tensorflow\r\nTue Dec 20 11:47:45 CET 2016 : === Building wheel\r\n/usr/lib/python2.7/distutils/dist.py:267: UserWarning: Unknown distribution option: 'install_requires'\r\n  warnings.warn(msg)\r\n/usr/lib/python2.7/distutils/dist.py:267: UserWarning: Unknown distribution option: 'tests_require'\r\n  warnings.warn(msg)\r\n/usr/lib/python2.7/distutils/dist.py:267: UserWarning: Unknown distribution option: 'zip_safe'\r\n  warnings.warn(msg)\r\n/usr/lib/python2.7/distutils/dist.py:267: UserWarning: Unknown distribution option: 'entry_points'\r\n  warnings.warn(msg)\r\n/usr/lib/python2.7/distutils/dist.py:267: UserWarning: Unknown distribution option: 'include_package_data'\r\n  warnings.warn(msg)\r\nerror: cannot copy tree 'build/scripts-2.7': not a directory\r\n``` \r\n\r\nCan anyone help ?\r\n\r\nThanks", "comments": ["Which version? I just tried it with 12rc1 successfully\r\n\r\n```\r\nexport HOME=/local_home/yaroslav\r\nmkdir -p ~/tensorflow_avx2.git\r\ncd ~/tensorflow_avx2.git\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\ngit tag -l\r\ngit checkout tags/0.12.0-rc1 -b 12rc1\r\n./configure\r\nexport flags=\"-c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --config=cuda\"\r\nbazel build $flags -k //tensorflow/tools/pip_package:build_pip_package\r\n....\r\nyaroslav@53:~/tensorflow_avx2.git/tensorflow$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\nTue Dec 20 10:16:10 PST 2016 : === Using tmpdir: /tmp/tmp.4FiW7g3OBw\r\n~/tensorflow_avx2.git/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles ~/tensorflow_avx2.git/tensorflow\r\n~/tensorflow_avx2.git/tensorflow\r\n/tmp/tmp.4FiW7g3OBw ~/tensorflow_avx2.git/tensorflow\r\nTue Dec 20 10:16:13 PST 2016 : === Building wheel\r\n~/tensorflow_avx2.git/tensorflow\r\nTue Dec 20 10:16:48 PST 2016 : === Output wheel file is in: /tmp/tensorflow_pkg\r\n\r\n```", "Also, what operating system version?  ", "Looks like this is an issue with pip installation:\r\nhttps://github.com/pypa/pip/issues/1117\r\n\r\n", "Hi,\r\nsorry for the delay.\r\nIt is ubuntu 16.10.\r\nI still have the problem.\r\n\r\n", "Actually just solved it. Gunan was right. it is a problem on the pip installation.\r\n\r\nTHANKS!"]}, {"number": 6418, "title": "Create examples/how_tos/__init__.py", "body": "To make modules under examples/how_tos executable from command line.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "Jenkins, test this please"]}, {"number": 6417, "title": "Saver can't handle filename only ", "body": "Hey everyone,\r\n\r\nit seems to me like - at least on Windows - the `tf` saver can't save model files whose path consists only of the file's name with no parent path, relative nor absolute. The issue lies at or around [saver.py:1363](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/saver.py#L1363) where it tries to check whether the parent directory of the file is actually a directory. There is no parent directory given (i.e. an empty string) and as such `gFile.IsDirectory` can't check anything. It fails and raises a ValueError that the parent directory does not exist. \r\n\r\nExpected behavior _in my opinion_ would be that the current working directory is used as the parent path when using no path/just a filename (i.e. a relative path). \r\n\r\n## Some details about my system specs:\r\n\r\n* Windows 10\r\n* Python 3.5.2\r\n* TF 0.12.0 (in a virtual environment; `pip install tensorflow --upgrade` just executed; issue persists)\r\n\r\n\r\nSo, I'm wondering whether this is an expected behavior and how to deal with it or if it is an actual bug that needs to be addressed.", "comments": ["This may be a known limitation. I think most people are using fully qualified paths in batch systems to avoid current directory surprises. @mrry, do you have any insight. @concretevitamin, do you have any insight?", "I think this isn't OS-specific, but agree that it's a bit strange. As a workaround, it might be possible to prepend `\"./\"` to the save path, and perhaps we should do that in the cases when `os.path.dirname()` returns `\"\"`?", "I think this is the usual and as such expectable behavior that when you don't provide an absolute/fully qualified path that it is treated as relative to the current working directory. So... yes, I think this should also be handled like this by TF. ", "Sounds fair to me. I'll defer to @concretevitamin on how best to fix that, since he wrote the most recent version of the `Saver` code.", "Awesome! Thanks a lot!", "@mrry or @concretevitamin Could you give a quick status update on this as soon as there's something new to mention? ", "That particular check was contributed from the community -- @PuchatekwSzortach, could you fix that?", "@HWiese1980 , @concretevitamin \r\nSomeone has spotted the same problem a week ago and I already fixed it - but the code is still awaiting review.\r\nHere's the original post that mentions the issue: https://github.com/tensorflow/tensorflow/pull/3695#r94327053\r\nAnd here's the pull request fixing it:\r\nhttps://github.com/tensorflow/tensorflow/pull/6601", "Great! I've got to wait for it to get included into the native Windows version, though... haven't managed yet to build TF on my Windows 10 from sources...", "@HWiese1980 In the meantime prepending \"./\" to filename should solve the issue. Also remember to do the same when loading the file - this is a problem I discovered while working on above issue and is also addressed in my recent pull request.", "`gfile.IsDirectory('')` returns True. @HWiese1980 are you sure this is the function that causes the issue?", "change the path to be exactly directory.\r\nFor example:\r\ncheckpoint_name = \"C:\\\\Users\\\\Minkun\\\\Desktop\\\\VTIS_Project\\\\model.ckpt\"\r\nnote that you used '\\\\' instead '\\'", "I am seeing the same issue as noted in the original post, in Tensorflow-gpu version 1.4.0 with Python 3.6.2, on Windows 7.\r\n\r\nThe restore function does not have a problem with just a string of a filename, but the save function can't handle it. If I put './filename.ckpt', as suggested by other poster, then the saver is fine.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "I recently updated TF on Ubuntu 16.04 to 1.5.0rc1 and now see the same \"problem\" there, again only with the saver (not the loader), when it wasn't an issue on that platform previously. I have to put \"./\" in order for the saver to work.\r\n\r\nCan someone please comment on this or close it if it's not going to be a thing that anyone cares to fix? Again, I'm talking specifically (both on Windows and Ubuntu) about the saver and that it will error unless you put \"./\" in front of the filename. Thanks!", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Yeah, we need to update saver.py. A simply way is just follow @mrry 's advice: prepend \"./\" to the save path in the cases when os.path.dirname() returns \"\".\r\n\r\nCommunity contributions are welcomed.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "@gunan It seems there was a good amount of activity on this over a year ago (Jan 2017), in particular in pull request 6601. But reading that PR, towards the bottom a problem was discovered and I think the changes were reverted. Then nothing since?\r\n\r\nSo is this something that likely isn't going to ever be \"fixed\" and we just need to plan on attaching './' to the front of file path string, for saver, going forward?", "Hi @tylerlekang,\r\nWe currently are working on other issues, but this issue staying open means it is worth fixing.\r\nAlso, the issue is marked contributions welcome, this means we do not have the time to look into this now, but we would appreciate any fixes.\r\nOne potential path forward is to pick up that PR and modify it with the last suggestion on the PR thread.", "@HWiese1980 ,\r\nWe see that you are using older version of tensorflow (1.x) which is not actively supported. We recommend that you upgrade to latest stable version of tensorflow 2.6.0 and let us know if the issue still persists in newer versions .Thanks!", "I think this issue has been resolved in the meantime. It's already been quite a while since I opened it, and I must confess, I totally forgot about it. Thanks for waking me up from my slumber again. :-) I guess, considering the last activity is from 2018, I can simply close it. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/6417\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/6417\">No</a>\n", "@google-ml-butler \r\n\r\nSeriously, can't tell... (I'm aware it's a bot)"]}, {"number": 6416, "title": "Tensorflow fault tolerance", "body": "Hello,\r\n\r\nI have been using the following simple code to learn about the behavior of distributed tensorflow (which claims to be fault tolerant):-\r\n\r\n  ```\r\nimport tensorflow as tf\r\ncluster = tf.train.ClusterSpec({\"local\": [\"localhost:2222\", \"localhost:2223\",\"localhost:2224\", \"localhost:2225\"]})\r\nx = tf.constant(2)\r\nwith tf.device(\"/job:local/task:1\"):\r\n    y1 = x + 300\r\n with tf.device(\"/job:local/task:2\"):\r\n    y2 = x**2\r\nwith tf.device(\"/job:local/task:3\"):\r\n    y3 = 5*x\r\nwith tf.device(\"/job:local/task:0\"):\r\n    y0 = x - 66\r\n    y = y0 + y1 + y2 + y3\r\n\r\nmodel = tf.initialize_all_variables()\r\n\r\nwith tf.Session(\"grpc://localhost:2222\") as sess:\r\n    sess.run(model)\r\n    print(sess.run(y0))\r\n    print('\\n')\r\n    print(sess.run(y1))\r\n    print('\\n')\r\n    print(sess.run(y2))\r\n    print('\\n')\r\n    print(sess.run(y3))\r\n    print('\\n')\r\n    result = sess.run(y)\r\n    print(result)\r\n```\r\n\r\nI create workers by running a separate program as given below:-\r\n\r\n```\r\n#get task number from command line\r\nimport sys\r\ntask_number = int(sys.argv[1])\r\n\r\nimport tensorflow as tf\r\n\r\ncluster = tf.train.ClusterSpec({\"local\": [\"localhost:2222\", \"localhost:2223\", \"localhost:2224\", \"localhost:2225\"]})\r\nserver = tf.train.Server(cluster, job_name=\"local\", task_index=task_number)\r\n\r\nprint(\"Starting server #{}\".format(task_number))\r\n\r\nserver.start()\r\nserver.join()\r\n\r\n```\r\nBut when I stop one of the servers(machines), the whole program stops functioning instead of assigning the job to another machine. Is tensorflow fault tolerant when a machine goes down? ", "comments": ["No, TensorFlow doesn't reschedule job when machine goes down, that's job of cluster manager.\r\n\r\nYou could specify your cluster using hostnames rather than IP numbers like this\r\n`cluster = tf.train.ClusterSpec({\"local\": [\"machine1:2222\", \"machine2:2223\"]})\r\n`\r\nThen when one machine1 goes down, it's job of cluster manager to spin up another instance, and update DNS records so that `machine1` points to that machine.\r\n\r\nWith current distributed implementation, resetting a connection will terminate your `session.run` call (like in https://github.com/tensorflow/tensorflow/issues/6319 which was due to AWS resetting idle connections), so you'll need to add extra logic to restart your `session.run`", "+1 to what @yaroslavvb said above. The [`tf.train.MonitoredSession`](https://www.tensorflow.org/api_docs/python/train/distributed_execution#MonitoredSession) class contains utilities that help with recovery after a failure.\r\n\r\nIn the current implementation, after a `tf.Session.run()` call fails due to a worker failure, the `tf.Session` object is no longer usable. The `tf.train.MonitoredSession` takes care of re-creating a new session and running code to restore from a checkpoint, if necessary.", "@mrry what if `.run` calls due to connection getting reset in the middle of `run` call like in https://github.com/tensorflow/tensorflow/issues/6319, can I still use the same Session object?", "The session may be in an undefined state at that point, so YMMV if you try to keep using it. The various wrapper libraries will re-create the session in those cases.", "I have written a very simple program but couldn't get the desired output. I have also [posted an issue](https://github.com/tensorflow/tensorflow/issues/6593) on github.\r\n\r\nCould you please let me know if I am going the right way or atleast redirect me towards a tutorial to better understand the usage of monitored session? ", "Using DNS isn't ideal, given ports might be taken after a reboot or after failed workers are assigned to new machines. Plus, DNS introduced another dependency.\r\n\r\nI am wondering has anyone tried to use grpc name resolvers with services such as zookeeper?"]}, {"number": 6415, "title": "Merging inception retrained models", "body": "Hi,\r\n\r\nI know this might not be the place to ask this but I did not get any replies nor comments on SO so I thought to give it a shot.\r\n\r\nI have done my own training of tensorflow inception a while ago, recently I wanted to add new classes to the original model. However, when I retrain tensorflow it generates a new graph and labels files completely separate from the older one.\r\n\r\nIs there a way to combine models? Or do I have to retrain again with the old and new classes?\r\n", "comments": ["@shlens, do you have any suggestions?", "There is no easy solution to this but it is feasible. There would be several manual steps:\r\n1. Build a new Inception model.\r\n2. Load all of the Tensor's except for the last layer with the `Saver`\r\n3. For the final classification layer, selectively load Tensor's for the classes you wish to preserve or use in your new model making sure to associate each Tensor with the desired class index. For the other classes, allow the model to use randomly initialized weights.\r\n4. Retrain the model probably starting from a lower learning rate.\r\n", "Automatically closing due to lack of recent activity. We hope that you were able to resolve it on your own. However, since this is a support issue rather than a bug or feature request, you will probably get more information by posting it on StackOverflow."]}, {"number": 6414, "title": "Feature suggestion: Loss Normalization", "body": "I would like to suggest the following feature be added to TensorFlow. As far as I know, the feature does not exist in TensorFlow. There are some issues that must be discussed and solved before making the pull-request.\r\n\r\n### Motivation\r\n\r\nWhen optimizing multiple objectives in TensorFlow, the combined loss-function is typically a weighted sum of the individual loss-functions. Finding the weights for the loss-functions is done experimentally as the weights may depend on a number of factors.\r\n\r\nFor example, in Style Transfer we want to combine two images so the result has the 'style' of one input image and the 'content' of another image. We do this by creating two loss-functions that we will optimize together. The problem is that when we choose different layers in the neural network for the loss-functions, then their magnitudes may change dramatically, so the weights for the loss-functions will also need to be modified.\r\n\r\nThe solution I came up with in my recent tutorial on Style Transfer, is to automatically normalize each loss-function so we can define the weights independently of any given choice of layers:\r\n\r\nhttps://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/15_Style_Transfer.ipynb\r\n\r\nThe solution I propose below is a bit more elegant than in the tutorial, and I think it can be made even better.\r\n\r\n### Overall Idea\r\n\r\nThe idea is to evaluate each loss-function to its scalar-value and then divide the loss-function with this scalar-value. This creates a new node in the TensorFlow graph which holds the original loss-function divided by some scalar-value. The new node in the graph is still a mathematical function that can be differentiated, and for the given input it evaluates to the value 1. We do this normalization for each of the loss-functions so they all evaluate to 1. Then we can define loss-weights more easily and independently of e.g. the exact choice of layers in the Style Transfer algorithm.\r\n\r\n### Code\r\n\r\nIn terms of pseudo-code, what we want to do is simply:\r\n\r\n    def loss_normalize(loss):\r\n        return loss / value_of(loss)\r\n\r\nBut if we merely return `loss/loss` then it does not calculate the value of `loss` for the denominator, instead we get a mathematical function divided by itself. I also tried `tf.identity(loss)` but that did not work either. Neither does type-casting.\r\n\r\nSo we need a small hack. We create a scalar float-variable and assign the loss-value to it. When the variable is used in an assignment like this, then it is not necessary to initialize it first. This works:\r\n\r\n    def loss_normalize(loss):\r\n        loss_value = tf.Variable(1.0)\r\n        \r\n        loss_normalized = loss / loss_value.assign(loss)\r\n\r\n        return loss_normalized\r\n\r\n### Usage\r\n\r\nImagine that we have defined two loss functions, e.g.:\r\n\r\n    style_loss = ...\r\n    content_loss = ...\r\n\r\nNormally we would have some strange weights that might need to be changed if we choose different layers in the network for the loss-functions. These weights do not indicate how we are actually weighting the loss-functions for the style and content, for example:\r\n\r\n    combined_loss = 1e-10 * style_loss + 1e-3 * content_loss\r\n\r\nIf instead we normalize the loss-functions first, then we can define the weights more intuitively. For example, we might want 90% of the combined-loss to be for the style and only 10% for the content. We would do it like this:\r\n\r\n    combined_loss = 0.9 * loss_normalize(style_loss) + \\\r\n                    0.1 * loss_normalize(content_loss)\r\n\r\nEven if you change the layers used in the two loss-functions, we do not need to change our weights. This may be useful in many other applications than just Style Transfer.\r\n\r\n### Issues\r\n\r\nBefore I make a pull-request there are several issues that must be discussed and solved.\r\n\r\n1) The hack using a variable to get the loss-value is not very elegant. I wonder if it would be possible to add something like `value_of(loss)` to TensorFlow? I guess it would be a bit similar to `loss.eval()` except that we don't want to run the session.\r\n\r\n2) Division by zero should be handled somehow. If the loss-function is inherently non-negative then we can add a small number such as 1e-10 to the denominator. But this would not be bomb-proof for loss-functions that can take on negative values. Is there a good standard solution for this in TensorFlow?\r\n\r\n3) It might be a better design to make `loss_normalize()` take a list of loss-functions so we don't have to wrap all of the loss-functions individually. What do you think? Is there an elegant way of implementing this in TensorFlow, or do I have to resort to Python's list comprehension?\r\n\r\n4) It might be useful to have a placeholder bool that decides whether or not to update the normalization. This would allow us to normalize the loss-functions only in the first iteration, or we could normalize every n'th iteration. The reason this might be useful, is that the optimizer might get trapped in a local optimum if we normalize the loss-functions in every iteration. I experimented a bit with `tf.placeholder_with_default()` and `tf.cond()` but it gave me some strange errors about tensor-shapes. Any ideas on how to make an elegant implementation for this?\r\n", "comments": ["Seems like learn might be a good place for this? Could you two @ilblackdragon  and @martinwicke  comment on your opinion of the feature. Thanks!", "@Hvass-Labs You current implementation of `loss_normalize` will always return 1.0 if losses are already scalars [which they usually are after calling for example `tf.losses.*`]:\r\n```\r\nIn [8]: def loss_normalize(loss):\r\n        loss_value = tf.Variable(1.0)\r\n        return loss / loss_value.assign(loss)\r\n\r\nIn [10]: with tf.Session() as sess:\r\n    print(sess.run(loss_normalize(1.5)))\r\n\r\n1.0\r\n```\r\n\r\nIf you are just trying to normalize for example a vector or matrix of raw errors [for example `tf.abs(a - b)` part of `tf.reduce_mean(tf.abs(a - b))` - the absolute mean error loss], you can use one of https://www.tensorflow.org/api_docs/python/nn/normalization tools.  \r\n\r\nIf I understood your Ipython Notebook - you are suggesting to use normalization by dividing on previous value of this tensor. You can add it to the tools in the Normalization list - this doesn't need to be loss specific. To perform updates at correct time add the assign op to `GraphKeys.UPDATE_OPS`, which will be auto-added to list of ran tensors by `optimize_loss`. \r\n\r\nAlso consider using tf.nn.moments and tf.nn.normalize_moments as it may be a better estimation of what you are trying to do.\r\n\r\nLet me know if this makes sense to you!", "Thanks for the detailed comments.\r\n\r\nThe Notebook for my Tutorial 15 has a slightly awkward implementation of this idea. The implementation I wrote above is cleaner.\r\n\r\nHowever, from your comments I can see that the idea is still not perfectly clear. I think it would help if I write a Python Notebook to demonstrate it, and then perhaps you can give suggestions on where it might belong in TensorFlow and possibly how to improve the implementation.\r\n\r\nBut I'm currently busy doing some research related to TensorFlow, so I'll have to come back to this thread later. It will probably be several weeks or more.\r\n", "I'm sorry for the delay, I was doing some research that dragged out.\r\n\r\nWhen I proposed this feature I thought it might be useful in many other scenarios. But after more consideration, I don't think the idea is sufficiently general and mature at this stage to be included in TensorFlow core.\r\n\r\nI have made a Python Notebook which demonstrates the idea I had in mind:\r\n\r\nhttps://gist.github.com/Hvass-Labs/3744f05fa05d6ef82dc873c4c32737ef\r\n\r\nTry switching the bool `use_loss_normalization` and see what happens to the resulting image at the bottom. However, this is not the best example because the two loss-functions are fairly close in value.\r\n\r\nThis type of loss-normalization would work better for loss-functions that are several orders of magnitude in difference, where one loss-function would tend to dominate, and whenever you change something in the configuration of the problem, the relative values of the loss-functions would change so you also needed to change the weights. In such cases, automatic loss-normalization would be useful.\r\n\r\nPerhaps someone in the future might find the idea useful and could build on this. This type of loss-normalization might make a nice little research paper if someone is interested in doing it (I don't have time and I have many other ideas to work on).\r\n"]}, {"number": 6413, "title": "Running Tests in Docker (on Mac) Fails with Cryptic Download Error", "body": "I am currently trying to run the Docker tests on my machine (a Mac).\r\n\r\nBuilding the container succeeds, but then when it builds TensorFlow after `./configure`, it fails with the following error:\r\n\r\n```\r\nERROR: /workspace/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@numericjs_numeric_min_js//file': Error downloading [https://cdnjs.cloudflare.com/ajax/libs/numeric/1.2.6/numeric.min.js] to /Users/gibiansky/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_gibiansky/eab0d61a99b6696edb3d2aff87b585e8/external/numericjs_numeric_min_js/numeric.min.js: sun.security.validator.ValidatorException: PKIX path validation failed: java.security.cert.CertPathValidatorException: timestamp check failed and referenced by '//tensorflow/tensorboard/bower:bower'.\r\n```\r\n\r\nThe error is quite consistent and from my machine I can `wget` that `numeric.min.js` URL.\r\n\r\nHave you see this? What does `java.security.cert.CertPathValidatorException` even mean?\r\n\r\nThanks! ", "comments": ["Ah, and by the way, it is always with `numeric.min.js`, it does not change which package it is if I invoke the `ci_build.sh` command multiple times.", "My apologies, the issue was entirely on my end.\r\n\r\n(For anyone who ends up on this thread: This is caused by the time inside the container being out of sync with the realtime. You can verify this by running `date` within the container and outside of the container; if they are significantly different, this is likely the cause of the error I mentioned above. Fix this by restarting the virtual machine running Docker. If you are using Docker for Mac, click on the little whale icon and then click \"restart\".)"]}, {"number": 6412, "title": "contrib\\cmake: configure step errors", "body": "Config & commands used: https://gist.github.com/derofim/b4f150da1269b81af8d12744df730708\r\n\r\n`cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release  -DSWIG_EXECUTABLE=\"D:/Tensorflow/swigwin-3.0.10/swig.exe\" -DPYTHON_EXECUTABLE=\"C:/Program Files/Anaconda3/python.exe\" -DPYTHON_LIBRARIES=\"C:/Program Files/Anaconda3/libs/python35.lib\" -Dtensorflow_ENABLE_GPU=ON -DCUDNN_HOME=\"D:\\Tensorflow\\cudnn\\cuda\" -Dtensorflow_BUILD_CC_EXAMPLE=ON`\r\n\r\nErrors:\r\n**CMake Error: CMake can not determine linker language for target: tf_losses\r\nCMake Error: CMake can not determine linker language for target: tf_models_word2vec_ops\r\nCMake Error: CMake can not determine linker language for target: tf_models_word2vec_kernels**\r\n\r\nFull log:\r\n`D:\\Tensorflow\\tensorflow-master\\tensorflow\\contrib\\cmake\\build>cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release  -DSWIG_EXECUTABLE=\"D:/Tensorflow/swigwin-3.0.10/swig.exe\" -DPYTHON_EXECUTABLE=\"C:/Program Files/Anaconda3/python.exe\" -DPYTHON_LIBRARIES=\"C:/Program Files/Anaconda3/libs/python35.lib\" -Dtensorflow_ENABLE_GPU=ON -DCUDNN_HOME=\"D:\\Tensorflow\\cudnn\\cuda\" -Dtensorflow_BUILD_CC_EXAMPLE=ON\r\n-- Building for: Visual Studio 14 2015\r\n-- The C compiler identification is MSVC 19.0.24215.1\r\n-- The CXX compiler identification is MSVC 19.0.24215.1\r\n-- Check for working C compiler: D:/Soft/MicrosoftVisualStudio14/VC/bin/x86_amd64/cl.exe\r\n-- Check for working C compiler: D:/Soft/MicrosoftVisualStudio14/VC/bin/x86_amd64/cl.exe -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Check for working CXX compiler: D:/Soft/MicrosoftVisualStudio14/VC/bin/x86_amd64/cl.exe\r\n-- Check for working CXX compiler: D:/Soft/MicrosoftVisualStudio14/VC/bin/x86_amd64/cl.exe -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Found CUDA: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0 (found suitable version \"8.0\", minimum required is \"8.0\")\r\nYou have called ADD_LIBRARY for library tf_models_word2vec_ops without any source files. This typically indicates a problem with your CMakeLists.txt file\r\nYou have called ADD_LIBRARY for library tf_models_word2vec_kernels without any source files. This typically indicates a problem with your CMakeLists.txt file\r\nYou have called ADD_LIBRARY for library tf_losses without any source files. This typically indicates a problem with your CMakeLists.txt file\r\n-- Found PythonInterp: C:/Program Files/Anaconda3/python.exe (found version \"3.5.2\")\r\n-- Found PythonLibs: C:/Program Files/Anaconda3/libs/python35.lib (found version \"3.5.2\")\r\n-- Found SWIG: D:/Tensorflow/swigwin-3.0.10/swig.exe (found version \"3.0.10\")\r\n-- Configuring done\r\nCMake Error: CMake can not determine linker language for target: tf_losses\r\nCMake Error: CMake can not determine linker language for target: tf_models_word2vec_ops\r\nCMake Error: CMake can not determine linker language for target: tf_models_word2vec_kernels\r\n-- Generating done\r\n-- Build files have been written to: D:/Tensorflow/tensorflow-master/tensorflow/contrib/cmake/build`\r\n\r\nP.S.\r\nFor now im ignoring build errors and tf_label_image_example works fine! (But cannot link in side project https://github.com/tensorflow/tensorflow/issues/6382)\r\n\r\nWorkaround im using:\r\n\r\n```\r\nFor now tf_label_image_example works fine in generated .sln!\r\n\r\nTo stop rebuilding whole tensorflow each time i used workaround\r\n\"Configuration Manager\". Un-check the \"Build\" for all except tf_label_image_example as in http://stackoverflow.com/a/1433680\r\n\r\nAlso in contrib\\cmake\\build i created folders tensorflow/examples/label_image/data/ and placed in it:\r\ngrace_hopper.jpg\r\nimagenet_comp_graph_label_strings.txt\r\ntensorflow_inception_graph.pb\r\n\r\nData may be found at https://storage.googleapis.com/download.tensorflow.org/models/inception_dec_2015.zip in tutorial https://www.tensorflow.org/how_tos/tool_developers/\r\n```", "comments": ["@derofim Please try pulling the latest master. #6380 should have fixed the issues you encountered.", "Tried pulling the latest master. Works fine."]}, {"number": 6411, "title": "Testing in python - import tensorflow as tf throws error \"Exception: Versioning for this project requires either an sdist tarball\"", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nSome of the issues when searching for \"import tensorflow as tf\" gave answers related to either mock not being installed correctly or protobuf.  In my case, all were installed correctly and successfully.\r\n \r\n### Environment info\r\nOperating System:\r\nAmazon Linux\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\nI installed a CPU only version.  So no CUDA\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nthe installation completed successfully.  Here is the output:\r\n----------\r\nsudo -H /usr/local/bin/pip install tensorflow\r\nRequirement already satisfied: tensorflow in /usr/local/lib64/python2.7/site-packages\r\nRequirement already satisfied: numpy>=1.11.0 in /usr/local/lib64/python2.7/site-packages (from tensorflow)\r\nRequirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/site-packages (from tensorflow)\r\nRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/site-packages (from tensorflow)\r\nRequirement already satisfied: protobuf==3.1.0 in /usr/local/lib/python2.7/site-packages (from tensorflow)\r\nRequirement already satisfied: wheel in /usr/local/lib/python2.7/site-packages (from tensorflow)\r\nRequirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow)\r\nRequirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow)\r\nRequirement already satisfied: setuptools in /usr/local/lib/python2.7/site-packages/setuptools-28.3.0-py2.7.egg (from protobuf==3.1.0->tensorflow)\r\n--------\r\n### What other attempted solutions have you tried?\r\nInstalled each of the dependencies individually.  Still same issue.\r\n\r\n### Logs or other output that would be helpful\r\n----------\r\n python -c \"import tensorflow; print(tensorflow.__version__)\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 106, in <module>\r\n    from tensorflow.python.platform import test\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/platform/test.py\", line 81, in <module>\r\n    import mock                # pylint: disable=g-import-not-at-top,unused-import\r\n  File \"/usr/local/lib/python2.7/site-packages/mock/__init__.py\", line 2, in <module>\r\n    import mock.mock as _mock\r\n  File \"/usr/local/lib/python2.7/site-packages/mock/mock.py\", line 71, in <module>\r\n    _v = VersionInfo('mock').semantic_version()\r\n  File \"/usr/local/lib/python2.7/site-packages/pbr/version.py\", line 460, in semantic_version\r\n    self._semantic = self._get_version_from_pkg_resources()\r\n  File \"/usr/local/lib/python2.7/site-packages/pbr/version.py\", line 447, in _get_version_from_pkg_resources\r\n    result_string = packaging.get_version(self.package)\r\n  File \"/usr/local/lib/python2.7/site-packages/pbr/packaging.py\", line 725, in get_version\r\n    raise Exception(\"Versioning for this project requires either an sdist\"\r\nException: Versioning for this project requires either an sdist tarball, or access to an upstream git repository. Are you sure that git is installed?\r\n-----------------\r\n", "comments": ["Ok, this was resolved.  The only thing I had to do was to upgrade the distribute module - \"pip install --upgrade distribute\"."]}, {"number": 6410, "title": "Fix broken links for examples", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please"]}, {"number": 6409, "title": "Merge r0.12 back to master", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Merge PR, CLA ok.\r\n\r\nPlease wait for all the tests before merging."]}, {"number": 6408, "title": "Revised 'Added FAQ for cannot connect to Docker daemon'", "body": "Linked to popular SO answer.", "comments": ["Can one of the admins verify this patch?", "Note that you can edit the existing pull request but adding commits to the branch :)"]}, {"number": 6407, "title": "Update cudnn version in instruction for mac gpu.", "body": "", "comments": []}, {"number": 6406, "title": "Added FAQ for cannot connect to Docker daemon", "body": "Linked to popular SO answer.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please"]}, {"number": 6405, "title": "tf.split_v() is removed in master?", "body": "A bit of history:\r\n\r\n- Until <= 0.12, [`tf.split()`](https://www.tensorflow.org/api_docs/python/array_ops/slicing_and_joining#split) has the following signature:\r\n```tf.split(split_dim, num_split, value, name='split')```\r\n- In commit f803bd7c (since 0.12), a new op `split_v()` is introduced:\r\n```tf.split_v(value, size_splits, split_dim=0, num=None, name=\"split_v\")```\r\n- In commit a46b6d211 (master), `tf.split_v()` is finally **renamed** to `tf.split()`:\r\n  ```tf.split(value, num_or_size_splits, axis=0, num=None, name=\"split\")```\r\n\r\nDue to these changes, the signature of `tf.split()` has been changed. AFAIK TensorFlow will having some of breaking changes after 0.12 (after the end of 2016); is this backward-incompatible API change intended?\r\n\r\nIf so, **my suggestion is** that `tf.split_v()`, which has been introduced in 0.12, should not be removed in the newer versions as well. In the current master, `tf.split_v` is non-existent.\r\n```\r\nAttributeError: 'module' object has no attribute 'split_v'\r\n```\r\n\r\nI am reporthing this (minor) issue because I am frequently switching the tensorflow versions, from r0.12 (stable branch) to master (the breaking? future), and thus I need a way to write a code that is **both compatible** in those two versions. However, due to the change of `tf.split()`, it seems that I cannot achieve it at the moment.\r\n\r\nThanks,", "comments": ["We are making several backward incompatible changes. The final state is what you are seeing in the latest master branch where split_v() is removed and split() exists with the keyword arguments \r\n```python\r\ntf.split(value, num_or_size_splits, axis=0, num=None, name=\"split\")\r\n```\r\nTo switch between versions, you will need to creat a local compatiblity wrapper that keys off of `tf.__version__` or uses exceptions to try the newest form and if the keyword arguments don't match use the previous form (this is probably easier). Hope this helps.\r\n\r\n\r\n\r\n", "Thanks for the feedback. I was assuming backward-incompatible changes, but also was confused since I could find there is a similar change on `tf.concat()`: in r0.12 we have both `concat_v2()` and `concat()` (I expect the v2 method will replace the original soon).\r\n\r\nHowever, if the changes are intended, perhaps we might not need to add a method such as `tf.split_v()` because it eventually would be a legacy, though the changes seem quite radical. For now, it would be better for me to try alternative approaches as you suggested.", "Automatically closing due to lack of recent activity. We hope that you were able to resolve it on your own. However, since this is a support issue rather than a bug or feature request, you will probably get more information by posting it on StackOverflow.", "Yeah, I think this is not necessary any more since it has been a while after TF 1.0 came out."]}, {"number": 6404, "title": "Confusion about Cuda install docs for Mac", "body": "### Environment info\r\nOperating System: Mac OSX 10.11\r\n\r\nThe docs have 2 issues related to this section at the top:\r\n\r\n\"The GPU version works best with Cuda Toolkit 8.0 and cuDNN v5. Other versions are supported (Cuda toolkit >= 7.0 and cuDNN >= v3) only when installing from sources. Please see Cuda installation for details. For Mac OS X, please see Setup GPU for Mac.\"\r\n\r\n1) The link to \"Setup GPU for Mac\" doesn't work; presumably the href target is missing or misspelled.\r\n\r\n2) The Setup GPU for Mac section is included in the \"Install Tensorflow from Source\" part of the docs. Several people I know interpreted that to mean that GPU support is only available when installing from source. Furthermore, in the same section is a requirement to use homebrew, and many people I know refuse to use homebrew, so they assumed that Tensorflow was unusable on mac for them. \r\n\r\nI recommend separating out the Setup GPU for Mac instructions to a separate page, unless it truly is dependent on installing from source. If it is, that should also be clarified. \r\n", "comments": ["The link problem is the same as #6389", "Part 1 of this issue has been fixed. ", "Part 2 of this has also been fixed, see:\r\n\r\nhttps://www.tensorflow.org/install/install_mac"]}, {"number": 6403, "title": "Improved OpenCL support", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 6402, "title": "Please provide protobuf 3.1.0 download", "body": "Since #4572, TensorFlow depends on protobuf==3.1.0.\r\n\r\n[This section](https://github.com/kashif/tensorflow/blob/f2e17755e9466ff3c3f8a5806264d3baccd0f74e/tensorflow/g3doc/get_started/os_setup.md#protobuf-library-related-issues) of the installation instructions continues to point to a wheel for version 3.0.0, which doesn't help at all.\r\n\r\nTherefore: please provide pre-built wheels for protobuf 3.1.0.\r\n\r\nAlso, those instructions tell me that I should only install the protobuf package _after_ installing TensorFlow via pip.  But this isn't currently helpful advice: the TensorFlow package won't install at all, because there is no protobuf==3.1.0:\r\n\r\n```\r\nDownloading/unpacking protobuf==3.1.0 (from tensorflow==0.12.0rc1)\r\n  Could not find a version that satisfies the requirement protobuf==3.1.0 (from tensorflow==0.12.0rc1) (from versions: 3.0.0b4, 3.0.0, 3.0.0b2.post2, 3.0.0a2,\r\n3.0.0b2, 2.6.1, 2.0.3, 2.0.0beta, 2.5.0, 2.4.1, 2.6.0, 3.0.0b2.post1, 3.0.0b3, 3.0.0b1.post2, 3.0.0b2.post2, 3.0.0b2.post1, 2.3.0, 3.0.0a3, 3.1.0.post1)\r\nCleaning up...\r\nNo distributions matching the version for protobuf==3.1.0 (from tensorflow==0.12.0rc1)\r\n```\r\n\r\nI think what you're trying to say is that if I had been able to succeed in installing TensorFlow then I ought anyway to overwrite whatever protobuf pip had selected.  If so I'd suggest that this belongs higher up in the instructions, and not in some sort of trouble-shooting afterthought section that would easily be missed.", "comments": ["Related StackOverflow question: http://stackoverflow.com/questions/40989168/protobuf-issue-during-tensorflow-installation\r\n\r\n(but it's just someone hitting the same problem, and getting an unhelpful answer).", "@jhseu I thought the ones we built were 3.1.0?\r\nLooks like we need to push new protobuf packages out?\r\n\r\n@yifeif FYI", "Fixed now.", "BTW, running `python setup.py develop` on 0.12 tag fails with the following\r\n\r\n```\r\nProcessing dependencies for tensorflow==0.12.0\r\nSearching for protobuf==3.1.0\r\nReading https://pypi.python.org/simple/protobuf/\r\nNo local packages or working download links found for protobuf==3.1.0\r\nerror: Could not find suitable distribution for Requirement.parse('protobuf==3.1.0')\r\n\r\n```\r\nWorkaround it to install `protobuf-3.1.0-py2.py3-none-any.whl` manually:\r\n`pip install --upgrade https://pypi.python.org/packages/b2/30/ab593c6ae73b45a5ef0b0af24908e8aec27f79efcda2e64a3df7af0b92a2/protobuf-3.1.0-py2.py3-none-any.whl#md5=f02742e46128f1e0655b44c33d8c9718\r\n`", "Ah, you are right.\r\nThe reason is, pypi only has 3.1.0post1, or 3.0.0\r\nhttps://pypi.python.org/pypi/protobuf\r\n@jhseu Which one makes more sense for us to depend on?", "It'll work if you patch in 0b133f0cec50636c67174441be70188625a5f474. Perhaps add that to the r0.12 branch, but don't cut a release? It'll help users like @yaroslavvb who build on their own.", "Or we can do a patch release. Just might not be worth the effort given the timing with the next release.", "@yifeif @martinwicke \r\nThis affects 0.12.\r\nThis breaks `pip install tensorflow` command in a clean system.\r\nShould we patch and roll out 0.12.1 ?", "Sigh. I suppose we should.\n\nShould we also take the opportunity to rethink how we do pip tests? This\nshould have been caught by those ideally.\n", "This is actually something we do not have a test for, and should be simple to test (But a non hermetic test I hate writing).\r\nWe have to check pypi for the dependencies we have. Without our tests contacting pypi, we cannot really catch this issue.", "I think of this as a release test not an integration test so I'm not too\nconcerned about hermeticity.\n", "Closing this out because 0.12.1 was released.", "@jhseu I am still getting the same error for 0.12.1 release. Please help to resolve the issue.\r\n\r\nD:\\Users\\user1\\tensorflow\\softwares\\anaconda>python.exe -m pip install\r\n tensorflow-0.12.1-cp35-cp35m-win_amd64.whl\r\nProcessing d:\\users\\user1\\tensorflow\\softwares\\anaconda\\tensorflow-0.1\r\n2.1-cp35-cp35m-win_amd64.whl\r\nRequirement already satisfied (use --upgrade to upgrade): numpy>=1.11.0 in d:\\us\r\ners\\user1\\tensorflow\\softwares\\anaconda\\lib\\site-packages (from tensor\r\nflow==0.12.1)\r\nRequirement already satisfied (use --upgrade to upgrade): wheel>=0.26 in d:\\user\r\ns\\user1\\tensorflow\\softwares\\anaconda\\lib\\site-packages (from tensorfl\r\now==0.12.1)\r\nRequirement already satisfied (use --upgrade to upgrade): six>=1.10.0 in d:\\user\r\ns\\user1\\tensorflow\\softwares\\anaconda\\lib\\site-packages (from tensorfl\r\now==0.12.1)\r\nCollecting protobuf>=3.1.0 (from tensorflow==0.12.1)\r\n  Could not fetch URL https://pypi.python.org/simple/protobuf/: There was a prob\r\nlem confirming the ssl certificate: [SSL: UNKNOWN_PROTOCOL] unknown protocol (_s\r\nsl.c:645) - skipping\r\n  Could not find a version that satisfies the requirement protobuf>=3.1.0 (from\r\ntensorflow==0.12.1) (from versions: )\r\nNo matching distribution found for protobuf>=3.1.0 (from tensorflow==0.12.1)", "Works for me on my Windows machine. The problem seems to be this error:\r\n`Could not fetch URL https://pypi.python.org/simple/protobuf/: There was a prob\r\nlem confirming the ssl certificate: [SSL: UNKNOWN_PROTOCOL] unknown protocol (_s\r\nsl.c:645) - skipping`\r\n\r\nI'm guessing you're sitting behind a proxy. I don't have any advice for you since I'm not too familiar with that error, but stackoverflow has some questions about it.", "@jhseu Thanks for your response. Yes, working behind firewall. Got the solution to fix this.\r\nNeed to download the protobuf separately and install using pip install.\r\n1) Link to download protobuf : https://pypi.python.org/pypi/protobuf/3.1.0\r\n2) install protobuf-3.1.0-py2.py3-none-any.whl\r\n\r\nAfter this step TensorFlow installation works for me.\r\n"]}, {"number": 6401, "title": "Numpy.FFT2() vs. TF.FFT2D() gives wrong values", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nNone. \r\n\r\n### Environment info\r\nOperating System:\r\nUbuntu 16.04 LTS\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\nToolkit-version 8.0\r\n\r\nIf installed from binary pip package, provide:\r\nInstalled it using pip install und Anaconda\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```\r\n# check if np.fft2d of TF.fft2d and NP have the same result\r\n\r\ntestimage = np.random.rand(256, 256)\r\ntestimage = testimage+0j\r\n\r\nft_testimage = np.fft.fft2(testimage)\r\nnp_result = np.sum(ft_testimage)\r\nprint(np_result)\r\n\r\ntf_ft_testimage = tf.fft2d(testimage)\r\ntf_result = np.sum(tf_ft_testimage.eval())\r\nprint(tf_result)\r\n\r\nresult_div = np.abs(tf_ft_testimage.eval())\r\n\r\nplt.imshow(np.log(result_div))\r\n\r\nprint(np_result)\r\n(56368.5840888+9.09494701773e-13j)\r\n\r\nprint(tf_result)\r\n(56368.6+0.00390625j)\r\n\r\n```\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n\r\n\r\n\r\nI was encountering this problem, when I did some numerical calculations using the FFT. I simply ported one algorithm from pure python to Tensorflow and the result were not the same. I think one of the reasons could be the wrong interpretation of the FFT2D used in Tensorflow? Do I need to normalize it somehow? Is the usage somehow different compared to Numpy? \r\n\r\nThanks a lot! \r\n\r\n", "comments": ["To my eye, it just looks like the GPU version of fft is lower accuracy than the CPU (double precision) version that numpy uses.", "Maybe the way I calculate the result is inappropriate. The dynamic range of the FFT is quiet big. So adding all the tiny and large values maybe results into this error on the GPU? \r\nI just ported a python code to Tensorflow and the numerical results are completely wrong. I'm still searching for the error and I was taking the numerical precision into account, but maybe it's also another problem. Thanks1 ", "You might try computing the infinity norm of the concatenated vector (reshaped to 1d)... i.e. take the differences of the two results per element, and then you can take the maxabs over all the elements. That's a much better measure of if the results are similar and avoids numeric precision loss. Summing arbitrary magnitude numbers in arbitrary orders will not give you an accurate sum (you'd need to order from smallest to largest to get an accurate result of a large sum like that, but that could still miss errors, hence the max abs norm being best)\r\n", "Thank you very much! I'll try once I'm back at the GPU-enabled Computer ;-) ", "Closing due to lack of recent activity. We will reopen when additional information becomes available. Thanks!"]}, {"number": 6400, "title": "Images missing in api_docs", "body": "A few images are missing at https://www.tensorflow.org/api_docs/python/math_ops/segmentation, e.g. [SegmentSum.png](https://www.tensorflow.org/api_docs/images/SegmentSum.png) and [SegmentProd.png](https://www.tensorflow.org/api_docs/images/SegmentProd.png) are not found ", "comments": ["Thanks. We'll look into it.", "Images are back on that page and others where the same transcription error happened.  "]}, {"number": 6399, "title": "Make an op compute its input once and hold.", "body": "Currently, tf.constant can take NumPy arrays as input, but it cannot take TensorFlow tensors as input. It would be useful to allow tf.constant to take tensors (that change with every evaluation, e.g. the tensor created by a dequeue op) and turn it into a constant tensor by evaluating the input tensor only once.", "comments": ["Constants are special in that their value is inlined into the graph, so the value is defined before session is created. Functionality like static shape-inference relies on this to figure out shapes before the first run call. If you allow constants to be undefined until the first session run, that would complicate that logic. Also, how would you deal with single Graph, but multiple Session objects? Currently constant is in the graph, so the value is unambiguous.\r\n\r\nWhat if you create two sessions and evaluate the \"constant\". Would the \"constant\" value be the one initialized in the current session, or will it transfer the value of constant from the other session? If latter, that would need a store that's shared between Sessions, which is a lot like variables.\r\n\r\nSo you could implement the functionality you want by using variables. Use `collections=[]` to exempt the variable from `tf.initialize_all_variables` and run the initializer manually\r\n\r\n```\r\nqueue1 = tf.FIFOQueue(capacity=10, dtypes=[tf.int32], shapes=[()])\r\nsess = tf.Session()\r\nsess.run(queue1.enqueue(1))\r\na = tf.Variable(queue1.dequeue(), collections=[])\r\nsess.run(a.initializer)\r\n\r\n```", "@yaroslavvb's comment seems to resolve this issue. please comment if this does not resolve your question @jonasrauber . Thanks for the issue.\r\n", "Kind of, but not really. I am aware that it's possible to use workarounds like the one proposed by @yaroslavvb. I don't care so much how it relates to real constants, I just thought it would be nice to have an op that consumes it's input once and then outputs this initial input whenever it's evaluated. A possible use case would be to turn a data loading queue into a fake data queue, to measure network performance independent of IO performance, etc. (basically like tf.zeros_like or tf.ones_like, just with a more natural data distribution based on a single example from the queue). If you don't consider this important enough, I am fine with closing this issue and using workarounds.", "@jonasrauber: I changed the title, because essentially, I don't think what your describing could be a constant.  I would be worried that such an op would be somewhat confusing. What if upstream values had been computed with some temporary feed and then the feed was changed. The first value evaluated was not useful, so you don't want to keep that cached value, you want the second input value. @mrry, @martinwicke, do you have any thoughts on the semantics of this operation?", "Off the top of my head, there are a few different possibilities here (although I'm sure there are others...):\r\n\r\n* The easiest would be to implement a write-once version of `tf.Variable`. It would still have an explicit `initialize` op, but it could be treated as a constant subsequently (e.g. in graph optimizations constant folding, CSE, etc.). Depending on how you want to use it, it could have blocking read semantics (block until initialized) or it could raise an error (like a normal `tf.Variable`). With the blocking read semantics, it would essentially be an [I-Structure](http://www.cse.chalmers.se/edu/year/2016/course/pfp/Papers/IStructures.pdf) (see Section 5 for details), but it would also be another potential source of deadlock :).\r\n\r\n* You could build a constant-like construct using a TensorFlow function (or perhaps a `tf.Variable` and `tf.cond()`, if you're not worried about concurrent assignments). On the first execution, it would call the function and store the result in the \"constant\", then produce that value. Subsequent executions would produce the same value. We discussed doing something like this for `tf.Variable` initialization a long time ago, but neither functions nor control flow were stable enough at the time.\r\n\r\nTensorFlow's push semantics make it tricky to implement a construct that takes a `tf.Tensor` and conditionally executes the subgraph needed to produce that tensor. I'm sure it could be done, but it would require changes in the core executor, so either of the above two options would be preferable.", "BTW, regarding recipe 2, I've been using `cond` to initialize variables once as [here](https://gist.github.com/yaroslavvb/d592394c0cedd32513f8fbb87ca05938#file-smart_initialize-py-L22). Without `tf.cond` you get unexpected results when initialization is split over several `sess.run` calls (https://github.com/tensorflow/tensorflow/issues/4920)", "@yaroslavvb Right, that code should work, as long as you're okay with the possibility that the initializer might run more than once.", "Closing due to lack of recent activity. if you provide additional information, we can reopen this. Thanks!\r\n", "> BTW, regarding recipe 2, I've been using `cond` to initialize variables once as [here](https://gist.github.com/yaroslavvb/d592394c0cedd32513f8fbb87ca05938#file-smart_initialize-py-L22). Without `tf.cond` you get unexpected results when initialization is split over several `sess.run` calls (#4920)\r\n\r\nThanks for the nice tip. I tried adding the final assign op of an initialization function to the TABLE_INITIALIZERS collection before, but only recently realized that it still computes the whole function every time a request comes in during serving. Could you help me understand why it behaves this way?"]}, {"number": 6398, "title": "Continuation line over-indented for visual indent", "body": "", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "Looks good, but mind addressing the CLA issue?", "Closing due to lack of CLA -- please update this thread if you are able to sign the CLA, or send a new PR.  Thanks!"]}, {"number": 6397, "title": "Feature suggestion: Total Variation Denoising", "body": "I would like to suggest the following feature be added to TensorFlow and discuss it before making the pull-request. As far as I can tell, the feature does not exist in TensorFlow.\r\n\r\n### Motivation\r\n\r\nTotal Variation Denoising (TVD) is sometimes used as a regularizer in image processing to suppress noise. It is commonly used in Style Transfer implementations. Here is my recent tutorial which uses it:\r\n\r\nhttps://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/15_Style_Transfer.ipynb\r\n\r\nWhile researching this tutorial, I saw some bizarre implementations of TVD. In reality it is just a simple array-slicing operation. I thought it would be useful to include it in TensorFlow so it is easy to use for everyone.\r\n\r\n### Math Formula\r\n\r\nThe code below implements the 'anisotropic' version of the formula, because it should be easier to optimize. The formula can be seen here:\r\n\r\nhttps://en.wikipedia.org/wiki/Total_variation_denoising#2D_digital_signals\r\n\r\n### Code\r\n\r\nThis is the code I propose to add to TensorFlow:\r\n\r\n    def total_variation(images):\r\n        \"\"\"\r\n        Calculate the Total Variation for one or more images for use in denoising.\r\n\r\n        https://en.wikipedia.org/wiki/Total_variation_denoising\r\n\r\n        Args:\r\n            images: `Tensor` with one or more images.\r\n                    The shape is `[batch, height, width, channel]`.\r\n\r\n        Returns:\r\n            A scalar `Tensor` representing the value.\r\n        \"\"\"\r\n\r\n        value = tf.reduce_sum(tf.abs(images[:,1:,:,:] - images[:,:-1,:,:])) + \\\r\n                tf.reduce_sum(tf.abs(images[:,:,1:,:] - images[:,:,:-1,:]))\r\n    \r\n        return value\r\n\r\nStyle note: For more complicated expressions like this, I prefer to assign the value before I return it. It makes the code cleaner and is helpful in debugging.\r\n\r\n### Discussion\r\n\r\nI studied [this TensorFlow module](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/losses/losses.py) but did not fully understand the design-philosophy and how to make this new function fit in.\r\n\r\nBefore making the pull-request I have some questions and issues to discuss:\r\n\r\n1) Would this belong under losses or regularizers in TensorFlow? Where specifically?\r\n\r\n2) Should I add a scope-block?\r\n\r\n3) I made an implementation that only reduced the sum for `axis=[1, 2, 3]` thus calculating the value on a per-image basis in case there are multiple images in the batch. But I don't know if it would ever be used in that way and it made the implementation more complicated. Perhaps it is best to have the simple version above and extend it if necessary.\r\n", "comments": ["It seems reasonable to me. @martinwicke, do you have any opinions here, or do you know who might?", "People who may have an informed opinion on this: @shlens, @sguada, @vincentvanhoucke.\r\n\r\nTo your questions: \r\n\r\n1) This function should probably go into the image module, since it's specific to image data.\r\n2) Yes, please add a name_scope. It'll make the graph more readable in tensorboard.\r\n3) It would be good to have a batch version which returns a vector. I would expect to be used more than the single image version. \r\n\r\n", "SGTM. +1 to everything  @martinwicke suggested.\r\n\r\nI would suggest adding the function to:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/image_ops_impl.py \r\n\r\nThe function should check that its argument `images` is of the appropriate shape using `_Check3DImage` or `_CheckAtLeast3DImage`", "Thanks for the comments.\r\n\r\nHere is the function I propose. I have tried to follow the coding style in `image_ops_impl.py`, except for the indentation which I will fix to 2 spaces in the pull request, and my comments are more verbose because I find that helpful when reading code and quickly trying to grasp what the intention is.\r\n\r\nI did not use `_CheckAtLeast3DImage` because it requires the shape to be fully defined.\r\n\r\nPlease have a look and let me know if this looks fine. Then I will make a pull request.\r\n\r\n    def total_variation(images, name=None):\r\n        \"\"\"Calculate and return the Total Variation for one or more images.\r\n\r\n        The total variation is the sum of the absolute differences for neighboring\r\n        pixel-values in the input images. This measures how much noise is in the images.\r\n\r\n        This can be used as a loss-function during optimization so as to suppress noise\r\n        in images. If you have a batch of images, then you should calculate the scalar\r\n        loss-value as the sum: `loss = tf.reduce_sum(tf.image.total_variation(images))`\r\n\r\n        This implements the anisotropic 2-D version of the formula described here:\r\n\r\n        https://en.wikipedia.org/wiki/Total_variation_denoising\r\n\r\n        Args:\r\n            images: 4-D Tensor of shape `[batch, height, width, channels]` or\r\n                    3-D Tensor of shape `[height, width, channels]`.\r\n\r\n            name: A name for the operation (optional).\r\n\r\n        Raises:\r\n            ValueError: if images.shape is not a 3-D or 4-D vector.\r\n\r\n        Returns:\r\n            The total variation of `images`.\r\n\r\n            If `images` was 4-D, a 1-D float Tensor of shape `[batch]` with the\r\n            total variation for each image in the batch.\r\n            If `images` was 3-D, a scalar float with the total variation for that image.\r\n        \"\"\"\r\n\r\n        with ops.name_scope(name, 'total_variation'):\r\n            ndims = images.get_shape().ndims\r\n\r\n            if ndims == 3:\r\n                # The input is a single image with shape [height, width, channels].\r\n\r\n                # Calculate the difference of neighboring pixel-values.\r\n                # The images are shifted one pixel along the height and width by slicing.\r\n                pixel_dif1 = images[1:,:,:] - images[:-1,:,:]\r\n                pixel_dif2 = images[:,1:,:] - images[:,:-1,:]\r\n\r\n                # Sum for all axis. (None is an alias for all axis.)\r\n                sum_axis = None\r\n            elif ndims == 4:\r\n                # The input is a batch of images with shape [batch, height, width, channels].\r\n\r\n                # Calculate the difference of neighboring pixel-values.\r\n                # The images are shifted one pixel along the height and width by slicing.\r\n                pixel_dif1 = images[:,1:,:,:] - images[:,:-1,:,:]\r\n                pixel_dif2 = images[:,:,1:,:] - images[:,:,:-1,:]\r\n\r\n                # Only sum for the last 3 axis.\r\n                # This results in a 1-D tensor with the total variation for each image.\r\n                sum_axis = [1, 2, 3]\r\n            else:\r\n                raise ValueError('\\'images\\' must be either 3 or 4-dimensional.')\r\n\r\n            # Calculate the total variation by taking the absolute value of the\r\n            # pixel-differences and summing over the appropriate axis.\r\n            tot_var = tf.reduce_sum(tf.abs(pixel_dif1), axis=sum_axis) + \\\r\n                      tf.reduce_sum(tf.abs(pixel_dif2), axis=sum_axis)\r\n\r\n        return tot_var\r\n\r\n### Testing\r\n\r\nI have tested this in a Python Notebook, but it is messy and undocumented so I have not put it online. If you would like to run these tests yourself then I can cleanup the Notebook and put it online somewhere. (Where?)\r\n\r\nI have tested both single- and multi-image input tensors. I have also tested incorrect inputs such as None, too low and too high dimensions. I have done a basic gradient descent and it produces the same results for a single and multiple input images, provided you use `tf.reduce_sum()` in the loss-function as described in the comments to the function.\r\n\r\nHere are two example images that were smoothed using this as the loss function. The optimizer performed 100 iterations with a gradient step-size of 1.0.\r\n\r\n![image1](https://cloud.githubusercontent.com/assets/13588114/21567810/83450792-ceaf-11e6-9222-a1c66207a2f7.png)\r\n\r\n![image1_smoothed](https://cloud.githubusercontent.com/assets/13588114/21567811/88a82282-ceaf-11e6-93ff-6ce86422bf47.png)\r\n\r\n![image2](https://cloud.githubusercontent.com/assets/13588114/21567816/9304ef62-ceaf-11e6-9d7d-d25ab2c687fc.png)\r\n\r\n![image2_smoothed](https://cloud.githubusercontent.com/assets/13588114/21567819/957c7738-ceaf-11e6-9265-4247a84b1732.png)\r\n", "@Hvass-Labs Would you be able to make a proper pull request for this code? Please include a unit test in:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/image_ops_test.py.\r\n\r\nNote that the second argument to `images` seems to have more indices then the dimensions. Not sure if this is a potential bug.", "I have now opened a pull request https://github.com/tensorflow/tensorflow/pull/6662\r\n\r\n@shlens I don't understand your remark about a potential bug.\r\n\r\nPlease have a look at the PR and let's continue the discussion there."]}, {"number": 6396, "title": "Cmake static library build issue", "body": "Hi all,\r\n\r\nI have successfully build tensorflow in windows/visual studio using the [cmake project](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake). \r\n\r\nNow due to large build time of tensorflow, I am building the tensorflow as libraries (static library) and trying to use them in an application (tf_label_image_example). However, I get the following runtime error at the points of calling env constructor in the SessionOptions at the following source code:\r\n`session->reset(tensorflow::NewSession(tensorflow::SessionOptions()));.`\r\nError is: `No session factory registered for the given session options: {target: \"\" config: } Registered factories are {}.`\r\n\r\nIt is similar to [this error](https://github.com/tensorflow/tensorflow/issues/3308) in makefiles and MAC os where the compiler is stripping the multiple defined symbols. I tried to add visual studio equivalent of this compiler options (/FORCE:MULTIPLE and /WHOLEARCHIVE) to build of tf_core_lib.lib . however, the compiler options are not applicable to static libraries. Alternative option to is make a DLL, however, that would require modification of all functions with __declspec(dllexport).\r\nIs there a better solution?\r\n\r\nThe tf_label_image_example application contains main.cc and the following additional lib dependencies:\r\ntf_core_cpu.lib\r\ntf_core_framework.lib\r\ntf_core_kernels.lib\r\ntf_cc_framework.lib\r\ntf_cc_ops.lib\r\ntf_core_ops.lib\r\ntf_stream_executor.lib\r\ntf_core_direct_session.lib\r\ntf_core_lib.lib\r\n \r\nThanks in advance.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttps://github.com/tensorflow/tensorflow/issues/3308\r\n\r\n### Environment info\r\nOperating System: Win 10\r\n\r\nInstalled version of CUDA and cuDNN:  Cuda 8\r\nThe same configuration [here ](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake#known-good-configurations)is used.", "comments": ["@shakith This is caused by MSVC throwing out object files in static libraries during link time if no symbols from them are referenced from anywhere else (which is in most cases a desirable behavior). Many TF objects (ops, kernels, sessions) are not used directly by other code, but through accessing them via global 'registry' collections, hence this behavior.\r\n\r\nYou have two options - either link the object files directly into executables/dlls (without putting them in static libraries), or (starting with VS 2015 Update 2) use [/WHOLEARCHIVE](https://msdn.microsoft.com/en-us/library/mt732963.aspx?f=255&MSPPError=-2147217396) option to force the linker to include all objects from your static library. You would use that option when linking the DLL/executable, not when building your static libraries.", "please re-open if the above answer does not resolve your issue.", "I have similar problem. \r\n\r\nIn generated with cmake .sln file tf_tutorials_example_trainer.vcxproj tensorflow works fine, but linking tensorflow in existing project (same code as in tf_tutorials_example_trainer) gives error \"No session factory registered for the given session options\".\r\n\r\nWhen i use /WHOLEARCHIVE and tensorflow .lib files in existing project (console release x64) - Error:\r\n/WHOLEARCHIVE = Error LNK1000: Internal error during IncrBuildImage\r\n\r\nDetails of problem: https://github.com/tensorflow/tensorflow/issues/6382\r\n\r\nMaybe some separate Cmake / Visual Studio example projects linking tensorflow .lib files exist (to stop rebuilding tensorflow each time)?", "Got another strange error trying to link fresh custom Release x64 console project like tf_label_image_example (same code) when added /WHOLEARCHIVE:\r\n\r\nLNK1000\tInternal error during CImplib::EmitThunk\r\n\r\nC/C++ settins:\r\n\r\n`/MP /GS /TP /W3 /wd\"4267\" /wd\"4244\" /wd\"4800\" /wd\"4503\" /wd\"4554\" /wd\"4996\" /wd\"4348\" /wd\"4018\" /wd\"4099\" /wd\"4146\" /wd\"4305\" /wd\"4307\" /wd\"4715\" /wd\"4722\" /wd\"4723\" /wd\"4838\" /wd\"4309\" /wd\"4334\" /Gy- /Zc:wchar_t /I\"D:\\Tensorflow\\tensorflow-master\" /I\"D:\\Tensorflow\\tensorflow-master\\tensorflow\\contrib\\cmake\\build\" /I\"D:\\Tensorflow\\tensorflow-master\\tensorflow\\contrib\\cmake\\build\\external\\zlib_archive\" /I\"D:\\Tensorflow\\tensorflow-master\\tensorflow\\contrib\\cmake\\build\\external\\gif_archive\\giflib-5.1.4\" /I\"D:\\Tensorflow\\tensorflow-master\\tensorflow\\contrib\\cmake\\build\\external\\png_archive\" /I\"D:\\Tensorflow\\tensorflow-master\\tensorflow\\contrib\\cmake\\build\\external\\jpeg_archive\" /I\"D:\\Tensorflow\\tensorflow-master\\tensorflow\\contrib\\cmake\\build\\external\\eigen_archive\" /I\"D:\\Tensorflow\\tensorflow-master\\third_party\\eigen3\" /I\"D:\\Tensorflow\\tensorflow-master\\tensorflow\\contrib\\cmake\\build\\gemmlowp\\src\\gemmlowp\" /I\"D:\\Tensorflow\\tensorflow-master\\tensorflow\\contrib\\cmake\\build\\jsoncpp\\src\\jsoncpp\" /I\"D:\\Tensorflow\\tensorflow-master\\tensorflow\\contrib\\cmake\\build\\external\\farmhash_archive\" /I\"D:\\Tensorflow\\tensorflow-master\\tensorflow\\contrib\\cmake\\build\\external\\farmhash_archive\\util\" /I\"D:\\Tensorflow\\tensorflow-master\\tensorflow\\contrib\\cmake\\build\\external\\highwayhash\" /I\"D:\\Tensorflow\\tensorflow-master\\tensorflow\\contrib\\cmake\\build\\protobuf\\src\\protobuf\\src\" /I\"D:\\Tensorflow\\tensorflow-master\\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\include\" /I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\" /I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\extras\\CUPTI\\include\" /I\"D:\\Tensorflow\\cudnn\\cuda\" /I\"D:\\Tensorflow\\tensorflow-master\\third_party\\gpus\" /I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\include\" /Gm- /O2 /Ob2 /Fd\"x64\\Release\\vc140.pdb\" /Zc:inline /fp:precise /D \"WIN32\" /D \"_WINDOWS\" /D \"NDEBUG\" /D \"EIGEN_AVOID_STL_ARRAY\" /D \"NOMINMAX\" /D \"_WIN32_WINNT=0x0A00\" /D \"LANG_CXX11\" /D \"COMPILER_MSVC\" /D \"__VERSION__=\\\"MSVC\\\"\" /D \"OS_WIN\" /D \"_MBCS\" /D \"WIN64\" /D \"WIN32_LEAN_AND_MEAN\" /D \"NOGDI\" /D \"PLATFORM_WINDOWS\" /D \"TENSORFLOW_USE_EIGEN_THREADPOOL\" /D \"EIGEN_HAS_C99_MATH\" /D \"_ITERATOR_DEBUG_LEVEL=0\" /D \"GOOGLE_CUDA=1\" /D \"TF_EXTRA_CUDA_CAPABILITIES=3.0,3.5,5.2\" /D \"CMAKE_INTDIR=\\\"Release\\\"\" /errorReport:prompt /GF /WX- /Zc:forScope /GR /Gd /MD /FC /Fa\"x64\\Release\\\" /EHsc /nologo /Fo\"x64\\Release\\\" /Fp\"x64\\Release\\TF.pch\" `\r\n /bigobj \r\n\r\nLinker settings:\r\n`/OUT:\"D:\\CppTests\\FuckMail\\TF\\x64\\Release\\TF.exe\" /MANIFEST /NXCOMPAT /PDB:\"D:\\CppTests\\FuckMail\\TF\\x64\\Release\\TF.pdb\" /DYNAMICBASE \"kernel32.lib\" \"user32.lib\" \"gdi32.lib\" \"winspool.lib\" \"shell32.lib\" \"ole32.lib\" \"oleaut32.lib\" \"uuid.lib\" \"comdlg32.lib\" \"advapi32.lib\" \"Release\\tf_protos_cc.lib\" \"Release\\tf_core_gpu_kernels.lib\" \"zlib\\install\\lib\\zlibstatic.lib\" \"gif\\install\\lib\\giflib.lib\" \"png\\install\\lib\\libpng12_static.lib\" \"jpeg\\install\\lib\\libjpeg.lib\" \"jsoncpp\\src\\jsoncpp\\src\\lib_json\\Release\\jsoncpp.lib\" \"farmhash\\install\\lib\\farmhash.lib\" \"highwayhash\\install\\lib\\highwayhash.lib\" \"protobuf\\src\\protobuf\\Release\\libprotobuf.lib\" \"grpc\\src\\grpc\\Release\\grpc++_unsecure.lib\" \"grpc\\src\\grpc\\Release\\grpc_unsecure.lib\" \"grpc\\src\\grpc\\Release\\gpr.lib\" \"wsock32.lib\" \"ws2_32.lib\" \"shlwapi.lib\" \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\lib\\x64\\cudart_static.lib\" \"D:\\Tensorflow\\cudnn\\cuda\\lib\\x64\\cudnn.lib\" \"tf_cc_framework.dir/Release/tf_cc_framework.lib\" \"tf_cc_ops.dir/Release/tf_cc_ops.lib\" \"tf_core_cpu.dir/Release/tf_core_cpu.lib\" \"tf_core_direct_session.dir/Release/tf_core_direct_session.lib\" \"tf_core_framework.dir/Release/tf_core_framework.lib\" \"tf_core_kernels.dir/Release/tf_core_kernels.lib\" \"tf_core_lib.dir/Release/tf_core_lib.lib\" \"tf_core_ops.dir/Release/tf_core_ops.lib\" /MACHINE:X64 /OPT:NOREF /INCREMENTAL:NO /PGD:\"D:\\CppTests\\FuckMail\\TF\\x64\\Release\\TF.pgd\" /SUBSYSTEM:CONSOLE /MANIFESTUAC:\"level='asInvoker' uiAccess='false'\" /ManifestFile:\"x64\\Release\\TF.exe.intermediate.manifest\" /OPT:NOICF /ERRORREPORT:PROMPT /NOLOGO /LIBPATH:\"D:\\Tensorflow\\tensorflow-master\\tensorflow\\contrib\\cmake\\build\" /TLBID:1 `\r\n /WHOLEARCHIVE /machine:x64 \r\n", "Found similar tensorflow issue forum https://groups.google.com/forum/#!topic/bazel-discuss/tDGrTAtaxYw\r\n ( Internal error during CImplib::EmitThunk ) \r\n\r\nWorkaround as manually \"drag-n-drop all .obj files in solution\" seems to complex.\r\n\r\nAdded as suggested in forum #pragma comment( lib, .. ) - no effect\r\n\r\n`\r\n#pragma comment( lib, \"D:/Tensorflow/tensorflow-master/tensorflow/contrib/cmake/build/Release/tf_protos_cc.lib\" )\r\n#pragma comment( lib, \"D:/Tensorflow/tensorflow-master/tensorflow/contrib/cmake/build/Release/tf_core_gpu_kernels.lib\" )\r\n#pragma comment( lib, \"D:/Tensorflow/tensorflow-master/tensorflow/contrib/cmake/build/zlib/install/lib/zlibstatic.lib\" )\r\n#pragma comment( lib, \"D:/Tensorflow/tensorflow-master/tensorflow/contrib/cmake/build/gif/install/lib/giflib.lib\" )\r\n#pragma comment( lib, \"D:/Tensorflow/tensorflow-master/tensorflow/contrib/cmake/build/png/install/lib/libpng12_static.lib\" )\r\n#pragma comment( lib, \"D:/Tensorflow/tensorflow-master/tensorflow/contrib/cmake/build/jpeg/install/lib/libjpeg.lib\" )\r\n#pragma comment( lib, \"D:/Tensorflow/tensorflow-master/tensorflow/contrib/cmake/build/jsoncpp/src/jsoncpp/src/lib_json/Release/jsoncpp.lib\" )\r\n#pragma comment( lib, \"D:/Tensorflow/tensorflow-master/tensorflow/contrib/cmake/build/farmhash/install/lib/farmhash.lib\" )\r\n#pragma comment( lib, \"D:/Tensorflow/tensorflow-master/tensorflow/contrib/cmake/build/highwayhash/install/lib/highwayhash.lib\" )\r\n#pragma comment( lib, \"D:/Tensorflow/tensorflow-master/tensorflow/contrib/cmake/build/protobuf/src/protobuf/Release/libprotobuf.lib\" )\r\n#pragma comment( lib, \"D:/Tensorflow/tensorflow-master/tensorflow/contrib/cmake/build/grpc/src/grpc/Release/grpc++_unsecure.lib\" )\r\n#pragma comment( lib, \"D:/Tensorflow/tensorflow-master/tensorflow/contrib/cmake/build/grpc/src/grpc/Release/grpc_unsecure.lib\" )\r\n#pragma comment( lib, \"D:/Tensorflow/tensorflow-master/tensorflow/contrib/cmake/build/grpc/src/grpc/Release/gpr.lib\" )\r\n#pragma comment( lib, \"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0/lib/x64/cudart_static.lib\" )\r\n#pragma comment( lib, \"D:/Tensorflow/cudnn/cuda/lib/x64/cudnn.lib\" )\r\n#pragma comment( lib, \"D:/Tensorflow/tensorflow-master/tensorflow/contrib/cmake/build/tf_cc_framework.dir/Release/tf_cc_framework.lib\" )\r\n#pragma comment( lib, \"D:/Tensorflow/tensorflow-master/tensorflow/contrib/cmake/build/tf_cc_ops.dir/Release/tf_cc_ops.lib\" )\r\n#pragma comment( lib, \"D:/Tensorflow/tensorflow-master/tensorflow/contrib/cmake/build/tf_core_cpu.dir/Release/tf_core_cpu.lib\" )\r\n#pragma comment( lib, \"D:/Tensorflow/tensorflow-master/tensorflow/contrib/cmake/build/tf_core_direct_session.dir/Release/tf_core_direct_session.lib\" )\r\n#pragma comment( lib, \"D:/Tensorflow/tensorflow-master/tensorflow/contrib/cmake/build/tf_core_framework.dir/Release/tf_core_framework.lib\" )\r\n#pragma comment( lib, \"D:/Tensorflow/tensorflow-master/tensorflow/contrib/cmake/build/tf_core_kernels.dir/Release/tf_core_kernels.lib\" )\r\n#pragma comment( lib, \"D:/Tensorflow/tensorflow-master/tensorflow/contrib/cmake/build/tf_core_lib.dir/Release/tf_core_lib.lib\" )\r\n#pragma comment( lib, \"D:/Tensorflow/tensorflow-master/tensorflow/contrib/cmake/build/tf_core_ops.dir/Release/tf_core_ops.lib\" )\r\n#pragma comment( lib, \"D:/Tensorflow/tensorflow-master/tensorflow/contrib/cmake/build/tf_stream_executor.dir/Release/tf_stream_executor.lib\" )\r\n#pragma comment( lib, \"D:/Tensorflow/tensorflow-master/tensorflow/contrib/cmake/build/Release/tf_python_protos_cc.lib\" )\r\n`", "Workaround:\r\n\r\nFor now tf_label_image_example works fine in generated .sln!\r\n\r\nTo stop rebuilding whole tensorflow each time i used workaround \r\n\"Configuration Manager\". Un-check the \"Build\" for all except tf_label_image_example as in http://stackoverflow.com/a/1433680\r\n\r\nAlso in contrib\\cmake\\build i created folders tensorflow/examples/label_image/data/ and placed in it:\r\ngrace_hopper.jpg\r\nimagenet_comp_graph_label_strings.txt\r\ntensorflow_inception_graph.pb\r\n\r\nData may be found at https://storage.googleapis.com/download.tensorflow.org/models/inception_dec_2015.zip in tutorial https://www.tensorflow.org/how_tos/tool_developers/\r\n\r\n", "@vit-stepanovs , Thanks for the info.\r\n\r\nI have followed the adding all obj file solution and it works. However, it will be great to have lib version (with small amounts of libs). I added /WHOLEARCHIVE compiler option (on Visual Studi 2015 Update 3) to the tf_label_image_example application project with the lib dependencies. It gives the same error as for @derofim:\r\n\r\n`LNK1000: Internal error during CImplib::EmitThunk`\r\n\r\nTo verify whether this is a configuration issue with my setup, has anyone managed to build static libraries and link them to an application through Visual Studio/CMake project. Or is it a Visual Studio issue with these specific libraries?\r\n\r\n@andydavis1, could you open the issue till a static library solution can be found or if needed till a bug report can be reported for visual studio/Cmake project? Thanks.\r\n\r\n\r\n\r\n", "@shakith Thanks for the detailed description of the problem. Can you please say how you added all obj files to solution? \r\n\r\n```\r\nI saw list of .obj files needed to build tf_label_image_example in solution explorer and was amazed how big it is. Maybe some of them may be skipped (or added automatically)?\r\n\r\n![image](https://cloud.githubusercontent.com/assets/9510143/21359502/84694102-c6ed-11e6-862b-03905aa74e98.png)\r\n```\r\n\r\n**UPDATE**:\r\n@shakith Thank you very much! Tensorflow works fine after i added .obj files.\r\n\r\nIf somebody faced same issue:\r\n\r\nI copied from tf_label_image_example.vcxproj these lines to custom.vcxproj: https://gist.github.com/derofim/fd461d42104567b4fc0cd687dec9f464\r\n\r\nNote that `<ClCompile Include=\"D:\\Tensorflow\\tensorflow-master\\tensorflow\\examples\\label_image\\main.cc\"  />` removed (I replaced main.cc with custom source code).\r\n\r\nAlso i created filter \"Object Libraries\" and copied from tf_label_image_example.vcxproj.filters these lines to custom.vcxproj.filters:\r\nhttps://gist.github.com/derofim/9ba812c9234f50a6d5142c73fdc0f5cc\r\n\r\n\r\n", "You can bypass the LNK1000 error by selectively applying /WHOLEARCHIVE option only to the necessary static libraries. I managed to get with this set as a starting point for my experiments:\r\n`/WHOLEARCHIVE:tf_core_direct_session.lib /WHOLEARCHIVE:tf_core_ops.lib /WHOLEARCHIVE:tf_core_lib.lib /WHOLEARCHIVE:tf_cc_framework.lib /WHOLEARCHIVE:tf_core_framework.lib /WHOLEARCHIVE:tf_core_cpu.lib /WHOLEARCHIVE:tf_stream_executor.lib`", "I can compile successively adding manually all the *.obj of some libraries. But it is quite awkward because link is slow, and updating the library requires to check whether a new file is added... Therefore I've tried the /WHOLEARCHIVE trick. \r\n\r\nIt works for all the libraries I was linking with (through *.obj) **but not for tf_core_gpu_kernels.lib**; a symbol (TensorCuBlasGemm<float>) keeps missing. This symbol comes from blas_gemm.cc (explicit template instantiation) either from _gru_ops or _lstm_ops libraries. But linking with any of this does not fix the missing symbol. Hence my question: which library should I link against (dumpbin does not tell it)?"]}, {"number": 6395, "title": "calculate partial gradient", "body": "Hi,\r\n\r\nFor example, in such a multi-task network.\r\n\r\nA=net(input)\r\no1=softmax(A)\r\nB=fullconnect(A)\r\no2=softmax(B)\r\n\r\nNow, the gradient of  'A' from both 'o1' and 'B'.\r\nBut, I only want the gradient of  'A' from  'o1'\r\n\r\nIs there a way to do this in tensorflow?\r\n\r\nThanks.", "comments": ["This is a question better suited for StackOverflow, which we also monitor. Please ask it there and tag it with the `tensorflow` tag."]}, {"number": 6394, "title": "Tensorflow GPU Support for Windows?", "body": "Hi I installed tensorflow-GPU through pip on my Windows 10 machine and I have also installed the Nvidia Cuda Toolkit and have downloaded cuDNN. But I came to the realization that there was no GPU version for Windows 10. Is it still possible to use the tensorflow-GPU windows 10 given that it is install-able via pip and all the other dependencies can be installed and configued?", "comments": ["What do you mean there was no GPU version for windows?\r\nif you run the following command, you should get the GPU(Nvidia Cuda) version of tensorflow installed.\r\n```\r\npip install tensorflow-gpu\r\n```\r\nWe do have tensorflow GPU version available for windows 10, which also should be compatible with older versions of windows to some extent.\r\n\r\nIt is correct that you have to install CUDA and CuDNN yourself, as they are separate pieces of software which need to be distributed separately.", "I was confused since the build passing icon was only on windows-cpu. If that is the case than that is great. But I am also having trouble installing CuDNN. Where would I install it on a windows machine?", "We try to keep the build passing icons to a minimum.\r\nBut any released package means all builds/tests have passed for that package.\r\n\r\nFor CuDNN, you have to go through https://developer.nvidia.com/cudnn\r\n\r\nCLosing this issue as it seems to be resolved on TF side.", "Having this issue now..\r\nPS C:\\Users\\> pip install tensorflow-gpu\r\nCollecting tensorflow-gpu\r\n Could not find a version that satisfies the requirement tensorflow-gpu (from versions: )\r\nNo matching distribution found for tensorflow-gpu\r\n", "TF at the moment only has prebuilt windows pip packages for python 3.5.\r\nAny other python version, you will see this error."]}, {"number": 6393, "title": "tensorflow.python.framework.errors_impl.PermissionDeniedError: data", "body": "hi,when am run a TensorFlow demo model as this\r\nhttps://www.tensorflow.org/get_started/os_setup#test_the_tensorflow_installation\r\ni have the problem descripte as follows:\r\n\r\n\r\n$ python /usr/local/lib/python2.7/dist-packages/tensorflow/models/imag\r\ne/mnist/convolutional.py\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/models/image/mnist/convolutional.py\", line 339, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/models/image/mnist/convolutional.py\", line 130, in main\r\n    train_data_filename = maybe_download('train-images-idx3-ubyte.gz')\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/models/image/mnist/convolutional.py\", line 65, in maybe_download\r\n    tf.gfile.MakeDirs(WORK_DIRECTORY)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.py\", line 299, in recursive_create_dir\r\n    pywrap_tensorflow.RecursivelyCreateDir(compat.as_bytes(dirname), status)\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.PermissionDeniedError: data\r\n\r\n\r\n\r\nhow can i fix it?\r\n\r\n\r\nthe tensorflow install environment as follows:\r\nubuntu 16.04\r\npython 2.7\r\nnumpy 1.11.2\r\nsix 1.10.0\r\nprotobuf 3.1.0\r\n\r\ni installed tensorflow as follows:\r\n    sudo -H pip install tensorflow\r\n\r\n\r\n\r\n", "comments": ["You may not have permissions to write/read in your working directory. This is nothing about tensorflow.", "thanks a lot. i had already find the reason.", "@soliders  Hello, I have the same problem with you. So I try to fix it by ' chmod -R 777 /usr/local/lib/python2.7/dist-packages/tensorflow'. But this treatment is invalid. Can you share your approach?", "@feiyue1230 did you install tensorflow in conda environment? if so you need to run: python /home/runsheng/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/models/image/mnist.\r\n\r\nAt least this was my case.\r\n\r\n", "How did you solved that? run it as sudo doesnt looks the best option...", "I got the same error. The directory where i wanted to write a file is located in same directory where my code is present however i wrote path as \"/log_dir\". I changed this to \"log_dir/\" and it worked. I think when I write \"/log_dir\", linux tried to look at root where no such directory was found and there was no permission by system to write. ", "np.savez_compressed(code_file, shape=int_codes.shape, codes=export) is changed to  np.savez_compressed('a', shape=int_codes.shape, codes=export)\r\nthen, it works.", "I try:\r\n`sudo su`\r\nRun it again, it works!", "> I got the same error. The directory where i wanted to write a file is located in same directory where my code is present however i wrote path as \"/log_dir\". I changed this to \"log_dir/\" and it worked. I think when I write \"/log_dir\", linux tried to look at root where no such directory was found and there was no permission by system to write.\r\n\r\nme\u00a02", "This is a permission issue. Change the output file path strength. For example, my output path is 'output', and the run parameter needs to be written as \"output/\" instead of \"/output/\"\r\nI will share my .sh file for your reference.\r\nexport BERT_BASE_DIR=/home/I348655/Document/Project/transfer_learn/bert/bert/uncased_L-12_H-768_A-12\r\nexport GLUE_DIR=/home/I348655/Document/Project/transfer_learn/bert/bert/glue\r\n\r\npython run_classifier.py \\\r\n  --task_name=MRPC \\\r\n  --do_train=true \\\r\n  --do_eval=true \\\r\n  --data_dir=$GLUE_DIR/MRPC \\\r\n  --vocab_file=$BERT_BASE_DIR/vocab.txt \\\r\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\r\n  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\r\n  --max_seq_length=128 \\\r\n  --train_batch_size=32 \\\r\n  --learning_rate=2e-5 \\\r\n  --num_train_epochs=3.0 \\\r\n  **--output_dir=mrpc_output/**\r\n\r\n\uff08Please pay attention to my bold position\uff09", "Guys I solved this problem using\r\npython3 generate_tf_record.py --csv_input=data/train_labels.csv  --output_path=train.record\r\npython3 generate_tf_record.py --csv_input=data/test_labels.csv  --output_path=test.record\r\n\r\nall the images were in one folder i.e images and flags were as below\r\n\r\n\r\nflags.DEFINE_string('csv_input', '','data/train_labels.csv')\r\nflags.DEFINE_string('output_path','', 'train.record')\r\nflags.DEFINE_string('image_dir', '', 'images')\r\n", "I saved record file in main folder rather then in data,,,,later copied it in data folder", "I bet we shoud use os.makedirs(WORK_DIRECTORY, exist_ok=True) instead of tf.gfile.MakeDirs(WORK_DIRECTORY)"]}, {"number": 6392, "title": "Monitors (contrib) documentation", "body": "The monitors (contrib) documentation (https://www.tensorflow.org/api_docs/python/contrib.learn.monitors) does not appear to have been deployed properly.", "comments": ["This is now fixed.  Thanks for pointing it out."]}]