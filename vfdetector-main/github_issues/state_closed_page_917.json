[{"number": 25946, "title": "BUG: symbolic layer triggers device creation", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):linux ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):b'v1.13.0-rc2-0-gc865ec5621' 1.13.0-rc2\r\n- Python version:3.7\r\n- Bazel version (if compiling from source):n/a\r\n- GCC/Compiler version (if compiling from source):n/a\r\n- CUDA/cuDNN version:10.0 / 7.4.2\r\n- GPU model and memory:gtx960M\r\n\r\n**Describe the current behavior**\r\nThe following code:\r\n```python\r\nimport tensorflow as tf\r\na = tf.placeholder(tf.float32, [100, 100, 100, 100])\r\nb = tf.layers.Conv2DTranspose(3, 3, data_format='channels_first')\r\noutput = b.apply(a)\r\n```\r\nprints:\r\n```\r\n2019-02-20 10:20:05.505595: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-02-20 10:20:05.578782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-02-20 10:20:05.579477: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55fd579f65d0 executing computations on platform CUDA. Devices:\r\n2019-02-20 10:20:05.579513: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce GTX 960M, Compute Capability 5.0\r\n2019-02-20 10:20:05.606095: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2592000000 Hz                                \r\n2019-02-20 10:20:05.606746: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55fd57b39b00 executing computations on platform Host. Devices:\r\n2019-02-20 10:20:05.606785: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>               \r\n2019-02-20 10:20:05.607093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:                              \r\nname: GeForce GTX 960M major: 5 minor: 0 memoryClockRate(GHz): 1.0975\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 1.96GiB freeMemory: 1.92GiB\r\n2019-02-20 10:20:05.607118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0                                \r\n2019-02-20 10:20:05.608205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-02-20 10:20:05.608229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0                                                        \r\n2019-02-20 10:20:05.608240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N                                                       \r\n2019-02-20 10:20:05.608504: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1742 MB memory) -> physical GPU (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0)        \r\n```\r\nIt can be seen that it initializes the GPU devices. However this should not happen in symbolic functions.\r\n\r\nInitializing the GPU devices has many side effects. \r\nIt can lead to different types of failures, such as https://github.com/tensorflow/tensorflow/issues/8136#issuecomment-361727732. The largest side effect is that: any GPU-related flags given to a `tf.Session` created after device initialization will not take effect.\r\nIt will also make it much harder to use horovod because horovod requires initializing the GPU in specific ways (with `visible_device_list`). If a graph with `Conv2DTranspose` was created before creating the session (which is the standard way of using TF 1.0), horovod will fail to initialize the session. (cc @alsrgv ).\r\n\r\nThis bug exists for Conv2DTranspose, but not for Conv2D.\r\nThis bug exists in 1.13.0rc0. It does not exist in 1.12.0", "comments": ["This bug comes from `keras/backend.py`, where `conv2d_transpose` listed available devices to check data_format.\r\n\r\nIn fact, the entire `keras/backend.py` file heavily relies on looking at the available devices.", "I'm guessing we'll have to stick with https://github.com/horovod/horovod/blob/master/examples/keras_imagenet_resnet50.py#L59 in a preamble for any Keras API usage.", "3 weeks with no response?", "@qlzh727 Are you a good person to look at this?", "I am quite occupied right now with some RNN work, but I will reroute this to the correct owner.", "It's not obvious to me how one would get around this given that checking devices triggers initialization code if the device is not already initialized. NHWC vs. NCHW device compatibility issues are one of the more common difficulties encountered, hence why we check for it. Ultimately, I think @alsrgv's solution is probably correct: if you need to set specific process level config it will have to be done at the very start.\r\n\r\nThat said, if you can think of a better solution feel free to suggest it or open a PR.", "Device initialization is not the only issue here.\r\nA summary of the cause:\r\nCertain Keras layers call the following function:\r\n\r\n```python\r\ndef _has_nchw_support():\r\n  explicitly_on_cpu = _is_current_explicit_device('CPU')\r\n  gpus_available = bool(_get_available_gpus())\r\n  return not explicitly_on_cpu and gpus_available\r\n```\r\n\r\nin `keras/backend.py`. When the function returns False but the layer is called with NCHW format, the layer will apply some format conversions, such as transpose.\r\n\r\nThere are at least three issues with this approach:\r\n\r\n1. The function `_has_nchw_support` is clearly wrong.\r\n   Many of the involved ops supports NCHW on CPUs with MKL build, and on TPUs.\r\n\r\n   Consequences: These Keras layers do not behave properly (transpose may be added) on CPUs with MKL build or on TPUs.\r\n\r\n2. Graph construction should be conceptually independent of execution.\r\n\t-- This IMHO is the core beauty of a graph computation framework.\r\n\r\n\tBy looking at available devices for graph construction, it is making an implicit assumption that the graph will be executed on the same device, which is often not a valid assumption.\r\n\r\n\tConsequences: These Keras layers do not behave properly if the graph is not executed on the same device. Examples include:\r\n\t(1) Creating a graph for deployment (on different machines)\r\n\t(2) Architecture search (where some worker generates graphs and other workers run it)\r\n\t(3) Distributed graph with heterogeneous workers, where the whole graph can be constructed on one single worker.\r\n\r\n\tThe automatic format conversion, if needed, should be done on the execution level instead.\r\n\r\n3. Looking at GPU devices has side effects. This is an unfortunate fact.\r\n  \r\n   Consequences: After constructing the graph with these Keras layers, users cannot create sessions with custom configs, and as a result cannot use Horovod, set memory fraction, and many others.\r\n\r\n   Workaround: Create session before graph. But this would break the define-and-run standard paradigm of TF 1.0. Most code using TF is not written like this.\r\n\r\n\r\nMy recommendations:\r\n+ The first issue obviously needs to be addressed.\r\n+ For backward compability with previous versions, adds a switch so that these layers do not look at devices when called from `tf.layers`, but can look at devices when called from `tf.keras.layers`.\r\n\tI personally prefer to see the code crash (rather than secretly transpose many times) when there are no appropriate kernels registered on the devices.\r\n+ In the long run it's best to not look at devices at all and transform the graph in execution.\r\n", "The implementation of \r\n```\r\ndef _has_nchw_support():\r\n  explicitly_on_cpu = _is_current_explicit_device('CPU')\r\n  gpus_available = bool(_get_available_gpus())\r\n  return not explicitly_on_cpu and gpus_available\r\n```\r\nappears to have more bugs than what I pointed out above: it does not handle `DeviceSpec` correctly. This makes valid code to crash, reported in https://github.com/tensorflow/tensorflow/issues/27259 and https://github.com/tensorflow/tensorflow/pull/23197.\r\n\r\nThese issues do not exist in TF 1.12 when the implementation of `Conv2DTranspose` is not backed by Keras.", "@ppwwyyxx,\r\nSorry for the delayed response. When we execute the code,\r\n\r\n```python\r\nimport tensorflow as tf\r\na = tf.placeholder(tf.float32, [100, 100, 100, 100])\r\nb = tf.layers.Conv2DTranspose(3, 3, data_format='channels_first')\r\noutput = b.apply(a)\r\n```\r\n\r\nusing the latest version of `Tensorflow` with slight modifications with respect to Compatibility, we see that `GPUs` are no more initialized. \r\n\r\nPlease find [the Gist](https://colab.research.google.com/gist/rmothukuru/723092b9790d3cad3600d53841da6809/25946.ipynb) of the working code. Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25946\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25946\">No</a>\n"]}, {"number": 25945, "title": "Measuring the prediction accuracy for the tensorflow-lite android version", "body": "I am using the tensorflow-lite version (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo). Can anyone please tell me how to compute the object detection accuracy or prediction accuracy while using this?\r\n\r\n", "comments": ["This is a very general question, and better suited for posting on StackOverflow. Are you interesting in testing the runtime accuracy against novel images? Or against the original source training/validation data set? In any case, if you wouldn't mind posting on StackOverflow you'll get a more appropriate response.", "@jdduke, thanks for the reply and the suggestion. FYI, I want to get the runtime accuracy against novel images. AS I observed the detection time on the phone using the GPU is really fast (~30 ms) for floatMobileNet classifier but what I observed the accuracy is really bad. can the prediction probability be treated as runtime prediction accuracy? although the threshold value seems really less 0.3, mentioned in the code. Any suggestion on calculating runtime prediction accuracy will be highly helpful.", "The probability, while related to accuracy, is quite different.\r\n\r\nIf you're seeing dramatically different results between the GPU path and the CPU path, that could be a bug. If you could provide an image/screenshot of an example that is wrong on the GPU (and right on the CPU), that would help.", "@jdduke , for CPU and GPU the results are the same but predictions are mostly wrong. That's my concern. ", "In general, the MobileNet classification models only make claims about the source data set (ImageNet), for which it achieves ~70% accuracy (see https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet). There are only 1000 labels in that set, so it's unsurprising that you're not seeing perfect accuracy across arbitrary images, but you should be getting close for common objects."]}, {"number": 25944, "title": "Is tensorflow-mkl version same as tensorflow?", "body": "OS: windows 10\r\nIntel said that if I want to install the mkl version of tensorflow on windows 10, I have to use the command as following:\r\n_conda install tensorflow-mkl_\r\n\r\nHowever, Anaconda said that if I want to install the mkl version of tensorflow, I just need to execute the following command no matter my OS is windows or linux:\r\n_conda install tensorflow_\r\n\r\nI just want to know if tensorflow is the same as tensorflow-mkl when I install tensorflow using the _conda install_ command ?", "comments": ["No, they are not the same on windows yet. Ideally, tensorflow from Anaconda must install mkl optimizations by default. However, due to old run time dependencies on windows, eigen version of tensorflow takes precedence over mkl version. This discrepancy has already been reported to Anaconda, and they are working on fixing their SAT solver to prefer mkl by default. In the meantime, please do ```conda install tensorflow-mkl``` to install mkl version. We'll notify once Anaconda makes the neccessary changes. ", "addressed the original question.  Please reopen if you have further comments/questions", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25944\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25944\">No</a>\n"]}, {"number": 25943, "title": "\"Issue #22387: Compilation Error MSVC 2017\"", "body": "Added const specifier in op_kernel.h for compilation to go through on Microsoft Visual Studio 2017. See https://github.com/tensorflow/tensorflow/issues/22387\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\nGooglers can find more info about SignCLA and this PR by [following this link](go/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F25943).\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\nGooglers can find more info about SignCLA and this PR by [following this link](go/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F25943).\n\n<!-- ok -->", "sorry, already fixed in https://github.com/tensorflow/tensorflow/commit/ec727016282383aacf9d26386b01f6bdbd65b14b "]}, {"number": 25942, "title": "Tensorflow Import Error (numpy.core.multiarray failed to import)", "body": "Trying to import Tensorflow in iPython while in an conda environment. An issue with Numpy seems to be breaking the import. I have uninstalled and reinstalled numpy and tensorflow (CPU) using Pip commands within the environment. Thanks everyone\r\n\r\n*System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow version: 1.13\r\n- Python version: 3.7.2\r\n- Installed using virtualenv? pip? conda?: pip install --upgrade tensorflow\r\n\r\n\r\nIn [1]: import tensorflow\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\nImportError: DLL load failed: The specified module could not be found.\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\nImportError: numpy.core.multiarray failed to import\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nSystemError                               Traceback (most recent call last)\r\n~\\AppData\\Local\\conda\\conda\\envs\\objectdetection\\lib\\importlib\\_bootstrap.py in _find_and_load(name, import_)\r\n\r\nSystemError: <class '_frozen_importlib._ModuleLockManager'> returned a result with an error set\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\nImportError: numpy.core._multiarray_umath failed to import\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\nImportError: numpy.core.umath failed to import\r\n2019-02-20 07:42:30.475025: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr", "comments": ["Take a look at #559 . Usually this seems to happen when you have had an older version of numpy installed somewhere. pip isn't always to be trusted when upgrading and installing new packages. \r\n\r\nI got the same exact error in one conda env where I had a previous tensorflow version installed and tried to install a new one. It was easily solved by starting a new conda environment and installing everything from scratch. But you can look in to the referenced issue and try with manually deleting numpy installations.", "In addition to @Lauler 's suggestion, Python 3.7 is supported in TensorFlow 1.13.0-rc2 build.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "In case this is still an issue for some users, it happened with TensorFlow 1.13.1:\r\n```\r\nModuleNotFoundError: No module named 'numpy.core._multiarray_umath'\r\nImportError: numpy.core.multiarray failed to import\r\n```\r\nUpgrading numpy from 1.15.4 to 1.16.4 in a virtual environment with `pip3 install --upgrade numpy` can fix it.\r\n", "Didn't worked. Still same issue and kernel dies once I try importing tensorflow", "maybe  installed tensorflow-gpu first and tensorflow after that \r\njust uninstall the later one or reinstall only one version can repair this issue ", "Broken again in numpy 1.17.0", "I also think the same", "I had the same problem and this worked for me:\r\n`````\r\npip uninstall numpy\r\npip install -U numpy\r\n`````", "![image](https://user-images.githubusercontent.com/10083536/80094568-1bfd8c00-8584-11ea-8e93-a21e1faa4c63.png)\r\n", "i am getting numpy but unable to use cv2\r\n\r\nplease help", "Try This...\r\nRun this Code One by One...\r\n\r\npip uninstall numpy\r\npip install numpy==1.19.3", "**pip install numpy==1.19.3** is ok, it works", "finally something works of me 1.19.3"]}, {"number": 25941, "title": "Fix #24818 by updating linalg_ops.py", "body": "Fix #24818 by replacing the circular reference to `tf.linalg.eigvalsh`  with `tf.linalg.eigh`  [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_ops.py#L338) which also returns two outputs.\r\nI believe that `tf.linalg.eigh` is the correct equivalent to resolve:\r\n1. Circular Reference\r\n2. Two outputs instead of one output, highlighted as \"(possibly ignoring the second output)\" in [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_ops.py#L338)", "comments": ["Nagging Reviewer @annarev: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied."]}, {"number": 25940, "title": "removing extra bool array as it is not needed", "body": "removing extra bool array as it is not needed for the search of reachable input nodes", "comments": []}, {"number": 25939, "title": "Docker containers with Python 3.7", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version 1.12:\r\n- Are you willing to contribute it (Yes/No): yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.** TensorFlow docker image is not available for Python 3.6 or Python 3.7. It would be good if TensorFlow is supported in Python 3.6 or 3.7 to take advantage of the latest Python version capabilities.\r\n\r\n**Will this change the current api? How?** I don't believe so. \r\n\r\n**Who will benefit with this feature?** All users who want to use the Python 3.6 and 3.7 capabilities.\r\n\r\n**Any Other info.** \r\n", "comments": ["TensorFlow 1.13's release candidates support Python 3.7 (there are a few other issues that cover this) -- you're welcome to try out one of the pip pre-releases on your own machine or on a custom Docker image. \r\n\r\nOur Docker containers are based on Ubuntu 16.04, which doesn't offer Python 3.7. I don't think we have any plans to upgrade them to support Python 3.7 right now (which would need some PPA work, probably), so if that functionality is very important to you, please consider making a contribution to our [Dockerfiles](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/dockerfiles).", "Thanks for the information! \r\nCould you please clarify? Can I use TensorFlow 1.13 release images/tags to get Python 3.7? That is does image tags - 1.13.0rc2-gpu , 1.13.0rc2-gpu-py3 etc . come by default with python 3.7? Then, I can use any of the 1.13 images and I don't need ubuntu 18.x. Could you please confirm? \r\n\r\nThanks!", "Docker containers run an OS different from the host machine, so when you run one of those images you are effectively dropped into an Ubuntu 16.04 environment. That's the case for all of our images, so none of them will get you Python 3.7.\r\n\r\nYou would have to create your own Docker image to use Python 3.7, or modify an existing container to install Python 3.7. If you have any more questions about that, please head to Stack Overflow, which is more fitting for how-can-I... questions.", "Thanks for the reply! Looking forward to a stable 3.7 release, for now I will make my own dockerfile based off https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.gpu\r\nCheers!", "Hi! I'm new to this, but noticed the last comment is from March.  Does a new dockerfile need to be created, or is this resolved?", "IIRC the dockerfiles are still using the same older Ubuntu base.", "I can try creating a dockerfile to be used to build an image and subsequent containers supporting Python 3.7 - might be helpful.", "@Ademord  have you managed to create such a Dockerfile? If so, would you mind sharing it, please? Or, even better, directly the image. Cheers! ", "It's not too hard to write a dockerfile based on the dockerfiles from the tensorflow repo.\r\nMost of the code below is copied over fromt ensorflow with a little help of googling around.\r\n\r\nThe idea is to install python3.7 (or any other version) seperately on the system an then use it to install tensorflow. It should be possible to automate this to build the tensorflow image for different python versions, but this would change the tagging behaviour of the tensorflow docker repo.\r\n\r\nIf this is a desired feature I woul be happy to contribute (although I never contributed to tensorflow before and therefore I am a complete newbie).\r\n```\r\nARG UBUNTU_VERSION=16.04\r\n\r\nARG CUDA=10.0\r\nFROM nvidia/cuda:${CUDA}-base-ubuntu${UBUNTU_VERSION} as base\r\nARG CUDA=10.0\r\nARG CUDADASH=10-0\r\nARG CUDNN=7.4.1.5-1\r\n\r\nRUN apt-get update && apt-get install -y --no-install-recommends \\\r\n        build-essential \\\r\n        cuda-command-line-tools-${CUDADASH} \\\r\n        cuda-cublas-${CUDADASH} \\\r\n        cuda-cufft-${CUDADASH} \\\r\n        cuda-curand-${CUDADASH} \\\r\n        cuda-cusolver-${CUDADASH} \\\r\n        cuda-cusparse-${CUDADASH} \\\r\n        curl \\\r\n        libcudnn7=${CUDNN}+cuda${CUDA} \\\r\n        libfreetype6-dev \\\r\n        libhdf5-serial-dev \\\r\n        libzmq3-dev \\\r\n        pkg-config \\\r\n        software-properties-common \\\r\n        unzip \\\r\n        wget\r\n\r\nRUN apt-get update && \\\r\n        apt-get install nvinfer-runtime-trt-repo-ubuntu1604-5.0.2-ga-cuda${CUDA} \\\r\n        && apt-get update \\\r\n        && apt-get install -y --no-install-recommends libnvinfer5=5.0.2-1+cuda${CUDA} \\\r\n        && apt-get clean \\\r\n        && rm -rf /var/lib/apt/lists/*\r\n\r\nENV LD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH\r\n\r\nARG PYTHON=python3.7\r\n\r\nENV LANG C.UTF-8\r\n\r\nRUN add-apt-repository ppa:deadsnakes/ppa && apt-get update && apt-get install -y ${PYTHON}\r\n\r\nRUN wget https://bootstrap.pypa.io/get-pip.py\r\nRUN ${PYTHON} get-pip.py\r\nRUN ln -sf /usr/bin/${PYTHON} /usr/local/bin/python3\r\nRUN ln -sf /usr/local/bin/pip /usr/local/bin/pip3\r\n\r\nRUN pip3 --no-cache-dir install --upgrade \\\r\n    pip \\\r\n    setuptools\r\n\r\nRUN ln -s $(which ${PYTHON}) /usr/local/bin/python\r\n\r\nARG TF_PACKAGE=tensorflow-gpu==1.13.1\r\nRUN pip3 install --upgrade ${TF_PACKAGE}\r\n```", "In case someone struggle as well we packed https://github.com/echoes-ai/tensorflow-dockerfiles\r\nboth with cuda and cpu versions. Contributions are welcome!", "https://github.com/puzl-ee/tensorflow \r\n\r\n```\r\ndocker build \\\r\n    --build-arg INTERPRETER=python3.8 \\\r\n    ...\r\n```", "wow, I came here to ask for python3.8, but I see that all are waiting for python3.7 for 2 years already", "maybe this can help someone in 2021. \r\n- use base dockerfile from tensorflow github (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/dockerfiles/gpu.Dockerfile)\r\n- modify it for your cuda version \r\n\r\nEdit the base image, which in this case is nvidia/cuda:11.0-base-ubuntu18.04. Check tags on tensorflow dockerhub (https://hub.docker.com/r/tensorflow/tensorflow/tags?page=1&ordering=last_updated&name=11.0)\r\n\r\nFor this dockerfile, the libcudnn8 version was modified according to the versions available on the cudnn archive (https://developer.nvidia.com/rdp/cudnn-archive). \r\nSpecify the tf version at the end. \r\n\r\nAfter that add your workload.\r\n\r\n```bash\r\nARG UBUNTU_VERSION=18.04\r\n\r\nARG ARCH=\r\nARG CUDA=11.0\r\nFROM nvidia/cuda${ARCH:+-$ARCH}:${CUDA}-base-ubuntu${UBUNTU_VERSION} as base\r\n# ARCH and CUDA are specified again because the FROM directive resets ARGs\r\n# (but their default value is retained if set previously)\r\nARG ARCH\r\nARG CUDA\r\nARG CUDNN=8.1.0.77-1\r\nARG CUDNN_MAJOR_VERSION=8\r\nARG LIB_DIR_PREFIX=x86_64\r\nARG LIBNVINFER=7.2.2-1\r\nARG LIBNVINFER_MAJOR_VERSION=7\r\n\r\n# Needed for string substitution\r\nSHELL [\"/bin/bash\", \"-c\"]\r\n# Pick up some TF dependencies\r\nRUN apt-get update && apt-get install -y --no-install-recommends \\\r\n        build-essential \\\r\n        cuda-command-line-tools-${CUDA/./-} \\\r\n        libcublas-${CUDA/./-} \\\r\n        cuda-nvrtc-${CUDA/./-} \\\r\n        libcufft-${CUDA/./-} \\\r\n        libcurand-${CUDA/./-} \\\r\n        libcusolver-${CUDA/./-} \\\r\n        libcusparse-${CUDA/./-} \\\r\n        curl \\\r\n        libcudnn8=${CUDNN}+cuda11.2 \\\r\n        libfreetype6-dev \\\r\n        libhdf5-serial-dev \\\r\n        libzmq3-dev \\\r\n        pkg-config \\\r\n        software-properties-common \\\r\n        unzip\r\n\r\n# Install TensorRT if not building for PowerPC\r\n# NOTE: libnvinfer uses cuda11.1 versions\r\nRUN [[ \"${ARCH}\" = \"ppc64le\" ]] || { apt-get update && \\\r\n        apt-get install -y --no-install-recommends libnvinfer${LIBNVINFER_MAJOR_VERSION}=${LIBNVINFER}+cuda11.1 \\\r\n        libnvinfer-plugin${LIBNVINFER_MAJOR_VERSION}=${LIBNVINFER}+cuda11.1 \\\r\n        && apt-get clean \\\r\n        && rm -rf /var/lib/apt/lists/*; }\r\n\r\n# For CUDA profiling, TensorFlow requires CUPTI.\r\nENV LD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:$LD_LIBRARY_PATH\r\n\r\n# Link the libcuda stub to the location where tensorflow is searching for it and reconfigure\r\n# dynamic linker run-time bindings\r\nRUN ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1 \\\r\n    && echo \"/usr/local/cuda/lib64/stubs\" > /etc/ld.so.conf.d/z-cuda-stubs.conf \\\r\n    && ldconfig\r\n\r\n# See http://bugs.python.org/issue19846\r\nENV LANG C.UTF-8\r\n\r\nRUN apt-get update && apt-get install -y \\\r\n    python3 \\\r\n    python3-pip\r\n\r\nRUN python3 -m pip --no-cache-dir install --upgrade \\\r\n    \"pip<20.3\" \\\r\n    setuptools\r\n\r\n# Some TF tools expect a \"python\" binary\r\nRUN ln -s $(which python3) /usr/local/bin/python\r\n\r\n# Options:\r\n#   tensorflow\r\n#   tensorflow-gpu\r\n#   tf-nightly\r\n#   tf-nightly-gpu\r\n# Set --build-arg TF_PACKAGE_VERSION=1.11.0rc0 to install a specific version.\r\n# Installs the latest version by default.\r\nARG TF_PACKAGE=tensorflow\r\nARG TF_PACKAGE_VERSION=2.4.1\r\nRUN python3 -m pip install --no-cache-dir ${TF_PACKAGE}${TF_PACKAGE_VERSION:+==${TF_PACKAGE_VERSION}}\r\n\r\n#COPY bashrc /etc/bash.bashrc\r\n#RUN chmod a+rwx /etc/bash.bashrc\r\n\r\n\r\nCOPY ./app /opt\r\n\r\nWORKDIR /opt\r\n\r\nCMD [\"python\",\"app-test.py\"]\r\n```\r\n\r\n\r\n\r\n\r\n\r\n", "We have this PR now for some images https://github.com/tensorflow/tensorflow/pull/48371", "Hi @2020testuser !  Python 3.7 is the minimum supported version now for the [2.x version](https://hub.docker.com/r/tensorflow/tensorflow). Can we close this issue now?", "@mohantym  It seems that images for 2.5 and below still have python 3.6.\r\nIs the fix you mention only for 2.6 and above?", "@ironcadiz ! It is showing python version 3.6.9 for TF 2.6.0 and Python 3.8.10 for 2.7 and 2.8 when I run this command. \r\n`docker run -it --rm - tensorflow/tensorflow:2.6.0 python -c \"import sys; print(sys.version)\" `  . So I thought it might have been addressed already. Can you please confirm from your side too?", "It is fixed on the release after https://github.com/tensorflow/tensorflow/pull/51914", "Running that same command I got `3.8.10` for `tensorflow/tensorflow:2.6.1-gpu` and `3.6.9` for `tensorflow/tensorflow:2.5.1-gpu`. Which is consistent with the behavior we've seen in our builds which is we are forced to use python 3.6 when using tf<2.6.\r\n\r\nI guess the issue is fixed but only for more recent versions\r\n", "Thanks @bhack @ironcadiz for confirmation. Moving this issue to closed status  as it has been updates on versions greater then 2.6.1.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25939\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25939\">No</a>\n"]}, {"number": 25938, "title": "keras.models.load_model() fails when the model uses a keras.losses.Loss subclass", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMac OS X 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\ntf.version.VERSION: '2.0.0-dev20190220'\r\ntf.version.GIT_VERSION: 'v1.12.0-8385-gaaef4e8e43'\r\n- Python version:\r\n3.6.8\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\n`keras.models.load_model()` raises a `ValueError` when the model to be loaded uses a `keras.losses.Loss` subclass, such as `keras.losses.Huber`.\r\n\r\n**Describe the expected behavior**\r\nShould load normally, and I should be able to continue training where it left off using the loss.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nX_train = np.random.randn(100, 2)\r\ny_train = np.random.randn(100, 1)\r\n\r\nmodel = keras.models.Sequential([keras.layers.Dense(1, input_dim=2)])\r\nmodel.compile(loss=keras.losses.Huber(2.0), optimizer=\"sgd\")\r\nmodel.fit(X_train, y_train, epochs=2)\r\nmodel.save(\"my_model.h5\")\r\nmodel = keras.models.load_model(\"my_model.h5\") # Raises a ValueErro\r\n```\r\n\r\n**Other info / logs**\r\nHere is the stacktrace:\r\n\r\n```pycon\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 248, in load_model\r\n    sample_weight_mode=sample_weight_mode)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\", line 456, in _method_wrapper\r\n    method(self, *args, **kwargs)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 281, in compile\r\n    loss, self.output_names)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 1142, in prepare_loss_functions\r\n    'following keys: {}'.format(name, output_names))\r\nValueError: Unknown entry in loss dictionary: class_name. Only expected following keys: ['dense']\r\n```\r\n\r\nI did some debugging, and I think I found the origin of the problem.  In `hdf5_format.py`, around line 233, the following lines use `convert_custom_objects()`, but they should be using `losses.deserialize()` and `metrics.deserialize()`.  I'll send a PR.\r\n\r\n```python\r\n      # Recover loss functions and metrics.\r\n      loss = convert_custom_objects(training_config['loss'])\r\n      metrics = convert_custom_objects(training_config['metrics'])\r\n      weighted_metrics = convert_custom_objects(\r\n          training_config.get('weighted_metrics', None))\r\n```", "comments": ["but it raise  NameError: name 'losses' is not defined", "Hi @zhaoyingjun , \r\n\r\n```python\r\nfrom tensorflow.python.keras import losses\r\n```\r\n\r\nPlease check out my PR #25956", "@ageron hi, i check  out your PR,but i load the model and train,the accuracy is 0.0000e+00 ,no any update", "@zhaoyingjun , I'm not sure the problem you are having is related to this issue, you might want to share some code (in a [gist](https://gist.github.com) if it's big).", "i find the reason, in my code:\r\nloss_object=tf.losses.SparseCategoricalCrossentropy()\r\n model.complie(loss=loss_object, optimizer=\"sgd\")\r\n\r\nit raise the error.\r\n\r\nthe i change my code to\r\n model.complie(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\")\r\n\r\nit is ok\r\n", "Can confirm changing the loss function to a string removes this error.", "This is fixed TF 2.0 nightly '2.0.0-dev20190718'\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25938\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25938\">No</a>\n", "> This is fixed TF 2.0 nightly '2.0.0-dev20190718'\r\n> Thanks!\r\n\r\n\r\n\r\n> Can confirm changing the loss function to a string removes this error.\r\n\r\nyeah, It solved for me.", "@ymodak can you please refer to the commit\\PR that fixes this?", "Hi there, I am still seeing the same issue. \r\n\r\nBelow is my model code.\r\n```\r\nclass HardNegativeContrastiveLoss(keras.losses.Loss):\r\n    \"\"\"\r\n    Args:\r\n      margin: Control the similarity threshold between two vectors\r\n      name: Name of the loss function.\r\n    \"\"\"\r\n    def __init__(self, margin=1.0, name='hard_negative_contrastive_loss'):\r\n        super(HardNegativeContrastiveLoss, self).__init__(name=name)\r\n        self.margin = margin\r\n\r\n    def call(self, y_true, y_pred):\r\n        \"\"\"y_true are all positive, they are passed in but not used,\r\n        y_pred contains the embeddings of two sources, the two sources have dimensions\r\n        (batch_size, embedding_dim), they have been concatenated in a new axis to form\r\n        the y_pred: (batch_size, embdding_dim, 2)\r\n        \"\"\"\r\n        \r\n        embedding1 = y_pred[:, :, 0]\r\n        embedding2 = y_pred[:, :, 1]\r\n        embedding1_norm = K.l2_normalize(embedding1, axis=1)\r\n        embedding1_norm_t = K.transpose(embedding1_norm)\r\n\r\n        embedding2_norm = K.l2_normalize(embedding2, axis=1)\r\n\r\n        dot_product = K.dot(embedding2_norm, embedding1_norm_t)\r\n        distance = 2.0 - 2.0 * dot_product\r\n\r\n        distance = tf.maximum(distance, 0.0)\r\n        mask = tf.cast(tf.equal(distance, 0.0), 'float32') # correction to avoid sqrt(0.0) that would cause gradient of sqrt to be infinite\r\n        distance = distance + mask * 1e-16\r\n        distance = tf.sqrt(distance)\r\n        distance = distance * (1.0 - mask)  # correct distance for epsilon added\r\n\r\n        positive_score = tf.linalg.diag_part(distance)\r\n\r\n        num_sample = K.shape(embedding1)[0]\r\n        indices_equal = tf.cast(tf.eye(num_sample), tf.bool)\r\n        indices_not_equal = tf.logical_not(indices_equal)\r\n        mask_indices_not_equal = tf.cast(indices_not_equal, 'float32')\r\n\r\n        negative_scores = tf.multiply(mask_indices_not_equal, distance)\r\n\r\n        hardest_negative_score_embedding1 = tf.reduce_min(negative_scores + 4.0 * tf.cast(indices_equal, 'float32'), axis=1)\r\n        hardest_negative_score_embedding2 = tf.reduce_min(negative_scores + 4.0 * tf.cast(indices_equal, 'float32'), axis=0)\r\n\r\n        score = K.mean(2.0 * positive_score + tf.maximum(self.margin - hardest_negative_score_embedding1, 0)  + tf.maximum(self.margin - hardest_negative_score_embedding2, 0))\r\n        return score\r\n\r\ndef basic_embedding_model_cosine(query_vocab_size, title_vocab_size, embedding_dim = 128, use_mask_zero=True):\r\n    \"\"\"model that evaluates similarity of query and title embeddings using dot product\r\n    uses word tokenization\"\"\"\r\n\r\n    print('query vocab size:', query_vocab_size)\r\n    print('title vocab size:', title_vocab_size)\r\n    mirrored_strategy = tf.distribute.MirroredStrategy()\r\n    with mirrored_strategy.scope():\r\n        query_input = keras.Input(name=\"query_input\", shape=(constants.MAX_QUERY_WORD_LENGTH,))\r\n        query_embedding = layers.Embedding(input_dim=query_vocab_size + 1,\r\n                                    output_dim=embedding_dim,\r\n                                    mask_zero=use_mask_zero,\r\n                                    name=\"query_embedding\")(query_input)\r\n        query_embedding_normalized = layers.BatchNormalization(name='query_embedding_normalized')(query_embedding)\r\n        query_embedding_vector = layers.GlobalAveragePooling1D(name='query_mean')(query_embedding_normalized)\r\n\r\n        title_input = keras.Input(shape=(constants.MAX_TITLE_WORD_LENGTH,), name='title_input')\r\n        title_embedding = layers.Embedding(input_dim=title_vocab_size + 1,\r\n                                   output_dim=embedding_dim,\r\n                                   mask_zero=use_mask_zero,\r\n                                   name=\"title_embedding\")(title_input)\r\n        title_embedding_normalized = layers.BatchNormalization(name='title_embedding_normalized')(title_embedding)\r\n        title_embedding_vector = layers.GlobalAveragePooling1D(name='title_mean')(title_embedding_normalized)\r\n\r\n        loss_output = layers.concatenate([tf.expand_dims(query_embedding_vector,-1), tf.expand_dims(title_embedding_vector,-1)],name='loss_output')\r\n        similarity_output = layers.Dot(axes=1,normalize=True,name='similarity_output')([title_embedding_vector, query_embedding_vector])\r\n\r\n        model = keras.Model(inputs=[query_input, title_input], outputs=[loss_output, similarity_output])\r\n\r\n        optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\r\n        custom_loss = HardNegativeContrastiveLoss(margin=1.0)\r\n\r\n        model.compile(\r\n            optimizer=optimizer,\r\n            loss={'loss_output': custome_loss},\r\n            metrics={'similarity_output':binary_accuracy}\r\n        )\r\n\r\n        return model\r\n```\r\nAfter successfully train and save the model, I come across with 'Unknown entries in loss dictionary' error. Full error message as follows:\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-55-8b153b1e1e9a> in <module>()\r\n----> 1 unreplicated_model = tf.keras.models.load_model(model_path)\r\n      2 # unreplicated_model.compile(\r\n      3 #     optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\r\n      4 #     loss={'output_1':HardNegativeContrastiveLoss(margin=1.0)}\r\n      5 # )\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/save.py in load_model(filepath, custom_objects, compile)\r\n    148   if isinstance(filepath, six.string_types):\r\n    149     loader_impl.parse_saved_model(filepath)\r\n--> 150     return saved_model_load.load(filepath, compile)\r\n    151 \r\n    152   raise IOError(\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py in load(path, compile)\r\n     91     if model._training_config is not None:  # pylint: disable=protected-access\r\n     92       model.compile(**saving_utils.compile_args_from_training_config(\r\n---> 93           model._training_config))  # pylint: disable=protected-access\r\n     94 \r\n     95   return model\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\r\n    334     # Prepare list of loss functions, same size of model outputs.\r\n    335     self.loss_functions = training_utils.prepare_loss_functions(\r\n--> 336         self.loss, self.output_names)\r\n    337 \r\n    338     target_tensors = self._process_target_tensor_for_compile(target_tensors)\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_utils.py in prepare_loss_functions(loss, output_names)\r\n   1339   \"\"\"\r\n   1340   if isinstance(loss, collections_abc.Mapping):\r\n-> 1341     generic_utils.check_for_unexpected_keys('loss', loss, output_names)\r\n   1342     loss_functions = []\r\n   1343     for name in output_names:\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/generic_utils.py in check_for_unexpected_keys(name, input_dict, expected_values)\r\n    589     raise ValueError('Unknown entries in {} dictionary: {}. Only expected '\r\n    590                      'following keys: {}'.format(name, list(unknown),\r\n--> 591                                                  expected_values))\r\n    592 \r\n    593 \r\n\r\nValueError: Unknown entries in loss dictionary: ['loss_output']. Only expected following keys: ['output_1', 'output_2']\r\n```\r\n\r\nAnyone can help?\r\n\r\nI am using tf.__version__='2.0.0' on Ubuntu 16.6.", "> Can confirm changing the loss function to a string removes this error.\r\n\r\nHow about my above case that we cannot assign a string in customer loss", "Same issue on the stable 2.0.0 release :( ", "I have a similar problem.\r\nI trained and saved the model with TripletLoss which is a subclass of tf.keras.losses.Loss.\r\nWhen I load model as follows: \r\n```python\r\nmodel = load_model(\"test/test_model_ep09.h5\", {\"TripletLoss\": TripletLoss, \"PlainBlock\": PlainBlock})\r\nmodel.predict(x_val)\r\n```\r\nI got error message:\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-22-4ccd0b311ac4> in <module>()\r\n----> 1 model = load_model(\"test/test_model_ep09.h5\", {\"TripletLoss\": TripletLoss, \"PlainBlock\": PlainBlock})\r\n      2 model.predict(x_val)\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/saving/save.py in load_model(filepath, custom_objects, compile)\r\n    144   if (h5py is not None and (\r\n    145       isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\r\n--> 146     return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\r\n    147 \r\n    148   if isinstance(filepath, six.string_types):\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/saving/hdf5_format.py in load_model_from_hdf5(filepath, custom_objects, compile)\r\n    182       # Compile model.\r\n    183       model.compile(**saving_utils.compile_args_from_training_config(\r\n--> 184           training_config, custom_objects))\r\n    185 \r\n    186       # Set optimizer weights.\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/saving/saving_utils.py in compile_args_from_training_config(training_config, custom_objects)\r\n    232   loss_config = training_config['loss']  # Deserialize loss class.\r\n    233   if isinstance(loss_config, dict) and 'class_name' in loss_config:\r\n--> 234     loss_config = losses.get(loss_config)\r\n    235   loss = nest.map_structure(\r\n    236       lambda obj: custom_objects.get(obj, obj), loss_config)\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/losses.py in get(identifier)\r\n   1184     return deserialize(identifier)\r\n   1185   if isinstance(identifier, dict):\r\n-> 1186     return deserialize(identifier)\r\n   1187   elif callable(identifier):\r\n   1188     return identifier\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/losses.py in deserialize(name, custom_objects)\r\n   1173       module_objects=globals(),\r\n   1174       custom_objects=custom_objects,\r\n-> 1175       printable_module_name='loss function')\r\n   1176 \r\n   1177 \r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)\r\n    290     config = identifier\r\n    291     (cls, cls_config) = class_and_config_for_serialized_keras_object(\r\n--> 292         config, module_objects, custom_objects, printable_module_name)\r\n    293 \r\n    294     if hasattr(cls, 'from_config'):\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/utils/generic_utils.py in class_and_config_for_serialized_keras_object(config, module_objects, custom_objects, printable_module_name)\r\n    248     cls = module_objects.get(class_name)\r\n    249     if cls is None:\r\n--> 250       raise ValueError('Unknown ' + printable_module_name + ': ' + class_name)\r\n    251 \r\n    252   cls_config = config['config']\r\n\r\nValueError: Unknown loss function: TripletLoss\r\n```\r\nI had implemented in get_config() in TripletLoss, but still get this error\r\nDoes anyone can help me ?", "> i find the reason, in my code:\r\n> loss_object=tf.losses.SparseCategoricalCrossentropy()\r\n> model.complie(loss=loss_object, optimizer=\"sgd\")\r\n> \r\n> it raise the error.\r\n> \r\n> the i change my code to\r\n> model.complie(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\")\r\n> \r\n> it is ok\r\n\r\nHi, I change to use \r\nmodel.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\",metrics=['accuracy'])\r\n, but the accuracy keeps to be 0.58 and not change. When I use loss = tf.losses.SparseCategoricalCrossentropy(), the accuracy rised and looked normal. Do you have this problem?"]}, {"number": 25937, "title": "error import tensorflow", "body": "System information\r\nOS Platform: Windows 10 x64\r\nTensorFlow installed from: pip install tensorflow-gpu\r\nPython version: 3.5.3\r\nCUDA/cuDNN version: CUDA10.0 / cuDNN v7.4.2\r\nGPU model and memory: GTX 1060\r\nExact command to reproduce: import tensorflow\r\nDescribe the problem\r\nIt is impossible to import tensorflow in my present environment.I am getting the following error.\r\n---------------------------------------------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n~\\Anaconda2\\envs\\python3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\n~\\Anaconda2\\envs\\python3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n~\\Anaconda2\\envs\\python3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\n~\\Anaconda2\\envs\\python3\\lib\\imp.py in load_module(name, file, filename, details)\r\n    242         else:\r\n--> 243             return load_dynamic(name, filename, file)\r\n    244     elif type_ == PKG_DIRECTORY:\r\n\r\n~\\Anaconda2\\envs\\python3\\lib\\imp.py in load_dynamic(name, path, file)\r\n    342             name=name, loader=loader, origin=path)\r\n--> 343         return _load(spec)\r\n    344 \r\n\r\nImportError: DLL load failed: \u0394\u03b5\u03bd \u03ae\u03c4\u03b1\u03bd \u03b4\u03c5\u03bd\u03b1\u03c4\u03cc \u03bd\u03b1 \u03b5\u03bd\u03c4\u03bf\u03c0\u03b9\u03c3\u03c4\u03b5\u03af \u03b7 \u03ba\u03b1\u03b8\u03bf\u03c1\u03b9\u03c3\u03bc\u03ad\u03bd\u03b7 \u03bb\u03b5\u03b9\u03c4. \u03bc\u03bf\u03bd\u03ac\u03b4\u03b1.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-2-566d4aa4fae9> in <module>\r\n      1 \r\n----> 2 import tensorflow\r\n\r\n~\\Anaconda2\\envs\\python3\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     22 \r\n     23 # pylint: disable=g-bad-import-order\r\n---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     25 \r\n     26 try:\r\n\r\n~\\Anaconda2\\envs\\python3\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 from tensorflow.python.tools import component_api_helper\r\n\r\n~\\Anaconda2\\envs\\python3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \u0394\u03b5\u03bd \u03ae\u03c4\u03b1\u03bd \u03b4\u03c5\u03bd\u03b1\u03c4\u03cc \u03bd\u03b1 \u03b5\u03bd\u03c4\u03bf\u03c0\u03b9\u03c3\u03c4\u03b5\u03af \u03b7 \u03ba\u03b1\u03b8\u03bf\u03c1\u03b9\u03c3\u03bc\u03ad\u03bd\u03b7 \u03bb\u03b5\u03b9\u03c4. \u03bc\u03bf\u03bd\u03ac\u03b4\u03b1.\r\n", "comments": ["@quartermaine Could you follow more detailed instructions [here](https://www.tensorflow.org/install/pip) and let me know how it progresses. Please check the resources that worked for another windows10 [user](https://github.com/tensorflow/tensorflow/issues/25692). Thanks!", "@jvishnuvardhan I follow the instructions that you mention and the resources given but still no success.\r\n  ", "@quartermaine Could you uninstall tensorflow and python completely and please follow the instructions [here](https://github.com/jvishnuvardhan/Installing-TensorFlow-GPU-on-Windows-10-TF1.12). Please let me know how it progresses. Thanks!", "Yes,I finally manage to make the installation everything works.Thank you for the detailed guide ", "@quartermaine Good to know it worked. I am closing this issue. Thanks again!"]}, {"number": 25936, "title": "Adding labels to images while training", "body": "Hi @nsthorat @tensorflower-gardener,  we were about to implement image recognization\r\n\r\nI'm able to convert the image to tensor, now I want to add the label and give training to that image. If this gets succeeded then I want to do the same for multiple images with their respective labels. Any suggestions would be helpful.\r\n\r\nThanks", "comments": ["Could you please tell me more about the model you are using and clarify more about the problem you are facing with labelling and training?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n\r\n"]}, {"number": 25935, "title": "Bazel incompatible changes", "body": "Tensorflow doesn't build with Bazel incompatible flags.\r\nBased on our CI (https://buildkite.com/bazel/bazel-at-release-plus-incompatible-flags/builds/95), the following flags will fail:\r\n\r\n* `--incompatible_new_actions_api`\r\n* `--incompatible_no_support_tools_in_action_inputs`\r\n* `--incompatible_no_transitive_loads`\r\n* `--incompatible_disable_deprecated_attr_params`\r\n* `--incompatible_disallow_load_labels_to_cross_package_boundaries`\r\n\r\n\r\nFrom what I've seen, updating the dependency on rules_closure will fix some of the problems.\r\n\r\nFor `--incompatible_no_transitive_loads`, the simplest is to reexport some symbols explicitly (see https://github.com/bazelbuild/bazel/issues/5636).\r\n\r\nIf you run `buildifier --lint=fix tensorflow/tensorflow.bzl`, it will also fix some issues.\r\n\r\nLet me know if you need help\r\n(bug also filed inside Google: b/124051098)", "comments": ["I got error when converting tfmodel (.pb) to tflite (.lite): Unable to load package for '//tensorflow/tools/def_file_filter:def_file_filter_configure.bzl': BUILD file not found on package path\r\nIs that because of incompatibility?", "Add `--incompatible_remove_old_python_version_api` to that list. I'm flipping this flag to true now, for release in Bazel 0.25. The fix #26752 will bump TF's min required Bazel version to 0.22.", "I am sorry for dropping this issue.\r\nI saw that Yun has done many of these changes.\r\n", "I am sorry to forget. I think this is complete. We even were able to build TF with bazel 3.0 now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25935\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25935\">No</a>\n"]}, {"number": 25934, "title": "numpy.dtype size changed, may indicate binary incompatibility", "body": "## Cleanest reproduction:\r\n\r\nRun in a GPU Google Colaboratory session:\r\n\r\n    !pip3 install tensorflow-gpu==1.13.0rc2 numpy==1.15.0\r\n    !python3.6 -c 'import tensorflow'\r\n\r\nHowever, how I came across the issue:\r\n\r\n**System information**\r\n- Ubuntu 18.04\r\n- Dell XPS 15 2016 laptop\r\n- installed a binary using pip3 install tensorflow-gpu==1.13.0rc2\r\n- TensorFlow version: 1.13.0rc2\r\n- Python 3.6.5\r\n- virtualenv with python3.6 and pip3\r\n- N/A did not installed from source\r\n- N/A did not compiled from source\r\n- cuda 10.0 / libcudnn7_7.4.2.24-1+cuda10.0_amd64.deb\r\n- GPU model and memory:\r\n\r\n    NVIDIA-SMI 410.79       Driver Version: 410.79       CUDA Version: 10.0\r\n    GeForce GTX 960M\r\n    91MiB /  2004MiB\r\n\r\n## Problem:\r\n\r\nWarning when importing tensorflow, only when numpy version is 1.15.0\r\n\r\n    /home/herbert/.virtualenvs/actigraph/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\r\n      return f(*args, **kwds)\r\n    /home/herbert/.virtualenvs/actigraph/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\r\n      return f(*args, **kwds)\r\n\r\n## Commands to reproduce:\r\n\r\n    pip3 install tensorflow-gpu==1.13.0rc2 numpy==1.15.0\r\n    python3.6 -c 'import tensorflow'\r\n\r\n## Commands to fix:\r\n\r\n    pip3 install tensorflow-gpu==1.13.0rc2 numpy==1.16.1\r\n    python3.6 -c 'import tensorflow'\r\n\r\nOr probably, leave the version of numpy out (`==1.16.1`) and include `-U` to upgrade to the newest numpy.\r\n\r\n    pip3 install -U tensorflow-gpu==1.13.0rc2 numpy\r\n    python3.6 -c 'import tensorflow'", "comments": ["If wondering, I installed `tensorflow-gpu==1.13.0rc2` because I upgraded to cuda 10.0 from cuda 9.0 and didn't want to go through the hassle of downgrading cuda for compatibility with a stable tf, due to the risk of messing up the nvidia-drivers and killing my desktop environment.\r\n\r\nTherefore I figured using a `rc2` tf is less troublesome, expecting the stable tf `1.13` to arrive soon.\r\n\r\nWhen I came across this issue, I figured the numpy-requirement for tf==1.13 should be tightened or the warning should be fixed for `np==1.15.0`.", "duplication of #21225\r\n\r\nAs @feranick said:\r\n> The problem is in numpy itself. This will be fixed in numpy v. 1.15.1\r\n> [numpy/numpy#11628](https://github.com/numpy/numpy/issues/11628)", "@av8ramit Hi, Amit, can we increase the minimum version of numpy to 1.15.1?  https://github.com/tensorflow/tensorflow/blob/9d508106b32eb6518912501d29a80ff9967dfe05/tensorflow/tools/pip_package/setup.py#L57", "> @av8ramit Hi, Amit, can we increase the minimum version of numpy to 1.15.1?\r\n\r\nYes, that was my point, these warnings and the potential side-effects should not appear after `pip install`ing tensorflow, especially since the actual library causing the error already fixed it.", "> duplication of #21225\r\n\r\nSorry, I checked for all issues with dtype in the title, but missed this one", "Thanks for pointing this out. @facaiy or @prinsherbert please create a PR and I'll approve it.", "I can't for at least the next two months, sorry\n\nOp di 2 apr. 2019 20:00 schreef Amit Patankar <notifications@github.com>:\n\n> Thanks for pointing this out. @facaiy <https://github.com/facaiy> or\n> @prinsherbert <https://github.com/prinsherbert> please create a PR and\n> I'll approve it.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/25934#issuecomment-479120107>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AGr7Jq09SBweWaGNtR9BI-Lw0vYFKCGwks5vc5qygaJpZM4bFYsC>\n> .\n>\n", "We see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.5  version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25934\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25934\">No</a>\n"]}, {"number": 25933, "title": "tensorflow Module object has no attribute keras.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Pip install...\r\n- TensorFlow version: latest\r\n- Python version: python 2.12.15\r\n- Installed using virtualen? pip? conda?:pip/virtualenv\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: Cuda 9.2\r\n- GPU model and memory: NVIDIA MX150\r\n\r\n\r\n\r\n**Describe the problem**\r\nI have tried to install tensorflow with gpu support both with pip and virualenv several times, I have got following erros. anyone has idea?\r\n\r\n> `python\r\nPython 2.7.15rc1 (default, Nov 12 2018, 14:31:15) \r\n[GCC 7.3.0] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"tensorflow.py\", line 2, in <module>\r\n    mnist = tf.keras.datasets.mnist\r\nAttributeError: 'module' object has no attribute 'keras'`\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I suspect its a due to lower tf version.  What's your tf version? Can you try updating tf\r\n```pip install -U tensorflow```", "Hi, I have 1.12.0\r\n\r\nIs there any requirement for CUDA version?\r\n\r\nIt looks bit ridiculous because previously I have installed tensorflow on my old laptop.\r\n\r\n`Name: tensorflow\r\nVersion: 1.12.0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: opensource@google.com\r\nLicense: Apache 2.0\r\nLocation: /.local/lib/python3.6/site-packages\r\nRequires: tensorboard, protobuf, keras-applications, wheel, astor, absl-py, termcolor, grpcio, numpy, six, gast, keras-preprocessing\r\nRequired-by: \r\n`", "I think there was some CUDA toolkit problem, it worked after following \r\n\r\n[https://medium.com/@taylordenouden/installing-tensorflow-gpu-on-ubuntu-18-04-89a142325138](url)", "Yes, if you are using TF 1.12 gpu version then you have to install cuda 9.0. I will close this issue since you have found the solution. Thanks!"]}, {"number": 25932, "title": "tf.data.experimental.CsvDataset iterates forever", "body": "I have a csv file that has number of rows equal to 1 batch but when I run my code it loops that batch forever and never throws an exception. How can I make the iterator stop after it reaches the end of the dataset? Is this a bug?\r\n\r\n```\r\ntrain_dataset_1 = tf.data.experimental.make_csv_dataset([os.path.join(FLAGS.data_path, \"small_interpolated_2_4_6_ONE.csv\")],\r\n                                                       batch_size=BATCH_SIZE*SEQ_LEN,\r\n                                                       select_columns=[5,6],\r\n                                                       label_name='angle',\r\n                                                       shuffle=False,\r\n                                                       column_defaults=[tf.string, tf.float32])\r\n\r\niterator_1 = Iterator.from_structure(train_dataset_1.output_types, train_dataset_1.output_shapes)\r\n\r\ntrain_iter_init_op_1 = iterator_1.make_initializer(train_dataset_1)\r\nnext_batch_op_1 = iterator_1.get_next()\r\n \r\n \r\n \r\nconfig = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth = True), \r\n                        allow_soft_placement=True, \r\n                        log_device_placement=False)                      \r\nsess = tf.Session(config=config)\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(train_iter_init_op_1)\r\n\r\n\r\nwhile True:\r\n    try:      \r\n        img_batch_dict, label_batch = sess.run(next_batch_op_1)\r\n        \r\n\r\n        for k in img_batch_dict[\"filename\"]:\r\n            print(k)\r\n            \r\n        print()           \r\n    except tf.errors.OutOfRangeError:\r\n        print(\"Epoch ended 11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\")\r\n        break\r\n```", "comments": ["Can you provide all necessary environment information required by issue template?", "It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n", "Is this issue resolved? @chris12766 "]}, {"number": 25931, "title": "Model with mirror pad can run using Desktop tflite but can't run using Android tflite", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu16.04\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (or github SHA if from source):'1.13.0-dev20190215'\r\n\r\nI notice that since 11/20/2018, [mirror pad](https://github.com/tensorflow/tensorflow/issues/23962#issuecomment-442147097) was supported in TFlite, and I can successfully convert and interpret the converted model with newest tensorflow on computer.\r\n\r\nHowever, when I try to use that model on Android phone , it can't be interpreted. Here is message from Android studio:\r\n```\r\nE/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: android.example.com.tflitecamerademo, PID: 22917\r\n    java.lang.RuntimeException: Unable to start activity ComponentInfo{android.example.com.tflitecamerademo/com.example.android.tflitecamerademo.CameraActivity}: java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model\r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2856)\r\n        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2931)\r\n        at android.app.ActivityThread.-wrap11(Unknown Source:0)\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1620)\r\n        at android.os.Handler.dispatchMessage(Handler.java:105)\r\n        at android.os.Looper.loop(Looper.java:176)\r\n        at android.app.ActivityThread.main(ActivityThread.java:6701)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:246)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:783)\r\n     Caused by: java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.createModelWithBuffer(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:59)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:188)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:176)\r\n        at com.example.android.tflitecamerademo.ImageClassifier.<init>(ImageClassifier.java:106)\r\n        at com.example.android.tflitecamerademo.Camera2BasicFragment.onActivityCreated(Camera2BasicFragment.java:303)\r\n        at android.app.Fragment.performActivityCreated(Fragment.java:2620)\r\n        at android.app.FragmentManagerImpl.moveToState(FragmentManager.java:1296)\r\n        at android.app.FragmentManagerImpl.addAddedFragments(FragmentManager.java:2420)\r\n        at android.app.FragmentManagerImpl.executeOpsTogether(FragmentManager.java:2199)\r\n        at android.app.FragmentManagerImpl.removeRedundantOperationsAndExecute(FragmentManager.java:2153)\r\n        at android.app.FragmentManagerImpl.execPendingActions(FragmentManager.java:2054)\r\n        at android.app.FragmentManagerImpl.dispatchMoveToState(FragmentManager.java:3049)\r\n        at android.app.FragmentManagerImpl.dispatchActivityCreated(FragmentManager.java:2996)\r\n        at android.app.FragmentController.dispatchActivityCreated(FragmentController.java:178)\r\n        at android.app.Activity.performCreateCommon(Activity.java:7044)\r\n        at android.app.Activity.performCreate(Activity.java:7052)\r\n        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1214)\r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2809)\r\n        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2931)\u00a0\r\n        at android.app.ActivityThread.-wrap11(Unknown Source:0)\u00a0\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1620)\u00a0\r\n        at android.os.Handler.dispatchMessage(Handler.java:105)\u00a0\r\n        at android.os.Looper.loop(Looper.java:176)\u00a0\r\n        at android.app.ActivityThread.main(ActivityThread.java:6701)\u00a0\r\n        at java.lang.reflect.Method.invoke(Native Method)\u00a0\r\n        at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:246)\u00a0\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:783)\u00a0\r\n```\r\nand this is part of  build.gradle:\r\n```\r\nrepositories {\r\n    maven {\r\n        url 'https://google.bintray.com/tensorflow'\r\n    }\r\n    google()\r\n}\r\n\r\ndependencies {\r\n    compile fileTree(dir: 'libs', include: ['*.jar'])\r\n    androidTestCompile('com.android.support.test.espresso:espresso-core:2.2.2', {\r\n        exclude group: 'com.android.support', module: 'support-annotations'\r\n    })\r\n    compile 'com.android.support:appcompat-v7:25.2.0'\r\n    compile 'com.android.support.constraint:constraint-layout:1.1.0'\r\n    compile 'com.android.support:design:25.2.0'\r\n    compile 'com.android.support:support-annotations:25.3.1'\r\n    compile 'com.android.support:support-v13:25.2.0'\r\n\r\n    compile 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\n\r\n    testCompile 'junit:junit:4.12'\r\n}\r\n\r\n```\r\nBased on what I have experienced, `ByteBuffer is not a valid flatbuffer model` may caused by unsupported ops in model.\r\n\r\nSo I delete all the mirror pad(tf.pad), and use 'SAME' padding in conv, this time the model can run on both desktop and Android phone.\r\n\r\nBecause the tflite I use on android phone is `org.tensorflow:tensorflow-lite:0.0.0-nightly`, it should be the newest and should support mirror pad, then why it can't be interpreted?\r\n\r\nAny suggestion would be appreciated !", "comments": ["Hi,\r\nIs it possible to share the model, or if you have mini model that can reproduce this failure. So we can check what is failing.\r\n\r\nThanks", "Thanks for your reply @karimnosseir , \r\n\r\nI am trying to reproduce the error with a mini model but fail. I suspect it was caused by the third party api(tensorlayer) I used, and I am rewriting the model using tf-slim, if the error persist, I will leave a comment.\r\n\r\n", "Hi, @karimnosseir \r\n\r\nWith tf-slim, the `ByteBuffer is not a valid flatbuffer model` no longer exist.\r\n\r\nBut a strange thing still persist.\r\n\r\nIf I use \"same\" padding, a inference of an image use converted tflite model takes about 0.7g ram on my computer;  If I use \"reflect + 'valid'\" padding, the same process takes 5g ram.\r\n\r\nHere is the mini model:\r\n[issue.tar.gz](https://github.com/tensorflow/tensorflow/files/2926225/issue.tar.gz)\r\nit is very shallow but needs 5g ram to run with reflect padding. As it goes deeper, even more memory is needed. Is that normal?\r\n\r\nThanks\r\n\r\n", "HI, @karimnosseir  \r\nIs that issue reproducible?", "Hi PK15946,\r\n\r\nSorry for the delay. I didn't have time to check it. Will do next week and get back to you.\r\nSorry for the delay again.", "Ok, thank you in advance for your time @karimnosseir ", "Hi PK15946\r\n\r\nA fix was submitted earlier today. Can you try from master branch or nightly (after it includes the latest change) and let me know if you are still having problem.\r\n\r\nI am going to close this bug now. Feel free to create new issue if you still facing problems and assign it to me.\r\n\r\nThanks\r\n"]}, {"number": 25930, "title": "Typo error fixed in resolve_multiply_by_zero.cc", "body": "", "comments": ["`f9a5fdc` pushed this changes to master, closing this "]}, {"number": 25929, "title": "Fix warning in threadpool_dataset_op.cc", "body": "num_threads should be greater than 1 and its initialised with 0", "comments": ["`b0d8082 ` has pushed this changes to master, closing this PR"]}, {"number": 25928, "title": "Update mkl_concat_op.cc", "body": "accross -> across", "comments": []}, {"number": 25927, "title": "Update profiler.cc", "body": "acclerator -> accelerator", "comments": []}, {"number": 25926, "title": "Update types.cc", "body": "accesing->accessing", "comments": []}, {"number": 25925, "title": "Update utils_impl.py", "body": "typo issue", "comments": []}, {"number": 25924, "title": "Typo issue in xla", "body": "constitutent -> constituent", "comments": []}, {"number": 25923, "title": "[performance] CPU is idle even when there are operations ready to be executed", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.19.2- (@non-git)\r\n- GCC/Compiler version (if compiling from source): 4.8.5\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI'm trying to feed tensors produces by a ParseExample OP when inference an exported graph to avoid \r\nserialization/deserialization tf record of input examples.\r\nI have got the correct prediction results. But the performance is not consistent with my expectation.\r\nThere is several idle time in the timeline. And the feed tensors are not processing in parallel.\r\n\r\n**Describe the expected behavior**\r\nNo CPU idle time in prediction. No ParseExample OP in inference should be faster.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n`saved_bundle.session->Run(run_options, input_tensor_pairs, {output_tensor_name}, {}, &output, nullptr);`\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nchrome tracing results:\r\nwith ParseExample, no cpu idle during inference\r\n![image](https://user-images.githubusercontent.com/32092715/53081857-a7216300-3536-11e9-807c-791c1f47f4c2.png)\r\n[with_parse_example.json.txt](https://github.com/tensorflow/tensorflow/files/2883952/with_parse_example.json.txt)\r\nwithout ParseExample, cpu idle during inference with red marks\r\n![image](https://user-images.githubusercontent.com/32092715/53082111-231bab00-3537-11e9-922f-3ab8c2f5f743.png)\r\n[without_parse_example.json.txt](https://github.com/tensorflow/tensorflow/files/2883961/without_parse_example.json.txt)\r\n\r\nAny advices will be appreciated, Thanks", "comments": ["Is there any progress on this issue?", "I think this issue could be closed as is not active for a long time.", "@yann-yy did you end making headway? I'm seeing alot idle CPU and not alot of info about what causes this (IO bound?)", "@yann-yy  We see that you are using old version of tensorflow (1.x)which is officially considered as end of life, We recommend that you upgrade to 2.5 and let us know if the issue still persists in newer versions.Please open a new issue in case you face any errors, we will get you the right help .Hence moving this to closed status.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25923\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25923\">No</a>\n"]}, {"number": 25922, "title": "tflite_convert - ValueError: Invalid tensors 'input' were found.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): pip install tensorflow\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\n**Describe the current behavior**\r\nI want to convert mobilenet_v1_0.50_128 frozen graph using tflite_convert or toco to a TensorFlow Lite model, but I get the following error:\r\n```\r\n2019-02-20 16:09:37.352780: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\alice\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n\r\n  File \"c:\\users\\alice\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n\r\n  File \"C:\\Users\\Alice\\Anaconda3\\Scripts\\tflite_convert.exe\\__main__.py\", line 9, in <module>\r\n\r\n  File \"c:\\users\\alice\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\tflite_convert.py\", line 442, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n\r\n  File \"c:\\users\\alice\\anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n\r\n  File \"c:\\users\\alice\\anaconda3\\lib\\site-packages\\absl\\app.py\", line 300, in run\r\n    _run_main(main, args)\r\n\r\n  File \"c:\\users\\alice\\anaconda3\\lib\\site-packages\\absl\\app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n\r\n  File \"c:\\users\\alice\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\tflite_convert.py\", line 438, in run_main\r\n    _convert_model(tflite_flags)\r\n\r\n  File \"c:\\users\\alice\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\tflite_convert.py\", line 122, in _convert_model\r\n    converter = _get_toco_converter(flags)\r\n\r\n  File \"c:\\users\\alice\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\tflite_convert.py\", line 109, in _get_toco_converter\r\n    return converter_fn(**converter_kwargs)\r\n\r\n  File \"c:\\users\\alice\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 387, in from_frozen_graph\r\n    sess.graph, input_arrays)\r\n\r\n  File \"c:\\users\\alice\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\convert_saved_model.py\", line 189, in get_tensors_from_tensor_names\r\n    \",\".join(invalid_tensors)))\r\n\r\nValueError: Invalid tensors 'input' were found.\r\n```\r\n\r\n**Code to reproduce the issue**\r\nI run the following command:\r\n`tflite_convert --output_file=tflite_convert --output_file=F:/mobilenet_v1_0.50_128/frozen_graph.tflite --graph_def_file=F:/mobilenet_v1_0.50_128/frozen_graph.pb --input_arrays=input --output_arrays=MobilenetV1/Predictions/Reshape_1`\r\n", "comments": ["I have the same problem in window10.", "I don't know what happened but this morning the `mobilenet_v1_0.50_128/frozen_graph.pb` is converted successfully by `tflite_convert`:\r\n![image](https://user-images.githubusercontent.com/20389638/53139285-be566400-35bb-11e9-9c27-e2f4536e1de0.png)\r\nBut if I change to my trained model, the mentioned error appear. Anyone please tell me about the `--output_arrays=MobilenetV1/Predictions/Reshape_1`, if I change to my trained model (SSD-FPN Mobilenet V1), what output arrays of mine?\r\n", "I have the same problem when I am trying to convert frozen_face_grapy.pb file to .tflite file.\r\n\r\nBut if you try with the code in [document](https://www.tensorflow.org/lite/convert/python_api) with files provided by them, it is converting properly.\r\n\r\nCan anyone tell me, what is this input_tensor? And what is the expected value?", "Hello,\r\n\r\nI'm facing same problem here\r\n\r\nHere is the command that i'm executing to generate .pb i am successfully generate it.\r\n\r\n```\r\nIMAGE_SIZE=224\r\nARCHITECTURE=\"mobilenet_1_1.0_${IMAGE_SIZE}\"\r\n\r\npython retrain.py  \r\n --bottleneck_dir=tf_files/bottlenecks   \r\n --how_many_training_steps=500   \r\n --model_dir=tf_files/models/   \r\n --summaries_dir=tf_files/training_summaries/\"${ARCHITECTURE}\"  \r\n  --output_graph=tf_files/retrained_graph.pb   \r\n  --output_labels=tf_files/retrained_labels.txt   \r\n  --architecture=\"${ARCHITECTURE}\"  \r\n  --image_dir=tf_files/flower_photos\r\n```\r\n\r\nOnce i am trying to create that .pb to .tflite get fail with same error \"ValueError: Invalid tensors 'input' were found.\"\r\n\r\n```\r\ntflite_convert \\\r\n  --output_file=foo.tflite \\\r\n  --graph_def_file=retrained_graph.pb \\\r\n  --input_arrays=input \\\r\n  --output_arrays=MobilenetV1/Predictions/Reshape_1\r\n```", "same issue, any ideas?", "Same issue here as well", "any solution for this ?", "not sure if this approach is right. im using mobilnetv2\r\ni tried printing available values from convert_saved_model.py\r\nprint(tensor_name_to_tensor) -  this will give all the valid values. \r\ni used  --input_arrays=module_apply_default/MobilenetV2/input (which is available in print)\r\nthis is working for me. converted model tested in android device.", "Same error i am also facing.... any solution?", "Hello guys, please follow this stackoverflow url it may help you for it, i just use older one poets demo and its working fine with latest mobilenet version.  \r\nhttps://stackoverflow.com/questions/55585210/valueerror-invalid-tensors-input-were-found", "> Hello,\r\n> \r\n> I'm facing same problem here\r\n> \r\n> Here is the command that i'm executing to generate .pb i am successfully generate it.\r\n> \r\n> ```\r\n> IMAGE_SIZE=224\r\n> ARCHITECTURE=\"mobilenet_1_1.0_${IMAGE_SIZE}\"\r\n> \r\n> python retrain.py  \r\n>  --bottleneck_dir=tf_files/bottlenecks   \r\n>  --how_many_training_steps=500   \r\n>  --model_dir=tf_files/models/   \r\n>  --summaries_dir=tf_files/training_summaries/\"${ARCHITECTURE}\"  \r\n>   --output_graph=tf_files/retrained_graph.pb   \r\n>   --output_labels=tf_files/retrained_labels.txt   \r\n>   --architecture=\"${ARCHITECTURE}\"  \r\n>   --image_dir=tf_files/flower_photos\r\n> ```\r\n> \r\n> Once i am trying to create that .pb to .tflite get fail with same error \"ValueError: Invalid tensors 'input' were found.\"\r\n> \r\n> ```\r\n> tflite_convert \\\r\n>   --output_file=foo.tflite \\\r\n>   --graph_def_file=retrained_graph.pb \\\r\n>   --input_arrays=input \\\r\n>   --output_arrays=MobilenetV1/Predictions/Reshape_1\r\n> ```\r\n\r\ni want to know what is input and output array actually?what should i put there?", "Try loading your model into TensorBoard to try and visualize what the input array might be. In general, it's the Placeholders that are being passed into the graph at the start.\r\n\r\nIt is not possible to debug this issue without investigating the specific model. If you are unable to determine the input arrays, please provide a TensorFlow model file.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "\u9047\u5230ValueError: Invalid tensors 'input/input_data:0' were found.\r\n\u540c\u6837\u95ee\u9898\uff0c\u8bf7\u95ee\u6709\u8c01\u89e3\u51b3\u4e86\u5417\uff1f\u5e2e\u6211\uff0c 'input/input_data:0' \u662f\u6211\u6d4b\u8bd5pb\u6a21\u578b\u662f\u7684\u8f93\u5165\u53d8\u91cf\u540d\uff0c\u4f46\u662f\u8f6ctflite\u65f6\uff0c\u62a5\u4e0a\u8ff0\u9519\u8bef\u3002\u8c22\u8c22\u3002\u3002\u3002", "> \u9047\u5230ValueError: Invalid tensors 'input/input_data:0' were found.\r\n> \u540c\u6837\u95ee\u9898\uff0c\u8bf7\u95ee\u6709\u8c01\u89e3\u51b3\u4e86\u5417\uff1f\u5e2e\u6211\uff0c 'input/input_data:0' \u662f\u6211\u6d4b\u8bd5pb\u6a21\u578b\u662f\u7684\u8f93\u5165\u53d8\u91cf\u540d\uff0c\u4f46\u662f\u8f6ctflite\u65f6\uff0c\u62a5\u4e0a\u8ff0\u9519\u8bef\u3002\u8c22\u8c22\u3002\u3002\u3002\r\n\r\nHi, \r\nI had the same issue. Don't pass the port ':0' in your input array. \r\nTry just 'input/input_data'. \r\n\r\nShould work. \r\nAlso the input/output tensors names can be found using \"model.inputs\" and \"model.outputs\".\r\n\r\nRemove the port ':0' from the tensor names found above and pass it to your input/output arrays required by TOCO. \r\n", "I had the same issue when I tried to convert my trained model that used the inception_v3 model. There are the parameters that finally worked for me when I tried converting using tflite_convert:\r\n\r\n`\u2013graph_def_file=myfile.pb \u2013output_file=output.tflite \u2013input_format=TENSORFLOW_GRAPHDEF \u2013output_format=TFLITE \u2013-input_shape=1,299,299,3 \u2014input_array=ResizeBilinear \u2013output_array=final_result \u2013inference_type=FLOAT \u2013input_type=FLOAT`"]}, {"number": 25921, "title": "Updated tooling_util.cc", "body": "Fixed typo error", "comments": ["@rthadur, I'm not familiar with Toco (which is owned by the TF Lite team). Can you find a more appropriate reviewer, please?", "@rthadur, i have checked the failures, it has nothing to do with this changes, can you pls check and merge this PR.", "@rthadur , i have checked the failure is because of some other reason and not because of the PR, kindly check and merge."]}, {"number": 25920, "title": " undefined reference to `tensorflow::Tensor::ComputeFlatOuterDims", "body": "ubuntu 18.04\r\n\r\ncreate libtensorflow_cc.so and libtensorflow_framework.so in tensorflow1.12\r\n\r\nCode:\r\n\r\n`tensorflow::TTypes<float, 3>::Tensor boxes = outputs[0].flat_outer_dims<float,3>(); `\r\n\r\nError:\r\n\r\n`In function `tensorflow::TTypes<float, 3ul, long>::Tensor tensorflow::Tensor::flat_outer_dims<float, 3ul>()':(.text._ZN10tensorflow6Tensor15flat_outer_dimsIfLm3EEENS_6TTypesIT_XT0_ElE6TensorEv[_ZN10tensorflow6Tensor15flat_outer_dimsIfLm3EEENS_6TTypesIT_XT0_ElE6TensorEv]+0x70): undefined reference to `tensorflow::Tensor::ComputeFlatOuterDims(absl::lts_2018_12_18::Span<long long const>, long long)'`\r\n\r\ncheck libtensorflow_cc.so\r\n\r\nlink:\r\n\r\n` nm /usr/local/lib/tensorflow/libtensorflow_cc.so| grep ComputeFlatOuterDims\r\n\r\n0000000004348af0 T _ZN10tensorflow6Tensor20ComputeFlatOuterDimsEN4absl4SpanIKxEEx\r\n`\r\n\r\nthanks a lot!\r\n", "comments": ["Fixed\r\n`tensorflow::TTypes<float, 3>::Tensor boxes = outputs[0].tensor<float,3>(); `"]}, {"number": 25919, "title": "TF Lite 'dict_keys' object has no attribute 'sort' error fix", "body": "This PR solve 'dict_keys' object has no attribute 'sort' error fix for above python3 version environment.", "comments": ["@nutsiepully  \r\n\r\nPlease review the changes, thanks.", "This has already been fixed in master. Thanks."]}, {"number": 25918, "title": "TF Lite micro_speech package test build error fix", "body": "This PR solve micro_speech package test run compilation error fix.", "comments": ["@nutsiepully  \r\n\r\nPlease review the changes, thanks.", "Nagging Reviewer @nutsiepully: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 36 days with no activity and the `awaiting review` label has been applied."]}, {"number": 25917, "title": "TF Lite profile_summarizer_test.cc warning fix", "body": "Warning fix.", "comments": ["@nutsiepully  \r\n\r\nPlease review the changes, thanks.", "This change would break the code under TFLITE_PROFILING_ENABLED."]}]