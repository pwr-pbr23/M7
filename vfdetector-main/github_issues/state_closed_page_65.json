[{"number": 53311, "title": "Extract a function for parsing operator MIRROR_PAD", "body": "Extract the parsing of op parameters from the flatbuffer out of a\r\nswitch statement case, into a standalone function which can be\r\ncalled by the micro op resolver.\r\n\r\nThis PR is part of the work to port operator MIRROR_PAD\r\nfrom TFL to TFLM, as tracked in issue https://github.com/tensorflow/tflite-micro/issues/741.", "comments": []}, {"number": 53310, "title": "Freeze ReadVariableOp if the handle name is forwarded by Identity", "body": "`FreezeSavedModel` does not convert variables inside functions. To freeze variables in a model with functions, we need to inline the functions first. After inlining a function with resource variables, we have the following node sequence:\r\n\r\n`VarHandleOp` -> `Identity` -> `ReadVariableOP`\r\n\r\nThis PR updates `FreezeSavedModel` to handle SavedModels where the `GraphDef` was inlined. This involves two steps:\r\n- Remove the assumption that `VarHandleOp` is an immediate parent of `ReadVariableOp`.\r\n- Change the dtype of `Identity` to match the dtype of the variable.", "comments": ["This is needed for #53082. Tagging @bixia1 for visibility.", "I am trying to understand whether the Identity node is necessary and whether there is an existing optimization to remove such Identity nodes.", "@tfeher Can you please check @bixia1's comments and keep us posted ? Thanks!", "We had some discussion on this within google, and it seems that grappler should optimize it. Will close this PR once we have a grappler fix in place.", "Thanks @bixia1 for the suggestion, I have updated the PR.", "Thanks @bixia1 for sharing the feedback. I have updated the PR, please have another look."]}, {"number": 53309, "title": "Mac M1 tensorflow installation error", "body": "**System information**\r\n- OS Platform and Distribution: Mac OS Big Sur Version 11.6 Apple M1 chip Memory 8G\r\n\r\nHere is how I install:\r\n```\r\nconda create -n tensorflow python=3.9.5\r\nconda activate tensorflow\r\nconda install -c apple tensorflow-deps\r\nconda install -c apple tensorflow-deps==2.6.0\r\npython -m pip install tensorflow-macos\r\npython -m pip install tensorflow-metal\r\nbrew install libjpeg\r\nconda install -y matplotlib jupyterlab\r\n```\r\nThen I run:\r\n```\r\npython\r\nimport tensorflow as tf\r\n```\r\nThen I got error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/zhangjing/miniforge3/envs/tf3.7/lib/python3.8/site-packages/tensorflow/__init__.py\", line 449, in <module>\r\n    _ll.load_library(_plugin_dir)\r\n  File \"/Users/zhangjing/miniforge3/envs/tf3.7/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py\", line 155, in load_library\r\n    py_tf.TF_LoadLibrary(lib)\r\ntensorflow.python.framework.errors_impl.NotFoundError: dlopen(/Users/zhangjing/miniforge3/envs/tf3.7/lib/python3.8/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 6): Symbol not found: _OBJC_CLASS_$_MPSGraphCompilationDescriptor\r\n  Referenced from: /Users/zhangjing/miniforge3/envs/tf3.7/lib/python3.8/site-packages/tensorflow-plugins/libmetal_plugin.dylib (which was built for Mac OS X 12.0)\r\n  Expected in: /System/Library/Frameworks/MetalPerformanceShadersGraph.framework/Versions/A/MetalPerformanceShadersGraph\r\n```\r\n\r\n\r\n", "comments": ["@JingZhang918 \r\nWe don't support the M1 chip.\r\nCan you refer to similar issues:#42482,46178,48808\r\nCan you please create this on [Apple Developer Forum.](https://developer.apple.com/forums/tags/tensorflow-metal) ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53309\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53309\">No</a>\n"]}, {"number": 53308, "title": "Remove wheel requirement as it is not needed for the pip package", "body": "https://github.com/simonpercivall/astunparse/pull/65\r\n\r\n```\r\nastunparse $ for tag in v1.6.0 v1.6.1 v1.6.2 v1.6.3; do git checkout $tag; echo $tag; grep wheel . -R --exclude-dir=.git; done\r\nPrevious HEAD position was d7ba156 Bump version\r\nHEAD is now at df08255 Bump version and add HISTORY entry.\r\nv1.6.0\r\n./setup.cfg:[wheel]\r\n./Makefile:\tpython setup.py bdist_wheel\r\n./requirements.txt:wheel >= 0.23.0, < 1.0\r\nPrevious HEAD position was df08255 Bump version and add HISTORY entry.\r\nHEAD is now at c73b675 Bump version.\r\nv1.6.1\r\n./setup.cfg:[wheel]\r\n./Makefile:\tpython setup.py bdist_wheel\r\n./requirements.txt:wheel >= 0.23.0, < 1.0\r\nPrevious HEAD position was c73b675 Bump version.\r\nHEAD is now at d7ba156 Bump version\r\nv1.6.2\r\n./setup.cfg:[wheel]\r\n./Makefile:\tpython setup.py bdist_wheel\r\n./requirements.txt:wheel >= 0.23.0, < 1.0\r\nPrevious HEAD position was d7ba156 Bump version\r\nHEAD is now at 2acce01 Merge pull request #42 from simonpercivall/merge/python3.8-support\r\nv1.6.3\r\n./setup.cfg:[wheel]\r\n./Makefile:\tpython setup.py bdist_wheel\r\n./requirements.txt:wheel >= 0.23.0, < 1.0\r\n```", "comments": ["I'm not even sure why wheel is a requirement in the first place:\r\n\r\n```\r\n~/git/tensorflow [unpin_wheel|\u2714] $ grep wheel . -R --exclude-dir=.git | grep \"import\"\r\n~/git/tensorflow [unpin_wheel|\u2714] $ grep wheel . -R --exclude-dir=.git | grep \"from\"\r\n./tensorflow/lite/g3doc/guide/python.md:If you'd like to manually install a Python wheel, you can select one from\r\n```", "For other packagers, it also seems to be true that the python package of tensorflow never imports wheel in 2.6.2 as well\r\n```\r\n(base) \u2714 ~/git/tensorflow [v2.6.2|\u2714]\r\n11:42 $ grep wheel . -R --exclude-dir=.git | grep \"import\"\r\n(base) \u2718-1 ~/git/tensorflow [v2.6.2|\u2714]\r\n11:42 $ grep wheel . -R --exclude-dir=.git | grep \"from\"\r\n./tensorflow/lite/micro/examples/hello_world/train/train_hello_world_model.ipynb:            \"Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.4.0rc0) (0.35.1)\\n\",\r\n./tensorflow/lite/g3doc/guide/python.md:If you'd like to manually install a Python wheel, you can select one from\r\n```"]}, {"number": 53307, "title": "How to pass a switch python argument into input_signature when using tf.function?", "body": "Hi, I\u2019m using Tensorflow 2.7.0 currently. There is no any other configuration things related to my issue.\r\n\r\nI\u2019m trying to set a switch for a tf.function, like set the input_signature for the call method of a Karas Model, so I can use the tf.saved_model.save API to save it. But it seems like I can not pass a python argument into the input_signature.\r\n\r\nFor example, I built a Transformer translation Model, I set an argument named \u201cistraining\u201d for the call method of this Model. When istraining = True, the Model run train mode and use the \u201cteacher forcing\u201d pattern to decode target language input. When istraining = False,  the model run translating mode and use the \u201cauto regression\u201d pattern to generate target language.\r\n\r\nBut I can not find a way to pass the python True/False argument into the input_signature of the tf.function decorator.\r\n\r\nMy currently solution is set a translating class function and a training class function separately and run get_concrete_function for these two functions separately when saving them. \r\n\r\nIs there a way to integrate these two functions into the model\u2019s call method, and use a python argument True/False to control it after I save the model and reload it?\r\n\r\nMore generally, there is a reserved argument named \u201ctraining\u201d in Karas Layers and Karas Model, which is a python argument. if I wanna decorate the call method of a Karas Model, how should I set the input signature for this \u201ctraining\u201d argument?\r\n", "comments": ["@LionQYW \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThanks!", "Hi, I don\u2019t think this is a pure Karas Problem. It is mainly about the tf.function and tf.saved_model API. And this is also a question for tf.Module class.", "@LionQYW We could see that you have mentioned this issue on [keras-team/keras repo ](https://github.com/keras-team/keras/issues/15751).Could you please move this ticket to closed status as we will track the other one there.Thanks!"]}, {"number": 53306, "title": "cannot get high precision arithmetic to work", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 12.0.1\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.6.2\r\n- Python version: 3.9.7\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nHigh precision when casting with numpy, but low precision when casting with tf:\r\n```python\r\n>>> import numpy as np; import tensorflow as tf\r\n>>> tf.math.sin(np.array(np.pi, dtype=np.float64))\r\n<tf.Tensor: shape=(), dtype=float64, numpy=1.2246467991473532e-16>\r\n\r\n>>> tf.math.sin(tf.cast(np.pi, dtype=tf.float64))\r\n<tf.Tensor: shape=(), dtype=float64, numpy=-8.742278000372475e-08>  # why low precision?\r\n\r\n>>> tf.math.sin(tf.cast(3.141592653589793, dtype=tf.float64))\r\n<tf.Tensor: shape=(), dtype=float64, numpy=-8.742278000372475e-08>  # how to get high precision??\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\nI would expect `tf.cast(np.pi, dtype=tf.float64)` to behave like a float64 value when passed to functions.\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n\r\n**Standalone code to reproduce the issue**\r\nsee above\r\n", "comments": ["@sachinprasadhs, I was able to reproduce the issue on colab using TF 2.7 and TF-nightly(2.8.0-dev20211207). Please find the [gist here](https://colab.research.google.com/gist/chunduriv/c5748c0e1934a18ec9aac38daf131d30/53306.ipynb) for reference.Thanks!", "The reason is while doing the `tf.cast` it does the approximation because you are converting `tf.float32` to `tf.float64` because of this it changes the value of `pi` from `3.141592653589793 `to `3.1415927410125732` and hence the change in precision.\r\nTo avoid that you can feed the` tf.float64` variable by providing `tf.constant` value as below.\r\n`tf.math.sin(tf.cast(tf.constant(np.pi, dtype=tf.float64),dtype=tf.float64))`\r\n`<tf.Tensor: shape=(), dtype=float64, numpy=1.2246467991473532e-16>`\r\n\r\nPlease find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/e4acc8151448ba264f8fbff1804dcf24/53306.ipynb#scrollTo=-Y0xzKd9_AZi) for detailed reference, Thanks!\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53306\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53306\">No</a>\n"]}, {"number": 53305, "title": "Could not find the DLL(s) 'msvcp140_1.dll'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: (Windows 10)\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.6\r\n- Python version: 3.8.0\r\n- Installed using virtualenv? pip? conda?: pip (on a fresh anaconda environment)\r\n- CUDA/cuDNN version: CUDA 11.2.2/ cuDNN 8.1.0.77\r\n- GPU model and memory: Nvidia GeForce GTX 1650 Ti with 4096 MB GDDR6 \r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n`import tensorflow`\r\n\r\n\r\n**Any other info / logs**\r\n\r\n```\r\nTraceback (most recent call last):\r\n\r\n  File \"C:\\Users\\Muril\\AppData\\Local\\Temp/ipykernel_6432/4294963926.py\", line 1, in <module>\r\n    import tensorflow\r\n\r\n  File \"C:\\Users\\Muril\\anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n\r\n  File \"C:\\Users\\Muril\\anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n\r\n  File \"C:\\Users\\Muril\\anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n\r\n  File \"C:\\Users\\Muril\\anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\n  File \"C:\\Users\\Muril\\anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    self_check.preload_check()\r\n\r\n  File \"C:\\Users\\Muril\\anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\", line 54, in preload_check\r\n    raise ImportError(\r\n\r\nImportError: Could not find the DLL(s) 'msvcp140_1.dll'. TensorFlow requires that these DLLs be installed in a directory that is named in your %PATH% environment variable. You may install these DLLs by downloading \"Microsoft C++ Redistributable for Visual Studio 2015, 2017 and 2019\" for your platform from this URL: https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads\r\n```\r\n\r\nI have installed the DLLs from that link, restarted my PC, but the problem remains.\r\n\r\nThe msvcp140_1.dll is in C:\\Windows\\System32, wich is already in my path environemnt variable.\r\n\r\n![Capturar1](https://user-images.githubusercontent.com/40600889/144713654-f1f7e0f4-c657-4479-97c8-c12bc6dbb00d.JPG)\r\n\r\n\r\n", "comments": ["@muriloasouza \r\nPlease check these: [link](https://github.com/tensorflow/tensorflow/issues/18116#issuecomment-854379687), [link1](https://stackoverflow.com/questions/60157335/cant-pip-install-tensorflow-msvcp140-1-dll-missing).\r\n#50955,#43193,#35618\r\nPlease verify your python/os version os not 32 bit.", "@Saduf2019\r\n\r\nI don't have a Program folder inside AppData\\Local. Instead, i got Python38 folder inside Roaming\\Python.\r\nPython 64 bit.\r\n![Capturar1](https://user-images.githubusercontent.com/40600889/144757777-aa36c8a3-561e-4cd3-8eed-ffc02dc96ba1.JPG)\r\n\r\nWell, i just tried upgrading tensorflow (`pip install tensoflow` ). Now i am with tf 2.7.  Did one try, and the error was there. I restarted my PC, and when i was about to post this message, i did one last try and `import tensorflow` worked. So, i trained a small MLP just fine. Closed my IDE (Spyder), opened it again, and the error reappeared.", "@muriloasouza \r\nIt clearly says windows 32, like i explained earlier there is no support for 32 bits.", "@Saduf2019 \r\n\r\nwin32 is just saying my Anaconda is installed on Windows OS. My OS is 64 bit, and AMD64 indicates that my code is running in a 64 bit process\r\n\r\n[link1](https://stackoverflow.com/questions/28526062/does-64-bit-anaconda-on-win32-uses-32-bit-or-64-bit)\r\n[link2](https://www.w3resource.com/python-exercises/python-basic-exercise-42.php)\r\n[link3](https://www.scivision.dev/python-check-32-64-bit/)\r\n\r\nSo, according to those links, i'm using python 64bit. I'm confused now.", "@muriloasouza \r\nCould you please try on virtual env and let us know.\r\nYou may also refer to #35618,#42482,#32625,[link](https://github.com/tensorflow/tensorflow/issues/43193#issuecomment-700315079)\r\nVerify if your Does your CPU support AVX.\r\nensure you are following the [versions.](https://www.tensorflow.org/install/source#gpu)\r\nPlease check if you have the latest version of pip installed. As per the [documentation](https://www.tensorflow.org/install) TensorFlow 2 packages require a pip version >19.0.\r\npip install --upgrade pip", "Yes, everything is in the correct version and my CPU does support AVX.\r\n\r\nAt this point i'm just giving up, too much of a headache to make this work. I'm sticking with Google Colab.\r\n\r\nThanks for your help @Saduf2019 ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53305\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53305\">No</a>\n", "I had the same problem, this guide helped me:\r\nhttps://winbindex.m417z.com/?file=msvcp140_1.dll"]}, {"number": 53304, "title": "Warnings with Arduino Nano 33 BLE", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): with pip\r\n- TensorFlow version (or github SHA if from source): 2.7.0\r\n\r\n**Standalone code to reproduce the issue** \r\nRun the DigitsExample.ino with eloquenttinyml library in arduino of version 0.0.10.\r\n\r\n**Any other info / logs**\r\n\r\nWhen I try compiling the code, I'm getting the following warnings. It's ultimately compiling the code but showing the following warnings:\r\n\r\nIn file included from C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/core/api/op_resolver.h:20:0,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/micro/micro_mutable_op_resolver.h:19,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/micro/kernels/all_ops_resolver.h:16,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/EloquentTinyML.h:13,\r\nfrom C:\\Users\\saisa\\UROP\\EloquentTinyML-master\\EloquentTinyML-master\\examples\\DigitsExample\\DigitsExample.ino:1:\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:407:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < TensorType_FLOAT32 || e > TensorType_INT8) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:437:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/core/api/op_resolver.h:20:0,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/micro/micro_mutable_op_resolver.h:19,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/micro/kernels/all_ops_resolver.h:16,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/EloquentTinyML.h:13,\r\nfrom C:\\Users\\saisa\\UROP\\EloquentTinyML-master\\EloquentTinyML-master\\examples\\DigitsExample\\DigitsExample.ino:1:\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameDimensionType(tflite::DimensionType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < DimensionType_DENSE || e > DimensionType_SPARSE_CSR) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:922:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < BuiltinOperator_ADD || e > BuiltinOperator_SEGMENT_SUM) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:1249:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < BuiltinOptions_NONE || e > BuiltinOptions_SegmentSumOptions) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < Padding_SAME || e > Padding_VALID) return \"\";\r\n~~^~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2562:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2595:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2625:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2655:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2688:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < CombinerType_SUM || e > CombinerType_SQRTN) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2718:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2745:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/core/api/op_resolver.h:20:0,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src\\tensorflow\\lite\\core\\api\\op_resolver.cpp:16:\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:407:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < TensorType_FLOAT32 || e > TensorType_INT8) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:437:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/core/api/op_resolver.h:20:0,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src\\tensorflow\\lite\\core\\api\\op_resolver.cpp:16:\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameDimensionType(tflite::DimensionType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < DimensionType_DENSE || e > DimensionType_SPARSE_CSR) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:922:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < BuiltinOperator_ADD || e > BuiltinOperator_SEGMENT_SUM) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:1249:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < BuiltinOptions_NONE || e > BuiltinOptions_SegmentSumOptions) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < Padding_SAME || e > Padding_VALID) return \"\";\r\n~~^~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2562:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2595:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2625:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2655:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2688:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < CombinerType_SUM || e > CombinerType_SQRTN) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2718:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2745:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/core/api/op_resolver.h:20:0,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/core/api/flatbuffer_conversions.h:24,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src\\tensorflow\\lite\\core\\api\\flatbuffer_conversions.cpp:16:\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:407:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < TensorType_FLOAT32 || e > TensorType_INT8) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:437:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/core/api/op_resolver.h:20:0,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/core/api/flatbuffer_conversions.h:24,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src\\tensorflow\\lite\\core\\api\\flatbuffer_conversions.cpp:16:\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameDimensionType(tflite::DimensionType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < DimensionType_DENSE || e > DimensionType_SPARSE_CSR) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:922:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < BuiltinOperator_ADD || e > BuiltinOperator_SEGMENT_SUM) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:1249:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < BuiltinOptions_NONE || e > BuiltinOptions_SegmentSumOptions) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < Padding_SAME || e > Padding_VALID) return \"\";\r\n~~^~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2562:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2595:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2625:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2655:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2688:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < CombinerType_SUM || e > CombinerType_SQRTN) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2718:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2745:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src\\tensorflow\\lite\\core\\api\\op_resolver.cpp: In function 'TfLiteStatus tflite::GetRegistrationFromOpCode(const tflite::OperatorCode*, const tflite::OpResolver&, tflite::ErrorReporter*, const TfLiteRegistration**)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src\\tensorflow\\lite\\core\\api\\op_resolver.cpp:29:20: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nbuiltin_code < BuiltinOperator_MIN) {\r\n~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src\\tensorflow\\lite\\micro\\debug_log_numbers.cpp: In function 'char* {anonymous}::FastFloatToBufferLeft(float, char*)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src\\tensorflow\\lite\\micro\\debug_log_numbers.cpp:117:53: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\nconst uint32_t u = reinterpret_cast<uint32_t>(&f);\r\n^\r\nIn file included from C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/core/api/op_resolver.h:20:0,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/core/api/flatbuffer_conversions.h:24,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/micro/micro_allocator.h:20,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src\\tensorflow\\lite\\micro\\micro_allocator.cpp:16:\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:407:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < TensorType_FLOAT32 || e > TensorType_INT8) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:437:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/core/api/op_resolver.h:20:0,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/core/api/flatbuffer_conversions.h:24,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/micro/micro_allocator.h:20,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src\\tensorflow\\lite\\micro\\micro_allocator.cpp:16:\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameDimensionType(tflite::DimensionType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < DimensionType_DENSE || e > DimensionType_SPARSE_CSR) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:922:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < BuiltinOperator_ADD || e > BuiltinOperator_SEGMENT_SUM) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:1249:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < BuiltinOptions_NONE || e > BuiltinOptions_SegmentSumOptions) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < Padding_SAME || e > Padding_VALID) return \"\";\r\n~~^~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2562:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/micro/memory_helpers.h:20:0,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src\\tensorflow\\lite\\micro\\memory_helpers.cpp:16:\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:407:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < TensorType_FLOAT32 || e > TensorType_INT8) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:437:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2595:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2625:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2655:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2688:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < CombinerType_SUM || e > CombinerType_SQRTN) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2718:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2745:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/micro/memory_helpers.h:20:0,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src\\tensorflow\\lite\\micro\\memory_helpers.cpp:16:\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameDimensionType(tflite::DimensionType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < DimensionType_DENSE || e > DimensionType_SPARSE_CSR) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:922:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < BuiltinOperator_ADD || e > BuiltinOperator_SEGMENT_SUM) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:1249:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < BuiltinOptions_NONE || e > BuiltinOptions_SegmentSumOptions) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < Padding_SAME || e > Padding_VALID) return \"\";\r\n~~^~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2562:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2595:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2625:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2655:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2688:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < CombinerType_SUM || e > CombinerType_SQRTN) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2718:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2745:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/core/api/op_resolver.h:20:0,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/micro/micro_interpreter.h:20,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src\\tensorflow\\lite\\micro\\micro_interpreter.cpp:15:\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:407:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < TensorType_FLOAT32 || e > TensorType_INT8) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:437:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/core/api/op_resolver.h:20:0,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/micro/micro_interpreter.h:20,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src\\tensorflow\\lite\\micro\\micro_interpreter.cpp:15:\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameDimensionType(tflite::DimensionType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < DimensionType_DENSE || e > DimensionType_SPARSE_CSR) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:922:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < BuiltinOperator_ADD || e > BuiltinOperator_SEGMENT_SUM) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:1249:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < BuiltinOptions_NONE || e > BuiltinOptions_SegmentSumOptions) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < Padding_SAME || e > Padding_VALID) return \"\";\r\n~~^~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2562:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2595:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2625:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2655:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2688:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < CombinerType_SUM || e > CombinerType_SQRTN) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2718:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2745:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/core/api/op_resolver.h:20:0,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/micro/micro_interpreter.h:20,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/micro/micro_optional_debug_tools.h:20,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src\\tensorflow\\lite\\micro\\micro_optional_debug_tools.cpp:15:\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:407:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < TensorType_FLOAT32 || e > TensorType_INT8) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:437:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/core/api/op_resolver.h:20:0,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/micro/micro_interpreter.h:20,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/micro/micro_optional_debug_tools.h:20,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src\\tensorflow\\lite\\micro\\micro_optional_debug_tools.cpp:15:\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameDimensionType(tflite::DimensionType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < DimensionType_DENSE || e > DimensionType_SPARSE_CSR) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:922:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < BuiltinOperator_ADD || e > BuiltinOperator_SEGMENT_SUM) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:1249:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < BuiltinOptions_NONE || e > BuiltinOptions_SegmentSumOptions) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < Padding_SAME || e > Padding_VALID) return \"\";\r\n~~^~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2562:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2595:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2625:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2655:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2688:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < CombinerType_SUM || e > CombinerType_SQRTN) return \"\";\r\n^~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2718:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return \"\";\r\n^~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2745:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return \"\";\r\n^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src\\tensorflow\\lite\\micro\\micro_interpreter.cpp: In member function 'TfLiteTensor* tflite::MicroInterpreter::input(size_t)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src\\tensorflow\\lite\\micro\\micro_interpreter.cpp:215:14: warning: comparison of unsigned expression < 0 is always false [-Wtype-limits]\r\nif ((index < 0) || (index >= length)) {\r\n~~~~~~^\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src\\tensorflow\\lite\\micro\\micro_interpreter.cpp: In member function 'TfLiteTensor* tflite::MicroInterpreter::output(size_t)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src\\tensorflow\\lite\\micro\\micro_interpreter.cpp:226:14: warning: comparison of unsigned expression < 0 is always false [-Wtype-limits]\r\nif ((index < 0) || (index >= outputs->size())) {\r\n~~~~~~^\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src\\tensorflow\\lite\\micro\\micro_interpreter.cpp: In member function 'TfLiteTensor* tflite::MicroInterpreter::tensor(size_t)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src\\tensorflow\\lite\\micro\\micro_interpreter.cpp:236:14: warning: comparison of unsigned expression < 0 is always false [-Wtype-limits]\r\nif ((index < 0) || (index >= tensors_size())) {\r\n~~~~~~^\r\nIn file included from C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/micro/simple_memory_allocator.h:21:0,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src\\tensorflow\\lite\\micro\\simple_memory_allocator.cpp:16:\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:407:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < TensorType_FLOAT32 || e > TensorType_INT8) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:437:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/micro/simple_memory_allocator.h:21:0,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src\\tensorflow\\lite\\micro\\simple_memory_allocator.cpp:16:\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameDimensionType(tflite::DimensionType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < DimensionType_DENSE || e > DimensionType_SPARSE_CSR) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:922:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < BuiltinOperator_ADD || e > BuiltinOperator_SEGMENT_SUM) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:1249:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < BuiltinOptions_NONE || e > BuiltinOptions_SegmentSumOptions) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < Padding_SAME || e > Padding_VALID) return \"\";\r\n~~^~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2562:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2595:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2625:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2655:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2688:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < CombinerType_SUM || e > CombinerType_SQRTN) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2718:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2745:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/micro/test_helpers.h:23:0,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src\\tensorflow\\lite\\micro\\test_helpers.cpp:16:\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:407:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < TensorType_FLOAT32 || e > TensorType_INT8) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:437:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/micro/test_helpers.h:23:0,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src\\tensorflow\\lite\\micro\\test_helpers.cpp:16:\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameDimensionType(tflite::DimensionType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < DimensionType_DENSE || e > DimensionType_SPARSE_CSR) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:922:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < BuiltinOperator_ADD || e > BuiltinOperator_SEGMENT_SUM) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:1249:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < BuiltinOptions_NONE || e > BuiltinOptions_SegmentSumOptions) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < Padding_SAME || e > Padding_VALID) return \"\";\r\n~~^~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2562:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2595:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2625:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2655:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2688:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < CombinerType_SUM || e > CombinerType_SQRTN) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2718:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2745:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/core/api/op_resolver.h:20:0,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/micro/micro_mutable_op_resolver.h:19,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/micro/kernels/all_ops_resolver.h:16,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src\\tensorflow\\lite\\micro\\kernels\\all_ops_resolver.cpp:13:\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:407:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < TensorType_FLOAT32 || e > TensorType_INT8) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:437:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/core/api/op_resolver.h:20:0,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/micro/micro_mutable_op_resolver.h:19,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/micro/kernels/all_ops_resolver.h:16,\r\nfrom C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src\\tensorflow\\lite\\micro\\kernels\\all_ops_resolver.cpp:13:\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameDimensionType(tflite::DimensionType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < DimensionType_DENSE || e > DimensionType_SPARSE_CSR) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:922:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < BuiltinOperator_ADD || e > BuiltinOperator_SEGMENT_SUM) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:1249:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < BuiltinOptions_NONE || e > BuiltinOptions_SegmentSumOptions) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < Padding_SAME || e > Padding_VALID) return \"\";\r\n~~^~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2562:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2595:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2625:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2655:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2688:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < CombinerType_SUM || e > CombinerType_SQRTN) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2718:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/schema/schema_generated.h:2745:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\nif (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return \"\";\r\n~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from C:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src\\tensorflow\\lite\\micro\\kernels\\svdf.cpp:25:0:\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/micro/kernels/activation_utils.h: In function 'float tflite::ops::micro::ActivationValFloat(TfLiteFusedActivation, float)':\r\nC:\\Users\\saisa\\OneDrive\\Documents\\Arduino\\libraries\\EloquentTinyML\\src/tensorflow/lite/micro/kernels/activation_utils.h:53:1: warning: control reaches end of non-void function [-Wreturn-type]\r\n}\r\n^\r\nSketch uses 208608 bytes (21%) of program storage space. Maximum is 983040 bytes.\r\nGlobal variables use 57920 bytes (22%) of dynamic memory, leaving 204224 bytes for local variables. Maximum is 262144 bytes.\r\n", "comments": ["@saisahil2015 \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Please do refer this [thread ](https://forum.arduino.cc/t/trouble-with-arduinoble-library-on-arduino-nano-33-ble-sense/864545)and let us know if it helps?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53304\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53304\">No</a>\n"]}, {"number": 53303, "title": "LSTM expansion", "body": "**System information**\r\n- TensorFlow version (you are using): 1.3\r\n- Are you willing to contribute it (Yes/No): Maybe\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nSimple LSTMs have limitations. It cannot extract some features, i.e. some mathematical operands, conditional logic, etc. I don't think the LSTM advancements cover this. This paper kind of shows what logic needs to be part of LSTM out of the box: https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8610074 \r\nCan we improve the LSTM to include ability to figure out some basic logic like mathematical operands and conditional logic?\r\n\r\n**Will this change the current api? How?**\r\nShouldn't.\r\n\r\n**Who will benefit with this feature?**\r\nEveryone.\r\n\r\n**Any Other info.**\r\nLSTM is brokened at the moment. Pretty rudimentary.", "comments": ["@allhavebrainimplantsandmore \r\nPlease feel free to create a pr or let us know the changes to be made to which file for the requested change.\r\nYou may also refer to this [link](https://towardsdatascience.com/tutorial-on-lstm-a-computational-perspective-f3417442c2cd)", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53302, "title": "Random seed in tf.keras.initializers.RandomNormal does not work in Tensorflow 2.7.0", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.7.12\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nSetting the seed in tf.keras.initializers.RandomNormal(seed = [some integer]) does not guarantee that the same sequence is generated when constructed with the same seed value.\r\n\r\n**Describe the expected behavior**\r\nSetting the seed in tf.keras.initializers.RandomNormal(seed = [some integer]) should guarantee that the same sequence is generated when two initializers are constructed with the same seed value.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing): n/a\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nUsing tensorflow version 2.7.0:\r\nhttps://colab.research.google.com/drive/1paNe8kBRzwkrZHBi7QF0yhEX8pXO0f1w?usp=sharing\r\n\r\nHowever, using tensorflow 2.6.0, this issue does not occur (note one has to restart the notebook after the pip install in the first cell before running the second cell):\r\nhttps://colab.research.google.com/drive/10KrXpJtxiaAmnI2vKa61hDNheqjiJyu9?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Hi @lubin-liu! As this [document ](https://keras.io/api/layers/initializers/)suggests ,They are meant to give random weight values to input layer. So,This is working as expected. Attaching [Gist ](https://colab.research.google.com/gist/mohantym/84541e4f6348446445e395410304825c/tf-keras-initializer-random-seed-test-using-version-2-7-0.ipynb#scrollTo=0UOgrynLV7Vh)for reference.", "Hi @mohantym! From the documentation [here](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/RandomNormal):\r\n\r\n```\r\n tf.keras.initializers.RandomNormal(\r\n     mean=0.0, stddev=0.05, seed=None\r\n )\r\n```\r\n\r\nRegarding the seed parameter, the documentation states: \r\n\r\n> ... Note that seeded initializer will not produce same random values across multiple calls, but multiple initializers will produce same sequence when constructed with same seed value.\r\n\r\nSo in your gist, I would expect that if I pass the same seed as a parameter into the two instantiations of initializers.RandomNormal, temp and temp2, both should give the same weights on the first call, which is what I'm observing in Tensorflow 2.3.0, 2.5.0, and 2.6.0 ([example](https://colab.research.google.com/drive/10KrXpJtxiaAmnI2vKa61hDNheqjiJyu9?usp=sharing)), but not 2.7.0.\r\n\r\n", "Ok @lubin-liu ! Thanks for reporting this bug . \r\nPlease post this issue on [keras-team/keras repo](https://github.com/keras-team/keras/issues) too.\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) . Thanks!", "Thanks! I have done so here:\r\n\r\nhttps://github.com/keras-team/keras/issues/15755", "Ok @lubin-liu ! Please close this issue here as it will be tracked in [#15755](https://github.com/keras-team/keras/issues/15755). Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53302\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53302\">No</a>\n"]}, {"number": 53300, "title": "tf.stack silently output wrong result with 0-dimension tensor", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0 and 2.8.0-dev20211203 (nightly)\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nx = tf.random.uniform(shape=[0,3])\r\ny = tf.random.uniform(shape=[1,3])\r\nprint(tf.stack([x,y]).shape)\r\n```\r\n\r\n**Describe the current behavior**\r\nOutputs:\r\n```\r\n(2, 0, 3)\r\n```\r\nStacking `x` and `y`, and we got an empty tensor! \r\nI found that this issue occurs in both tf2.7.0 and tf-nightly.\r\n\r\n**Describe the expected behavior**\r\nAccording to the documentation, the stacked tensors should have the same shape. Here the input tensor `x` and `y` don't have the same shape, so an `InvalidArgumentError` error should be raised.", "comments": ["Added a PR #53367  for the fix.", "Fixed by PR #53367 by @yongtang.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53300\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53300\">No</a>\n"]}, {"number": 53299, "title": "[oneDNN] Conv3D + Add fusion which has BiasAdd semantics", "body": "This PR extends https://github.com/tensorflow/tensorflow/pull/52358 and adds support for Conv3D NDHWC format.\r\nIt also adds support for Conv + Add fusion where the Add is of the form {1,1,1,C} for Conv2D or {1,1,1,1,C} for Conv3D\r\ni.e For NHWC/NDHWC, bias should have 4/5 or less dimensions and should be all 1's except for the channel dimension.", "comments": ["@kanvi-nervana Could you please help solve the merge conflicts? Thank you!", "> @kanvi-nervana Could you please help solve the merge conflicts? Thank you!\r\n\r\n@penpornk  I have resolved the merge conflicts.", "Manually closing this PR as it is merged in https://github.com/tensorflow/tensorflow/commit/2ed1ca2e69a3200f70d7025f260e5d97f82b18b9."]}, {"number": 53298, "title": "Update CODEOWNERS", "body": null, "comments": []}, {"number": 53297, "title": "Update https://www.tensorflow.org/install/gpu#install_cuda_with_apt for Ubuntu 21.04", "body": "\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/install/gpu#install_cuda_with_apt\r\n\r\n## Description of issue (what needs changing):\r\n\r\nPlease add a section for Ubuntu 21.04:\r\n\r\nFor Ubuntu 21.04 the manual installation instructions are a thing of the past assuming that you have installed your NVIDIA drivers from the Canonical default apt repo .  All one needs to do is:\r\npip3 install tensorflow\r\nsudo apt install nvidia-cuda-toolkit\r\n\r\nThis resolves the library error issues: \"Could not load dynamic library 'libcudart.so.11.0'\"\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@mperino \r\nCould you install sudo apt instead of install nvidia-cuda-toolkit \r\nusing specific cuda toolkit, it needs to be installed based on Tensorflow version accordingly.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53295, "title": "SymbolAlreadyExposedError: Symbol random_rotation is already exposed as ('keras.preprocessing.image.random_rotation',)", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10\r\n- TensorFlow installed from (source or binary): cmd pip\r\n- TensorFlow version: \r\n                    tensorboard                        2.7.0\r\n                    tensorboard-data-server            0.6.1\r\n                    tensorboard-plugin-wit             1.8.0\r\n                    tensorflow                         2.7.0\r\n                    tensorflow-addons                  0.15.0\r\n                    tensorflow-estimator               2.7.0\r\n                    tensorflow-io                      0.22.0\r\n                    tensorflow-io-gcs-filesystem       0.22.0\r\n                      tf-estimator-nightly               2.8.0.dev2021120309\r\n                      tf-models-official                 2.7.0\r\n                      tf-nightly                         2.8.0.dev20211202\r\n                      tf-slim                            1.1.0\r\n- Python version:  \r\n                    Python 3.9.7\r\n                    conda 4.10.3\r\n- Installed using virtualenv? pip? conda?:              pip         21.2.4\r\n\r\n- GPU model and memory:  NO GPU\r\n\r\n\r\n\r\n**Describe the problem**\r\n            \r\n            raise SymbolAlreadyExposedError(\r\n            tensorflow.python.util.tf_export.SymbolAlreadyExposedError: Symbol random_rotation is already exposed as ('keras.preprocessing.image.random_rotation',).\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n          VERIFICATION_SCRIPT = os.path.join('models', 'research', 'object_detection', 'builders', 'model_builder_tf2_test.py')\r\n          # Verify Installation\r\n          !python {VERIFICATION_SCRIPT}\r\n\r\n\r\n**Any other info / logs**\r\n                Traceback (most recent call last):\r\n                  File \"G:\\object2\\models\\research\\object_detection\\builders\\model_builder_tf2_test.py\", line 25, in <module>\r\n                    from object_detection.builders import model_builder\r\n                  File \"G:\\anaconda3\\lib\\site-packages\\object_detection-0.1-py3.9.egg\\object_detection\\builders\\model_builder.py\", line 70, in <module>\r\n                    from object_detection.models import ssd_efficientnet_bifpn_feature_extractor as ssd_efficientnet_bifpn\r\n                  File \"G:\\anaconda3\\lib\\site-packages\\object_detection-0.1-py3.9.egg\\object_detection\\models\\ssd_efficientnet_bifpn_feature_extractor.py\", line 34, in <module>\r\n                    from official.vision.image_classification.efficientnet import efficientnet_model\r\n                  File \"G:\\anaconda3\\lib\\site-packages\\tf_models_official-2.7.0-py3.9.egg\\official\\vision\\image_classification\\efficientnet\\efficientnet_model.py\", line 37, in <module>\r\n                    from official.vision.image_classification import preprocessing\r\n                  File \"G:\\anaconda3\\lib\\site-packages\\tf_models_official-2.7.0-py3.9.egg\\official\\vision\\image_classification\\preprocessing.py\", line 25, in <module>\r\n                    from official.vision.image_classification import augment\r\n                  File \"G:\\anaconda3\\lib\\site-packages\\tf_models_official-2.7.0-py3.9.egg\\official\\vision\\image_classification\\augment.py\", line 31, in <module>\r\n                    from tensorflow.python.keras.layers.preprocessing import image_preprocessing as image_ops\r\n                  File \"G:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\preprocessing\\image_preprocessing.py\", line 28, in <module>\r\n                    from tensorflow.python.keras.preprocessing import image as image_preprocessing\r\n                  File \"G:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\preprocessing\\__init__.py\", line 22, in <module>\r\n                    from tensorflow.python.keras.preprocessing import image\r\n                  File \"G:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\preprocessing\\image.py\", line 1140, in <module>\r\n                    keras_export('keras.preprocessing.image.random_rotation')(random_rotation)\r\n                  File \"G:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_export.py\", line 336, in __call__\r\n                    self.set_attr(undecorated_func, api_names_attr, self._names)\r\n                  File \"G:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_export.py\", line 351, in set_attr\r\n", "comments": ["@akzenith \r\nPlease follow [this link](https://github.com/tensorflow/tensorflow/issues/39483#issuecomment-629006902) and let us know.[[link](https://stackoverflow.com/questions/50596341/import-tensor-flow-library-error],[link1](#52980))", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53295\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53295\">No</a>\n", "Got same error in the version:  tf-nightly==2.9.0.dev20220109,\r\nFixed by installing tf-nightly==2.8.0. dev20211018.", "Got same error in the version tf-nightly==2.9.0.dev20220110\r\nI could not install another version of tf-nightly without error."]}, {"number": 53294, "title": "compiled tf.function and tf.data.Iterator error", "body": "**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.1-61734-g21f65a888d6 2.7.0-dev20210806\r\n- Python version: Python 3.6.9\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: GeForce RTX 2080 Ti / 11019MiB\r\n\r\n**Describe the current behavior**\r\nHitting an error when calling next on a tf.data.Iterator within a compiled (jit_compiled=True) tf.function:\r\n\r\n`tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run __inference_run_compiled_61: No unary variant device copy function found for direction: 1 and Variant type_index: tensorflow::ResourceDeleter [Op:__inference_run_compiled_61]\r\n`\r\n**Describe the expected behavior**\r\n\r\nThe same code without compilation works fine (does not hit the error above). I am expecting the same behavior.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing): N/A\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ndata = tf.data.Dataset.from_tensor_slices(list(range(10)))\r\ndata = data.repeat()\r\niterator = iter(data)\r\n\r\n@tf.function(jit_compile=False)\r\ndef run(it):\r\n    t = 0\r\n    for _ in range(10):\r\n        x = next(it)\r\n        t += x\r\n    return t\r\n\r\n@tf.function(jit_compile=True)\r\ndef run_compiled(it):\r\n    t = 0\r\n    for _ in range(10):\r\n        x = next(it)\r\n        t += x\r\n    return t\r\n\r\nprint(run(iterator))\r\nprint(run_compiled(iterator))\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nFull output of the test above:\r\n\r\n`tf.Tensor(45, shape=(), dtype=int32)\r\nTraceback (most recent call last):\r\n  File \"../error.py\", line 24, in <module>\r\n    print(run_compiled(iterator))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 904, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 976, in _call\r\n    filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1965, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 597, in call\r\n    ctx=ctx)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run __inference_run_compiled_61: No unary variant device copy function found for direction: 1 and Variant type_index: tensorflow::ResourceDeleter [Op:__inference_run_compiled_61]`", "comments": ["I ran your example on Google Colab and got a different error message which might be more informative\r\n\r\n```InvalidArgumentError: Detected unsupported operations when trying to compile graph __inference_run_compiled_61[_XlaMustCompile=true,config_proto=3175580994766145631,executor_type=11160318154034397263] on XLA_CPU_JIT: IteratorGetNext (No registered 'IteratorGetNext' OpKernel for XLA_CPU_JIT devices compatible with node {{node IteratorGetNext}}){{node IteratorGetNext}}```\r\n\r\nI'm not sure what tensorflow version you're using, but mine is 2.7.0", "Quickly scanning some of the documentation, `jit_compile=True` invokes compilation using XLA (Accelerated Linear Algebra) while very fast is known to have some [problems](https://www.tensorflow.org/xla/known_issues). \r\n\r\nUnfortunately it doesn't look like your use case fits neatly into any of those listed. If I had to hazard a guess, it may have something to do with an iterator being unbounded which may be related to the issue of XLA not liking infinite loops.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53294\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53294\">No</a>\n"]}, {"number": 53293, "title": "TfLiteStatus invoke_status = interpreter.Invoke(); Failed", "body": "### 0. Summary\r\n\r\nHelp is appreciated!\r\nDetails are given below.\r\n\r\nTo do my first project with TF lite on microcontrollers (TFLM) I follow the Hello World example for TFLM.\r\nI do this with my own network, though. The Structure is\r\n\r\n- Input 1x16x16x1 \r\n- Two CNN layers with maxpooling\r\n- Flatten\r\n- Dense layer\r\n- Output 1x2\r\n\r\nI encounter the following problem:\r\n\r\nIn the python world (nearly) everything works.\r\n> Sidenote: It did not work to use converter.inference_input_type = tf.int8 (The quantization scaling seems to expect a uint8 input eventough int8 was specified) Therefore I use uint8.\r\n\r\nAlso in the Cpp world everything looks fine. Till I do interpreter.Invoke() This creates the follwoing error message in the terminal:\r\n> Input UINT8, output INT8 not supported.\r\n> Node QUANTIZE (number 0) failed to invoke with status 1\r\n\r\nHowever, I still get a prediction that is incorrect.\r\nI wanted to note that I use\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_input_type = tf.uint8\r\n\r\nDetails follow:\r\n\r\n### 1. System information\r\n\r\n- Windows 10, use Cygwin to run the make process\r\n- Packages (pip):\r\n\tnumpy==1.19.2\r\n\tpandas==1.2.2\r\n\ttensorflow==2.4.0\r\n\ttflite==2.4.0\r\n\ttensorflow_hub==0.12.0\r\n\topencv-python>=4.5.4\r\n\ttqdm==4.62.3\r\n\tmatplotlib==3.4.3\r\n\tscikit-learn>=0.0.0\r\n\tpillow>=0.0.0\r\n\tscikit-image>=0.0.0\r\n\ttensorflow_model_optimization>=0.0.0\r\n\tipykernel==6.4.1\r\n\ttensorboard>=0.0.0\r\n\r\n### 2. Code\r\n\r\nProvide code to help us reproduce your issues using one of the following options:\r\n\r\n#### Option B: Paste your code here or provide a link to a custom end-to-end colab\r\n\r\n## Python-world (model produces reasonable results. Everything seems to be fine here.)\r\n\t# imports etc.\r\n\r\n\r\n\t# Load model\r\n\tmodel = tf.keras.models.load_model(\"src/mdl/mdl_KERAS_trained\")\r\n\r\n\t# Define Representative Dataset\r\n\tdata_obj = data_prep.gesture_data(\"../Data/lala\")\r\n\tdata_obj.load_data()\r\n\tdata_obj.proc_data_picbased()\r\n\tdata_obj.split_data()\r\n\tx_train = tf.convert_to_tensor(data_obj.x_train, dtype=tf.float32)\r\n\ty_train = tf.convert_to_tensor(data_obj.y_train, dtype=tf.float32)\r\n\r\n\t# Quick Check\r\n\tfor i in range(12):\r\n\t\tprint(model.predict(np.expand_dims(x_train[i, :, :, :], 0)))\r\n\t\tprint(y_train[i, :])\r\n\r\n\tdef representative_dataset_train():\r\n\t\tdataset = tf.data.Dataset.from_tensor_slices(x_train).batch(1)\r\n\t\tfor data in dataset:\r\n\t\t\tyield [data]\r\n\r\n\r\n\t# Define converter\r\n\tconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\tconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\tconverter.inference_input_type = tf.uint8\r\n\tconverter.inference_output_type = tf.uint8\r\n\tconverter.representative_dataset = representative_dataset_train\r\n\tconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n\tconverter.target_spec.supported_types = [tf.int8]\r\n\r\n\r\n\t# Convert\r\n\ttflite_model = converter.convert()\r\n\r\n\t# Save\r\n\twith open(\"src/mdl/mdl_tflite.tflite\", \"wb\") as f:\r\n\t\tf.write(tflite_model)\r\n\r\n\r\n\t# Load TFLite\r\n\tinterpreter = tf.lite.Interpreter(model_path=\"src/mdl/mdl_tflite.tflite\")\r\n\tinterpreter.allocate_tensors()\r\n\r\n\t# Get input and output tensors.\r\n\tinput_details = interpreter.get_input_details()\r\n\toutput_details = interpreter.get_output_details()\r\n\r\n\t# Scaling to UINT8\r\n\tinput_quant = input_details[0][\"quantization\"]\r\n\toutput_quant = output_details[0][\"quantization\"]\r\n\r\n\tx_train8 = np.fix((x_train) / input_quant[0] + input_quant[1])\r\n\ty_train8 = y_train\r\n\r\n\t# Quick check\r\n\tprint(\"prediction\")\r\n\tfor i in range(20):\r\n\t\tinput_data = np.array(\r\n\t\t\tnp.expand_dims(x_train8[i, :, :, :], axis=0), dtype=np.uint8\r\n\t\t)\r\n\t\tinterpreter.set_tensor(input_details[0][\"index\"], input_data)\r\n\t\tinterpreter.invoke()\r\n\t\toutput_data = interpreter.get_tensor(output_details[0][\"index\"])\r\n\t\tprint(\r\n\t\t\toutput_data, \"   \",\r\n\t\t\tnp.array(output_quant[0]) * (np.array(output_data) - np.array(output_quant[1])), \"   \",\r\n\t\t\ty_train8[i],\r\n\t\t)\r\n\r\n\r\n\r\n\r\n\t\r\n\t\r\n## Cpp-world: \r\n```\r\n\r\nTF_LITE_MICRO_TESTS_BEGIN\r\n\r\nTF_LITE_MICRO_TEST(LoadModelAndPerformInference) {\r\n\t// Define the input\r\n\tdouble x[16][16];\t// Some input data which are numbers between -1. and 1. Import not shown for clarity\r\n\r\n\t// Set up logging\r\n\ttflite::MicroErrorReporter micro_error_reporter;\r\n\r\n\t// Model\r\n\tconst tflite::Model* model = ::tflite::GetModel(g_mdl_model_data);\r\n\tif (model->version() != TFLITE_SCHEMA_VERSION) {\r\n\t  TF_LITE_REPORT_ERROR(&micro_error_reporter,\r\n\t                         \"Model provided is schema version %d not equal \"\r\n\t                         \"to supported version %d.\\n\",\r\n\t                         model->version(), TFLITE_SCHEMA_VERSION);\r\n\t}\r\n\r\n\t// Resolver\r\n\ttflite::AllOpsResolver resolver;\r\n\r\n\t// Arena\r\n\tconstexpr int kTensorArenaSize = 500*1024;\r\n\tuint8_t tensor_arena[kTensorArenaSize];\r\n\r\n\t// Interpreter\r\n\ttflite::MicroInterpreter interpreter(model, resolver, tensor_arena,\r\n\t                                       kTensorArenaSize, &micro_error_reporter);\r\n\r\n\tinterpreter.AllocateTensors();\r\n\r\n\t// Input\r\n\tTfLiteTensor* input = interpreter.input(0);\r\n\r\n\t// Does Input match\r\n\tTF_LITE_MICRO_EXPECT_NE(nullptr, input);\r\n\tTF_LITE_MICRO_EXPECT_EQ(4, input->dims->size);\r\n\tTF_LITE_MICRO_EXPECT_EQ( 1, input->dims->data[0]);\r\n\tTF_LITE_MICRO_EXPECT_EQ(16, input->dims->data[1]);\r\n\tTF_LITE_MICRO_EXPECT_EQ(16, input->dims->data[2]);\r\n\tTF_LITE_MICRO_EXPECT_EQ( 1, input->dims->data[3]);\r\n\tTF_LITE_MICRO_EXPECT_EQ(kTfLiteUInt8, input->type);\t\t//kTfLiteUInt8\r\n\r\n\r\n\t// Get quantization parameters\r\n\tdouble input_scale = input->params.scale;\r\n\tint input_zero_point = input->params.zero_point;\r\n\r\n\r\n\t// Quantize\r\n\tuint8_t x_quantized[256];\r\n\tfor(int i=0;i<ni;i++){\r\n\t\tfor(int j=0;j<nj;j++){\r\n\t\t\tx_quantized[i*nj+j] = (uint8_t) ((x[i][j]/input_scale) + input_zero_point);\r\n\t\t\tinput->data.uint8[i*nj+j] = x_quantized[i*nj+j];\r\n\t\t}\r\n\t}\r\n\r\n\r\n\t// Run the model\r\n\tMicroPrintf(\"-----invoke-----\");\r\n\tTfLiteStatus invoke_status = interpreter.Invoke();\r\n\tMicroPrintf(\"-----check-----\");\r\n\tTF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, invoke_status);\r\n\r\n\tMicroPrintf(\"End of File\");\r\n\r\n\r\n}\r\n\r\nTF_LITE_MICRO_TESTS_END\r\n```\r\n\r\n### 3. Failure after conversion\r\n\r\nOutput of make process:\r\n```\r\n\r\n\t$ make --no-silent --trace -f tensorflow/lite/micro/tools/make/Makefile test_mdl_test\r\n\t\r\n\t...\r\n\r\n\t[Some lines of output]\r\n\t...\r\n\t\r\n\ttensorflow/lite/micro/tools/make/gen/windows_x86_64_default/bin/mdl_test '~~~ALL TESTS PASSED~~~' windows\r\n\tTesting LoadModelAndPerformInference\r\n\t----------\r\n\tinput_scale 1.0039215*2^-7\r\n\tinput_zero_point 127\r\n\t----------\r\n\t-----invoke-----\r\n\tInput UINT8, output INT8 not supported.\r\n\tNode QUANTIZE (number 0) failed to invoke with status 1\r\n\t-----check-----\r\n\tkTfLiteOk == invoke_status failed at tensorflow/lite/micro/examples/mdl/mdl_test.cc:200 (0 vs 1)\r\n\toutput_scale 1.0*2^-8\r\n\toutput_zero_point 0\r\n\t----------\r\n\tprediction:   3   4\r\n\tprediction:   3   4\r\n\tEnd of File\r\n\t0/1 tests passed\r\n\t~~~SOME TESTS FAILED~~~\r\n\t~~~SOME TESTS FAILED~~~\r\n\r\n\tmake: *** [tensorflow/lite/micro/examples/mdl/Makefile.inc:40: test_mdl_test] Error 1\r\n\r\n```", "comments": ["@aetherwind Could you please try with the latest TF v2.7.0 and let us know the outcome ? Thanks!", "@sushreebarsa Thanks for helping. \r\n\r\n#### I went to TF 2.7 However the result stays the same. \r\npip list:\r\n> tensorboard                  2.7.0\r\n> tensorboard-data-server      0.6.1\r\n> tensorboard-plugin-wit       1.8.0\r\n> tensorflow                   2.7.0\r\n> tensorflow-estimator         2.7.0\r\n> tensorflow-io-gcs-filesystem 0.22.0\r\n\r\n\r\n\r\n#### What is in my opinion interesting is the following: I run the checks:\r\n```\r\nTF_LITE_MICRO_EXPECT_EQ(kTfLiteUInt8, input->type);\r\nTF_LITE_MICRO_EXPECT_EQ(kTfLiteUInt8, ouptut->type);\r\n```\r\nThey do not raise an error.\r\nHowever I get an output that says 'Input UINT8, output INT8 not supported.' (I mean, where does the latter come from? The checks pointed out that everything should be fine.)\r\n\r\n\r\n\r\n#### Also, I get the following warnings during conversion in Python:\r\n2021-12-06 09:08:47.600087: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\n2021-12-06 09:08:49.281014: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.\r\n2021-12-06 09:08:49.282491: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.\r\n\r\nHowever those warnings also show up in https://www.tensorflow.org/lite/guide/signatures. Therefore, I considered them 'normal'.\r\n\r\nDo you have any ideas what the problem could be?\r\n", "Did you follow the step mentioned [here](https://www.tensorflow.org/lite/microcontrollers/get_started_low_level#7_allocate_memory). Thanks!", "Hello sachinprasadhs,\r\n\r\nThanks for your reply. \r\n\r\nTo be honest I simply went for a large memory size for testing, like 500*1024, assuming this would cover all my needs and worry about getting it smaller at a latter point. \r\n\r\nI now tried and it works wit 5*1024.\r\n\r\nDo I have to worry? As this is approx. 5000 bytes, but my model has approx. 17000 parameters. Somehow this does not seem to match. The length given in the .cc file for the model is 23696.\r\n\r\nCould there be a problem with how I assign the input data?", "The error seems to be invoked in micro_graph.cc at line 183\r\n\r\n```\r\n    if (invoke_status == kTfLiteError) {\r\n      MicroPrintf(\"Node %s (number %d) failed to invoke with status %d\",\r\n                  OpNameFromRegistration(registration), i, invoke_status);\r\n      return kTfLiteError;\r\n    } else if (invoke_status != kTfLiteOk) {\r\n      return invoke_status;\r\n    }\r\n```", "When using \r\n```\r\nconverter.inference_input_type = tf.int8\r\nconverter.inference_output_type = tf.int8\r\n```\r\ninstead of\r\n```\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\n```\r\nthe tests do not invoke errors anymore. ~~However, the results of the quantized model are wrong - already in the python world.~~ (This was my mistake, sorry for the confusion.)", "If your issue is resolved could you please move this to closed. Thanks!", "From my perspective the key issue is not resolved:  The uint8 quantisation is not working in cpp for me and i cannot figure out at which point it goes wrong.\r\n\r\n", "When running the original Hello World after changing input and output quantization from int8 to uint8 I get the following error when making the Cpp (running with int8 quantization works):\r\n\r\n```\r\n$ make -f tensorflow/lite/micro/tools/make/Makefile test_hello_world_test\r\nfind: \u2018../google/\u2019: No such file or directory\r\ntensorflow/lite/micro/tools/make/downloads/flatbuffers already exists, skipping the download.\r\ntensorflow/lite/micro/tools/make/downloads/kissfft already exists, skipping the download.\r\ntensorflow/lite/micro/tools/make/downloads/pigweed already exists, skipping the download.\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -Werror -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -DTF_LITE_DISABLE_X86_NEON -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_USE_CTIME -Os -I. -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/gen/windows_x86_64_default/genfiles/ -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/tools/make/gen/windows_x86_64_default/genfiles/tensorflow/lite/micro/examples/hello_world/hello_world_model_data.cc -o tensorflow/lite/micro/tools/make/gen/windows_x86_64_default/obj/core/tensorflow/lite/micro/tools/make/gen/windows_x86_64_default/genfiles/tensorflow/lite/micro/examples/hello_world/hello_world_model_data.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -Werror -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -DTF_LITE_DISABLE_X86_NEON -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_USE_CTIME -I. -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/gen/windows_x86_64_default/genfiles/ -Itensorflow/lite/micro/tools/make/downloads/kissfft -o tensorflow/lite/micro/tools/make/gen/windows_x86_64_default/bin/hello_world_test tensorflow/lite/micro/tools/make/gen/windows_x86_64_default/obj/core/tensorflow/lite/micro/examples/hello_world/hello_world_test.o tensorflow/lite/micro/tools/make/gen/windows_x86_64_default/obj/core/tensorflow/lite/micro/tools/make/gen/windows_x86_64_default/genfiles/tensorflow/lite/micro/examples/hello_world/hello_world_model_data.o tensorflow/lite/micro/tools/make/gen/windows_x86_64_default/lib/libtensorflow-microlite.a -Wl,--fatal-warnings -Wl,--gc-sections -lm\r\ntensorflow/lite/micro/tools/make/gen/windows_x86_64_default/bin/hello_world_test '~~~ALL TESTS PASSED~~~' windows\r\nTesting LoadModelAndPerformInference\r\ntensorflow/lite/micro/kernels/quantize_common.cc:54 input->type == kTfLiteFloat32 || input->type == kTfLiteInt32 || input->type == kTfLiteInt16 || input-> was not true.\r\nNode QUANTIZE (number 0f) failed to prepare with status 1\r\ninterpreter.AllocateTensors() == kTfLiteOk failed at tensorflow/lite/micro/examples/hello_world/hello_world_test.cc:58 (1 vs 0)\r\nmake: *** [tensorflow/lite/micro/examples/hello_world/Makefile.inc:38: test_hello_world_test] Segmentation fault (core dumped)\r\n```\r\n\r\nIs uint8 currently really an implemented quantization?", "You can refer [this](https://www.tensorflow.org/lite/performance/quantization_spec) document for details.", "Hello sachinprasadhs,\r\nThis information is really helpful. Thank you very much!! :)\r\n\r\n> Note: In the past our quantization tooling used per-tensor, asymmetric, uint8 quantization. New tooling, reference kernels, and optimized kernels for 8-bit quantization will use this spec [(int8)].\r\n\r\nI did not really find this information, especially, there was no need looking for it, as [https://www.tensorflow.org/lite/performance/post_training_integer_quant](url) (which is the same website) talks about uint8 as it would be the thing to do, e.g. this code example:\r\n```\r\n# Set the input and output tensors to uint8 (APIs added in r2.3)\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\n```\r\nBut seems the feature was removed again.\r\n\r\nAnyway! Thanks very much for your support!!"]}, {"number": 53292, "title": "tf.data.experimental.load does not work on temporary files", "body": "**System information**\r\n- TensorFlow version (you are using): 2.6.0\r\n- Are you willing to contribute it (Yes/No): No (I don't have the required knowledge)\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nWhen loading a dataset with `tf.data.experimental.load`, the produced dataset will always be a lazily-loaded dataset that doesn't read the files until the dataset object is consumed (e.g. by iterating over it with Python code, or using it to train a model). This poses a problem if the path provided to `load` is a temporary path, since depending on when the dataset is used, the files could be gone when the dataset object attempts to read them.\r\n\r\nConsider the following load function:\r\n```python\r\ndef load_data(source):\r\n    with tempfile.TemporaryDirectory() as tmpdir:\r\n        prepare_files(source, tmpdir.name)\r\n        return tf.data.experimental.load(tmpdir.name)\r\n```\r\nThe dataset returned by this function will be unusable since, as soon as the function returns, the `TemporaryDirectory` context manager will exit and destroy the files. (The `prepare_files` function is just there to represent any logic that could be used to get the files to the temp. dir, such as downloading them over network, extracting them from an archive, or decrypting them.)\r\n\r\nAside from the temporary files issue, there could some other cases in which the developpers would want their datasets to be loaded in memory immediately.\r\n\r\n**Will this change the current api? How?**\r\n\r\nThe feature I propose is to allow callers of `tf.data.experimental.load` to specify whether they want a lazily-loaded dataset, or an eagerly-loaded dataset (that reads the files only once, when `load` is called, and stores the data in memory).\r\n\r\nThe way I see it, this evolution would add a new optional parameter to `tf.data.experimental.load`. Its default value needs to correspond to lazy loading for backwards compatibility.\r\n\r\nThis would not pose a big issue since 1. it will be backwards-compatible and 2. only the experimetal API will be modified (and the documentation explicetely says the experimental API may evolve at any time).\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone who wants to separate their dataset loading logic in a similar function, using a temporary directory or another context manager that destroys the files upon exit. This would be the case when:\r\n- using files downloaded from the Internet\r\n- using encrypted files, e.g. if the data is sensitive or private\r\n- using files compressed externally, to have more control over compression\r\n- using `importlib.resources.as_file` in some cases\r\n\r\n**Any Other info.**\r\n\r\nThe fact that the datasets are lazy-loaded is not documented, it would be nice to add this to the current documentation of `tf.data.experimental.load`. Also on the subject of documentation, once (if) the eager load option is added, the documentation should warn the user about the potentially high memory usage when eager-loading large datasets. In my use case this is not a problem, since 1. the datasets are created with `Dataset.from_tensor_slices` before being saved (so they have already been entirely in memory before), 2. I only use one dataset at a time and 3. I make sure it is garbage-collected as soon as possible.\r\n\r\nHere is my current placeholder:\r\n```python\r\ndef load_data(source):\r\n    tmppath = tempfile.mkdtemp()\r\n    def deleter():\r\n        shutil.rmtree(tmppath)\r\n    atexit.register(tmppath)\r\n    prepare_files(source, tmppath)\r\n    return tf.data.experimental.load(tmppath)\r\n```\r\nDepending on how many times this function is called during the program's lifetime, this can quickly clutter the disk. It would be nice to have a better solution.\r\n\r\nNote that, to fit **my** *specific* use case, it would also work for me to have a variant of `save` and `load` that accept a Python file-like object (and save to/load from a single file). The logic I use in my version of `prepare_files` turns a single file into a directory.\r\n", "comments": ["By the way, I have already tried calling `cache` on the dataset and iterating over it prior to exiting the `with` block, but this still results in an exception being thrown when the dataset is used outside of the function.", "I have re-created the error in the Python console:\r\n```\r\n>>> next(iter(loaded_dataset))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Program Files\\Python39\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 411, in __iter__\r\n    return iterator_ops.OwnedIterator(self)\r\n  File \"C:\\Program Files\\Python39\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 696, in __init__\r\n    self._create_iterator(dataset)\r\n  File \"C:\\Program Files\\Python39\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 719, in _create_iterator\r\n    gen_dataset_ops.make_iterator(ds_variant, self._iterator_resource)\r\n  File \"C:\\Program Files\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\", line 3123, in make_iterator\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"C:\\Program Files\\Python39\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 6941, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.NotFoundError: Could not find metadata file.\r\n         [[{{node LoadDataset/_1}}]] [Op:MakeIterator]\r\n```\r\n`loaded_dataset` is a dataset loaded from a now deleted temporary directory, on which I have done the following in order:\r\n- Call `tf.data.experimental.load` on the temporary path to load the dataset\r\n- Add the `.cache()` operation with no argument (cache to memory)\r\n- Consume the dataset with `for _ in cached_dataset: pass`\r\n- Exit the temporary directory context manager (_after_ the cached dataset has been consumed once)\r\n\r\nThe error happens as soon as I try to use the dataset outside of the load function.", "I'm surprised using `cache` doesn't avoid the issue. Here is a self-contained repro: https://colab.research.google.com/drive/188Kvhi5bVCtcEP54h3nRix-nrW9-LN3X?usp=sharing. Will need further investigation to see what's going on.", "I just submitted a fix (https://github.com/tensorflow/tensorflow/commit/c41e88b59209906).  Before this change, dataset finalization (which allows options to propagate through datasets) was performed each time an iterator for a dataset was created.  When finalizing a cached LoadDataset, an error was thrown if the underlying file was no longer present. This would occur if the saved file had been in a temporary directory and the temporary directory was now out of scope. My change fixes the issue by performing finalization only the first time an iterator for a dataset is created. Subsequent iterations use a cached version of the finalized dataset rather than performing the finalization again. Can you let me know on your end if it works? You will have to use the TF nightly build."]}, {"number": 53291, "title": "Wrong channel ordering in space_to_depth ", "body": "In the example provided for `space_to_depth` function here: https://www.tensorflow.org/api_docs/python/tf/nn/space_to_depth\r\n\r\nIf an input tensor has a larger depth\r\n\r\n```python\r\nx = [[[[1, 2, 3], [4, 5, 6]],\r\n      [[7, 8, 9], [10, 11, 12]]]]\r\n```\r\nThen, `space_to_depth` output is: `[ 1  2  3  4  5  6  7  8  9 10 11 12]`, slice along channel dimension (i.e., `x[0, 0, 0, :]`)\r\n\r\nWhile Pytorch `pixel_unshuffle` return: `[ 1  4  7 10  2  5  8 11  3  6  9 12]`\r\n\r\nThe issue is the inconsistent in Tensorflow's behavior that is when the number of channels is _1_\r\n\r\n```python\r\nx= [[[[1], [4]],\r\n       [[7], [10]]]]\r\n```\r\n\r\nthen, the output is `[ 1  4  7 10]` which is the same as Pytorch. It is expected that when the number of channels is increasing, it should apply the `space_to_depth` to each channel separately, then concatenate the outputs along channel dimension.\r\n\r\nHowever, the way Tensorflow does space_to_depth in the case of multiple channels is that it does select the elements along channel-axis which is wrong.\r\n\r\n---\r\n\r\nAnother example, with the full code, if the input is RGGB Bayer Raw image, then TensorFlow output would be in the wrong order in case the number of channels is greater than 1.\r\n\r\n```python\r\n# pip install pytorch==1.10.0\r\n# pip install tensorflow==2.6.1\r\n# pip install einops==0.3.2\r\nimport os\r\n\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n\r\nimport torch\r\nfrom torch.nn.functional import pixel_unshuffle\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom einops import rearrange, repeat\r\n\r\nrggb = np.asarray([[1, 2],\r\n                   [2, 3]])\r\n\r\n# === Only 1 channel\r\nimage = rearrange(rggb, 'h w -> h w 1')\r\nimage = repeat(image, 'h w c -> (2 h) (2 w) (1 c)')\r\nprint('RGGB image:', image.shape)\r\nprint(image[:, :, 0])\r\n\r\nimg1 = tf.nn.space_to_depth(\r\n    rearrange(image, 'h w c -> 1 h w c'),\r\n    2).numpy()\r\nimg2 = pixel_unshuffle(\r\n    rearrange(torch.from_numpy(image), 'h w c -> 1 c h w'),\r\n    2).numpy()\r\n\r\nprint('Tensorflow:', img1.shape)\r\nprint('Pytorch:', img2.shape)\r\n\r\nimg1 = rearrange(img1, '1 h w c -> h w c')\r\nimg2 = rearrange(img2, '1 c h w -> h w c')\r\n\r\nprint('Tensoflow:')\r\nprint(img1[0, 0, :])\r\n\r\nprint('Pytorch:')\r\nprint(img2[0, 0, :])\r\n\r\nprint('Match:', np.allclose(img1, img2))\r\n\r\n\r\n# === Repeat to have 2 channels\r\nprint()\r\nimage = rearrange(rggb, 'h w -> h w 1')\r\nimage = repeat(image, 'h w c -> (2 h) (2 w) (2 c)')\r\nprint('RGGB image:', image.shape)\r\nprint(image[:, :, 0])\r\n\r\nimg1 = tf.nn.space_to_depth(\r\n    rearrange(image, 'h w c -> 1 h w c'),\r\n    2).numpy()\r\nimg2 = pixel_unshuffle(\r\n    rearrange(torch.from_numpy(image), 'h w c -> 1 c h w'),\r\n    2).numpy()\r\n\r\nprint('Tensorflow:', img1.shape)\r\nprint('Pytorch:', img2.shape)\r\n\r\nimg1 = rearrange(img1, '1 h w c -> h w c')\r\nimg2 = rearrange(img2, '1 c h w -> h w c')\r\n\r\nprint('Tensoflow:')\r\nprint(img1[0, 0, :])\r\n\r\nprint('Pytorch:')\r\nprint(img2[0, 0, :])\r\n\r\nprint('Match:', np.allclose(img1, img2))\r\n```\r\n\r\noutput\r\n\r\n```\r\nRGGB image: (4, 4, 1)\r\n[[1 2 1 2]\r\n [2 3 2 3]\r\n [1 2 1 2]\r\n [2 3 2 3]]\r\nTensorflow: (1, 2, 2, 4)\r\nPytorch: (1, 4, 2, 2)\r\nTensoflow:\r\n[1 2 2 3]\r\nPytorch:\r\n[1 2 2 3]\r\nMatch: True\r\n\r\nRGGB image: (4, 4, 2)\r\n[[1 2 1 2]\r\n [2 3 2 3]\r\n [1 2 1 2]\r\n [2 3 2 3]]\r\nTensorflow: (1, 2, 2, 8)\r\nPytorch: (1, 8, 2, 2)\r\nTensoflow:\r\n[1 1 2 2 2 2 3 3]\r\nPytorch:\r\n[1 2 2 3 1 2 2 3]\r\nMatch: False\r\n```", "comments": ["Hi @Saduf2019 ! Could you please look at this issue! Attaching Gist in [2.6](https://colab.research.google.com/gist/mohantym/c8ae0482ec0fad0e6d4c6f84702a8c29/github_53291.ipynb#scrollTo=fesXfhtJEtHx),[2.7](https://colab.research.google.com/gist/mohantym/b524955337f79e4df87cdc1620e3be9a/github_53291.ipynb#scrollTo=7lUUAnumE11k) and [nightly ](https://colab.research.google.com/gist/mohantym/97c088f37bc56bba94f7253c34a4d908/github_53291.ipynb#scrollTo=fesXfhtJEtHx)for reference. Thanks!", "Hi, did you try with different `data_format`, the default one is channel last(\"NHWC\"), you can try channel first(\"NCHW\") to see if it is matching.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53290, "title": "fix: GIT_REPOSITORY for eigen", "body": "A missing trailing .git of GIT_REPOSITORY is causing git clone error on centos 6 machine.\r\n\r\nIt seems like this error didn't occur in a ubuntu machine, but it can be reproduced in a centos6 machine.\r\n\r\nReproduce:\r\n1. On a centos 6 machine, add `set(FETCHCONTENT_QUIET FALSE)` to eigen.cmake.\r\n2. run cmake with `cmake ../tensorflow/lite/` (following instructions of https://www.tensorflow.org/lite/guide/build_cmake)\r\n3. Found the following error message:\r\n```\r\n-- Build files have been written to: /tensorflow/tflite_build_centos/_deps/eigen-subbuild\r\n[ 11%] Performing download step (git clone) for 'eigen-populate'\r\nInitialized empty Git repository in \r\n/tensorflow/tflite_build_centos/eigen/.git/\r\nerror: RPC failed; result=22, HTTP code = 422\r\n```\r\n\r\nOn the other hand, the cmake process hung up after this error occured, maybe we can consider adding `set(FETCHCONTENT_QUIET FALSE)` in the `tensorflow/lite/CMakeLists.txt`.", "comments": []}, {"number": 53289, "title": "java.lang.UnsatisfiedLinkError:java.lang.String[] org.tensorflow.lite.NativeInterpreterWrapper.getSignatureKeys(long)", "body": "AndroidRuntime: java.lang.UnsatisfiedLinkError: No implementation found for java.lang.String[] org.tensorflow.lite.NativeInterpreterWrapper.getSignatureKeys(long) (tried Java_org_tensorflow_lite_NativeInterpreterWrapper_getSignatureKeys and Java_org_tensorflow_lite_NativeInterpreterWrapper_getSignatureKeys__J)\r\n10-16 08:37:52.057 32388 32435 E AndroidRuntime:        at org.tensorflow.lite.NativeInterpreterWrapper.getSignatureKeys(Native Method)\r\n10-16 08:37:52.057 32388 32435 E AndroidRuntime:        at org.tensorflow.lite.NativeInterpreterWrapper.getSignatureKeys(NativeInterpreterWrapper.java:414)\r\n10-16 08:37:52.057 32388 32435 E AndroidRuntime:        at org.tensorflow.lite.Interpreter.getSignatureKeys(Interpreter.java:313)\r\n10-16 08:37:52.057 32388 32435 E AndroidRuntime:        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:232)\r\n10-16 08:37:52.057 32388 32435 E AndroidRuntime:        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:226)", "comments": ["implementation 'org.tensorflow:tensorflow-lite-support:0.3.1'", "@JencharFung \r\nan you please refer to [this link](https://stackoverflow.com/questions/48223347/unsatisfiedlinkerror-while-building-tensorflow-lite-demo-source-code) and let us know.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53289\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53289\">No</a>\n"]}, {"number": 53288, "title": "[INTEL oneDNN] Disable oneDNN blocked format support", "body": "\r\nThis PR intends to disable oneDNN blocked format support in TensorFlow 2.8. \r\n\r\nWe will refactor related code in TensorFlow 2.9  (Note: refactoring work has been done but PR is BIG;  \r\nwe do not submit it because TF 2.8 cutting date is coming). ", "comments": ["@gzmkl Just curious, do you plan to document somewhere (e.g. on your website) that this environment variable is no longer supported? I wonder if we should make the code print a warning if users have this var set for 2.8 (that setting the var has no effect) and remove the print in 2.9. We can also add this change to TF release note as a minor change note, I guess.", "@penpornk  Hi Penporn, thank you for the approval. \r\n\r\nYes, it is proper to add this change in TF release notes. Please help!\r\n"]}, {"number": 53287, "title": "Refactor TOSA Legalizations to allow a legalization pass, to selectiv\u2026", "body": "\u2026ely apply the DECL_CONVERT_OP(*) allowing to choose whether to legalize the OP to TOSA on OP by OP case.", "comments": ["@rsuderman for review"]}, {"number": 53286, "title": "Refactor the strip_quant_types.cc inorder to only strip quantization \u2026", "body": "\u2026types from values that are produced by TOSA and consumed by TOSA ops", "comments": ["@rsuderman for review"]}, {"number": 53285, "title": "Remove type casting that removed quantization information as part of \u2026", "body": "\u2026the TOSA legalizations\r\n", "comments": ["@rsuderman for review"]}, {"number": 53283, "title": "Bug for training GAN using TensorFlow v2.0", "body": "Hi, I run GAN (train as following code) using TensorFlow 2.0.\r\n\r\n\r\n\r\n```python\r\n\r\n  @tf.function\r\n  def train_step(self, X, y):\r\n      with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\r\n\r\n          predictions = self.generator(X, training=True) \r\n\r\n          generated_frame = predictions\r\n          real_frame = tf.cast(y, tf.float32)\r\n\r\n          concatenate_inputs = tf.concat([real_frame, generated_frame], axis=0)            \r\n          concatenate_outputs = self.discriminator(concatenate_inputs, training=True)\r\n\r\n          score_real, score_generated = tf.split(concatenate_outputs, 2, axis=0)\r\n          discriminator_loss = self.loss_hinge_disc(score_real, score_generated)\r\n          generator_loss = self.generator_loss(real_frame, generated_frame) + 0.05*K.mean(score_generated)\r\n\r\n          gen_gradients = gen_tape.gradient(generator_loss, self.generator.trainable_variables)\r\n          disc_gradients = disc_tape.gradient(discriminator_loss, self.discriminator.trainable_variables)\r\n          \r\n          self.gen_optimizer.apply_gradients(zip(gen_gradients, self.generator.trainable_variables))\r\n          self.disc_optimizer.apply_gradients(zip(disc_gradients, self.discriminator.trainable_variables))\r\n```\r\n\r\nBut I got the following error and the system kill this task.\r\n\r\n```\r\n2021-12-02 20:52:43.795872: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 8137342976 exceeds 10% of free system memory.\r\n2021-12-02 20:52:48.641269: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 8137342976 exceeds 10% of free system memory.\r\n2021-12-02 20:52:59.829117: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-12-02 20:53:00.006395: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3699850000 Hz\r\n2021-12-02 20:53:00.818017: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:592] layout failed: Invalid argument: MutableGraphView::SortTopologically error: detected edge(s) creating cycle(s) {'Func/gradient_tape/model/conv_lst_m2d/while/model/conv_lst_m2d/while_grad/body/_915/input/_3381' -> 'gradient_tape/model/conv_lst_m2d/while/model/conv_lst_m2d/while_grad/body/_915/gradient_tape/model/conv_lst_m2d/while/gradients/AddN', 'Func/gradient_tape/model_1/conv_lst_m2d_5/while/model_1/conv_lst_m2d_5/while_grad/body/_1629/input/_3841' -> 'gradient_tape/model_1/conv_lst_m2d_5/while/model_1/conv_lst_m2d_5/while_grad/body/_1629/gradient_tape/model_1/conv_lst_m2d_5/while/gradients/AddN', 'model_1/conv_lst_m2d_5/while/body/_1451/model_1/conv_lst_m2d_5/while/mul_2' -> 'model_1/conv_lst_m2d_5/while/body/_1451/model_1/conv_lst_m2d_5/while/add_5', 'model_1/conv_lst_m2d_5/while/body/_1451/model_1/conv_lst_m2d_5/while/convolution_6' -> 'model_1/conv_lst_m2d_5/while/body/_1451/model_1/conv_lst_m2d_5/while/add_4', 'model_1/conv_lst_m2d_5/while/body/_1451/model_1/conv_lst_m2d_5/while/clip_by_value' -> 'model_1/conv_lst_m2d_5/while/body/_1451/model_1/conv_lst_m2d_5/while/mul_3', 'model_1/conv_lst_m2d_5/while/body/_1451/model_1/conv_lst_m2d_5/while/clip_by_value_2' -> 'model_1/conv_lst_m2d_5/while/body/_1451/model_1/conv_lst_m2d_5/while/mul_5', 'Func/gradient_tape/model_1/conv_lst_m2d_4/while/model_1/conv_lst_m2d_4/while_grad/body/_1819/input/_3957' -> 'gradient_tape/model_1/conv_lst_m2d_4/while/model_1/conv_lst_m2d_4/while_grad/body/_1819/gradient_tape/model_1/conv_lst_m2d_4/while/gradients/AddN', 'model_1/conv_lst_m2d_4/while/body/_1273/model_1/conv_lst_m2d_4/while/mul_2' -> 'model_1/conv_lst_m2d_4/while/body/_1273/model_1/conv_lst_m2d_4/while/add_5', 'model_1/conv_lst_m2d_4/while/body/_1273/model_1/conv_lst_m2d_4/while/clip_by_value_2' -> 'model_1/conv_lst_m2d_4/while/body/_1273/model_1/conv_lst_m2d_4/while/mul_5', 'model_1/conv_lst_m2d_4/while/body/_1273/model_1/conv_lst_m2d_4/while/clip_by_value' -> 'model_1/conv_lst_m2d_4/while/body/_1273/model_1/conv_lst_m2d_4/while/mul_3', 'model_1/conv_lst_m2d_4/while/body/_1273/model_1/conv_lst_m2d_4/while/convolution_6' -> 'model_1/conv_lst_m2d_4/while/body/_1273/model_1/conv_lst_m2d_4/while/add_4', 'Func/gradient_tape/model_1/conv_lst_m2d_3/while/model_1/conv_lst_m2d_3/while_grad/body/_2009/input/_4073' -> 'gradient_tape/model_1/conv_lst_m2d_3/while/model_1/conv_lst_m2d_3/while_grad/body/_2009/gradient_tape/model_1/conv_lst_m2d_3/while/gradients/AddN', 'model_1/conv_lst_m2d_3/while/body/_1095/model_1/conv_lst_m2d_3/while/mul_2' -> 'model_1/conv_lst_m2d_3/while/body/_1095/model_1/conv_lst_m2d_3/while/add_5', 'model_1/conv_lst_m2d_3/while/body/_1095/model_1/conv_lst_m2d_3/while/convolution_6' -> 'model_1/conv_lst_m2d_3/while/body/_1095/model_1/conv_lst_m2d_3/while/add_4', 'model_1/conv_lst_m2d_3/while/body/_1095/model_1/conv_lst_m2d_3/while/clip_by_value_2' -> 'model_1/conv_lst_m2d_3/while/body/_1095/model_1/conv_lst_m2d_3/while/mul_5', 'model_1/conv_lst_m2d_3/while/body/_1095/model_1/conv_lst_m2d_3/while/clip_by_value' -> 'model_1/conv_lst_m2d_3/while/body/_1095/model_1/conv_lst_m2d_3/while/mul_3', 'model/conv_lst_m2d_2/while/body/_357/model/conv_lst_m2d_2/while/mul_2' -> 'model/conv_lst_m2d_2/while/body/_357/model/conv_lst_m2d_2/while/add_5', 'model/conv_lst_m2d_2/while/body/_357/model/conv_lst_m2d_2/while/convolution_6' -> 'model/conv_lst_m2d_2/while/body/_357/model/conv_lst_m2d_2/while/add_4', 'model/conv_lst_m2d_2/while/body/_357/model/conv_lst_m2d_2/while/clip_by_value_2' -> 'model/conv_lst_m2d_2/while/body/_357/model/conv_lst_m2d_2/while/mul_5', 'model/conv_lst_m2d_2/while/body/_357/model/conv_lst_m2d_2/while/clip_by_value' -> 'model/conv_lst_m2d_2/while/body/_357/model/conv_lst_m2d_2/while/mul_3', 'Func/gradient_tape/model/conv_lst_m2d_1/while/model/conv_lst_m2d_1/while_grad/body/_725/input/_3267' -> 'gradient_tape/model/conv_lst_m2d_1/while/model/conv_lst_m2d_1/while_grad/body/_725/gradient_tape/model/conv_lst_m2d_1/while/gradients/AddN', 'Func/gradient_tape/model/conv_lst_m2d_2/while/model/conv_lst_m2d_2/while_grad/body/_535/input/_3151' -> 'gradient_tape/model/conv_lst_m2d_2/while/model/conv_lst_m2d_2/while_grad/body/_535/gradient_tape/model/conv_lst_m2d_2/while/gradients/AddN', 'model/conv_lst_m2d_1/while/body/_179/model/conv_lst_m2d_1/while/mul_2' -> 'model/conv_lst_m2d_1/while/body/_179/model/conv_lst_m2d_1/while/add_5', 'model/conv_lst_m2d_1/while/body/_179/model/conv_lst_m2d_1/while/clip_by_value' -> 'model/conv_lst_m2d_1/while/body/_179/model/conv_lst_m2d_1/while/mul_3', 'model/conv_lst_m2d_1/while/body/_179/model/conv_lst_m2d_1/while/convolution_6' -> 'model/conv_lst_m2d_1/while/body/_179/model/conv_lst_m2d_1/while/add_4', 'model/conv_lst_m2d_1/while/body/_179/model/conv_lst_m2d_1/while/clip_by_value_2' -> 'model/conv_lst_m2d_1/while/body/_179/model/conv_lst_m2d_1/while/mul_5', 'Func/model/conv_lst_m2d/while/body/_1/input/_2804' -> 'model/conv_lst_m2d/while/body/_1/model/conv_lst_m2d/while/mul_2', 'model/conv_lst_m2d/while/body/_1/model/conv_lst_m2d/while/mul_5' -> 'model/conv_lst_m2d/while/body/_1/model/conv_lst_m2d/while/Identity_4'}.\r\n```\r\n\r\nSo how can I train GAN using Keras properly?\r\n\r\nThanks!\r\n\r\n\r\n", "comments": ["@leelew \r\nI ran the code shared on tf 2.7 and do not see any error, this issue reported has been addresed in latest version please refer to gist [here](https://colab.research.google.com/gist/Saduf2019/89d232cda4f6631a055a7bdcc53c8460/untitled.ipynb)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53283\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53283\">No</a>\n"]}, {"number": 53282, "title": "Update autoclustering_xla.ipynb", "body": "Changed `lr` to `learning_rate`, which removes a deprecation warning.", "comments": ["Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/53282\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n See visual diffs & provide feedback on Jupyter Notebooks. \n\n---\n\n <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>", "This PR is duplicate of [#53012](https://github.com/tensorflow/tensorflow/pull/53012). Hence closing this. Thanks!"]}, {"number": 53281, "title": "Getting this issue while running my fit method (For CNN)", "body": "AttributeError                            Traceback (most recent call last)\r\n~\\AppData\\Local\\Temp/ipykernel_20164/3579053964.py in <module>\r\n----> 1 history = model.fit(\r\n      2     train_ds,\r\n      3     epochs = EPOCH,\r\n      4     batch_size = BATCH_SIZE,\r\n      5     verbose = 1,\r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py in error_handler(*args, **kwargs)\r\n     65     except Exception as e:  # pylint: disable=broad-except\r\n     66       filtered_tb = _process_traceback_frames(e.__traceback__)\r\n---> 67       raise e.with_traceback(filtered_tb) from None\r\n     68     finally:\r\n     69       del filtered_tb\r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py in autograph_handler(*args, **kwargs)\r\n   1127           except Exception as e:  # pylint:disable=broad-except\r\n   1128             if hasattr(e, \"ag_error_metadata\"):\r\n-> 1129               raise e.ag_error_metadata.to_exception(e)\r\n   1130             else:\r\n   1131               raise\r\n\r\nAttributeError: in user code:\r\n\r\n    File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 946, in train_function  *\r\n        return step_function(self, iterator)\r\n    File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 935, in step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 928, in run_step  **\r\n        outputs = model.train_step(data)\r\n    File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 846, in train_step\r\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\r\n    File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 532, in minimize\r\n        return self.apply_gradients(grads_and_vars, name=name)\r\n    File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 668, in apply_gradients\r\n        grads_and_vars = self._aggregate_gradients(grads_and_vars)\r\n    File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 484, in _aggregate_gradients\r\n        return self.gradient_aggregator(grads_and_vars)\r\n    File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\optimizer_v2\\utils.py\", line 33, in all_reduce_sum_gradients\r\n        if tf.__internal__.distribute.strategy_supports_no_merge_call():\r\n\r\n    AttributeError: module 'tensorflow.compat.v2.__internal__.distribute' has no attribute 'strategy_supports_no_merge_call'\r\n", "comments": ["@lionelsamrat10,\r\n\r\nCan you please fill the issue template, and also share the reproducible code to expedite the trouble-shooting process? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Hello, in my case I am importing the following libraries while I run the example that comes as [basic text classification](https://www.tensorflow.org/tutorials/keras/text_classification?hl=es-419) in the official documentation of Tensorflow. But I get the same error. My version of tensorflow is 2.7.0\r\n\r\nRunning code for which the error occurred:\r\n`epochs = 10`\r\n`history = model.fit(\r\n    train_ds,\r\n    validation_data=val_ds,\r\n    epochs=epochs)`\r\n\r\nLibraries loads:\r\n`import matplotlib.pyplot as plt`\r\n`import os`\r\n`import re`\r\n`import shutil`\r\n`import string`\r\n`import tensorflow as tf`\r\n`from tensorflow.keras import layers`\r\n`from tensorflow.keras import losses`", "> Hello, in my case I am importing the following libraries while I run the example that comes as [basic text classification](https://www.tensorflow.org/tutorials/keras/text_classification?hl=es-419) in the official documentation of Tensorflow. But I get the same error. My version of tensorflow is 2.7.0\r\n> \r\n> Running code for which the error occurred: `epochs = 10` `history = model.fit( train_ds, validation_data=val_ds, epochs=epochs)`\r\n> \r\n> Libraries loads: `import matplotlib.pyplot as plt` `import os` `import re` `import shutil` `import string` `import tensorflow as tf` `from tensorflow.keras import layers` `from tensorflow.keras import losses`\r\n\r\nI also am getting the same problem! I have been trying to debug it, but with no success so far. \r\n"]}, {"number": 53280, "title": "'_VariantDataset' object has no attribute 'numpy'", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes and no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu18.04\r\n- TensorFlow installed from (source or binary): pypi \r\n- TensorFlow version (use command below): 2.5.0.dev20210108\r\n- Python version:  3.8.5\r\n\r\n**Describe the current behavior**\r\n```\r\nfor window in win_train_dataset:\r\n  print([item.numpy() for item in window])\r\n<output>:\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n/tmp/ipykernel_877/1363250068.py in <module>\r\n      1 for window in win_train_dataset:\r\n----> 2   print([item.numpy() for item in window])\r\n\r\n/tmp/ipykernel_877/1363250068.py in <listcomp>(.0)\r\n      1 for window in win_train_dataset:\r\n----> 2   print([item.numpy() for item in window])\r\n\r\nAttributeError: '_VariantDataset' object has no attribute 'numpy'\r\n```\r\n\r\n**Describe the expected behavior**\r\n```\r\nfor window in win_train_dataset:\r\n  print([item.numpy() for item in window])\r\n<output>:\r\nitem#1\r\nitem#2\r\n...\r\nitem#n\r\n```\r\n\r\n**Pseudocode to reproduce the issue**\r\n\r\n```\r\ntraining_dataset = tf.keras.utils.image_dataset_from_directory(training_data_dir,\r\n                                                               shuffle=False,\r\n                                                               label_mode='categorical',\r\n                                                               batch_size=hyperparameters[\"BATCH_SIZE\"],\r\n                                                               image_size=IMG_SIZE)) \r\n# Returns a batched dataset, proceed to unbatch it\r\nunb_training_dataset = training_dataset.unbatch()\r\n\r\n# Window the unbatched dataset\r\nwin_train_dataset = unb_train_dataset.window(3, \r\n                                    shift=1, \r\n                                    stride=1,\r\n                                    drop_remainder=True,\r\n                                    )\r\n\r\nfor window in win_train_dataset:\r\n  print([item.numpy() for item in window])\r\n```\r\n", "comments": ["I'd like to add:\r\n\r\n```\r\nfor window in win_train_dataset:\r\n  print([item.numpy() for item in window])\r\n```\r\n\r\nis code from the tf.data.Dataset.window() [documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#window)\r\n\r\nI am using categorical labels, I don't know if that somehow affects how the dataset object is constructed, or if it has some effect on how the WindowDataset object is created", "@ghylander ,\r\nCan you please take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/30664#issuecomment-511035643) from the similar issue and also please try to execute the code in latest stable v2.7 and let us know if the issue still persists.Thanks!", "I believe I am already doing that, let me change the variable names so it looks closer to the comment you linked.\r\n\r\nMy code:\r\n```\r\nfor window in win_train_dataset:\r\n  print([item.numpy() for item in window])\r\n```\r\n\r\nThe code in #30664:\r\n```\r\nfor i in y:\r\n  for j in i:\r\n    print(j.numpy())\r\n```\r\n\r\nMy code using y, i and j as variable names:\r\n```\r\nfor i in y:\r\n  print([j.numpy() for j in i])\r\n```\r\n\r\nEither  way, I tried the exact code in #30664 and it produces the same result:\r\n![image](https://user-images.githubusercontent.com/74593034/144563208-f567f9de-1156-4571-98be-8298c7b3aa62.png)\r\n", "this is the output of:\r\n\r\n```\r\nfor i in y:\r\n  for j in i:\r\n    print(dir(j))\r\n<output>:\r\n['_GeneratorState', '__abstractmethods__', '__bool__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__nonzero__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_add_variable_with_custom_getter', '_apply_options', '_as_serialized_graph', '_checkpoint_dependencies', '_consumers', '_deferred_dependencies', '_flat_shapes', '_flat_structure', '_flat_types', '_functions', '_gather_saveables_for_checkpoint', '_graph', '_graph_attr', '_handle_deferred_dependencies', '_has_captured_ref', '_inputs', '_list_extra_dependencies_for_serialization', '_list_functions_for_serialization', '_lookup_dependency', '_map_resources', '_maybe_initialize_trackable', '_name_based_attribute_restore', '_name_based_restores', '_no_dependency', '_object_identifier', '_options_attr', '_preload_simple_restoration', '_restore_from_checkpoint_position', '_self_name_based_restores', '_self_saveable_object_factories', '_self_setattr_tracking', '_self_unconditional_checkpoint_dependencies', '_self_unconditional_deferred_dependencies', '_self_unconditional_dependency_names', '_self_update_uid', '_setattr_tracking', '_shape_invariant_to_type_spec', '_single_restoration_from_checkpoint_position', '_structure', '_tf_api_names', '_tf_api_names_v1', '_trace_variant_creation', '_track_trackable', '_tracking_metadata', '_type_spec', '_unconditional_checkpoint_dependencies', '_unconditional_dependency_names', '_update_uid', '_variant_tensor', '_variant_tensor_attr', '_variant_tracker', 'apply', 'as_numpy_iterator', 'batch', 'cache', 'cardinality', 'concatenate', 'element_spec', 'enumerate', 'filter', 'flat_map', 'from_generator', 'from_tensor_slices', 'from_tensors', 'interleave', 'list_files', 'map', 'options', 'padded_batch', 'prefetch', 'range', 'reduce', 'repeat', 'shard', 'shuffle', 'skip', 'take', 'unbatch', 'window', 'with_options', 'zip']\r\n\r\n```", "@ghylander ,\r\nCan you please provide the complete code to reproduce the issue.Thanks!", "I'll also add this since I didn't mention it before: I did try updating to tf2.7, but produces the same result\r\n\r\nAbout the code, you can plug the code in the OP, all you need is a keras-style dataset for the tf.keras.utils.image_dataset_from_directory() function:\r\n```\r\nmain_directory/\r\n...class_a/\r\n......a_image_1.jpg\r\n......a_image_2.jpg\r\n...class_b/\r\n......b_image_1.jpg\r\n......b_image_2.jpg\r\n```", "@ghylander ,\r\nWe don't see any code to reproduce the issue reported.Can you please provide the complete code or colab gist to debug the issue.Thanks!", "find gist [here](https://gist.github.com/ghylander/71e55a5d676dcc468fc2911a7f9af77d)", "@ghylander ,\r\nI ran the code shared and face a different error, please find the gist [here](https://colab.research.google.com/gist/tilakrayal/0660e2f442c47c8b2b1615bae854501b/untitled146.ipynb) and share all dependencies to replicate the issue or share a colab gist with the reported error.\r\n\r\n\r\n", "Hi, \r\nupdated the gist [here](https://colab.research.google.com/gist/ghylander/e64c0274e0c4d08e11c817272f57a519/untitled146.ipynb) ", "@Saduf2019 ,\r\nI was able to reproduce the issue tf v2.7, v2.5 and tf-nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/d8e8255540c8f87e88f50a98ec304513/53280.ipynb).", "let me also add:\r\nultimately, what i'm trying to achieve is to get the filenames of the elements in the dataset.\r\n\r\nWhen first creating the tf.data.Dataset object with `tf.keras.preprocessing.image_dataset_from_directory()`, the object has the `file_paths` attribute, which returns a list of the full file path including file name, of every element in the object\r\n\r\nThe `file_paths` attribute however is lost when a method is applied", "@ghylander \r\nOn making a few changes I do not see the error, please find the gist [here](https://colab.research.google.com/gist/Saduf2019/86455b19d976848a414ce8a0719f65d9/untitled650.ipynb), you may also try:\r\nfor i in win_train_dataset:\r\n  for np.j in i:\r\n    print(j)\r\n\r\nPlease create an issue at discussion forum for any further queries as this does not seem like a bug or feature request.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53280\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53280\">No</a>\n", "@Saduf2019 \r\nfrom the [docs](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#window) :\r\n```\r\nfor window in dataset:\r\n  print([item.numpy() for item in window])\r\n```\r\nwhich does not work\r\nyour code, as you stated, is modified from the one on the docs. It would seem that the behaviour of the dataset API changed at some point, making the code in the docs stop working"]}, {"number": 53277, "title": "Different results using same code but different CPU type", "body": "**System information**\r\n- Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.8\r\n\r\n**Describe the current behavior**\r\n\r\nWhen tuning TF models we noted that we were getting different results when using different instance types on AWS. We started removing possible sources of variability until the only difference was the CPU type of the instances. We created a minimal example that trains a simple MLP on a training set and predicts probabilities for a validation set. The following figure compares the predictions obtained with an Intel CPU to those with an AMD CPU:\r\n \r\n![image](https://user-images.githubusercontent.com/3641872/144308840-b0415669-dcbb-4200-bb55-9815843c3e50.png)\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nThe predicted probabilities should be the same with both CPU types and the points in the above plot should all be on the diagonal.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nCode to reproduce issue: https://github.com/rluethy/tf-cpu-type-diff\r\n", "comments": ["Hi @rluethy! Did you check with latest version 2.7 too?\r\nPlease post this issue on [keras-team/keras repo](https://github.com/keras-team/keras/issues) if it is not getting resolved.\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) . Thanks!", "Hi @mohantym , yes, the difference is also observable with version 2.7. I am reposting in keras-team/keras repo.\r\nThanks", "Ok @rluethy ! Please close this issue here as it will be tracked there.", "Reposted here https://github.com/keras-team/keras/issues/15740"]}]