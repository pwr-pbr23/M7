[{"number": 25425, "title": "tf.contrib.distribute performance issue with asymmetrical multi GPU setup", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04, Kernel 4.13.16\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): tensorflow-gpu binary from pip\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.6.3\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: CUDA 9.0 with NCCL 2.3.7-1\r\n- GPU model and memory: 1080ti 12G + 1070ti 8G\r\n\r\n**Description**\r\nI've been trying to port the Google BERT language model to support multi-GPU transfer learning using the new tf.contrib.distribute API. My hardware setup is a 1070ti + 1080ti, and it's a bit strange because I'm getting similar performance between the dual-GPU and a single 1070ti.\r\n\r\nTo reproduce this issue I built a much simpler model (see link) with a timer. And here's the results I got (in secs):\r\n3.0106631300004665 1070Ti + 1080Ti\r\n2.8305218839959707 1070Ti\r\n1.6665251980011817 1080Ti\r\n\r\nI can imagine the 1080Ti being throttled back with a MirroredStrategy, but at least it should perform like a 1070Ti and with the two cards gives 1.6~1.8X of the performance of a 1070Ti? Maybe this is a load-balancing issue with the NCCL?\r\n\r\n**Code to reproduce the issue**\r\nhttps://drive.google.com/file/d/15nJlVV9PWzKcF1AI24H2XSfzzCAhwQ5f/view?usp=sharing\r\n\r\n(Change line 30 to specify different GPUs)\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@peidaqi thank you for trying out MirroredStrategy. \r\nI can't comment on what you're seeing with BERT without more information. But for the small example you provided - my hypothesis is that the graph building time is likely majority of the time that you measured. MirroredStrategy has a known overhead at graph building time as it tries to build 2 copies of your computations, one for each replica, and it does this sequentially. Once this is done, however, the training itself should be faster with 2 comparable GPUs than with one. \r\n\r\nOne thing to remember when you're comparing performances is that your dataset will get consumed in half the number of steps in 2 replica case, as with 1. So if you're using steps / s as the measurement of performance when looking at BERT, then you should expect to get similar or slightly lower number of steps as you increase #GPUs (because you're just consuming more data in each step).\r\n\r\nLet me know if these hypothesis match up with what you see. \r\n\r\n\r\n\r\n\r\n\r\n", "Hi @peidaqi!\r\nIt seems you are using older versions(1.x versions) of Tensorflow which is not supported any more. Please upgrade your code base to 2.7 and create a new issue if you need further assistance.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 25424, "title": "[ROCM] Adding support for rocblas, rocfft, and rocrand", "body": "@timshen91, @whchung \r\n\r\nThis PR is to add the support for rocblas, rocfft, and rocrand plugins. These libraries will be dynamically loaded (similar to their CUDA counterparts).\r\n\r\nNote that even though the \"--config=rocm\" compile will (continue to) build successfully after this PR merged, we will still upstream a few more PRs before the tensorflow functionality can be tested with \"--config=rocm\" builds in the upstream repo.\r\n\r\nTim, \r\n\r\nOnce you have reviewed the changes (which are broken down into 5 commits for ease of review), I can squash them into one if need be. Please let me know if that is required.\r\n\r\nThanks\r\n\r\ndeven", "comments": ["@deven-amd could you cherry-pick commits in https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/pull/305 into this PR? it makes the initialization logic a bit more robust.", "update : got it from the \"buildtools\" repo\r\n\r\n--\r\n\r\nWhere can I get the \"buildifier\" tool? \r\n\r\nThe \"Ubuntu Sanity\" failure seems to due to incorrectly formatted `BUILD` file, and the recommended solution is to run it through the \"buildifier\" tool.\r\n\r\nthanks", "@deven-amd `tensorflow/tools/ci_build/ci_sanity.sh`. check `do_buildifier()`.", "Unfortunately I don't have cycles for the following reviews. I'll leave them to other reviewers.\r\n\r\nI'll juggle this back to @jlebar.", "@chsigg are you willing and able to take this review?", "> @chsigg are you willing and able to take this review?\r\nYes, I can do the review.\r\n", "@deven-amd a rebase might be needed for this PR to move forward.", "@chsigg a gentle ping", "@chsigg another gentle ping. the test results show no impact to existing test targets. the single failure on \"Ubuntu Python3 PIP\" doesn't seem to be related to this PR.", "Generally this looks good. I am going to generate an internal CL from this to continue my review. I will have to mark this PR as 'ready to pull' for that. It doesn't mean it's really ready to be merged, it just allows me to view the difference in our source tree. Thanks for you patience.", "@chsigg \r\n\r\ngentle ping. Also I just rebased to remove the merge conflicts.", "Thanks for your help! I'm working on getting this PR submitted."]}, {"number": 25423, "title": "BeamSearchDecoder.step bug with lengths_to_add OHE dtype ", "body": "Hello,\r\nI have problem with calling .step on BeamSearchDecoder obect:\r\n\r\n```\r\n/home/ms/venv36/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py in step(self, time, inputs, state, name)\r\n    673           end_token=end_token,\r\n    674           length_penalty_weight=length_penalty_weight,\r\n--> 675           coverage_penalty_weight=coverage_penalty_weight)\r\n    676 \r\n    677       finished = beam_search_state.finished\r\n\r\n/home/ms/venv36/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py in _beam_search_step(time, logits, next_cell_state, beam_state, batch_size, beam_width, end_token, length_penalty_weight, coverage_penalty_weight)\r\n    729       on_value=np.int64(0),\r\n    730       off_value=np.int64(1),\r\n--> 731       dtype=dtypes.int64)\r\n    732   add_mask = math_ops.to_int64(not_finished)\r\n    733   lengths_to_add *= array_ops.expand_dims(add_mask, 2)\r\n\r\n/home/ms/venv36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in one_hot(indices, depth, on_value, off_value, axis, dtype, name)\r\n   2418         if on_exists and on_dtype != dtype:\r\n   2419           raise TypeError(\"dtype {0} of on_value does not match \"\r\n-> 2420                           \"dtype parameter {1}\".format(on_dtype, dtype))\r\n   2421         if off_exists and off_dtype != dtype:\r\n   2422           raise TypeError(\"dtype {0} of off_value does not match \"\r\n\r\nTypeError: dtype <dtype: 'int32'> of on_value does not match dtype parameter <dtype: 'int64'>\r\n```\r\nI started this code with Eager Execution. I think, this problem depended with bad on_value, off_value type.  \r\n\r\n\r\n**System information**\r\n- Tensorflow 1.12:\r\n- Ubuntu 18.04:\r\n- TensorFlow installed from https://pypi.org/project/tensorflow-gpu/\r\n- Python version: 3.6\r\n- CUDA version: 9.0\r\n- GPU model and memory: GTX 1080, 8gb\r\n\r\n**Code to reproduce the issue**\r\n```\r\n#in model:\r\n        self.beam_search_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\r\n            cell=model.decoder_cell,\r\n            embedding=model.embedding_decoder,\r\n            start_tokens=tf.fill((batch_size.numpy(), ), tf.Variable(body_vocab['<SOS>'], dtype=tf.int32)),\r\n            end_token=tf.Variable(body_vocab['<EOS>'], dtype=tf.int32),\r\n            initial_state=tf.contrib.seq2seq.tile_batch(encoder_final_state, self.hparams.beaw_width),\r\n            beam_width=self.hparams.beaw_width)\r\n\r\n\r\ninitial_finished, initial_inputs, initial_state = model.beam_search_decoder.initialize()\r\nmodel.beam_search_decoder.step(0, \r\n                               initial_inputs, \r\n                               initial_state)\r\n``` ", "comments": ["I think I found a solution for this problem:\r\n```\r\n#modification for tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py\r\n 726   lengths_to_add = array_ops.one_hot(\r\n 727       indices=array_ops.fill([batch_size, beam_width], end_token),\r\n 728       depth=vocab_size,\r\n 729       on_value=math_ops.to_int64(0),\r\n 730       off_value=math_ops.to_int64(1),\r\n 731       dtype=dtypes.int64)\r\n```\r\n\r\nI'll try to add PR in a few days. ", "Closing this issue since it was resolved. Feel free to send PR to fix this. Thanks!"]}, {"number": 25422, "title": "Add check_ops.assert_shapes and tests", "body": "Issue #24779\r\n\r\nNow supports static and dynamic shape checks. \r\n\r\nNames are now passed via tensors only. \r\n\r\nA suggestion included is also the 'event_shape_only' flag for when only checks on the rightmost dimensions are warranted.", "comments": ["Anything else I can do on this?", "@rthadur can we merge this?", "> @rthadur can we merge this?\r\n\r\nsure", "I understand you must be busy with things relating to tf 2, but if there is anything I can do on this to speed things up let me know", "@rthadur what is blocking this merge?", "> @rthadur what is blocking this merge?\r\n\r\ni started internal merge after approval , will update soon once it is done", "@bodin-e can you please check failed builds ", "> @bodin-e can you please check failed builds\r\n\r\nWas a missing check for __getitem__ implementation of the shape dict entries, as an alternative to __iter__, as either is sufficient for the iter() call (made inside the tuple construction of tuple(shapes[x])). The python 2 string type does not implement __iter__ which caused the errors.\r\n\r\nFixed and pushed now.", "@bodin-e can you please check build failures.", "> @bodin-e can you please check build failures.\r\n\r\nI think I had made a mistake with regards to the tf_export annotation. \r\n\r\nI have now annotated the v1 version with @tf_export(v1=['debugging.assert_shapes']). (I assume v1 is for graph mode as the other v1 versions of assert functions return the op explicitly)\r\n\r\nAnd I have added a v2 version (assert_shapes_v2) with @tf_export('debugging.assert_shapes', v1=[]).\r\n\r\nIs this the correct way? ", "@bodin-e yes this is how we handle different exports for v1 and v2.\r\n", "I will look into what differs in the golden as when the latest master is merged into my branch locally, to see the same diff as happening server-side.", "> I will look into what differs in the golden as when the latest master is merged into my branch locally, to see the same diff as happening server-side.\r\n\r\nThere was a related difference in a golden that I had missed, added it now.\r\n\r\nThere were also some unrelated differences in the golden in master, with path \"tensorflow.summary\", see below. \r\n\r\n```javascript\r\nE0426 20:57:09.711776 4620424640 api_compatibility_test.py:254] 1 differences found between API and golden.\r\nIssue 1\t:\r\n  path: \"tensorflow.summary\"\r\n  tf_module {\r\n    member {\r\n      name: \"SummaryWriter\"\r\n      mtype: \"<type \\'type\\'>\"\r\n    }\r\n    member {\r\n      name: \"experimental\"\r\n      mtype: \"<type \\'module\\'>\"\r\n+   }\r\n+   member_method {\r\n+     name: \"audio\"\r\n+     argspec: \"args=[\\'name\\', \\'data\\', \\'sample_rate\\', \\'step\\', \\'max_outputs\\', \\'encoding\\', \\'description\\'], varargs=None, keywords=None, defaults=[\\'3\\', \\'None\\', \\'None\\'], \"\r\n    }\r\n    member_method {\r\n      name: \"create_file_writer\"\r\n      argspec: \"args=[\\'logdir\\', \\'max_queue\\', \\'flush_millis\\', \\'filename_suffix\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'None\\', \\'None\\', \\'None\\', \\'None\\'], \"\r\n    }\r\n    member_method {\r\n      name: \"create_noop_writer\"\r\n      argspec: \"args=[], varargs=None, keywords=None, defaults=None\"\r\n    }\r\n    member_method {\r\n      name: \"flush\"\r\n      argspec: \"args=[\\'writer\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'None\\', \\'None\\'], \"\r\n    }\r\n    member_method {\r\n+     name: \"histogram\"\r\n+     argspec: \"args=[\\'name\\', \\'data\\', \\'step\\', \\'buckets\\', \\'description\\'], varargs=None, keywords=None, defaults=[\\'None\\', \\'None\\'], \"\r\n+   }\r\n+   member_method {\r\n+     name: \"image\"\r\n+     argspec: \"args=[\\'name\\', \\'data\\', \\'step\\', \\'max_outputs\\', \\'description\\'], varargs=None, keywords=None, defaults=[\\'3\\', \\'None\\'], \"\r\n+   }\r\n+   member_method {\r\n      name: \"record_if\"\r\n      argspec: \"args=[\\'condition\\'], varargs=None, keywords=None, defaults=None\"\r\n+   }\r\n+   member_method {\r\n+     name: \"scalar\"\r\n+     argspec: \"args=[\\'name\\', \\'data\\', \\'step\\', \\'description\\'], varargs=None, keywords=None, defaults=[\\'None\\'], \"\r\n+   }\r\n+   member_method {\r\n+     name: \"text\"\r\n+     argspec: \"args=[\\'name\\', \\'data\\', \\'step\\', \\'description\\'], varargs=None, keywords=None, defaults=[\\'None\\'], \"\r\n    }\r\n    member_method {\r\n      name: \"trace_export\"\r\n      argspec: \"args=[\\'name\\', \\'step\\', \\'profiler_outdir\\'], varargs=None, keywords=None, defaults=[\\'None\\', \\'None\\'], \"\r\n    }\r\n    member_method {\r\n      name: \"trace_off\"\r\n      argspec: \"args=[], varargs=None, keywords=None, defaults=None\"\r\n    }\r\n    member_method {\r\n      name: \"trace_on\"\r\n      argspec: \"args=[\\'graph\\', \\'profiler\\'], varargs=None, keywords=None, defaults=[\\'True\\', \\'False\\'], \"\r\n    }\r\n    member_method {\r\n      name: \"write\"\r\n      argspec: \"args=[\\'tag\\', \\'tensor\\', \\'step\\', \\'metadata\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'None\\', \\'None\\', \\'None\\'], \"\r\n    }\r\n  }\r\n```"]}, {"number": 25421, "title": "Passing flattened tensor into RNN results in an error", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n\r\n- TensorFlow installed from (source or binary): pip install tensorflow-gpu\r\n\r\n- TensorFlow version (use command below):  b'v1.12.0-rc2-3-ga6d8ffae09' 1.12.0\r\n\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source):  N/A\r\n- CUDA/cuDNN version:  9.0\r\n- GPU model and memory:  GeForce RTX 2070\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nPassing flattened tensor into RNN results in an error:  \"Shape (608, ?) must have rank at least 3\"\r\nMore:  I have been trying to find some way to squash down my data via CNNs.  I keep hitting strange dimension issues, where TF complains about None.\r\n\r\n**Describe the expected behavior**\r\nI would like to pass my flattened data into either a Dense or an RNN layer.\r\n\r\n**Code to reproduce the issue**\r\n\r\nfrom collections import namedtuple\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nConvArgs = namedtuple('ConvArgs',\r\n                      'layerInput, numFilters, filterSize, stride, init, namePrefix')\r\nConvArgs.__new__.__defaults__ = (None,) * len(ConvArgs._fields)\r\n\r\n# Set to false to see shape output of the final, flattened layer.\r\nshowError = True\r\n\r\ndef BuildConv2D(convArgs):\r\n    channelAxis = 3\r\n    filterShape = [convArgs.filterSize, \r\n                   convArgs.filterSize, \r\n                   convArgs.layerInput.get_shape()[channelAxis], \r\n                   convArgs.numFilters]\r\n\r\n    filters = tf.get_variable(shape=filterShape, \r\n                              dtype=tf.float32,\r\n                              initializer=convArgs.init,\r\n                              name=convArgs.namePrefix + 'filters')\r\n    \r\n    conv = tf.nn.conv2d(input=convArgs.layerInput,\r\n                        filter=filters, \r\n                        strides=[1,convArgs.stride,convArgs.stride,1], \r\n                        padding='VALID')\r\n\r\n    activated = tf.nn.relu(conv)\r\n    return activated\r\n\r\ntf.reset_default_graph()\r\n\r\nwith tf.Session() as sess:\r\n    data = tf.placeholder(dtype=tf.float32, \r\n                          shape=(None,150000,1), \r\n                          name='data')\r\n\r\n    dataShape = tf.shape(data)\r\n\r\n\r\n    numCols = 80\r\n    numRows = int(150000 / numCols)\r\n    data2D = tf.reshape(data, (tf.shape(data)[0], numRows, numCols, 1))\r\n\r\n    kernelInit = tf.orthogonal_initializer()\r\n    conv1 = BuildConv2D(\r\n            ConvArgs(layerInput = data2D, \r\n                     numFilters = 8,\r\n                     filterSize = 8,\r\n                     stride = 4,\r\n                     init = kernelInit,\r\n                     namePrefix='c1'))\r\n\r\n    conv2 = BuildConv2D(\r\n            ConvArgs(layerInput = conv1, \r\n                     numFilters = 16,\r\n                     filterSize = 8,\r\n                     stride = 4,\r\n                     init = kernelInit,\r\n                     namePrefix='c2'))\r\n\r\n    conv3 = BuildConv2D(\r\n            ConvArgs(layerInput = conv2, \r\n                     numFilters = 16,\r\n                     filterSize = 3,\r\n                     stride = 3,\r\n                     init = kernelInit,\r\n                     namePrefix='c3'))\r\n\r\n    c3_flattened = tf.layers.Flatten()(conv3)\r\n \r\n    if(showError == True):\r\n        lstmCell = tf.contrib.rnn.LSTMBlockCell(num_units=16)\r\n    \r\n        rawOutputs, final_state = tf.nn.dynamic_rnn(\r\n                lstmCell, \r\n                c3_flattened, \r\n                dtype=tf.float32)\r\n\r\n    sess.run(tf.global_variables_initializer())\r\n    result = sess.run(c3_flattened, feed_dict={data:  np.random.randint(-5, 12, size=(2,150000,1))})\r\n    print(result.shape)\r\n    print(result[:, 0:20])\r\n    \r\n    if(showError == True):\r\n        resultA, resultB = sess.run([rawOutputs, final_state], feed_dict={data:  np.random.randint(-5, 12, size=(2,150000,1))})\r\n        print(resultA.shape)\r\n        print(resultB.shape)\r\n\r\n**Other info / logs**\r\nWhen attempting to use a Dense layer, I ran into the issue described here:\r\nhttps://github.com/tensorflow/tensorflow/issues/13348\r\n", "comments": ["Sorry for the late reply.\r\n\r\nIf I read your code correctly, the input to LSTM is the output from a flatten layer, which should be a 2D tensor. If that's the case, then it is expected to throw an error since RNN will expect a 3D tensor as input (batch, timestep, feature).", "Closing this issue for now since the input the does not meet the expectation for RNN. Please reopen the issue if you feel this is not the case."]}, {"number": 25420, "title": "Added TC in convert test file, invalid inference", "body": "Added TC for the invalid interence type", "comments": ["@gargn  can you pls review the PR and let me know your comments", "@gargn & @rthadur , I have updated the code as per your comments, kindly check, if everything is ok, kindly approve the PR.", "@rthadur , can you pls update the label to review requested", "@gargn , thanks for approving the PR, can you please help to get this merged.\r\n\r\nRegards\r\nAmit", "@rthadur , can you please help to get this merged.\r\n\r\nRegards\r\nAmit", "@rthadur , can you please help to get this merged.\r\n\r\nRegards\r\nAmit", "It seems support for `int8` was added so this test no longer causes a failure."]}, {"number": 25419, "title": "tf-nightly-gpu-2.0 import problem", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): pip install tf-nightly-gpu-2.0-preview\r\n- TensorFlow version: tf-nightly-gpu-2.0-preview 2.0.0.dev20190201\r\n- Python version: 3.6.5\r\n- Installed using virtualenv? pip? conda?: virtual env with pip\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: Tesla k40d 11GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nCannot import tensorflow in python\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nimport tensorflow as tf\r\n\r\n**Any other info / logs**\r\n`>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"/net/scratch/people/plgmkowalski94/master/new/yolo3_tf2/tf2/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/net/scratch/people/plgmkowalski94/master/new/yolo3_tf2/tf2/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/net/scratch/people/plgmkowalski94/master/new/yolo3_tf2/tf2/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/net/scratch/people/plgmkowalski94/master/new/yolo3_tf2/tf2/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/net/scratch/people/plgmkowalski94/master/new/yolo3_tf2/tf2/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/net/scratch/people/plgmkowalski94/master/new/yolo3_tf2/tf2/lib/python3.6/site-packages/tensorflow/__init__.py\", line 27, in <module>\r\n    from tensorflow._api.v2 import audio\r\n  File \"/net/scratch/people/plgmkowalski94/master/new/yolo3_tf2/tf2/lib/python3.6/site-packages/tensorflow/_api/v2/audio/__init__.py\", line 8, in <module>\r\n    from tensorflow.python.ops.gen_audio_ops import encode_wav\r\n  File \"/net/scratch/people/plgmkowalski94/master/new/yolo3_tf2/tf2/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/net/scratch/people/plgmkowalski94/master/new/yolo3_tf2/tf2/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/net/scratch/people/plgmkowalski94/master/new/yolo3_tf2/tf2/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/net/scratch/people/plgmkowalski94/master/new/yolo3_tf2/tf2/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/net/scratch/people/plgmkowalski94/master/new/yolo3_tf2/tf2/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/net/scratch/people/plgmkowalski94/master/new/yolo3_tf2/tf2/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/net/scratch/people/plgmkowalski94/master/new/yolo3_tf2/tf2/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.`\r\n", "comments": ["Based on what you've written in the report, you have CUDA/cuDNN version: 9.0, but tf-nightly-gpu-2.0 requires CUDA 10, as you can see from the error\r\n\r\n```\r\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n```", "then I've got huge problem due to cluster apps \r\n` plgrid/apps/cuda/5.5    plgrid/apps/cuda/7.0 (D)    plgrid/apps/cuda/8.0    plgrid/apps/cuda/9.1\r\n   plgrid/apps/cuda/6.5    plgrid/apps/cuda/7.5        plgrid/apps/cuda/9.0`", "I'm confused what are you looking for as a solution though. Are you looking for a CUDA 9.0 build of nightly 2.0 tensorflow?", "No, I'm just pointing out that I have much bigger problem with cluster apps\ndue to availability of CUDA 10\n\nPt., 1 lut 2019, 14:28: Jakub Arnold <notifications@github.com> napisa\u0142(a):\n\n> I'm confused what are you looking for as a solution though. Are you\n> looking for a CUDA 9.0 build of nightly 2.0 tensorflow?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/25419#issuecomment-459721525>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AmQBuKYUKJqBmZlNH_qgmuMpPk9riJsMks5vJEDxgaJpZM4aeSdp>\n> .\n>\n", "If you need the nightly build with 9.0 you can try compiling it yourself on 9.0. Though I'm not sure if it uses 10.0 because it is backwards incompatible with 9.0 or just because it was more convenient to move to the current version.", "@gunan Can you PTAL? Thanks!", "@darthdeus 's assessment is correct.\r\nTF builds require cuda 10.\r\nUnfortunately, if you need to use cuda 9.0, you either need to use an older version (such as TF 1.12) or build the latest branch from sources.\r\n\r\nWe are working on solutions where supporting multiple CUDA versions would be easier for TF,  but those are still months away."]}, {"number": 25418, "title": "`tf.contrib.framework.is_tensor` is not available/exported in 2.0", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-dev20190126\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nThe [tf.contrib.framework.is_tensor](https://www.tensorflow.org/api_docs/python/tf/contrib/framework/is_tensor) function is not exported in TF 2.0, even though it is defined outside of contrib in the current master version, in [tensorflow/python/framework/tensor_util.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/tensor_util.py#L935-L950).\r\n\r\nIt is neither in the [list of exported symbols](https://www.tensorflow.org/versions/r2.0/api_docs/python?hl=en), nor in [the tensorflow/addons repo](https://github.com/tensorflow/addons).\r\n\r\n**Describe the expected behavior**\r\n\r\n`is_tensor` should probably be exported in regular TF 2.0 API, since it is already defined outside of contrib and being used all over the place within the TF sources.\r\n\r\n**Code to reproduce the issue**\r\n\r\nN/A\r\n\r\n**Other info / logs**\r\n\r\nN/A", "comments": ["@alextp has a fix for this I think. ", "It's @tomerk who has it", "This has been fixed in head."]}, {"number": 25417, "title": " TensorRT loses defined shapes", "body": "\r\nSystem information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\nBazel version: N/A\r\nGPU model and memory: P4 8G\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\nTensorFlow installed from (source or binary): docker\r\nTensorFlow version (use command below): 1.12\r\nPython version: 3.5\r\nCUDA/cuDNN version: 10/7.4.1\r\nDescribe the problem\r\n\r\nWhen tf.contrib.tensorrt compiles a graph which can be fully compiled it loses shape information.\r\n\r\nhttps://github.com/tensorflow/tensorrt.git\r\njust run the tensorrt/tftrt/examples/image-classification/image_classification.py with int8 parameter like the follow command and get the frozen model from graphs directory.\r\n\r\n python image_classification.py --model resnet_v1_50 --data_dir /data/ImageNetVal/ --precision fp32   --batch_size=1 --use_trt\r\n\r\nthe one frozen from tensorrt:\r\nTensor(\"input:0\", shape=(?, 224, 224, 3), dtype=float32)\r\nTensor(\"logits:0\", dtype=float32)\r\n\r\nand the native one:\r\n\r\nTensor(\"input:0\", shape=(?, 224, 224, 3), dtype=float32)\r\nTensor(\"logits:0\", shape=(?, 1001), dtype=float32)\r\n\r\nthe difference is the shape in logits from tensorrt have been lost.\r\n\r\nscript as follow:\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import tensorrt as trt\r\nimport os\r\nimport shutil\r\ngraph_pb = 'frozen_graph_resnet_v1_50_1_fp32_1.pb'\r\n\r\nwith tf.gfile.GFile(graph_pb, \"rb\") as f:\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\nsigs = {}\r\nwith tf.Session(graph=tf.Graph()) as sess:\r\n    for n in graph_def.node:\r\n        if n.name == \"logits\":\r\n            print(n)\r\n        if n.name == \"input\":\r\n            print(n)\r\n\r\n@joeyearsley ", "comments": ["@trevor-m Can you PTAL? Thanks!", "@rankeey , It is not trivial to estimate the output shapes of TRTEngineOp for given inputs, so its shape inference is disabled as of now. What you print is output of shape inference. Your network will run fine, it just can't do shape inference to print the shapes."]}, {"number": 25416, "title": "tf.keras.utils.multi  does not work with WGAN-GP Model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\ntf.keras.utils.multi  does not work with WGAN-GP Model\r\nI also try on Keras(not tf.keras) and it was work!\r\n\r\n**Code to reproduce the issue**\r\ndiscriminator model code:\r\n```\r\ngenerator = gan_generator()\r\ndiscriminator = gen_discriminator()\r\n\r\n\r\n# Image input (real sample)\r\nreal_img = tf.keras.Input(shape=img_shape)\r\n# Noise input\r\nz_disc = tf.keras.Input(shape=(100,))\r\n# Generate image based of noise (fake sample)\r\nfake_img = generator(z_disc)\r\n\r\n# Discriminator determines validity of the real and fake images\r\nfake = discriminator(fake_img)\r\nvalid = discriminator(real_img)\r\n\r\n# Construct weighted average between real and fake images\r\ninterpolated_img = RandomWeightedAverage()([real_img, fake_img])\r\n# Determine validity of weighted sample\r\nvalidity_interpolated = discriminator(interpolated_img)\r\n\r\n# Use Python partial to provide loss function with additional\r\n# 'averaged_samples' argument\r\npartial_gp_loss = partial(gradient_penalty_loss,\r\n                          averaged_samples=interpolated_img)\r\npartial_gp_loss.__name__ = 'gradient_penalty'   # Keras requires function names\r\n\r\ndiscriminator_model = tf.keras.Model(inputs=[real_img, z_disc], outputs=[valid, fake, validity_interpolated])\r\nparallel_discriminator_model = tf.keras.utils.multi_gpu_model(discriminator_model, gpus=2)\r\n```\r\n\r\ngenerator & dircriminator body\r\n```\r\ndef gan_generator(self):\r\n    inputs = tf.keras.Input(shape=(100,))\r\n\r\n    x = tf.keras.layers.Dense(7 * 7 * self.dims * 4, activation='relu', input_dim=self.latent_dim)(inputs)\r\n    x = tf.keras.layers.Reshape((7, 7, self.dims * 4))(x)\r\n\r\n    x = tf.keras.layers.UpSampling2D()(x)\r\n    x = tf.keras.layers.Conv2D(self.dims * 4, kernel_size=5, padding=\"same\")(x)\r\n    x = tf.keras.layers.BatchNormalization(momentum=0.8)(x)\r\n    x = tf.keras.layers.Activation(\"relu\")(x)\r\n\r\n    x = tf.keras.layers.UpSampling2D()(x)\r\n    x = tf.keras.layers.Conv2D(self.dims * 2, kernel_size=5, padding=\"same\")(x)\r\n    x = tf.keras.layers.BatchNormalization(momentum=0.8)(x)\r\n    x = tf.keras.layers.Activation(\"relu\")(x)\r\n\r\n    # x = UpSampling2D()(x)\r\n    # x = Conv2D(self.dims * 1, kernel_size=5, padding=\"same\")(x)\r\n    # x = BatchNormalization(momentum=0.8)(x)\r\n    # x = Activation(\"relu\")(x)\r\n\r\n    x = tf.keras.layers.Conv2D(self.channels, kernel_size=5, padding=\"same\")(x)\r\n    x = tf.keras.layers.Activation(\"tanh\")(x)\r\n\r\n    return tf.keras.Model(inputs, x)\r\n\r\ndef gen_discriminator(self):\r\n    inputs = tf.keras.Input(shape=self.img_shape)\r\n\r\n    x = tf.keras.layers.Conv2D(self.dims, kernel_size=5, strides=2, padding=\"same\", input_shape=self.img_shape)(inputs)\r\n    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\r\n    x = tf.keras.layers.Dropout(0.25)(x)\r\n\r\n    x = tf.keras.layers.Conv2D(self.dims * 2, kernel_size=5, strides=2, padding=\"same\")(x)\r\n    x = tf.keras.layers.ZeroPadding2D(padding=((0, 1), (0, 1)))(x)\r\n    x = tf.keras.layers.BatchNormalization(momentum=0.8)(x)\r\n    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\r\n    x = tf.keras.layers.Dropout(0.25)(x)\r\n\r\n    x = tf.keras.layers.Conv2D(self.dims * 4, kernel_size=5, strides=2, padding=\"same\")(x)\r\n    x = tf.keras.layers.BatchNormalization(momentum=0.8)(x)\r\n    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\r\n    x = tf.keras.layers.Dropout(0.25)(x)\r\n\r\n    x = tf.keras.layers.Conv2D(self.dims * 4, kernel_size=5, strides=2, padding=\"same\")(x)\r\n    x = tf.keras.layers.BatchNormalization(momentum=0.8)(x)\r\n    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\r\n    x = tf.keras.layers.Dropout(0.25)(x)\r\n    x = tf.keras.layers.Flatten()(x)\r\n    x = tf.keras.layers.Dense(1)(x)\r\n    return tf.keras.Model(inputs, x)\r\n\r\n```\r\n\r\n**Other info / logs**\r\n```\r\n2019-02-01 10:23:31.330533: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-02-01 10:23:31.441144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-02-01 10:23:31.441617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \r\nname: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.797\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.93GiB freeMemory: 7.18GiB\r\n2019-02-01 10:23:31.530899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-02-01 10:23:31.531436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 1 with properties: \r\nname: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.797\r\npciBusID: 0000:03:00.0\r\ntotalMemory: 7.93GiB freeMemory: 7.83GiB\r\n2019-02-01 10:23:31.532281: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1\r\n2019-02-01 10:23:32.028009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-02-01 10:23:32.028044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 \r\n2019-02-01 10:23:32.028053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y \r\n2019-02-01 10:23:32.028061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N \r\n2019-02-01 10:23:32.028301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6920 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2019-02-01 10:23:32.028599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7558 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1070, pci bus id: 0000:03:00.0, compute capability: 6.1)\r\nTraceback (most recent call last):\r\n  File \"/home/share/user/John/WGAN_GP/tf_main.py\", line 258, in <module>\r\n    wgan = WGANGP()\r\n  File \"/home/share/user/John/WGAN_GP/tf_main.py\", line 82, in __init__\r\n    parallel_discriminator_model = tf.keras.utils.multi_gpu_model(self.discriminator_model, gpus=2)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/utils/multi_gpu_utils.py\", line 252, in multi_gpu_model\r\n    return Model(model.inputs, merged)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\", line 121, in __init__\r\n    super(Model, self).__init__(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py\", line 80, in __init__\r\n    self._init_graph_network(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/checkpointable/base.py\", line 474, in _method_wrapper\r\n    method(self, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py\", line 268, in _init_graph_network\r\n    self.inputs, self.outputs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py\", line 1833, in _map_graph_network\r\n    str(all_names.count(name)) + ' times in the model. '\r\nValueError: The name \"model_1\" is used 3 times in the model. All layer names should be unique.\r\n```", "comments": ["@KUASWoodyLIN,\r\n[Distribution Strategy](https://www.tensorflow.org/tutorials/distribute/keras) can be a good option if you want to run your model with Multiple GPUs. Can you please take a look and let us know if this resolves your issue? Also, please refer the documentation for [Distributed Training](https://www.tensorflow.org/guide/distributed_training), [tf.distribute](https://www.tensorflow.org/api_docs/python/tf/distribute) and [Custom Training with tf.distribute.strategy](https://www.tensorflow.org/tutorials/distribute/custom_training) Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 25415, "title": "There are some error when convert Mask RCNN model to .tflite", "body": "### System information\r\n- **What is the top-level directory of the model you are using**:\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.12\r\n- **Bazel version (if compiling from source)**: 0.22.0\r\n- **CUDA/cuDNN version**: cuda9.0/cuDNN7.4\r\n- **GPU model and memory**: GTX 1080/8G\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\ni trained mask rcnn using object detection API, and frozen the mode successfully, but i encountered some problems when i convert  .pb file to .tflite file, following is my command\r\n\r\n```\r\ntflite_convert \\\r\n  --graph_def_file=/path/to/frozen_inference_graph.pb \\\r\n  --output_file=optimized_graph.lite \\\r\n  --output_format=TFLITE \\\r\n  --output_arrays=detection_masks,detection_classes,detection_boxes,detection_scores,num_detections \\\r\n  --inference_input_type=FLOAT \\\r\n  --input_shapes=1,224,224,3 \\\r\n  --input_arrays=image_tensor \\\r\n  --inference_type=FLOAT\r\n```\r\nthe output_array is the same with output_node_names form research/object_detection/exporter.py\r\n### Source code / logs\r\nerror\r\n```\r\ntensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: CropAndResize_1\\n2019-01-31 16:16:52.071330: I \r\ntensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 7262 operators, 13617 arrays (0 quantized)\\n2019-01-31 16:16:52.843219: I \r\ntensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 7208 operators, 13526 arrays (0 quantized)\\n2019-01-31 16:16:54.116685: I \r\ntensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 7208 operators, 13526 arrays (0 quantized)\\n2019-01-31 16:16:54.363824: F \r\ntensorflow/contrib/lite/toco/graph_transformations/resolve_constant_slice.cc:59] Check failed: dim_size >= 1 (0 vs. 1)\\nAborted (core dumped)\\n'None\r\n```\r\n", "comments": ["@noah003 did u find a solution?", "@NicholaiStaalung haven't not", "@noah003 are you converting the mask rcnn model found in this repo https://github.com/matterport/Mask_RCNN/releases/tag/v2.0 ?", "Hi @noah003 ! Can you check these threads for MRCNN to tflite conversion?  [1](https://github.com/matterport/Mask_RCNN/issues/563#issuecomment-832002326),[ 2](https://github.com/matterport/Mask_RCNN/issues/563#issuecomment-571236568) . Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 25414, "title": "Tensorflow 2.0 Preview - tf.function and tensorflow_dataset incompatibility", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\n- TensorFlow installed from (source or binary): pip \r\n- TensorFlow version (use command below): tf-nightly-2.0-preview\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: GTX 1080Ti\r\n\r\n**Describe the current behavior**\r\nCreating a dataset using tensorflow_dataset and passing it to a function decorated with `@tf.function` gives the error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/emanuele/development/gan-toolbox/try2.py\", line 308, in <module>\r\n    main()\r\n  File \"/home/emanuele/development/gan-toolbox/try2.py\", line 304, in main\r\n    _ = gan.train(dataset, G_opt, D_opt)\r\n  File \"/home/emanuele/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 398, in __call__\r\n    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n  File \"/home/emanuele/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 460, in _filtered_call\r\n    (t for t in nest.flatten((args, kwargs))\r\n  File \"/home/emanuele/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 507, in _call_flat\r\n    outputs = self._inference_function.call(ctx, args)\r\n  File \"/home/emanuele/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 312, in call\r\n    ctx=ctx)\r\n  File \"/home/emanuele/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\", line 66, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.NotFoundError: Function Dataset_interleave_TFRecordExampleAdapter.dataset_from_filename_4 is not defined.\r\n\t [[{{node ReduceDataset}}]] [Op:__inference_train_3786]\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\nNot encountering the error. I think the problem is related to the magic behind tf.function. The issue is that there isn't a complete guide on how to use correctly tf.function.\r\n\r\n\r\n**Code to reproduce the issue**\r\n```python\r\n\"\"\"\r\nImplement DCGAN using the new TF 2.0 API.\r\n\r\nAlso test tensorflow-datasets.\r\n\r\nCeleb-A dataset.\r\n\"\"\"\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\nfrom typing import Dict\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\nfrom tensorflow import keras as k\r\nfrom tensorflow.python.ops import control_flow_util\r\n\r\n\r\ncontrol_flow_util.ENABLE_CONTROL_FLOW_V2 = True\r\n\r\n\r\n@tf.function\r\ndef bce(x, label, label_smoothing=0.0):\r\n    \"\"\"Returns the discrete binary cross entropy between x and the discrete label\r\n    Args:\r\n        x: a 2D tensor\r\n        label: the discrete label, aka, the distribution to match\r\n        label_smoothing: if greater than zero, smooth the labels\r\n\r\n    Returns:\r\n        The binary cros entropy\r\n    \"\"\"\r\n    return k.losses.BinaryCrossentropy()(tf.ones_like(x) * label, x)\r\n\r\n\r\ndef min_max(positive, negative, label_smoothing=0.0):\r\n    \"\"\"Returns the discriminator (min max) loss\r\n    Args:\r\n        positive: the discriminator output for the positive class: 2D tensor\r\n        negative: the discriminator output for the negative class: 2D tensor\r\n        smooth: if greater than zero, appiles one-sided label smoothing\r\n    Returns:\r\n        The sum of 2 BCE\r\n    \"\"\"\r\n    one = tf.constant(1.0)\r\n    zero = tf.constant(0.0)\r\n    d_loss = bce(positive, one, label_smoothing) + bce(negative, zero)\r\n    return d_loss\r\n\r\n\r\nclass Generator(k.Model):\r\n    def __init__(self):\r\n        super(Generator, self).__init__()\r\n        self.fc1 = k.layers.Dense(4 * 4 * 1024)\r\n        self.batchnorm1 = k.layers.BatchNormalization()\r\n\r\n        self.conv2 = k.layers.Conv2DTranspose(\r\n            filters=512,\r\n            kernel_size=(5, 5),\r\n            strides=(2, 2),\r\n            padding=\"same\",\r\n            use_bias=False,\r\n        )\r\n        self.batchnorm2 = k.layers.BatchNormalization()\r\n\r\n        self.conv3 = k.layers.Conv2DTranspose(\r\n            filters=256,\r\n            kernel_size=(5, 5),\r\n            strides=(2, 2),\r\n            padding=\"same\",\r\n            use_bias=False,\r\n        )\r\n        self.batchnorm3 = k.layers.BatchNormalization()\r\n\r\n        self.conv4 = k.layers.Conv2DTranspose(\r\n            filters=128,\r\n            kernel_size=(5, 5),\r\n            strides=(2, 2),\r\n            padding=\"same\",\r\n            use_bias=False,\r\n        )\r\n        self.batchnorm4 = k.layers.BatchNormalization()\r\n\r\n        self.conv5 = k.layers.Conv2DTranspose(\r\n            filters=3,\r\n            kernel_size=(5, 5),\r\n            strides=(2, 2),\r\n            padding=\"same\",\r\n            use_bias=False,\r\n        )\r\n        self.batchnorm5 = k.layers.BatchNormalization()\r\n\r\n    def call(self, x: tf.Tensor, training: bool = True) -> tf.Tensor:\r\n        x = self.fc1(x)\r\n        x = self.batchnorm1(x, training=training)\r\n        x = tf.nn.relu(x)\r\n        x = tf.reshape(x, shape=(-1, 4, 4, 1024))\r\n\r\n        x = self.conv2(x)\r\n        x = self.batchnorm2(x, training=training)\r\n        x = tf.nn.relu(x)\r\n\r\n        x = self.conv3(x)\r\n        x = self.batchnorm3(x, training=training)\r\n        x = tf.nn.relu(x)\r\n\r\n        x = self.conv4(x)\r\n        x = self.batchnorm4(x, training=training)\r\n        x = tf.nn.relu(x)\r\n\r\n        x = self.conv5(x)\r\n        x = self.batchnorm5(x, training=training)\r\n\r\n        x = tf.nn.tanh(x)\r\n        return x\r\n\r\n\r\nclass Discriminator(k.Model):\r\n    def __init__(self):\r\n        super(Discriminator, self).__init__()\r\n        self.conv1 = k.layers.Conv2D(128, (5, 5), strides=(2, 2), padding=\"same\")\r\n        self.conv2 = k.layers.Conv2D(256, (5, 5), strides=(2, 2), padding=\"same\")\r\n        self.batchnorm2 = k.layers.BatchNormalization()\r\n        self.conv3 = k.layers.Conv2D(512, (5, 5), strides=(2, 2), padding=\"same\")\r\n        self.batchnorm3 = k.layers.BatchNormalization()\r\n        self.conv4 = k.layers.Conv2D(1024, (5, 5), strides=(2, 2), padding=\"same\")\r\n        self.batchnorm4 = k.layers.BatchNormalization()\r\n        self.flatten = k.layers.Flatten()\r\n        self.fc5 = k.layers.Dense(1)\r\n\r\n    def call(self, x, training=True):\r\n        x = self.conv1(x)\r\n        x = tf.nn.leaky_relu(x)\r\n\r\n        x = self.conv2(x)\r\n        x = self.batchnorm2(x)\r\n        x = tf.nn.leaky_relu(x)\r\n\r\n        x = self.conv3(x)\r\n        x = self.batchnorm3(x)\r\n        x = tf.nn.leaky_relu(x)\r\n\r\n        x = self.conv4(x)\r\n        x = self.batchnorm4(x)\r\n        x = tf.nn.leaky_relu(x)\r\n\r\n        x = self.flatten(x)\r\n        x = self.fc5(x)\r\n        return x\r\n\r\n\r\nclass GAN:\r\n    def __init__(self, generator, discriminator, encoder=None):\r\n        \"\"\"\r\n        GAN initializer.\r\n\r\n        Args:\r\n            generator: A ``tensorflow.keras.Model`` to use as Generator.\r\n            discriminator: A ``tensorflow.keras.Model`` to use as Discriminator.\r\n            encoder: A ``tensorflow.keras.Model`` to use as Encoder.\r\n\r\n        Returns:\r\n            Trained GAN model (?).\r\n\r\n        \"\"\"\r\n        self.G = generator()\r\n        self.D = discriminator()\r\n        # self.E = encoder() if encoder is not None else None\r\n        self.latent_vector_dims = 100\r\n\r\n    @tf.function()\r\n    def train(self, dataset, opt1, opt2):\r\n        \"\"\"\r\n        Train.\r\n        \"\"\"\r\n        step = 0\r\n        for f in dataset:\r\n            x = f[\"image\"]\r\n            step += 1\r\n            z = tf.random.normal((32, self.latent_vector_dims))\r\n\r\n            # We record all the operations in the tape\r\n            with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\r\n                G_z = self.G(z, training=True)\r\n\r\n                D_x = self.D(x, training=True)\r\n                D_Gz = self.D(G_z, training=True)\r\n\r\n                g_loss = bce(D_Gz, 1.0)\r\n                d_loss = min_max(D_x, D_Gz, label_smoothing=0.0)\r\n\r\n            # We retrieve the gradients from our records\r\n            G_grads = gen_tape.gradient(g_loss, self.G.trainable_variables)\r\n            D_grads = disc_tape.gradient(d_loss, self.D.trainable_variables)\r\n\r\n            # Optimize and apply the gradients\r\n            opt1.apply_gradients(zip(G_grads, self.G.trainable_variables))\r\n            opt2.apply_gradients(zip(D_grads, self.D.trainable_variables))\r\n\r\n            if step % 1 == 0:\r\n                print(\"--------------------------\")\r\n                print(\"STEP\", step)\r\n                print(\"D_LOSS\", d_loss)\r\n                print(\"G_LOSS:\", g_loss)\r\n        return step\r\n\r\n\r\nclass InputPipeline:\r\n    def __init__(\r\n        self, dataset, batch_size, epochs, shuffle_buffer, prefetched_items, size\r\n    ):\r\n        self.batch_size = batch_size\r\n        self.dataset_name = dataset\r\n        self.epochs = epochs\r\n        self.prefetched_items = prefetched_items\r\n        self.shuffle_buffer = shuffle_buffer\r\n        self.size = size\r\n\r\n    def get_input_fn(self):\r\n        \"\"\"Input fn.\"\"\"\r\n        return self.input_fn\r\n\r\n    def load_public_dataset(self):\r\n        \"\"\"\r\n        Load one of the publicly available datasets, will merge together all the splits.\r\n\r\n        Args:\r\n            chosen_dataset: dataset to use.\r\n\r\n        Return:\r\n            The chosen dataset as a ``tf.data.Dataset``\r\n\r\n        \"\"\"\r\n        # Construct a tf.data.Dataset\r\n        datasets = tfds.load(name=self.dataset_name, split=tfds.Split.ALL)\r\n        return datasets\r\n\r\n    def resize_images(self, features):\r\n        \"\"\"\r\n        Overwrite the \\\"image\\\" feature in order to resize them.\r\n\r\n        Args:\r\n            features: features dictionary.\r\n            size: desired target size.\r\n\r\n        Returns:\r\n            Features with \\\"image\\\" resized to the correct shape.\r\n\r\n        \"\"\"\r\n        features[\"image\"] = tf.image.resize(features[\"image\"], self.size)\r\n        return features\r\n\r\n    def input_fn(self):\r\n        dataset = self.load_public_dataset()\r\n        dataset = (\r\n            dataset.map(self.resize_images)\r\n            .shuffle(self.shuffle_buffer)\r\n            .batch(self.batch_size)\r\n            .prefetch(self.prefetched_items)\r\n            .repeat(self.epochs)\r\n        )\r\n        return dataset\r\n\r\n\r\ndef main():\r\n    # See available datasets\r\n    public_datasets = tfds.list_builders()\r\n\r\n    gan = GAN(Generator, Discriminator)\r\n    G_opt = k.optimizers.Adam(learning_rate=1e-5, beta_1=0.5)\r\n    D_opt = k.optimizers.Adam(learning_rate=1e-5, beta_1=0.5)\r\n\r\n    input_pipeline = InputPipeline(\r\n        dataset=\"celeb_a\",\r\n        batch_size=32,\r\n        epochs=2,\r\n        prefetched_items=1,\r\n        shuffle_buffer=1000,\r\n        size=(64, 64),\r\n    )\r\n    dataset = input_pipeline.input_fn()\r\n    _ = gan.train(dataset, G_opt, D_opt)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n**Other info / logs**\r\nIf I remove the `@tf.function` annotation the code works as expected. \r\nIf I create the dataset inside the annotate function the code works as expected.\r\n\r\nBasically in the code below I create a td.data.Dataset using the new package tensorflow_dataset. Then I pass the created dataset to a function annotated with `tf.function` that should perform the training loop. The errors I get are not informative.\r\nUnfortunately the tensorflow docs do not explain well how to use tf.function and the admitted operations.\r\n\r\n", "comments": ["@jsimsa I think this is the same function registration issue we've discussed before, right?", "Correct. It is not a TF 2.0 issue. The same behavior would happen in TF 1.x nightly.\r\n\r\nThe use program will need to pass a dataset factory into train method so that the dataset is created in the same graph in which it is iterated over.", "@EmanueleGhelfi Can you try the workaround @jsimsa  suggested by creating the dataset inside the tf.function? Alternatively you can create the iterator outside.\r\n\r\nWe're still working on a fix for this so I'd like to leave this open.", "@alextp @jsimsa If I create the dataset inside the tf.function the code works as expected. I think this should be at least documented. For me it is not clear how to use correctly tf.function, the things that are allowed and the things not allowed. If I need to use tf.function in every function I call I think it would be nicer to use a global setting.", "The reason why this is not documented is that this is a bug we're still\nworking on a fix for (and documenting broken behavior leads to workarounds\npersisting for long after the issue has been fixed).\n\nOn Sat, Feb 2, 2019 at 1:54 AM Emanuele Ghelfi <notifications@github.com>\nwrote:\n\n> @alextp <https://github.com/alextp> @jsimsa <https://github.com/jsimsa>\n> If I create the dataset inside the tf.function the code works as expected.\n> I think this should be at least documented. For me it is not clear how to\n> use correctly tf.function, the things that are allowed and the things not\n> allowed. If I need to use tf.function in every function I call I think it\n> would be nicer to use a global setting.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/25414#issuecomment-459951747>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxSMmfdXlXCR3Em58TmVEJGbk2R0Jks5vJWBNgaJpZM4ad2jI>\n> .\n>\n\n\n-- \n - Alex\n", "Ok, thank you @alextp for your support.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25414\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25414\">No</a>\n"]}, {"number": 25413, "title": "How can I do some process like segment_sum to a batch data in parrelel", "body": "For example..\r\nI got an embedding matrix as follow:\r\nembed_matrix = [\r\n[0,0,0,0],\r\n[1,1,1,1],\r\n[2,2,2,2],\r\n]\r\n\r\nI have another input which decides which embedding should I use for summation to get a new embedding. \r\nids = [\r\n[0,1,],\r\n[1,2,],\r\n[0,1,2]\r\n]\r\n\r\nAnd I want a function f as folow.\r\nf(embed_matrix, ids) \r\n= [\r\n [0, 0, 0] + [1, 1, 1],\r\n [1, 1, 1] + [2, 2, 2],\r\n [0, 0, 0] + [1, 1, 1] + [2, 2, 2],\r\n]\r\n= [\r\n[1,1,1],\r\n[3,3,3],\r\n[3,3,3],\r\n]", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 25412, "title": "Tensorflow Lite Multiple Choice Test example app", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.12\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI would like an example app where the user is presented with a multiple choice test, 4-5 questions. One of the questions should allow a numerical input with a decimal (float). The questions could be simple things like 2x+1=3... x=?\r\nThe example app would take these user inputs and classify the set of responses by a letter grade \"A\", \"B\", \"C\" or \"F\". After hitting a \"Submit\" button the app runs the model and displays a big letter grade on a new page. Then we give the user the option to re-do the test.\r\n\r\nI could contribute a trained model from a sample data set that I could generate from test questions/answers in the desktop version of Tensorflow.\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nMobile developers trying to get a handle on Tensorflow Lite. This would give a better starting point in Tensorflow Lite for passing other forms of inputs (not sensor based) through a pre-trained model. \r\n\r\n**Any Other info.**\r\nThis particular demo app would be more useful in its symbolic sense than the practical function.", "comments": ["This is a stale issue. Since the issue opened, there were lot of improvements in the TF lite guides and tutorials that can be used to accomplish your task. The following is the link to the TF Lite Guides\r\n\r\nhttps://www.tensorflow.org/lite/guide/\r\n\r\nI am closing this issue. Feel free to open a new issue if there is any specific bug or performance issue related TF lite. Thanks!"]}, {"number": 25411, "title": "Tensorflow: ImportError: DLL load failed: The specified procedure could not be found.", "body": "Hello,\r\nAm a newbie and just started learning about deep learning and in the process,\r\nI've installed Tensorflow on my Windows 10 laptop(cpu only no gpu) using Anaconda3 64bit and I have python3.6.2 installed on it. I verified the installation of Tensorflow at the command prompt by typing in for c:\\'activate tensorflow' and typed python at (tensorflow) C:\\python to get to python shell with >>> prompt and then I had print a  'Hello' string assigned to a variable and printed it by using Session() and it displayed  successfully.\r\n\r\nHowever, when I opened up my jupyter notebook from Anaconda and typed the below command  \r\nimport tensorflow as tf\r\ntf.__version__\r\nto verify if everything was good, it gave the below errors.\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-b6f7b46cc0dd> in <module>()\r\n----> 1 import tensorflow as tf\r\n      2 tf.__version__\r\nImportError: DLL load failed: The specified procedure could not be found.\r\n\r\nI checked on google but couldn't find this error..all I saw was errors related to 'specific module could not be founded and some other error issues related to specific module. But, what am getting is 'specific procedure could not be found.\r\n\r\nAppreciate if you can help me fix this burning issue..\r\nThank you!\r\n-----------------------------------------------------------------------------------------\r\nError Log:\r\nC:\\Users\\kshet\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\__init__.py in <module>()\r\n     22 \r\n     23 # pylint: disable=g-bad-import-order\r\n---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     25 \r\n     26 try:\r\n\r\nC:\\Users\\kshet\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\__init__.py in <module>()\r\n     57 \r\n     58 # Protocol buffers\r\n---> 59 from tensorflow.core.framework.graph_pb2 import *\r\n     60 from tensorflow.core.framework.node_def_pb2 import *\r\n     61 from tensorflow.core.framework.summary_pb2 import *\r\n\r\nC:\\Users\\kshet\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\core\\framework\\graph_pb2.py in <module>()\r\n      4 import sys\r\n      5 _b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode('latin1'))\r\n----> 6 from google.protobuf import descriptor as _descriptor\r\n      7 from google.protobuf import message as _message\r\n      8 from google.protobuf import reflection as _reflection\r\n\r\nC:\\Users\\kshet\\AppData\\Roaming\\Python\\Python36\\site-packages\\google\\protobuf\\descriptor.py in <module>()\r\n     45   import binascii\r\n     46   import os\r\n---> 47   from google.protobuf.pyext import _message\r\n     48   _USE_C_DESCRIPTORS = getattr(_message, '_USE_C_DESCRIPTORS', False)\r\n     49 \r\n\r\nImportError: DLL load failed: The specified procedure could not be found.\r\n", "comments": ["Please refer this [link](http://mathalope.co.uk/2017/05/31/how-to-import-tensorflow-from-python-console-jupyter-console-qtconsole-notebook-and-ipython-notebook/)  for  importing TF in Jupyter.\r\n", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 25410, "title": "Optimize wording in README.md for APIs and their backwards compatibility", "body": "", "comments": []}, {"number": 25409, "title": "complex64 and complex128 support for tf.decode_raw", "body": "Support `complex64` and `complex128` types for `tf.decode_raw`.\r\nFixes #25408", "comments": []}, {"number": 25408, "title": "[Feature Request] complex64 and complex128 support for tf.decode_raw", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): Latest (master branch)\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n`tf.decode_raw` does not support `complex64` and `complex128` yet, which causes issues when reading TFRecord files with complex data.\r\n\r\n**Will this change the current api? How?** \r\nLittle change needs to be made as the current algorithm is in a way type-agnostic: `REGISTER`ing `complex64` and `complex128` suffices.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone handling serialized `complex64` and `complex128` data.\r\n\r\n**Any Other info.**\r\nPR #25409", "comments": []}, {"number": 25407, "title": "Update the link for MKL optimized wheels", "body": "Please merge this only after `v1.13` packages with MKL have been published to GCS.\r\nI'll remove the `WIP` flag once that is the case.", "comments": ["Please ping me on that thread when this is ready. Thanks!", "Hi @drpngx this is ready to be merged and packages for `v1.13.1` have been uploaded.", "@drpngx and @rthadur,\r\nIs there anything else that need to be done on this PR?\r\nI'm not sure why it's sitting there in `pending` state to run `import/copybara` since `March 13th` \ud83e\udd14", "Thanks for pinging. It's in the internal approval queue, which I have pushed one step further."]}, {"number": 25406, "title": "error on  tf.estimator.train_and_evaluate", "body": "I used  tf.estimator.train_and_evaluate and I try to train my model in a amazon linux machine with multi-gpu, I try batch_size of evulating with 64,256 and 512 , in all cases,  I get the following error:\r\nCaused by op 'split_inputs/split_1', defined at:\r\n  File \"esmm_test_multi_gpu_256eval.py\", line 342, in <module>\r\n    tf.app.run(main=main)\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"esmm_test_multi_gpu_256eval.py\", line 322, in main\r\n    tf.estimator.train_and_evaluate(classifier, train_spec, eval_spec)\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\", line 471, in train_and_evaluate\r\n    return executor.run()\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\", line 610, in run\r\n    return self.run_local()\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\", line 711, in run_local\r\n    saving_listeners=saving_listeners)\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 354, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1207, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1241, in _train_model_default\r\n    saving_listeners)\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1471, in _train_with_estimator_spec\r\n    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 671, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1156, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1240, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1320, in run\r\n    run_metadata=run_metadata))\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 582, in after_run\r\n    if self._save(run_context.session, global_step):\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 607, in _save\r\n    if l.after_save(session, step):\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\", line 517, in after_save\r\n    self._evaluate(global_step_value)  # updates self.eval_result\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\", line 537, in _evaluate\r\n    self._evaluator.evaluate_and_export())\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\", line 912, in evaluate_and_export\r\n    hooks=self._eval_spec.hooks)\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 478, in evaluate\r\n    return _evaluate()\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 460, in _evaluate\r\n    self._evaluate_build_graph(input_fn, hooks, checkpoint_path))\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1484, in _evaluate_build_graph\r\n    self._call_model_fn_eval(input_fn, self.config))\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1520, in _call_model_fn_eval\r\n    features, labels, model_fn_lib.ModeKeys.EVAL, config)\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1195, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/estimator/python/estimator/replicate_model_fn.py\", line 230, in replicated_model_fn\r\n    features, labels, len(devices), device=consolidation_device)\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/estimator/python/estimator/replicate_model_fn.py\", line 501, in _split_batch\r\n    label_shards = split_dictionary(labels)\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/estimator/python/estimator/replicate_model_fn.py\", line 486, in split_dictionary\r\n    for i, shard in enumerate(array_ops.split(tensor, number_of_shards)):\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1327, in split\r\n    axis=axis, num_split=num_or_size_splits, value=value, name=name)\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 8083, in split\r\n    \"Split\", split_dim=axis, value=value, num_split=num_split, name=name)\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\r\n    op_def=op_def)\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Number of ways to split should evenly divide the split dimension, but got split_dim 0 (size = 145) and num_split 8\r\n\t [[node split_inputs/split_1 (defined at /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/estimator/python/estimator/replicate_model_fn.py:486)  = Split[T=DT_FLOAT, num_split=8, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](split_inputs/split/split_dim, IteratorGetNext:24)]]\r\n\t [[{{node tower_1/my_model/ctr_model/input_layer/CategoryID_embedding/CategoryID_embedding_weights/embedding_lookup_sparse/_5309}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_10102...kup_sparse\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"]()]]", "comments": ["In order to expedite the troubleshooting process, please provide a minimal code snippet to reproduce the issue reported here. Thanks!", "from __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow import feature_column as fc\r\n\r\n\r\nflags = tf.app.flags\r\nflags.DEFINE_string(\"model_dir\", \"./model_dir\", \"Base directory for the model.\")\r\nflags.DEFINE_string(\"output_model\", \"./model_output\", \"Path to the training data.\")\r\nflags.DEFINE_string(\"train_data\", \"/home/ec2-user/zhangpu/cvr/yang_xudong_script/input/yang-train-all.tfrecords\",\r\n                    \"Directory for storing mnist data\")\r\nflags.DEFINE_string(\"eval_data\", \"/home/ec2-user/zhangpu/cvr/yang_xudong_script/input/yang-test-all.tfrecords\",\r\n                    \"Path to the evaluation data.\")\r\nflags.DEFINE_string(\"hidden_units\", \"200,80,20\",\r\n                    \"Comma-separated list of number of units in each hidden layer of the NN\")\r\nflags.DEFINE_integer(\"train_steps\", 50000,\r\n                     \"Number of (global) training steps to perform\")\r\nflags.DEFINE_integer(\"batch_size\", 512, \"Training batch size\")\r\nflags.DEFINE_integer(\"eval_batch_size\", 512, \"evaluate batch size\")\r\nflags.DEFINE_integer(\"shuffle_buffer_size\", 10000, \"dataset shuffle buffer size\")\r\nflags.DEFINE_float(\"learning_rate\", 0.01, \"Learning rate\")\r\nflags.DEFINE_float(\"dropout_rate\", 0.25, \"Drop out rate\")\r\nflags.DEFINE_integer(\"num_parallel_readers\", 28, \"number of parallel readers for training data\")\r\nflags.DEFINE_integer(\"save_checkpoints_steps\", 5000, \"Save checkpoints every this many steps\")\r\nflags.DEFINE_string(\"ps_hosts\", \"s-xiasha-10-2-176-43.hx:2222\",\r\n                    \"Comma-separated list of hostname:port pairs\")\r\nflags.DEFINE_string(\"worker_hosts\", \"s-xiasha-10-2-176-42.hx:2223,s-xiasha-10-2-176-44.hx:2224\",\r\n                    \"Comma-separated list of hostname:port pairs\")\r\nflags.DEFINE_string(\"job_name\", \"worker\", \"job name: worker or ps\")\r\nflags.DEFINE_integer(\"task_index\", 0,\r\n                     \"Worker task index, should be >= 0. task_index=0 is \"\r\n                     \"the master worker task the performs the variable \"\r\n                     \"initialization \")\r\nflags.DEFINE_boolean(\"run_on_cluster\", False, \"Whether the cluster info need to be passed in as input\")\r\n\r\nFLAGS = flags.FLAGS\r\nmy_feature_columns = []\r\n\r\n\r\ndef create_feature_columns():\r\n    embed_dim = 12\r\n    UserID = fc.categorical_column_with_hash_bucket(\"UserID\", 11176, dtype=tf.int64)\r\n    UserID_embeding = fc.embedding_column(UserID, embed_dim, combiner='sum')\r\n    ItemID = fc.categorical_column_with_hash_bucket(\"ItemID\", 10240, dtype=tf.int64)\r\n    ItemID_embeding = fc.embedding_column(ItemID, embed_dim, combiner='sum')\r\n    User_Cluster = fc.categorical_column_with_hash_bucket(\"User_Cluster\", 90, dtype=tf.int64)\r\n    User_Cluster_embeding = fc.embedding_column(User_Cluster, embed_dim, combiner='sum')\r\n    CategoryID = fc.categorical_column_with_hash_bucket(\"CategoryID\", 6111, dtype=tf.int64)\r\n    CategoryID_embeding = fc.embedding_column(CategoryID, embed_dim, combiner='sum')\r\n    ShopID = fc.categorical_column_with_hash_bucket(\"ShopID\", 10240, dtype=tf.int64)\r\n    ShopID_embeding = fc.embedding_column(ShopID, embed_dim, combiner='sum')\r\n    BrandID = fc.categorical_column_with_hash_bucket(\"BrandID\", 10240, dtype=tf.int64)\r\n    BrandID_embeding = fc.embedding_column(BrandID, embed_dim, combiner='sum')\r\n    Com_CateID = fc.categorical_column_with_hash_bucket(\"Com_CateID\", 4695, dtype=tf.int64)\r\n    Com_CateID_embeding = fc.embedding_column(Com_CateID, embed_dim, combiner='sum')\r\n    Com_ShopID = fc.categorical_column_with_hash_bucket(\"Com_ShopID\", 10240, dtype=tf.int64)\r\n    Com_ShopID_embeding = fc.embedding_column(Com_ShopID, embed_dim, combiner='sum')\r\n    Com_BrandID = fc.categorical_column_with_hash_bucket(\"Com_BrandID\", 10240, dtype=tf.int64)\r\n    Com_BrandID_embeding = fc.embedding_column(Com_BrandID, embed_dim, combiner='sum')\r\n    PID = fc.indicator_column(\r\n        fc.categorical_column_with_vocabulary_list(\"PID\", [0, 1, 2], dtype=tf.int64, default_value=-1))\r\n    User_CateIDs = fc.categorical_column_with_hash_bucket(\"User_CateIDs\", 8660, dtype=tf.int64)\r\n    User_CateIDs_embeding = fc.embedding_column(User_CateIDs, embed_dim, combiner='sum')\r\n    User_BrandIDs = fc.categorical_column_with_hash_bucket(\"User_BrandIDs\", 10240, dtype=tf.int64)\r\n    User_BrandIDs_embeding = fc.embedding_column(User_BrandIDs, embed_dim, combiner='sum')\r\n    NodeID = fc.categorical_column_with_hash_bucket(\"NodeID\", 10240, dtype=tf.int64)\r\n    NodeID_embeding = fc.embedding_column(NodeID, embed_dim, combiner='sum')\r\n    Com_NodeID = fc.categorical_column_with_hash_bucket(\"Com_NodeID\", 10240, dtype=tf.int64)\r\n    Com_NodeID_embeding = fc.embedding_column(Com_NodeID, embed_dim, combiner='sum')\r\n    User_ShopIDs = fc.categorical_column_with_hash_bucket(\"User_ShopIDs\", 10240, dtype=tf.int64)\r\n    User_ShopIDs_embeding = fc.embedding_column(User_ShopIDs, embed_dim, combiner='sum')\r\n    User_NodeIDs = fc.categorical_column_with_hash_bucket(\"User_NodeIDs\", 10240, dtype=tf.int64)\r\n    User_NodeIDs_embeding = fc.embedding_column(User_NodeIDs, embed_dim, combiner='sum')\r\n    User_ClusterID = fc.indicator_column(\r\n        fc.categorical_column_with_vocabulary_list(\"User_ClusterID\", list(range(14)), dtype=tf.int64, default_value=-1))\r\n    User_Gender = fc.indicator_column(\r\n        fc.categorical_column_with_vocabulary_list(\"User_Gender\", list(range(3)), dtype=tf.int64, default_value=-1))\r\n    User_Age = fc.indicator_column(\r\n        fc.categorical_column_with_vocabulary_list(\"User_Age\", list(range(8)), dtype=tf.int64, default_value=-1))\r\n    User_Level1 = fc.indicator_column(\r\n        fc.categorical_column_with_vocabulary_list(\"User_Level1\", list(range(4)), dtype=tf.int64, default_value=-1))\r\n    User_Level2 = fc.indicator_column(\r\n        fc.categorical_column_with_vocabulary_list(\"User_Level2\", list(range(4)), dtype=tf.int64, default_value=-1))\r\n    User_Occupation = fc.indicator_column(\r\n        fc.categorical_column_with_vocabulary_list(\"User_Occupation\", list(range(3)), dtype=tf.int64, default_value=-1))\r\n    User_Geo = fc.indicator_column(\r\n        fc.categorical_column_with_vocabulary_list(\"User_Geo\", list(range(5)), dtype=tf.int64, default_value=-1))\r\n    global my_feature_columns\r\n    my_feature_columns = [UserID_embeding, ItemID_embeding, User_Cluster_embeding, CategoryID_embeding, ShopID_embeding, BrandID_embeding,\r\n                          Com_CateID_embeding, Com_ShopID_embeding,Com_BrandID_embeding, PID, User_CateIDs_embeding, User_BrandIDs_embeding,\r\n                          NodeID_embeding, Com_NodeID_embeding, User_ShopIDs_embeding,User_NodeIDs_embeding,User_ClusterID,User_Gender,\r\n                          User_Age,User_Level1,User_Level2,User_Occupation,User_Geo]\r\n    return my_feature_columns\r\n\r\ndef parse_exmp(serial_exmp):\r\n    click = fc.numeric_column(\"click\", default_value=0, dtype=tf.int64)\r\n    pay = fc.numeric_column(\"pay\", default_value=0, dtype=tf.int64)\r\n    fea_columns = [click, pay]\r\n    fea_columns += my_feature_columns\r\n    feature_spec = tf.feature_column.make_parse_example_spec(fea_columns)\r\n    feats = tf.parse_single_example(serial_exmp, features=feature_spec)\r\n    click = feats.pop('click')\r\n    pay = feats.pop('pay')\r\n    return feats, {'ctr': tf.to_float(click), 'cvr': tf.to_float(pay)}\r\n\r\ndef train_input_fn(filenames, batch_size, shuffle_buffer_size):\r\n    files = tf.data.Dataset.list_files(filenames)\r\n    dataset = files.apply(\r\n        tf.contrib.data.parallel_interleave(tf.data.TFRecordDataset, cycle_length=FLAGS.num_parallel_readers))\r\n    if shuffle_buffer_size > 0:\r\n        dataset = dataset.shuffle(shuffle_buffer_size)\r\n    dataset = dataset.map(parse_exmp, num_parallel_calls=28)\r\n    dataset = dataset.repeat().batch(batch_size).prefetch(1)\r\n    return dataset\r\n\r\ndef eval_input_fn(filename, batch_size):\r\n    dataset = tf.data.TFRecordDataset(filename)\r\n    dataset = dataset.map(parse_exmp, num_parallel_calls=28)\r\n    dataset = dataset.batch(FLAGS.eval_batch_size)\r\n    return dataset\r\n\r\ndef build_mode(features, mode, params):\r\n    net = fc.input_layer(features, params['feature_columns'])\r\n    for units in params['hidden_units']:\r\n        net = tf.layers.dense(net, units=units, activation=tf.nn.relu)\r\n        if 'dropout_rate' in params and params['dropout_rate'] > 0.0:\r\n            net = tf.layers.dropout(net, params['dropout_rate'], training=(mode == tf.estimator.ModeKeys.TRAIN))\r\n    logits = tf.layers.dense(net, 1, activation=None)\r\n    return logits\r\n\r\ndef my_model(features, labels, mode, params):\r\n    with tf.variable_scope('my_model', reuse=tf.AUTO_REUSE):\r\n        with tf.variable_scope('ctr_model'):\r\n            ctr_logits = build_mode(features, mode, params)\r\n        with tf.variable_scope('cvr_model'):\r\n            cvr_logits = build_mode(features, mode, params)\r\n        ctr_predictions = tf.sigmoid(ctr_logits, name=\"CTR\")\r\n        cvr_predictions = tf.sigmoid(cvr_logits, name=\"CVR\")\r\n        prop = tf.multiply(ctr_predictions, cvr_predictions, name=\"CTCVR\")\r\n        if mode == tf.estimator.ModeKeys.PREDICT:\r\n            predictions = {\r\n                'probabilities': prop,\r\n                'ctr_probabilities': ctr_predictions,\r\n                'cvr_probabilities': cvr_predictions\r\n            }\r\n            export_outputs = {\r\n                'prediction': tf.estimator.export.PredictOutput(predictions)\r\n            }\r\n            return tf.estimator.EstimatorSpec(mode, predictions=predictions, export_outputs=export_outputs)\r\n        y = labels['cvr']\r\n        cvr_loss = tf.reduce_sum(tf.keras.backend.binary_crossentropy(y, prop), name=\"cvr_loss\")\r\n        ctr_loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels['ctr'], logits=ctr_logits), name=\"ctr_loss\")\r\n        loss = tf.add(ctr_loss, cvr_loss, name=\"ctcvr_loss\")\r\n        ctr_accuracy = tf.metrics.accuracy(labels=labels['ctr'],\r\n                                           predictions=tf.to_float(tf.greater_equal(ctr_predictions, 0.5)))\r\n        cvr_accuracy = tf.metrics.accuracy(labels=y, predictions=tf.to_float(tf.greater_equal(prop, 0.5)))\r\n        ctr_auc = tf.metrics.auc(labels['ctr'], ctr_predictions)\r\n        cvr_auc = tf.metrics.auc(y, prop)\r\n        metrics = {'cvr_accuracy': cvr_accuracy, 'ctr_accuracy': ctr_accuracy, 'ctr_auc': ctr_auc, 'cvr_auc': cvr_auc}\r\n        tf.summary.scalar('ctr_accuracy', ctr_accuracy[1])\r\n        tf.summary.scalar('cvr_accuracy', cvr_accuracy[1])\r\n        tf.summary.scalar('ctr_auc', ctr_auc[1])\r\n        tf.summary.scalar('cvr_auc', cvr_auc[1])\r\n        if mode == tf.estimator.ModeKeys.EVAL:\r\n            return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\r\n        assert mode == tf.estimator.ModeKeys.TRAIN\r\n        optimizer = tf.train.AdagradOptimizer(learning_rate=params['learning_rate'])\r\n        optimizer = tf.contrib.estimator.TowerOptimizer(optimizer)\r\n        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\r\n        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\r\n\r\n\r\ndef main(unused_argv):\r\n    create_feature_columns()\r\n    classifier = tf.estimator.Estimator(\r\n        model_fn=tf.contrib.estimator.replicate_model_fn(my_model),\r\n        params={\r\n            'feature_columns': my_feature_columns,\r\n            'hidden_units': FLAGS.hidden_units.split(','),\r\n            'learning_rate': FLAGS.learning_rate,\r\n            'dropout_rate': FLAGS.dropout_rate\r\n        },\r\n        config=tf.estimator.RunConfig(model_dir=FLAGS.model_dir, save_checkpoints_steps=FLAGS.save_checkpoints_steps)\r\n    )\r\n    batch_size = FLAGS.batch_size\r\n    print(\"train steps:\", FLAGS.train_steps, \"batch_size:\", batch_size)\r\n    if isinstance(FLAGS.train_data, str) and os.path.isdir(FLAGS.train_data):\r\n        train_files = [FLAGS.train_data + '/' + x for x in os.listdir(FLAGS.train_data)] if os.path.isdir(\r\n            FLAGS.train_data) else FLAGS.train_data\r\n    else:\r\n        train_files = FLAGS.train_data\r\n    if isinstance(FLAGS.eval_data, str) and os.path.isdir(FLAGS.eval_data):\r\n        eval_files = [FLAGS.eval_data + '/' + x for x in os.listdir(FLAGS.eval_data)] if os.path.isdir(\r\n            FLAGS.eval_data) else FLAGS.eval_data\r\n    else:\r\n        eval_files = FLAGS.eval_data\r\n    shuffle_buffer_size = FLAGS.shuffle_buffer_size\r\n    train_spec = tf.estimator.TrainSpec(\r\n        input_fn=lambda: train_input_fn(train_files, batch_size, shuffle_buffer_size),\r\n        max_steps=FLAGS.train_steps\r\n    )\r\n    input_fn_for_eval = lambda: eval_input_fn(eval_files, batch_size)\r\n    eval_spec = tf.estimator.EvalSpec(input_fn=input_fn_for_eval, throttle_secs=600, steps=None)\r\n    tf.estimator.train_and_evaluate(classifier, train_spec, eval_spec)\r\n    results = classifier.evaluate(input_fn=input_fn_for_eval)\r\n    for key in sorted(results): print('%s: %s' % (key, results[key]))\r\n    if FLAGS.job_name == \"worker\" and FLAGS.task_index == 0:\r\n        feature_spec = tf.feature_column.make_parse_example_spec(my_feature_columns)\r\n        serving_input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)\r\n        classifier.export_savedmodel(FLAGS.output_model, serving_input_receiver_fn)\r\n    print(\"quit main\")\r\n\r\nif __name__ == \"__main__\":\r\n    tf.app.run(main=main)\r\n", "I tried \"tf.contrib.distribute.MirroredStrategy\" and it works."]}, {"number": 25405, "title": "FP16 profermance on GTX1080", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows10\r\n- TensorFlow installed from (source or binary):\r\nBinary(PIP install)\r\n- TensorFlow version (use command below):\r\n1.12.0\r\n- Python version:\r\n3.6.2\r\n- CUDA/cuDNN version:\r\nCUDA 9.0 / cuDNN 7.3.1\r\n- GPU model and memory:\r\nGeForce GTX 1080, 8GB\r\n\r\n**Describe the current behavior**\r\nI try to evaluate the accuracy of fp16 precision on GTX 1080 GPU. As we known, GTX1080 doesn't have\r\ntensor core to supporting FP16 calc, but my test reuslt shows that inference with fp16 is faster than fp32 by 20%-30%, could you give an explanation? \r\n\r\nHere is my operation step:\r\n1. train a model using fp32 precision and save the checkpoint.\r\n2. in order to inference with fp16, load the checkpoint and convert each param to fp16 type\r\n```\r\n    reader = tf.train.NewCheckpointReader(model_file)\r\n    var_to_map = reader.get_variable_to_shape_map()\r\n\r\n    val_f16 = {}\r\n    for key, val in var_to_map.items():\r\n        val_f16[key] = reader.get_tensor(key).astype(np.float16)\r\n```\r\n3. rewrite the network and pass the fp16 params to initialize each layer, then run the network.\r\n```\r\n    weight_name = scope_name + '/' + get_layer_str() + '/' + 'weight'\r\n    initw = inits[weight_name]\r\n    weight = tf.get_variable('weight', dtype=initw.dtype, initializer=initw)\r\n    out = tf.nn.conv2d(self.get_output(), weight, strides=[1, stride, stride, 1], padding='SAME')\r\n```\r\n\r\nI got a approximate accuracy from fp16 reuslts, and its faster than fp32 by 20%-30%. In order to verify the speed of fp16, I construct a simple net with several dense layers, which are initialzed with fp16 random values, and run the forward procedure only. But fp16 is slower than fp32. ...\r\n\r\n**Question**\r\n1. Why FP16 is faster on a Pascal GTX GPU, what hardware units were used?\r\n2. Is there any better way to inference with fp16 using a pretrained fp32 model?\r\n3. If the TensorRT is the normal method to inference with fp16?\r\n4. If using RTX 2080 with tensor core, we will get a dramatic performance in fp16?\r\n", "comments": ["I am closing the issue as it a support question. In future, please post this kind of support questions at Stackoverflow. Thanks!"]}, {"number": 25404, "title": "Improves docstring for tf.contrib.nn.conv1d_transpose", "body": "This was mainly prompted by noting that the documented valid `data_format` options would raise an exception.", "comments": []}, {"number": 25403, "title": "Eager Execution error: Blas GEMM launch failed", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code: no\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): pip3 install tensorflow-gpu\r\n- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0\r\n- Python version: 3.5.2\r\n- CUDA/cuDNN version: CUDA 9.0, cudnn 7.4.2\r\n- GPU model and memory:  GeForce RTX 2080 Ti\r\n\r\n**Describe the current behavior**\r\nCrashes with error \"Blas GEMM launch failed\"\r\n\r\n**Describe the expected behavior**\r\nCorrectly print the matmul result\r\n\r\n**Code to reproduce the issue**\r\nI was trying to use eager execution. I tried the following simple code\r\n```python\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\nprint(tf.matmul([[1., 2.],[3., 4.]], [[1., 2.],[3., 4.]]))\r\n```\r\nOther eager mode code under at https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples fails with the same error.\r\n\r\nHowever, non eager mode code can correctly run.\r\n\r\n**Other info / logs**\r\noutput\r\n```\r\n2019-01-31 17:00:20.744826: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2019-01-31 17:00:21.150735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:\r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545\r\npciBusID: 0000:17:00.0\r\ntotalMemory: 10.73GiB freeMemory: 9.36GiB\r\n2019-01-31 17:00:21.399702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 1 with properties:\r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545\r\npciBusID: 0000:65:00.0\r\ntotalMemory: 10.73GiB freeMemory: 10.53GiB\r\n2019-01-31 17:00:21.399746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1\r\n2019-01-31 17:00:21.906842: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-01-31 17:00:21.906877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1\r\n2019-01-31 17:00:21.906882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y\r\n2019-01-31 17:00:21.906886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N\r\n2019-01-31 17:00:21.907143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9026 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:17:00.0, compute capability: 7.5)\r\n2019-01-31 17:00:21.907488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10167 MB memory) -> physical GPU (device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:65:00.0, compute capability: 7.5)\r\n2019-01-31 17:00:22.144957: E tensorflow/stream_executor/cuda/cuda_blas.cc:652] failed to run cuBLAS routine cublasSgemm_v2: CUBLAS_STATUS_EXECUTION_FAILED\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 5, in <module>\r\n    print(tf.matmul([[1., 2.],[3., 4.]], [[1., 2.],[3., 4.]]))\r\n  File \"/home/weixu/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 2057, in matmul\r\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\r\n  File \"/home/weixu/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 4586, in mat_mul\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(2, 2), b.shape=(2, 2), m=2, n=2, k=2 [Op:MatMul] name: MatMul/\r\n```\r\n", "comments": ["Can you please kill all running notebooks which utilize your GPU. Then restart the kernel and execute the code again?", "Thanks for finally having someone looking at this issue. I don't have any notebooks running. There are several ubuntu processes using the GPU 0. But even with CUDA_VISIBLE_DEVICES=1, I got the same error. I need to note that non-eager mode training program can run correctly though.", "I am having the same issue. Is there a fix to this?", "This error is a cuda error because the GPU is running out of memory, doesn't have enough compute capacity, or there's a driver issue.\r\n\r\nCan you confirm that none of these issues is happening with you here?", "I am almost a hundred percent sure the GPU truly running out of memory. It doesn't even run for one epoch. \r\n\r\nThe same code, same dataset, same everything, runs fine in 1080ti, the GPUs of collab, 1080, 1070. \r\n\r\nMy feeling tells me something is wrong with CUDA 9.0 on 2080 ti or eager on 2080ti with CUDA 9.0. ", "It should\u2019t run out of memory with the sample code I gave. The strange thing is that non-eager mode or pytorch runs without any issue. So it\u2019s likely something wrong with eager ", "Can you inspect the memory usage with nvidia-smi? If you have any other\nprocesses using the GPU tf eager might fail to allocate its memory.\n\nOn Tue, Feb 19, 2019 at 11:22 PM emailweixu <notifications@github.com>\nwrote:\n\n> It should\u2019t run out of memory with the sample code I gave. The strange\n> thing is that non-eager mode or pytorch runs without any issue. So it\u2019s\n> likely something wrong with eager\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/25403#issuecomment-465452482>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxVCh1bW0LN0ip540oe_6nJAiSY4Vks5vPPedgaJpZM4addj6>\n> .\n>\n\n\n-- \n - Alex\n", "@emailweixu I noticed the same thing. But this issue is only with the 2080 GPUs. If you use any other GPU it works fine\r\n@alextp I applied nvidia-smi. There isn't any other processes using the GPU.", "@azaks2 do you know someone with more low-level GPU work to investigate this? It makes no sense to me because eager doesn't treat GPUs any different than graph mode does.", "NVIDIA suggests to use CUDA 10 for RTX 2080 instead of earlier versions of CUDA.\r\nhttps://devtalk.nvidia.com/default/topic/1042920/which-version-of-cuda-can-work-with-rtx-2080/ \r\n\r\nTensorFlow now supports CUDA 10. See https://github.com/tensorflow/tensorflow/issues/22706#issuecomment-461120061\r\n\r\nCould you upgrade to CUDA 10 and give it a try? Let us know if it still does not work. Also, include nvidia-smi command output which shows the driver version in the system.", "Closing this issue. Let us know if it is reproducible with CUDA 10 as well.", "Issue solved with tf-nightly-gpu and CUDA 10", "> hundred percent sure the GPU truly running out of memory. It doesn't even run for one epoch.\r\n> \r\n> The same code, same dataset, same everything, runs fine in 1080ti, the GPUs of collab, 1080, 1070.\r\n\r\nI have the same problem. Fine with 1080 or 1070, wrong with 2080. What's wrong? Have you fixed this problem?", "> Closing this issue. Let us know if it is reproducible with CUDA 10 as well.\r\n\r\nIt does not work with CUDA 10.0.130. ", "The following what I found,\r\n\r\n- IF you have RTX 20 series (ex. 2070 or 2080), you need to use CUDA 10.0 with CuDNN 7.5 and TF > 1.12. \r\n- IF you use CUDA 10.1, you must use CuDNN 7.6\r\n- IF you have RTX 20 series and still wants to use CUDA 9.0 and CuDNN 7.5, you must use TF <= 1.12\r\n\r\nAlso, IF you have RTX 20 series and CUDA 10 you must put this in your code\r\n```\r\ntf_config = tf.ConfigProto()\r\ntf_config.gpu_options.allow_growth = True\r\ntf_config.gpu_options.per_process_gpu_memory_fraction = 0.9\r\ntf_config.allow_soft_placement = True\r\n```", ">  solved with tf-nightly-gpu and CUDA 10\r\n\r\nHi,I have meet this problem.my cuda is 9.0,cudnn is 7.1,tensorflow-gpu is 1.9.can you provide the guidance how do i  solver problem?\r\nInternalError (see above for traceback): Blas xGEMM launch failed ", "> The following what I found,\r\n> \r\n>     * IF you have RTX 20 series (ex. 2070 or 2080), you need to use CUDA 10.0 with CuDNN 7.5 and TF > 1.12.\r\n> \r\n>     * IF you use CUDA 10.1, you must use CuDNN 7.6\r\n> \r\n>     * IF you have RTX 20 series and still wants to use CUDA 9.0 and CuDNN 7.5, you must use TF <= 1.12\r\n> \r\n> \r\n> Also, IF you have RTX 20 series and CUDA 10 you must put this in your code\r\n> \r\n> ```\r\n> tf_config = tf.ConfigProto()\r\n> tf_config.gpu_options.allow_growth = True\r\n> tf_config.gpu_options.per_process_gpu_memory_fraction = 0.9\r\n> tf_config.allow_soft_placement = True\r\n> ```\r\n\r\nHi,I have meet this problem.my cuda is 9.0,cudnn is 7.1,tensorflow-gpu is 1.9.can you provide the guidance how do i  solver problem?\r\nInternalError (see above for traceback): Blas xGEMM launch failed ", "What is your GPU?\n\nOn Fri, Oct 18, 2019, 10:23 PM longma <notifications@github.com> wrote:\n\n> The following what I found,\n>\n> * IF you have RTX 20 series (ex. 2070 or 2080), you need to use CUDA 10.0 with CuDNN 7.5 and TF > 1.12.\n>\n> * IF you use CUDA 10.1, you must use CuDNN 7.6\n>\n> * IF you have RTX 20 series and still wants to use CUDA 9.0 and CuDNN 7.5, you must use TF <= 1.12\n>\n> Also, IF you have RTX 20 series and CUDA 10 you must put this in your code\n>\n> tf_config = tf.ConfigProto()\n> tf_config.gpu_options.allow_growth = True\n> tf_config.gpu_options.per_process_gpu_memory_fraction = 0.9\n> tf_config.allow_soft_placement = True\n>\n> Hi,I have meet this problem.my cuda is 9.0,cudnn is 7.1,tensorflow-gpu is\n> 1.9.can you provide the guidance how do i solver problem?\n> InternalError (see above for traceback): Blas xGEMM launch failed\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/25403?email_source=notifications&email_token=AH7VFK5WND62UTTXHECUZT3QPJVSVA5CNFSM4GTV3D5KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEBWZY7A#issuecomment-544054396>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AH7VFK3XW55KQ7OIQJLBLS3QPJVSVANCNFSM4GTV3D5A>\n> .\n>\n", "my gpu is rtx 2080(8g).Do it has enough memory?\r\n\r\n\r\n\r\n\r\n------------------ \u539f\u59cb\u90ae\u4ef6 ------------------\r\n\u53d1\u4ef6\u4eba: \"Rony Kalfarisi\"<notifications@github.com>;\r\n\u53d1\u9001\u65f6\u95f4: 2019\u5e7410\u670819\u65e5(\u661f\u671f\u516d) \u4e0a\u534810:49\r\n\u6536\u4ef6\u4eba: \"tensorflow/tensorflow\"<tensorflow@noreply.github.com>;\r\n\u6284\u9001: \"longma\"<1442342449@qq.com>;\"Comment\"<comment@noreply.github.com>;\r\n\u4e3b\u9898: Re: [tensorflow/tensorflow] Eager Execution error: Blas GEMM launchfailed (#25403)\r\n\r\n\r\n\r\nWhat is your GPU?\r\n \r\n On Fri, Oct 18, 2019, 10:23 PM longma <notifications@github.com> wrote:\r\n \r\n > The following what I found,\r\n >\r\n > * IF you have RTX 20 series (ex. 2070 or 2080), you need to use CUDA 10.0 with CuDNN 7.5 and TF > 1.12.\r\n >\r\n > * IF you use CUDA 10.1, you must use CuDNN 7.6\r\n >\r\n > * IF you have RTX 20 series and still wants to use CUDA 9.0 and CuDNN 7.5, you must use TF <= 1.12\r\n >\r\n > Also, IF you have RTX 20 series and CUDA 10 you must put this in your code\r\n >\r\n > tf_config = tf.ConfigProto()\r\n > tf_config.gpu_options.allow_growth = True\r\n > tf_config.gpu_options.per_process_gpu_memory_fraction = 0.9\r\n > tf_config.allow_soft_placement = True\r\n >\r\n > Hi,I have meet this problem.my cuda is 9.0,cudnn is 7.1,tensorflow-gpu is\r\n > 1.9.can you provide the guidance how do i solver problem?\r\n > InternalError (see above for traceback): Blas xGEMM launch failed\r\n >\r\n > \u2014\r\n > You are receiving this because you commented.\r\n > Reply to this email directly, view it on GitHub\r\n > <https://github.com/tensorflow/tensorflow/issues/25403?email_source=notifications&email_token=AH7VFK5WND62UTTXHECUZT3QPJVSVA5CNFSM4GTV3D5KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEBWZY7A#issuecomment-544054396>,\r\n > or unsubscribe\r\n > <https://github.com/notifications/unsubscribe-auth/AH7VFK3XW55KQ7OIQJLBLS3QPJVSVANCNFSM4GTV3D5A>\r\n > .\r\n >\r\n \r\n\u2014\r\nYou are receiving this because you commented.\r\nReply to this email directly, view it on GitHub, or unsubscribe.", "If it's out of memory error it will say that, so I believe you should be\nfine. Since you have RTX 2080, I'd suggest you use CUDA 10 with CuDNN 7.5.\nNvidia released CUDA 10 specifically for RTX series. If you still wanna use\nCUDA 9.0, I'd suggest use CuDNN 7.5\n\nOn Fri, Oct 18, 2019, 10:59 PM longma <notifications@github.com> wrote:\n\n> my gpu is rtx 2080(8g).Do it has enough memory?\n>\n>\n>\n>\n> ------------------ \u539f\u59cb\u90ae\u4ef6 ------------------\n> \u53d1\u4ef6\u4eba: \"Rony Kalfarisi\"<notifications@github.com>;\n> \u53d1\u9001\u65f6\u95f4: 2019\u5e7410\u670819\u65e5(\u661f\u671f\u516d) \u4e0a\u534810:49\n> \u6536\u4ef6\u4eba: \"tensorflow/tensorflow\"<tensorflow@noreply.github.com>;\n> \u6284\u9001: \"longma\"<1442342449@qq.com>;\"Comment\"<comment@noreply.github.com>;\n> \u4e3b\u9898: Re: [tensorflow/tensorflow] Eager Execution error: Blas GEMM\n> launchfailed (#25403)\n>\n>\n>\n> What is your GPU?\n>\n> On Fri, Oct 18, 2019, 10:23 PM longma <notifications@github.com> wrote:\n>\n> > The following what I found,\n> >\n> > * IF you have RTX 20 series (ex. 2070 or 2080), you need to use CUDA\n> 10.0 with CuDNN 7.5 and TF > 1.12.\n> >\n> > * IF you use CUDA 10.1, you must use CuDNN 7.6\n> >\n> > * IF you have RTX 20 series and still wants to use CUDA 9.0 and CuDNN\n> 7.5, you must use TF <= 1.12\n> >\n> > Also, IF you have RTX 20 series and CUDA 10 you must put this in your\n> code\n> >\n> > tf_config = tf.ConfigProto()\n> > tf_config.gpu_options.allow_growth = True\n> > tf_config.gpu_options.per_process_gpu_memory_fraction = 0.9\n> > tf_config.allow_soft_placement = True\n> >\n> > Hi,I have meet this problem.my cuda is 9.0,cudnn is 7.1,tensorflow-gpu\n> is\n> > 1.9.can you provide the guidance how do i solver problem?\n> > InternalError (see above for traceback): Blas xGEMM launch failed\n> >\n> > \u2014\n> > You are receiving this because you commented.\n> > Reply to this email directly, view it on GitHub\n> > <\n> https://github.com/tensorflow/tensorflow/issues/25403?email_source=notifications&email_token=AH7VFK5WND62UTTXHECUZT3QPJVSVA5CNFSM4GTV3D5KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEBWZY7A#issuecomment-544054396>,\n>\n> > or unsubscribe\n> > <\n> https://github.com/notifications/unsubscribe-auth/AH7VFK3XW55KQ7OIQJLBLS3QPJVSVANCNFSM4GTV3D5A>\n>\n> > .\n> >\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/25403?email_source=notifications&email_token=AH7VFKYQXPZ6TV7O6D4KPQDQPJZ2LA5CNFSM4GTV3D5KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEBW4OKA#issuecomment-544065320>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AH7VFK7XJ6OFNGP3MPT2HOTQPJZ2LANCNFSM4GTV3D5A>\n> .\n>\n", "Than your for your reply,I will try to change it according to your advice right now.\r\n\r\n\r\n\r\n\r\n------------------ \u539f\u59cb\u90ae\u4ef6 ------------------\r\n\u53d1\u4ef6\u4eba: \"Rony Kalfarisi\"<notifications@github.com>;\r\n\u53d1\u9001\u65f6\u95f4: 2019\u5e7410\u670819\u65e5(\u661f\u671f\u516d) \u4e2d\u534811:18\r\n\u6536\u4ef6\u4eba: \"tensorflow/tensorflow\"<tensorflow@noreply.github.com>;\r\n\u6284\u9001: \"longma\"<1442342449@qq.com>;\"Comment\"<comment@noreply.github.com>;\r\n\u4e3b\u9898: Re: [tensorflow/tensorflow] Eager Execution error: Blas GEMM launchfailed (#25403)\r\n\r\n\r\n\r\nIf it's out of memory error it will say that, so I believe you should be\r\n fine. Since you have RTX 2080, I'd suggest you use CUDA 10 with CuDNN 7.5.\r\n Nvidia released CUDA 10 specifically for RTX series. If you still wanna use\r\n CUDA 9.0, I'd suggest use CuDNN 7.5\r\n \r\n On Fri, Oct 18, 2019, 10:59 PM longma <notifications@github.com> wrote:\r\n \r\n > my gpu is rtx 2080(8g).Do it has enough memory?\r\n >\r\n >\r\n >\r\n >\r\n > ------------------ \u539f\u59cb\u90ae\u4ef6 ------------------\r\n > \u53d1\u4ef6\u4eba: \"Rony Kalfarisi\"<notifications@github.com>;\r\n > \u53d1\u9001\u65f6\u95f4: 2019\u5e7410\u670819\u65e5(\u661f\u671f\u516d) \u4e0a\u534810:49\r\n > \u6536\u4ef6\u4eba: \"tensorflow/tensorflow\"<tensorflow@noreply.github.com>;\r\n > \u6284\u9001: \"longma\"<1442342449@qq.com>;\"Comment\"<comment@noreply.github.com>;\r\n > \u4e3b\u9898: Re: [tensorflow/tensorflow] Eager Execution error: Blas GEMM\r\n > launchfailed (#25403)\r\n >\r\n >\r\n >\r\n > What is your GPU?\r\n >\r\n > On Fri, Oct 18, 2019, 10:23 PM longma <notifications@github.com> wrote:\r\n >\r\n > > The following what I found,\r\n > >\r\n > > * IF you have RTX 20 series (ex. 2070 or 2080), you need to use CUDA\r\n > 10.0 with CuDNN 7.5 and TF > 1.12.\r\n > >\r\n > > * IF you use CUDA 10.1, you must use CuDNN 7.6\r\n > >\r\n > > * IF you have RTX 20 series and still wants to use CUDA 9.0 and CuDNN\r\n > 7.5, you must use TF <= 1.12\r\n > >\r\n > > Also, IF you have RTX 20 series and CUDA 10 you must put this in your\r\n > code\r\n > >\r\n > > tf_config = tf.ConfigProto()\r\n > > tf_config.gpu_options.allow_growth = True\r\n > > tf_config.gpu_options.per_process_gpu_memory_fraction = 0.9\r\n > > tf_config.allow_soft_placement = True\r\n > >\r\n > > Hi,I have meet this problem.my cuda is 9.0,cudnn is 7.1,tensorflow-gpu\r\n > is\r\n > > 1.9.can you provide the guidance how do i solver problem?\r\n > > InternalError (see above for traceback): Blas xGEMM launch failed\r\n > >\r\n > > \u2014\r\n > > You are receiving this because you commented.\r\n > > Reply to this email directly, view it on GitHub\r\n > > <\r\n > https://github.com/tensorflow/tensorflow/issues/25403?email_source=notifications&email_token=AH7VFK5WND62UTTXHECUZT3QPJVSVA5CNFSM4GTV3D5KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEBWZY7A#issuecomment-544054396>,\r\n >\r\n > > or unsubscribe\r\n > > <\r\n > https://github.com/notifications/unsubscribe-auth/AH7VFK3XW55KQ7OIQJLBLS3QPJVSVANCNFSM4GTV3D5A>\r\n >\r\n > > .\r\n > >\r\n >\r\n > \u2014\r\n > You are receiving this because you commented.\r\n > Reply to this email directly, view it on GitHub, or unsubscribe.\r\n >\r\n > \u2014\r\n > You are receiving this because you commented.\r\n > Reply to this email directly, view it on GitHub\r\n > <https://github.com/tensorflow/tensorflow/issues/25403?email_source=notifications&email_token=AH7VFKYQXPZ6TV7O6D4KPQDQPJZ2LA5CNFSM4GTV3D5KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEBW4OKA#issuecomment-544065320>,\r\n > or unsubscribe\r\n > <https://github.com/notifications/unsubscribe-auth/AH7VFK7XJ6OFNGP3MPT2HOTQPJZ2LANCNFSM4GTV3D5A>\r\n > .\r\n >\r\n \r\n\u2014\r\nYou are receiving this because you commented.\r\nReply to this email directly, view it on GitHub, or unsubscribe.", "I've got the same error today. My GPU is 2080 Ti, CUDA and cuDNN are latest (10.1), and I'm using Windows 10. I've just installed all that and tried to launch https://www.tensorflow.org/tutorials/quickstart/advanced. Here's the full error text:\r\n\r\nInternalError:  Blas GEMM launch failed : a.shape=(32, 21632), b.shape=(21632, 128), m=32, n=128, k=21632\r\n\t [[node my_model/dense/MatMul (defined at D:\\Anaconda\\envs\\TF2\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]] [Op:__inference_train_step_638]\r\n\r\nFunction call stack:\r\ntrain_step\r\n\r\nI don't think my GPU was busy at that time, but I don't know how to check that.\r\nUPDATE: I've tried **ronykalfarisi**'s code with ConfigProto, but it didn't help at all.", "My problem is solved. Tensorflow failed to load **cublas64_100.dll** because it was called **cublas64_10.dll**. I am simply shocked to encounter such errors. Anyway, thanks everybody, and I hope my stupid messages will help another newb that can't believe in DLL names change :)", "> My problem is solved. Tensorflow failed to load **cublas64_100.dll** because it was called **cublas64_10.dll**. I am simply shocked to encounter such errors. Anyway, thanks everybody, and I hope my stupid messages will help another newb that can't believe in DLL names change :)\r\n\r\nHi,can you provide the detail about solving this problem. I am very confuse to this problem.", "I've just copied cublas64_10.dll to cublas64_100.dll, and it worked :)", "> I've just copied cublas64_10.dll to cublas64_100.dll, and it worked :)\r\n\r\nThink you for your reply,where do you find cublas64_10.dll and cublas64_100,dll?I do not meet it.", "Cublas64_10.dll should be in **bin** subfolder of your CUDA installation. My path is C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin\\\r\nThis is default installation path for CUDA (I didn't change it).", "Do you mean to replace cublas64 100.dll with&nbsp;cublas64_10.dll\uff1f\r\n\r\n\r\n\r\n\r\n------------------ Original ------------------\r\nFrom: Edremelech <notifications@github.com&gt;\r\nDate: Tue,Oct 29,2019 0:23 PM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com&gt;\r\nCc: longma <1442342449@qq.com&gt;, Comment <comment@noreply.github.com&gt;\r\nSubject: Re: [tensorflow/tensorflow] Eager Execution error: Blas GEMM launch failed (#25403)\r\n\r\n\r\n\r\n\r\nI've just copied to cublas64_100.dll, and it worked :)\r\n \r\n\u2014\r\nYou are receiving this because you commented.\r\nReply to this email directly, view it on GitHub, or unsubscribe.", "> \r\n> Do you mean to replace cublas64 100.dll with&nbsp;cublas64_10.dll\uff1f\r\n> [\u2026](#)\r\n> ------------------ Original ------------------ From: Edremelech <notifications@github.com&gt; Date: Tue,Oct 29,2019 0:23 PM To: tensorflow/tensorflow <tensorflow@noreply.github.com&gt; Cc: longma <1442342449@qq.com&gt;, Comment <comment@noreply.github.com&gt; Subject: Re: [tensorflow/tensorflow] Eager Execution error: Blas GEMM launch failed (#25403) I've just copied to cublas64_100.dll, and it worked :) \u2014 You are receiving this because you commented. Reply to this email directly, view it on GitHub, or unsubscribe.\r\n\r\nThere was NO cublas64_100.dll, only cublas64_10.dll. I guess NVIDIA have suddenly changed its naming policy.", "thank you very much \uff0cI will try to run it according\r\n&nbsp;to your guidance\r\n\r\n\r\n\r\n\r\n\r\n------------------ Original ------------------\r\nFrom: Edremelech <notifications@github.com&gt;\r\nDate: Tue,Oct 29,2019 1:23 PM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com&gt;\r\nCc: longma <1442342449@qq.com&gt;, Comment <comment@noreply.github.com&gt;\r\nSubject: Re: [tensorflow/tensorflow] Eager Execution error: Blas GEMM launch failed (#25403)\r\n\r\n\r\n\r\n \r\nDo you mean to replace cublas64 100.dll with&nbsp;cublas64_10.dll\uff1f\r\n \u2026\r\n ------------------ Original ------------------ From: Edremelech <notifications@github.com&gt; Date: Tue,Oct 29,2019 0:23 PM To: tensorflow/tensorflow <tensorflow@noreply.github.com&gt; Cc: longma <1442342449@qq.com&gt;, Comment <comment@noreply.github.com&gt; Subject: Re: [tensorflow/tensorflow] Eager Execution error: Blas GEMM launch failed (#25403) I've just copied to cublas64_100.dll, and it worked :) \u2014 You are receiving this because you commented. Reply to this email directly, view it on GitHub, or unsubscribe.\r\n  \r\nThere was NO cublas64_100.dll, only cublas64_10.dll. I guess NVIDIA have suddenly changed its naming policy.\r\n \r\n\u2014\r\nYou are receiving this because you commented.\r\nReply to this email directly, view it on GitHub, or unsubscribe.", "> > Do you mean to replace cublas64 100.dll with cublas64_10.dll\uff1f\r\n> > [\u2026](#)\r\n> > ------------------ Original ------------------ From: Edremelech <[notifications@github.com](mailto:notifications@github.com)> Date: Tue,Oct 29,2019 0:23 PM To: tensorflow/tensorflow <[tensorflow@noreply.github.com](mailto:tensorflow@noreply.github.com)> Cc: longma <[1442342449@qq.com](mailto:1442342449@qq.com)>, Comment <[comment@noreply.github.com](mailto:comment@noreply.github.com)> Subject: Re: [tensorflow/tensorflow] Eager Execution error: Blas GEMM launch failed (#25403) I've just copied to cublas64_100.dll, and it worked :) \u2014 You are receiving this because you commented. Reply to this email directly, view it on GitHub, or unsubscribe.\r\n> \r\n> There was NO cublas64_100.dll, only cublas64_10.dll. I guess NVIDIA have suddenly changed its naming policy.\r\n\r\nsorry, I used ubuntu,I can not find cublas64_10.dll or cublas64_100dll.", "@Edremelech Thank you!\r\nAfter renaming cublas64_10.dll to cublas64_100.dll, my program runs as well.", "hello\uff0cwhat is your computer system\uff1fwindows or ubuntu\uff1f\r\n\r\n\r\n\r\n\r\n\r\n------------------ Original ------------------\r\nFrom: Uros Ogrizovic <notifications@github.com&gt;\r\nDate: Wed,Dec 11,2019 2:11 AM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com&gt;\r\nCc: longma <1442342449@qq.com&gt;, Comment <comment@noreply.github.com&gt;\r\nSubject: Re: [tensorflow/tensorflow] Eager Execution error: Blas GEMM launch failed (#25403)\r\n\r\n\r\n\r\n\r\n@Edremelech Thank you!\r\n After renaming cublas64_10.dll to cublas64_100.dll, my program runs as well.\r\n \r\n\u2014\r\nYou are receiving this because you commented.\r\nReply to this email directly, view it on GitHub, or unsubscribe.", "@longmalongma Windows 10.", "my computer system is ubuntu16.0\r\n\r\n\r\n\r\n\r\n\r\n------------------ Original ------------------\r\nFrom: Uros Ogrizovic <notifications@github.com&gt;\r\nDate: Wed,Dec 11,2019 6:12 PM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com&gt;\r\nCc: longma <1442342449@qq.com&gt;, Mention <mention@noreply.github.com&gt;\r\nSubject: Re: [tensorflow/tensorflow] Eager Execution error: Blas GEMM launch failed (#25403)\r\n\r\n\r\n\r\n\r\n@longmalongma Windows 10.\r\n \r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub, or unsubscribe.", "@smit-hinsu I have the same problem with CUDA 10.1 tf2.1 in eager mode only (fitting a keras model works fine for example).\r\n\r\nI tried the same code as the OP (without enabling the eager mode because I am in tf 2.1) and it failed. I first noticed the bug because I am training GANs so with a custom training loop.\r\n\r\nIf I disable eager execution, I get the following error: `RuntimeError: Attempting to capture an EagerTensor without building a function.`, but I guess this has to do with my custom code.\r\n\r\nMy GPU is a Quadro P5000.\r\n\r\nMy logs:\r\n```\r\n2020-01-10 17:49:10.323707: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:/usr/local/cuda-10.1/lib64\r\n2020-01-10 17:49:10.323788: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:/usr/local/cuda-10.1/lib64\r\n2020-01-10 17:49:10.323801: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n2020-01-10 17:49:11.223387: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-01-10 17:49:11.239819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:73:00.0 name: Quadro P5000 computeCapability: 6.1\r\ncoreClock: 1.7335GHz coreCount: 20 deviceMemorySize: 15.89GiB deviceMemoryBandwidth: 269.00GiB/s\r\n2020-01-10 17:49:11.240026: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-01-10 17:49:11.241796: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-01-10 17:49:11.243488: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-01-10 17:49:11.243796: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-01-10 17:49:11.245661: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-01-10 17:49:11.246735: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-01-10 17:49:11.250838: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-01-10 17:49:11.252938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-01-10 17:49:11.253316: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2020-01-10 17:49:11.282108: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\r\n2020-01-10 17:49:11.287535: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4eb6cf0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-01-10 17:49:11.287609: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-01-10 17:49:11.432018: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4f1d0b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-01-10 17:49:11.432099: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P5000, Compute Capability 6.1\r\n2020-01-10 17:49:11.438317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:73:00.0 name: Quadro P5000 computeCapability: 6.1\r\ncoreClock: 1.7335GHz coreCount: 20 deviceMemorySize: 15.89GiB deviceMemoryBandwidth: 269.00GiB/s\r\n2020-01-10 17:49:11.438472: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-01-10 17:49:11.438525: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-01-10 17:49:11.438569: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-01-10 17:49:11.438610: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-01-10 17:49:11.438651: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-01-10 17:49:11.438692: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-01-10 17:49:11.438734: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-01-10 17:49:11.443078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-01-10 17:49:11.443190: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-01-10 17:49:11.447251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-01-10 17:49:11.447292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2020-01-10 17:49:11.447313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2020-01-10 17:49:11.452088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8679 MB memory) -> physical GPU (device: 0, name: Quadro P5000, pci bus id: 0000:73:00.0, compute capability: 6.1)\r\n2020-01-10 17:49:11.459693: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-01-10 17:49:11.461527: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2020-01-10 17:49:11.644306: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2020-01-10 17:49:11.644349: W tensorflow/stream_executor/stream.cc:2041] attempting to perform BLAS operation using StreamExecutor without BLAS support\r\n---------------------------------------------------------------------------\r\nInternalError                             Traceback (most recent call last)\r\n<ipython-input-1-5bdb1e6082c6> in <module>\r\n      1 import tensorflow as tf\r\n----> 2 print(tf.matmul([[1., 2.],[3., 4.]], [[1., 2.],[3., 4.]]))\r\n\r\n~/workspace/fastmri-reproducible-benchmark/venv/lib/python3.6/site-packages/tensorflow_core/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    178     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    179     try:\r\n--> 180       return target(*args, **kwargs)\r\n    181     except (TypeError, ValueError):\r\n    182       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n~/workspace/fastmri-reproducible-benchmark/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/math_ops.py in matmul(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\r\n   2796     else:\r\n   2797       return gen_math_ops.mat_mul(\r\n-> 2798           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\r\n   2799 \r\n   2800 \r\n\r\n~/workspace/fastmri-reproducible-benchmark/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_math_ops.py in mat_mul(a, b, transpose_a, transpose_b, name)\r\n   5614         pass  # Add nodes to the TensorFlow graph.\r\n   5615     except _core._NotOkStatusException as e:\r\n-> 5616       _ops.raise_from_not_ok_status(e, name)\r\n   5617   # Add nodes to the TensorFlow graph.\r\n   5618   if transpose_a is None:\r\n\r\n~/workspace/fastmri-reproducible-benchmark/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6604   message = e.message + (\" name: \" + name if name is not None else \"\")\r\n   6605   # pylint: disable=protected-access\r\n-> 6606   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6607   # pylint: enable=protected-access\r\n   6608 \r\n\r\n~/workspace/fastmri-reproducible-benchmark/venv/lib/python3.6/site-packages/six.py in raise_from(value, from_value)\r\n\r\nInternalError: Blas GEMM launch failed : a.shape=(2, 2), b.shape=(2, 2), m=2, n=2, k=2 [Op:MatMul] name: MatMul/\r\n```\r\n\r\nEDIT\r\n-----\r\n\r\nDowngrading libcublas to 10.1 seems to have solved my problem (at least with the example of the OP). I noticed I had these blas errors (`CUBLAS_STATUS_NOT_INITIALIZED` for instance) and followed this advice. I also cleared the nv cache.", "@Edremelech Thank you!\r\nAfter renaming cublas64_10.dll to cublas64_100.dll, my program runs as well.", "this error continues to exist for me on TF2.1 when trying to use tf hub modules... I can confirm that GPU is not running out of memory or there is no other notebook using the GPU ", "Previous:\r\ncudatoolkit 9.0\r\ncudnn 7.6\r\ntensorflow-gpu 1.12\r\n\r\nSolved:\r\ncudatoolkit 9.0\r\ncudnn 7.6\r\ntensorflow-gpu 1.13", "> Can you please kill all running notebooks which utilize your GPU. Then restart the kernel and execute the code again?\r\n\r\nIt worked for me. Thanks.", "> Previous:\r\n> cudatoolkit 9.0\r\n> cudnn 7.6\r\n> tensorflow-gpu 1.12\r\n> \r\n> Solved:\r\n> cudatoolkit 9.0\r\n> cudnn 7.6\r\n> tensorflow-gpu 1.13\r\n\r\nBut it seems that the tensorflow-gpu 1.13 need CUDA 10.0, how to solve it?", "IF running an RTX series check first if TF2 is using Cuda 10\r\nThen call set_memory_growth and done!\r\n\r\n```\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\ntry:\r\n  tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n  assert tf.config.experimental.get_memory_growth(physical_devices[0])\r\nexcept:\r\n  pass\r\n```", "After renaming cublas64_10.dll to cublas64_100.dll, I solved the error too. Thank you!", "In windows ended python tasks worked"]}, {"number": 25402, "title": "Flatten is not constant folded properly by constfold grappler.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): master (1/31/19)\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): 0.19.2\r\n- GCC/Compiler version (if compiling from source): c++ (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\n- CUDA/cuDNN version: CUDA 10.0.130\r\n- GPU model and memory: Quadro RTX 6000, 24GiB\r\n\r\n\r\n**Describe the current behavior**\r\ntf.layers.Flatten uses Shape, StridedSlice, Pack ops to build the new shape for Reshape. These ops are not constant folded.\r\n\r\n**Describe the expected behavior**\r\n Since the shape of `x` is known at compile time, the constant folder should be able to fold away these ops to create a single Const for the new shape for Reshape.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nimport math\r\nfrom tensorflow.core.protobuf import meta_graph_pb2\r\nfrom tensorflow.core.protobuf import rewriter_config_pb2\r\nfrom tensorflow.python.grappler import tf_optimizer\r\n\r\ndef build_graph(x, output_name='output'):\r\n  # x.shape = (None, 1, 1, 1536)\r\n  x = tf.layers.Flatten()(x)\r\n  # This should perform exactly the same as:\r\n  # x = tf.reshape(x, [-1, 1536])\r\n  return tf.identity(x, name=output_name)\r\n\r\ndef apply_constfold(frozen_graph, output_nodes):\r\n  graph = tf.Graph()\r\n  with graph.as_default():\r\n    tf.import_graph_def(frozen_graph, name=\"\")\r\n  grappler_meta_graph_def = tf.train.export_meta_graph(graph_def=graph.as_graph_def(add_shapes=True), graph=graph)\r\n\r\n  _to_bytes = lambda s: s.encode(\"utf-8\", errors=\"surrogateescape\")\r\n  output_collection = meta_graph_pb2.CollectionDef()\r\n  output_list = output_collection.node_list.value\r\n  for i in output_nodes:\r\n    if isinstance(i, tf.Tensor):\r\n      output_list.append(_to_bytes(i.name))\r\n    else:\r\n      output_list.append(_to_bytes(i))\r\n  # TODO(laigd): use another key as the outputs are really not train_op.\r\n  grappler_meta_graph_def.collection_def[\"train_op\"].CopyFrom(output_collection)\r\n  rewriter_config = rewriter_config_pb2.RewriterConfig()\r\n  rewriter_config.optimizers.extend([\"constfold\"])\r\n\r\n  session_config_with_trt = tf.ConfigProto()\r\n  session_config_with_trt.graph_options.rewrite_options.CopyFrom(\r\n      rewriter_config)\r\n  frozen_graph = tf_optimizer.OptimizeGraph(session_config_with_trt, grappler_meta_graph_def, graph_id=b\"tf_graph\")\r\n  return frozen_graph\r\n\r\nif __name__ == '__main__':\r\n  with tf.Graph().as_default():\r\n    # Create graph\r\n    x = tf.placeholder(dtype=tf.float32, shape=(None, 1, 1, 1536), name='input')\r\n    y = build_graph(x)\r\n    # Initialize\r\n    with tf.Session() as sess:\r\n      sess.run(tf.global_variables_initializer())\r\n      # Freeze graph\r\n      frozen_graph = tf.graph_util.convert_variables_to_constants(\r\n          sess,\r\n          sess.graph_def,\r\n          output_node_names=['output'])\r\n\r\n  print('Nodes before:')\r\n  [print(n.name, n.op) for n in frozen_graph.node]\r\n\r\n  # const folding\r\n  frozen_graph = apply_constfold(frozen_graph, output_nodes=['output'])\r\n\r\n  print('----------------------------------------')\r\n  print('Nodes after:')\r\n  [print(n.name, n.op) for n in frozen_graph.node]\r\n```\r\n\r\n**Output**\r\n```\r\nNodes before:\r\ninput Placeholder\r\nflatten/Shape Shape\r\nflatten/strided_slice/stack Const\r\nflatten/strided_slice/stack_1 Const\r\nflatten/strided_slice/stack_2 Const\r\nflatten/strided_slice StridedSlice\r\nflatten/Reshape/shape/1 Const\r\nflatten/Reshape/shape Pack\r\nflatten/Reshape Reshape\r\noutput Identity\r\n-------------------------\r\nNodes after:\r\ninput Placeholder\r\nflatten/strided_slice/stack Const\r\nflatten/strided_slice/stack_1 Const\r\nflatten/strided_slice/stack_2 Const\r\nflatten/Reshape/shape/1 Const\r\nflatten/Shape Shape\r\nflatten/strided_slice StridedSlice\r\nflatten/Reshape/shape Pack\r\nflatten/Reshape Reshape\r\noutput Identity\r\n```\r\n\r\n** Expected Output **\r\n```\r\nNodes before:\r\ninput Placeholder\r\nflatten/Shape Shape\r\nflatten/strided_slice/stack Const\r\nflatten/strided_slice/stack_1 Const\r\nflatten/strided_slice/stack_2 Const\r\nflatten/strided_slice StridedSlice\r\nflatten/Reshape/shape/1 Const\r\nflatten/Reshape/shape Pack\r\nflatten/Reshape Reshape\r\noutput Identity\r\n-------------------------\r\nNodes after:\r\ninput Placeholder\r\nReshape/shape Const\r\nReshape Reshape\r\noutput Identity\r\n```", "comments": ["@rmlarsen ", "@aaroey Could you take a look at this? This is affecting InceptionV4 in TF-TRT since the Flatten() layer at the end of the network is not able to be converted.", "@trevor-m sorry for being late. I can reproduce the problem, but I think it'll be hard to solve this problem in Grappler unless with explicit pattern matching. So, the output `flatten/strided_slice:0` is the batch dimension of the input (lets name it `batch_dim`), then the output of `flatten/Reshape/shape:0` is `[batch_dim, -1]` (note, not `[-1, 1536]` even though the outcome are same with this particular graph). Since `batch_dim` is not known at compile time, it won't be able to fold it, and I don't know any existing framework that can track the `batch_dim` and flow it along the edges.\r\n\r\n@rmlarsen any suggestions?", "+@andyly as well.", "As @aaroey mentioned, because the shape is not fully defined for the PlaceHolder, it cannot be constant folded down into the Shape, resulting in no further constant folding. One option is to write an explicit transformation looking for this pattern, but I am not sure how generalized this sort of pattern can be.", "Thanks everyone for investigating!\r\nSo currently `Flatten` is doing the following (simplified):\r\n```\r\nx = tf.reshape(x, shape=(x.shape[0], -1))\r\n```\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/keras/layers/core.py#L551-L553\r\n\r\nCould we instead change flatten to do the following: If all dims other than the batch dim are fully defined we use `[-1, 1536]` instead of `[batch_dim, -1]`?\r\n```\r\nif -1 not in x.shape[1:]:\r\n  x = tf.reshape(x, shape=(-1, prod(x.shape[1:]))\r\nelse:\r\n  x = tf.reshape(x, shape=(x.shape[0], -1))\r\n```"]}, {"number": 25401, "title": "[INTEL MKL] Check if filter is a constant only in the forward pass for Depthwise Conv2D.", "body": "This fix causes `//tensorflow/core:graph_mkl_layout_pass_test` and `//tensorflow/python/keras:convolutional_test` unit tests to pass.", "comments": ["@ymodak the failed checks don't seem to be related to this PR. \r\n\r\nPlease let me know if there's anything else required from my side. Thanks"]}, {"number": 25399, "title": "[v1.13][CP Request] Prevent null pointer dereference in decode_gif.", "body": "PiperOrigin-RevId: 231841542\r\n\r\nThis has a fix for a security vulnerability discovered last night. We will have to patch other releases but for 1.13 we can cherry-pick this now.\r\n\r\nNot including this will result in people being able to do denial of service attacks on tensorflow by using specially crafted GIF images.", "comments": []}, {"number": 25398, "title": "Feature Request: tf.diag with a parameter to choose the diagonal ", "body": "\r\n**System information**\r\n- TensorFlow version (you are using):'1.12.0'\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nIn the current version tf.diag only allow creating a tensor with the main diagonal filled. \r\n```python\r\n# 'diagonal' is [[1, 2, 3, 4], [5, 6, 7, 8]]\r\n\r\nand diagonal.shape = (2, 4)\r\n\r\ntf.matrix_diag(diagonal) ==> [[[1, 0, 0, 0]\r\n                                     [0, 2, 0, 0]\r\n                                     [0, 0, 3, 0]\r\n                                     [0, 0, 0, 4]],\r\n                                    [[5, 0, 0, 0]\r\n                                     [0, 6, 0, 0]\r\n                                     [0, 0, 7, 0]\r\n                                     [0, 0, 0, 8]]]\r\n\r\nwhich has shape (2, 4, 4)\r\n```\r\nNumpy has an extra parameter, k. The default value of k is zero. When k=0  np.diag has the same behavior than tf.diag. However, setting k>0 (k<0) the user fill, or select, a diagonal above (below) the main diagonal.\r\n\r\n```python\r\nx = np.arange(9).reshape((3,3))\r\n np.diag(x, k=0)\r\n#array([0, 4, 8])\r\nnp.diag(x, k=1)\r\n# array([1, 5])\r\nnp.diag([1,5], k=1)\r\n#array([[0, 1, 0],\r\n#      [0, 0, 5],\r\n#       [0, 0, 0]])\r\n```\r\n\r\n**Who will benefit with this feature?**\r\n In general, tridiagonal matrices are quite common in mathematics and physics.", "comments": ["@rmlarsen @ebrevdo What do you think? [numpy.diag](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.diag.html)", "@rmlarsen @facaiy @ebrevdo \r\nIs there any progress on implementing the above request? We are currently working a project that desperately needs this. Currently, using the _kwarg_ k gives \"Unexpected keyword argument k\" error, despite the following documentation pages showing it as a valid keyword argument: \r\n[https://www.tensorflow.org/api_docs/python/tf/linalg/set_diag](url)\r\n[https://www.tensorflow.org/api_docs/python/tf/linalg/diag](url)", "I had missed this feature request when I opened #31304.\r\nI need it as well, it seems like a logical improvement of the current diagonal constructor.", "This functionality was recently added \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py#L1955", "@JCatesPH If you're really desperate I got around that\r\n\r\nhttps://github.com/stdogpkg/emate/blob/0d94da36ab0f1128b5614d3176d472622e3cbc88/emate/utils/tfops/misc.py#L99\r\n\r\nhttps://github.com/stdogpkg/emate/blob/0d94da36ab0f1128b5614d3176d472622e3cbc88/emate/symmetric/tfops/slq.py#L85"]}, {"number": 25397, "title": "[Docs] Minor nit in the performance overview page", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: 1.12.0\r\n- Doc Link: https://www.tensorflow.org/guide/performance/overview\r\n\r\n\r\n**Describe the documentation issue**\r\nA minor nit, i the performance overview page change /Alexne**x**/Alexne**t**/\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\nyes\r\n", "comments": ["Thanks for the report. Can you please send a PR for this?"]}, {"number": 25396, "title": "Make install_deb_packages.sh platform independent", "body": "install_deb_packages.sh currently installs platform dependent packages.  These should go into the platform-specific Dockerfiles instead, like this relevant commit to Dockerfile.gpu:  \r\nhttps://github.com/tensorflow/tensorflow/commit/03ec3f7b11572b70ef7f33f2a9af9bebbb6ce473", "comments": []}, {"number": 25395, "title": "TF debug crashes when nan in loss", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No, beyond the trivial fix needed for  https://github.com/tensorflow/models/issues/5857, and wrapping the tf.Session in a debug session\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: 1080ti / 11GB\r\n\r\n\r\n**Describe the current behavior**\r\nWhen run in the debugger, if there is a NaN in the loss, the debugger crashes.\r\n\r\n**Describe the expected behavior**\r\n\r\nExpected behavior is that the debugger does not crash and I can examine the tensors.\r\n**Code to reproduce the issue**\r\n\r\nIn models/research/struct2depth, run: \r\nckpt_dir=\"ckpt\"\r\ndata_dir=\"/opt/baddata/tfrep\"\r\n\r\npython train.py \\\r\n  --logtostderr \\\r\n  --checkpoint_dir=$ckpt_dir  \\\r\n  --data_dir=$data_dir \\\r\n  --architecture=resnet \\\r\n  --imagenet_norm=true \\\r\n  --joint_encoder=false\r\n\r\n**Other info / logs**\r\n\r\nI'm attaching a screenshot of the terminal after tfdbg crashes (the terminal gets corrupted so I can't paste this into a text file) :  \r\n<img width=\"1422\" alt=\"screen shot 2019-01-28 at 12 06 26\" src=\"https://user-images.githubusercontent.com/4651708/52070300-8628ac00-254e-11e9-835e-e5ff4ff84282.png\">\r\n\r\nA set of input data that triggers this crash will be uploaded soon.", "comments": ["I'm attaching a tgz file with a set of ten images that will trigger this crash.  They're originally from the kitti dataset.   The \"train.txt\" that corresponds to this looks something like:\r\n\r\n/opt/bigdata/kitti_processed/2011_09_30_drive_0018_sync_03 0000001706\r\n/opt/bigdata/kitti_processed/2011_09_30_drive_0018_sync_03 0000000312\r\n/opt/bigdata/kitti_processed/2011_09_30_drive_0018_sync_02 0000002585\r\n/opt/bigdata/kitti_processed/2011_09_30_drive_0018_sync_03 0000001151\r\n/opt/bigdata/kitti_processed/2011_09_30_drive_0018_sync_02 0000000365\r\n/opt/bigdata/kitti_processed/2011_09_30_drive_0018_sync_03 0000000829\r\n/opt/bigdata/kitti_processed/2011_09_30_drive_0018_sync_03 0000000092\r\n/opt/bigdata/kitti_processed/2011_09_30_drive_0018_sync_02 0000001900\r\n/opt/bigdata/kitti_processed/2011_09_30_drive_0018_sync_03 0000002574\r\n/opt/bigdata/kitti_processed/2011_09_30_drive_0018_sync_02 0000000327\r\n\r\n[badkitti.tar.gz](https://github.com/tensorflow/tensorflow/files/2818949/badkitti.tar.gz)\r\n\r\n", "Thanks for reporting this issue, @kesinger Looks like it's related to the fact that the model is loaded from a checkpoint and there is no partition graph available (?)\r\nHere is the last two places in the stack trace.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/debug/wrappers/local_cli_wrapper.py#L439\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/debug/lib/debug_data.py#L1036\r\n\r\nI'll investigate further.", "@kesinger TF1.x is deprecated. Please update to TF2.x debugger and open a new issue with a simple standalone code to reproduce the issue.\r\n\r\nI am closing this as the TF1.x is deprecated. Please feel free to reopen I am mistaken. Thanks!"]}]