[{"number": 3377, "title": "Moving data from CPU to GPU is slow ", "body": "I have noticed that when moving MNIST data from the CPU to the GPU, there is a significant time lag when using TensorFlow in comparison to Theano. Specifically, we have noticed this problem in the context of feed_dict, which moves information from the CPU to the GPU when running minibatches. I am using python 2.7. Our current solution to this problem is to move all of the data directly to the GPU at the beginning of the program, which is of course not sustainable unless one has a significant amount of space on their GPU. \n\nWhen we time 25000 minibatches of size 100 each, TensorFlow is approximately four times as slow as Theano. I have attached both files, and the time difference is evident in the final output. \n### Environment info\n\nOperating System: Ubuntu 14.04\n\nCPU Information:\n\nArchitecture:          x86_64\nCPU op-mode(s):        32-bit, 64-bit\nByte Order:            Little Endian\nCPU(s):                12\nOn-line CPU(s) list:   0-11\nThread(s) per core:    2\nCore(s) per socket:    6\nSocket(s):             1\nNUMA node(s):          1\nVendor ID:             GenuineIntel\nCPU family:            6\nModel:                 63\nStepping:              2\nCPU MHz:               1246.328\nBogoMIPS:              6995.89\nVirtualization:        VT-x\nL1d cache:             32K\nL1i cache:             32K\nL2 cache:              256K\nL3 cache:              15360K\nNUMA node0 CPU(s):     0-11\n\nGPU: NVidia GeForce GTX TITAN X Graphics Card (12GB) \n\nInstalled version of CUDA and cuDNN: 7.5.17\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n\nRelevant files (I apologize for the poor naming conventions):\n\n1.[mnist_softmax.txt](https://github.com/tensorflow/tensorflow/files/370296/mnist_softmax.txt)\nThis file contains the relevant program that uses TensorFlow. \n1. [mnist.pkl.gz](https://github.com/tensorflow/tensorflow/files/370306/mnist.pkl.gz)\n   This file contains the dataset for the TensorFlow file. \n2.  [mnist_softmax_theano.txt(https://github.com/tensorflow/tensorflow/files/370314/mnist_softmax_theano.txt)\n   This file contains the relevant program that uses Theano.\n3. [tf_data.pkl.gz](https://github.com/tensorflow/tensorflow/files/370310/tf_data.pkl.gz)\n   This file contains the dataset necessary for the _Theano_ file.\n", "comments": ["Just curious: have you (and how have you) isolated the problem to host-to-device transfer with feeding?  \n\nSearch for `StepStats` and \"chrome timeline\" in the repo.  It should tell you how to get a basic performance profile out.\n", "Lemme clarify the question a little bit. \n\nWe took the tutorial code for logistic regression for MNIST from the tensorflow website.\n\nWe used the same MNIST dataset from the tensorflow repository and used an equivalent logistic regression model in Theano. \n\nWe're not sure where the four-time timing difference comes from, although I suspect it's from the feed_dict and placeholder feature. \n\nI don't see any documentation for the StepStats and timeline function. So I don't know how to use those to do profiling. \n\nCan you please provide more instruction as to how to profile?\n\nThank you.\n", "You can look at report #3320 as I have example code there to capture a run in the \"discrete_deepq_model.py\" file starting near line 257.  This will give you a trace file to open in Crome with:\nchrome://tracing\n\nThere is a link to the reference I used to create the trace file in that post as well.\n", "We generated a basic performance profile for a single step and opened it in chrome://tracing, but we are unsure of how to interpret it.\n\nHere is the JSON file: \n[timeline.ctf.json.zip](https://github.com/tensorflow/tensorflow/files/382705/timeline.ctf.json.zip)\n", "@zheng-xq would you take a look?\n", "The above is for a single step, here is a JSON file with 500 steps concatenated together:\n[timeline_500.ctf.json.zip](https://github.com/tensorflow/tensorflow/files/382810/timeline_500.ctf.json.zip)\n", "@Brandon-Tai, the best performing TensorFlow models tend to feed the examples into a queue with a different Python thread, or use one of the reader op to read in the examples, instead through feed_dict. \n\nhttps://www.tensorflow.org/versions/r0.10/api_docs/python/io_ops.html#readers\n\nWe can take a look a this model and see if there are other issues beside feed_dict. \n", "@zheng-xq It seems that if we only use one GPU to train the model, then even if we load the data from disk to memory in parallel threads we still have to load the data into the GPU sequentially. Do you mean we should train the models using multiple GPUs so that we can also load the data into GPU in parallel? But in that case for each GPU the loading still has to be sequential.\n", "feed_dict has more overhead than just copying CPU to GPU. It also moves the data from Python to TensorFlow among other things. Someone will have to look further to determine whether this is the only issue. \n\nBut even with only one GPU, you can hide data transfer between python and TensorFlow through: \n\n1.a Having one python thread to read the data, and feed them into a tensorflow queue, managed by queue runner.\n1.b Having your model read from that queue in parallel, instead of through feed_dict. That way, the data copy from Python to TensorFlow is parallelized behind the real training. \n\nWhen you have many replicas running on multiple GPU over multiple machines, you want to use the reader class to read the data locally on each replica, instead of through feed_dict. For example: \n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_input.py#L79\nhttps://github.com/tensorflow/models/blob/master/inception/inception/dataset.py#L95\n", "@lizallendorf, is your original issue resolved? Did @zheng-xq's suggestions help?\n", "BTW, official MNIST examples on the website Howto are not tuned. /mnist/fully_connected_feed.py using feed_dict runs is 10x faster than other 3 examples which use queues, [due to how](http://stackoverflow.com/a/39842628/419116) Python schedules pre-fetching threads\n", "Closing due to lack of activity.\n"]}, {"number": 3376, "title": "Element-wise tf.cond (like theano switch)", "body": "When I try the following code:\n\n```\nimport tensorflow as tf\nx = tf.placeholder('float32', shape=(None, None))\ntf.cond(x >= 0.9, lambda: 1., lambda: x)\n```\n\nI get this error:\n\n```\n/Users/stas/.pyenv/versions/anaconda-2.4.0/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc in cond(pred, fn1, fn2, name)\n   1314     if isinstance(pred, bool):\n   1315       raise TypeError(\"pred must not be a Python bool\")\n-> 1316     p_2, p_1 = switch(pred, pred)\n   1317     pivot_1 = array_ops.identity(p_1, name=\"switch_t\")\n   1318     pivot_2 = array_ops.identity(p_2, name=\"switch_f\")\n\n/Users/stas/.pyenv/versions/anaconda-2.4.0/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc in switch(data, pred, dtype, name)\n    258     pred = ops.convert_to_tensor(pred, name=\"pred\")\n    259     if isinstance(data, ops.Tensor):\n--> 260       return gen_control_flow_ops._switch(data, pred, name=name)\n    261     else:\n    262       if not isinstance(data, (ops.IndexedSlices, ops.SparseTensor)):\n\n/Users/stas/.pyenv/versions/anaconda-2.4.0/lib/python2.7/site-packages/tensorflow/python/ops/gen_control_flow_ops.pyc in _switch(data, pred, name)\n    368     output_true: A `Tensor`. Has the same type as `data`. If `pred` is true, data will be forwarded to this output.\n    369   \"\"\"\n--> 370   result = _op_def_lib.apply_op(\"Switch\", data=data, pred=pred, name=name)\n    371   return _SwitchOutput._make(result)\n    372\n\n/Users/stas/.pyenv/versions/anaconda-2.4.0/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)\n    702           op = g.create_op(op_type_name, inputs, output_types, name=scope,\n    703                            input_types=input_types, attrs=attr_protos,\n--> 704                            op_def=op_def)\n    705           outputs = op.outputs\n    706           return _Restructure(ops.convert_n_to_tensor(outputs),\n\n/Users/stas/.pyenv/versions/anaconda-2.4.0/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\n   2260                     original_op=self._default_original_op, op_def=op_def)\n   2261     if compute_shapes:\n-> 2262       set_shapes_for_outputs(ret)\n   2263     self._add_op(ret)\n   2264     self._record_op_seen_by_control_dependencies(ret)\n\n/Users/stas/.pyenv/versions/anaconda-2.4.0/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in set_shapes_for_outputs(op)\n   1700       raise RuntimeError(\"No shape function registered for standard op: %s\"\n   1701                          % op.type)\n-> 1702   shapes = shape_func(op)\n   1703   if shapes is None:\n   1704     raise RuntimeError(\n\n/Users/stas/.pyenv/versions/anaconda-2.4.0/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc in _SwitchShape(op)\n   2345 def _SwitchShape(op):\n   2346   input_shape = op.inputs[0].get_shape()\n-> 2347   unused_pred_shape = op.inputs[1].get_shape().merge_with(tensor_shape.scalar())\n   2348   return [input_shape] * 2\n\n/Users/stas/.pyenv/versions/anaconda-2.4.0/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.pyc in merge_with(self, other)\n    568       except ValueError:\n    569         raise ValueError(\"Shapes %s and %s are not compatible\" %\n--> 570                          (self, other))\n    571\n    572   def concatenate(self, other):\n\nValueError: Shapes (?, ?) and () are not compatible\n```\n\nAs I understand _pred_ argument should be scalar, but what I need is element-wise condition.\n\nThough theano switch is working as expected:\n\n```\nimport theano.tensor as T\nx = T.matrix()\nT.switch(x >= 0.9, 1, x).eval({x: np.zeros((2,2), 'float32')})\n```\n", "comments": ["Your understanding of `tf.cond()` seems to be correct: `pred` needs to be a boolean scalar.  Marking as contributions welcome.\n", "The problem is that the result type of the if operation has different dimensions. You can easily solve the problem by having one ore more operations in the else branch such  that they return a scalar too, or modify the if part by returning also a dim-2 tensor.\n", "Does  tf.cond need to support element-wise or just convert pred to scalar? And  does BuildCondBranch argument fn need to check fn type since when fn is constant which has no attr name?\nand my patch  reshape pred to scalar , check the fn type and convert it to tensor  \n--- ./tensorflow/python/ops/control_flow_ops_org.py 2016-07-20 21:54:14.103755118 +0800\n+++ ./tensorflow/python/ops/control_flow_ops.py 2016-07-20 21:58:40.927753017 +0800\n@@ -1234,6 +1234,9 @@\n     original_r = r\n     result = []\n     if r is not None:\n-      #convert to Tensor\n-      if not isinstance(r, ops.Tensor)\n-        r = ops.convert_to_tensor(r)\n     if not isinstance(r, list) and not isinstance(r, _basetuple):\n       r = [r]\n       original_r = [original_r]\n  @@ -1316,6 +1319,12 @@\n   # Add the Switch to the graph.\n   if isinstance(pred, bool):\n     raise TypeError(\"pred must not be a Python bool\")\n  +\n-    # rashape pred to scalar\n-    pred_shape = pred.get_shape()\n-    if pred_shape.ndims is not None and pred_shape.ndims != 0\n-        pred = gen_array_ops.reshape(pred, [])\n  +\n   p_2, p_1 = switch(pred, pred)\n   pivot_1 = array_ops.identity(p_1, name=\"switch_t\")\n   pivot_2 = array_ops.identity(p_2, name=\"switch_f\")\n", "Isn't this just `tf.select`?\n", "FYI: From v1.0 `tf.select` has been replaced with `tf.where`."]}, {"number": 3375, "title": "wrong use of sequence_loss_by_example in ptb_word_lm.py?", "body": "Hi, I noticed \"sequence_loss_by_example\" with default of \"average_across_timesteps\":\n\ndef sequence_loss_by_example(logits, targets, weights,\n                             average_across_timesteps=True,\n                             softmax_loss_function=None, name=None):\n\nbut in pub_word_lm.py:\n\nloss = tf.nn.seq2seq.sequence_loss_by_example(\n        [logits],\n        [tf.reshape(self._targets, [-1])],\n        [tf.ones([batch_size \\* num_steps])])\n    self._cost = cost = tf.reduce_sum(loss) / batch_size\n\nweights sum of batch_size*num_steps was used in \"sequence_loss_by_example\" to normalize the score, so there is no need to divide loss by batch_size again, right? cost was used for gradient later.\n", "comments": ["@ebrevdo Eugene, can you take a look?\n", "Lukasz knows this code better.\n", "Hi!\n\nI think the use of sequence_loss_by_example in ptb_word_lm in ok, because sequence_loss_by_example averages across time-steps (not batch), but ptb_word_lm uses it with just 1 time-step -- it first puts the time component into the batch dimenstion. That's why it has the reshape, but the lists are all of length 1 -- that's the time dimenstion and that's why you see \"batch_size \\* num_steps\" there in weights.\n\nHope that clarifies it!\n\nLukasz\n", "When I run the following demo:\r\n\r\nimport tensorflow as tf\r\n\r\nA = tf.constant([[0.1,0.2,0.3,0.4],[0.2,0.1,0.4,0.3],[0.4,0.3,0.2,0.1],[0.3,0.2,0.1,0.4],[0.1,0.4,0.3,0.2]], dtype=tf.float32)\r\nB = tf.constant([1, 2, 1, 3, 3], dtype=tf.int32)\r\nw_1 = tf.constant(value=[1,1,1,1,1], dtype=tf.float32)\r\nw_2 = tf.constant(value=[1,2,3,4,5], dtype=tf.float32)\r\n\r\nD = tf.contrib.legacy_seq2seq.sequence_loss_by_example([A], [B], [w_1])\r\nD_1 = tf.contrib.legacy_seq2seq.sequence_loss_by_example([A], [B], [w_1], average_across_timesteps=False)\r\n\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(D))\r\n    print(sess.run(D_1))\r\n    \r\n\r\n\r\nThe outputs are as following:\r\n\r\n[1.4425355 1.2425355 1.3425356 1.2425356 1.4425356]\r\n[1.4425355 1.2425355 1.3425356 1.2425356 1.4425356]\r\n\r\nI found that whether the parm 'average_across_timesteps' is set as  'True' or 'False' the result is the same, why?"]}, {"number": 3374, "title": "Android app compilation fails: fatal error: gif_lib_private.h: No such file or directory", "body": "Hi.\n\nI'm unable to compile the Android app. Here is the error message:\n\n```\nERROR: /home/arnaud/.cache/bazel/_bazel_arnaud/5d461d587a2aa81b3eca305842d75a99/external/gif_archive/BUILD:14:1: C++ compilation of rule '@gif_archive//:gif' failed: namespace-sandbox failed: error executing command /home/arnaud/.cache/bazel/_bazel_arnaud/5d461d587a2aa81b3eca305842d75a99/execroot/tensorflow/_bin/namespace-sandbox ... (remaining 46 argument(s) skipped).\nexternal/gif_archive/giflib-5.1.4/lib/quantize.c:17:29: fatal error: gif_lib_private.h: No such file or directory\ncompilation terminated.\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\nUse --verbose_failures to see the command lines of failed build steps.\n```\n### Environment info\n\nOperating System: Ubuntu 16.04\n\nInstalled version of CUDA and cuDNN: N/A\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from sources, provide the commit hash: https://github.com/tensorflow/tensorflow/commit/a3f61c1d5c76339e6c9655dac426bb3822659772\n### Steps to reproduce\n1. bazel build //tensorflow/examples/android:tensorflow_demo\n   2.\n   3.\n### What have you tried?\n1. install the giflib from Ubuntu, but I understand that bazel downloads during the build process\n### Logs or other output that would be helpful\n\nWith max verbosity:\n\n```\nERROR: /home/arnaud/.cache/bazel/_bazel_arnaud/5d461d587a2aa81b3eca305842d75a99/external/gif_archive/BUILD:14:1: C++ compilation of rule '@gif_archive//:gif' failed: namespace-sandbox failed: error executing command \n  (cd /home/arnaud/.cache/bazel/_bazel_arnaud/5d461d587a2aa81b3eca305842d75a99/execroot/tensorflow && \\\n  exec env - \\\n    PATH=/home/arnaud/Documents/Python/anaconda2/bin:/home/arnaud/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/arnaud/.rvm/bin:/home/arnaud/.rvm/bin:/home/arnaud/Documents/Android/android-studio/bin \\\n  /home/arnaud/.cache/bazel/_bazel_arnaud/5d461d587a2aa81b3eca305842d75a99/execroot/tensorflow/_bin/namespace-sandbox @/home/arnaud/.cache/bazel/_bazel_arnaud/5d461d587a2aa81b3eca305842d75a99/execroot/tensorflow/bazel-sandbox/aeac231e-239e-4110-a556-7f20a424a56a-197.params -- /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wl,-z,-relro,-z,now -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 -DHAVE_CONFIG_H -iquote external/gif_archive -iquote bazel-out/host/genfiles/external/gif_archive -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -isystem external/gif_archive/giflib-5.1.4/lib -isystem bazel-out/host/genfiles/external/gif_archive/giflib-5.1.4/lib -isystem external/bazel_tools/tools/cpp/gcc3 -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -MD -MF bazel-out/host/bin/external/gif_archive/_objs/gif/external/gif_archive/giflib-5.1.4/lib/gif_err.d -c external/gif_archive/giflib-5.1.4/lib/gif_err.c -o bazel-out/host/bin/external/gif_archive/_objs/gif/external/gif_archive/giflib-5.1.4/lib/gif_err.o).\nexternal/gif_archive/giflib-5.1.4/lib/gif_err.c:10:29: fatal error: gif_lib_private.h: No such file or directory\ncompilation terminated.\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\n```\n", "comments": ["I'm not certain why this is happening yet, but running ./configure seems to make it go away for me.\n", "There is a dependency chain between //tensorflow/core:android_tensorflow_lib_lite and @gif_archive//:gif:\n\n> $ bazel query \"somepath(tensorflow/core:android_tensorflow_lib_lite, @gif_archive//:gif)\"\n> //tensorflow/core:android_tensorflow_lib_lite\n> //tensorflow/core:android_srcs\n> //tensorflow/core:proto_text_srcs_all\n> //tensorflow/tools/proto_text:gen_proto_text_functions\n> //tensorflow/core:lib\n> //tensorflow/core:lib_internal\n> //tensorflow/core/platform/default/build_config:platformlib\n> @gif_archive//:gif\n\n@martinwicke @raingo I think this may be from https://github.com/tensorflow/tensorflow/pull/3264\n\nAs gif support is unnecessary for the Android build, is there any easy way we can make the build proceed despite gif_archive not having been configured?\n", "platformlib also depends on png and jpeg -- are those also unnecessary in android? If so, we could split all image support out into a separate target, but it would be a fair amount of work to separate core:lib_internal into two (or more) targets.\n", "All that the Android build needs from platformlib is the proto_text_srcs_all output which depends on platformlib. It doesn't use the actual platformlib binary for compilation -- all srcs are compiled from scratch using the resulting android_srcs filegroup.\n\nSo if somehow we could reduce the complexity of generating proto_text_srcs_all, that would reduce Android issues going forward.\n", "@keveman can the protos be built without any of the other lib stuff? It seems this quickly turning into a requirement for mobile.\n", "`//tensorflow/core:proto_text_srcs_all` is not part the proto library, but something that @cwhipkey wrote. I doubt it can be built independent of `core:lib`, but maybe @cwhipkey can answer it better.\n", "`//tensorflow/core:proto_text_srcs_all` is generated by the cc_binary tool `///tensorflow/tools/proto_text:gen_proto_text_functions` (built for host, not android), which only uses the following basic TF includes:\n\n```\n#include \"third_party/tensorflow/core/lib/strings/str_util.h\"\n#include \"third_party/tensorflow/core/lib/strings/strcat.h\"\n#include \"third_party/tensorflow/core/platform/macros.h\"\n#include \"third_party/tensorflow/core/platform/types.h\"\n#include \"third_party/tensorflow/core/platform/init_main.h\"\n#include \"third_party/tensorflow/core/platform/logging.h\"\n#include \"third_party/tensorflow/core/platform/protobuf.h\"\n```\n\nThe entirety of platformlib apart from protos_cc should not be required for this binary, so maybe there is some way to refactor the targets involved.\n", "I think it's possible to get it down to just the dependency on protobuf.h\n(I have a change somewhere where I did that, but it was ultimately not\nneeded).  Would that help?  The protobuf.h code would still need to be made\navailable somehow.\n\nOn Mon, Jul 18, 2016 at 5:31 PM, Andrew Harp notifications@github.com\nwrote:\n\n> It's generated by the cc_binary tool gen_proto_text_functions, which only\n> uses the following basic TF includes:\n> #include \"third_party/tensorflow/core/lib/strings/str_util.h\"\n> #include \"third_party/tensorflow/core/lib/strings/strcat.h\"\n> #include \"third_party/tensorflow/core/platform/macros.h\"\n> #include \"third_party/tensorflow/core/platform/types.h\"\n> #include \"third_party/tensorflow/core/platform/init_main.h\"\n> #include \"third_party/tensorflow/core/platform/logging.h\"\n> #include \"third_party/tensorflow/core/platform/protobuf.h\"\n> \n> The entirety of platformlib apart from protos_cc should not be required\n> for this binary, so maybe there is some way to refactor the targets\n> involved.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3374#issuecomment-233498639,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AQw4wTIMRhpw5AqW_-mgPcWCNpet80S1ks5qXBr3gaJpZM4JPJoG\n> .\n", "@cwhipkey Yes, that was exactly what was needed. Starting from that CL I was able to get something building that fixes this issue by creating a minimal dependency tree for gen_proto_text_functions.\n", "I can confirm that the `./configure` works for me. So we can close this issue, or keep it open for reference, if someone wants to remove the unnecessary dependencies.\n", "I've submitted a fix internally; it should show up externally in the next push -- I'll close the issue when it does.\n", "Separate compilation path added for proto tool in https://github.com/tensorflow/tensorflow/commit/4e9a42b7d3764928bdb6625c26f2dbd8f0415834, so Android build should be simpler/faster now.\n"]}, {"number": 3373, "title": "Cannot use more than 50% of available memory for a single variable", "body": "I'm running Tensorflow 0.9.0 on a K40m (with 12GB of memory) with CUDA 7.5.0.\n\nI'm attempting to preload data as described here: https://www.tensorflow.org/versions/r0.9/how_tos/reading_data/index.html#preloaded-data\n\nTreating the data as a constant doesn't work because constants are limited to 2GB, and a minibatch approach does not make sense for my application.\n\nPerhaps Tensorflow wants to allocate space for the placeholder on GPU, allocate space for the variable, copy from host to placeholder (on GPU), and then copy from placeholder to variable?\n\nHere's a minimal failing test case. \n\nThe following program:\n\n``` python\nimport numpy as np\nimport tensorflow as tf\n\n# allocate 6GB of zeros                                                                                                                                                                                                                                                                   \n_data = np.zeros(1536 * (1 << 20), dtype=np.float32)\n\ndata_init = tf.placeholder(tf.float32, shape=_data.shape)\ndata = tf.Variable(data_init, trainable=False, collections=[])\n\nwith tf.Session() as sess:\n    sess.run(data.initializer, feed_dict={data_init: _data})\n```\n\nproduces the following output:\n\n```\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: Tesla K40m\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.745\npciBusID 0000:04:00.0\nTotal memory: 11.25GiB\nFree memory: 11.12GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x16034f0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties: \nname: Tesla K40m\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.745\npciBusID 0000:42:00.0\nTotal memory: 11.25GiB\nFree memory: 11.12GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 0 to device ordinal 1\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 1 to device ordinal 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y N \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   N Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:806] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40m, pci bus id: 0000:04:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:806] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K40m, pci bus id: 0000:42:00.0)\nI tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (256):       Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (512):       Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (1024):      Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (2048):      Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (4096):      Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (8192):      Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (16384):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (32768):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (65536):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (131072):    Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (262144):    Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (524288):    Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (1048576):   Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (2097152):   Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (4194304):   Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (8388608):   Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (16777216):  Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (33554432):  Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (67108864):  Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (134217728):         Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (268435456):         Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:656] Bin for 6.00GiB was 256.00MiB, Chunk State: \nI tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x4208f40000 of size 11344840448\nI tensorflow/core/common_runtime/bfc_allocator.cc:689]      Summary of in-use Chunks by size: \nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 1 Chunks of size 11344840448 totalling 10.57GiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] Sum Total of in-use chunks: 10.57GiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:698] Stats: \nLimit:                 11344840295\nInUse:                 11344840448\nMaxInUse:              11344840448\nNumAllocs:                       1\nMaxAllocSize:          11344840448\n\nW tensorflow/core/common_runtime/bfc_allocator.cc:270] *********************************************************xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nW tensorflow/core/common_runtime/bfc_allocator.cc:271] Ran out of memory trying to allocate 6.00GiB.  See logs for memory state.\nW tensorflow/core/framework/op_kernel.cc:909] Resource exhausted: OOM when allocating tensor with shape[1610612736]\nTraceback (most recent call last):\n  File \"tf_fail.py\", line 11, in <module>\n    sess.run(data.initializer, feed_dict={data_init: _data})\n  File \"/software/rhel7/tensorflow-0.9.0/lib/tensorflow/python/client/session.py\", line 372, in run\n    run_metadata_ptr)\n  File \"/software/rhel7/tensorflow-0.9.0/lib/tensorflow/python/client/session.py\", line 636, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/software/rhel7/tensorflow-0.9.0/lib/tensorflow/python/client/session.py\", line 708, in _do_run\n    target_list, options, run_metadata)\n  File \"/software/rhel7/tensorflow-0.9.0/lib/tensorflow/python/client/session.py\", line 728, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shape[1610612736]\n         [[Node: Variable/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Variable, _recv_Placeholder_0/_2)]]\nCaused by op u'Variable/Assign', defined at:\n  File \"tf_fail.py\", line 8, in <module>\n    data = tf.Variable(data_init, trainable=False, collections=[])\n  File \"/software/rhel7/tensorflow-0.9.0/lib/tensorflow/python/ops/variables.py\", line 211, in __init__\n    dtype=dtype)\n  File \"/software/rhel7/tensorflow-0.9.0/lib/tensorflow/python/ops/variables.py\", line 309, in _init_from_args\n    validate_shape=validate_shape).op\n  File \"/software/rhel7/tensorflow-0.9.0/lib/tensorflow/python/ops/gen_state_ops.py\", line 45, in assign\n    use_locking=use_locking, name=name)\n  File \"/software/rhel7/tensorflow-0.9.0/lib/tensorflow/python/ops/op_def_library.py\", line 704, in apply_op\n    op_def=op_def)\n  File \"/software/rhel7/tensorflow-0.9.0/lib/tensorflow/python/framework/ops.py\", line 2260, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/software/rhel7/tensorflow-0.9.0/lib/tensorflow/python/framework/ops.py\", line 1230, in __init__\n    self._traceback = _extract_stack()\n```\n", "comments": ["@zheng-xq: seems like an issue of bfc_allocator.  Could you take a look?  Thanks.\n", "@concretevitamin, the initializer is fundamentally a tf.Assign. So there will be at least two copies of the tensors in that operation, lhs is the variable, and rhs is the initial value. Since TensorFlow didn't get all the GPU memory, it only allocates about 11GB memory. So half of that is about 5.5GB. If you change the size of your variable to (1400 << (1 <<20)), it should work.\n\nShort of an in-place initialization kernel, there is no current way to save the second copy of tensor for initial values. In most cases, this is not a problem, since the memory is only temporary, and will be released immediately after the assignment. \n", "Sounds good.  @lightcatcher can you verify?\n", "`1350 * (1 << 20)` (5.27GiB) works while `1360 * (1 << 20)` (5.31GiB) does not. This confirms the cutoff is half of 10.57GiB mentioned by log (I'm assuming this the BFC allocator's memory pool size).\n\nA fix or at least a documented workaround for this bug (only being able to use 50% of GPU memory for a single variable) would be very nice. \nUsing full GPU memory for data enables efficient batch gradient descent as well as work on very large single training examples (such as a 2 hour video) without getting bottlenecked by PCI-e transfer speeds.\n\n@zheng-xq mentioned that variable initialization is a tf.Assign. Is it possible to do Assign across device boundaries (such as assigning a GPU variable lhs to a CPU tensor rhs)? This seems like it could take nearly identical code paths to whatever mechanism TF uses to move tensors between devices (in this case just a `cudaMemcpy`, no need for custom \"in-place initialization kernel\"). \n\nI'm not familiar with how the code actually looks, but if there's a \"receive\" node directly feeding into an \"assign\" node, maybe the \"receive\"'s data pointer can be initialized with the array underlying the variable for \"assign\", and then the \"assign\" can become a no-op as the \"receive\" tensor and the variable's data are aliased?\n", "> >  Is it possible to do Assign across device boundaries\n\n@lightcatcher, not at this point. All ops on a device are required to have all the input Tensors on that same device. \n\n> > on very large single training examples (such as a 2 hour video) \n\nYou can still use multiple very large tensors, such as around 1GB each. \n", "This requires large changes/re-designs.  I am not sure if we will get to it in the short term, so I'm marking as contributions welcome.\n\n(Workaround for your issue, of course, is to use `tf.zeros()`.  Although I suspect you don't just want to initialize with zeros?)\n", "@concretevitamin does `tf.zeros` use some kind of compact representation?\n", "@concretevitamin Correct, I don't want to initialize with zeros.\nIs there any way to do a partial assign? This would allow multiple assigns into a single large (GPU memory sized) variable.\nIf there's no partial assign, the only work around seems to be to use multiple variables (which makes the model code uglier).\n", "@lightcatcher, you can try tf.scatter_update and see if you can have better luck there. \n", "@yaroslavvb @concretevitamin Initializing a >50% GPU memory size array from `tf.zeros` does not work either.\n\n@zheng-xq Assuming a reasonable implementation of scatter_update, that would work if any type of initialization worked (or if there wasn't an error thrown while trying to use an uninitialized variable).\n\nThe following is my attempt to use tf.zeros and scatter_update. If I comment out the `sess.run(data.initializer)` line, I get an error about using a non-initialized variable. If I don't comment out the line, I get a very similar OOM error message to what I put in the initial issue description.\n\n``` python\nimport numpy as np\nimport tensorflow as tf\n\n# allocate 1K x 2M matrix (8GB)                                                                                                             \n_data = np.ones((1 << 10, 1 << 21), dtype=np.float32)\n\ndata = tf.Variable(tf.zeros(_data.shape), trainable=False, collections=[])\n\nidx_ph = tf.placeholder(tf.int32)\ndata_ph = tf.placeholder(tf.float32, shape=[_data.shape[1]])\nscatter_update_op = tf.scatter_update(data, idx_ph, data_ph)\n\nwith tf.Session() as sess:\n    sess.run(data.initializer)\n\n    # assign 1 row at a time                                                                                                                \n    for r in xrange(1 << 10):\n        sess.run(scatter_update_op, feed_dict={idx_ph: r, data_ph: _data[r, :]})\n\n    out = sess.run(data)\n```\n\nAre there any hacks to disable variable initialization checks? If not, seems like the problem is there's no copy free initialization of variables.\n", "I'm starting working on this.\n", "this line `data = tf.Variable(tf.zeros(_data.shape), trainable=False, collections=[])` will create two variables and an assign op, and `scatter_update_op` has no special treatment for memory usage. I thought only op's kernel would check tensor's initialization, I find sess.run will do it too.\n", "@lightcatcher Consider also using partitioned variables.  E.g. `tf.get_variable(..., partitioner=<...>)`.\n", "Thanks for the PR @suiyuan2009 ! :)\n\nFrom reading the diff, it looks like your `zero_initializer` op works in place. \nDoes `scatter_update` work as expected, only requiring memory for the update and not to the tensor being written into? \nIn other words, has anyone tested that I can move arbitrary data larger than half of GPU memory from the host into a variable on the GPU? An example is the code I posted on this issue on July 21 (which could be modified to use the `zero_initializer` op).\n", "I've tested on CPU, it truly reduced memory cost.\n"]}, {"number": 3372, "title": "Multi-GPU for LSTM", "body": "I am experimenting with multi-gpu feedforwarding for LSTM models, by feeding two batches of sequences through a LSTM model. Here are two cases I am testing:\n\n**Single-GPU**: simply pass the two batches sequentially on 1 GPU\n**Two-GPU**: 2 parallel GPUs where each GPU takes care of one batch. **My code is in the attached file** (.txt --> .py).\n\nHowever, instead of being 2X faster, the two-GPU case runs even slightly slower than the single-GPU case. It seems that even if I am initiating 2 GPU threads, the 2 batches are processed still sequentially, rather than in a parallel manner. \n\nDoes anyone see anything wrong with my code, or give any insight on how to do debugging? Your help would be highly appreciated.\n\n[lstm_multi_gpu.txt](https://github.com/tensorflow/tensorflow/files/370021/lstm_multi_gpu.txt)\n", "comments": ["Several issues I see:\n1. `log_device_placement=False,allow_soft_placement=True` are passed to your Session ctor.  You should flip both, so as to verify that indeed two GPUs are used, but not just one.\n2. For benchmark purposes, you should not just report one iter.  Best practices are to warm up for a few iters, then start timing (the trial runs), and report avg with std.\n", "Automatically closing due to lack of recent activity. Please reopen when further information becomes available.\n"]}, {"number": 3371, "title": "GridRNNCell -> RNN TypeError: unsupported operand type(s) for +: 'int'", "body": "I am unsure whether this is an issue in my code, or a bug in the contrib -> GridRNNCell.\n\nPlease see this SO post: http://stackoverflow.com/questions/38442025/tensorflow-grid-lstm-rnn-typeerror\n", "comments": ["Marking as community support.  \n", "@concretevitamin can you find the pull request for `GridRNNCell`?\n"]}, {"number": 3370, "title": "Tests for examples/learn", "body": "A little while ago I caught a bug with the [resnet example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/skflow/resnet.py) that could have been caught by an automated test.\n\nWhat do people think about bringing in some kind of automated tests to examples/skflow? What sort of tests should these be? I think the first step would be to simply run the code regularly. That would have caught the bug with the resnet example.\n", "comments": ["@ilblackdragon: can you comment on this, thanks.\n", "@gideonite That would be great. I think @zhangyaobit was planning to work on tests for some of the examples, but contributions definitely will be welcome.\n", "Yes, I actually added the \"first step\" to run code regularly :) \nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/skflow/examples_test.sh\n\nThis list only covers about half of the examples in examples/skflow though. I have moved to and are currently occupied by other projects, so contributions are welcome to add other examples!\n", "@zhangyaobit Awesome!\n", "@ilblackdragon if nobody in the TensorFlow org is going to work on this should I mark it 'contributions welcome'?\n", "I'm going to get started on it tonight. Been trying to get to it all week.\nSorry for the lack of communication.\n", "Stupid question: how do you run this test script? I can't find any documentation. I've done some grepping around the code base and determined that the most reasonable thing should be something like:\n\n```\nbazel test //tensorflow/examples/skflow\n```\n\nBut running this gives:\n\n```\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nERROR: no such target '//tensorflow/examples/skflow:skflow': target 'skflow' not declared in package 'tensorflow/examples/skflow' defined by /path/to/my/install/gideonite-tensorflow/tensorflow/examples/skflow/BUILD.\nINFO: Elapsed time: 0.727s\nERROR: Couldn't start the build. Unable to run tests.\n```\n", "@gideonite  You need to have py_test targets in the `tensorflow/examples/skflow/BUILD` file.\nFor example target `mnist_test`, then you can run it with `blaze test //tensorflow/examples/skflow:mnist_test`.\n", "Current status: we have script to run tests, but they are manual and long - which at the end is going to still lead to rotting of the code. Still pending a more robust solution.\n", "Closing since it's a bit open-ended. Feel free to open a new issue if we need to reconsider."]}, {"number": 3369, "title": "AttributeError: 'module' object has no attribute 'constant'", "body": "Hi everyone, I tried to run Tensorflow 0.9.0 with Python 3.4 but I've received this error:\n\nPython 3.4.5 |Continuum Analytics, Inc.| (default, Jul  2 2016, 17:47:57)\n[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\nimport tensorflow as tf\nhello = tf.constant('Hello, TensorFlow!')\nTraceback (most recent call last):\n File \"<stdin>\", line 1, in <module>\nAttributeError: 'module' object has no attribute 'constant'\n### Steps to reproduce\n1. Installed Anaconda 64-bit Python 3.5\n2. Installed Tensorflow following these [instructions](https://www.tensorflow.org/versions/r0.9/get_started/os_setup.html#anaconda-installation)\n### What have you tried?\n\nTried with Python 3.5 but no luck as well. Version 2.7 works fine but I need to use version 3.x as part of my coursework requirement.\n### Environment info\n\nOperating System: OSX 10.11.1\n", "comments": ["I'm also running into this problem, after having followed the instructions for installing TensorFlow via pip on www.tensorflow.org, under Python 3.5.2 in Debian unstable. Here's what `dir(tf)` says after I `import tensorflow as tf`:\n\n`['__doc__', '__loader__', '__name__', '__package__', '__path__', '__spec__']`\n\n`help(tf)` is similarly unhelpful, giving some simple boilerplate and nothing else.\n\nEdit: I note that I mistakenly installed TensorFlow for GPU processing, using the URL with `/gpu/` in it. I do not have CUDA toolkit or CuDNN installed. But uninstalling TensorFlow and installing it from the correct URL has not gotten things working.\n", "I just tried to follow the Anaconda 64-bit Python 3.4 on my Ubuntu, and it worked fine.  One thing to check: if you used `sudo` in the installation process, directly launching `python3` might fail to find tensorflow; instead try `sudo python3`.\n\nDid you use the GPU version or CPU-only version?\n", "`sudo python3` gives me a full tensorflow module. See my edit about RE the CPU version vs. the GPU version.\n\nIt looks like `/usr/local/lib/python3.5/dist-packages/tensorflow/` is set `drwxr-S---` for some bizarre reason. This might not be a problem with your packaging; if I install another third-party module (like flask) using `pip3 install`, I also have problems viewing its contents as a non-root user. I'll try using a virtualenv, instead.\n", "Actually, as per http://caligari.treboada.net/2014/09/23/python-pip-and-the-staff-group/, it should be sufficient to add your user to the group `staff` and then install with `pip/pip3 install` (rather than `sudo pip/pip3`).\n", "Since I'm using OSX, the binary provided is CPU-only therefore I'm not having any problem with GPU vs CPU version. \n\nOk, so far I got python 3.5 working by installing TF via pip. Which means it has something to do with the conda package manager maybe...? It works with python 2.7 so I'm not sure. Tried sudo python3, nothing worked.\n", "Looks like it has been workaround-ed.  Closing for now.\n", "In case this helps others:\n\nRelated: I had an issue (tensorflow 0.9), in that scripts stopped working: \" AttributeError: 'module' object has no attribute 'constant' \". Thanks to this answer [http://stackoverflow.com/questions/37383812/tensorflow-module-object-has-no-attribute-placeholder], I looked and noticed that I had a folder named \"tensorflow\". I renamed that folder, and my scripts are working again.\n", "In my case i had named the file as \"tensorflow.py\" which was the cause of above Error. I changed the file name which fixed the problem. Also deleted mt script file \"tensorflow.py\". ", "i have the same issue and I am not using a file name and nor am i under a source tree. problem goes away if i install tensorflow-gpu but comes back when i install native build from source using bazel. I also notice that __https://www.tensorflow.org/install/install_linux#InstallingAnaconda__ had a LOT more files than local pip package create from bazel.\r\nI have ubuntu 16 xenial with nvidia ge force 780i. I built local pip package following https://www.tensorflow.org/install/install_sources\r\ni have python as anaconda3.\r\n\r\nissue\r\npython\r\nPython 3.6.0 |Anaconda 4.3.1 (64-bit)| (default, Dec 23 2016, 12:22:00) \r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> hello = tf.constant(\"Hello, TensorFlow!\")\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute 'constant'\r\n>>> hello = tf.constant('Hello, TensorFlow!')\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute 'constant'\r\n\r\n\r\nThe directory permissions look ok as well where this package was distributed. Am i missing something?\r\n\r\n\r\n", "Same thing.  Simple direct install.  \r\n ```\r\n2017-08-02T07:02:19Z\r\n\r\n$ lsb_release -a\r\nNo LSB modules are available.\r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 17.04\r\nRelease:\t17.04\r\nCodename:\tzesty\r\n\r\n$ sudo su -l\r\n\r\n# pip3 install tensorflow\r\n       Collecting tensorflow\r\n  Downloading tensorflow-1.2.1-cp35-cp35m-manylinux1_x86_64.whl (34.5MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 34.5MB 45kB/s \r\nRequirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from tensorflow)\r\nCollecting backports.weakref==1.0rc1 (from tensorflow)\r\n  Downloading backports.weakref-1.0rc1-py3-none-any.whl\r\nCollecting numpy>=1.11.0 (from tensorflow)\r\n  Downloading numpy-1.13.1-cp35-cp35m-manylinux1_x86_64.whl (16.9MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16.9MB 91kB/s \r\nCollecting werkzeug>=0.11.10 (from tensorflow)\r\n  Downloading Werkzeug-0.12.2-py2.py3-none-any.whl (312kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 317kB 2.2MB/s \r\nRequirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow)\r\nCollecting protobuf>=3.2.0 (from tensorflow)\r\n  Downloading protobuf-3.3.0-cp35-cp35m-manylinux1_x86_64.whl (5.7MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.7MB 258kB/s \r\nCollecting html5lib==0.9999999 (from tensorflow)\r\n  Downloading html5lib-0.9999999.tar.gz (889kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 890kB 1.2MB/s \r\nCollecting bleach==1.5.0 (from tensorflow)\r\n  Downloading bleach-1.5.0-py2.py3-none-any.whl\r\nCollecting markdown>=2.6.8 (from tensorflow)\r\n  Downloading Markdown-2.6.8.tar.gz (307kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 317kB 2.8MB/s \r\nRequirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from protobuf>=3.2.0->tensorflow)\r\nBuilding wheels for collected packages: html5lib, markdown\r\n  Running setup.py bdist_wheel for html5lib ... done\r\n  Stored in directory: /root/.cache/pip/wheels/6f/85/6c/56b8e1292c6214c4eb73b9dda50f53e8e977bf65989373c962\r\n  Running setup.py bdist_wheel for markdown ... done\r\n  Stored in directory: /root/.cache/pip/wheels/85/a7/08/33ee5cd488d0365d8bed79d1d4e5c28dd3fbfc7f6d0ad4bb09\r\nSuccessfully built html5lib markdown\r\nInstalling collected packages: backports.weakref, numpy, werkzeug, protobuf, html5lib, bleach, markdown, tensorflow\r\n  Found existing installation: html5lib 0.999999999\r\n    Not uninstalling html5lib at /usr/lib/python3/dist-packages, outside environment /usr\r\nSuccessfully installed backports.weakref-1.0rc1 bleach-1.5.0 html5lib-0.9999999 markdown-2.6.8 numpy-1.13.1 protobuf-3.3.0 tensorflow-1.2.1 werkzeug-0.12.2\r\n\r\n^d\r\n $ Python 3.5.3 (default, Jan 19 2017, 14:11:04) \r\n [GCC 6.3.0 20170118] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n >>> python.el: native completion setup loaded\r\n >>> import tensorflow as tf\r\n >>> hello = tf.constant('Hello, TensorFlow!')\r\n\r\nTraceback (most recent call last):\r\nFile \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute 'constant'\r\n\r\n```\r\nas that didn't work, I went back and did the optional step 2, getting the storage file from the link as suggested:\r\n\r\n```\r\n# pip3 install tensoflow --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.2.1-cp35-cp35m-linux_x86_64.whl\r\nCollecting tensoflow\r\n  Could not find a version that satisfies the requirement tensoflow (from versions: )\r\nNo matching distribution found for tensoflow\r\n```\r\n\r\n\r\nSo what does this mean?  Is Tensorflow installed?  How should I proceed??", "@Antigonus, you're misspelling the command. You have tensoflow instead of tensorflow.\r\n\r\nIt should be;\r\n`sudo pip3 install tensorflow --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.2.1-cp35-cp35m-linux_x86_64.whl`\r\n\r\nIf you get another, similar error, attempt to install from a different package.\r\n\r\n`sudo pip3 install tensorflow --upgrade install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.2.1-cp35-cp35m-manylinux1_x86_64.whl`", "@faullath,   thank you!   I suspect the first problem was something about permissions having run pip3 as root.  (Other posts have noted such problems with pip3.)  That misspelling is incredible as that command was cut and paste from the dialog to the shell then to here. Somehow a character was deleted, or wasn't there in the first place.  I didn't think to examine it closely.   I ended up replacing Ubuntu 17  with a Debian Stretch,  ran pip3 as a user on the new system using a direct install by the first step (not the optional 2nd step),  and everything went smoothly.", "After directly installing with pip3 install tensorflow, I am struggling with the same problem.", "Run into the same error.\r\n**Reason**: File was named `logging.py`\r\n**Solution**: Rename file", "I installed tensorflow from Source had the same problem. I noticed that while executing the command \"sudo pip install /tmp/tensorflow_pkg/tensorflow-1.5.0-py2-none-any.whl\", I was executing it by copying and pasting it directly which was wrong because the term \"tensorflow-1.5.0-py2-none-any.whl\" need to be replaced with the right name of the .whl file. First time running it gave some error but when I ran it again, it worked like a charm. You can find it in location /tmp/tensorflow_pkg. Just putting it here hoping it would save someone else's load of time.", "@concretevitamin  sudo python helps!!\r\n\r\nI had an error\r\n\r\n> python\r\nPython 2.7.15 |Anaconda, Inc.| (default, May  1 2018, 23:32:55) \r\n[GCC 7.2.0] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/ujjval/anaconda2/lib/python2.7/site-packages/tensorflow/__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/ujjval/anaconda2/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/ujjval/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/ujjval/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/ujjval/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/ujjval/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n"]}, {"number": 3368, "title": "download tensorboard vis export to image files", "body": "Has there been discussion of exporting tensorboard visualizations as images (e.g. `.png`)? This would be helpful for rough drafts and work-in-progress type presentations.\n", "comments": ["We haven't discussed this much, it's not on the radar at the moment.\n", "The work around to this is just to download the data as csv and use something like excel to create visuals.\n", "Ah right. Awesome.\n\nFrom the currently running tutorial csv download appears to be broken. Should I open an issue?\n\nhttps://www.tensorflow.org/tensorboard/index.html#events\n\nI'm also a bit confused why the csv and json download buttons are hidden behind a few clicks. What's the idea behind that?\n", "Yeah seems like csv file didn't get generated. I would try it on local data to double check and if it still fails open an issue, as for the UI I have no idea. If you look here there is some helper scripts, but they sound sort of hacky atm and they make it sound like csv isn't supported yet.\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/README.md#how-can-i-export-data-from-tensorboard\n", "The easiest way I've found is to save the `<svg>` element to a .svg file, and then in-line all the CSS. There's a handy bookmarklet that does it for you, found here: http://nytimes.github.io/svg-crowbar/\r\n\r\nEither use the SVG file as-is, or convert it to PNG with a drawing program of your choice, or an online converter.", "CSV should be working fine in the current release, as should JSON.\r\n\r\n@CJxD, thanks for sharing that workaround! Like @dandelionmane said, this isn't really high-priority for us at this time, so I'm closing it for now. If you like, feel free to open an issue in our new repository at https://github.com/tensorflow/tensorboard/issues. Thanks!"]}, {"number": 3367, "title": "model with batchnorm runs overfit in multigpu training,but is ok in single gpu. mutilgpu model is based on cifar10_multi_gpu_train.py", "body": "1. mutil gpu code based on cifar10_multi_gpu_train.py(https://github.com/tensorflow/tensorflow/blob/d42facc3cc9611f0c9722c81551a7404a0bd3f6b/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py)\n2. two implementation of batch norm  is overfit in multigpu ,but is ok in sigle gpu.<br>\n   one implementation based on inception batch_norm function: (https://github.com/tensorflow/models/blob/master/inception/inception/slim/ops.py)\n   <br>\n   the other implementation base on :\n   https://codegists.com/snippet/python/batchnormpy_thouis_python\n", "comments": ["This does not sound like a bug in TensorFlow.  Maybe StackOverflow is a better venue?  Although, please do re-open if you've isolated more down to a TF component/op.\n"]}, {"number": 3366, "title": "bus error (core dumped), when build with GPU support", "body": "I followed the installing from source tutorial, however in step\n\n```\nbazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu\n```\n\nI encountered the following message:\n\n```\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\n[1]    5462 bus error (core dumped)  bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu\n```\n\nAny helps would be great\n### Environment info\n\nOperating System:  Ubuntu 16.04\n\nInstalled version of CUDA and cuDNN: CUDA 8.0, cuDNN 5\n\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n/usr/local/cuda/lib64/libcudadevrt.a\n/usr/local/cuda/lib64/libcudart.so\n/usr/local/cuda/lib64/libcudart.so.8.0\n/usr/local/cuda/lib64/libcudart.so.8.0.27\n/usr/local/cuda/lib64/libcudart_static.a\n/usr/local/cuda/lib64/libcudnn.so\n/usr/local/cuda/lib64/libcudnn.so.5\n/usr/local/cuda/lib64/libcudnn.so.5.0.5\n```\n\nIf installed from sources, provide the commit hash:\ne95f4e760c6b6713b6b686ebeff9a1586a5831dd\n### Steps to reproduce\n1. flowed the tutorial of installing Tensorflow from source \n### What have you tried?\n1. goolged around, but with no help\n", "comments": ["I don't think CUDA 8.0 is fully, if at all, supported yet.  CUDA 7.0 to 7.5 should work.\n"]}, {"number": 3365, "title": "Remove read_analogies() from word2vec class initialization", "body": "- Fix inconsistent between word2vec and word2vec_optimized.\n- Remove read_analogies() from word2vec class initialization.\n\nWhen I used word2vec model for training my target words directly, it lacked of analogies file for evaluation. Thus, it would raise file not found exception when model initialization. So I made read_analogies() public and called it after initialization.\n", "comments": ["Can one of the admins verify this patch?\n", "Is there any suggestions?\n", "It looks good to me.\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "Thanks for the pull request!\n"]}, {"number": 3364, "title": "dnn_autoencoder_iris.py example error", "body": "The example in skflow folder gives this error:\n\n```\nTraceback (most recent call last):\n  File \"/home/rmasad/test/dnn_autoencoder_iris.py\", line 33, in <module>\n    transformed = autoencoder.fit_transform(iris.data)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/base.py\", line 403, in fit_transform\n    return self.fit(x, y, monitor=None, logdir=None).transform(x)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/base.py\", line 394, in transform\n    x, axis=1, batch_size=None))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/base.py\", line 237, in predict\n    return self._predict(x, axis=axis, batch_size=batch_size)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/base.py\", line 211, in _predict\n    feed_fn=predict_data_feeder.get_feed_dict_fn())\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 566, in _infer_model\n    predictions = self._get_predict_ops(features)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 720, in _get_predict_ops\n    self._targets_info)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/tensor_signature.py\", line 133, in create_placeholders_from_signatures\n    return signatures.get_placeholder()\nAttributeError: 'NoneType' object has no attribute 'get_placeholder'\n```\n", "comments": ["@wicke: assigning to you to redirect.  Who is the owner of the `tensorflow/examples/skflow` directory?  \n", "Sorry, the autoencoder example is terribly out of date and we haven't deprecated it properly. We need to clean up the examples. This should be converted to use a custom `Estimator` until we add a new DNNAutoencoder. \n", "@sandersk has a change in progress which will fix this. \n"]}, {"number": 3363, "title": "embeddings tutorial has dead link to googlecode svn", "body": "The page:\n\nhttps://www.tensorflow.org/versions/r0.9/tutorials/word2vec/index.html#evaluating-embeddings-analogical-reasoning\n\nHas a dead (404) link to https://word2vec.googlecode.com/svn/trunk/questions-words.txt\n\nI'd submit a PR if I knew where it _should_ point, but I don't :)\n", "comments": ["The project has been archived on Google code and it seems that there's no simple way to link to the file. It also looks like there's no official Github mirror.\nHere's a link to an unofficial mirror, but I'm not sure whether it's a good idea to use this one:\nhttps://raw.githubusercontent.com/dav/word2vec/master/data/questions-words.txt\n\nI think the best solution would be to include the file in the TensorFlow repo itself right next to the word2vec example code.\nThe example code also needs to be adjusted, as it contains the old link:\nhttps://github.com/tensorflow/tensorflow/blob/r0.9/tensorflow/models/embedding/word2vec.py#L57\nLet's see what the tensorflow maintainers think.\n", "@martinwicke Can you take a look? Thanks.\n", "Fixed in r0.10. Data is now available at http://download.tensorflow.org/data/questions-words.txt .\n"]}, {"number": 3362, "title": "Using a batchsize greater than 1 when using C++ API", "body": "I have a Tensorflow model trained in Python and frozen with the freeze_graph script. I've succesfully loaded the model in C++ and made inference for a single image. However, whenever I attempt to feed the model a batch of images in C++ I get the error:\n\n`tensorflow/core/framework/tensor.cc:433] Check failed: 1 == NumElements() (1 vs. 16)Must have a one element tensor`\n\nIf I print the frozen graph def, the input placeholder seems to have the correct shape for batch inference:\n\n```\nnode {\n  name: \"inputs\"\n  op: \"Placeholder\"\n  attr {\n    key: \"dtype\"\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: \"shape\"\n    value {\n      shape {\n        dim {\n          size: 16\n        }\n        dim {\n          size: 50\n        }\n        dim {\n          size: 50\n        }\n        dim {\n          size: 3\n        }\n      }\n    }\n  }\n}\n```\n\nI'm using the `LoadGraph` function from the Tensorflow `label_image` C++ example, and I can't see anything in that function that should limit me to one image at a time.\n\nIs this a limitation of the C++ API or am I doing something wrong?\n### Environment info\n\nOperating System: Ubuntu 14.04\n\nInstalled version of CUDA and cuDNN: 7.5, 5.0.5\n\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`): \n\n```\n-rw-r--r-- 1 root root   322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root       19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root   383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root   720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a\n-rwxr-xr-x 1 root root 59909104 Jul 14 11:54 /usr/local/cuda/lib64/libcudnn.so\n-rwxr-xr-x 1 root root 59909104 Jul 14 11:54 /usr/local/cuda/lib64/libcudnn.so.5\n-rwxr-xr-x 1 root root 59909104 Jul 14 11:54 /usr/local/cuda/lib64/libcudnn.so.5.0.5\n-rw-r--r-- 1 root root 58775484 Jul 14 11:54 /usr/local/cuda/lib64/libcudnn_static.a\n```\n\nIf installed from sources, provide the commit hash: 115f5185e56c3fc4c8ada87c56651434f6be585c (r0.9)\n### Steps to reproduce\n1. Create and train model in python.\n2. Save model and checkpoint, and use `freeze_graph` to prepare for C++.\n3. Load model in C++ and feed it an input tensor with a batch size larger than 1.\n### What have you tried?\n1. Not using a variable batch size.\n2. Sanity checking the input placeholder shape of the frozen graph.\n", "comments": ["What is the stack trace corresponding to `tensorflow/core/framework/tensor.cc:433]`?  Can you add some LOG messages to see which node is creating issues?  \n", "So, I'm not that proficient at C++, but I tried using `gdb` to get something that looks like a stack trace (I added `-g -rdynamic` to the compiler flags and ran it in `gdb` with `run`):\n\n```\n(gdb) bt\n#0  0x00007fffddf0fc37 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56\n#1  0x00007fffddf13028 in __GI_abort () at abort.c:89\n#2  0x00007fffe93a5189 in tensorflow::internal::LogMessageFatal::~LogMessageFatal() () from include/tensorflow/bazel-bin/tensorflow/libtensorflow_cc.so\n#3  0x00007fffe920e6bc in tensorflow::Tensor::CheckIsAlignedAndSingleElement() const () from include/tensorflow/bazel-bin/tensorflow/libtensorflow_cc.so\n#4  0x000000000056d5aa in tensorflow::Tensor::scalar<float> (this=0x31330d0) at include/tensorflow/tensorflow/core/framework/tensor.h:524\n#5  0x000000000056bb7c in BlockFilter::getFrame (this=0x32f1dc0, t_seconds=0) at src/BlockFilter.cpp:193\n#6  0x000000000055c5fb in BlockCamera::getFrame (this=0x22137f0, t_seconds=0) at src/BlockCamera.cpp:35\n#7  0x000000000056f81e in BlockMixDev::getFrame (this=0x36bbc80, t_seconds=0) at src/BlockMixDev.cpp:16\n#8  0x000000000057dbc8 in main (argc=4, argv=0x7fffffffe3d8) at src/sportcaster.cpp:314\n```\n\nAs to the log messages, I'm afraid you'll have to give me a hint on how to do that - sorry! \n", "It appears that your code `BlockFilter::getFrame` seems to create a `Tensor`, populate it with 16 elements, and then call `tensor.scalar<float>()` on it.  \n\nI suspect this can be fixed once you change the `getFrame()` to not call `scalar<>()`.  (If you need to access the `i`-th element, call `flat<T>()(i)`.)\n", "I completely forgot about that -- I was calling `.scalar<float>` on the output tensor, which obviously won't work now that the tensor has a first dimension greater than one.\n\nThank you so much for the help!\n", "@ppries I realize that this thread isn't exactly for asking questions like this, but the TensorFlow c++ docs and examples are so sparse that I'll take help from just about anywhere at this point. I'm attempting to load a multi-layer CNN in c++ that I've successfully trained in python, but there aren't great references to see how this works. I've attempted this many different ways, and none of them seem to be going down the correct path. If you're comfortable with it, would you be able to share your python and c++ code with me for reference?\n", "Apologies for the late reply -- forgot about it over the weekend!\n\nI'm mostly just following [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc) example to load a Python trained graph in C++. Where are you having issues?\n", "Thanks for the response, but I went with the easier route of using python to serve requests to my model instead of c++ and NodeJS, which I had planned on before. The all-Python approach works great so I'm shelving the c++ version of TensorFlow until I can take more time to understand it. Again, I appreciate the response and good luck with your future projects!\n", "I'm having lots of trouble with simply getting the images in the right format to pass into the model. In that example it says:\n\n```\n\nconst Tensor& resized_tensor = resized_tensor[0]\nStatus run_status = session->Run({{input_layer, resized_tensor}}, {output_layer}, {}, &outputs);\n```\n\nWhich takes the return from ReadTensorFromImageFile and runs it through the model. How can I get it to take multiple images in a 4D array? Or would i do that inside ReadTensorFromImage before it subtracts the mean and divides by the scale? I'm having lots of trouble with the syntax. Please help. Thanks.\n", "Look at https://github.com/beniz/deepdetect/blob/tf/src/tflib.cc#L197 it may give you the solution.\n", "Thank you. That worked. But I'm not sure the graph from the tensorflow label_image c++ example can handle a batch. I get this error when I tried two images:\n\n```\nE tensorflow/examples/label_batch/main.cc:344] Running model failed: Invalid argument: Input to reshape is a tensor with 4096 values, but the requested shape has 2048\n     [[Node: pool_3/_reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](pool_3, pool_3/_reshape/shape)]]\n```\n\nIs there a way to modify the graph to take a batch? Or am I still making a mistake with the input?\n", "Read the TF FAQ, you need export / freeze the graph without specifying the batch size.\n", "Hi, look at the deepdetect `tf` branch, in tflib.cc file, somewhere in the `predict` function you will find the piece of code that puts images into a batch through a combat operation, and a dedicated session. I only have limited connectivity right now otherwise I'd have the exact link for you. I'll update the message when I can.\n\nEm.\n\n\u2063\n\nSent from BlueMail\n\n\u200b\n\nOn Oct 22, 2016, 11:40, at 11:40, ryanjay0 notifications@github.com wrote:\n\n> I'm having lots of trouble with simply getting the images in the right\n> format to pass into the model. In that example it says:\n> \n> const Tensor& resized_tensor = resized_tensor[0]\n> Status run_status = session->Run({{input_layer, resized_tensor}},\n> {output_layer}, {}, &outputs);\n> \n> Which takes the return from ReadTensorFromImageFile and runs it through\n> the model. How can I get it to take multiple images in a 4D array? Or\n> would i do that inside ReadTensorFromImage before it subtracts the mean\n> and divides by the scale? I'm having lots of trouble with the syntax.\n> Please help. Thanks.\n> \n> ## \n> \n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly or view it on GitHub:\n> https://github.com/tensorflow/tensorflow/issues/3362#issuecomment-255518147\n", "@ppries  Were you ever able to change the batch size in C++?", "Change batch size \u2013 no. But that should not be necessary if you freeze the graph with an unspecified graph size (e.g. a placeholder with dimensions [None, H, W, C]).", "@beniz,the link https://github.com/beniz/deepdetect/blob/tf/src/tflib.cc#L197 has not been existed", "@ppries ,how do you change one image to a batch of image, What changes have been made in the file of label_image/main.cc", "any luck? take a look at this [Link ](https://github.com/tensorflow/tensorflow/issues/8033) \r\n\r\n```\r\nstd::vector<TF_Tensor*> input_values;\r\ninput_values.push_back(tensor);\r\n...\r\n\r\nTF_SessionRun(session, nullptr,\r\n              &inputs[0], &input_values[0], inputs.size(),\r\n              &outputs[0], &output_values[0], outputs.size(),\r\n              nullptr, 0, nullptr, s);\r\n```", "@xt479910344 There you go https://github.com/beniz/deepdetect/blob/master/src/tflib.cc#L235"]}, {"number": 3361, "title": "Support streaming from hdf5", "body": "It would be nice if streaming HDF5 (which is required in out-of-core situations) would be implemented in Tensorflow. \n", "comments": ["This feature request is very broad, and we will likely not work on it in the foreseeable future. To keep the issue tracker focused, I will close this issue.\n", "Well, what I'm actually asking for is something along the lines of a `tf.TextLineReader` that supports both streaming / random access. The request came up before e.g. in #2089 . The problem with always closing these feature requests is that people who are looking for easy, new contributions might not see them, although they might be a good first step into the TF code base.\n", "+1. For reference, in https://www.tensorflow.org/api_guides/python/reading_data, the file format supported are only csv, binary and tfrecord. But hdf5 is a pretty common format. For big datasets, it is not possible to load a whole dataset with format .hdf5 once like this example. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/hdf5_classification.py. Instead, we use small hdf5 files for each sample.\r\n\r\nThe only feasible way to deal with this is to transfer hdf5 file to tfrecord or binary file first."]}, {"number": 3360, "title": "5:1: C++ compilation of rule '//tensorflow/stream_executor:stream_executor' failed:", "body": "I am using the latest branch of tensorflow \nError:/home/xxxxx/bazel/output/bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer --verbose_failures\n/home/xxxxx/bazel/output/bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer --verbose_failures\n/home/newSpace/projects2/tensorflow/tensorflow/stream_executor/BUILD:5:1: C++ compilation of rule '//tensorflow/stream_executor:stream_executor' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object ... (remaining 117 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\ntensorflow/stream_executor/cuda/cuda_dnn.cc: In function 'cudnnConvolutionFwdAlgo_t perftools::gputools::cuda::{anonymous}::ToConvForwardAlgo(perftools::gputools::dnn::AlgorithmType)':\ntensorflow/stream_executor/cuda/cuda_dnn.cc:266:10: error: 'CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING' was not declared in this scope\n     case CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING:\n          ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc: In function 'cudnnConvolutionBwdDataAlgo_t perftools::gputools::cuda::{anonymous}::ToConvBackwardDataAlgo(perftools::gputools::dnn::AlgorithmType)':\ntensorflow/stream_executor/cuda/cuda_dnn.cc:284:10: error: 'CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT_TILING' was not declared in this scope\n     case CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT_TILING:\n          ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc: In member function 'virtual bool perftools::gputools::cuda::CudnnSupport::GetConvolveAlgorithms(std::vector<long long int>*)':\ntensorflow/stream_executor/cuda/cuda_dnn.cc:942:7: error: 'CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING' was not declared in this scope\n       CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING,\n       ^\ntensorflow/stream_executor/cuda/cuda_dnn.cc:947:4: error: no matching function for call to 'std::vector<long long int>::assign(<brace-enclosed initializer list>)'\n   });\n    ^\n", "comments": ["Which version of cuDNN are you using?\ncuDNN v4 seems to be working at the moment.\n", "Please report your complete environment info (see other issues).  Thanks!\n", "ubuntu 14.04.4 LTS  \ncuda7.0 \ncudnn v4  \ngcc 4.9.4\n", "Thank you for your comments.I check my install steps again and find that I choose the cudnn 7.0,maybe it is the problem! when I change it to 4,it works.Thank you again.\n"]}, {"number": 3359, "title": "Gradient computation fails concatenation in while_loop() body", "body": "### Description\n\nWhen I was trying to implement RNN with while_loop(), I tried to concatenate output to a matrix.\nThis worked in forward passes but not in applying gradients.\n\nAlso, I saw that there are discussions (#2237) about supporting Recursive NN. There's a workaround by transforming tree structures to a matrix. For example, if we have a binary tree with its node values and structure be like\n\n```\n10------20------40\n |       |------50\n | \n |------30------60\n         |------70\n```\n\nThen we could transform it to a value vector V\n\n```\n[70 60 50 40 30 20 10]\n```\n\nand a (strictly bottom-up) structure matrix M\n\n```\n[[0 1 4]  # V[0] and V[1] are V[4]'s children\n [2 3 5]\n [4 5 6]]\n```\n\nThen we could build our graph with while_loop() by iteratively index into previous output. Note that this is without any specific inputs; only knowing they'll be a vector and a matrix instead.\nFor more details, see\nhttps://github.com/jacobvsdanniel/tf_rnn\ninspired by\nhttps://github.com/ofirnachum/tree_rnn\nI also ran into problems of computing gradients for nested gather inside while_loop(), but managed to worked around before fixes come up to #418 and #206 .\n### Tensorflow version\n\n0.9.0\n### Reproduction steps\n\n``` python\ndef test_concat_loop():\n    x = tf.constant([[1.,2.]])\n    X = tf.get_variable(\"X\", initializer=x)\n\n    i = tf.constant(0)\n    H = tf.zeros([0, 2])\n\n    def condition(i, H):\n        return i < 2\n\n    def body(i, H):\n        return i+1, tf.concat(0, [H, X])\n\n    _, H = tf.while_loop(condition, body, [i, H])\n    s = tf.reduce_sum(H)\n\n    sess = tf.Session()\n    sess.run(tf.initialize_all_variables())\n    print sess.run(X)\n    print sess.run(H)\n    print sess.run(s)\n\n    optimizer = tf.train.GradientDescentOptimizer(0.01)\n    op = optimizer.minimize(s) #Raise\n    print sess.run(op)\n    print sess.run(X)\n    return\n```\n### Workaround\n\n``` python\ndef test_concat_loop_workaround():\n    x = tf.constant([[1.,2.]])\n    X = tf.get_variable(\"X\", initializer=x)\n\n    i = tf.constant(0)\n    H = tf.zeros([5, 2])\n\n    def condition(i, H):\n        return i < 5\n\n    def body(i, H):\n        past = tf.zeros([i, 2])\n        future = tf.zeros([4-i, 2])\n        return i+1, H + tf.concat(0, [past, X, future])\n\n    _, H = tf.while_loop(condition, body, [i, H])\n    s = tf.reduce_sum(H)\n\n    sess = tf.Session()\n    sess.run(tf.initialize_all_variables())\n    print sess.run(X)\n    print sess.run(H)\n    print sess.run(s)\n\n    optimizer = tf.train.GradientDescentOptimizer(0.01)\n    op = optimizer.minimize(s)\n    print sess.run(op)\n    print sess.run(X)\n    return\n```\n### Error Logs\n\n```\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:924] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 980\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.329\npciBusID 0000:01:00.0\nTotal memory: 4.00GiB\nFree memory: 3.86GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:806] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0)\n[[ 1.  2.]]\n[[ 1.  2.]\n [ 1.  2.]]\n6.0\nTraceback (most recent call last):\n  File \"issue-418.py\", line 244, in <module>\n    main()\n  File \"issue-418.py\", line 233, in main\n    test_concat_loop()\n  File \"issue-418.py\", line 53, in test_concat_loop\n    op = optimizer.minimize(s)\n  File \"/home/danniel/.local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 193, in minimize\n    grad_loss=grad_loss)\n  File \"/home/danniel/.local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 250, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/home/danniel/.local/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 494, in gradients\n    in_grad.set_shape(t_in.get_shape())\n  File \"/home/danniel/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 404, in set_shape\n    self._shape = self._shape.merge_with(shape)\n  File \"/home/danniel/.local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 570, in merge_with\n    (self, other))\nValueError: Shapes (0, 2) and (1, 2) are not compatible\n```\n", "comments": ["Assigning to @ebrevdo: can you take a look?  Thanks.\n", "Have you tried using a TensorArray?  See tf.nn.dynamic_rnn for example usage.\n", "Line 498 of gradients.py is too strong for while loop:\n\n`in_grad.set_shape(t_in.get_shape())`\n\nIt looks like a bug introduced recently, and could potentially cause other problems.  We probably want to add a test case to guard this when we fix it.\n", "Can't quite understand neither TensorArray nor any rnn implementations based on RNNCell at this moment :| \n", "You're experiencing a bug because of the way `while_loop` gradients handle shapes of inputs and outputs.  If your input and output shapes are different, gradients can get confused.  For this reason, @yuanbyu will soon add a parameter called shape_invariants to while_loop, in which you can declare what shape information is static and what can change from iteration to iteration.  gradients will respect these shapes.  the default behavior will now require that shapes do not change from iteration to iteration (so your original code, without passing shape_invariants, would fail early with a useful error message)\n", "Expect a fix to show up in a week. However the fix requires the user to provide a shape invariant for a loop variable if its shape is changed by the loop body. \n", "This was fixed with https://github.com/tensorflow/tensorflow/commit/c46abae176717ef6c6649ba0ca1099c35b90194e.  Please reopen if you are still experiencing this bug with the nightlies/master branch.\n"]}, {"number": 3358, "title": "IOS - No OpKernel was registered to support Op 'Add' with these attrs", "body": "Hi everyone,\n\nWe are trying to load a very simple graph inside IOS \nRight now, we have this following error:\n\n```\nRunning model failed: Invalid argument: No OpKernel was registered to support Op 'Add' with these attrs\n     [[Node: deconv2d_1_1/add_3 = Add[T=DT_INT32](deconv2d_1_1/Squeeze_2, deconv2d_1_1/add_3/y)]]\n```\n", "comments": ["a code in which an error is thrown\n\n``` python\ndef deconvolution_2d(x, in_channels, out_channels, ksize, stride=1, pad=0, name=None):\n        with tf.variable_scope(name) as scope:\n            kernel = FastStyleNet.make_var('weights', [ksize, ksize, out_channels, in_channels])\n\n            def get_deconv_outsize(size, k, s, p, cover_all=False):\n                res = s * (size - 1) + k - 2 * p\n                return res\n\n            x_s = tf.shape(x)\n            x_s0 = x_s[0]\n            x_s012 = get_deconv_outsize(x_s, ksize, stride, pad)\n            output_shape = tf.pack(\n                [x_s0, x_s012[1], x_s012[2],\n                 out_channels])\n\n            return tf.nn.conv2d_transpose(x, kernel, output_shape, [1, stride, stride, 1])\n```\n", "@petewarden  Could you take a look at this please?\n", "Are you still hitting this error @gorBaghdasaryan ? Can you send me a link to a graphdef .pb file that I can try loading if so?\n", "Closing due to inactivity. Feel free to open a new github issue if the problem still persists in recent versions.", "Hi, I have the same issue.\r\n\r\nNo OpKernel was registered to support Op 'Add' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  device='CPU'; T in [DT_FLOAT]\r\n\r\n\t [[Node: resnet_v1_101/block4/unit_1/bottleneck_v1/conv2/convolution/required_space_to_batch_paddings/add = Add[T=DT_INT32](resnet_v1_101/block4/unit_1/bottleneck_v1/conv2/convolution/Gather, resnet_v1_101/block4/unit_1/bottleneck_v1/conv2/convolution/required_space_to_batch_paddings/strided_slice)]]\r\n\r\nI froze the graph using the python tool, and when I create a session loaging this graph in c++ it creashes with this error", "I have the same error. \r\nTensorflow version: 1.1.0", "@gepolv Did you solve the problem?  I met a similar error.\r\n\r\n> No OpKernel was registered to support Op 'Maximum' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  device='CPU'; T in [DT_FLOAT]\r\n\r\n\t [[Node: gradients/Mean_grad/Maximum = Maximum[T=DT_INT32, _output_shapes=[[]], _device=\"/device:CPU:0\"](gradients/Mean_grad/Prod_1, gradients/Mean_grad/Maximum/y)]]\r\n\r\n> "]}, {"number": 3357, "title": "./tensorflow/core/platform/jpeg.h:31:2: error: #error Define the appropriate PLATFORM_<foo> macro for this platform", "body": "I got a problem when building tensorflow from source in Raspberrypi platform. Anyone knows how to solve it ? Thanks \npi@raspberrypi:~/tensorflow/tensorflow $ bazel build -c opt --local_resources 1024,1.0,1.0 --verbose_failures tensorflow/tools/pip_package:build_pip_package\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nINFO: Found 1 target...\nERROR: /home/pi/tensorflow/tensorflow/tensorflow/core/BUILD:807:1: C++ compilation of rule '//tensorflow/core:lib_internal' failed: gcc failed: error executing command \n  (cd /home/pi/.cache/bazel/_bazel_pi/72651946b65f096261d7acfbb49ff852/tensorflow && \\\n  exec env - \\\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/games:/usr/games \\\n  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-std=c++0x' -DHAVE_CONFIG_H -iquote . -iquote bazel-out/host/genfiles -iquote external/protobuf -iquote bazel-out/host/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -iquote external/farmhash_archive -iquote bazel-out/host/genfiles/external/farmhash_archive -iquote external/jpeg_archive -iquote bazel-out/host/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/host/genfiles/external/png_archive -iquote external/gif_archive -iquote bazel-out/host/genfiles/external/gif_archive -iquote external/highwayhash -iquote bazel-out/host/genfiles/external/highwayhash -iquote external/re2 -iquote bazel-out/host/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/host/genfiles/external/eigen_archive -iquote external/zlib_archive -iquote bazel-out/host/genfiles/external/zlib_archive -isystem external/protobuf/src -isystem bazel-out/host/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/farmhash_archive/farmhash-34c13ddfab0e35422f4c3979f360635a8c050260 -isystem bazel-out/host/genfiles/external/farmhash_archive/farmhash-34c13ddfab0e35422f4c3979f360635a8c050260 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/host/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/host/genfiles/external/png_archive/libpng-1.2.53 -isystem external/gif_archive/giflib-5.1.4/lib -isystem bazel-out/host/genfiles/external/gif_archive/giflib-5.1.4/lib -isystem external/highwayhash -isystem bazel-out/host/genfiles/external/highwayhash -isystem external/re2 -isystem bazel-out/host/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/host/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-b4fa9622b809 -isystem bazel-out/host/genfiles/external/eigen_archive/eigen-eigen-b4fa9622b809 -isystem external/zlib_archive/zlib-1.2.8 -isystem bazel-out/host/genfiles/external/zlib_archive/zlib-1.2.8 -fno-exceptions -DEIGEN_AVOID_STL_ARRAY -pthread -no-canonical-prefixes -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/host/bin/tensorflow/core/_objs/lib_internal/tensorflow/core/lib/jpeg/jpeg_handle.o' -MD -MF bazel-out/host/bin/tensorflow/core/_objs/lib_internal/tensorflow/core/lib/jpeg/jpeg_handle.d -c tensorflow/core/lib/jpeg/jpeg_handle.cc -o bazel-out/host/bin/tensorflow/core/_objs/lib_internal/tensorflow/core/lib/jpeg/jpeg_handle.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nIn file included from ./tensorflow/core/lib/jpeg/jpeg_handle.h:22:0,\n                 from tensorflow/core/lib/jpeg/jpeg_handle.cc:22:\n./tensorflow/core/platform/jpeg.h:31:2: error: #error Define the appropriate PLATFORM_<foo> macro for this platform\n #error Define the appropriate PLATFORM_<foo> macro for this platform\n  ^\nIn file included from tensorflow/core/lib/jpeg/jpeg_handle.cc:22:0:\n./tensorflow/core/lib/jpeg/jpeg_handle.h:29:17: error: variable or field 'CatchError' declared void\n void CatchError(j_common_ptr cinfo);\n                 ^\n./tensorflow/core/lib/jpeg/jpeg_handle.h:29:17: error: 'j_common_ptr' was not declared in this scope\n./tensorflow/core/lib/jpeg/jpeg_handle.h:32:31: error: field 'pub' has incomplete type 'tensorflow::jpeg::jpeg_destination_mgr'\n   struct jpeg_destination_mgr pub;\n                               ^\n./tensorflow/core/lib/jpeg/jpeg_handle.h:33:3: error: 'JOCTET' does not name a type\n   JOCTET _buffer;\n   ^\n./tensorflow/core/lib/jpeg/jpeg_handle.h:40:26: error: field 'pub' has incomplete type 'tensorflow::jpeg::jpeg_source_mgr'\n   struct jpeg_source_mgr pub;\n                          ^\n./tensorflow/core/lib/jpeg/jpeg_handle.h:46:13: error: variable or field 'SetSrc' declared void\n void SetSrc(j_decompress_ptr cinfo, const void *data,\n             ^\n./tensorflow/core/lib/jpeg/jpeg_handle.h:46:13: error: 'j_decompress_ptr' was not declared in this scope\n./tensorflow/core/lib/jpeg/jpeg_handle.h:46:37: error: expected primary-expression before 'const'\n void SetSrc(j_decompress_ptr cinfo, const void *data,\n                                     ^\n./tensorflow/core/lib/jpeg/jpeg_handle.h:47:13: error: expected primary-expression before 'unsigned'\n             unsigned long int datasize, bool try_recover_truncated_jpeg);\n             ^\n./tensorflow/core/lib/jpeg/jpeg_handle.h:47:41: error: expected primary-expression before 'bool'\n             unsigned long int datasize, bool try_recover_truncated_jpeg);\n                                         ^\n./tensorflow/core/lib/jpeg/jpeg_handle.h:51:14: error: variable or field 'SetDest' declared void\n void SetDest(j_compress_ptr cinfo, void *buffer, int bufsize);\n              ^\n./tensorflow/core/lib/jpeg/jpeg_handle.h:51:14: error: 'j_compress_ptr' was not declared in this scope\n./tensorflow/core/lib/jpeg/jpeg_handle.h:51:36: error: expected primary-expression before 'void'\n void SetDest(j_compress_ptr cinfo, void *buffer, int bufsize);\n                                    ^\n./tensorflow/core/lib/jpeg/jpeg_handle.h:51:50: error: expected primary-expression before 'int'\n void SetDest(j_compress_ptr cinfo, void *buffer, int bufsize);\n                                                  ^\n./tensorflow/core/lib/jpeg/jpeg_handle.h:54:14: error: variable or field 'SetDest' declared void\n void SetDest(j_compress_ptr cinfo, void *buffer, int bufsize,\n              ^\n./tensorflow/core/lib/jpeg/jpeg_handle.h:54:14: error: 'j_compress_ptr' was not declared in this scope\n./tensorflow/core/lib/jpeg/jpeg_handle.h:54:36: error: expected primary-expression before 'void'\n void SetDest(j_compress_ptr cinfo, void *buffer, int bufsize,\n                                    ^\n./tensorflow/core/lib/jpeg/jpeg_handle.h:54:50: error: expected primary-expression before 'int'\n void SetDest(j_compress_ptr cinfo, void *buffer, int bufsize,\n                                                  ^\n./tensorflow/core/lib/jpeg/jpeg_handle.h:55:21: error: expected primary-expression before '_' token\n              string *destination);\n                     ^\n./tensorflow/core/lib/jpeg/jpeg_handle.h:55:22: error: 'destination' was not declared in this scope\n              string *destination);\n                      ^\ntensorflow/core/lib/jpeg/jpeg_handle.cc:32:17: error: variable or field 'CatchError' declared void\n void CatchError(j_common_ptr cinfo) {\n                 ^\ntensorflow/core/lib/jpeg/jpeg_handle.cc:32:17: error: 'j_common_ptr' was not declared in this scope\ntensorflow/core/lib/jpeg/jpeg_handle.cc:45:25: error: variable or field 'MemInitDestination' declared void\n void MemInitDestination(j_compress_ptr cinfo) {\n                         ^\ntensorflow/core/lib/jpeg/jpeg_handle.cc:45:25: error: 'j_compress_ptr' was not declared in this scope\ntensorflow/core/lib/jpeg/jpeg_handle.cc:177:1: error: expected '}' at end of input\n }  // namespace tensorflow\n ^\ntensorflow/core/lib/jpeg/jpeg_handle.cc:177:1: error: expected '}' at end of input\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nINFO: Elapsed time: 10.892s, Critical Path: 7.94s\n", "comments": ["@petewarden Pete, could you take a look?  Thanks.\n", "I think this is now obsolete, because we switched to libjpeg-turbo.\r\nPlease file a new issue if you still run into this problem."]}, {"number": 3356, "title": "can't construct gradient of gradient", "body": "In general, I've had no trouble constructing the gradient of a gradient in Tensorflow, but I hit on a weird edge case involving complex numbers where I couldn't.  Below, I show a case where the double gradient works, and one that is minimally different where it doesn't.\n\nHere's where double gradient works:\n\n``` python\nx = tf.Variable(tf.zeros([10]))\nxx = x * x\ng = tf.gradients(tf.reduce_sum(xx), x)\nh = tf.gradients(tf.reduce_sum(g), x)\n```\n\nAs expected, evaluating `h` produces:\n\n```\n>>> sess.run(h, feed_dict={x:10*[1.]})\n[array([ 2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.], dtype=float32)]\n```\n\nHere's where double gradient doesn't work:\n\n``` python\nx = tf.Variable(tf.zeros([10]))\nxx = tf.real(tf.complex(x*x, tf.zeros([10])))\ng = tf.gradients(tf.reduce_sum(xx), x)\nh = tf.gradients(tf.reduce_sum(g), x)\n```\n\nRunning the last line gives:\n\n```\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 494, in gradients\n    in_grad.set_shape(t_in.get_shape())\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 404, in set_shape\n    self._shape = self._shape.merge_with(shape)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 570, in merge_with\n    (self, other))\nValueError: Shapes (10,) and () are not compatible\n```\n\nI'm running version 0.9.0, CPU only, in OS X.\n", "comments": ["@girving Can you take a look?\n", "In the complex case `h = tf.gradients(tf.reduce_sum(g), x)` is trying to take the gradient of 2 reals w.r.t. 10 reals, which TensorFlow doesn't support.  We only support gradients of single real scalars.  I believe `h = tf.gradients(tf.real(tf.reduce_sum(g)), x)` should work fine.\n", "@girving, `xx` is a tensor of reals, so `g` is a gradient of a real w.r.t. reals.  Supporting this is the fact that the expression `tf.real(tf.reduce_sum(g))` produces the following error:\n\n```\n>>> tf.real(tf.reduce_sum(g))\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py\", line 398, in real\n    return gen_math_ops.real(input, Tout=input.dtype.real_dtype, name=name)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1516, in real\n    result = _op_def_lib.apply_op(\"Real\", input=input, Tout=Tout, name=name)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 530, in apply_op\n    _Attr(op_def, input_arg.type_attr))\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 61, in _SatisfiesTypeConstraint\n    \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\nTypeError: DataType float32 for attr 'T' not in list of allowed values: complex64, complex128\n```\n", "Whoops, you're right.  Apologies for misreading.\n", "Assigning to @mrry to take a look, although he's currently out. \n", "I think this was fixed by 946f515, which doesn't appear to be included in r0.9. It doesn't reproduce for me at HEAD, but it does with the 0.9 binary. Can you try upgrading to a nightly (or to the imminent r0.10rc0) and see if this is still a problem?\n", "I'm going to assume this is fixed now, and close the issue. Feel free to reopen if the problem persists!\n"]}, {"number": 3355, "title": "feature request - tf.stack", "body": "Given a tensor `x[i,j,k]` in a graph, I often want to construct tensors like `X[i,j,k,p,q] = x[i,j,k]`. \n\nYou can get this done using `tf.pack`, but it can get kind of messy if you are adding a bunch of dimensions.\n\nIt would be nice if there was a function `tf.stack(x,list)` which added dimensions to a tensor as above. The name is motivated by the following situation. If x is a matrix, then tf.stack(x,[n]) would create a 3-tensor by stacking the matrix up n times. I imagine that tf.stack would fit nicely under tf.tile in the documentation\n\nIt is possible that there is already a better way to do this than using tf.pack, but if so, it is not in the docs. \n", "comments": ["Try tf.tile ?\n", "If I understand correctly, tf.tile can't change the dimension of a tensor. What is the best way to cast a tensor of dimension `[a,b,c]` to a tensor of dimension `[a,b,c,1,1]`? It feels strange writting things like `tf.pack([tf.pack([x])])`\n", "You can use tf.expand_dims, then followed by tf.tile, perhaps!\n", "@vrv that works very well. Sorry for the stupid question and thanks for the fast response!\n", "Glad I could help, and no worries -- you might also ask these types of questions on StackOverflow -- the community is also quite helpful for these kind of modeling / programming questions. \n"]}, {"number": 3354, "title": "Problem trying to install iOS example", "body": "I get this problem when running `compile_ios_tensorflow.sh`\n\nI tried cloning TensorFlow again, download dependencies and `compile_ios_protobuf.sh` but the problem persists.\n\n```\nUndefined symbols for architecture armv7:\n  \"tensorflow::OptimizationPassRegistry::Global()\", referenced from:\n      tensorflow::SimpleGraphExecutionState::InitBaseGraph(tensorflow::BuildGraphOptions const&) in libtensorflow-core-armv7.a(simple_graph_execution_state.o)\n      tensorflow::SimpleGraphExecutionState::BuildGraph(tensorflow::BuildGraphOptions const&, std::__1::unique_ptr<tensorflow::SimpleClientGraph, std::__1::default_delete<tensorflow::SimpleClientGraph> >*) in libtensorflow-core-armv7.a(simple_graph_execution_state.o)\n  \"tensorflow::OptimizationPassRegistry::RunGrouping(tensorflow::OptimizationPassRegistry::Grouping, tensorflow::GraphOptimizationPassOptions const&)\", referenced from:\n      tensorflow::SimpleGraphExecutionState::InitBaseGraph(tensorflow::BuildGraphOptions const&) in libtensorflow-core-armv7.a(simple_graph_execution_state.o)\n      tensorflow::SimpleGraphExecutionState::BuildGraph(tensorflow::BuildGraphOptions const&, std::__1::unique_ptr<tensorflow::SimpleClientGraph, std::__1::default_delete<tensorflow::SimpleClientGraph> >*) in libtensorflow-core-armv7.a(simple_graph_execution_state.o)\nld: symbol(s) not found for architecture armv7\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\nmake: *** [/Users/Kevin/Developer/tensorflow/tensorflow/contrib/makefile/gen/bin/benchmark] Error 1\n+ '[' 2 -ne 0 ']'\n+ echo 'armv7 compilation failed.'\narmv7 compilation failed.\n+ exit 1\n\n```\n", "comments": ["@petewarden Could you take a look at this please?\n", "I'm getting the same error\n", "I am getting the same error too.. \n", "Same error found here\n", "Could this be due to the Xcode 8 beta? I have it installed and I'm not sure if it affects the command line tools.\n", "No, it's not linked to Xcode 8 beta. I faced the same xith Xcode 7.3\nIf you want to build the library, I'd advise you to checkout an older commit (e.g. 7520953ac6ee65f7cca8ab3f124fb2cb1629918e ) and then run build_all_ios.sh , it worked for me. But this issue is still open because the compilation fails with the current master tensorflow.\n", "I just checked in PR #3382 which fixes the immediate problem, and also makes it less likely that it will happen in the future. The summary is that we used to have an explicit list of files that were needed for the core library, and this was fragile because individual .cc files are often added, breaking the build. To make things more robust, I've switched over to a wildcard-based approach together with a blacklist of excluded files, which mirrors the Bazel build rule more closely and so should break less often.\n\nPlease give this a try if you get a chance and let me know how it goes.\n", "It works great now. \n![img_5043](https://cloud.githubusercontent.com/assets/7187949/16957491/26dc3e48-4dfa-11e6-881a-5b623c6227bd.jpg)\n", "It works for me now as well! thanks!\n", "I have installed everything correctly and I also ran the provided inception graph from image-net. I created my own graph which gives great predictions using label_image.py but all predictions are wrong using xcode simulator or camera..."]}, {"number": 3353, "title": "Fix whitespace issue in contrib/makefile/README.md", "body": "These paragraphs were indented by an extra space, causing Markdown rendering quirks that affected readability.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins Test this please\n"]}, {"number": 3352, "title": "iOS Example with fine-tuned inception network memory error & quantization attempt", "body": "For bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System: iOS\n### Steps to reproduce\n1. Follow the contrib/makefile/README to install the tensorflow iOS core lib\n2. Create my own model using flower's example and shards (https://github.com/tensorflow/models/tree/master/inception)\n3. Create inference graph by removing shard preprocessing ops and replacing them with:\n   input_ = tf.placeholder(tf.float32, shape=(1, FLAGS.image_size, FLAGS.image_size, 3),\n            name='input')\n   logits, _ = inception_model.inference(input_, FLAGS.num_classes + 1)\n   softmax = tf.nn.softmax(logits, name='softmax')\n   \n   And loading a saved checkpoint, restoring it, then writing out a graph def.\n4. Apply tensorflow/python/tools/freeze_graph on  graph def\n5. Apply tensorflow/python/tools/strip_unused on graph def\n6. Copy graph def and labels file into camera example\n7. Change objective-C image preprocessing to 299x299, with std & image_mean of 128.\n### What have you tried?\n1. Running the 'simple' example on the grace hopper image. This works fine\n2. Running the 'camera' example. This crashes after 3 seconds of the app being open with the output below:\n### Logs or other output that would be helpful\n\n/tensorflow/tensorflow/contrib/ios_examples/skinscan/tensorflow_utils.mm:130] Session created.\n/tensorflow/tensorflow/contrib/ios_examples/skinscan/tensorflow_utils.mm:133] Graph created.\ntensorflow/tensorflow/contrib/ios_examples/skinscan/CameraExampleViewController.mm:306] Running model failed:Invalid argument: Session was not created with a graph before Run()!\n2016-07-17 15:47:44.317 CameraExample[1109:39568] Received memory warning.\n\nIt appears that this issue is similar to #2927 . I tried quantizing the graph (tensorflow/contrib/quantization/tools/quantize_graph with --mode=weights), but then ran into a 'quantize ops dont exist' type of error on iOS. It seems those ops need to be built by modifying tensorflow/contrib/makefile/tf_cc_files.txt and tensorflow/contrib/makefile/proto_text_pb_h_files.txt. I have attempting including into both every non-test file and header located in tensorflow/contrib/quantization/kernels, but am running into build-rule errors from the makefile. Unfortunately, I have little knowledge of adjusting makefiles for this type of application.\n", "comments": ["@petewarden Could you take a look at this please?\n", "I'm seeing this exact issue when attempting to run a fine-tuned Inception v3 network. Also tried quantizing and ran into the same issue. \n\nPlease note I am working with commit #3382 since the latest commit (#3288) results in the following error:\n- curl https://bitbucket.org/eigen/eigen/get/.tar.gz -o /tmp/eigen-.tar.gz\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  100 26011  100 26011    0     0  37142      0 --:--:-- --:--:-- --:--:-- 37105\n- tar xzf /tmp/eigen-.tar.gz -C tensorflow/contrib/makefile/downloads\n  tar: Unrecognized archive format\n  tar: Error exit delayed from previous errors.\n\n...as a result of 'archive_dir = \"eigen-eigen-bf4....\" having been removed from tensorflow/eigen.BUILD\n", "I've got a new tutorial up at https://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/ that shows how to process the retrained model to avoid memory issues and run successfully. I'm hoping this solves these problems, so I'm closing this bug. Please re-open if you aren't able to get around the original issue with this approach.\n"]}, {"number": 3351, "title": "Fix gradient of prod op using cumprod", "body": "This is an initial shot at fixing #2641 (NaNs in the gradient of `tf.reduce_prod` when the input is `0`).\n@girving suggested to use two calls to `cumprod` and to multiply the outputs to produce an array containing the products of all values without the current one, like this:\n\n```\ninput: [0, 1, 2, 3, 4]\ncumprod1: [1, 0, 0, 0, 0]\ncumprod2 (reverse): [24, 24, 12, 4, 1]\ngradient: [24, 0, 0, 0, 0]\n```\n\nThe difficulty with this is handling the fact that `tf.reduce_prod` can calculate the product over a subset of the tensor dimensions.\nWhen solving this in Python, quite a bit of transposing and reshaping is required.\nThe idea is to reshape the input into a tensor with shape `[N, M]`, where the first dimension contains all the entries that we reduced over, and the second dimension contains the remaining ones.\nI've pushed a Python solution using the approach that makes the reduction op tests pass.\nAs you can see, the gradient code is a bit complicated.\nMaybe someone knows a shortcut that makes this simpler?\n\nThere might be other solutions that could be better here:\n@benoitsteiner suggested that `tf.cumprod` could be changed to take a list of dimensions to scan over instead of a single axis. This would bring it more in line with the reduction ops.\nI think this could be implemented without modifying Eigen, by using various Eigen methods to reshape and transpose the input.\n", "comments": ["Can one of the admins verify this patch?\n", "Looks good besides the testing comment.  I don't see any obvious simplifications.\n", "Okay, I've expanded the tests and also added a few more comments.\n", "Jenkins, test this please.\n", "Awesome, nice to have this fixed!\n", "Cheers \ud83c\udf89 \n", "#3957 seems to be related to this commit.\n"]}, {"number": 3350, "title": "No GPU implementation of determinant and no gradient of determinant?", "body": "We do not have GPU implementation of determinant calculation and its gradient is not available? It is really nice to have them.\n\nThanks!\n", "comments": ["Thanks for reporting this.  We most likely will not work on it in the near future.  Marking as \"contributions welcome\".\n", "[Issue on computing log det by Cholesky decomposition](https://github.com/tensorflow/tensorflow/issues/367). Applies to also regular determinant.\n", "Thanks for pointing it out!\n\nOn 4 August 2016 at 03:00, Heikki Arponen notifications@github.com wrote:\n\n> Issue on computing log det by Cholesky decomposition\n> https://github.com/tensorflow/tensorflow/issues/367. Applies to also\n> regular determinant.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3350#issuecomment-237336651,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ACUNSfgLt9PIbmomYKbkx_QWyl324eFkks5qcOVDgaJpZM4JOLs_\n> .\n", "Using tf.matrix_determinant() in the compute graph for a GPU is not currently possible?\n", "It is now (I think), but I guess usually people would like to compute e.g. log det instead of det, and computing det first and then taking log is numerically very unstable. So one would prefer to use e.g. the Cholesky decomposition for symmetric pos def matrices or e.g. LU decomp in the general case.\n\nEDIT: I take that back, sorry... I meant the gradient exists, but I guess there's no CUDA code for the det(?)\n", "Does tensorflow have an op for log det or the ops required to do it yourself?\n", "As mentioned by RuiShu in the issue 367 thread, you can compute `logdetA = 2*sum(log(diag(chol(A))))` (for a symmetric pos def matrix, such as a correlation matrix), which currently should now be doable with tf ops which also have gradients. Haven't tried that out myself though (I would need logdet for general non-symmetric matrices anyway).\n", "Yes likewise, having the pos def constraint is of no use to me. LU decomposition is not differentiable? Or simply not doable with tf ops?\n", "Well basically LU decomposition would be used for computing the _value_ of the logdet(M), while there's an explicit expression for the logdet gradient as inv(transpose(M))... so it would be enough if there was a fast GPU op for e.g. LU decomposition (there was some talk in some of the issues about SVD op coming soon, which would also do the trick). Then it would be simple to define a tf op for the logdet and register the gradient as the inverse transpose. So the LU/cholesky etc. wouldn't need to be differentiable (I actually don't know what's the point in having a differentiable Cholesky etc., but then again I haven't read the paper cited in the other issue).\n\nIf an LU/SVD op comes up, I could probably work out the logdet...\n\nEDIT: also, another issue could be the numerical stability of the tf.inverse... :/ Haven't thought about that yet\n", "@harpone What is the latest progress of the tf op for logdet? I found that the SVD op has been implemented but not its gradient yet. Also do you know how can I see the code for determinant in TF?", "@EverettYou to see the code search for REGISTER_OP(\"Determinant\") in the repo. ", "I think the GPU version of the determinant has been added recently by commit (https://github.com/tensorflow/tensorflow/commit/7d0afb0c7ab86c169fdf12c841155409b89126b9)?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 3349, "title": "-mavx2 or -mavx prevents eigen from compiling", "body": "I'm compiling the master branch of TensorFlow as follows:\n\n> bazel build -c opt --copt=-mavx2 --config=cuda //tensorflow/tools/pip_package:build_pip_package\n\nand receive the error:\n\n> ERROR: /tmp/tensorflow/tensorflow/contrib/ffmpeg/BUILD:22:1: C++ compilation of rule '//tensorflow/contrib/ffmpeg:decode_audio_op_cc' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object ... (remaining 57 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n> In file included from ./third_party/eigen3/unsupported/Eigen/CXX11/FixedPoint:36:0,\n>                  from ./tensorflow/core/framework/types.h:27,\n>                  from ./tensorflow/core/framework/type_traits.h:22,\n>                  from ./tensorflow/core/framework/allocator.h:25,\n>                  from ./tensorflow/core/framework/op_kernel.h:22,\n>                  from tensorflow/contrib/ffmpeg/decode_audio_op.cc:23:\n> ./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:347:8: error: 'scalar_multiple2_op' is not a class template\n>  struct scalar_multiple2_op<QInt32, double> {\n>         ^\n> ./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:347:44: error: explicit specialization of non-template 'Eigen::internal::scalar_multiple2_op'\n>  struct scalar_multiple2_op<QInt32, double> {\n>                                             ^\n> ./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:369:23: error: 'Eigen::internal::scalar_multiple2_op' is not a template\n>  struct functor_traits<scalar_multiple2_op<QInt32, double>> {\n>                        ^\n> In file included from ./tensorflow/core/framework/op_kernel.h:22:0,\n>                  from tensorflow/contrib/ffmpeg/decode_audio_op.cc:23:\n> ./tensorflow/core/framework/allocator.h: In member function 'virtual std::size_t tensorflow::Allocator::RequestedSize(void_)':\n> ./tensorflow/core/framework/allocator.h:155:3: warning: control reaches end of non-void function [-Wreturn-type]\n>    }\n>    ^\n> In file included from ./tensorflow/core/framework/op_kernel.h:25:0,\n>                  from tensorflow/contrib/ffmpeg/decode_audio_op.cc:23:\n> ./tensorflow/core/framework/device_base.h: In member function 'virtual tensorflow::Allocator_ tensorflow::DeviceBase::GetAllocator(tensorflow::AllocatorAttributes)':\n> ./tensorflow/core/framework/device_base.h:151:3: warning: control reaches end of non-void function [-Wreturn-type]\n>    }\n>    ^\n> ./tensorflow/core/framework/device_base.h: In member function 'virtual const tensorflow::DeviceAttributes& tensorflow::DeviceBase::attributes() const':\n> ./tensorflow/core/framework/device_base.h:182:3: warning: control reaches end of non-void function [-Wreturn-type]\n>    }\n>    ^\n\n/proc/cpuinfo shows my CPU has avx2 support:\n\n> processor       : 31\n> model name      : Intel(R) Xeon(R) CPU E5-2630 v3 @ 2.40GHz\n> flags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb pln pts dtherm tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 **avx2** smep bmi2 erms invpcid cqm xsaveopt cqm_llc \n", "comments": ["I met with the same problem, and after I copied the declaration of the template scalar_multiple2_op from eigen, the problem is fixed. Here is the template: \n\n```\ntemplate<typename Scalar1, typename Scalar2>\nstruct scalar_multiple2_op {\n  typedef typename scalar_product_traits<Scalar1,Scalar2>::ReturnType result_type;\n  EIGEN_STRONG_INLINE scalar_multiple2_op(const scalar_multiple2_op& other) : m_other(other.m_other) { }\n  EIGEN_STRONG_INLINE scalar_multiple2_op(const Scalar2& other) : m_other(other) { }\n  EIGEN_STRONG_INLINE result_type operator() (const Scalar1& a) const { return a * m_other; }\n  typename add_const_on_value_type<typename NumTraits<Scalar2>::Nested>::type m_other;\n};\n\n```\n", "I had this problem on Ubuntu 14.04, but not on 16.04; 16.04 has a newer version of Eigen (3.3), which, according to the Eigen website, adds AVX support. However, I've compiled on 14.04 with AVX in the past, so I'm not sure why this is suddenly a problem now.\n", "My apologies for missing this. Looking...\n", "@mccajm - Can you try this again on latest master? Should be fixed now (see #3792).\n", "I had the same problem and now it's fixed for me.\n", "Can we ge this backported to r0.10 branch? It does not compile in Ubuntu 16.04 LTS.\n"]}, {"number": 3348, "title": "tf.gradients() gives the conjugate of what is expected", "body": "`tf.gradients()`, when used on complex numbers, erroneously flips the sign of the imaginary part:\n\n```\n>>> x = tf.Variable(0. + 0.j)\n>>> sess.run(tf.gradients(x*x, x), feed_dict={x:0.1j})\n[-0.20000000000000001j]\n>>> sess.run(tf.gradients(tf.exp(x), x), feed_dict={x:0.1j})\n[(0.99500416527802571-0.099833416646828155j)]\n```\n\nI expect `0.2j` and `0.99500416527802571+0.099833416646828155j`.\n\nI'm running version 0.9.0, CPU only, on OS X.\n", "comments": ["@girving Could you take a look at this please?\n", "The gradient of a holomorphic function is the conjugate of its complex derivative.\n", "@girving can you explain this statement?\n\nUsing the complex analogue of the definition of the derivative, for `f(z) = z*z`, `f'(z) = z` as one would expect (for `z` complex). \n\nFor the derivative of `f(z)` in terms of its partial derivatives with respect to the real and imaginary components of `z` (referring to these partial derivatives as  `f_x` and `f_y`), you get `f'(z) = 0.5*(f_x - j*f_y)`, which sort of looks like a conjugate, but still would return `0.2j` in the first example.\n\nWhat am I missing here?\n", "I don't have time to show you the proof, but you can check yourself that if `w = f(z)`, then `dL/dz = conj(f'(z)) dL/dw` for gradients w.r.t. a loss `L` and `f'(z)` the complex derivative.\n", "I'll attempt to clarify for any future readers who also get confused by this.\r\n\r\n### tl;dr\r\nI'm reasonably confident the \"gradient\" returned by `tf.gradients` is `conj(df/dz + dconj(f)/dz)` (which reduces to `conj(df/dz)` for holomorphic `f`).\r\n\r\n### More details\r\nThe \"gradient\" mentioned by girving as the conjugate of the derivative is (related to) the gradient of the corresponding real map, when we express that gradient as a complex value. To expand on that, by definition of the Wirtinger derivative we have `df/dz = 0.5 * (df/dx - i df/dy)` (where `x` and `y` are the real and imaginary parts of `z`). If `f` is real-valued (e.g. a loss), then we have `conj(df/dz) = 0.5 * (df/dx + i df/dy)`, and `df/dx + i df/dy` in Cartesian coordinates is `(df/dx, df/dy)`, which is the gradient of `f` when considered as a map between real spaces.\r\n\r\nThat's not the usual definition of \"gradient\" of a complex map though--in my experience \"gradient\" is synonymous with \"derivative\". The terms certainly seem to be used interchangeably in the `tf.gradients` [docs](https://github.com/tensorflow/tensorflow/blob/5e2a91f65cfb23f996136b9201d9312c9c36b941/tensorflow/python/ops/gradients_impl.py#L53) (e.g \"`gradients()` adds ops to the graph to output the derivatives of `ys` with respect to `xs`\").\r\n\r\nI imagine the reason for using that definition of \"gradient\" is because TF is generally concerned with gradients in order to find the direction of maximum increase, and the direction of maximum increase is `(df/dx, df/dy)`. FWIW I would have thought it was more reasonable to define the \"gradient\" as `df/dz + dconj(f)/dz`, and then take the conjugate in the optimiser when deciding on the direction of maximum increase, but I expect that ship has sailed.\r\n\r\n### Even more details\r\nTo expand further, for anybody who's interested, another way to write the computed quantity is `dR(f)/dx + i * dR(f)/dy`, or in Cartesian coordinates, `(dR(f)/dx, dR(f)/dy)`. That is, it's the gradient of the real part of the function, when viewed as a function of real variables. If you run an optimisation using a complex function, therefore, what you'll end up optimising is the real part of the function.\r\n\r\nThe operator `D(f) := conj(df/dz + dconj(f)/dz` turns out to obey pretty standard chain and product rules, so the auto-differentiation still goes through correctly once the gradient functions are modified appropriately. Specifically, defining `D(C, f) := conj(conj(C) * df/dz + C * dconj(f)/dz)`, which represents the \"gradient\" of `f`(as defined before) with accumulation `C` (which is the form of the TF gradient methods, where `C` is `grad` and `f` is the function whose gradient is being defined), one can easily verify:\r\n```\r\nD(1, f) = D(f) [the initial state of the auto-differentiation]\r\nD(C, z) = C [the end/base state]\r\nD(C, f o g) = D(conj(conj(C) * df/dg + C * dconj(f)/dg), g) [chain rule]\r\n``` \r\nImplementing one of the gradient functions therefore boils down to evaluating that expression in the chain rule for the function in question. For example, for z -> conj(z) we have\r\n```\r\nconj(conj(C) * dconj(z)/dz + C * dconj(conj(z))/dz) = conj(C),\r\n```\r\nconsistent with the [code](https://github.com/tensorflow/tensorflow/blob/70fd0a4436e3b49139653dc5b85d1c7df23f403d/tensorflow/python/ops/math_grad.py#L1583).\r\n\r\nFor a holomorphic function (multiplication, exp, etc...) the expression simply reduces to `C * conj(df/dg)` (because `dconj(f)/dg` vanishes for holomorphic f), which is the reason for all the conjugation introduced [here](https://github.com/tensorflow/tensorflow/commit/821063df9f0e6a0eec8cb78cb0ddc5c5b2b91b9f).", "> I'll attempt to clarify for any future readers who also get confused by this.\r\n> \r\n> ### tl;dr\r\n> I'm reasonably confident the \"gradient\" returned by `tf.gradients` is `conj(df/dz + dconj(f)/dz)` (which reduces to `conj(df/dz)` for holomorphic `f`).\r\n> \r\n> ### More details\r\n> The \"gradient\" mentioned by girving as the conjugate of the derivative is (related to) the gradient of the corresponding real map, when we express that gradient as a complex value. To expand on that, by definition of the Wirtinger derivative we have `df/dz = 0.5 * (df/dx - i df/dy)` (where `x` and `y` are the real and imaginary parts of `z`). If `f` is real-valued (e.g. a loss), then we have `conj(df/dz) = 0.5 * (df/dx + i df/dy)`, and `df/dx + i df/dy` in Cartesian coordinates is `(df/dx, df/dy)`, which is the gradient of `f` when considered as a map between real spaces.\r\n> \r\n> That's not the usual definition of \"gradient\" of a complex map though--in my experience \"gradient\" is synonymous with \"derivative\". The terms certainly seem to be used interchangeably in the `tf.gradients` [docs](https://github.com/tensorflow/tensorflow/blob/5e2a91f65cfb23f996136b9201d9312c9c36b941/tensorflow/python/ops/gradients_impl.py#L53) (e.g \"`gradients()` adds ops to the graph to output the derivatives of `ys` with respect to `xs`\").\r\n> \r\n> I imagine the reason for using that definition of \"gradient\" is because TF is generally concerned with gradients in order to find the direction of maximum increase, and the direction of maximum increase is `(df/dx, df/dy)`. FWIW I would have thought it was more reasonable to define the \"gradient\" as `df/dz + dconj(f)/dz`, and then take the conjugate in the optimiser when deciding on the direction of maximum increase, but I expect that ship has sailed.\r\n> \r\n> ### Even more details\r\n> To expand further, for anybody who's interested, another way to write the computed quantity is `dR(f)/dx + i * dR(f)/dy`, or in Cartesian coordinates, `(dR(f)/dx, dR(f)/dy)`. That is, it's the gradient of the real part of the function, when viewed as a function of real variables. If you run an optimisation using a complex function, therefore, what you'll end up optimising is the real part of the function.\r\n> \r\n> The operator `D(f) := conj(df/dz + dconj(f)/dz` turns out to obey pretty standard chain and product rules, so the auto-differentiation still goes through correctly once the gradient functions are modified appropriately. Specifically, defining `D(C, f) := conj(conj(C) * df/dz + C * dconj(f)/dz)`, which represents the \"gradient\" of `f`(as defined before) with accumulation `C` (which is the form of the TF gradient methods, where `C` is `grad` and `f` is the function whose gradient is being defined), one can easily verify:\r\n> \r\n> ```\r\n> D(1, f) = D(f) [the initial state of the auto-differentiation]\r\n> D(C, z) = C [the end/base state]\r\n> D(C, f o g) = D(conj(conj(C) * df/dg + C * dconj(f)/dg), g) [chain rule]\r\n> ```\r\n> \r\n> Implementing one of the gradient functions therefore boils down to evaluating that expression in the chain rule for the function in question. For example, for z -> conj(z) we have\r\n> \r\n> ```\r\n> conj(conj(C) * dconj(z)/dz + C * dconj(conj(z))/dz) = conj(C),\r\n> ```\r\n> \r\n> consistent with the [code](https://github.com/tensorflow/tensorflow/blob/70fd0a4436e3b49139653dc5b85d1c7df23f403d/tensorflow/python/ops/math_grad.py#L1583).\r\n> \r\n> For a holomorphic function (multiplication, exp, etc...) the expression simply reduces to `C * conj(df/dg)` (because `dconj(f)/dg` vanishes for holomorphic f), which is the reason for all the conjugation introduced [here](https://github.com/tensorflow/tensorflow/commit/821063df9f0e6a0eec8cb78cb0ddc5c5b2b91b9f).\r\n\r\nThanks for that detailed explanation! \r\nHow did you find out this definition (`conj(df/dz + dconj(f)/dz)`) was the one used by tensorflow? Couldn't find it anywhere and I've been searching/asking around a lot.\r\nDo you have any clue on WHY this definition of the gradient? Any paper which says for non-holomorphic functions this mathematical definition will give you what you want? ", "If anyone is curious: @charmasaur suggests that it would have been \"more correct\" to define gradients the other way.  This isn't correct: the easiest way to see this is to imagine that the inputs to a network are real, the outputs are real, and in the middle of the network is a holomorphic function.  In this case, the optimizer has no idea that there's a holomorphic function involved in the middle of a computation: it sees a bunch of real stuff, and would have to do a full graph traversal to notice the issue.\r\n\r\nA better option would be to define the gradient to be mathematically correct, and as @charmasaur has helpfully shown this is what TensorFlow does.", "@martinwicke I gave a talk once inside Google with a slide showing the proof that the gradient should be the conjugate.  It's the TensorFlow documentation talk about gradients.  Could you take a picture of that slide and attach it here?", "> @martinwicke I gave a talk once inside Google with a slide showing the proof that the gradient should be the conjugate. It's the TensorFlow documentation talk about gradients. Could you take a picture of that slide and attach it here?\r\n\r\nI didn't understood, you gave the talk or this @martinwicke ? In any case, this talk would be much appreciated if you could provide the link. Also references if any.\r\n\r\nThank you.", "This one?\r\n<img width=\"995\" alt=\"Screen Shot 2019-09-24 at 13 49 24\" src=\"https://user-images.githubusercontent.com/577277/65549364-3b3f1880-ded2-11e9-90db-2bee3a25f5cc.png\">\r\n", "Thanks for the replies!\r\n\r\n> If anyone is curious: @charmasaur suggests that it would have been \"more correct\" to define gradients the other way. This isn't correct: the easiest way to see this is to imagine that the inputs to a network are real, the outputs are real, and in the middle of the network is a holomorphic function. In this case, the optimizer has no idea that there's a holomorphic function involved in the middle of a computation: it sees a bunch of real stuff, and would have to do a full graph traversal to notice the issue.\r\n\r\nIn that example (and any other situation involving real inputs and outputs), wouldn't the eventual gradient be real anyway, so taking the conjugate would be a no-op?\r\n\r\nIn any case, to me it just seems like a matter of convention/definition: without the conjugate, your \"gradient\" is the coefficient of the complex-linear function that approximates your function (well, approximates in real parts in the case of non-holomorphic functions); with the conjugate, your \"gradient\" is the direction of maximum increase. No matter which you choose, you can always get the other just by taking a conjugate.\r\n\r\n\r\n> How did you find out this definition (`conj(df/dz + dconj(f)/dz)`) was the one used by tensorflow? Couldn't find it anywhere and I've been searching/asking around a lot.\r\n\r\nI found out that was the definition by evaluating a bunch of gradients and spotting a pattern, then reading the code to convince myself that what was being computed was consistent with that pattern.\r\n\r\n> Do you have any clue on WHY this definition of the gradient? Any paper which says for non-holomorphic functions this mathematical definition will give you what you want?\r\n\r\nAs discussed above, I think this is the definition of gradient because it's the gradient of the real part of your function when viewed as a map of real variables. That means you can plug it into a standard gradient descent optimizer and the result will be to optimize the real part of your function (which seems like a decent interpretation of \"please optimize this complex-valued function\").\r\n\r\nAs to your second question, to my knowledge there's not really any well-accepted \"gradient of a general complex-valued function of complex variables\". For holomorphic functions you have the Wirtinger derivative, which is the coefficient of the complex-linear approximation to your function, and is well-accepted. For non-holomorphic functions though, there is no complex-linear approximation (by definition), so it's not even clear what you want when you ask for a derivative or gradient. Your best bet is probably to view your function as a map between real spaces and look at a Jacobian.\r\n\r\nIn general I think it really boils down to a question of what you're actually doing with the gradient -- that will determine exactly which quantity you want to calculate. Once you know that quantity, which will almost certainly be some linear combination of `df/dz` and `dconj(f)/dz`, TF will be able to calculate it by choosing an appropriate `grad_ys` input to `tf.gradients`. Specifically, in the operator `D(C, f) := conj(conj(C) * df/dz + C * dconj(f)/dz)` I mentioned in my earlier reply, `C` corresponds to `grad_ys`.\r\n\r\nFor example, if you wanted to compute `df/dz` for some non-holomorphic `f`, you could use `grad_ys=[1]` to get `conj(df/dz + dconj(f)/dz)`, then use `grad_ys=[1j]` to get `conj(-i df/dz + i dconj(f)/dz)`. From those two quantities you can get `df/dz`.\r\n\r\nDoes that help?", "As a side note for the above discussion, I believe [this technical report](https://mediatum.ub.tum.de/doc/631019/631019.pdf) is a great material for derivatives and gradients definition of complex function in a more mathematical rigorous way. Specifically, in chapter 4, the author shows exactly why \"gradient\" of real valued complex variable functions is (two times) the complex conjugate of the corresponding **partial** derivatives (note no derivative can be well defined for non-holomorphic function, only partial derivatives can).", "@martinwicke Yep.   Presumably that will resolve the confusion. :)", "> I found out that was the definition by evaluating a bunch of gradients and spotting a pattern, then reading the code to convince myself that what was being computed was consistent with that pattern.\r\n\r\nYOU ARE THE BOSS!!! I was trying to reverse engineer it myself. I had some theories and all where taken down at some point. I was expecting the definition to be `2*df/dconj(z)` based on a paper of CVNN (Akira Hirose) which did work for `f : C -> R` but not for `f : C -> C`. (Always using Wirtinger calculus of course)\r\n\r\nOne last question maybe off topic. I tried reading the code myself but I'm more an electronic engineer and not software/informatic engineer. I got to a point where the Python code calls an API of a C/C++ code for the gradient but couldn't really find where to continue from there. You that have read the code can maybe point me in the right direction. Do you know where to start reading from the C code? I would like to read it too to further understand it.\r\nFor what I do I think I will have to understand it eventually anyway.\r\n\r\n> As a side note for the above discussion, I believe [this technical report](https://mediatum.ub.tum.de/doc/631019/631019.pdf) is a great material for derivatives and gradients definition of complex function in a more mathematical rigorous way. Specifically, in chapter 4, the author shows exactly why \"gradient\" of real valued complex variable functions is (two times) the complex conjugate of the corresponding **partial** derivatives (note no derivative can be well defined for non-holomorphic function, only partial derivatives can).\r\n\r\nGood, I will give it a look. Thank you!", "> One last question maybe off topic. I tried reading the code myself but I'm more an electronic engineer and not software/informatic engineer. I got to a point where the Python code calls an API of a C/C++ code for the gradient but couldn't really find where to continue from there. You that have read the code can maybe point me in the right direction. Do you know where to start reading from the C code? I would like to read it too to further understand it.\r\n> For what I do I think I will have to understand it eventually anyway.\r\n\r\nI won't be much help on this one, unfortunately. For my investigation I just dug around until I found the python code defining gradients (math_grad.py, linked earlier), and used that in isolation. I've never actually tried to trace through all the way from a `gradients` call to the C++ code.\r\n\r\nMaybe one of the TF folks can point to a resource for learning more about that?", "Just some thought I have been given lately. Another (more compressed) way to write tensorflow's definition of the gradient is: `2*dreal(f) / dconj(z)`.", "I need to implement a function f: C --> C with a `tf.custom_gradient` decorator, so I'm trying to understand TensorFlow's behaviour with **non-holomorphic** functions.\r\n\r\nThe total differential of f is:\r\n\r\ndf = (J_z)dz + (J_z*)dz* (eq. 3.7 in the [technical report](https://mediatum.ub.tum.de/doc/631019/631019.pdf))\r\n\r\nwhere J_z = partial f/partial z and J_z* = partial f/partial z*. How does this reduce to the cases in use by TensorFlow?\r\n\r\nAlso, I seem to have found that the seemingly correct way to implement the gradient when using tf.custom_gradient is\r\n\r\n```python\r\ndef grad(dy):\r\n    return tf.reduce_sum(dy*tf.math.conj(J_z) + tf.math.conj(dy)*J_zc)\r\n```\r\nwith `J_z` and `J_zc` defined above. However, there seem to be cases where this doesn't work. Is it wrong?", "I wrote the equation of Tensorflow in [here](https://stackoverflow.com/questions/57108959/how-does-tf-gradients-manages-non-holomorphic-functions) but based again in @charmasaur findings (THANKS!). \r\n\r\nBasically the equation for the function will be: \r\n\r\n![tf-grad-def](https://chart.googleapis.com/chart?cht=tx&chl=%5Cnabla_z%20f%20%3D%20%5Cleft(%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20z%7D%20%2B%20%5Cfrac%7B%5Cpartial%20f*%7D%7B%5Cpartial%20z%7D%20%5Cright)*%3D2%5Cfrac%7B%5Cpartial%20Real(f)%7D%7B%5Cpartial%20z*%7D)\r\n\r\nI am still not sure how to get the definition listed on your paper.\r\n\r\nPS: I am not 100% into the case of f: C --> C as I work with neural networks (cost function is real) but I am very interested in the topic anyway (cracking TF gradient), please keep me inform of your findings.\r\nPS 2: I have seen you work just next to me. I am currently doing my PhD in CentraleSupelec. We can get in touch to discuss the topic in person if you want. Contact me if you are interested, my info is in my GitHub profile.", "I also work with neural networks with a real loss function, but I also have components in the middle that have complex inputs and complex outputs. \r\n\r\nThe definition that you wrote, which apparently TF is using, matches (I think) the following:\r\n<img width=\"802\" alt=\"Capture d\u2019e\u0301cran 2020-02-28 a\u0300 11 15 47\" src=\"https://user-images.githubusercontent.com/8944955/75540094-afe72780-5a1b-11ea-8925-4b85bcfd6911.png\">\r\n\r\nwhich works for f: C -> R.\r\nMy C to C function is not a loss function, but just an intermediate component, that's why I only want to propagate the gradient correctly without losing any part.", "@ziofil at first glance that custom grad looks reasonable to me, at least in terms of the conjugates and derivatives being taken. Could you give an example of where it doesn't work?", "Not without copypasting a ton of code+math... which makes me suspect that the way I'm computing `J_z` and `J_zc` might be buggy.\r\nAt least, as you endorse the custom grad, I can concentrate on tracking down the bug in the math or in its implementation. Thank you both! (I'll get back here if I find out the custom grad was wrong)", "> Maybe one of the TF folks can point to a resource for learning more about that?\r\n\r\nI actually need a formal verification that the equation gave by @charmasaur  is indeed used for Tensorflow `tf.gradient`. Can someone within Tensorflow assert this is indeed the equation or at least give me a reference for this?\r\n\r\nOr shall I read the code and do the verification myself as @charmasaur did?\r\n\r\nThis may also interest you @sylviemonet.\r\n\r\n\r\n\r\n", "> I wrote the equation of Tensorflow in [here](https://stackoverflow.com/questions/57108959/how-does-tf-gradients-manages-non-holomorphic-functions) but based again in @charmasaur findings (THANKS!).\r\n> \r\n> Basically the equation for the function will be:\r\n> \r\n> ![tf-grad-def](https://camo.githubusercontent.com/f25d7f7aaf9e72a7cd53b7c0f4e26e6e3e5d10e7/68747470733a2f2f63686172742e676f6f676c65617069732e636f6d2f63686172743f6368743d74782663686c3d2535436e61626c615f7a253230662532302533442532302535436c65667428253230253543667261632537422535437061727469616c253230662537442537422535437061727469616c2532307a253744253230253242253230253543667261632537422535437061727469616c253230662a2537442537422535437061727469616c2532307a2537442532302535437269676874292a25334432253543667261632537422535437061727469616c2532305265616c2866292537442537422535437061727469616c2532307a2a253744)\r\n\r\nAt first, I have a small question about this equation.\r\n\r\nWhat does <img width=\"40\" alt=\"nabla\" src=\"https://user-images.githubusercontent.com/16817699/75710328-ac29fe00-5cc4-11ea-92d7-3b5d16012bfc.png\"> mean in this equation?\r\n\r\nThen I want to know how to write down this in the form with df = XX dz + XX dz*?\r\n\r\nIf I simply change <img width=\"40\" alt=\"nabla\" src=\"https://user-images.githubusercontent.com/16817699/75710328-ac29fe00-5cc4-11ea-92d7-3b5d16012bfc.png\"> to the form df =  ( XXXX ) dz [or replace dz by dz*? I dont know.], it would be like:\r\n\r\n<img width=\"180\" alt=\"gradient\" src=\"https://user-images.githubusercontent.com/16817699/75710042-1a21f580-5cc4-11ea-8308-a997d99e4e9f.png\">\r\n\r\nIn the same <a href=\"https://mediatum.ub.tum.de/doc/631019/631019.pdf\">tutorial</a> as @ziofil mentioned before, it looks like:\r\n<img width=\"738\" alt=\"\u622a\u5c4f2020-03-0220 24 03\" src=\"https://user-images.githubusercontent.com/16817699/75709878-d29b6980-5cc3-11ea-8fd8-30322675af6a.png\">\r\n\r\nHere I want to know, how to explain these two expressions? (Note: I've also tried to calculate them by replacing partial derivative by the Wirtinger derivatives, they are not equal )\r\n\r\nAll the equation above is about the function f with complex value.\r\n\r\nNext, we consider the function f is a real-valued function.\r\n\r\nI agree with this result ![tf-grad-def](https://camo.githubusercontent.com/f25d7f7aaf9e72a7cd53b7c0f4e26e6e3e5d10e7/68747470733a2f2f63686172742e676f6f676c65617069732e636f6d2f63686172743f6368743d74782663686c3d2535436e61626c615f7a253230662532302533442532302535436c65667428253230253543667261632537422535437061727469616c253230662537442537422535437061727469616c2532307a253744253230253242253230253543667261632537422535437061727469616c253230662a2537442537422535437061727469616c2532307a2537442532302535437269676874292a25334432253543667261632537422535437061727469616c2532305265616c2866292537442537422535437061727469616c2532307a2a253744) in the case that the definition of the gradient is <img width=\"120\" alt=\"gradientdef\" src=\"https://user-images.githubusercontent.com/16817699/75710821-881aec80-5cc5-11ea-8198-e995ee4de768.png\">. And here we use the same idea to change <img width=\"40\" alt=\"nabla\" src=\"https://user-images.githubusercontent.com/16817699/75710328-ac29fe00-5cc4-11ea-92d7-3b5d16012bfc.png\"> to the form df =  ( XXXX ) dz, it would be like:\r\n\r\n<img width=\"180\" alt=\"test\" src=\"https://user-images.githubusercontent.com/16817699/75712134-c31e1f80-5cc7-11ea-9bda-959296314b32.png\">.\r\n\r\n\r\nWe can also quote the equation from the <a href=\"https://mediatum.ub.tum.de/doc/631019/631019.pdf\">tutorial</a> to compare two equations given the f is a real-valued function:\r\n\r\n<img width=\"753\" alt=\"\u622a\u5c4f2020-03-0220 39 34\" src=\"https://user-images.githubusercontent.com/16817699/75711019-ef38a100-5cc5-11ea-852f-babe14c2b599.png\">.\r\n\r\nThe REAL operator functions for all parts but not only the function f. Here I don't know either how to explain or am I wrong in some definitions or concepts?\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "Unfortunately, I have no answers about that @sylviemonet. However, I don't agree on:\r\n\r\n> <img alt=\"test\" width=\"180\" src=\"https://user-images.githubusercontent.com/16817699/75712134-c31e1f80-5cc7-11ea-9bda-959296314b32.png\">.\r\n\r\nThat would assume the gradient is df/dz which is not right?\r\n\r\nThis thread seems to be having some interest. I asked the question on [stackoverflow](https://stackoverflow.com/questions/57108959/how-does-tf-gradients-manages-non-holomorphic-functions) because google asks us to do that (https://www.tensorflow.org/community). \r\n\r\nShall we move the discussion [there](https://stackoverflow.com/questions/57108959/how-does-tf-gradients-manages-non-holomorphic-functions)? Or create a new issue in GitHub (Although I believe TensorFlow encourages to use StackOverflow for this kind of question). Or even a new question on StackOverflow?", "To customize gradient for  f: C-->C in tensorflow, follows the procedures below.\r\n\r\nAlways imagine a final real output as L, then the forward pass looks like: y=f(x), L=g(y), where x and y can be complex vectors. The full differentiation would be: \r\n<img width=\"353\" src=\"https://user-images.githubusercontent.com/35157286/75765931-477daa80-5d7b-11ea-84f9-e15e7f0ea058.png\">\r\nwhere \\bar{x} is defined as \\partial L/\\partial x and the same for \\bar{y}.\r\n\r\nWe can express the differentiation of dy as dy(dx) from y=f(x), then we plug dy(dx) into the above formula. Since dx and dx^* are independent, and dx are on the both side of the equation, the coefficient before dx on the right hand side is just $\\bar{x}$ which is expressed by $\\bar{y}$, and $\\bar{x}(\\bar{y})$ is the derivative back propagate formula the user should customize in autograd. For tensorflow, since it back propagates gradients instead of derivatives, one should make conjugate of $\\bar{x}(\\bar{y})$ as customized gradient primitive. \r\nThis gradient is consistent and as expected when final output is real.\r\n\r\nMaybe the above clarification resolve some confusions :)\r\n", "> To customize gradient for f: C-->C in tensorflow, follows the procedures below.\r\n> \r\n> Always imagine a final real output as L, then the forward pass looks like: y=f(x), L=g(y), where x and y can be complex vectors. The full differentiation would be:\r\n> <img alt=\"\" width=\"353\" src=\"https://user-images.githubusercontent.com/35157286/75765931-477daa80-5d7b-11ea-84f9-e15e7f0ea058.png\">\r\n> where \\bar{x} is defined as \\partial L/\\partial x and the same for \\bar{y}.\r\n> \r\n> We can express the differentiation of dy as dy(dx) from y=f(x), then we plug dy(dx) into the above formula. Since dx and dx^* are independent, and dx are on the both side of the equation, the coefficient before dx on the right hand side is just $\\bar{x}$ which is expressed by $\\bar{y}$, and $\\bar{x}(\\bar{y})$ is the derivative back propagate formula the user should customize in autograd. For tensorflow, since it back propagates gradients instead of derivatives, one should make conjugate of $\\bar{x}(\\bar{y})$ as customized gradient primitive.\r\n> This gradient is consistent and as expected when final output is real.\r\n> \r\n> Maybe the above clarification resolve some confusions :)\r\n\r\nCan you cite the source of how can you assert that @refraction-ray?\r\n\r\n", "@NEGU93 , for the general approach to get back propagation formula, any paper on auto differentiation of linear algebra can serve as a reference, eg. 3.5.2 in https://arxiv.org/pdf/1701.00392.pdf.\r\n\r\nFor why such back propagation formula (actually its conjugate) is utilized in tf customize gradient, I have no specific references. I am sure it is true since I have contributed gradients for some operations for tensorflow. And you can double-check it by comparison with numerical derivatives.", "> @NEGU93 , for the general approach to get back propagation formula, any paper on auto differentiation of linear algebra can serve as a reference, eg. 3.5.2 in https://arxiv.org/pdf/1701.00392.pdf.\r\n\r\nWow, that paper was indeed great! Helped a lot, thanks!\r\n", "I hope the following can help those in need of an explanation.\r\n\r\n## Parameter update rule\r\nThe gradient descent update of a complex parameter needs the gradient of the loss function with respect to the _conjugate_ of the parameter ([proof](https://mediatum.ub.tum.de/doc/631019/631019.pdf)):\r\n\r\n<img width=\"215\" alt=\"z_leftarrow_z_-_-1\" src=\"https://user-images.githubusercontent.com/8944955/79046193-b4545200-7c0f-11ea-9412-26b7d282aaae.png\">\r\n\r\nClearly this update rule falls back to the usual update rule in case of a real parameter. \r\n\r\n## Computation of the update\r\nThere are two rules to remember when computing the gradient for the update:\r\n1. Treat variables and their complex conjugate as __independent__. This allows you to handle non-holomorphic functions. For example, suppose there is a non-holomorphic function f: C->C between z and the loss, then the chain rule is\r\n<img width=\"402\" alt=\"frac_partial_L_p\" src=\"https://user-images.githubusercontent.com/8944955/79047858-9ab80800-7c19-11ea-9a95-3a673764a14c.png\">\r\n\r\n2. Numerator and denominator of partial derivative expressions behave \"independently\" with respect to complex conjugation, so the following identities hold (in the first one, remember that L is real):\r\n<img width=\"228\" alt=\"frac_partial_L_p-1\" src=\"https://user-images.githubusercontent.com/8944955/79047961-1ca83100-7c1a-11ea-957d-b7dd89764018.png\">\r\n<img width=\"228\" alt=\"frac_partial_f^*\" src=\"https://user-images.githubusercontent.com/8944955/79047980-2c277a00-7c1a-11ea-90f2-c9bc7f33ad2e.png\">\r\n\r\n## Custom gradients in Tensorflow\r\nIn Tensorflow when we write a new expression and we want to customize its gradient, we need to use the `@tf.custom_gradient` decorator and within the scope of the function we define a gradient function and return it after the output of the expression. This function is then used internally by TF, which calls it and passes the upstream gradient to it. So for example, if we want to customize the function `f` in the example above, we would write \r\n\r\n```python \r\n@tf.custom_gradient\r\ndef f(z):\r\n    # code to compute output = f(z)\r\n    def grad(dy):\r\n        # see below\r\n        return dL\r\n    return output, grad\r\n```\r\nThe role of `grad(dy)` is to propagate the gradient of the final value (i.e. the value of the loss function L) backwards, past the expression that we are customizing. So `grad` is given the upstream gradient dy = dL/df* (notice that TF diligently supplies the gradient with respect to the _conjugate_ of our complex function) and `grad` has to return dL/dz*.\r\n\r\nWhy doesn't TF supply two gradients given all this fuss about treating variables and their conjugate independently? Because dL/dz* and dL/dz are not independent: they are the conjugate of each other (L is real), and TF _always assumes_ that at the end of the line there is a real loss function to optimize.\r\n\r\nSo in the body of `grad(dy)` we should compute `df1 = df/dz` and `df2 = df/dz*` (this time independently because `f` is complex!) and combine them with `dy` and its conjugate as prescribed by the chain rule:\r\n\r\n```python\r\ndef grad(dy):\r\n    # code to compute df1=df/dz and df2=df/dz*\r\n    # recall that dy = dL/df*\r\n    dL = dy*tf.math.conj(df1) + tf.math.conj(dy)*df2\r\n    return dL\r\n```\r\n\r\n### Special cases\r\n\r\nIf f: R->C (i.e. if z is a real variable) then `df1` and `df2` are the conjugate of each other. So we can simplify the chain rule and write `dL = 2*tf.math.real(dy*tf.math.conj(df1))`\r\n\r\nIf f:C->C is holomorphic, then f doesn't depend on z*, which means that df2 = 0. So we can simplify the chain rule and write `dL = dy*tf.math.conj(df1)`\r\n\r\n### Multiple variables\r\nThe shape of `dy` is the same as the _output_ of f (i.e. if f outputs a tensor of shape `(a,b,c...)` then `dy` will also be a tensor of the same shape). On the other hand, the return values of `grad`, (being the gradients of the loss with respect to the various inputs of f) have to match the shape of each input.\r\n\r\nFor example, if f takes two complex vectors of shape `(a,)` and `(b,)` as input variables and it returns a matrix of shape `(c,d)`, then `dy`  has shape `(c,d)` and grad has to return two vectors of shape`(a,)` and `(b,)`.\r\n", "I want to point out that Tensorflow disagrees with JAX on what the grad should be:\r\n\r\n```\r\nfrom jax import grad\r\n\r\ndef f(z):\r\n  return z * z\r\n\r\nz = 0.1j\r\n\r\nprint(grad(f, holomorphic=True)(z))\r\n```\r\n\r\nprints\r\n\r\n```\r\n0.2j\r\n```", "> I want to point out that Tensorflow disagrees with JAX on what the grad should be:\n> \n> ```\n> from jax import grad\n> \n> def f(z):\n>   return z * z\n> \n> z = 0.1j\n> \n> print(grad(f, holomorphic=True)(z))\n> ```\n> \n> prints\n> \n> ```\n> 0.2j\n> ```\n\nI don't know about `jax` but if that's the case, then `jax` is computing the derivative of f wrt z. When you compute a gradient to minimize a function you should compute the derivative wrt the conjugate of z. Therefore, gradient of f is NOT equal to derivate wrt z.\n[Here](https://dsp.stackexchange.com/questions/51248/derivative-with-respect-to-complex-conjugate) there's a good explanation why this is the case.\n\nConclusion: tensorflow is computing what should be expected.", "> This isn't correct: the easiest way to see this is to imagine that the inputs to a network are real, the outputs are real, and in the middle of the network is a holomorphic function. In this case, the optimizer has no idea that there's a holomorphic function involved in the middle of a computation: it sees a bunch of real stuff, and would have to do a full graph traversal to notice the issue.\r\n\r\nI also chime in that this reasoning isn't correct, @girving. The optimizer doesn't need to traverse the entire function. This is because when you use the d/dz Wirtinger derivative, you have to conjugate the adjoint when you view real-as-complex or complex-as-real (which is how you ostensibly embedded the holomorphic function inside a R-to-R function). \r\n\r\n(None of this changes the fact that which derivative definition you use is a matter of convention, and there are reasons to prefer one or the other.)", "Jax computes complex derivatives in a different way than TF. \r\n\r\nTake a function f(z) : C -> C\r\n\r\nJax breaks down the input into real and imaginary parts: z = x + 1j*y and defines the function as f(z) = u(x,y) + 1j*v(x,y) where u and v are real functions with real arguments.\r\n\r\nThen, Jax defines the Jacobian of f at z as a 2x2 matrix (two functions, two variables). Now, in your example f is holomorphic, so its jacobian has a special structure (it's a rescaled version of a rotation matrix). In turn this means that you can ignore the complex part v(x,y), and everything is still working as expected. The derivative that Jax computes is then given by the vector-jacobian product and it's 2z.\r\n\r\nHave a look here: https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#Complex-numbers-and-differentiation\r\n\r\n", "```\r\n>>> a=tf.Variable(1+1j)\r\n>>> @tf.function\r\n... def example(a):\r\n...     b=tf.math.conj(a)\r\n...     return tf.gradients(b, a, grad_ys=tf.constant(1+1j))\r\n...\r\n>>> example(a)\r\nWARNING:tensorflow:AutoGraph could not transform <function example at 0x7f8f5a013620> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Unable to locate the source code of <function example at 0x7f8f5a013620>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING: AutoGraph could not transform <function example at 0x7f8f5a013620> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Unable to locate the source code of <function example at 0x7f8f5a013620>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n[<tf.Tensor: shape=(), dtype=complex128, numpy=(1-1j)>]\r\n```\r\n\r\nshouldn't the gradient returned be `1+1j` according to the tensorflow gradient convention?", "Nope:\r\nYou are passing the upstream gradient as dL/db* = 1+1j, therefore:\r\n\r\ndL/da* = dL/db* db*/da* + dL/db db/da* = (1+1j) x 0 + (1-1j) x 1 = 1-1j\r\n\r\nbecause b = a* (your function) and TF gives you the gradient for the update of a, which is dL/da* (i.e. the gradient with respect to the conjugate of a). Note that the upstram gradient is also interpreted as dL/db* and not dL/db. (Here L is a hypothetical real loss function which TF assumes to exist at the end of the computation).", "In Pytorch [documentation](https://pytorch.org/docs/stable/notes/autograd.html#autograd-for-complex-numbers) it says:\r\n\r\n> This convention matches TensorFlow\u2019s convention for complex differentiation [...]\r\n\r\nThis just reinforces what we know, that it is done with the conjugate, however, not that wirtinger calculus is used (I think).\r\n\r\nPS: A documentation well explained like that one is what Tensorflow is lacking."]}]