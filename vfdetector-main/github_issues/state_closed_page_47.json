[{"number": 53949, "title": "Fix crash when importing an invalid graph with an empty op type to TFG", "body": "Found by proto fuzzing.\r\n\r\nPiperOrigin-RevId: 413847837\r\nChange-Id: Icac24d1b389c5661800fb4d622dff0b31d846cca", "comments": []}, {"number": 53948, "title": "Prevent crash due to integer overflow followed by allocating negative\u2026", "body": "\u2026 sized array.\r\n\r\nPiperOrigin-RevId: 414891322\r\nChange-Id: I5df390e0dc1d9f115209293708950cdf9306931c", "comments": []}, {"number": 53947, "title": "Prevent crash due to integer overflow followed by allocating negative\u2026", "body": "\u2026 sized array.\r\n\r\nPiperOrigin-RevId: 414891322\r\nChange-Id: I5df390e0dc1d9f115209293708950cdf9306931c", "comments": []}, {"number": 53946, "title": "Prevent crash due to integer overflow followed by allocating negative\u2026", "body": "\u2026 sized array.\r\n\r\nPiperOrigin-RevId: 414891322\r\nChange-Id: I5df390e0dc1d9f115209293708950cdf9306931c", "comments": []}, {"number": 53945, "title": "Fix heap OOB", "body": "Cherrypick 2b7100d6cdff36aa21010a82269bc05a6d1cc74a and adbbabdb0d3abb3cdeac69e38a96de1d678b24b3 on r2.5", "comments": []}, {"number": 53944, "title": "Fix heap OOB", "body": "Cherrypick 2b7100d6cdff36aa21010a82269bc05a6d1cc74a and adbbabdb0d3abb3cdeac69e38a96de1d678b24b3 on r2.6", "comments": []}, {"number": 53943, "title": "Fix heap OOB", "body": "Cherrypick 2b7100d6cdff36aa21010a82269bc05a6d1cc74a and adbbabdb0d3abb3cdeac69e38a96de1d678b24b3 on r2.7", "comments": []}, {"number": 53942, "title": "Fix Null-pointer dereference in BuildXlaCompilationCache", "body": "If ConfigProto is not used, then use the default settings which is to allow all devices.\r\n\r\nPiperOrigin-RevId: 420391800\r\nChange-Id: I88161ad7042990aef678e77b597a2fb2c8f815be", "comments": []}, {"number": 53941, "title": "Fix Null-pointer dereference in BuildXlaCompilationCache", "body": "If ConfigProto is not used, then use the default settings which is to allow all devices.\r\n\r\nPiperOrigin-RevId: 420391800\r\nChange-Id: I88161ad7042990aef678e77b597a2fb2c8f815be", "comments": []}, {"number": 53940, "title": "Fix Null-pointer dereference in BuildXlaCompilationCache", "body": "If ConfigProto is not used, then use the default settings which is to allow all devices.\r\n\r\nPiperOrigin-RevId: 420391800\r\nChange-Id: I88161ad7042990aef678e77b597a2fb2c8f815be", "comments": []}, {"number": 53938, "title": "Unify the Cudnn Execution Plan Builder", "body": "Note: This PR is on the top of https://github.com/tensorflow/tensorflow/pull/53843.\r\n\r\nThis PR tries to unify the code paths from the cudnn op graph to the execution plans for both unfused and fused conv ops. Both code paths follow: build op graph -> query heuristics/fallback plans -> filter out undesired plans (e.g. non-deterministic or winograd engines) -> filter out errata plans (e.g. engines that might be buggy).\r\n\r\ncc. @nluehr ", "comments": ["@awpr, can you PTAL?", "Thanks for the comments. I have committed the new change (the top one). PTAL. @awpr", "Same with this one. I see \"Windows Bazel GPU\" fails but the details are not accessible. @awpr @jurahul ", "@awpr I noticed the test below is failed. Can you help with it?\r\n```\r\nimport/copybara \u2014 An error happened while migrating the change\r\n```", "Still trying to figure out how to get it to retry."]}, {"number": 53937, "title": "[oneDNN] Enable auto_mixed_precision_mkl for saved_model", "body": "Add auto_mixed_precision_mkl as an optimizer option to be enabled for saved_model in eager mode using the script change\r\nfor example:\r\nUsing the following one line in the code the user can enable/disable the optimizer\r\ntf.config.optimizer.set_experimental_options({'auto_mixed_precision_mkl':True})", "comments": []}, {"number": 53936, "title": "TFRT saved_model_test registers type names in variant_op_registry multiple times", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 9.9 (stretch)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: master\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): 4.2.2\r\n- GCC/Compiler version (if compiling from source): 8.2.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nThe target `//tensorflow/core/tfrt/saved_model/tests:saved_model_test` somehow caused multiple call to register actions in variant_op_registry.cc.\r\n\r\nE.g. This line [`REGISTER_UNARY_VARIANT_DECODE_FUNCTION(Tensor, \"tensorflow::Tensor\");`](https://github.com/tensorflow/tensorflow/blob/ae9e5b2aa9cad5da4e614dfb40766c8e64bfc2d6/tensorflow/core/framework/tensor.cc#L71) is called two times and fail check:\r\n> 2022-01-24 19:21:42.384846: F tensorflow/core/framework/variant_op_registry.cc:77] Check failed: existing == nullptr (0x6d2e2e0 vs. nullptr)Unary VariantDecodeFn for type_name: tensorflow::Tensor already registered\r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI did some preprocessing steps to get this test to run:\r\n1. Set visibility to public for `//tensorflow/core/tfrt/callback:op_kernel_runner` (because build fails on visibility)\r\n2. Add `\"@com_google_absl//absl/flags:flag\"` dependency to `//tensorflow/core/tfrt/saved_model:saved_model_testutil`; Add\r\n    ```\r\n    #include \"absl/flags/declare.h\"\r\n    #include \"absl/flags/flag.h\"\r\n    ```\r\n    to `//tensorflow/core/tfrt/saved_model/saved_model_testutil.h`\r\n (Otherwise `ABSL_DECLARE_FLAG` is not defined)\r\n3. Remove the `no_oss` tag in`//tensorflow/core/tfrt/saved_model/tests:saved_model_test` (Otherwise it won't run)\r\n\r\nAnd then run\r\n```\r\nbazel test --config=tfrt tensorflow/core/tfrt/saved_model/tests:saved_model_test\r\n```\r\n\r\n\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Can you log a stack trace before [REGISTER_UNARY_VARIANT_DECODE_FUNCTION(Tensor, \"tensorflow::Tensor\");](https://github.com/tensorflow/tensorflow/blob/ae9e5b2aa9cad5da4e614dfb40766c8e64bfc2d6/tensorflow/core/framework/tensor.cc#L71)?  Then we may know why it is linked twice.\r\n\r\nBazel should be able to dedup automatically. However, if it is linked as an external library (e.g. tf_runtime is an external library to TensorFlow), then it is possible to have this issue.", "Looking from the stack trace, the code is linked via two .so: `libtensorflow_Score_Sframework_Slibtensor.so` and `libtensorflow_framework.so.2`:\r\n\r\n```\r\nstack trace:\r\n  /data00/home/zhuoran.liu/.cache/bazel/_bazel_zhuoran.liu/c9b3369ae2db47e431aa7b0a87cbd540/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/tfrt/saved_model/tests/../../../../../_solib_k8/libtensorflow_Score_Sframework_Slibtensor.so : ()+0x278de\r\n  /lib64/ld-linux-x86-64.so.2 : ()+0xf79a\r\n  /lib64/ld-linux-x86-64.so.2 : ()+0xf8ab\r\n  /lib64/ld-linux-x86-64.so.2 : ()+0xc5a\r\n2022-01-24 23:35:59.418926: E tensorflow/core/framework/variant_op_registry.cc:74] *** UnaryVariantOpRegistry::RegisterDecodeFn tensorflow::Tensor\r\nstack trace:\r\n  /data00/home/zhuoran.liu/.cache/bazel/_bazel_zhuoran.liu/c9b3369ae2db47e431aa7b0a87cbd540/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/tfrt/saved_model/tests/../../../../../_solib_k8/_U_S_Stensorflow_Score_Stfrt_Ssaved_Umodel_Stests_Csaved_Umodel_Utest___Utensorflow/libtensorflow_framework.so.2 : ()+0x777d0e\r\n  /lib64/ld-linux-x86-64.so.2 : ()+0xf79a\r\n  /lib64/ld-linux-x86-64.so.2 : ()+0xf8ab\r\n  /lib64/ld-linux-x86-64.so.2 : ()+0xc5a\r\n2022-01-24 23:35:59.423506: E tensorflow/core/framework/variant_op_registry.cc:74] *** UnaryVariantOpRegistry::RegisterDecodeFn tensorflow::Tensor\r\n2022-01-24 23:35:59.424534: F tensorflow/core/framework/variant_op_registry.cc:77] Check failed: existing == nullptr (0x67d0400 vs. nullptr)Unary VariantDecodeFn for type_name: tensorflow::Tensor already registered\r\n```\r\n\r\nIt seems to me like the two .so are coming from `//tensorflow/core:framework` and `//tensorflow/core/framework:tensor`, where the former contains the latter. Both are from within tensorflow but not tf_runtime. (not sure if I understood correctly)\r\n\r\nI also observed another test which works normally ([xla_compiler_test](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2xla/xla_compiler_test.cc)) that also depend on `//tensorflow/core/framework:tensor`. `ldd` of the test binary shows that it only links `libtensorflow_framework.so.2` but not linking `libtensorflow_Score_Sframework_Slibtensor.so`.\r\n\r\n\r\n", "Changing to `linkstatic=True` for this test would result in a build error:\r\n```\r\nERROR: /data00/home/zhuoran.liu/.cache/bazel/_bazel_zhuoran.liu/c9b3369ae2db47e431aa7b0a87cbd540/external/llvm-project/llvm/BUILD.bazel:1909:16: Compiling llvm/lib/Target/ARM/Thumb2InstrInfo.cpp failed: (Exit 1): gcc failed: error executing command /usr/local/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 73 argument(s) skipped)\r\nduring RTL pass: expand\r\nexternal/llvm-project/llvm/lib/Target/ARM/Thumb2InstrInfo.cpp: In function 'bool llvm::rewriteT2FrameIndex(llvm::MachineInstr&, unsigned int, llvm::Register, int&, const llvm::ARMBaseInstrInfo&, const llvm::TargetRegisterInfo*)':\r\nexternal/llvm-project/llvm/lib/Target/ARM/Thumb2InstrInfo.cpp:764:1: internal compiler error: in commit_one_edge_insertion, at cfgrtl.c:2021\r\n }\r\n ^\r\n0x5d188d commit_one_edge_insertion(edge_def*)\r\n        ../.././gcc/cfgrtl.c:2021\r\n0x7f2f55 execute\r\n        ../.././gcc/cfgexpand.c:6498\r\n\r\n```", "The issue might be that `//tensorflow/core/tfrt/saved_model/tests:saved_model_test` is a `tf_cc_test`, which links in shared object `tensorflow_framework` sources via [`tf_binary_additional_srcs`](https://github.com/tensorflow/tensorflow/blob/ee62dbd6d71243c0d7c79123f6b34d26cc048599/tensorflow/tensorflow.bzl#L558-L572).\r\n\r\nWe worked around in the following way:\r\n1. Change `tf_cc_test` to `cc_test`;\r\n2. Link the [dependencies](https://github.com/tensorflow/tensorflow/blob/ee62dbd6d71243c0d7c79123f6b34d26cc048599/tensorflow/BUILD#L951-L974) of `tensorflow_framework` shared object back into our `cc_test`\r\n```\r\ndeps = original_deps + [\r\n        \"//tensorflow:tf_private_symbols.lds\",\r\n        \"//tensorflow/c/experimental/filesystem:filesystem_interface\",\r\n        \"//tensorflow/c/experimental/stream_executor:stream_executor\",\r\n        \"//tensorflow/c:env\",\r\n        \"//tensorflow/c:kernels\",\r\n        \"//tensorflow/c:kernels_experimental\",\r\n        \"//tensorflow/c:logging\",\r\n        \"//tensorflow/c:ops\",\r\n        \"//tensorflow/cc/saved_model:loader_lite_impl\",\r\n        \"//tensorflow/cc/saved_model:metrics_impl\",\r\n        \"//tensorflow/core/common_runtime:core_cpu_impl\",\r\n        \"//tensorflow/core:framework_internal_impl\",\r\n        \"//tensorflow/core/common_runtime/gpu:gpu_runtime_impl\",\r\n        \"//tensorflow/core/common_runtime/pluggable_device:pluggable_device_runtime_impl\",\r\n        \"//tensorflow/core/grappler/optimizers:custom_graph_optimizer_registry_impl\",\r\n        \"//tensorflow/core:lib_internal_impl\",\r\n        \"//tensorflow/core/profiler:profiler_impl\",\r\n        \"//tensorflow/core/util:determinism\",\r\n        \"//tensorflow/lite/kernels/shim:tf_kernel_shim\",\r\n        \"//tensorflow/stream_executor:stream_executor_impl\",\r\n        \"//tensorflow:tf_framework_version_script.lds\",\r\n    ] + tf_additional_binary_deps(),\r\n```\r\n\r\nThe multiple registration issue is gone with these settings.\r\n\r\n(Also need to remove the [`if_google`](https://github.com/tensorflow/tensorflow/blob/d851305b3cddbf9dea979e7cf7977986a575a4f6/tensorflow/core/tfrt/saved_model/tests/gen_saved_model.bzl#L14) part to correctly generate test models )", "Looks like the test fails because `tfrt.merge.chains` kernel is not open-sourced yet?\r\n\r\n```\r\n[ RUN      ] SavedModelTest.BasicV2\r\n2022-01-26 05:35:55.123556: I tensorflow/core/tfrt/saved_model/saved_model.cc:418] TFRT reading v1 savedmodel: /data00/home/zhuoran.liu/.cache/bazel/_bazel_zhuoran.liu/c9b3369ae2db47e431aa7b0a87cbd540/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/tfrt/saved_model/tests/saved_model_test.runfiles/org_t\r\nensorflow/tensorflow/core/tfrt/saved_model/tests/toy_v2                                                                                                                                               \r\n2022-01-26 05:35:55.123597: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /data00/home/zhuoran.liu/.cache/bazel/_bazel_zhuoran.liu/c9b3369ae2db47e431aa7b0a87cbd540/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/tfrt/saved_model/tests/saved_model_test.runfiles/org_tensorflow/tensor\r\nflow/core/tfrt/saved_model/tests/toy_v2\r\n2022-01-26 05:35:55.124004: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\r\n2022-01-26 05:35:55.124057: I tensorflow/core/tfrt/saved_model/saved_model.cc:434] TFRT finished reading meta graph. Took 0 ms.\r\n2022-01-26 05:35:55.124070: I tensorflow/core/tfrt/saved_model/saved_model.cc:485] TFRT loading v1 savedmodel: /data00/home/zhuoran.liu/.cache/bazel/_bazel_zhuoran.liu/c9b3369ae2db47e431aa7b0a87cbd540/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/tfrt/saved_model/tests/saved_model_test.runfiles/org_t\r\nensorflow/tensorflow/core/tfrt/saved_model/tests/toy_v2                                                                                                                                                           \r\n2022-01-26 05:35:55.124082: I tensorflow/core/tfrt/saved_model/saved_model.cc:501] TFRT Savedmodel use TPU target NoTpu                                                                                                              \r\n2022-01-26 05:35:55.125893: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2022-01-26 05:35:55.156251: I tensorflow/core/tfrt/saved_model/saved_model_import_input.cc:47] TFRT importing savedmodel signature: serving_default\r\n2022-01-26 05:35:55.157061: W tensorflow/core/tfrt/utils/tfrt_graph_execution_state.cc:263] TFRT failed to optimize graph: INVALID_ARGUMENT: Meta Optimizer disabled\r\n2022-01-26 05:35:55.158508: I tensorflow/core/tfrt/saved_model/saved_model_import_input.cc:47] TFRT importing savedmodel signature: StatefulPartitionedCall_2\r\n2022-01-26 05:35:55.159069: W tensorflow/core/tfrt/utils/tfrt_graph_execution_state.cc:263] TFRT failed to optimize graph: INVALID_ARGUMENT: Meta Optimizer disabled\r\n2022-01-26 05:35:55.160018: I tensorflow/core/tfrt/saved_model/saved_model_import_input.cc:47] TFRT importing savedmodel signature: NoOp\r\n2022-01-26 05:35:55.160362: W tensorflow/core/tfrt/utils/tfrt_graph_execution_state.cc:263] TFRT failed to optimize graph: INVALID_ARGUMENT: Meta Optimizer disabled\r\n2022-01-26 05:35:55.160779: I tensorflow/core/tfrt/saved_model/saved_model.cc:363] TFRT ImportSavedModel: Functionalization took 0 ms.                                                                        \r\n2022-01-26 05:35:55.160796: I tensorflow/core/tfrt/saved_model/saved_model.cc:367] TFRT ImportSavedModel: Grappler took 0 ms.                                                                                                    \r\n2022-01-26 05:35:55.161013: I tensorflow/core/tfrt/saved_model/saved_model.cc:535] TFRT finished importing savedmodel. Took 36 ms.                                                                                               \r\n2022-01-26 05:35:55.171788: I tensorflow/core/tfrt/saved_model/saved_model.cc:558] TFRT finished compiling savedmodel. Took 10 ms.\r\n2022-01-26 05:35:55.171831: E tensorflow/core/tfrt/runtime/runtime.cc:133] unknown kernel name 'tfrt.merge.chains'\r\n2022-01-26 05:35:55.176632: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/core/tfrt/utils/utils.cc:117) bef_file failed to open BEF\r\n*** Begin stack trace ***\r\n\r\n        xla::status_macros::MakeErrorStream::Impl::GetStatus()\r\n        tfrt::CreateBefFileFromBefBuffer(tensorflow::tfrt_stub::Runtime const&, std::vector<unsigned char, tfrt::internal::AlignedAllocator<unsigned char, 8ul> > const&)\r\n        tensorflow::tfrt_stub::SavedModelImpl::LoadSavedModel(tensorflow::tfrt_stub::SavedModel::Options, absl::lts_20210324::string_view, tensorflow::MetaGraphDef, tensorflow::Status*)\r\n        tensorflow::tfrt_stub::SavedModelImpl::LoadSavedModel(tensorflow::tfrt_stub::SavedModel::Options, absl::lts_20210324::string_view, std::unordered_set<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::alloc\r\nator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, tensorflow::Status*)\r\n        tensorflow::tfrt_stub::TFRTSavedModelTest::TFRTSavedModelTest(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unique_ptr<tensorflow::tfrt_stub::Runtime, std::default_delete<tensorflow::tfrt_stub::Runtime> >)\r\n        tensorflow::tfrt_stub::TFRTSavedModelTest::TFRTSavedModelTest(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)\r\n\r\n        void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*)\r\n        testing::Test::Run()\r\n        testing::TestInfo::Run()\r\n        testing::TestSuite::Run()\r\n        testing::internal::UnitTestImpl::RunAllTests()\r\n        bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*)\r\n        testing::UnitTest::Run()\r\n        main\r\n        __libc_start_main\r\n\r\n*** End stack trace ***\r\n```", "Turns out some kernel registrations are not always linked. Adding `*_alwayslink` targets of kernel registrations from `@tf_runtime` to `//tensorflow/core/tfrt/saved_model:saved_model` resolves the issue. E.g:\r\n```\r\n        \"@tf_runtime//:basic_kernels_alwayslink\",\r\n        \"@tf_runtime//:test_kernels_alwayslink\",\r\n        \"@tf_runtime//:io_alwayslink\",\r\n        \"@tf_runtime//:data_alwayslink\",\r\n```\r\n\r\nA few other notes on unblocking this test from oss:\r\n* `//tensorflow/core/tfrt/saved_model/tests:gen_saved_model_v1`: Remove `\"//tensorflow/python/framework:test_ops_kernels\"` from dependency;\r\n*  Disable v2 behavior for all v1 models: `//tensorflow/core/tfrt/saved_model/tests/gen_saved_model_v1.py`: Call `disable_v2_behavior()` in main;\r\n* Remove the `if_google` part in gen_rules to correctly generate test models", "Closing this issue since all blockers are worked-around. Thanks @qqfish and others for triaging!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53936\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53936\">No</a>\n", "I tried to compare the `TraceMe` of [TfrtModelRun](https://github.com/tensorflow/tensorflow/blob/eca06dbd5d190645d10686494ee222fafbb12094/tensorflow/core/tfrt/graph_executor/graph_executor.cc#L160) and [SessionRun](https://github.com/tensorflow/tensorflow/blob/eca06dbd5d190645d10686494ee222fafbb12094/tensorflow/core/common_runtime/direct_session.cc#L525), and find that for a `MatMul`-dominant model TFRT is much slower.\r\n**Session Run:**\r\n![image](https://user-images.githubusercontent.com/49047362/152891918-ca757f2c-b0cb-411c-ad30-f85da900a383.png)\r\n**TFRT:**\r\n![image](https://user-images.githubusercontent.com/49047362/152891976-24d44841-c8b1-430d-837c-f3169a639218.png)\r\n\r\n\r\nI have two questions about this:\r\n1. Are these two `TraceMe` comparable? If not, which section should be timed in order to compare the performance of TFRT and current runtime?\r\n2. It looks like current runtime comes with breakdowns of smaller `TraceMe` into op-level (e.g. showing which op / function consumes what amount of time). Are there equivalents in TFRT? Is there some config I need to turn on to see them?\r\n**TFRT:**\r\n![image](https://user-images.githubusercontent.com/49047362/152892055-472b0b7f-37a0-430e-9000-bb7f75f61a2a.png)\r\n(Only some fallback tensor conversion is shown)\r\n![image](https://user-images.githubusercontent.com/49047362/152892471-5cb87679-9b43-48d0-8357-5e435c6dba3e.png)\r\n\r\n**Session Run:**\r\n![image](https://user-images.githubusercontent.com/49047362/152892133-4f7ff578-1b88-423f-865e-0bea69802e4d.png)\r\n", "The threading looks suspicious: in TFRT, looks like only one thread is using for computation.\r\n", "Can you set the options as the following?\r\n\r\nIn SavedModel::Options.graph_execution_options.compile_options\r\n\r\nenable_native_ops = false\r\nenable_grappler = true\r\nhoist_invariant_ops = true // set this only if you don't modify variables\r\n\r\nYou can also tune \"cost_threshold\" to change threading behavior. A smaller value will utilize more parallelism.\r\n\r\nI think I can also change those defaults to a better value.", "Thanks @qqfish and @cky9301 ! After increasing number of threads and setting compile options as suggested, time cost of `TfrtModelRun` is almost on par with `SessionRun` (variating run to run with ~10% fluctuation).\r\n\r\nOne more question is: What is a general guideline for tuning `num_threads` and `num_blocking_threads` in\r\n```cpp\r\ntensorflow::tfrt_stub::Runtime::Create(\r\n      tensorflow::tfrt_stub::WrapDefaultWorkQueue(\r\n          tfrt::CreateMultiThreadedWorkQueue(num_threads, num_blocking_threads)));\r\n```", "You can set \"num_threads\" to the number of CPU cores you want to utilize for executing ops. \"num_blocking_threads\" is roughly not used for CPU ops. \"num_threads\" here is the same as \"inter_op_parallelism_threads\" in SessionOptions. By default, IIRC, SessionRun() uses tensorflow::port::MaxParallelism(). You can also use this number."]}, {"number": 53935, "title": "Fix nullptr exception in QuantizedMaxPool op when empty list is sent \u2026", "body": "\u2026to min_input or max_input parameters.\r\n\r\nPiperOrigin-RevId: 413960973\r\nChange-Id: I9e3ded593f3c4eabf0d6d5dc356e6a19a3ad2682", "comments": []}, {"number": 53934, "title": "Fix nullptr exception in QuantizedMaxPool op when empty list is sent \u2026", "body": "\u2026to min_input or max_input parameters.\r\n\r\nPiperOrigin-RevId: 413960973\r\nChange-Id: I9e3ded593f3c4eabf0d6d5dc356e6a19a3ad2682", "comments": []}, {"number": 53933, "title": "Fix nullptr exception in QuantizedMaxPool op when empty list is sent \u2026", "body": "\u2026to min_input or max_input parameters.\r\n\r\nPiperOrigin-RevId: 413960973\r\nChange-Id: I9e3ded593f3c4eabf0d6d5dc356e6a19a3ad2682", "comments": []}, {"number": 53932, "title": "Fix check-fail when bincount ops are passed invalid values.", "body": "PiperOrigin-RevId: 415063028\r\nChange-Id: I20f8dc09933ddca1111c4efbf9a3a1e863215d02", "comments": []}, {"number": 53931, "title": "Fix check-fail when bincount ops are passed invalid values.", "body": "PiperOrigin-RevId: 415063028\r\nChange-Id: I20f8dc09933ddca1111c4efbf9a3a1e863215d02", "comments": []}, {"number": 53930, "title": "Fix check-fail when bincount ops are passed invalid values.", "body": "PiperOrigin-RevId: 415063028\r\nChange-Id: I20f8dc09933ddca1111c4efbf9a3a1e863215d02", "comments": []}, {"number": 53929, "title": "Properly validate sparse tensor in `SparseTensorSliceDataset`", "body": "Existing validation was incomplete.\r\n\r\nPiperOrigin-RevId: 415375048\r\nChange-Id: I14cd18f29ede73286f3ffac35171bd15828997e9", "comments": []}, {"number": 53928, "title": "Properly validate sparse tensor in `SparseTensorSliceDataset`", "body": "Existing validation was incomplete.\r\n\r\nPiperOrigin-RevId: 415375048\r\nChange-Id: I14cd18f29ede73286f3ffac35171bd15828997e9", "comments": []}, {"number": 53927, "title": "Properly validate sparse tensor in `SparseTensorSliceDataset`", "body": "Existing validation was incomplete.\r\n\r\nPiperOrigin-RevId: 415375048\r\nChange-Id: I14cd18f29ede73286f3ffac35171bd15828997e9", "comments": []}, {"number": 53926, "title": "Cherrypick fix check failures on r2.5", "body": null, "comments": []}, {"number": 53925, "title": "Fix `CHECK` failures", "body": null, "comments": []}, {"number": 53924, "title": "Fix `CHECK` failures", "body": null, "comments": []}, {"number": 53923, "title": "Fix potential divide by zero error when executing FractionalMaxPool, \u2026", "body": "\u2026when pooling ratio is higher than input size for a particular dimension.\r\n\r\nPiperOrigin-RevId: 412151722\r\nChange-Id: I06e57cbb8eca43816eff79eac264fa7aae8f7163", "comments": []}, {"number": 53922, "title": "Fix potential divide by zero error when executing FractionalMaxPool, \u2026", "body": "\u2026when pooling ratio is higher than input size for a particular dimension.\r\n\r\nPiperOrigin-RevId: 412151722\r\nChange-Id: I06e57cbb8eca43816eff79eac264fa7aae8f7163", "comments": []}, {"number": 53921, "title": "Fix potential divide by zero error when executing FractionalMaxPool, \u2026", "body": "\u2026when pooling ratio is higher than input size for a particular dimension.\r\n\r\nPiperOrigin-RevId: 412151722\r\nChange-Id: I06e57cbb8eca43816eff79eac264fa7aae8f7163", "comments": []}, {"number": 53920, "title": "Add a check for Key being scalar tensor for MapStage and OrderedMapSt\u2026", "body": "\u2026age ops.\r\n\r\nAccording to documentation[1][2], key must be int64 value, but this wasn't enforced and the ops would fail with check failure for non-scalar key value.\r\n\r\n[1]https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/ordered-map-stage\r\n[2]https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/map-stage\r\n\r\nPiperOrigin-RevId: 413822112\r\nChange-Id: I9d118faf990e6361900aa32272eff486ad9f0e2e", "comments": []}, {"number": 53919, "title": "Add a check for Key being scalar tensor for MapStage and OrderedMapSt\u2026", "body": "\u2026age ops.\r\n\r\nAccording to documentation[1][2], key must be int64 value, but this wasn't enforced and the ops would fail with check failure for non-scalar key value.\r\n\r\n[1]https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/ordered-map-stage\r\n[2]https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/map-stage\r\n\r\nPiperOrigin-RevId: 413822112\r\nChange-Id: I9d118faf990e6361900aa32272eff486ad9f0e2e", "comments": []}]