[{"number": 13928, "title": "'Numpy dangling symbolic links' when building from source", "body": "### System information\r\n- **Fedora 26 x64**:\r\n- **TensorFlow installed from source**:\r\n- **TensorFlow version commit 53e7541cf7efa61ba22c9f042e07031d87c8f145 (oct 23 11:46)**:\r\n- **Python version 3.6**: \r\n- **Bazel version 0.5.4**:\r\n- **CUDA 8.0**\r\n- **cuDNN 8.0 v7**:\r\n- **GPU model GTX 1060**:\r\n- **bazel build -c opt --copt=-march=native --config=cuda //tensorflow/tools/pip_package:build_pip_package**:\r\n\r\n```\r\nYou have bazel 0.5.4- (@non-git) installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3\r\nFound possible Python library paths:\r\n  /usr/lib/python3.6/site-packages\r\n  /usr/lib64/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/lib/python3.6/site-packages]\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: Y\r\njemalloc as malloc support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n\r\nNo Google Cloud Platform support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: n\r\nNo Hadoop File System support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n\r\nNo Amazon S3 File System support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with GDR support? [y/N]: n\r\nNo GDR support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: n\r\nNo VERBS support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with OpenCL support? [y/N]: n\r\nNo OpenCL support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: \r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]: 7\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 6.1]\r\nDo you want to use clang as CUDA compiler? [y/N]: n\r\nnvcc will be used as CUDA compiler.\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\nDo you wish to build TensorFlow with MPI support? [y/N]: n\r\nNo MPI support will be enabled for TensorFlow.\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:   \r\n\r\n\r\nAdd \"--config=mkl\" to your bazel command to build with MKL support.\r\nPlease note that MKL on MacOS or windows is still not supported.\r\nIf you would like to use a local MKL instead of downloading, please set the environment variable \"TF_MKL_ROOT\" every time before build.\r\nConfiguration finished\r\n ~ \ue0b0 progs \ue0b1 tensorflow \ue0b0 master \ue0b0 $ \ue0b0 bazel build -c opt --copt=-march=native --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n......................................................................................................................................................................................................................................................................................................................\r\nWARNING: /home/torstein/progs/tensorflow/tensorflow/core/BUILD:1786:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/torstein/progs/tensorflow/tensorflow/tensorflow.bzl:1048:30.\r\nWARNING: /home/torstein/progs/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/torstein/progs/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nINFO: Found 1 target...\r\nINFO: From Compiling external/snappy/snappy-c.cc:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\nINFO: From Compiling external/snappy/snappy-sinksource.cc:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\nINFO: From Compiling external/snappy/snappy-stubs-internal.cc:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\nINFO: From Compiling external/snappy/snappy.cc:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\nexternal/snappy/snappy.cc: In member function 'void snappy::SnappySinkAllocator::Flush(size_t)':\r\nexternal/snappy/snappy.cc:1403:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     for (int i = 0; i < blocks_.size(); ++i) {\r\n                     ~~^~~~~~~~~~~~~~~~\r\nIn file included from external/snappy/snappy-internal.h:34:0,\r\n                 from external/snappy/snappy.cc:30:\r\nexternal/snappy/snappy.cc: In instantiation of 'bool snappy::SnappyScatteredWriter<Allocator>::AppendFromSelf(size_t, size_t) [with Allocator = snappy::SnappySinkAllocator; size_t = long unsigned int]':\r\nexternal/snappy/snappy.cc:715:13:   required from 'void snappy::SnappyDecompressor::DecompressAllTags(Writer*) [with Writer = snappy::SnappyScatteredWriter<snappy::SnappySinkAllocator>]'\r\nexternal/snappy/snappy.cc:799:3:   required from 'bool snappy::InternalUncompressAllTags(snappy::SnappyDecompressor*, Writer*, snappy::uint32) [with Writer = snappy::SnappyScatteredWriter<snappy::SnappySinkAllocator>; snappy::uint32 = unsigned int]'\r\nexternal/snappy/snappy.cc:1460:78:   required from here\r\nexternal/snappy/snappy.cc:1316:34: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     if (PREDICT_TRUE(offset - 1u < op_ptr_ - op_base_ && op_end <= op_limit_)) {\r\n                      ~~~~~~~~~~~~^~~~~~~~~~~~~\r\nexternal/snappy/snappy-stubs-internal.h:80:25: note: in definition of macro 'PREDICT_TRUE'\r\n #define PREDICT_TRUE(x) x\r\n                         ^\r\nERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/npy_3kcompat.h' is a dangling symbolic link.\r\nERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/ufunc_api.txt' is a dangling symbolic link.\r\nERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/arrayscalars.h' is a dangling symbolic link.\r\nERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/noprefix.h' is a dangling symbolic link.\r\nERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/utils.h' is a dangling symbolic link.\r\nERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/ufuncobject.h' is a dangling symbolic link.\r\nERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/npy_endian.h' is a dangling symbolic link.\r\nERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/ndarrayobject.h' is a dangling symbolic link.\r\nERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/npy_cpu.h' is a dangling symbolic link.\r\nERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/npy_no_deprecated_api.h' is a dangling symbolic link.\r\nERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/multiarray_api.txt' is a dangling symbolic link.\r\nERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/_numpyconfig.h' is a dangling symbolic link.\r\nERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/old_defines.h' is a dangling symbolic link.\r\nERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/__ufunc_api.h' is a dangling symbolic link.\r\nERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/__multiarray_api.h' is a dangling symbolic link.\r\nERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/npy_interrupt.h' is a dangling symbolic link.\r\nERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/npy_1_7_deprecated_api.h' is a dangling symbolic link.\r\nERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/halffloat.h' is a dangling symbolic link.\r\nERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/_neighborhood_iterator_imp.h' is a dangling symbolic link.\r\nERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/npy_common.h' is a dangling symbolic link.\r\nERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/ndarraytypes.h' is a dangling symbolic link.\r\nERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/numpyconfig.h' is a dangling symbolic link.\r\nERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/npy_os.h' is a dangling symbolic link.\r\nERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/npy_math.h' is a dangling symbolic link.\r\nERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/arrayobject.h' is a dangling symbolic link.\r\nERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/oldnumeric.h' is a dangling symbolic link.\r\nERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: not all outputs were created or valid.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 58.925s, Critical Path: 20.53s\r\n```", "comments": ["```\r\nsudo pip3 install --no-cache-dir --upgrade --force-reinstall --user numpy\r\nsudo pip3 install --no-cache-dir --upgrade --force-reinstall numpy\r\n```\r\nSolved this issue.", "Before doing this, whoever encounters with this problem needs to \"rm -rf /home/[usrname]/.cache\" first.", "The error persists after doing all the steps above\r\n```\r\nsudo pip2 install --no-cache-dir --upgrade --force-reinstall --user numpy\r\nsudo pip2 install --no-cache-dir --upgrade --force-reinstall numpy\r\nsudo rm -rf /home/[usrname]/.cache\r\n```\r\n\r\nOn inspection of the error, it seems the folder `/home/user/.cache/bazel/_bazel_user/1f82.....` does not even exist and `bazel clean` did not solve this issue.\r\n\r\nI deleted tensorflow folder to clone it again from scratch and this did it for me.\r\n"]}, {"number": 13927, "title": "R1.3", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "r1.3 fixes were already merged back into master."]}, {"number": 13926, "title": "Build from source issue (CUDA 7.5)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.4 RC0\r\n- **Python version**: 3.4.3\r\n- **Bazel version (if compiling from source)**: 0.7\r\n- **CUDA/cuDNN version**: CUDA7.5 , cudnn v5.1\r\n- **GPU model and memory**: GeForce GTX TITAN \r\n- **Exact command to reproduce**: `bazel build --config opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n### Describe the problem\r\nI am trying to build tensorflow 1.4 RC0 from source, getting compilation error that 'cusolverEigMode_t' has not been declared.\r\n\r\nLooks like this is the commit where this code was added:\r\nhttps://github.com/tensorflow/tensorflow/commit/e3413de529c3f762885efd62932f76445ed22653#diff-e4b1fa736000720d06dab76006540ec4R467\r\n\r\nI tried grepping for `cusolverEigMode_t`in my `/usr/local/cuda/` but could not find any reference, is it possible that `cusolverEigMode_t` is not supported in CUDA 7.5? \r\nIn that case, it should be noted that 1.4 is only supported for CUDA 8.0 and above\r\n\r\n### Source code / logs\r\n```\r\nERROR: /home/xxxxx/downloads/tensorflow/tensorflow/core/kernels/BUILD:839:1: C++ compilation of rule '//tensorflow/core/kernels:where_op' failed (Exit 1).\r\nIn file included from tensorflow/core/kernels/where_op.cc:42:0:\r\n./tensorflow/core/kernels/cuda_solvers.h:299:16: error: 'cusolverEigMode_t' has not been declared\r\n   Status Heevd(cusolverEigMode_t jobz, cublasFillMode_t uplo, int n,\r\n                ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n", "comments": ["@tfboyd do you know if we expect CUDA 7.5 to work with 1.4?", "Older versions of Cuda are handled as a if it works that is nice but not supported.  Not to pass this around but I think Amit might have a table he was going to publish to show compatibility.  @av8ramit And agreed, we should post something if we have not and if we could make it clear in the release notes that would be useful.  ", "https://www.tensorflow.org/versions/r1.4/install/install_sources#tested_source_configurations\r\nThis will be in master docs soon as well. ", "@case540 can we make the release notes for 1.4 more clear with regards to cuda compatibility?", "Not super familiar with CUDA compatibility. Any suggestions for text to add to the release notes about this issue?", "I do not think we should point this out in our release notes, as they mostly include which cuda/cudnn versions we support. but maybe we can make this clear in our install from sources page in our website?", "Hi,\r\n\r\nI can confirm TF 1.4.0 (stable release) fails to build with the following error on CUDA 7.5.", "I'm getting build errors as well with TF 1.4.0 and CUDA 7.5, but it works with CUDA 8. Unfortunately I can't use CUDA 8 because of outdated Nvidia drivers that I can't upgrade, so it would be great if CUDA 7.5 were supported.", "Officially we will  only support the latest CUDA version. So we are currently actively working to upgrade to CUDA 9 and drop support for CUDA 8.\r\nHowever, we will be happy to accept any contributions that fix the build issues on CUDA 7.5.", "Just a heads up that CUDA 9 does not appear to be supported on Ubuntu 14.04, which has a large installed base in ML.", "From the documentation here https://www.tensorflow.org/install/install_linux I understand that CUDA 7.0 is also officially supported by Tensorflow (given that I compile it myself).", "This policy of only supporting the most bleeding edge dependencies is very problematic for HPC centers. Kernel module updates required for the latest Cuda versions are a much bigger deal on a supercomputer than they are on a workstation. On Blue Waters, we are still on Cuda 7.5, although we hope to get 8 at some point. We won't be getting 9. Please understand that \"just run the most bleeding edge version of everything\" doesn't work for everyone and makes your software \"alpha\" in terms of reliability.", "I couldn't agree more.\r\n\r\nThis is why I build my own Debian/Ubuntu packages against distribution provided CUDA framework.\r\n\r\nI don't see installing latest bleeding edge CUDA toolkit by hands on production servers as a good practise...", "Supporting only one version of CUDA officially just means we have limited resources and other priorities unfortunately preempted older CUDA versions. But I definitely understand your side of the problem.\r\nI will escalate the issue, and see what we can do, but as always, we are happy to accept any contributions that will keep TF working for older CUDA versions. If you have any fixes, I can try to get patch releases of 1.4 our with these fixes to ensure TF 1.4 works with older CUDA versions.\r\n", "I was able to succesfully build TF 1.4.1 for CUDA 7.5 with a few modifications as in https://github.com/mtngld/tensorflow/commit/a6b7a05331670870a635a1da30c5e7ad0754e526\r\n\r\nI think it is a fairly small amount of changes required in order to support old CUDA versions, however I haven't fully tested this and there is probably more work required.", "Hi @mtngld!\r\nIt seems you are using older versions(1.x versions) of Tensorflow. Have you tried latest versions (TF 2.6/2.7 ) yet? Thanks!", "Well that was back in 2017 when we only had tf 1.4.x...\r\n\r\nI guess we can close this ticket as I don't think it is needed anymore with newer versions of both TF and CUDA.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 13925, "title": "Examples of GANs using tensorflow estimator", "body": "I found all the estimator examples in the tutorial assumes there is `x`(features) and `y`(labels).\r\n\r\nHowever, in the context of GANs-based method, there is no `y` in the dataset. Would there be any example of GANs using tensorflow estimator?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "For anyone who is also curious about this question, I posted a thread on stackoverflow: https://stackoverflow.com/questions/46940837/how-to-implement-gans-by-tensorflow-estimator"]}, {"number": 13924, "title": "Add clang style check as part of the sanity check", "body": "This fix is an effort to add clang style check as part of the sanity check.\r\n\r\n**NOTE: Not sure if this PR is relevant so feel free to close it if it doesn't make sense**\r\n\r\nIn `CONTRIBUTING.md` it has been advised to run `clang-format --style=google file.cc` so that Google coding style is conformed. However, there is no sanity check in the current Jenkins build so current .cc and .h files in the repo are not really conforming to the coding style.\r\n\r\nThis actually causes issues. In case a PR is submitted with `clang-format --style=google file.cc`, the reviewer may see additional unrelated changes which might be a distraction. The developer may also spent additional time to manually check for any discrepancies manually with additional unrelated style changes.\r\n\r\nThis fix adds the clang-format check to the ci build so that when `ci_sanity.sh` is running, it will use clang-format to make sure the code is conforming to the coding style as specified in `CONTRIBUTING.md`.\r\n\r\nOne thing that might need to take notice is the header order of the Eigen library. See\r\nhttps://github.com/tensorflow/tensorflow/pull/13907#issuecomment-338718110 for further details.\r\n\r\nBasically, if Eigen headers could be placed in any order, then no additional steps are needed. Otherwise, it is always possible to place the Eigen headers at the top, then leave one empty line like:\r\n```cpp\r\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\r\n\r\n#include \"tensorflow/core/framework/op.h\"\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n...\r\n```\r\n\r\nIn this way, even a run of `clang-format -i --style=google file.cc` will still respect the order and leave Eigen header at the top.\r\n\r\nThis PR is experimeal so it only checks `tensorflow/core/ops` directory. Other files could be added if this PR is OK.\r\n\r\nThis PR also sanitizes all files in `tensorflow/core/ops` directory so that it conforms to coding style requirement.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "/cc @vrv", "Thanks for the PR @yongtang, this will be very helpful! cc: @gunan ", "Jenkins, test this please.", "Thanks @yifeif @gunan, the PR has been updated with incremental options added. Please take a look and let me know if there are any issues.", "@tensorflow-jenkins test this please", "@yifeif The previous Jenkins error was caused by the fact that full check (instead of `--incremental`) was done on Jenkins' `Sanity Checks`.\r\n\r\nI have updated the PR with all `.cc` and `.h` files sanitized with `clang-format -i --style=Google` though a script I wrote.\r\n\r\nI also checked the header and make sure all `third_party/eigen3` headers remains in place (without recording). That could be verified through (no output is expected if all order remains):\r\n```sh\r\n$ git diff HEAD~XXX | grep -E '#include \"third_party/eigen3' | grep -E '^[-+]'\r\n```\r\n\r\nI think the PR will help the community to spend less time on style check and more time on implementation.\r\n\r\nThe PR was split into multiple commits for easy review. It could be squashed later.\r\n\r\nPlease take a look and let me know if there are any issues.", "Can we add the sanity checks to ensure new PRs are correctly formatted, but grandfather the existing style violations ? That will simplify the merging of this PR into our internal repository by reducing the number of merge conflicts to deal with. If needs be, we can always gradually fix the formatting in several smaller PRs spread over time.", "Thanks @benoitsteiner for the suggestion. The gradual strategy makes sense and it should help avoiding the merge conflict.\r\n\r\nI have stripped out the commits with style format changes, and enabled `--incremental` by default. The `--incremental` could be removed after all files are touched over time.\r\n\r\nPlease take a look and let me know if there are any issues.", "@tensorflow-jenkins test this please", "Jenkins, test this please."]}, {"number": 13923, "title": "Branch 173127955", "body": "", "comments": ["@annarev is working on fixing the android failure.\r\n@vrv FYI.", "Should we merge this as is then ? We can always merge the fix for the android failure later.", "Sounds good. Let's avoid increasing the skew.\r\nPlease feel free to merge right away. We can do a smaller push when the fix is in."]}, {"number": 13922, "title": "Add GPU support for Bucketize op", "body": "This fix tries to add GPU support for `Bucketize` op. Before this PR only CPU implementation is available.\r\n\r\nThis PR add GPU implementation with a CUDA kernel.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Thanks @rmlarsen for the review. The PR has been updated. Please take a look and let me know if there are any issues.", "Thanks @rmlarsen. The PR has been updated with CudaDeviceArrayOnHost used. Please take a look.", "@tensorflow-jenkins test this please", "@yongtang Looks good. Thanks for the contribution!"]}, {"number": 13921, "title": "Add int64 type `multiples` support for TileGrad.", "body": "This fix is a follow up of #13884 to add int64 type of `multiples` support for TileGrad for completeness.\r\n\r\n/cc @vrv\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "All tests passed except `MacOS Contrib`. I couldn't see the log but assume that is a CI or network error?"]}, {"number": 13920, "title": "Update build_all_ios.sh to support per arch builds", "body": "", "comments": ["Can one of the admins verify this patch?", "Will update with a new commit defaulting the libs built to what we have currently have. ", "closing this PR will submit a new one with the ability to build just one arch. "]}, {"number": 13919, "title": "tensorflow.python.framework.errors_impl.InvalidArgumentError", "body": "I have run a tensorflow code, which always give following errors:\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: The node 'div_1' has inputs from different frames. The input 'while/Mean_1' is in frame 'while/while/'. The input 'div_1/y' is in frame ''.\r\n\r\nAnd I have searched for a long time, but find no proper solutions. I really wonder that anyone could give me some advice.", "comments": ["This is likely better asked on stackoverflow.\r\n\r\nCan you should the code?", "Agreed, stackoverflow is the best place for this question. Without seeing any code, my best guess is that you have captured a Python Tensor object created within a tf.while_loop and are attempting to use it outside the loop. The error message in such a case is not very helpful, but the basic problem is that the value you are trying to use is not well-defined: which iteration should it pick? What if the loop doesn't execute at all (the condition is initially false)?"]}, {"number": 13918, "title": "build tensorflow for gpu faild ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:r1.3\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**:0.7.0\r\n- **CUDA/cuDNN version**:CUDA9.0/cuDNN7\r\n- **GPU model and memory**:GTX 660M / 2G memory\r\n\r\n\r\n### Describe the problem\r\nI'm building tensorflow for gpu from source according to the official guide https://www.tensorflow.org/install/install_sources , but alway faild, error message show in below:\r\n`ERROR: /home/dangerous/.cache/bazel/_bazel_dangerous/821f9ca421a3e885f021819a154f9a6e/external/nccl_archive/BUILD:33:1: output 'external/nccl_archive/_objs/nccl/external/nccl_archive/src/broadcast.cu.pic.o' was not created\r\nERROR: /home/dangerous/.cache/bazel/_bazel_dangerous/821f9ca421a3e885f021819a154f9a6e/external/nccl_archive/BUILD:33:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.`\r\n\r\n", "comments": ["cc @gunan @yifeif @benoitsteiner @vrv \r\n \r\nI'm tackling this build failure (among other build issues) today. I expect the fix to be sync'ed to GitHub within 1-2 days.\r\n\r\n"]}, {"number": 13917, "title": "make data file configurable(issue#13876)", "body": "Fix [issue#13876](https://github.com/tensorflow/tensorflow/issues/13876).\r\n\r\n- Add `fname` argument to set the data file of CIFAR10 dataset.\r\n\r\n@drpngx Please check if it is acceptable to add the flexibility in this way. If so, I'd like to update the [cifar100.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/_impl/keras/datasets/cifar100.py), [mnist.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/_impl/keras/datasets/mnist.py)  and so on. \r\n\r\n@fchollet Should I make the PR in keras repo or here?\r\n\r\n@uZeroJ Do you think whether it is necessary to add the `untar` argument?", "comments": ["Can one of the admins verify this patch?", "This would introduce an API discrepancy, so don't submit this PR. Instead, you can open a PR in the Keras GitHub repo. Such updates will later end up in `tf.keras`.", "@fchollet Ok. BTW: how often do they synchronize?"]}, {"number": 13916, "title": "can't able to build android_tensorflow_inference_java file in tensorflow", "body": "iam not able to android_tensorflow_inference_java.jar file in tensorflow using bazel.when i build i got the following error how can i solve this:\r\n\r\n**$ bazel build //tensorflow/contrib/android:android_tensorflow_inference_java**\r\n\r\n____Loading package: @local_config_xcode//\r\n____Loading complete.  Analyzing...\r\n____Loading package: tensorflow/java\r\n____Loading package: @bazel_tools//third_party/java/jarjar\r\n____Loading package: @bazel_tools//third_party/py/six\r\n____Loading package: @bazel_tools//src/main/native/windows\r\n____Loading package: @androidsdk//\r\nERROR: C:/tools/msys64/tmp/_bazel_deemsys/28x4acs5/external/androidsdk/BUILD.bazel:64:1: Traceback (most recent call last):\r\n        File \"C:/tools/msys64/tmp/_bazel_deemsys/28x4acs5/external/androidsdk/BUILD.bazel\", line 64\r\n                create_system_images_filegroups(system_image_dirs = [\"system-ima...\"])\r\n        File \"C:/tools/msys64/tmp/_bazel_deemsys/28x4acs5/external/bazel_tools/tools/android/android_sdk_repository_template.bzl\", line 298, in create_system_images_filegroups\r\n                int(apidir.split(\"-\")[1])\r\ninvalid literal for int() with base 10: \"N\".\r\nERROR: C:/tools/msys64/tmp/_bazel_deemsys/28x4acs5/external/androidsdk/BUILD.bazel:8:1: Target '@androidsdk//:windows' contains an error and its package is in error and referenced by '@androidsdk//:sdk-26'.\r\nERROR: C:/tools/msys64/tmp/_bazel_deemsys/28x4acs5/external/androidsdk/BUILD.bazel:8:1: Target '@androidsdk//:windows_msvc' contains an error and its package is in error and referenced by '@androidsdk//:sdk-26'.\r\nERROR: C:/tools/msys64/tmp/_bazel_deemsys/28x4acs5/external/androidsdk/BUILD.bazel:8:1: Target '@androidsdk//:windows_msys' contains an error and its package is in error and referenced by '@androidsdk//:sdk-26'.\r\nERROR: C:/tools/msys64/tmp/_bazel_deemsys/28x4acs5/external/androidsdk/BUILD.bazel:8:1: Target '@androidsdk//:windows' contains an error and its package is in error and referenced by '@androidsdk//:sdk-26'.\r\nERROR: C:/tools/msys64/tmp/_bazel_deemsys/28x4acs5/external/androidsdk/BUILD.bazel:8:1: Target '@androidsdk//:windows_msvc' contains an error and its package is in error and referenced by '@androidsdk//:sdk-26'.\r\nERROR: C:/tools/msys64/tmp/_bazel_deemsys/28x4acs5/external/androidsdk/BUILD.bazel:8:1: Target '@androidsdk//:windows_msys' contains an error and its package is in error and referenced by '@androidsdk//:sdk-26'.\r\nERROR: C:/tools/msys64/tmp/_bazel_deemsys/28x4acs5/external/androidsdk/BUILD.bazel:8:1: Target '@androidsdk//:windows' contains an error and its package is in error and referenced by '@androidsdk//:sdk-26'.\r\nERROR: C:/tools/msys64/tmp/_bazel_deemsys/28x4acs5/external/androidsdk/BUILD.bazel:8:1: Target '@androidsdk//:windows_msvc' contains an error and its package is in error and referenced by '@androidsdk//:sdk-26'.\r\nERROR: C:/tools/msys64/tmp/_bazel_deemsys/28x4acs5/external/androidsdk/BUILD.bazel:8:1: Target '@androidsdk//:windows_msys' contains an error and its package is in error and referenced by '@androidsdk//:sdk-26'.\r\nERROR: C:/tools/msys64/tmp/_bazel_deemsys/28x4acs5/external/androidsdk/BUILD.bazel:8:1: Target '@androidsdk//:windows' contains an error and its package is in error and referenced by '@androidsdk//:sdk-26'.\r\nERROR: C:/tools/msys64/tmp/_bazel_deemsys/28x4acs5/external/androidsdk/BUILD.bazel:8:1: Target '@androidsdk//:windows_msvc' contains an error and its package is in error and referenced by '@androidsdk//:sdk-26'.\r\nERROR: C:/tools/msys64/tmp/_bazel_deemsys/28x4acs5/external/androidsdk/BUILD.bazel:8:1: Target '@androidsdk//:windows_msys' contains an error and its package is in error and referenced by '@androidsdk//:sdk-26'.\r\nERROR: C:/tools/msys64/tmp/_bazel_deemsys/28x4acs5/external/androidsdk/BUILD.bazel:8:1: Target '@androidsdk//:sdk-26' contains an error and its package is in error and referenced by '@androidsdk//:sdk'.\r\nERROR: E:/arisai1/tensor/tensorflow/WORKSPACE:20:1: Target '@androidsdk//:sdk' contains an error and its package is in error and referenced by '//external:android/sdk'.\r\nERROR: Analysis of target '//tensorflow/contrib/android:android_tensorflow_inference_java' failed; build aborted.\r\n____Elapsed time: 25.324s\r\n", "comments": ["Please include the information requested in the new issue template.", "@ArigarasuthanRepo It appears you're attempting to build for Android on Windows, which is not expected to be supported until [bazel 1.0](https://bazel.build/roadmap.html). Until then please see https://github.com/tensorflow/tensorflow/issues/6385 for various workarounds, including building via Linux for Windows and the recommended prebuilt AAR integration via gradle + Android Studio (also described in tensorflow/contrib/android/README.me)."]}, {"number": 13915, "title": "Error when install tf from source", "body": "### Branch:\r\nmaster\r\n\r\n### Command:\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package \\\r\n--verbose_failures\r\n\r\n### Error Info:\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: /home/tianhz/project/tensorflow/tensorflow/tools/pip_package/BUILD:139:1 C++ compilation of rule '//tensorflow/stream_executor:cuda_platform' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command\r\n  (cd /home/tianhz/.cache/bazel/_bazel_FAREAST.tianhz/1aaf3c53d4483e0897f98d9f35329906/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda-8.0 \\\r\n    CUDNN_INSTALL_PATH=/usr/local/cuda-8.0 \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_CUDA_CLANG=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=3.0 \\\r\n    TF_CUDA_VERSION=8.0 \\\r\n    TF_CUDNN_VERSION=5.1.10 \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_OPENCL=0 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-march=native' '-std=c++11' '-march=native' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/stream_executor/_objs/cuda_platform/tensorflow/stream_executor/cuda/cuda_dnn.pic.d '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/stream_executor/_objs/cuda_platform/tensorflow/stream_executor/cuda/cuda_dnn.pic.o' -fPIC -DEIGEN_MPL2_ONLY -DTENSORFLOW_USE_JEMALLOC -DTF_USE_SNAPPY -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/nsync -iquote bazel-out/local_linux-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/jemalloc -iquote bazel-out/local_linux-opt/genfiles/external/jemalloc -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/local_linux-opt/genfiles/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/local_linux-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/local_linux-opt/genfiles/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/local_linux-opt/genfiles/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/local_linux-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/local_linux-opt/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/local_linux-opt/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/local_linux-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/local_linux-opt/genfiles/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/local_linux-opt/genfiles/external/local_config_cuda -isystem external/nsync/public -isystem bazel-out/local_linux-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jemalloc/include -isystem bazel-out/local_linux-opt/genfiles/external/jemalloc/include -isystem external/eigen_archive -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/local_linux-opt/genfiles/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/local_linux-opt/genfiles/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/local_linux-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/local_linux-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/local_linux-opt/genfiles/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/local_linux-opt/genfiles/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/local_linux-opt/genfiles/external/local_config_cuda/cuda/cuda/include -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers -c tensorflow/stream_executor/cuda/cuda_dnn.cc -o bazel-out/local_linux-opt/bin/tensorflow/stream_executor/_objs/cuda_platform/tensorflow/stream_executor/cuda/cuda_dnn.pic.o)^M\r\nINFO: Elapsed time: 654.834s, Critical Path: 213.07s^M\r\nFAILED: Build did NOT complete successfully^M\r\n\r\n## Other info:\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master(r1.4)\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.7.0\r\n- **CUDA/cuDNN version**:CUDA 8.0, cnDNN 5.1.10", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.", "Updated, I try to install tensorflow from source, but some errors occurred.", "Without more information we won't be able to help. "]}, {"number": 13914, "title": "s390x Tensorflow CI build failure ", "body": "Hello, \r\nTensorflow CI build fails with an error: \r\n```\r\njava.lang.NoClassDefFoundError: Could not initialize class jenkins.model.Jenkins$MasterComputer\r\n\tat org.jenkinsci.plugins.gitclient.AbstractGitAPIImpl.withRepository(AbstractGitAPIImpl.java:29)\r\n\tat org.jenkinsci.plugins.gitclient.CliGitAPIImpl.withRepository(CliGitAPIImpl.java:71)\r\n\tat sun.reflect.GeneratedMethodAccessor66.invoke(Unknown Source)\r\n\r\n```", "comments": ["@gunan Continuing Tensorflow s390x CI discussion here. \r\nFor VM-1 (http://ci.tensorflow.org/computer/ibm-contrib-s390x-1/) , jnlp service is up and running. Not sure about the error mentioned above. \r\nDo we need to upgrade/install any package?\r\n", "For VM-0, http://ci.tensorflow.org/computer/ibm-contrib-s390x-0/\r\nI have again started JNLP service still it shows 'Ping response time is too long or timed out.'.\r\n", "Sent you an email to debug the issue with s390x-0.\r\non the other machine, could you try replacing jenkins slave.jar with this one:\r\nhttp://ci.tensorflow.org/jnlpJars/slave.jar\r\n", "@gunan Issue resolved  after replacing jenkins slave.jar on s390x-1. Now builds are getting triggered on this node. About s390x-0, I have sent an email with details to you.", "s390x-0 is also now up. builds are getting triggered on this node also. ", "Looks like this specific issue was resolved?\r\nClosing, please reopen if this specific issue still persists."]}, {"number": 13913, "title": "R1.3", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->"]}, {"number": 13912, "title": "tensorflow is not importing even after successful installation ", "body": "in Centos 6.9 i am unable to import tensorflow-1.3.0 in any anaconda (2,3) and getting this error please resolve this issue ..\r\n\r\n\r\n\r\n\r\n![screenshot-7](https://user-images.githubusercontent.com/24870531/31872238-d008d862-b7d7-11e7-8f4a-0f2ad9b159a1.png)\r\n![screenshot-6](https://user-images.githubusercontent.com/24870531/31872240-d04142c4-b7d7-11e7-8cab-9bd5b2f931c7.png)\r\n", "comments": ["Hi!\r\n\r\nPlease try this:\r\n\r\n> conda install -c jjhelmus tensorflow \r\n\r\nIf it does not work, please try googling [glibc_2.17 not found centos 6](https://www.google.com/search?q=glibc_2.17+not+found+centos+6).\r\n\r\nThe previous solution has been taken from this [link](https://github.com/conda-forge/tensorflow-feedstock/issues/11).\r\n\r\nI hope it helps!\r\n\r\nHave a great day,\r\n\r\nMiguel \u00c1ngel", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "sorry for inconvenience the issue was resolved.", "Thanks!"]}, {"number": 13911, "title": "Branch 173060283", "body": "", "comments": ["@gunan @benoitsteiner Something in this PR is causing all non-kokoro infrastructure to fail, but I can't figure out why.  I'll leave this push out so you can look at logs to debug.", "Maybe add `apt-key adv --keyserver keyserver.ubuntu.com  --recv 084ECFC5828AB726` to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/install/install_deb_packages.sh#L30? ", "Tried that and let's see what happens.\r\n\r\nLooks like it got past the previous failures, thanks @yifeif !  Still can't merge yet due to the failure below, but this is close.", "@annarev can you take a look for this possible failure?\r\n\r\n```\r\n22:42:50 In file included from /workspace/tensorflow/contrib/makefile/gen/proto_text/tensorflow/core/framework/node_def.pb_text-impl.h:5:0,\r\n22:42:50                  from /workspace/tensorflow/contrib/makefile/gen/proto_text/tensorflow/core/framework/node_def.pb_text.cc:2:\r\n22:42:50 /workspace/tensorflow/contrib/makefile/gen/proto/tensorflow/core/framework/attr_value.pb.h:9:42: fatal error: google/protobuf/stubs/common.h: No such file or directory\r\n22:42:50  #include <google/protobuf/stubs/common.h>\r\n22:42:50                                           ^\r\n22:42:50 compilation terminated.\r\n22:42:50 In file included from /workspace/tensorflow/contrib/makefile/gen/proto_text/tensorflow/core/framework/api_def.pb_text-impl.h:5:0,\r\n22:42:50                  from /workspace/tensorflow/contrib/makefile/gen/proto_text/tensorflow/core/framework/api_def.pb_text.cc:2:\r\n22:42:50 /workspace/tensorflow/contrib/makefile/gen/proto/tensorflow/core/framework/api_def.pb.h:9:42: fatal error: google/protobuf/stubs/common.h: No such file or directory\r\n22:42:50  #include <google/protobuf/stubs/common.h>\r\n22:42:50                                           ^\r\n```\r\n"]}, {"number": 13910, "title": "Feature Request: Getting a collection of variable from specific Graph", "body": "Hi,\r\n\r\nFor debug purposes, I need to create a collection variables from a specific graph.\r\nI have defined my model in a predefined graph using something like:\r\n\r\npg=tf.Graph()\r\nwith pg.as_default():\r\n    ...\r\n\r\nI define my session as:\r\n\r\nsess = tf.Session(config=config, graph=pg)\r\nIf I need to create a collection of trainable variable in graph pg, I try to use:\r\ntf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES) or simply tf.trainable_variables(), It doesn't return anything .\r\n\r\nIf I do the same experiment, using the default 'default graph', and without using 'with', constructs, I am able to get the collection of variables.\r\n\r\nI suspect, tf.get_variables is not looking in pg, I believe a method for extracting collection of variables from a given graph, or device would be useful for everybody.\r\n\r\nIf the feature already exist, please accept my apology for wasting your precious time.\r\nI did check on stack overflow and other avenues suggested by Google.\r\n\r\n\r\n-Regards", "comments": ["It exists. I could call the get_Collection on the graph itself. Sorry for confusion."]}, {"number": 13909, "title": "Add int64 Tperm type support for `Transpose`", "body": "This fix adds int64 Tperm support for `Transpose`. In `array_ops.cc`, `Transpose` and `ConjugateTranspose` have been specified as accepting int32 and int64 perm types. However, only int32 kernels has been registered.\r\n\r\nThis fix adds the int64 perm support by removing the constraint on Tperm, resolve the type at runtime, and copying the data type accordingly to correctly handle the int64/int32 types.\r\n\r\nAdditional tests have been added as well.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 13908, "title": "Add int64 shape support on GPU for stateless random ops.", "body": "This fix adds int64 shape support on GPU for stateless random ops `StatelessRandomUniform`, `StatelessRandomNormal`, `StatelessTruncatedNormal`.\r\n\r\nThe int64 shape for stateless random ops is already supported on CPU with int32/int64 processed properly through `MakeShape`.\r\n\r\nHowever, on GPU a type constraint `.TypeConstraint<int32>(\"T\")` has been improperly added. Such a type constraint actually prevents an int64 shape type to run on GPU. (As a comparision, no type constraint on CPU).\r\n\r\nThis fix removes the type constraint and allows int64 shape to be run on GPU. This fix also adds test cases for int64 shape support on stateless random ops.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 13907, "title": "Add int64 padding support for MirrorPad", "body": "This fix adds int64 padding support for `MirrorPad`. In the `array_ops.cc` the `MirrorPad`/`MirrorPadGrad` has been specified as supporting int64 padding. The related kernels does not have the int64 padding registered though.\r\n\r\nThis fix adds the int64 padding support. This fix also adds additional test cases for coverage.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "@vrv Thanks for the review and update in the PR. And sorry for missing the Eigen header issue.\r\n\r\nOn a side note, I did notice that if we put an additional empty line in between Eigen header and the rest of the header, then a `clang-format -i --style=google file.cc` will actually respect the order that Eigen header is at the top.\r\n\r\nMaybe it is worth the effort to scan through and do that, so that any developer could safely run `clang-format` (or automatically run by jenkins) without worrying about the order mess-up?", "Yes, that sounds like a good idea!  But I also might be wrong about the eigen headers thing.  It was required at some point because Eigen on CPU required a define to use multiple threads, but someone might want to double check before doing this so we don't end up causing a lot of wasted work."]}, {"number": 13906, "title": "Conditional input to a sequence to sequence model (word+character hybrid network)", "body": "I need to create a sequence to sequence model where in the encoder and decoder are both LSTM networks but the encoder takes the inputs from either of the following cases\r\n\r\n1.Normal vector representation of a word (Embedding vector) - when the word input is present in the vocabulary\r\n\r\n2.Output of another LSTM network - when the word is out of vocabulary and a separate character based LSTM is used to generate an embedding on the fly\r\n\r\nConsider the following example sentence:\r\n\"The brown fox jumped over the lazy dog\"\r\n\r\nAssume these are the words present in the vocabulary: The, brown, jumped, over, dog - These words are fed to the seq2seq encoder as such\r\n\r\nout of vocabulary(OOV) words are: fox, lazy - These words are passed to a character LSTM and the output of the same is passed to the seq2seq model along with the above words\r\n\r\nThese both word level and character level encoder needs to be trained end to end simultaneously. How do we model the input layer of the seq2seq model to achieve this scenario?\r\n\r\nReference paper:\r\nhttp://aclweb.org/anthology/P/P16/P16-1100.pdf\r\n", "comments": ["@Raghava14 Please review guideline before raising an issue [here](https://github.com/tensorflow/tensorflow/blob/master/ISSUE_TEMPLATE.md)\r\nYour concern can be solved on [stackoverflow.com](https://stackoverflow.com/questions/tagged/tensorflow) where thousands of machine learning practitioners can suggest you the best way.", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Ah didn't see your reply @printdhruv, thanks :)", "@skye Missed to save your time:)"]}, {"number": 13905, "title": "Add a GPU kernel for tf.dynamic_partition.", "body": "This PR partially addresses issue #5965.\r\nThe implementation follows closely the outline proposed by @ekelsen in the comments:\r\n- use cub to radix-sort the information in partitions,\r\n- compute the dimension of the output and allocate it,\r\n- use tf.gather to move data to the correct output tensor.\r\n\r\nI wrote a benchmark test to compare it to the CPU implementation. The speedup is substantial:\r\n\r\n```\r\nBenchmark                          Time(ns) Iterations\r\n------------------------------------------------------\r\nBM_cpu_dynpart_float_2/1         2124112900        100\t 15.8M items/s\r\nBM_cpu_dynpart_float_2/256         28420220        100\t 1180.7M items/s\r\nBM_cpu_dynpart_float_100/1       1944110960        100\t 17.3M items/s\r\nBM_cpu_dynpart_float_100/256       48420780        100\t 693.0M items/s\r\nBM_cpu_dynpart_double_2/1         976543960        100\t 17.2M items/s\r\nBM_cpu_dynpart_double_2/256        24370610        100\t 688.4M items/s\r\nBM_cpu_dynpart_double_100/1       905003160        100\t 18.5M items/s\r\nBM_cpu_dynpart_double_100/256      50220970        100\t 334.1M items/s\r\nBM_cpu_dynpart_complex64_2/1     1065424130        100\t 15.7M items/s\r\nBM_cpu_dynpart_complex64_2/256     24578800        100\t 682.6M items/s\r\nBM_cpu_dynpart_complex64_100/1    991734960        100\t 16.9M items/s\r\nBM_cpu_dynpart_complex64_100/256   50632520        100\t 331.4M items/s\r\n\r\nBM_gpu_dynpart_float_2/1           57177980        100\t 586.8M items/s\r\nBM_gpu_dynpart_float_2/256          5857340        100\t 5728.6M items/s\r\nBM_gpu_dynpart_float_100/1         66666680        100\t 503.3M items/s\r\nBM_gpu_dynpart_float_100/256        6203580        100\t 5408.9M items/s\r\nBM_gpu_dynpart_double_2/1          30886780        100\t 543.2M items/s\r\nBM_gpu_dynpart_double_2/256         3717482        164\t 4513.1M items/s\r\nBM_gpu_dynpart_double_100/1        37068810        100\t 452.6M items/s\r\nBM_gpu_dynpart_double_100/256       4507507        142\t 3722.1M items/s\r\nBM_gpu_dynpart_complex64_2/1       32092400        100\t 522.8M items/s\r\nBM_gpu_dynpart_complex64_2/256      3940877        154\t 4257.2M items/s\r\nBM_gpu_dynpart_complex64_100/1     38496700        100\t 435.8M items/s\r\nBM_gpu_dynpart_complex64_100/256    5081670        100\t 3301.5M  #items/s\r\n```\r\n\r\nThe gpu version runs 4.8 - 37 times faster, with the biggest gain obtained for 1D data.\r\nThe benchmark tests used a 128MD data buffer, either 1D or with 256 columns, and num_partitions was either 2 or 100.\r\n\r\nThe drawback to the implementation is that it requires some additional device memory.\r\nIf I is the size of the input, O the size of the output, N the size of partitions, and P the number of partitions, the total memory cost is roughly\r\nI + P + max(5N, O + N),\r\nso about 4N additional memory is needed in the worst-case.\r\nMost of it comes from using cub::RadixSort.\r\n\r\nHowever, N is large only if data is 1D, and this additional memory cost becomes prohibitive only for gigantic 1D vectors (>= 512MB), which I don't think is a common use case.", "comments": ["Can one of the admins verify this patch?", "Thanks for this PR!  It will be great to have a GPU version of this function.", "Hi @ekelsen! Thank you for taking the time to do the review! I made an update following your comments.\r\nHowever, I'm not sure I did everything you wanted. I used GetCudaStream() in the call for cub::DeviceRadixSort, but I still need c->op_device_context()->stream() to do ThenMemcpy. Do you want me to change this too?", "Hi @vrv. Is this good to go? Or is there anything else I should do?", "@tensorflow-jenkins test this please", "@gunan I am not sure but it looks like PR testing sometimes does not sync to HEAD before testing, so fixed failures in other PRs don't get reflected here?", "@andrewharp looks safe to merge.", "It is possible. I think the system just tests the source branch.\r\nI agree the branch seems to be safe to merge", "I could merge master into my branch and then you could test again. Should I?", "I think this is safe to merge.  ping @andrewharp (for this and all other PRs i've marked as 'awaiting testing and ready to merge' where the only on failures are these two).", "With this change, dynamic_partition_op_test fails on GPU. \r\nHere is your failure log:\r\n\r\n```\r\nRunning test tensorflow/python/kernel_tests/dynamic_partition_op_test on GPU 0\r\n2017-11-02 00:04:33.441040: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2017-11-02 00:04:38.353134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-11-02 00:04:38.389940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1031] Found device 0 with properties: \r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:04.0\r\ntotalMemory: 11.17GiB freeMemory: 11.11GiB\r\n2017-11-02 00:04:38.389972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\r\n..2017-11-02 00:04:38.556994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\r\n2017-11-02 00:04:40.024293: F ./tensorflow/core/util/cuda_kernel_helper.h:160] Check failed: work_element_count > 0 (0 vs. 0)\r\n/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/tensorflow/python/kernel_tests/dynamic_partition_op_test.runfiles/org_tensorflow/tensorflow/tools/ci_build/gpu_build/parallel_gpu_execute: line 40: 28832 Aborted                 (core dumped) $@\r\n```\r\n\r\nCould you fix the failure?", "I think I fixed it. Would you mind testing again?", "Jenkins, test this please.", "There is a single broken target: //tensorflow/examples/speech_commands:accuracy_utils_test \r\nIt does not seem related to my change and I don't know what to do. Please, can you help me?"]}, {"number": 13904, "title": "Make special_math._ndtri work with partially-specified shapes", "body": "Current implementation of the `special_math._ndtri` function does not work with tensors of partially-known shape, which prevents one from computing Normal distribution's quantile function for non-fixed size batches. ", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please", "@vrv, what happened to the tests? Looks like some kind of an internal CI error.", "Let's try it again, our CI infra is having lots of problems lately as we transition to a new system."]}, {"number": 13903, "title": "Feature request: make `smart_cond` public API", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.3.0-24-g658866597\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**: None\r\n\r\n### Describe the problem\r\n\r\nCurrently `tf.cond` does not work if predicate is a Python boolean. As a result, people frequently have to write conditional statements twice, one with `if` statement, and one with `tf.cond` call. There is a `smart_cond` in `tensorflow/python/layers/utils.py`, but it is not in the public API or searchable in documentation. Petition to make it public or just integrate the smartness in `tf.cond` altogether. It is not a big change and will not impact backwards compatibility.", "comments": ["Here's the code: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/utils.py#L180\r\n\r\n/CC @martinwicke \r\n", "(From API review: It seems find to add it, contributions welcome :)", "I can work on it and send a PR later today.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you."]}, {"number": 13902, "title": "Op type not registered 'GatherTree' in binary running on..", "body": "Hi,\r\n\r\nI'm trying to use the GTT (Graph Transform Tool) on NMT model (From the NMT sample project),\r\nhttps://github.com/tensorflow/nmt\r\n\r\nI get the following error :\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=/tmp/NMT_frozenGraphPB.pb --out_graph=Quant_NMT.pb --inputs='' --outputs='attention' --transforms='add_default_attributes quantize weights quantize_nodes'\r\n\r\n\"Op type not registered 'GatherTree' in binary running on .... Make sure the Op and Kernel are registered in the binary running in this process.\"\r\n\r\nAny idea how to solve this issue?\r\n\r\nThx\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo code change , using GTT tool example\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nFrom Source\r\n- **TensorFlow version (use command below)**:\r\nLatest\r\n- **Python version**: \r\n3.5 , 3.6 and 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\nNone\r\n- **GPU model and memory**:\r\nTried in several systems and installations \r\n- **Exact command to reproduce**:\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=/tmp/NMT_frozenGraphPB.pb --out_graph=Quant_NMT.pb --inputs='' --outputs='attention' --transforms='add_default_attributes quantize weights quantize_nodes'\r\n", "comments": ["Sounds like the same issue as https://github.com/tensorflow/tensorflow/issues/12927. One workaround would be to add \"from tensorflow.contrib.seq2seq.python.ops import beam_search_ops\" to the graph transform tool so it picks up the kernel.\r\n\r\nNot closing this immediately because there may be different solutions. Even if we don't load contrib eagerly we may want to import some contrib kernels in the graph transform tool.", "Thanks for your reply , \r\nsince i'm using the C++ in transform_graph\r\nsolved it using small change in the BUILD file of graph transform.\r\n...\r\nLine 216:        \"//tensorflow/contrib/seq2seq:beam_search_ops_op_lib\",\r\n...\r\n\r\n*It will help to add a short guide or flag in the GTT tools for using ops from contrib  (not part of the default builds)\r\n\r\n\r\n\r\n\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly."]}, {"number": 13901, "title": "add segment_reduction_ops to tf_op_files", "body": "fix problem:\r\n`No OpKernel was registered to support Op 'SegmentSum'`", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 13900, "title": "tfdbg doesn't work with tensorflow GPU version?", "body": "Could the dear developers confirm that tfdbg works with tensorflow GPU version or not?\r\nAt least on my platform it doesn't work.\r\nOS: Red Hat Enterprise Linux Workstation release 7.3 (Maipo)\r\nPython: Python 2.7.5\r\ntensorflow (1.4.0rc0) (compiled from source with CUDA enabled, CUDA 8.0.61, cuDNN8-7.0.1)\r\nnVidia Quadro P5000\r\n\r\nthe code for reproducing the problem is as follows:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.python import debug as tf_debug\r\n\r\nt = tf.constant(np.ones((10, 16)), tf.float32)\r\n\r\ndef weight_variable(shape):\r\n    initial = tf.truncated_normal(shape, stddev = 0.1)\r\n    return tf.Variable(initial)\r\n\r\ndef bias_variable(shape):\r\n    initial = tf.constant(0., shape = shape)\r\n    return tf.Variable(initial)\r\n\r\nlin1_output_size = 32\r\nlin2_output_size = 1\r\n\r\nwith tf.name_scope(\"lin1\"):\r\n    weight_shape = t.get_shape().as_list()[1:] + [lin1_output_size]\r\n    lin1_output = tf.matmul(t, weight_variable(weight_shape)) + bias_variable([lin1_output_size])\r\n\r\nwith tf.name_scope(\"lin2\"):\r\n    weight_shape = [lin1_output_size, lin2_output_size]\r\n    lin2_output = tf.matmul(lin1_output, weight_variable(weight_shape)) + bias_variable([lin2_output_size])\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    sess = tf_debug.LocalCLIDebugWrapperSession(sess)\r\n\r\n    print(sess.run(lin2_output))\r\n\r\n``` \r\n\r\nThe phenomenon is after invoking stepper and steps for several steps, it is reporting segmentation fault and exit.\r\n\r\nI just wonder is it due to GPU issue?", "comments": ["The \"run\" mode of tfdbg should be compatible with tensorflow GPU. The \"stepper\" mode of tfdbg is being deprecated and it not guaranteed to work with GPUs.", "@caisq Hi, shanqing, what's the story behind this? I mean why stepping is gonna be deprecated? Sometimes it is quite useful for looking into under the hood. Any thread or discussion relates to this decision? Thanks. ", "@mingyr, the information you can get from stepping, with the exception from tensor value overriding, should all be available from the `tfdbg> run` mode. The `tfdbg> run` mode also provides visibility into intermediate graph/tensor states at runtime. It is the main topic of our guide at:\r\nhttps://www.tensorflow.org/programmers_guide/debugger\r\n\r\nJust to be abundantly clear, tfdbg is not deprecated; only its stepper part is. The stepping support is being moved to a TensorBoard feature based on tfdbg. The new TensorBoard feature will provide a more interactive and visual step-debugging experience. We are currently working on the new feature. Please see https://github.com/tensorflow/tensorflow/issues/10478 for more related discussion.", "@caisq Great, appreciating the detailed explanation.  Please feel free to close this thread (or give me an indication if shall I close it). Many thanks."]}, {"number": 13899, "title": "nsync lib name change", "body": "Now, the lib name is `nsync.a`. It will cause problem `lnsync` not found in cmake. So, change it to `libnsync.a`", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "This doesn't seem to work, is there some kind of update that needs to be done first?", "Yes. I found the name in origin nsync project is `nsync.a`. So, I add `mv nsync.a libnsync.a` here.\r\n", "Can you do this instead? https://stackoverflow.com/questions/31038963/how-do-you-rename-a-library-filename-in-cmake", "Fine. I will use absolute path for `nsync.a`."]}]