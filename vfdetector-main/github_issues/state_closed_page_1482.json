[{"number": 8482, "title": "[Windows - Bazel] ERROR: Executing genrule @llvm//:datatypes_gen failed", "body": "Windows 10\r\nBazel 0.4.5\r\nVisual C++ 2015\r\nMsys2 v20160205\r\nPython 3.5 (python.org distribution)\r\n\r\nI am getting these genrule errors when building.\r\n\r\nConfiguring:\r\n```\r\nAdriano@Adriano MSYS /c/tensorflow\r\n$ ./configure\r\nPlease specify the location of python. [Default is /c/Users/Adriano/AppData/Local/Programs/Python/Python35/python]:\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] y\r\nXLA JIT support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  C:\\Users\\Adriano\\AppData\\Local\\Programs\\Python\\Python35\r\n  C:\\Users\\Adriano\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\Adriano\\AppData\\Local\\Programs\\Python\\Python35]\r\n\r\nUsing python library path: C:\\Users\\Adriano\\AppData\\Local\\Programs\\Python\\Python35\r\nJunction created for util\\python\\python_include <<===>> C:\\Users\\Adriano\\AppData\\Local\\Programs\\Python\\Python35\\include\r\nJunction created for util\\python\\python_lib <<===>> C:\\Users\\Adriano\\AppData\\Local\\Programs\\Python\\Python35\r\nJunction created for third_party\\py\\numpy\\numpy_include <<===>> C:\\Users\\Adriano\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\numpy\\core\\include\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0]:\r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5.1\r\nPlease specify the location where cuDNN 5.1 library is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0]:\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 5.0\r\n.......................................................................................\r\nINFO: All external dependencies fetched successfully.\r\nConfiguration finished\r\n```\r\n\r\nBuilding:\r\n```\r\n\r\n$ export BUILD_OPTS='--cpu=x64_windows_msvc --host_cpu=x64_windows_msvc --copt=/w --verbose_failures --experimental_ui'\r\n\r\nAdriano@Adriano MSYS /c/tensorflow\r\n$ bazel build -c opt $BUILD_OPTS --config=win-cuda tensorflow/tools/pip_package:build_pip_package\r\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package.\r\nINFO: Found 1 target...\r\nINFO: From Compiling tensorflow/core/util/version_info.cc [for host]:\r\nWarning: Unmatched arguments: -Iexternal/gemmlowp -fno-exceptions -msse3\r\nINFO: From Compiling tensorflow/core/lib/hash/crc32c_accelerate.cc [for host]:\r\nWarning: Unmatched arguments: -Iexternal/gemmlowp -fno-exceptions -msse3 -msse4.2\r\nSlow read: a 67403059-byte read from C:/tools/msys64/tmp/_bazel_adriano/x1e5egqw/external/local_config_cuda/cuda/include/sobol_direction_vectors.h took 9044ms.\r\nINFO: From Compiling external/farmhash_archive/src/farmhash.cc [for host]:\r\nC:\\tools\\msys64\\tmp\\_bazel_adriano\\x1e5egqw\\execroot\\tensorflow\\external\\farmhash_archive\\src\\farmhash.cc(394): note: see reference to function template instantiation 'T util::DebugTweak<uint64_t>(T)' being compiled\r\n        with\r\n        [\r\n            T=uint64_t\r\n        ]\r\nC:\\tools\\msys64\\tmp\\_bazel_adriano\\x1e5egqw\\execroot\\tensorflow\\external\\farmhash_archive\\src\\farmhash.cc(1879): note: see reference to function template instantiation 'T util::DebugTweak<uint32_t>(T)' being compiled\r\n        with\r\n        [\r\n            T=uint32_t\r\n        ]\r\nERROR: C:/tools/msys64/tmp/_bazel_adriano/x1e5egqw/external/llvm/BUILD:162:1: Executing genrule @llvm//:datatypes_gen failed: bash.exe failed: error executing command\r\n  cd C:/tools/msys64/tmp/_bazel_adriano/x1e5egqw/execroot/tensorflow\r\n  SET PATH=C:\\tools\\msys64\\usr\\bin;C:\\Users\\Adriano\\AppData\\Local\\Programs\\Python\\Python35\\;C:\\bazel\\;c;C:\\tools\\msys64\\Program Files\\Java\\jdk1.8.0_112\\;C:\\tools\\msys64\\usr\\local\\bin;C:\\tools\\msys64\\usr\\bin;C:\\tools\\msys64\\usr\\bin;C:\\tools\\msys64\\opt\\bin;C:\\Windows\\System32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\tools\\msys64\\usr\\bin\\site_perl;C:\\tools\\msys64\\usr\\bin\\vendor_perl;C:\\tools\\msys64\\usr\\bin\\core_perl\r\n  C:/tools/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/third_party/llvm/expand_cmake_vars \"HAVE_DIRENT_H=1\" \"HAVE_DLFCN_H=1\" \"HAVE_ERRNO_H=1\" \"HAVE_EXECINFO_H=1\" \"HAVE_FCNTL_H=1\" \"HAVE_INTTYPES_H=1\" \"HAVE_PTHREAD_H=1\" \"HAVE_SIGNAL_H=1\" \"HAVE_STDINT_H=1\" \"HAVE_SYS_IOCTL_H=1\" \"HAVE_SYS_MMAN_H=1\" \"HAVE_SYS_PARAM_H=1\" \"HAVE_SYS_RESOURCE_H=1\" \"HAVE_SYS_STAT_H=1\" \"HAVE_SYS_TIME_H=1\" \"HAVE_SYS_TYPES_H=1\" \"HAVE_TERMIOS_H=1\" \"HAVE_UNISTD_H=1\" \"HAVE_ZLIB_H=1\" \"HAVE_BACKTRACE=1\" \"HAVE_DLOPEN=1\" \"HAVE_FUTIMES=1\" \"HAVE_GETCWD=1\" \"HAVE_GETPAGESIZE=1\" \"HAVE_GETRLIMIT=1\" \"HAVE_GETRUSAGE=1\" \"HAVE_GETTIMEOFDAY=1\" \"HAVE_INT64_T=1\" \"HAVE_ISATTY=1\" \"HAVE_LIBEDIT=1\" \"HAVE_LIBPTHREAD=1\" \"HAVE_LIBZ=1\" \"HAVE_MKDTEMP=1\" \"HAVE_MKSTEMP=1\" \"HAVE_MKTEMP=1\" \"HAVE_PREAD=1\" \"HAVE_PTHREAD_GETSPECIFIC=1\" \"HAVE_PTHREAD_MUTEX_LOCK=1\" \"HAVE_PTHREAD_RWLOCK_INIT=1\" \"HAVE_REALPATH=1\" \"HAVE_SBRK=1\" \"HAVE_SETENV=1\" \"HAVE_SETRLIMIT=1\" \"HAVE_SIGALTSTACK=1\" \"HAVE_STRERROR=1\" \"HAVE_STRERROR_R=1\" \"HAVE_STRTOLL=1\" \"HAVE_SYSCONF=1\" \"HAVE_UINT64_T=1\" \"HAVE__UNWIND_BACKTRACE=1\" \"ENABLE_BACKTRACES=1\" \"LLVM_BINDIR=/dev/null\" \"LLVM_DISABLE_ABI_BREAKING_CHECKS_ENFORCING=0\" \"LLVM_ENABLE_ABI_BREAKING_CHECKS=0\" \"LLVM_ENABLE_THREADS=1\" \"LLVM_ENABLE_ZLIB=1\" \"LLVM_HAS_ATOMICS=1\" \"LLVM_INCLUDEDIR=/dev/null\" \"LLVM_INFODIR=/dev/null\" \"LLVM_MANDIR=/dev/null\" \"LLVM_NATIVE_TARGET=1\" \"LLVM_NATIVE_TARGETINFO=1\" \"LLVM_NATIVE_TARGETMC=1\" \"LLVM_NATIVE_ASMPRINTER=1\" \"LLVM_NATIVE_ASMPARSER=1\" \"LLVM_NATIVE_DISASSEMBLER=1\" \"LLVM_ON_UNIX=1\" \"LLVM_PREFIX=/dev/null\" \"LLVM_VERSION_MAJOR=0\" \"LLVM_VERSION_MINOR=0\" \"LLVM_VERSION_PATCH=0\" \"LTDL_SHLIB_EXT=.so\" \"PACKAGE_NAME=llvm\" \"PACKAGE_STRING=llvm tensorflow-trunk\" \"PACKAGE_VERSION=tensorflow-trunk\" \"RETSIGTYPE=void\" \"LLVM_HOST_TRIPLE=x86_64-unknown-linux_gnu\" \"LLVM_DEFAULT_TARGET_TRIPLE=x86_64-unknown-linux_gnu\" \"LLVM_NATIVE_ARCH=X86\" \"HAVE_MALLOC_H=1\" \"HAVE_LINK_H=1\" \"HAVE_MALLINFO=1\" \"HAVE_FUTIMENS=1\"< external/llvm/include/llvm/Support/DataTypes.h.cmake > bazel-out/host/genfiles/external/llvm/include/llvm/Support/DataTypes.h: com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Adriano\\AppData\\Local\\Programs\\Python\\Python35\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"C:\\Users\\Adriano\\AppData\\Local\\Programs\\Python\\Python35\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"bazel-out\\host\\bin\\third_party\\llvm\\expand_cmake_vars\\__main__.py\", line 168, in <module>\r\n  File \"bazel-out\\host\\bin\\third_party\\llvm\\expand_cmake_vars\\__main__.py\", line 115, in Main\r\n  File \"bazel-out\\host\\bin\\third_party\\llvm\\expand_cmake_vars\\__main__.py\", line 98, in CreateModuleSpace\r\n  File \"C:\\Users\\Adriano\\AppData\\Local\\Programs\\Python\\Python35\\lib\\zipfile.py\", line 1009, in __init__\r\n    self.fp = io.open(file, filemode)\r\nFileNotFoundError: [Errno 2] No such file or directory: '\\\\\\\\?\\\\bazel-out\\\\host\\\\bin\\\\third_party\\\\llvm\\\\expand_cmake_vars'\r\nERROR: C:/tools/msys64/tmp/_bazel_adriano/x1e5egqw/external/llvm/BUILD:183:1: declared output 'external/llvm/include/llvm/Config/abi-breaking.h' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)\r\nERROR: C:/tools/msys64/tmp/_bazel_adriano/x1e5egqw/external/llvm/BUILD:176:1: declared output 'external/llvm/include/llvm/Config/llvm-config.h' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 205.884s, Critical Path: 69.60s\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["cc @gunan @meteorcloudy ", "```\r\nFileNotFoundError: [Errno 2] No such file or directory: '\\\\\\\\?\\\\bazel-out\\\\host\\\\bin\\\\third_party\\\\llvm\\\\expand_cmake_vars'\r\n```\r\nThis is obviously a bug in Bazel's [python stub template](https://github.com/bazelbuild/bazel/blob/master/src/main/java/com/google/devtools/build/lib/bazel/rules/python/stub_template.txt#L15), sorry I didn't test 0.4.5 for TensorFlow GPU build, I'll fix it at Bazel HEAD soon.\r\n\r\n", "This issue was fixed at Bazel's HEAD by [this commit](https://github.com/bazelbuild/bazel/commit/dce45d3151742338057f88f0a459ff442e05726f) so I am closing it.\r\nThanks @meteorcloudy. \r\n\r\n\r\n", "@meteorcloudy  Do you know when there will be an rc with the fix? Unfortunately when I tried compiling from source at HEAD it terminates immediately as in ([issue](https://github.com/bazelbuild/bazel/issues/1275)):\r\n\r\n```\r\n$ ./compile.sh\r\nINFO: You can skip this first step by providing a path to the bazel binary as second argument:\r\nINFO:    ./compile.sh compile /path/to/bazel\r\n\ud83c\udf43  Building Bazel from scratch\r\n```\r\nThanks", "@Carmezim you can just build Bazel from HEAD by Bazel, `bazel build src:bazel`", "But the GPU build is lack of maintenance for a long time, not sure it still works...", "@meteorcloudy Ah, sorry, I've made a little confusion, I'd built with bazel afterwards but wasn't finding the binary, that is at bazel-bin. Thanks a lot.  I can update you if it builds successfully for GPU.", "@meteorcloudy it indeed does not work for GPU support\r\n\r\n```\r\n$ bazel build -c opt $BUILD_OPTS --config=win-cuda tensorflow/tools/pip_package:build_pip_package\r\nWARNING: C:/tensorflow/tensorflow/workspace.bzl:30:5:\r\nCurrent Bazel is not a release version, cannot check for compatibility.\r\nWARNING: C:/tensorflow/tensorflow/workspace.bzl:31:5: Make sure that you are running at least Bazel 0.4.5.\r\n.\r\nERROR: no such target '//:GPU': target 'GPU' not declared in package '' defined by C:/tensorflow/BUILD.\r\nINFO: Elapsed time: 9.422s\r\n```", "Same here on Gentoo Linux:\r\n\r\nbazel build --config=opt --verbose_failures --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: /home/marek/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': Use SavedModel Builder instead.\r\nWARNING: /home/marek/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': Use SavedModel instead.\r\nINFO: Found 1 target...\r\nINFO: From Compiling external/protobuf/src/google/protobuf/compiler/js/embed.cc [for host]:\r\nexternal/protobuf/src/google/protobuf/compiler/js/embed.cc:37:12: warning: unused variable 'output_file' [-Wunused-const-variable]\r\nconst char output_file[] = \"well_known_types_embed.cc\";\r\n           ^\r\n1 warning generated.\r\nERROR: /home/marek/.cache/bazel/_bazel_marek/c5662dc3bfe34c32bb2b10b5482b0731/external/llvm/BUILD:168:1: Executing genrule @llvm//:datatypes_gen failed: bash failed: error executing command \r\n  (cd /home/marek/.cache/bazel/_bazel_marek/c5662dc3bfe34c32bb2b10b5482b0731/execroot/tensorflow && \\\r\n  exec env - \\\r\n    CLANG_CUDA_COMPILER_PATH=/usr/lib/llvm/4/bin/clang \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda \\\r\n    CUDNN_INSTALL_PATH=/opt/cuda \\\r\n    PATH=/usr/local/bin:/usr/bin:/bin:/opt/bin:/usr/x86_64-pc-linux-gnu/gcc-bin/5.4.0:/usr/lib/llvm/4/bin:/opt/cuda/bin:/opt/cuda/libnvvp \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    TF_CUDA_CLANG=1 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=5.0 \\\r\n    TF_CUDA_VERSION=8.0 \\\r\n    TF_CUDNN_VERSION=6 \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_OPENCL=0 \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/third_party/llvm/expand_cmake_vars \"HAVE_DIRENT_H=1\" \"HAVE_DLFCN_H=1\" \"HAVE_ERRNO_H=1\" \"HAVE_EXECINFO_H=1\" \"HAVE_FCNTL_H=1\" \"HAVE_INTTYPES_H=1\" \"HAVE_PTHREAD_H=1\" \"HAVE_SIGNAL_H=1\" \"HAVE_STDINT_H=1\" \"HAVE_SYS_IOCTL_H=1\" \"HAVE_SYS_MMAN_H=1\" \"HAVE_SYS_PARAM_H=1\" \"HAVE_SYS_RESOURCE_H=1\" \"HAVE_SYS_STAT_H=1\" \"HAVE_SYS_TIME_H=1\" \"HAVE_SYS_TYPES_H=1\" \"HAVE_TERMIOS_H=1\" \"HAVE_UNISTD_H=1\" \"HAVE_ZLIB_H=1\" \"HAVE_BACKTRACE=1\" \"BACKTRACE_HEADER=execinfo.h\" \"HAVE_DLOPEN=1\" \"HAVE_FUTIMES=1\" \"HAVE_GETCWD=1\" \"HAVE_GETPAGESIZE=1\" \"HAVE_GETRLIMIT=1\" \"HAVE_GETRUSAGE=1\" \"HAVE_GETTIMEOFDAY=1\" \"HAVE_INT64_T=1\" \"HAVE_ISATTY=1\" \"HAVE_LIBEDIT=1\" \"HAVE_LIBPTHREAD=1\" \"HAVE_LIBZ=1\" \"HAVE_MKDTEMP=1\" \"HAVE_MKSTEMP=1\" \"HAVE_MKTEMP=1\" \"HAVE_PREAD=1\" \"HAVE_PTHREAD_GETSPECIFIC=1\" \"HAVE_PTHREAD_MUTEX_LOCK=1\" \"HAVE_PTHREAD_RWLOCK_INIT=1\" \"HAVE_REALPATH=1\" \"HAVE_SBRK=1\" \"HAVE_SETENV=1\" \"HAVE_SETRLIMIT=1\" \"HAVE_SIGALTSTACK=1\" \"HAVE_STRERROR=1\" \"HAVE_STRERROR_R=1\" \"HAVE_STRTOLL=1\" \"HAVE_SYSCONF=1\" \"HAVE_UINT64_T=1\" \"HAVE__UNWIND_BACKTRACE=1\" \"ENABLE_BACKTRACES=1\" \"LLVM_BINDIR=/dev/null\" \"LLVM_DISABLE_ABI_BREAKING_CHECKS_ENFORCING=0\" \"LLVM_ENABLE_ABI_BREAKING_CHECKS=0\" \"LLVM_ENABLE_THREADS=1\" \"LLVM_ENABLE_ZLIB=1\" \"LLVM_HAS_ATOMICS=1\" \"LLVM_INCLUDEDIR=/dev/null\" \"LLVM_INFODIR=/dev/null\" \"LLVM_MANDIR=/dev/null\" \"LLVM_NATIVE_TARGET=1\" \"LLVM_NATIVE_TARGETINFO=1\" \"LLVM_NATIVE_TARGETMC=1\" \"LLVM_NATIVE_ASMPRINTER=1\" \"LLVM_NATIVE_ASMPARSER=1\" \"LLVM_NATIVE_DISASSEMBLER=1\" \"LLVM_ON_UNIX=1\" \"LLVM_PREFIX=/dev/null\" \"LLVM_VERSION_MAJOR=0\" \"LLVM_VERSION_MINOR=0\" \"LLVM_VERSION_PATCH=0\" \"LTDL_SHLIB_EXT=.so\" \"PACKAGE_NAME=llvm\" \"PACKAGE_STRING=llvm tensorflow-trunk\" \"PACKAGE_VERSION=tensorflow-trunk\" \"RETSIGTYPE=void\" \"LLVM_HOST_TRIPLE=x86_64-unknown-linux_gnu\" \"LLVM_DEFAULT_TARGET_TRIPLE=x86_64-unknown-linux_gnu\" \"LLVM_NATIVE_ARCH=X86\" \"HAVE_MALLOC_H=1\" \"HAVE_LINK_H=1\" \"HAVE_MALLINFO=1\" \"HAVE_FUTIMENS=1\"< external/llvm/include/llvm/Support/DataTypes.h.cmake > bazel-out/local_linux-py3-opt/genfiles/external/llvm/include/llvm/Support/DataTypes.h'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 127.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 20.079s, Critical Path: 10.94s\r\n"]}, {"number": 8481, "title": "Cannot import CudnnRNN from contrib.cudnn_rnn in 1.0.1", "body": "For some reason all the \"__init__.py\" in python directory of _contrib.cudnn_rnn_ have been removed,\r\nand it is impossible to import anything from there.", "comments": ["Just an update, I have a fix for this, but am still working on getting it in.", "https://github.com/tensorflow/tensorflow/commit/986d337e7da04de627218c12e20be1e3abbf6097 should address this."]}, {"number": 8480, "title": "Documentation Error", "body": "On website page:\r\nhttps://www.tensorflow.org/install/install_windows\r\n\r\nThere is a documentation error:\r\n\r\n\r\n----\r\n\r\nTo install TensorFlow, start a ~terminal~ **command prompt instance**. Then issue the appropriate pip3 install command below.\r\n\r\nTo install the CPU-only version of TensorFlow, enter the following command:\r\n\r\n```\r\ncd %appdata%\\..\\Local\\Programs\\ython\\Python35\\Scripts\r\npip3 install --upgrade tensorflow\r\n```\r\n\r\nTo install the GPU version of TensorFlow, enter the following command:\r\n\r\n```\r\ncd %appdata%\\..\\Local\\Programs\\ython\\Python35\\Scripts\r\npip3 install --upgrade tensorflow-gpu\r\n```\r\n\r\n----\r\n\r\nI.E. pip3 isn't registered as an executable on the system while following the standard Python 3.5 installation executable. Thus you have to navigate to the folder containing the executable first.", "comments": ["I think it is strongly implied/expected that the users should launch `pip3`, the binary, however they want and it's not meant to be followed verbatim."]}, {"number": 8479, "title": "Fix broken link in `tensorflow/examples/learn/README.md`", "body": "This fix fixes broken link in `tensorflow/examples/learn/README.md`\r\n```\r\n../../g3doc/tutorials/tflearn/index.md => https://www.tensorflow.org/get_started/tflearn\r\n```\r\n\r\nThis fix change the g3doc into `www.tensorflow.org` link, as it is outside of docs_src (comment https://github.com/tensorflow/tensorflow/pull/8169#issuecomment-284800453)", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please"]}, {"number": 8478, "title": "synchronous posix file mmap", "body": "TensorFlow's [PosixFileSystem::NewReadOnlyMemoryRegionFromFile method](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/posix/posix_file_system.cc#L172) calls `mmap` with [only the `MAP_PRIVATE` flag specified](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/posix/posix_file_system.cc#L183).\r\n\r\nHaving the ability to specify that flag, at least to specify `MAP_POPULATE` would be very useful for many applications with high disk I/O throughput. For the `MAP_POPULATE` case, I sped up one of my graphs by ~4x by adding that flag because I have a pipeline that:\r\n\r\n1. reads in a file of N chunks in size\r\n2. enqueues a pointer to each of those N chunks in an output queue\r\n3. downstream nodes process one chunk at a time\r\n\r\nBecause of the lack of coordination between the downstream nodes (each being independent), they would take page faults in an uncoordinated manner. When I added `MAP_POPULATE` this went away.\r\n\r\nI have a patch to add this feature where I change the [base method](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/file_system.h#L102) to look like this:\r\n\r\n    virtual Status NewReadOnlyMemoryRegionFromFile(\r\n      const string& fname, std::unique_ptr<ReadOnlyMemoryRegion>* result, bool synchronous = false) = 0;\r\n\r\nand then changed each corresponding subclass method signature. For the Posix subclass, I added `MAP_POPULATE` to the flags if `synchronous == true`.\r\n\r\nI can submit a pull request for this, but I find this approach hacky (e.g. that flag might not have the same meaning in non-Posix contexts). If there is a better approach to do this, I'm happy to work on it.", "comments": ["My hunch is that changing a well-established interface such as `FileSystem` is not ideal. \r\n\r\n@keveman @vrv do you see any harm in just or'ing `MAP_POPULATE` in the implementation of `PosixFileSystem::NewReadOnlyMemoryRegionFromFile`?", "Let me find one of the original authors to chime in here...", "I'm the author of this code. I agree that there are many different use cases.\r\n\r\nThe cases that I was focused on is on-device (Android) models with big embedding tensors for NLP tasks. For this case, MAP_POPULATE is a really bad idea because usually only a small amount of these tensors are touched during every inference. \r\n\r\nI also don't like the idea with additional low-level flags because tomorrow we'd see another special solutions for Windows, iOS, etc. I'd rather add to the optimizer of a session, similar to constant elimination, a pass to pre-heat small memmaped tensors. Does it look like a cleaner solution that solves your problem?", "> I'd rather add to the optimizer of a session, similar to constant elimination, a pass to pre-heat small memmaped tensors. Does it look like a cleaner solution that solves your problem?\r\n\r\nI'm not quite sure how this works, as I am not knowledgeable about this part of the codebase. By \"pre-heat\" do you mean a for loop to touch every page (to trigger the page fault)? At least for my use case, I really need to call `mmap` with `MAP_POPULATE` in order to have Linux load all the pages in a single system call. I tried the \"for-loop-to-touch-every-page\" solution prior to this, but that didn't help so much. I read all the data in every file (either to decompress, or to scan if uncompressed) in my graphs / kernels where I use this file mapping.", "I've been thinking about it a bit. \r\nFirst \"pre-heat\" I meant to tell the runtime that during a run over the graph all pages must be in memory. I think that \"synchronous\" is ambiguous because an OS not necessary have to load everything during the call but before the first usage of the tensor. \r\nA generic implementation can simply touch all pages, but for a specific OS a special flag can be specified that effectively does the same.\r\n\r\nReturning to the initial idea, I like the optimization, but I'd like to have some optimization/control and not switch all memmapped tensors to the new mode. Yesterday I thought that a good option is to create a [GraphOptimizer](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/graph_optimizer.h) that sets to ImmutableConst the \"pre-heat\" attribute based on some optimization parameters (e.g. amount of memory). But it might be an over-kill, a simpler approach would be a flag in the script that adds ImmutableConstants that are specified by a name pattern that they need to be pre-heated. How about this solution?\r\n\r\n", "I'm not sure I fully understand this (I haven't looked at the graph optimization part of the code), but I don't think this would work for my use case because I don't mmap tensors. I am not mmaping Tensors directly; instead, my graph works something like this\r\n\r\n1. source: a list of filenames\r\n2. read kernel: takes one filename, calls `NewReadOnlyMemoryRegionFromFile` (with the boolean flag hack I currently have), and puts that region into a custom data type that I output as a `resource` type from the read kernel\r\n3. downstream kernels: read in this custom `resource` type that owns the mmaped region, and deletes / recycles it when down (calling the `ReadOnlyMemoryRegion`'s destructor)\r\n\r\nThis \"sidestepping\" of tensorflow concepts (e.g. not reading my custom binary format and converting it into a large `string` Tensor) might be hard to accommodate for `MAP_POPULATE` in this case, but I think this could prove useful for other use cases.", "OK, I understood, you just want to extent the file system interface for your own custom kernel. Because your solution works only on Linux, why you need the portable interface? Just implement everything in the kernel. It's more transparent and doesn't introduce not needed dependencies.", "Makes sense. I was moving my code out of `core/user_ops` and into `contrib/`, and found this piece for synchronous mapping that I thought could be useful, but I agree it can make things messy in general.\r\n\r\nShould I (or someone else) close this issue at this point?", "Let's close for now -- we can reopen later if it becomes relevant again -- thanks for your efforts on this issue!"]}, {"number": 8477, "title": "No scalars and images show up in tensorboard", "body": "I'm trying to get some visualization on tensorboard, the graph is shown up, but scalars and images are not. I also tried the mnist demo code, still the same problem.\r\nMy colleagues could see the visualization from tensorboard on their machines, so the event/data itself has no problem.\r\nAnyone has any clue?\r\n\r\nI have checked this thread: https://github.com/tensorflow/tensorflow/issues/1421, and tried everything suggested. I do have css file under /usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/lib/css/\r\n\r\n### Environment info\r\nOperating System: Linux Ubuntu 14.04\r\nInstalled version of CUDA and cuDNN:  CUDA 8, cuDNN 5\r\npip installed tensorflow 0.12.1\r\n### What other attempted solutions have you tried?\r\nI tried different browser, chrome and firefox got the same results, no images and sclars show up.\r\nI tried on other machines with same enviorment settings, and had no problem.", "comments": ["Are you outputting the data to TensorBoard? It won't display the images if you don't call `tf.summary.image`, for instance.\r\n\r\nA reproducible example would help here...", "Thank you Micael, I did call tf.summary.image. I could see those images from other machines without any problem. Other machines mean ubuntu 14.04, chrome and tensorflow 0.12.1. ", "Let's call the machine that doesn't work `A`, and another machine that works `B`. Try this:\r\n1) Run your code on B;\r\n2) Copy the log folder from B to A;\r\n3) Run TensorBoard on A.\r\n\r\nAnd then check if the images are visible on TensorBoard. This will tell us if the problem happens while TensorBoard is loading or if the output of TensorFlow is different on your machine (A).", "Hi Micael, I tried what you suggested. \r\n1. Run my code on B: everything looks fine on tensorboard;\r\n2. Copy the log folder from to B to A;\r\n3. Run tensorboard on A: no scalars and images show up (the same as before). ", "Now try copying the TensorBoard folder from B to A. Not the logs, the TensorBoard folder with the sources. If it works, you had somehow a bad/corrupted installation. If it doesn't, your computer has something incompatible with TensorBoard.\r\n\r\nAny error/warning logs when you launch TB?", "What do you mean TensorBoard folder with the sources?\r\nBTW, I don't have any error/warning logs when I launch TB.\r\nThank you!", "For example, in my installation it's this folder:\r\n`~/.local/lib/python3.5/site-packages/tensorflow/tensorboard/`\r\n\r\nInside of it is the source for tensorboard (tensorboard.py) and other folders with more source (like the backend). This test is just a sort of sanity check, to see if you're running the exact same tensorboard code of your colleagues.", "Hi Micael, thank you. I have tried the tensorboard sources, and it is still not working on my machine. ", "when I run tensorboard with --debug, I see this:\r\nINFO:tensorflow:path ../external/weblas_weblas_js/file/weblas.map.json not found, sending 404\r\nINFO:tensorflow:returning 404 to 127.0.0.1 for /weblas_weblas_js/file/weblas.map.json\r\n\r\nIs it related to the problem?", "It could definitely be. At least this is a strong indicator of the problem.\r\n\r\nCheck python packages to see if you have a folder called `external`, and if inside of it there is another folder called `weblas_weblas_js`:\r\n`~/.local/lib/python3.5/site-packages/external/`\r\n\r\nIf you don't, that's a problem with TF's installation. This folder (external) is inside the python wheel package for tensorflow, and is copied to that location. If you want a quick/easy solution, reinstall tensorflow using the pip command, and everything should work fine:\r\n`pip3 install tensorflow-gpu`", "Thank you! I think there might be network port issue in my computer. I have tried reinstall, and it still does not work for me. However, I launch tensorboard on machine A, and use machine B to ssh to A, and I can see the results on tensorboard on B's browser. "]}, {"number": 8476, "title": "Distributed TensorFlow running in parallel and Session Running time problem.", "body": "I 'm trying to using tensorflow to do the inference task in a distributed way. I follow the doc here(https://www.tensorflow.org/deploy/distributed), and I'm using the example naive MNIST network. \r\n(codes shown below.)\r\n\r\nI use different workers on local machine. And since the network is small, I put different node on different workers.\r\n**What I want to know is:**\r\n**1. If I divide the graph into several parts, can they run in parallel?** \r\nI know this graph is sequential/linear, (one part of the computation is dependent on the result of previous part, so if there's only 1 batch of data, it has to run sequentially.) \r\nBut I want to know, if I have multiple batches of data, can they run parallelly in a pipeline fashion? (like Part 1 finished computing batch i data, send it to Part 2, and continue working on batch i+1 data)  ??\r\n\r\n**2. Like shown in the code, I want to output the computation for each part of the graph.** \r\nHowever after running it, I noticed that, the output time is only the time for \"constructing the node/graph\", instead of running the graph. All the running is in the \"sess.run()\". And in this case, I cannot know the computation time for each part of the graph...\r\n\r\n**I 'm wondering if there's a way to show the computation time of each part of the graph?(or each node maybe?)**\r\n\r\nThe code:\r\n```\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\r\nimport time\r\nimport tensorflow as tf\r\n\r\nstart=time.time()\r\n\r\nx = tf.placeholder(tf.float32, [None, 784], name=\"input\")\r\n\r\ncluster = tf.train.ClusterSpec({\"local\": [\"localhost:2222\", \"localhost:2223\", \"localhost:2224\"]})\r\n\r\nwith tf.device(\"/job:local/task:0\"):\r\n    W = tf.Variable(tf.zeros([784, 10]), name = \"w1\")\r\n    b = tf.Variable(tf.zeros([10]), name = \"b1\")\r\n\r\ntime1=time.time()\r\n#print (\"Time1: \"+str(time1-start)+\" seconds\")\r\n\r\nwith tf.device(\"/job:local/task:1\"):\r\n    y = tf.nn.softmax(tf.matmul(x, W) + b)\r\n    y_ = tf.placeholder(tf.float32, [None, 10], name = \"output\")\r\n\r\ntime2=time.time()\r\n#print (\"Time2: \"+str(time2-time1)+\" seconds\")\r\nwith tf.device(\"/job:local/task:2\"):\r\n    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\r\n    train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\r\n\r\ntime3=time.time()\r\n#print (\"Time3: \"+str(time3-time2)+\" seconds\")\r\n\r\n# swd--save the Checkpoint file\r\nsaver = tf.train.Saver()\r\n\r\n#with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\r\nwith tf.Session(\"grpc://localhost:2224\") as sess:\r\n    # swd save ckpt\r\n    saver.restore(sess, \"saved_model/model.ckpt\")\r\n    print(\"\\nModel restored.\\n\")\r\n\r\n    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\r\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n    print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\r\n    \r\ntime4=time.time()\r\n#print (\"Time4: \"+str(time4-time3)+\" seconds\")\r\n\r\nend=time.time()\r\nprint (\"Computing time: \"+str(end-start)+\" seconds\")\r\n```", "comments": ["About your last question: Yes, there is [Timeline](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/timeline.py) that can help you with that.\r\n\r\nThe usage is simple:\r\n```\r\ndef save_timeline(run_metadata, output_folder):\r\n  tl = timeline.Timeline(run_metadata.step_stats)\r\n  ctf = tl.generate_chrome_trace_format()\r\n  with open('%s/timeline.json' % output_folder, 'w') as f:\r\n    f.write(ctf)\r\n\r\n\r\nFULL_TRACE_OPTION = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\nRUN_METADATA = tf.RunMetadata()\r\ntraining_results = sess.run(your_ops_here, feed_dict=your_feed_dict_here, options=FULL_TRACE_OPTION, run_metadata=RUN_METADATA)\r\nsave_timeline(RUN_METADATA, '~/')\r\n```\r\n\r\nAnd then you can open Google Chrome (or Chromium) and type `chrome://tracing` in the URL. On the new page, `Load` the file you just saved, and you'll see the execution time for each part of your graph.", "+1 to what @MicaelCarvalho said.  Since this is not an issue/bug, I'm closing this.", "(FYI, there's also the https://www.tensorflow.org/versions/master/how_tos/debugger/.)"]}, {"number": 8475, "title": "Cherry pick 1.0.1 release notes into master.", "body": "", "comments": []}, {"number": 8474, "title": "Branch 150328131", "body": "", "comments": ["Abandoning. Need to apply some fixes to get the tests working."]}, {"number": 8473, "title": "Segmentation Fault in Shape Function When Accessing Attr", "body": "Adding this to shape function causes segfault (full example below):\r\n```c++\r\n      string data_format;\r\n      Status s = c->GetAttr(\"test_str\", &data_format);\r\n```\r\n\r\nOn OS X:\r\n```\r\n$ bazel version\r\nBuild label: 0.4.3-homebrew\r\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Dec 22 15:20:22 2016 (1482420022)\r\nBuild timestamp: 1482420022\r\nBuild timestamp as int: 1482420022\r\n```\r\n\r\n```\r\n$git rev-parse\r\n07bb8ea2379bd459832b23951fb20ec47f3fdbd4\r\n```\r\n\r\nUsing the [ZeroOut op](https://www.tensorflow.org/extend/adding_an_op) but modifying the shape function:\r\n```c++\r\n#include \"tensorflow/core/framework/op.h\"\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n#include \"tensorflow/core/framework/shape_inference.h\"\r\n\r\nusing namespace tensorflow;\r\n\r\nREGISTER_OP(\"ZeroOut\")\r\n    .Input(\"to_zero: int32\")\r\n    .Output(\"zeroed: int32\")\r\n    .Attr(\"test_str: {'alex', 'bob'} = 'alex'\")\r\n    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\r\n      string data_format;\r\n      Status s = c->GetAttr(\"test_str\", &data_format);\r\n      c->set_output(0, c->input(0));\r\n      return Status::OK();\r\n    });\r\n\r\nclass ZeroOutOp : public OpKernel {\r\n public:\r\n  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}\r\n\r\n  void Compute(OpKernelContext* context) override {\r\n    // Grab the input tensor\r\n    const Tensor& input_tensor = context->input(0);\r\n\r\n    // Create an output tensor\r\n    Tensor* output_tensor = NULL;\r\n    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),\r\n                                                     &output_tensor));\r\n\r\n  }\r\n};\r\n\r\nREGISTER_KERNEL_BUILDER(Name(\"ZeroOut\").Device(DEVICE_CPU), ZeroOutOp);\r\n```\r\n\r\nand then running (within `tf.TestCase`):\r\n```python\r\nclass Alex(tf.test.TestCase):\r\n    def test_alex(self):\r\n        so_path = (cur_dir + \"/../../../../\" +\r\n            \"bazel-bin/tensorflow/core/user_ops/zero.so\")\r\n        my_module = tf.load_op_library(so_path)\r\n        with self.test_session():\r\n            print my_module.zero_out([1,1,2,1]).eval()\r\n```\r\n\r\nThe segfault it self seems to be coming from protobuf. I filed a ticket there as well: https://github.com/google/protobuf/issues/2863.", "comments": ["I was unable to reproduce this on 1.0. I didn't run in a test session:\r\n```python\r\nimport tensorflow as tf\r\nzero_out_module = tf.load_op_library('zero_out.so')\r\nwith tf.Session(''):\r\n  zero_out_module.zero_out([[1, 1, 2, 4]]).eval()\r\n```\r\n\r\nI also ran with gcc instead of bazel.\r\n\r\nI don't have any super compelling theories, but maybe try running in a regular session like above, using the g++ command given in the tutorial, and/or upgrading bazel?", "I upgraded bazel and am still seeing the segfault:\r\n```\r\n$ bazel version\r\nBuild label: 0.4.5-homebrew\r\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Mar 16 13:39:33 2017 (1489671573)\r\nBuild timestamp: 1489671573\r\nBuild timestamp as int: 1489671573\r\n```\r\nI also tested with regular `Session`. Still get seg fault\r\n\r\nWhat is best way to compile with GCC?\r\nRunning within the `tensorflow/core/user_ops` directory: `TF_INC=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_include())') g++ -std=c++11 -shared zero_out.cc -o zero_out.so -fPIC -I $TF_INC -O2 -undefined dynamic_lookup` gives:\r\n```\r\nld: warning: directory not found for option '-L/lib'\r\nld: warning: directory not found for option '-L/lib64'\r\n```\r\n*UPDATE*: ^ was caused by bad `LIBRARY_PATH`.\r\n\r\n*UPDATE* Compiling with GCC (using line above) works! So there is something wrong with the so created when building with bazel. This is my `BUILD` file:\r\n```\r\ntf_custom_op_library(\r\n    name = \"zero.so\",\r\n    srcs = [\"zero_out.cc\", ],\r\n)\r\n```", "Cool, thanks for narrowing that down! I'll work on reproing the bad bazel artifact, and I'm glad you found a workaround for now.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "wait... the last comment on this thread was @skye taking a look at the issue."]}, {"number": 8472, "title": "multi core cpu issue(server vs local)", "body": "My local computer is mac : i7 quad core 2.2GHz .  (2015 model)\r\nMy server computer is centos7 : 120-core (8 sockets \u00d7 15cores) Intel Xeon E7-8870\r\nMy tensorflow version : v1.0.1\r\n\r\nI build tensorflow source with this option :\r\nbazel build -c opt --copt=-msse4.2 --copt=-msse4.1 --copt=-mavx --copt=-mavx2 --copt=-mfma --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\nWith under simple code, my local computer time is 8 seconds and my server computer time is 22 second. \r\n`\r\nimport tensorflow as tf\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nimport time\r\n\r\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\r\n\r\nx = tf.placeholder(tf.float32, [None, 784]);\r\ny_ = tf.placeholder(tf.float32, [None, 10]);\r\n\r\nW1 = tf.Variable(tf.random_normal([784,240], stddev=0.35));\r\nW2 = tf.Variable(tf.random_normal([240,120], stddev=0.35));\r\nW3 = tf.Variable(tf.random_normal([120,10], stddev=0.35));\r\n\r\nb1 = tf.Variable(tf.zeros([240]));\r\nb2 = tf.Variable(tf.zeros([120]));\r\nb3 = tf.Variable(tf.zeros([10]));\r\n\r\ny1 = tf.nn.sigmoid(tf.matmul(x,W1)+b1);\r\ny2 = tf.nn.sigmoid(tf.matmul(y1,W2)+b2);\r\ny = tf.nn.softmax(tf.matmul(y2,W3)+b3);\r\n\r\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_*tf.log(y), reduction_indices=[1]))\r\ntrain = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\r\n\r\nNUM_THREADS = 100\r\nsess = tf.InteractiveSession(config=tf.ConfigProto(intra_op_parallelism_threads=NUM_THREADS))\r\n\r\ntf.global_variables_initializer().run()\r\n\r\nstart_time = time.time()\r\n\r\nfor _ in range(3000):\r\n    batch_xs, batch_ys = mnist.train.next_batch(100)\r\n    sess.run(train, feed_dict={x: batch_xs, y_: batch_ys})\r\n\r\nend_time = time.time()\r\nprint (end_time - start_time)\r\n\r\ncorrect_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\r\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\nprint(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\r\n`\r\nI think it is impossible because my server computer have so many cores.\r\nIsn't it multi-core issue?", "comments": ["1. I7 cores are faster than Xeon cores. My 4 year old macbook outperforms brand new Xeon V3s on single-core stuff\r\n2. Your network is tiny, so it's hard to use more than one core\r\n3. Increasing number `intra_op_parallelism_threads` above 1 can hurt performance for small work-loads. I've recently dealt with a network where setting all parallelism to 1 gave a 50% speed-up in average time.\r\n\r\nTherefore I think this is WAI\r\n"]}, {"number": 8471, "title": "InvalidArgumentError (traceback): BiasGrad requires tensor size <= int32 max in 3D CNN, Tensorflow", "body": "I am using 3D CNN model in tensorflow. My input image dimension is 3D i.e. (192 * 256 * 256) with one color channel image and i have used three Convolutional layers (with patches of 5 * 5 * 5) and 3 pooling layers (kernal size : 3 * 3 * 3 and stride : 2 * 2 * 2), one full-connection-layer with 128 nodes and output layer with two nodes. The number of samples are 120 for training. Batch size : 10\r\n\r\nI am facing the below error on the cluster:\r\n\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nTraceback (most recent call last):\r\n  File \"./mri_cnn.py\", line 362, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"./mri_cnn.py\", line 332, in main\r\n    sess.run(optimizer, feed_dict={train_data_node: batch_data,train_labels_node: batch_labels})\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 767, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 965, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: BiasGrad requires tensor size <= int32 max\r\n         [[Node: gradients/BiasAdd_grad/BiasAddGrad = BiasAddGrad[T=DT_FLOAT, data_format=\"NHWC\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](gradients/Relu_grad/ReluGrad)]]\r\n\r\nCaused by op u'gradients/BiasAdd_grad/BiasAddGrad', defined at:\r\n  File \"./mri_cnn.py\", line 362, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"./mri_cnn.py\", line 290, in main\r\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.0).minimize(train_loss) # Adam Optimizer\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 288, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 354, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 482, in gradients\r\n    in_grads = grad_fn(op, *out_grads)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_grad.py\", line 204, in _BiasAddGrad\r\n    data_format=data_format))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 313, in bias_add_grad\r\n    data_format=data_format, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\n...which was originally created as op u'BiasAdd', defined at:\r\n  File \"./mri_cnn.py\", line 362, in <module>\r\n    tf.app.run()\r\n[elided 0 identical lines from previous traceback]\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"./mri_cnn.py\", line 272, in main\r\n    train_prediction = model(train_data_node, True)\r\n  File \"./mri_cnn.py\", line 181, in model\r\n    relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py\", line 1316, in bias_add\r\n    return gen_nn_ops._bias_add(value, bias, data_format=data_format, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 281, in _bias_add\r\n    data_format=data_format, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): BiasGrad requires tensor size <= int32 max\r\n         [[Node: gradients/BiasAdd_grad/BiasAddGrad = BiasAddGrad[T=DT_FLOAT, data_format=\"NHWC\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](gradients/Relu_grad/ReluGrad)]]\r\n\r\nPlease suggest, how to fix it?", "comments": ["Please fill the template for issue reporting (it's the standard text when you submit a new issue, you just have to fill the gaps), without those information it's very hard to find the problem.\r\n\r\nFor example: You don't provide a reproducible example, nor any information about TF version, the optimizer you're using, or the functions you're calling to construct your network. And the problem can be related to either your loss, your optimizer, or your network construction.\r\n\r\nAlso, if you don't think it's a bug in TF's code nor a feature request, but rather a question, you should post it on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) \u2014 it is the standard place for asking for help and the community over there is bigger. ;-)", "Check your tensor sizes - they don't sound like to be `> kInt32Max`.", "@concretevitamin : My tensor size is very big as i mentioned in question. so what is the solution", "You're going to have a hard time finding someone willing to help you if you don't fill the template, as I suggested 4 days ago... Big error messages often aren't informative enough to solve the problem.", "Hi @suchetasu , as @concretevitamin mentioned, it looks like you're exceeding the max tensor size implemented by the BiasAdd op's gradient. Unfortunately there is no simple solution, you'll either need to change your model to avoid this limitation or we'll need to find someone to modify BiasAddGrad (probably a non-trivial change).\r\n\r\nIf you share more information as suggested in the template, especially a minimal repro, that will help us find a more specific solution.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "please change your job on GPU"]}, {"number": 8470, "title": "Change Python print to be compatible with Python3", "body": "This PR is trying to fix issue #8349 . I searched the docs to find out all the print calls which is python2 style. Please review, thanks a lot.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 8469, "title": "misleading comments", "body": "The comments for the function `def _linear(args, output_size, bias, bias_start=0.0):` in tensorflow/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py says:\r\n\r\n```\r\ndef _linear(args, output_size, bias, bias_start=0.0):\r\n  \"\"\"Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\r\n  Args:\r\n    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\r\n    output_size: int, second dimension of W[i].\r\n    bias: boolean, whether to add a bias term or not.\r\n    bias_start: starting value to initialize the bias; 0 by default.\r\n```\r\nHowever, the function is not implemented as `sum_i(args[i] * W[i])`. \r\nThe key parts in the implementation in line 894-897:\r\n```\r\n    if len(args) == 1:\r\n      res = math_ops.matmul(args[0], weights)\r\n    else:\r\n      res = math_ops.matmul(array_ops.concat(args, 1), weights)\r\n```\r\nThe `args` list is firstly concatenated, then `matmul` with the weights tensor. This is totally different from what said in the comments that `Linear map: sum_i(args[i] * W[i]), where W[i] is a variable`. \r\nSo, the comments should probably be rewritten like `Linear map: first, concatenate the args if it is a list, then matmul with a weight tensor`.", "comments": ["Sorry, I just found that they are in fact the same.\r\nSorry for proposing a wrong issue."]}, {"number": 8468, "title": "Fix minor error-handling bug", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please"]}, {"number": 8467, "title": "Better control of logging verbosity", "body": "I'm creating a simple LSTM in Keras, and during training I get these warnings:\r\n\r\n![screenshot from 2017-03-16 13-25-23](https://cloud.githubusercontent.com/assets/23310996/23995972/170f3a10-0a4c-11e7-92f6-f7ee865e65e6.png)\r\n\r\nI know I can set the `TF_CPP_MIN_LOG_LEVEL` according to this question: http://stackoverflow.com/questions/35869137/avoid-tensorflow-print-on-standard-error\r\n\r\nor use the `tf.logging.set_verbosity(verbosity)` to control this, but for my example I would like to \r\n\r\n1. Hide the warnings that Tensorflow wasn't compiled to use ... instructions.\r\n2. Show the logging of device when starting a new session, that is very useful to confirm that the GPU support is working.\r\n3. Hide the Pool allocator warnings since they clutter the console output from Keras during training.\r\n\r\nI haven't found a way to do this, perhaps it could be added as a feature?\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["I totally back this up.\r\n\r\nThere are some irritating situations caused by the attempts to control the verbosity of TF. For example, _if we set the min log level to warning or error, tf.Print stops working_. I just happen to want a tf.Print in my program without cluttering all my logs with the mostly useless information TF outputs.\r\n\r\nThere should be a \"silent mode\" with two levels that displays errors and warnings, while still allowing the users to print/debug whatever they want. On one occasion, I spent more than 30 minutes asking myself why my code was crashing without printing my debugs, and changing it in many ways, until I remembered log levels.", "I run my TF scripts as `tf.sh myscript.py`\r\n\r\nWhere tf.sh is\r\n\r\n`python $* 3>&1 1>&2 2>&3 3>&- | grep -v \":\\ I\\ \" | grep -v \"WARNING:tensorflow\" | grep -v ^pciBusID | grep -v ^major: | grep -v ^name: |grep -v ^Total\\ memory:|grep -v ^Free\\ memory:`\r\n\r\nWhen something annoys me, I add another \"grep -v\" clause.\r\nGetting rid of \"Tensorflow was compiled without SSE\" messages by default would reduce number of complains about too many useless messages, and increase number of complains about TensorFlow being too slow.", "/cc @petewarden ", "Can we close this as it was a TF 1. api (`tf.compat.v1.logging.set_verbosity`)?", "Hi @3h4 ! Aside using TF_CPP_MIN_LOG_LEVEL and  tf.compat.v1.logging.set_verbosity ,You can also use tf.device and [tf.debugging.set_log_device_placement(True)](https://www.tensorflow.org/guide/gpu) to show device usage while compiling the code. Please move this issue to closed status if it has been resolved now. Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 8466, "title": "python: ../nptl/pthread_mutex_lock.c:349: __pthread_mutex_lock_full: Assertion `INTERNAL_SYSCALL_ERRNO (e, __err) != EDEADLK || (kind != PTHREAD_MUTEX_ERRORCHECK_NP && kind != PTHREAD_MUTEX_RECURSIVE_NP)' failed.", "body": "Tensorflow is crashing when run using multiple GPUs (2,3,and 4) on cifar10_multi_gpu .py example in a Power8 computer.\r\n\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nWe have found some suggestions regarding updating glibc (https://bugs.launchpad.net/ubuntu/+source/glibc/+bug/1642390?cm_mc_uid=49967414488814888059863&cm_mc_sid_50200000=1489663906), but our system is already updated to the latest version and the problem remains.\r\nWe have also used ppc64_pc to reduce SMT (to 2)  and also turned off. Still the problem remains. \r\n\r\n### Environment info\r\n_Operating System:\r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 16.04.2 LTS\r\nRelease:\t16.04\r\nCodename:\txenial\r\nLinux minsky31 4.4.0-64-generic #85-Ubuntu SMP Mon Feb 20 17:05:51 UTC 2017 ppc64le ppc64le ppc64le GNU/Linux_\r\n\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n_CUDA\r\n-rw-r--r-- 1 root root 559800 gen 27 00:10 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root     16 gen 27 00:13 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root     19 gen 27 00:13 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61\r\n-rw-r--r-- 1 root root 476024 gen 27 00:10 /usr/local/cuda/lib64/libcudart.so.8.0.61\r\n-rw-r--r-- 1 root root 966166 gen 27 00:10 /usr/local/cuda/lib64/libcudart_static.a\r\ncuDNN\r\n/usr/lib/powerpc64le-linux-gnu/libcudnn_static.a\r\n/usr/lib/powerpc64le-linux-gnu/libcudnn.so.5.1.10\r\n/usr/lib/powerpc64le-linux-gnu/libcudnn.so\r\n/usr/lib/powerpc64le-linux-gnu/libcudnn_static_v5.a\r\n/usr/lib/powerpc64le-linux-gnu/libcudnn.so.5_\r\n\r\n\r\nIf installed from source, provide \r\n_1. The commit hash (`git rev-parse HEAD`):\r\n012f9c10dcc27a838ed4b170b9036483bc2c5869_\r\n\r\n2. The output of `bazel version`:\r\n_Build label: 0.4.3-2017-01-24 (@6fc5c53)\r\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Jan 24 20:34:16 2017 (1485290056)\r\nBuild timestamp: 1485290056\r\nBuild timestamp as int: 1485290056_\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n_We downloaded the files from https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10\r\nand tried to run the cifar10_multi_gpu example using four P100 gpus._ \r\n\r\n### What other attempted solutions have you tried?\r\nWe tried removing the calls from measuring time, logging etc, but the problem remains. \r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\n_When it blocks, it simply hangs \r\n2017-03-16 12:09:56.391752: step 800, loss = 3.33 (2982.8 examples/sec; 0.011 sec/batch)\r\n2017-03-16 12:09:57.002419: step 810, loss = 3.49 (3998.9 examples/sec; 0.008 sec/batch)\r\n2017-03-16 12:09:57.476172: step 820, loss = 3.24 (3003.8 examples/sec; 0.011 sec/batch)\r\nand the result from strace when it blocks:\r\nstrace: Process 111228 attached\r\nfutex(0x3fffd5d49744, FUTEX_WAIT_PRIVATE, 1, NULL_\r\n\r\nOtherwise, when it gives the error : \r\nfutex(0x10013f8cf10, FUTEX_WAIT_BITSET_PRIVATE|FUTEX_CLOCK_REALTIME, 0, NULL, ffffffff) = 0\r\nfutex(0x10013f8cf10, FUTEX_WAIT_BITSET_PRIVATE|FUTEX_CLOCK_REALTIME, 0, NULL, ffffffff) = 0\r\nfutex(0x10013f8cf10, FUTEX_WAKE_PRIVATE, 1) = 1\r\nfutex(0x10013f8cf10, FUTEX_WAIT_BITSET_PRIVATE|FUTEX_CLOCK_REALTIME, 0, NULL, ffffffff) = 0\r\nfutex(0x10013f8cf10, FUTEX_WAIT_BITSET_PRIVATE|FUTEX_CLOCK_REALTIME, 0, NULL, ffffffff) = 0\r\nfutex(0x10013f8cf10, FUTEX_WAIT_BITSET_PRIVATE|FUTEX_CLOCK_REALTIME, 0, NULL, ffffffff) = -1 EAGAIN (Resource temporarily unavailable)\r\nfutex(0x10013f8cf10, FUTEX_WAKE_PRIVATE, 1) = 1\r\nfutex(0x3fffd1afc324, FUTEX_WAIT_PRIVATE, 1, NULL) = 0\r\nfutex(0x3fffd1afc2f8, FUTEX_WAKE_PRIVATE, 1) = 0\r\nfutex(0x3fffd1afc324, FUTEX_WAIT_PRIVATE, 1, NULL <unfinished ...>\r\n+++ killed by SIGABRT (core dumped) +++\r\n\r\n\r\nstrace gives as last lines:\r\nfutex(0x1001efacf10, FUTEX_WAIT_BITSET_PRIVATE|FUTEX_CLOCK_REALTIME, 0, NULL, ffffffff) = 0\r\nfutex(0x1001efacf10, FUTEX_WAIT_BITSET_PRIVATE|FUTEX_CLOCK_REALTIME, 0, NULL, ffffffff) = 0\r\nfutex(0x1001efacf10, FUTEX_WAIT_BITSET_PRIVATE|FUTEX_CLOCK_REALTIME, 0, NULL, ffffffff) = -1 EAGAIN (Resource temporarily unavailable)\r\nfutex(0x3ffff8dbd184, FUTEX_WAIT_PRIVATE, 1, NULL) = 0\r\nfutex(0x3ffff8dbd158, FUTEX_WAKE_PRIVATE, 1) = 0\r\nfutex(0x3ffff8dbd184, FUTEX_WAIT_PRIVATE, 1, NULL) = 0\r\nfutex(0x3ffff8dbd158, FUTEX_WAKE_PRIVATE, 1) = 0\r\nfutex(0x3ffff8dbd184, FUTEX_WAIT_PRIVATE, 1, NULL) = 0\r\nfutex(0x3ffff8dbd158, FUTEX_WAKE_PRIVATE, 1) = 0\r\nfutex(0x3ffff8dbd184, FUTEX_WAIT_PRIVATE, 1, NULL) = 0\r\nfutex(0x3ffff8dbd158, FUTEX_WAKE_PRIVATE, 1) = 0\r\nfutex(0x3ffff8dbd184, FUTEX_WAIT_PRIVATE, 1, NULL) = 0\r\nfutex(0x3ffff8dbd158, FUTEX_WAKE_PRIVATE, 1) = 0\r\nfutex(0x1001efacf10, FUTEX_WAIT_BITSET_PRIVATE|FUTEX_CLOCK_REALTIME, 0, NULL, ffffffff) = 0\r\nfutex(0x3ffff8dbd184, FUTEX_WAIT_PRIVATE, 1, NULL) = 0\r\nfutex(0x3ffff8dbd158, FUTEX_WAKE_PRIVATE, 1) = 0\r\nfutex(0x3bff90000020, FUTEX_WAKE_PRIVATE, 1) = 1\r\nfutex(0x3ffff8dbd184, FUTEX_WAIT_PRIVATE, 1, NULL) = 0\r\nfutex(0x3ffff8dbd158, FUTEX_WAKE_PRIVATE, 1) = 0\r\nstat(\"/etc/localtime\", {st_mode=S_IFREG|0644, st_size=2692, ...}) = 0\r\nwrite(1, \"2017-03-16 12:22:38.084717: step\"..., 90) = 90\r\nfutex(0x1001efacf10, FUTEX_WAKE_PRIVATE, 1) = 1\r\nfutex(0x1001efacf10, FUTEX_WAIT_BITSET_PRIVATE|FUTEX_CLOCK_REALTIME, 0, NULL, ffffffff) = 0\r\nfutex(0x1001efacf10, FUTEX_WAIT_BITSET_PRIVATE|FUTEX_CLOCK_REALTIME, 0, NULL, ffffffff) = -1 EAGAIN (Resource temporarily unavailable)\r\nfutex(0x3ffff8dbd184, FUTEX_WAIT_PRIVATE, 1, NULL\r\n\r\n\r\n", "comments": ["Also, increasing the batch_size for a single GPU (from 128 to 512) gives the same error, i.e. the process starts but then after a few iterations it halts again. \r\n_2017-03-16 13:56:24.380589: step 390, loss = 3.90 (918.7 examples/sec; 0.557 sec/batch)\r\n2017-03-16 13:56:30.259091: step 400, loss = 3.89 (831.1 examples/sec; 0.616 sec/batch)\r\n2017-03-16 13:56:36.581468: step 410, loss = 3.87 (909.2 examples/sec; 0.563 sec/batch)\r\npython: ../nptl/pthread_mutex_lock.c:349: __pthread_mutex_lock_full: Assertion `INTERNAL_SYSCALL_ERRNO (e, __err) != EDEADLK || (kind != PTHREAD_MUTEX_ERRORCHECK_NP && kind != PTHREAD_MUTEX_RECURSIVE_NP)' failed.\r\nAborted (core dumped)_\r\n", "Exactly same issue I am getting.", "I am using Power8 box.. and dont see this issue. \r\npython ./cifar10_multi_gpu_train.py --data_dir=/mldl/NVME/scratch/inception_output_data/ --train_dir=/tmp/ --num_gpus 4 --batch_size 512 --max_steps 200\r\n\r\nAs you can see in the arguments.. i am using 4 gpu's and batch size of 512\r\n\r\nI tried it with TensorFlow 1.0.1 and using libc6_2.23-0ubuntu5_ppc64el.deb \r\nKernel : 4.4.0-59-generic", "@pedropgusmao would it be possible to try version 1.0.1? That will help narrow down the problem.\r\n\r\nSimilarly, @abdasgupta which TensorFlow version are you running?", "Hi @skye  and  @antoajayraj ,\r\n\r\nFirst let me describe my setup - I am running tensorflow training on inception v3 model to fine tune for Flowers data set(see https://github.com/tensorflow/models/tree/master/inception), through docker in a small kubernetes cluster made of power 8 machines. I have 4GPUs at my disposal.\r\nMy container and host OS are Ubuntu 16.04.\r\nHost kernel is : 4.4.0-64-generic\r\nGLibc version: Ubuntu GLIBC 2.23-0ubuntu7\r\n\r\nI was hitting the above mentioned issue with tensorflow version 1.0.0 only when I ran 1000 iterations of training or above. The above error was appearing approx after 600-900 iterations.\r\n\r\nAfter I upgraded to version 1.0.1, I am seeing no such error within 10000 iterations.\r\nBut if I run 100000 iterations, the training is getting stuck at around 50k and stack trace is showing that it is executing some waitpid(-1..) and stuck there. \r\n\r\nCommand to run the training in docker container:\r\n`docker run -it --privileged -v /usr/local/cuda-8.0/:/usr/local/cuda-8.0/ -v /usr/lib/powerpc64le-linux-gnu/:/usr/lib/powerpc64le-linux-gnu/ -v /usr/lib/nvidia-375/:/usr/lib/nvidia-375/  tf-inception-trainer-flowers-v101 /bin/bash -c \"./run-trainer.sh 100000\"`\r\n\r\nImage name : tf-inception-trainer-flowers-v101 \r\nThis is the same image I am using for Kubernetes. This is a locally built image with tensorflow 1.0.1 installed.\r\n\r\nContent of run-trainer.sh:\r\n```bash\r\n#!/bin/bash\r\n\r\nif [ $1 ];  then\r\n  ITERATIONS=$1\r\nelse\r\n  echo $0\": usage: ./run-traner.sh [iterations]\"\r\n  exit 1\r\nfi\r\n\r\ncurl -O http://download.tensorflow.org/models/image/imagenet/inception-v3-2016-03-01.tar.gz && \\\r\ntar xzf inception-v3-2016-03-01.tar.gz && \\\r\nls inception-v3\r\n\r\nmkdir /flowers-data\r\n\r\ncd /models/inception && \\\r\nexport LD_LIBRARY_PATH=/usr/lib/powerpc64le-linux-gnu/ && \\\r\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-8.0/lib64/ && \\\r\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/nvidia-375/ && \\\r\nbazel build inception/download_and_preprocess_flowers && \\\r\nbazel-bin/inception/download_and_preprocess_flowers /flowers-data && \\\r\n\r\n# Build the model. Note that we need to make sure the TensorFlow is ready to\r\n# use before this as this command will not build TensorFlow.\r\nbazel build inception/flowers_train\r\n\r\n# Path to the downloaded Inception-v3 model.\r\nMODEL_PATH=\"/inception-v3/model.ckpt-157585\"\r\n\r\n# Directory where the flowers data resides.\r\nFLOWERS_DATA_DIR=/flowers-data/\r\n\r\n# Directory where to save the checkpoint and events files.\r\nTRAIN_DIR=/flowers_train/\r\n\r\n# Run the fine-tuning on the flowers data set starting from the pre-trained\r\n# Imagenet-v3 model.\r\nbazel-bin/inception/flowers_train \\\r\n  --train_dir=\"${TRAIN_DIR}\" \\\r\n  --data_dir=\"${FLOWERS_DATA_DIR}\" \\\r\n  --pretrained_model_checkpoint_path=\"${MODEL_PATH}\" \\\r\n  --fine_tune=True \\\r\n  --initial_learning_rate=0.001 \\\r\n  --max_steps=\"${ITERATIONS}\" \\\r\n  --input_queue_memory_factor=1\r\n```\r\nIs the behaviour expected? Am I doing something wrong? Please, guide me. Though my current purpose is served as I can run the training now till 5000 iterations through docker. It gives me about 99.8% of accuracy. And accuracy is same while the iterations go up to 50k. But the accuracy is hurt while I lower the iterations to , say 2000 (Only 93.4%)\r\n\r\nI will update my findings after I run the training on P8 bare-metal till 100000 iterations.\r\n\r\nThank you.", "So it sounds like the crash was resolved in 1.0.1. That's good. @pedropgusmao please let us know if you still see this after upgrading to 1.0.1.\r\n\r\n@abdasgupta you mention a stacktrace with waitpid(-1), could you post the full stacktrace? Also how are you collecting it?", "Hi everyone. I was also using a Power8 with IBM's packages (https://www-01.ibm.com/common/ssi/cgi-bin/ssialias?htmlfid=POO03514WWEN&) which included Tensorflow 1.0.0 . I solved the problem by using CUDA8 for Power8, which is NOT(!!!!) the same for the general pc64_el architecture. You must use the cuda-repo-ubuntu1604-8-0-local-ga2_8.0.54-1_ppc64el-deb from https://developer.nvidia.com/cuda-downloads-power8 .", "@skye  I ran the tensorflow training on bare-metal. The error is sporadic; though after using Tensorflow 1.0.1, the error is seldom coming within 50000 iterations.  I am collecting stacktrace with \"strace -p <pid>\" and during the error , it only shows waitpid(-1), nothing more.\r\n@pedropgusmao Yes, I am using CUDA8 from cuda-repo-ubuntu1604-8-0-local-ga2_8.0.54-1_ppc64el-deb from https://developer.nvidia.com/cuda-downloads-power8 .", "@abdasgupta , I am also using  Nvidia  Driver Version: 361.10. From line `export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/nvidia-375/ && \\` and `docker run` it seems that you are using the new 375 driver. Could you try and downgrade to 361.10? ", "I've also experienced this error using TensorFlow on POWER8, and have now also\r\njust hit it on a source installation of caffe2 on POWER8. So this may not be\r\nan issue with TensorFlow. My environment:\r\n\r\nSystem:        Power8 Minsky, 4xP100\r\nOS:            Ubuntu 16.04.2\r\nCUDA:          cuda-repo-ubuntu1604-8-0-local-ga2v2_8.0.61-1_ppc64el.deb\r\ncuDNN:         5.1.10\r\nPython:        2.7\r\nNVIDIA Driver: 375.51\r\n\r\nI'm curious, what kinds of GPUs is everyone running? @pedrogusmao and I are on\r\nP100 and having this issue. How about @abdasgupta and @antoajayraj?\r\n", "@bcbrock I have the same issue on a Minsky machine:\r\n\r\nSystem: Power8 Minsky, 4xP100\r\nOS: Ubuntu 16.04.2\r\nCUDA: 8.0.61\r\ncuDNN: 5.1.10/6.0.20\r\nPython: 2.7.12\r\nNVIDIA Driver: 375.51\r\nTensorFlow: 1.0.1\r\n\r\nAny more ideas on this issue?\r\n", "@dahlem What is the command or recreate steps ? and is it consistent ?\r\n@bcbrock I used a P100 as well", "@antoajayraj I'm using Mozilla's deepspeech [1] to build a speech transcription model using the LibriSpeech dataset. It does not happen consistently.\r\n\r\n[1] https://github.com/mozilla/DeepSpeech", "I'm able to reproduce this on a POWER system with:\r\n\r\n- TensorFlow        1.1.0\r\n- Ubuntu            16.04.2 (kernel 4.4.0-77-generic)\r\n- libc6:ppc64el     2.23-0ubuntu7\r\n- cuda-cudart-8-0   8.0.61-1\r\n- GPU driver        375.51\r\n\r\nIn a crash I caught, it looks like there are 2 mutexes and 2 or 3 threads involved.\r\n\r\nFirst mutex is wanted by thread 1 (110471 / 0x1af87), and seems to be owned by thead 17 (110465 / 0x1af81). Thread 1 detects a deadlock while trying to acquire and asserts.\r\n\r\n(In the core file, this mutex appears locked by thread 113 (110449 / 0x1af71) which conflicts with the ownership recorded in the mutex. But it also appears to be marked as if the owner has died, so maybe thread 113 had been waiting and made a bit of progress after thread 1 hit the assert().)\r\n\r\n    (gdb) x/20x 0x100056c7e40\r\n    0x100056c7e40:  0xc001af71      0x00000001      0x0001af81      0x00000001\r\n                    __lock          __count         __owner         __nusers\r\n    0x100056c7e50:  0x00000021      0x00000000      0x00000000      0x00000000\r\n                    __kind\r\n\r\nSecond mutex is wanted by thead 17 (110465 / 0x1af81) and is locked by thread 1 (110471 / 0x1af87).\r\n\r\n    (gdb) x/8x 0x3fff57d39e78\r\n    0x3fff57d39e78: 0x8001af87      0x00000001      0x00000000      0x00000000\r\n                    __lock          __count         __owner         __nusers\r\n    0x3fff57d39e88: 0x00000021      0x00000000      0x00000000      0x00000000\r\n                    __kind\r\n\r\nAll three of the involved threads had come through cudaLaunch() and are somewhere in libcuda:\r\n\r\n    Thread 1 (Thread 0x3bffad57f1a0 (LWP 110471)):\r\n    #0  0x00003fffa027edb0 in __GI_raise (sig=<optimized out>) at ../sysdeps/unix/sysv/linux/raise.c:54\r\n    #1  0x00003fffa0281270 in __GI_abort () at abort.c:89\r\n    #2  0x00003fffa0274290 in __assert_fail_base (fmt=<optimized out>,\r\n        assertion=0x3fffa04296e8 \"INTERNAL_SYSCALL_ERRNO (e, __err) != EDEADLK || (kind != PTHREAD_MUTEX_ERRORCHECK_NP && kind != PTHREAD_MUTEX_RECURSIVE_NP)\",\r\n        file=0x3fffa04296c8 \"../nptl/pthread_mutex_lock.c\", line=<optimized out>, function=<optimized out>) at assert.c:92\r\n    #3  0x00003fffa0274384 in __GI___assert_fail (\r\n        assertion=0x3fffa04296e8 \"INTERNAL_SYSCALL_ERRNO (e, __err) != EDEADLK || (kind != PTHREAD_MUTEX_ERRORCHECK_NP && kind != PTHREAD_MUTEX_RECURSIVE_NP)\",\r\n        file=0x3fffa04296c8 \"../nptl/pthread_mutex_lock.c\", line=<optimized out>, function=0x3fffa0429830 <__PRETTY_FUNCTION__.9294> \"__pthread_mutex_lock_full\") at assert.c:101\r\n    #4  0x00003fffa041b400 in __pthread_mutex_lock_full (mutex=0x100056c7e40) at ../nptl/pthread_mutex_lock.c:347\r\n    #5  0x00003fff8b94e268 in ?? () from /usr/lib/powerpc64le-linux-gnu/libcuda.so.1\r\n    #6  0x00003fff8b9202d8 in ?? () from /usr/lib/powerpc64le-linux-gnu/libcuda.so.1\r\n    #7  0x00003fff8b806ef4 in ?? () from /usr/lib/powerpc64le-linux-gnu/libcuda.so.1\r\n    #8  0x00003fff8b9a46f8 in cuLaunchKernel () from /usr/lib/powerpc64le-linux-gnu/libcuda.so.1\r\n    #9  0x00003fff7596f0b8 in ?? () from /usr/local/cuda-8.0/lib64/libcudart.so.8.0\r\n    #10 0x00003fff759bda88 in cudaLaunch () from /usr/local/cuda-8.0/lib64/libcudart.so.8.0\r\n    #11 0x00003fff909d4678 in ?? () from /opt/DL/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n    #12 0x00003fff909e95e0 in tensorflow::functor::CastFunctor<Eigen::GpuDevice, float, unsigned char>::operator()(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 16,\r\n     Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<unsigned char const, 1, 1, long>, 16, Eigen::MakePointer>) ()\r\n       from /opt/DL/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n    #13 0x00003fff909d0c6c in ?? () from /opt/DL/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n    #14 0x00003fff9096c03c in tensorflow::CastOpBase::Compute(tensorflow::OpKernelContext*) ()\r\n       from /opt/DL/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n    #15 0x00003fff91a8332c in tensorflow::BaseGPUDevice::ComputeHelper(tensorflow::OpKernel*, tensorflow::OpKernelContext*) ()\r\n       from /opt/DL/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n    #16 0x00003fff91a83778 in tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) ()\r\n       from /opt/DL/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n    #17 0x00003fff91ac6e20 in ?? () from /opt/DL/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n    #18 0x00003fff91ee0d30 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()\r\n       from /opt/DL/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n    #19 0x00003fff91ee1688 in std::_Function_handler<void (), Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::NonBlockingThreadPoolTempl(int, bool, tensorflow::thread::E\r\n    igenEnvironment)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /opt/DL/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n    #20 0x00003fff91eddf74 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n       from /opt/DL/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n    #21 0x00003fff91f12f90 in ?? () from /opt/DL/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n    #22 0x00003fff758049a4 in ?? () from /usr/lib/powerpc64le-linux-gnu/libstdc++.so.6\r\n    #23 0x00003fffa0418070 in start_thread (arg=0x3bffad57f1a0) at pthread_create.c:335\r\n    #24 0x00003fffa0363230 in clone () at ../sysdeps/unix/sysv/linux/powerpc/powerpc64/clone.S:96\r\n\r\n\r\n    Thread 17 (Thread 0x3bffb057f1a0 (LWP 110465)):\r\n    #0  0x00003fffa041adc4 in __pthread_mutex_lock_full (mutex=0x3fff57d39e78) at ../nptl/pthread_mutex_lock.c:339\r\n    #1  0x00003fff8b94e268 in ?? () from /usr/lib/powerpc64le-linux-gnu/libcuda.so.1\r\n    #2  0x00003fff8b9202d8 in ?? () from /usr/lib/powerpc64le-linux-gnu/libcuda.so.1\r\n    #3  0x00003fff8b929e28 in ?? () from /usr/lib/powerpc64le-linux-gnu/libcuda.so.1\r\n    #4  0x00003fff8b8f95ac in ?? () from /usr/lib/powerpc64le-linux-gnu/libcuda.so.1\r\n    #5  0x00003fff8b90cab0 in ?? () from /usr/lib/powerpc64le-linux-gnu/libcuda.so.1\r\n    #6  0x00003fff8ba9a454 in ?? () from /usr/lib/powerpc64le-linux-gnu/libcuda.so.1\r\n    #7  0x00003fff8b806cec in ?? () from /usr/lib/powerpc64le-linux-gnu/libcuda.so.1\r\n    #8  0x00003fff8b806f38 in ?? () from /usr/lib/powerpc64le-linux-gnu/libcuda.so.1\r\n    #9  0x00003fff8b9a46f8 in cuLaunchKernel () from /usr/lib/powerpc64le-linux-gnu/libcuda.so.1\r\n    #10 0x00003fff7596f0b8 in ?? () from /usr/local/cuda-8.0/lib64/libcudart.so.8.0\r\n    #11 0x00003fff759bda88 in cudaLaunch () from /usr/local/cuda-8.0/lib64/libcudart.so.8.0\r\n    #12 0x00003fff90bc9e08 in ?? () from /opt/DL/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n    #13 0x00003fff90be4a50 in void tensorflow::functor::ReduceFunctor<Eigen::GpuDevice, Eigen::internal::ProdReducer<int> >::Reduce<Eigen::TensorMap<Eigen::Tensor<int, 0, 1, long>, 16, Eigen::Ma\r\n    kePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 1, 1, long>, 16, Eigen::MakePointer>, Eigen::array<long, 1ul> >(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<int, 0, 1, long>\r\n    , 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 1, 1, long>, 16, Eigen::MakePointer>, Eigen::array<long, 1ul> const&, Eigen::internal::ProdReducer<int> const&) ()\r\n       from /opt/DL/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n    #14 0x00003fff90b919c4 in tensorflow::ReductionOp<Eigen::GpuDevice, int, Eigen::internal::ProdReducer<int> >::Compute(tensorflow::OpKernelContext*) ()\r\n       from /opt/DL/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n    #15 0x00003fff91a8332c in tensorflow::BaseGPUDevice::ComputeHelper(tensorflow::OpKernel*, tensorflow::OpKernelContext*) ()\r\n       from /opt/DL/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n    #16 0x00003fff91a83778 in tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) ()\r\n       from /opt/DL/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n    #17 0x00003fff91ac6e20 in ?? () from /opt/DL/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n    #18 0x00003fff91ee0d30 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()\r\n       from /opt/DL/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n    #19 0x00003fff91ee1688 in std::_Function_handler<void (), Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::NonBlockingThreadPoolTempl(int, bool, tensorflow::thread::E\r\n    igenEnvironment)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /opt/DL/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n    #20 0x00003fff91eddf74 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n       from /opt/DL/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n    #21 0x00003fff91f12f90 in ?? () from /opt/DL/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n    #22 0x00003fff758049a4 in ?? () from /usr/lib/powerpc64le-linux-gnu/libstdc++.so.6\r\n    #23 0x00003fffa0418070 in start_thread (arg=0x3bffb057f1a0) at pthread_create.c:335\r\n    #24 0x00003fffa0363230 in clone () at ../sysdeps/unix/sysv/linux/powerpc/powerpc64/clone.S:96\r\n\r\n\r\n    Thread 113 (Thread 0x3bffb857f1a0 (LWP 110449)):\r\n    #0  0x00003fffa041adc4 in __pthread_mutex_lock_full (mutex=0x100056c7e40) at ../nptl/pthread_mutex_lock.c:339\r\n    #1  0x00003fff8b94e268 in ?? () from /usr/lib/powerpc64le-linux-gnu/libcuda.so.1\r\n    #2  0x00003fff8b9202d8 in ?? () from /usr/lib/powerpc64le-linux-gnu/libcuda.so.1\r\n    #3  0x00003fff8b806ef4 in ?? () from /usr/lib/powerpc64le-linux-gnu/libcuda.so.1\r\n    #4  0x00003fff8b9a46f8 in cuLaunchKernel () from /usr/lib/powerpc64le-linux-gnu/libcuda.so.1\r\n    #5  0x00003fff7596f0b8 in ?? () from /usr/local/cuda-8.0/lib64/libcudart.so.8.0\r\n    #6  0x00003fff759bda88 in cudaLaunch () from /usr/local/cuda-8.0/lib64/libcudart.so.8.0\r\n    #7  0x00003fff909d3238 in ?? () from /opt/DL/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n    #8  0x00003fff909e38e0 in tensorflow::functor::CastFunctor<Eigen::GpuDevice, int, unsigned char>::operator()(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<int, 1, 1, long>, 16, Eig\r\n    en::MakePointer>, Eigen::TensorMap<Eigen::Tensor<unsigned char const, 1, 1, long>, 16, Eigen::MakePointer>) ()\r\n       from /opt/DL/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n    #9  0x00003fff909d099c in ?? () from /opt/DL/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n    #10 0x00003fff9096c03c in tensorflow::CastOpBase::Compute(tensorflow::OpKernelContext*) ()\r\n       from /opt/DL/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n    #11 0x00003fff91a8332c in tensorflow::BaseGPUDevice::ComputeHelper(tensorflow::OpKernel*, tensorflow::OpKernelContext*) ()\r\n       from /opt/DL/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n    #12 0x00003fff91a83778 in tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) ()\r\n       from /opt/DL/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n    #13 0x00003fff91ac6e20 in ?? () from /opt/DL/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n    #14 0x00003fff91ee0d30 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()\r\n       from /opt/DL/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n    #15 0x00003fff91ee1688 in std::_Function_handler<void (), Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::NonBlockingThreadPoolTempl(int, bool, tensorflow::thread::E\r\n    igenEnvironment)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /opt/DL/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n    #16 0x00003fff91eddf74 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n       from /opt/DL/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n    #17 0x00003fff91f12f90 in ?? () from /opt/DL/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n    #18 0x00003fff758049a4 in ?? () from /usr/lib/powerpc64le-linux-gnu/libstdc++.so.6\r\n    ---Type <return> to continue, or q <return> to quit---\r\n    #19 0x00003fffa0418070 in start_thread (arg=0x3bffb857f1a0) at pthread_create.c:335\r\n    #20 0x00003fffa0363230 in clone () at ../sysdeps/unix/sysv/linux/powerpc/powerpc64/clone.S:96\r\n", "Confirmed that I also see this with NVIDIA driver 375.66.  I've asked NVIDIA to take a look as well.", "The results of my testing so far as as follows. All tests with Ubuntu 16.04.2 (kernel 4.4.0-72 or -78) and CUDA 8.0.61:\r\n\r\n    TF      Driver    cuDNN     Comment\r\n    1.0.1   361.119   5.0.10    OK\r\n    1.0.1   375.51    5.1.10    Hang/Assert\r\n    1.0.1   375.51    6.0.20    Hang/Assert\r\n    1.1.0   361.119   5.0.10    OK\r\n    1.1.0   375.51    6.0.20    Assert\r\n    1.1.0   375.66    6.0.20    Assert/Hang\r\n\r\nThe problem seems not to occur with the 361 series driver, but does with the 375 series driver.\r\n\r\nGenerally, I see much higher throughput (images / sec) with TF 1.1.0 and with the system more aggressively tuned (with adjustments to performance governor, SMT mode, and GPU clock settings).\r\n\r\n- In the lower-throughput configs the symptom is more likely a process hang.\r\n- In the higher-throughput configs the symptom is more likely the assert.\r\n\r\nWill share these results with NVIDIA.\r\n\r\nFor now, I'd suggest using the 361-series driver, and would be interested in any reports of hangs or asserts with that driver.", "Just got recheck from our IBM PowerAI experts: this is a known bug in the current Nvidia 375 GPU driver: back-leveling the driver to the 361 level will fix the issue.\r\nNvidia will need to reissue the device driver once proble understood/fixed I guess, hopefully they also are in GitHub\r\n"]}, {"number": 8464, "title": "what ide should I use with c++", "body": "I would like to use c++ programming, I would like to ask what I should use ide tools, I tried vs and eclipse, but failed to import tensorflow", "comments": ["It seems that you should ask stackoverflow, not propose a github issue."]}, {"number": 8463, "title": "customize parse_op.py and recompile", "body": "Hi, my problem is that I want to hack code in the parse_op.py file\uff0c especially I want to have an option of Dict or OrderedDict for the output of parse_single_example and many other parser that return Dict. Should I submit a PR or you guys can add that in the next version ?\r\n\r\nBy installing from source , I got following errors \r\n\r\n`INFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\n.........\r\nINFO: All external dependencies fetched successfully.\r\nConfiguration finished\r\n[zaikun@greina0 tensorflow]$ python -c 'import tensorflow'\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"tensorflow/python/__init__.py\", line 51, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\nImportError: No module named pywrap_tensorflow_internal\r\n`", "comments": ["Please properly fill out the issue template in order for us to help diagnose. ", "version : 1.0.0\r\nreproducible example : \r\n`  \r\n\r\n    reader = tf.TFRecordReader()\r\n\r\n    _, serialized_example = reader.read(filename_queue_video)\r\n\r\n    # set the mapping from the fields to data types in the proto\r\n    num_features = len(self.video_feature_names)\r\n\r\n    assert num_features > 0, \"self.feature_names is empty!\"\r\n    assert len(self.video_feature_names) == len(self.video_feature_sizes), \\\r\n    \"length of feature_names (={}) != length of feature_sizes (={})\".format( \\\r\n    len(self.video_feature_names), len(self.video_feature_sizes))\r\n\r\n    feature_map = {\"video_id\": tf.FixedLenFeature([], tf.string),\r\n                   \"labels\": tf.VarLenFeature(tf.int64)}\r\n    for feature_index in range(num_features):\r\n      feature_map[self.video_feature_names[feature_index]] = tf.FixedLenFeature(\r\n          [self.video_feature_sizes[feature_index]], tf.float32)\r\n\r\n    features = tf.parse_single_example(serialized_example,\r\n                                       features=feature_map)\r\n`\r\nIn the implementation of parse_single_example, a Dict is returned. However, I would like to have an orderedDict in python", "I am not sure if anyone is working on this.  My personal preference is to keep interfaces simple.  You could easily add a utility function in your own model to return an ordered dict."]}, {"number": 8462, "title": "Error while compiling tensorflow from sources", "body": "Hello, I am trying to install tensorflow from sources in a server where I haven't root access, so I can't update Cuda nor cuDNN. However, the versions installed are currently supported as said [here](https://www.tensorflow.org/install/install_sources).\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04.4 LTS\r\ngcc version: 4.8.4\r\npython 2.7\r\n\r\nInstalled version of CUDA and cuDNN:\r\nCuda compilation tools, release 7.5, V7.5.17\r\nCUDNN_MAJOR 5  CUDNN_MINOR 0  CUDNN_PATCHLEVEL 4\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n```\r\nls -l /usr/local/cuda/lib64/libcud*\r\n-rw-r--r-- 1 root root 322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root     16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\r\nlrwxrwxrwx 1 root root     19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\r\n-rwxr-xr-x 1 root root 383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\r\n-rw-r--r-- 1 root root 720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a\r\n```\r\n\r\nIf installed from source, provide\r\n\r\n1. The commit hash (`git rev-parse HEAD`): 12a98726e769e988f6368a029ec2f5b0ac3ccbd4\r\n2. The output of `bazel version`:  Build label: 0.4.4\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n[#5191](https://github.com/tensorflow/tensorflow/issues/5191)\r\n[#817](https://github.com/tensorflow/tensorflow/issues/817)\r\nhttp://stackoverflow.com/questions/42756614/tensorflow-compiling-from-source-c-compilation-of-rule-tensorflow-stream-e\r\n[#3074](https://github.com/tensorflow/tensorflow/issues/3074)\r\n[#3360](https://github.com/tensorflow/tensorflow/issues/3360)\r\n[#8061](https://github.com/tensorflow/tensorflow/issues/8061)\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nThe configure works fine but when I run `bazel build --config=opt --config=cuda --verbose_failures  //tensorflow/tools/pip_package:build_pip_package` I get the following error:\r\n\r\n```\r\nERROR: /home/angomez/tensorflow_cuda/tensorflow/tensorflow/stream_executor/BUILD:39:1: C++ compilation of rule '//tensorflow/stream_executor:cuda_platform' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command\r\n  (cd /home/angomez/.cache/bazel/_bazel_angomez/95301c24f4425af6d9c850e03508fc5b/execroot/tensorflow && \\\r\n  exec env - \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-march=native' -D_FORCE_INLINES '-std=c++11' '-march=native' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/stream_executor/_objs/cuda_platform/tensorflow/stream_executor/cuda/cuda_dnn.pic.d '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/stream_executor/_objs/cuda_platform/tensorflow/stream_executor/cuda/cuda_dnn.pic.o' -fPIC -DEIGEN_MPL2_ONLY -DTENSORFLOW_USE_JEMALLOC -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/jemalloc -iquote bazel-out/local_linux-opt/genfiles/external/jemalloc -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/protobuf -iquote bazel-out/local_linux-opt/genfiles/external/protobuf -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/local_linux-opt/genfiles/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/local_linux-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/local_linux-opt/genfiles/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/local_linux-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/local_linux-opt/genfiles/external/farmhash_archive -iquote external/highwayhash -iquote bazel-out/local_linux-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/local_linux-opt/genfiles/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/local_linux-opt/genfiles/external/local_config_cuda -isystem external/jemalloc/include -isystem bazel-out/local_linux-opt/genfiles/external/jemalloc/include -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/external/protobuf/src -isystem external/eigen_archive -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/local_linux-opt/genfiles/external/gif_archive/lib -isystem external/farmhash_archive/src -isystem bazel-out/local_linux-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/local_linux-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/local_linux-opt/genfiles/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/local_linux-opt/genfiles/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/include -isystem bazel-out/local_linux-opt/genfiles/external/local_config_cuda/cuda/include -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers -c tensorflow/stream_executor/cuda/cuda_dnn.cc -o bazel-out/local_linux-opt/bin/tensorflow/stream_executor/_objs/cuda_platform/tensorflow/stream_executor/cuda/cuda_dnn.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc: In instantiation of 'cudnnStatus_t perftools::gputools::cuda::wrap::WrapperShim__cudnnSetRNNDescriptor::operator()(perftools::gputools::cuda::CUDAExecutor*, Args ...) [with Args = {cudnnRNNStruct*, int, int, cudnnDropoutStruct*, cudnnRNNInputMode_t, cudnnDirectionMode_t, cudnnRNNMode_t, cudnnDataType_t}]':\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:986:50:   required from here\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:140:46: error: invalid conversion from 'cudnnDropoutStruct*' to 'int' [-fpermissive]\r\n       cudnnStatus_t retval = ::__name(args...);                    \\\r\n...\r\n```\r\nThe complete error log is here http://pastebin.com/VXKnh8ga\r\n\r\n### What other attempted solutions have you tried?\r\nI've tried to compile with gcc version 4.8 since I've seen [this](http://stackoverflow.com/questions/37313212/tensorflow-bazel-build-failing), but it didn't work.\r\n\r\nThank you!", "comments": ["Hi,\r\nto me this looks like cudnn isn't installed on your machine or is located at a different path than ` /usr/local/cuda/lib64/`. If it's installed in the cuda directory, `ls -l /path/to/cuda/lib/libcud*` should usually also include some output like `/usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5`.\r\nSo you either need to find the path where cudnn lives or you could try the solution below.\r\n\r\nTo install cuda and cudnn you don't need root permissions on your machine, as long as your drivers are recent enough to support a specific cuda version:\r\n\r\n### Local installation of cuda and cudnn\r\nDownload [Cuda](https://developer.nvidia.com/cuda-downloads) (download the .run file) and [Cudnn](https://developer.nvidia.com/cudnn) (needs an account). \r\n\r\n- Execute the cuda run file and install cuda to a local path, for example $HOME/lib/cuda-7.5/\r\n    - Don't create a symlink to /usr/local/cuda.\r\n    - Don't install the nvidia-driver, ignore the incomplete installation warning at the end of the cuda installation.\r\n- Extract the cudnn folder and copy the files in the cudnn-subfolder to the same cuda-8.0 subfolders.\r\n\r\nNow you can specify ` $HOME/lib/cuda-7.5/` or whatever your directory is when running the tensorflow configure script.", "Hi! \r\nNo, Cudnn isn't installed in `/usr/local/cuda/lib64`, it is installed in `/opt/cuda/lib64`, which is the path I give to the tensorflow configure script when asking for Cudnn directory. Is it necessary for Cudnn to be installed in `/usr/local/cuda/lib64` i.e., the same directory as Cuda?\r\n\r\nI'm going to try the local installation of Cuda and Cudnn.\r\nThank you for your answer!", "Hello,\r\nI asked the system operator to copy the cudnn files to the cuda directory, `/usr/local/cuda`, and now the output of `ls -l /path/to/cuda/lib/libcud*` is the following:\r\n```\r\nls -l /usr/local/cuda/lib64/libcud*\r\n-rw-r--r-- 1 root root 322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root     16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\r\nlrwxrwxrwx 1 root root     19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\r\n-rwxr-xr-x 1 root root 383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\r\n-rw-r--r-- 1 root root 720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a\r\nlrwxrwxrwx 1 root root     27 Mar 21 16:03 /usr/local/cuda/lib64/libcudnn.so -> /opt/cuda/lib64/libcudnn.so\r\nlrwxrwxrwx 1 root root     29 Mar 21 16:04 /usr/local/cuda/lib64/libcudnn.so.5 -> /opt/cuda/lib64/libcudnn.so.5\r\nlrwxrwxrwx 1 root root     33 Mar 21 16:04 /usr/local/cuda/lib64/libcudnn.so.5.0.4 -> /opt/cuda/lib64/libcudnn.so.5.0.4\r\nlrwxrwxrwx 1 root root     33 Mar 21 16:04 /usr/local/cuda/lib64/libcudnn_static.a -> /opt/cuda/lib64/libcudnn_static.a\r\n```\r\nHowever, the error in the bazel build command is still there.", "Hi,\r\nsome ideas to debug:\r\n- Maybe your cudnn version is somehow incompatible with the cuda version, for example if cuda was installed via apt and cudnn was installed manually. Ubuntu 14.04 currently has cudnn version 5.0.{5,6} and 5.1.{3, 5, 10} in the repos (says `apt-cache madison libcudnn5`. You seem to have version 5.0.4. Probably try the local solution with one of the versions above.\r\n- Try to run bazel clean manually before running ./configure\r\n- Explicitly state the cuda version, or maybe even better overwrite all default values (even if they match your requirements)\r\n- Did you try a local install of cuda-8.0? #5447 mentions the tensorflow team only tests with cuda 8.0. This issue also seems to be related to yours.\r\n\r\nIf all of the above doesn't work, it might be useful to paste the output of the ./configure script.\r\n\r\nGood luck!\r\n\r\n", "I get the same error when compiling from code.\r\n\r\nI want to update my TF from 0.12.0 (which I had compiled myself, too) to the most recent version. The current settings work fine: Ubuntu 16.04 LTS with gcc 4.9.3, TF 0.12.0, Cuda 8.0, cudNN 5.1.5, python 3.5 But compiling the most recent version of TF (with the same setup) returns the following error:\r\n\r\n`ni@WV8782:~/Downloads/tensorflow$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package` produces \r\n```\r\nERROR: /home/ni/Downloads/tensorflow/tensorflow/stream_executor/BUILD:39:1: C++ compilation of rule '//tensorflow/stream_executor:cuda_platform' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/ni/.cache/bazel/_bazel_ni/ee42072b03580a9e2812c30ca6c3e85a/execroot/tensorflow && \\\r\n  exec env - \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-march=native' '-std=c++11' '-march=native' -MD -MF bazel-out/local_linux-py3-opt/bin/tensorflow/stream_executor/_objs/cuda_platform/tensorflow/stream_executor/cuda/cuda_blas.pic.d '-frandom-seed=bazel-out/local_linux-py3-opt/bin/tensorflow/stream_executor/_objs/cuda_platform/tensorflow/stream_executor/cuda/cuda_blas.pic.o' -fPIC -DEIGEN_MPL2_ONLY -DTENSORFLOW_USE_JEMALLOC -iquote . -iquote bazel-out/local_linux-py3-opt/genfiles -iquote external/jemalloc -iquote bazel-out/local_linux-py3-opt/genfiles/external/jemalloc -iquote external/bazel_tools -iquote bazel-out/local_linux-py3-opt/genfiles/external/bazel_tools -iquote external/protobuf -iquote bazel-out/local_linux-py3-opt/genfiles/external/protobuf -iquote external/eigen_archive -iquote bazel-out/local_linux-py3-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/local_linux-py3-opt/genfiles/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/local_linux-py3-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/local_linux-py3-opt/genfiles/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/local_linux-py3-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/local_linux-py3-opt/genfiles/external/farmhash_archive -iquote external/highwayhash -iquote bazel-out/local_linux-py3-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/local_linux-py3-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/local_linux-py3-opt/genfiles/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/local_linux-py3-opt/genfiles/external/local_config_cuda -isystem external/jemalloc/include -isystem bazel-out/local_linux-py3-opt/genfiles/external/jemalloc/include -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/protobuf/src -isystem bazel-out/local_linux-py3-opt/genfiles/external/protobuf/src -isystem external/eigen_archive -isystem bazel-out/local_linux-py3-opt/genfiles/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/local_linux-py3-opt/genfiles/external/gif_archive/lib -isystem external/farmhash_archive/src -isystem bazel-out/local_linux-py3-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/local_linux-py3-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/local_linux-py3-opt/genfiles/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/local_linux-py3-opt/genfiles/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/include -isystem bazel-out/local_linux-py3-opt/genfiles/external/local_config_cuda/cuda/include -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers -c tensorflow/stream_executor/cuda/cuda_blas.cc -o bazel-out/local_linux-py3-opt/bin/tensorflow/stream_executor/_objs/cuda_platform/tensorflow/stream_executor/cuda/cuda_blas.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\n```\r\n\r\n\r\nThanks for your help, PhilJd.\r\n\r\n> Maybe your cudnn version is somehow incompatible with the cuda version, for example if cuda was installed via apt and cudnn was installed manually. Ubuntu 14.04 currently has cudnn version 5.0.{5,6} and 5.1.{3, 5, 10} in the repos (says apt-cache madison libcudnn5. You seem to have version 5.0.4. Probably try the local solution with one of the versions above.\r\n\r\nDoesn't seem to be the case considering that previous TensorFlow version runs smoothly.\r\n\r\n> Try to run bazel clean manually before running ./configure\r\n\r\nTried this. Sadly, the same error.\r\n\r\n> Explicitly state the cuda version, or maybe even better overwrite all default values (even if they match your requirements)\r\n\r\nTried this, too. Same error.\r\n\r\nDoes anyone have some recommendations or even a solution? I appreciate your help.\r\n", "It seems your error message is incomplete - does the same `invalid conversion` error follow?", "Hi, I've been trying to install tensorflow with the ideas you gave me.\r\nI tried this\r\n\r\n> Try to run bazel clean manually before running ./configure\r\n\r\nand this\r\n\r\n> Explicitly state the cuda version, or maybe even better overwrite all default values (even if they match your requirements)\r\n\r\nbut it didn't work. Then I tried this\r\n\r\n> Maybe your cudnn version is somehow incompatible with the cuda version, for example if cuda was installed via apt and cudnn was installed manually. Ubuntu 14.04 currently has cudnn version 5.0.{5,6} and 5.1.{3, 5, 10} in the repos (says apt-cache madison libcudnn5. You seem to have version 5.0.4. Probably try the local solution with one of the versions above.\r\n\r\nI installed cudNN version 5.1.10 locally. The error changes, but still failing.\r\n\r\nI couldn't try this\r\n\r\n> Did you try a local install of cuda-8.0? #5447 mentions the tensorflow team only tests with cuda 8.0. This issue also seems to be related to yours.\r\n\r\nbecause of the nvidia drivers installed.\r\n\r\nAt the end, the system operator updated the global cudnn version and now it works.\r\n\r\nThank you so much for your help.\r\n\r\n\r\n"]}, {"number": 8461, "title": "Introduce conv3d_transpose in tf.layers", "body": "This PR attempts to introduce `Conv3DTranspose` class and an equivalent functional interface named `conv3d_transpose`. Additions have been done in **tensorflow/python/layers/convolution.py**\r\n\r\nInclusion consists of additions of class, method, documentation, tests and aliases.\r\n\r\nFixes #8359", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please", "Does a member need to always write \"Jenkins, test this please\" to run the tests ? I made a minor typo, just fixed it.", "Sorry, I'm not too fluent in conv layers and have never used the 3d ones. Unassigning myself.", "Jenkins, test this please", "Francois, mind reviewing this pull request?", "I'm on vacation for a few days. I'll get to it, but it will take a while.\n\nOn Mar 17, 2017 11:22 AM, \"Jonathan Hseu\" <notifications@github.com> wrote:\n\nFrancois, mind reviewing this pull request?\n\n\u2014\nYou are receiving this because your review was requested.\n\nReply to this email directly, view it on GitHub\n<https://github.com/tensorflow/tensorflow/pull/8461#issuecomment-287433408>,\nor mute the thread\n<https://github.com/notifications/unsubscribe-auth/AArWb4y5gUy6Nbz48n8v1xbziECTCbrQks5rms9kgaJpZM4MfE2m>\n.\n", "Jenkins, test this please.", "@fchollet please take a look at new set of commits. Thanks!", "Jenkins, test this please.", "Finally Jenkins goes green ! :checkered_flag: Making the final set of changes now.", "Woohoo!", "Jenkins, test this please.\r\n\r\nI'll take a look.", "Jenkins, test this please.", "Just to be clear, this does not expose anything until you modify the [`tf.layers` interface](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/layers.py#L52). In that case, it would need an API review.", "/CC: @martinwicke potentially for API review.", "_ping_", "We still need an API review. @martinwicke @jhseu perhaps?", "@drpngx the API is good.", "Added this PR to the TF API review list for their meeting next week.", "Okay so if everything is well, then I shall expose this layer in functional API.\r\nAlso, is it required to manually make a RELEASE.md entry about a new layer interface or does some bot do it ?", "It's manual. Feel free to add an entry in this PR.", "@karandesai-96 feel free to push a new commit with the release notes then let me know.\r\n\r\n@fchollet thanks!", "@drpngx I made a changelog entry and as the master had moved forward a lot, I did a rebase of this branch to make sure that my code is compatible with current master. ", "Thanks!\r\n\r\nJenkins, test this please.", "@drpngx I just checked on master, there were a couple of commits which have these failures on MacOS CPU, but it was rectified a few hours ago. I guess if I rebase again, this shall be solved. Rebasing to master again ! ", "Done ! Thanks for too many \"Jenkins, test this please\" @drpngx. I believe I would let you do that less frequently from my next PR, got to know a lot of things about Tensorflow dev from my first one :smile: ", "Jenkins, test this please.\r\n\r\nWe'll have to plant a lot of trees to offset this PR ;-)", "Jenkins, test this please.", "Hooray!", "Thanks @martinwicke @fchollet @drpngx and others involved, first contribution to tensorflow! Feels good to contribute back to the community :smile: ", "Thank you and congratulations!\n\nOn Apr 26, 2017 9:22 AM, \"Karan Desai\" <notifications@github.com> wrote:\n\n> Thanks @martinwicke <https://github.com/martinwicke> @fchollet\n> <https://github.com/fchollet> @drpngx <https://github.com/drpngx>, first\n> contribution to tensorflow! Feels good to contribute back to the community\n> \ud83d\ude04\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/8461#issuecomment-297464714>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_Sbc3vVR346fG4KVt2-dTyfNdJeQWEks5rz29KgaJpZM4MfE2m>\n> .\n>\n"]}, {"number": 8460, "title": "No nigthly build for Python3.5 GPU", "body": "According to [the build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3.5,label=gpu-linux/) the last time the GPU version for python 3.5 was built was almost a month ago. That actually leads the download link from the README.md front page to point to a [file not found page](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3.5,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.0.1-cp35-cp35m-linux_x86_64.whl).\r\n\r\nThat's too bad because there are some bug-fixes that I needed which are then not included (#7585 for instance).", "comments": ["What about this link?\r\nhttps://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-linux/\r\n\r\n( I clicked on your link, then clicked on \"nightly matrix GPU\"). I think perhaps Python 3.5 got renamed to Python 3, to support the fact that 3.6 and 3.5 can use the same wheel", "I originally tried without success, python complaining about the version.\r\n\r\nThough it can work indeed, but ONLY after renaming it to `tensorflow_gpu-1.0.1-py3-none-linux_x86_64.whl`. Which is what I did in the end after some googling around.\r\n\r\nMaybe adding some clarification somewhere?", "So to summarize the issue:\r\nIf you are Linux GPU Python 3.5 user, and you go through \"Installation\" section on https://github.com/tensorflow/tensorflow and click the appropriate link, you will not get the nightly, but instead you will get a month old version.\r\n\r\n", "@caisq @jhseu looks like we forgot to bring this build back.\r\nDo we have the configs in place?\r\ndo we just need to change jenkins?", "Yes, @gunan. There are a bunch of manual things to do after the rollback to 14.04 for build. I'll take care of those today.", "Seems it's up again already, great!"]}, {"number": 8459, "title": "Broken curl URL for flower_photos.tgz", "body": "The tutorial for retraining Inception's final layer has a malformed URL, the instructions for downloading the dataset are:\r\n\r\n```\r\ncd ~\r\ncurl -O http://download.tensorflow.org/example_../images/flower_photos.tgz\r\ntar xzf flower_photos.tgz\r\n```\r\n\r\nThe URL is broken, I think it's supposed to be:\r\n\r\n```\r\ncurl -O http://download.tensorflow.org/example_images/flower_photos.tgz\r\n```\r\n", "comments": ["Already fixed. The docs should be updated with the release of TF's next version: https://github.com/drpngx/tensorflow/commit/1f8eae975b16940d60041065b08d15d47af6c820", "Great, thanks!"]}, {"number": 8458, "title": "Move module-level comment out of _RNNCell class", "body": "The _RNNCell class docstring contained a description of the module\r\nitself. I moved it back to the module's docstring.\r\nI also fixed a comment that wasn't correctly indented.", "comments": ["Can one of the admins verify this patch?", "Perhaps the comment belongs in `tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py` instead?", "@jhseu what's the rationale behind the existence of both `tensorflow/python/ops/rnn_cell_impl.py` and `tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py`? why not merge them in a single file?", "contrib specifies APIs that aren't stable yet. Some parts of the RNN API have stabilized, and others haven't.", "Jenkins, test this please.", "It is merged already."]}, {"number": 8457, "title": "import android example to android studio fail", "body": "When I follow the step by android READ.ME, It failed .\r\n\r\n Error:(145, 0) A problem occurred evaluating root project 'android'.\r\nand the location is: from  from file(externalModelData).listFiles() \r\ntask copyExternalAssets(type: Copy) {\r\n    from file(externalModelData).listFiles()\r\n    include '*.pb'\r\n    include '*.txt'\r\n    include 'thumbnails/*.jpg'\r\n    into 'assets'\r\n    fileMode 0644\r\n    dependsOn buildExternalAssets\r\n}\r\nI dont know how to solve this .", "comments": ["Run `bazel build -c opt tensorflow/examples/android:tensorflow_demo` once manually. This will initialize the bazel-tensorflow directory that Gradle is looking for the model files in. After that you'll be able to build normally with Android Studio.\r\n\r\nThere's a fix pending to make this work in a more generic fashion without needing Bazel.", "Thanks, It's running. But there is a 'bug', if my root directory is \"tensorflow-master\", the \"bazel-tensorflow-master\" would be initialized, but the build.gradle is looking for the \"bazel-tensorflow\". So ,it need to rename the directory"]}, {"number": 8456, "title": "how to reduce  libtensorflow_demo.so size ?", "body": "I build my own libtensorflow_demo.so file use   `bazel build -c opt tensorflow/examples/android:libtensorflow_demo.so --crosstool_top=//external:android/crosstool --cpu=x86 --host_crosstool_top=@bazel_tools//tools/cpp:toolchain` \r\nbut it is too large for me . how to reduce it ?\r\nx86   --> 66M\r\nx86_64 --> 76.2M\r\narmeabi-v7a --> 17.9M\r\narm64-v8a -->33M", "comments": ["Strange, libtensorflow_demo.so should be less than 1mb.\r\n\r\nWhich NDK do you have installed? The recommended version for Bazel right now is r12b. Others may have unexpected behavior.\r\n\r\nIf you copy the following flags from libtensorflow_inference.so, does that help at all?\r\n\r\n```\r\n    copts = tf_copts() + [\r\n        \"-ffunction-sections\",\r\n        \"-fdata-sections\",\r\n    ],\r\n    linkopts = if_android([\r\n...\r\n        \"-Wl,--gc-sections\",\r\n...\r\n    ]),\r\n```\r\n\r\nAnother option is to download the prebuilt binaries from [https://ci.tensorflow.org/view/Nightly/job/nightly-android/](https://ci.tensorflow.org/view/Nightly/job/nightly-android/)", "@zhangpao17 you are probably building debug version rather than release.", "Closing due to inactivity; please update with additional info if problems persist.", "@victorv Hi victorv, can you elaborate how to build a release version for Android? I cannot find related documents. Thanks.", "@Kika-Xumengwei - from Android Studio, select release build from Build Variants pane."]}, {"number": 8455, "title": "R1.0", "body": "update\r\n", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "Thanks! Looks like we forgot to pull these notes back in.\r\n\r\nJenkins, test this please.", "Mind handling the CLA?", "We're cutting 1.1 today, so I'll need to cherry-pick these release notes in from the r1.0 branch in another pull request. Please sign the CLA for any future pull requests. Thanks!"]}, {"number": 8454, "title": "No OpKernel was registered to support Op 'FIFOQueueV2' with these attrs.", "body": "### Description\r\n\r\nI'm trying to use the [Inception-ResNet-V2](https://github.com/tensorflow/models/tree/master/slim) model from TF-slim in the Android demo app. I'm now seeing the following inference exception:\r\n\r\n    E/TensorFlowInferenceInterface: Inference exception: java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'FIFOQueueV2' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n                                                                                     <no registered kernels>\r\n                                                                                   \r\n                                                                                   \t [[Node: batch/fifo_queue = FIFOQueueV2[capacity=128, component_types=[DT_FLOAT, DT_UINT8, DT_INT64], container=\"\", shapes=[[299,299,3], [299,299,3], []], shared_name=\"\"]()]]\r\n\r\nTo ensure this issue does not come from my build, I've download the latest [libtensorflow_inference.so](http://ci.tensorflow.org/view/Nightly/job/nightly-android/70/artifact/out/native/) from Jenkins. I've verified that this library still provides the same exception.\r\n\r\nI'm not familiar with the error message but it seems the Op `FIFOQueueV2` is defined in [fifo_queue_op.cc](https://github.com/tensorflow/tensorflow/blob/4433079e7f317724eaa92ec120c6b1c3c0c52f2f/tensorflow/core/kernels/fifo_queue_op.cc#L61). Can someone help take a look?", "comments": ["@nathansilberman - does it look like it's an issue of the BUILD file needing to be updated to depend on the FIFOQueueV2 kernel? ", "@concretevitamin Thanks for your reply.\r\n\r\n@nathansilberman: I've found a workaround by loading this model checkpoint, create a `tf.placeholder` and then use this as the `input_node` instead. \r\n\r\nPlease correct me if I am wrong. I guess there is no easy way to use the current queue-based input graph on Android. If we use that then the input node needs to be a `batch/queue` or a `shuffle_batch/random_shuffle_queue`, but there is no supporting OpKernel for these Ops right now.", "@derekhh how did you get it working by simply creating a placeholder? The FIFOQueueV2 node is still there and shoudl cause a NoOp on android? I am struggling with the same issue", "@aleksab Sorry for not being clear.\r\n\r\nBasically I've created a new Python script that looks like this. This restores the TF-Slim Inception model from your previous checkpoint and reads from a placeholder. The model `inception_v1_final.pb` created can be used for Android.\r\n\r\nI'm quite new to TensorFlow so I'm sure this won't be the best solution - creating a new Python script for this purpose already seems stupid. If there's a better solution please let me know. Thanks!\r\n\r\n    import os\r\n    import tensorflow as tf\r\n    import tensorflow.contrib.slim as slim\r\n\r\n    from nets import inception\r\n    from nets import inception_v1\r\n    from nets import nets_factory\r\n\r\n    from tensorflow.python.framework import graph_util\r\n    from tensorflow.python.platform import gfile\r\n\r\n    checkpoint_path = tf.train.latest_checkpoint('./log')\r\n\r\n    with tf.Graph().as_default() as graph:\r\n        input_tensor = tf.placeholder(tf.float32, shape=(None, 224, 224, 3), name='input_image')\r\n        with tf.Session() as sess:\r\n            with tf.variable_scope('model') as scope:\r\n                with slim.arg_scope(inception.inception_v3_arg_scope()):\r\n                    logits, end_points = inception.inception_v1(input_tensor, num_classes=45, is_training=False)\r\n\r\n        saver = tf.train.Saver()\r\n        saver.restore(sess, checkpoint_path)\r\n\r\n        output_node_names = 'model/InceptionV1/Logits/Predictions/Softmax'\r\n        input_graph_def = graph.as_graph_def()\r\n        output_graph_def = graph_util.convert_variables_to_constants(sess, input_graph_def, output_node_names.split(\",\"))\r\n        with open('./output_graph_nodes.txt', 'w') as f:\r\n            for node in output_graph_def.node:\r\n                f.write(node.name + '\\n')\r\n\r\n        output_graph = './inception_v1_final.pb'\r\n        with gfile.FastGFile(output_graph, 'wb') as f:\r\n            f.write(output_graph_def.SerializeToString())\r\n", "Many ops are disabled on Android for code size reasons, so this is WAI unfortunately.", "Hey, guys. \r\nI was encountered that problem recently. And I fix it just by put \"fifo_queue_op.cc\" in file 'tensorflow/core/kernels/BUILD'  inside the `filegroup(\r\n    name = \"android_core_ops\", srcs = [ ...`, which configure the core kernel in Android.  ", "@nesadiankemo  I have encountered the same issue. . but your fix did not help.. I added \"fifo_queue_op.cc\" in android_core_ops and also in tensorflow/contrib/makefile/tf_op_files.txt . But the same error still comes up..  Did you take any other steps to fix it. Could you please explain?", "@anandcu3 In the source code, I only modify two source file. Another file is I commented at #2680 to fix that issue. Then I follow the instruction [https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/android](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/android) to build the .so file. And it fix the issue in my App.\r\nHere is all the steps i had taken. Hope it will help you."]}, {"number": 8453, "title": "Cant run code on GPU -- Windows 10", "body": "i have installed CUDA v8.0 and have put the cudnn-8.0 files in my cuda program files directory and have set the bin folder of the cuda directory in my environment variables PATH.\r\ni have installed tensorflow-gpu as per the instructions given in the tensorflow website and installed it on my anaconda which has python 3.5.2  after running the matmul example I cant get it to be run on the gpu.\r\n```E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"CountExtremelyRandomStats\" device_type: \"CPU\"') for unknown op: CountExtremelyRandomStats\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"FinishedNodes\" device_type: \"CPU\"') for unknown op: FinishedNodes\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"GrowTree\" device_type: \"CPU\"') for unknown op: GrowTree\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ReinterpretStringToFloat\" device_type: \"CPU\"') for unknown op: ReinterpretStringToFloat\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"SampleInputs\" device_type: \"CPU\"') for unknown op: SampleInputs\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ScatterAddNdim\" device_type: \"CPU\"') for unknown op: ScatterAddNdim\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNInsert\" device_type: \"CPU\"') for unknown op: TopNInsert\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNRemove\" device_type: \"CPU\"') for unknown op: TopNRemove\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TreePredictions\" device_type: \"CPU\"') for unknown op: TreePredictions\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"UpdateFertileSlots\" device_type: \"CPU\"') for unknown op: UpdateFertileSlots\r\nDevice mapping: no known devices.\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\common_runtime\\direct_session.cc:257] Device mapping:\r\n\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:841] MatMul: (MatMul)/job:localhost/replica:0/task:0/cpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:841] b: (Const)/job:localhost/replica:0/task:0/cpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:841] a: (Const)/job:localhost/replica:0/task:0/cpu:0\r\nMatMul: (MatMul): /job:localhost/replica:0/task:0/cpu:0\r\nb: (Const): /job:localhost/replica:0/task:0/cpu:0\r\na: (Const): /job:localhost/replica:0/task:0/cpu:0\r\n[[ 22.  28.]\r\n [ 49.  64.]]```", "comments": ["I ran the code below to get the following result:\r\n```import tensorflow as tf\r\n\r\n# Creates a graph.\r\nwith tf.device('/gpu:0'):\r\n  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\n  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\n  c = tf.matmul(a, b)\r\n# Creates a session with log_device_placement set to True.\r\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n# Runs the op.\r\nprint(sess.run(c))\r\n\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"CountExtremelyRandomStats\" device_type: \"CPU\"') for unknown op: CountExtremelyRandomStats\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"FinishedNodes\" device_type: \"CPU\"') for unknown op: FinishedNodes\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"GrowTree\" device_type: \"CPU\"') for unknown op: GrowTree\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ReinterpretStringToFloat\" device_type: \"CPU\"') for unknown op: ReinterpretStringToFloat\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"SampleInputs\" device_type: \"CPU\"') for unknown op: SampleInputs\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ScatterAddNdim\" device_type: \"CPU\"') for unknown op: ScatterAddNdim\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNInsert\" device_type: \"CPU\"') for unknown op: TopNInsert\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNRemove\" device_type: \"CPU\"') for unknown op: TopNRemove\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TreePredictions\" device_type: \"CPU\"') for unknown op: TreePredictions\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"UpdateFertileSlots\" device_type: \"CPU\"') for unknown op: UpdateFertileSlots\r\nDevice mapping: no known devices.\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\common_runtime\\direct_session.cc:257] Device mapping:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1022, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1000, in _run_fn\r\n    self._extend_graph()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1049, in _extend_graph\r\n    self._session, graph_def.SerializeToString(), status)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device to node 'MatMul': Could not satisfy explicit device specification '/device:GPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0\r\n\t [[Node: MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/device:GPU:0\"](a, b)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/Ali J/PycharmProjects/GPU_TEST/main.py\", line 11, in <module>\r\n    print(sess.run(c))\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 767, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 965, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1015, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1035, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device to node 'MatMul': Could not satisfy explicit device specification '/device:GPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0\r\n\t [[Node: MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/device:GPU:0\"](a, b)]]\r\n\r\nCaused by op 'MatMul', defined at:\r\n  File \"C:/Users/Ali J/PycharmProjects/GPU_TEST/main.py\", line 7, in <module>\r\n    c = tf.matmul(a, b)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1765, in matmul\r\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 1454, in _mat_mul\r\n    transpose_b=transpose_b, name=name)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2327, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1226, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Cannot assign a device to node 'MatMul': Could not satisfy explicit device specification '/device:GPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0\r\n\t [[Node: MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/device:GPU:0\"](a, b)]]\r\n\r\n\r\nProcess finished with exit code 1```", "pycuda works perfectly fine though", "Could you please try installing [Visual C++ Redistributable 2015 x64](https://www.microsoft.com/en-us/download/details.aspx?id=53587), uninstall TensorFlow then reinstall it again and follow up if it solves the OpKernel errors? \r\nDo you have both, CPU and GPU TF installations on your machine?\r\n\r\n", "I have Visual C++ Redistributable 2015 x64 already installed.\r\nYes, I have both CPU and GPU TF installations on my machine.", "Please try downloading a [nightly build](http://ci.tensorflow.org/view/Nightly/job/nightly-win/112/), uninstalling TF and installing with it.\r\nIf you want to run with GPU support uninstall the CPU version and to toggle between CPU/GPU use:\r\n```\r\nwith tf.Session() as sess:\r\n     with tf.device(\"/cpu:0\"):\r\n```\r\n", "In my second comment on this issue, i toggle onto GPU device and it crashes", "Sorry, I skimmed and let this pass. You don't need two TF versions to use CPU and GPU simultaneously though. \r\nDid you try with the nightly build? ", "i dont know how to use it to be honest", "No problem. You need to click on the `DEVICE=GPU` then download `tensorflow_gpu-1.0.1-cp35-cp35m-win_amd64.whl`, a wheel that you will use to install TensorFlow with.\r\n\r\nFor instance, you downloaded the file to your home directory, you cd in the terminal to where the home directory is and do `pip install tensorflow_gpu-1.0.1-cp35-cp35m-win_amd64.whl\t`.\r\nBefore though, make sure you uninstalled both TensorFlow versions. Try `pip uninstall tensorflow` followed by the same but with `tensorflow-gpu`.\r\n\r\nIt is as installing TensorFlow by the website instructions but instead of the url tgiven to the wheel hosted online you will downloading a TensorFlow build locally and installing it with pip.", "ok but, isnt running with an official release better?", "Those builds are official, the point is maybe there was a bug on the build provided by the website and the nightly build has the fix, as they update the codebase and make the builds daily but didn't yet replace the url wheel. \r\nI honestly don't have other suggestion by now and this one has solved the same problem before but feel free to wait other folks as they might have other suggestions or solutions.", "omg...it worked!! thanks so much man", "You're welcome. This issue can be closed now.", "Thanks @Carmezim for the great support and @ajanaliz for confirming the fix!", "@ajanaliz Can you tell me briefly, how could you solve the problem? I am also facing the same problem. My operating system is windows 10 and I have not installed visual studio on my this computer. The link of nightly build doesn't work. Thanks in advance."]}, {"number": 8452, "title": "Tensordot partial shape inference", "body": "Addresses #6682.  Tensordot now correctly infers partial shapes when possible.  In some cases, such inference is still impossible, such as when axes are specified as a tensor or when either of the operand tensors has an unknown number of dimensions.\r\n\r\nThe tests for tensordot were updated as well in order to test the new behavior", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "@tensorflow-jenkins test this please", "@person594 Thanks for the fix!"]}]