[{"number": 47407, "title": "Different outputs from GradientTape and CropAndResizeGradImage ", "body": "**System information**\r\n== tensorflow installed from info ==================\r\nName: tensorflow\r\nVersion: 2.4.1\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: /home/user/.venv/lib/python3.6/site-packages\r\nRequired-by: \r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(3, 6, 8, 'final', 0)\r\n\r\n\r\n**Describe the current behavior**\r\nI am calculating gradients using GradientTape.gradient() and comparing those against CropAndResizeGradImage().\r\n\r\n**Describe the expected behavior**\r\nThis check passes for some configurations (like one given on https://www.tensorflow.org/api_docs/python/tf/image/crop_and_resize#example) not always.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef test_crop_and_resize(BATCH_SIZE=1, NUM_BOXES=41, IMAGE_DIM=15, CHANNELS=6, CROP_SIZE=8):\r\n\r\n    image = tf.random.normal(shape=(BATCH_SIZE, IMAGE_DIM, IMAGE_DIM, CHANNELS) )\r\n    boxes = tf.random.uniform(shape=(NUM_BOXES, 4))\r\n\r\n    box_indices = tf.random.uniform(shape=(NUM_BOXES,), minval=0, maxval=BATCH_SIZE, dtype=tf.int32)\r\n\r\n    with tf.GradientTape() as g:\r\n        g.watch(image)\r\n        output = tf.image.crop_and_resize(image, boxes, box_indices, (CROP_SIZE, CROP_SIZE))\r\n\r\n    grad_in = tf.random.normal(output.shape)\r\n    grad_out = g.gradient(output, image, grad_in)\r\n    grad_out_raw = tf.raw_ops.CropAndResizeGradImage(grads=grad_in, boxes=boxes, box_ind=box_indices, image_size=image.shape, T=tf.float32, method='bilinear')\r\n    np.testing.assert_array_equal(grad_out.numpy(), grad_out_raw.numpy())\r\n\r\ntest_crop_and_resize()\r\n```\r\n\r\n```\r\nE       AssertionError: \r\nE       Arrays are not equal\r\nE       \r\nE       Mismatched elements: 962 / 1350 (71.3%)\r\nE       Max absolute difference: 3.6710215\r\nE       Max relative difference: 320.43555\r\nE        x: array([[[[ 4.556600e-01,  3.979062e-01,  1.036789e-01,  4.944029e-01,\r\nE                  2.423330e-01,  1.300183e-01],\r\nE                [ 5.906850e-01,  3.972412e-01,  2.205431e-01,  1.333644e+00,...\r\nE        y: array([[[[ 4.556600e-01,  3.979062e-01,  1.036789e-01,  4.944029e-01,\r\nE                  2.423330e-01,  1.300183e-01],\r\nE                [ 5.906850e-01,  3.972412e-01,  2.205431e-01,  1.333644e+00,...\r\n```", "comments": ["@Verma-Rajat \r\nI ran the code and do not face any error as reported, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/50fab505fc9ba2bdbb07a48ec0edf3a1/untitled552.ipynb#scrollTo=cPgBtJFj-1MP)", "Hmm, in your gist, I don't see test_crop_and_resize() getting invoked. If I do invoke it, I see the mismatch.\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef test_crop_and_resize(BATCH_SIZE=1, NUM_BOXES=41, IMAGE_DIM=15, CHANNELS=6, CROP_SIZE=8):\r\n\r\n    image = tf.random.normal(shape=(BATCH_SIZE, IMAGE_DIM, IMAGE_DIM, CHANNELS) )\r\n    boxes = tf.random.uniform(shape=(NUM_BOXES, 4))\r\n\r\n    box_indices = tf.random.uniform(shape=(NUM_BOXES,), minval=0, maxval=BATCH_SIZE, dtype=tf.int32)\r\n\r\n    with tf.GradientTape() as g:\r\n        g.watch(image)\r\n        output = tf.image.crop_and_resize(image, boxes, box_indices, (CROP_SIZE, CROP_SIZE))\r\n\r\n    grad_in = tf.random.normal(output.shape)\r\n    grad_out = g.gradient(output, image, grad_in)\r\n    grad_out_raw = tf.raw_ops.CropAndResizeGradImage(grads=grad_in, boxes=boxes, box_ind=box_indices, image_size=image.shape, T=tf.float32, method='bilinear')\r\n    np.testing.assert_array_equal(grad_out.numpy(), grad_out_raw.numpy())\r\n\r\ntest_crop_and_resize()\r\n```\r\n\r\nOutput from modified gist\r\n```\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-11-d80c2cf0190e> in <module>()\r\n     18     np.testing.assert_array_equal(grad_out.numpy(), grad_out_raw.numpy())\r\n     19 \r\n---> 20 test_crop_and_resize()\r\n\r\n2 frames\r\n/usr/local/lib/python3.7/dist-packages/numpy/testing/_private/utils.py in assert_array_compare(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\r\n    838                                 verbose=verbose, header=header,\r\n    839                                 names=('x', 'y'), precision=precision)\r\n--> 840             raise AssertionError(msg)\r\n    841     except ValueError:\r\n    842         import traceback\r\n\r\nAssertionError: \r\nArrays are not equal\r\n\r\nMismatched elements: 501 / 1350 (37.1%)\r\nMax absolute difference: 2.3841858e-06\r\nMax relative difference: 5.073249e-05\r\n x: array([[[[-1.132136e+00,  1.010646e+00,  8.798134e-03, -1.515116e+00,\r\n          -2.033477e-01,  1.113951e+00],\r\n         [ 8.912135e-01, -1.255336e+00, -1.182982e+00, -2.225933e+00,...\r\n y: array([[[[-1.132136e+00,  1.010646e+00,  8.798134e-03, -1.515116e+00,\r\n          -2.033477e-01,  1.113951e+00],\r\n         [ 8.912135e-01, -1.255336e+00, -1.182982e+00, -2.225933e+00,...\r\n```", "@Saduf2019  One interesting thing is on my local system with 'tf-nightly' I see mismatches of larger magnitude.\r\n```\r\nE       Mismatched elements: 947 / 1350 (70.1%)\r\nE       Max absolute difference: 3.3557742\r\nE       Max relative difference: 114.614235\r\n```\r\n\r\nWhile on jupyter notebook, they are of very small magnitude.\r\n```\r\nMismatched elements: 501 / 1350 (37.1%)\r\nMax absolute difference: 2.3841858e-06\r\nMax relative difference: 5.073249e-05\r\n```", "I am able to replicate the issue reported on tf 2.4 and tf-nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/66fa2cb4ec7f25013f81c209b3c887a3/untitled554.ipynb).", "I am able to replicate the issue reported on tf 2.5,please check this [gist](https://colab.research.google.com/gist/sushreebarsa/fe0849ce85eae2441adb0347abd15c90/untitled117.ipynb?authuser=1)...Thanks !", "Was able to replicate the issue with TF 2.6.0-dev20210606,please find the gist [here ](https://colab.research.google.com/gist/sushreebarsa/1ff870a1087c225a03f5ca4b7a396e31/untitled167.ipynb?authuser=1)..Thanks!", "I see `Max absolute difference: 2.861023e-06` and `Max relative difference: 0.00034471` in Tensorflow 2.8, which is very small fraction. Could you please confirm if you find this difference good enough. Thanks!"]}, {"number": 47400, "title": "Compile TensorFlow Lite for iOS Simulator on Apple Silicon", "body": "TensorFlowLite isn't available for iOS Simulator running on Apple Silicon.\r\n\r\nIt seems that this depends on [Bazel supporting this](https://github.com/bazelbuild/rules_apple/issues/980), as well as on TensorFlowLite being built as a .xcframework (fat frameworks have an arm64 slice, but cannot have an arm64-mac slice).\r\n\r\n", "comments": ["@yyoon could you take a look?", "@bvirlet Yes, you are correct that we're blocked by the missing bazel support. Once bazel supports it, we will be migrating to .xcframework to incorporate the different target architectures, including the simulators on the new Apple silicon.", "Thanks. For reference, I'm linking this issue here that's related/the same: https://github.com/tensorflow/tensorflow/issues/44609", "+1", "@yyoon do you think [this PR](https://github.com/bazelbuild/bazel/pull/12900) is what we want from Bazel?", "@bvirlet I've looked at that PR before, but that's for building bazel itself to work natively on M1, which is different from building an ios_static_framework for arm64 simulator target. I think this would involve more work from apple build rules side.\r\n\r\nEven better, if bazel + apple rules start to support building an XCFramework with multiple target architectures, we could switch to using that for releasing our iOS binaries.", "Thanks for clarifying that!", "Looks like a new version of Bazel's `rules_apple` has been released with support for `ios_sim_arm64` architecture: https://github.com/bazelbuild/rules_apple/releases/tag/0.32.0\r\n\r\nIs it enough to be able to build TFLite for iOS Simulator on Apple Silicon?", "That's good to hear. Roughly speaking, this could be done in the following steps.\r\n\r\n1. Update bazel to 5.0.0 or higher.\r\n2. Update `rules_apple` version to 0.32.0 or higher. This version dependency is defined in: https://github.com/tensorflow/tensorflow/blob/592f3797f814eb280eb39bda8aea193e5d5c60bc/tensorflow/workspace2.bzl#L801-L805\r\n3. Add a new `.bazelrc` config for `ios_sim_arm64`: https://github.com/tensorflow/tensorflow/blob/592f3797f814eb280eb39bda8aea193e5d5c60bc/.bazelrc#L193-L206\r\n4. Build the iOS static famework following [this guide](https://www.tensorflow.org/lite/guide/build_ios#build_tensorflowlitec_dynamic_framework_recommended) but with `--config=ios_sim_arm64` option instead.\r\n\r\nI didn't have time to try this out myself unfortunately and there might be other roadblocks ahead, but it'd be worth trying if you're in need.", "same issue for me", "@yyoon \r\nTried you suggestion and got this error:\r\n```\r\nERROR: Config expansion has a cycle: config value ios_sim_arm64 expands to itself, see inheritance chain [ios_sim_arm64]\r\n```\r\n\r\nAny updates on this issue? What is the way to compile for arm64 simulator?", "@yishuangP Could you take a look when you get a chance? Thanks."]}, {"number": 47391, "title": "tf.errors.UnknownError should include __cause__", "body": "**System information**\r\n- TensorFlow version (you are using): 2.4.1\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nWhen a [`tf.errors.UnknownError`](https://www.tensorflow.org/api_docs/python/tf/errors/UnknownError) is raised, the `exception.__cause__` is not set (it is `None`), so if someone wants to find the underlying error, the best they can do is trying to infer it through the `exception.message` attribute, which is suboptimal at best.\r\n\r\n`exception.__cause__` should be set to the chained exception when known. See https://docs.python.org/3/library/exceptions.html.\r\n\r\n**Will this change the current api? How?**\r\n\r\nIt will change the `tf.errors.UnknownError` behaviour to include more information.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nEveryone who wants more control over `UnknownError`. For example, it is currently not possible for me to detect that the `UnknownError` is caused due to a `PIL.UnidentifiedImageError` unless I find the string in the `message`.\r\n\r\n**Any Other info.**\r\n\r\nCode causing the `UnknownError` that prompted the request:\r\n\r\n```python\r\ndef my_generator(img):\r\n    img = Image.open(io.BytesIO(img))  # <- here's the `PIL.UnidentifiedImageError`\r\n    img = tf.image.convert_image_dtype(img, tf.float32)\r\n    yield img, tf.constant([[]], dtype=tf.float32), tf.constant([], dtype=tf.int32)\r\n\r\nds = tf.data.Dataset.from_generator(lambda: my_generator((image_bytes))\r\n# ...trying to use ds...\r\n```\r\n", "comments": []}, {"number": 47362, "title": "Ampere bfloat16 support", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.4.1/built from head\r\n- Are you willing to contribute it (Yes/No): If you point me at the necessary code, I'll take a crack at it.\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI attempted to use bfloat16 training by setting:\r\n\r\n```\r\ntf.keras.mixed_precision.set_global_policy('mixed_bfloat16')\r\n```\r\n\r\nI got this error message:\r\n\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'Conv2D' used by {{node model/conv2d/Conv2D}} with these attrs: [dilations=[1, 1, 1, 1], T=DT_BFLOAT16, data_format=\"NHWC\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, explicit_paddings=[], padding=\"SAME\"]\r\nRegistered devices: [CPU, GPU]\r\nRegistered kernels:\r\n  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_BFLOAT16, DT_HALF]\r\n  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_BFLOAT16, DT_HALF]\r\n  device='GPU'; T in [DT_INT32]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_INT32]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_HALF]\r\n```\r\n\r\nI read the [cuDNN release notes](https://docs.nvidia.com/deeplearning/cudnn/release-notes/rel_8.html#rel-810) which said that support for bfloat16 came with cuDNN version 8.1, so I built tensorflow from scratch, using CUDA 11.2 and cuDNN 8.1.0.77.\r\n\r\nHowever, I got the same error message.  I've tried this on a trival example (the fashion mnist tutorial) with the same result, so it's not some advanced kernel that I'm using in my code.\r\n\r\nI am a little confused that I'm getting this error message even when running with XLA, since the error message says that XLA_GPU_JIT supports DT_BFLOAT16.\r\n\r\n\r\n**Will this change the current api? How?**\r\nNo, you can already specify a bfloat16 mixed precision policy.\r\n**Who will benefit with this feature?**\r\nAnyone with Ampere gpus who wants the speed of 16 bit floats and the numerical stability of bfloat16.\r\n", "comments": ["Is there an ETA for this feature request?", "For reference, result of [mixed precision tutorial](https://www.tensorflow.org/guide/mixed_precision) on RTX 3070 is as follows.\r\n* TF32 -> 0.9116, ~70ms per step (default)\r\n* FP16 -> 0.9274, ~45ms per step\r\n* BF16 -> 0.9477, ~3000ms per step\r\n\r\nIf model doesn't contain convolution layer, it runs correctly, even if it's slow. \r\n\r\n(tested environment : Ubuntu 20.04, [nvidia tensorflow container image 21.03](https://docs.nvidia.com/deeplearning/frameworks/tensorflow-release-notes/rel_21-04.html#rel_21-04))\r\n"]}, {"number": 47347, "title": "Custom objects in tf.keras have conflicting interface", "body": "Custom tf.keras layers (tf.2.4.)  use `call` for invocation.  However, custom constraints use `__call__`.  This seems confusing.  Furthermore, if you implement a constraint using `call` instead of `__ call __` no warning or error given. I wonder if its reasonable to expect a Not Implemented error to occur in this case of `__call__` missing.\r\n\r\nRef: \r\n1. Custom layer documentation: https://keras.io/guides/making_new_layers_and_models_via_subclassing/\r\n2. Custom constraint documentation: https://keras.io/api/layers/constraints/\r\n\r\nEDIT 1. Fixed link.\r\nEDIT 2. Added tensorflow version. Updated ticket title to reflect tf.keras.", "comments": ["You might want to update your links, you've provided two of the same links (directing to the constraint documentation) rather than two separate ones. \r\n\r\nThe link to the custom layer documentation: https://keras.io/guides/making_new_layers_and_models_via_subclassing/\r\n\r\nAdditionally, the case that you have mentioned where you implement a constraint using `call` vs `__call__`, while not returning an error, will not work with the intended functionality, since it will simply use the default `__call__` method of the `tf.keras.constraints.Constraint` superclass. \r\n\r\nFor example,\r\n\r\n```python\r\n>>> from tensorflow.keras.constraints import Constraint\r\n>>> class MyConstraint(Constraint):\r\n...   def __init__(self, value):\r\n...     self.value = value\r\n...   def call(self, w):\r\n...     return w * 2\r\n>>>\r\n>>> c = MyConstraint(3)\r\n>>> print(c(3))\r\n... 3\r\n```\r\n \r\nYou would expect it to return `6`, since the `call` function implements `w * 2`, but it instead returns the default superclass value of just `w`.\r\n\r\nSo agreed, there should likely be some form of consistency between the rest of the API, which uses `call`.", "Thanks for the pointing out my error.  I have corrected my initial post.\r\n\r\nIt seems to me that throwing Not Implemented would be the most appropriate.  I can't imagine a usecase where someone would make a constraint and want the parent class behavior of simply passing the vector through.", "@isaacgerg \r\n\r\nPlease, close this thread if your issue was resolved.Thanks!", "@ravikyram The issue has not been resolved. ", "Hello! Which version of keras does tensorflow-2.4.0 correspond to?", "You can see Keras v2.4.0; at the current stage Keras is simply being developed as part of TensorFlow so there's no exact version which it corresponds to.", "@dz1135508698  This affects tf.keras 2.4.  I have updated the issue so that it is more specific.", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210603, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/1af17b567364bcf350c53dfbc6370d12/47347.ipynb). Thanks!", "Hi, Could you please refer to the updated documentation from the Tensorflow documents on `tf.keras.constraints` [here](https://www.tensorflow.org/api_docs/python/tf/keras/constraints?version=nightly) and for Making new Layers and Models via subclassing [here](https://www.tensorflow.org/guide/keras/custom_layers_and_models) and let us know if you still feel the details are conflicting. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 47285, "title": "`tf.keras.backend` python api docs don't include most symbols exported", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/backend\r\n\r\n\r\n## Description of issue (what needs changing):\r\n### Clear description\r\n\r\n`tensorflow.keras.backend` is a re-export module; The generated api document page for `tf.keras.backend` only exposes a very small portion of the exports.\r\n\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nBut all of these methods _should_ be included:\r\n\r\n```\r\nIn [8]: print(dir(tf.keras.backend))\r\n['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_sys', 'abs', 'all', 'any', 'arange', 'argmax', 'argmin', 'backend', 'batch_dot', 'batch_flatten', 'batch_get_value', 'batch_normalization', 'batch_set_value', 'bias_add', 'binary_crossentropy', 'cast', 'cast_to_floatx', 'categorical_crossentropy', 'clear_session', 'clip', 'concatenate', 'constant', 'conv1d', 'conv2d', 'conv2d_transpose', 'conv3d', 'cos', 'count_params', 'ctc_batch_cost', 'ctc_decode', 'ctc_label_dense_to_sparse', 'cumprod', 'cumsum', 'depthwise_conv2d', 'dot', 'dropout', 'dtype', 'elu', 'epsilon', 'equal', 'eval', 'exp', 'expand_dims', 'eye', 'flatten', 'floatx', 'foldl', 'foldr', 'function', 'gather', 'get_uid', 'get_value', 'gradients', 'greater', 'greater_equal', 'hard_sigmoid', 'image_data_format', 'in_test_phase', 'in_top_k', 'in_train_phase', 'int_shape', 'is_keras_tensor', 'is_sparse', 'l2_normalize', 'learning_phase', 'learning_phase_scope', 'less', 'less_equal', 'local_conv1d', 'local_conv2d', 'log', 'manual_variable_initialization', 'map_fn', 'max', 'maximum', 'mean', 'min', 'minimum', 'moving_average_update', 'name_scope', 'ndim', 'normalize_batch_in_training', 'not_equal', 'one_hot', 'ones', 'ones_like', 'permute_dimensions', 'placeholder', 'pool2d', 'pool3d', 'pow', 'print_tensor', 'prod', 'random_bernoulli', 'random_binomial', 'random_normal', 'random_normal_variable', 'random_uniform', 'random_uniform_variable', 'relu', 'repeat', 'repeat_elements', 'reset_uids', 'reshape', 'resize_images', 'resize_volumes', 'reverse', 'rnn', 'round', 'separable_conv2d', 'set_epsilon', 'set_floatx', 'set_image_data_format', 'set_learning_phase', 'set_value', 'shape', 'sigmoid', 'sign', 'sin', 'softmax', 'softplus', 'softsign', 'sparse_categorical_crossentropy', 'spatial_2d_padding', 'spatial_3d_padding', 'sqrt', 'square', 'squeeze', 'stack', 'std', 'stop_gradient', 'sum', 'switch', 'tanh', 'temporal_padding', 'tile', 'to_dense', 'transpose', 'truncated_normal', 'update', 'update_add', 'update_sub', 'var', 'variable', 'zeros', 'zeros_like']\r\n```\r\n\r\n### Submit a pull request?\r\n\r\nI don't know how the doc generator works; so no\r\n", "comments": ["I've tested out some of the methods, specifically in_train_phase, and it works in 2.4.1. so indeed looks like a documentation issue"]}, {"number": 47277, "title": "tf.raw_ops.Send/Recv fail within MultiWorkerMirroredStrategy", "body": "I want to use `tf.raw_ops.Send` and `tf.raw_ops.Recv` to transfer tensor between different machines, but the `Recv` fails after `Send` operation is finished.\r\n\r\nHere is the error log:\r\n```shell\r\n2021-02-20 01:42:11.421182: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-02-20 01:42:11.424571: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2194875000 Hz\r\nWARNING:tensorflow:/job:worker/replica:0/task:0 seems down, retrying 1/3\r\nWARNING:tensorflow:/job:worker/replica:0/task:0 seems down, retrying 2/3\r\nERROR:tensorflow:Cluster check alive failed, /job:worker/replica:0/task:0 is down, aborting collectives: Deadline Exceeded\r\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:\r\n:{\"created\":\"@1613785391.258566931\",\"description\":\"Deadline Exceeded\",\"file\":\"external/com_github_grpc_grpc/src/core/ext/filters/deadline/deadline_filter.cc\",\"file_line\":69,\"grpc_status\":4}\r\n```\r\n\r\nAnd here is my code to reproduce this issue:\r\n```python\r\ndef test_multi_worker_mirrored_strategy_send_recv(args):\r\n    # specify cluster resolver\r\n    import os, json\r\n\r\n    # set visible gpus\r\n    os.environ['CUDA_VISIBLE_DEVICES']= \"\"\r\n\r\n    # set log leverl\r\n    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' # 1 show all, 2 for warning and error, 3 for only error\r\n\r\n    port = \"12345\"\r\n    ips = [host1, host2]\r\n    os.environ['TF_CONFIG'] = json.dumps({\r\n        'cluster': { \"worker\": [ip + \":\" + port for ip in ips] },\r\n        \"task\" : {'type': 'worker', \"index\": args.task_id}\r\n    })\r\n    resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\r\n\r\n    # create stratey\r\n    options = tf.distribute.experimental.CommunicationOptions(\r\n        implementation=tf.distribute.experimental.CommunicationImplementation.AUTO)\r\n    strategy = tf.distribute.MultiWorkerMirroredStrategy(resolver, options)\r\n\r\n    print(\"num_accelerators = \", strategy.cluster_resolver.num_accelerators())\r\n    print(\"task_type = %s, task_id = %s\" %(strategy.cluster_resolver.task_type, strategy.cluster_resolver.task_id))\r\n\r\n    @tf.function\r\n    def _test_step(task_id):\r\n        send_device = \"/job:worker/replica:0/task:0/device:CPU:0\"\r\n        recv_device = \"/job:worker/replica:0/task:1/device:CPU:0\"\r\n        if task_id == 0: # send\r\n            tensor = tf.constant([1.0, 2.0], dtype=tf.float32)\r\n            tf.raw_ops.Send(tensor=tensor, \r\n                            tensor_name=\"test_send_tensor\", \r\n                            send_device=send_device, \r\n                            send_device_incarnation=123, \r\n                            recv_device=recv_device,\r\n                            client_terminated=True)\r\n            print(\"Send tensor from: \", tensor.device)\r\n        else: # recv\r\n            tensor = tf.raw_ops.Recv(tensor_type=tf.float32, \r\n                                     tensor_name=\"test_send_tensor\", \r\n                                     send_device=send_device, \r\n                                     send_device_incarnation=123, \r\n                                     recv_device=recv_device,\r\n                                     client_terminated=False)\r\n            print(\"Recv tensor from: \", tensor.device)\r\n        return tensor\r\n\r\n    tensor = strategy.run(_test_step, args=(args.task_id,))\r\n    print(tensor)\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser(description='test')\r\n    parser.add_argument('--task_id', type=int, \r\n                        help='specify the task id in Multi Worker Mirrored Strategy cluster.',\r\n                        required=True)\r\n    args = parser.parse_args()\r\n\r\n    test_multi_worker_mirrored_strategy_send_recv(args)\r\n```", "comments": ["This is not supported. With MultiWorkerMirroredStrategy, you only have access to your local devices. Normally people use collective operations like all-reduce and all-gather to communicate between devices.", "> This is not supported. With MultiWorkerMirroredStrategy, you only have access to your local devices. Normally people use collective operations like all-reduce and all-gather to communicate between devices.\r\n\r\nThen what is the mechanism to communicate between remote machines?", "Normally people use collective operations like all-reduce and all-gather to communicate between devices.", "> Normally people use collective operations like all-reduce and all-gather to communicate between devices.\r\n\r\nCould you please give an usage example about how to do that between two remote workers?", "> Normally people use collective operations like all-reduce and all-gather to communicate between devices.\r\n\r\nAnd if I want to do `broadcast` between those remote workers, what is the option?", "It's not supported yet but is on our roadmap. You can try use all-gather for the interim.", "OK. Thanks.\r\n"]}, {"number": 47275, "title": "HDF5 locking issues when using concurrent.futures.ProcessPoolExecutor", "body": "I was working in TensorFlow and concurrent futures on **Windows 10** using anaconda.  Below is the MWE:\r\n\r\n    import tensorflow as tf\r\n    from tensorflow import keras\r\n    \r\n    import numpy as np\r\n    import concurrent.futures \r\n    import time\r\n\r\n    def simple_model():\r\n        model = keras.models.Sequential([\r\n            keras.layers.Dense(units = 10, input_shape = [1]),\r\n            keras.layers.Dense(units = 1, activation = 'sigmoid')\r\n        ])\r\n        model.compile(optimizer = 'sgd', loss = 'mean_squared_error')\r\n        return model\r\n    \r\n    def clone_model(model):\r\n        model_clone = tf.keras.models.clone_model(model)\r\n        model_clone.set_weights(model.get_weights())\r\n        return model_clone\r\n    \r\n    def work(model_path, seq):\r\n        # model = clone_model(model)# model_list[model_id]\r\n        # print(model)\r\n        # import tensorflow as tf\r\n        model = tf.keras.models.load_model(model_path)\r\n        return model.predict(seq)\r\n    \r\n    def workers(model, num_of_seq = 4):\r\n        seqences = np.arange(0,num_of_seq*10).reshape(num_of_seq, -1)\r\n        model_savepath = './simple_model.h5'\r\n        model.save(model_savepath)\r\n        path_list = [model_savepath for _ in range(num_of_seq)]\r\n    \r\n        with concurrent.futures.ProcessPoolExecutor(max_workers=None) as executor:        \r\n            t0 = time.perf_counter()\r\n            # model_list = [clone_model(model) for _ in range(num_of_seq)]\r\n            index_list = np.arange(1, num_of_seq)\r\n            # [clone_model(model) for _ in range(num_of_seq)]\r\n            # print(model_list)\r\n            future_to_samples = {executor.submit(work, path, seq): seq for path, seq in zip(path_list,seqences)}\r\n        Seq_out = []\r\n        for future in concurrent.futures.as_completed(future_to_samples):\r\n            out = future.result()\r\n            Seq_out.append(out)\r\n        t1 = time.perf_counter()\r\n        print(t1-t0)\r\n        return np.reshape(Seq_out, (-1, )), t1-t0\r\n    \r\n    \r\n    \r\n    if __name__ == '__main__':\r\n        model = simple_model()\r\n        num_of_seq = 400\r\n        # model_list = [clone_model(model) for _ in range(4)]\r\n        out = workers(model, num_of_seq=num_of_seq)\r\n        print(out)\r\n\r\nreturns:\r\n\r\n    2021-02-19 16:31:13.665341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]\r\n    15.456169300000003\r\n    (array([1., 1., 1., ..., 1., 1., 1.], dtype=float32), 15.456169300000003)\r\n\r\nWhen I try to run the script in Ubuntu it keeps running with no output and until I force quit the process. And some time it gives the following error:\r\n\r\n    OSError: Unable to open file (unable to lock file, errno = 37, error message = 'No locks available')\r\n\r\nWhat I have tried:\r\n\r\n 1. I used `conda tf-gpu export > environment.yml` to get all the installed files to a different Windows 10 and saw the same behavior.\r\n2. Made a new environment in Windows 10 (where the code is working) with various different TensorFlow versions. All TF-2.x versions worked out.\r\n3. Tried the same code on Docker by pulling TensorFlow images. Same issues\r\n4. The issue is with locking the HDF5 file, so tried: https://stackoverflow.com/questions/57310333/can-we-disable-h5py-file-locking-for-python-file-like-object. It did not work\r\n\r\nSimilar issues have been reported in:\r\n\r\n - https://stackoverflow.com/questions/49438814/opening-already-opened-hdf5-file-in-write-mode-using-h5py\r\n -  https://github.com/h5py/h5py/issues/1066\r\n -  https://github.com/h5py/h5py/issues/1101\r\n -  https://github.com/keras-team/keras/issues/11796\r\n - https://github.com/keras-team/keras/issues/11101\r\n", "comments": ["@asokraju \r\nI ran the code shared on tf 2.4 and face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/e714682cd4a968a2df453d30c950cbb3/untitled546.ipynb)", "For some reason multiprocessing  codes does not work on collab please try on a local\ncomputer. \n", "Above MWE, the aim is to predict the output of a saved model in parallel. \r\n\r\nWORKER: \r\n1. Saves the model to the disk and saves the path in `model_savepath `\r\n2. It then calls four workers by sending the model path and the `work `function (data that needs to be predicted).\r\n3. Each one then clones a model from the path (using `clone_model`) and then uses it to predict.\r\n\r\nPS: \r\n1. It might not work in COLAB. So try running it on a local computer.\r\n2. You can not parse the model directly in `future_to_samples `. For more information see #46917.\r\n3. The issue might be from read and write permissions of HDF5 file format resulting from:\r\n     - `model.save(model_savepath)`\r\n     - `tf.keras.models.clone_model(model_savepath)`\r\n     - also `tf.keras.models.load_model(model_savepath)`\r\n\r\n", "Have you tried saving your model to `tf` format? Does that work?", "@ymodak, as per your suggestion, I modified the code as follows:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nimport numpy as np\r\nimport concurrent.futures \r\nimport time\r\nimport os\r\n# gpus = tf.config.experimental.list_physical_devices('GPU')\r\n# if len(gpus) > 0:\r\n#     print(f'GPUs {gpus}')\r\n#     try: tf.config.experimental.set_memory_growth(gpus[0], True)\r\n#     except RuntimeError: pass\r\ndef simple_model():\r\n    model = keras.models.Sequential([\r\n        keras.layers.Dense(units = 10, input_shape = [1]),\r\n        keras.layers.Dense(units = 1, activation = 'sigmoid')\r\n    ])\r\n    model.compile(optimizer = 'sgd', loss = 'mean_squared_error')\r\n    return model\r\n\r\ndef clone_model(model):\r\n    model_clone = tf.keras.models.clone_model(model)\r\n    model_clone.set_weights(model.get_weights())\r\n    return model_clone\r\n\r\ndef work(model_path, seq):\r\n    t0 = time.perf_counter()\r\n    model = tf.keras.models.load_model(model_path)\r\n    # model = tf.saved_model.load(model_path)\r\n    t1 = time.perf_counter()\r\n    print(\"Time taken to load the model\", t1-t0)\r\n    return model.predict(seq)\r\n\r\ndef workers(model, num_of_seq = 4):\r\n    seqences = np.arange(0,num_of_seq*10).reshape(num_of_seq, -1)\r\n    # model_savepath = './testing/simple_model.h5'\r\n    model_savepath = './testing'\r\n    try:\r\n        os.mkdir(model_savepath)\r\n    except:\r\n        pass\r\n    model.save(model_savepath)\r\n    path_list = [model_savepath for _ in range(num_of_seq)]\r\n\r\n    with concurrent.futures.ProcessPoolExecutor(max_workers=None) as executor:        \r\n        t0 = time.perf_counter()\r\n        # model_list = [clone_model(model) for _ in range(num_of_seq)]\r\n        index_list = np.arange(1, num_of_seq)\r\n        # [clone_model(model) for _ in range(num_of_seq)]\r\n        # print(model_list)\r\n        future_to_samples = {executor.submit(work, path, seq): seq for path, seq in zip(path_list,seqences)}\r\n    Seq_out = []\r\n    for future in concurrent.futures.as_completed(future_to_samples):\r\n        out = future.result()\r\n        Seq_out.append(out)\r\n    t1 = time.perf_counter()\r\n    print(t1-t0)\r\n    return np.reshape(Seq_out, (-1, )), t1-t0\r\n\r\n\r\n\r\nif __name__ == '__main__':\r\n    model = simple_model()\r\n    num_of_seq = 400\r\n    # model_list = [clone_model(model) for _ in range(4)]\r\n    out = workers(model, num_of_seq=num_of_seq)\r\n    print(out)\r\n```\r\n\r\nAnd the output on my windows pc is \r\n\r\n```\r\n......\r\nTime taken to load the model 1.0405409000000034\r\n57.1098902\r\n(array([1., 1., 1., ..., 1., 1., 1.], dtype=float32), 57.1098902)\r\n```\r\nThis still has the same issue in Ubuntu. \r\nPS: The above out is generated on my windows computer, and the time taken to load the model in the earlier method is quite fast, (~0.05) compared to 1 second. I also noted that as the model complexity increases, the time increases ( a model with 4 hidden layers of 200 parmas each and a custom cost function took around 25 seconds to load).  #", "Hey, are you waiting for my comments?", "Was able to reproduce the issue in TF 2.6.0-dev20210527 ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/9cbb3f165d0f628edd9adafa1f0999d6/untitled31.ipynb#scrollTo=sCzU_uAv_oox)..Thanks !"]}, {"number": 47271, "title": "Export tensorflow-lite target in CMake build", "body": "**System information**\r\n- TensorFlow version (you are using): 2.4.1\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently it is only possible to integrate the tensorflow-lite CMake build as a source build into an external project.\r\nSee [https://www.tensorflow.org/lite/guide/build_cmake](https://www.tensorflow.org/lite/guide/build_cmake): \"Create a CMake project which uses TensorFlow Lite\".\r\nAdditionally the compiled libtensorflow-lite.a does not include all dependencies (ruy, absl):\r\nSee _\"Note: This generates a static library libtensorflow-lite.a in the current directory but the library isn't self-contained since all the transitive dependencies are not included.\"_. This makes it hard to manually import the prebuilt libary into new projects.\r\n\r\nThe suggestion would be to export the tensorflow-lite target using CMakes importing and exporting feature ([https://cmake.org/cmake/help/git-stage/guide/importing-exporting/index.html](https://cmake.org/cmake/help/git-stage/guide/importing-exporting/index.html)). All dependencies, like ruy and absl, should be either bundled into the tensorflow-lite library or also be exported.\r\n\r\n**Will this change the current api? How?**\r\nThis will add the possibility to import the tensorflow-lite CMake build into a separate external project using find_package() and without an additional source build. No modification, only an addition.\r\n\r\n**Who will benefit with this feature?**\r\n- Users of the Tensorflow Lite C/C++ API\r\n- Teams can build tensorflow-lite once and distribute the library in a package to all members\r\n- Allows to deliver a prebuilt tensorflow-lite and be easily integrated into projects\r\n", "comments": ["@terryheo could you take a look?", "One question, do you prefer to use it as shared library or static library?", "Using BUILD_SHARED_LIBS as an option so the user can decide would of course be ideal.\r\n\r\nOtherwise I would default to a static lib like in the make build.\r\n", "Using C++ requires lots of header files, so you'd better use C API for now.\r\nhttps://www.tensorflow.org/lite/guide/build_cmake#build_tensorflow_lite_c_library\r\n\r\nAnd I'll check the feasibility of providing C++ library later.", "The C Library also does not use CMake Exporting and Importing features as far as I can see.", "@terryheo Can I use the libtensorflowlite_c.so without the ruy and absl libraries? \r\n\r\nFor the Cpp interface it says \"This generates a static library libtensorflow-lite.a in the current directory but the library isn't self-contained since all the transitive dependencies are not included. \"\r\n\r\nI guess this is also the case for the C API?\r\n", "@terryheo Hi, is there any news on improving the export of the C++ tensorflow-lite target in CMake?", "Would love to see this happen as well!", "> @terryheo Can I use the libtensorflowlite_c.so without the ruy and absl libraries?\r\n> \r\n> For the Cpp interface it says \"This generates a static library libtensorflow-lite.a in the current directory but the library isn't self-contained since all the transitive dependencies are not included. \"\r\n> \r\n> I guess this is also the case for the C API?\r\n\r\nNo. The generated libtensorflowlite_c.so is self-contained so you don't need any additional libraries. So please use C API if you're not using CMake or Bazel."]}, {"number": 47270, "title": "LinearOperatorKronecker for SparseTensors", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): tensorflow 2.2(gpu) & 2.3, linux64, anaconda\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently there is no kronecker product operator that works on sparse tensors. Calling tf.linalg.LinearOperatorKronecker on sparse tensors raises `AttributeError: 'SparseTensor' object has no attribute 'is_non_singular'`\r\n\r\n**Will this change the current api? How?**\r\nPossibly?\r\n\r\nLinearOperatorKronecker checks attributes: `.is_non_singular`, `.is_self_adjoint`, and `.is_self_adjoint`, and then calls `.extend` during initialization, and looks for `.name`. SparseTensor doesn't have any of these. It's probably easier to build a kronecker product operator specifically for sparse tensors.\r\n\r\n**Who will benefit with this feature?**\r\nKronecker products on sparse tensors can be useful in building coefficient matrices for finite difference simulations. I could see wide application building grey box models for use in physics, industrial engineering, aerospace, weather, and so on. Pretty much anyone looking to solve PDEs on a regular grid.", "comments": []}, {"number": 47245, "title": "Illegal instruction (core dumped)", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 20.04 LTS\r\n- TensorFlow installed from: source\r\n- TensorFlow version: 2.4.1\r\n- Python version: 3.8.5\r\n- Installed using: pip\r\n- Bazel version: 4.0.0\r\n- GCC/Compiler version: 8.4.0\r\n- CUDA/cuDNN version: 11.1/8\r\n- GPU model and memory: GeForce GTX 1080 Ti (11.7G)\r\n\r\n**Describe the problem**\r\nAt first, I tried installing `tensorflow` from `pip` repositories, but it crashed with `Illegal instruction (core dumped)` upon importing. I've tracked this down to my CPU (Intel G4400) not supporting AVX instructions (https://github.com/tensorflow/tensorflow/issues/17411#issuecomment-608027554), so I built tensorflow from source, using the tools described above. I can successfully import tensorflow now, but when I try running the `deepdream` tutorial: https://www.tensorflow.org/tutorials/generative/deepdream.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. Downloaded notebook from https://www.tensorflow.org/tutorials/generative/deepdream\r\n2. `jupyter notebook`\r\n3. Click \"Run All\"\r\n4. `Illegal instruction (core dumped)`\r\n\r\nAlternatively:\r\n1. Downloaded notebook from https://www.tensorflow.org/tutorials/generative/deepdream\r\n2. `ipython nbconvert --to script deepdream.ipynb`\r\n3. `python3 deepdream.py`\r\n4. `Illegal instruction (core dumped)`\r\n\r\n**Any other info / logs**\r\nThis is the full log of the \"alternative\" sequence:\r\n\r\n```\r\n$ python3 deepdream.py                                                                                                                                                                                                                          \r\n2021-02-18 15:28:33.944636: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-02-18 15:28:35.342188: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-18 15:28:35.342976: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-02-18 15:28:35.372123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:35.372799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.645GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-02-18 15:28:35.372953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:35.373537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \r\npciBusID: 0000:04:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.6575GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-02-18 15:28:35.373661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:35.374241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 2 with properties: \r\npciBusID: 0000:06:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.6575GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-02-18 15:28:35.374363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:35.374945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 3 with properties: \r\npciBusID: 0000:08:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.6575GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-02-18 15:28:35.375035: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-02-18 15:28:35.419511: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-02-18 15:28:35.419904: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-02-18 15:28:35.430928: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-02-18 15:28:35.431822: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-02-18 15:28:35.440650: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\r\n2021-02-18 15:28:35.453660: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-02-18 15:28:35.454131: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-02-18 15:28:35.454448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:35.456315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:35.458227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:35.460016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:35.461794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:35.463552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:35.465343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:35.467095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:35.468689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1, 2, 3\r\n2021-02-18 15:28:35.469399: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-02-18 15:28:35.469726: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-18 15:28:36.139881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:36.140462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.645GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-02-18 15:28:36.140558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:36.141120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \r\npciBusID: 0000:04:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.6575GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-02-18 15:28:36.141188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:36.141733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 2 with properties: \r\npciBusID: 0000:06:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.6575GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-02-18 15:28:36.141793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:36.142330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 3 with properties: \r\npciBusID: 0000:08:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.6575GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-02-18 15:28:36.142348: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-02-18 15:28:36.142369: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-02-18 15:28:36.142380: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-02-18 15:28:36.142389: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-02-18 15:28:36.142399: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-02-18 15:28:36.142408: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\r\n2021-02-18 15:28:36.142418: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-02-18 15:28:36.142427: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-02-18 15:28:36.142482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:36.143058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:36.143631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:36.144209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:36.144794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:36.145431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:36.146029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:36.146610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:36.147133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1, 2, 3\r\n2021-02-18 15:28:36.147166: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-02-18 15:28:40.439754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-02-18 15:28:40.439843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 2 3 \r\n2021-02-18 15:28:40.439851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N Y Y Y \r\n2021-02-18 15:28:40.439856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   Y N Y Y \r\n2021-02-18 15:28:40.439859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 2:   Y Y N Y \r\n2021-02-18 15:28:40.439863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 3:   Y Y Y N \r\n2021-02-18 15:28:40.440129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:40.440819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:40.441455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:40.442052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:40.442640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:40.443222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:40.443766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6064 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2021-02-18 15:28:40.444097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:40.444766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:40.445324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 6122 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)\r\n2021-02-18 15:28:40.445584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:40.446200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:40.446752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 6122 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:06:00.0, compute capability: 6.1)\r\n2021-02-18 15:28:40.446992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:40.447637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 15:28:40.448199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 6122 MB memory) -> physical GPU (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:08:00.0, compute capability: 6.1)\r\n2021-02-18 15:28:44.046400: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-02-18 15:28:44.124782: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3299990000 Hz\r\n2021-02-18 15:28:44.980720: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\nIllegal instruction (core dumped)\r\n```\r\n\r\n(I believe this is a `build/install` problem rather than a bug because I've already encountered this error in the context of an install issue)", "comments": ["@tmladek \r\n\r\nCan you please run below code and share the output\r\n\r\n```\r\nimport tensorflow as tf\r\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\r\n```", "I meant to! But since the output is duplicated with what's already in the log, I decided not to. Anyway, here it is:\r\n```\r\n$ python3\r\nPython 3.8.5 (default, Jul 28 2020, 12:59:40)\r\n[GCC 9.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2021-02-18 18:06:54.212504: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n>>> print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\r\n2021-02-18 18:07:00.212753: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-18 18:07:00.215419: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-02-18 18:07:00.264637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 18:07:00.265742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.645GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-02-18 18:07:00.265940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 18:07:00.266883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties:\r\npciBusID: 0000:04:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.6575GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-02-18 18:07:00.267009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 18:07:00.267928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 2 with properties:\r\npciBusID: 0000:06:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.6575GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-02-18 18:07:00.268038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 18:07:00.268992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 3 with properties:\r\npciBusID: 0000:08:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.6575GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-02-18 18:07:00.269024: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-02-18 18:07:00.273164: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-02-18 18:07:00.273260: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-02-18 18:07:00.274762: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-02-18 18:07:00.275199: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-02-18 18:07:00.276520: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\r\n2021-02-18 18:07:00.277575: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-02-18 18:07:00.277829: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-02-18 18:07:00.277964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 18:07:00.278853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 18:07:00.279623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 18:07:00.280358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 18:07:00.281121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 18:07:00.281858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 18:07:00.282598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 18:07:00.283358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-18 18:07:00.284042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1, 2, 3\r\nNum GPUs Available:  4\r\n```\r\n", "(I just tried recompiling tensorflow with gcc 9.3.0, to match the major version of my Python install, no luck.)", "I wonder if this issue is tutorial specific since you can successfully import tf and it also detects your gpu. Can you please try some [basic tutorial](https://www.tensorflow.org/tutorials/keras/classification) notebook and check if you see core dumped error?", "Good question! This \"Basic classification\" notebook seems to have finished just fine.\r\n\r\nI suppose this may not a build/install issue then.", "Since your current config does not support avx instructions sets it is not optimized for higher versions of TF and perhaps causing the errors you see in executing deepdream tutorial on your local machine.", "Do you believe this to be the case even though I built TensorFlow from source on that very machine?", "If you compile on your own machine AVX issue should not occur, **assuming** you are using the compiled TF and not the one from PyPI. Can you check to make sure that this is the case?", "Yes; the one from PyPi did not even import without crashing. These issues I get using TensorFlow I compiled on the AVX-less machine, following the [official instructions](https://www.tensorflow.org/install/source).", "Right. So now this is a GPU issue. Assigning @sanjoy to take a look", "Can I say \"me too\", in case it helps you with debugging this?  Here's the result of running the two-line script above:\r\n\r\n```\r\n$ python3\r\nPython 3.9.2 (default, Feb 28 2021, 17:03:44) \r\n[GCC 10.2.1 20210110] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2021-03-24 14:44:39.111867: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n>>> print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\r\n2021-03-24 14:44:49.652549: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-03-24 14:44:49.653460: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-03-24 14:44:49.691660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-24 14:44:49.692173: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:41:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.755GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2021-03-24 14:44:49.692196: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-03-24 14:44:49.695547: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-03-24 14:44:49.695655: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-03-24 14:44:49.696506: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-03-24 14:44:49.696911: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-03-24 14:44:49.697576: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\r\n2021-03-24 14:44:49.698125: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-03-24 14:44:49.698215: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-03-24 14:44:49.698351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-24 14:44:49.698969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-24 14:44:49.699433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\nNum GPUs Available:  1\r\n>>> \r\n```\r\n\r\nI have also just built TensorFlow from source (see [my blog post](https://blog.d-and-j.net/deep-learning/2021/03/24/tensorflow-debian-11.html) if you want to see the details of how I did that).\r\n\r\nInterestingly, the result of running `grep . /sys/bus/pci/devices/0000\\:*/numa_node` is lots of `-1`s and one `0`; the only `0` corresponds to the device `09:00.0 Non-Volatile memory controller: Samsung Electronics Co Ltd NVMe SSD Controller SM981/PM981/PM983`, whereas the GPU returns a `-1`.", "One additional piece of information: if I run `echo 0 >> /sys/bus/pci/devices/0000\\:41\\:00.0/numa_node` (which is the GPU device) and then run the above script, the warning disappears:\r\n\r\n```\r\n$ python3\r\nPython 3.9.2 (default, Feb 28 2021, 17:03:44) \r\n[GCC 10.2.1 20210110] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2021-03-24 14:52:46.647000: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n>>> print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\r\n2021-03-24 14:52:49.403932: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-03-24 14:52:49.404942: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-03-24 14:52:49.446001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:41:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.755GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2021-03-24 14:52:49.446040: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-03-24 14:52:49.449441: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-03-24 14:52:49.449552: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-03-24 14:52:49.450441: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-03-24 14:52:49.450859: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-03-24 14:52:49.451547: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\r\n2021-03-24 14:52:49.452122: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-03-24 14:52:49.452212: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-03-24 14:52:49.453396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\nNum GPUs Available:  1\r\n>>> \r\n```", "@tmladek are you able to run TF under GDB?  If yes, can you please attach the stack trace at the point TF crashes?", "I am also seeing this (or a similar issue) on an Ubuntu 20.04 system with an older CPU (Xeon X5675; no AVX) built with CUDA support (Geforce 970 GTX; cuda compute 5.2). I compiled the r2.5 branch and specified `-march=native` and CUDA compute 5.2 during compile. Used the CUDA 11 toolkit.\r\n\r\nCompile and install went OK. I can import _tensorflow_ and basic commands like `tf.config.list_physical_devices` work as expected. However, when I call into `.fit` or `.predict` though, python crashes with an Illegal Instruction exception.\r\n\r\nFWIW I also see this warning: `successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero`. Unclear to me if it is red herring.", "Well, I think I may have answered my own (and possibly the OP's question). I ran my script using _gdb_. Python receives a SIGILL,(illegal instruction) in _libcudnn_cnn_train.so.8_\r\n\r\nDisassembly:\r\n\r\n```\r\n\u2502-->0x7ffd5ebbf2a7      vmovdqa 0x3c1831(%rip),%ymm0        # 0x7ffd5ef80ae0                                                                                                          \r\n\u2502   0x7ffd5ebbf2af      lea    0x53ce33a(%rip),%rax        # 0x7ffd63f8d5f0                                                                                                           \r\n\u2502   0x7ffd5ebbf2b6      lea    0x53ce4cb(%rip),%rdx        # 0x7ffd63f8d788                                                                                                           \r\n\u2502   0x7ffd5ebbf2bd      lea    0x3bdda4(%rip),%rcx        # 0x7ffd5ef7d068                                                                                                            \r\n\u2502   0x7ffd5ebbf2c4      lea    0x542f375(%rip),%rdi        # 0x7ffd63fee640 \r\n```\r\n_vmovdqa_ is an AVX instruction.\r\n\r\nSo it looks like the problem is that CUDNN is compiled using AVX instructions even though Tensorflow is not. When Tensorflow calls into that DLL, it crashes with SIGILL. Now to see if I can find a version of CUDNN without AVX that works with the build...\r\n\r\nJust for the record I compiled with `cudnn-11.2-linux-x64-v8.1.1.33`. I don't see anything in the release notes that explicitly states AVX is required.", "I found an older CUDNN version that doesn't use AVX instructions. You can check by downloading the CUDNN package, disassembling the DLL in question, and grepping for AVX instructions. I used:\r\n\r\n`objdump -d cuda/lib64/libcudnn.so.7 | grep vmovdqa`\r\n\r\nThat didn't turn up any hits on `cudnn-10.1-linux-x64-v7.6.5.32`, while equivalent DLLs in newer packages did.\r\n\r\nI swapped out the toolkits and libraries and tried compiling with r2.5, but it failed... the structure of the CUDNN library changed in more recent versions and tensorflow r2.5 depends on that new layout. Switching to the r2.4 branch worked. However, I also needed to switch to GCC 7 instead of 9.\r\n\r\nFinal configuration that worked (no more SIGILL crash now!):\r\n\r\n`cudnn-10.1-linux-x64-v7.6.5.32`\r\n`cuda_10.1.243_418.87.00_linux`\r\n`GCC 7`\r\n`tensorflow r2.4`\r\n\r\nGoing forward, if all future CUDNN releases require AVX support, then non-AVX computers will be stuck at Tensorflow 2.4.\r\n\r\nHope this is helpful to other.", "> Well, I think I may have answered my own (and possibly the OP's question). I ran my script using _gdb_. Python receives a SIGILL,(illegal instruction) in _libcudnn_cnn_train.so.8_\r\n> \r\n> Disassembly:\r\n> \r\n> ```\r\n> \u2502-->0x7ffd5ebbf2a7      vmovdqa 0x3c1831(%rip),%ymm0        # 0x7ffd5ef80ae0                                                                                                          \r\n> \u2502   0x7ffd5ebbf2af      lea    0x53ce33a(%rip),%rax        # 0x7ffd63f8d5f0                                                                                                           \r\n> \u2502   0x7ffd5ebbf2b6      lea    0x53ce4cb(%rip),%rdx        # 0x7ffd63f8d788                                                                                                           \r\n> \u2502   0x7ffd5ebbf2bd      lea    0x3bdda4(%rip),%rcx        # 0x7ffd5ef7d068                                                                                                            \r\n> \u2502   0x7ffd5ebbf2c4      lea    0x542f375(%rip),%rdi        # 0x7ffd63fee640 \r\n> ```\r\n> \r\n> _vmovdqa_ is an AVX instruction.\r\n> \r\n> So it looks like the problem is that CUDNN is compiled using AVX instructions even though Tensorflow is not. When Tensorflow calls into that DLL, it crashes with SIGILL. Now to see if I can find a version of CUDNN without AVX that works with the build...\r\n> \r\n> Just for the record I compiled with `cudnn-11.2-linux-x64-v8.1.1.33`. I don't see anything in the release notes that explicitly states AVX is required.\r\n\r\nThank you @nicholastoddsmith , I did encounter the same issue as you. I built tensorflow from source (such a pain sometimes :) ) from a workstation with a 1080Ti and a processor without AVX support. I was happy to finish the build after some hours but finally got the \"Illegal Instruction\" when trying to run a tensorflow training (importing tensorflow was not an issue); For the record, I list here slightly more details for the deassemble; \r\n\r\nAs you suggest, running gdb on the program with a backtrace I got :\r\n\r\n```\r\nThread 37 \"python3\" received signal SIGILL, Illegal instruction.\r\n[Switching to Thread 0x7ffeabfff700 (LWP 11822)]\r\n0x00007ffb3abbf2a7 in ?? () from /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8\r\n```\r\nThen, within gdb , I did \r\n\r\n```\r\n(gdb) disassemble 0x00007ffb3abbf2a7, +20\r\n=> 0x00007ffb3abbf2a7:  vmovdqa 0x3c1831(%rip),%ymm0        # 0x7ffb3af80ae0\r\n   0x00007ffb3abbf2af:  lea    0x53ce33a(%rip),%rax        # 0x7ffb3ff8d5f0\r\n   0x00007ffb3abbf2b6:  lea    0x53ce4cb(%rip),%rdx        # 0x7ffb3ff8d788\r\n```\r\n\r\nAnyway thank you for pointing to that . It helped me understanding the issue. One last note, I was using an installation with libcudnn 8.1.1.33-1+cuda11.2   and tensorflow 2.6.2 built successfully from the source.\r\n\r\nAnd I noticed afterwhile the [release notes of cudnn](https://docs.nvidia.com/deeplearning/cudnn/release-notes/rel_8.html) where they mention \r\n\r\n> Starting in version 8.1, cuDNN uses AVX intrinsics on the x86_64 architecture; users of this architecture without support for AVX intrinsics may see illegal instruction errors. \r\n\r\nYes, that's true .\r\n"]}, {"number": 47239, "title": "ParameterServerStrategy supporting GPU workers", "body": "**Describe the feature and the current behavior/state.**\r\nHi TF team,\r\n\r\nwe can train very efficiently our models thanks to the PS strategy using tf 1.15. \r\nWe are moving our codebase to 2.3 but it looks like you don't support GPU workers in the PS strategy right now.\r\nDo you have any (even tentative) dates for shipping that feature?\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit from this feature?**\r\n\r\n**Any Other info.**\r\n", "comments": ["@pawel-polyai,\r\nPlease take a look at the [documentation](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/ParameterServerStrategy) and [tutorial](https://www.tensorflow.org/tutorials/distribute/parameter_server_training) for `ParameterServerStrategy` using TensorFlow 2.x and check if it helps. Thanks!", "Hi @pawel-polyai, GPU support is actively being worked on right now. I unfortunately can't provide a date, but I'll keep this thread updated."]}, {"number": 47224, "title": "Keras cannot restore custom functions for subclassed Model, but for subclassed Layer custom functions can be restored", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n- Python version: 3.8.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nKeras cannot restore custom functions for subclassed Model, even with custom_object arguments; but for subclassed Layer, custom functions can be restored. See two code snippet attached.\r\n\r\n**Describe the expected behavior**\r\nI expect the consistent behavior between subclassed Model and subclassed Layer\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\n import tensorflow as tf\r\n import numpy as np\r\n\r\n class MyModel(tf.keras.models.Model):\r\n\r\n   def __init__(self, **kargs):\r\n     super(MyModel, self).__init__(**kargs)\r\n     self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\r\n     self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\r\n\r\n   def call(self, inputs):\r\n     x = self.dense1(inputs)\r\n     return self.dense2(x)\r\n\r\n   def about(self):\r\n       print(\"123\")\r\n\r\n model = MyModel()\r\n\r\n x = np.random.random((2, 3))\r\n model(x)\r\n\r\n model_file = \"model_file\"\r\n model.save(model_file)\r\n\r\n loaded_model = tf.keras.models.load_model(model_file, custom_objects={\"MyModel\": MyModel})\r\n loaded_model.about()\r\n```\r\n\r\n```\r\n import tensorflow as tf\r\n import numpy as np\r\n\r\n\r\n class MyDense(tf.keras.layers.Dense):\r\n     def __init__(self, units, **kargs):\r\n         super(MyDense, self).__init__(units=units, **kargs)\r\n     def call(self, inputs):\r\n         return super(MyDense, self).call(inputs)\r\n     def my_units(self):\r\n         return self.units\r\n     @classmethod\r\n     def convert_to_p(cls, p_inputs):\r\n         print(\"now converting to p\")\r\n\r\n x = tf.keras.Input(shape=(10,))\r\n dense1 = MyDense(units=20)(x)\r\n y = MyDense(units=30)(dense1)\r\n model = tf.keras.models.Model(inputs=[x], outputs=[y])\r\n # tf.keras.utils.plot_model(model)\r\n outputs = model(np.random.random((2, 10)))\r\n model_path = \"./foo\"\r\n model.save(model_path)\r\n loaded_model = tf.keras.models.load_model(model_path, custom_objects={\"MyDense\": MyDense})\r\n for layer in loaded_model.layers:\r\n     if hasattr(layer, \"my_units\"):\r\n         print(layer.name, layer.my_units())\r\n         layer.convert_to_pynchpin([1])\r\n         print()\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@blackyang \r\n\r\nI tried in colab with TF 2.4 .Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/3393146ba2def5305e8b0aa9b8451a8d/untitled675.ipynb).You are also seeing the same behavior?\r\nThanks!", "@ravikyram the colab exactly showed the issue, calling `loaded_model.about()` will result in an error, but calling `layer.my_units()` is fine", "Was able to reproduce the issue in TF v2.5 ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/f8ecafff555980d70914d32e97e319d9/untitled29.ipynb)..Thanks !"]}, {"number": 47223, "title": "Long prediction time when using tensorflow lite optimize", "body": "I wrote a model based on the VGG16 using mostly TensorFlow documentation. After training, I saved the model into a .h5 file.\r\nWhen I run the prediction, it takes about a second and a half to return the results. As I try to optimize the prediction time using `tensorflow.lite.Optimize.DEFAULT` (code available below) the prediction time increases drastically. I'm talking about almost 600 times slower than the normal code (when I don't use the Optimize). Why is that? Shouldn't optimize suppose to decrease the prediction time instead of increasing it? I used the *default* optimizer, *latency* optimizer, and *size* optimizer without any change in the result.\r\nI used both CPU and GPU versions of TensorFlow without any significant change in the result.\r\n\r\n**My system is:**\r\n-  Intel Core i7 9700H\r\n-  16 Gb DDR4 2300Mhz\r\n-  GTX 1660ti (mobile version)\r\n-  Windows 10 (latest build)\r\n\r\nThe code:\r\n```\r\nimport tensorflow as tf\r\nimport os\r\nimport time\r\nimport numpy as np\r\nfrom tensorflow import keras\r\nimport pathlib\r\n\r\nsaved_model_dir= 'model/'\r\nsaved_modelh5 = 'model.h5'\r\ndataset_path = 'bound box dataset/img'\r\nout_path = 'converted_model.tflite'\r\nnum_calibration_steps = 1\r\n\r\nprint(\"Im doing something\")\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(keras.models.load_model(saved_modelh5))  \r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT  ]\r\ntflite_model = converter.convert()\r\n\r\n\r\n#------------------------------------------------------------\r\n#with open(out_path, \"wb\")as f:\r\nprint('converted')\r\ntflite_model_file = pathlib.Path(out_path)\r\ntflite_model_file.write_bytes(tflite_model)\r\nprint('Saved')\r\n\r\n\r\ninput_data = np.random.rand(1,512,512,3).astype(np.float32)\r\n\r\ninterpreter = tf.lite.Interpreter( model_content = tflite_model)\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\n# Test model on random input data.\r\nt = time.time()\r\ninput_shape = input_details[0]['shape']\r\n#input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\nprint('start invoke')\r\ninterpreter.invoke()\r\n\r\n# The function `get_tensor()` returns a copy of the tensor data.\r\n# Use `tensor()` in order to get a pointer to the tensor.\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\n\r\nt = time.time() - t\r\nprint('predict time:',t)\r\n```\r\n\r\n\r\n\r\n", "comments": ["@ark1375,\r\nOn running the code I'm facing an error stating `OSError: SavedModel file does not exist at: model.h5/{saved_model.pbtxt|saved_model.pb}`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/02ac68a2cc96b459305597c3727bd87f/47223.ipynb).\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the TensorFlow version and all the files required to run the code. Thanks!\r\n", "@amahendrakar\r\nI uploaded the required file [here](https://drive.google.com/file/d/12EmRy4zf3Bpz5rQKLKortPnKYn0KcAlj/view?usp=sharing). Because the model data was too large, I had to upload it to Google Drive. \r\nI ran the model using Python 3.7 and the latest version of TensorFlow that I could install from pip (2.4). ", "@amahendrakar\r\nDid it work? Could you run the code with the file that I uploaded?", "Hi @ark1375,\r\n\r\nThe quantized op kernels are usually optimized for ARM architecture, but not so for x86. TensorFlow Lite model doesn't work with desktop GPUs and even if it's supported, GPUs don't support integer calculations.\r\n\r\nYou can try build your custom tflite runtime with `ruy` enabled, but the inference speed is still not going to be as fast as the float one.", "@teijeong First of all thanks for the answer. Second, you think if I ran the model on a Raspberry pi, then I could see the difference in speed?", "Was able to reproduce the issue with [TF v2.4](https://colab.research.google.com/gist/amahendrakar/4947b00a5b3eb902506eebdf37d4b575/47223.ipynb) and TF-nightly, where prediction time is ~ 260 seconds.\r\n\r\nWhereas with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/34520b50b6ff3f535ce33ecba8876896/47223-2-3.ipynb), the predict time is ~6 seconds. Please check the linked gist for reference. Thanks!", "@ark1375 sorry for late reply. Yes, I think you might observe some sppedup when running on rpi, depending on its hardware capability.\r\n\r\n@terryheo It seems weird to see latency getting higher from ~6s to ~260s. Can you take a look? We need to verify first if they had run on the same machine though.", "Regarding the performance, you'd better specify num_threads since TF 2.4.\r\n\r\n```\r\ninterpreter = tf.lite.Interpreter( model_content = tflite_model, num_threads=16)\r\n```\r\n\r\nIt looks like a regression, anyway it solves your issue.", "Hi,\r\n\r\nThanks for reporting this issue. With recent versions of TensorFlow, the conversion to TF Lite format with the optimization options you mention will use the new \"dynamic range per-channel\" quantization method, whereas previous versions (e.g. v2.3) use the older \"dynamic range per-tensor\" quantization method. For this new method, when running the converted model on x86 you need to build with a more recent version of GCC (9 or higher) or a more recent version of MS Visual C++ `__MSC_VER` 1920 or higher to get optimized code.\r\n\r\nWe are going to land a workaround that will permit running the old path for for models converted in v2.3 (dynamic range per-tensor). Upgrading your compiler version is another way to fix this issue. I will close once we land the workaround. Cheers!"]}, {"number": 47222, "title": "LSTM - Issue with model fitting using ragged tensor", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution : Linux Ubuntu 16.04 / Google Colab\r\n- TensorFlow installed from (source or binary): Google Colab\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: Not used\r\n- GPU model and memory: None\r\n\r\n** Problem Description**\r\n\r\nI'm working on classification of ECG signals into 4 classes. Each sequence to be classified has potentially different lengths. Sequences are stored into a ragged tensor. \r\n\r\nThe model is based on LSTM layers. Unfortunatly when I try to fit the model using the function \"fit\" there is a problem about matrix size incompatibility:\r\n\r\n```\r\nInvalidArgumentError:  Matrix size-incompatible: In[0]: [32,18000], In[1]: [18286,256]\r\n\t [[node sequential/LSTM_1/lstm_cell/MatMul (defined at <ipython-input-7-36997f53d4b2>:19) ]] [Op:__inference_train_function_6985]\r\n```\r\nNevertheless, when I use a subset of this exact dataframe (25 sequence of 8500+), the training is going well to the end. \r\n\r\nI have verified, there are no Nan of None in my sequences. \r\n\r\n**Describe the current behavior**\r\n\r\nThe training of the model returns an error concerning matrix size incompatibility. The training can be performed when using a subset of the dataset.\r\n\r\n**Describe the expected behavior**\r\n\r\nModel can be trained well with the entire dataset\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n_Notebook link :_  https://colab.research.google.com/drive/1RdtafxvD_5nqh9-xuG6o8WL4GOd9IZnN?authuser=1#scrollTo=Dzq3tRK9cwbe\r\n\r\n_Data link :_ https://drive.google.com/file/d/1V9nNxMehtiwAjNVS-8usTP6Jol9jC6et/view?usp=sharing\r\n\r\n**Other info / logs** \r\n\r\n```\r\nEpoch 1/10\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-7-36997f53d4b2> in <module>()\r\n     17 model.summary()\r\n     18 \r\n---> 19 model.fit(xx,yy, epochs=10) # --> Doesn t working with the full dataset, matrix size error\r\n\r\n6 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nInvalidArgumentError:  Matrix size-incompatible: In[0]: [32,18000], In[1]: [18286,256]\r\n\t [[node sequential/LSTM_1/lstm_cell/MatMul (defined at <ipython-input-7-36997f53d4b2>:19) ]] [Op:__inference_train_function_6985]\r\n\r\nFunction call stack:\r\ntrain_function\r\n```\r\nThanks in advance \r\n\r\n\r\n", "comments": ["@sebgra \r\nI am unable to access the notebook shared, please refer to [this link](https://stackoverflow.com/questions/50158475/invalidargumenterror-matrix-size-incompatible-in0-256-2048-in1-256-1) meanwhile and let us know if it helps.", "@Saduf2019 \r\n[cardiology_challenge_2017.zip](https://github.com/tensorflow/tensorflow/files/6001993/cardiology_challenge_2017.zip)\r\n Here's the .py code\r\n```\r\nOriginal file is located at\r\n    https://colab.research.google.com/drive/1RdtafxvD_5nqh9-xuG6o8WL4GOd9IZnN\r\n\"\"\"\r\n\r\nfrom google.colab import drive\r\ndrive.mount(\"/content/drive\")\r\n\r\n\"\"\"# Importing Libraries\"\"\"\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport keras\r\nimport pandas as pd\r\nimport plotly.express as px\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn.model_selection import train_test_split\r\n\r\n\r\n\"\"\"# Reading Database\"\"\"\r\n\r\ndf = pd.read_pickle('/content/drive/MyDrive/Cardiology/Data_Set/Cardiology_Dataset') # Full dataset : Cardiology_Dataset file \r\n#--> https://drive.google.com/file/d/1V9nNxMehtiwAjNVS-8usTP6Jol9jC6et/view?usp=sharing\r\ndf\r\n\r\n\"\"\"# Implementing Model\"\"\"\r\n\r\nxx = tf.ragged.constant(df['Sequence'])\r\n\r\nyy = tf.one_hot(df['Label'],4)\r\nxx.bounding_shape()[-1]\r\n\r\n#max_seq = X_train.bounding_shape()[-1]\r\nmax_seq = xx.bounding_shape()[-1]\r\nprint(f'Maximum length sequence : {max_seq}')\r\n\r\nmodel = keras.Sequential([\r\n                          tf.keras.layers.Input(shape=[None, max_seq], dtype=tf.float32, ragged=True),\r\n                          keras.layers.LSTM(units=64, activation='tanh', dropout=0.2, name = 'LSTM_1', return_sequences=True),\r\n                          keras.layers.LSTM(units=256, activation='tanh', dropout=0.2, name = 'LSTM_2',return_sequences=True),\r\n                          keras.layers.LSTM(units=100, activation='tanh', dropout=0.2, name = 'LSTM_3', return_sequences=False),\r\n                          tf.keras.layers.Dense(units=4, activation='sigmoid', name = 'Dense_1')\r\n])\r\n\r\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\r\n\r\nmodel.summary()\r\n\r\nmodel.fit(xx,yy, epochs=10) # --> Doesn t working with the full dataset, matrix size error\r\n\r\n\"\"\"# Subset of the original dataset --> Working\"\"\"\r\n\r\nsub_df = df.head(25)\r\n\r\nxx_sub = tf.ragged.constant(sub_df['Sequence'])\r\n\r\nyy_sub = tf.one_hot(sub_df['Label'],4)\r\n\r\n\r\nmax_seq = xx_sub.bounding_shape()[-1]\r\n\r\nmodel = keras.Sequential([\r\n                          tf.keras.layers.Input(shape=[None, max_seq], dtype=tf.float32, ragged=True),\r\n                          keras.layers.LSTM(units=64, activation='tanh', dropout=0.2, name = 'LSTM_1', return_sequences=True),\r\n                          keras.layers.LSTM(units=256, activation='tanh', dropout=0.2, name = 'LSTM_2',return_sequences=True),\r\n                          keras.layers.LSTM(units=100, activation='tanh', dropout=0.2, name = 'LSTM_3', return_sequences=False),\r\n                          # tf.keras.layers.LSTM(4),\r\n                          #tf.keras.layers.Flatten(name = 'Flatten'),\r\n                          tf.keras.layers.Dense(units=4, activation='sigmoid', name = 'Dense_1')\r\n])\r\n\r\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\r\n\r\nmodel.summary()\r\n\r\nmodel.fit(xx_sub,yy_sub, epochs=10, shuffle=False) # --> No problem with a subset of the original dataset, why ?\r\n```\r\n\r\n", "@Saduf2019 , @sebgra , Even though I didn't run your code, I think that your **Input Layer's** `shape` declaration is causing the error. You declared the shape to **[None, max_seq]** , but it would only be compatible for tensors having that exact shape. Since you used **ragged tensors** and since the shape of all the sequences are not same i.e. not equal to [None, max_seq] , the code is running into the error. Try changing the **Input Layer's** shape to `[None, None]` and check if it works.", "@around-star This modification returns an error concerning the dense layer : \r\n\r\n`TypeError: int() argument must be a string, a bytes-like object or a number, not 'NoneType'`", "@sebgra i see . Thats one contraint with `Dense` layers. The feature of the data along the axis which they perform the dot product , needs to be of constant len/size. I would suggest to try padding along the last axis or make some changes in the network so as not to include any `Dense` layer.", "@around-star I have made a version of this network with padding. But I am still confused about the performance of the network with zero padding which seems to descrease the performances.", "@sebgra \r\nI ran the code shared and face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/b880db2093b941f34036930007bcf804/untitled549.ipynb).\r\n", "@Saduf2019 That is because you're trying to read a .py file, which is a script and not the data. \r\nTry to replace  :\r\n\r\n`df = pd.read_pickle('/content/cardiology_challenge_2017.py') `\r\n\r\nBy : \r\n\r\n`df = pd.read_pickle('/content/drive/MyDrive/Cardiology/Data_Set/Cardiology_Dataset')`\r\n\r\nWhich should correspond to the path of the data available at : \r\n\r\n[https://drive.google.com/file/d/1V9nNxMehtiwAjNVS-8usTP6Jol9jC6et/view?usp=sharing](url)", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210603, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/4695fd45e14c353b84515c11bf87840b/47222.ipynb). Thanks!", "> **System information**\r\n> \r\n>     * Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n> \r\n>     * OS Platform and Distribution : Linux Ubuntu 16.04 / Google Colab\r\n> \r\n>     * TensorFlow installed from (source or binary): Google Colab\r\n> \r\n>     * TensorFlow version (use command below): 2.4.1\r\n> \r\n>     * Python version: 3.6.9\r\n> \r\n>     * CUDA/cuDNN version: Not used\r\n> \r\n>     * GPU model and memory: None\r\n> \r\n> \r\n> ** Problem Description**\r\n> \r\n> I'm working on classification of ECG signals into 4 classes. Each sequence to be classified has potentially different lengths. Sequences are stored into a ragged tensor.\r\n> \r\n> The model is based on LSTM layers. Unfortunatly when I try to fit the model using the function \"fit\" there is a problem about matrix size incompatibility:\r\n> \r\n> ```\r\n> InvalidArgumentError:  Matrix size-incompatible: In[0]: [32,18000], In[1]: [18286,256]\r\n> \t [[node sequential/LSTM_1/lstm_cell/MatMul (defined at <ipython-input-7-36997f53d4b2>:19) ]] [Op:__inference_train_function_6985]\r\n> ```\r\n> \r\n> Nevertheless, when I use a subset of this exact dataframe (25 sequence of 8500+), the training is going well to the end.\r\n> \r\n> I have verified, there are no Nan of None in my sequences.\r\n> \r\n> **Describe the current behavior**\r\n> \r\n> The training of the model returns an error concerning matrix size incompatibility. The training can be performed when using a subset of the dataset.\r\n> \r\n> **Describe the expected behavior**\r\n> \r\n> Model can be trained well with the entire dataset\r\n> \r\n> **Standalone code to reproduce the issue**\r\n> \r\n> _Notebook link :_ https://colab.research.google.com/drive/1RdtafxvD_5nqh9-xuG6o8WL4GOd9IZnN?authuser=1#scrollTo=Dzq3tRK9cwbe\r\n> \r\n> _Data link :_ https://drive.google.com/file/d/1V9nNxMehtiwAjNVS-8usTP6Jol9jC6et/view?usp=sharing\r\n> \r\n> **Other info / logs**\r\n> \r\n> ```\r\n> Epoch 1/10\r\n> ---------------------------------------------------------------------------\r\n> InvalidArgumentError                      Traceback (most recent call last)\r\n> <ipython-input-7-36997f53d4b2> in <module>()\r\n>      17 model.summary()\r\n>      18 \r\n> ---> 19 model.fit(xx,yy, epochs=10) # --> Doesn t working with the full dataset, matrix size error\r\n> \r\n> 6 frames\r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n>      58     ctx.ensure_initialized()\r\n>      59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n> ---> 60                                         inputs, attrs, num_outputs)\r\n>      61   except core._NotOkStatusException as e:\r\n>      62     if name is not None:\r\n> \r\n> InvalidArgumentError:  Matrix size-incompatible: In[0]: [32,18000], In[1]: [18286,256]\r\n> \t [[node sequential/LSTM_1/lstm_cell/MatMul (defined at <ipython-input-7-36997f53d4b2>:19) ]] [Op:__inference_train_function_6985]\r\n> \r\n> Function call stack:\r\n> train_function\r\n> ```\r\n> \r\n> Thanks in advance\r\n\r\ndid you solve it ? \r\n", "This hasn't be solved yet. The only solution that I've found is to pad sequence with zeros to get series of the same time. Indeed, the power of the model is unfortunatly decreasing.", "sorry for my late reply . yes, this is my problem too .. can i use reshape ?", "Only zero padding help me to make it works but not as efficient as it could. I'm still searching for a slolution", "@around-star \r\n> i see . Thats one contraint with `Dense` layers. The feature of the data along the axis which they perform the dot product , needs to be of constant len/size. I would suggest to try padding along the last axis or make some changes in the network so as not to include any `Dense` layer.\r\n\r\nCould you elaborate on this please? I don't get it why the Dense layer can't infer the shape in that network. As I understand even if input was [None, None], shape before Dense would be [None, 100] (None for batch) as we don't return sequences in the last LSTM, but only a final state.\r\n\r\n@sebgra Tried to reproduce the code and had the same problem. However, Embedding layer helps and I think it might be crucial there:\r\n\r\n```\r\nx1 = tf.ragged.constant([[1, 2, 3, 4, 5], [8, 9], [10, 11, 12, 13], [1, 2, 3]])\r\ny1 = tf.constant([[0, 1, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [1, 0, 0, 0],])\r\nmodel1 = tf.keras.Sequential([\r\n                          tf.keras.layers.Input(shape=[None], dtype=tf.int32, ragged=True),\r\n                          tf.keras.layers.Embedding(15, 4),\r\n                          tf.keras.layers.LSTM(units=64, activation='tanh', dropout=0.2, name = 'LSTM_1', return_sequences=True),\r\n                          tf.keras.layers.LSTM(units=256, activation='tanh', dropout=0.2, name = 'LSTM_2',return_sequences=True),\r\n                          tf.keras.layers.LSTM(units=100, activation='tanh', dropout=0.2, name = 'LSTM_3', return_sequences=False),\r\n                          tf.keras.layers.Dense(units=4, activation='sigmoid', name = 'Dense_1')\r\n])\r\n\r\n\r\nmodel1.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=1e-3),\r\n          loss='categorical_crossentropy',\r\n          metrics=['accuracy'])\r\nmodel1.summary()\r\nmodel1.fit(x1, y1, batch_size=2)\r\n```\r\n\r\nDocs: https://www.tensorflow.org/guide/ragged_tensor#keras\r\n\r\nHow I see it, just my understanding not more. Without embedding you need to feed x1 represented as [batch_size, time_series, features], e.g. `[[[1], [2], [3], [4], [5]], [[8], [9]], [[10], [11], [12], [13]], [[1], [2], [3]]]`\r\nx1.shape is [4, None, None]\r\nLSTM just doesn't know how to connect unknown number of inputs to 64 neurons.\r\nIf you have an Embedding layer, you don't have this problem as there is a fixed shape in the last dimension.\r\n", "@mathshangw , [I played a bit with a notebook you have shared](https://colab.research.google.com/drive/1VbTFVBWacHJ0zTNqXCGUvOgfhP3rRH25?usp=sharing) and found, \r\n\r\n1. **Dimensions are mixed up**:  in`tf.keras.layers.Input(shape=[None,max_seq],` we want `max_seq` to be a sequence len, but instead it defines the features vector size of a timestep, see the `tf.print` output from `call`:  `[32 1 18000]`\r\n \r\n 2. **Keras somehow failed in getting the bounding shape of the ragged tensor**, it used `18000` instead of `18170`. However, **it doesn't seem like a Keras bug to me, because no one wants to train LSTM with a variable size feature vector**. \r\n\r\n3. The solution is mentioned in @GlaIZier [comment](https://github.com/tensorflow/tensorflow/issues/47222#issuecomment-1033766102): \"Embedding layer helps\" to add the feature vector dimension, it's size it just a constant\r\n\r\nIt sounds like this issue can be closed.", "@vitaly-d I'm still trying to make @GlaIZier 's answer to work, but I'm facing a new error which is :\r\n\r\n`InvalidArgumentError:  indices[32] = -9 is not in [0, 18287)\r\n\t [[node sequential_18/embedding_15/embedding_lookup_ragged/embedding_lookup\r\n (defined at /usr/local/lib/python3.7/dist-packages/keras/layers/embeddings.py:191)\r\n]] [Op:__inference_train_function_219565]\r\n`\r\n\r\nThe corrected code is : \r\n\r\n```\r\nmodel = keras.Sequential([\r\n                          tf.keras.layers.Input(shape=[None], dtype=tf.float32, ragged=True),\r\n                          tf.keras.layers.Embedding(18286, 4),\r\n                          keras.layers.LSTM(units=64, activation='tanh', dropout=0.2, name = 'LSTM_1', return_sequences=True),\r\n                          keras.layers.LSTM(units=256, activation='tanh', dropout=0.2, name = 'LSTM_2',return_sequences=True),\r\n                          keras.layers.LSTM(units=100, activation='tanh', dropout=0.2, name = 'LSTM_3', return_sequences=False),\r\n                          # tf.keras.layers.LSTM(4),\r\n                          #tf.keras.layers.Flatten(name = 'Flatten'),\r\n                          tf.keras.layers.Dense(units=4, activation='sigmoid', name = 'Dense_1')\r\n])\r\n\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\r\n\r\nmodel.summary()\r\n\r\nmodel.fit(tf_X_train, y, epochs=20, batch_size=2)\r\n```\r\n\r\n\r\nWhere y is the one hot encoded on 4 classes of my corresponding sequences, and 18286 the result of my tf_Xtrain.bounding_shape()[1]\r\n\r\nWhat would be a good reproducible solution ?\r\n\r\nThanks in advance", "As @vitaly-d mentioned you have problems with your data.  You need to preprocess it properly.\r\nLSTM expects data with [batch, time_step, features] shape. Your shape is [batch_size, 1, time_steps]. \r\nEmbedding expects data in [batch, time_step] (taking into account that your feature length is 1 like for strings/words) and makes it [batch, time_step, feature/embedded vector], which is fully compatible with LSTM. Furthermore, my guess is that Embedding layer doesn't support negative numbers, which can be found in your sequences\r\n\r\nSo, \r\nwithout Embedding you input should be smth like: [[[128], [157], [189], [226]], [[123], [232]]]. It might not work though as I mentioned in my previous post, thus padding to the same length might be necessary.\r\nwith Embedding: [[128, 157, 189], [123, 232]] without negative numbers", "@sebgra , here is a [notebook](https://colab.research.google.com/drive/1oEN-NoLrsq0Cv3F49Bukk6uFsebotWA8?usp=sharing) where I have been trying to answer your question\ud83d\ude0a \r\n\r\nUnfortunately, I did not get good enough validation accuracy.\r\n\r\nBut you can find in the notebook: \r\n - how to convert integers into float vectors\r\n - a couple of working examples: for RNN and CNN\r\n\r\nGPU Colab kernel has been used. \r\n\r\nP.S. Long sequences makes your task much more challenging. RNNs just do not work well with sequences with 20_000 timesteps length due to performance issues, gradient exploding/vanishing, etc If you don't need to exploit long-term dependencies, where, for example, timestep 18000 depends on timestep 10, it well could be you don't need long sequences at all.  I would suggest to split up sequences into shorter chunks... \r\n\r\nUPD: The \"Sliding CNN sensor\" architecture with Embedding layer initialised with 'positional encodings' seems to start learning something, see the last experiment, that also confirms that local features seems more important for this data than long-term dependencies\r\n"]}, {"number": 47213, "title": "Tensorflow Lite QAT: Double QUANT/DEQUANT in half-quantized and failure in full-quantized model conversion", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04.1-Ubuntu\r\n- TensorFlow installation (pip package or built from source): pip install tensorflow==2.3.0-gpu\r\n- TensorFlow library (version, if pip package or github SHA, if built from source):\r\n\r\n### 2. Code\r\n\r\nimport tensorflow as tf\r\n\r\nimport tensorflow_model_optimization as tfmot\r\n\r\nfrom constants import *\r\nfrom dataset_loader import DatasetLoader\r\n\r\nimport cv2\r\nfrom constants import *\r\nimport numpy as np\r\n\r\nimport numpy as np\r\nimport pathlib\r\nfrom imutils import paths\r\nimport argparse\r\n\r\nif __name__ == '__main__':\r\n\r\n    ap = argparse.ArgumentParser()\r\n    ap.add_argument(\"-m\", \"--model\", type=str, default=\"none\", help=\"name of pre-trained network to use\")\r\n    ap.add_argument(\"-q\", \"--qat\", type=int, default=1, help=\"use qat\")\r\n    args = vars(ap.parse_args())\r\n\r\n    #\r\n    # LOAD MODEL\r\n    #\r\n    print(\"[INFO] load model from file... {}\".format(args[\"model\"]))\r\n    model = tf.keras.models.load_model(args[\"model\"])\r\n\r\n    # LOAD DATASET\r\n    dl = DatasetLoader()\r\n    dl.load()\r\n    trainX = dl.images\r\n    valX   = dl.images_test\r\n    trainY = dl.labels\r\n    valY   = dl.labels_test\r\n\r\n    # .npy data are UINT8_T, SIZE_FACExSIZE_FACE, convert to float\r\n    trainX = trainX.astype(\"float\")\r\n    valX = valX.astype(\"float\")\r\n\r\n    #\r\n    # QAT\r\n    #\r\n    if(args[\"qat\"]):\r\n        print(\"[INFO] build QAT model\")\r\n\r\n        quantize_model = tfmot.quantization.keras.quantize_model\r\n\r\n        q_aware_model = quantize_model(model)\r\n\r\n        q_aware_model.compile(optimizer='adam',\r\n                  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\r\n                  metrics=['accuracy'])\r\n\r\n        q_aware_model.fit( trainX,\r\n                           trainY,\r\n                           validation_data=(valX, valY),\r\n                           shuffle=True,                       \r\n                           batch_size=256, \r\n                           epochs=1, \r\n                           validation_split=0.1)\r\n\r\n        baseline_model_accuracy = model.evaluate( valX, valY, verbose=0)\r\n        q_aware_model_accuracy = q_aware_model.evaluate(valX,valY, verbose=0)\r\n\r\n        _, baseline_model_accuracy = model.evaluate(\r\n            valX, valY, verbose=0)\r\n\r\n        _, q_aware_model_accuracy = q_aware_model.evaluate(\r\n            valX, valY, verbose=0)\r\n\r\n        print(baseline_model_accuracy )\r\n        print(q_aware_model_accuracy)\r\n\r\n\r\n    if(args[\"qat\"]):\r\n        print(\"\\n[INFO] set converter...\")\r\n        my_model=q_aware_model\r\n    else:\r\n        print(\"\\n[INFO] set converter...\")\r\n        my_model = model\r\n\r\n    converter = tf.lite.TFLiteConverter.from_keras_model( my_model ) \r\n\r\n\r\n    print(\"\\n[INFO] convert to TFLITE/FLOAT32\")\r\n    tflite_model = converter.convert()\r\n\r\n    #\r\n    # STORE: tflite model where all the parameters are FLOAT32\r\n    #\r\n    tflite_models_dir = pathlib.Path(\"./output\")\r\n    tflite_models_dir.mkdir(exist_ok=True, parents=True)\r\n\r\n    tflite_model_file = tflite_models_dir/\"fer_model_float.tflite\"\r\n    print(\"[INFO] writing TFLITE/FLOAT32    : {}\".format(tflite_model_file))\r\n    tflite_model_file.write_bytes(tflite_model)\r\n\r\n\r\n    #\r\n    # STORE: tflite model where all the parameters are UINT8\r\n    #\r\n    converter2 = tf.lite.TFLiteConverter.from_keras_model(my_model ) \r\n    \r\n    representative_ds = tf.data.Dataset.from_tensor_slices((trainX)).batch(32)\r\n\r\n    def representative_data_float_gen():\r\n      for input_value in representative_ds.take(100):\r\n        yield [input_value]\r\n\r\n    # convert again, in/out & parameters are now UINT8\r\n    converter2.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter2.representative_dataset = representative_data_float_gen\r\n\r\n    print(\"\\n[INFO] convert to TFLITE/HALF-QUANT\")\r\n    tflite_model_quant      = converter2.convert()\r\n    tflite_model_quant_file = tflite_models_dir/\"fer_model_half-quant.tflite\"\r\n    print(\"[INFO] writing TFLITE/HALF-QUANT : {}\".format(tflite_model_quant_file))\r\n    tflite_model_quant_file.write_bytes(tflite_model_quant)\r\n\r\n\r\n    #\r\n    # STORE: tflite model where all the parameters are still using float32\r\n    #\r\n    converter3 = tf.lite.TFLiteConverter.from_keras_model(my_model ) \r\n    converter3.representative_dataset      = representative_data_float_gen\r\n    converter3.optimizations               = [tf.lite.Optimize.DEFAULT]\r\n    converter3.target_spec.supported_ops   = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n    converter3.target_spec.supported_types = [tf.int8]    \r\n    converter3.inference_input_type        = tf.uint8\r\n    converter3.inference_output_type       = tf.uint8\r\n\r\n    print(\"\\n[INFO] convert to TFLITE/FULL-QUANT\")\r\n    tflite_model_quant_io = converter3.convert()\r\n    tflite_model_quant_io_file = tflite_models_dir/\"fer_model_full-quant.tflite\"\r\n    print(\"[INFO] writing TFLITE/FULL-QUANT : {}\".format(tflite_model_quant_io_file))\r\n    tflite_model_quant_io_file.write_bytes(tflite_model_quant_io)\r\n\r\n\r\n    # END\r\n    print(\"\\nbye.\\n\\n\")\r\n\r\nThe keras model is very simple:\r\n\r\nclass SimpleNet:\r\n    @staticmethod\r\n    def build(width, height, depth, classes, stages, filters, reg=0.0001, bnEps=2e-5, bnMom=0.9):\r\n\r\n        inputShape = (height, width, depth)\r\n        chanDim = -1\r\n\r\n        if K.image_data_format() == \"channels_first\":\r\n            inputShape = (depth, height, width)\r\n            chanDim = 1\r\n\r\n\r\n        model = tf.keras.Sequential()\r\n\r\n        model.add(Conv2D(filters=64, kernel_size=(5, 5), activation='relu', input_shape=inputShape ))\r\n        model.add(MaxPooling2D((3, 3), strides=(2, 2)))\r\n        model.add(Conv2D(filters=64, kernel_size=(5, 5), activation='relu'))\r\n        model.add(MaxPooling2D((3, 3), strides=(2, 2)))\r\n        model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\r\n    \r\n        model.add(Flatten())\r\n        model.add(Dropout(0.3))\r\n        model.add(Dense(512, kernel_regularizer=l2(reg)))\r\n        model.add(Dropout(0.3))\r\n        model.add(Dense(256, kernel_regularizer=l2(reg)))\r\n        model.add(Dropout(0.3))\r\n        model.add(Dense(classes, activation=\"softmax\"))        \r\n\r\n        return model\r\n\r\n### 3. Failure after conversion\r\nIf the conversion is successful, but the generated model is wrong, then state what is wrong:\r\nI'm training the keras model and the converting to tflite using QAT as per official documentation.\r\n\r\nConversion to half-quantization is successful but the .tflite model has n.2 QUANTIZE at the input and n.2 DEQUANTIZE at the output.\r\n\r\nConversion to full-quantized model fails to convert but I cannot detect what op is not supported, here is my console log\r\n\r\n[INFO] load model from file... ./output/fer_model_float.hdf5\r\n[INFO] build QAT model\r\n69/69 [==============================] - 3s 43ms/step - loss: 1.8564 - accuracy: 0.6415 - val_loss: 1.7545 - val_accuracy: 0.6627\r\n0.6648070812225342\r\n0.6155276894569397\r\n\r\n[INFO] set converter...\r\n\r\n[INFO] convert to TFLITE/FLOAT32\r\n[INFO] writing TFLITE/FLOAT32    : output/fer_model_float.tflite\r\n\r\n[INFO] convert to TFLITE/HALF-QUANT\r\n[INFO] writing TFLITE/HALF-QUANT : output/fer_model_half-quant.tflite\r\n\r\n[INFO] convert to TFLITE/FULL-QUANT\r\nTraceback (most recent call last):\r\n  File \"./tflite.py\", line 217, in <module>\r\n    tflite_model_quant_io_file.write_bytes(tflite_model_quant_io)\r\n  File \"/home/gfilippi/.virtualenvs/tf23/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 831, in convert\r\n    self).convert(graph_def, input_tensors, output_tensors)\r\n  File \"/home/gfilippi/.virtualenvs/tf23/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 638, in convert\r\n    result = self._calibrate_quantize_model(result, **flags)\r\n  File \"/home/gfilippi/.virtualenvs/tf23/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 452, in _calibrate_quantize_model\r\n    inference_output_type, allow_float, activations_type)\r\n  File \"/home/gfilippi/.virtualenvs/tf23/lib/python3.6/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 98, in calibrate_and_quantize\r\n    np.dtype(activations_type.as_numpy_dtype()).num)\r\nRuntimeError: Quantization not yet supported for op:\r\n\r\n\r\n### 5. (optional) Any other info / logs\r\nthis seems similar to the problem documented here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/41308\r\n\r\n![double_q-deq](https://user-images.githubusercontent.com/1153348/108218102-99f2db00-7134-11eb-827a-26a09017a89f.png)\r\n", "comments": ["adding one more info: if I use TF2.4 for conversion I still have the same problem (double quant/dequant on the half-quantized model) and the full-quantized seems to have a different failure related to the output type ... but I don't know how to set this properly other than the code I wrote as per official documentation on QAT\r\n\r\n[INFO] convert to TFLITE/FULL-QUANT\r\nTraceback (most recent call last):\r\n  File \"./tflite.py\", line 214, in <module>\r\n    tflite_model_quant_io = converter3.convert()\r\n  File \"/home/gfilippi/.virtualenvs/tf24/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 874, in convert\r\n    self).convert(graph_def, input_tensors, output_tensors)\r\n  File \"/home/gfilippi/.virtualenvs/tf24/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 637, in convert\r\n    result = _modify_model_io_type(result, **flags_modify_model_io_type)\r\n  File \"/home/gfilippi/.virtualenvs/tf24/lib/python3.6/site-packages/tensorflow/lite/python/util.py\", line 843, in modify_model_io_type\r\n    _modify_model_output_type(model_object, inference_output_type)\r\n  File \"/home/gfilippi/.virtualenvs/tf24/lib/python3.6/site-packages/tensorflow/lite/python/util.py\", line 753, in _modify_model_output_type\r\n    _get_tf_type_name(quant_type)))\r\nValueError: Initial model output is not dequantized. Expected type for tensor with name 'b'sequential/quant_dense_2/Softmax_float'' should be in ('tf.int8', 'tf.int16'), instead type is tf.float32\r\n", "Hi, with tf-nightly, would you try setting `experimental_new_quantizer` and see if there's difference?\r\n\r\n```python\r\nconverter.experimental_new_quantizer = True\r\n```"]}, {"number": 47203, "title": "Training with MultiWorkerMirroredStrategy for machines with heterogeneous number of GPUs", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 20.04**\r\n- TensorFlow installed from (source or binary): **From docker image tensorflow:latest-gpu**\r\n- TensorFlow version (use command below): **2.4.1**\r\n- Python version: **3.6.9**\r\n- CUDA/cuDNN version: **CUDA 11.2**\r\n- GPU model and memory: **chief (2x Quadro P2000), worker (4x RTX 2080)** \r\n\r\n**Describe the current behavior**\r\nWith the [keras tutorial](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras), I made the relevant changes to my own model. \r\n\r\nI encounter the following error when the worker runs the main.py and starts the training:\r\n\r\n> tensorflow.python.framework.errors_impl.InternalError: Collective Op has group_size 8 and group_key 1 but that group has size 4\r\n> Encountered` when executing an operation using EagerExecutor. This error cancels all future operations and poisons their output tensors.\r\n\r\nHowever, the code runs the training fine when I limit the number of CUDA visible GPUs for each machine to 2 (such that worker and chief have the same number of GPUs).\r\n\r\n**Describe the expected behavior**\r\nI want to be able to utilise all 6 GPUs for training.\r\n\r\n**Standalone code to reproduce the issue**\r\ndocker command for chief and worker (change index to 1 for worker):\r\n```\r\ndocker run --gpus all -it -p 12345:12345 -v /home:/home tensorflow/tensorflow:latest-gpu\r\nexport TF_CONFIG='{\"cluster\": {\"worker\": [\"x.x.x.x:12345\", \"x.x.x.x:12345\"]}, \"task\": {\"index\": 0, \"type\": \"worker\"}}'\r\npython main.py\r\n```\r\n\r\n**Other info / logs** ", "comments": ["Hi @unfazing, just my thoughts on this: in `MultiWorkerMirroredStrategy` the forwards and backwards passes are computed on each replica and the gradients synchronized across all GPUs on a machine and all machines in the cluster.  So if you have a machine that has 4 GPUs, it's going to complete the forward/backwards passes on the data in a particular step faster than the machine with 2 GPUs (assuming your data is evenly distributed across the machines). Thus, the machine with 4 GPUs will be sitting around waiting for the 2 GPU machine before it can start the next step and you won't actually get a performance improvement.\r\n\r\nThere's probably a way to make this scenario work by sending more data to the machine with 4 GPUs, but I'm not entirely sure how that functionality would work. I'll mark this as a feature request for now.", "Okay, so the error is by design. In the situation that 4 weaker GPUs end up having the same computational power as 2 more powerful GPUs, is there a way to run the training without encountering the error?"]}, {"number": 47192, "title": "Bidirection Masked LSTM breaks in graph mode.", "body": "tf version: 2.4.1\r\npython: 3.6\r\n\r\nUsing the Bidirectional layer is returning the following error in graph mode\r\n```\r\nValueError: Shape must be rank 1 but is rank 2 for '{{node cond/ReverseSequence}} = ReverseSequence[T=DT_FLOAT, Tlen=DT_INT32, batch_dim=0, seq_dim=1](cond/ReverseSequence/inputs, cond/Sum)' with input shapes: [?,?,1], [?,1].\r\n```\r\nTo reproduce:\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\nclass Bad(tf.keras.models.Model):\r\n    def __init__(self):\r\n        super(Bad, self).__init__()\r\n        # self.rec_net = tf.keras.layers.LSTM(10)\r\n        self.rec_net = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(10))\r\n    \r\n    @tf.function(input_signature=\r\n                 [tf.TensorSpec(shape=(None, None, 1), dtype=tf.float32),\r\n                  tf.TensorSpec(shape=(None, None, 1), dtype=tf.int32), ])\r\n    def call(self, x, mask):\r\n        return self.rec_net(x, mask=tf.cast(mask, tf.bool), training=False)\r\n        # return self.rec_net(x, training=False)\r\n\r\n\r\nif __name__ == '__main__':\r\n    inp = tf.random.uniform((3, 4, 1))\r\n    mask = tf.convert_to_tensor([[[1], [1], [1], [1]],\r\n                                 [[1], [1], [1], [0]],\r\n                                 [[1], [1], [0], [0]]])\r\n    \r\n    b = Bad()\r\n    out = b(inp, mask)\r\n```\r\n\r\nIt works if any of tf.function, bidirectional or mask is removed.\r\nAlso works if a GRU is used.", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/b54b47a85946b967a2c5dd5388ed2886/47192.ipynb). Thanks!", "Was able to reproduce the issue with TF v2.5 .Please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/293a6df1b5e439ec710ec76370eae413/untitled124.ipynb?authuser=1) ..Thanks !"]}, {"number": 47186, "title": "Add tf.nn.log_gaussian_loss", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.4.1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nImplement the Gaussian negative log-likelihood loss for the tf.nn class, similar to the [log poisson loss](https://www.tensorflow.org/api_docs/python/tf/nn/log_poisson_loss). See also the [PyTorch version](https://pytorch.org/docs/master/generated/torch.nn.GaussianNLLLoss.html). \r\n\r\n**Will this change the current api? How?**\r\ntf.nn class will have the log_gaussian_loss function, which could look like:\r\n`tf.nn.log_gaussian_loss(\r\n    targets, inputs, variances, compute_full_loss=False, name=None\r\n)`\r\nto keep in line with the `log_poisson_loss` implementation.\r\n\r\n**Who will benefit with this feature?**\r\nThis loss function is used extensively in Bayesian deep ensembles, see for example [Lakshminarayanan et al. 2017](https://scholar.google.co.uk/citations?user=QYn8RbgAAAAJ&hl=en&oi=sra#d=gs_md_cita-d&u=%2Fcitations%3Fview_op%3Dview_citation%26hl%3Den%26user%3DQYn8RbgAAAAJ%26citation_for_view%3DQYn8RbgAAAAJ%3AhCrLmN-GePgC%26tzom%3D0), though the loss function has been around since the 90s (Mean and Variance Estimation, [Nix et al. 1994](https://ieeexplore.ieee.org/document/374138)).\r\n\r\n**Any Other info.**\r\n", "comments": ["Hi @rmothukuru, any update on this? Thanks", "@nailimixaM , @rmothukuru if I am not wrong, isn't the one you are looking for is [sigmoid_cross_entropy_with_logits](https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/ops/nn_impl.py#L115-L191)", "Hi @around-star, I'm afraid that's not it, this loss function is for regression, not classification "]}, {"number": 47179, "title": "unique operation on strings returns bogus indices", "body": "**System information**\r\n- Have I written custom code: No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7.9\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.6.8.\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 8.3.0\r\n- CUDA/cuDNN version: 10.1.243\r\n- GPU model and memory: None\r\n\r\n**Describe the current behavior**\r\n\r\nExecuting the test that runs `unique` on strings/chars returns invalid values for the index tensor. Some values are plain out of bounds (very big or negative) and others seem to point to wrong values. This very much looks like some kind of memory corruption.\r\nSo far this has only been reproducible on a cascade lake CPU, no GPUs in the system but TF was built with CUDA support.\r\n\r\n**Describe the expected behavior**\r\n\r\nValid values returned\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\n    import numpy as np\r\n    from tensorflow.python.ops import array_ops\r\n\r\n    def testString():\r\n        indx = np.random.randint(65, high=122, size=7000)\r\n        x = [chr(i) for i in indx]\r\n        y, idx = array_ops.unique(x)\r\n        tf_y, tf_idx = (y.numpy(), idx.numpy())\r\n        print(','.join(str(i) for i in tf_idx))\r\n\r\n        assert len(x) == len(tf_idx)\r\n        assert len(tf_y) == len(np.unique(x))\r\n\r\n        for i in range(len(x)):\r\n            assert x[i] == tf_y[tf_idx[i]].decode('ascii')\r\n\r\n    testString()\r\n```\r\nThis test was extracted from the TF test suite as-is.\r\nErrors include the last assert failing as well as errors like `IndexError: index 7566446 is out of bounds for axis 0 with size 57`\r\n\r\n**Other info / logs**\r\n\r\nFor a part of the log from the TF test suite see https://gist.github.com/boegel/26f768c82080e593add3924fc7bc76cf\r\nThe `print` of `tf_idx` shows things like:\r\n`-15829468,126746879,293189,50530320,291592,3,-16616924,58720256,66085632,66085120,1,0,7352690,946952,1538050,51149349,10,126746628,11,10,7,33583628,-16551388,126747138,126746628,50491941,101480976,126746879,16742418,14,20,752147,19,134209791,126749222,17,126747141,3840,16,2,33585676,18,387021328,70664,219117604,486963728,27,66343024,50530320,24,50756133,27,126762239,4096,7,-16223708,126749187,33977217,20,9,4,293189,20,4,40,24,23,8,13,-16025575,31,26,2375,40,22,126749187,8,19,36,6,26,126746627,22,36,133292287,133292287,44,21,34,36,13,31,26,13,24,43,10,-16354780,31,41,419459596,38,39,8,1,43,2,44,1,4,11,2,45,7,22,41,29,6,18,39,126746630,5,8,3,1,26,14,40,36,32,8,988163,25,7,42,18,3,41,50,32,2,8,22,22,17,39,18,5,21,30,14,22,34,43,43,48,6,24,50,20,32,24,29,35,8,40,9,16,18,46,50,18,7,13,31,37,47,12,2,27,39,6,37,38,44,45,29,16,0,2,31,1,29,8,26,9,30,32,66084976,29,29,29,24,15,39,35,49,37,30,0,27,39,386169362,9,41,4,7,126749211,5,24,46,4,0,49 [...]` (remaining values look ok)\r\n\r\n\r\nTo stress that: This seems to be highly system dependent. We compile with `-march=native` and see it only on the cascade lake system. The string test is the only one that fails, all other unique tests succeed.\r\nThis sounds to me like some case of undefined behavior where an optimization corrupts the absl hashmap used.", "comments": ["I did not face any errors on running the code with TF v2.3, TF v2.4 and TF-nightly on Colab. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/0a4de2b0cdd225845b9d5ce803303ec4/47179.ipynb). Thanks!", "I did not face any errors on running the code with TF v2.5 on Colab,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/d21225a3d2b482f1fc23b9d2461ce525/untitled122.ipynb?authuser=1)..Thanks !", "I just reproduced that again. See the part:\r\n> We compile with -march=native and see it only on the cascade lake system.\r\n\r\nI switched the compiler from GCC 8.3.0 to 10.2.0 which also resolved this. This IMO leaves 2 possibilities:\r\n- A bug in GCC 8 which was fixed in the meantime\r\n- Usage of undefined behavior in the TF code which may or may not result in a bug depending on what the optimizer choses to do", "I traced the issue to something which looks like a compiler bug. Abseils use of a union over `std::pair<const K, V>` & `std::pair<K, V>` leads to a write to the non-const version at https://github.com/abseil/abseil-cpp/blob/8f92175783c9685045c50f227e7c10f1cddb4d58/absl/container/internal/container_memory.h#L384 and a read via the const version which may return a wrong value. Changing the `if` to always create via the const-value (else branch) can resolve this.\r\n\r\nI tested this failure to happen with GCC 8.3 and 8.4 even on -O2 but it disappeared with GCC 9.1 through 10.3. It also seems to be unrelated to AVX512 instructions contrary to #49944 but it also does disappear when e.g. changing `-mtune=skylake` to `-mtune=broadwell` which is very odd. And it likely being an aliasing/optimization issue it also disappears when adding code after the construct call or printing stuff or prior access to that value and some more.\r\n\r\nEdit: Further narrowing this down lead to `-ftree-vrp`, so `-O2 -fno-tree-vrp` also makes the code work", "After a lively discussion with a GCC developer I conclude that this bug is caused by use of unsupported behavior inside Abseil. See https://github.com/abseil/abseil-cpp/issues/975 for the gory details. I'd suggest to fix the bug in Abseil and/or apply a patch to the download copy of abseil"]}, {"number": 47174, "title": "EfficientNet models from TensorFlow.Keras not being reproducible on GPU", "body": "After downloading an EfficientNet model from [tensorflow.keras.applications.efficientnet][1], and retraining it on our own data, I've noticed that the results are not reproducible. The results are reproducible for other architectures like *VGG16*, *ResNet101*, *InceptionV3*, and *InceptionResNetV2*, but not for any of the *EfficientNetBx* models.\r\n\r\nPlease note that I've set all the following seeds, and even have tensorflow-determinism:\r\n\r\n    random.seed(1)\r\n    np.random.seed(1)\r\n    tf.random.set_seed(1)\r\n    os.environ['TF_CUDNN_DETERMINISTIC'] = TRUE\r\n    os.environ['TF_DETERMINISTIC_OPS'] = TRUE\r\n\r\nTensorFlow Version:  tensorflow-gpu==2.3\r\n\r\n  [1]: https://www.tensorflow.org/api_docs/python/tf/keras/applications/efficientnet", "comments": ["@mhaghighat \r\nPlease share simple indented stand alone code with all dependencies to replicate the issue or if possible share a colab gist with the error reported.", "```\r\nimport os\r\nimport numpy as np\r\nimport tensorflow as tf\r\nif tf.test.gpu_device_name():\r\n    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\r\nelse:\r\n    print(\"Please install GPU version of TF\")\r\n\r\nnp.random.seed(1)\r\ntf.random.set_seed(2)\r\nos.environ['TF_CUDNN_DETERMINISTIC'] = 'true'\r\nos.environ['TF_DETERMINISTIC_OPS'] = 'true'\r\n\r\ntf.config.threading.set_inter_op_parallelism_threads(1)\r\ntf.config.threading.set_intra_op_parallelism_threads(1)\r\n\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nimport matplotlib.pyplot as plt\r\nfrom keras.utils import np_utils\r\nfrom keras.optimizers import SGD\r\nfrom keras.callbacks import CSVLogger\r\n\r\ncsv_logger = CSVLogger('log.csv', append=True, separator=';')\r\n\r\nbatch_size = 32 # 32 examples in a mini-batch, smaller batch size means more updates in one epoch\r\nnum_classes = 10#\r\nepochs = 3# repeat 200 times\r\ndata_augmentation = True\r\n\r\n\r\n## Model \r\ninput_shape = (32,32,3)\r\nbase_model  = tf.keras.applications.efficientnet.EfficientNetB0(include_top=False, weights=\"imagenet\", input_shape=input_shape)\r\n#base_model = tf.keras.applications.VGG16(include_top=False, weights=\"imagenet\", input_shape=input_shape)\r\n#base_model = tf.keras.applications.ResNet101(include_top=False, weights=\"imagenet\", input_shape=input_shape)\r\n#base_model = tf.keras.applications.MobileNetV2(include_top=False, weights=\"imagenet\", input_shape=input_shape)\r\n#base_model = tf.keras.applications.MobileNet(include_top=False, weights=\"imagenet\", input_shape=input_shape)\r\n\r\ninitializer = tf.keras.initializers.GlorotUniform(seed=1)\r\nregularizer = tf.keras.regularizers.l2(0.0001) # 0.0001\r\nglobal_average_layer = layers.GlobalAveragePooling2D()\r\nflatten_layer = layers.Flatten()\r\ndense_layer = layers.Dense(10, use_bias=False, kernel_initializer=initializer, bias_initializer='zeros', name='Bottleneck', activation='softmax')\r\n\r\nModel  = tf.keras.Sequential([base_model, global_average_layer, flatten_layer, dense_layer])\r\n\r\n\r\n## Training Data\r\n(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\r\nprint('x_train shape:', x_train.shape)\r\nprint(x_train.shape[0], 'train samples')\r\nprint(x_test.shape[0], 'test samples')\r\nclass_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\r\ny_train = np_utils.to_categorical(y_train, num_classes)\r\ny_test = np_utils.to_categorical(y_test, num_classes)\r\nx_train = x_train.astype('float32')\r\nx_test = x_test.astype('float32')\r\nx_train  /= 255\r\nx_test /= 255\r\n\r\n\r\n## Training\r\nsgd = SGD(lr=0.01, momentum=0.9, decay=1e-6, nesterov=False)\r\nModel.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\r\ncnn = Model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test,y_test), shuffle=False,callbacks=[csv_logger])\r\n\r\n```", "@mhaghighat \r\nI ran the code on tf 2.3 ,2.4 and nightly with gpu and it did reproduce output, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/67ce5f207dccd8b28d5593532bb569fc/untitled541.ipynb).", "@Saduf2019 \r\nLooking at your experiments, I can see that it is NOT reproducible for you either. The losses are different for each run. For example, the loss after the first epoch is 2.2377,  2.1338, and 1.9896 for the first, second, and third runs, respectively. \r\nI also get different losses running the experiments in each version twice.", "@mhaghighat I ran your code two times restarting the kernel when I ran second time. I see the results are reproducible. Please check the gist [here](https://colab.research.google.com/gist/jvishnuvardhan/4c12e097430d9ef78ea1bd19d63bac39/untitled541.ipynb).\r\n\r\nI got these results (first time)\r\n```\r\n# First time these are the results. After these results I ran one more time and the results are above\r\nEpoch 1/3\r\n1563/1563 [==============================] - 511s 323ms/step - loss: 2.1790 - accuracy: 0.2155 - val_loss: 3.5879 - val_accuracy: 0.1000\r\nEpoch 2/3\r\n1563/1563 [==============================] - 495s 317ms/step - loss: 1.6280 - accuracy: 0.4084 - val_loss: 5.4849 - val_accuracy: 0.1000\r\nEpoch 3/3\r\n1563/1563 [==============================] - 496s 318ms/step - loss: 1.4605 - accuracy: 0.4793 - val_loss: 4.0663 - val_accuracy: 0.1000\r\n```\r\n\r\nRestarted kernel and ran again (second time) and noticed these results are reproducible. Please let me know if I am missing anything. Thanks!\r\n```\r\nEpoch 1/3\r\n1563/1563 [==============================] - 544s 344ms/step - loss: 2.1790 - accuracy: 0.2155 - val_loss: 3.5879 - val_accuracy: 0.1000\r\nEpoch 2/3\r\n1563/1563 [==============================] - 506s 324ms/step - loss: 1.6280 - accuracy: 0.4084 - val_loss: 5.4849 - val_accuracy: 0.1000\r\nEpoch 3/3\r\n1563/1563 [==============================] - 514s 329ms/step - loss: 1.4605 - accuracy: 0.4793 - val_loss: 4.0663 - val_accuracy: 0.1000\r\n```", "@jvishnuvardhan : The reason you are getting reproducible results is that you have installed the `tf-nightly` version, which runs on CPU. Using the GPU version (please use `!pip install tensorflow-gpu==2.3`), I am getting different results for each run.\r\nI got these results (first time):\r\n```\r\nEpoch 1/3\r\n1563/1563 [==============================] - 43s 27ms/step - loss: 1.8422 - accuracy: 0.3357 - val_loss: 2.4678 - val_accuracy: 0.1021\r\nEpoch 2/3\r\n1563/1563 [==============================] - 42s 27ms/step - loss: 1.3999 - accuracy: 0.5037 - val_loss: 2.6971 - val_accuracy: 0.0991\r\nEpoch 3/3\r\n1563/1563 [==============================] - 41s 26ms/step - loss: 1.1729 - accuracy: 0.5852 - val_loss: 2.3490 - val_accuracy: 0.1156\r\n```\r\nAnd the second time:\r\n```\r\nEpoch 1/3\r\n1563/1563 [==============================] - 42s 27ms/step - loss: 1.9248 - accuracy: 0.3008 - val_loss: 2.4087 - val_accuracy: 0.0989\r\nEpoch 2/3\r\n1563/1563 [==============================] - 42s 27ms/step - loss: 1.5121 - accuracy: 0.4559 - val_loss: 3.2401 - val_accuracy: 0.1014\r\nEpoch 3/3\r\n1563/1563 [==============================] - 42s 27ms/step - loss: 1.2872 - accuracy: 0.5445 - val_loss: 2.9831 - val_accuracy: 0.1000\r\n```", "@mhaghighat Can you please try with `tf-nightly-gpu`? I tried with ` tf-nightly-gpu` and I see same reproducible results as I had shown earlier. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/92d00558c2e8a4a22d06505eb49bf8d0/untitled541.ipynb).  Thanks!\r\n\r\nAlso, I noticed that this is an issue with `TF2.3`. I think this was resolved in recent `tf-nightly`. Please check the [gist](https://colab.research.google.com/gist/jvishnuvardhan/4480849ce2b9852f37a2858521b1e619/untitled541.ipynb) with `TF2.3`. Thanks!", "I can also see reproducibility with `TF2.4-gpu`. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/1edb2d0c2e0468984184c2ae93823f70/untitled541.ipynb). Thanks!", "@jvishnuvardhan : I noticed that your run-times are around 500s for each epoch. For some reason, your kernel is still running the tf-cpu. Using the GPU, each epoch takes around 42s, and they are not reproducible. To make sure you are running the tf-gpu, please add this check after your tensorflow import:\r\n\r\n```\r\nif tf.test.gpu_device_name():\r\n    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\r\nelse:\r\n    print(\"Please install GPU version of TF!\")\r\n```", "@mhaghighat Thanks! For some reason, colab has some problem when installing `TF-gpu`.  We will look into the issue. Thanks again. ", "cc @ymodak ", "@ymodak @jvishnuvardhan Any updates on this issue? ", "I am also having the same issue to reproduce the results of EfficientNet models. Just wondering if anyone has updates on this?", "I am facing similar issue with the EfficientNet models. Not able to reproduce to result when using `tf-gpu`.", "Facing the same problem. Any updates on this yet?", "I think this is caused by https://github.com/tensorflow/tensorflow/issues/38185, so closing as a duplicate.\r\n\r\n@duncanriach is working on making `tf.nn.softmax_cross_entropy_with_logits` deterministic.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47174\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47174\">No</a>\n", "@reedwm @sanjoy @jvishnuvardhan I don't think this issue is related to the GPU-deterministic backprop. I already have the [tensorflow-determinism](https://github.com/NVIDIA/framework-determinism) installed and the results are reproducible for other architectures like _VGG16_, _ResNet101_, _InceptionV3_, and _InceptionResNetV2_, which use the same backprop. This issue is unique to the EfficientNet models. ", "`tf.nn.softmax_cross_entropy_with_logits` is being called, which is nondeterministic. But if I replace it with a softmax followed by a cross-entropy, which is deterministic, the model still has nondeterminism. So there's another determinism issue, so I'll reopen.\r\n\r\n@duncanriach is adding errors to all currently known nondeterministic ops when `TF_DETERMINISTIC_OPS` is used (@duncanriach, let me know if you want help with this). Once that is done, it will be easy to identify the source of the nondeterminisim.", "[Disclosure: I have not (yet) run this code and debugged the source of nondeterminism. The following is just from looking at the models.]\r\n\r\n1. The code from @mhaghighat (above) implements the currently recommended [workaround](https://github.com/NVIDIA/framework-determinism/blob/master/tensorflow_status.md#softmax-xent) for fused softmax/cross-entropy nondeterminism by configuring the final `dense_layer` to have a `'softmax'` activation. This explains why the other four architectures operate deterministically even though they use that loss function.\r\n2. Several of these model architectures use `tf.keras.layers.BatchNormalization` which, [when used for fine-tuning](https://github.com/NVIDIA/framework-determinism/blob/master/tensorflow_status.md#fused-batch-norm), will introduce nondeterminism in the backprop. This is something that @reedwm discovered recently. However, all of these models are being trained using `model.fit()` and I don't think that the `is_training` parameter of the underlying op is ever getting to set to `True` while gradients are being generated. The batch-norms are being used in inference mode, during the evaluation step, but in those cases the gradients are not being used. So, I don't think that batch-norm is the issue here.\r\n3. No other significant differences currently stand-out to me between these model architectures (e.g. more than one of them uses dropout, so I'm assuming that it's being initialized correctly even in the troublesome model architecture).\r\n4.  I'm wondering if there is one or more fields missing in the loaded initialization state for EfficientNet. Whenever attempting to achieve reproducibility, it's always good to start by confirming that the initial state is reproducible. After calling `Model.compile` and before calling `Model.fit`, please will you (@mhaghighat) confirm that the initial trainable variables are reproducible by calling the following function.\r\n\r\n```\r\ndef summarize_keras_trainable_variables(model, message):\r\n  s = sum(map(lambda x: x.sum(), model.get_weights()))\r\n  print(\"summary of trainable variables %s: %.13f\" % (message, s))\r\n  return s\r\n```\r\n\r\nLess important aside: you don't need to set `TF_CUDNN_DETERMINISTIC`. That switch has been superseded by `TF_DETERMINISTIC_OPS`, which implements the same functionality, and more.", "Thank you, @duncanriach. Using your suggested function, I checked the model state after the compilation, and it is the same for multiple runs. So, the initial state is reproducible. \r\nMy guess is that the problem is related to the `DepthwiseConv2D` layers. They only exist in EfficientNet and MobileNet architectures, and neither of them are reproducible. ", "@mhaghighat, well spotted; I missed that. I had also assumed that all the commented-out model architectures were operating deterministically. Now I see that the list in the code does not match the list in your original comment.\r\n\r\n`tf.keras.layers.DepthwiseConv2D` and `tf.nn.depthwise_conv2d` (upon which it is built) are suspect, as you probably saw [here](https://github.com/NVIDIA/framework-determinism#other-possible-gpu-specific-sources-of-non-determinism). This op has shown up in other model architectures that I'm aware of as being a probable source of truly random noise. However, I have not yet gotten around to creating a reproducer for it. I will task myself with that and prioritize it. I'll return here with findings.\r\n\r\nThe first-order work-around I can think of is to force the underlying op to run on the CPU, which will slow training down a bit, but not as much as running the whole model on the CPU.\r\n\r\nTry running the following code near the beginning of your program (before training). I have not run this code, and I'm not sure that the generic all-arg-passing will work; you might need to pass the args explicitly. There might be other bugs. Please let me know how it goes.\r\n\r\n```\r\nfrom tensorflow.python.ops import nn\r\nx = nn.depthwise_conv2d\r\n\r\ndef cpu_op(*args, **kwargs):\r\n  tf.device('/cpu:0'):\r\n    return x(*args, **kwargs)\r\n\r\nnn.depthwise_conv2d = cpu_op\r\n```", "@duncanriach: I couldn't separate that op run on the CPU. I tried different ways to pass the arguments but no success so far. \r\nWould you mind taking a look at it in this [notebook](https://colab.research.google.com/gist/mhaghighat/5d6d33b78531ccf9664501dc166508d4/untitled541.ipynb).\r\nThanks!", "@mhaghighat, thanks for making this really easy for me to help.\r\n\r\nThe call to `x` needed to look like this: `x(*args, **kwargs)` to unpack the `args` and `kwargs`. I have changed the code above to be the working version, including returning the result the call to `x`, which you discovered was necessary.\r\n\r\nThe model now trains. Step time increased from 27ms (all on GPU) to 100ms (DepthwiseConv2D on CPU), but it's not as slow as 320ms (everything on CPU).\r\n\r\nHowever, this does not seem to have resolved the nondeterminism.", "In this [notebook](https://colab.research.google.com/drive/1SAg0ghLHQOirjJLBTrJYr4aXFizTftS_?usp=sharing), I have simplified things so that the model will run twice in a few seconds. I have also included some code and instructions for hiding the GPU. I spent a while trying to identify what the remaining source of nondeterminism might be. Since the model trains reproducibly on CPU, it's almost certainly a nondeterministic GPU op (BatchNormalization? Normalization?). My next step would be to clone the model code and use the (as-yet unreleased) nondeterminism debug tool that I have developed to isolate the source; this could be related to a new source that I was not aware of. I have to go do some other urgent things and I'll come back to this.", "@duncanriach \r\nIt's probably not the BatchNormalization. As a point of reference, InceptionResNetV2 model, which is reproducible in GPU, has BatchNormalization layer (and so does ResNet101). \r\n\r\nIn case it is helpful, following layers are unique to the EfficientNetB0 in comparison to the InceptionResNetV2: \r\n`Rescaling`\r\n`Normalization`\r\n`ZeroPadding2D`\r\n`DepthwiseConv2D`\r\n`GlobalAveragePooling2D`\r\n`Reshape`\r\n`Multiply`\r\n`Dropout` and\r\n`Add`\r\n\r\nMy initial guess was the `Dropout` layer that is causing the reproducibility issue, as dropout has an inherent randomness to it. I did try to freeze the randomness in the dropout layers by setting `layer.seed = 1` and `layer.dropout_rate = 0.0`. But it did not solve the reproducibility issue either.\r\n\r\nAccording to the discussion so far, and, as you suggest, it can be Normalization that is causing the issue. Although, MobileNet architectures do not have Normalization layer and yet they are not reproducible. \r\n\r\nMay be we are missing something else?", "Thanks for digging deeper, @nasirml; that's very helpful.\r\n\r\nIt is possible the the model architectures are using BatchNormalization differently. If BatchNormalization is configured for [fine-tuning](https://github.com/NVIDIA/framework-determinism/blob/master/tensorflow_status.md#fused-batch-norm), then it would introduce nondeterminism, otherwise not. I have yet to clarify (and document) the configuration mapping with respect to this between `tf.compat.v1.nn.fused_batch_norm` and `tf.keras.layers.BatchNormalization`.", "I'm returning here with findings on a repro for `tf.keras.layers.DepthwiseConv2D` nondeterminism, as promised in [this previous comment](https://github.com/tensorflow/tensorflow/issues/47174#issuecomment-820009465).\r\n\r\nPlease see [this gist](https://gist.github.com/duncanriach/4c18cb07a73510c5fcb2deb52adbffaa) for a complete repro, forward and backward, CPU and GPU. I'll copy/paste the findings here:\r\n\r\nThis repro demonstrates that:\r\n\r\n1. `tf.nn.depthwise_conv2d` operates deterministically* in the forward direction when running on both CPU and GPU;\r\n2. when running on CPU, `tf.nn.depthwise_conv2d` operates deterministically* in backprop to both `input` and `filter`;\r\n3. when running on GPU and when op-determinism is expected, `tf.nn.depthwise_conv2d` operates deterministically* in backprop to `input`;\r\n4. when running on GPU and when op-determinism is expected, `tf.nn.depthwise_conv2d` operates determinsitically* in backprop to `filter` when it is using cuDNN convolution (more info below);\r\n5. when running on GPU and when op-determinism is expected, `tf.nn.depthwise_conv2d` operates nondeterminstically* in backprop to `filter` when it is not using cuDNN convolution (more info below); and\r\n6. when running on GPU and when op-determinism is not expected, operation* is as would be expected given that cuDNN backprop algorithms to both `input` and `filter` operate nondeterministically* by default and the specialized algorithms for depthwise conv operate deterministically* in backprop to `input` and nondeterministically* in backprop to `filter`.\r\n\r\n*: for the tested parameters\r\n\r\n\"when op-determinism is expected\" currently means when `TF_DETERMINISTIC_OPS` is set to `\"true\"` or `\"1\"`\r\n\r\n\"when op-determinism is not expected\" currently means when `TF_DETERMINISTIC_OPS` either has not been set or has been set to `\"false\"` or `\"0\"`.\r\n\r\ncuDNN convolution is selected when there is only one input channel and cuDNN supports the desired filter dimensions and number of output channels.\r\n\r\n`tf.keras.layers.DepthwiseConv2D` uses `tf.nn.depthwise_conv2d` simply and direcly. Therefore, any assertions about the latter apply equally to the former.\r\n\r\nThe findings above are summarised in the following table, where `D` stands for deterministic and `ND` stands for nondeterministic:\r\n\r\n```\r\n    | col                     |  1  |  2  |  3  |  4  |  5\r\n    | device                  | CPU | GPU | GPU | GPU | GPU\r\n    | op-determinism expected |     |  Y  |  Y  |  N  |  N\r\nrow | cuDNN used              |     |  Y  |  N  |  Y  |  N\r\n====o=========================o=====o=====o=====o=====o====\r\n 1  | forward                 |  D  |  D  |  D  |  D  |  D\r\n 2  | backprop to input       |  D  |  D  |  D  |  ND |  D\r\n 3  | backprop to filter      |  D  |  D  |  ND |  ND |  ND\r\n```\r\n\r\nNote that the only problematic case here is the cell at row 3, col 3 (GPU, op-determinism expected, cuDNN not used) because we expect op-determinism and we're not getting it. On the other hand, we clearly don't care about nondeterminism when op-determinism is not expected (i.e. col 4 and col 5).\r\n\r\nFor GPU functionality, these findings are consistent with looking at the code, in which the CUDA kernel for the depthwise backprop to filter uses CUDA atomicAdd and the CUDA kernel for the depthwise backprop to input does not. For CPU functionality, some of the implementation code uses multi-threaded sharding, but that doesn't appear to result in nondeterminism, which is not unusual.\r\n\r\nSo far in this discussion, cuDNN auto-tuning has not been considered. cuDNN auto-tuning would not (and should not) be disabled in col 4 of the table above (GPU, op-determinism not expected, cuDNN used). Therefore, the `D` at col 4, row 1 is not actually correct: auto-tuning could (and ultimately would) introduce nondeterminism.\r\n\r\n~~Finally, while looking at the code-paths via which `tf.nn.depthwise_conv2d` uses cuDNN convolution (e.g. in `conv_grad_filter_ops.cc`), it seems that there might be other (perhaps partial) implementations of cuDNN auto-tuning, independent from those used for the regular convolution ops (e.g. `tf.nn.conv2d`). Because it's very difficult (or perhaps impossible) to test that auto-tuning nondeterminism is not present, a careful code-level inspection and understanding of the cuDNN auto-tuning mechanism for this op should be performed as part of addressing nondeterminism in this op.~~\r\n\r\n2022-03-17 Update:\r\nFinally, I have confirmed that the code-paths via which tf.nn.depthwise_conv2d uses cuDNN convolution (e.g. in conv_grad_filter_ops.cc) also benefit from the existing deterministic selection of deterministic cuDNN convolution algorithms (when op-determinism is expected).", "Update: MR [51920](https://github.com/tensorflow/tensorflow/pull/51920) adds determinism-unimplemented exception-throwing to `tf.nn.depthwise_conv2d`. This will be included in the TF 2.7 release. Meanwhile, you can (relatively easily) try out the latest determinism functionality in the top-of-tree by using the `tensorflow/tensorflow:nifgtly-gpu` Docker container image.", "For some reason, when I install the `tf-nightly-gpu`, it doesn't see the GPU. It acts as the CPU version, and it returns null for `tf.test.gpu_device_name()`. I don't have this problem with r2.6 on the same device. We thought maybe it is CUDA incompatibility and tried other versions of CUDA, but it didn't help. I hope r2.7 releases soon and it won't have this issue. ", "I installed the released tensorflow-gpu==2.7. This version throws an error when the determinism flag is on (`os.environ['TF_DETERMINISTIC_OPS'] = '1'`): \r\n```\r\n  File \"/home/mo/anaconda3/envs/tf27/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"/home/mo/anaconda3/envs/tf27/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 58, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.UnimplementedError:  A deterministic GPU implementation of DepthwiseConvBackpropFilter is not currently available.\r\n\t [[node gradient_tape/efficientnet-lite0/efficientnet-lite0/model/blocks_15/depthwise_conv2d/depthwise_0/depthwise/DepthwiseConv2dNativeBackpropFilter\r\n (defined at /home/mo/whr/tmp/food2vec/magnet_loss/train.py:151)\r\n]] [Op:__inference_train_step_19847]\r\n```\r\nAnd if I comment out the line regarding TF_DETERMINISTIC_OPS, the results are still not reproducible. \r\n\r\n\r\n", "Thank you for letting us know, @mhaghighat. This is expected and correct behavior. This confirms that the model is attempting to use `tf.nn.depthwise_conv2d` (as we suspected), which does not currently (or yet) have a deterministic GPU implementation. I'm going to make a note of this with the intention that this use-case can be taken into consideration as part of prioritizing a deterministic GPU implementation. It may be possible to implement an acceptable, though perhaps non-optimal, solution using the cuDNN-based non-depthwise convolution ops, which _can_ operate nondeterministically.\r\n\r\n@reedwm, this is just something for us to consider: if this exception could be easily disabled, then it would be possible to run further into the model and thereby discover if there are other ops for which there is not yet a deterministic implementation. I'm wondering if it makes sense to have a per-op switch that selects [A] exception (default), [B] skip exception and run on default device (e.g. GPU), and [C] skip exception and run on \"slower\" device (e.g. CPU).", "In principle I think adding such a per-op switch is sensible, but it is difficult to justify adding an API to do this because eventually we hope to make all ops deterministic. For now, I think we should try fixing such ops or unconditionally implementing [C] when determinism is enabled.\r\n\r\nUnfortunately, [C] is surprisingly difficult to implement. For Scatter ops, I added some code to do this, but it required writing a [fairly long function](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/kernels/scatter_nd_op.cc;l=868;drc=0858918774ab172b8739ca441cef19d0eea1b6b9) which manually copies tensors to the CPU, runs the CPU version of the op, then copying outputs back to the GPU. \r\n\r\nInstead, we could register a GPU kernel that runs on the CPU, by marking all inputs/outputs on host memory and using the CPU implementation, but there's currently no way to get such kernels to run if and only if determinism is enabled. Perhaps we should add a way, but it would be difficult since normally the set of kernels that can be run can is determined once per graph, but determinism can be enabled after a graph is already run. In any case, it's probably a bad idea to run depthwise conv ops on the CPU since I think these ops often make up a significant portion of a model and running them on the CPU might slow it down significantly.\r\n\r\nFor depthwise conv2d specifically, I wonder if it's possible to always use cuDNN when determinism is enabled. I can't find any reference in the cudnn documentation that states depthwise convolutions are only supported for certain shapes\r\n\r\n", "Thank you for describing all of that, @reedwm.\r\n\r\n**Exception Throwing**\r\n\r\nI agree that the exception-throwing is intended to be temporary and it's also a mechanism for flagging important ops that we want to provide deterministic GPU implementations for, a user-accessible nondeterminism debugging tool to help us prioritize effort. In a model with more than one op without a deterministic GPU implementation, it would be preferable to be able to hit all the exceptions (or confirm that there are no others).\r\n\r\nI'm wondering if a good middle-ground would be to add an environment-variable-based exception-disable switch to ops on a case-by-case bass. One of these [was added](https://github.com/tensorflow/tensorflow/pull/47925/files#diff-28078d1a90c8ab6eece5f8b54883114f6e8a4e20f87fd5ccf44627fa1e0c11c1R66-R75) for `tf.nn.softmax_cross_entropy_with_logits` before it ran deterministically on the GPU, so that it could be used for a higher-level determinism work-around. A similar disable switch could be added for `tf.nn.depthwise_conv2d` (without testing) very easily and then be available in the nightly build. Then @mhaghighat could confirm that `tf.nn.depthwise_conv2d` is the only remaining op preventing this model from operating deterministically.\r\n\r\n**Depthwise Convolution using cuDNN**\r\n\r\nAre you suggesting using cuDNN grouped convolutions to implement depthwise convolution? On the surface, that seems to be possible. Regarding restrictions on shapes, I remember there being shape restrictions on when cuDNN is used in the existing code for the single-input-channel cases, in which depthwise convolution and regular convolution are the same thing. I find it hard to believe that cuDNN doesn't support all the required shapes.\r\n\r\nAnother possibility might be to implement diagonalwise refactorization as described in [this 2018 paper](https://arxiv.org/pdf/1803.09926.pdf). This is the kind of thing I was beginning to think about in [this earlier comment](https://github.com/tensorflow/tensorflow/issues/47174#issuecomment-965986440) and I'm so happy to find that it's already fleshed out. But I wonder if this would actually be equivalent to using grouped convolution in terms of how the underlying computation gets done, and so therefore have similar performance.\r\n\r\nEither way, shouldn't it be possible to delegate to the regular conv2d op, which should handle any shapes that might be needed? But does the regular conv2d op take a `groupCount` parameter at the API level from which we would be calling it?", "@mhaghighat, PR [54119](https://github.com/tensorflow/tensorflow/pull/54119) added a disable switch for the exception. Please will you run again using the `tensorflow/tensorflow:latest-gpu` docker image (I think that will have the change in it). Enable determinism using [`tf.config.experimental.enable_op_determinism`](https://www.tensorflow.org/api_docs/python/tf/config/experimental/enable_op_determinism) rather than `TF_DETERMINISTIC_OPS`. Also, disable the `tf.nn.depthwise_conv2d` exception by setting `TF_DISABLE_DEPTHWISE_CONV_DETERMINISM_EXCEPTIONS` to `\"true\"` or `\"1\"`.\r\n\r\nI want to see if another determinism-unimplemented exception is thrown, or if `tf.nn.depthwise_conv2d` is the only op that is now preventing this model from operating deterministically.", "@mhaghighat Can you please check @duncanriach suggestions and respond with your findings? Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Let's not close this. I intend to execute the experiment I asked for [above](https://github.com/tensorflow/tensorflow/issues/47174#issuecomment-1028486016). I will return here with the result from that experiment.", "By disabling the exception on `tf.nn.depwise_conv2d`, I have confirmed that it is the only remaining nondeterministic op standing in the way of the EfficientNet models operating fully reproducibly on GPU. @reedwm, I intend to start work on making this op function deterministically on GPU.", "> By disabling the exception on `tf.nn.depwise_conv2d`, I have confirmed that it is the only remaining nondeterministic op standing in the way of the EfficientNet models operating fully reproducibly on GPU. @reedwm, I intend to start work on making this op function deterministically on GPU.\r\n\r\nWhy my EfficientNet model can be reproduced in 2.8? I did not use the TF_DISABLE_DEPTHWISE_CONV_DETERMINISM_EXCEPTIONS ? @duncanriach ?\r\n\r\nAlso, my other model which use depthwise conv also can be reproduced.\r\n\r\nhttps://github.com/edwardyehuang/iSeg/blob/master/backbones/efficientnet.py\r\n\r\nI just confirmed again. I tested with 3 machines, each one has 8\u00d7V100.", "@edwardyehuang, to trigger the determinism-unimplemented exception in `tf.nn.depthwise_conv2d` (in TensorFlow version 2.8) your model must use the back-prop paths through the GPU implementation of the op. It's possible to run GPU inference only or train on CPU using the op without the exception being thrown. Please confirm that you're definitely _training_ your models on _GPU_.", "> @edwardyehuang, to trigger the determinism-unimplemented exception in `tf.nn.depthwise_conv2d` (in TensorFlow version 2.8) your model must use the back-prop paths through the GPU implementation of the op. It's possible to run GPU inference only or train on CPU using the op without the exception being thrown. Please confirm that you're definitely _training_ your models on _GPU_.\r\n\r\nI am using keras.mode.fit and tf.distribute.MirroredStrategy on 8 GPUs\r\n\r\nTraining speed is fast and GPU usage is very high (EfficientNet-B5 or B7)\r\n\r\n```\r\n2022-03-12 22:39:39.158 2022-03-12 22:39:39.158082: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA\r\n\r\n2022-03-12 22:39:39.158 To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n\r\n2022-03-12 22:39:45.181 2022-03-12 22:39:45.180902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30985 MB memory: -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0\r\n\r\n2022-03-12 22:39:45.183 2022-03-12 22:39:45.183427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30985 MB memory: -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1b:00.0, compute capability: 7.0\r\n\r\n2022-03-12 22:39:45.185 2022-03-12 22:39:45.185836: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 30985 MB memory: -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:3d:00.0, compute capability: 7.0\r\n\r\n2022-03-12 22:39:45.188 2022-03-12 22:39:45.188230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 30985 MB memory: -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:3e:00.0, compute capability: 7.0\r\n\r\n2022-03-12 22:39:45.190 2022-03-12 22:39:45.190580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 30985 MB memory: -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:88:00.0, compute capability: 7.0\r\n\r\n2022-03-12 22:39:45.192 2022-03-12 22:39:45.192957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 30985 MB memory: -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0\r\n\r\n2022-03-12 22:39:45.195 2022-03-12 22:39:45.195342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 30985 MB memory: -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:b1:00.0, compute capability: 7.0\r\n\r\n2022-03-12 22:39:45.197 2022-03-12 22:39:45.197723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 30985 MB memory: -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:b2:00.0, compute capability: 7.0\r\n\r\n2022-03-12 22:39:45.239 INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5', '/job:localhost/replica:0/task:0/device:GPU:6', '/job:localhost/replica:0/task:0/device:GPU:7')\r\n\r\n2022-03-12 22:39:45.239 I0312 22:39:45.239010 140523358885696 mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5', '/job:localhost/replica:0/task:0/device:GPU:6', '/job:localhost/replica:0/task:0/device:GPU:7')\r\n\r\n2022-03-12 22:39:45.254 INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\r\n\r\n2022-03-12 22:39:45.254 Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0\r\n\r\n2022-03-12 22:39:45.254 I0312 22:39:45.254395 140523358885696 device_compatibility_check.py:123] Mixed precision compatibility check (mixed_float16): OK\r\n\r\n2022-03-12 22:39:45.254 Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0\r\n\r\n2022-03-12 22:39:48.584 INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\n2022-03-12 22:39:48.584 I0312 22:39:48.583973 140523358885696 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\n2022-03-12 22:39:48.585 INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\n2022-03-12 22:39:48.585 I0312 22:39:48.585851 140523358885696 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\n2022-03-12 22:39:48.591 INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\n2022-03-12 22:39:48.591 I0312 22:39:48.591116 140523358885696 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\n2022-03-12 22:39:48.592 INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\n2022-03-12 22:39:48.592 I0312 22:39:48.592334 140523358885696 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\n2022-03-12 22:39:48.594 INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\n2022-03-12 22:39:48.594 I0312 22:39:48.594471 140523358885696 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\n2022-03-12 22:39:48.601 INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\n2022-03-12 22:39:48.601 I0312 22:39:48.601806 140523358885696 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\n2022-03-12 22:39:48.808 INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\n2022-03-12 22:39:48.808 I0312 22:39:48.807963 140523358885696 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\n2022-03-12 22:39:48.809 INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\n2022-03-12 22:39:48.809 I0312 22:39:48.809195 140523358885696 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\n2022-03-12 22:39:48.814 INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\n2022-03-12 22:39:48.814 I0312 22:39:48.813995 140523358885696 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\n2022-03-12 22:39:48.815 INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\n2022-03-12 22:39:48.815 I0312 22:39:48.815169 140523358885696 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\n\r\n```\r\n\r\nAlso my code should fully run on GPU path, since I tested it on TPU before.", "@edwardyehuang, the original repro program for this issue uses `tf.keras.applications.efficientnet.EfficientNetB0`, not B5 or B7, but I just ran the repro program with B0, B5, and B7 on TF version 2.9.0-dev20220316 and again reproduced the issue on all three versions of the program, confirming that the nondeterministic path is certainly being traversed by the repro program (as evidenced by both the exception and by nondeterminism when the exception is disabled).\r\n\r\nYour program(s) appears to not be traversing the nondeterministic GPU backprop-to-filter path of `tf.nn.depthwise_conv2d`. If it was, the exception would be getting thrown. There are several possible reasons for that path to not get traversed, such as the cases where the operation is delegated to cuDNN. If your program is not traversing that path, but is operating correctly, then I wouldn't worry too much about it; it's fortunate.\r\n\r\nIf you're concerned that your program is not doing what you intend it to be doing, I recommend taking a look at the `tf.keras.layers.DepthwiseConv2D` filter weights for each instance in your model and checking that they're different between before and after training and that the weights are not all the same value after training. You could also stub-out, or change the parameters of, the depthwise convolution layers to at least confirm that they are contributing roughly as expected to the overall performance of your model.\r\n\r\nDo you have any concerns specifically with regard to this current issue?", "@duncanriach \r\n\r\nI don't have concerns anymore. However, can you explain \" the cases where the operation is delegated to cuDNN\" for me? Or provide me some link/topic to read. In my weak knowledge, I used to think the GPU operation in TensorFlow only has a single implementation.", "@edwardyehuang, the CUDA GPU functionality of any given op in TensorFlow is implemented using one or more (usually several) different CUDA kernels (at least one CUDA kernel for each path through the op, including forward paths and backward paths); for some ops, different CUDA kernels are used for different sections of the op parameter space. Some ops use functionality provided via the cuDNN library, which, in turn, often uses different underlying CUDA kernels (also referred to as algorithms in the cuDNN documentation) for different sections of the op parameter space. This particular op (`tf.nn.depthwize_conv2d`) depends on several custom CUDA kernels, but delegates to the convolution machinery in TensorFlow that is based on cuDNN when possible. [This earlier comment](https://github.com/tensorflow/tensorflow/issues/47174#issuecomment-828126821) from me in this current issue documents the partitioning between the functionality that is based on cuDNN versus the functionality that is not. To understand the exact boundaries of the partitioning requires reverse-engineering the existing code. The last time I looked into this in detail, delegation was passed to cuDNN when the number of input channels was one and the size of the filter was supported by cuDNN. I used this knowledge to create the study code.\r\n\r\nAside: Note also that a given algorithm or implementation that operates nondeterministically under some conditions usually will operate deterministically under some other (usually more constrained) conditions. However, for an algorithm or implementation to be considered truly deterministic (by us), its functionality must operate deterministically under all conditions (with possibly certain subtle caveats that I'm not going to get into here). This means that it is possible to disable a determinism-unimplemented exception and still use the nondeterministic path in a way that will not, in fact, introduce nondeterminism. It is usually not recommended to rely on determinism in this way, however, because the determinism would typically be very fragile. On the other hand, if your program is not even traversing the path that contains the nondeterministic algorithm, then the determinism that is experienced is, by definition, robust."]}, {"number": 47173, "title": "`ValueError: None values not supported` when using the Estimator API", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: Yes\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Fedora 33\r\n-   **TensorFlow installed from (source or binary)**: Attempted both\r\n-   **TensorFlow version (use command below)**: Attempted v2.4.1-0-g85c8b2a817f 2.4.1 (source) and v2.4.0-49-g85c8b2a817f 2.4.1 (binary)\r\n-   **Python version**: Python 3.9.1 and Python 3.7.7 :: Intel(R) Corporation\r\n-   **Bazel version (if compiling from source)**: Used Bazelisk, which listed 3.1.0 as the version it's using\r\n-   **GCC/Compiler version (if compiling from source)**: gcc (GCC) 10.2.1 20201125 (Red Hat 10.2.1-9)\r\n-   **CUDA/cuDNN version**: N/A\r\n-   **GPU model and memory**: N/A\r\n-   **Exact command to reproduce**: Attached code snippet\r\n\r\n### Describe the problem\r\nI have been attempting to use TensorFlow to build a random forest classifier, in this case using the built-in boosted trees classifier. I have attempted two different datasets, but the same general idea in the code. After these attempts, I'm thinking I might be hitting a bug of some sort\r\nThe code errors out with: `ValueError: None values not supported.` in a way that's extremely unclear what's going on\r\n\r\nTest1.py:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/user/tensorflow-random-forest/test.py\", line 38, in <module>\r\n    estimator.train(input_fn=create_input_fn('train'), max_steps=100)\r\n  File \"/usr/local/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 349, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1175, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1203, in _train_model_default\r\n    estimator_spec = self._call_model_fn(features, labels, ModeKeys.TRAIN,\r\n  File \"/usr/local/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1163, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/usr/local/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/canned/boosted_trees.py\", line 2081, in _model_fn\r\n    return _bt_model_fn(\r\n  File \"/usr/local/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/canned/boosted_trees.py\", line 1259, in _bt_model_fn\r\n    batch_size = tf.compat.v1.shape(labels)[0]\r\n  File \"/usr/local/lib64/python3.9/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/usr/local/lib64/python3.9/site-packages/tensorflow/python/ops/array_ops.py\", line 649, in shape\r\n    return shape_internal(input, name, optimize=True, out_type=out_type)\r\n  File \"/usr/local/lib64/python3.9/site-packages/tensorflow/python/ops/array_ops.py\", line 673, in shape_internal\r\n    input = ops.convert_to_tensor(input)\r\n  File \"/usr/local/lib64/python3.9/site-packages/tensorflow/python/profiler/trace.py\", line 163, in wrapped\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib64/python3.9/site-packages/tensorflow/python/framework/ops.py\", line 1540, in convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib64/python3.9/site-packages/tensorflow/python/framework/constant_op.py\", line 339, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/usr/local/lib64/python3.9/site-packages/tensorflow/python/framework/constant_op.py\", line 264, in constant\r\n    return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n  File \"/usr/local/lib64/python3.9/site-packages/tensorflow/python/framework/constant_op.py\", line 281, in _constant_impl\r\n    tensor_util.make_tensor_proto(\r\n  File \"/usr/local/lib64/python3.9/site-packages/tensorflow/python/framework/tensor_util.py\", line 445, in make_tensor_proto\r\n    raise ValueError(\"None values not supported.\")\r\nValueError: None values not supported.\r\n```\r\n\r\nTest2.py:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/user/tensorflow-random-forest/test2.py\", line 61, in <module>\r\n    estimator.train(input_fn=create_input_fn(train=True))\r\n  File \"/usr/local/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 349, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1175, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1203, in _train_model_default\r\n    estimator_spec = self._call_model_fn(features, labels, ModeKeys.TRAIN,\r\n  File \"/usr/local/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1163, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/usr/local/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/canned/boosted_trees.py\", line 2233, in _model_fn\r\n    return _bt_model_fn(\r\n  File \"/usr/local/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/canned/boosted_trees.py\", line 1222, in _bt_model_fn\r\n    batch_size = tf.compat.v1.shape(labels)[0]\r\n  File \"/usr/local/lib64/python3.9/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/usr/local/lib64/python3.9/site-packages/tensorflow/python/ops/array_ops.py\", line 649, in shape\r\n    return shape_internal(input, name, optimize=True, out_type=out_type)\r\n  File \"/usr/local/lib64/python3.9/site-packages/tensorflow/python/ops/array_ops.py\", line 673, in shape_internal\r\n    input = ops.convert_to_tensor(input)\r\n  File \"/usr/local/lib64/python3.9/site-packages/tensorflow/python/profiler/trace.py\", line 163, in wrapped\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib64/python3.9/site-packages/tensorflow/python/framework/ops.py\", line 1540, in convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib64/python3.9/site-packages/tensorflow/python/framework/constant_op.py\", line 339, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/usr/local/lib64/python3.9/site-packages/tensorflow/python/framework/constant_op.py\", line 264, in constant\r\n    return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n  File \"/usr/local/lib64/python3.9/site-packages/tensorflow/python/framework/constant_op.py\", line 281, in _constant_impl\r\n    tensor_util.make_tensor_proto(\r\n  File \"/usr/local/lib64/python3.9/site-packages/tensorflow/python/framework/tensor_util.py\", line 445, in make_tensor_proto\r\n    raise ValueError(\"None values not supported.\")\r\nValueError: None values not supported.\r\n```\r\n\r\n\r\n\r\n### Source code / logs\r\nThese two snippets produce the same type of error, it's possible I'm making some fundamental coding mistake, but I'm not seeing how.\r\n(github wanted me to change the .py to .txt to attach them, just remove to run)\r\n[test2.py.txt](https://github.com/tensorflow/tensorflow/files/5984443/test2.py.txt)\r\n[test.py.txt](https://github.com/tensorflow/tensorflow/files/5984444/test.py.txt)\r\n", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/70710486ff57db6a9cafe8629dbcf4cf/47173.ipynb). Thanks!", "Does that mean I found an actual bug rather than it just being a coding issue? o.O\r\nThere seemed to be a few instances of similar on SO, but nothing that looked similar enough that I was able to find", "@Xaenalt,\r\nCan you please confirm if you have gone through the instructions specified in the [Estimator Guide](https://www.tensorflow.org/guide/estimator) and have build your models accordingly? Thanks!", "Well, I had thought so, the only major difference was using TFDS, which I had assumed set the label_name field to inform the model of what the labels are. Though the error hasn't given me much to go on"]}, {"number": 47171, "title": "Slow as_numpy_iterator() and StringLookup.adapt() with make_csv_dataset()", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, attached below\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04 under WSL 2 and Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Doesn't apply.\r\n- TensorFlow installed from (source or binary): Binary.\r\n- TensorFlow version (use command below): v1.12.1-50923-g01fe322bbb5 2.5.0-dev20210215 (also tried 2.4.1 via pip and colab)\r\n- Python version: 3.8.6\r\n- Bazel version (if compiling from source): Doesn't apply.\r\n- GCC/Compiler version (if compiling from source): Doesn't apply.\r\n- CUDA/cuDNN version: Doesn't apply.\r\n- GPU model and memory: Doesn't apply. CPU only.\r\n\r\n\r\n**Describe the current behavior**\r\nUsing the dataset API to read CSV data (via `list(data.as_numpy_iterator())`) takes around 4-5x longer compared to regular pandas' `read_csv()`.\r\nRunning a `StringLookup.adapt(...)` is also slow. (Made even worse when multiple adapt()s need to run?) But can't be easily compared to pandas' unique() since the data is in memory by then.\r\nSetting `num_parallel_reads` didn't really make a difference.\r\n\r\nThe used dataset has 678014 lines with 30MB worth of data.\r\n\r\n**Describe the expected behavior**\r\nBeing near pandas read performance would be nice or faster via num_parallel_reads. ;-) This should make `adapt()` fast as well, hopefully.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1wySl43lLZ5P3ntjpD3TmYn0ZcYrTYve-?usp=sharing\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport pandas as pd\r\nimport logging\r\nfrom pathlib import Path\r\n\r\nFORMAT = '%(asctime)-15s %(message)s'\r\nlogging.basicConfig(format=FORMAT, level = logging.INFO)\r\n\r\npath = \"https://www.openml.org/data/get_csv/20649148/freMTPL2freq.arff\"\r\nif not Path(\"freMTPL2freq.csv\").exists():\r\n  logging.info(\"Loading data file\")\r\n  df = pd.read_csv(path)\r\n  df.to_csv(\"freMTPL2freq.csv\")\r\nelse:\r\n  logging.info(\"Data file already exists\")\r\n  df = pd.read_csv(path)\r\n\r\nlogging.info(\"Opening data\")\r\ndata = tf.data.experimental.make_csv_dataset(\"freMTPL2freq.csv\",\r\n                                             num_rows_for_inference=100,\r\n                                             batch_size=256,\r\n                                             shuffle=False,\r\n                                             sloppy=False,\r\n                                             #num_parallel_reads=4,\r\n                                             num_epochs=1,)\r\n# this is even slower?\r\n#data = tf.data.Dataset.from_tensor_slices(df.to_dict('list'))\r\n\r\nlogging.info(\"Reading data\")\r\n# This part is slower than expected\r\ntmp = list(data.as_numpy_iterator())\r\nlogging.info(\"Done reading data\")\r\nlookup = tf.keras.layers.experimental.preprocessing.StringLookup()\r\nfeature_ds = data.map(lambda x: x['Area'])\r\nlogging.info(\"Starting adapt\")\r\n# This part is also slower than expected\r\nlookup.adapt(feature_ds)\r\nlogging.info(\"Finished adapt\")\r\n\r\n# Commented out IPython magic to ensure Python compatibility.\r\n# %timeit tmp = list(data.as_numpy_iterator())\r\n\r\n# Commented out IPython magic to ensure Python compatibility.\r\n# %timeit lookup.adapt(feature_ds)\r\n\r\n# Commented out IPython magic to ensure Python compatibility.\r\n# %timeit df[\"Area\"].unique()\r\n\r\n# Commented out IPython magic to ensure Python compatibility.\r\n# %timeit pd.read_csv(path)\r\n```", "comments": ["Was able to reproduce the issue with  TF v2.4 and TF-nightly(`2.5.0-dev20210217`). Please find the gist of it [here](https://colab.research.google.com/gist/ravikyram/356c38f0d6431d6c38ffcd18abef29ae/untitled672.ipynb). Thanks!", "I'm not sure this is directly connected, but Stringlookup.adapt() performance gotten significantly worse for me ever since `tf-nightly-2.5.0.dev20210114` to he point where it's looking it's just stuck there.", "I have similar issues when I try to load a parquet file using:\r\n\r\n```\r\nds = tfio.IODataset.from_parquet(...)\r\niter = ds.as_numpy_iterator()\r\n```\r\nIn this case, is it better to use spark to manipulate the data before conversion to a tf dataset for training? \r\n"]}, {"number": 47170, "title": "Can't compile Tensorarray with XLA.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: CPU\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nI have some code that is expensive to compile in XLA (~10 minutes). I am looking for a way to speed up this compilation. I noticed that my codebase produces a lot of computational graph nodes in one of the for loops. Therefore I plan to replace this for-loop with a `tf.while_loop`. In order to do that, I need to use TensorArrays. When I do that the compilation fails.\r\n\r\nIf I understand correctly, I have to build the Tensorarray inside the XLA context due to XLA limitations. I believe the issue might be caused by the fact that on compile time the shape of the tensors have to be known and the compiler assumes all the shapes within the tensoarray will be shared.\r\n\r\nWhen I change the shapes of the tensors in such a way that all of them have the same size, the code compiles fine.\r\n\r\nThen a question arises: Is there a way to construct a `tf.while_loop` (or a similiar construct) with some kind of a data structure so that I don't compile most of the iterations?\r\n\r\n**Describe the expected behavior**\r\n\r\nThe XLA compilation runs smoothly and my compilation time goes down.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nhttps://colab.research.google.com/drive/1gpzRRggNNcuYDnCWy-VLXNvLi3O81tAN?usp=sharing\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\narray = [\r\n    tf.random.uniform((19,1,3,2)),\r\n    tf.random.uniform((19,1,2,2)),\r\n    tf.random.uniform((3,1,3,2)),\r\n    tf.random.uniform((18,1,2,2)),\r\n]\r\n\r\nsize = len(array)\r\n\r\n@tf.function(experimental_compile=True) # The same happens with autograph=False\r\ndef add(array):\r\n    \r\n    tensor_array = tf.TensorArray(\r\n        dtype=tf.float32,\r\n        size=size,\r\n        infer_shape=False,\r\n        element_shape=tf.TensorShape([None, 1, None, 2]),\r\n    )\r\n    for i in range(size):\r\n        tensor_array = tensor_array.write(i, array[i])\r\n    # There would be a while_loop here\r\n\r\nr = add(array)\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```\r\n---------------------------------------------------------------------------\r\n\r\nInternalError                             Traceback (most recent call last)\r\n\r\n<ipython-input-1-c5dfc3274b00> in <module>()\r\n     23     # There would be a while_loop here\r\n     24 \r\n---> 25 r = add(array)\r\n\r\n4 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    826     tracing_count = self.experimental_get_tracing_count()\r\n    827     with trace.Trace(self._name) as tm:\r\n--> 828       result = self._call(*args, **kwds)\r\n    829       compiler = \"xla\" if self._experimental_compile else \"nonXla\"\r\n    830       new_tracing_count = self.experimental_get_tracing_count()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    893       # If we did not create any variables the trace we have is good enough.\r\n    894       return self._concrete_stateful_fn._call_flat(\r\n--> 895           filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access\r\n    896 \r\n    897     def fn_with_cond(inner_args, inner_kwds, inner_filtered_flat_args):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1917       # No tape is watching; skip to running the function.\r\n   1918       return self._build_call_outputs(self._inference_function.call(\r\n-> 1919           ctx, args, cancellation_manager=cancellation_manager))\r\n   1920     forward_backward = self._select_forward_and_backward_functions(\r\n   1921         args,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n    558               inputs=args,\r\n    559               attrs=attrs,\r\n--> 560               ctx=ctx)\r\n    561         else:\r\n    562           outputs = execute.execute_with_cancellation(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nInternalError: Invalid TensorList shape: element_type: TUPLE\r\ntuple_shapes {\r\n  element_type: F32\r\n  dimensions: 4\r\n  dimensions: 19\r\n  dimensions: 1\r\n  dimensions: 3\r\n  dimensions: 2\r\n  layout {\r\n    minor_to_major: 4\r\n    minor_to_major: 3\r\n    minor_to_major: 2\r\n    minor_to_major: 1\r\n    minor_to_major: 0\r\n    format: DENSE\r\n  }\r\n  is_dynamic_dimension: false\r\n  is_dynamic_dimension: false\r\n  is_dynamic_dimension: false\r\n  is_dynamic_dimension: false\r\n  is_dynamic_dimension: false\r\n}\r\ntuple_shapes {\r\n  element_type: S32\r\n  layout {\r\n    format: DENSE\r\n  }\r\n}\r\n, expected: element_type: TUPLE\r\ntuple_shapes {\r\n  element_type: F32\r\n  dimensions: 4\r\n  dimensions: 19\r\n  dimensions: 1\r\n  dimensions: 2\r\n  dimensions: 2\r\n  layout {\r\n    minor_to_major: 4\r\n    minor_to_major: 3\r\n    minor_to_major: 2\r\n    minor_to_major: 1\r\n    minor_to_major: 0\r\n    format: DENSE\r\n  }\r\n  is_dynamic_dimension: false\r\n  is_dynamic_dimension: false\r\n  is_dynamic_dimension: false\r\n  is_dynamic_dimension: false\r\n  is_dynamic_dimension: false\r\n}\r\ntuple_shapes {\r\n  element_type: S32\r\n  layout {\r\n    format: DENSE\r\n  }\r\n}\r\n\r\n\t [[{{node TensorArrayV2Write_1/TensorListSetItem}}]] [Op:__inference_add_26]\r\n```", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/4103de4b0de4fa7c1f3b56a709132fc4/47170.ipynb). Thanks!", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210603, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/4f8e6a971e70d8b5b65cc9363d8c7f96/47170.ipynb). Thanks!"]}, {"number": 47168, "title": "TFLite GPU Delegate Fails ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Google Pixel XL\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.0,  2.5.0-dev20210212\r\n- Python version: 3.6.12\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 7.1.3-1+cuda11.0\r\n- GPU model and memory: NVIDIA Tesla V100, 16GB\r\n\r\n**Describe the current behavior**\r\nI am attempting to use the GPU delegate with a TFLite model on an Android device. I am testing using the benchmark app from: https://www.tensorflow.org/lite/performance/measurement.\r\n\r\nI am able to benchmark performance using the default (None/CPU) and XNNPACK delegates, but am running into an error when attempting to use the GPU delegate (--use_gpu=true). \r\n\r\nWith an undefined (None) batch size, I get the error: \r\n```\r\nAttempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.\r\n```\r\n\r\nWith a batch size of 1 (to avoid dynamic shapes):\r\n```python\r\ninputs = tf.keras.Input(shape=(48,), dtype=tf.int32, batch_size=1)\r\n```\r\n\r\nor forcing the input shape after saving:\r\n``` python\r\nmodel = tf.saved_model.load(save_dir)\r\nconcrete_func = model.signatures[\r\n  tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\r\n\r\n# set to batch size 1 so there are no dynamic shapes\r\nconcrete_func.inputs[0].set_shape([1, 48])\r\n```\r\n\r\nI am getting: \r\n```\r\n02-15 10:04:31.620 32268 32268 I tflite_BenchmarkModelActivity: Running TensorFlow Lite benchmark with args: --graph=/data/local/tmp/test.tflite               --num_threads=4              --use_gpu=true\r\n02-15 10:04:31.628 32268 32268 I tflite  : Initialized TensorFlow Lite runtime.\r\n02-15 10:04:31.630 32268 32268 I tflite  : Created TensorFlow Lite delegate for GPU.\r\n02-15 10:04:31.631 32268 32268 E tflite  : Following operations are not supported by GPU delegate:\r\n02-15 10:04:31.631 32268 32268 E tflite  : DEQUANTIZE:\r\n02-15 10:04:31.631 32268 32268 E tflite  : GATHER: Operation is not supported.\r\n02-15 10:04:31.631 32268 32268 E tflite  : 233 operations will run on the GPU, and the remaining 2 operations will run on the CPU.\r\n02-15 10:04:31.632 32268 32268 E tflite  : TfLiteGpuDelegate Init: ADD: Expected a 3D tensor of shape HxWxC or a 4D tensor of shape 1xHxWxC but got 48x100\r\n02-15 10:04:31.632 32268 32268 I tflite  : Created 0 GPU delegate kernels.\r\n02-15 10:04:31.632 32268 32268 E tflite  : TfLiteGpuDelegate Prepare: delegate is not initialized\r\n02-15 10:04:31.632 32268 32268 E tflite  : Node number 235 (TfLiteGpuDelegateV2) failed to prepare.\r\n02-15 10:04:31.632 32268 32268 E tflite  : Restored original execution plan after delegate application failure.\r\n```\r\n**Describe the expected behavior**\r\nThe GPU delegate does not raise an error, and the delegate is successful.\r\n\r\n**Standalone code to reproduce the issue**\r\n``` python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# from: https://github.com/keras-team/keras-io/blob/master/examples/nlp/text_classification_with_transformer.py\r\nclass TransformerBlock(tf.keras.layers.Layer):\r\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\r\n        super(TransformerBlock, self).__init__()\r\n        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\r\n        self.ffn = tf.keras.Sequential(\r\n            [tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim),]\r\n        )\r\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n        self.dropout1 = tf.keras.layers.Dropout(rate)\r\n        self.dropout2 = tf.keras.layers.Dropout(rate)\r\n\r\n    def call(self, inputs, training):\r\n        attn_output = self.att(inputs, inputs)\r\n        attn_output = self.dropout1(attn_output, training=training)\r\n        out1 = self.layernorm1(inputs + attn_output)\r\n        ffn_output = self.ffn(out1)\r\n        ffn_output = self.dropout2(ffn_output, training=training)\r\n        return self.layernorm2(out1 + ffn_output)\r\n\r\n# from: https://github.com/keras-team/keras-io/blob/master/examples/nlp/text_classification_with_transformer.py\r\nclass TokenAndPositionEmbedding(tf.keras.layers.Layer):\r\n    def __init__(self, maxlen, vocab_size, embed_dim):\r\n        super(TokenAndPositionEmbedding, self).__init__()\r\n        self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\r\n        self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\r\n\r\n    def call(self, x):\r\n        maxlen = tf.shape(x)[-1]\r\n        positions = tf.range(start=0, limit=maxlen, delta=1)\r\n        positions = self.pos_emb(positions)\r\n        x = self.token_emb(x)\r\n        return x + positions\r\n    \r\nX_train = np.ones((10, 48))\r\nX_test = np.ones((10,48))\r\ny_train = np.ones(10)\r\ny_test = np.ones(10)\r\n\r\ninputs = tf.keras.Input(shape=(48,), dtype=tf.int32, batch_size=1)\r\n\r\nembeddings = TokenAndPositionEmbedding(\r\n            maxlen=48, vocab_size=10, embed_dim=100)(inputs)\r\n\r\nsequence_layer = TransformerBlock(100, 12, 16)(embeddings)\r\nglobal_pool = tf.keras.layers.GlobalAveragePooling1D()(sequence_layer)\r\ndense_layer = tf.keras.layers.Dense(16, activation='relu')(global_pool)\r\npred = tf.keras.layers.Dense(2, activation='softmax', name=\"prediction\")(dense_layer)\r\n\r\nmodel = tf.keras.Model(inputs, outputs=pred)\r\n\r\nmodel.compile(optimizer=\"Adam\",\r\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\r\n              metrics=[\"accuracy\"],\r\n             )\r\n\r\nmodel.fit(\r\n    x=X_train,\r\n    y=y_train,\r\n    epochs=1,\r\n    steps_per_epoch=10,\r\n    verbose=1,\r\n)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_types = [tf.float16]\r\n\r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS,\r\n]\r\n\r\ntflite_model = converter.convert()\r\n\r\nwith open('models/test.tflite', 'wb') as f:\r\n    f.write(tflite_model)\r\n```\r\n", "comments": ["colab: https://colab.research.google.com/gist/teijeong/502d046bda89a20675628928becfd01e/47168.ipynb\r\n", "Ran benchmark with generated model with Galaxy S10e (Exynos) and Pixel 2 XL, and both works fine with latest benchmark apk. Can you make sure if your TFLite runtime is up to date?", "I redownloaded the latest apk (https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/android_aarch64_benchmark_model.apk) and tested on Pixel XL and Pixel 3a, both gave the following error:\r\n\r\n```\r\n02-16 09:37:17.374  8474  8474 I tflite_BenchmarkModelActivity: Running TensorFlow Lite benchmark with args: --graph=/data/local/tmp/test.tflite               --num_threads=4              --use_gpu=true\r\n02-16 09:37:17.398  8474  8474 I tflite  : Initialized TensorFlow Lite runtime.\r\n02-16 09:37:17.403  8474  8474 I tflite  : Created TensorFlow Lite delegate for GPU.\r\n02-16 09:37:17.405  8474  8474 E tflite  : Following operations are not supported by GPU delegate:\r\n02-16 09:37:17.405  8474  8474 E tflite  : DEQUANTIZE:\r\n02-16 09:37:17.405  8474  8474 E tflite  : GATHER: Operation is not supported.\r\n02-16 09:37:17.405  8474  8474 E tflite  : 243 operations will run on the GPU, and the remaining 2 operations will run on the CPU.\r\n02-16 09:37:17.407  8474  8474 E tflite  : TfLiteGpuDelegate Init: ADD: Expected a 3D tensor of shape HxWxC or a 4D tensor of shape 1xHxWxC but got 48x100\r\n02-16 09:37:17.407  8474  8474 I tflite  : Created 0 GPU delegate kernels.\r\n02-16 09:37:17.407  8474  8474 E tflite  : TfLiteGpuDelegate Prepare: delegate is not initialized\r\n02-16 09:37:17.407  8474  8474 E tflite  : Node number 245 (TfLiteGpuDelegateV2) failed to prepare.\r\n```\r\n\r\nwhen using command:\r\n```\r\nadb shell am start -S \\\r\n  -n org.tensorflow.lite.benchmark/.BenchmarkModelActivity \\\r\n  --es args '\"--graph=/data/local/tmp/test.tflite \\\r\n              --num_threads=4\\\r\n              --use_gpu=true\"'\r\n```\r\n\r\nAnd using a model with a fixed (1, 48) input size.", "Many of the GPU ops are optimized for the image domain and works in a 3D mode with width height channel dimensions.  You are doing text classification with a 2D input tensor (that's what I assume from the error message; I haven't generated or taken a look at the .tflite model).  Is it possible to insert a RESHAPE op to convert 48x100 to e.g. 1x48x100 ?", "I've tried using both `reshape` and a combination of `expand_dims` and `squeeze` (looking at: https://www.tensorflow.org/mlir/tfl_ops for compatibility). I am able to get both approaches to convert using `tf.lite.OpsSet.SELECT_TF_OPS`\r\n\r\nI'm not able to test it in the flex delegate apk (https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/android_aarch64_benchmark_model_plus_flex.apk), as I'm getting an error:\r\n```\r\n02-17 10:19:39.294 19111 19111 I tflite_BenchmarkModelActivity: Running TensorFlow Lite benchmark with args: --graph=/data/local/tmp/test.tflite               --num_threads=4              --use_gpu=true\r\n02-17 10:19:39.307 19111 19111 E AndroidRuntime: Process: org.tensorflow.lite.benchmark, PID: 19111\r\n02-17 10:19:39.307 19111 19111 E AndroidRuntime: java.lang.UnsatisfiedLinkError: dalvik.system.PathClassLoader[DexPathList[[zip file \"/data/app/org.tensorflow.lite.benchmark-gjEn3C-N8sQzuah5t0ZG-w==/base.apk\"],nativeLibraryDirectories=[/data/app/org.tensorflow.lite.benchmark-gjEn3C-N8sQzuah5t0ZG-w==/lib/arm64, /system/fake-libs64, /data/app/org.tensorflow.lite.benchmark-gjEn3C-N8sQzuah5t0ZG-w==/base.apk!/lib/arm64-v8a, /system/lib64, /vendor/lib64, /system/product/lib64]]] couldn't find \"libtensorflowlite_benchmark.so\"\r\n02-17 10:19:39.307 19111 19111 E AndroidRuntime: \tat org.tensorflow.lite.benchmark.BenchmarkModel.<clinit>(BenchmarkModel.java:21)\r\n02-17 10:19:39.307 19111 19111 E AndroidRuntime: \tat org.tensorflow.lite.benchmark.BenchmarkModel.run(BenchmarkModel.java:28)\r\n02-17 10:19:39.307 19111 19111 E AndroidRuntime: \tat org.tensorflow.lite.benchmark.BenchmarkModelActivity.onCreate(BenchmarkModelActivity.java:46)\r\n02-17 10:19:39.317   926  3811 W ActivityTaskManager:   Force finishing activity org.tensorflow.lite.benchmark/.BenchmarkModelActivity\r\n02-17 10:19:39.349   926  1412 I ActivityManager: Process org.tensorflow.lite.benchmark (pid 19111) has died: prcp TPSL\r\n02-17 10:19:39.593   926  1013 W ActivityTaskManager: Activity top resumed state loss timeout for ActivityRecord{e2543f1 u0 org.tensorflow.lite.benchmark/.BenchmarkModelActivity t-1 f}\r\n```\r\n", "@jasonw247 Hi try to use 2.3.0 gpu delegate. It looks like bug in 2.4.0 and 2.5.0. I have the same error.\r\nlooks like the same issue as I posted https://github.com/tensorflow/tensorflow/issues/45845", "BTW, if you're using tf-nightly you can use Model Analyzer API to check which node has the compatibility issue.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.lite.experimental.Analyzer.analyze(model_path='model.tflite', gpu_compatibility=True)\r\n```"]}, {"number": 47162, "title": "Compiled metrics cannot deal with dictionaries in update_state", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: 3.7.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.0/8.1.0\r\n- GPU model and memory: RTX2080Ti 11GB\r\n\r\n**Describe the current behavior**\r\nI have written a custom metric, subclassing from tf.keras.metrics.Metric. I also have a custom model,\r\nsubclassing from tf.keras.Model. I compile the model, passing this custom metric. When doing a single train step,\r\nI update the state of the custom metric with prediction and target as **dictionaries**.\r\n\r\nHowever, the autograph operation turns these dictionaries into lists, making it impossible for me to access the right item in the dictionary within the metric.\r\n\r\nAn error is thrown:\r\n\r\n```\r\nFile \"bug.py\", line 41, in <module>\r\n    model.train_step([])\r\n  File \"bug.py\", line 17, in train_step\r\n    self.compiled_metrics.update_state(y_true, y_pred)\r\n  File \"/home/mhueting/.pyenv/versions/tts_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/compile_utils.py\", line 408, in update_state\r\n    metric_obj.update_state(y_t, y_p, sample_weight=mask)\r\n  File \"/home/mhueting/.pyenv/versions/tts_env/lib/python3.7/site-packages/tensorflow/python/keras/utils/metrics_utils.py\", line 90, in decorated\r\n    update_op = update_state_fn(*args, **kwargs)\r\n  File \"/home/mhueting/.pyenv/versions/tts_env/lib/python3.7/site-packages/tensorflow/python/keras/metrics.py\", line 177, in update_state_fn\r\n    return ag_update_state(*args, **kwargs)\r\n  File \"/home/mhueting/.pyenv/versions/tts_env/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 670, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nTypeError: in user code:\r\n\r\n    bug.py:27 update_state  *\r\n        self.value.assign_add((y_true[\"a\"] - y_pred[\"a\"]) ** 2)\r\n    /home/mhueting/.pyenv/versions/tts_env/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\r\n        return target(*args, **kwargs)\r\n    /home/mhueting/.pyenv/versions/tts_env/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1009 _slice_helper\r\n        _check_index(s)\r\n    /home/mhueting/.pyenv/versions/tts_env/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:886 _check_index\r\n        raise TypeError(_SLICE_TYPE_ERROR + \", got {!r}\".format(idx))\r\n\r\n    TypeError: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got 'a'\r\n```\r\n\r\n**Describe the expected behavior**\r\nI expect the dictionaries passed to the compiled metric container's `update_state` function to be preserved as dictionaries instead of converted into keyless lists.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nhttps://colab.research.google.com/drive/1DCYM2W3C5H2StxrdPeqqsvVGBtbVfNfJ?usp=sharing\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nclass Model(tf.keras.Model):\r\n    def __init__(self, **kwargs):\r\n        super().__init__(**kwargs)\r\n\r\n    def call(self, x, training=True):\r\n        # this model returns a dictionary of values\r\n        return {\r\n            \"a\": tf.constant([5]),\r\n            \"b\": tf.constant([5]),\r\n        }\r\n\r\n    def train_step(self, data):\r\n        # each sample also consists of a dictionary of values\r\n        y_true = {\"a\": tf.constant([3]),\r\n                  \"b\": tf.constant([4])}\r\n        y_pred = self(4)\r\n\r\n        # the metrics on this compiled metric container each care about\r\n        # only one of the values in the dictionaries y_true/y_pred\r\n        self.compiled_metrics.update_state(y_true, y_pred)\r\n\r\n        return {m.name: m.result() for m in self.metrics}\r\n\r\nclass MetricForA(tf.keras.metrics.Metric):\r\n    def __init__(self, name=\"metricForA\", **kwargs):\r\n        super(MetricForA, self).__init__(name=name, **kwargs)\r\n        self.value = self.add_weight(name='value', initializer='zeros')\r\n\r\n    def update_state(self, y_true, y_pred, sample_weight=None):\r\n        self.value.assign_add((y_true[\"a\"] - y_pred[\"a\"]) ** 2)\r\n\r\n    def result(self):\r\n        return self.value\r\n\r\n    def reset_states(self):\r\n        self.value.assign(0)\r\n\r\nclass MetricForB(tf.keras.metrics.Metric):\r\n    def __init__(self, name=\"metricForB\", **kwargs):\r\n        super(MetricForB, self).__init__(name=name, **kwargs)\r\n        self.value = self.add_weight(name='value', initializer='zeros')\r\n\r\n    def update_state(self, y_true, y_pred, sample_weight=None):\r\n        self.value.assign_add((y_true[\"b\"] - y_pred[\"b\"]) ** 2)\r\n\r\n    def result(self):\r\n        return self.value\r\n\r\n    def reset_states(self):\r\n        self.value.assign(0)\r\n\r\nif __name__ == \"__main__\":\r\n    model = Model()\r\n    metrics = [MetricForA(), MetricForB()]\r\n\r\n    model.compile(metrics=metrics)\r\n\r\n    # this results in an error, showing us that the dictionaries passed to update_state\r\n    # are no longer dictionaries\r\n    model.train_step([])\r\n```", "comments": ["@mh-synth \r\nI ran the code and face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/5a398d5d487ecccbbb5a12b857e5955a/untitled537.ipynb)", "Apologies - updated the colab and the standalone code. Running the Colab in its entirety now gives the error.", "I am able to replicate the issue reported on tf 2.4 and nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/fbfb06b8cd1b0837bded70f7b86de31f/untitled544.ipynb).", "I am able to replicate the issue reported on [TF nightly \r\n](https://colab.research.google.com/gist/sushreebarsa/f115121b5bf99be036d376841c401d8f/untitled120.ipynb?authuser=1#scrollTo=xa4YZlORuhmf)and [TF v2.5 ](https://colab.research.google.com/gist/sushreebarsa/868ec754d9ca3a8ef4b7ec444602e223/untitled121.ipynb?authuser=1)..Thanks !\r\n\r\n"]}, {"number": 47161, "title": "Keras fails to load saved model / properly infer dtypes in `tf.math.maximum`", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur 11.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n- Python version: 3.8.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nKeras fails to load model due to problems with inferring data types in `tf.math.maximum`. In particular: take the input to the network to be `float32`, then cast the tensor to `float64` and feed into maximum layer (with the second input to that layer being `float64` constant tensor) - creating this model is successful. Save that model (below I used SavedModel format, but .h5 case is similar) and try to load it - error is raised.\r\n\r\nMoreover, the error is raised only when the constant tensor is passed as first input to `tf.math.maxiumum` and does not occur if it is passed as the second input - see the Colab notebook I attached below.\r\n\r\nI believe this is strictly Keras-related, as I was able to successfully convert this failing-to-load model to to .tflite version (with proper options of converter set, using `from_saved_model` method) and this tflite model works and its' data types are correct.\r\n\r\n**Describe the expected behavior**\r\nThe model should load properly.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nThis code snipper should reproduce the issue:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\ninp = keras.Input(shape=(1))\r\nx = tf.cast(inp, dtype=tf.float64)\r\na = tf.constant(1.0, dtype=tf.float64)\r\nx = tf.maximum(a, x)\r\nout = tf.cast(x, dtype=tf.float32)\r\n\r\nmodel = keras.models.Model(inp, out)\r\nmodel.summary()\r\n\r\nmodel.save('dummy_model')\r\ndel model\r\n\r\nloaded_model = tf.keras.models.load_model('dummy_model')\r\n```\r\n\r\nBTW: When using `x = tf.maximum(x, a)` instead of `x = tf.maximum(a, x)` in the example above, the error is not raised!\r\n\r\nSee also slightly more elaborate [Colab notebook](https://colab.research.google.com/drive/1ysEjTkfkCDUzMQDrlbRGPfP8d8M5VZu-?usp=sharing).\r\n\r\n**Other info / logs** \r\n```python                                                                  \r\n2021-02-15 12:45:01.772147: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-15 12:45:01.772422: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 1)]               0         \r\n_________________________________________________________________\r\ntf.cast (TFOpLambda)         (None, 1)                 0         \r\n_________________________________________________________________\r\ntf.math.maximum (TFOpLambda) (None, 1)                 0         \r\n_________________________________________________________________\r\ntf.cast_1 (TFOpLambda)       (None, 1)                 0         \r\n=================================================================\r\nTotal params: 0\r\nTrainable params: 0\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n2021-02-15 12:45:01.835730: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nTraceback (most recent call last):\r\n  File \"/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\", line 517, in _apply_op_helper\r\n    values = ops.convert_to_tensor(\r\n  File \"/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py\", line 163, in wrapped\r\n    return func(*args, **kwargs)\r\n  File \"/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1507, in convert_to_tensor\r\n    raise ValueError(\r\nValueError: Tensor conversion requested dtype float32 for Tensor with dtype float64: <tf.Tensor 'Placeholder:0' shape=(None, 1) dtype=float64>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"tmp.py\", line 32, in <module>\r\n    loaded_model = tf.keras.models.load_model('dummy_model')\r\n  File \"/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py\", line 212, in load_model\r\n    return saved_model_load.load(filepath, compile, options)\r\n  File \"/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\", line 147, in load\r\n    keras_loader.finalize_objects()\r\n  File \"/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\", line 600, in finalize_objects\r\n    self._reconstruct_all_models()\r\n  File \"/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\", line 619, in _reconstruct_all_models\r\n    self._reconstruct_model(model_id, model, layers)\r\n  File \"/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\", line 665, in _reconstruct_model\r\n    created_layers) = functional_lib.reconstruct_from_config(\r\n  File \"/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py\", line 1285, in reconstruct_from_config\r\n    process_node(layer, node_data)\r\n  File \"/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py\", line 1233, in process_node\r\n    output_tensors = layer(input_tensors, **kwargs)\r\n  File \"/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 951, in __call__\r\n    return self._functional_construction_call(inputs, args, kwargs,\r\n  File \"/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1090, in _functional_construction_call\r\n    outputs = self._keras_tensor_symbolic_call(\r\n  File \"/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 822, in _keras_tensor_symbolic_call\r\n    return self._infer_output_signature(inputs, args, kwargs, input_masks)\r\n  File \"/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 863, in _infer_output_signature\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py\", line 1327, in _call_wrapper\r\n    return self._call_wrapper(*args, **kwargs)\r\n  File \"/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py\", line 1359, in _call_wrapper\r\n    result = self.function(*args, **kwargs)\r\n  File \"/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 5704, in maximum\r\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\r\n  File \"/Users/mikolajpabiszczak/.pyenv/versions/abnormal-sounds-lite-p3.8.6-tf2.4/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\", line 555, in _apply_op_helper\r\n    raise TypeError(\r\nTypeError: Input 'y' of 'Maximum' Op has type float64 that does not match type float32 of argument 'x'.\r\n```\r\n", "comments": ["Facing an error stating `ValueError: 2 errors while building NodeDef 'tf_op_layer_Maximum/Maximum' using Op<name=Maximum; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT16, DT_INT32, DT_INT64]>:\r\n` on running the code with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/999f218f6988a8ff13bd612c9bb13d5b/47161-2-3.ipynb).\r\n\r\nWas able to reproduce the issue with [TF v2.4](https://colab.research.google.com/gist/amahendrakar/3c0d2e488038d2a2c9e67134fbfdbb0a/47161.ipynb) and TF-nightly. \r\n\r\n\r\nPlease check the linked gist for reference. Thanks!", "I was able to reproduce the issue with tf v2.5  ,Please check the [gist](https://colab.research.google.com/gist/sushreebarsa/e65bd8a0d69eca872a7ed655ace5eab5/untitled119.ipynb?authuser=1) ..Thanks !", "Just bumping here: it still persists."]}, {"number": 47134, "title": "[tflite] Problem reading multi-input tflite model with Java", "body": "Hi,\r\n\r\nwith a colleague of mine, we've build a two-input model using TF2's Functional API. We want to deploy this model in Android, using `tflite` and Java.\r\n\r\n## Model\r\n\r\nThe model has the following architecture\r\n\r\n```{python}\r\n# Inputs\r\nin1 = keras.Input(shape = (24, 7), name = 'in_1')\r\nin2 = keras.Input(shape = (2,), name = 'in_2')\r\n\r\n# Model\r\nx1 = keras.layers.LSTM(48, return_sequences = True)(in1)\r\nx1 = keras.layers.LSTM(48)(x1)\r\n\r\nx2 = keras.layers.Dense(16, activation = 'relu')(in2)\r\nx2 = keras.layers.Dense(8, activation = 'relu')(x2)\r\n\r\nx = keras.layers.concatenate([x1, x2], name = 'concat_00')\r\n\r\nx = keras.layers.Dense(128, activation = 'relu', name = 'concat_dense_00')(x)\r\nx = keras.layers.Dropout(.2)(x)\r\nx = keras.layers.Dense(64, activation = 'relu', name = 'concat_dense_02')(x)\r\nx = keras.layers.Dropout(.2)(x)\r\noutput = keras.layers.Dense(12)(x)\r\n\r\nmodel_lstm_3 = keras.Model(inputs = [in1, in2], outputs = output)\r\n```\r\n\r\n## `tflite` conversion\r\n\r\nThe model had been converted to `tflite` format, using TF 2.3 and the following code:\r\n\r\n```{python}\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model_lstm_3)\r\ntflite_model = converter.convert()\r\n````\r\n\r\n## Reading the model in Java\r\n\r\nWe're trying to read in the model in Java using the following code:\r\n\r\n```{java}\r\ntflite = Interpreter(loadModelFile(activity, \"model_simple.tflite\"))\r\n\r\nvar output = arrayOf(0f, 0f, 0f, 0f, 0f, 0f, 0f, 0f, 0f, 0f, 0f, 0f)\r\nval geoData = arrayOf(arrayOf(arrayOf(52.26240269140683f, 20.971026685407484f)))\r\nval historicData = arrayOf(arrayOf(\r\n    // arrayListOf(....),\r\n    arrayOf(1f, 1f, 0.0f, 1f, 1f, 1f, 2020f),\r\n    arrayOf(1f, 1f, 1.0f, 1f, 1f, 1f, 2020f),\r\n    arrayOf(1f, 1f, 2.0f, 1f, 1f, 1f, 2020f),\r\n    arrayOf(1f, 1f, 3.0f, 1f, 1f, 1f, 2020f),\r\n    arrayOf(1f, 1f, 4.0f, 1f, 1f, 1f, 2020f),\r\n    arrayOf(1f, 1f, 5.0f, 1f, 1f, 1f, 2020f),\r\n    arrayOf(1f, 1f, 6.0f, 1f, 1f, 1f, 2020f),\r\n    arrayOf(1f, 1f, 7.0f, 1f, 1f, 1f, 2020f),\r\n    arrayOf(1f, 1f, 8.0f, 1f, 1f, 1f, 2020f),\r\n    arrayOf(1f, 1f, 9.0f, 1f, 1f, 1f, 2020f),\r\n    arrayOf(1f, 1f, 10.0f, 1f, 1f, 1f, 2020f),\r\n    arrayOf(1f, 1f, 11.0f, 1f, 1f, 1f, 2020f),\r\n    arrayOf(1f, 1f, 12.0f, 1f, 1f, 1f, 2020f),\r\n    arrayOf(1f, 1f, 13.0f, 1f, 1f, 1f, 2020f),\r\n    arrayOf(1f, 1f, 14.0f, 1f, 1f, 1f, 2020f),\r\n    arrayOf(1f, 1f, 15.0f, 1f, 1f, 1f, 2020f),\r\n    arrayOf(1f, 1f, 16.0f, 1f, 1f, 1f, 2020f),\r\n    arrayOf(1f, 1f, 17.0f, 1f, 1f, 1f, 2020f),\r\n    arrayOf(1f, 1f, 18.0f, 1f, 1f, 1f, 2020f),\r\n    arrayOf(1f, 1f, 19.0f, 1f, 1f, 1f, 2020f),\r\n    arrayOf(1f, 1f, 20.0f, 1f, 1f, 1f, 2020f),\r\n    arrayOf(1f, 1f, 21.0f, 1f, 1f, 1f, 2020f),\r\n    arrayOf(1f, 1f, 22.0f, 1f, 1f, 1f, 2020f),\r\n    arrayOf(1f, 1f, 23.0f, 1f, 1f, 1f, 2020f),\r\n))\r\nvar floatInput = arrayOf<Array<Array<Array<Float>>>>(geoData, historicData)\r\n\r\nTimber.d(\"Input model $floatInput\")\r\ntflite!!.run(floatInput, output)\r\nTimber.d(\"Output model $output\")\r\n```\r\n\r\n## Exception\r\n\r\nThe line `tflite!!.run(floatInput, output)` generates the following exception:\r\n\r\n```{java}\r\njava.lang.IllegalArgumentException: DataType error: cannot resolve DataType of [[[[Ljava.lang.Float;\r\n        at org.tensorflow.lite.Tensor.dataTypeOf(Tensor.java:352)\r\n        at org.tensorflow.lite.Tensor.throwIfTypeIsIncompatible(Tensor.java:418)\r\n        at org.tensorflow.lite.Tensor.getInputShapeIfDifferent(Tensor.java:287)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:146)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:360)\r\n        at org.tensorflow.lite.Interpreter.run(Interpreter.java:319)\r\n```\r\n\r\n## Java libs\r\n\r\n```\r\nimplementation('org.tensorflow:tensorflow-lite:2.4.0')\r\nimplementation('org.tensorflow:tensorflow-lite-gpu:2.4.0')\r\nimplementation('org.tensorflow:tensorflow-lite-support:0.0.0-nightly')\r\n```\r\n\r\nWe're not sure how to solve this and we'll be grateful for any hints.\r\n\r\nThank you!\r\n\r\n\r\n\r\n", "comments": ["You might want to use `runForMultipleInputsOutputs` instead of `run`.", "@teijeong - thank you, we'll check this option.", "Sadly there is still an issue:\r\njava.lang.IllegalArgumentException: DataType error: cannot resolve DataType of [[[Ljava.lang.Float;\r\n\r\n![image](https://user-images.githubusercontent.com/2962633/108109758-a161a800-7092-11eb-9194-7ee8ebc8013a.png)\r\n", "@teijeong, unfortunately `runForMultipleInputsOutputs` did not solve the problem.\r\n\r\nI also exported the model once again, usinf TF 2.4.1, but the outcome is the same.\r\n\r\nAny other suggestions?"]}, {"number": 47116, "title": "Custom Model Data Cardinality Check Ambiguous", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.2/2.4.1\r\n- Python version: 3.7.8\r\n\r\n**Describe the current behavior**\r\n\r\nTraining Keras custom model when inputs are lists fail data cardinality check if leading axis is not batch. The code below works in 2.3.2 but not 2.4.1. An example of why you would not want a leading axis as batch is a per-batch weight or dynamic parameter (see example code). \r\n\r\n**Describe the expected behavior**\r\n\r\nThis worked in 2.3.2 but was changed. I read the release notes and cannot seem to understand why this behavior changed. I am unsure how to fix. Either a flag to remove this check, a way to change the input spec of model call, or clarification on how to allow inputs that do not have a leading batch axis would help me fix this problem. \r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass MyModel(tf.keras.Model):    \r\n    def call(self, inputs, training):\r\n        # make a model with a batched input (x) and per-batch tensor (s)\r\n        x = inputs[0]\r\n        s = inputs[1]\r\n        return tf.reduce_mean(x * s, axis=1)\r\n    \r\nm = MyModel()\r\nx = np.ones((10, 2))\r\ns = np.ones(1)\r\ny = np.ones(10)\r\n\r\nm.compile('sgd', loss='mean_squared_error')\r\n\r\n# works as call\r\nm([x, s])\r\n\r\n# fails on TF 2.4.1\r\n# succeeds on TF 2.3.2\r\nm.train_on_batch(x=[x, s], y=y)\r\n```\r\n\r\n** Error Message**\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-5-1e85b8f34d3e> in <module>\r\n     21 # fails on 2.4.1\r\n     22 # succeeds on 2.3.2\r\n---> 23 m.train_on_batch(x=[x, s], y=y)\r\n\r\n~/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\r\n   1723       iterator = data_adapter.single_batch_iterator(self.distribute_strategy, x,\r\n   1724                                                     y, sample_weight,\r\n-> 1725                                                     class_weight)\r\n   1726       self.train_function = self.make_train_function()\r\n   1727       logs = self.train_function(iterator)\r\n\r\n~/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py in single_batch_iterator(strategy, x, y, sample_weight, class_weight)\r\n   1511     data = (x, y, sample_weight)\r\n   1512 \r\n-> 1513   _check_data_cardinality(data)\r\n   1514   dataset = dataset_ops.DatasetV2.from_tensors(data)\r\n   1515   if class_weight:\r\n\r\n~/miniconda3/envs/mmm/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py in _check_data_cardinality(data)\r\n   1527           label, \", \".join(str(i.shape[0]) for i in nest.flatten(single_data)))\r\n   1528     msg += \"Make sure all arrays contain the same number of samples.\"\r\n-> 1529     raise ValueError(msg)\r\n   1530 \r\n   1531 \r\n\r\nValueError: Data cardinality is ambiguous:\r\n  x sizes: 10, 1\r\n  y sizes: 10\r\nMake sure all arrays contain the same number of samples.\r\n```\r\n", "comments": ["I have replicated the issue on tf 2.4 and nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/fc90a75ba92287cc74a01f38ec994b5f/untitled534.ipynb).", "@whitead \r\nI ran the code shared on tf 2.3 and face a different error whereas the issue description says the code works fine on tf 2.3, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/afb7bb1cdb128f68bed920a2c2aa0308/untitled536.ipynb).", "Sorry about that @Saduf2019. I've edited the post for the shape error. I've attached output for 2.3.2 below and [here is a gist](https://colab.research.google.com/gist/whitead/450901fc91f614de682ae650a1759b91/untitled536.ipynb)\r\n\r\n![image](https://user-images.githubusercontent.com/908389/107965494-9a467700-6f78-11eb-8499-094310addc44.png)\r\n", "@whitead\r\nYou can safely ignore the warning, to suppress the warnings please use.\r\n\r\n```import os```\r\n```os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  ```\r\n```import tensorflow as tf```", "Hi @Saduf2019 that was the message from 2.3.2. I will repost the error message for 2.4.1 below. My issue is that the code works in 2.3.2 but not in 2.4.1 and I do not know how to fix. \r\n\r\n![image](https://user-images.githubusercontent.com/908389/108085037-e362fd80-7042-11eb-92cb-4dd288ccceac.png)\r\n", "Issue still exists in TF 2.5. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/30286a532a5655003761b720cbcba5b0/untitled536.ipynb).Thanks!"]}, {"number": 47110, "title": "TF Lite Benchmark randomly throws errors.", "body": "Hi,\r\n\r\nI'm using TF Lite benchmark to benchmark a model we use internally in company (therefore I can't share it, sorry). Unfortunately when trying to run the model with given command:\r\n\r\n`\r\n./benchmark_model --graph=models/fp-full.v3.tflite --input_layer=\"in1,in2,in3,in4\" --input_layer_shape=\"1,50:1:1:1,10\" \u2014input_layer_value_range=\u201cin1,30,30:in2,0,0:in3,0,0:in4,0,0\"        \r\n`\r\n\r\nThe benchmark randomly either runs correctly or throws the given error:\r\n\r\n`ERROR: tensorflow/lite/kernels/range.cc:45 (start > limit && delta < 0) || (start < limit && delta > 0) was not true.\r\nERROR: Node number 198 (RANGE) failed to invoke.`\r\n\r\nAs you see in the command I've specified the range of values to single value (I've also tried other values and it always ended the same, with randomly throwing the error) therefore there should be nothing random when running the model, but still it randomly fails. \r\n\r\nCould you please give me some tips what might be causing this behaviour? \r\n\r\nThe inputs to the model are as follows (checked in Netron):\r\n**in1**: int32[1,1]\r\n**in2**: int32[1]\r\n**in3**: float32[1]\r\n**in4**: float32[1,1]\r\n\r\nI use Tensorflow 2.3.1 for both model and benchmark.\r\n", "comments": ["@Jkeezuz \r\nplease refer to similar resolved issues and let us know, #35661, #40504", "Hi @Jkeezuz \r\n\r\nThe TensorFlow Lite benchmark tools feeds random input data to the model. It has a constraint that it only works if the model can handle arbitrary data. It seems your model contains a number of integer ID inputs, which are only valid in a specific range. In this case, you may need to implement your own benchmark instead of using the existing tool. \r\n\r\nLet us know if you have any further questions. ", "Thanks @Saduf2019, I will take a look at this.\r\n@miaout17 but as far as I'm aware I can specify the input ranges with --input_layer_value_range, right? And I even do this in the command in the post and I specify the range to be single value range.\r\n\r\nI've also tried to run the benchmark with --input_layer_value_files argument providing fixed input as files, but the benchmark fails with segmentation fault. \r\n\r\nAs far as I understand with given input to the model: \r\ninput_ids = [1, 48, 51, 84, 2, 61, 54, 2, 56, 41, 2, 18, 63, 74, 41, 10]\r\nI should be able to run the model if I encode those values as binary as in attached file, right?\r\n\r\n[input_ids_4b_new.bin.zip](https://github.com/tensorflow/tensorflow/files/5995835/input_ids_4b_new.bin.zip)\r\n"]}]