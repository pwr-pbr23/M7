[{"number": 40970, "title": "Added \"Get Started with CNNs\" example", "body": "Added a \"[Getting Started with CNN](https://github.com/Rishit-dagli/tensorflow/tree/get-started-with-cnn/tensorflow/examples/get_started/get-started-with-cnn)\" example under the [get_started](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/get_started) section. I have made sure that this example is strictly a getting started example with TensorFlow 2.x .\r\n\r\nThe example contains-\r\n\r\n- What is a CNN?\r\n- Loading the data\r\n  * The Fashion MNIST dataset\r\n- Some preprocessing\r\n- Understanding Filters and Convolutions\r\n- Performing simple feature extraction with filters\r\n  * Identifying Vertical lines\r\n- Understanding Pooling\r\n- Implementing Convolutional layers in TensorFlow\r\n  * Convolutions\r\n  * Pooling\r\n  * Accuracy and loss curves\r\n  * Test accuracy\r\n- Visualizing the Convolutions and Pooling\r\n- Save the model for future use", "comments": ["Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/40970\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n Review Jupyter notebook visual diffs & provide feedback on notebooks. \n\n---\n\n <i>Powered by <a href='https://www.reviewnb.com'>ReviewNB</a></i>", "The commit [`f5d8597`](https://github.com/tensorflow/tensorflow/pull/40970/commits/f5d859777b3340319d44cc4f8897cad9039d0ddd) was a mistake from my end as mentioned [here](https://github.com/tensorflow/tensorflow/pull/40828#issuecomment-651910995) I messed up two branches in my TF fork. I have however fixed it.", "We already have a tutorial here: https://www.tensorflow.org/tutorials/keras/classification demonstrating how to do that. Also there are many other guides/tutorials that explain.\r\n\r\nCan you please tell why do you want to add this example? ", "Agree with @yashk2810 \r\nRegardless, tutorials and examples should go in either the https://github.com/tensorflow/docs or https://github.com/tensorflow/examples repo so closing this issue.", "> We already have a tutorial here: https://www.tensorflow.org/tutorials/keras/classification demonstrating how to do that. Also there are many other guides/tutorials that explain.\r\n> \r\n> Can you please tell why do you want to add this example?\r\n\r\n@yashk2810 \r\nI am aware of this tutorial however this tutorial does not demonstrate using CNNs with TensorFlow. I feel this could be a good place for beginners to get started with CNNs in TF.", "> Agree with @yashk2810\r\n> Regardless, tutorials and examples should go in either the https://github.com/tensorflow/docs or https://github.com/tensorflow/examples repo so closing this issue.\r\n\r\nI feel this would be a good fit under the [get_started](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/get_started) section in [examples](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples) in this repo"]}, {"number": 40969, "title": "Missing \"Connected to\" column in model summary", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 18.04.2 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nBinary\r\n- TensorFlow version (use command below):\r\nv2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version:\r\nPython 3.7.3\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\n10.2\r\n- GPU model and memory:\r\nTITAN X (Pascal), ~12GB\r\n\r\n**Describe the current behavior**\r\n`model.summary()` output is missing column `Connected to`\r\n\r\n**Describe the expected behavior**\r\nit should contain the column\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\ndef res_net_block(shape):\r\n    filters = shape[-1]\r\n\r\n    inputs = tf.keras.layers.Input(shape)\r\n    x = tf.keras.layers.Conv3D(filters=filters, kernel_size=3, padding='same')(inputs)\r\n    outputs = x + inputs\r\n\r\n    return tf.keras.Model(inputs, outputs)\r\n\r\ndef encoder(shape):\r\n    kernel_size = 3\r\n    strides = 2\r\n\r\n    inputs = tf.keras.layers.Input(shape)\r\n    outputs = res_net_block(inputs.shape[1:])(inputs)\r\n\r\n    return tf.keras.Model(inputs, outputs)\r\n\r\nshape = [256,256,128,1]\r\nmodel = encoder(shape)\r\nmodel.summary()\r\n```\r\nSummary output looks like below:\r\n```\r\nModel: \"model_36\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_42 (InputLayer)        [(None, 256, 256, 128, 1) 0         \r\n_________________________________________________________________\r\nmodel_35 (Model)             (None, 256, 256, 128, 1)  28        \r\n=================================================================\r\nTotal params: 28\r\nTrainable params: 28\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```\r\n\r\n**Other info / logs**\r\nInterestingly, if I move the addition out of `res_net_block` into `encoder`, display is as expected.\r\n```\r\ndef res_net_block(shape):\r\n    filters = shape[-1]\r\n\r\n    inputs = tf.keras.layers.Input(shape)\r\n    outputs = tf.keras.layers.Conv3D(filters=filters, kernel_size=3, padding='same')(inputs)\r\n\r\n    return tf.keras.Model(inputs, outputs)\r\n\r\ndef encoder(shape):\r\n    kernel_size = 3\r\n    strides = 2\r\n\r\n    inputs = tf.keras.layers.Input(shape)\r\n    x = res_net_block(inputs.shape[1:])(inputs)\r\n    outputs = x + inputs # This addition is not here in previous code, instead it's inside `res_net_block` function\r\n\r\n    return tf.keras.Model(inputs, outputs)\r\n```\r\nOutput with above function definitions:\r\n```\r\nModel: \"model_38\"\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_44 (InputLayer)           [(None, 256, 256, 12 0                                            \r\n__________________________________________________________________________________________________\r\nmodel_37 (Model)                (None, 256, 256, 128 28          input_44[0][0]                   \r\n__________________________________________________________________________________________________\r\ntf_op_layer_AddV2_20 (TensorFlo [(None, 256, 256, 12 0           model_37[1][0]                   \r\n                                                                 input_44[0][0]                   \r\n==================================================================================================\r\nTotal params: 28\r\nTrainable params: 28\r\nNon-trainable params: 0\r\n__________________________________________________________________________________________________\r\n```", "comments": ["@MasterJEET,\r\nPlease take a look at [this](https://stackoverflow.com/a/54465485) similar StackOverflow comment and let us know if it resolves your query. Thanks!", "I see, this makes sense. When the addition is inside the `res_net_block`, the `encoder` simply stacks one layer on top of another and is essentially a sequential like model where we can safely assume that each layer in `model.summary()` is connected to its previous layer, hence `Connected to` column would be redundant, whereas if addition is inside `encoder` it deviates from simple sequential like structure and `Connected to` column is necessary to know inter-layer connectivity."]}, {"number": 40968, "title": "Add test and fix one error for writable file", "body": "@mihaimaruseac \r\nThis PR adds test for `writable` and `read_only_memory_region`", "comments": ["Hmm, this will take longer to merge as US is in a 4 days weekend due to 4th of July. Sorry it takes longer", "No need to worry. Happy Independence Day \ud83c\udf89 "]}, {"number": 40967, "title": "Plase Help!", "body": "package github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto\" in any of:\r\n        c:\\go\\src\\github.com\\tensorflow\\tensorflow\\tensorflow\\go\\core\\protobuf\\for_core_protos_go_proto (from $GOROOT)\r\n        C:\\Users\\JairForero\\go\\src\\github.com\\tensorflow\\tensorflow\\tensorflow\\go\\core\\protobuf\\for_core_protos_go_proto (from $GOPATH)", "comments": ["@jairforero \r\nPlease mention the tensor flow version on which this was encountered.\r\nCan you refer to [this issue](https://github.com/tensorflow/tensorflow/issues/39744#issuecomment-636431128) with similar error and let us know.", "Thanks! I'm not sure which version I just run\r\ngo run github.com/tensorflow/tensorflow/tensorflow/go", "@jairforero \r\nCould you please let us know if you have referred to the issue shared, the link in above comment does not exist. please fill in the template for us to help.", "Thanks This problems is to \r\n![image](https://user-images.githubusercontent.com/57493863/86364879-5d727d80-bc3e-11ea-93bc-fbb0e1fafe93.png)\r\n", "Please paste the error message (using makrdown formatting around it) instead of screenshotting. Screenshots are not searchable so they don't help in looking for the issue and also don't help other people having the same error from finding about the issue.", "@jairforero\r\nPlease update as per above comment.", "Hi This is error\r\n\r\n```\r\nPS E:\\go\\jairforero\\bot> go run ocr.go\r\nE:\\Go\\go\\src\\github.com\\tensorflow\\tensorflow\\tensorflow\\go\\saved_model.go:25:2: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto\" in any of:\r\n        c:\\go\\src\\github.com\\tensorflow\\tensorflow\\tensorflow\\go\\core\\protobuf\\for_core_protos_go_proto (from $GOROOT)\r\n        E:\\Go\\go\\src\\github.com\\tensorflow\\tensorflow\\tensorflow\\go\\core\\protobuf\\for_core_protos_go_proto (from $GOPATH)\r\n```", "@jairforero \r\nCan you please follow [this comment](https://github.com/tensorflow/tensorflow/issues/34580#issuecomment-646753289) on same error faced and let us know.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40967\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40967\">No</a>\n", "Same Error still is continues", "Still the same issue present. Any idea what causes this? I've followed the official install guide to the point."]}, {"number": 40966, "title": "refact huber loss", "body": "Refact Huber loss for easy understanding.", "comments": []}, {"number": 40964, "title": "[tf.data] Use output_shapes from python for batch dataset", "body": "This is a PR from JIZHI, the AI platform in Tencent.\r\n\r\nThis pr is related to #40938 . It removes the output shape calculation in C++ for `BatchDatasetOp` and `PaddedBatchDatasetOp` and use the shapes passed from python instead.\r\n\r\nThank you for your time on reviewing this pr.", "comments": ["@jsimsa \r\nCould you have a look at this pr? Thank you!", "@jsimsa\r\nCould you have a look at this pr? Thank you!", "Hi @zhuzilin sorry for the delay in response. I was out of office for an extended period of time and am still catching up.\r\n\r\nWhat is the motivation for your PR? I would prefer to keep the shape inference in C++ for the following reasons.\r\n\r\nIn general, we could rewrite the tf.data graph in a way that would allow the C++ shape inference to be more accurate. For instance, if the rewrite would set the `drop_remainder` attribute to `True`. If use the shapes from the original Python shape inference, than we lose this ability.\r\n\r\nIn other words, there are reasons for having separate C++ and Python shape inference and I would prefer to keep both.", "@jsimsa \r\nThank you for your nice explanation! This pr is just me wondering why we need both c++ version and python version of the shape inference."]}, {"number": 40963, "title": "[tf.data] Add SkipNext interface to iterator", "body": "This is a PR from JIZHI, the AI platform in Tencent.\r\n\r\nThis pr adds a `SkipNext` interface to `IteratorBase` and use this method in `SkipDatasetOp` and `ShardDatasetOp`.\r\n\r\nIf this interface is added, we can gradually implement it for all dataset ops so that some unnecesscary calculation can be avoided.\r\n\r\nThank you for your time on reviewing this pr.", "comments": ["@aaudiber \r\nCould you have a look at  this pr? Thank you!", "Thanks for putting this together @zhuzilin! Sorry for the delay in review. Some questions:\r\n\r\n1. Is there a specific use case motivating this change?\r\n2. Which datasets are you planning to implement skipping for?\r\n3. What do you think of changing `SkipNext(ctx, end_of_input)` to `Skip(ctx, num_to_skip, end_of_input)`? In some cases skipping multiple could be cheaper than skipping one at a time.\r\n", "@aaudiber \r\nThank you for your nice questions. Here are some of my thoughts.\r\n> Is there a specific use case motivating this change?\r\n\r\nThe specific use case would be using `shard` when there are uneven amount of data in the files of the dataset. \r\nFor now, we encourage to shard by file names, so that each shard won't process unnecessary data. But if there are not enough files to allocate one for each worker or the size of the file varies a lot, then the user has to shard the dataset after reading them. In such situation, he or she will meet performance loss, especially when the cpu resource is limited.\r\n\r\n> Which datasets are you planning to implement skipping for?\r\n\r\nI think probablity most of them, except `TFRecordDataset`, for it is using a `SequentialRecordReader` (I wonder if we can only move the offset without actually reading from the file...). Therefore, it's likely to be lots of additions. I hope we can settle down the design in this pr in order to avoid large scale of refactor.\r\n\r\n> What do you think of changing SkipNext(ctx, end_of_input) to Skip(ctx, num_to_skip, end_of_input)? In some cases skipping multiple could be cheaper than skipping one at a time.\r\n\r\nThis is one of the design issues I hope to discuss \ud83d\ude04 . I agree that `Skip(num_to_skip)` would potentially have better performance than `SkipNext`, but I'm not sure how to deal with `input_impl_.reset()` when `end_of_sequence` is met. Are we going to put the reset in the `Skip` or shall we return the number of successful skipping back to the iterator?", "@zhuzilin Thanks for explaining the motivation, I think it makes sense.\r\n\r\n> This is one of the design issues I hope to discuss \ud83d\ude04 . I agree that `Skip(num_to_skip)` would potentially have better performance than `SkipNext`, but I'm not sure how to deal with `input_impl_.reset()` when `end_of_sequence` is met. Are we going to put the reset in the `Skip` or shall we return the number of successful skipping back to the iterator?\r\n\r\nGood point, I think we will need to report the number of skipped elements as well so that the caller can decide what to do when end_of_sequence is reached before the requested number of elements are skipped.", "@aaudiber \r\nI've changed the `SkipNext` to `Skip`. Could you have another look? Thank you!", "@aaudiber \r\nCould you take a look at this pr? Thank you!", "@aaudiber \r\nThe build failures are due to me forget the `&` in `RecordElement`... I've fixed the mistake.Sorry for my carelessness...", "Gently ping @aaudiber \r\nI've fixed the build failure in the last commit. Could you have another look? \ud83d\ude04 ", "@aaudiber \r\nThe new build failure is related to commit 94ca0bd702, where the signatures of `RecordStart` and `RecordStop`are changed. I've updated the `Skip` method to make it coherent with the `GetNext` in the current master branch. Could you have another look? Thank you!", "@aaudiber \r\nCould you take another look at this pr? Thank you \ud83d\ude04 .", "@aaudiber \r\nIt seems that the current implementation of `Skip` may have some conflict with `DistributedIteratorMultiWorker` and `AutoShard`. I'm not familiar with the mechanism of auto-sharding and I wonder if you have any idea how we could fix those conflicts? Thank you :).", "@zhuzilin `AutoShard` will update a dataset to insert a `shard` transformation, either at the start after a list of filenames, or at the very end if a list of filenames cannot be found. It seems like the new skip-based shard implementation must change the behavior somehow. The test failure messages suggest that the test is hitting `end_of_sequence` too early now.\r\n\r\nI recommend using `tensorflow/python/distribute:input_ops_test` to debug, since the test is simpler than `DistributedIteratorMultiWorker`, but still fails.", "@aaudiber \r\nThe reason of early end_of_sequence is that the shard op did not initialize end_of_sequence to false. However, ops like concatenate, which use end_of_sequence as a switch, will pass the `*end_of_sequence = true` to shard. The new commit add `*end_of_sequence = false` in the shard op, which should fix the end_of_sequence failure.", "@aaudiber \r\nThis pr is finally merged \ud83c\udf89 Thank you for your help and your great patience on it \ud83d\ude04.", "> @aaudiber\r\n> This pr is finally merged \ud83c\udf89 Thank you for your help and your great patience on it \ud83d\ude04.\r\n\r\nYou did all the work, I just reported the test failures :-) Thanks for your patience with my slow responses!", "Seems auto-merge is not happening but the changes are merged into master now, so we can close this. Thank you for the PR."]}, {"number": 40962, "title": "[INTEL MKL] Replace tensorflow::bfloat16 with Eigen::bfloat16", "body": "This PR mainly changes tensorflow::bfloat16 to Eigen::bfloat16 to get benefit from its vectorization", "comments": ["@rmlarsen Can you please review this PR ? Thanks!", "@ShengYang1 thank you for the PR! I had to make a few trivial changes to internal code, but the PR looks good. I expect to submit it tomorrow."]}, {"number": 40961, "title": "Layout grappler optimization for NDHWC Conv3D", "body": "This PR is to allow the layout grappler optimizer to support Conv3D/Conv3DBackpropFilterV2/Conv3DBackpropInputV2. Specifically, the grappler pass will convert the conv3d nodes to NDHWC/NCDHW when the target format is NHWC/NCHW respectively.\r\n\r\nThis PR is corresponding to the recent CUDNN v8 support for NDHWC 3D ( https://github.com/tensorflow/tensorflow/pull/40399).\r\n\r\nfyi: @nluehr ", "comments": ["@rmlarsen Can you please review this PR ? Thanks!", "Any update on this PR? Thanks.", "Just added one change to add the support for counting the conv3d nodes in deciding whether to use NHWC or NCHW for the grappler pass. ", "@kaixih Can you please resolve conflicts? Thanks!", "The conflicts have been solved. ", "Just push one more commit to fix a narrow-conversion issue for list initialization.\r\n\r\nNow, it seems all the tests pass. @ezhulenev PTAL. Thanks."]}, {"number": 40960, "title": "Don't call Graph.control_dependencies() when inside v2 tf.functions", "body": "As discussed in https://github.com/tensorflow/tensorflow/pull/40564#issuecomment-652060166 `control_dependencies` should not be necessary when using `tf.function` in eager mode.\r\n\r\n/cc @alextp ", "comments": []}, {"number": 40959, "title": "How to verify Tensorflow install", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/install/pip#windows_1\r\n\r\n## Description of issue (what needs changing): The documentation gives a command to \"verify the install\"... but NO clear indication of what should be the result. I get a list of 9 warnings (all CUDA-related) interspersed with 8 information messages. Finally there is a \"tf.tensor(72.93745, shape=(), dtype=float32)\" at the end. Is this correct? How would I know?\r\n\r\n### Clear description\r\n\r\nSee above\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct? n/a\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly? n/a\r\n\r\n### Returns defined\r\n\r\nAre return values defined? n/a\r\n\r\n### Raises listed and defined\r\n\r\nn/a\r\n\r\n### Usage example\r\n\r\nIs there a usage example? Half of one - it lacks any output to check against, or instructions on how to interpret the result\r\n\r\n### Request visuals, if applicable\r\n\r\nn/a\r\n\r\n### Submit a pull request?\r\n\r\nNo - I am far too new to github/tensorflow to ask sensible questions, let alone give sensible answers\r\n", "comments": ["@omatai,\r\nWhen you run \r\n`python3 -c \"import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"`\r\n\r\nif the code prints the tensor, in your case `tf.tensor(72.93745, shape=(), dtype=float32)`, it means that TensorFlow was successfully imported.\r\n\r\nPlease take a look at the breakdown of the code from this [gist](https://colab.research.google.com/gist/amahendrakar/9b82b578e1dacdfc3f097811963d392a/40959.ipynb). Thanks!", "OK - so you know that, and now I know that... but how will anyone else know that if it is not in the documentation? And how should all the information, warnings and errors be interpreted? When I follow the link you provide, it gives an error... which kinda makes it worse than when I first did it and only got warnings. So I'm not sure you have helped :-)\r\n\r\nThe point is: all this needs to be in the documentation, not here, because nobody will ever find this again :-)", "@omatai, \r\nSorry for the confusion, the error was raised as I was running the code on CPU. Switching to GPU does not throw the error.\r\n\r\nAlso, the logs are displayed based on the logging level. Changing the default log level does not print `INFO` messages. Please check [this updated gist](https://colab.research.google.com/gist/amahendrakar/185c45f1c1027f13699e3e700502fcbd/40959.ipynb) for reference. Thanks!", "@omatai, can you please send a PR to fix documentation?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "> \r\n> \r\n> @omatai, can you please send a PR to fix documentation?\r\n\r\nI have no idea what is involved in a PR, no time available for several weeks to learn... and am deeply confused by why the results I got do not match the results shown by @amahendrakar - I get a tensor with value 72.something; he gets -573 or something.\r\n\r\nIf the purpose of the test is to validate the install, there needs to be a definitive and meaningful result. I am in no position to say what this result should be, nor how to interpret variations on it. Sorry - I have nothing constructive to contribute - I'm unsubscribing.", "Hey, @omatai. The code example listed there is just a set of TF APIs that should run. Any non-error output is fine. The numbers don't match because you are summing a bunch of random numbers. It is expected that people using TF would at least know the public API. But anyway, we can change the test.\r\n\r\nWould you be ok if we replaced it with something like `python3 -c \"import tensorflow as tf; print(tf.constant(\"Everything installed successfully))`? In this the output either contains \"Everything installed successfully\" (\u2705) or some error (so installation fails). ", "That certainly makes more sense than printing some random number and leaving people no advice on how to interpret it :-)\r\n\r\nBut I would suggest it might be better to think about something that tests if the tensorflow installation is using GPU or CPU, and detects if that is installed correctly. As it turns out, I had installed the latest (incompatible) version of CUDA, so was not using GPU. When I ran my first heavy duty training, the CPU went to 100%, overheated and restarted my PC after 20 seconds.\r\n\r\nI don't know if what you suggest is sufficient to detect installation problems like that (CUDA is separate, after all...) but the computation recommended might be suitable. It just lacks any guidance as to how to interpret the result... which is clearly different on my machine from @amahendrakar 's. That doesn't help anyone.", "That is actually a good idea. I'll try to write a `diagnose_me.py` script over the weekend and then the verify instructions would be just `python tools/diagnose_me.py` or something like that. Thank you for the reminder for adding this tool (we have many `DLL import failed` issues that would be quickly solved by this).", "As suggested by @mihaimaruseac, I would like to work on making the diagnose_me.py script.", "With the merging of https://github.com/tensorflow/docs/pull/1749 this can be now closed.\r\n\r\nFor `diagnose_me.py` script, we should probably open a feature request."]}, {"number": 40958, "title": "[Intel MKL] Code clean up based on static code analysis", "body": "", "comments": ["Is it the problem with \"maybe null pointer\"? Or it actually fails at runtime with nullptr dereferencing?", "> Is it the problem with \"maybe null pointer\"? Or it actually fails at runtime with nullptr dereferencing?\r\n\r\nIt is a \"maybe null pointer\", and is reported by static code analyzer.", "It can't actually be null because of the properties of graph view, if it is null it's a serious bug that I'd prefer to get a segfault then silently returning false."]}, {"number": 40957, "title": "Fix tf32.", "body": "Before, allowing TF32 would have no effect. The issue was the tf32_util.cc file was linked in twice, so there were two copies of the `tf32_allowed` global variable.\r\n\r\nAlso add a test.\r\n\r\n/CC @nluehr ", "comments": ["I merged with master, but the Windows build currently fails at master so I cannot check if this PR breaks the Windows build. I'll wait until the Windows build is fixed before further trying to fix this PR"]}, {"number": 40956, "title": "ESP32 doenst run correct anymore - Person Detection with Esp32 Camera", "body": "Hello Folks,\r\n\r\nthe project build in ESP32 even 4.0, 4.1, 4.2 or latest master of esp-idf build correct but when running there is an error with the camera acquisition.\r\n\r\n./components/esp32-camera/driver/camera.c:1412 (discriminator 2)\r\n\r\nor in ./components/esp32-camera/driver/camera.c:1415 (discriminator 2)\r\n\r\nSo, right now cant tell if a tensorflow lite/micro problem or with the camera driver/settings on it.\r\n\r\nCan someone in this area test it again???", "comments": ["@filzek \r\nPlease share simple stand alone code to replicate the issue faced, along with error logs for us to analyse or if possible share a colab gist with the issue faced.", "Hi\n\nThe code is the Person Detector example from tensorflow micro github, as it\nis, without any addition, following the Readme to build and compile it.\n\nUpload to the device and run.\n\nAnd return the error and rebooting loop forever.\n\nEm qua, 1 de jul de 2020 07:00, Saduf2019 <notifications@github.com>\nescreveu:\n\n> @filzek <https://github.com/filzek>\n> Please share simple stand alone code to replicate the issue faced, along\n> with error logs for us to analyse or if possible share a colab gist with\n> the issue faced.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/40956#issuecomment-652322135>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AG7JIU25K3GDN45IAZQUSALRZMCL3ANCNFSM4OMUQXBA>\n> .\n>\n", "@filzek \r\nPlease share the tensor flow version on which the issue is faced.", "It is the latest version as of in master.\r\n\r\ngit describe --tags\r\nv1.12.1-35363-gf3ccf59812\r\n\r\ngit branch -a --contains\r\n* master\r\n  remotes/origin/HEAD -> origin/master\r\n  remotes/origin/master\r\n\r\ngit show-branch\r\n[master] compat: Update forward compatibility horizon to 2020-06-29\r\n", "Adding @petewarden and @fredrec for ESP32 integration issues.", "@filzek few questions:\r\n\r\n1. Are you using ESP-EYE dev board or something else?\r\n2. Can you please add the logs?\r\n", "@filzek \r\nwhich TF version you have used for person detection model training?", "@filzek \r\nIt looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Could you please execute your code using Latest stable version of  2.6.0  and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40956\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40956\">No</a>\n"]}, {"number": 40955, "title": "BinaryOp vs OpKernel in Im2Col", "body": "I have just started to create my custom OP based on \"conv_ops_using_gemm.cc\" and \"quantized_conv_ops.cc\". I want to modify the im2col algorithm inside TensorFlow to improve the performance of the inference. \r\n\r\nI have some questions in both files. I have some question from what I understood from the tutorial and  conv_ops_using_gemm.cc shown is the following:\r\n\r\n1. There are some methods called **ClassName+OP** or **ClassName+functor**, I did not get the difference between them in the TensorFlow library context.\r\n2. These classes OpKernel or BinaryOp, I do not get what are they responsible for and How I can understand them more deeply from the documentation. \r\n3. In [Part(1),](https://github.com/tensorflow/tensorflow/blob/d9a3a849edc198e90172bc58eb293de457f9d986/tensorflow/core/kernels/conv_ops_using_gemm.cc#L565) what I understood, it is a registration for the kernel on the CPU to be called from the python by \"tf.nn.conv2d()\", but I did not get how conv2d are called Im2ColConvFunctor from Conv2DUsingGemmOp class at the end of this class using \"conv_factor\" in [Part(2)](https://github.com/tensorflow/tensorflow/blob/d9a3a849edc198e90172bc58eb293de457f9d986/tensorflow/core/kernels/conv_ops_using_gemm.cc#L551).\r\n\r\nPart(1):\r\n\r\n```\r\n#define REGISTER_CPU(T) \\\r\n REGISTER_KERNEL_BUILDER( \\\r\n Name(\"Conv2D\").Device(DEVICE_CPU).TypeConstraint<T>(\"T\"), \\\r\n Conv2DUsingGemmOp< \\\r\n T, Im2ColConvFunctor<T, T, T, FastGemmFunctor<T, T, T>>>);\r\n```\r\n\r\nPart(2):\r\n\r\n```\r\n TConvFunctor conv_functor;\r\n conv_functor(context, input.flat<T>().data(), batch, input_rows, input_cols,\r\n in_depth, filter.flat<T>().data(), filter_rows, filter_cols,\r\n out_depth, stride_rows, stride_cols, padding_,\r\n output->flat<T>().data(), out_rows, out_cols);\r\n```", "comments": ["@AhmedHussKhalifa \r\nThis is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40955\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40955\">No</a>\n"]}, {"number": 40954, "title": "[ROCm] Fix for ROCm CSB Breakage - 200630", "body": "The following commit (which switched G's internal CI to use ROCm 3.5) breaks the ROCm CSB build (which still uses ROCm 3.3)\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/22def20bae7be6d5b790b360abed5919385b16c2\r\n\r\nThis PR/commit simply puts back a couple of codes that were removed the the previous commit, and makes them conditional on ROCm 3.5.\r\n\r\nNote that the ROCm CSB build will be switching to ROCm 3.5 or higher in the near future, at which point all codes within the `true` block for `#if TENSORFLOW_COMPILER_IS_HIP_CLANG` will become default, and those in the `false / #else` block will be removed.\r\n\r\n\r\n------------------------\r\n\r\n\r\n/cc @chsigg @cheshire @nvining-work ", "comments": ["@chsigg gentle ping", "@chsigg gentle ping\r\n\r\nNeed this PR to fix the broken ROCm CSB", "@chsigg gentle ping", "@chsigg I do not havbe visibility into the results for the internal ROCm build at your end. Let me know if it passed and whether we can take this PR\r\n\r\n", "@chsigg gentle ping", "@chsigg gentle ping"]}, {"number": 40953, "title": "Support convert from UpperLeft, Center in BoundingBoxUtils", "body": "", "comments": ["@am15h Can you please resolve conflicts? Thanks!\r\n", "Let's make a PR in the new repo. Thanks for the efforts in advance!", "New PR opened https://github.com/tensorflow/tflite-support/pull/15"]}, {"number": 40952, "title": "Eliminate array copy in NormalizeOp", "body": "This minor optimization eliminates array copy in NormalizeOp and improves efficiency by getting float value by absIndex.", "comments": ["@am15h Can you please resolve conflicts? Thanks!\r\n", "I had some experiments (but might be stale) that getting values pixel by pixel is even slower.\r\n\r\nDo you have any numbers about how performance will be improved with this change?\r\n\r\nThanks!", "@xunkai55 This optimization reduced the normalization time by more than half in the [tflite flutter helper library](https://github.com/am15h/tflite_flutter_helper) .  Does this change make it slower on Android? \r\n\r\n\r\n\r\n\r\n> @am15h Can you please resolve conflicts? Thanks!\r\n\r\nThe support library was moved to its [own repo](https://github.com/tensorflow/tflite-support) a few hours ago, I will need to open a PR there. ", "Sorry for the code move. Indeed, let's have a PR in that repo.\r\n\r\nIf you have no testing environment, I would help run a benchmark internally. Once you have the PR please let me know.", "> If you have no testing environment, I would help run a benchmark internally. Once you have the PR please let me know.\r\n\r\nThat would be great. I have opened a new PR https://github.com/tensorflow/tflite-support/pull/14.\r\n\r\n"]}, {"number": 40951, "title": "Problems building TensorRT engine. 'TypeError: Expected at most 1 positional arguments'", "body": "[MCNet_Model_30%_SNR10+.zip](https://github.com/tensorflow/tensorflow/files/4880911/MCNet_Model_30._SNR10%2B.zip)\r\n(https://github.com/tensorflow/tensorflow/files/4858531/saved_model.zip)\r\n<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Nvidia JetPack 4.4 (binary)\r\n- TensorFlow version (use command below): tensorflow 2.1.0+nv20.4\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: \r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nReceiving \"TypeError\" when I try to build the TensorRT engine. The dataset is custom and the initial saved TF model file (saved_model.pb) is attached below. This is a similar issue as seen at #34708, and I have tried everything suggested there with no luck. I have also tried changing the input shape (changing channels and batch size), but this seems to have no effect. \r\n\r\n**Describe the expected behavior**\r\nTrying to convert a TF saved model and build a tensorRT engine to be saved.\r\n\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n`import tensorflow as tf \r\nimport numpy as np\r\nfrom tensorflow.python.compiler.tensorrt import trt_convert as trt\r\n\r\ntf.enable_eager_execution()\r\ntf.keras.backend.set_learning_phase(0)\r\n\r\n# User input\r\ninput_path = '/home/xv/my-recognition-python/custom_model/MCNet_Model_30%_SNR10+'\r\noutput_path = '/home/xv/my-recognition-python/custom_model/TensorRT'\r\nprecision = 'FP16'\r\n\r\n#params = tf.experimental.tensorrt.ConversionParams(precision_mode=precision)\r\n#converter = tf.experimental.tensorrt.Converter(input)\r\n\r\n# Conversion Parameters\r\nconversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS\r\nconversion_params = conversion_params._replace(\r\n    max_workspace_size_bytes=(1<<32))\r\nconversion_params = conversion_params._replace(precision_mode=\"FP16\")\r\nconversion_params = conversion_params._replace(\r\n    maximum_cached_engines=100)\r\n\r\n# Convert, Build and Save\r\ntf.compat.v1.enable_eager_execution()\r\nconverter = trt.TrtGraphConverterV2(input_saved_model_dir=input_path,conversion_params=conversion_params)\r\nconverter.convert()\r\ndef my_input_fn():\r\n    input = np.random.normal(size=(32,2,1024,1)).astype(np.float32)\r\n    return input\r\nconverter.build(input_fn=my_input_fn)\r\nconverter.save(output_path)`\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n`Traceback (most recent call last):\r\n  File \"tf2tensorrt.py\", line 31, in <module>\r\n    converter.build(input_fn=my_input_fn)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compiler/tensorrt/trt_convert.py\", line 1040, in build\r\n    self._converted_func(*map(ops.convert_to_tensor, inp))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 1551, in __call__\r\n    return self._call_impl(args, kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 1570, in _call_impl\r\n    ).format(self._num_positional_args, self._arg_keywords, args))\r\nTypeError: Expected at most 1 positional arguments (and the rest keywords, of ['input_1']), got (<tf.Tensor: shape=(2, 1024, 1), dtype=float32, numpy=\r\narray([[[-0.6344961 ],\r\n        [-1.4673657 ],\r\n        [ 0.15545663],\r\n        ...,\r\n        [-0.0928062 ],\r\n        [-1.3549978 ],\r\n        [ 0.9633423 ]],\r\n\r\n       [[-1.0417418 ],\r\n        [-2.0699146 ],\r\n        [-0.30908048],\r\n        ...,\r\n        [-0.11305276],\r\n        [-1.1771021 ],\r\n        [ 1.3303916 ]]], dtype=float32)>, <tf.Tensor: shape=(2, 1024, 1), dtype=float32, numpy=\r\narray([[[-0.02007438],\r\n        [ 0.5109657 ],\r\n        [-1.2260785 ],\r\n        ...,\r\n        [-0.54254615],\r\n        [-0.8100358 ],\r\n        [ 0.27891546]],\r\n\r\n       [[ 1.3610845 ],\r\n        [ 0.24926041],\r\n        [-0.16650145],\r\n        ...,\r\n        [-0.54429305],\r\n        [-0.32332402],\r\n        [-1.0127826 ]]], dtype=float32)>, <tf.Tensor: shape=(2, 1024, 1), dtype=float32, numpy=\r\narray([[[ 0.62289035],\r\n        [ 0.29980063],\r\n        [ 0.2861899 ],\r\n        ...,\r\n        [ 0.9005576 ],\r\n        [-0.62317663],\r\n        [-0.06627446]],\r\n\r\n       [[-0.10040612],\r\n        [-0.09332627],\r\n        [ 1.1089867 ],\r\n        ...,\r\n        [ 1.0935025 ],\r\n        [ 0.08065958],\r\n        [-1.2275263 ]]], dtype=float32)>, <tf.Tensor: shape=(2, 1024, 1), dtype=float32, numpy=\r\narray([[[-0.0846673 ],\r\n        [ 0.51114583],\r\n        [ 1.4352379 ],\r\n        ...,\r\n        [ 0.95028365],\r\n        [ 2.7874076 ],\r\n        [ 0.77085507]],\r\n\r\n       [[ 0.58681023],\r\n        [ 1.0490478 ],\r\n        [-1.3703693 ],\r\n        ...,\r\n        [ 1.8135824 ],\r\n        [-0.23638606],\r\n        [ 0.18856032]]], dtype=float32)>, <tf.Tensor: shape=(2, 1024, 1), dtype=float32, numpy=\r\narray([[[-0.45678732],\r\n        [ 1.6005052 ],\r\n        [-0.8796155 ],\r\n        ...,\r\n        [-0.9854982 ],\r\n        [-0.9157079 ],\r\n        [-0.65166223]],\r\n\r\n       [[ 1.6831336 ],\r\n        [-1.6157633 ],\r\n        [-1.553059  ],\r\n        ...,\r\n        [ 0.34703007],\r\n        [-0.14187814],\r\n        [ 0.73059773]]], dtype=float32)>, <tf.Tensor: shape=(2, 1024, 1), dtype=float32, numpy=\r\narray([[[ 0.4139533 ],\r\n        [ 0.50489587],\r\n        [ 0.65386546],\r\n        ...,\r\n        [-0.40917957],\r\n        [ 1.7364669 ],\r\n        [-0.99362046]],\r\n\r\n       [[ 0.27688766],\r\n        [ 0.58144623],\r\n        [ 0.40299714],\r\n        ...,\r\n        [ 0.7578483 ],\r\n        [ 0.9832838 ],\r\n        [ 1.002946  ]]], dtype=float32)>, <tf.Tensor: shape=(2, 1024, 1), dtype=float32, numpy=\r\narray([[[-1.0991313 ],\r\n        [-2.3144197 ],\r\n        [-0.98326117],\r\n        ...,\r\n        [ 0.37580004],\r\n        [-1.4070382 ],\r\n        [ 0.95411766]],\r\n\r\n       [[ 1.328169  ],\r\n        [ 0.704368  ],\r\n        [ 1.4499707 ],\r\n        ...,\r\n        [ 0.60652196],\r\n        [ 1.4832685 ],\r\n        [ 0.684714  ]]], dtype=float32)>, <tf.Tensor: shape=(2, 1024, 1), dtype=float32, numpy=\r\narray([[[-0.11087849],\r\n        [-0.07940213],\r\n        [ 0.50273675],\r\n        ...,\r\n        [-0.5097304 ],\r\n        [ 1.4458168 ],\r\n        [-1.2201035 ]],\r\n\r\n       [[-0.5431153 ],\r\n        [-0.42222708],\r\n        [ 1.092256  ],\r\n        ...,\r\n        [-0.04186005],\r\n        [ 0.9809653 ],\r\n        [-0.7850843 ]]], dtype=float32)>, <tf.Tensor: shape=(2, 1024, 1), dtype=float32, numpy=\r\narray([[[ 1.0818578e+00],\r\n        [-4.6179372e-01],\r\n        [ 4.2473122e-01],\r\n        ...,\r\n        [-1.0006120e+00],\r\n        [ 1.9919460e-03],\r\n        [ 8.5118961e-01]],\r\n\r\n       [[-1.8546258e+00],\r\n        [ 7.0895630e-01],\r\n        [ 2.0718536e+00],\r\n        ...,\r\n        [-2.4367233e-01],\r\n        [ 6.7389584e-01],\r\n        [ 9.7365260e-01]]], dtype=float32)>, <tf.Tensor: shape=(2, 1024, 1), dtype=float32, numpy=\r\narray([[[ 1.0906012 ],\r\n        [-0.06402431],\r\n        [ 0.91972494],\r\n        ...,\r\n        [-0.52831906],\r\n        [ 1.2781252 ],\r\n        [-0.06841598]],\r\n\r\n       [[-1.341631  ],\r\n        [ 0.53109026],\r\n        [-0.11353733],\r\n        ...,\r\n        [ 1.1344278 ],\r\n        [ 1.4150394 ],\r\n        [ 1.2143872 ]]], dtype=float32)>, <tf.Tensor: shape=(2, 1024, 1), dtype=float32, numpy=\r\narray([[[-0.36829016],\r\n        [ 1.0754083 ],\r\n        [ 0.16599853],\r\n        ...,\r\n        [-1.1099495 ],\r\n        [-0.70543027],\r\n        [-2.4914129 ]],\r\n\r\n       [[-0.25961915],\r\n        [-0.33185184],\r\n        [-1.9378744 ],\r\n        ...,\r\n        [-0.64549893],\r\n        [ 0.21132562],\r\n        [-0.6391495 ]]], dtype=float32)>, <tf.Tensor: shape=(2, 1024, 1), dtype=float32, numpy=\r\narray([[[-0.6435254 ],\r\n        [-0.1136331 ],\r\n        [ 0.25582156],\r\n        ...,\r\n        [ 0.00787924],\r\n        [ 1.2224247 ],\r\n        [ 1.2220552 ]],\r\n\r\n       [[ 1.384727  ],\r\n        [ 1.9391284 ],\r\n        [ 0.04586202],\r\n        ...,\r\n        [ 0.06961127],\r\n        [-0.03484035],\r\n        [ 0.17856297]]], dtype=float32)>, <tf.Tensor: shape=(2, 1024, 1), dtype=float32, numpy=\r\narray([[[ 0.28783402],\r\n        [-1.0524169 ],\r\n        [-1.0904337 ],\r\n        ...,\r\n        [ 0.43865943],\r\n        [-0.97926116],\r\n        [ 0.2378908 ]],\r\n\r\n       [[ 0.9016964 ],\r\n        [-0.06834929],\r\n        [-1.0139452 ],\r\n        ...,\r\n        [-0.15281294],\r\n        [ 1.0535744 ],\r\n        [-0.21410678]]], dtype=float32)>, <tf.Tensor: shape=(2, 1024, 1), dtype=float32, numpy=\r\narray([[[ 1.0744122 ],\r\n        [-0.56937873],\r\n        [-0.11793306],\r\n        ...,\r\n        [ 1.4719427 ],\r\n        [-0.44377735],\r\n        [-0.9831226 ]],\r\n\r\n       [[-1.5557859 ],\r\n        [ 1.0128443 ],\r\n        [ 2.067592  ],\r\n        ...,\r\n        [-0.48884177],\r\n        [-0.5740634 ],\r\n        [-0.293522  ]]], dtype=float32)>, <tf.Tensor: shape=(2, 1024, 1), dtype=float32, numpy=\r\narray([[[ 0.8241517 ],\r\n        [ 1.8754789 ],\r\n        [-0.8107462 ],\r\n        ...,\r\n        [-0.7464624 ],\r\n        [-1.9949435 ],\r\n        [-1.5022945 ]],\r\n\r\n       [[ 0.63984567],\r\n        [ 0.47466007],\r\n        [-0.0431034 ],\r\n        ...,\r\n        [-0.8277903 ],\r\n        [-0.30004153],\r\n        [-1.1758301 ]]], dtype=float32)>, <tf.Tensor: shape=(2, 1024, 1), dtype=float32, numpy=\r\narray([[[-0.13899833],\r\n        [-0.6309596 ],\r\n        [-0.54699975],\r\n        ...,\r\n        [ 0.5980539 ],\r\n        [-0.5291892 ],\r\n        [ 0.20535517]],\r\n\r\n       [[ 0.07299482],\r\n        [-1.5210488 ],\r\n        [-2.2613964 ],\r\n        ...,\r\n        [ 1.0103334 ],\r\n        [ 0.8114666 ],\r\n        [-0.6411918 ]]], dtype=float32)>, <tf.Tensor: shape=(2, 1024, 1), dtype=float32, numpy=\r\narray([[[-0.9746104 ],\r\n        [ 0.43699232],\r\n        [ 0.86292136],\r\n        ...,\r\n        [-2.000652  ],\r\n        [ 0.19808927],\r\n        [-0.58109105]],\r\n\r\n       [[ 3.002488  ],\r\n        [-0.27825996],\r\n        [ 0.7989333 ],\r\n        ...,\r\n        [-0.34547755],\r\n        [-0.576817  ],\r\n        [ 0.47699323]]], dtype=float32)>, <tf.Tensor: shape=(2, 1024, 1), dtype=float32, numpy=\r\narray([[[ 0.7425528 ],\r\n        [-0.88114923],\r\n        [ 1.2816385 ],\r\n        ...,\r\n        [ 0.61273754],\r\n        [-0.43834418],\r\n        [-1.0978663 ]],\r\n\r\n       [[-0.21802038],\r\n        [-0.10342378],\r\n        [-0.6027409 ],\r\n        ...,\r\n        [-0.09381349],\r\n        [-2.0900886 ],\r\n        [ 0.34896147]]], dtype=float32)>, <tf.Tensor: shape=(2, 1024, 1), dtype=float32, numpy=\r\narray([[[-1.4537004 ],\r\n        [-0.23114291],\r\n        [-0.5871747 ],\r\n        ...,\r\n        [ 1.7588977 ],\r\n        [-0.64653444],\r\n        [-0.8479624 ]],\r\n\r\n       [[-0.8888948 ],\r\n        [ 0.667874  ],\r\n        [-0.11591944],\r\n        ...,\r\n        [ 1.5969365 ],\r\n        [ 0.3887264 ],\r\n        [ 2.6164393 ]]], dtype=float32)>, <tf.Tensor: shape=(2, 1024, 1), dtype=float32, numpy=\r\narray([[[-0.71152514],\r\n        [ 0.31899413],\r\n        [ 0.59584296],\r\n        ...,\r\n        [-0.4681486 ],\r\n        [-0.18287487],\r\n        [-0.6718022 ]],\r\n\r\n       [[-0.536758  ],\r\n        [ 0.16100785],\r\n        [ 1.1793295 ],\r\n        ...,\r\n        [-1.6710243 ],\r\n        [-1.2068127 ],\r\n        [-0.71454585]]], dtype=float32)>, <tf.Tensor: shape=(2, 1024, 1), dtype=float32, numpy=\r\narray([[[-1.8764349 ],\r\n        [-0.4220016 ],\r\n        [-0.3832036 ],\r\n        ...,\r\n        [ 0.396611  ],\r\n        [ 0.4031959 ],\r\n        [ 0.35724488]],\r\n\r\n       [[-0.65015966],\r\n        [ 0.2224573 ],\r\n        [ 0.20525123],\r\n        ...,\r\n        [-0.45926374],\r\n        [ 0.6471609 ],\r\n        [ 0.3332496 ]]], dtype=float32)>, <tf.Tensor: shape=(2, 1024, 1), dtype=float32, numpy=\r\narray([[[ 0.80413526],\r\n        [ 1.1347907 ],\r\n        [ 0.94075084],\r\n        ...,\r\n        [ 0.00755476],\r\n        [ 0.06562049],\r\n        [ 1.1542311 ]],\r\n\r\n       [[-1.5574476 ],\r\n        [ 0.69488364],\r\n        [ 0.6990288 ],\r\n        ...,\r\n        [ 1.4017487 ],\r\n        [-0.59805775],\r\n        [-1.2751728 ]]], dtype=float32)>, <tf.Tensor: shape=(2, 1024, 1), dtype=float32, numpy=\r\narray([[[-0.40774116],\r\n        [-0.9957925 ],\r\n        [ 0.7808731 ],\r\n        ...,\r\n        [-0.41433212],\r\n        [ 0.5268629 ],\r\n        [-0.9951206 ]],\r\n\r\n       [[-1.3662847 ],\r\n        [ 0.4806584 ],\r\n        [-0.9774015 ],\r\n        ...,\r\n        [ 1.8218778 ],\r\n        [-1.0483685 ],\r\n        [-0.40153572]]], dtype=float32)>, <tf.Tensor: shape=(2, 1024, 1), dtype=float32, numpy=\r\narray([[[ 0.03537908],\r\n        [ 0.30871132],\r\n        [-0.43272868],\r\n        ...,\r\n        [ 0.78636587],\r\n        [ 0.12341707],\r\n        [-0.4693344 ]],\r\n\r\n       [[ 0.08255825],\r\n        [-1.047672  ],\r\n        [ 0.5959074 ],\r\n        ...,\r\n        [-1.0461427 ],\r\n        [-1.0181618 ],\r\n        [ 1.9093345 ]]], dtype=float32)>, <tf.Tensor: shape=(2, 1024, 1), dtype=float32, numpy=\r\narray([[[-0.66075665],\r\n        [ 1.8946798 ],\r\n        [-0.23847736],\r\n        ...,\r\n        [ 1.020657  ],\r\n        [ 0.20324162],\r\n        [ 1.6452681 ]],\r\n\r\n       [[ 0.4298775 ],\r\n        [-1.6225889 ],\r\n        [-0.75322384],\r\n        ...,\r\n        [ 0.02752435],\r\n        [ 1.7630118 ],\r\n        [ 0.6711645 ]]], dtype=float32)>, <tf.Tensor: shape=(2, 1024, 1), dtype=float32, numpy=\r\narray([[[-0.46049008],\r\n        [-0.7228323 ],\r\n        [ 0.49198708],\r\n        ...,\r\n        [-2.6658268 ],\r\n        [ 0.5688557 ],\r\n        [ 1.2088995 ]],\r\n\r\n       [[-0.32607743],\r\n        [-0.4941861 ],\r\n        [-0.4495856 ],\r\n        ...,\r\n        [ 0.46143514],\r\n        [ 0.04375907],\r\n        [-1.2433708 ]]], dtype=float32)>, <tf.Tensor: shape=(2, 1024, 1), dtype=float32, numpy=\r\narray([[[-2.1594412 ],\r\n        [-1.7406673 ],\r\n        [-0.34877333],\r\n        ...,\r\n        [-0.18708955],\r\n        [-1.3062778 ],\r\n        [ 1.0276945 ]],\r\n\r\n       [[ 0.8192844 ],\r\n        [ 1.2739064 ],\r\n        [-1.1833884 ],\r\n        ...,\r\n        [ 0.5369094 ],\r\n        [-1.4730651 ],\r\n        [-0.5227384 ]]], dtype=float32)>, <tf.Tensor: shape=(2, 1024, 1), dtype=float32, numpy=\r\narray([[[ 0.9111539 ],\r\n        [ 1.0799477 ],\r\n        [ 0.5658374 ],\r\n        ...,\r\n        [-0.05691434],\r\n        [ 0.50694185],\r\n        [-0.5104034 ]],\r\n\r\n       [[-0.5456287 ],\r\n        [ 0.5273403 ],\r\n        [ 0.7272648 ],\r\n        ...,\r\n        [-0.35712332],\r\n        [ 0.8400177 ],\r\n        [ 1.5469983 ]]], dtype=float32)>, <tf.Tensor: shape=(2, 1024, 1), dtype=float32, numpy=\r\narray([[[-0.02717604],\r\n        [-2.182759  ],\r\n        [-0.95963657],\r\n        ...,\r\n        [-0.1727186 ],\r\n        [ 1.0064832 ],\r\n        [ 1.1638952 ]],\r\n\r\n       [[ 0.9377752 ],\r\n        [ 0.15913202],\r\n        [ 0.39370653],\r\n        ...,\r\n        [-0.26473373],\r\n        [-0.09181631],\r\n        [-2.8304796 ]]], dtype=float32)>, <tf.Tensor: shape=(2, 1024, 1), dtype=float32, numpy=\r\narray([[[ 0.37767184],\r\n        [-1.506959  ],\r\n        [-0.28660896],\r\n        ...,\r\n        [-0.3206233 ],\r\n        [ 1.5128583 ],\r\n        [ 0.5174895 ]],\r\n\r\n       [[ 0.23141655],\r\n        [ 1.4028056 ],\r\n        [-0.33944097],\r\n        ...,\r\n        [ 0.45577055],\r\n        [ 1.2758701 ],\r\n        [ 0.7810977 ]]], dtype=float32)>, <tf.Tensor: shape=(2, 1024, 1), dtype=float32, numpy=\r\narray([[[-0.72600114],\r\n        [ 0.75190115],\r\n        [-0.8629814 ],\r\n        ...,\r\n        [-0.6400315 ],\r\n        [ 0.45446384],\r\n        [ 0.4231863 ]],\r\n\r\n       [[ 0.66919667],\r\n        [-1.4157479 ],\r\n        [-1.7317768 ],\r\n        ...,\r\n        [-0.06310534],\r\n        [ 0.3752666 ],\r\n        [ 1.365589  ]]], dtype=float32)>, <tf.Tensor: shape=(2, 1024, 1), dtype=float32, numpy=\r\narray([[[ 0.34559423],\r\n        [-1.4900172 ],\r\n        [ 0.913667  ],\r\n        ...,\r\n        [ 2.4927645 ],\r\n        [-1.9354457 ],\r\n        [-0.31414458]],\r\n\r\n       [[-0.93749714],\r\n        [-0.3028151 ],\r\n        [ 0.6019532 ],\r\n        ...,\r\n        [-0.18828262],\r\n        [ 0.52485734],\r\n        [ 0.5387431 ]]], dtype=float32)>). When calling a concrete function, positional arguments may not be bound to Tensors within nested structures.\r\n`", "comments": ["@ltorlay,\r\nWhile running the code I am facing an error stating `OSError: SavedModel file does not exist at: /home/xv/my-recognition-python/custom_model/MCNet_Model_30%_SNR10+/{saved_model.pbtxt|saved_model.pb}`. \r\n\r\nCould you please share the saved model file you are using in the code. Zip the contents of the folder, then drag and drop it in the input text box to share it. Thanks!", "@amahendrakar It is now added. ", "Still awaiting a response on this. It's happening with different network architectures as well. I believe it is related to the input dimensions. Please direct me to help if you can @amahendrakar \r\n\r\nThanks", "@ltorlay,\r\nSorry for the delayed response.\r\n\r\nOn running the code I am facing an error stating `Failed to get matching files on ./model/1/variables/variables: Not found: ./model/1/variables; No such file or directory`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/a9e0f35e9ff58889290f1f0c7a2add10/40951-1-15.ipynb#scrollTo=smU-wOE7-VqU).\r\n\r\nCould you please share the contents of the whole parent folder including the assets and variables folder. Thanks!", "> @ltorlay,\r\n> Sorry for the delayed response.\r\n> \r\n> On running the code I am facing an error stating `Failed to get matching files on ./model/1/variables/variables: Not found: ./model/1/variables; No such file or directory`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/a9e0f35e9ff58889290f1f0c7a2add10/40951-1-15.ipynb#scrollTo=smU-wOE7-VqU).\r\n> \r\n> Could you please share the contents of the whole parent folder including the assets and variables folder. Thanks!\r\n\r\nMy mistake. I added the zip file with the variables included", "@ltorlay,\r\nI was able to reproduce the issue with [TF v1.15](https://colab.research.google.com/gist/amahendrakar/b4bda2f5dfbba7aa3a57c00368091fc0/40951-1-15.ipynb) and [TF v2.2](https://colab.research.google.com/gist/amahendrakar/6ca96b2cd8dc481d9996ac1d4ef334a6/40951-2-2.ipynb). Please find the attached gist. \r\n\r\n\r\nAlso, please take a look at these similar issues, [issue #1](https://github.com/huggingface/transformers/issues/2021) and [issue #2](https://github.com/tensorflow/tensorflow/issues/34603), and let us know if it helps. Thanks!", "@amahendrakar I have gone ahead and looked at all of it. Do you think it is more of a problem related to the batch dimensions as mentioned in [https://github.com/huggingface/transformers/issues/2021]? I don't see a clear point of action.  \r\n\r\nThanks!", "Looking at [this solution](https://github.com/tensorflow/tensorflow/issues/34708#issuecomment-582622230), using yield instead of return worked for me.\r\n", "@varunsapre I used: \"yield [input]\" rather than return and it worked! \r\nfor the sake of completeness, here is my new function: \r\ndef my_input_fn():\r\n    num_runs=10\r\n    for _ in range(num_runs):\r\n        input = np.random.normal(size=(32,2,1024,1)).astype(np.float32)\r\n        yield [input]"]}, {"number": 40950, "title": "Update BUILD", "body": "", "comments": ["This is a test PR , closing this "]}, {"number": 40949, "title": "ssd_mobilenet_v2_mnasfpn_shared_box_predictor_320x320_coco_sync_2020_05_18.tar.gz error run in android", "body": "\r\nuse tensflow_models/ssd_mobilenet_v2_mnasfpn_shared_box_predictor_320x320_coco_sync_2020_05_18/model.tflite in \r\nhttps://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android  get this error:\r\n\r\nE/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: org.tensorflow.lite.examples.detection, PID: 19711\r\n    java.lang.RuntimeException: java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:133)\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:99)\r\n        at org.tensorflow.lite.examples.detection.CameraActivity$7.onPreviewSizeChosen(CameraActivity.java:446)\r\n        at org.tensorflow.lite.examples.detection.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:357)\r\n        at org.tensorflow.lite.examples.detection.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:362)\r\n        at org.tensorflow.lite.examples.detection.CameraConnectionFragment.access$300(CameraConnectionFragment.java:66)\r\n        at org.tensorflow.lite.examples.detection.CameraConnectionFragment$3.onSurfaceTextureAvailable(CameraConnectionFragment.java:171)\r\n        at android.view.TextureView.getTextureLayer(TextureView.java:390)\r\n        at android.view.TextureView.draw(TextureView.java:339)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:20754)\r\n        at android.view.View.draw(View.java:21607)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4558)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4333)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:20740)\r\n        at android.view.View.draw(View.java:21607)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4558)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4333)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:20740)\r\n        at android.view.View.draw(View.java:21607)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4558)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4333)\r\n        at android.view.View.draw(View.java:21884)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:20754)\r\n        at android.view.View.draw(View.java:21607)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4558)\r\n        at androidx.coordinatorlayout.widget.CoordinatorLayout.drawChild(CoordinatorLayout.java:1246)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4333)\r\n        at android.view.View.draw(View.java:21884)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:20754)\r\n        at android.view.View.draw(View.java:21607)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4558)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4333)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:20740)\r\n        at android.view.View.draw(View.java:21607)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4558)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4333)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:20740)\r\n        at android.view.View.draw(View.java:21607)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4558)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4333)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:20740)\r\n        at android.view.View.draw(View.java:21607)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4558)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4333)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:20740)\r\n        at android.view.View.draw(View.java:21607)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4558)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4333)\r\n        at android.view.View.draw(View.java:21884)\r\n        at com.android.internal.policy.DecorView.draw(DecorView.java:1082)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:20754)\r\n        at android.view.ThreadedRenderer.updateViewTreeDisplayList(ThreadedRenderer.java:725)\r\n        at android.view.ThreadedRenderer.updateRootDisplayList(ThreadedRenderer.java:731)\r\n        at android.view.ThreadedRenderer.draw(ThreadedRenderer.java:840)\r\n        at android.view.ViewRootImpl.draw(ViewRootImpl.java:3935)\r\n        at android.view.ViewRootImpl.performDraw(ViewRootImpl.java:3709)\r\n        at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:3017)\r\nE/AndroidRuntime:     at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:1876)\r\n        at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:8499)\r\n        at android.view.Choreographer$CallbackRecord.run(Choreographer.java:949)\r\n        at android.view.Choreographer.doCallbacks(Choreographer.java:761)\r\n        at android.view.Choreographer.doFrame(Choreographer.java:696)\r\n        at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:935)\r\n        at android.os.Handler.handleCallback(Handler.java:873)\r\n        at android.os.Handler.dispatchMessage(Handler.java:99)\r\n        at android.os.Looper.loop(Looper.java:214)\r\n        at android.app.ActivityThread.main(ActivityThread.java:7050)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:494)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:965)\r\n     Caused by: java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.createModelWithBuffer(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:72)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:52)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:114)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:131)\r\n        \t... 69 more\r\nE/System: Uncaught exception thrown by finalizer\r\nE/System: java.lang.NullPointerException: Attempt to invoke virtual method 'void org.tensorflow.lite.NativeInterpreterWrapper.close()' on a null object reference\r\n        at org.tensorflow.lite.Interpreter.close(Interpreter.java:252)\r\n        at org.tensorflow.lite.Interpreter.finalize(Interpreter.java:259)\r\n        at java.lang.Daemons$FinalizerDaemon.doFinalize(Daemons.java:250)\r\n        at java.lang.Daemons$FinalizerDaemon.runInternal(Daemons.java:237)\r\n        at java.lang.Daemons$Daemon.run(Daemons.java:103)\r\n        at java.lang.Thread.run(Thread.java:764)\r\n", "comments": ["Did you change configuration values such as the [`TF_OD_API_INPUT_SIZE`](https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/android/app/src/main/java/org/tensorflow/lite/examples/detection/DetectorActivity.java#L53)?", "yes I have change TF_OD_API_INPUT_SIZE =320 ,the same like ssd_mobilenet_v2_mnasfpn_shared_box_predictor_320x320_coco_sync_2020_05_18.tar.gz pipconfig input_size", "if you checked the `ssd_mobilenet_v2_mnasfpn_shared_box_predictor_320x320_coco_sync_2020_05_18.tar.gz` carefully, you can see there are at least 3 differences.\r\n\r\n1. the input size 320x320 vs 320x320\r\n2. float vs. quantized model\r\n3. number of detections (`max_number_of_boxes` in the pipeline.config): 100 vs. 10\r\n\r\nYou should change corresponding configurations / settings in the source code.\r\n", "I have chaged that but still get the same error", " @iot-hunter Could you please let us know if you still need help on this ? if it is resolved then please feel free to move this issue to close status ? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40949\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40949\">No</a>\n"]}, {"number": 40948, "title": "Add gcs read only memory region", "body": "@mihaimaruseac \r\nThis PR adds memory region. Test will be added later since Tensorflow failed to build on Windows and I got a problem with curl on my linux machine", "comments": []}, {"number": 40947, "title": "Tensorflow2.2 not installing on python 3.8", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip3 install tensorflow\r\n- TensorFlow version:2.2.0\r\n- Python version:3.8\r\n- Installed using virtualenv? pip? conda?:using pip 20.1.1\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI am getting this error even i satisfied all the requirement\r\n\r\nERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\r\nERROR: No matching distribution found for tensorflow\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40947\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40947\">No</a>\n"]}, {"number": 40946, "title": "bazel build label_image", "body": "i change some code in label_image/main.cc,   when i use \r\n\r\nbazel build //tensorflow/examples/label_image:label_image\r\n\r\ncommand to compile label_image,  it builds all the source code. how can I only compile this one specific file \uff08label_image/main.cc\uff09?\r\nthanks ", "comments": ["@guxiwuruo \r\nCould you please share the stand alone indented code or a colab gist with the error faced or share colab gist with error faced.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing this issue as it has been inactive for more than 2 weeks. Please add additional comments for us to open this issue again. Thanks!"]}, {"number": 40945, "title": "Error:while importing Tensorflow in Jupyter notebook", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA\r\n- TensorFlow installed from (source or binary):NA\r\n- TensorFlow version:2.2\r\n- Python version:3.7\r\n- Installed using virtualenv? pip? conda?:pip\r\n- Bazel version (if compiling from source):NA\r\n- GCC/Compiler version (if compiling from source):NA\r\n- CUDA/cuDNN version:NA\r\n- GPU model and memory:NA\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhile i am trying to import Tensorflow in Jupyter notebook i am facing this issue.I have uninstalled anaconda and reinstalled ,still i am facing this issue.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nimport tensorflow as tf\r\n\r\n**Any other info / logs**---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n~/.local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59 \r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\n~/anaconda3/lib/python3.7/imp.py in load_module(name, file, filename, details)\r\n    241         else:\r\n--> 242             return load_dynamic(name, filename, file)\r\n    243     elif type_ == PKG_DIRECTORY:\r\n\r\n~/anaconda3/lib/python3.7/imp.py in load_dynamic(name, path, file)\r\n    341             name=name, loader=loader, origin=path)\r\n--> 342         return _load(spec)\r\n    343 \r\n\r\nImportError: libhipsparse.so.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-64156d691fe5> in <module>\r\n----> 1 import tensorflow as tf\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/__init__.py in <module>\r\n     39 import sys as _sys\r\n     40 \r\n---> 41 from tensorflow.python.tools import module_util as _module_util\r\n     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader\r\n     43 \r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/python/__init__.py in <module>\r\n     48 import numpy as np\r\n     49 \r\n---> 50 from tensorflow.python import pywrap_tensorflow\r\n     51 \r\n     52 # Protocol buffers\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>\r\n     67 for some common reasons and solutions.  Include the entire stack trace\r\n     68 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 69   raise ImportError(msg)\r\n     70 \r\n     71 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/ravikrishnak/.local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/ravikrishnak/.local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/ravikrishnak/.local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/ravikrishnak/anaconda3/lib/python3.7/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/ravikrishnak/anaconda3/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libhipsparse.so.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "@ravikairam I guess this is an issue with mixing two or more versions and may not be related to Jupyter notebook. Can you confirm it by running directly in the python IDE? Just start python at the command prompt and execute `import tensorflow as tf`. Please let us know what you notice.\r\n\r\nI had similar issue with windows and I noticed multiple versions of Tensorflow under `site-packages/`. \r\n\r\nI followed the following approach and successfully installed TF.\r\n\r\n1. Check multiple tensorflow folder under 'site-packages/` folder and delete them\r\n2. check environmental file and check for `path` system variable, if the path are not pointing to correct version of software (python), then adjust accordingly\r\n3. Uninstalled tensorflow-->uninstalled python-->restart computer\r\n4. reinstall python--> set and check system variables in the environmental file\r\n5. install TF and then check whether everything is working as expected or not.\r\n\r\nPlease follow this approach and let us know if you have any problems. Thanks! ", "Please close all sessions of Jupyter Notebook and relaunch. Try to import again as a first step and it should work fine.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40945\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40945\">No</a>\n"]}, {"number": 40944, "title": "tensorflow:Layer will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria (using with GRU layer and dropout)", "body": "When trying to follow along F. Chollet's \"Deep Learning with Python\" listing 6.40 I encounter this warning:\r\n`WARNING:tensorflow:Layer gru_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU`\r\nafter executing the following code:\r\n```python\r\ninput_tensor = layers.Input((None, float_data.shape[-1]))\r\nkmodel = layers.GRU(32, dropout=0.2, recurrent_dropout=0.2)(input_tensor)\r\noutput_tensor = layers.Dense(1)(kmodel)\r\nmodel = models.Model(input_tensor, output_tensor)\r\n```\r\nMy imports are:\r\n```python\r\nimport os\r\n\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nfrom typing import Tuple\r\n\r\nfrom tensorflow.keras import models, layers\r\nfrom tensorflow.keras.optimizers import RMSprop\r\n```\r\n\r\nNote that if I don't use `dropout` and `recurrent_dropout` in the `GRU` layer everything works fine and fast. In the case I do use dropout like in the code above, it still works but with **very** slow performance.\r\n\r\nSystem information:\r\nPython 3.7.7\r\ntensorflow-gpu 2.2.0\r\nGPU: Cuda compilation tools, release 10.1, V10.1.243 on GeForce RTX 2080 Ti 11016MiB\r\nOS: Ubuntu 18.04.4 LTS\r\n", "comments": ["@oren0e,\r\nOn running the code I am facing an error stating `NameError: name 'float_data' is not defined`.\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and the dataset you are using. Thanks!", "Sure. This is taken from F. Chollet's book:\r\nFirst download the data:\r\n`wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip`\r\nThen unzip it to a folder and specify the full path to this folder in `data_dir`.\r\nThe code:\r\n```python\r\nimport os\r\n\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nfrom typing import Tuple\r\n\r\nfrom tensorflow.keras import models, layers\r\nfrom tensorflow.keras.optimizers import RMSprop\r\nfrom tensorflow.keras.datasets import imdb\r\nfrom tensorflow.keras.preprocessing import sequence\r\n\r\ndata_dir = '**path_to_your_folder**'\r\nfname = os.path.join(data_dir, 'jena_climate_2009_2016.csv')\r\n\r\nf = open(fname)\r\ndata = f.read()\r\nf.close()\r\n\r\nlines = data.split('\\n')\r\nheader = lines[0].split(',')\r\nlines = lines[1:]\r\nprint(header)\r\nprint(len(lines))\r\n\r\n# convert the data into numpy array\r\nfloat_data = np.zeros((len(lines), len(header) - 1))\r\nfor i, line in enumerate(lines):\r\n    values = [float(x) for x in line.split(',')[1:]]    # drop the timestamp\r\n    float_data[i, :] = values\r\n\r\ntemp = float_data[:, 1]  # temperature (in degrees celsius)\r\nplt.plot(range(len(temp)), temp)\r\nplt.show()\r\n\r\n# first 10 days (temp is recorded every 10 minutes))\r\nplt.plot(range(1440), temp[:1440])\r\nplt.show()\r\n\r\nmean = float_data[:200000].mean(axis=0)\r\nfloat_data -= mean\r\nstd = float_data[:200000].std(axis=0)\r\nfloat_data /= std\r\n\r\n\r\ndef generator(data: np.ndarray, lookback: int, delay: int, min_index: int, max_index: int,\r\n              shuffle: bool = False, batch_size: int = 128, step: int = 6) -> Tuple[np.ndarray, np.ndarray]:\r\n    if max_index is None:\r\n        max_index = len(data) - delay - 1\r\n    i = min_index + lookback\r\n    while 1:\r\n        if shuffle:\r\n            rows = np.random.randint(min_index + lookback, max_index, size=batch_size)\r\n        else:\r\n            if i + batch_size >= max_index:\r\n                i = min_index + lookback\r\n            rows = np.arange(i, min(i + batch_size, max_index))\r\n            i += len(rows)\r\n\r\n        samples = np.zeros((len(rows), lookback // step, data.shape[-1]))\r\n        targets = np.zeros((len(rows),))\r\n        for j, row in enumerate(rows):\r\n            indices = range(rows[j] - lookback, rows[j], step)\r\n            samples[j] = data[indices]\r\n            targets[j] = data[rows[j] + delay][1]\r\n        yield samples, targets\r\n\r\nlookback = 1440\r\nstep = 6\r\ndelay = 144\r\nbatch_size = 128\r\n\r\ntrain_gen = generator(float_data, lookback=lookback, delay=delay, min_index=0, max_index=200000,\r\n                      shuffle=True, step=step, batch_size=batch_size)\r\nval_gen = generator(float_data, lookback=lookback, delay=delay, min_index=200001, max_index=300000,\r\n                      shuffle=True, step=step, batch_size=batch_size)\r\ntest_gen = generator(float_data, lookback=lookback, delay=delay, min_index=300001, max_index=None,\r\n                      shuffle=True, step=step, batch_size=batch_size)\r\n\r\n# these are the forecasted parts that we will want to look at\r\nval_steps = (300000 - 200001 - lookback) // batch_size   # how many steps to draw from val_gen in order to see\r\n                                            # the entire validation set\r\ntest_steps = (len(float_data) - 300001 - lookback) // batch_size     # \" \" for test\r\n\r\ninput_tensor = layers.Input((None, float_data.shape[-1]))\r\nkmodel = layers.GRU(32, recurrent_dropout=0.2, dropout=0.2)(input_tensor)\r\noutput_tensor = layers.Dense(1)(kmodel)\r\nmodel = models.Model(input_tensor, output_tensor)\r\n\r\nmodel.compile(optimizer=RMSprop(), loss='mae')\r\nhistory = model.fit_generator(train_gen, steps_per_epoch=500,\r\n                              epochs=40,\r\n                              validation_data=val_gen,\r\n                              validation_steps=val_steps)\r\n```\r\nIn my understanding, the problem is with the `recurrent_dropout` argument.", "Was able to reproduce the issue with TF v2.2 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/730299a66d1086a1986f69b830d5b999/40944-gpu.ipynb). Thanks!", "@oren0e,\r\nPlease take a look at the [requirements](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU#used-in-the-notebooks_1) to use cuDNN implementation for the GRU layer and let us know if it helps. Thanks! ", "@amahendrakar,\r\nThe link you have provided just reconfirms that the problem is with the `recurrent_dropout` argument. The requirement is to set it to  0, i.e., not using it. I think this should be implemented in the TF backend since it is an important option that highly affects performance (training time).", "@oren0e, this issue is expected since the recurrent dropout is not implemented in the Nvidia's cudnn kernel. We have to fallback to use the generic kernel when user specify the recurrent dropout. See the docstring of GRU and LSTM for the criteria of using cudnn kernel. https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU#used-in-the-notebooks_1. "]}, {"number": 40943, "title": "Cadence HiFi4 NN Library v2.2.0 update", "body": "The following changes are done to the HiFi4 implementation of TF Lite Micro\r\n\r\n1. Update the xtensa_hifi kernel files and the make setup to use HiFi4 NN Library v2.2.0 .\r\n2. Update the kernel files as per the latest reference implementation.\r\n3. Add xtenas_hifi kernel implementations for the add and mul operators.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40943) for more info**.\n\n<!-- need_sender_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40943) for more info**.\n\n<!-- ok -->", "I have updated the PR to address the above suggestions, please review."]}, {"number": 40942, "title": "High memory consumption with model.fit in TF 2.x", "body": "**System information**\r\n\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: CentOS Linux 7\r\n- Mobile device: Not verified on mobile devices\r\n- TensorFlow installed from: binary, via `pip install tf-nightly`\r\n- TensorFlow version: 2.5.0-dev20200626\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.1 / 7\r\n- GPU model and memory: Tesla V100 32 GB\r\n\r\n**Describe the current behavior**\r\n\r\nModel training with the Keras API consumes high amount of system memory. It looks like the memory used by `model.fit` is proportional to the size of the training data provided as numpy arrays, with the proportionality constant being approximately 1. In other words, if the numpy arrays `x` and `y` are, say, 8 GB in total, then `model.fit(x,y,...)` will use another 8 GB (plus some overhead). So the memory usage by `model.fit` uses is twice the data size (plus some overhead).\r\n\r\nThe same concerns the validation data. If validation data are passed as numpy arrays to `model.fit` via the argument `validation_data`, then the memory use of `model.fit` seems to duplicate the size of the validation data arrays.\r\n\r\nThe described effect is also present if I wrap the numpy arrays containing the data in TF Datasets.\r\n\r\nIn the code attached below, one may change the variable `K` to vary the size of the data and test the above described behavior. It is straightforward to estimate the data size (e.g. with `K=5000` the data arrays in the below code should be ca. 7.32 GB in total). The whole Python process associated with this code uses approximately twice this much RAM plus some overhead independent of the data size. One may comment out the line containing `model.fit` to check that it is the point at which the high memory consumption starts.\r\n\r\n**Describe the expected behavior**\r\n\r\nIt would be reasonable to expect that the memory usage by the test code was approximately the data size plus some overhead independent of the data size (not twice the data size plus overhead).\r\n\r\n**A bit of history**\r\n\r\nThis is a continuation of the issue #35030, concerning TF 2.0 and 2.1. I opened the latter issue in December 2019 and now @karmel have stated that that issue is very long and asked me to test if the issue persists in TF-nightly and open a new issue if necessary. So yes, the problem persists, and here I open a new issue.\r\n\r\nThe problem appeared first in the release `2.0.0-rc0`. In the earlier releases up to `2.0.0-b1` inclusive the memery usage by the below test code was ca. the size of the data arrays plus an overhead independent of the data size. Starting from `2.0.0-rc0` it became twice the data size plus overhead and it was true at least until `2.1.0`.\r\n\r\nNext, in `2.2.0`, the situation changed a bit:\r\n\r\n- When using numpy arrays to pass data to `model.fit`, there was a memory leak about 0.5 x data size in each epoch. In other words, if the size of the data arrays was ca. 8 GB, then the memory usage was increasing ca. 4 GB each epoch.\r\n- When wrapping the data arrays in TF datasets and then passing to `model.fit`, then the behavior was the same in TF 2.2 as in 2.1 and 2.0, namely the memory usage was twice the data size plus overhead.\r\n\r\nNow, in the nightly release `2.5.0-dev20200626` we are back to the previous situation, namely the memory usage is twice the data size plus overhead, regardless of whether numpy arrays or datasets are used to pass the data to `model.fit`.\r\n\r\n**An important note on reproducibility**\r\n\r\nThe issue has occurred to be not reproducible in colab! In #35030, I reported the issue for my local machine and some other participants also managed to reproduce it locally but not in colab. Some were trying to reproduce it in colab without success. Similarly, the results I report now are not from colab.\r\n\r\nAlso, for some reason the issue cannot be captured when using `libmemusage.so` to measure the memory usage. To capture the issue, I use `ps au` in Linux terminal or Python module `psutil`.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nSince this issue is in fact a continuation of #35030, I use the same test code here.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Input, Lambda, Conv2D\r\n\r\nprint(\"Tensorflow version: {}\".format(tf.__version__),flush=True)\r\n\r\nK = 5000 # Number of images\r\nN = 512  # Image size\r\n\r\nMAX_SIGNAL = 5000 # The values of the training data range from 0 to this\r\n\r\ndef build_model():\r\n  '''Create a simple test model.'''\r\n  \r\n  inputs = Input((N,N,1))\r\n  s = Lambda(lambda x: x / MAX_SIGNAL) (inputs)\r\n  s = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(s)\r\n  outputs = s\r\n\r\n  return Model(inputs=[inputs], outputs=[outputs])\r\n\r\n# Generate some random data\r\nx_train = np.random.randint(MAX_SIGNAL+1,size=(K,N,N,1),dtype=np.uint16) # Should be 2 560 000 kB\r\ny_train = np.random.randint(1+1         ,size=(K,N,N,1),dtype=np.bool)   # Should be 1 280 000 kB\r\nx_val   = np.random.randint(MAX_SIGNAL+1,size=(K,N,N,1),dtype=np.uint16) # Should be 2 560 000 kB\r\ny_val   = np.random.randint(1+1         ,size=(K,N,N,1),dtype=np.bool)   # Should be 1 280 000 kB\r\n# In total, the above arrays should be 7 680 000 kB\r\n\r\nmodel = build_model()\r\n\r\noptimizer = tf.keras.optimizers.Adam()\r\nloss = tf.keras.losses.BinaryCrossentropy()\r\n\r\nmodel.compile(optimizer=optimizer, loss=loss)\r\nmodel.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=10)\r\n```\r\n\r\nThe above is meant to reproduce the issue with data passed to `model.fit` as numpy arrays. To test the behavior with TF datasets, replace the last line with the following:\r\n\r\n```python\r\nds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)).batch(8)\r\nds_val = tf.data.Dataset.from_tensor_slices((x_val,y_val)).batch(8)\r\nmodel.fit(ds_train, validation_data=ds_val, epochs=10)\r\n```", "comments": ["@gdudziuk \r\nI ran the code shared above please fin the [gist here](https://colab.research.google.com/gist/Saduf2019/12f843ce9e8e30807edbe271d307a5e5/untitled251.ipynb), Please let us know if it confirms your issue.", "@Saduf2019, thanks for your gist but it doesn't confirm the issue. As I have explicitly stated in the issue description, the problem is not reproducible in colab. It is possible to reproduce the issue on local machines. It has been confirmed for older versions of TF 2.x in #35030.\r\n\r\nAlso, for some reason, not every memory profiling tool can capture the issue. I can capture it measuring the memory usage with `ps au` in the Linux terminal or with the Python module `psutil`. Some participants of #35030 were using Python's `memory_profiler` to capture the issue. On the other hand, it was not possible to reproduce it with `libmemusage.so`.", "I guess I should carry out some memory measurements in my test code. So, here is the original test code enriched with RSS memory measurements:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport psutil\r\nimport os\r\n\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Input, Lambda, Conv2D\r\nfrom tensorflow.keras.callbacks import Callback\r\n\r\nprint(\"Tensorflow version: {}\".format(tf.__version__),flush=True)\r\n\r\nK = 5000 # Number of images\r\nN = 512  # Image size\r\n\r\nMAX_SIGNAL = 5000 # The values of the training data range from 0 to this\r\n\r\nclass MemoryUsageCallback(Callback):\r\n  '''Monitor memory usage on epoch begin and end.'''\r\n\r\n  def on_epoch_begin(self,epoch,logs=None):\r\n    print('**Epoch {}**'.format(epoch))\r\n    print('Memory usage on epoch begin: {}'.format(psutil.Process(os.getpid()).memory_info().rss))\r\n\r\n  def on_epoch_end(self,epoch,logs=None):\r\n    print('Memory usage on epoch end:   {}'.format(psutil.Process(os.getpid()).memory_info().rss))\r\n    \r\ndef build_model():\r\n  '''Create a simple test model.'''\r\n  \r\n  inputs = Input((N,N,1))\r\n  s = Lambda(lambda x: x / MAX_SIGNAL) (inputs)\r\n  s = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(s)\r\n  outputs = s\r\n\r\n  return Model(inputs=[inputs], outputs=[outputs])\r\n\r\n# Generate some random data\r\nx_train = np.random.randint(MAX_SIGNAL+1,size=(K,N,N,1),dtype=np.uint16) # Should be 2 560 000 kB\r\ny_train = np.random.randint(1+1         ,size=(K,N,N,1),dtype=np.bool)   # Should be 1 280 000 kB\r\nx_val   = np.random.randint(MAX_SIGNAL+1,size=(K,N,N,1),dtype=np.uint16) # Should be 2 560 000 kB\r\ny_val   = np.random.randint(1+1         ,size=(K,N,N,1),dtype=np.bool)   # Should be 1 280 000 kB\r\n# In total, the above arrays should be 7 680 000 kB\r\n\r\nmodel = build_model()\r\n\r\ncallbacks = [MemoryUsageCallback()]\r\noptimizer = tf.keras.optimizers.Adam()\r\nloss = tf.keras.losses.BinaryCrossentropy()\r\n\r\nmodel.compile(optimizer=optimizer, loss=loss)\r\nmodel.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=10, callbacks=callbacks, verbose=0)\r\n```\r\n\r\nto test the memory usage with TF datasets, replace the last line with the following:\r\n\r\n```python\r\nds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)).batch(8)\r\nds_val = tf.data.Dataset.from_tensor_slices((x_val,y_val)).batch(8)\r\nmodel.fit(ds_train, validation_data=ds_val, batch_size=8, epochs=10, callbacks=callbacks, verbose=0)\r\n```", "I have tested this code on a different machine (Kubuntu 18.04, GeForce GTX 1050 Ti 4GB). The tests have been carried out with tf-nightly version `2.4.0-dev20200702`. The originally reported version `2.5.0-dev20200626` seems to be not available via pip at the moment.\r\n\r\nOutput for numpy arrays:\r\n\r\n```\r\nTensorflow version: 2.4.0-dev20200702\r\n**Epoch 0**\r\nMemory usage on epoch begin: 12781158400\r\nMemory usage on epoch end:   18143223808\r\n**Epoch 1**\r\nMemory usage on epoch begin: 18143223808\r\nMemory usage on epoch end:   18434109440\r\n**Epoch 2**\r\nMemory usage on epoch begin: 18434109440\r\nMemory usage on epoch end:   18492579840\r\n**Epoch 3**\r\nMemory usage on epoch begin: 18492579840\r\nMemory usage on epoch end:   18593259520\r\n**Epoch 4**\r\nMemory usage on epoch begin: 18593259520\r\nMemory usage on epoch end:   18589122560\r\n**Epoch 5**\r\nMemory usage on epoch begin: 18589122560\r\nMemory usage on epoch end:   18598289408\r\n**Epoch 6**\r\nMemory usage on epoch begin: 18598289408\r\nMemory usage on epoch end:   18610733056\r\n**Epoch 7**\r\nMemory usage on epoch begin: 18610733056\r\nMemory usage on epoch end:   18616848384\r\n**Epoch 8**\r\nMemory usage on epoch begin: 18616848384\r\nMemory usage on epoch end:   18624794624\r\n**Epoch 9**\r\nMemory usage on epoch begin: 18624794624\r\nMemory usage on epoch end:   18623217664\r\n```\r\n\r\nThe output for datasets:\r\n\r\n```\r\nTensorflow version: 2.4.0-dev20200702\r\n**Epoch 0**\r\nMemory usage on epoch begin: 16711708672\r\nMemory usage on epoch end:   17472192512\r\n**Epoch 1**\r\nMemory usage on epoch begin: 17472225280\r\nMemory usage on epoch end:   17472245760\r\n**Epoch 2**\r\nMemory usage on epoch begin: 17472266240\r\nMemory usage on epoch end:   17472266240\r\n**Epoch 3**\r\nMemory usage on epoch begin: 17472266240\r\nMemory usage on epoch end:   17490911232\r\n**Epoch 4**\r\nMemory usage on epoch begin: 17490911232\r\nMemory usage on epoch end:   17490911232\r\n**Epoch 5**\r\nMemory usage on epoch begin: 17490911232\r\nMemory usage on epoch end:   17490911232\r\n**Epoch 6**\r\nMemory usage on epoch begin: 17490911232\r\nMemory usage on epoch end:   17490911232\r\n**Epoch 7**\r\nMemory usage on epoch begin: 17490911232\r\nMemory usage on epoch end:   17490911232\r\n**Epoch 8**\r\nMemory usage on epoch begin: 17490911232\r\nMemory usage on epoch end:   17490911232\r\n**Epoch 9**\r\nMemory usage on epoch begin: 17490911232\r\nMemory usage on epoch end:   17490911232\r\n```\r\nOne can check how the memory usage changes manipulating the variable `K` in the code. The above results are for `K=5000`, for which the size of the data arrays is 7 680 000 kB. The resulting memory usage is definitely too high.\r\n\r\nLater I will try test on the original machine reported for this issue (Centos 7, Tesla V100 32 GB).", "> I have tested this code on a different machine (Kubuntu 18.04, GeForce GTX 1050 Ti 4GB). The tests have been carried out with tf-nightly version `2.4.0-dev20200702`. The originally reported version `2.5.0-dev20200626` seems to be not available via pip at the moment.\r\n> \r\n> Output for numpy arrays:\r\n> \r\n> ```\r\n> Tensorflow version: 2.4.0-dev20200702\r\n> **Epoch 0**\r\n> Memory usage on epoch begin: 12781158400\r\n> Memory usage on epoch end:   18143223808\r\n> **Epoch 1**\r\n> Memory usage on epoch begin: 18143223808\r\n> Memory usage on epoch end:   18434109440\r\n> **Epoch 2**\r\n> Memory usage on epoch begin: 18434109440\r\n> Memory usage on epoch end:   18492579840\r\n> **Epoch 3**\r\n> Memory usage on epoch begin: 18492579840\r\n> Memory usage on epoch end:   18593259520\r\n> **Epoch 4**\r\n> Memory usage on epoch begin: 18593259520\r\n> Memory usage on epoch end:   18589122560\r\n> **Epoch 5**\r\n> Memory usage on epoch begin: 18589122560\r\n> Memory usage on epoch end:   18598289408\r\n> **Epoch 6**\r\n> Memory usage on epoch begin: 18598289408\r\n> Memory usage on epoch end:   18610733056\r\n> **Epoch 7**\r\n> Memory usage on epoch begin: 18610733056\r\n> Memory usage on epoch end:   18616848384\r\n> **Epoch 8**\r\n> Memory usage on epoch begin: 18616848384\r\n> Memory usage on epoch end:   18624794624\r\n> **Epoch 9**\r\n> Memory usage on epoch begin: 18624794624\r\n> Memory usage on epoch end:   18623217664\r\n> ```\r\n> \r\n> The output for datasets:\r\n> \r\n> ```\r\n> Tensorflow version: 2.4.0-dev20200702\r\n> **Epoch 0**\r\n> Memory usage on epoch begin: 16711708672\r\n> Memory usage on epoch end:   17472192512\r\n> **Epoch 1**\r\n> Memory usage on epoch begin: 17472225280\r\n> Memory usage on epoch end:   17472245760\r\n> **Epoch 2**\r\n> Memory usage on epoch begin: 17472266240\r\n> Memory usage on epoch end:   17472266240\r\n> **Epoch 3**\r\n> Memory usage on epoch begin: 17472266240\r\n> Memory usage on epoch end:   17490911232\r\n> **Epoch 4**\r\n> Memory usage on epoch begin: 17490911232\r\n> Memory usage on epoch end:   17490911232\r\n> **Epoch 5**\r\n> Memory usage on epoch begin: 17490911232\r\n> Memory usage on epoch end:   17490911232\r\n> **Epoch 6**\r\n> Memory usage on epoch begin: 17490911232\r\n> Memory usage on epoch end:   17490911232\r\n> **Epoch 7**\r\n> Memory usage on epoch begin: 17490911232\r\n> Memory usage on epoch end:   17490911232\r\n> **Epoch 8**\r\n> Memory usage on epoch begin: 17490911232\r\n> Memory usage on epoch end:   17490911232\r\n> **Epoch 9**\r\n> Memory usage on epoch begin: 17490911232\r\n> Memory usage on epoch end:   17490911232\r\n> ```\r\n> \r\n> One can check how the memory usage changes manipulating the variable `K` in the code. The above results are for `K=5000`, for which the size of the data arrays is 7 680 000 kB. The resulting memory usage is definitely too high.\r\n> \r\n> Later I will try test on the original machine reported for this issue (Centos 7, Tesla V100 32 GB).\r\n\r\nCan you try adding the following in the beginning on your code and then run it on a multi-core machine?\r\n\r\n```\r\ntf.config.threading.set_inter_op_parallelism_threads(1)\r\n\r\ntf.config.threading.set_intra_op_parallelism_threads(1)\r\n```\r\n\r\nIf after calling the above you don't have high memory usage, then the underlying issue could be related to https://github.com/tensorflow/tensorflow/issues/41718", "Thanks for your suggestion but as I understand #41718 is a CPU-related issue while the present issue occurs for both CPU and GPU versions of TensorFlow.\r\n\r\nTo be sure, I have just run a test (for both `tf-nightly` and `tf-nightly-cpu`, version `2.4.0-dev20200807`). I can confirm that the issue occurs regardless of whether I add the lines you suggest or no.\r\n\r\nSaid that, there is some small difference for the CPU version. Namely, with your extra lines the memory usage is ca. 1 GB lower than without them.  Interestingly, for the case of passing data as numpy arrays, this effect occurs also with the GPU version. However, lowering the memory usage by 1 GB does not solve the present issue.", "@gdudziuk I have the same issue as you, unable to even finish your example before I run out of memory. However if I use the tensorflow/tensorflow:latest-gpu-jupyter docker image - for example, the problem does not occur. Before I was installing from conda.", "To temporarily escape this problem, the function `gc.collect()` works:\r\n```\r\nimport gc\r\n...\r\nmodel.fit(x=x, y=y)\r\ngc.collect()\r\n```", "@gdudziuk Is this still an issue for you? There were some good performance related updated over the last few months. \r\n\r\nCan you please check with a recent `tf` version and also adding `gc.collect()` as mentioned by @tirear . \r\n\r\nThere ([1](https://github.com/tensorflow/tensorflow/issues/44711#issuecomment-736186819) and [2](https://github.com/tensorflow/tensorflow/issues/41973#issuecomment-668361074)) are couple of responses from @tomerk on performance related issues that might help you. \r\n\r\nPlease verify once and let us know whether the issue persists with recent `tf` versions. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Thank you for the information, @jvishnuvardhan. I will run tests with the most recent TF release in the forthcoming week and let you know about the results.", "I have just run the test with TF `2.4.1`. Unfortunately, the issue is still there (again, for Kubuntu 18.04, GeForce GTX 1050 Ti 4GB). The results are very similar to those reported above for `2.4.0-dev20200702`. I will check with `tf-nightly` in a moment...", "@gdudziuk If possible, Can you please check with `TF2.5rc0` also. Thanks!", "Ok, it was a longer moment, because both TF `2.5.0-rc0` and tf-nightly (`2.6.0-dev20210412` at the moment) depend on `libcusolver.so.11` while it seems that the latter is not available via APT (or at least I haven't succeeded to install it). So eventually, I have just run the test code on my Kubuntu 18.04 machine in CPU mode only.\r\n\r\nAnd I don't have good news to share. The issue is there, both in `2.5.0-rc0` and in `2.6.0-dev20210412`, and both for data passed to `model.fit` as numpy arrays and as tf.Dataset. The output is very similar to the one [already posted in this issue](https://github.com/tensorflow/tensorflow/issues/40942#issuecomment-652975769), but for reference I attach it as text files.\r\n\r\n[TF-2.5.0-rc0_data_as_datasets.txt](https://github.com/tensorflow/tensorflow/files/6299769/TF-2.5.0-rc0_data_as_datasets.txt)\r\n[TF-2.5.0-rc0_data_as_numpy.txt](https://github.com/tensorflow/tensorflow/files/6299771/TF-2.5.0-rc0_data_as_numpy.txt)\r\n[TF-2.6.0-dev20210412_data_as_datasets.txt](https://github.com/tensorflow/tensorflow/files/6299773/TF-2.6.0-dev20210412_data_as_datasets.txt)\r\n[TF-2.6.0-dev20210412_data_as_numpy.txt](https://github.com/tensorflow/tensorflow/files/6299774/TF-2.6.0-dev20210412_data_as_numpy.txt)\r\n\r\n\r\n", "@tomerk Any insights on root-cause of this performance issue? Thanks!", "Calling a fit (CPU, for testing mostly) a ESRGAN GAN model with 46M parameters can suddently consume +100G bytes of RAM and keep growing, finally it crashes machine with 256G :( ... ! TF 2.4.1.\r\n\r\nHELP !\r\n\r\nSteve", "Found multiple copies during training\r\ndata size: 20GB\r\nmemory used\uff1a70GB", "I encountered the issue while on version `2.5.0-dev20200626` but it seems that the release version of `2.5.0` has fixed ihis..\r\nThe training function I've used is as follows along with a routine batching loop.\r\n```python\r\n@tf.function\r\ndef train_step(images, labels):\r\n    with tf.GradientTape() as tape:\r\n      predictions = model(images, training=True) #Keras model subclass\r\n      loss = loss_object(labels, predictions)\r\n    gradients = tape.gradient(loss, model.trainable_variables)\r\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n    train_loss(loss)\r\n```\r\nI however have not tested it against `model.fit` or `train_on_batch`. ", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Did 2.7 fix this issue?", "No, it doesn't. With TF 2.7 I get scores similar as before.\r\n\r\nThe output for numpy arrays:\r\n\r\n```\r\n**Epoch 0**\r\nMemory usage on epoch begin: 12127035392\r\nMemory usage on epoch end:   18409664512\r\n**Epoch 1**\r\nMemory usage on epoch begin: 18409664512\r\nMemory usage on epoch end:   18484998144\r\n**Epoch 2**\r\nMemory usage on epoch begin: 18484998144\r\nMemory usage on epoch end:   18638782464\r\n**Epoch 3**\r\nMemory usage on epoch begin: 18638782464\r\nMemory usage on epoch end:   18586607616\r\n**Epoch 4**\r\nMemory usage on epoch begin: 18586607616\r\nMemory usage on epoch end:   18652651520\r\n**Epoch 5**\r\nMemory usage on epoch begin: 18652651520\r\nMemory usage on epoch end:   18744152064\r\n**Epoch 6**\r\nMemory usage on epoch begin: 18744152064\r\nMemory usage on epoch end:   18654404608\r\n**Epoch 7**\r\nMemory usage on epoch begin: 18654404608\r\nMemory usage on epoch end:   18792071168\r\n**Epoch 8**\r\nMemory usage on epoch begin: 18792071168\r\nMemory usage on epoch end:   18691829760\r\n**Epoch 9**\r\nMemory usage on epoch begin: 18691829760\r\nMemory usage on epoch end:   18831183872\r\n```\r\n\r\nThe output for datasets:\r\n\r\n```\r\n**Epoch 0**\r\nMemory usage on epoch begin: 16053436416\r\nMemory usage on epoch end:   17169522688\r\n**Epoch 1**\r\nMemory usage on epoch begin: 17170001920\r\nMemory usage on epoch end:   17233149952\r\n**Epoch 2**\r\nMemory usage on epoch begin: 17233141760\r\nMemory usage on epoch end:   17204260864\r\n**Epoch 3**\r\nMemory usage on epoch begin: 17204252672\r\nMemory usage on epoch end:   17161564160\r\n**Epoch 4**\r\nMemory usage on epoch begin: 17161555968\r\nMemory usage on epoch end:   17226149888\r\n**Epoch 5**\r\nMemory usage on epoch begin: 17226141696\r\nMemory usage on epoch end:   17219477504\r\n**Epoch 6**\r\nMemory usage on epoch begin: 17219497984\r\nMemory usage on epoch end:   17210036224\r\n**Epoch 7**\r\nMemory usage on epoch begin: 17210028032\r\nMemory usage on epoch end:   17234489344\r\n**Epoch 8**\r\nMemory usage on epoch begin: 17234481152\r\nMemory usage on epoch end:   17211543552\r\n**Epoch 9**\r\nMemory usage on epoch begin: 17211535360\r\nMemory usage on epoch end:   17218691072\r\n```\r\n\r\n(Kubuntu 18.04, CPU mode)", "Duplicate of #https://github.com/keras-team/keras/issues/15887\r\n\r\nLet's track the progress in the above lined issue. Thanks!"]}, {"number": 40941, "title": "fix nvcc compiler-options", "body": "ref. https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/#options-for-passing-specific-phase-options-compiler-options\r\n\r\nThe host compiler parameter should be separated by commas instead of spaces.", "comments": []}, {"number": 40940, "title": "Question about how tensorflow mlir library works ?", "body": "I was wondering as to how the tensorflow mlir library is used to reduce code from the tf dialect to an executable for a gpu, which dialects are involved in this conversion ?, are there multiple choices and are there any advantages to each of them ?. (and does ths have anything to do with the gpu dialect specified in the mlir documentation : https://mlir.llvm.org/docs/Dialects/GPU/)", "comments": ["Hey,\r\n\r\nYou will get more responses on mlir@tensorflow.org as more folks monitor it and the folks working on the GPU dialect MLIR core side are active there too. Would you mind re-asking there instead if you want more info?\r\n\r\nIn short/from a high-level there are multiple dialects involved depends on the lowering path (e.g., Affine, LinAlg, Shape, GPU, Standard, HLO, LLVM are along some of those paths) but the exact dialects depend on the lowering path and there are multiple, both as initial staging/ideas develop/technology matures as well as in limit there will still be multiple paths (e.g., if one knew at TF dialect level already the best way to lower an op is to a library call, then one might lower directly - although there are cases where the optimal choice for an op in isolation is not the optimal for the model as a whole, so its not \"trivial\" knowledge at work). The flexibility and ability to incorporate different code-generation strategies at different levels is important to best target the evolving HW space.\r\n\r\nBest,\r\n\r\nJacques "]}]