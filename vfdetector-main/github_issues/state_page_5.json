[{"number": 55417, "title": "How to update tf.Variable inplace using custom op?", "body": "Hi,\r\n\r\nI want to write a custom op which updates the contents of `tf.Variable` inplace on a GPU (cuda) device. I haven't found any relevant documentation to achieve this, hence creating this issue. It'll be great if you can point me out to any relevant doc/code implementation for this.  I can do the same with assign op, but that requires creation of extra tensor and will prefer to update it inplace.\r\n\r\nI looked at the implementation of Adam op registration from [here](https://github.com/tensorflow/tensorflow/blob/94964d0a95ff50a2fc6d1b9856c85e9ce40b2682/tensorflow/core/kernels/training_ops.cc#L3613-L3615) and noticed that the variables to be updated are copied to host memory. Also, it appears the same approach is followed [here](https://github.com/tensorflow/tensorflow/blob/94964d0a95ff50a2fc6d1b9856c85e9ce40b2682/tensorflow/core/kernels/resource_variable_ops.cc#L538) in assign op implementation. \r\n\r\nIs this really needed to update variables and can this be avoided? I suspect this might have some performance implications of copying data to CPU every time it needs to be updated. \r\n\r\nThanks\r\n", "comments": ["@pranavladkat TensorFlow variable is the recommended way to represent a shared, persistent state your program manipulates.For more details could you please have a look at this [link](https://www.tensorflow.org/api_docs/python/tf/Variable) and refer this [thread](https://stackoverflow.com/questions/63878497/how-to-update-trainable-variables-on-my-custom-optimizer-using-tensorflow-2)? Please let us know if it helps?\r\nThanks!\r\n ", "Hi,\r\n\r\nThanks for the response. However it does not answer my question of how to update `tf.Variable` inplace using custom C++ op. If I were to use `var.assign()` op, I will need to return a tf.Tensor from the op and then use that to update variable. That consumes more memory of my use case and training fails with out of memory errors.\r\n\r\nThanks", "@pranavladkat, \r\nHi, Thanks for reporting this issue.\r\nIf you'd like to create an op that isn't covered by the existing TensorFlow library,  you can create a custom C++ op.\r\nHere are some reference documents which help community to create custom of with GPU support \r\n[Create custom op](https://www.tensorflow.org/guide/create_op#gpu_support) and building setting up [custom op](https://github.com/tensorflow/custom-op). Thanks!", "Hi, thanks for the response. I'm using the resourced that you have linked to create custom op. But my question is `How to update tf.Variable inplace using custom op?`\r\n\r\nThanks", "The links which you mentioned does the gpu kernel registration for resource variables. \r\nIt can impact the performance little bit but you can create custom op without registering GPU kernel. \r\nYou can refer [this](https://www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables) document about `ResourceVariable`.\r\nAlso, refer [this](https://www.tensorflow.org/guide/create_op#implement_the_kernel_for_the_op) document on implementing kernel for the custom op.\r\n"]}, {"number": 55414, "title": "Known issues with new RUN_EAGER_OP_AS_FUNCTION feature", "body": "This refers to the unified eager op/tf.function execution mode available for optional use in 2.9.\r\n\r\nTracking known issues here:\r\n\r\n1. With CUDA enabled, unnecessary data transfers to/from the GPU causing additional slowdowns. (already under investigation)\r\n", "comments": []}, {"number": 55411, "title": "[oneDNN] Enable Softmax with oneDNN library", "body": "This PR enables softmax forward op using oneDNN library. It uses in-place computation whenever possible. It improves performance of some models wherein the softmax takes up significant time.\r\n\r\nThe following are performance data on some micro-benchmarks. The performance data were collected on Intel Xeon CPUs using Eigen ThreadPool.\r\n\r\n## Microbenchmarks\r\n\r\nTensor Dims | Numeric Type | Intra-op Threads | HW | oneDNN (ns) | Eigen (ns) | Speedup\r\n-- | -- | -- | -- | -- | -- | --\r\n1x16x384x384 | FLOAT32 | 4 | Xeon 28 core | 435505 | 849837 | 1.95x\r\n16x16x384x384 | FLOAT32 | 4 | Xeon 28 core | 30608250 | 39503349 | 1.29x\r\n32x1008 | FLOAT32 | 1 | Xeon 28 core | 28455 | 33078 | 1.16x\r\n128x1008 | FLOAT32 | 1 | Xeon 28 core | 83065 | 123516 | 1.49x\r\n32x1008 | FLOAT32 | 4 | Xeon 28 core | 30585 | 32165 | 1.05x\r\n128x1008 | FLOAT32 | 4 | Xeon 28 core | 41348 | 127743 | 3.09x\r\n1x16x384x384 | BFLOAT16 | 4 | Xeon 26 core | 430448 | 867489 | 2.02x\r\n16x16x384x384 | BFLOAT16 | 4 | Xeon 26 core | 17430198 | 25598959 | 1.47x\r\n32x1008 | BFLOAT16 | 1 | Xeon 26 core | 31498 | 49853 | 1.58x\r\n128x1008 | BFLOAT16 | 1 | Xeon 26 core | 91812 | 159045 | 1.73x\r\n32x1008 | BFLOAT16 | 4 | Xeon 26 core | 29836 | 49224 | 1.65x\r\n128x1008 | BFLOAT16 | 4 | Xeon 26 core | 43047 | 122662 | 2.85x\r\n\r\nThis PR improves inference performance by 12% on some models that use softmax.", "comments": ["@penpornk When I submitted this PR, github automatically assigned you as reviewer. Anyway, may I request for a review.", "@penpornk Can you please review this PR ? Thank you!"]}, {"number": 55405, "title": "tf.nn.ctc_loss is more numerically unstable when providing sparse instead of dense labels", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS BigSur\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.8\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\n\r\n`tf.nn.ctc_loss` can return `Inf` value for an input example if the labels are given as sparse tensor, while it computes the value correctly for the same inputs when the labels are given as dense tensor. So it appears that the implementation used when providing dense labels is more numerically stable - this occurs only once the logits coming from the model are more \"extreme\", as in, very large or very small. However, this issue can and was encountered by me during training an actual model, which broke training.\r\n\r\nSee example below.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe loss when providing sparse labels is not `Inf` but rather the same value that you get when providing dense labels.\r\n\r\nIf that cannot be achieved, we should offer a fallback mechanism as shown in my example code, where the dense codepath is used whenever an Inf is encountered.\r\n\r\nAt the bare minimum, the docs for `tf.nn.ctc_loss` should warn about the numerical instability of the sparse version.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nlabels = tf.ones((1, 50), dtype=tf.int64)\r\nlogits = tf.concat([tf.ones((1, 1000, 1)), -100 * tf.ones((1, 1000, 1))], axis=2)\r\nlabel_length = tf.constant([50])\r\nlogit_length = tf.constant([1000])\r\n\r\ndense_ctc_losses = tf.nn.ctc_loss(\r\n    labels=labels,\r\n    logits=logits,\r\n    label_length=label_length,\r\n    logit_length=logit_length,\r\n    logits_time_major=False,\r\n    blank_index=0,\r\n)\r\n\r\nsparse_ctc_losses = tf.nn.ctc_loss(\r\n    labels=tf.sparse.from_dense(tf.cast(tf.convert_to_tensor(labels), dtype=tf.int32)),\r\n    logits=logits,\r\n    label_length=label_length,\r\n    logit_length=tf.cast(logit_length, tf.int32),\r\n    logits_time_major=False,\r\n    blank_index=0,\r\n)\r\n\r\nctc_losses = tf.cond(\r\n    tf.math.reduce_all(tf.math.is_finite(sparse_ctc_losses)),\r\n    lambda: sparse_ctc_losses,\r\n    lambda: tf.nn.ctc_loss(\r\n        labels=labels,\r\n        logits=logits,\r\n        label_length=label_length,\r\n        logit_length=logit_length,\r\n        logits_time_major=False,\r\n        blank_index=0,\r\n    ),\r\n)\r\n\r\n# This works fine and produces a proper value\r\nprint(f\"Dense labels: {dense_ctc_losses}\")\r\n# Sparse version gives us an inf value\r\nprint(f\"Sparse labels: {sparse_ctc_losses}\")\r\n# Uses sparse, falls back to dense if sparse gives Inf\r\nprint(f\"Fallback CTC: {ctc_losses}\")\r\n```\r\n\r\nOutput:\r\n```\r\n2022-03-28 14:35:13.481252: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nWARNING:tensorflow:From /Users/dstoller/.pyenv/versions/lyric-align-baseline/lib/python3.7/site-packages/tensorflow/python/ops/ctc_ops.py:1443: alias_inplace_add (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPrefer tf.tensor_scatter_nd_add, which offers the same functionality with well-defined read-write semantics.\r\nWARNING:tensorflow:From /Users/dstoller/.pyenv/versions/lyric-align-baseline/lib/python3.7/site-packages/tensorflow/python/ops/ctc_ops.py:1426: alias_inplace_update (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPrefer tf.tensor_scatter_nd_update, which offers the same functionality with well-defined read-write semantics.\r\n2022-03-28 14:35:15.541318: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.\r\nDense labels: [706.8936]\r\nSparse labels: [inf]\r\nCombined CTC: [706.8936]\r\n```\r\n", "comments": ["To add on to this, this is a serious issue because the sparse label version currently runs 50x faster on GPU than the dense label version, but due to the above issue, cannot be used", "@chunduriv I was able to replicate the issue on colab using TF v2.8.0 and tf-nightly ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/5253243ce6846a9b22215a120182b647/55405.ipynb#scrollTo=SVSCfrOLP6W_) .Thank you!", "Looks like issue with `SparseTensor support int32`. \r\nGenerally, we see this behaviour because when we are passing a very small number into a function. I could able to reproduce the issue with Tf v2.8 [gist](https://colab.sandbox.google.com/gist/sushreebarsa/5253243ce6846a9b22215a120182b647/55405.ipynb#scrollTo=SVSCfrOLP6W_). Thanks!"]}, {"number": 55402, "title": "TFLite evaluation utils: Fix `elinux_armhf` build", "body": "On `master` building the evaluation utils for `elinux_armhf` using\r\n```\r\nbazel build tensorflow/lite/tools/delegates:hexagon_delegate_provider -c opt --config=elinux_armhf\r\n```\r\nfails with\r\n```\r\nERROR: /code/tensorflow/lite/tools/delegates/BUILD:113:11: Compiling tensorflow/lite/tools/delegates/hexagon_delegate_provider.cc failed: undeclared inclusion(s) in rule '//tensorflow/lite/tools/delegates:hexagon_delegate_provider':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/lite/tools/delegates/hexagon_delegate_provider.cc':\r\n  'tensorflow/lite/delegates/xnnpack/xnnpack_delegate.h'\r\nTarget //tensorflow/lite/tools/delegates:hexagon_delegate_provider failed to build\r\n```\r\n\r\nThis PR fixes the issue by effectively reverting 44c05422ee00000472f1abc2611ea46b3093f93f. The workaround is not needed anymore since the `elinux_armhf` XNNPACK build has been properly fixed in 4ec84aa99372ca7899fb12715edd2bfe3c947c88.\r\n\r\n/cc @terryheo", "comments": ["@nutsiepully Can you please review this PR ? Thank you!"]}, {"number": 55399, "title": "TFLite Failed to Allocate Tensor", "body": "### 2. Code\r\nI have an exported Frozen Graph .pb file, and converted it to tflite using\r\n```\r\ngraph_def_file = \"model.pb\"\r\ninput_arrays = [\"Placeholder\"]\r\ninput_shape = {\"Placeholder\": [1024, 2048, 3]}\r\noutput_arrays = [\"final_output\"]\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(\r\n    graph_def_file, input_arrays, output_arrays, input_shape)\r\n\r\ntflite_model = converter.convert()\r\nwith open(\"my_model.tflite\", \"wb\") as f:\r\n        f.write(tflite_model)\r\n```\r\nAnd I could not restore my model with\r\n```\r\ninterpreter = tf.lite.Interpreter(model_path=\"my_model.tflite\")\r\ninterpreter.resize_tensor_input(0, [1024, 2048, 3], strict=True)\r\ninterpreter.allocate_tensors()\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-28-3cfaee7dc51c> in <module>\r\n      1 interpreter = tf.lite.Interpreter(model_path=model_path)\r\n      2 interpreter.resize_tensor_input(0, [1024, 2048, 3], strict=True)\r\n----> 3 interpreter.allocate_tensors()\r\n\r\n~/.local/lib/python3.9/site-packages/tensorflow/lite/python/interpreter.py in allocate_tensors(self)\r\n    512   def allocate_tensors(self):\r\n    513     self._ensure_safe()\r\n--> 514     return self._interpreter.AllocateTensors()\r\n    515 \r\n    516   def _safe_to_run(self):\r\n\r\nRuntimeError: tensorflow/lite/kernels/conv.cc:349 input->dims->data[3] != filter->dims->data[3] (65 != 64)Node number 11 (CONV_2D) failed to prepare.Failed to apply the default TensorFlow Lite delegate indexed at 0.\r\n```\r\n\r\nHowever, I can successfully reload my .pb file frozen graph by using\r\n```\r\nwith tf.gfile.GFile('my_model.pb', \"rb\") as pb:\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(pb.read())\r\nwith tf.Graph().as_default() as graph:\r\n    tf.import_graph_def(\r\n            graph_def,\r\n            name=\"\", \r\n            )\r\n```\r\nand also successfully generated result by\r\n\r\n```\r\nnode_in = graph.get_tensor_by_name('Placeholder:0') \r\nnode_out = graph.get_tensor_by_name('final_output:0') \r\n\r\nwith tf.Session(graph=graph) as sess:  # Session()\r\n    # sess.run(tf.global_variables_initializer()) \r\n    feed_dict = {node_in: input_img}  \r\n    pred = sess.run(node_out, feed_dict) \r\n    print(pred)\r\n    sess.close()\r\n```\r\n\r\nI've checked the node 11 of the .tflite file in Netron, but everything seemed fine.\r\n![node 11](https://user-images.githubusercontent.com/42362331/160350576-92c32abb-2ccc-4ee5-9003-a6ec3538dd78.png)\r\nWhat could be the problem? \r\n\r\nThe file attached is my .pb frozen graph\r\n [model.zip](https://github.com/tensorflow/tensorflow/files/8359810/model.zip)", "comments": ["Hi @ZikeiWong ! Could you check this [migration document](https://tensorflow.google.cn/guide/migrate/tflite) on converting frozen graphs to Tflite ? ", "Hi @mohantym! Thank you so much for your reply.\r\n\r\nYeah, I am sorry that I forgot to put my coverting code up there, and I've updated it.\r\n\r\nI've tried to convert from frozen graph to tflite with:\r\n```\r\ngraph_def_file = \"model.pb\"\r\ninput_arrays = [\"Placeholder\"]\r\ninput_shape = {\"Placeholder\": [1024, 2048, 3]}\r\noutput_arrays = [\"final_output\"]\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(\r\n    graph_def_file, input_arrays, output_arrays, input_shape)\r\n\r\ntflite_model = converter.convert()\r\nwith open(\"my_model.tflite\", \"wb\") as f:\r\n        f.write(tflite_model)\r\n```\r\n\r\nI've also tried to convert my frozen graph to SavedModel first using\r\n```\r\nfrom tensorflow.python.saved_model import signature_constants\r\nfrom tensorflow.python.saved_model import tag_constants\r\n\r\nexport_dir = './convert_model/saved'\r\ngraph_pb = './convert_model/model.pb'\r\n\r\nbuilder = tf.saved_model.builder.SavedModelBuilder(export_dir)\r\n\r\nwith tf.gfile.GFile(graph_pb, \"rb\") as f:\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n\r\nsigs = {}\r\n\r\nwith tf.Session(graph=tf.Graph()) as sess:\r\n    # name=\"\" is important to ensure we don't get spurious prefixing\r\n    tf.import_graph_def(graph_def, name=\"\")\r\n    g = tf.get_default_graph()\r\n    inp = g.get_tensor_by_name(\"Placeholder:0\")\r\n    out = g.get_tensor_by_name(\"final_output:0\")\r\n\r\n    sigs[signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY] = \\\r\n        tf.saved_model.signature_def_utils.predict_signature_def(\r\n            {\"in\": inp}, {\"out\": out})\r\n\r\n    builder.add_meta_graph_and_variables(sess,\r\n                                         [tag_constants.SERVING],\r\n                                         signature_def_map=sigs)\r\n\r\nbuilder.save()\r\n```\r\nand then convert the SavedModel to tflite with\r\n```\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_saved_model(\r\n    './convert_model/saved')\r\ntflite_model = converter.convert()\r\nopen(\"./convert_model/icnet_savedmodel.tflite\", \"wb\").write(tflite_model)\r\n```\r\nbut ```interpreter.allocate_tensors()``` still gives me the same debugging information ```untimeError: tensorflow/lite/kernels/conv.cc:349 input->dims->data[3] != filter->dims->data[3] (65 != 64)Node number 11 (CONV_2D) failed to prepare.Failed to apply the default TensorFlow Lite delegate indexed at 0.``` \r\n\r\n\r\nI suppose it's the problem of the .pb file of the frozen graph, but it could still be used to restore session and give inference.\r\n\r\nThis is how I saved my frozen graph .pb file:\r\n```\r\nsaver = tf.train.Saver()\r\ninit = tf.global_variables_initializer()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    tf.train.write_graph(sess.graph_def, './convert_model', 'model.pb') \r\n    saver.save(sess, \"convert_model/model.ckpt\") \r\n\r\nconstant_graph = graph_util.convert_variables_to_constants(net.sess, \r\n                                                           net.sess.graph_def,\r\n                                                           ['final_output'])\r\n\r\nwith tf.gfile.FastGFile('./convert_model/model.pb', mode='wb') as f:\r\n        f.write(constant_graph.SerializeToString())\r\n```\r\n", "Hi Sachin! Could you please look at this issue? It is replicating in [2.7](https://colab.sandbox.google.com/gist/mohantym/b6d40ef800f27a5853c515fbf9104629/github_55399.ipynb#scrollTo=HBHgb24w3V3l) ,[ 2.8](https://colab.sandbox.google.com/gist/mohantym/69634a06b55281b56cb5f105e6152ab8/github_55399.ipynb#scrollTo=-DZ4o0WaCDIZ) and[ nightly](https://colab.sandbox.google.com/gist/mohantym/48c3236fb6c45083d7ebc6fb56de68df/github_55399.ipynb#scrollTo=sGje8x42Frfn).", "Hi, thank you for your responses. I wonder if anyone could help me with the issuse?", "Hi,  thank you again for your previous responses. I am still waiting for any response."]}, {"number": 55394, "title": "tanh float32 over 1.", "body": "issues: https://github.com/tensorflow/tensorflow/issues/55390 [-9.0, 9.0] is too precise causing tanh function overflow \r\n in float32. \r\n", "comments": ["I got the correct answer under float type by XLA, but my change maybe  bring precision problem of double64 type, because the implementation of TANH in EIGEN3 is ```template<float>```.\r\nShould we implement ```void tanh_float``` or ```void tanh_double``` separately?\r\nAnd I don't find xla service runtime unittest.", "@r4nt Can you please review this PR ? Thank you!"]}, {"number": 55390, "title": "tanh(float(7~8)) = 1.0000001 in XLA", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux or mac \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version (use command below): 2.8 or 1.15\r\n- Python version: 3.9 or 2.7\r\n- Bazel version (if compiling from source): bazel release 5.0.0-pre.20211011.2\r\n- GCC/Compiler version (if compiling from source): Apple clang version 13.0.0 (clang-1300.0.29.3) \r\n- CUDA/cuDNN version: no\r\n- GPU model and memory: no \r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` \r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n    zsh: command not found: v1.12.1-68911-gea661077441\r\n\r\n**Describe the current behavior**\r\nThe return value of tanh function is  1.0000001, but tanh can not return more than 1.\r\n\r\n**Describe the expected behavior**\r\nThe return value of tanh function is 1 or less than 1.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): yes\r\n- Briefly describe your candidate solution(if contributing):\r\nThere is a description in the xla comment that the xla implementation of the tanh function is copied from eigen3.(tensorflow/compiler/xla/service/llvm_ir/math_ops.h:25), but I compare the implementation of the tanh function within xla and eigen3, they not have same implementation .so I copy the implementation of tanh function from eigen3 to tensorflow ....\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport os\r\nos.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit --tf_xla_min_cluster_size=0'\r\n\r\nimport tensorflow as tf\r\n\r\ntf.compat.v1.disable_eager_execution()\r\n\r\n#with tf.compat.v1.device('gpu'):\r\nctr_y = tf.compat.v1.random.uniform([1], minval=8, maxval=9, dtype=tf.compat.v1.float32)\r\nctr_pred_ori = tf.compat.v1.tanh(ctr_y)\r\n\r\n\r\nsession_config = tf.compat.v1.ConfigProto(allow_soft_placement=True, log_device_placement=True)\r\nsession_config.graph_options.rewrite_options.disable_meta_optimizer=True\r\nwith tf.compat.v1.Session(config=session_config) as sess:\r\n  while  True:\r\n    ret = sess.run([ctr_pred_ori])[0]\r\n    if ret > 1.0:\r\n      print(ret);\r\n```\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\n 212 [array([0.99999964], dtype=float32)]\r\n13702 [array([0.99999976], dtype=float32)]\r\n 621 [array([0.9999997], dtype=float32)]\r\n17916 [array([0.9999998], dtype=float32)]\r\n21312 [array([0.99999994], dtype=float32)]\r\n48624 [array([0.9999999], dtype=float32)]\r\n8097 [array([1.0000001], dtype=float32)]\r\n   5 [array([1.0000002], dtype=float32)]\r\n42322 [array([1.], dtype=float32)]\r\n```\r\ntanh return 1.0000001 8097 times", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55390\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55390\">No</a>\n", "@chunduriv  I was able to reproduce this issue on colab using TF v2.8.0 and tf-nightly,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/917d52c54aec646ed8c22a90bd8c4ac3/55390.ipynb).Thanks!", "@sushreebarsa  @chunduriv  I change my example in issues context.\r\nPlease add TF_XLA_FLAGS=\"--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit --tf_xla_min_cluster_size=0\", because there is only 1 node in my example xla cluster, 1 node does not meet the tf_xla_min_cluster_size(default:4) condition\r\n![image](https://user-images.githubusercontent.com/33950866/160961857-bcf84532-eb56-4bbc-9f30-37e449a52af5.png)\r\n\r\n\r\n"]}, {"number": 55389, "title": "TensorflowLite Undefined symbol: _TfLiteTensorCopy", "body": "**System information**\r\nXcode13.3, complie app for arm64 iPhone\r\n\r\nThe step I following is here:\r\nhttps://www.tensorflow.org/lite/guide/ops_select#ios\r\n\r\nDemo code is here:\r\n[iOSDemo 2.zip](https://github.com/tensorflow/tensorflow/files/8356734/iOSDemo.2.zip)\r\n\r\n**Step to reproduce issue**\r\n1. Download the zip,and `cd` to the project root folder\r\n2. Run `pod update` \r\n3. Compile project.\r\n4. Get the error and compile failed.\r\n\r\n<img width=\"352\" alt=\"QQ20220327-110056@2x\" src=\"https://user-images.githubusercontent.com/49340347/160264810-524cfabc-5cfd-48bd-a93c-9ff9966374a6.png\">\r\n\r\n", "comments": ["Hi @luckysmg ! Have you force loaded the Select TF ops to your app after the \"pod install\" step? \r\n`-force_load $(SRCROOT)/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps`\r\n", "Yes", "emm\uff0cWhen I change it to the lastest version\r\n```\r\n pod 'TensorFlowLiteSwift', '~> 0.0.1-nightly'\r\n  pod 'TensorFlowLiteSelectTfOps', '~> 0.0.1-nightly'\r\n```\r\nIt raise these error..\r\n<img width=\"1105\" alt=\"QQ20220328-164724@2x\" src=\"https://user-images.githubusercontent.com/49340347/160361317-939261b8-df0e-47a7-9e42-b3cff5d49c04.png\">\r\n.", "@luckysmg ! Good to hear that.  Can you let us know the results with the[ same approach ](https://github.com/tensorflow/tensorflow/issues/55389#issuecomment-1080367899) in a new app ?", "Here is demo code.\r\n[iOSDemo 2.zip](https://github.com/tensorflow/tensorflow/files/8356734/iOSDemo.2.zip)\r\n@mohantym \r\n \r\nI have force load..\r\n\r\n<img width=\"708\" alt=\"image\" src=\"https://user-images.githubusercontent.com/49340347/160970417-45ef5475-10ab-47bd-b576-21de43287542.png\">\r\n", "Any updates on this? We are facing the same issue. "]}, {"number": 55383, "title": "Print error message when failed to load external delegate.", "body": "Improve error message when failed to load external TFLite delegate.\r\n\r\nBefore:\r\n```\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Unable to load external delegate from : /data/local/tmp/libvx_delegate.so\r\nEXTERNAL delegate created.\r\n```\r\n\r\nAfter:\r\n```\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Unable to load external delegate from : /data/local/tmp/libvx_delegate.so (dlopen failed: library \"libtim-vx.so\" not found)\r\nEXTERNAL delegate created.\r\n```", "comments": ["Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nFor more information, open the [CLA check for this pull request](https://github.com/tensorflow/tensorflow/pull/55383/checks?check_run_id=5701924355).", "@nutsiepully Can you please review this PR ? Thank you!"]}, {"number": 55374, "title": "Factor limit of `tf.image.adjust_contrast` and `tf.image.adjust_saturation`? ", "body": "- [tf.image.adjust_saturation(image, saturation_factor, name=None)](https://www.tensorflow.org/api_docs/python/tf/image/adjust_saturation)\r\n- [tf.image.adjust_contrast(images, contrast_factor)](https://www.tensorflow.org/api_docs/python/tf/image/adjust_contrast)\r\n\r\nCould you please help understand what is the **expected range** or **limit** of the `saturation_factor` and `contrast_factor` in the above two functions? In the documentation, it's not stated clearly. ", "comments": ["I think contrast goes from -127 to 127 and saturation from 0 to 256, but need to confirm. Once confirmed, I can raise a PR that fixes the documentation.", "cc. @qlzh727 ", "@LukeWood as well.", "Thanks", "@qlzh727 @LukeWood Could you please confirm the @nirnayroy 's reply? "]}, {"number": 55373, "title": "Failed to call tf.convert_to_tensor for a tf.RaggedTensor with a PlaceHolder value", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 12.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): https://developer.apple.com/metal/tensorflow-plugin/\r\n- TensorFlow version (use command below): unknown 2.8.0\r\n- Python version: 3.8.12\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: Apple M1 16.00 GB\r\n \r\n**Describe the current behavior**\r\nrelated issue in tensorflow-text github : https://github.com/tensorflow/text/issues/867\r\n\r\n```\r\nMetal device set to: Apple M1\r\n\r\nsystemMemory: 16.00 GB\r\nmaxCacheSize: 5.33 GB\r\n\r\n2022-03-23 16:43:41.295408: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\r\n2022-03-23 16:43:41.295626: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\r\n2022-03-23 16:43:44.494936: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\r\n2022-03-23 16:43:44.530574: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\r\n2022-03-23 16:43:44.554177: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\r\nTraceback (most recent call last):\r\n  File \"_text.py\", line 31, in <module>\r\n    y_txt = trimmer.trim([y_txt])[0]\r\n  File \"/Users/user/miniforge3/envs/tf/lib/python3.8/site-packages/tensorflow_text/python/ops/trimmer_ops.py\", line 49, in trim\r\n    segments = [\r\n  File \"/Users/user/miniforge3/envs/tf/lib/python3.8/site-packages/tensorflow_text/python/ops/trimmer_ops.py\", line 50, in <listcomp>\r\n    ragged_tensor.convert_to_tensor_or_ragged_tensor(s) for s in segments\r\n  File \"/Users/user/miniforge3/envs/tf/lib/python3.8/site-packages/tensorflow/python/ops/ragged/ragged_tensor.py\", line 2658, in convert_to_tensor_or_ragged_tensor\r\n    return ops.convert_to_tensor_v2_with_dispatch(\r\n  File \"/Users/user/miniforge3/envs/tf/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"/Users/user/miniforge3/envs/tf/lib/python3.8/site-packages/keras/layers/core/tf_op_layer.py\", line 107, in handle\r\n    return TFOpLambda(op)(*args, **kwargs)\r\n  File \"/Users/user/miniforge3/envs/tf/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\nTypeError: Exception encountered when calling layer \"tf.convert_to_tensor\" (type TFOpLambda).\r\n\r\nFailed to convert elements of tf.RaggedTensor(values=tf.RaggedTensor(values=Tensor(\"Placeholder:0\", shape=(None,), dtype=int32), row_splits=Tensor(\"Placeholder_1:0\", shape=(None,), dtype=int64)), row_splits=Tensor(\"Placeholder_2:0\", shape=(None,), dtype=int64)) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\r\n\r\nCall arguments received:\r\n  \u2022 value=tf.RaggedTensor(values=tf.RaggedTensor(values=Tensor(\"Placeholder:0\", shape=(None,), dtype=int32), row_splits=Tensor(\"Placeholder_1:0\", shape=(None,), dtype=int64)), row_splits=Tensor(\"Placeholder_2:0\", shape=(None,), dtype=int64))\r\n  \u2022 dtype=None\r\n  \u2022 dtype_hint=None\r\n  \u2022 name=None\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n```\r\nMetal device set to: Apple M1\r\n\r\nsystemMemory: 16.00 GB\r\nmaxCacheSize: 5.33 GB\r\n\r\n2022-03-23 16:42:31.925269: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\r\n2022-03-23 16:42:31.925539: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\r\n2022-03-23 16:42:36.016294: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\r\n2022-03-23 16:42:36.057489: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\r\n2022-03-23 16:42:36.081872: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\r\n2022-03-23 16:42:36.096263: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\r\n<tf.RaggedTensor [[[101],\r\n  [7592],\r\n  [2088],\r\n  [999],\r\n  [102]]]>\r\n```\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\nimport tensorflow_text as text\r\n\r\nx_txt = tf.keras.layers.Input(shape=(), dtype=tf.string)\r\nbert_pre = hub.load(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\r\ntokenize = hub.KerasLayer(bert_pre.tokenize)\r\nbert_info = bert_pre.tokenize.get_special_tokens_dict()\r\n\r\ntrimmer = text.RoundRobinTrimmer(max_seq_length=40)\r\nmlm_selector = text.RandomItemSelector(\r\n    max_selections_per_batch=1000,\r\n    selection_rate=0.15,\r\n    unselectable_ids=[\r\n        bert_info[\"end_of_segment_id\"],\r\n        bert_info[\"mask_id\"],\r\n        bert_info[\"padding_id\"],\r\n        bert_info[\"start_of_sequence_id\"],\r\n    ],\r\n)\r\nmask_values_chooser = text.MaskValuesChooser(\r\n    bert_info[\"vocab_size\"], bert_info[\"mask_id\"], mask_token_rate=0.8\r\n)\r\n\r\ny_txt = tokenize(x_txt)\r\ny_txt, _segment_id = text.combine_segments(\r\n    [y_txt],\r\n    bert_info[\"start_of_sequence_id\"],\r\n    bert_info[\"end_of_segment_id\"],\r\n)\r\ny_txt = trimmer.trim([y_txt])[0]\r\n\r\nmodel = tf.keras.models.Model(inputs=[x_txt], outputs=[y_txt])\r\nprint(model.predict(tf.constant([\"hello world!\"])))\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nThis eager execution equivalent returns expected output.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\nimport tensorflow_text as text\r\n\r\nx_txt = tf.constant([\"hello world!\"])\r\nbert_pre = hub.load(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\r\ntokenize = hub.KerasLayer(bert_pre.tokenize)\r\nbert_info = bert_pre.tokenize.get_special_tokens_dict()\r\n\r\ntrimmer = text.RoundRobinTrimmer(max_seq_length=40)\r\nmlm_selector = text.RandomItemSelector(\r\n    max_selections_per_batch=1000,\r\n    selection_rate=0.15,\r\n    unselectable_ids=[\r\n        bert_info[\"end_of_segment_id\"],\r\n        bert_info[\"mask_id\"],\r\n        bert_info[\"padding_id\"],\r\n        bert_info[\"start_of_sequence_id\"],\r\n    ],\r\n)\r\nmask_values_chooser = text.MaskValuesChooser(\r\n    bert_info[\"vocab_size\"], bert_info[\"mask_id\"], mask_token_rate=0.8\r\n)\r\n\r\ny_txt = tokenize(x_txt)\r\ny_txt, _segment_id = text.combine_segments(\r\n    [y_txt],\r\n    bert_info[\"start_of_sequence_id\"],\r\n    bert_info[\"end_of_segment_id\"],\r\n)\r\ny_txt = trimmer.trim([y_txt])[0]\r\nprint(y_txt)\r\n```", "comments": ["@chunduriv ,\r\nI was able to reproduce the issue in tf v2.8, v2.7 and nightly.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/ec9f4feafa9e373daf725ba6cbbbca74/55373-2-8.ipynb)."]}, {"number": 55371, "title": "Feature request: Add left_side flag and functionality to tf.linalg.triangular_solve", "body": "**System information**\r\n- TensorFlow version (you are using): 2.8.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nAdd `left_side` flag and functionality to `tf.linalg.triangular_solve` as described in documentation for XLA TriangularSolve:\r\nhttps://www.tensorflow.org/xla/operation_semantics?hl=sl&skip_cache=true#triangularsolve\r\n\r\n**Will this change the current api? How?**\r\nIt will add an additional flag, which can be defaulted to maintain current behavior.\r\n\r\n**Who will benefit with this feature?**\r\nThose who want to solve triangular systems without a bulk of unnecessary transpose operations.\r\n", "comments": ["@jmmarschke ,\r\nCan you please elaborate about your Feature. Also, please specify the Use Cases for this feature. Thanks!", "@tilakrayal,\r\nI'd be glad to. Thank you!\r\n\r\n**Current functionality**\r\n`output = tf.linalg.triangular_solve(matrix, rhs, lower=True, adjoint=False, name=None)`\r\n\r\nWith `matrix` either upper or lower triangular, solves equations of the form:\r\n\r\n`matrix*output = rhs`\r\n\r\n\r\n**Proposed feature**\r\n`output = tf.linalg.triangular_solve(matrix, rhs, lower=True, adjoint=False, name=None, left_side=True)`\r\n\r\n\r\nWith `matrix` either upper or lower triangular, \r\nsolves:\r\n` matrix * output = rhs` (if `left_side == True`, default, same as above) \r\n\r\nor solves:\r\n` output * matrix = rhs` (if `left_side==False`, new functionality)\r\n\r\n**Use Case**\r\nCurrently, one can arrive at the same result by permuting the inputs beforehand and then reverse-permuting the result. However, since the cleaner, more performant approach already appears to be implemented in XLA (see link), it would make sense to expose this through `tf.linalg.triangular_solve`. Also, I believe the non-optimzed implementation of tf.linalg.triangular_solve uses Eigen's triangular solver under the hood... this functionality is already available there via the OnTheLeft/OnTheRight template input. Should be a pretty easy add.\r\n\r\nOld code:\r\n```\r\nSyyT    =   tf.transpose(Syy)\r\nKSyy_T  =   tf.transpose(KSyy)\r\n\r\nK_T     =   tf.linalg.triangular_solve(SyyT, KSyy_T, lower=False)\r\n\r\nK       =   tf.transpose(K_T)\r\n```\r\n\r\nNew code:\r\n```\r\nK       =   tf.linalg.triangular_solve(Syy, KSyy, left_side=False)\r\n```\r\n(`lower=True` now, OK to leave as default)\r\n\r\nCleaner, and more performant, since (even under the hood) it circumvents three unnecessary transpose operations.\r\n\r\n**Implementation Doc Links** \r\nIf my understanding of the implementation is correct, these might be useful...\r\n\r\nhttps://eigen.tuxfamily.org/dox/classEigen_1_1TriangularViewImpl_3_01__MatrixType_00_01__Mode_00_01Dense_01_4.html#a911664ccf5522c778ffc5405247e8a59\r\n\r\nhttps://www.tensorflow.org/xla/operation_semantics?hl=sl&skip_cache=true#triangularsolve\r\n", "I would like to work on this feature and contribute a PR. Can you please assign it to me?", "@sachinprasadhs - (or whoever this ends up with) please feel free to reach out if I can help. I do have some experience with Eigen... just not enough experience in the overall TensorFlow ecosystem (XLA etc.) to be comfortable contributing this myself. Thank you! "]}, {"number": 55370, "title": "mlflow.start_run() cases internal XLA error on TPU v2-8 with TF 2.8.0", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): pre-installed on GCP TPU instance\r\n- TensorFlow version (use command below): tpu-vm-tf-2.8.0\r\n- Python version: 3.8.10\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: tpu v2-8, tpu-vm-tf-2.8.0\r\n\r\n**Describe the current behavior**\r\n```\r\n2022-03-25 03:01:14.053145: I tensorflow/core/tpu/tpu_api_dlsym_initializer.cc:116] Libtpu path is: libtpu.so\r\n2022-03-25 03:01:15.555809: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2022-03-25 03:01:29.331594: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x429e9c0 initialized for platform TPU (this does not guarantee that XLA will be used). Devices:\r\n2022-03-25 03:01:29.331633: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): TPU, 2a886c8\r\n2022-03-25 03:01:29.331640: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (1): TPU, 2a886c8\r\n2022-03-25 03:01:29.331646: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (2): TPU, 2a886c8\r\n2022-03-25 03:01:29.331656: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (3): TPU, 2a886c8\r\n2022-03-25 03:01:29.331664: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (4): TPU, 2a886c8\r\n2022-03-25 03:01:29.331671: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (5): TPU, 2a886c8\r\n2022-03-25 03:01:29.331681: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (6): TPU, 2a886c8\r\n2022-03-25 03:01:29.331694: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (7): TPU, 2a886c8\r\nAll devices:  [LogicalDevice(name='/device:TPU:0', device_type='TPU'), LogicalDevice(name='/device:TPU:1', device_type='TPU'), LogicalDevice(name='/device:TPU:2', device_type='TPU'), LogicalDevice(name='/device:TPU:3', device_type='TPU'), LogicalDevice(name='/device:TPU:4', device_type='TPU'), LogicalDevice(name='/device:TPU:5', device_type='TPU'), LogicalDevice(name='/device:TPU:6', device_type='TPU'), LogicalDevice(name='/device:TPU:7', device_type='TPU')]\r\n2022-03-25 03:01:56.679371: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n2022-03-25 03:01:56.701447: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\r\n2022-03-25 03:01:56.703897: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at xla_compile_on_demand_op.cc:183 : INTERNAL: Core halted unexpectedly: No error message available as no compiler metadata was provided.\r\nTraceback (most recent call last):\r\n  File \"testcase.py\", line 23, in <module>\r\n    main()\r\n  File \"testcase.py\", line 19, in main\r\n    seq = tf.keras.Sequential([tf.keras.layers.Dense(512)])\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/tracking/base.py\", line 629, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/constant_op.py\", line 102, in convert_to_eager_tensor\r\n    return ops.EagerTensor(value, ctx.device_name, dtype)\r\ntensorflow.python.framework.errors_impl.InternalError: Core halted unexpectedly: No error message available as no compiler metadata was provided.\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n`tf.keras.Sequential([tf.keras.layers.Dense(512)])` returns successfully\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n - I doubt I can fix this myself\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\n# testcase.py\r\nimport mlflow\r\nimport tensorflow as tf\r\n\r\ndef init_tf_gpus():\r\n    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='local')\r\n    tf.config.experimental_connect_to_cluster(resolver)\r\n    # This is the TPU initialization code that has to be at the beginning.\r\n    tf.tpu.experimental.initialize_tpu_system(resolver)\r\n    print(\"All devices: \", tf.config.list_logical_devices('TPU'))\r\n\r\n    tf.config.optimizer.set_jit(True)\r\n\r\n    return tf.distribute.TPUStrategy(resolver)\r\n\r\ndef main():\r\n    strategy = init_tf_gpus()\r\n    with mlflow.start_run(run_name=\"test\"): # disable this line to make it work\r\n        with strategy.scope():\r\n            seq = tf.keras.Sequential([tf.keras.layers.Dense(512)])\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n\r\nInstall mlflow via `pip install mlflow`, I am using 1.24.0", "comments": ["issue in mlflow: https://github.com/mlflow/mlflow/issues/5528", "Hi, Did you observe similar behavior in Tf-Nightly version. ", "@sachinprasadhs is that \"nightly\" or \"tpu_driver_nightly\" in the tpu software version menu?", "It's a Tensorflow nightly version, if you are using pip then you can install using \r\n`pip install tf-nightly`"]}, {"number": 55365, "title": "Update GPU details for ctc_loss", "body": "Updated GPU details for ctc_loss which supports dense padded labels and SparseTensor.\r\nCloses #55290 ", "comments": ["Hey, thanks for looking into this. Can we add to the docs here a note that the code paths for sparse labels and dense labels are different on GPU? I.e. if you provide sparse labels, you are using the cuDNN implementation and you can be 50x faster than with dense labels. People will otherwise keep using dense labels and have no way of knowing why the loss is so slow even with a GPU", "+1 to @f90's comment - this patch would still result in misleading documentation and would not adequately resolve https://github.com/tensorflow/tensorflow/issues/55290.", "@rohan100jain  Can you please review this PR ? Thank you!"]}, {"number": 55364, "title": "benchmark_model: Could not create Hexagon delegate: platform may not support delegate or required libraries are missing", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 11\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung S21\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): nightly\r\n\r\n**Describe the current behavior**\r\n* Follow instructions in https://www.tensorflow.org/lite/performance/measurement#native_benchmark_binary and https://www.tensorflow.org/lite/performance/hexagon_delegate to run TFLite `benchmark_model` with `use_hexagon=true`.\r\n* `adb push` contents of these to `/data/local/tmp`:\r\n  * [android_aarch64_benchmark_model](https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/android_aarch64_benchmark_model)\r\n  * [libhexagon_interface.so ](https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/android_aarch64_libhexagon_interface.so)\r\n  * [hexagon_nn_skel v1.20.0.1](https://storage.cloud.google.com/download.tensorflow.org/tflite/hexagon_nn_skel_v1.20.0.1.run)\r\n  * [mobilenet_v2_1.0_224_quant](https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224_quant.tgz)\r\n* Verify with `adb shell ls /data/local/tmp`: \r\n  ```\r\n  android_aarch64_benchmark_model\r\n  libhexagon_interface.so\r\n  libhexagon_nn_skel.so\r\n  libhexagon_nn_skel_v65.so\r\n  libhexagon_nn_skel_v66.so\r\n  mobilenet_v2_1.0_224.tflite\r\n  mobilenet_v2_1.0_224_quant.tflite\r\n  ```\r\n* Run `adb shell /data/local/tmp/android_aarch64_benchmark_model --graph=/data/local/tmp/mobilenet_v2_1.0_224_quant.tflite --use_hexagon=true --hexagon_lib_path=/data/local/tmp`\r\n* Hexagon delegate fail to create:\r\n  ```\r\n  STARTING!\r\n  Log parameter values verbosely: [0]\r\n  Graph: [/data/local/tmp/mobilenet_v2_1.0_224_quant.tflite]\r\n  Use Hexagon: [1]\r\n  Hexagon lib path: [/data/local/tmp]\r\n  Loaded model /data/local/tmp/mobilenet_v2_1.0_224_quant.tflite\r\n  INFO: Initialized TensorFlow Lite runtime.\r\n  WARNING: Failed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide.\r\n  INFO: Hexagon Delegate is not supported.\r\n  \r\n  loaded libcdsprpc.so\r\n  Could not create Hexagon delegate: platform may not support delegate or required libraries are missing\r\n  The input model file size (MB): 3.57776\r\n  Initialized session in 51.739ms.\r\n  Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\n  count=66 first=23605 curr=6778 min=6765 max=23605 avg=7583.89 std=2197\r\n  \r\n  Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\n  count=82 first=6870 curr=850063 min=6765 max=850063 avg=17097.5 std=92551\r\n  \r\n  Inference timings in us: Init: 51739, First inference: 23605, Warmup (avg): 7583.89, Inference (avg): 17097.5\r\n  Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\n  Memory footprint delta from the start of the tool (MB): init=4.73828 overall=8.75\r\n  ``` \r\n\r\n**Describe the expected behavior**\r\n\r\n`benchmark_model` runs with hexagon delegate enabled.", "comments": ["Hi @tonysung ! I see you have pushed **multiple versions of libhexagon_nn_skel.so** . Can you let us know the results with only one libhexagon_nn_skel.so ? Thanks!", "Tried keeping only each one of `libhexagon_nn_skel.so`, `libhexagon_nn_skel_v65.so`, `libhexagon_nn_skel_v66.so`, both with their names unchanged, or renamed to `libhexagon_nn_skel.so` (total 6 combinations). Same result:\r\n\r\n```\r\nWARNING: Failed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide.\r\nINFO: Hexagon Delegate is not supported.\r\n\r\nloaded libcdsprpc.so\r\nCould not create Hexagon delegate: platform may not support delegate or required libraries are missing\r\n```", "as you can see from the [hexagon delegate page](https://www.tensorflow.org/lite/performance/hexagon_delegate), Samsung S21 (supposedly Snapdragon 888, which has Hexagon 780) is not supported. ", "I believe that's not the cause, as the page said, \"Currently the following Hexagon architecture are supported, **including but not limited to** ...\"", "Could you please follow the steps mentioned [here](https://github.com/tensorflow/tensorflow/issues/45014#issuecomment-731474874) and let us know if this helps to solve your problem. Thanks!", "Hi @sachinprasadhs If this has anything to do with `nativeLibraryDir` as mentioned [here](https://github.com/tensorflow/tensorflow/issues/45014#issuecomment-731474874), then I'm afraid it means a bug in `benchmark_model`, because the library should NOT be searched from `nativeLibraryDir` but from the path specified in `--hexagon_lib_path`:\r\n\r\n```\r\n$ adb shell /data/local/tmp/android_aarch64_benchmark_model --help\r\n...\r\n--hexagon_lib_path=/data/local/tmp string optional The library path for the underlying Hexagon libraries.\r\n...\r\n```\r\n\r\nAnd here's the original command when this bug is reproduced:\r\n\r\n```\r\nadb shell /data/local/tmp/android_aarch64_benchmark_model \\\r\n--graph=/data/local/tmp/mobilenet_v2_1.0_224_quant.tflite \\\r\n--use_hexagon=true \\\r\n--hexagon_lib_path=/data/local/tmp\r\n```\r\n\r\nAnd the content of `/data/local/tmp`:\r\n\r\n```\r\n$ adb shell ls /data/local/tmp:\r\nandroid_aarch64_benchmark_model\r\nlibhexagon_interface.so\r\nlibhexagon_nn_skel.so\r\n```", "> I believe that's not the cause, as the page said, \"Currently the following Hexagon architecture are supported, **including but not limited to** ...\"\r\n\r\nPlease check https://github.com/tensorflow/tensorflow/issues/47246.", "I tried the same on both Samsung Galaxy S8 (Snapdragon 835) and Google Pixel 3 (Snapdragon 845), and both failed.\r\n\r\nBoth listed to be supported according to https://www.tensorflow.org/lite/performance/hexagon_delegate\r\n\r\nThe error message is a bit different though.\r\n\r\nSamsung Galaxy S21:\r\n\r\n> WARNING: Failed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide.\r\n\r\nSamsung Galaxy S8 & Google Pixel 3:\r\n\r\n> ERROR: Failed to load libhexagon_interface.so, Error: dlopen failed: library \"libhexagon_interface.so\" not found\r\n\r\n```\r\nadb shell ls -l /data/local/tmp/android-benchmark-tools/aarch64\r\ntotal 16776\r\n-rwxrw-rw- 1 shell shell 6707096 2022-04-01 16:01 benchmark_model\r\n-rwxrw-rw- 1 shell shell 6727576 2022-04-01 16:09 benchmark_model_performance_options\r\n-rwxrw-rw- 1 shell shell  249008 2022-04-01 16:01 libhexagon_interface.so\r\n-rwxrwxrwx 1 shell shell 1148720 2022-04-12 22:31 libhexagon_nn_skel.so\r\n-rwxrwxrwx 1 shell shell 1165168 2022-04-12 22:31 libhexagon_nn_skel_v65.so\r\n-rwxrwxrwx 1 shell shell 1161072 2022-04-12 22:31 libhexagon_nn_skel_v66.so\r\n```\r\n\r\nFull error log:\r\n\r\n```\r\nSTARTING!\r\nLog parameter values verbosely: [0]\r\nGraph: [/data/local/tmp/android-benchmark-tools/tflite/mobilenet_v2_1.0_224_quant.tflite]\r\nUse Hexagon: [1]\r\nHexagon lib path: [/data/local/tmp/android-benchmark-tools/aarch64]\r\nLoaded model /data/local/tmp/android-benchmark-tools/tflite/mobilenet_v2_1.0_224_quant.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nERROR: Failed to load libhexagon_interface.so, Error: dlopen failed: library \"libhexagon_interface.so\" not found\r\nINFO: Hexagon Delegate is not supported.\r\n\r\nCould not create Hexagon delegate: platform may not support delegate or required libraries are missing\r\nThe input model file size (MB): 3.57776\r\nInitialized session in 2.229ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=15 first=38031 curr=34185 min=34119 max=38031 avg=34471.7 std=954\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=50 first=34574 curr=34198 min=34125 max=35839 avg=34541.4 std=450\r\n\r\nInference timings in us: Init: 2229, First inference: 38031, Warmup (avg): 34471.7, Inference (avg): 34541.4\r\nNote: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\nMemory footprint delta from the start of the tool (MB): init=1.24219 overall=6.80078\r\n```", "@tonysung as [freedomtan](https://github.com/freedomtan) mentioned new chipset doesn't use the Hexagon NN, so we can't support it currently.\r\n\r\nFor the other supported use cases of Samsung S8\r\nYou will need to make libhexagon_interface.so in the LD library path so it can be loaded\r\nExample\r\n```\r\nLD_LIBRARY_PATH=/data/local/tmp/android-benchmark-tools:${LD_LIBRARY_PATH}\r\n```\r\n\r\nSee example [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/hexagon/builders/tests/run_tests.sh#L45) when it is done for testing.\r\nFor benchmark tool we set [rpath](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/BUILD#L37) to /data/lcoal/tmp/ by default to ease running. See [instructions](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark)", "One more thing for pixel phones DSP access is disabled so it will not work.\r\nIf you have the Pixel phone rooted you can run these commands first\r\n```\r\nadb root\r\nadb shell setenforce 0\r\n```"]}, {"number": 55344, "title": "Can't link with TF Lite C API static lib - undefined reference to `tflite::DefaultErrorReporter()'", "body": "I've built the TensorFlow Lite **static** library, the **C API** version. Cloned [the sources][1] and called CMake while turning OFF the shared build flag (`DTFLITE_C_BUILD_SHARED_LIBS:BOOL=OFF`):  \r\n```\r\ncmake -S ../tensorflow_src/tensorflow/lite/c -DTFLITE_C_BUILD_SHARED_LIBS:BOOL=OFF\r\ncmake --build . -j\r\n```\r\nThis built the `libtensorflowlite_c.a`. \r\n\r\nI also linked all other `.a` sub-dependencies from the `_dep` directory into a single `libtensorflowdep.a` library (this includes `absl`, `clog` and all other transient dependencies listed here in the [TensorFlow CMake file][2]).\r\n\r\nI tried to test the lib using a minimal example: \r\n```C\r\n#include <stdio.h>\r\n#include \"tensorflow/lite/c/c_api.h\"\r\n\r\nint main(void) {\r\n\tprintf(\"Hello!\");\r\n\r\n\tconst char *version = TfLiteVersion();\r\n\tprintf(\"Hello from TensorFlow Lite C API library version = %s\\n\", version);\r\n\r\n\treturn 0;\r\n}\r\n```\r\n\r\n`gcc -L\"/home/joedoe/out/subprojects/tensorflow\" -o \"MAIN\"  ./src/MAIN.o   -ltensorflowlite_c -ltensorflowdep`\r\n\r\nBut linking fails:\r\n\r\n```\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `TfLiteModelCreate':\r\nc_api.cc:(.text+0x210): undefined reference to `tflite::DefaultErrorReporter()'\r\n/usr/bin/ld: c_api.cc:(.text+0x22f): undefined reference to `tflite::FlatBufferModel::VerifyAndBuildFromBuffer(char const*, unsigned long, tflite::TfLiteVerifier*, tflite::ErrorReporter*)'\r\n/usr/bin/ld: c_api.cc:(.text+0x276): undefined reference to `operator new(unsigned long)'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `TfLiteModelCreateFromFile':\r\nc_api.cc:(.text+0x322): undefined reference to `tflite::DefaultErrorReporter()'\r\n/usr/bin/ld: c_api.cc:(.text+0x33d): undefined reference to `tflite::FlatBufferModel::VerifyAndBuildFromFile(char const*, tflite::TfLiteVerifier*, tflite::ErrorReporter*)'\r\n/usr/bin/ld: c_api.cc:(.text+0x384): undefined reference to `operator new(unsigned long)'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `TfLiteModelDelete':\r\nc_api.cc:(.text+0x433): undefined reference to `operator delete(void*)'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `TfLiteInterpreterOptionsCreate':\r\nc_api.cc:(.text+0x452): undefined reference to `operator new(unsigned long)'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `TfLiteInterpreterOptionsDelete':\r\nc_api.cc:(.text+0x4a7): undefined reference to `operator delete(void*)'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `TfLiteInterpreterCreate':\r\nc_api.cc:(.text+0x560): undefined reference to `tflite::CreateOpResolver()'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `TfLiteInterpreterDelete':\r\nc_api.cc:(.text+0x5fa): undefined reference to `operator delete(void*)'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `TfLiteInterpreterResizeInputTensor':\r\nc_api.cc:(.text+0x75a): undefined reference to `tflite::Interpreter::ResizeInputTensor(int, std::vector<int, std::allocator<int> > const&)'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `TfLiteInterpreterAllocateTensors':\r\nc_api.cc:(.text+0x7ee): undefined reference to `tflite::Interpreter::AllocateTensors()'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `TfLiteInterpreterInvoke':\r\nc_api.cc:(.text+0x824): undefined reference to `tflite::delegates::InterpreterUtils::InvokeWithCPUFallback(tflite::Interpreter*)'\r\n/usr/bin/ld: c_api.cc:(.text+0x83e): undefined reference to `tflite::Interpreter::Invoke()'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `tflite::internal::InterpreterCreateWithOpResolver(TfLiteModel const*, TfLiteInterpreterOptions const*, tflite::MutableOpResolver*)':\r\nc_api.cc:(.text+0xaf5): undefined reference to `operator new(unsigned long)'\r\n/usr/bin/ld: c_api.cc:(.text+0xb5b): undefined reference to `tflite::MutableOpResolver::AddAll(tflite::MutableOpResolver const&)'\r\n/usr/bin/ld: c_api.cc:(.text+0xbf1): undefined reference to `tflite::DefaultErrorReporter()'\r\n/usr/bin/ld: c_api.cc:(.text+0xc2f): undefined reference to `tflite::InterpreterBuilder::InterpreterBuilder(tflite::Model const*, tflite::OpResolver const&, tflite::ErrorReporter*)'\r\n/usr/bin/ld: c_api.cc:(.text+0xc53): undefined reference to `tflite::InterpreterBuilder::operator()(std::unique_ptr<tflite::Interpreter, std::default_delete<tflite::Interpreter> >*)'\r\n/usr/bin/ld: c_api.cc:(.text+0xca7): undefined reference to `tflite::Interpreter::SetNumThreads(int)'\r\n/usr/bin/ld: c_api.cc:(.text+0xcd0): undefined reference to `tflite::NnApiDelegate()'\r\n/usr/bin/ld: c_api.cc:(.text+0xcdb): undefined reference to `tflite::Interpreter::ModifyGraphWithDelegate(TfLiteDelegate*)'\r\n/usr/bin/ld: c_api.cc:(.text+0xd88): undefined reference to `tflite::Interpreter::ModifyGraphWithDelegate(TfLiteDelegate*)'\r\n/usr/bin/ld: c_api.cc:(.text+0xe08): undefined reference to `operator new(unsigned long)'\r\n/usr/bin/ld: c_api.cc:(.text+0xe67): undefined reference to `tflite::InterpreterBuilder::~InterpreterBuilder()'\r\n/usr/bin/ld: c_api.cc:(.text+0xebe): undefined reference to `tflite::InterpreterBuilder::~InterpreterBuilder()'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `(anonymous namespace)::CallbackOpResolver::~CallbackOpResolver()':\r\nc_api.cc:(.text+0xf62): undefined reference to `operator delete(void*)'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `(anonymous namespace)::CallbackErrorReporter::~CallbackErrorReporter()':\r\nc_api.cc:(.text+0xfba): undefined reference to `operator delete(void*)'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o):(.data.rel.ro+0x8): undefined reference to `vtable for __cxxabiv1::__si_class_type_info'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o):(.data.rel.ro+0x20): undefined reference to `vtable for __cxxabiv1::__si_class_type_info'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `tflite::ErrorReporter::~ErrorReporter()':\r\nc_api.cc:(.text._ZN6tflite13ErrorReporterD0Ev[_ZN6tflite13ErrorReporterD5Ev]+0x24): undefined reference to `operator delete(void*)'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `tflite::OpResolver::~OpResolver()':\r\nc_api.cc:(.text._ZN6tflite10OpResolverD0Ev[_ZN6tflite10OpResolverD5Ev]+0x24): undefined reference to `operator delete(void*)'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `tflite::MutableOpResolver::MutableOpResolver()':\r\nc_api.cc:(.text._ZN6tflite17MutableOpResolverC2Ev[_ZN6tflite17MutableOpResolverC5Ev]+0x1f): undefined reference to `vtable for tflite::MutableOpResolver'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `std::default_delete<tflite::FlatBufferModel>::operator()(tflite::FlatBufferModel*) const':\r\nc_api.cc:(.text._ZNKSt14default_deleteIN6tflite15FlatBufferModelEEclEPS1_[_ZNKSt14default_deleteIN6tflite15FlatBufferModelEEclEPS1_]+0x22): undefined reference to `tflite::FlatBufferModel::~FlatBufferModel()'\r\n/usr/bin/ld: c_api.cc:(.text._ZNKSt14default_deleteIN6tflite15FlatBufferModelEEclEPS1_[_ZNKSt14default_deleteIN6tflite15FlatBufferModelEEclEPS1_]+0x2a): undefined reference to `operator delete(void*)'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `void std::vector<TfLiteDelegate*, std::allocator<TfLiteDelegate*> >::_M_realloc_insert<TfLiteDelegate* const&>(__gnu_cxx::__normal_iterator<TfLiteDelegate**, std::vector<TfLiteDelegate*, std::allocator<TfLiteDelegate*> > >, TfLiteDelegate* const&)':\r\nc_api.cc:(.text._ZNSt6vectorIP14TfLiteDelegateSaIS1_EE17_M_realloc_insertIJRKS1_EEEvN9__gnu_cxx17__normal_iteratorIPS1_S3_EEDpOT_[_ZNSt6vectorIP14TfLiteDelegateSaIS1_EE17_M_realloc_insertIJRKS1_EEEvN9__gnu_cxx17__normal_iteratorIPS1_S3_EEDpOT_]+0x267): undefined reference to `__cxa_begin_catch'\r\n/usr/bin/ld: c_api.cc:(.text._ZNSt6vectorIP14TfLiteDelegateSaIS1_EE17_M_realloc_insertIJRKS1_EEEvN9__gnu_cxx17__normal_iteratorIPS1_S3_EEDpOT_[_ZNSt6vectorIP14TfLiteDelegateSaIS1_EE17_M_realloc_insertIJRKS1_EEEvN9__gnu_cxx17__normal_iteratorIPS1_S3_EEDpOT_]+0x2d0): undefined reference to `__cxa_rethrow'\r\n/usr/bin/ld: c_api.cc:(.text._ZNSt6vectorIP14TfLiteDelegateSaIS1_EE17_M_realloc_insertIJRKS1_EEEvN9__gnu_cxx17__normal_iteratorIPS1_S3_EEDpOT_[_ZNSt6vectorIP14TfLiteDelegateSaIS1_EE17_M_realloc_insertIJRKS1_EEEvN9__gnu_cxx17__normal_iteratorIPS1_S3_EEDpOT_]+0x2dc): undefined reference to `__cxa_end_catch'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `tflite::MutableOpResolver::~MutableOpResolver()':\r\nc_api.cc:(.text._ZN6tflite17MutableOpResolverD2Ev[_ZN6tflite17MutableOpResolverD5Ev]+0x13): undefined reference to `vtable for tflite::MutableOpResolver'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `tflite::MutableOpResolver::~MutableOpResolver()':\r\nc_api.cc:(.text._ZN6tflite17MutableOpResolverD0Ev[_ZN6tflite17MutableOpResolverD5Ev]+0x24): undefined reference to `operator delete(void*)'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `std::default_delete<tflite::Interpreter>::operator()(tflite::Interpreter*) const':\r\nc_api.cc:(.text._ZNKSt14default_deleteIN6tflite11InterpreterEEclEPS1_[_ZNKSt14default_deleteIN6tflite11InterpreterEEclEPS1_]+0x22): undefined reference to `tflite::Interpreter::~Interpreter()'\r\n/usr/bin/ld: c_api.cc:(.text._ZNKSt14default_deleteIN6tflite11InterpreterEEclEPS1_[_ZNKSt14default_deleteIN6tflite11InterpreterEEclEPS1_]+0x2a): undefined reference to `operator delete(void*)'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `std::vector<TfLiteDelegate*, std::allocator<TfLiteDelegate*> >::_M_check_len(unsigned long, char const*) const':\r\nc_api.cc:(.text._ZNKSt6vectorIP14TfLiteDelegateSaIS1_EE12_M_check_lenEmPKc[_ZNKSt6vectorIP14TfLiteDelegateSaIS1_EE12_M_check_lenEmPKc]+0x5f): undefined reference to `std::__throw_length_error(char const*)'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `std::vector<int, std::allocator<int> >::_S_check_init_len(unsigned long, std::allocator<int> const&)':\r\nc_api.cc:(.text._ZNSt6vectorIiSaIiEE17_S_check_init_lenEmRKS0_[_ZNSt6vectorIiSaIiEE17_S_check_init_lenEmRKS0_]+0x62): undefined reference to `std::__throw_length_error(char const*)'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `__gnu_cxx::new_allocator<int>::deallocate(int*, unsigned long)':\r\nc_api.cc:(.text._ZN9__gnu_cxx13new_allocatorIiE10deallocateEPim[_ZN9__gnu_cxx13new_allocatorIiE10deallocateEPim]+0x20): undefined reference to `operator delete(void*)'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `__gnu_cxx::new_allocator<std::function<std::unique_ptr<TfLiteDelegate, void (*)(TfLiteDelegate*)> (int)> >::deallocate(std::function<std::unique_ptr<TfLiteDelegate, void (*)(TfLiteDelegate*)> (int)>*, unsigned long)':\r\nc_api.cc:(.text._ZN9__gnu_cxx13new_allocatorISt8functionIFSt10unique_ptrI14TfLiteDelegatePFvPS3_EEiEEE10deallocateEPS9_m[_ZN9__gnu_cxx13new_allocatorISt8functionIFSt10unique_ptrI14TfLiteDelegatePFvPS3_EEiEEE10deallocateEPS9_m]+0x20): undefined reference to `operator delete(void*)'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `std::__shared_count<(__gnu_cxx::_Lock_policy)2>::__shared_count<tflite::FlatBufferModel*>(tflite::FlatBufferModel*)':\r\nc_api.cc:(.text._ZNSt14__shared_countILN9__gnu_cxx12_Lock_policyE2EEC2IPN6tflite15FlatBufferModelEEET_[_ZNSt14__shared_countILN9__gnu_cxx12_Lock_policyE2EEC5IPN6tflite15FlatBufferModelEEET_]+0x26): undefined reference to `operator new(unsigned long)'\r\n/usr/bin/ld: c_api.cc:(.text._ZNSt14__shared_countILN9__gnu_cxx12_Lock_policyE2EEC2IPN6tflite15FlatBufferModelEEET_[_ZNSt14__shared_countILN9__gnu_cxx12_Lock_policyE2EEC5IPN6tflite15FlatBufferModelEEET_]+0x4d): undefined reference to `__cxa_begin_catch'\r\n/usr/bin/ld: c_api.cc:(.text._ZNSt14__shared_countILN9__gnu_cxx12_Lock_policyE2EEC2IPN6tflite15FlatBufferModelEEET_[_ZNSt14__shared_countILN9__gnu_cxx12_Lock_policyE2EEC5IPN6tflite15FlatBufferModelEEET_]+0x5e): undefined reference to `tflite::FlatBufferModel::~FlatBufferModel()'\r\n/usr/bin/ld: c_api.cc:(.text._ZNSt14__shared_countILN9__gnu_cxx12_Lock_policyE2EEC2IPN6tflite15FlatBufferModelEEET_[_ZNSt14__shared_countILN9__gnu_cxx12_Lock_policyE2EEC5IPN6tflite15FlatBufferModelEEET_]+0x66): undefined reference to `operator delete(void*)'\r\n/usr/bin/ld: c_api.cc:(.text._ZNSt14__shared_countILN9__gnu_cxx12_Lock_policyE2EEC2IPN6tflite15FlatBufferModelEEET_[_ZNSt14__shared_countILN9__gnu_cxx12_Lock_policyE2EEC5IPN6tflite15FlatBufferModelEEET_]+0x6b): undefined reference to `__cxa_rethrow'\r\n/usr/bin/ld: c_api.cc:(.text._ZNSt14__shared_countILN9__gnu_cxx12_Lock_policyE2EEC2IPN6tflite15FlatBufferModelEEET_[_ZNSt14__shared_countILN9__gnu_cxx12_Lock_policyE2EEC5IPN6tflite15FlatBufferModelEEET_]+0x77): undefined reference to `__cxa_end_catch'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::~_Sp_counted_base()':\r\nc_api.cc:(.text._ZNSt16_Sp_counted_baseILN9__gnu_cxx12_Lock_policyE2EED0Ev[_ZNSt16_Sp_counted_baseILN9__gnu_cxx12_Lock_policyE2EED5Ev]+0x24): undefined reference to `operator delete(void*)'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `__gnu_cxx::new_allocator<tflite::OpResolver const*>::deallocate(tflite::OpResolver const**, unsigned long)':\r\nc_api.cc:(.text._ZN9__gnu_cxx13new_allocatorIPKN6tflite10OpResolverEE10deallocateEPS4_m[_ZN9__gnu_cxx13new_allocatorIPKN6tflite10OpResolverEE10deallocateEPS4_m]+0x20): undefined reference to `operator delete(void*)'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `__gnu_cxx::new_allocator<TfLiteDelegate*>::deallocate(TfLiteDelegate**, unsigned long)':\r\nc_api.cc:(.text._ZN9__gnu_cxx13new_allocatorIP14TfLiteDelegateE10deallocateEPS2_m[_ZN9__gnu_cxx13new_allocatorIP14TfLiteDelegateE10deallocateEPS2_m]+0x20): undefined reference to `operator delete(void*)'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `__gnu_cxx::new_allocator<int>::allocate(unsigned long, void const*)':\r\nc_api.cc:(.text._ZN9__gnu_cxx13new_allocatorIiE8allocateEmPKv[_ZN9__gnu_cxx13new_allocatorIiE8allocateEmPKv]+0x30): undefined reference to `std::__throw_bad_alloc()'\r\n/usr/bin/ld: c_api.cc:(.text._ZN9__gnu_cxx13new_allocatorIiE8allocateEmPKv[_ZN9__gnu_cxx13new_allocatorIiE8allocateEmPKv]+0x40): undefined reference to `operator new(unsigned long)'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `__gnu_cxx::new_allocator<TfLiteDelegate*>::allocate(unsigned long, void const*)':\r\nc_api.cc:(.text._ZN9__gnu_cxx13new_allocatorIP14TfLiteDelegateE8allocateEmPKv[_ZN9__gnu_cxx13new_allocatorIP14TfLiteDelegateE8allocateEmPKv]+0x30): undefined reference to `std::__throw_bad_alloc()'\r\n/usr/bin/ld: c_api.cc:(.text._ZN9__gnu_cxx13new_allocatorIP14TfLiteDelegateE8allocateEmPKv[_ZN9__gnu_cxx13new_allocatorIP14TfLiteDelegateE8allocateEmPKv]+0x40): undefined reference to `operator new(unsigned long)'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `__gnu_cxx::new_allocator<std::__detail::_Hash_node_base*>::deallocate(std::__detail::_Hash_node_base**, unsigned long)':\r\nc_api.cc:(.text._ZN9__gnu_cxx13new_allocatorIPNSt8__detail15_Hash_node_baseEE10deallocateEPS3_m[_ZN9__gnu_cxx13new_allocatorIPNSt8__detail15_Hash_node_baseEE10deallocateEPS3_m]+0x20): undefined reference to `operator delete(void*)'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int>::~pair()':\r\nc_api.cc:(.text._ZNSt4pairINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEiED2Ev[_ZNSt4pairINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEiED5Ev]+0x18): undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string()'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `__gnu_cxx::new_allocator<std::__detail::_Hash_node<std::pair<std::pair<tflite::BuiltinOperator, int> const, TfLiteRegistration>, true> >::deallocate(std::__detail::_Hash_node<std::pair<std::pair<tflite::BuiltinOperator, int> const, TfLiteRegistration>, true>*, unsigned long)':\r\nc_api.cc:(.text._ZN9__gnu_cxx13new_allocatorINSt8__detail10_Hash_nodeISt4pairIKS3_IN6tflite15BuiltinOperatorEiE18TfLiteRegistrationELb1EEEE10deallocateEPSA_m[_ZN9__gnu_cxx13new_allocatorINSt8__detail10_Hash_nodeISt4pairIKS3_IN6tflite15BuiltinOperatorEiE18TfLiteRegistrationELb1EEEE10deallocateEPSA_m]+0x20): undefined reference to `operator delete(void*)'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `__gnu_cxx::new_allocator<std::__detail::_Hash_node<std::pair<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int> const, TfLiteRegistration>, true> >::deallocate(std::__detail::_Hash_node<std::pair<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int> const, TfLiteRegistration>, true>*, unsigned long)':\r\nc_api.cc:(.text._ZN9__gnu_cxx13new_allocatorINSt8__detail10_Hash_nodeISt4pairIKS3_INSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEiE18TfLiteRegistrationELb1EEEE10deallocateEPSE_m[_ZN9__gnu_cxx13new_allocatorINSt8__detail10_Hash_nodeISt4pairIKS3_INSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEiE18TfLiteRegistrationELb1EEEE10deallocateEPSE_m]+0x20): undefined reference to `operator delete(void*)'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `std::_Sp_counted_ptr<tflite::FlatBufferModel*, (__gnu_cxx::_Lock_policy)2>::~_Sp_counted_ptr()':\r\nc_api.cc:(.text._ZNSt15_Sp_counted_ptrIPN6tflite15FlatBufferModelELN9__gnu_cxx12_Lock_policyE2EED0Ev[_ZNSt15_Sp_counted_ptrIPN6tflite15FlatBufferModelELN9__gnu_cxx12_Lock_policyE2EED5Ev]+0x24): undefined reference to `operator delete(void*)'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o):(.data.rel.ro._ZTVN6tflite10OpResolverE[_ZTVN6tflite10OpResolverE]+0x10): undefined reference to `__cxa_pure_virtual'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o):(.data.rel.ro._ZTVN6tflite10OpResolverE[_ZTVN6tflite10OpResolverE]+0x18): undefined reference to `__cxa_pure_virtual'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o):(.data.rel.ro._ZTVN6tflite13ErrorReporterE[_ZTVN6tflite13ErrorReporterE]+0x20): undefined reference to `__cxa_pure_virtual'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o):(.data.rel.ro._ZTVSt16_Sp_counted_baseILN9__gnu_cxx12_Lock_policyE2EE[_ZTVSt16_Sp_counted_baseILN9__gnu_cxx12_Lock_policyE2EE]+0x20): undefined reference to `__cxa_pure_virtual'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o):(.data.rel.ro._ZTVSt16_Sp_counted_baseILN9__gnu_cxx12_Lock_policyE2EE[_ZTVSt16_Sp_counted_baseILN9__gnu_cxx12_Lock_policyE2EE]+0x30): undefined reference to `__cxa_pure_virtual'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o):(.data.rel.ro._ZTISt15_Sp_counted_ptrIPN6tflite15FlatBufferModelELN9__gnu_cxx12_Lock_policyE2EE[_ZTISt15_Sp_counted_ptrIPN6tflite15FlatBufferModelELN9__gnu_cxx12_Lock_policyE2EE]+0x0): undefined reference to `vtable for __cxxabiv1::__si_class_type_info'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o):(.data.rel.ro._ZTIN6tflite10OpResolverE[_ZTIN6tflite10OpResolverE]+0x0): undefined reference to `vtable for __cxxabiv1::__class_type_info'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o):(.data.rel.ro._ZTIN6tflite13ErrorReporterE[_ZTIN6tflite13ErrorReporterE]+0x0): undefined reference to `vtable for __cxxabiv1::__class_type_info'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o):(.data.rel.ro._ZTISt16_Sp_counted_baseILN9__gnu_cxx12_Lock_policyE2EE[_ZTISt16_Sp_counted_baseILN9__gnu_cxx12_Lock_policyE2EE]+0x0): undefined reference to `vtable for __cxxabiv1::__si_class_type_info'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o): in function `std::_Sp_counted_ptr<tflite::FlatBufferModel*, (__gnu_cxx::_Lock_policy)2>::_M_dispose()':\r\nc_api.cc:(.text._ZNSt15_Sp_counted_ptrIPN6tflite15FlatBufferModelELN9__gnu_cxx12_Lock_policyE2EE10_M_disposeEv[_ZNSt15_Sp_counted_ptrIPN6tflite15FlatBufferModelELN9__gnu_cxx12_Lock_policyE2EE10_M_disposeEv]+0x22): undefined reference to `tflite::FlatBufferModel::~FlatBufferModel()'\r\n/usr/bin/ld: c_api.cc:(.text._ZNSt15_Sp_counted_ptrIPN6tflite15FlatBufferModelELN9__gnu_cxx12_Lock_policyE2EE10_M_disposeEv[_ZNSt15_Sp_counted_ptrIPN6tflite15FlatBufferModelELN9__gnu_cxx12_Lock_policyE2EE10_M_disposeEv]+0x2a): undefined reference to `operator delete(void*)'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o):(.data.rel.ro._ZTISt11_Mutex_baseILN9__gnu_cxx12_Lock_policyE2EE[_ZTISt11_Mutex_baseILN9__gnu_cxx12_Lock_policyE2EE]+0x0): undefined reference to `vtable for __cxxabiv1::__class_type_info'\r\n/usr/bin/ld: /home/joedoe/out/subprojects/tensorflow/libtensorflowlite_c.a(c_api.cc.o):(.data.rel.local.DW.ref.__gxx_personality_v0[DW.ref.__gxx_personality_v0]+0x0): undefined reference to `__gxx_personality_v0'\r\ncollect2: error: ld returned 1 exit status\r\nmake: *** [makefile:45: TF] Error 1\r\n```\r\n\r\n  [1]: https://github.com/tensorflow/tensorflow\r\n  [2]: https://github.com/tensorflow/tensorflow/blob/e41d9eb9edcc1ff2cf52736edde76e6f5d62f6ed/tensorflow/lite/CMakeLists.txt#L139", "comments": ["@sushreebarsa Any idea? Thanks.", "Hi @DanijelDomazet !  Attaching relevant threads for reference. [1](https://github.com/tensorflow/tensorflow/issues/36661#issuecomment-631975005) , [2](https://github.com/tensorflow/tensorflow/issues/36661#issuecomment-585054685) . Can you also switch to g++ instead of gcc and let us know the results? Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@mohantym but how to fix this error on MSVC?", "> Hi @DanijelDomazet ! Attaching relevant threads for reference. [1](https://github.com/tensorflow/tensorflow/issues/36661#issuecomment-631975005) , [2](https://github.com/tensorflow/tensorflow/issues/36661#issuecomment-585054685) . Can you also switch to g++ instead of gcc and let us know the results? Thank you!\r\n\r\nHi, \r\n- [1](https://github.com/tensorflow/tensorflow/issues/36661#issuecomment-631975005) didn't help as it is about shared `.so` lib, and I am trying to build the static lib. \r\n- tried `g++`, but it too didn't help\r\n\r\nSo still can't build the static library for C API. \r\n", "Is there maybe a way to make the target `tensorflowdep` into upstream?\r\n"]}, {"number": 55337, "title": "[TF-TRT] Add support for non-frozen models with legacy reference variables (VariableV2)", "body": "This is the first step of upstreaming full support of non-frozen models in TF-TRT (in order to avoid the 2GB Protobuf limit that prevents freezing large models). We start with reference variables because it is a smaller change than resource variables.\r\n\r\nThis PR adds:\r\n\r\n- A `VariableV2` converter that outputs weights (i.e for TRT it is similar to a constant)\r\n- An environment variable to disable freezing (cf #55332)\r\n- A Python integration test and a C++ unit test", "comments": ["i don't believe remote implications from data test mode insertion? error running plugin action maybe?", "fig666 is a bot, please ignore.", "@Nyrio Can you please resolve conflicts? Thanks!", "@bixia1 I have done the requested changes, rebased on top of master, and squashed commits.", "@Nyrio Can you please address Ubuntu Sanity errors? Thank you!", "@Nyrio  Can you please resolve conflicts? Thank you!", "@gbaned Conflicts solved."]}, {"number": 55335, "title": "Add lowering for tf.ImageProjectiveTransformV3 for translation", "body": "Add TF to TF lowering for projective image transformations modeled by\r\nthe tf.ImageProjectiveTransformV3 ops. Add this op to the TF dialect.\r\nLower projective transformations in the case of \"translations\" to pad +\r\nslice ops.", "comments": ["CC: @joker-eph @jpienaar @stellaraccident  ", "Thank you. \nHonestly it is still a little bit esoteric to contribute these kind of things if you are not working every day in the compiler stack boundaries.", "@bondhugula  to test this in a `tf.function` is it  required to use `tf.config.experimental.enable_mlir_bridge()`?", "> @bondhugula to test this in a `tf.function` is it required to use `tf.config.experimental.enable_mlir_bridge()`?\r\n\r\nNo, you won't need that. You can simply use a tf.function with a `tfa.image.translate` call in it. It'll generate this op (tf.ImageProjectiveTranslation) when it's imported via any of the import Python bindings available for MLIR.", "But can we use this direclty with the standard TF runtime?", "@joker-eph Can you please review this PR ? Thank you!"]}, {"number": 55334, "title": "Cannot predict or serialize model that performs tf.image.resize after decoding image bytearray.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 (Google Colab)\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.8.0\r\n- Python version: 3.7.12\r\n\r\n**Describe the current behavior**\r\nWhen creating a model that receives a bytearray of an image, decodes it, resizes it and feeds it to the model, it won't serialize nor work with the `predict` method, we can only use `__call__`. \r\n\r\n**Describe the expected behavior**\r\nThe model would interpolate the tensor for the size inputed, serialize and save it.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nColab link: https://colab.research.google.com/drive/11VDEMesHjflnG7YytMI-KC9CgbA7hRYa\r\n\r\n```python\r\nfrom PIL import Image\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\nclass MyModel(tf.keras.models.Model):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.model = tf.keras.applications.ResNet101(\r\n          include_top=True,\r\n          weights=\"imagenet\",\r\n          input_tensor=None,\r\n          input_shape=None,\r\n          pooling=None,\r\n          classes=1000,\r\n      )\r\n\r\n    def call(self, x):\r\n        x = self.preprocess(x)\r\n        return self.model(x)\r\n\r\n    def preprocess(self, x):\r\n        x = tf.map_fn(\r\n            self.decode_bytes,\r\n            x,\r\n            dtype=tf.float32\r\n        )\r\n        x = tf.image.resize(x, (224, 224))\r\n\r\n        return x\r\n\r\n    def decode_bytes(self, x):\r\n        return tf.io.decode_image(x, channels=3, dtype=tf.float32)\r\n\r\n\r\ndef create_input_tensor():\r\n    fname = \"/tmp/image.png\"\r\n\r\n    mock_input = np.random.random((330, 330, 3)).astype(np.float32)\r\n    mock_image = Image.fromarray(mock_input, \"RGB\")\r\n    mock_image.save(fname)\r\n    b = tf.io.read_file(fname)\r\n    return tf.expand_dims(b, axis=0)\r\n\r\n\r\nmodel = MyModel()\r\nin_tensor = create_input_tensor()\r\n\r\nmodel.predict(in_tensor)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\nWARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:616: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse fn_output_signature instead\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n[<ipython-input-7-efcc1d4139ed>](https://localhost:8080/#) in <module>()\r\n----> 1 model.predict(in_tensor)\r\n\r\n1 frames\r\n[/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py](https://localhost:8080/#) in autograph_handler(*args, **kwargs)\r\n   1145           except Exception as e:  # pylint:disable=broad-except\r\n   1146             if hasattr(e, \"ag_error_metadata\"):\r\n-> 1147               raise e.ag_error_metadata.to_exception(e)\r\n   1148             else:\r\n   1149               raise\r\n\r\nValueError: in user code:\r\n\r\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1801, in predict_function  *\r\n        return step_function(self, iterator)\r\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1790, in step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1783, in run_step  **\r\n        outputs = model.predict_step(data)\r\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1751, in predict_step\r\n        return self(x, training=False)\r\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\r\n        raise e.with_traceback(filtered_tb) from None\r\n\r\n    ValueError: Exception encountered when calling layer \"my_model\" (type MyModel).\r\n    \r\n    in user code:\r\n    \r\n        File \"<ipython-input-4-f4258c3336bd>\", line 17, in call  *\r\n            x = self.preprocess(x)\r\n        File \"<ipython-input-4-f4258c3336bd>\", line 26, in preprocess  *\r\n            x = tf.image.resize(x, (224, 224))\r\n    \r\n        ValueError: 'images' contains no shape.\r\n    \r\n    \r\n    Call arguments received:\r\n      \u2022 x=tf.Tensor(shape=(None,), dtype=string)\r\n```\r\n\r\n**EDIT 1: workaround**\r\n\r\nColab link: https://colab.research.google.com/drive/1_VF5TWbehTECy-33TjdzqQyoOvSOF0bZ?usp=sharing\r\n\r\nIt seems that a workaround is to save the model with the resize operation but receiving the tensor (not a tf.string tensor with the bytearray), then load it, wrap it into a model that receives the bytearray then save it again. \r\n\r\nWhen the operation is already serialized and loaded into a model it seems to do no harm. \r\n\r\nAnyway, this is hacky and despite we can make it work like that it is not ideal. ", "comments": ["**EDIT 1: workaround**\r\n\r\nColab link: https://colab.research.google.com/drive/1_VF5TWbehTECy-33TjdzqQyoOvSOF0bZ?usp=sharing\r\n\r\nIt seems that a workaround is to save the model with the resize operation but receiving the tensor (not a tf.string tensor with the bytearray), then load it, wrap it into a model that receives the bytearray then save it again. \r\n\r\nWhen the operation is already serialized and loaded into a model it seems to do no harm. \r\n\r\nAnyway, this is hacky and despite we can make it work like that it is not ideal. ", "@sachinprasadhs I was able to replicate the issue on colab using TF v2.8.0 and tf-nightly, please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/6270b7c976b878b6dcaea0cc11032734/55334.ipynb).Thanks!", "@piEsposito This is a very well old know issue and `decode_image`, `decode_jpg` and `decode_gif` are \"unfied\" you need to use `expand_animations = False`.\r\nAs you can see, the thread is active since 2017 https://github.com/tensorflow/tensorflow/issues/9356#issuecomment-298469582\r\n\r\n\r\n", "@bhack thanks, that worked as of now and got me rid of my hacky workaround.\r\n\r\nAnyway, this is still a bug so I will keep this issue open. "]}, {"number": 55331, "title": "XNNPack delegate subgraph creation in TensorFlow Lite non-deterministically crashes when initializing tensors", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur 11.6.4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone 13\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.8.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): 4.2.1\r\n- GCC/Compiler version (if compiling from source): Apple clang version 12.0.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nI have a custom model that has its graph modified by the XNNPack delegate using the C API. When the XNNPack delegate modifies the interpreter graph, it works maybe 80% of the time and crashes another 20% of the time due to an error such as seen below.\r\n```\r\nlibc++abi: terminating with uncaught exception of type std::length_error: vector\r\n\u00a0 dyld4 config: DYLD_LIBRARY_PATH=/usr/lib/system/introspection DYLD_INSERT_LIBRARIES=/Developer/usr/lib/libBacktraceRecording.dylib:/Developer/usr/lib/libMainThreadChecker.dylib:/Developer/Library/PrivateFrameworks/DTDDISupport.framework/libViewDebuggerSupport.dylib:/usr/lib/libMTLCapture.dylib\r\n\u00a0 terminating with uncaught exception of type std::length_error: vector\r\n```\r\n\r\nI'm using the C API so I can use the TensorFlow Lite library across native applications (e.g. iOS, Android, Desktop, etc). \r\n\r\n**Describe the expected behavior**\r\nI would not expect it to crash and to always succeed.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): Yes\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n   See https://github.com/tensorflow/tensorflow/pull/55330. Basically, when debugging, I found that the graph sometimes ended up with all unused tensors (-1 in the vector). As a result, the `tensors` end up having a size of 0 and calling `back()` on an empty vector leads to undefined behavior according to C++ spec: https://www.cplusplus.com/reference/vector/vector/back/\r\n   ```\r\n   Calling this function on an [empty](https://www.cplusplus.com/vector::empty) container causes undefined behavior.\r\n   ```\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nI'm unfortunately not able to share my model or code, but I'm happy to adjust any unit tests or look more into coming up with a smaller reproducible use case if needed. I think the change is relatively clear though, so I haven't done anything else yet.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nN/A. ", "comments": ["Ok @richhx ! This issue will be closed once PR #55330 is merged . Thanks!"]}, {"number": 55330, "title": "Fix nondeterminstic crash when initializing xnnpack tensors", "body": null, "comments": ["See https://github.com/tensorflow/tensorflow/issues/55331", "Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nFor more information, open the [CLA check for this pull request](https://github.com/tensorflow/tensorflow/pull/55330/checks?check_run_id=5647812830).", "@nutsiepully Can you please review this PR ? Thank you!"]}, {"number": 55329, "title": "Add axis validations for tf.experimental.numpy.stack", "body": "Added validation to check axis is within the range [-Rank(array),Rank(array)) as per the issue #55217", "comments": ["@rohan100jain Can you please review this PR ? Thank you!"]}, {"number": 55328, "title": "tf.nn.ctc_loss behaviour changes depending on whether dense or sparse labels are provided", "body": "The [tf.nn.ctc_loss](https://www.tensorflow.org/api_docs/python/tf/nn/ctc_loss) changes its behaviour unexpectedly based on whether the labels provided are sparse or dense.\r\n\r\nIn particular, it accepts empty target label sequences in the dense version, and maximises (as one would expect) the log probability of the blank token at each time frame in the `logits` prediction matrix. It does not output any warning or error.\r\n\r\nHowever, if you convert the label to be sparse by using `tf.sparse_from_dense`, given the same inputs (with empty target label sequence), it errors out with\r\n\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Labels length is zero in batch 0 [Op:CTCLoss]\r\n```\r\n\r\nThis inconsistency in behaviour should be fixed.\r\n\r\nOption 1: Ideally, we would implement support for empty labels in the case of sparse tensors.\r\n\r\nOption 2: Add a warning to the CTC loss doc that empty labels are not supported in the case of sparse tensors.\r\n\r\nAnother inconsistency is that the dtype for `logit_length` can be int64 for the dense version, but needs to be int32 for the sparse version, and errors out if int64 is provided. Again, ideally this should be converted internally, but if it really cannot be fixed, the docs should reflect this, which they do not at the moment.\r\n\r\n## Function affected:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/ctc_loss\r\n\r\n### Usage example\r\n\r\nThis runs fine:\r\n```\r\ntf.nn.ctc_loss(\r\n    labels=tf.ones((4, 50), dtype=tf.int32),\r\n    logits=tf.zeros((4, 200, 10), dtype=tf.float32),\r\n    label_length=tf.constant(50, dtype=tf.int64, shape=4),\r\n    logit_length=tf.constant(200, dtype=tf.int64, shape=4),\r\n    logits_time_major=False,\r\n    blank_index=0,\r\n)\r\n```\r\n\r\nConverting to sparse labels:\r\n```\r\ntf.nn.ctc_loss(\r\n    labels=tf.sparse.from_dense(tf.ones((4, 50), dtype=tf.int32)),\r\n    logits=tf.zeros((4, 200, 10), dtype=tf.float32),\r\n    label_length=tf.constant(50, dtype=tf.int64, shape=4),\r\n    logit_length=tf.constant(200, dtype=tf.int64, shape=4),\r\n    logits_time_major=False,\r\n    blank_index=0,\r\n)\r\n```\r\nsuddenly raises \r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: cannot compute CTCLoss as input #3(zero-based) was expected to be a int32 tensor but is a int64 tensor [Op:CTCLoss]\r\n```\r\nwhich is fixed by running\r\n\r\n```\r\ntf.nn.ctc_loss(\r\n    labels=tf.sparse.from_dense(tf.ones((4, 50), dtype=tf.int32)),\r\n    logits=tf.zeros((4, 200, 10), dtype=tf.float32),\r\n    label_length=tf.constant(50, dtype=tf.int64, shape=4),\r\n    logit_length=tf.cast(tf.constant(200, dtype=tf.int64, shape=4), tf.int32),\r\n    logits_time_major=False,\r\n    blank_index=0,\r\n)\r\n```\r\n\r\nNow introducing empty label sequences:\r\n```\r\ntf.nn.ctc_loss(\r\n    labels=tf.sparse.from_dense(tf.ones((4, 0), dtype=tf.int32)),\r\n    logits=tf.zeros((4, 200, 10), dtype=tf.float32),\r\n    label_length=tf.constant(0, dtype=tf.int64, shape=4),\r\n    logit_length=tf.cast(tf.constant(200, dtype=tf.int64, shape=4), tf.int32),\r\n    logits_time_major=False,\r\n    blank_index=0,\r\n)\r\n```\r\nraises\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Labels length is zero in batch 0 [Op:CTCLoss]\r\n```\r\nwheras the dense version is fine:\r\n```\r\ntf.nn.ctc_loss(\r\n    labels=tf.ones((4, 0), dtype=tf.int32),\r\n    logits=tf.zeros((4, 200, 10), dtype=tf.float32),\r\n    label_length=tf.constant(0, dtype=tf.int64, shape=4),\r\n    logit_length=tf.cast(tf.constant(200, dtype=tf.int64, shape=4), tf.int32),\r\n    logits_time_major=False,\r\n    blank_index=0,\r\n)\r\n```", "comments": ["@f90 Could you please have a look at the gist [here](https://colab.research.google.com/gist/sushreebarsa/ac978a66eecb697559954d57e8f7fa86/untitled293.ipynb) in which code fails  when converting to sparse labels, and working fine with empty labels ? Please confirm the same.\r\nThanks!", "Yes, can confirm, I get \r\n```\r\nInvalidArgumentError: cannot compute CTCLossV2 as input #3(zero-based) was expected to be a int32 tensor but is a int64 tensor [Op:CTCLossV2]\r\n```\r\nfor box 4 (with sparse, non-empty labels)\r\n\r\nEverything else runs fine. However, you didn't include the run with sparse and empty labels that I talked about above\r\n```\r\ntf.nn.ctc_loss(\r\n    labels=tf.sparse.from_dense(tf.ones((4, 50), dtype=tf.int32)),\r\n    logits=tf.zeros((4, 200, 10), dtype=tf.float32),\r\n    label_length=tf.constant(50, dtype=tf.int64, shape=4),\r\n    logit_length=tf.cast(tf.constant(200, dtype=tf.int64, shape=4), tf.int32),\r\n    logits_time_major=False,\r\n    blank_index=0,\r\n)\r\n```\r\n", "@f90 Thank you for the quick update!\r\nI have included  the run with sparse and empty labels ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/ac978a66eecb697559954d57e8f7fa86/untitled293.ipynb#scrollTo=Rhxh24bipb4M).\r\nThanks!", "No problem. I actually still see only the dense empty [labels](https://colab.research.google.com/gist/sushreebarsa/ac978a66eecb697559954d57e8f7fa86/untitled293.ipynb#scrollTo=F2JjoYpQF8RS&line=4&uniqifier=1) in the notebook. Do you want to link to the cell that you mean?", "@f90, \r\nI could able to reproduce the issue.Could you please confirm.\r\n\r\n**Sparse Tensors**\r\n```\r\nimport tensorflow as tf\r\ntf.nn.ctc_loss(\r\n    labels=tf.sparse.from_dense(tf.ones((4, 50), dtype=tf.int32)),\r\n    logits=tf.zeros((4, 200, 10), dtype=tf.float32),\r\n    label_length=tf.constant(50, dtype=tf.int64, shape=4),\r\n    logit_length=tf.constant(200, dtype=tf.int64, shape=4),\r\n    logits_time_major=False,\r\n    blank_index=0,\r\n)\r\n```\r\n**Output**\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n[<ipython-input-2-70fe3e7f7321>](https://localhost:8080/#) in <module>()\r\n      6     logit_length=tf.constant(200, dtype=tf.int64, shape=4),\r\n      7     logits_time_major=False,\r\n----> 8     blank_index=0,\r\n      9 )\r\n\r\n1 frames\r\n[/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py](https://localhost:8080/#) in raise_from_not_ok_status(e, name)\r\n   7184 def raise_from_not_ok_status(e, name):\r\n   7185   e.message += (\" name: \" + name if name is not None else \"\")\r\n-> 7186   raise core._status_to_exception(e) from None  # pylint: disable=protected-access\r\n   7187 \r\n   7188 \r\n\r\nInvalidArgumentError: cannot compute CTCLossV2 as input #3(zero-based) was expected to be a int32 tensor but is a int64 tensor [Op:CTCLossV2]\r\n```\r\n**Sparse tensor with cast int32**\r\n```\r\nimport tensorflow as tf\r\ntf.nn.ctc_loss(\r\n    labels=tf.sparse.from_dense(tf.ones((4, 50), dtype=tf.int32)),\r\n    logits=tf.zeros((4, 200, 10), dtype=tf.float32),\r\n    label_length=tf.constant(50, dtype=tf.int64, shape=4),\r\n    logit_length=tf.cast(tf.constant(200, dtype=tf.int64, shape=4), tf.int32),\r\n    logits_time_major=False,\r\n    blank_index=0,\r\n)\r\n```\r\n**Output**\r\n`<tf.Tensor: shape=(4,), dtype=float32, numpy=array([324.07513, 324.07513, 324.07513, 324.07513], dtype=float32)>`\r\n\r\n**Sparse tensor with empty labels**\r\n```\r\nimport tensorflow as tf\r\ntf.nn.ctc_loss(\r\n    labels=tf.sparse.from_dense(tf.ones((4, 0), dtype=tf.int32)),\r\n    logits=tf.zeros((4, 200, 10), dtype=tf.float32),\r\n    label_length=tf.constant(0, dtype=tf.int64, shape=4),\r\n    logit_length=tf.cast(tf.constant(200, dtype=tf.int64, shape=4), tf.int32),\r\n    logits_time_major=False,\r\n    blank_index=0,\r\n)\r\n```\r\n**Output**\r\n`<tf.Tensor: shape=(4,), dtype=float32, numpy=array([458.2139, 458.2139, 458.2139, 458.2139], dtype=float32)>`\r\n\r\n**Dense tensor with Empty labels**\r\n```\r\nimport tensorflow as tf\r\ntf.nn.ctc_loss(\r\n    labels=tf.ones((4, 0), dtype=tf.int32),\r\n    logits=tf.zeros((4, 200, 10), dtype=tf.float32),\r\n    label_length=tf.constant(0, dtype=tf.int64, shape=4),\r\n    logit_length=tf.constant(200, dtype=tf.int64, shape=4),\r\n    logits_time_major=False,\r\n    blank_index=0,\r\n)\r\n```\r\n**Output**\r\n`<tf.Tensor: shape=(4,), dtype=float32, numpy=array([459.82333, 459.82333, 459.82333, 459.82333], dtype=float32)>`\r\n\r\nIssue is with `logit_length`, It expects `int32` Tensor.\r\n", "@gadagashwini I can confirm these results with one exception: Running empty sparse labels does not work on my end:\r\n```\r\nimport tensorflow as tf\r\ntf.nn.ctc_loss(\r\n    labels=tf.sparse.from_dense(tf.ones((4, 0), dtype=tf.int32)),\r\n    logits=tf.zeros((4, 200, 10), dtype=tf.float32),\r\n    label_length=tf.constant(0, dtype=tf.int64, shape=4),\r\n    logit_length=tf.cast(tf.constant(200, dtype=tf.int64, shape=4), tf.int32),\r\n    logits_time_major=False,\r\n    blank_index=0,\r\n)\r\n```\r\ngives me\r\n```\r\n2022-04-05 11:59:04.820963: W ./tensorflow/core/util/ctc/ctc_loss_calculator.h:499] No valid path found.\r\n2022-04-05 11:59:04.822817: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at ctc_loss_op.cc:213 : INVALID_ARGUMENT: Labels length is zero in batch 0\r\nTraceback (most recent call last):\r\n  File \"/Users/dstoller/PycharmProjects/lyric-align-baseline/lyric_align_baseline/ctc_example.py\", line 32, in <module>\r\n    blank_index=0,\r\n  File \"/Users/dstoller/.pyenv/versions/lyric-align-baseline/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"/Users/dstoller/.pyenv/versions/lyric-align-baseline/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 7186, in raise_from_not_ok_status\r\n    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Labels length is zero in batch 0 [Op:CTCLoss]\r\n```"]}, {"number": 55323, "title": "`tf.image.non_max_suppression_padded` pads indices with a valid index value (0)", "body": "`tf.image.non_max_suppression` returns a vector of surviving indices of dynamic shape.\r\nTo hold static shapes, you use `tf.image.non_max_suppression_padded` which returns a vector of surviving indices (and a scalar indicates the amount).\r\n\r\nThis vector however, is padded with zeros, to match the input shape (dimension 0).\r\nSince [0] is a valid index in the input, I find it weird and confusing -- does the first zero encountered in the surviving indices is a survivor or just a padding?\r\nFor instance, I cannot use `>=0` since that will take all indices, and I cannot use `>0` since it will skip index 0 in case it does survive\r\n```python\r\nreturn tf.where(tf.expand_dims(indices, axis=1) >= 0, x, tf.zeros_like(x))\r\n```\r\n\r\nI understand that this is solvable using the second returned value which indicates the amount of survivors, but I wish the vector itself would be sufficient.\r\n\r\nA current workaround is prepending a dummy box with confidence=0 just to make sure that valid indices start from [1].\r\n\r\nCoreML handles this issue by padding -1s -- which makes more sense, I guess that might be helpful here as well.\r\n\r\n\r\nTF 2.8.0", "comments": ["@NatanBagrov ,\r\n In order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here.\r\n", "Hi @tilakrayal and @fig666, please see the following example code, together with output.\r\n```python\r\nboxes = tf.constant([\r\n        [1., 2., 3., 4.],\r\n        [10., 20., 30., 40.],\r\n        [1., 2., 3., 4.],\r\n])\r\nscores = tf.constant([0.6, 0.8, 0.2])\r\nassert len(boxes) == len(scores)\r\n\r\nindices, num_survivors = tf.image.non_max_suppression_padded(boxes, scores,\r\n                                                             max_output_size=len(boxes),\r\n                                                             pad_to_max_output_size=True)\r\nprint('INDICES:', indices)\r\nprint('SURVIVORS:', num_survivors)\r\n```\r\n\r\nWe expect boxes [1] and [0] to survive, because both with `score>0.5` and there is no overlap between them. When looking at the output, we see this:\r\n```python\r\nINDICES: tf.Tensor([1 0 0], shape=(3,), dtype=int32)\r\nSURVIVORS: tf.Tensor(2, shape=(), dtype=int32)\r\n```\r\nIt is not possible to understand from `indices` whether box [0] survived or it is just a padding. Only when looking at `num_survivors` you see that there are 2 survivors.\r\n\r\nI can try to PR a fix if you'd like.", "@sachinprasadhs ,\r\nI was able to reproduce the issue in tf v2.8, 2.7 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/9e6dd3f8456e63e0245292ec3ad11c8b/55323.ipynb).", "Could you please create a PR and link to this issue. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 55322, "title": "[TFLite] Failed to build iOS framework: thread-local storage is not supported for the current target", "body": "Hi I use the [official guide](https://tensorflow.google.cn/lite/guide/build_ios) to build a framework for iOS, however, when I run this command:\r\n```\r\nbazel build --config=ios_fat -c opt \\\r\n  //tensorflow/lite/ios:TensorFlowLiteC_framework\r\n```\r\nI'm getting this error:\r\n\r\n```\r\nERROR: tensorflow-master/tensorflow/lite/kernels/BUILD:256:11: Compiling tensorflow/lite/kernels/eigen_support.cc failed: (Aborted): wrapped_clang_pp failed: error executing command external/local_config_cc/wrapped_clang_pp '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 -DNDEBUG ... (remaining 77 arguments skipped)\r\nIn file included from tensorflow/lite/kernels/eigen_support.cc:23:\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/eigen_spatial_convolutions.h:42:\r\nIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:45:\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/ThreadPool:67:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/ThreadPool/NonBlockingThreadPool.h:468:5: error: thread-local storage is not supported for the current target\r\n    EIGEN_THREAD_LOCAL PerThread per_thread_;\r\n    ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/ThreadPool/ThreadLocal.h:22:35: note: expanded from macro 'EIGEN_THREAD_LOCAL'\r\n#define EIGEN_THREAD_LOCAL static thread_local\r\n                                  ^\r\n1 error generated.\r\nError in child process '/usr/bin/xcrun'. 1\r\nTarget //tensorflow/lite/ios:TensorFlowLiteC_framework failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 4.019s, Critical Path: 2.75s\r\nINFO: 17 processes: 17 internal.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nMy System Info:\r\n\r\n+ tensorflow: from master branch, the latest code\r\n+ bazel: 5.0.0\r\n\r\nI think a similar issue is #18356, however the solution didn't work for me, perhaps it works for tensorflow version below 2.0.\r\n", "comments": ["@BILLXZY1215,\r\nPlease post this issue on Eigen repo for faster resolution: https://gitlab.com/libeigen/eigen/issues"]}, {"number": 55319, "title": "Refactoring of the converter for Activation operators.", "body": "This PR Refactors the converter for Activation operators. It also replaces the special implemented converters for `Relu6` and `LeakyRelu`  operator with the activation operator implementation.\r\n\r\nAdd a new check and corresponding subtests for Validation: at least 1 dimension is required for input of any Activation operation.", "comments": ["This PR should be merged after [PR#55428](https://github.com/tensorflow/tensorflow/pull/55428/): **Converter for LogicalNot operation.**", "@drivanov Can you please resolve conflicts? Thanks!", "> @drivanov Can you please resolve conflicts? Thanks!\r\n\r\nI did this yesterday but it looks like my changes are gone probably because [PR#55428](https://github.com/tensorflow/tensorflow/pull/55428) was not merged. \r\n\r\nNOTE:  [PR#55428](https://github.com/tensorflow/tensorflow/pull/55428) should be merged **BEFORE** that one.", "I edited the PR description, please check.", "Hi @drivanov Can you please check @bixia1's comments and keep us posted ? Thank you!", "@gbaned : I am done with the changes, suggested by @bixia1 "]}, {"number": 55314, "title": "[oneDNN] bug fix in optimize_for_inference", "body": "For some model, I see this error :\r\ntensorflow/python/tools/optimize_for_inference_lib.py\", line 341, in fold_batch_norms\r\nmean_value = mean_value - values_from_const(bias)\r\nValueError: operands could not be broadcast together with shapes (0,) (96,)\r\n\r\nWith the fix in this PR, I am checking the values before using them. If the shapes don't match, it will now throw a warning & continue. \r\nThis will make sure optimize_for_inference doesn't crash & other optimizations are effective.\r\n", "comments": ["@rohan100jain Can you please review this PR ? Thank you!"]}, {"number": 55304, "title": "`tf.recompute_grad` completely broken inside graph loop with big tensors", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, provided.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro 21H1\r\n- TensorFlow installed from: `pip`\r\n- TensorFlow version (use command below): `v2.8.0-rc1-32-g3f878cff5b6 2.8.0`\r\n- Python version: 3.7.8\r\n\r\n**Describe the current behavior**\r\nUse of the `tf.recompute_grad` decorator inside a graph-based loop (versus Python-based with Eager) results in invalid-argument `TypeError`s. Editing the TensorFlow source to realign the arguments then results in other errors (e.g. invoking graph-mode attributes on tensors in Eager mode).\r\n\r\n**Describe the expected behavior**\r\nNo differences (beyond what `tf.recompute_grad` is advertised to do) from when `tf.recompute_grad` is removed.\r\n\r\n**Standalone code to reproduce the issue**\r\n```py\r\nimport tensorflow as tf\r\nimport tensorflow.keras.layers as layers\r\n\r\nmodel = tf.keras.Sequential([\r\n    layers.Conv2D(8, kernel_size=(5, 5), padding='same', input_shape=(24, 24, 3)),\r\n    layers.MaxPooling2D(pool_size=(24, 24), padding='valid'),\r\n    layers.Flatten(),\r\n    layers.Dense(3),\r\n])\r\n\r\n@tf.function\r\ndef run_episode(\r\n        model: tf.keras.Model,\r\n        max_steps: int):\r\n    \r\n    outs = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\r\n    for t in tf.range(max_steps):\r\n        ins = tf.random.uniform((16, 24, 24, 3))\r\n        out = tf.recompute_grad(model)(ins)\r\n        outs = outs.write(t, out)\r\n            \r\n    return outs.stack()\r\n\r\nwith tf.GradientTape() as tape:\r\n    lp = run_episode(model, max_steps=500)\r\n    sigma = tf.reduce_sum(lp)\r\ng = tape.gradient(sigma, model.trainable_variables)\r\n```\r\n\r\n```\r\n[...]\r\nTypeError: Exception encountered when calling layer \"conv2d_2\" (type Conv2D).\r\n\r\nExpected float32 passed to parameter 'filter' of op 'Conv2D', got <tf.Variable 'conv2d_2/kernel:0' shape=(5, 5, 3, 8) dtype=float32> of type 'ResourceVariable' instead. Error: Expected resource passed to parameter 'resource' of op 'ReadVariableOp', got <tf.Tensor: shape=(), dtype=resource, value=<Resource Tensor>> of type 'EagerTensor' instead. Error: _capture_helper() takes 3 positional arguments but 4 were given\r\n\r\nCall arguments received:\r\n  \u2022 inputs=tf.Tensor(shape=(16, 24, 24, 3), dtype=float32)\r\n```", "comments": ["@chunduriv I was able to replicate the issue on colab using TF v2.8.0 and tf-nightly , please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/c35a6c318840c96bfbf530f6b795a6cb/55304.ipynb).Thanks!"]}, {"number": 55303, "title": "Code under tf.init_scope behaves differently than code manually lifted from graph.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab with TPU\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.8.0\r\n- Python version: 3.7.12\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: TPUv2\r\n\r\n**Describe the current behavior**\r\nI'm trying to implement sharding algorithm from paper https://arxiv.org/pdf/2010.05222.pdf. To emulate sharded variable I wrote wrappers for SaveableObject and tf.Variable with tf.VariableSynchronization.ON_READ and tf.VariableAggregation.NONE. At the restore from checkpoint moment I need to run Variable.assign in replica context to assign different values to shards on different TPU replicas.\r\nSaveableObject.restore called under tf.init_scope: train_function -> step_function -> train_step -> optimizer.minimize -> apply_gradients -> tf.init_scope -> optimizer._create_all_weights -> _create_slots -> add_slot -> _restore_slot_variable -> restore.\r\nThere are two problems with tf.init_scope:\r\n1) ReplicaContext.replica_id_in_sync_group is unusable. Attempt to use it leads to an error \"TypeError: <tf.Tensor 'replicated_input_0:0' shape=() dtype=int32> is out of scope and cannot be used here. Use return values, explicit Python locals or TensorFlow collections to access it.\".\r\nThis colab demonstrates the issue https://colab.research.google.com/drive/1BjRHdrWG9mM75zHe43rpOlp-pfjE0kta?usp=sharing\r\n2) Strategy.run (after merge_call) leads to an error \"NotImplementedError: tpu_shard_context cannot be nested.If you're using TPUEstimator with inference_on_tpu, make sure you have set export_saved_model_api_version=ExportSavedModelApiVersion.V2 in the creation of TPUEstimator.\"\r\nThis colab demonstrates the issue https://colab.research.google.com/drive/1MVAWCQO5F6Pf0f2R0NZJd_djC1FTzrHU?usp=sharing\r\n\r\n**Describe the expected behavior**\r\n1) tf.distribute.ReplicaContext.replica_id_in_sync_group should be usable under tf.init_scope. It should be from current graph.\r\n2) tf.distribute.Strategy.run should restart nesting counter under tf.init_scope and should be usable from tf.init_scope.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\n1) https://colab.research.google.com/drive/1BjRHdrWG9mM75zHe43rpOlp-pfjE0kta?usp=sharing\r\n2) https://colab.research.google.com/drive/1MVAWCQO5F6Pf0f2R0NZJd_djC1FTzrHU?usp=sharing\r\n", "comments": ["@API92 ,\r\n In order to expedite the trouble-shooting process, could you please provide  code dependencies you are using as I was facing error to provide `input for the checkpoints`.Thanks!\r\n", "@tilakrayal , you should enter path to your bucket in Google Cloud Storage, because tensorflow doesn't support storing checkpoints locally on TPU. This folder may be empty. For example, I used this gs://dc795ab9-90a2-4cb9-b1dd-badcf556350b/init_scope_replica_id , but it deleted now. There is no other code dependencies for notebooks.", "@API92 ,\r\nUnfortunately we can reproduce the issue only in colab. Can you please provide the required checkpoints which are required for avoiding the above mentioned error.It helps to debug the issue.Thanks!", "[model.zip](https://github.com/tensorflow/tensorflow/files/8341334/model.zip)\r\nThis checkpoints generated by notebooks itself in first call `train(strategy, False)` and used in second call `train(strategy, True)`. There is no need to use external checkpoints. But here is those checkpoints in attached model.zip.\\\r\nDon't forget to give \"Storage Admin\" role to \"allUsers\" in permissions of your bucket to use it with TPU, else you'll get access error. As you understand I can't give you my bucket because publishing bucket with read/write access to any user in public discussion would be too expensive for me."]}]